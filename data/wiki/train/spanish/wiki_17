<doc id="3256" url="https://es.wikipedia.org/wiki?curid=3256" title="Lípido">
Lípido

Los lípidos son un conjunto de moléculas orgánicas (la mayoría biomoléculas), que están constituidas principalmente por carbono e hidrógeno y en menor medida por oxígeno. También pueden contener fósforo, azufre y nitrógeno.

Debido a su estructura, son moléculas hidrófobas (insolubles en agua), pero son solubles en disolventes orgánicos no polares como la bencina, el benceno y el cloroformo lo que permite su extracción mediante este tipo de disolventes. A los lípidos se les llama incorrectamente grasas, ya que las grasas son solo un tipo de lípidos procedentes de animales y son los más ampliamente distribuidos en la naturaleza.

Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), estructural (como los fosfolípidos de las bicapas) y reguladora (como las hormonas esteroides).

Los lípidos son moléculas muy diversas; unos están formados por cadenas alifáticas saturadas o insaturadas, en general lineales, pero algunos tienen anillos (aromáticos). Algunos son flexibles, mientras que otros son rígidos o semiflexibles hasta alcanzar casi una total Flexibilidad mecánica molecular; algunos comparten carbonos libres y otros forman puentes de hidrógeno.

La mayoría de los lípidos tienen algún tipo de carácter no polar, es decir, poseen una gran parte apolar o hidrofóbico ("que le teme al agua" o "rechaza el agua"), lo que significa que no interactúa bien con solventes polares como el agua, pero sí con la gasolina, el éter o el cloroformo. Otra parte de su estructura es polar o hidrofílica ("que tiene afinidad por el agua") y tenderá a asociarse con solventes polares como el agua; cuando una molécula tiene una región hidrófoba y otra hidrófila se dice que tiene carácter de anfipático. La región hidrófoba de los lípidos es la que presenta solo átomos de carbono unidos a átomos de hidrógeno, como la larga "cola" alifática de los ácidos grasos o los anillos de esterano del colesterol; la región hidrófila es la que posee grupos polares o con cargas eléctricas, como el hidroxilo (–OH) del colesterol, el carboxilo (–COOH) de los ácidos grasos, el fosfato (–PO) de los fosfolípidos.

Los lípidos son hidrofóbicos, esto se debe a que el agua está compuesta por un átomo de oxígeno y dos de hidrógeno a su alrededor, unidos entre sí por un enlace de hidrógeno. El núcleo de oxígeno es más grande que el del hidrógeno, presentando mayor electronegatividad. Como los electrones tienen mayor carga negativa, la transacción de un átomo de oxígeno tiene una carga suficiente como para atraer a los de hidrógeno con carga opuesta, uniéndose así el hidrógeno y el agua en una estructura molecular polar.

Por otra parte, los lípidos son largas cadenas de hidrocarburos y pueden tomar ambas formas: cadenas alifáticas saturadas (un enlace simple entre diferentes enlaces de carbono) o insaturadas (unidos por enlaces dobles o triples). Esta estructura molecular es no polar.

Los lípidos son un grupo muy heterogéneo que usualmente se subdivide en dos, atendiendo a que posean en su composición ácidos grasos (lípidos saponificables) o no los posean (lípidos insaponificables):

Para que los ácidos grasos puedan ser utilizados a nivel celular se transportan en forma de triglicéridos, que consisten en una molécula de glicerol unida a tres ácidos grasos por lo que también es llamado triester de glicerilo.
Son las unidades básicas de los lípidos saponificables, y consisten en moléculas formadas por una larga cadena hidrocarbonada(CH2) con un número par de átomos de carbono (2-24) y un grupo carboxilo(COOH) terminal. La presencia de dobles enlaces en el ácido graso reduce el punto de fusión.
Los ácidos grasos se dividen en saturados e insaturados.
Los denominados ácidos grasos esenciales no pueden ser sintetizados por el organismo humano y son el ácido linoleico, el ácido linolénico y el ácido araquidónico, que deben ingerirse en la dieta.


Los acilglicéridos o acilgliceroles son ésteres de ácidos grasos con glicerol (glicerina), formados mediante una reacción de condensación llamada esterificación. Una molécula de glicerol puede reaccionar con hasta tres moléculas de ácidos grasos, puesto que tiene tres grupos hidroxilo.

Según el número de ácidos grasos que se unan a la molécula de glicerina, existen tres tipos de acilgliceroles:

Los triglicéridos constituyen la principal reserva energética de los animales, en los que constituyen las grasas; en los vegetales constituyen los aceites. El exceso de lípidos es almacenado en grandes depósitos en el tejido adiposo de los animales.

Las ceras son moléculas que se obtienen por esterificación de un ácido graso con un alcohol monovalente lineal de cadena larga. Por ejemplo la cera de abeja. Son sustancias altamente insolubles en medios acuosos y a temperatura ambiente se presentan sólidas y duras. En los animales las podemos encontrar en la superficie del cuerpo, piel, plumas, cutícula, etc. En los vegetales, las ceras recubren en la epidermis de frutos, tallos, junto con la cutícula o la suberina, que evitan la pérdida de agua por evaporación.

Los complejos además de contener carbono, hidrógeno y oxígeno, pueden tener azufre, fosfato y nitrógeno e inclusive glúcido.

Los fosfolípidos se caracterizan por poseer un grupo de naturaleza de fosfato que les otorga una marcada polaridad. Se clasifican en dos grupos, según posean glicerol o esfingosina.

Los fosfoglicéridos están compuestos por ácido fosfatídico, una molécula compleja compuesta por glicerol, al que se unen dos ácidos grasos (uno saturado y otro insaturado) y un grupo fosfato; el grupo fosfato posee un alcohol o un aminoalcohol, y el conjunto posee una marcada polaridad y forma lo que se denomina la "cabeza" polar del fosfoglicérido; los dos ácidos grasos forman las dos "colas" hidrófobas; por tanto, los fosfoglicéridos son moléculas con un fuerte carácter anfipático que les permite formar bicapas, que son la arquitectura básica de todas las membranas biológicas.

Los principales alcoholes y aminos de los fosfoglicéridos que se encuentran en las membranas biológicas son la colina (para formar la fosfatidilcolina o lecitina), la etanolamina (fosfatidiletanolamina o cefalina), serina (fosfatidilserina) y el inositol (fosfatidilinositol).

Los fosfoesfingolípidos son esfingolípidos con un grupo fosfato, tienen una arquitectura molecular y unas propiedades similares a los fosfoglicéridos. No obstante, no contienen glicerol, sino esfingosina, un aminoalcohol de cadena larga al que se unen un ácido graso, conjunto conocido con el nombre de ceramida; a dicho conjunto se le une un grupo fosfato y a este un aminoalcohol; el más abundante es la esfingomielina, en la que el ácido graso es el ácido lignocérico y el aminoalcohol la colina; es el componente principal de la vaina de mielina que recubre los axones de las neuronas.

Los glucolípidos son esfingolípidos formados por una ceramida (aminoalcohol + ácido graso) unida a un glúcido, careciendo, por tanto, de grupo fosfato. Al igual que los fosfoesfingolípidos poseen ceramida, pero a diferencia de ellos, no tienen fosfato ni alcohol. Se hallan en las bicapas lipídicas de todas las membranas celulares, y son especialmente abundantes en el tejido nervioso; el nombre de los dos tipos principales de glucolípidos alude a este hecho:

Los glucolípidos se localizan en la cara externa de la bicapa de las membranas celulares donde actúan de receptores.

Los terpenos, terpenoides o isoprenoides, son lípidos derivados del hidrocarburo isopreno (o 2-metil-1,3-butadieno). Los terpenos biológicos constan, como mínimo de dos moléculas de isopreno. Algunos terpenos importantes son los aceites esenciales (mentol, limoneno, geraniol), el fitol (que forma parte de la molécula de clorofila), las vitaminas A, K y E, los carotenoides (que son pigmentos fotosintéticos) y el caucho (que se obtiene del árbol "Hevea brasiliensis"). Desde el punto de vista farmacéutico, los grupos de principios activos de naturaleza terpénica más interesantes son: monoterpenos y sesquiterpenos constituyentes de los aceites esenciales, derivados de monoterpenos correspondientes a los iridoides, lactonas sesquiterpénicas que forman parte de los principios amargos, algunos diterpenos que poseen actividades farmacológicas de aplicación a la terapéutica y por último, triterpenos y esteroides entre los cuales se encuentran las saponinas y los heterósidos cardiotónicos.

Los esteroides son lípidos derivados del núcleo del hidrocarburo esterano (o ciclopentanoperhidrofenantreno), esto es, se componen de cuatro anillos fusionados de carbono que posee diversos grupos funcionales (carbonilo, hidroxilo) por lo que la molécula tiene partes hidrofílicas e hidrofóbicas (carácter anfipático).

Entre los esteroides más destacados se encuentran los ácidos biliares, las hormonas sexuales, las corticosteroides, la vitamina D y el colesterol. El colesterol es el precursor de numerosos esteroides y es un componente más de la bicapa de las membranas celulares. Esteroides Anabólicos es la forma como se conoce a las substancias sintéticas basadas en hormonas sexuales masculinas (andrógenos). Estas hormonas promueven el crecimiento de músculos (efecto anabólico) así como también en desarrollo de las características sexuales masculinas (efecto andrógeno).

Los esteroides anabólicos fueron desarrollados a finales de 1930 principalmente para tratar el Hipogonadismo, una condición en la cual los testículos no producen suficiente testosterona para garantizar un crecimiento, desarrollo y función sexual normal del individuo. Precisamente a finales de 1930 los científicos también descubrieron que estos esteroides facilitaban el crecimiento de músculos en los animales de laboratorio, lo cual llevó al uso de estas sustancias por parte de físicoculturistas y levantadores de pesas y después por atletas de otras especialidades.

El abuso de los esteroides se ha diseminado tanto que hoy en día afecta el resultado de los eventos deportivos.

Los eicosanoides o prostaglandinas son lípidos derivados de los ácidos grasos esenciales de 20 carbonos tipo omega-3 y omega-6. Los principales precursores de los eicosanoides son el ácido araquidónico, el ácido linoleico y el ácido linolénico. Todos los eicosanoides son moléculas de 20 átomos de carbono y pueden clasificarse en tres tipos: prostaglandinas, tromboxanos y leucotrienos.

Cumplen amplias funciones como mediadores para el sistema nervioso central, los procesos de la inflamación y de la respuesta inmune tanto de vertebrados como invertebrados. Constituyen las moléculas involucradas en las redes de comunicación celular más complejas del organismo animal, incluyendo el hombre.

Los lípidos desempeñan diferentes tipos de funciones biológicas:

Las vitaminas A, D, E y K son liposolubles, lo que significa que solo pueden ser digeridas, absorbidas y transportadas junto con las grasas. Las grasas juegan un papel vital en el mantenimiento de una piel y cabellos saludables, en el aislamiento de los órganos corporales contra el shock, en el mantenimiento de la temperatura corporal y promoviendo la función celular saludable. Además, sirven como reserva energética para el organismo. Las grasas son degradadas en el organismo para liberar glicerol y ácidos grasos libres.

El contenido de grasas de los alimentos puede ser analizado por extracción. El método exacto varía según el tipo de grasa a analizar. Por ejemplo, las grasas poliinsaturadas y monoinsaturadas son analizadas de forma muy diferente.

Las grasas también pueden servir como un tampón muy útil de una gran cantidad de sustancias extrañas. Cuando una sustancia particular, sea química o biótica, alcanza niveles no seguros en el torrente sanguíneo, el organismo puede efectivamente diluir (o al menos mantener un equilibrio) estas sustancias dañinas almacenándolas en nuevo tejido adiposo. Esto ayuda a proteger órganos vitales, hasta que la sustancia dañina pueda ser metabolizada o retirada de la sangre a través de la excreción, orina, desangramiento accidental o intencional, excreción de sebo y crecimiento del pelo.

Es prácticamente imposible eliminar completamente las grasas de la dieta, y, además, sería equivocado hacerlo. Algunos ácidos grasos son nutrientes esenciales, significando esto que ellos no pueden ser producidos en el organismo a partir de otros componentes y por lo tanto necesitan ser consumidos mediante la dieta. Todas las demás grasas requeridas por el organismo no son esenciales y pueden ser producidas en el organismo a partir de otros componentes.

El tejido adiposo o graso es el medio utilizado por el organismo humano para almacenar energía a lo largo de extensos períodos de tiempo. Dependiendo de las condiciones fisiológicas actuales, los adipocitos almacenan triglicéridos derivadas de la dieta y el metabolismo hepático o degrada las grasas almacenadas para proveer ácidos grasos y glicerol a la circulación. Estas actividades metabólicas son reguladas por varias hormonas (insulina, glucagón y epinefrina). La localización del tejido determina su perfil metabólico: la grasa visceral está localizada dentro de la pared abdominal (debajo de los músculos de la pared abdominal) mientras que la grasa subcutánea está localizada debajo de la piel (incluye la grasa que está localizada en el área abdominal debajo de la piel pero por encima de los músculos de la pared abdominal).




</doc>
<doc id="3258" url="https://es.wikipedia.org/wiki?curid=3258" title="Ácido ribonucleico">
Ácido ribonucleico

El ácido ribonucleico (ARN o RNA) es un ácido nucleico formado por una cadena de ribonucleótidos. Está presente tanto en las células procariotas como en los eucariotas, y es el único material genético de ciertos virus (virus ARN).

El ARN se puede definir como la molécula formada por una cadena simple de ribonucleótidos, cada uno de ellos formado por ribosa, un fosfato y una de las cuatro bases nitrogenadas (adenina, guanina, citosina y uracilo). El ARN celular es lineal y monocatenario (de una sola cadena), pero en el genoma de algunos virus es de doble hebra.

En los organismos celulares desempeña diversas funciones. Es la molécula que dirige las etapas intermedias de la síntesis proteica; el ADN no puede actuar solo, y se vale del ARN para transferir esta información vital durante la síntesis de proteínas (producción de las proteínas que necesita la célula para sus actividades y su desarrollo). Varios tipos de ARN regulan la expresión génica, mientras que otros tienen actividad catalítica. El ARN es, pues, mucho más versátil que el ADN.

Los ácidos nucleicos fueron descubiertos en 1867 por Friedrich Miescher, que los llamó nucleína ya que los aisló del núcleo celular. Más tarde, se comprobó que las células procariotas, que carecen de núcleo, también contenían ácidos nucleicos. El papel del ARN en la síntesis de proteínas fue sospechado en 1939. Severo Ochoa ganó el Premio Nobel de Medicina en 1959 tras descubrir cómo se sintetizaba el ARN.

En 1965 Robert W. Holley halló la secuencia de 77 nucleótidos de un ARN de transferencia de una levadura, con lo que obtuvo el Premio Nobel de Medicina en 1968. En 1967, Carl Woese comprobó las propiedades catalíticas de algunos ARN y sugirió que las primeras formas de vida usaron ARN como portador de la información genética tanto como catalizador de sus reacciones metabólicas (hipótesis del mundo de ARN). En 1976, Walter Fiers y sus colaboradores determinaron la secuencia completa del ARN del genoma de un virus ARN (bacteriófago MS2).

En 1990 se descubrió en "Petunia" que genes introducidos pueden silenciar genes similares de la misma planta, lo que condujo al descubrimiento del ARN interferente. Aproximadamente al mismo tiempo se hallaron los micro ARN, pequeñas moléculas de 22 nucleótidos que tenían algún papel en el desarrollo de "Caenorhabditis elegans". El descubrimiento de ARN que regulan la expresión génica ha permitido el desarrollo de medicamentos hechos de ARN, como los ARN pequeños de interferencia que silencian genes.
En el año 2016 se tiene prácticamente por comprobado que las moléculas de ARN fueron la primera forma de vida propiamente dicha en habitar el planeta Tierra (Hipótesis del mundo de ARN).

Como el ADN, el ARN está formado por una cadena de monómeros repetitivos llamados nucleótidos. Los nucleótidos se unen uno tras otro mediante enlaces fosfodiéster cargados negativamente.

Cada nucleótido está formado por tres componentes:

Los carbonos de la ribosa se numeran de 1' a 5' en sentido horario. La base nitrogenada se une al carbono 1'; el grupo fosfato se une al carbono 5' y al carbono 3' de la ribosa del siguiente nucleótido. El pico tiene una carga negativa a pH fisiológico lo que confiere al ARN carácter polianiónico. Las bases púricas (adenina y guanina) pueden formar puentes de hidrógeno con las pirimidínicas (uracilo y citosina) según el esquema C=G y A=U. Además, son posibles otras interacciones, como el apilamiento de bases o tetrabucles con apareamientos G=A.
Muchos ARN contienen además de los nucleótidos habituales, nucleótidos modificados, que se originan por transformación de los nucleótidos típicos; son característicos de los ARN de transferencia (ARNt) y el ARN ribosómico (ARNr); también se encuentran nucleótidos metilados en el ARN mensajero eucariótico.

La interacción por puentes de hidrógeno descrita por Watson y Crick forma pares de bases entre una purina y una pirimidina. A este patrón se le conoce como apareamiento Watson y Crick. En éste, la adenina se aparea con el uracilo (timina, en ADN) y la citosina con la guanina. Sin embargo en el ARN se presentan muchas otras formas de apareamiento, de las cuales la más ubicua es el apareamiento wobble (también apareamiento por balanceo o apareamiento titubeante) para la pareja G-U. Éste fue propuesto por primera vez por Crick para explicar el apareamiento codón-anticodón en los tRNAs y ha sido confirmado en casi todas las clases de RNA en los tres dominios filogenéticos.

Se refiere a la secuencia lineal de nucleótidos en la molécula de ARN. Los siguientes niveles estructurales (estructura secundaria, terciaria) son consecuencia de la estructura primaria. Además, la secuencia misma puede ser información funcional; ésta puede traducirse para sintetizar proteínas (en el caso del mRNA) o funcionar como región de reconocimiento, región catalítica, entre otras. <br>
<br>
<br>

El ARN se pliega como resultado de la presencia de regiones cortas con apareamiento intramolecular de bases, es decir, pares de bases formados por secuencias complementarias más o menos distantes dentro de la misma hebra. La estructura secundaria se refiere, entonces, a las relaciones de apareamiento de bases: «El término ‘estructura secundaria’ denota cualquier patrón plano de contactos por apareamiento de bases. Es un concepto topológico y no debe ser confundido con algún tipo de estructura bidimensional». La estructura secundaria puede ser descrita a partir de "motivos estructurales" que se suelen clasificar de la siguiente manera:

La estructura terciaria es el resultado de las interacciones en el espacio entre los átomos que conforman la molécula. Algunas interacciones de este tipo incluyen el apilamiento de bases y los apareamientos de bases distintos a los propuestos por Watson y Crick, como el apareamiento Hoogsteen, los apareamientos triples y los zippers de ribosa. 

A diferencia del ADN las moléculas de ARN suelen ser de cadena simple y no forman dobles hélices extensas, no obstante, en las regiones con bases apareadas sí forma hélices como motivo estructural terciario. Una importante característica estructural del ARN que lo distingue del ADN es la presencia de un grupo hidroxil en posición 2' de la ribosa, que causa que las dobles hélices de ARN adopten una conformación A, en vez de la conformación B que es la más común en el ADN. Esta hélice A tiene un surco mayor muy profundo y estrecho y un surco menor amplio y superficial. Una segunda consecuencia de la presencia de dicho hidroxilo es que los enlaces fosfodiéster del ARN de las regiones en que no se forma doble hélice son más susceptibles de hidrólisis química que los del ADN; los enlaces fosfodiéster del ARN se hidrolizan rápidamente en disolución alcalina, mientras que los enlaces del ADN son estables. La vida media de las moléculas de ARN es mucho más corta que las del ADN, de unos minutos en algunos ARN bacterianos o de unos días en los ARNt humanos.

La biosíntesis de ARN está catalizada normalmente por la enzima ARN polimerasa que usa una hebra de ADN como molde, proceso conocido con el nombre de transcripción. Por tanto, todos los ARN celulares provienen de copias de genes presentes en el ADN.

La transcripción comienza con el reconocimiento por parte de la enzima de un promotor, una secuencia característica de nucleótidos en el ADN situada antes del segmento que va a transcribirse; la doble hélice del ADN es abierta por la actividad helicasa de la propia enzima. A continuación, la ARN polimerasa progresa a lo largo de la hebra de ADN en sentido 3' → 5', sintetizando una molécula complementaria de ARN; este proceso se conoce como elongación, y el crecimiento de la molécula de ARN se produce en sentido 5' → 3'. La secuencia de nucleótidos del ADN determina también dónde acaba la síntesis del ARN, gracias a que posee secuencias características que la ARN polimerasa reconoce como señales de terminación.

Tras la transcripción, la mayoría de los ARN son modificados por enzimas. Por ejemplo, al pre-ARN mensajero eucariota recién transcrito se le añade un nucleótido de guanina modificado (7-Metilguanosina) en el extremo 5' por medio de un puente de trifosfato formando un enlace 5'→ 5' único, también conocido como "capucha" o "caperuza", y una larga secuencia de nucleótidos de adenina en el extremo 3' (cola poli-A); posteriormente se le eliminan los intrones (segmentos no codificantes) en un proceso conocido como "splicing".

En virus, hay también varias ARN polimerasas ARN-dependientes que usan ARN como molde para la síntesis de nuevas moléculas de ARN. Por ejemplo, varios virus ARN, como los poliovirus, usan este tipo de enzimas para replicar su genoma.

El ARN mensajero (ARNm) es el tipo de ARN que lleva la información del ADN a los ribosomas, el lugar de la síntesis de proteínas. La secuencia de nucleótidos del ARNm determina la secuencia de aminoácidos de la proteína. Por ello, el ARNm es denominado ARN codificante.

No obstante, muchos ARN no codifican proteínas, y reciben el nombre de ARN no codificantes; se originan a partir de genes propios (genes ARN), o son los intrones rechazados durante el proceso de "splicing". Son ARN no codificantes el ARN de transferencia (ARNt) y el ARN ribosómico (ARNr), que son elementos fundamentales en el proceso de traducción, y diversos tipos de ARN reguladores.

Ciertos ARN no codificantes, denominados ribozimas, son capaces de catalizar reacciones químicas como cortar y unir otras moléculas de ARN, o formar enlaces peptídicos entre aminoácidos en el ribosoma durante la síntesis de proteínas.

El ARN mensajero (ARNm o RNAm) lleva la información sobre la secuencia de aminoácidos de la proteína desde el ADN, lugar en que está inscrita, hasta el ribosoma, lugar en que se sintetizan las proteínas de la célula. Es, por tanto, una molécula intermediaria entre el ADN y la proteína y apelativo de "mensajero" es del todo descriptivo. En eucariotas, el ARNm se sintetiza en el nucleoplasma del núcleo celular y donde es procesado antes de acceder al citosol, donde se hallan los ribosomas, a través de los poros de la envoltura nuclear.

Los ARN de transferencia (ARNt o tRNA) son cortos polímeros de unos 80 nucleótidos que transfiere un aminoácido específico al polipéptido en crecimiento; se unen a lugares específicos del ribosoma durante la traducción. Tienen un sitio específico para la fijación del aminoácido (extremo 3') y un anticodón formado por un triplete de nucleótidos que se une al codón complementario del ARNm mediante puentes de hidrógeno. Estos ARNt, al igual que otros tipos de ARN, pueden ser modificados post-transcripcionalmente por enzimas. La modificación de alguna de sus bases es crucial para la descodificación de ARNm y para mantener la estructura tridimensional del ARNt.

El ARN ribosómico o ribosomal (ARNr o RNAr) se halla combinado con proteínas para formar los ribosomas, donde representa unas 2/3 partes de los mismos. En procariotas, la subunidad mayor del ribosoma contiene dos moléculas de ARNr y la subunidad menor, una. En los eucariotas, la subunidad mayor contiene tres moléculas de ARNr y la menor, una. En ambos casos, sobre el armazón constituido por los ARNm se asocian proteínas específicas. El ARNr es muy abundante y representa el 80 % del ARN hallado en el citoplasma de las células eucariotas. Los ARN ribosómicos son el componente catalítico de los ribosomas; se encargan de crear los enlaces peptídicos entre los aminoácidos del polipéptido en formación durante la síntesis de proteínas; actúan, pues, como ribozimas.

Muchos tipos de ARN regulan la expresión génica gracias a que son complementarios de regiones específicas del ARNm o de genes del ADN.

Los ARN interferentes (ARNi o iRNA) son moléculas de ARN que suprimen la expresión de genes específicos mediante mecanismos conocidos globalmente como ribointerferencia o interferencia por ARN. Los ARN interferentes son moléculas pequeñas (de 20 a 25 nucléotidos) que se generan por fragmentación de precursores más largos. Se pueden clasificar en tres grandes grupos:

Los micro ARN (miARN o RNAmi) son cadenas cortas de 21 o 22 nucleótidos hallados en células eucariotas que se generan a partir de precursores específicos codificados en el genoma. Al transcribirse, se pliegan en horquillas intramoleculares y luego se unen a enzimas formando un complejo efector que puede bloquear la traducción del ARNm o acelerar su degradación comenzando por la eliminación enzimática de la cola poli A.

Los ARN interferentes pequeños (ARNip o siARN), formados por 20-25 nucleótidos, se producen con frecuencia por rotura de ARN virales, pero pueden ser también de origen endógeno. Tras la transcripción se ensambla en un complejo proteico denominado RISC ("RNA-induced silencing complex") que identifica el ARNm complementario que es cortado en dos mitades que son degradadas por la maquinaria celular, bloquean así la expresión del gen.

Los ARN asociados a Piwi son cadenas de 29-30 nucleótidos, propias de animales; se generan a partir de precursores largos monocatenarios (formados por una sola cadena), en un proceso que es independiente de Drosha y Dicer. Estos ARN pequeños se asocian con una subfamilia de las proteínas "Argonauta" denominada proteínas Piwi. Son activos las células de la línea germinal; se cree que son un sistema defensivo contra los transposones y que juegan algún papel en la gametogénesis.

Un ARN antisentido es la hebra complementaria (no codificadora) de un hebra ARNm (codificadora). La mayoría inhiben genes, pero unos pocos activan la transcripción. El ARN antisentido se aparea con su ARNm complementario formando una molécula de doble hebra que no puede traducirse y es degradada enzimáticamente. La introducción de un transgen codificante para un ARNm antisentido es una técnica usada para bloquear la expresión de un gen de interés. Un mARN antisentido marcado radioactivamente puede usarse para mostrar el nivel de transcripción de genes en varios tipos de células. Algunos tipos estructurales antisentidos son experimentales, ya que se usan como terapia antisentido.

Muchos ARN largos no codificantes (ARNnc largo o long ncARN) regulan la expresión génica en eucariotas; uno de ellos es el Xist que recubre uno de los dos cromosomas X en las hembras de los mamíferos inactivándolo (corpúsculo de Barr).
Diversos estudios revelan que es activo a bajos niveles. En determinadas poblaciones celulares, una cuarta parte de los genes que codifican para proteínas y el 80 % de los lncRNA detectados en el genoma humano están presentes en una o ninguna copia por célula, ya que existe una restricción en determinados ARN.

Un riboswitch es una parte del ARNm (ácido ribonucleico mensajero) al cual pueden unirse pequeñas moléculas que afectan la actividad del gen. Por tanto, un ARNm que contenga un riboswitch está directamente implicado en la regulación de su propia actividad que depende de la presencia o ausencia de la molécula señalizadora. Tales riboswitchs se hallan en la región no traducida 5' (5'-UTR), situada antes del codón de inicio (AUG), y/o en la región no traducida 3' (3'-UTR), también llamada secuencia de arrastre, situada entre el codón de terminación (UAG, UAA o UGA) y la cola poli A.

El ARN puede actuar como biocatalizador. Ciertos ARN se asocian a proteínas formando ribonucleoproteínas y se ha comprobado que es la subunidad de ARN la que lleva a cabo las reacciones catalíticas; estos ARN realizan las reacciones "in vitro" en ausencia de proteína. Se conocen cinco tipos de ribozimas; tres de ellos llevan a cabo reacciones de automodificación, como eliminación de intrones o autocorte, mientras que los otros (ribonucleasa P y ARN ribosómico) actúan sobre substratos distintos. Así, la ribonucleasa P corta un ARN precursor en moléculas de ARNt, mientras que el ARN ribosómico realiza el enlace peptídico durante la síntesis proteica ribosomal.

Los intrones son separados del pre-ARNm durante el proceso conocido como "splicing" por los espliceosomas, que contienen numerosos ARN pequeños nucleares (ARNpn o snRNA). En otros casos, los propios intrones actúan como ribozimas y se separan a si mismos de los exones.

Los ARN pequeños nucleolares (ARNpno o snoRNA), hallados en el nucléolo y en los cuerpos de Cajal, dirigen la modificación de nucleótidos de otros ARN; el proceso consiste en transformar alguna de las cuatro bases nitrogenadas típicas (A, C, U, G) en otras. Los ARNpno se asocian con enzimas y los guían apareándose con secuencias específicas del ARN al que modificarán. Los ARNr y los ARNt contienen muchos nucleótidos modificados.

La mitocondrias tienen su propio aparato de síntesis proteica, que incluye ARNr (en los ribosomas), ARNt y ARNm.
Los ARN mitocondriales (ARNmt o mtARN) representan el 4 % del ARN celular total. Son transcritos por una ARN polimerasa mitocondrial específica.

El ADN es la molécula portadora de la información genética en todos los organismos celulares, pero, al igual que el ADN, el ARN puede guardar información genética. Los virus ARN carecen por completo de ADN y su genoma está formado por ARN, el cual codifica las proteínas del virus, como las de la cápside y algunos enzimas. Dichos enzimas realizan la replicación del genoma vírico. Los viroides son otro tipo de patógenos que consisten exclusivamente en una molécula de ARN que no codifica ninguna proteína y que es replicado por la maquinaria de la célula hospedadora.

La hipótesis del mundo de ARN propone que el ARN fue la primera forma de vida en la Tierra, desarrollando posteriormente una membrana celular a su alrededor y convirtiéndose así en la primera célula. Se basa en la comprobación de que el ARN puede contener información genética, de un modo análogo a como lo hace el ADN, y que algunos tipos son capaces de llevar a cabo reacciones metabólicas, como autocorte o formación de enlaces peptídicos.

Durante años se especuló en qué fue primero, el ADN o las enzimas, ya que las enzimas se sintetizan a partir del ADN y la síntesis de ADN es llevada a cabo por enzimas. Si se supone que las primeras formas de vida usaron el ARN tanto para almacenar su información genética como realizar su metabolismo, se supera este escollo. Experimentos con los ribozimas básicos, como el ARN viral Q-beta, han demostrado que las estructuras de ARN autorreplicantes sencillas pueden resistir incluso a fuertes presiones selectivas (como los terminadores de cadena de quiralidad opuesta).



</doc>
<doc id="3260" url="https://es.wikipedia.org/wiki?curid=3260" title="Ribosoma">
Ribosoma

Los ribosomas son complejos macromoleculares de proteínas y ácido ribonucleico (ARN) presentes en todas las células (excepto en los espermatozoides). Son los centros celulares de traducción que hacen posible la expresión de los genes. Es decir, se encargan de sintetizar proteínas a partir de la información contenida en el ADN, que llega transcrita a los ribosomas en forma de ARN mensajero (ARNm).

Los ribosomas son responsables por la síntesis de proteínas, en un proceso conocido como traducción. La información necesaria para esa síntesis se encuentra en el ARN mensajero (ARNm), cuya secuencia de nucleótidos determina la secuencia de aminoácidos de la proteína; a su vez, la secuencia del ARNm proviene de la transcripción de un gen del ADN. El ARN de transferencia lleva los aminoácidos a los ribosomas donde se incorporan al polipéptido en crecimiento.

El ribosoma lee el ARN mensajero y ensambla los aminoácidos suministrados por los ARN de transferencia a la proteína en crecimiento, proceso conocido como traducción o síntesis de proteínas.

Todas las proteínas están formadas por aminoácidos. Entre los seres vivos se han descubierto hasta ahora 20 aminoácidos. En el código genético, cada aminoácido está codificado por uno o varios codones. En total hay 64 codones que codifican 20 aminoácidos y 3 señales de parada de la traducción. Esto hace que el código sea redundante y que haya varios codones diferentes para un mismo aminoácido.

La traducción comienza, en general, con el codón AUG que codifica el aminoácido metionina. Al final de la secuencia se ubica un codón que indica el final de la proteína; es el codón de terminación. El código genético es "universal" porque cada codón codifica el mismo aminoácido para la mayoría de los organismos (no todos).

El ribosoma consta de dos partes, la subunidad mayor y una menor, estas salen del núcleo celular por separado. Las subunidades se mantienen unidas por cargas. Al disminuir experimentalmente la concentración de Mg, las subunidades tienden a separarse.

Por ejemplo, en el citoplasma de una célula eucariota, el proceso con la siguiente secuencia de ARN mensajero sería este:


Por tanto, la cadena polipeptídica ensamblada ha sido: Alanina-Asparagina-Glicina-Metionina-Prolina-Treonina.

Se les encuentra en el citosol, en las mitocondrias, en el retículo endoplasmático y en los cloroplastos. Sólo son visibles al microscopio electrónico, debido a su reducido tamaño (29 nm en células procariotas y 32 nm en eucariotas). Bajo el microscopio electrónico se observan como estructuras redondeadas, densas a los electrones. Bajo el microscopio óptico se observa que son los responsables de la basofilia que presentan algunas células. Los "ribosomas" están considerados en muchos textos como orgánulos no membranosos, ya que no existen endomembranas en su estructura, aunque otros biólogos no los consideran orgánulos propiamente por esta misma razón.

Están formados por ARN ribosómico (ARNr) y por proteínas. Estructuralmente, tienen siempre dos subunidades: la mayor o grande y la menor o pequeña. En las células, estas macromoléculas aparecen en diferentes estados de disociación. Cuando están completas, pueden estar aisladas o formando grupos (polisomas). En células eucariotas, los ribosomas se elaboran en el núcleo pero desempeñan su función de síntesis en el citosol. Las proteínas sintetizadas por los ribosomas actúan principalmente en el citosol; también pueden aparecer asociados al retículo endoplasmático rugoso o a la membrana nuclear externa, y las proteínas que sintetizan son sobre todo para la secreción.

Tanto el ARNr como las subunidades de los ribosomas se suelen nombrar por su coeficiente de sedimentación en unidades Svedberg. En las células eucariotas, los ribosomas del citoplasma alcanzan 80 S. En plastos de eucariotas, así como en procariotas, son 70 S. Los ribosomas mitocondriales son de tamaño variado, entre 55 y 70 S.

En la célula procariota, los ribosomas tienen un coeficiente de sedimentación de 70 S. Contienen un 66% de ARNr y se dividen en dos subunidades de distinto tamaño:



En la célula eucariota, los ribosomas tienen un coeficiente de sedimentación de 80 S. Su peso molecular es de 4.194 Kd. Contienen un 60% de ARNr y 40% de proteínas. Al igual que los procariotas se dividen en dos subunidades de distinto tamaño:



Los ribosomas mitocondriales o "mitorribosomas" junto con ARNt y ARNm, son parte del aparato propio de síntesis proteica que tienen las mitocondrias. Son de tamaño variable, desde los 50S de Leishmania hasta 72S en Candida. Los mitorribosomas de las células animales son 55S y sus dos tipos de ARN ribosómicos, el 12S y 16S, se transcriben a partir de genes del ADN mitocondrial, y son transcritos por una ARN polimerasa mitocondrial específica. Todas las proteínas que forman parte de los ribosomas mitocondriales están codificadas por genes del núcleo celular, que son traducidos en el citosol y transportados hasta las mitocondrias.

Los ribosomas que aparecen en plastos son similares a los procariotas. Son, al igual que los procariotas, 70 S, pero en la subunidad mayor hay un ARNr de 4 S que es equivalente al 5 S procariota.

La subunidad mayor 50S tiene unas 33 proteínas y la subunidad menor 30S tiene unas 25 proteínas. La gran mayoría de estas proteínas son homólogas (ortólogas) a las proteínas ribosomales bacterianas y unas pocas son específicas de los cloroplastos.

Los ribosomas fueron observados por primera ocasión por el biólogo celular rumano George Emil Palade a mediados de la década de 1950, para lo cuál usó el microscopio electrónico. El término "ribosoma" fue propuesto por el científico Richard B. Roberts en 1958.

En 1974 Albert Claude, Christian de Duve y George Emil Palade recibieron el premio Nobel en la categoría de fisiología o medicina por su descubrimiento. El premio Nobel de química de 2009 fue entregado a Venkatraman Ramakrishnan, Thomas A. Steitz y Ada E. Yonath por motivo de la especificación detallada de la estructura y mecanismo de operación del ribosoma.

El ribosoma podría haber aparecido en un mundo de ARN, primero como un complejo autoreplicante que después evolucionó con la habilidad para sintetizar aminoácidos. Estudios sugieren que un ribosoma compuesto únicamente de ARNr sería capaz de propiciar enlaces peptídicos. Adicionalmente, otras evidencias aducen la autosuficiencia genética de los ribosomas, característica ausente en el complejo aparato de replicación del ADN. El ARNr funge como catalizador de su propia replicación, haciendo uso de su capacidad para codificar y sintetizar ARNt y proteínas para llevarla a cabo. Conforme fueron apareciendo aminoácidos en las condiciones prebióticas del mundo de ARN, sus interacciones con el ARN catalítico habrían conferido a este último con mayor alcance y eficiencia. Bajo esta rúbrica, la fuerza motora para la evolución del ribosoma actual a partir de una máquina autoreplicante arcaica podría haber sido la presión selectiva por incorporar proteínas a la maquinaria, de manera que aumentara su capacidad de autoreplicación.



</doc>
<doc id="3261" url="https://es.wikipedia.org/wiki?curid=3261" title="Célula eucariota">
Célula eucariota

Se llama células eucariotas —del griego "eu",'verdadero', y "karyon", ‘nuez’ o ‘núcleo’— a las que tienen un citoplasma, compartimentado por membranas, destacando la existencia de un núcleo celular organizado, limitado por una envoltura nuclear, en el cual está contenido el material hereditario, que incluye al ADN y es la base de la herencia; se distinguen así de las células procariotas que carecen de núcleo definido, por lo que el material genético se encuentra disperso en su citoplasma. A los organismos formados por células eucariotas se los denomina eucariontes.

El paso de procariotas a eucariotas significó el gran salto en complejidad de la vida y uno de los más importantes de su evolución. Sin este paso, sin la complejidad que adquirieron las células eucariotas no habrían sido posibles ulteriores pasos como la aparición de los seres pluricelulares; la vida, probablemente, se habría limitado a constituirse en un conglomerado de bacterias. De hecho, a excepción de procariotas (del que proceden), los cuatro reinos restantes (animales, plantas, hongos y protistas) son el resultado de ese salto cualitativo. El éxito de estas células eucariotas posibilitó las posteriores radiaciones adaptativas de la vida que han desembocado en la gran variedad de especies que existe en la actualidad.

Las células eucariotas presentan un citoplasma organizado en compartimentos, con orgánulos (semimembranosos) separados o interconectados, limitados por membranas biológicas que tienen la misma naturaleza que la membrana plasmática. El núcleo es el más notable y característico de los compartimentos en que se divide el protoplasma, es decir, la parte activa de la célula. En el núcleo se encuentra el material genético en forma de cromosomas. Desde este se da toda la información necesaria para que se lleve a cabo todos los procesos tanto intracelulares como fuera de la célula, es decir, en el organismo en sí.

En el protoplasma distinguimos tres componentes principales, a saber la "membrana plasmática", el "núcleo" y el "citoplasma", constituido por todo lo demás. Las células eucariotas están dotadas en su citoplasma de un citoesqueleto complejo, muy estructurado y dinámico, formado por microtúbulos y diversos filamentos proteicos. Además puede haber pared celular, que es lo típico de plantas, hongos y protistas pluricelulares, o algún otro tipo de recubrimiento externo al protoplasma.

Para su comparación con la célula procariota, véase la "Tabla comparativa"

Las células eucariotas contienen en principio mitocondrias, orgánulos que habrían adquirido por endosimbiosis de ciertas bacterias primitivas, lo que les dota de la capacidad de desarrollar un metabolismo aerobio. Sin embargo, en algunas eucariotas del reino protistas las mitocondrias han desaparecido secundariamente en el curso de la evolución, en general derivando a otros orgánulos, como los hidrogenosomas.

Algunos eucariontes realizan la fotosíntesis, A diferencia de la célula animal, gracias a la presencia en su citoplasma de orgánulos llamados plastos, los cuales derivan por endosimbiosis de bacterias del grupo denominado cianobacterias (algas azules).

Aunque demuestran una diversidad increíble en su forma, comparten las características fundamentales de su organización celular, arriba resumidas, y una gran catálisis homogénea en lo relativo a su bioquímica (composición), y metabolismo, que contrasta con la inmensa heterogeneidad que en este terreno presentan los procariontes (bacteria en sentido amplio).

El origen de los eucariontes es un complejo proceso que tiene un origen procariota. Si bien hay varias teorías que explican este proceso, según la mayoría de estudios se produjo por endosimbiosis entre varios organismos procariotas, en donde el ancestro principal protoeucariota es de tipo arqueano y las mitocondrias y cloroplastos son de origen bacteriano. Es discutible la incorporación de otros organismos procariotas. La teoría más difundida al respecto es la endosimbiosis seriada, postulada por Lynn Margulis.

Los organismos eucariontes forman el dominio Eukarya que incluye a los organismos más conocidos, repartidos en cuatro reinos: Animalia (animales), Plantae (plantas), Fungi (Hongos) y Protista (que no pueden clasificarse dentro de los tres primeros reinos). Incluyen a la gran mayoría de los organismos extintos morfológicamente reconocibles que estudian los paleontólogos. Los ejemplos de la disparidad eucariótica van desde un dinoflagelado (un protista unicelular fotosintetizador), un árbol como la sequoia, un calamar, o un racimo de setas (órganos reproductivos de hongos), cada uno con células distintas y, en el caso de los pluricelulares, a menudo muy variadas.

Existen diversos tipos de células eucariotas entre las que destacan las células de animales y plantas. Los hongos y muchos protistas tienen, sin embargo, algunas diferencias substanciales.

Las células animales componen los tejidos de los animales y se distinguen de las células vegetales en que carecen de paredes celulares y de cloroplastos y poseen centriolos y vacuolas más pequeñas y, generalmente, más abundantes. Debido a la carencia de pared celular rígida, las células animales pueden adoptar variedad de formas e incluso pueden fagocitar otras estructuras.

Las características distintivas de las células de las plantas son:


Las células de los hongos, en su mayor parte, son similares a las células animales, con las excepciones siguientes:


Algunos organismos Protistas, son conformados por una única célula que pueden alcanzar tamaños macroscópicos (el organismo unicelular "Syringammina fragilissima" alcanza los 20 cm de diámetro).

Las células eucariotas se pueden reproducir de tres maneras distintas, principalmente:






</doc>
<doc id="3263" url="https://es.wikipedia.org/wiki?curid=3263" title="Codón">
Codón

La información genética, en el ARN, se escribe a partir de cuatro letras, que corresponden a las bases nitrogenadas (A, C, G y U), las cuales van funcionalmente agrupadas de tres en tres. Cada grupo de tres se llama codón y lo que hace es codificar un aminoácido o un símbolo de puntuación (Comienzo, parada).

La estructura celular, de la que cada célula tiene muchas, que sintetiza las proteínas a partir de aminoácidos con la información contenida en el ARNm, leyendo los codones, es un agregado molecular complejo llamado ribosoma.

Un codón es un triplete de nucleótidos. En el código genético, cada aminoácido está codificado por uno o varios codones. El codón es la unidad de información básica en el proceso de traducción del ARNm. Cada uno de los codones codifica un aminoácido y esta correlación es la base del código genético que permite la traducción de la secuencia de ARNm a la secuencia de aminoácidos que compone la proteína. A toda la secuencia de codones de un gen, desde el codón de inicio hasta el último codón antes del de terminación, se le conoce como «Marco de Lectura Abierto» (ORF, por sus siglas en inglés), debido a que esta es la secuencia que se va a "leer" para dar lugar a un polipéptido.

Cada codón porta la información para pasar la secuencia de nucleótidos del ARNm a la secuencia de aminoácidos de la proteína en el proceso de traducción. Dado que cada codón codifica un aminoácido, hay 64 codones diferentes por combinación de los 4 nucleótidos en cada una de las 3 posiciones del triplete (ver tabla más abajo), de los cuales se codifican 20 aminoácidos,3 codones de terminación de la traducción y un codón de inicio de la traducción, el AUG, que codifica la metionina.
Salvo la metionina y el triptófano que están codificados por un único codón, los aminoácidos pueden estar codificados por 2, 3, 4 ó 6 codones diferentes. Esto hace que el código sea redundante, lo que se denomina código degenerado, porque hay varios codones diferentes que codifican para un solo aminoácido.
Los 3 codones de terminación conocidos como codón de terminación, codón de parada o codón stop llamados ocre (UAA), ámbar (UAG) y ópalo (UGA) son los tres tripletes que al no codificar ningún aminoácido ocasionan el cese de la síntesis proteica.
Hay un codón de inicio de la traducción, el AUG, que codifica la metionina, es el primer codón de una transcripción de ARNm traducido por un ribosoma.

Los codones que codifican un mismo aminoácido muchas veces tienen los dos primeros nucleótidos iguales, cambiando sólo el tercero. Así, cambios en el nucleótido de la tercera posición no suponen cambios en el aminoácido (mutaciones silenciosas). De este modo se minimiza el impacto de mutaciones puntuales cuando éstas ocurren en la tercera posición del codón. En cambio las mutaciones en la primera y segunda posición del codón suelen suponer un cambio de aminoácido (mutaciones "missense"). "." Normalmente los aminoácidos con las mismas características físico-químicas presentan el mismo nucleótido en la segunda posición del codón. Así los aminoácidos polares presentan adenina mientras que los apolares presentan uracilo. Mutaciones puntuales en la primera posición dan lugar a aminoácidos similares mientras que cambios en la segunda posición del codón, dan lugar a la incorporación de aminoácidos de propiedades muy diferentes. Mutaciones en cualquiera de las tres posiciones del codón pueden dar lugar a la aparición de codones stop provocando una terminación de la traducción prematura lo que ocasiona que se traduzca una proteína incompleta y, en la mayoría de los casos, no funcional (mutaciones "nonsense)."



</doc>
<doc id="3264" url="https://es.wikipedia.org/wiki?curid=3264" title="Ácido aspártico">
Ácido aspártico

El ácido aspártico o su forma ionizada, el aspartato (símbolos Asp y D) es uno de los veinte aminoácidos con los que las células forman las proteínas. En el ARN se encuentra codificado por los codones GAU o GAC.
Presenta un grupo carboxilo (-COOH) en el extremo de la cadena lateral. Su fórmula química es .

A pH fisiológico, tiene una carga negativa (es ácido); pertenece al grupo de aminoácidos con cadenas laterales polares cargadas. No es un aminoácido esencial ya que puede ser sintetizado por el organismo humano.
Su biosíntesis tiene lugar por transaminación del ácido oxalacético, un metabolito intermediario del ciclo de Krebs.

El ácido aspártico fue descubierto en 1827 por los químicos franceses Auguste-Arthur Plisson y Étienne Ossian Henry, derivado de la asparagina, que había sido aislada a partir de jugo de espárragos en 1806, por ebullición con una base.

El aspartato no es esencial en mamíferos, siendo producido a partir del oxalacetato por una reacción de transaminación. También se sintetiza del dietil sodio eftalimidomalonato, ().

El aspartato participa en la formación de glutamato a través de la glutamato-aspartato transaminasa citosólica.

El aspartato es también un metabolito del ciclo de la urea y participa en la gluconeogénesis. 

El mecanismo de inactivación es la recaptación. Se han descrito distintos sistemas de transporte en las membranas neuronales y gliales. En la neurona está el EGAC1, que transporta glutamato y aspartato. En la célula glial está el GLAST (aspartato-glutamato). Estos sistemas de transporte son dependientes de sodio e independiente de cloro.
y relaciones.

Los receptores para aspartato son un mundo muy complejo. Los hay ionotrópicos y metabotrópicos. Estimula los receptores NMDA, aunque no tan fuertemente como la hace el glutamato.

El aspartato es uno de los aminoácidos que actúan como neurotransmisores. Su función como neurotrasmisor es de carácter excitatorio del SNC.





</doc>
<doc id="3265" url="https://es.wikipedia.org/wiki?curid=3265" title="Catálisis">
Catálisis

La catálisis es el proceso por el cual se aumenta la velocidad de una reacción química, debido a la participación de una sustancia llamada catalizador y aquellas que desactivan la catálisis son denominados inhibidores. Una característica importante es que la masa de catalizador no se modifica durante la reacción química, lo que lo diferencia de un reactivo, cuya masa va disminuyendo a lo largo de la reacción.

En la síntesis de muchos de los productos químicos industriales más importantes existe una catálisis, ya que esta puede disminuir el tiempo que requiere. El envenenamiento de los catalizadores, que generalmente es un proceso no deseado, también es utilizado en la industria química. Por ejemplo, en la reducción del etino a eteno, el catalizador paladio (Pd) es "envenenado" parcialmente con acetato de plomo (II), Pb(CHCOO). Sin la desactivación del catalizador, el eteno producido se reduciría posteriormente a etano.

La catálisis interviene en muchos procesos industriales. Así mismo, la mayoría de los procesos “biológicamente” significativos son catalizados. La investigación en catálisis es uno de los principales campos en ciencia aplicada e involucra muchas áreas de la química, especialmente en química organometálica y ciencia de materiales. La catálisis es importante para muchos aspectos de las ciencias ambientales, por ejemplo, el convertidor catalítico de los automóviles y la dinámica del agujero de ozono. Las reacciones catalíticas son las preferidas en la química verde para un medioambiente amigable debido a la reducida cantidad de residuos que genera en lugar de las reacciones estequiométricas en las que se consumen todos los reactivos y se forman más productos secundarios. El catalizador más común es el protón (H). Muchos metales de transición y los complejos de los metales de transición se utilizan en la catálisis. Los catalizadores llamados enzimas son importantes en Biología.

El catalizador funciona proporcionando un camino de reacción alternativo al producto de reacción. La velocidad de la reacción aumenta a medida que esta ruta alternativa tiene una menor energía de activación que la ruta de reacción no mediada por el catalizador. La dismutación del peróxido de hidrógeno para dar agua y oxígeno es una reacción que está fuertemente afectada por los catalizadores:
Esta reacción está favorecida, en el sentido de que los productos de reacción son más estables que el material de partida, sin embargo, la reacción no catalizada es lenta. La descomposición del peróxido de hidrógeno es de hecho tan lenta que las soluciones de peróxido de hidrógeno están disponibles comercialmente. Tras la adición de una pequeña cantidad de dióxido de manganeso, el peróxido de hidrógeno reacciona rápidamente de acuerdo a la ecuación anterior. Este efecto se ve fácilmente por la efervescencia del oxígeno. El dióxido de manganeso puede ser recuperado sin cambios, y volver a utilizarse de forma indefinida, y por lo tanto no se consume en la reacción. En consecuencia, el dióxido de manganeso "cataliza esta reacción".

La característica general de la catálisis es que la reacción catalítica tiene un menor cambio de energía libre de la etapa limitante hasta el estado de transición que la reacción no catalizada correspondiente, resultando en una mayor velocidad de reacción a la misma temperatura. Sin embargo, el origen mecánico de la catálisis es complejo.

Los catalizadores pueden afectar favorablemente al entorno de reacción, por ejemplo, los catalizadores ácidos para las reacciones de los compuestos carbonílicos forman compuestos intermedios específicos que no se producen naturalmente, tales como los ésteres de Osmio en la dihidroxilación de alquenos catalizadas por el tetróxido de osmio, o hacer la ruptura de los reactivos a formas reactivas, como el hidrógeno atómico en la hidrogenación catalítica.

Cinéticamente, las reacciones catalíticas se comportan como las reacciones químicas típicas, es decir, la velocidad de reacción depende de la frecuencia de contacto de los reactivos en la etapa determinante de velocidad (ver ecuación de Arrhenius). Normalmente, el catalizador participa en esta etapa lenta, y las velocidades están limitadas por la cantidad de catalizador. En catálisis heterogénea, la difusión de los reactivos a la superficie de contacto y la difusión de los productos desde dicha superficie puede ser la etapa determinante de la velocidad. Eventos similares relacionados con la unión del sustrato y la disociación del producto se aplican en la catálisis homogénea.

Aunque los catalizadores no son consumidos por la propia reacción, pueden resultar inhibidos, desactivados o destruidos por procesos secundarios. En la catálisis heterogénea, procesos secundarios típicos incluyen el coqueo, donde el catalizador se cubre por productos secundarios poliméricos. Además, los catalizadores heterogéneos pueden disolverse en la solución en un sistema sólido-líquido o evaporarse en un sistema sólido-gas.

Los catalizadores generalmente reaccionan con uno o más de los reactivos para formar productos intermedios que, posteriormente, conducen al producto final de reacción. En el proceso se regenera el catalizador. El siguiente esquema es típico de una reacción catalítica, donde C representa el catalizador, X e Y son los reactivos, y Z es el producto de la reacción de X con Y:

Aunque el catalizador es consumido por la reacción 1, posteriormente es producido por la reacción 4, por lo que la reacción global es:
Como el catalizador se regenera en una reacción, a menudo bastan pequeñas cantidades del catalizador para incrementar la velocidad de una reacción. Sin embargo, en la práctica los catalizadores son algunas veces consumidos en procesos secundarios.

Como ejemplo de este proceso, en 2008, investigadores daneses revelaron por primera vez la secuencia de sucesos cuando el oxígeno y el hidrógeno se combinan en la superficie del dióxido de titanio (TiO, o "titania") para producir agua. Con una serie de imágenes de microscopía de efecto túnel a intervalos, determinaron que las moléculas sufren adsorción, disociación y difusión antes de reaccionar. Los estados intermedios de reacción fueron: HO, HO, luego HO y el producto final de la reacción (dímeros de la molécula de agua), tras lo cual la molécula de agua se desorbe de la superficie del catalizador.

Los catalizadores funcionan proporcionando un mecanismo (alternativo) que involucra un estado de transición diferente y una menor energía de activación. Por lo tanto, más colisiones moleculares tienen la energía necesaria para alcanzar el estado de transición. En consecuencia, los catalizadores permiten reacciones que de otro modo estarían bloqueadas o ralentizadas por una barrera cinética. El catalizador puede aumentar la velocidad de reacción o de la selectividad, o permitir que la reacción ocurra a menores temperaturas. Este efecto puede ser ilustrado con una distribución de Boltzmann y un diagrama de perfil de energía.

Los catalizadores no cambian el rendimiento de una reacción: no tienen efecto en el equilibrio químico de una reacción, debido a que la velocidad, tanto de la reacción directa como de la inversa, se ven afectadas (ver también termodinámica). El hecho de que un catalizador no cambie el equilibrio es una consecuencia de la segunda ley de la termodinámica. Supongamos que hay un catalizador que modifica el equilibrio. La introducción del catalizador en el sistema daría lugar a la reacción para ir de nuevo al equilibrio, produciendo energía. La producción de energía es un resultado necesario, puesto que las reacciones son espontáneas sí y sólo sí se produce energía libre de Gibbs, y si no hay una barrera energética no hay necesidad de un catalizador. En consecuencia, la eliminación del catalizador también resultaría en una reacción, produciendo energía; esto es, tanto la adición, como su proceso inverso, la eliminación, producirían energía. Así, un catalizador que pudiera cambiar el equilibrio sería un móvil perpetuo, en contradicción con las leyes de la termodinámica.

Si un catalizador cambia el equilibrio, entonces debe consumirse a medida que avanza la reacción, y por lo tanto también es un reactivo. Algunos ejemplos ilustrativos son la hidrólisis de los ésteres catalizada por bases, donde el ácido carboxílico producido reacciona inmediatamente con el catalizador básico, y así el equilibrio de la reacción se desplaza hacia la hidrólisis.

La unidad derivada SI para medir la actividad catalítica de un catalizador es el katal, que es igual a moles por segundo. La actividad de un catalizador puede ser descrita por el número de conversiones, o TON (del inglés "turn over number"), y la eficiencia catalítica por la "frecuencia de conversiones", TOF (del inglés "turn over frequency"). El equivalente bioquímico es la unidad de actividad enzimática. Para más información sobre la eficiencia de la catálisis enzimática, ver el artículo de Catálisis enzimática.

El catalizador estabiliza el estado de transición más que de los que estabiliza el material inicial. Disminuye la barrera cinética al disminuir la "diferencia" de energía entre el material inicial y el estado de transición.

La naturaleza química de los catalizadores es tan diversa como la catálisis misma, aunque pueden hacerse algunas generalizaciones. Los ácidos próticos son probablemente los catalizadores más ampliamente usados, especialmente para muchas reacciones que involucran agua, incluyendo la hidrólisis y su inversa. Los sólidos multifuncionales a menudo suelen ser catalíticamente activos, por ejemplo las zeolitas, la alúmina y ciertas formas de carbono grafítico. Los metales de transición son utilizados a menudo para catalizar reacciones redox (oxigenación, hidrogenación). Muchos procesos catalíticos, especialmente los que involucran hidrógeno, requieren metales del grupo del platino.

Algunos de los llamados catalizadores son, en realidad, precatalizadores. Los precatalizadores se convierten en el catalizador en el transcurso de la reacción. Por ejemplo, el catalizador de Wilkinson RhCl(PPh) pierde un ligando trifenilfosfina antes de entrar en el verdadero ciclo catalítico. Los precatalizadores son más fáciles de almacenar, pero son fácilmente activados "in situ". Debido a esta etapa de preactivación, muchas reacciones catalíticas involucran un período de inducción.

Las especies químicas que mejoran la actividad catalítica son denominadas co-catalizadores o promotores, en la catálisis cooperativa.

Los catalizadores pueden ser homogéneos o heterogéneos, dependiendo de si existe un catalizador en la misma fase que el sustrato. Los biocatalizadores son vistos a menudo como un grupo separado.

Los catalizadores heterogéneos son aquellos que actúan en una fase diferente que los reactivos. La mayoría de los catalizadores heterogéneos son sólidos que actúan sobre sustratos en una mezcla de reacción líquida o gaseosa. Se conocen diversos mecanismos para las reacciones en superficies, dependiendo de cómo se lleva a cabo la adsorción (Langmuir-Hinshelwood, Eley -Rideal, y Mars-van Krevelen). El área superficial total del sólido tiene un efecto importante en la velocidad de reacción. Cuanto menor sea el tamaño de partícula del catalizador, mayor es el área superficial para una masa dada de partículas.

Por ejemplo, en el proceso de Haber, el hierro finamente dividido sirve como un catalizador para la síntesis de amoníaco a partir de nitrógeno e hidrógeno. Los gases reactantes se adsorben en los "sitios activos" de las partículas de hierro. Una vez adsorbidos, los enlaces dentro de las moléculas reaccionantes se resienten, y se forman nuevos enlaces entre los fragmentos generados, en parte debido a su proximidad. De esta manera el particularmente fuerte triple enlace en el nitrógeno se debilita y los átomos de hidrógeno y nitrógeno se combinan más rápido de lo que lo harían el caso en la fase gaseosa, por lo que la velocidad de reacción aumenta.

Los catalizadores heterogéneos suelen estar "soportados", que significa que el catalizador se encuentra disperso en un segundo material que mejora la eficacia o minimiza su costo. A veces el soporte es más que una superficie sobre la que se transmite el catalizador para aumentar el área superficial. Más a menudo, el soporte y el catalizador interactúan, afectando a la reacción catalítica.

Normalmente los catalizadores homogéneos están disueltos en un disolvente con los sustratos. Un ejemplo de catálisis homogénea implica la influencia de H en la esterificación de los ésteres, por ejemplo, acetato de metilo a partir del ácido acético y el metanol. Para los químicos inorgánicos, la catálisis homogénea es a menudo sinónimo de catalizadores organometálicos.

En el contexto de la electroquímica, específicamente en la ingeniería de las pilas de combustible, que contienen varios metales los catalizadores se utilizan para mejorar las velocidades de las semirreacciones que conforman la pila de combustible. Un tipo común de electrocatalizador de pila de combustible se basa en nanopartículas de platino que están soportadas en partículas un poco mayores de carbón. Cuando este electrocatalizador de platino está en contacto con uno de los electrodos en una pila de combustible, aumenta la velocidad de reducción del oxígeno a agua (o hidróxido o peróxido de hidrógeno).

Mientras que los metales de transición a veces atraen más la atención en el estudio de la catálisis, las moléculas orgánicas que no contengan metales también pueden poseer propiedades catalíticas. Normalmente, los catalizadores orgánicos requieren una mayor carga (o cantidad de catalizador por unidad de cantidad de reactivo) que los catalizadores basados en metales de transición, pero estos catalizadores suelen estar disponibles comercialmente en grandes cantidades, ayudando a reducir los costos. A principios de los 2000, los organocatalizadores fueron considerados una "nueva generación" y eran competidores de los tradicionales catalizadores que contenían metales. Las reacciones enzimáticas operan a través de los principios de la catálisis orgánica.

El principio de la nanocatálisis se basa en la premisa de que los materiales catalíticos aplicados en la nanoescala tienen mejores propiedades, en comparación con lo que exhiben en una macroescala.
Se estima que el 90% de todos los productos químicos producidos comercialmente involucran catalizadores en alguna etapa del proceso de su fabricación. En 2005, los procesos catalíticos generaron cerca de 900.000 millones de dólares en productos de todo el mundo. La catálisis es tan penetrante que las subáreas no son fácilmente clasificables. Algunas áreas de particular concentración se estudian más adelante.

El refinado de petróleo hace un uso intensivo de la catálisis para la alquilación, craqueo catalítico (rotura de hidrocarburos de cadena larga en trozos más pequeños), reformado de nafta y el reformado con vapor (conversión de hidrocarburos en gas de síntesis). Incluso los gases de combustión de la quema de combustibles fósiles es tratada a través de la catálisis: convertidores catalíticos, normalmente compuestos de platino y rodio, rompen algunos de los subproductos más nocivos de los gases de escape de los automóviles.
Con respecto a los combustibles sintéticos, un viejo pero importante proceso es el síntesis de Fischer-Tropsch de hidrocarburos a partir del gas de síntesis, que a su vez se procesa a través de la reacción de cambio agua-gas, catalizada por el hierro. El Biodiésel y los biocombustibles relacionados requieren un procesamiento tanto a través de los catalizadores inorgánicos como de los biocatalizadores.

Las pilas de combustible se basan en catalizadores de las reacciones tanto anódicas como catódicas.

Algunos de los productos químicos obtenidos a gran escala se producen a través de la oxidación catalítica, a menudo usando oxígeno. Algunos ejemplos son el ácido nítrico (a partir de amoníaco), el ácido sulfúrico (a partir de dióxido de azufre a trióxido de azufre por el proceso de las cámaras de plomo), el ácido tereftálico a partir de p-xileno, y el acrilonitrilo a partir de propano y amoníaco.

Muchos otros productos químicos son generados por reducción a gran escala, a menudo a través de hidrogenación. El ejemplo a mayor escala es el amoníaco, que se prepara a través del proceso de Haber a partir de nitrógeno. El Metanol es preparado a partir de monóxido de carbono.

Los polímeros a granel derivados de etileno y propileno se preparan a menudo a través de la catálisis Ziegler-Natta. Los poliésteres, las poliamidas, y los isocianatos se obtienen a través de la catálisis ácido-base.

La mayoría de los procesos de carbonilación requieren catalizadores metálicos, los ejemplos incluyen la síntesis de ácido acético mediante el proceso Monsanto y la hidroformilación.

Muchos productos de química fina se preparan a través de la catálisis, los métodos incluyen a los de la industria pesada, así como procesos más especializados que serían prohibitivamente caros a gran escala. Algunos ejemplos son la metátesis de olefinas usando el catalizador de Grubbs, la reacción de Heck, y la reacción de Friedel-Crafts.

Debido a que la mayoría de los compuestos bioactivos son quirales, muchos productos farmacéuticos son producidos por catálisis enantioselectiva.

Una de las aplicaciones más obvias de la catálisis es la hidrogenación (reacción con el hidrógeno gas) de las grasas usando níquel como catalizador para producir la margarina. Muchos otros productos alimenticios se preparan a través de biocatálisis (véase más abajo).

En la naturaleza, las enzimas son catalizadores en el metabolismo y el catabolismo. La mayoría de biocatalizadores están basados en proteínas, es decir, enzimas, pero otras clases de biomoléculas también exhiben propiedades catalíticas incluyendo las ribozimas, y de desoxirribozimas sintéticas.

Los biocatalizadores se pueden considerar como intermedio entre los catalizadores homogéneos y los heterogéneos, aunque estrictamente hablando las enzimas solubles son catalizadores homogéneos y las enzimas enlazadas a membrana son heterogéneas. Varios factores afectan la actividad de las enzimas (y otros catalizadores), incluyendo la temperatura, el pH, la concentración de la enzima, el sustrato y los productos. Un reactivo particularmente importante en las reacciones enzimáticas es el agua, que es el producto de muchas de las reacciones en que se forman enlaces y un reactivo en muchos procesos en que se rompen enlaces.

Las enzimas se emplean para preparar los productos químicos básicos, incluyendo el jarabe de maíz y la acrilamida.

La catálisis tiene un impacto en el medio ambiente mediante el aumento de la eficiencia de los procesos industriales, pero la catálisis también juega un papel directo en el medio ambiente. Un ejemplo notable es el papel catalítico de los radicales libres en la destrucción del ozono. Estos radicales se forman por la acción de la radiación ultravioleta sobre los clorofluorocarburos (CFC)

En un sentido general, cualquier cosa que aumenta la velocidad de un proceso es un "catalizador", un término derivado del griego, que significa "anular", "desatar", o "recoger". La frase "procesos catalizados" fue acuñada por Jöns Jakob Berzelius en 1836 para describir las reacciones que son aceleradas por sustancias que permanecen sin cambios después de la reacción. Otro de los primeros químicos involucrados en la catálisis fue Alexander Mitscherlich quien se refirió a los "procesos de contacto" y Johann Wolfgang Döbereiner que habló de "acción de contacto" y cuyo encendedor basado en hidrógeno y una esponja de platino se convirtieron en un gran éxito comercial en la década de 1820. Humphry Davy descubrió el uso de platino en la catálisis. En la década de 1880, Wilhelm Ostwald en la Universidad de Leipzig inició una investigación sistemática de las reacciones que eran catalizadas por la presencia de los ácidos y las bases, y encontró que las reacciones químicas ocurren a una velocidad finita y que estas velocidades pueden utilizarse para determinar la fuerza de ácidos y bases. Por este trabajo, Ostwald fue galardonado en 1909 con el Premio Nobel de Química.

Las sustancias que reducen la acción de los catalizadores son llamadas inhibidores catalíticos si son reversibles, y venenos catalíticos si son irreversibles. Los promotores son sustancias que aumentan la actividad catalítica, en particular cuando no son catalizadores en sí mismos.

El inhibidor puede modificar la selectividad además de la velocidad. Por ejemplo, en la reducción del etino a eteno, el catalizador es paladio (Pd), parcialmente "envenenado" con acetato de plomo (II) (Pb(CHCOO)). Sin la desactivación del catalizador, el etileno producido se reducirá aún más, hasta etano.

El inhibidor puede producir este efecto por ejemplo, envenenando selectivamente sólo a ciertos tipos de sitios activos. Otro mecanismo es la modificación de la geometría de la superficie. Por ejemplo, en las operaciones de hidrogenación, grandes planchas de superficie metálica funcionan como lugares de catálisis hidrogenolítica mientras que los sitios que catalizan la hidrogenación de los insaturados son menores. Así, un veneno que cubre la superficie al azar tienden a reducir el número de grandes planchas no contaminada, pero dejan proporcionalmente más sitios pequeños libres, así se cambia la hidrogenación frente a la hidrogenolisis selectiva. También son posibles otros muchos mecanismos.

La figura muestra el diagrama de una reacción catalizada, mostrando como varía la energía (E) de las moléculas que participan en la reacción durante el proceso de reacción (tiempo, t). Todas las moléculas contienen una cantidad determinada de energía, que depende del número y del tipo de enlaces presentes en ella. Los sustratos o reactivos (A y B) tienen una energía determinada, y el o los productos (AB en el gráfico), otra.

Si la energía total de los sustratos es mayor que la de los productos (por ejemplo como se muestra en el diagrama), una reacción exotérmica, y el exceso de energía se desprende en forma de calor. Por el contrario, si la energía total de los sustratos es menor que la de los productos, se necesita tomar energía del exterior para que la reacción tenga lugar, lo que se denomina reacción endotérmica.

Cuando las moléculas de los sustratos se van acercando para reaccionar, pierden estabilidad (usando una analogía antropomórfica, a las moléculas "les gusta" mantener su espacio vital, y las intromisiones no son bienvenidas). La inestabilidad se manifiesta como un aumento de la energía del sistema (es el pico de energía que se ve en el diagrama). Cuando los sustratos se convierten en productos, las moléculas se separan y se relajan de nuevo, y el conjunto se estabiliza.

Las enzimas catalizan las reacciones estabilizando el intermedio de la reacción, de manera que el "pico" de energía necesario para pasar de los sustratos a los productos es menor. El resultado final es que hay muchas más moléculas de sustrato que chocan y reaccionan para dar lugar a los productos, y la reacción transcurre en general más deprisa. Un catalizador puede catalizar tanto reacciones endotérmicas como exotérmicas, porque en los dos casos es necesario superar una barrera energética. El catalizador (E) crea un microambiente en el que A y B pueden alcanzar el estado intermedio (A...E...B) más fácilmente, reduciendo la cantidad de energía necesaria (E2). Como resultado, la reacción es más fácil, optimizando la velocidad de dicha reacción.

Los catalizadores no alteran el equilibrio químico propio de la reacción en ningún caso.




</doc>
<doc id="3266" url="https://es.wikipedia.org/wiki?curid=3266" title="Enzima">
Enzima

Las enzimas

Debido a que las enzimas son extremadamente selectivas con sus sustratos y su velocidad crece solo con algunas reacciones, el conjunto ("set") de enzimas presentes en una célula determina el tipo de metabolismo que tiene esa célula. A su vez, esta presencia depende de la regulación de la expresión génica correspondiente a la enzima.

Como todos los catalizadores, las enzimas funcionan disminuyendo la energía de activación (ΔG) de una reacción, de forma que la presencia de la enzima acelera sustancialmente la tasa de reacción. Las enzimas no alteran el balance energético de las reacciones en que intervienen, ni modifican, por lo tanto, el equilibrio de la reacción, pero consiguen acelerar el proceso incluso en escalas de millones de veces. Una reacción que se produce bajo el control de una enzima, o de un catalizador en general, alcanza el equilibrio mucho más deprisa que la correspondiente reacción no catalizada.

Al igual que ocurre con otros catalizadores, las enzimas no son consumidas en las reacciones que catalizan, ni alteran su equilibrio químico. Sin embargo, las enzimas difieren de otros catalizadores por ser más específicas. La gran diversidad de enzimas existentes catalizan alrededor de 4000 reacciones bioquímicas distintas. No todos los catalizadores bioquímicos son proteínas, pues algunas moléculas de ARN son capaces de catalizar reacciones (como la subunidad 16S de los ribosomas en la que reside la actividad peptidil transferasa). También cabe nombrar unas moléculas sintéticas denominadas enzimas artificiales capaces de catalizar reacciones químicas como las enzimas clásicas.

La actividad de las enzimas puede ser afectada por otras moléculas. Los inhibidores enzimáticos son moléculas que disminuyen o impiden la actividad de las enzimas, mientras que los activadores son moléculas que incrementan dicha actividad. Asimismo, gran cantidad de enzimas requieren de cofactores para su actividad. Muchas drogas o fármacos son moléculas inhibidoras. Igualmente, la actividad es afectada por la temperatura, el pH, la concentración de la propia enzima y del sustrato, y otros factores físico-químicos.

Muchas enzimas son usadas comercialmente, por ejemplo, en la síntesis de antibióticos o de productos domésticos de limpieza. Además, son ampliamente utilizadas en diversos procesos industriales, como son la fabricación de alimentos, destinción de vaqueros o producción de biocombustibles.

Desde finales del siglo XVIII y principios del siglo XIX, se conocía la digestión de la carne por las secreciones del estómago y la conversión del almidón en azúcar por los extractos de plantas y la saliva. Sin embargo, no había sido identificado el mecanismo subyacente. aunque la primera enzima fue descubierta por Anselme Payen y Jean-François Persoz en 1833.

En el siglo XIX, cuando se estaba estudiando la fermentación del azúcar en el alcohol con levaduras, Louis Pasteur llegó a la conclusión de que esta fermentación era catalizada por una fuerza vital contenida en las células de la levadura, llamadas fermentos, e inicialmente se pensó que solo funcionaban con organismos vivos. Escribió que ""la fermentación del alcohol es un acto relacionado con la vida y la organización de las células de las levaduras, y no con la muerte y la putrefacción de las células"". Por el contrario, otros científicos de la época como Justus von Liebig, se mantuvieron en la posición que defendía el carácter puramente químico de la reacción de fermentación.

En 1878 el fisiólogo Wilhelm Kühne (1837-1900) acuñó el término "enzima", que viene del griego "ενζυμον" "en levadura", para describir este proceso. La palabra enzima fue usada después para referirse a sustancias inertes como la pepsina. Por otro lado, la palabra "fermento" solía referirse a la actividad química producida por organismos vivientes.

En 1897 Eduard Buchner comenzó a estudiar la capacidad de los extractos de levadura para fermentar azúcar a pesar de la ausencia de células vivientes de levadura. En una serie de experimentos en la Universidad Humboldt de Berlín, encontró que el azúcar era fermentado inclusive cuando no había elementos vivos en los cultivos de células de levaduras. Llamó a la enzima que causa la fermentación de la sacarosa, “zimasa”. En 1907 recibió el ""por sus investigaciones bioquímicas y el haber descubierto la fermentación libre de células"". Siguiendo el ejemplo de Buchner, las enzimas son usualmente nombradas de acuerdo a la reacción que producen. Normalmente, el sufijo "-asa" es agregado al nombre del sustrato (p. ej., la lactasa es la enzima que degrada lactosa) o al tipo de reacción (p. ej., la ADN polimerasa forma polímeros de ADN).

Tras haber mostrado que las enzimas pueden funcionar fuera de una célula viva, el próximo paso era determinar su naturaleza bioquímica. En muchos de los trabajos iniciales se notó que la actividad enzimática estaba asociada con proteínas, pero algunos científicos (como el premio Nobel Richard Willstätter) argumentaban que las proteínas eran simplemente el transporte para las verdaderas enzimas y que las proteínas "per se" no eran capaces de realizar catálisis. Sin embargo, en 1926, James B. Sumner demostró que la enzima ureasa era una proteína pura y la cristalizó. Summer hizo lo mismo con la enzima catalasa en 1937. La conclusión de que las proteínas puras podían ser enzimas fue definitivamente probada por John Howard Northrop y Wendell Meredith Stanley, quienes trabajaron con diversas enzimas digestivas como la pepsina (1930), la tripsina y la quimotripsina. Estos tres científicos recibieron el Premio Nobel de Química en 1946.

El descubrimiento de que las enzimas podían ser cristalizadas permitía que sus estructuras fuesen resueltas mediante técnicas de cristalografía y difracción de rayos X. Esto se llevó a cabo en primer lugar con la lisozima, una enzima encontrada en las lágrimas, la saliva y los huevos, capaces de digerir la pared de algunas bacterias. La estructura fue resuelta por un grupo liderado por David Chilton Phillips y publicada en 1965. Esta estructura de alta resolución de las lisozimas, marcó el comienzo en el campo de la biología estructural y el esfuerzo por entender cómo las enzimas trabajan en el orden molecular.

Las enzimas son generalmente proteínas globulares que pueden presentar tamaños muy variables, desde 62 aminoácidos como en el caso del monómero de la 4-oxalocrotonato tautomerasa, hasta los 2500 presentes en la sintasa de ácidos grasos.

Las actividades de las enzimas vienen determinadas por su estructura tridimensional, la cual viene a su vez determinada por la secuencia de aminoácidos. Sin embargo, aunque la estructura determina la función, predecir una nueva actividad enzimática basándose únicamente en la estructura de una proteína es muy difícil, y un problema aún no resuelto.

Casi todas las enzimas son mucho más grandes que los sustratos sobre los que actúan, y solo una pequeña parte de la enzima (alrededor de 3 a 4 aminoácidos) está directamente involucrada en la catálisis. La región que contiene estos residuos encargados de catalizar la reacción es denominada "centro activo". Las enzimas también pueden contener sitios con la capacidad de unir cofactores, necesarios a veces en el proceso de catálisis, o de unir pequeñas moléculas, como los sustratos o productos (directos o indirectos) de la reacción catalizada. Estas uniones de la enzima con sus propios sustratos o productos pueden incrementar o disminuir la actividad enzimática, dando lugar así a una regulación por retroalimentación positiva o negativa, según el caso.

Al igual que las demás proteínas, las enzimas se componen de una cadena lineal de aminoácidos que se pliegan durante el proceso de traducción para dar lugar a una estructura terciaria tridimensional de la enzima, susceptible de presentar actividad. Cada secuencia de aminoácidos es única y por tanto da lugar a una estructura única, con propiedades únicas. En ocasiones, proteínas individuales pueden unirse a otras proteínas para formar complejos, en lo que se denomina estructura cuaternaria de las proteínas.

La mayoría de las enzimas, al igual que el resto de las proteínas, pueden ser desnaturalizadas si se ven sometidas a agentes desnaturalizantes como el calor, los pHs extremos o ciertos compuestos como el SDS. Estos agentes destruyen la estructura terciaria de las proteínas de forma reversible o irreversible, dependiendo de la enzima y de la condición. Una consecuencia de la desnaturalización es la pérdida o merma de la función, de la capacidad enzimática.

Las enzimas suelen ser muy específicas tanto del tipo de reacción que catalizan como del sustrato involucrado en la reacción. La forma, la carga y las características hidrofílicas/hidrofóbicas de las enzimas y los sustratos son los responsables de dicha especificidad. La constante de especificidad, es una medida de la eficiencia de una enzima, ya que la velocidad de la reacción se encuentra directamente relacionada con la frecuencia con la que se encuentran las moléculas de enzima y sustrato. Las enzimas también pueden mostrar un elevado grado de estereoespecificidad, regioselectividad y quimioselectividad.

Algunas de estas enzimas que muestran una elevada especificidad y precisión en su actividad son aquellas involucrados en la replicación y expresión del genoma. Estas enzimas tienen eficientes sistemas de comprobación y corrección de errores, como en el caso de la ADN polimerasa, que cataliza una reacción de replicación en un primer paso, para comprobar posteriormente si el producto obtenido es el correcto. Este proceso, que tiene lugar en dos pasos, da como resultado una media de tasa de error increíblemente baja, en torno a 1 error cada 100 millones de reacciones en determinadas polimerasas de mamíferos. Este tipo de mecanismos de comprobación también han sido observados en la ARN polimerasa, en la ARNt aminoacil sintetasa y en la actividad de selección de los aminoacil-tRNAs.

Aquellas enzimas que producen metabolitos secundarios son denominadas promiscuas, ya que pueden actuar sobre una gran variedad de sustratos. Por ello, se ha sugerido que esta amplia especificidad de sustrato podría ser clave en la evolución y diseño de nuevas rutas biosintéticas.

Las enzimas son muy específicas, como sugirió Emil Fischer en 1894. Con base a sus resultados dedujo que ambas moléculas, la enzima y su sustrato, poseen complementariedad geométrica, es decir, sus estructuras encajan exactamente una en la otra, por lo que este modelo ha sido denominado como modelo de la «llave-cerradura», refiriéndose a la enzima como a una especie de cerradura y al sustrato como a una llave que encaja de forma perfecta en dicha cerradura. Una llave sólo funciona en su cerradura y no en otras cerraduras. Sin embargo, si bien este modelo explica la especificidad de las enzimas, falla al intentar explicar la estabilización del estado de transición que logran adquirir las enzimas.

En 1958, Daniel Koshland sugiere una modificación al modelo de la llave-cerradura: las enzimas son estructuras bastante flexibles y así el sitio activo podría cambiar su conformación estructural por la interacción con el sustrato. Como resultado de ello, la cadena aminoacídica que compone el sitio activo es moldeada en posiciones precisas, lo que permite a la enzima llevar a cabo su función catalítica. En algunos casos, como en las glicosidasas, el sustrato cambia ligeramente de forma para entrar en el sitio activo. El sitio activo continua dicho cambio hasta que el sustrato está completamente unido, momento en el cual queda determinada la forma y la carga final.

Las enzimas pueden actuar de diversas formas, como se verá a continuación, siempre dando lugar a una disminución del valor de ΔG:

Cabe destacar que este efecto entrópico implica la desestabilización del estado basal, y su contribución a la catálisis es relativamente pequeña.

La comprensión del origen de la reducción del valor de ΔG en una reacción enzimática requiere elucidar previamente cómo las enzimas pueden estabilizar su estado de transición, más que el estado de transición de la reacción. Aparentemente, la forma más efectiva para alcanzar la estabilización es la utilización de fuerzas electrostáticas, concretamente, poseyendo un ambiente polar relativamente fijado que pueda orientarse hacia la distribución de carga del estado de transición. Ese tipo de ambientes no existen ni se generan en ausencia de enzimas.

La dinámica interna de las enzimas está relacionada con sus mecanismos de catálisis. La dinámica interna se define como el movimiento de diferentes partes de la estructura de la enzima, desde residuos individuales de aminoácidos, hasta grupos de aminoácidos o incluso un dominio proteico entero. Estos movimientos se producen a diferentes escalas de tiempo que van desde femtosegundos hasta segundos. Casi cualquier residuo de la estructura de la enzima puede contribuir en el proceso de catálisis por medio de movimientos dinámicos. Los movimientos de las proteínas son vitales en muchas enzimas. Dichos movimientos podrán ser más o menos importantes según si los cambios conformacionales se producen por vibraciones pequeñas y rápidas o grandes y lentas, y dicha importancia dependerá del tipo de reacción que lleve a cabo la enzima. Sin embargo, aunque estos movimientos son importantes en el proceso de unión y liberación de sustratos y productos, aún no está claro si estos movimientos ayudan a acelerar los pasos químicos de las reacciones enzimáticas. Estos nuevos avances también tienen implicaciones en la comprensión de los efectos alostéricos y en el desarrollo de nuevos fármacos.

Los sitios alostéricos son zonas de la enzima con capacidad de reconocer y unir determinadas moléculas en la célula. Las uniones a las que dan lugar son débiles y no covalentes, y generan un cambio en la conformación estructural de la enzima que repercute en el sitio activo, afectando así a la velocidad de reacción. Las interacciones alostéricas pueden tanto inhibir como activar enzimas, y son una forma muy común de controlar las enzimas en las células.

Algunas enzimas no precisan ningún componente adicional para mostrar una total actividad. Sin embargo, otras enzimas requieren la unión de moléculas no proteicas denominadas cofactores para poder ejercer su actividad. Los cofactores pueden ser compuestos inorgánicos, como los iones metálicos y los complejos ferrosulfurosos, o compuestos orgánicos, como la flavina o el grupo hemo. Los cofactores orgánicos pueden ser a su vez grupos prostéticos, que se unen fuertemente a la enzima, o coenzimas, que son liberados del sitio activo de la enzima durante la reacción. Las coenzimas incluyen compuestos como el NADH, el NADPH y el adenosín trifosfato. Estas moléculas transfieren grupos funcionales entre enzimas.

Un ejemplo de una enzima que contiene un cofactor es la anhidrasa carbónica, en la cual el zinc (cofactor) se mantiene unido al sitio activo, tal y como se muestra en la figura anterior (situada al inicio de la sección "Estructuras y mecanismos"). Estas moléculas suelen encontrarse unidas al sitio activo y están implicadas en la catálisis. Por ejemplo, la flavina y el grupo hemo suelen estar implicados en reacciones redox.

Las enzimas que requieren un cofactor pero no lo tienen unido son denominadas "apoenzimas" o "apoproteínas". Una apoenzima junto con cofactor(es) es denominada "holoenzima" (que es la forma activa). La mayoría de los cofactores no se unen covalentemente a sus enzimas, pero sí lo hacen fuertemente. Sin embargo, los grupos prostéticos pueden estar covalentemente unidos, como en el caso de la tiamina pirofosfato en la enzima piruvato deshidrogenasa. El término "holoenzima" también puede ser aplicado a aquellas enzimas que contienen múltiples subunidades, como en el caso de la ADN polimerasa, donde la holoenzima es el complejo con todas las subunidades necesarias para llevar a cabo la actividad enzimática.

Las coenzimas son pequeñas moléculas orgánicas que transportan grupos químicos de una enzima a otra. Algunos de estos compuestos, como la riboflavina, la tiamina y el ácido fólico son vitaminas (las cuales no pueden ser sintetizados en cantidad suficiente por el cuerpo humano y deben ser incorporados en la dieta). Los grupos químicos intercambiados incluyen el ion hidruro (H) transportado por NAD o NADP, el grupo fosfato transportado por el ATP, el grupo acetilo transportado por la coenzima A, los grupos formil, metenil o metil transportados por el ácido fólico y el grupo metil transportado por la S-Adenosil metionina.

Debido a que las coenzimas sufren una modificación química como consecuencia de la actividad enzimática, es útil considerar a las coenzimas como una clase especial de sustratos, o como segundos sustratos, que son comunes a muchas enzimas diferentes. Por ejemplo, se conocen alrededor de 700 enzimas que utilizan la coenzima NADH.

Las coenzimas suelen estar continuamente regenerándose y sus concentraciones suelen mantenerse a unos niveles fijos en el interior de la célula: por ejemplo, el NADPH es regenerado a través de la ruta de las pentosas fosfato y la "S"-Adenosil metionina por medio de la metionina adenosiltransferasa. Esta regeneración continua significa que incluso pequeñas cantidades de coenzimas son utilizadas intensivamente. Por ejemplo, el cuerpo humano gasta su propio peso en ATP cada día.

Al igual que sucede con todos los catalizadores, las enzimas no alteran el equilibrio químico de la reacción. Generalmente, en presencia de una enzima, la reacción avanza en la misma dirección en la que lo haría en ausencia de enzima, solo que más rápido. Sin embargo, en ausencia de enzima, podría producirse una reacción espontánea que generase un producto diferente debido a que en esas condiciones, dicho producto diferente se forma más rápidamente.

Además, las enzimas pueden acoplar dos o más reacciones, por lo que una reacción termodinámicamente favorable puede ser utilizada para favorecer otra reacción termodinámicamente desfavorable. Por ejemplo, la hidrólisis de ATP suele ser utilizada para favorecer otras reacciones químicas.

Las enzimas catalizan reacciones químicas tanto en un sentido como en el contrario. Nunca alteran el equilibrio, sino únicamente la velocidad a la que es alcanzado. Por ejemplo, la anhidrasa carbónica cataliza su reacción en una u otra dirección dependiendo de la concentración de los reactantes, como se puede ver a continuación:

Si el equilibrio se ve muy desplazado en un sentido de la reacción, es decir, se convierte en una reacción muy exergónica, la reacción se hace efectivamente irreversible. Bajo estas condiciones, la enzima únicamente catalizará la reacción en la dirección permitida desde un punto de vista termodinámico.

La cinética enzimática es el estudio de cómo las enzimas se unen a sus sustratos y los transforman en productos. Los datos de equilibrios utilizados en los estudios cinéticos son obtenidos mediante ensayos enzimáticos.

En 1902, Victor Henri propuso una teoría cuantitativa sobre la cinética enzimática, pero sus datos experimentales no fueron muy útiles debido a que la importancia de la concentración del ion de hidrógeno aún no era considerada. Después de que Peter Lauritz Sørensen definiera la escala logarítmica del pH e introdujera el concepto de "tampón" ("buffer") en 1909, el químico alemán Leonor Michaelis y su postdoctoral canadiense Maud Leonora Menten repitieron los experimentos de Henri confirmando su ecuación, que actualmente es conocida como cinética de Henri-Michaelis-Menten (o simplemente cinética de Michaelis-Menten). Su trabajo fue desarrollado más en profundidad por George Edward Briggs y J. B. S. Haldane, quienes obtuvieron las ecuaciones cinéticas que se encuentran tan ampliamente extendidas en la actualidad.

La mayor contribución de Henri fue la idea de dividir las reacciones enzimáticas en dos etapas. En la primera, el sustrato se une reversiblemente a la enzima, formando el complejo enzima-sustrato (también denominado complejo Michaelis). En la segunda, la enzima cataliza la reacción y libera el producto.

Las enzimas pueden catalizar hasta varios millones de reacciones por segundo. Por ejemplo, la descarboxilación no enzimática de la orotidina 5'-monofosfato tiene una vida media de 78 millones de años. Sin embargo, cuando la enzima orotidina 5'-fosfato descarboxilasa está presente en el medio, ese mismo proceso tarda apenas 25 milisegundos. Las velocidades de las enzimas dependen de las condiciones de la solución y de la concentración de sustrato. Aquellas condiciones que desnaturalizan una proteína, como temperaturas elevadas, pHs extremos o altas concentraciones de sal, dificultan o impiden la actividad enzimática, mientras que elevadas concentraciones de sustrato tienden a incrementar la actividad. Para encontrar la máxima velocidad de una reacción enzimática, la concentración de sustrato se incrementa hasta que se obtiene una tasa constante de formación de producto (véase la curva de saturación representada en la figura de la derecha). La saturación ocurre porque, cuando la concentración de sustrato aumenta, disminuye la concentración de enzima libre, que se convierte en la forma con sustrato unido (ES). A la máxima velocidad ("V") de la enzima, todos los sitios activos de dicha enzima tienen sustrato unido, y la cantidad de complejos ES es igual a la cantidad total de enzima.
Sin embargo, "V" es solo una de las constantes cinéticas de la enzima. La cantidad de sustrato necesario para obtener una determinada velocidad de reacción también es importante. Este parámetro viene dado por la constante de Michaelis-Menten ("K"), que viene a ser la concentración de sustrato necesaria para que una enzima alcance la mitad de su velocidad máxima. Cada enzima tiene un valor de "K" característico para un determinado sustrato, el cual puede decirnos cómo de afín es la unión entre el sustrato y la enzima. Otra constante útil es "k", que es el número de moléculas de sustrato procesadas por cada sitio activo por segundo.

La eficiencia de una enzima puede ser expresada en términos de "k"/"K", en lo que se denomina constante de especificidad, que incorpora la constante de velocidad de todas las fases de la reacción. Debido a que la constante de especificidad contempla tanto la afinidad como la capacidad catalítica, es un parámetro muy útil para comparar diferentes enzimas o la misma enzima con diferentes sustratos. El valor máximo teórico de la constante de especificidad es denominado límite de difusión tiene un valor de 10-10 (M s). Llegados a este punto, cada colisión de la enzima con su sustrato da lugar a la catálisis, con lo que la velocidad de formación de producto no se ve limitada por la velocidad de reacción, sino por la velocidad de difusión. Las enzimas que poseen esta propiedad son llamadas "enzimas catalíticamente perfectas" o "cinéticamente perfectas". Ejemplos de este tipo de enzimas son la triosa fosfato isomerasa, la anhidrasa carbónica, la acetilcolinesterasa, la catalasa, la fumarasa, la beta-lactamasa y la superóxido dismutasa.

La cinética de Michaelis-Menten depende de la ley de acción de masas, que se deriva partiendo de los supuestos de difusión libre y colisión al azar. Sin embargo, muchos procesos bioquímicos o celulares se desvían significativamente de estas condiciones, a causa de fenómenos como el crowding macromolecular, la separación de etapas entre enzima-sustrato-producto, o los movimientos moleculares uni- o bidimensionales. No obstante, en estas situaciones se puede aplicar una cinética de Michaelis-Menten fractal.

Algunas enzimas presentan una cinética más rápida que la velocidad de difusión, lo que en principio parecería ser imposible. Se han propuesto diversos mecanismos para tratar de explicar este fenómeno. Uno de los modelos propone que algunas proteínas podrían tener la capacidad de acelerar la catálisis secuestrando el sustrato y orientándolo mediante campos eléctricos dipolares. Otro modelo propone un mecanismo de efecto túnel cuántico, donde un protón o un electrón pueden formar un túnel a través de barreras de activación, aunque existe cierta controversia en cuanto al efecto túnel que pueda generar un protón. El efecto túnel mediado por protones ha sido observado en triptamina. Esto sugiere que la catálisis enzimática podría ser definida más exactamente como una "barrera", en lugar de como hace el modelo tradicional, donde el sustrato requiere a la enzima para alcanzar una barrera energética más baja.

Los inhibidores son moléculas que regulan la actividad enzimática, inhibiendo su actividad. A grandes rasgos, pueden clasificarse en reversibles e irreversibles. Las irreversibles se unen covalentemente a la enzima sin posibilidad de revertir la modificación, siendo útiles en farmacología. Algunos de los fármacos que actúan de este modo son la eflornitina, utilizada para tratar la tripanosomiasis africana, la penicilina y la aspirina.

Las reversibles se unen de forma reversible a la enzima, pudiendo clasificarse a su vez, según la forma en que intervienen en la reacción, en competitivas, acompetitivas y mixtas. Habitualmente, por su amplia presencia en multitud de procesos, se habla también de inhibición no competitiva, que en realidad no es más que una variante de la ya mencionada inhibición mixta. Sin embargo, por sus características se suele presentar como opuesta a la competitiva, con la que es comparada frecuentemente.

En muchos organismos, los inhibidores pueden actuar como parte de un mecanismo de realimentación. Si una enzima produce una sustancia en demasiada cantidad en el organismo, esta misma sustancia podría actuar como un inhibidor de la enzima al inicio de la ruta que lo produce, deteniendo así dicha producción cuando haya una cantidad suficiente de la sustancia en cuestión. Este sería una forma de realimentación negativa. Las enzimas que se encuentran sujetas a este tipo de regulación suelen ser multiméricas y poseer sitios alostéricos donde se unen sustancias reguladoras. Las gráficas que representan la velocidad de la reacción frente a la concentración de sustrato de estas enzimas no son hipérboles, sino sigmoidales (forma de S).

Debido a que los inhibidores modulan la función de las enzimas, suelen ser utilizados como fármacos. Un típico ejemplo de un inhibidor que es utilizado como fármaco es la aspirina, la cual inhibe las enzimas COX-1 y COX-2 implicadas en la síntesis de un intermediario inflamatorio, las prostaglandinas, con lo que suprime así los efectos derivados, el dolor y la inflamación. Sin embargo, otros inhibidores enzimáticos actúan como venenos. Por ejemplo, el cianuro es un inhibidor irreversible que se une a los átomos de hierro y cobre en el sitio activo de la citocromo c oxidasa de células animales (las plantas son resistentes al cianuro), bloqueando así la respiración celular.

Las enzimas presentan una amplia variedad de funciones en los organismos vivos. Son indispensables en la transducción de señales y en procesos de regulación, normalmente por medio de quinasas y fosfatasas. También son capaces de producir movimiento, como es el caso de la miosina al hidrolizar ATP para generar la contracción muscular o el movimiento de vesículas por medio del citoesqueleto. Otro tipo de ATPasas en la membrana celular son las bombas de iones implicadas en procesos de transporte activo. Además, las enzimas también están implicadas en funciones mucho más exóticas, como la producción de luz por la luciferasa en las luciérnagas. Los virus también pueden contener enzimas implicadas en la infección celular, como es el caso de la integrasa del virus HIV y de la transcriptasa inversa, o en la liberación viral, como la neuraminidasa del virus de la gripe.

Una importante función de las enzimas es la que presentan en el sistema digestivo de los animales. Enzimas tales como las amilasas y las proteasas son capaces de degradar moléculas grandes (almidón o proteínas, respectivamente) en otras más pequeñas, de forma que puedan ser absorbidas en el intestino. Las moléculas de almidón, por ejemplo, que son demasiado grandes para ser absorbidas, son degradadas por diversas enzimas a moléculas más pequeñas como la maltosa, y finalmente a glucosa, la cual sí puede ser absorbida a través de las células del intestino. Diferentes enzimas digestivas son capaces de degradar diferentes tipos de alimentos. Los rumiantes que tienen una dieta herbívora, poseen en sus intestinos una serie de microorganismos que producen otra enzima, la celulasa, capaz de degradar la celulosa presente en la pared celular de las plantas.

Varias enzimas pueden actuar conjuntamente en un orden específico, creando así una ruta metabólica. En una ruta metabólica, una enzima toma como sustrato el producto de otra enzima. Tras la reacción catalítica, el producto se transfiere a la siguiente enzima y así sucesivamente. En ocasiones, existe más de una enzima capaz de catalizar la misma reacción en paralelo, lo que permite establecer una regulación más sofisticada: por ejemplo, en el caso en que una enzima presenta una actividad constitutiva pero con una baja constante de actividad y una segunda enzima cuya actividad es inducible, pero presenta una mayor constante de actividad.

Las enzimas determinan los pasos que siguen estas rutas metabólicas. Sin las enzimas, el metabolismo no se produciría a través de los mismos pasos, ni sería lo suficientemente rápido para atender las necesidades de la célula. De hecho, una ruta metabólica como la glucólisis no podría existir sin enzimas. La glucosa, por ejemplo, puede reaccionar directamente con el ATP de forma que quede fosforilada en uno o más carbonos. En ausencia de enzimas, esta reacción se produciría tan lentamente que sería insignificante. Sin embargo, si se añade la enzima hexoquinasa que fosforila el carbono 6 de la glucosa y se mide la concentración de la mezcla en un breve espacio de tiempo se podrá encontrar únicamente glucosa-6-fosfato a niveles significativos. Por tanto, las redes de rutas metabólicas dentro de la célula dependen del conjunto de enzimas funcionales que presenten.

La actividad enzimática puede ser controlada en la célula principalmente de estas cinco formas:

Debido a que es necesario un fuerte control de la actividad enzimática para la homeostasis, cualquier fallo en el funcionamiento (mutación, incremento o reducción de la expresión o deleción) de una única enzima crítica puede conducir al desarrollo de una enfermedad genética. La importancia de las enzimas se pone de manifiesto en el hecho de que una enfermedad letal puede ser causada por el mal funcionamiento de un único tipo de enzima de todos los miles de tipos que existen en nuestro cuerpo.

Un ejemplo de esto es el tipo más común de fenilcetonuria. En esta enfermedad genética se produce una mutación de un único aminoácido en la fenilalanina hidroxilasa, una enzima que cataliza la primera reacción de la ruta de degradación de la fenilalanina y de compuestos relacionados. Al ser esta enzima inactiva, se acumulan una serie de productos que terminan dando lugar a la aparición de retardo mental si no se recibe tratamiento.

Otro ejemplo es cuando se produce una mutación en los genes de la línea germinal que codifican las enzimas implicadas en la reparación del ADN. En este caso, al no repararse adecuadamente el ADN de las células, se acumulan mutaciones que suelen derivar en el desarrollo de diversos tipos de cáncer hereditarios, como la xerodermia pigmentosa.

El nombre de una enzima suele derivarse del sustrato o de la reacción química que cataliza, con la palabra terminada en -asa. Por ejemplo, lactasa proviene de su sustrato lactosa; alcohol deshidrogenasa proviene de la reacción que cataliza que consiste en "deshidrogenar" el alcohol; ADN polimerasa proviene también de la reacción que cataliza que consiste en polimerizar el ADN.

La Unión Internacional de Bioquímica y Biología Molecular ha desarrollado una nomenclatura para identificar a las enzimas basada en los denominados Números EC. De este modo, cada enzima queda registrada por una secuencia de cuatro números precedidos por las letras "EC". El primer número clasifica a la enzima según su mecanismo de acción. A continuación se indican las seis grandes clases de enzimas existentes en la actualidad:

Las enzimas son utilizadas en la industria química, y en otros tipos de industria, en donde se requiere el uso de catalizadores muy especializados. Sin embargo, las enzimas están limitadas tanto por el número de reacciones que pueden llevar a cabo como por su ausencia de estabilidad en solventes orgánicos y altas temperaturas. Por ello, la ingeniería de proteínas se ha convertido en un área de investigación muy activa donde se intentan crear enzimas con propiedades nuevas, bien mediante diseño racional, bien mediante evolución "in vitro". Estos esfuerzos han comenzado a tener algunos éxitos, obteniéndose algunas enzimas que catalizan reacciones no existentes en la naturaleza.

A continuación se muestra una tabla con diversas aplicaciones industriales de las enzimas:




</doc>
<doc id="3267" url="https://es.wikipedia.org/wiki?curid=3267" title="Oxígeno">
Oxígeno

El oxígeno es un elemento químico de número atómico 8 y representado por el símbolo O. Su nombre proviene de las raíces griegas ὀξύς (oxys) («ácido», literalmente «punzante», en referencia al sabor de los ácidos) y –γόνος (-gonos) («productor», literalmente «engendrador»; es decir, "productor de ácidos"), porque en la época en que se le dio esta denominación se creía, incorrectamente, que todos los ácidos requerían oxígeno para su composición. En condiciones normales de presión y temperatura, dos átomos del elemento se enlazan para formar el dioxígeno, un gas diatómico incoloro, inodoro e insípido con fórmula O. Esta sustancia comprende una importante parte de la atmósfera y resulta necesaria para sostener la vida terrestre.

Forma parte del grupo de los anfígenos en la tabla periódica y es un elemento no metálico altamente reactivo que forma fácilmente compuestos (especialmente óxidos) con la mayoría de elementos, excepto con los gases nobles helio y neón. Asimismo, es un fuerte agente oxidante y tiene la segunda electronegatividad más alta de todos los elementos, solo superado por el flúor. Medido por su masa, el oxígeno es el tercer elemento más abundante del universo, tras el hidrógeno y el helio, y el más abundante en la corteza terrestre, formando prácticamente la mitad de su masa. Debido a su reactividad química, no puede permanecer en la atmósfera terrestre como elemento libre sin ser reabastecido constantemente por la acción fotosintética de los organismos que utilizan la energía solar para producir oxígeno elemental a partir del agua. El oxígeno elemental O solamente empezó a acumularse en la atmósfera después de la aparición de estos organismos, aproximadamente hace 2500 millones de años. El oxígeno diatómico constituye el 20,8 % del volumen de la atmósfera terrestre.

Dado que constituye la mayor parte de la masa del agua, es también el componente mayoritario de la masa de los seres vivos. Muchas de las moléculas más importantes que forman parte de los seres vivos, como las proteínas, los ácidos nucleicos, los carbohidratos y los lípidos, contienen oxígeno, así como los principales compuestos inorgánicos que forman los caparazones, dientes y huesos animales. El oxígeno elemental se produce por cianobacterias, algas y plantas, y todas las formas complejas de vida lo usan para su respiración celular. Resulta tóxico para los organismos de tipo anaerobio obligado, las formas tempranas de vida que predominaban en la Tierra hasta que el O comenzó a acumularse en la atmósfera. Otra forma (alótropa) del oxígeno, el ozono (O), ayuda a proteger la biosfera de la radiación ultravioleta a gran altitud, en la llamada capa de ozono, pero es contaminante cerca de la superficie, donde es un subproducto del esmog. A altitudes aún mayores de la órbita baja terrestre, el oxígeno atómico tiene una presencia significativa y causa erosión en las naves espaciales.

Carl Wilhelm Scheele descubrió el oxígeno de forma independiente en Upsala en 1773, o incluso antes, y Joseph Priestley, en Wiltshire en 1774, pero el honor suele adjudicársele a Priestley debido a que publicó su trabajo antes. Antoine Lavoisier, cuyas investigaciones ayudaron a desacreditar la entonces popular teoría del flogisto de combustión y corrosión, acuñó el nombre «oxígeno» en 1777. Este se produce industrialmente mediante la destilación fraccionada de aire licuado, el uso de zeolita con ciclos de presión para concentrar el oxígeno del aire, la electrólisis del agua y otros medios. El oxígeno se utiliza en la producción de acero, plásticos y textiles; los propulsores de cohetes; la oxigenoterapia; y la asistencia para la respiración en aeronaves, submarinos, vuelos espaciales y submarinismo.

En condiciones normales de presión y temperatura, el oxígeno es un gas incoloro e inodoro con fórmula molecular O, en el que dos átomos de oxígeno se enlazan con una configuración electrónica en estado triplete. Este enlace tiene un orden de enlace de dos y se suele simplificar en las descripciones como un enlace doble o como una combinación de un enlace de dos electrones y dos enlaces de tres electrones.

El oxígeno triplete —no debe confundirse con el ozono, O— es el estado fundamental de la molécula O, que cuenta con dos electrones desparejados que ocupan dos orbitales moleculares degenerados. Estos orbitales se clasifican como antienlaces —debilitan el orden de enlace de tres a dos—, de manera que el enlace del dioxígeno es más débil que el triple enlace del nitrógeno diatómico, en el que todos los orbitales de los enlaces moleculares se rellenan, pero algunos orbitales de antienlace no lo están.

En su forma normal de triplete, las moléculas de O son paramagnéticas; es decir, que en presencia de un campo magnético forman un imán, debido al momento magnético del espín de los electrones desparejados en la molécula y la interacción de canje negativa entre moléculas de O contiguas. Un imán atrae al oxígeno líquido hasta tal punto que, en demostraciones de laboratorio, un hilo de oxígeno líquido puede sostenerse contra su propio peso entre los polos de un imán potente.

El oxígeno molecular singlete es un nombre dado a varias especies de O de mayor energía, en los que todos los espínes de los electrones se emparejan. Es mucho más reactivo con moléculas orgánicas habituales que el oxígeno molecular en sí mismo. En la naturaleza, el oxígeno singlete se suele formar con el agua en la fotosíntesis, usando la energía solar. También se produce en la troposfera a causa de la fotolisis del ozono por la luz de onda corta, así como por el sistema inmunitario como una fuente de oxígeno activo. En los organismos fotosintéticos —y posiblemente también en los animales—, los carotenoides juegan un papel fundamental en la absorción de energía del oxígeno singlete y la conversión de este a su estado no excitado antes de que pueda causar daño a los tejidos.
Energía de disociación de las moléculas diatómicas O-X a 25°C en kJ/mol (en condiciones de laboratorio):
El alótropo más normal del oxígeno elemental es el llamado dioxígeno (O), que tiene una longitud de enlace de 121 pm y una energía de enlace de 498 kJ•mol. Esta es la forma que usan las formas de vida complejas, como los animales, en su respiración celular (véase rol biológico) y es la forma que tiene una gran importancia en la composición de la atmósfera terrestre (véase Abundancia).

El trioxígeno (O) se conoce habitualmente como ozono y es un alótropo muy reactivo, dañino para el tejido pulmonar. El ozono se produce en la atmósfera superior cuando el O se combina con el oxígeno atómico a causa de la división del O por la radiación ultravioleta. Ya que el ozono es un poderoso absorbente en la región ultravioleta del espectro electromagnético, la capa de ozono de la atmósfera superior funciona como un escudo protector de la radiación que recibe el planeta. Cerca de la superficie terrestre, no obstante, es un contaminante formado como subproducto de las emisiones de automóviles. La molécula metaestable del tetraoxígeno (O) no fue descubierta hasta 2001, y se dio por descontado que existía en una de las seis fases del oxígeno sólido. En 2006 se demostró que esta fase, creada mediante la presurización del O a 20 GPa, es, de hecho, un clúster O de sistema trigonal. Este clúster tiene potencial para ser un oxidante mucho más potente que el O y el O y podría, por tanto, ser usado como propulsor de cohetes. En 1990 se descubrió una fase metálica cuando el oxígeno sólido se somete a una presión superior a 96 GPa y se demostró en 1998 que a temperaturas muy bajas se convierte en superconductor.

El oxígeno es más soluble en agua que el nitrógeno; esta contiene aproximadamente una molécula de O por cada dos moléculas de N, comparado con la proporción en la atmósfera, que viene a ser de 1:4. La solubilidad del oxígeno en el agua depende de la temperatura, disolviéndose alrededor del doble (14,6 mg•L) a 0 °C que a 20 °C (7,6 mg•L). A 25 °C y 1 atmósfera de presión, el agua dulce contiene alrededor de 6,04 mililitros (ml) de oxígeno por litro, mientras que el agua marina contiene alrededor de 4,95 ml por litro. A 5 °C la solubilidad se incrementa hasta 9,0 ml (un 50 % más que a 25 °C) por litro en el agua y 7,2 ml (45 % más) en el agua de mar.

El oxígeno se condensa a 90,20 K (−182,95 °C, −297,31 °F) y se congela a 54,36 K (−218,79 °C, −361,82 °F). Tanto el O líquido como el sólido son sustancias con un suave color azul cielo causado por la absorción en el rojo, en contraste con el color azul del cielo, que se debe a la dispersión de Rayleigh de la luz azul. El O líquido de gran pureza se suele obtener a través de la destilación fraccionada de aire licuado. El oxígeno líquido también puede producirse por condensación del aire, usando nitrógeno líquido como refrigerante. Es una sustancia altamente reactiva y debe separarse de materiales inflamables.

El oxígeno que encontramos en la naturaleza se compone de tres isótopos estables: O, O y O, siendo el O el más abundante (99,762 % de abundancia natural).

La mayor parte del O se sintetiza al final del proceso de combustión del helio en una estrella masiva, pero otra parte se produce en el proceso de combustión del neón. El O surge fundamentalmente por la combustión del hidrógeno en helio durante el ciclo CNO, convirtiéndolo en un isótopo común en las zonas de combustión de hidrógeno en las estrellas. Por su parte, la mayoría del O se produce cuando el N —que abunda debido a la combustión CNO— captura un núcleo de He, causando una abundancia de O en las zonas ricas en helio de las estrellas masivas.

Se han caracterizado catorce radioisótopos, de los que los más estables son el O con un periodo de semidesintegración de 70,606 segundos. Todos los restantes isótopos radiactivos tienen periodos de semidesintegración inferiores a 27 segundos y la mayor parte de estos, inferiores a 83 milisegundos. La forma de descomposición de los isótopos más ligeros que el O es la descomposición β para producir nitrógeno y, para los más pesados que el O, la desintegración beta para formar flúor.

El oxígeno es el elemento químico más abundante, por masa, en la biosfera, el aire, el mar y el suelo terrestres. Es, asimismo, el tercero más abundante en el universo, tras el hidrógeno y el helio. Alrededor del 0,9 % de la masa del Sol es oxígeno, que constituye también el 49,2 % de la masa de la corteza terrestre y es el principal componente de los océanos de la Tierra (88,8 % de su masa total). El oxígeno gaseoso es el segundo componente más abundante en la atmósfera terrestre, ya que supone un 20,8 % de su volumen y el 23,1 % de su masa (unas 10 toneladas). La Tierra es una excepción entre los planetas del Sistema Solar por la alta concentración de oxígeno gaseoso en su atmósfera; por ejemplo, Marte (con un 0,1 % de O del total de su volumen) y Venus tienen concentraciones mucho menores. Sin embargo, el O que rodea a estos planetas proviene exclusivamente de la reacción que sufren moléculas que contienen oxígeno, como el dióxido de carbono, por efecto de la radiación ultravioleta.

La inusualmente alta concentración de oxígeno gaseoso en la Tierra es el resultado del ciclo de circulación. Este ciclo biogeoquímico describe el movimiento del oxígeno en el interior de sus tres principales reservas en el planeta: la atmósfera, la biosfera y la litosfera. El factor de conducción más importante en este ciclo es la fotosíntesis, responsable de la atmósfera moderna de la Tierra, que libera oxígeno en la atmósfera, mientras que los procesos de respiración y descomposición lo eliminan. En el equilibrio actual, la producción y el consumo tienen lugar con un ratio aproximado de 1/2000 de la totalidad del oxígeno atmosférico por año.

El oxígeno no combinado también se da en soluciones en las masas de agua del planeta. La mayor solubilidad del O a baja temperatura (véase Propiedades físicas) tiene implicaciones importantes para la vida marina, ya que los océanos polares sostienen una densidad de vida mucho mayor debido a su superior contenido de oxígeno. La cantidad de O en el agua puede haberse visto reducida por la contaminación hídrica, debido a la acción de la descomposición de las algas y otros biomateriales por un proceso llamado eutrofización. Los científicos evalúan este aspecto de la calidad del agua a través de la medición de su demanda biológica de oxígeno, o cantidad de O necesaria para restaurarla a una concentración normal.

El oxígeno es liberado por las bacterias fotosintéticas, las algas y las plantas mediante la fotosíntesis. En el proceso inverso, los organismos aerobios mediante la respiración usan el oxígeno para convertir los nutrientes en energía (ATP). La disminución de oxígeno provoca hipoxemia y su falta total, anoxia, lo que puede provocar la muerte del organismo.

En la naturaleza, el oxígeno no combinado se produce por la fotodescomposición del agua durante la fotosíntesis. Según algunas estimaciones, las algas verdes y las cianobacterias de ambientes marinos proporcionan alrededor del 70 % del producido en la Tierra, y las plantas terrestres, el resto. Otros investigadores estiman que la contribución oceánica al oxígeno atmosférico es aún mayor, mientras que otros la sitúan por debajo, en torno a un 45 % del oxígeno atmosférico total del planeta cada año.

Una fórmula global simplificada de la fotosíntesis es:

6 CO + 6 HO + fotones → CHO + 6 O 
dióxido de carbono + agua + luz solar → glucosa + dioxígeno

La evolución fotolítica del oxígeno tiene lugar en las membranas tilacoides de los organismos fotosintéticos y requiere la energía de cuatro fotones. Están implicados muchos procesos, pero el resultado es la formación de un gradiente de un protón a través de la membrana tilacoide, que se usa para sintetizar adenosín trifosfato (ATP) por la fotofosforilación. El O restante tras la oxidación de la molécula de agua se libera a la atmósfera.

El dioxígeno molecular es esencial para la respiración celular en todos los organismos aerobios, ya que las mitocondrias lo usan para ayudar a generar adenosín trifosfato durante la fosforilación oxidativa. La reacción para la respiración aerobia es básicamente lo contrario que la fotosíntesis y se simplifica de la siguiente forma:

CHO + 6 O → 6 CO + 6 HO + 2880 kJ•mol

En los vertebrados, el O se difunde por membranas pulmonares hacia los glóbulos rojos. La hemoglobina envuelve el O cambiando su color de un rojo azulado a un rojo brillante (el CO se libera desde otra parte de la hemoglobina mediante el efecto Bohr). Otros animales usan la hemocianina (moluscos y algunos artrópodos) o la hemeritrina (arañas y langostas). Un litro de sangre puede disolver 200 cm³ de O.

Las especies reactivas de oxígeno, como el ion superóxido (O) y el peróxido de hidrógeno, son peligrosos subproductos del uso de oxígeno en los organismos. Algunas partes del sistema inmunitario de organismos más avanzados, sin embargo, crean peróxido, superóxido y oxígeno singlete para destruir microbios invasores. Las especies reactivas de oxígeno también tienen un rol importante en la respuesta hipersensible de las plantas contra ataques patógenos.

Un adulto humano en reposo respira de 1,8 a 2,4 gramos de oxígeno por minuto. Sumada la cantidad inhalada por todas las personas del planeta, hace un total de 6000 millones de toneladas de oxígeno por año.

El contenido de oxígeno en el cuerpo de un ser vivo es normalmente mayor en el sistema respiratorio y disminuye a lo largo de cualquier sistema arterial, los tejidos periféricos y el sistema venoso, respectivamente. El contenido de oxígeno en este sentido se suele dar como la presión parcial, que es la presión que tendría el oxígeno si ocupase por sí solo el volumen de las venas.

El oxígeno gaseoso no combinado era casi inexistente en la atmósfera terrestre antes de la evolución de las bacterias y arqueobacterias fotosintéticas. Apareció por primera vez en cantidades significativas durante el Paleoproterozoico (hace alrededor de 2500 y 1600 millones de años). En un principio, el oxígeno se combinó con Hierro disuelto en los océanos para crear formaciones de hierro bandeado. Los océanos comenzaron a exhalar oxígeno no combinado hace 2700 millones de años, alcanzando el 10 % de su nivel actual hace unos 1700 millones de años.

La presencia de grandes cantidades de oxígeno no combinado disuelto en los océanos y la atmósfera pudo haber conducido a la extinción de la mayoría de los organismos anaerobios que vivían entonces, durante la Gran Oxidación ("catástrofe del oxígeno") hace unos 2400 millones de años. Sin embargo, el uso de O en la respiración celular permite producir a los organismos aerobios mucho más ATP que los anaerobios, ayudando a los primeros a dominar a biosfera de la Tierra. La fotosíntesis y la respiración celular del O permitieron la evolución de las células eucariotas y, finalmente, la aparición de organismos multicelulares complejos como plantas y animales.

Desde el comienzo del periodo Cámbrico hace 540 millones de años, los niveles de O han fluctuado entre el 15 % y el 30 % por volumen. Hacia finales del Carbonífero (hace unos 300 millones de años) el nivel de O en la atmósfera alcanzó un volumen máximo del 35 %, que pudo haber contribuido al gran tamaño de los insectos y anfibios de aquella época. La actividad humana, incluyendo la combustión de 7000 millones de toneladas de combustible fósil cada año, ha tenido un impacto muy pequeño en la cantidad de oxígeno combinado en la atmósfera. Con los niveles actuales de fotosíntesis, llevaría unos 2000 años regenerar la cantidad total de O en la atmósfera actual.

Uno de los primeros experimentos conocidos sobre la relación entre la combustión y el aire lo desarrolló el escritor sobre mecánica de la Antigua Grecia Filón de Bizancio, en el S. II a. C. En su obra "Pneumática", Filón observó que invirtiendo un recipiente sobre una vela prendida y rodeando el cuello de este con agua, una parte del líquido subía por el cuello. Supuso, de forma incorrecta, que algunas partes del aire en el recipiente se convertían en elemento clásico del fuego y, entonces, era capaz de escapar a través de poros en el cristal. Muchos siglos después, Leonardo da Vinci observó que una porción del aire se consume durante la combustión y la respiración.

A finales del S. XVII, Robert Boyle probó que el aire es necesario para la combustión. El químico inglés John Mayow perfeccionó su trabajo mostrando que solo requería de una parte del aire, que llamó "spiritus nitroaereus" o simplemente "nitroaereus". En un experimento, descubrió que, colocando tanto un ratón como una vela encendida en un contenedor cerrado sobre agua, hacía que esta subiera y reemplazara un catorceavo del volumen del aire antes de que se apagara la vela y muriera el ratón. Debido a esto, supuso que el "nitroaereus" se consume tanto por la respiración como por la combustión.

Mayow observó que el antimonio incrementaba su peso al calentarse e infirió que el "nitroaereus" debía haberse combinado con él. Pensó también que los pulmones separaban el "nitroaereus" del aire y lo pasaba a la sangre, y que el calor animal y el movimiento muscular eran producto de la reacción del "nitroaereus" con ciertas sustancias en el cuerpo. Publicó informes sobre estos experimentos y otras ideas en 1668, en su obra "Tractatus duo", en el tratado «De respiratione».

Robert Hooke, Ole Borch, Mijaíl Lomonósov y Pierre Bayen produjeron oxígeno durante experimentos entre los siglos XVII y XVIII, pero ninguno de ellos lo reconoció como un elemento. Esto pudo deberse en parte a la prevalencia de la filosofía de la combustión y la corrosión, denominada teoría del flogisto, que por aquel entonces era la explicación predilecta para esos procesos.

Esta teoría, establecida en 1667 por el químico alemán Johann Joachim Becher y modificada por el también químico Georg Stahl en 1731, postulaba que todos los materiales combustibles constaban de dos partes; una, llamada flogisto, que era emitida al quemar la sustancia en cuestión, y otra, denominada desflogisticada, que se tenía por su verdadera forma o "calx" (ceniza; creta en latín).

Los materiales altamente combustibles que dejan poco residuo, como la madera o el carbón, se creían hechos en su mayor parte por flogisto, mientras las sustancias no combustibles que corroen, como el hierro, contienen muy poco. El aire no tenía ningún papel en la teoría del flogisto ni se realizaron experimentos cuantivativos para poner a prueba la idea; por el contrario, se basaba en observaciones de lo que sucedía cuando algo se quemaba: los objetos más comunes parecían volverse más ligeros y perder algo en el proceso. El hecho de que una sustancia como la madera realmente "ganara" peso en su conjunto durante el quemado se ocultaba por la flotabilidad de los productos gaseosos de la combustión. Una de las primeras pistas sobre la falsedad de la teoría del flogisto fue que los metales también ganaban peso en la oxidación (cuando supuestamente perdían flogisto).

El oxígeno fue descubierto por el farmacéutico sueco Carl Wilhelm Scheele, que produjo oxígeno gaseoso calentando óxido de mercurio y varios nitratos alrededor de 1772. Scheele llamó al gas «aire del fuego», porque era el único apoyo conocido para la combustión, y escribió un informe de su descubrimiento en un manuscrito que tituló «Chemische Abhandlung von der Luft und dem Feuer» («Tratado químico del aire y del fuego») y envió a su editor en 1775, si bien no se publicó hasta 1777.

Entre tanto, el 1 de agosto de 1774, el clérigo británico Joseph Priestley condujo un experimento en el que enfocó la luz solar sobre óxido de mercurio (II) (HgO) en el interior de un tubo de cristal, que liberó un gas que él llamó «aire desflogisticado». Notó que las velas prendían más vívamente en el gas y que el ratón estaba más activo y vivía más tiempo mientras lo respiraba. Tras inhalar él mismo el gas, escribió: «La sensación del gas en mis pulmones no era perceptiblemente diferente al del aire normal, pero sentí mi pecho particularmente ligero y desahogado durante un rato después». Priestley publicó sus hallazgos en 1775 en un artículo titulado «An Account of Further Discoveries in Air» («Informe de más descubrimientos en el aire»), que incluyó en el segundo volumen de su libro titulado "Experiments and Observations on Different Kinds of Air". Debido a que publicó sus hallazgos primero, Priestley suele ser considerado el autor del descubrimiento.

El renombrado químico francés Antoine Lavoisier reclamó posteriormente haber descubierto la sustancia de forma independiente. No obstante, Priestley visitó a Lavoisier en octubre de 1774 y le habló sobre su experimento y cómo había liberado el nuevo gas. Scheele también escribió una carta a Lavoisier el 30 de septiembre de ese mismo año, en la que describía su propio descubrimiento de la sustancia antes desconocida, pero el francés nunca accedió a recibirla. Después de la muerte de Scheele se encontró una copia de la carta entre sus pertenencias.

Aunque fue cuestionado en su época, Lavoisier condujo los primeros experimentos cuantitativos adecuados sobre la oxidación y dio la primera explicación correcta acerca del funcionamiento de la combustión. Usó estos y otros experimentos similares, que comenzaron en 1774, para desacreditar la teoría del flogisto y demostrar que la sustancia descubierta por Priestley y Scheele era un elemento químico.

En un experimento, Lavoisier observó que no se producía un incremento global en el peso cuando el estaño y el aire se calentaban en un contenedor cerrado. Notó que, cuando abrió el contenedor, el aire entró súbitamente en él, lo que indicaba que parte del aire atrapado se había consumido. También notó que el estaño había aumentado su peso y que el aumento era igual al del peso del aire que volvió al contenedor cuando lo abrió. Este y otros experimentos sobre la combustión se documentaron en su libro "Sur la combustion en général", publicado en 1777. En esa obra, probó que el aire es una mezcla de dos gases: el «aire esencial», fundamental para la combustión y la respiración, y el "azote" (del griego "ἄζωτον", sin vida), que no servía para ninguna de las dos y se denominaría posteriormente nitrógeno.

Lavoisier renombró al «aire esencial» como "oxígeno" en 1777, desde las raíces griegas "ὀξύς (oxys)" (ácido, literalmente «punzante», por el sabor de los ácidos) y "-γενής (-genēs)" (productor, literalmente «engendrador»), porque pensaba, erróneamente, que el oxígeno era un constituyente de todos los ácidos.Los químicos —en particular sir Humphry Davy en 1812— al cabo de un tiempo determinaron que Lavoisier se equivocó en su apreciación, pues, de hecho, es el hidrógeno el que forma la base de los ácidos, pero el nombre ya se había popularizado.

La hipótesis atómica original de John Dalton asumía que todos los elementos eran monoatómicos y que los átomos de los compuestos tendrían normalmente las relaciones atómicas más simples. Por ejemplo, Dalton pensaba que la fórmula del agua era HO, presentando la masa atómica del oxígeno como 8 veces la del hidrógeno, en vez de 16, el valor que se le da hoy en día. En 1805, Louis Joseph Gay-Lussac y Alexander von Humboldt mostraron que el agua está formada por dos volúmenes de hidrógeno y uno de oxígeno y, en 1811, Amedeo Avogadro dio con la correcta interpretación de la composición del líquido, basado en la que hoy se denomina Ley de Avogadro y en la suposición de moléculas diatómicas elementales.

A finales del S. XIX, los investigadores se dieron cuenta de que el aire podía licuarse y sus componentes aislarse mediante compresión y enfriamiento. Utilizando un método de cascada, el químico y físico suizo Raoul Pictet evaporó dióxido de azufre para licuar dióxido de carbono, que por su parte era evaporado para enfriar el oxígeno gaseoso lo suficiente para pasarlo a líquido. Envió un telegrama a la Academia de Ciencias de Francia el 22 de diciembre de 1877 anunciando su descubrimiento del oxígeno líquido. Solo dos días después, el físico francés Louis Paul Cailletet anunció su propio método para licuar oxígeno molecular. En ambos casos solo se produjeron unas pocas gotas del líquido, por lo que no se pudo llevar a cabo un análisis concluyente. El oxígeno fue licuado de forma estable por primera vez el 29 de marzo de 1883 por los científicos polacos de la Universidad Jagellónica Zygmunt Wróblewski y Karol Olszewski.

En 1891, el químico escocés James Dewar pudo producir la suficiente cantidad de oxígeno líquido para estudiarlo. El primer proceso viable comercialmente para producir oxígeno líquido fue desarrollado en 1895 de forma independiente por los ingenieros Carl von Linde, alemán, y William Hampson, británico. Redujeron la temperatura del aire hasta que se licuó y, entonces, destilaron los componentes gaseosos haciéndolos bullir uno a uno y capturándolos. Más tarde, en 1901, la soldadura de oxiacetileno se demostró por primera vez al quemar una mezcla de acetileno y O comprimido. Este método de soldaje y cortado del metal se convertiría después en corriente.

El físico William Thomson, en 1898, calculó que el oxígeno que permanece en el planeta tiene solo unos 400 o 500 años, basándose en el ritmo de uso de los combustibles fósiles en la combustión.

En 1923, el científico estadounidense Robert Goddard se convirtió en la primera persona en desarrollar un motor cohete, que usaba gasolina como combustible y oxígeno líquido como oxidante. El 16 de marzo, hizo volar con éxito un pequeño cohete propulsado por combustible líquido durante 56 m a 97 km/h, en Auburn (Massachusetts).

Se emplean principalmente dos métodos para producir 100 millones de toneladas de O extraídas del aire para usos industriales cada año. El más común consiste en destilar fraccionadamente aire licuado en sus diversos componentes, con el N destilado como vapor y el O dejado como líquido.

El otro método principal de obtención de O gaseoso consiste en pasar un chorro de aire limpio y seco a través de un lecho de tamices moleculares de zeolita, que adsorben el nitrógeno y dejan pasar un chorro de gas que es de un 90 a un 93 % O. Simultáneamente, el otro lecho de zeolita saturada de nitrógeno libera este gas al reducir la presión de funcionamiento de la cámara e introducir en ella a contracorriente parte del oxígeno separado en el lecho productor. Después de cada ciclo completo, los lechos se intercambian, permitiendo un suministro constante de oxígeno. Esto se conoce por adsorción por oscilación de presión y se utiliza para producir oxígeno a pequeña escala.

El oxígeno también puede producirse mediante la electrólisis del agua, descomponiéndola en oxígeno e hidrógeno, para lo cual debe usarse una corriente continua; si se usara una corriente alterna, los gases de cada extremo consistirían en hidrógeno y oxígeno en la explosiva relación 2:1. Contrariamente a la creencia popular, la relación 2:1 observada en la electrólisis de corriente continua del agua acidificada no demuestra que la fórmula empírica del agua sea HO, a menos que se asuman ciertas premisas sobre la fórmula molecular del hidrógeno y el oxígeno. Un método similar es la evolución electrocatalítica del O de óxidos a oxoácidos. También se pueden usar catalizadores químicos, como en el generador químico de oxígeno o en las velas de oxígeno que se usan en el equipamiento de apoyo en submarinos y que aún son parte del equipamiento estándar en aerolíneas comerciales para casos de despresurización. Otra tecnología de separación del aire consiste en forzar la disolución del aire a través de membranas de cerámica basadas en dióxido de zirconio, ya sea por alta presión o por corriente eléctrica, para producir O gaseoso prácticamente puro.

Para grandes cantidades, el precio del oxígeno líquido era en 2001 de aproximadamente 0,21 USD/kg. El coste de la energía necesaria para licuar el aire supone el principal coste de producción, por lo cual el coste del oxígeno varía en función del precio de la energía. Por razones de economía, el oxígeno se suele transportar en grandes cantidades en estado líquido, almacenado en tanques especialmente aislados, ya que un litro de oxígeno licuado equivale a 840 litros de oxígeno gaseoso a presión atmosférica y 20 °C (68 °F). Estas cisternas se usan para rellenar los grandes contenedores de oxígeno líquido que se encuentran en el exterior de los hospitales y demás instituciones que necesitan ingentes cantidades de oxígeno gaseoso puro. El oxígeno líquido se pasa por unos intercambiadores de calor que convierten el líquido criogénico en gas antes de que entre en el edificio. El oxígeno también se almacena y envía en cilindros que contienen el gas comprimido, lo que resulta útil para ciertas aplicaciones médicas portátiles y oxicorte.

El 55 % de la producción mundial de oxígeno se consume en la producción de acero Otro 25 % se dedica a la industria química. Del 20 % restante la mayor parte se usa para aplicaciones medicinales, oxicorte, como oxidante en combustible de cohetes y en tratamiento de aguas.

El propósito esencial de la respiración es tomar el O del aire y, en medicina, se usan suplementos de oxígeno. El tratamiento no solo incrementa los niveles de oxígeno en la sangre del paciente, sino que tiene el efecto secundario de disminuir la resistencia al flujo de la sangre en muchos tipos de pulmones enfermos, facilitando el trabajo de bombeo del corazón. La oxigenoterapia se usa para tratar el enfisema, la neumonía, algunas insuficiencias cardíacas, algunos desórdenes que causan una elevada presión arterial pulmonar y cualquier enfermedad que afecte a la capacidad del cuerpo para tomar y usar el oxígeno.

Los tratamientos son lo suficientemente flexibles como para ser usados en hospitales, la vivienda del paciente o, cada vez más, con instrumentos móviles. Así, las tiendas de oxígeno se solían usar como suplementos de oxígeno, pero han ido sustituyéndose por las máscaras de oxígeno y las cánulas nasales.

La medicina hiperbárica (de alta presión) usa cámaras especiales de oxígeno para aumentar la presión parcial del O en el paciente y, cuando son necesarias, en el personal médico. La intoxicación por monóxido de carbono, la mionecrosis (gangrena gaseosa) y el síndrome de descompresión a veces se tratan con estos aparatos. El aumento de la concentración del O en los pulmones ayuda a desplazar el monóxido de carbono del hemogrupo de hemoglobina. El oxígeno es tóxico para la bacteria anaerobia que causa la gangrena gaseosa, de manera que aumentar su presión parcial ayuda a acabar con ellas. El síndrome de descompresión les sucede a los buzos que salen demasiado rápido del mar, lo que resulta en la formación de burbujas de gas inerte, sobre todo nitrógeno, en su sangre.

También se usa oxígeno para pacientes que necesitan ventilación mecánica, normalmente a concentraciones superiores al 21 % encontrado en el aire ambiental. Por otra parte, el isótopo O se usó de forma experimental en la tomografía por emisión de positrones.

Una aplicación notable del O como gas respirable de baja presión se encuentra en los trajes espaciales modernos, que envuelven el cuerpo de sus ocupantes con aire presurizado. Estos dispositivos usan oxígeno casi puro a una presión de alrededor de un tercio de la común, resultando en una presión parcial normal en el O de la sangre. Este intercambio de oxígeno de alta concentración para una baja presión es necesario para mantener la flexibilidad de los trajes espaciales.

Los buceadores y los tripulantes de submarinos también usan O artificialmente proporcionado, pero la mayoría usan una presión normal o una mezcla de oxígeno y aire. El uso de O puro o casi puro en buceo a presiones por encima del nivel del mar se limita generalmente a los descansos, descompresiones y tratamientos de emergencia a relativamente poca profundidad (~6 metros o menos). El buceo a mayor profundidad requiere una dilución significativa de O con otros gases, como nitrógeno o helio, para ayudar a prevenir el efecto de Paul Bert (toxicidad del oxígeno).

Los escaladores de montaña y los que viajan en aviones no presurizados a veces tienen un suplemento de O. Los pasajeros de aviones comerciales (presurizados) tienen un suministro de O para emergencias, que les es puesto automáticamente a su disposición en caso de despresurización de la cabina. Una pérdida repentina de presión en la cabina activa generadores químicos de oxígeno sobre cada asiento y hacen caer máscaras de oxígeno. Al tirar de la máscara para comenzar el flujo de oxígeno, tal y como indican las instrucciones de seguridad, se fuerzan las limaduras de hierro en el clorato de sodio dentro del recipiente. Se produce, entonces, un chorro constante de oxígeno debido a la reacción exotérmica.

El oxígeno, como un supuesto eufórico suave, tiene una historia de uso recreativo en deportes y bares de oxígeno. Estos son establecimientos que aparecieron en Japón, California y Las Vegas a finales de los años 1990 que ofertan exposiciones a niveles de O superiores a lo normal a cambio de una determinada tarifa. Los atletas profesionales, especialmente en fútbol americano, también salen del campo en ocasiones, durante los descansos, para ponerse máscaras de oxígeno y obtener una estimulación en su juego. El efecto farmacológico es dudoso y el efecto placebo es la explicación más factible. Existen estudios que respaldan esa estimulación con mezclas de O enriquecido, pero solo si se inhalan "durante" el ejercicio aeróbico.

La fundición de mena de hierro en acero consume el 55 % del oxígeno producido comercialmente. En este proceso, el O es inyectado mediante una lanza de alta presión en el molde de hierro, que expulsa las impurezas de Azufre y el exceso de Carbono, en forma de sus respectivos óxidos, SO y CO. Las reacciones son exotérmicas y la temperatura asciende hasta los 1700 Cº.

Otro 25 % de este oxígeno se dedica a la industria química. El etileno reacciona con el O para crear óxido de etileno, que, a su vez, se convierte en etilenglicol, el material usado como base para fabricar una gran variedad de productos, incluyendo anticongelantes y polímeros de poliéster (los precursores de muchos plásticos y textiles).

El oxígeno se usa en el oxicorte quemando acetileno con O para producir una llama muy caliente. En este proceso, el metal de hasta 60 centímetros de grosor se calienta primero con una pequeña llama de oxiacetileno para después ser rápidamente cortado por un gran chorro de O.

Los paleoclimatólogos miden la relación entre el oxígeno-18 y el oxígeno-16 en los esqueletos y exoesqueletos de los organismos marinos para determinar cómo era el clima hace millones de años. Las moléculas de agua de mar que contienen el isótopo más ligero, el oxígeno-16, se evaporan a un ritmo ligeramente mayor que las moléculas que contienen oxígeno-18 (un 12 % más pesado); esta disparidad se incrementa a bajas temperaturas. En periodos con una temperatura global más baja, la nieve y la lluvia procedentes de esa agua evaporada tienden a ser más ricas en oxígeno-16, mientras que el agua marina que queda tiende a serlo en oxígeno-18. Los organismos marinos, por tanto, incorporan más oxígeno-18 en sus esqueletos y exoesqueletos de lo que harían en un medio más cálido. Los paleoclimatólogos también miden directamente esta relación en las moléculas de agua de muestras de núcleo de hielo que se han coservado durante varios cientos de miles de años.

Los geólogos planetarios han medido las diferencias en la abundancia de isótopos de oxígeno en muestras de la Tierra, la Luna, Marte y meteoritos, pero no han estado lejos de poder obtener valores de referencia para las relaciones entre isótopos del Sol, que se creen iguales a aquellas de la nebulosa protosolar. Sin embargo, el análisis de una oblea de Silicio expuesta al viento solar en el espacio y devuelta a la Tierra por la sonda Génesis desveló que el Sol tiene una proporción de oxígeno-16 mayor que nuestro planeta. La medición implica que un proceso desconocido agotó el oxígeno-16 del disco protoplanetario del Sol antes de la fusión de los granos de polvo que formaron la Tierra.

El oxígeno presenta dos bandas de absorción espectrofotométrica con máximos en longitudes de onda de 687 y 760 nanómetros. Algunos científicos de detección remota han propuesto usar la medición del resplandor procedente de los doseles de vegetación en aquellas bandas para caracterizar la salud de las plantas desde una plataforma satelital. Esta aproximación explota el hecho de que en esas bandas es posible distinguir la reflectividad de la vegetación de su fluorescencia, que es mucho más débil. La medición tiene una alta dificultad técnica, debido a la baja relación señal/ruido y la estructura física de la vegetación, pero se ha propuesto como un posible método de monitoreo del ciclo del carbono desde satélites a escala global.

El estado de oxidación del oxígeno es -2 en casi todos los compuestos conocidos del oxígeno. Por su parte, el estado de oxidación -1 se encuentra en unos cuantos compuestos, como los peróxidos. Los compuestos en otro estado de oxidación son muy poco comunes: −1/2 (superóxidos), −1/3 (ozónidos), 0 (elemental, hipofluoroso), +1/2 (dioxigenil), +1 (difluoruro de dioxígeno) y +2 (difluoruro de oxígeno).

El agua (HO) es el óxido de hidrógeno y es el compuesto de oxígeno más común. Los átomos de hidrógeno están enlazados covalentemente al oxígeno en la molécula de agua, pero también tienen una atracción adicional (sobre 23,3 kJ•mol por átomo de hidrógeno) con un átomo de oxígeno adyacente de una molécula diferente. Estos enlaces de hidrógeno entre las moléculas de agua las mantienen aproximadamente un 15 % más cerca de lo que sería esperable en un líquido simple solo con las fuerzas de Van der Waals.
Debido a su electronegatividad, el oxígeno forma enlaces químicos con casi todos los demás elementos a temperaturas elevadas para dar los óxidos correspondientes. Sin embargo, algunos elementos forman óxidos directamente a condiciones normales de presión y temperatura, como el orín formado del Hierro. La superficie de metales como el aluminio y el titanio se oxidan en presencia del aire y se cubren con una fina capa de óxido que pasiva el metal y ralentiza la corrosión. Algunos de los óxidos metálicos de transición se encuentran en la naturaleza como compuestos no estequiométricos, con ligeramente menos metal de lo que la fórmula química sugiere. Por ejemplo, el FeO (wustita), que se forma de manera natural, se escribe en realidad como FeO, donde la «x» está normalmente en torno a 0,05.

El oxígeno como compuesto está presente en la atmósfera en pequeñas cantidades en forma de dióxido de carbono (CO). La roca de la corteza terrestre se compone de grandes partes de óxidos de Silicio (dióxido de silicio SiO, que se encuentra en el granito y la arena), aluminio (alúmina AlO, en la bauxita y el corindón), hierro (óxido férrico FeO, en la hematita y el orín) y calcio (carbonato cálcico CaCO, en la caliza). El resto de la corteza terrestre se compone también de compuestos de oxígeno, en particular varios silicatos complejos. En el manto terrestre, de una masa mucho mayor que la corteza, abundan los silicatos de hierro y Magnesio.

Los silicatos solubles en agua con las formas NaSiO, NaSiO y NaSiO se utilizan como detergentes y adhesivos. El oxígeno también actúa como ligazón para metales de transición, formando enlaces de O metálico con el átomo de iridio en el complejo de Vaska, con el platino en el PtF y con el centro de hierro en el grupo hemo de la hemoglobina.

Entre las clases más importantes de compuestos orgánicos que contienen oxígeno están los siguientes (donde «R» es un grupo orgánico): alcoholes (R-OH), éteres (R-O-R), cetonas (R-CO-R), aldehídos (R-CO-H), ácidos carboxílicos (R-COOH), Ésteres (R-COO-R), anhídridos de ácido (R-CO-O-CO-R) y amidas (R-C(O)-NR). Hay muchos disolventes orgánicos importantes que contienen oxígeno, entre ellos: acetona, metanol, etanol, alcohol isopropílico, furano, tetrahidrofurano, éter etílico, dioxano, etanoato de etilo, dimetilformamida, dimetilsulfóxido, ácido acético y ácido fórmico. La acetona (CH(CO)CH) y el fenol (CHOH) se usan como materiales en la síntesis de muchas sustancias diferentes. Otros compuestos orgánicos importantes que contienen oxígeno son: glicerol, formaldehído, glutaraldehído, ácido acético y acetamida. Los epóxidos son éteres en los que el átomo de oxígeno forma parte de un anillo de tres átomos.

El oxígeno reacciona espontáneamente con muchos compuestos orgánicos a temperatura ambiente o inferior, en un proceso llamado autooxidación. La mayor parte de los compuestos orgánicos que contienen oxígeno no se producen por la acción directa del O. Los compuestos orgánicos importantes en la industria y el comercio producidos por oxidación directa de un precursor incluyen al óxido de etileno y el ácido peracético.

El elemento se encuentra en casi todas las biomoléculas importantes para (o generadas por) la vida. Solo unas cuantas biomoléculas complejas comunes, como el escualeno y el caroteno, no contienen oxígeno. De los compuestos orgánicos con relevancia biológica, los carbohidratos contienen la mayor proporción de oxígeno en su masa. Todas las grasas, ácidos grasos, aminoácidos y proteínas contienen oxígeno (debido a la presencia de grupos carbonilos en esos ácidos y sus residuos de éster). El oxígeno también está presente en grupos de fosfato (PO) en las moléculas biológicamente importantes que transportan energía, ATP y ADP, en la columna vertebral y las purinas (excepto la adenina y las pirimidinas de ARN y ADN) y en los huesos como fosfato cálcico e hidroxiapatita.

El O gaseoso puede ser tóxico a presiones parciales elevadas, produciendo convulsiones y otros problemas de salud. La toxicidad generalmente comienza a aparecer con presiones parciales de más de 50 kPa o 2,5 veces la presión parcial del O a nivel del mar (21 kPa; igual a alrededor del 50 % de la composición del oxígeno a presión normal). Esto no resulta un problema excepto para pacientes con ventilación mecánica, debido a que el gas administrado a través de las máscaras de oxígeno se compone típicamente de solo un 30 %-50 % de O por volumen (sobre 30 kPa a presión normal), aunque estas cifras varían sensiblemente dependiendo del tipo de máscara.

Durante un tiempo, los bebés prematuros se colocaban en incubadoras que contenían aire rico en O, pero esta práctica cesó después de que algunos de estos niños perdieran la visión.

La respiración de O puro en aplicaciones espaciales, como en algunos trajes aeroespaciales modernos o en naves pioneras como la Apolo, no causa daños debido a las bajas presiones totales utilizadas. En el caso de los trajes, la presión parcial del O en el gas respiratorio se encuentra, en general, sobre 30 kPa (1,4 veces lo normal) y la presión parcial resultante en la sangre arterial del astronauta solo está marginalmente por encima de lo normal al nivel del mar.

La toxicidad del oxígeno para los pulmones y el sistema nervioso central también puede darse en el buceo profundo y en el buceo profesional. La respiración prolongada de una mezcla de aire con una presión parcial de O mayor a 60 kPa puede llegar a producir una fibrosis pulmonar permanente. La exposición a presiones parciales superiores a 160 kPa (~1,6 atmósferas) podría causar convulsiones, normalmente fatales para los buzos. La toxicidad aguda puede producirse al respirar una mezcla de aire con más de un 21 % de O a 66 o más metros de profundidad; lo mismo puede ocurrir al respirar un 100 % de O a solo 6 metros.

Las fuentes de oxígeno que están altamente concentradas estimulan una rápida combustión. Los riesgos de fuego y explosión se dan cuando los oxidantes concentrados y los combustibles se sitúan demasiado cerca entre sí; sin embargo, la ignición, ya sea por el calor o por una chispa, es necesaria para iniciar la combustión. El oxígeno en sí mismo no es un combustible, sino un oxidante. Los riesgos de la combustión también se aplican a compuestos de oxígeno de alto potencial oxidante, como los peróxidos, cloratos, nitratos, percloratos y dicromatos, porque pueden dar oxígeno al fuego.

El O concentrado permite una combustión rápida y enérgica. Las tuberías y los recipientes de acero usados para almacenar y trasmitir tanto el oxígeno líquido como el gaseoso actúan como combustible; por tanto, el diseño y la fabricación de los sistemas de O requieren una atención especial para asegurar que las fuentes de ignición se minimizan. El incendio que acabó con la vida de la tripulación del Apolo 1 en una prueba en la plataforma de lanzamiento se extendió tan rápidamente debido a que la cápsula estaba presurizada con O puro, pero a una presión ligeramente mayor que la atmosférica, en lugar de una presión de 1/3 de la normal que debía usarse en la misión.

En caso de un derrame de oxígeno líquido, si este llega a empaparse en materia orgánica como madera, productos petroquímicos y asfalto puede provocar que estos materiales detonen de forma impredecible al sufrir un impacto mecánico posterior. Al igual que otros líquidos criogénicos, en contacto con el cuerpo humano puede causar congelamiento en piel y ojos.




</doc>
<doc id="3268" url="https://es.wikipedia.org/wiki?curid=3268" title="Masa atómica">
Masa atómica

La masa atómica es la masa de un átomo, más frecuentemente expresada en unidades de masa atómica unificada. *La masa atómica puede ser considerada como la masa total de protones y neutrones (pues la masa de los electrones en el átomo es prácticamente despreciable) en un solo átomo* (cuando el átomo no tiene movimiento). 
La masa atómica es algunas veces usada incorrectamente como un sinónimo de masa atómica relativa, masa atómica media y peso atómico; estos últimos difieren sutilmente de la masa atómica. *La masa atómica está definida como la masa de un átomo, que sólo puede ser de un isótopo a la vez, y no es un promedio ponderado en las abundancias de los isótopos.* En el caso de muchos elementos que tienen un isótopo dominante, la similitud/diferencia numérica real entre la masa atómica del isótopo más común y la masa atómica relativa o peso atómico estándar puede ser muy pequeña, tal que no afecta muchos cálculos bastos, pero tal error puede ser crítico cuando se consideran átomos individuales. Para elementos con más de un isótopo común, la diferencia puede llegar a ser de media unidad o más (por ejemplo, cloro). La masa atómica de un isótopo raro puede diferir de la masa atómica relativa o peso atómico estándar en varias unidades de masa.

El peso atómico estándar se refiere a la media de las masas atómicas relativas de un elemento en el medio local de la corteza terrestre y la atmósfera terrestre, como está determinado por la "Commission on Atomic Weights and Isotopic Abundances" (Comisión de Pesos Atómicos y Abundancias Isotópicas) de la IUPAC. Estos valores son los que están incluidos en una tabla periódica estándar, y es lo que es más usado para los cálculos ordinarios. Se incluye una incertidumbre en paréntesis que frecuentemente refleja la variabilidad natural en la distribución isotópica, en vez de la incertidumbre en la medida. Para los elementos sintéticos, el isótopo formado depende de los medios de síntesis, por lo que el concepto de abundancia isotópica natural no tiene sentido. En consecuencia, para elementos sintéticos, el conteo total de nucleones del isótopo más estable (esto es, el isótopo con la vida media más larga) está listado en paréntesis en el lugar del peso atómico estándar. El litio representa un caso único, donde la abundancia natural de los isótopos ha sido perturbada por las actividades humanas al punto de afectar la incertidumbre en su peso atómico estándar, incluso en muestras obtenidas de fuentes naturales, como los ríos.

La masa atómica relativa es un sinónimo para peso atómico y está cercanamente relacionado a masa atómica promedio (pero no es un sinónimo de masa atómica), la media ponderada de las masas atómicas de todos los átomos de un elemento químico encontrados en una muestra particular, ponderados por abundancia isotópica. Esto es usado frecuentemente como sinónimo para peso atómico relativo, y no es incorrecto hacer así, dado que los pesos atómicos estándar son masas atómicas relativas, aunque es menos específico. La masa atómica relativa también se refiere a ambientes no terrestres y ambientes terrestres altamente específicos que se desvían de la media o tienen diferentes certidumbres (número de cifras significativas) que los pesos atómicos estándar.

La masa isotópica relativa es la masa relativa de un isótopo dado (más específica, cualquier núclido solo), escalado con el carbono-12 como exactamente 12. No hay otros núclidos distintos al carbono-12 que tengan exactamente un número entero de masas en esta escala. Esto es debido a dos factores: [1] la diferente masa de neutrones y protones que actúan para cambiar la masa total en los núclidos con relaciones protón/neutrón distintos al cociente 1:1 del carbono-12; y [2] no se encontrará un número exacto si existe una pérdida/ganancia de masa diferente a la energía de enlace nuclear relativa a la energía de enlace nuclear media del carbono-12, sin embargo, puesto que cualquier defecto de masa debido a la energía de enlace nuclear es una fracción pequeña (menos del 1 %) comparada con la masa de un nucleón (incluso menos comparado con la masa media por nucleón en el carbono-12, que está moderada a fuertemente unido), y dado que los protones y neutrones difieren en masa unos de otros por una fracción pequeña (aproximadamente 0,0014 uma), la práctica de redondear la masa atómica de cualquier núclido dado o isótopo al número entero más cercano, siempre da el número entero simple de la suma total de nucleones. El conteo de neutrones puede ser derivado por sustracción del número atómico.

La cantidad que las masas atómicas se desvían de su número de masa es como sigue: la desviación empieza, positiva en el hidrógeno-1, disminuyendo hasta alcanzar un mínimo en el hierro-56, hierro-58 y níquel-62, luego aumenta a valores positivos en los isótopos más pesados, conforme aumenta el número atómico. Esto corresponde a lo siguiente: la fisión nuclear en un elemento más pesado que el hierro produce energía, y la fisión de cualquier elemento más ligero que el hierro requiere energía. Lo opuesto es verdadero para las reacciones de fusión nuclear: la fusión en los elementos más ligeros que el hierro produce energía, y la fusión en los elementos más pesados que el hierro requiere energía.

El proceso que se siguió históricamente para determinar las masas reales de los átomos de los diferentes elementos fue similar al seguido en el modelo clips, trabajando inicialmente con gases y comparando las masas de gases situados en recipientes con las mismas condiciones de presión, volumen y temperatura: como las masas eran distintas, pero había el mismo número de partículas (de acuerdo con el modelo de materia y el principio de Avogadro), se debía a que las partículas tenían masas reales diferentes.Actualmente la comparación directa y medición de las masas de los átomos se logra con la utilización de un espectrómetro de masas.

La unidad científica estándar para manejar átomos en cantidades macroscópicas es el mol, que está definido arbitrariamente como la cantidad de sustancia que tiene tantos átomos u otra unidad como átomos hay en 12 gramos de carbono del isótopo C-12. El número de átomos en un mol es denominado número de Avogadro, cuyo valor es aproximadamente 6,022 x 10 mol. Un mol de una sustancia siempre contiene exactamente la "masa atómica relativa" o "masa molar" de dicha sustancia, expresado en gramos; sin embargo, esto no es cierto para la "masa atómica". Por ejemplo, el peso atómico estándar del hierro es 55,847 g/mol, y en consecuencia un mol de hierro como se suele encontrar en la Tierra tiene una masa de 55,847 gramos. La "masa atómica" del isótopo Fe es 55,935 u, y un mol de Fe pesará, en teoría, 55,935 g, pero no se ha encontrado tales cantidades puras de isótopo Fe en la Tierra.

La fórmula para la conversión entre unidad de masa atómica y la masa SI en gramos para un solo átomo es:

donde formula_2 es la constante de masa molar y formula_3 es el número de Avogadro.

Se aplican definiciones similares a las moléculas. Se puede calcular la masa molecular de un compuesto por adición de las masas atómicas-moleculares de sus átomos constituyentes (núclidos). También se puede calcular la masa molar indefinida por la adición de las masas atómicas relativas de los elementos dados en la fórmula molecular. En ambos casos, la multiplicidad de los átomos (el número de veces que está presente) debe ser tomado en cuenta, generalmente multiplicando cada masa única por su multiplicidad inversa.

En la historia de la química, los primeros científicos en determinar los pesos atómicos fueron John Dalton entre 1803 y 1808, y Jöns Jakob Berzelius entre 1808 y 1826. Los pesos atómicos fueron definidos originalmente en relación al elemento hidrógeno, el más ligero, tomándolo como 1, y en 1820, la hipótesis de Prout indicaba que las masas atómicas de todos los elementos deberían ser un múltiplo entero del peso del hidrógeno. Sin embargo, Berzelius pronto probó que esta hipótesis no siempre se sostenía, y en algunos casos, como el cloro, el peso atómico caía casi exactamente entre dos múltiplos del peso del hidrógeno. Posteriormente, se mostró que esto se debía a un efecto causado por los isótopos, y que la masa atómica de los isótopos puros, o núclidos, era múltiplo de la masa del hidrógeno, en un margen de diferencia del 0,96%.

En la década de 1860, Stanislao Cannizzaro refinó los pesos atómicos aplicando la ley de Avogadro (en el Congreso de Karlsruhe de 1860). Formuló una ley para determinar los pesos atómicos de los elementos: "las distintas cantidades del mismo elemento contenido en distintas moléculas son todas múltiplos enteros del peso atómico", y determinó los pesos atómicos y pesos moleculares comparando la densidad de vapor de un conjunto de gases con moléculas conteniendo uno o más del elemento químico en cuestión.

A principios del siglo XX, hasta la década de 1960, los químicos y físicos utilizaban dos escalas de masa atómicas. Los químicos usaban una escala tal que la mezcla natural de isótopos de oxígeno tenía una masa atómica de 16, mientras que los físicos asignaron el mismo número 16 a la masa atómica del isótopo de oxígeno más común (que contiene ocho protones y ocho neutrones). Sin embargo, debido a que también están presentes en el oxígeno natural, tanto el oxígeno-17 como el oxígeno-18, esto conducía a 2 tablas diferentes de masas atómicas. La escala unificada, basada en el carbono-12, C, cumplía el requerimiento de los físicos de basar la escala en un isótopo puro, a la vez que se hacía numéricamente cercana a la escala de los químicos.




</doc>
<doc id="3269" url="https://es.wikipedia.org/wiki?curid=3269" title="Punto de fusión">
Punto de fusión

El punto de fusión es la temperatura a la cual se encuentra el equilibrio de fases sólido-líquido, es decir, la materia pasa de estado sólido a estado líquido, se funde. Cabe destacar que el cambio de fase ocurre a temperatura constante. El punto de fusión es una propiedad intensiva porque no depende de la masa de la muestra.

En la mayoría de las sustancias, el punto de fusión y de congelación, son iguales. Pero esto no siempre es así: por ejemplo, el agar-agar se funde a 85 °C y se solidifica a partir de los 31 a 40 °C; este proceso se conoce como histéresis.

A diferencia del punto de ebullición, el punto de fusión de una sustancia es poco afectado por la presión y, por lo tanto, puede ser utilizado para caracterizar compuestos orgánicos y para comprobar su pureza.

El punto de fusión de una sustancia pura es siempre más alto y tiene una gama más pequeña de variación que el punto de fusión de una sustancia impura. Cuanto más impura sea, más bajo es el punto de fusión y más amplia es la gama de variación. Eventualmente, se alcanza un punto de fusión mínimo. El cociente de la mezcla que da lugar al punto de fusión posible más bajo se conoce como el punto eutéctico, temperatura correspondiente a cada átomo de la sustancia a la que se somete a fusión.

El punto de fusión de un compuesto puro, en muchos casos se da con una sola temperatura, ya que el intervalo de fusión puede ser muy pequeño (menor a 1 °C). En cambio, si hay impurezas, estas provocan que el punto de fusión disminuya y el intervalo de fusión se amplíe. Por ejemplo, el punto de fusión del ácido benzoico impuro podría ser:

La tabla siguiente muestra las temperaturas de fusión de los elementos en °C (a una atmósfera de presión):



</doc>
<doc id="3271" url="https://es.wikipedia.org/wiki?curid=3271" title="Punto de ebullición">
Punto de ebullición

La definición formal de punto de ebullición es "aquella temperatura en la que la presión de vapor del líquido iguala la presión de vapor del medio en el que se encuentra". Coloquialmente, se dice que es la temperatura a la cual la materia cambia del estado líquido al estado gaseoso.

La temperatura de una sustancia o cuerpo depende de la energía cinética media de las moléculas. A temperaturas inferiores al punto de ebullición, solo una pequeña fracción de las moléculas en la superficie tiene energía suficiente para romper la tensión superficial y escapar. Este incremento de energía constituye un intercambio de calor que da lugar al aumento de la entropía del sistema (tendencia al desorden de las partículas que componen su cuerpo).

El punto de ebullición depende de la masa molecular de la sustancia y del tipo de las fuerzas intermoleculares de esta sustancia. Para ello se debe determinar si la sustancia es covalente polar, covalente no polar, y determinar el tipo de enlaces (dipolo permanente —dipolo inducido o puentes de hidrógeno—).

El punto de ebullición no puede elevarse en forma indefinida. Conforme se aumenta la presión, la densidad de la fase gaseosa aumenta hasta que, finalmente, se vuelve indistinguible de la fase líquida con la que está en equilibrio; esta es la temperatura crítica, por encima de la cual no existe una fase líquida clara. El helio tiene el punto normal de ebullición más bajo (–268,9 °C) de los correspondientes a cualquier sustancia, y el carburo de wolframio, uno de los más altos (5555 °C).

El punto de ebullición normal puede ser calculado mediante la fórmula de Clausius-Clapeyron:

formula_1

En las tablas termodinámicas de productos químicos, no se indica todo el diagrama de fase, solo la temperatura de ebullición en el estado estándar, es decir, con una presión de una atmósfera (1013,25 hPa). Este punto de ebullición se denomina punto de ebullición normal y la temperatura temperatura de ebullición normal. El término "punto de ebullición" se utiliza a menudo para referirse al punto de ebullición normal.

La siguiente tabla muestra las temperaturas de ebullición en el estado estándar (1 atm) en °C:


</doc>
<doc id="3273" url="https://es.wikipedia.org/wiki?curid=3273" title="Microscopio">
Microscopio

El microscopio (del griego μικρός "micrós", ‘pequeño’, y σκοπέω "scopéo", ‘mirar’) es un instrumento que permite observar objetos que son demasiado pequeños para ser observados a simple vista. El tipo más común y el primero que fue inventado es el microscopio óptico. Se trata de un instrumento óptico que contiene dos lentes que permiten obtener una imagen aumentada del objeto y que funciona por refracción. 
La ciencia que investiga los objetos pequeños utilizando este instrumento se llama microscopía.
El microscopio fue inventado por Zacharias Janssen en 1590. En 1665 aparece en la obra de William Harvey sobre la circulación sanguínea al mirar al microscopio los capilares sanguíneos, y Robert Hooke publicó su obra "Micrographia". 

En 1665 Robert Hooke observó con un microscopio un delgado corte de corcho y notó que el material era poroso, en su conjunto, formaban cavidades poco profundas a modo de celditas a las que llamó "células". Se trataba de la primera observación de células muertas. Unos años más tarde, Marcello Malpighi, anatomista y biólogo italiano, observó células vivas. Fue el primero en estudiar tejidos vivos al microscopio.

A mediados del siglo XVII el holandés, Anton van Leeuwenhoek, utilizando microscopios simples de fabricación propia, describió por primera vez protozoos, bacterias, espermatozoides y glóbulos rojos. El microscopista Leeuwenhoek, sin ninguna preparación científica, puede considerarse el fundador de la bacteriología. Tallaba él mismo sus lupas, sobre pequeñas esferas de cristal, cuyos diámetros no alcanzaban el milímetro (su campo de visión era muy limitado, de décimas de milímetro). Con estas pequeñas distancias focales alcanzaba los 275 aumentos. Observó los glóbulos de la sangre, las bacterias y los protozoos; examinó por primera vez los glóbulos rojos y descubrió que el semen contiene espermatozoides. Durante su vida no reveló sus métodos secretos y a su muerte, en 1723, 26 de sus aparatos fueron cedidos a la Royal Society de Londres.

Durante el siglo XVIII continuó el progreso y se lograron objetivos acromáticos por asociación de Chris Neros y Flint Crown obtenidos en 1740 por H. M. Hall y mejorados por John Dollond. De esta época son los estudios efectuados por Isaac Newton y Leonhard Euler. En el siglo XIX, al descubrirse que la dispersión y la refracción se podían modificar con combinaciones adecuadas de dos o más medios ópticos, se lanzan al mercado objetivos acromáticos excelentes.

Durante el siglo XVIII el microscopio tuvo diversos adelantos mecánicos que aumentaron su estabilidad y su facilidad de uso, aunque no se desarrollaron por el momento mejoras ópticas. Las mejoras más importantes de la óptica surgieron en 1877, cuando Ernst Abbe publicó su teoría del microscopio y, por encargo de Carl Zeiss, mejoró la microscopía de inmersión sustituyendo el agua por aceite de cedro, lo que permite obtener aumentos de 2000. A principios de los años 1930 se había alcanzado el límite teórico para los microscopios ópticos, no consiguiendo estos aumentos superiores a 500X o 1,000X. Sin embargo, existía un deseo científico de observar los detalles de estructuras celulares (núcleo, mitocondria, etc.).
El microscopio electrónico de transmisión (TEM) fue el primer tipo de microscopio electrónico desarrollado. Utiliza un haz de electrones en lugar de luz para enfocar la muestra consiguiendo aumentos de 100.000X. Fue desarrollado por Max Knoll y Ernst Ruska en Alemania en 1931. Posteriormente, en 1942 se desarrolla el microscopio electrónico de barrido.




</doc>
<doc id="3274" url="https://es.wikipedia.org/wiki?curid=3274" title="Evolución (desambiguación)">
Evolución (desambiguación)

El término evolución puede referirse a:

En biología y antropología:

En física:

En astronomía:

En el arte:

En automovilismo:

Otros usos:

</doc>
<doc id="3275" url="https://es.wikipedia.org/wiki?curid=3275" title="Evolución biológica">
Evolución biológica

La evolución biológica es el conjunto de cambios en caracteres fenotípicos y genéticos de poblaciones biológicas a través de generaciones. Dicho proceso ha originado la diversidad de formas de vida que existen sobre la Tierra a partir de un antepasado común. Los procesos evolutivos han producido la biodiversidad en cada nivel de la organización biológica, incluyendo los de especie, población, organismos individuales y molecular (evolución molecular). Toda la vida en la Tierra procede de un último antepasado común universal que existió entre hace 3800 y 3500 millones de años.

La palabra evolución para describir tales cambios fue aplicada por primera vez en el siglo XVIII por un biólogo suizo, Charles Bonnet, en su obra "Consideration sur les corps organisés".
No obstante, el concepto de que la vida en la Tierra evolucionó a partir de un ancestro común ya había sido formulado por varios filósofos griegos, y la hipótesis de que las especies se transforman continuamente fue postulada por numerosos científicos de los siglos XVIII y XIX, a los cuales Charles Darwin citó en el primer capítulo de su libro "El origen de las especies".
Sin embargo, fue el propio Darwin en 1859, quien sintetizó un cuerpo coherente de observaciones y aportó un mecanismo de cambio, al que llamó selección natural; lo que consolidó el concepto de la evolución biológica en una verdadera teoría científica.

La evolución como propiedad inherente a los seres vivos no es a la primera década del siglo XXI materia de debate en la comunidad científica relacionada con su estudio; sin embargo, los mecanismos que explican la transformación y diversificación de las especies se hallan bajo intensa y continua investigación científica.

Dos naturalistas, Charles Darwin y Alfred Russel Wallace, propusieron en forma independiente en 1858 que la selección natural era el mecanismo básico responsable del origen de nuevas variantes genotípicas y en última instancia, de nuevas especies.
Actualmente, la teoría de la evolución combina las propuestas de Darwin y Wallace con las leyes de Mendel y otros avances posteriores en la genética; por eso se la denomina síntesis moderna o «teoría sintética».
Según esta teoría, la evolución se define como un cambio en la frecuencia de los alelos de una población a lo largo de las generaciones. Este cambio puede ser causado por diferentes mecanismos, tales como la selección natural, la deriva genética, la mutación y la migración o flujo genético. La teoría sintética recibe en la actualidad una aceptación general de la comunidad científica, aunque también algunas críticas. Ha sido enriquecida desde su formulación, en torno a 1940, gracias a los avances de otras disciplinas relacionadas, como la biología molecular, la genética del desarrollo o la paleontología.
Actualmente se continúan elaborando hipótesis sobre los mecanismos del cambio evolutivo basándose en datos empíricos tomados de organismos vivos.

Las evidencias del proceso evolutivo son el conjunto de pruebas que los científicos han reunido para demostrar que la evolución es un proceso característico de la materia viva y que todos los organismos que viven en la Tierra descienden de un último antepasado común universal.
Las especies actuales son un estado en el proceso evolutivo y su riqueza relativa y niveles de complejidad biológica son el producto de una larga serie de eventos de especiación y de extinción.

La existencia de un ancestro común puede deducirse a partir de unas características simples de los organismos. Primero, existe evidencia proveniente de la biogeografía. El estudio de las áreas de distribución de las especies muestra que cuanto más alejadas o aisladas están dos áreas geográficas más diferentes son las especies que las ocupan, aunque ambas áreas tengan condiciones ecológicas similares (como la región ártica y la Antártida, o la región mediterránea y California). Segundo, la diversidad de la vida sobre la Tierra no se resuelve en un conjunto de organismos completamente únicos, sino que los mismos comparten una gran cantidad de similitudes morfológicas. Así, cuando se comparan los órganos de los distintos seres vivos, se encuentran semejanzas en su constitución que señalan el parentesco que existe entre las especies. Estas semejanzas y su origen permiten clasificar a los órganos en homólogos, si tienen un mismo origen embrionario y evolutivo, y análogos, si tienen diferente origen embrionario y evolutivo pero la misma función. Tercero, los estudios anatómicos también permiten reconocer en muchos organismos la presencia de órganos vestigiales, que están reducidos y no tienen función aparente, pero que muestran claramente que derivan de órganos funcionales presentes en otras especies, tales como los huesos rudimentarios de las patas posteriores presentes en algunas serpientes.

La embriología, a través de los estudios comparativos de las etapas embrionarias de distintas clases de animales, ofrece el cuarto conjunto de evidencias del proceso evolutivo. Se ha encontrado que en estas primeras etapas del desarrollo, muchos organismos muestran características comunes que sugieren la existencia de un patrón de desarrollo compartido entre ellas, lo que, a su vez, demuestra la existencia de un antepasado común. El sorprendente hecho de que los embriones tempranos de mamíferos posean hendiduras branquiales, que luego desaparecen conforme avanza el desarrollo, demuestra que los mamíferos se hallan emparentados con los peces.

El quinto grupo de evidencias proviene del campo de la sistemática. Los organismos pueden ser clasificados usando las similitudes mencionadas en grupos anidados jerárquicamente, muy similares a un árbol genealógico.
Si bien las investigaciones modernas sugieren que, debido a la transferencia horizontal de genes, este árbol de la vida puede ser más complicado que lo que se pensaba, ya que muchos genes se han distribuido independientemente entre especies distantemente relacionadas.

Las especies que han vivido en épocas remotas han dejado registros de su historia evolutiva. Los fósiles, conjuntamente con la anatomía comparada de los organismos actuales, constituyen la evidencia paleontológica del proceso evolutivo. Mediante la comparación de las anatomías de las especies modernas con las ya extintas, los paleontólogos pueden inferir los linajes a los que unas y otras pertenecen. Sin embargo, la aproximación paleontológica para buscar evidencia evolutiva tiene ciertas limitaciones. De hecho, es particularmente útil solo en aquellos organismos que presentan partes del cuerpo duras, tales como caparazones, dientes o huesos. Más aún, ciertos otros organismos, como los procariotas ―las bacterias y arqueas― presentan una cantidad limitada de características comunes, por lo que sus fósiles no proveen información sobre sus ancestros.

Una aproximación más reciente para hallar evidencia que respalde el proceso evolutivo es el estudio de las similitudes bioquímicas entre los organismos. Por ejemplo, todas las células utilizan el mismo conjunto básico de nucleótidos y aminoácidos.
El desarrollo de la genética molecular ha revelado que el registro evolutivo reside en el genoma de cada organismo y que es posible datar el momento de la divergencia de las especies a través del reloj molecular producido por las mutaciones acumuladas en el proceso de evolución molecular.
Por ejemplo, la comparación entre las secuencias del ADN del humano y del chimpancé ha confirmado la estrecha similitud entre las dos especies y han arrojado luz acerca de cuándo existió el ancestro común de ambas.

El origen de la vida, aunque atañe al estudio de los seres vivos, es un tema que no es abordado por la teoría de la evolución; pues esta última sólo se ocupa del cambio en los seres vivos, y no del origen, cambios e interacciones de las moléculas orgánicas de las que éstos proceden.
No se sabe mucho sobre las etapas más tempranas y previas al desarrollo de la vida, y los intentos realizados para tratar de desvelar la historia más temprana del origen de la vida generalmente se enfocan en el comportamiento de las macromoléculas, debido a que el consenso científico actual es que la compleja bioquímica que constituye la vida provino de reacciones químicas simples, si bien persisten las controversias acerca de cómo ocurrieron las mismas.
Tampoco está claro cuáles fueron los primeros desarrollos de la vida (protobiontes), la estructura de los primeros seres vivos o la identidad y la naturaleza del último antepasado común universal.
En consecuencia, no hay consenso científico sobre cómo comenzó la vida, si bien se ha propuesto que el inicio de la vida pueden haber sido moléculas autorreplicantes como el ARN, o ensamblajes de células simples denominadas nanocélulas.
Sin embargo, los científicos están de acuerdo en que todos los organismos existentes comparten ciertas características ―incluyendo la presencia de estructura celular y de código genético― que estarían relacionadas con el origen de la vida.

La razón biológica por la que todos los organismos vivos en la Tierra deben compartir un único y último antepasado común universal, es porque sería prácticamente imposible que dos o más linajes separados pudieran haber desarrollado de manera independiente los muchos complejos mecanismos bioquímicos comunes a todos los organismos vivos.
Se ha mencionado anteriormente que las bacterias son los primeros organismos en los que la evidencia fósil está disponible, las células son demasiado complejas para haber surgido directamente de los materiales no vivos.
La falta de evidencia geoquímica o fósil de organismos anteriores ha dejado un amplio campo libre para las hipótesis, que se dividen en dos ideas principales: 1) Que la vida surgió espontáneamente en la Tierra. 2) Que esta fue «sembrada» de otras partes del universo.

Detallados estudios químicos basados en isótopos de carbono de rocas del eón Arcaico sugieren que las primeras formas de vida emergieron en la Tierra probablemente hace más de 3800 millones de años, en la era Eoarcaica, y hay claras evidencias geoquímicas ―tales como la reducción microbiana de sulfatos― que la atestiguan en la era Paleoarcaica, hace 3470 millones de años.
Los estromatolitos ―capas de roca producidas por comunidades de microorganismos― más antiguos se conocen en estratos de 3450 millones de años, mientras que los microfósiles filiformes más antiguos, morfológicamente similares a cianobacterias, se encuentran en estratos de sílex de 3450 millones de años hallados en Australia.

Asimismo, los fósiles moleculares derivados de los lípidos de la membrana plasmática y del resto de la célula ―denominados «biomarcadores»― confirman que ciertos organismos similares a cianobacterias habitaron los océanos arcaicos hace más de 2700 millones de años. Estos microbios fotoautótrofos liberaron oxígeno a la atmósfera, el que comenzó a acumularse hace aproximadamente 2200 millones de años y subsecuentemente transformó definitivamente la atmósfera terrestre.
La aparición de la fotosíntesis y el posterior surgimiento de una atmósfera rica en oxígeno y no reductora, puede también rastrearse a través de los depósitos laminares de hierro y bandas rojas posteriores, producto de los óxidos de hierro. Éste fue un requisito necesario para el desarrollo de la respiración celular aeróbica, la cual se estima que emergió hace aproximadamente 2000 millones de años.

Los procariotas, entonces, habitaron la Tierra desde hace 3000 a 4000 millones de años.
Durante los siguientes miles de millones de años no ocurrió ningún cambio significativo en la morfología u organización celular en estos organismos.

El siguiente cambio sustancial en la estructura celular lo constituyen los eucariotas, los cuales surgieron a partir de bacterias antiguas envueltas, incluidas, en la estructura de los ancestros de las células eucariotas, formando una asociación cooperativa denominada endosimbiosis.
Las bacterias envueltas y su célula hospedante iniciaron un proceso de coevolución, por el cual las bacterias originaron las mitocondrias o hidrogenosomas.
Un segundo evento independiente de endosimbiosis con organismos similares a cianobacterias llevó a la formación de los cloroplastos en las algas y plantas. La evidencia tanto bioquímica como paleontológica indica que las primeras células eucarióticas surgieron hace unos 2000 a 1500 millones de años, a pesar de que los atributos clave de la fisiología de los eucariotas probablemente evolucionaron previamente.

La historia de la vida sobre la Tierra fue la de los eucariotas unicelulares, procariotas y arqueas hasta hace aproximadamente 610 millones de años, momento en el que los primeros organismos multicelulares aparecieron en los océanos en el período denominado Ediacárico.
Algunos organismos ediacáricos podrían haber estado estrechamente relacionados con grupos que más adelante se convertirían en prominentes; tales como los poríferos o los cnidarios.
No obstante, debido a la dificultad a la hora de deducir las relaciones evolutivas en estos organismos, algunos paleontólogos han sugerido que la biota de Ediacara representa una rama completamente extinta, un «experimento fallido» de la vida multicelular, que supuso que la vida multicelular posterior volviera a evolucionar más adelante a partir de organismos unicelulares no relacionados.

La evolución de los organismos pluricelulares ocurrió entonces en múltiples eventos independientes, en organismos tan diversos como las esponjas, algas pardas, cianobacterias, hongos mucosos y mixobacterias.

Poco después de la aparición de estos primeros organismos multicelulares, una gran diversidad biológica apareció en un período de diez millones de años, en un evento denominado explosión cámbrica, un lapso breve en términos geológicos pero que implicó una diversificación animal sin paralelo y el cual está documentado en los fósiles encontrados en los sedimentos de Burgess Shale, Canadá. Durante este período, la mayoría de los filos animales aparecieron en los registros fósiles, como así también una gran cantidad de linajes únicos que ulteriormente se extinguieron. La mayoría de los planes corporales de los animales modernos se originaron durante este período.
Varios desencadenantes de la explosión cámbrica han sido propuestos, incluyendo la acumulación de oxígeno en la atmósfera debido a la fotosíntesis.
Aproximadamente hace 500 millones de años, las plantas y los hongos colonizaron la tierra y fueron rápidamente seguidos por los artrópodos y otros animales.
Los anfibios aparecieron en la historia de la Tierra hace alrededor de 300 millones de años, seguidos por los primeros amniotas, luego los mamíferos hace unos 200 millones de años y las aves hace 100 millones de años. Sin embargo, a pesar de la evolución de estos filos, los organismos microscópicos, similares a aquellos que evolucionaron tempranamente en el proceso, continúan siendo altamente exitosos y dominan la Tierra ya que la mayor parte de las especies y la biomasa terrestre está constituida por procariotas.

Varios filósofos griegos de la antigüedad discutieron ideas que involucraban cambios en los organismos vivos a través del tiempo. Anaximandro (ca. 610-546 a. C.) propuso que los primeros animales vivían en el agua y que los animales terrestres fueron generados a partir de ellos.
Empédocles (ca. 490-430 a. C.) escribió acerca de un origen no sobrenatural de los seres vivos, sugiriendo que la adaptación no requiere un organizador o una causa final.
Aristóteles (384-322 a. C.), uno de los filósofos griegos más influyentes, es además el primer naturalista cuyo trabajo se ha conservado con detalle. Las obras de Aristóteles contiene algunas observaciones e interpretaciones muy astutas conjuntamente con mitos y errores diversos que reflejan el estado irregular del conocimiento en su época.
No obstante, es notable el esfuerzo de Aristóteles en exponer las relaciones existentes entre los seres vivos como una "scala naturae" ―tal como se describe en "Historia animalium"― en la que los organismos se clasifican de acuerdo con una estructura jerárquica, «escalera de la vida» o «cadena del Ser», ordenándolos según la complejidad de sus estructuras y funciones, con los organismos que muestran una mayor vitalidad y capacidad de movimiento descritos como «organismos superiores».

Algunos antiguos pensadores chinos expresaron ideas sobre el cambio de las especies biológicas. Zhuangzi, un filósofo taoísta que vivió alrededor del siglo IV a. C., mencionó que las formas de vida tienen una habilidad innata o el poder (hua 化) para transformarse y adaptarse a su entorno.
Según Joseph Needham, el taoísmo niega explícitamente la fijeza de las especies biológicas y los filósofos taoístas especularon que las mismas han desarrollado diferentes atributos en respuesta a distintos entornos. De hecho, el taoísmo se refiere a los seres humanos, la naturaleza y el cielo como existentes en un estado de «constante transformación», en contraste con la visión más estática de la naturaleza típica del pensamiento occidental.

Si bien la idea de la evolución biológica ha existido desde épocas remotas y en diferentes culturas (por ejemplo, en la musulmana la esbozó en el siglo IX Al-Jahiz y en el siglo XIII Nasir al-Din al-Tusi ), la teoría moderna no se estableció hasta llegados los siglos XVIII y XIX", con la contribución de científicos como Christian Pander, Jean-Baptiste Lamarck y Charles Darwin.

En el siglo XVIII la oposición entre fijismo y transformismo fue ambigua. Algunos autores, por ejemplo, admitieron la transformación de las especies limitada a los géneros, pero negaban la posibilidad de pasar de un género a otro. Otros naturalistas hablaban de «progresión» en la naturaleza orgánica, pero es muy difícil determinar si con ello hacían referencia a una transformación real de las especies o se trataba, simplemente, de una modulación de la clásica idea de la "scala naturae".

Jean-Baptiste Lamarck (1744-1829) formuló la primera teoría de la evolución. Propuso que la gran variedad de organismos, que en aquel tiempo se aceptaba, eran formas estáticas creadas por Dios, habían evolucionado desde formas simples; postulando que los protagonistas de esa evolución habían sido los propios organismos por su capacidad de adaptarse al ambiente: los cambios en ese ambiente generaban nuevas necesidades en los organismos y esas nuevas necesidades conllevarían una modificación de los mismos que sería heredable. Se apoyó para la formulación de su teoría en la existencia de restos de formas intermedias extintas.
Con esta teoría Lamarck se enfrentó a la creencia general por la que todas las especies habían sido creadas y permanecían inmutables desde su creación y también se enfrentó al influyente Georges Cuvier (1769-1832) que justificó la desaparición de las especies, no porque fueran formas intermedias entre las primigenias y las actuales, sino porque se trataba de formas de vida diferentes, extinguidas en los diferentes cataclismos geológicos sufridos por la Tierra.

No fue sino hasta la publicación de "El origen de las especies" de Charles Darwin cuando el hecho de la evolución comenzó a ser ampliamente aceptado. Una carta de Alfred Russel Wallace, en la cual revelaba su propio descubrimiento de la selección natural, impulsó a Darwin a publicar su trabajo en evolución. Por lo tanto, a veces se comparte el crédito con Wallace por la teoría de la evolución (a veces llamada también "teoría de Darwin-Wallace").

El debate más interesante que se realizó en el campo evolutivo los sostuvieron los naturalistas franceses Georges Cuvier y Étienne Geoffroy Saint-Hilaire por el año de 1830 en relación al uniformismo y el catastrofismo. 

A pesar de que la teoría de Darwin pudo sacudir profundamente la opinión científica con respecto al desarrollo de la vida (e incluso resultando en una pequeña revolución social), no pudo explicar la fuente de variación existente entre las especies, y además la propuesta de Darwin de la existencia de un mecanismo hereditario (pangénesis) no satisfizo a la mayoría de los biólogos. No fue recién hasta fines del siglo XIX y comienzos del XX, que estos mecanismos pudieron establecerse.

Cuando alrededor del 1900 se «redescubrió» el trabajo de Gregor Mendel sobre la naturaleza de la herencia que databa de fines del siglo XIX, se estableció una discusión entre los mendelianos (Charles Benedict Davenport) y los biométricos (Walter Frank Raphael Weldon y Karl Pearson), quienes insistían en que la mayoría de los caminos importantes para la evolución debían mostrar una variación continua que no era explicable a través del análisis mendeliano. Finalmente, los dos modelos fueron conciliados y fusionados, principalmente a través del trabajo del biólogo y estadístico Ronald Fisher.
Este enfoque combinado, que empleaba un modelo estadístico riguroso a las teorías de Mendel de la herencia vía genes, se dio a conocer en los años 1930 y 1940 y se conoce como la teoría sintética de la evolución.

En los años de la década de 1940, siguiendo el experimento de Griffith, Avery, MacLeod y McCarty lograron identificar de forma definitiva al ácido desoxirribonucléico (ADN) como el «principio transformante» responsable de la transmisión de la información genética.
En 1953, Francis Crick y James Watson publicaron su famoso trabajo sobre la estructura del ADN, basado en la investigación de Rosalind Franklin y Maurice Wilkins. Estos desarrollos iniciaron la era de la biología molecular y transformaron el entendimiento de la evolución en un proceso molecular.

A mediados de la década de 1970, Motoo Kimura formuló la teoría neutralista de la evolución molecular, estableciendo de manera firme la importancia de la deriva génica como el principal mecanismo de la evolución. Hasta la fecha continúan los debates en esta área de investigación. Uno de los más importantes es acerca de la teoría del equilibrio puntuado, una teoría propuesta por Niles Eldredge y Stephen Jay Gould para explicar la escasez de formas transicionales entre especies.

Históricamente, este estado del pensamiento evolutivo está representado por la publicación en agosto de 1858 de un trabajo conjunto de Darwin y Wallace, al que siguió en 1859 el libro de Darwin "El origen de las especies", el cual específicamente se refiere al principio de la selección natural como el motor más importante del proceso evolutivo. Debido a que Darwin aceptó el principio lamarckiano de la herencia de los caracteres adquiridos como una fuente de variabilidad biológica, es adecuado denominar a este período del pensamiento evolutivo como el de «Lamarck-Darwin-Wallace».

El trabajo de 1858 contenía «una muy ingeniosa teoría para explicar la aparición y perpetuación de las variedades y de las formas específicas en nuestro planeta» según palabras del prólogo escrito por Charles Lyell (1797-1895) y William Jackson Hooker (1785-1865). De hecho, este trabajo presentó por primera vez la hipótesis de la selección natural. Esta hipótesis contenía cinco afirmaciones fundamentales: (1) todos los organismos producen más descendencia de la que el ambiente puede sostener; (2) existe una abundante variabilidad intraespecífica para la mayoría de los caracteres; (3) la competencia por los recursos limitados lleva a la lucha «por la vida» (según Darwin) o «por la existencia» (según Wallace); (4) se produce descendencia con modificaciones heredables; y (5) como resultado, se originan nuevas especies.

A diferencia de Wallace, Darwin apoyó sus argumentos con una gran cantidad de hechos, elaborados en su mayoría a partir de experimentos de cruzamientos y del registro fósil. También aportó observaciones detalladas y directas de los organismos en su hábitat natural.
Treinta años más tarde, el codescubridor de la selección natural publicó una serie de conferencias bajo el título de «Darwinism» que tratan los mismos temas que ya había tratado Darwin, pero a la luz de los hechos y de los datos que eran desconocidos en tiempos de Darwin, quien falleció en 1882.
Un análisis comparativo detallado de las publicaciones de Darwin y Wallace revela que las contribuciones de este último fueron más importantes de lo que usualmente se suele reconocer, tanto es así que la frase el «mecanismo de selección natural de Darwin-Wallace» se ha propuesto para destacar su relevancia.

Sin embargo, Darwin fue el primero en resumir un conjunto coherente de observaciones que solidificó el concepto de la evolución de la vida en una verdadera teoría científica ―es decir, en un sistema de hipótesis―.
La lista de las propuestas de Darwin, extractada a partir de "El origen de las especies" se expone a continuación:
Neodarwinismo es un término acuñado en 1895 por el naturalista y psicólogo inglés George John Romanes (1848-1894) en su obra "Darwin and after Darwin".
El término describe un estado en el desarrollo de la teoría evolutiva que se remonta al citólogo y zoólogo germano August Weismann (1834-1914), quien en 1892 proveyó evidencia experimental en contra de la herencia lamarckiana y postuló que la reproducción sexual en cada generación crea una nueva y variable población de individuos. La selección natural, entonces, puede actuar sobre esa variabilidad y determina el curso del cambio evolutivo.
Por lo tanto, el neodarwinismo ―o sea, la ampliación de la teoría de Darwin― enriqueció el concepto original de Darwin, haciendo foco en el modo en que la variabilidad se genera y excluyendo la herencia lamarckiana como una explicación viable del mecanismo de herencia. Wallace, quien popularizó el término «darwinismo» para 1889, incorporó plenamente las nuevas conclusiones de Weismann y fue, por consiguiente, uno de los primeros proponentes del neodarwinismo.

Este sistema de hipótesis del proceso evolutivo se originó entre 1937 y 1950.
En contraste con el concepto neodarwiniano de Weismann y Wallace, la teoría sintética incorporó hechos de campos diversos de la biología, como la genética, la sistemática y la paleontología. Por esta razón, la frase «teoría neodarwiniana» no debe confundirse con la «teoría sintética».

De acuerdo a la gran mayoría de los historiadores de la Biología, los conceptos básicos de la teoría sintética están basados esencialmente en el contenido de seis libros, cuyos autores fueron: el naturalista y genetista ruso americano Theodosius Dobzhansky (1900-1975); el naturalista y taxónomo alemán americano Ernst Mayr (1904-2005); el zoólogo británico Julian Huxley (1887-1975); el paleontólogo americano George G. Simpson (1902-1984); el zoólogo germano Bernhard Rensch (1900-1990) y el botánico estadounidense George Ledyard Stebbins (1906-2000).

Los términos «síntesis evolutiva» y «teoría sintética» fueron acuñados por Julian Huxley en su libro "" (1942), en el que también introdujo el término Biología evolutiva en vez de la frase «estudio de la evolución».
De hecho Huxley fue el primero en señalar que la evolución «debía ser considerada el problema más central y el más importante de la biología y cuya explicación debía ser abordada mediante hechos y métodos de cada rama de la ciencia, desde la ecología, la genética, la paleontología, la embriología, la sistemática hasta la anatomía comparada y la distribución geográfica, sin olvidar los de otras disciplinas como la geología, la geografía y las matemáticas».

La llamada «síntesis evolutiva moderna» es una robusta teoría que actualmente proporciona explicaciones y modelos matemáticos sobre los mecanismos generales de la evolución o los fenómenos evolutivos, como la adaptación o la especiación. Como cualquier teoría científica, sus hipótesis están sujetas a constante crítica y comprobación experimental. Theodosius Dobzhansky, uno de los fundadores de la síntesis moderna, definió la evolución del siguiente modo: «La evolución es un cambio en la composición genética de las poblaciones. El estudio de los mecanismos evolutivos corresponde a la genética poblacional».







En la época de Darwin los científicos no conocían cómo se heredaban las características. Actualmente, el origen de la mayoría de las características hereditarias puede ser trazado hasta entidades persistentes llamadas genes, codificados en moléculas lineales de ácido desoxirribonucleico (ADN) del núcleo de las células. El ADN varía entre los miembros de una misma especie y también sufre cambios, mutaciones, o variaciones que se producen a través de procesos como la recombinación genética.

El fenotipo de un organismo individual es el resultado de su genotipo y la influencia del ambiente en el que vive y ha vivido. Una parte sustancial de la variación entre fenotipos dentro de una población está causada por las diferencias entre sus genotipos. La síntesis evolutiva moderna define a la evolución como el cambio de esa variación genética a través del tiempo. La frecuencia de un alelo en particular fluctuará, estando más o menos prevalente en relación con otras formas alternativas del mismo gen. Las fuerzas evolutivas actúan mediante el direccionamiento de esos cambios en las frecuencias alélicas en uno u otro sentido. La variación de una población para un dado gen desaparece cuando un alelo llega al punto de fijación, es decir, cuando ha desaparecido totalmente de la población o bien, cuando ha reemplazado enteramente a todas las otras formas alternativas de ese mismo gen.

La variabilidad surge en las poblaciones naturales por mutaciones en el material genético, migraciones entre poblaciones (flujo genético) y por la reorganización de los genes a través de la reproducción sexual. La variabilidad también puede provenir del intercambio de genes entre diferentes especies, por ejemplo a través de la transferencia horizontal de genes en las bacterias o la hibridación interespecífica en las plantas. A pesar de la constante introducción de variantes noveles a través de estos procesos, la mayor parte del genoma de una especie es idéntica en todos los individuos que pertenecen a la misma. Sin embargo, aún pequeños cambios en el genotipo pueden llevar a modificaciones sustanciales del fenotipo. Así, los chimpancés y los seres humanos, por ejemplo, solo difieren en aproximadamente el 5% de sus genomas.

Darwin no conocía la fuente de las variaciones en los organismos individuales, pero observó que las mismas parecían ocurrir aleatoriamente. En trabajos posteriores se atribuyó la mayor parte de estas variaciones a la mutación. La mutación es un cambio permanente y transmisible en el material genético ―usualmente el ADN o el ARN― de una célula, que puede ser producido por «errores de copia» en el material genético durante la división celular y por la exposición a radiación, químicos o la acción de virus. Las mutaciones aleatorias ocurren constantemente en el genoma de todos los organismos, creando nueva variabilidad genética. Las mutaciones pueden no tener efecto alguno sobre el fenotipo del organismo, pueden ser perjudiciales o beneficiosas. A modo de ejemplo, los estudios realizados sobre la mosca de la fruta ("Drosophila melanogaster"), sugieren que si una mutación determina un cambio en la proteína producida por un gen, ese cambio será perjudicial en el 70% de los casos y neutro o levemente beneficioso en los restantes.

La tasa de mutación de un gen o de una secuencia de ADN es la frecuencia en la que se producen nuevas mutaciones en ese gen o en esa secuencia en cada generación. Una alta tasa de mutación implica un mayor potencial de adaptación en el caso de un cambio ambiental, pues permite explorar más variantes genéticas, aumentando la probabilidad de obtener la variante adecuada necesaria para adaptarse al reto ambiental. A su vez, una alta tasa de mutación aumenta el número de mutaciones perjudiciales o deletéreas de los individuos, haciéndolos menos adaptados y aumentando por consiguiente la probabilidad de extinción de la población. Debido a los efectos deletéreos que las mutaciones pueden tener sobre los organismos, la tasa de mutación óptima para una población es una compensación entre costos y beneficios. Cada especie tiene una tasa de mutación propia que ha sido modulada por la selección natural para que la especie pueda enfrentarse de un modo más o menos óptimo a los compromisos contrapuestos de estabilidad-cambio que le impone su ambiente. Los virus, por ejemplo, presentan una alta tasa de mutación, lo que puede ser una ventaja adaptativa ya que deben evolucionar rápida y constantemente para sortear a los sistemas inmunes de los organismos que afectan.

La duplicación génica introduce en el genoma copias extras de un gen y, de ese modo, proporciona el material de base para que las nuevas copias inicien su propio camino evolutivo. Por ejemplo, en los seres humanos son necesarios cuatro genes para construir las estructuras necesarias para sensar la luz: tres para la visión de los colores y uno para la visión nocturna. Los cuatro genes han evolucionado a partir de un solo gen ancestral por duplicación y posterior divergencia. Asimismo, los genes duplicados pueden divergir lo suficiente como para adquirir nuevas funciones debido a que la copia original continua realizando la función inicial. Otros tipos de mutación pueden ocasionalmente crear nuevos genes a partir del denominado ADN no codificante. La creación de nuevos genes puede también involucrar pequeñas partes de varios genes que se han duplicado, las que recombinan para formar nuevas secuencias de ADN con nuevas funciones.

Las mutaciones cromosómicas ―también denominadas, aberraciones cromosómicas― son una fuente adicional de variabilidad hereditaria. Así, las translocaciones, inversiones, deleciones, translocaciones robertsonianas y duplicaciones, usualmente ocasionan variantes fenotípicas que se transmiten a la descendencia. Por ejemplo, dos cromosomas del género "Homo" se fusionaron para producir el cromosoma 2 de los seres humanos. Tal fusión cromosómica no ocurrió en los linajes de otros simios, los que han retenido ambos cromosomas separados. No obstante las consecuencias fenotípicas que pueden tener tales mutaciones cromosómicas, el papel evolutivo más importante de las mismas es el de acelerar la divergencia de las poblaciones que presentan diferencias en su constitución cromosómica. Debido a que los individuos heterocigóticos para las aberraciones cromosómicas son en general semiestériles, el flujo génico entre poblaciones que se diferencien para rearreglos cromosómicos estará severamente reducido. De este modo, las mutaciones cromosómicas actúan como mecanismos de aislamiento reproductivo que permiten que las diferentes poblaciones mantengan su identidad a través del tiempo.

Las secuencias de ADN que pueden moverse dentro del genoma, tales como los transposones, constituyen una importante fracción del material genético de plantas y animales y pueden haber desempeñado un papel destacado en su evolución. Su movilidad tiene una consecuencia importante desde el punto de vista evolutivo ya que al insertarse o escindirse del genoma pueden prender, apagar, mutar o eliminar otros genes y, por ende, crear nueva variabilidad genética. Asimismo, ciertas secuencias se hallan repetidas miles o millones de veces en el genoma y, muchas de ellas, han sido reclutadas para desempeñar funciones, como por ejemplo, la regulación de la expresión genética.

La recombinación genética es el proceso mediante el cual la información genética se redistribuye por transposición de fragmentos de ADN entre dos cromosomas durante la meiosis ―y más raramente en la mitosis―. Los efectos son similares a los de las mutaciones, es decir, si los cambios no son deletéreos se transmiten a la descendencia y contribuyen a incrementar la diversidad dentro de cada especie.

En los organismos asexuales, los genes se heredan en conjunto, o ligados, ya que no se mezclan con los de otros organismos durante los ciclos de recombinación que usualmente se producen durante la reproducción sexual. En contraste, los descendientes de los organismos que se reproducen sexualmente contienen una mezcla aleatoria de los cromosomas de sus progenitores, la cual se produce durante la recombinación meiótica y la posterior fecundación. La recombinación no altera las frecuencias alélicas sino que modifica la asociación existente entre alelos pertenecientes a genes diferentes, produciendo descendientes con combinaciones únicas de genes. La recombinación generalmente incrementa la variabilidad genética y puede incrementar también las tasas de evolución. No obstante, la existencia de organismos asexuales indica que este modo de reproducción puede también ser ventajoso en ciertos ambientes, tal como ocurre en las plantas apomícticas o en los animales partenogenéticos. Jens Christian Clausen fue uno de los primeros en reconocer formalmente que la apomixis, particularmente la apomixis facultativa, no necesariamente conduce a una pérdida de variabilidad genética y de potencial evolutivo. Utilizando una analogía entre el proceso adaptativo y la producción a gran escala de automóviles, Clausen arguyó que una combinación de sexualidad (que permite la producción de nuevos genotipos) y de apomixis (que permite la producción ilimitada de los genotipos más adaptados) podría incrementar, más que disminuir, la capacidad de una especie para el cambio adaptativo.

La recombinación permite que aún los genes que se hallan juntos en el mismo cromosoma puedan heredarse independientemente. No obstante, la tasa de recombinación es baja ―aproximadamente dos eventos por cromosoma y por generación―. Como resultado, los genes que se hallan físicamente cercanos entre sí tienden a heredarse en forma conjunta, un fenómeno que se denomina ligamiento. Un grupo de alelos que usualmente se heredan conjuntamente por hallarse ligados se denominan haplotipo. Cuando en un haplotipo uno de los alelos es altamente beneficioso la selección natural puede conducir a un barrido selectivo que causará que los otros alelos dentro del haplotipo se hagan más comunes dentro de la población; este efecto se denomina arrastre por ligamiento o «efecto autostop» (en inglés, "genetic hitchhiking").
Cuando los alelos no pueden ser separados por recombinación, tal como ocurre en el caso del cromosoma Y de los mamíferos o en las poblaciones de organismos asexuales, los genes con mutaciones deletéreas pueden acumularse, lo que se denomina trinquete de Muller ("Muller ratchet" en inglés). De este modo, al romper los conjuntos de genes ligados, la reproducción sexual permite la remoción de las mutaciones perjudiciales y la retención de las beneficiosas. Además, la recombinación y redistribución de los genes puede producir individuos con combinaciones genéticas nuevas y favorables. Estos efectos positivos se balancean con el hecho de que el sexo reduce la tasa reproductiva de las poblaciones de organismos sexuales y puede quebrar el ligamiento existente entre combinaciones favorables de genes. Este costo del sexo fue definido por primera vez en términos matemáticos por John Maynard Smith. En todas las especies sexuales, y con la excepción de los organismos hermafroditas, cada población está constituida por individuos de dos sexos, de los cuales solo uno es capaz de engendrar la prole. En una especie asexual, en cambio, todos los miembros de la población son capaces de engendrar descendencia. Esto implica que en cada generación una población asexual puede crecer más rápidamente. Un costo adicional del sexo es que los machos y las hembras deben buscarse entre ellos para aparearse y la selección sexual suele favorecer caracteres que reducen la aptitud de los individuos. Las razones de la evolución de la reproducción sexual son todavía poco claras y es un interrogante que constituye un área activa de investigación en Biología evolutiva, que ha inspirado ideas tales como la hipótesis de la Reina Roja. Esta hipótesis, cuyo nombre fue popularizado por el escritor científico Matt Ridley en su libro "", sostiene que los organismos se hallan involucrados en una carrera armamentista cíclica con sus parásitos lo que permite especular que el papel del sexo es el de preservar los genes que pueden ser circunstancialmente desfavorables pero potencialmente beneficiosos en el futuro ante futuros cambios en las poblaciones parásitas.

Como se ha descrito previamente, desde un punto de vista genético la evolución es un cambio intergeneracional en la frecuencia de los alelos dentro de una población que comparte un mismo patrimonio genético. Una población es un grupo de individuos de la misma especie que comparten un ámbito geográfico. Por ejemplo, todas las polillas de una misma especie que viven en un bosque aislado forman una población. Un gen determinado dentro de la población puede presentar diversas formas alternativas, que son las responsables de la variación entre los diferentes fenotipos de los organismos. Un ejemplo puede ser un gen de la coloración en las polillas que tenga dos alelos: uno para color blanco y otro para color negro. El patrimonio o acervo genético es el conjunto completo de los alelos de una población, de forma que cada alelo aparece un número determinado de veces en un acervo génico. La fracción de genes del patrimonio genético que están representadas por un alelo determinado recibe el nombre de frecuencia alélica, por ejemplo, la fracción de polillas en la población que presentan el alelo para color negro. La evolución tiene lugar cuando hay cambios en la frecuencia alélica en una población de organismos que se reproducen entre ellos, por ejemplo, si el alelo para color negro se hace más común en una población de polillas.

Para comprender los mecanismos que hacen que evolucione una población, es útil conocer las condiciones necesarias para que la población no evolucione. El principio de Hardy-Weinberg determina que la frecuencia de los alelos de una población suficientemente grande permanecerá constante solo si la única fuerza que actúa es la recombinación aleatoria de alelos durante la formación de los gametos y la posterior combinación de los mismos durante la fertilización. En ese caso, la población se encuentra en "equilibrio de Hardy-Weinberg" y, por lo tanto, no evoluciona.

El flujo genético es el intercambio de genes entre poblaciones, usualmente de la misma especie. El flujo génico dentro de una especie se puede producir por la inmigración y posterior cruzamiento de individuos de otras poblaciones o, simplemente, por el intercambio de polen entre poblaciones diferentes. La transferencia de genes entre especies involucra la formación de híbridos o la transferencia horizontal de genes.

La inmigración y la emigración de individuos en las poblaciones naturales pueden causar cambios en las frecuencias alélicas, como así también, la introducción ―o remoción― de variantes alélicas dentro de un acervo genético ya establecido. Las separaciones físicas en el tiempo, espacio o nichos ecológicos específicos que puede existir entre las poblaciones naturales restringen o imposibilitan el flujo génico. Además de estas restricciones al intercambio de genes entre poblaciones existen otras ―denominadas mecanismos de aislamiento reproductivo― las cuales son el conjunto de características, comportamientos y procesos fisiológicos que impiden que los miembros de dos especies diferentes puedan cruzarse o aparearse entre sí, producir descendencia o que la misma sea viable o fértil. Estas barreras constituyen una fase indispensable en la formación de nuevas especies ya que mantienen las características propias de las mismas a través del tiempo debido a que disminuyen, o directamente impiden, el flujo genético entre los individuos de diferentes especies.

Dependiendo de la distancia en la que dos especies han divergido desde su ancestro común más reciente, todavía puede ser posible que las mismas sean interfértiles, como es el caso del apareamiento entre la yegua y el asno para producir la mula. Tales híbridos son generalmente estériles, debido a las diferencias cromosómicas entre las especies parentales y a la incapacidad de los mismos de aparearse correctamente durante la meiosis. En este caso, las especies estrechamente relacionadas pueden cruzarse con regularidad, pero los híbridos serán seleccionados en contra. Sin embargo, de vez en cuando se forman híbridos viables y fértiles, los que pueden presentar propiedades intermedias entre sus especies paternales, o poseer un fenotipo totalmente nuevo. La importancia de la hibridación en la creación de nuevas especies de animales no es tan clara, aunque existen ejemplos bien documentados como el de la rana "Hyla versicolor".

La hibridación es, sin embargo, un mecanismo importante de formación de nuevas especies en las plantas, ya que la poliploidía ―la duplicación de todo el juego de cromosomas de un organismo― es tolerada más fácilmente en las plantas que en los animales, y restaura la fertilidad en los híbridos interespecíficos debido a que cada cromosoma es capaz de aparearse con un compañero idéntico durante la meiosis.

Hay dos mecanismos básicos de cambio evolutivo: la selección natural y la deriva genética. La selección natural favorece a los genes que mejoran la capacidad de supervivencia y reproducción del organismo. La deriva genética es el cambio aleatorio en la frecuencia de los alelos, provocado por muestreo aleatorio de los genes de una generación a la siguiente. El flujo genético es la transferencia de genes dentro de una población o entre poblaciones. La importancia relativa de la selección natural y de la deriva genética en una población varía dependiendo de la fuerza de la selección y del tamaño poblacional efectivo, que es el número de ejemplares de esa población capaces de reproducirse. La selección natural suele predominar en las poblaciones grandes, mientras que la deriva genética predomina en las pequeñas. El predominio de la deriva genética en poblaciones pequeñas puede llevar incluso a la fijación de mutaciones ligeramente deletéreas. Como resultado de ello, los cambios en el tamaño de una población pueden influir significativamente en el curso de la evolución. Los cuellos de botella en las poblaciones ―situaciones que llevan a un drástico descenso temporario del tamaño efectivo― determinan una pérdida o erosión de la variabilidad genética y conllevan, por lo tanto, a la formación de poblaciones genéticamente más uniformes. Los cuellos de botella también pueden ser el resultado de alteraciones en el flujo genético, como una migración reducida, la expansión a nuevos hábitats, o una subdivisión de la población.

La selección natural es el proceso por el cual las mutaciones genéticas que mejoran la capacidad reproductiva se vuelven, y permanecen, cada vez más frecuentes en las sucesivas generaciones de una población. Se la califica a menudo de «mecanismo autoevidente», pues es la consecuencia necesaria de tres hechos simples: (a) dentro de las poblaciones de organismos hay variación heredable (b) los organismos producen más descendientes de los que pueden sobrevivir, y (c) tales descendientes tienen diferentes capacidades para sobrevivir y reproducirse.

El concepto central de la selección natural es la aptitud biológica de un organismo. La aptitud, ajuste o adecuación se trata de la medida de la contribución genética de un organismo a la generación siguiente. Sin embargo, la aptitud no es simplemente igual al número total de descendientes de un determinado organismo, ya que también cuantifica la proporción de generaciones posteriores que llevan los genes de ese organismo. Por ejemplo, si un organismo puede sobrevivir y reproducirse pero sus descendientes son demasiado pequeños o enfermizos como para llegar a la edad reproductiva, la contribución genética de ese organismo a las futuras generaciones será muy baja y, por ende, su aptitud también lo es.

Por consiguiente, si un alelo aumenta la aptitud más que otros alelos del mismo gen, con cada generación el alelo será más común dentro de la población. Se dice que tales rasgos son «seleccionados a favor». Ejemplos de rasgos que pueden aumentar la aptitud son una mejora de la supervivencia o una mayor fecundidad. En cambio, la menor aptitud causada por un alelo menos beneficioso o deletéreo hace que el alelo sea cada vez más raro y se dice que es «seleccionado en contra». Hay que subrayar que la aptitud de un alelo no es una característica fija: si el ambiente cambia, los rasgos que antes eran neutros o nocivos pueden ser beneficiosos, y viceversa. Por ejemplo, la polilla "Biston betularia" presenta dos colores, uno claro denominado forma "typica" y otro oscuro llamado forma "carbonaria". La forma "typica", como su nombre indica, es la más frecuente en esta especie. No obstante, durante la revolución industrial en el Reino Unido los troncos de muchos de los árboles en los que las polillas se posaban resultaron ennegrecidos por el hollín, lo que les proporcionaba una ventaja a las polillas de color oscuro para pasar desapercibidas de los depredadores. Esto dio a las polillas de la forma melánica una mayor oportunidad de sobrevivir para producir más descendientes de color oscuro. Sólo cincuenta años después de que se descubriera la primera polilla melánica, casi la totalidad de las polillas del área industrial de Manchester eran oscuras. Este proceso se revirtió a causa de la «Ley del aire limpio» ("Clean Air Act") de 1956, por la cual se redujo la polución industrial y las polillas oscuras, más fácilmente visibles por los depredadores, volvieron a ser escasas nuevamente.

Sin embargo, aunque la dirección de la selección cambie, los rasgos que se hubiesen perdido en el pasado no pueden volver a obtenerse de forma idéntica ―situación que describe la Ley de Dollo o «Ley de la irreversibilidad evolutiva»―. De acuerdo con esta hipótesis, una estructura u órgano que se ha perdido o descartado durante el transcurso de la evolución no volverá a aparecer en ese mismo linaje de organismos.
Según Richard Dawkins, esta hipótesis es «una declaración sobre la improbabilidad estadística de seguir exactamente la misma trayectoria evolutiva dos veces o, de hecho, una misma trayectoria particular en ambas direcciones».

Dentro de una población, la selección natural para un determinado rasgo que varía en forma continua, como la altura, se puede categorizar en tres tipos diferentes. El primero es la «selección direccional», que es un cambio en el valor medio de un rasgo a lo largo del tiempo; por ejemplo, cuando los organismos cada vez son más altos. En segundo lugar se halla la «selección disruptiva» que es la selección de los valores extremos de un determinado rasgo, lo que a menudo determina que los valores extremos sean más comunes y que la selección actúe en contra del valor medio. Esto implicaría que los organismos bajos y altos tengan una ventaja, pero los de altura media no. Finalmente, en la «selección estabilizadora», la selección actúa en contra de los valores extremos, lo que determina una disminución de la varianza alrededor del promedio y una menor variabilidad de la población para ese carácter en particular. Esto haría, por ejemplo, que todos los organismos de una población, paulatinamente, adquirieran una altura similar.

Un tipo especial de selección natural es la selección sexual, que es la selección a favor de cualquier rasgo que aumente el éxito reproductivo haciendo aumentar el atractivo de un organismo ante parejas potenciales. Los rasgos que evolucionaron mediante la selección sexual son especialmente prominentes en los machos de algunas especies, aunque ciertos rasgos ―tales como cuernos voluminosos, cantos de apareamiento o colores brillantes― puedan atraer a los predadores, reduciendo las posibilidades de supervivencia de los machos. No obstante, esta desventaja reproductiva se compensa por un mayor éxito reproductivo de los machos que presentan estos rasgos sexualmente seleccionados.

Un área de estudio activo es la denominada «unidad de selección»; se ha dicho que la selección natural actúa a nivel de genes, células, organismos individuales, grupos de organismos e incluso especies. Ninguno de estos modelos es mutuamente exclusivo, y la selección puede actuar en múltiples niveles a la vez. Por ejemplo, debajo del nivel del individuo, hay genes denominados transposones que intentan replicarse en todo el genoma. La selección por sobre el nivel del individuo, como la selección de grupo, puede permitir la evolución de la cooperación.

La deriva genética es el cambio en la frecuencia de los alelos entre una generación y la siguiente, y tiene lugar porque los alelos de la descendencia son una muestra aleatoria de los padres, y por el papel que juega el azar en la hora de determinar si un ejemplar determinado sobrevivirá y se reproducirá. En términos matemáticos, los alelos están sujetos a errores de muestreo. Como resultado de ello, cuando las fuerzas selectivas están ausentes o son relativamente débiles, la frecuencia de los alelos tiende a «derivar» hacia arriba o hacia abajo aleatoriamente (en un paseo aleatorio). Esta deriva se detiene cuando un alelo se convierte finalmente fijado, es decir, o bien desaparece de la población, o bien sustituye totalmente el resto de genes. Así pues, la deriva genética puede eliminar algunos alelos de una población simplemente debido al azar. Incluso en la ausencia de fuerzas selectivas, la deriva genética puede hacer que dos poblaciones separadas que empiezan con la misma estructura genética se separen en dos poblaciones divergentes con un conjunto de alelos diferentes.

El tiempo necesario para que un alelo quede fijado por la deriva genética depende del tamaño de la población; la fijación tiene lugar más rápido en poblaciones más pequeñas. La medida precisa de las poblaciones que es importante en este caso recibe el nombre de tamaño poblacional efectivo, que fue definida por Sewall Wright como el número teórico de ejemplares reproductivos que presenten el mismo grado observado de consanguinidad.

Aunque la selección natural es responsable de la adaptación, la importancia relativa de las dos fuerzas, selección natural y deriva genética, como impulsoras del cambio evolutivo en general es actualmente un campo de investigación en la biología evolutiva. Estas investigaciones fueron inspiradas por la teoría neutralista de la evolución molecular, que postula que la mayoría de cambios evolutivos son el resultado de la fijación de mutaciones neutras, que no tienen ningún efecto inmediato sobre la aptitud de un organismo. Por tanto, en este modelo, la mayoría de los cambios genéticos en una población son el resultado de una presión de mutación constante y de deriva genética.

La adaptación es el proceso mediante el cual una población se adecua mejor a su hábitat y también el cambio en la estructura o en el funcionamiento de un organismo que lo hace más adecuado a su entorno. Este proceso tiene lugar durante muchas generaciones, se produce por selección natural, y es uno de los fenómenos básicos de la biología. 

De hecho, un principio fundamental de la ecología es el denominado principio de exclusión competitiva: dos especies no pueden ocupar el mismo nicho en el mismo ambiente por un largo tiempo. En consecuencia, la selección natural tenderá a forzar a las especies a adaptarse a diferentes nichos ecológicos para reducir al mínimo la competencia entre ellas.

La adaptación es, en primer lugar, un "proceso" en lugar de una parte física de un cuerpo. La distinción puede apreciarse, por ejemplo, en los trematodos ―parásitos internos con estructuras corporales muy simples pero con un ciclo de vida muy complejo― en los que sus adaptaciones a un medio ambiente tan inusual no son el producto de caracteres observables a simple vista sino en aspectos críticos de su ciclo vital. Sin embargo, el concepto de adaptación también incluye aquellos aspectos de los organismos, de las poblaciones o de las especies que son el resultado del proceso adaptativo. Mediante la utilización del término «adaptación» para el "proceso" evolutivo y «rasgo o carácter adaptativo» para el "producto" del mismo, los dos sentidos del concepto se distinguen perfectamente. Las definiciones de estos conceptos, debidas a Theodosius Dobzhansky, son básicas. Así, la «adaptación» es el proceso evolutivo por el cual un organismo se vuelve más capaz de vivir en su hábitat o hábitats, mientras que la «adaptabilidad» es el estado de estar adaptado, o sea, el grado en que un organismo es capaz de vivir y reproducirse en un determinado conjunto de hábitats. Finalmente, un «carácter adaptativo» es uno de los aspectos del patrón de desarrollo de un organismo, el cual le permite o le incrementa la probabilidad de sobrevivir y reproducirse.

La adaptación puede causar ya sea la ganancia de una nueva característica o la pérdida de una función ancestral. Un ejemplo que muestra los dos tipos de cambio es la adaptación de las bacterias a la selección por antibióticos, con cambios genéticos que causan resistencia a los antibióticos debido a que se modifica la diana de la droga o por el aumento de la actividad de los transportadores que extraen la droga fuera de la célula. Otros ejemplos notables son la evolución en laboratorio de las bacterias "Escherichia coli" para que puedan ser capaces de utilizar el ácido cítrico como un nutriente, cuando las bacterias de tipo silvestre no lo pueden hacer, la evolución de una nueva enzima en "Flavobacterium" que permite que estas bacterias puedan crecer en los subproductos de la fabricación del nylon, y la evolución de una vía metabólica completamente nueva en la bacteria del suelo "Sphingobium" que le permite degradar el pesticida sintético pentaclorofenol. Una idea interesante, aunque todavía controvertida, es que algunas adaptaciones pueden aumentar la capacidad de los organismos para generar diversidad genética y para adaptarse por selección natural ―o sea, aumentarían la capacidad de evolución―
La adaptación se produce a través de la modificación gradual de las estructuras existentes. En consecuencia, las estructuras con organización interna similar pueden tener diferentes funciones en organismos relacionados. Este es el resultado de una sola estructura ancestral que ha sido adaptada para funcionar de diferentes formas. Los huesos de las alas de los murciélagos, por ejemplo, son muy similares a los de los pies del ratón y los de las manos de los primates, debido a que todas estas estructuras descienden a partir de un ancestro común de los mamíferos. Dado que todos los organismos vivos están relacionados en cierta medida, incluso los órganos que parecen tener una estructura poco o nada similar, como los ojos de los artrópodos, del calamar y de los vertebrados, o las extremidades y las alas de artrópodos y vertebrados, pueden depender de un conjunto común de genes homólogos que controlan su montaje y funcionamiento, lo que se denomina homología profunda.

Durante la adaptación, algunas estructuras pueden perder su función original y convertirse en estructuras vestigiales. Estas estructuras pueden carecer de funcionalidad en una especie actual, sin embargo, pueden haber tenido una clara función en la especie ancestral o en otras especies estrechamente relacionadas. Los ejemplos incluyen pseudogenes, los restos no funcionales de los ojos de los peces cavernícolas ciegos, las alas en las especies de aves que no vuelan y la presencia de huesos de la cadera en las ballenas y en las serpientes. En los seres humanos también existen ejemplos de estructuras vestigiales, las que incluyen las muelas de juicio, el coxis, el apéndice vermiforme, e incluso, vestigios de comportamiento tales como la piel de gallina y otros reflejos primitivos.

Sin embargo, muchos rasgos que parecen ser simples adaptaciones son, de hecho, exaptaciones: estructuras originalmente adaptadas para una función, pero que coincidentemente se hicieron útiles para alguna otra función durante el proceso. Un ejemplo es el lagarto africano "Holaspis guentheri" que desarrolló una cabeza muy plana para esconderse en las grietas, hecho que puede observarse en sus parientes cercanos. Sin embargo, en esta especie, la cabeza se ha convertido en tan aplastada que le permite deslizarse de árbol en árbol. Las vejigas natatorias de los peces teleósteos son otro ejemplo de exaptación ya que, si bien derivan directamente de los pulmones de los peces pulmonados ancestrales, son empleadas como regulador de la flotación.

Un área de investigación actual en biología evolutiva del desarrollo es la base del desarrollo de las adaptaciones y de las exaptaciones. Esta área de investigación aborda el origen y la evolución de desarrollo embrionario y de qué modo las modificaciones de los procesos de desarrollo generan nuevas características. Estos estudios han demostrado que la evolución puede alterar el desarrollo para crear nuevas estructuras, tales como las estructuras óseas de los embriones que se desarrollan en la mandíbula en algunos animales, en cambio forman parte del oído medio en los mamíferos. También es posible que las estructuras que se han perdido en la evolución vuelvan a aparecer debido a los cambios que se producen en los genes del desarrollo, como por ejemplo una mutación en los pollos que determina que los embriones desarrollen dientes similares a los de cocodrilos. De hecho, es cada vez es más claro que la mayoría de las alteraciones en la forma de los organismos se deben a cambios en un pequeño conjunto de genes conservados.

La interacción entre organismos puede producir conflicto o cooperación. Cuando interactúan dos especies diferentes, como un patógeno y un hospedador, o un depredador y su presa, las especies pueden desarrollar conjuntos de adaptaciones complementarias. En este caso, la evolución de una especie provoca adaptaciones en la otra. A su vez, estos cambios en la segunda especie provocan adaptaciones en la primera. Este ciclo de selección y respuesta recibe el nombre de coevolución. Un ejemplo es la producción de tetradotoxina por parte del tritón de Oregón y la evolución de una resistencia a esta toxina en su predador, la serpiente de jarretera. En esta pareja predador-presa, la carrera armamentista evolutiva ha producido niveles altos de toxina en el tritón, y los correspondientes niveles altos de resistencia en la serpiente.

La especiación (o cladogénesis) es el proceso por el cual una especie diverge en dos o más especies descendientes. Los biólogos evolutivos ven las especies como fenómenos estadísticos y no como categorías o tipos. Este planteamiento es contrario a la intuición, ya que el concepto clásico de especie sigue estando muy arraigado, con la especie vista como una clase de organismos que se ejemplifica en un «espécimen tipo», el cual posee todas las características comunes a dicha especie. En su lugar, una especie se define ahora como un linaje que comparte un único acervo genético y evoluciona independiente. Esta definición tiene límites difusos, a pesar de que se utilizan propiedades tanto genéticas como morfológicas para ayudar a diferenciar los linajes estrechamente relacionados. De hecho, la definición exacta del término «especie» está todavía en discusión, particularmente para organismos basados en células procariotas; es lo que se denomina «problema de las especies». Diversos autores han propuesto una serie de definiciones basadas en criterios diferentes, pero la aplicación de una u otra es finalmente una cuestión práctica, dependiendo en cada caso concreto de las particularidades del grupo de organismos en estudio. Actualmente, la unidad de análisis principal en biología es la población, un conjunto observable de individuos "que interactúan", en lugar de la especie, un conjunto observable de individuos "que se parecen entre sí".
La especiación ha sido observada en múltiples ocasiones tanto en condiciones de laboratorio controladas como en la naturaleza. En los organismos que se reproducen sexualmente, la especiación es el resultado de un aislamiento reproductivo seguido de una divergencia genealógica. Hay cuatro modalidades de especiación. La más habitual en los animales es la especiación alopátrica, que tiene lugar en poblaciones que inicialmente están geográficamente aisladas, como en el caso de la fragmentación de hábitat o las migraciones. En estas condiciones, la selección puede causar cambios muy rápidos en la apariencia y el comportamiento de los organismos. Como la selección y la deriva actúan de manera independiente en poblaciones aisladas del resto de su especie, la separación puede crear finalmente organismos que no se pueden reproducir entre ellos.

La segunda modalidad de especiación es la especiación peripátrica, que tiene lugar cuando poblaciones pequeñas de organismos quedan aisladas en un nuevo medio. Se diferencia de la especiación alopátrica en que las poblaciones aisladas son numéricamente mucho más pequeñas que la población madre. Aquí, el efecto fundador causa una especiación rápida por medio de una rápida deriva genética y selección en un acervo génico pequeño.

La tercera modalidad de especiación es la especiación parapátrica. Se parece a la especiación peripátrica en que una pequeña población coloniza un nuevo hábitat, pero se diferencia en que no hay ninguna separación física entre las dos poblaciones. En cambio, la especiación es el resultado de la evolución de mecanismos que reducen el flujo génico entre ambas poblaciones. Generalmente, esto ocurre cuando ha habido un cambio drástico en el medio dentro del hábitat de la especie madre. Un ejemplo es la hierba "Anthoxanthum odoratum", que puede sufrir una especiación parapátrica en respuesta a contaminación metálica localizada proveniente de minas. En este caso, evolucionan plantas con una resistencia a niveles altos de metales en el suelo. La selección que desfavorece los cruces con la especie madre, sensible a los metales, produce un cambio en la época de floración de las plantas resistentes a los metales, causando el aislamiento reproductivo. La selección en contra de híbridos entre dos poblaciones puede causar refuerzo, como es la diferenciación de aquellos rasgos que promueven la reproducción dentro de la especie, así como el desplazamiento de caracteres, que es cuando dos especies se vuelven más diferentes en apariencia en el área geográfica en que se solapan.

Finalmente, en la especiación simpátrica, las especies divergen sin que haya aislamiento geográfico o cambios en el hábitat. Esta modalidad es rara, pues incluso una pequeña cantidad de flujo génico puede eliminar las diferencias genéticas entre partes de una población. En general, en los animales, la especiación simpátrica requiere la evolución de diferencias genéticas y un apareamiento no aleatorio, para que se pueda desarrollar un aislamiento reproductivo.

Un tipo de especiación simpátrica es el cruce de dos especies relacionadas para producir una nueva especie híbrida. Esto no es habitual en los animales, pues los híbridos animales suelen ser estériles, ya que durante la meiosis los cromosomas homólogos de cada padre, siendo de especies diferentes, no pueden aparearse con éxito. Es más habitual en las plantas, pues las plantas duplican a menudo su número de cromosomas, para formar poliploides. Esto permite a los cromosomas de cada especie parental formar una pareja complementaria durante la meiosis, ya que los cromosomas de cada padre ya son representados por una pareja. Un ejemplo de este tipo de especiación es cuando las especies vegetales "Arabidopsis thaliana" y "Arabidopsis arenosa" se cruzaron para producir la nueva especie "Arabidopsis suecica". Esto tuvo lugar hace aproximadamente 20 000 años, y el proceso de especiación ha sido repetido en el laboratorio, lo que permite estudiar los mecanismos genéticos implicados en este proceso. De hecho, la duplicación de cromosomas dentro de una especie puede ser una causa habitual de aislamiento reproductivo, pues la mitad de los cromosomas duplicados quedarán sin pareja cuando se aparean con los de organismos no duplicados.

Los episodios de especiación son importantes en la teoría del equilibrio puntuado, que contempla patrones en el registro fósil de rápidos momentos de especiación intercalados con periodos relativamente largos de estasis, durante los que las especies permanecen prácticamente sin modificar. En esta la teoría, la especiación y la evolución rápida están relacionadas, y la selección natural y la deriva genética actúan de forma particularmente intensa sobre los organismos que sufren una especiación en hábitats nuevos o pequeñas poblaciones. Como resultado de ello, los períodos de estasis del registro fósil corresponden a la población madre, y los organismos que sufren especiación y evolución rápida se encuentran en poblaciones pequeñas o hábitats geográficamente restringidos, por lo que raramente quedan preservados en forma de fósiles.

La extinción es la desaparición de una especie entera. La extinción no es un acontecimiento inusual, pues aparecen a menudo especies por especiación, y desaparecen por extinción. De hecho, la práctica totalidad de especies animales y vegetales que han vivido en la Tierra están actualmente extinguidas, y parece que la extinción es el destino final de todas las especies. Estas extinciones han tenido lugar continuamente durante la historia de la vida, aunque el ritmo de extinción aumenta drásticamente en los ocasionales eventos de extinción. La extinción del Cretácico-Terciario, durante la cual se extinguieron los dinosaurios, es la más conocida, pero la anterior extinción Permo-Triásica fue aún más severa, causando la extinción de casi el 96% de las especies. La extinción del Holoceno es una extinción en masa que todavía dura y que está asociada con la expansión de la humanidad por el globo terrestre en los últimos milenios. El ritmo de extinción actual es de 100 a 1000 veces mayor que el ritmo medio, y hasta un 30% de las especies pueden estar extintas a mediados del siglo XXI. Las actividades humanas son actualmente la causa principal de esta extinción que aún continúa; es posible que el calentamiento global acelere aún más en el futuro.

El papel que juega la extinción en la evolución depende de qué tipo de extinción se trate. Las causas de las continuas extinciones de «bajo nivel», que forman la mayoría de extinciones, no están bien comprendidas y podrían ser el resultado de la competencia entre especies por recursos limitados (exclusión competitiva). Si la competencia de otras especies altera la probabilidad de que se extinga una especie, esto podría situar la selección de especies como un nivel de la selección natural. Las extinciones masivas intermitentes también son importantes, pero en lugar de actuar como fuerza selectiva, reducen drásticamente la diversidad de manera indiscriminada y promueven explosiones de rápida evolución y especiación en los supervivientes.

Microevolución es un término usado para referirse a cambios de las frecuencias génicas en pequeña escala, en una población durante el transcurso de varias generaciones. Estos cambios pueden deberse a un cierto número de procesos: mutación, flujo génico, deriva génica, así como también por selección natural. La genética de poblaciones es la rama de la biología que provee la estructura matemática para el estudio de los procesos de la microevolución, como el color de la piel en la población mundial.

Los cambios a mayor escala, desde la especiación (aparición de una nueva especie) hasta las grandes transformaciones evolutivas ocurridas en largos períodos, son comúnmente denominados macroevolución (por ejemplo, los anfibios que evolucionaron a partir de un grupo de peces óseos). Los biólogos no acostumbran hacer una separación absoluta entre macroevolución y microevolución, pues consideran que macroevolución es simplemente microevolución acumulada y sometida a un rango mayor de circunstancias ambientales. Una minoría de teóricos, sin embargo, considera que los mecanismos de la teoría sintética para la microevolución no bastan para hacer esa extrapolación y que se necesitan otros mecanismos. La teoría de los equilibrios puntuados, propuesta por Gould y Eldredge, intenta explicar ciertas tendencias macroevolutivas que se observan en el registro fósil.

En las últimas décadas se ha hecho evidente que los patrones y los mecanismos evolutivos son mucho más variados que los que fueran postulados por los pioneros de la Biología evolutiva (Darwin, Wallace o Weismann) y los arquitectos de la teoría sintética (Dobzhansky, Mayr y Huxley, entre otros). Los nuevos conceptos e información en la biología molecular del desarrollo, la sistemática, la geología y el registro fósil de todos los grupos de organismos necesitan ser integrados en lo que se ha denominado «síntesis evolutiva ampliada». Los campos de estudio mencionados muestran que los fenómenos evolutivos no pueden ser comprendidos solamente a través de la extrapolación de los procesos observados a nivel de las poblaciones y especies modernas. En las próximas secciones se presentan los aspectos considerados como la ampliación de la síntesis moderna.

En el momento en que Darwin propuso su teoría de evolución, caracterizada por modificaciones pequeñas y sucesivas, el registro fósil disponible era todavía muy fragmentario. Más aún, se desconocían totalmente fósiles previos al período Cámbrico. El dilema de Darwin, o sea, la inexistencia aparente de registros fósiles del Precámbrico, fue utilizado como el principal argumento en contra de su propuesta de que todos los organismos de la Tierra provienen de un antepasado común.

Además de la inexistencia de un registro fósil completo, Darwin también estaba preocupado por la ausencia aparente de formas intermedias o enlaces conectores en el registro fósil, lo cual desafiaba su visión gradualística de la especiación y de la evolución. De hecho en tiempos de Darwin, con la excepción de "Archaeopteryx", que muestra una mezcla de características de ave y de reptil, virtualmente no se conocían otros ejemplos de formas intermedias o eslabones perdidos, como se los denominó coloquialmente.

Para 1944, cuando se publicó el libro de Simpson "Tempo and mode in evolution", ambos inconvenientes no pudieron subsanarse debido a que para esa época tampoco se conocían fósiles del Precámbrico y solo se disponía de unos pocos ejemplos de formas intermedias en el registro fósil que enlazaran las formas antiguas con las derivadas. Ninguno de los dos dilemas o preocupaciones de Darwin existen actualmente. Los científicos han explorado el período Precámbrico con detalle y se sabe que la vida es mucho más antigua de lo que se creía en los tiempos de Darwin. También se sabe que esas antiguas formas de vida fueron los ancestros de todos los organismos subsecuentes en el planeta. Asimismo, en los últimos 20 años se han descubierto, descrito y analizado una gran cantidad de ejemplos representativos de formas fósiles intermedias que enlazan a los principales grupos de vertebrados e, incluso, fósiles de las primeras plantas con flor. Como resultado de estos y otros avances científicos la Paleontología (originalmente una rama de la Geología) ha desarrollado en una nueva disciplina denominada Paleobiología.

Un ejemplo de forma transicional entre los peces y los anfibios es el género extinto "Panderichthys", que habitó la tierra hace unos 370 millones de años y es el enlace intermedio en la serie "Eustenopteron" (peces, 380 millones de años)-"Panderichthys"-"Acanthostega" (anfibios, 363 millones de años). Los anfibios y los vertebrados terrestres presentaron una forma intermedia, "Pederpes", de 350 millones de años que enlaza a los principales anfibios acuáticos del Devónico superior con los tetrápodos tempranos. Asimismo, la historia evolutiva de varios grupos de organismos extintos, tales como los dinosaurios, ha sido reconstruida con notable detalle. El enlace entre los reptiles y los mamíferos es el "Thrinaxodon", un reptil con características de mamífero que habitó el planeta hace 230 millones de años. El enlace entre los dinosaurios y las aves es el "Microraptor", un dromeosáurido con cuatro alas que podía planear y que vivió hace 126 millones de años, el cual representa el estado intermedio entre los terópodos y las primitivas aves voladoras como "Archaeopteryx". La forma transicional entre los mamíferos terrestres y la vaca marina es "Pezosiren", un sirénido cuadrúpedo primitivo con adaptaciones terrestres y acuáticas que vivió hace 50 millones de años. Los mamíferos terrestres con pezuñas y las ballenas se hallan conectados a través de los géneros extintos "Ambulocetus" y "Rodhocetus" que habitaron el planeta hace 48 a 47 millones de años. Para finalizar esta enumeración de ejemplos de formas transicionales, el ancestro de los chimpancés y de los seres humanos es el género "Sahelanthropus", un homínido con aspecto de mono que exhibía un mosaico de caracteres de chimpancé y de hominino y que habitó África hace 7 a 5 millones de años.

En su libro "Variation and evolution in plants" (1950), Stebbins también se lamentaba por la ausencia de un registro fósil que permitiera comprender el origen de las primeras plantas con flores, las angiospermas. De hecho, el propio Darwin caracterizó al origen de las angiospermas como un «abominable misterio». No obstante, este vacío de conocimiento está siendo rápidamente completado con los descubrimientos realizados desde fines del siglo XX y hasta la actualidad. En 1998 se descubrió en China, en los estratos provenientes del Jurásico Superior (de más de 125 millones de años de antigüedad), un fósil de un eje con frutos, que se ha denominado "Archaefructus" Semejante descubrimiento, que parecía datar la edad de las angiospermas más antiguas, hizo mundialmente famosa a la Formación Yixian, donde fue descubierto este fósil. Algunos años más tarde, el fósil de otra angiosperma, "Sinocarpus", fue también descubierto en esta misma formación. En 2007 se informó del hallazgo de una flor perfecta en la formación Yixian. Esta flor tiene la organización típica de las angiospermas, incluyendo la presencia de tépalos, estambres y gineceo. Esta especie ha sido bautizada como "Euanthus" (del griego, «flor verdadera») por sus descubridores, e indica que en el Cretácico inferior ya existían flores como las de las angiospermas actuales.

Darwin no solo discutió el origen sino también la disminución y la desaparición de las especies. Como una causa importante de la extinción de poblaciones y especies propuso a la competencia interespecífica debida a recursos limitados: durante el tiempo evolutivo, las especies superiores surgirían para reemplazar a especies menos adaptadas. Esta perspectiva ha cambiado en los últimos años con una mayor comprensión de las causas de las extinciones masivas, episodios de la historia de la Tierra, donde las «reglas» de la selección natural y de la adaptación parecen haber sido abandonadas.

Esta nueva perspectiva fue presagiada por Mayr en su libro "Animal species and evolution" en el que señaló que la extinción debe ser considerada como uno de los fenómenos evolutivos más conspicuos. Mayr discutió las causas de los eventos de extinción y propuso que nuevas enfermedades (o nuevos invasores de un ecosistema) o los cambios en el ambiente biótico pueden ser los responsables. Además, escribió: 

Esta hipótesis, no sustentada por hechos cuando fue propuesta, ha adquirido desde entonces un considerable apoyo. El término «extinción masiva», mencionado por Mayr pero sin una definición asociada, se utiliza cuando una gran cantidad de especies se extinguen en un plazo geológicamente breve; los eventos pueden estar relacionados con una causa única o con una combinación de causas, y las especies extintas son plantas y animales de todo tamaño, tanto marinos como terrestres. Al menos han ocurrido cinco extinciones masivas, y han dejado muchos huecos ecológicos que han permitido que fueran ocupados por los descendientes de las especies supervivientes: la extinción masiva del Cámbrico-Ordovícico, las extinciones masivas del Ordovícico-Silúrico, la extinción masiva del Devónico, la extinción masiva del Pérmico-Triásico y la extinción masiva del Cretácico-Terciario.

La extinción biológica que se produjo en el Pérmico-Triásico hace unos 250 millones de años representa el más grave evento de extinción en los últimos 550 millones de años. Se estima que en este evento se extinguieron alrededor del 70% de las familias de vertebrados terrestres, muchas gimnospermas leñosas y más del 90% de las especies oceánicas. Se han propuesto varias causas para explicar este evento, las que incluyen el vulcanismo, el impacto de un asteroide o un cometa, la anoxia oceánica y el cambio ambiental. No obstante, es aparente en la actualidad que las gigantescas erupciones volcánicas, que tuvieron lugar durante un intervalo de tiempo de sólo unos pocos cientos de miles de años, fueron la causa principal de la catástrofe de la biosfera durante el Pérmico tardío. El límite Cretácico-Terciario registra el segundo mayor evento de extinción masivo. Esta catástrofe mundial acabó con el 70% de todas las especies, entre las cuales los dinosaurios son el ejemplo más popularmente conocido. Los pequeños mamíferos sobrevivieron para heredar los nichos ecológicos vacantes, lo que permitió el ascenso y la radiación adaptativa de los linajes que en última instancia se convertirían en "Homo sapiens". Los paleontólogos han propuesto numerosas hipótesis para explicar este evento, las más aceptadas en la actualidad son las del impacto de un asteroide y la de fenómenos de vulcanismo.

En resumen, la hipótesis de los trastornos ambientales como causas de las extinciones masivas ha sido confirmada, lo cual indica que si bien gran parte de historia de la evolución puede ser gradual, de vez en cuando ciertos acontecimientos catastróficos han marcado su ritmo de fondo. Es evidente que los pocos «afortunados sobrevivientes» determinan los subsecuentes patrones de evolución en la historia de la vida.

Determinadas características en una especie son sexualmente atractivas aunque carezcan de otro significado adaptativo. Por ejemplo, en algunas especies de aves los machos pueden hinchar sus cuellos en una medida extraordinaria lo cual resulta atractivo para las hembras, por lo que ―en el transcurso de las generaciones― se seleccionan machos que pueden hinchar exageradamente sus cuellos. Darwin concluyó que si bien la selección natural guía el curso de la evolución, la selección sexual influye su curso aunque no parezca existir ninguna razón evidente para ello. Los argumentos de Darwin a favor de la selección sexual aparecen en el capítulo cuarto de "El origen de las especies" y, muy especialmente, en "The Descent of Man, and Selection in Relation to Sex" de 1871. En ambos casos, se esgrime la analogía con el mundo artificial:

En su libro "The Descent of Man" describió numerosos ejemplos, tales como la cola del pavo real y de la melena del león. Darwin argumentó que la competencia entre los machos es el resultado de la selección de los rasgos que incrementan el éxito del apareamiento de los machos competidores, rasgos que podrían, sin embargo, disminuir las posibilidades de supervivencia del individuo. De hecho, los colores brillantes hacen a los animales más visibles a los depredadores, el plumaje largo de los machos de pavo real y de las aves del paraíso, o la enorme cornamenta de los ciervos son cargas incómodas en el mejor de casos. Darwin sabía que no era esperable que la selección natural favoreciera la evolución de tales rasgos claramente desventajosos, y propuso que los mismos surgieron por selección sexual, 

Para Darwin, la selección sexual incluía fundamentalmente dos fenómenos: la preferencia de las hembras por ciertos machos ―"selección intersexual", "femenina", o "epigámica"― y, en las especies polígamas, las batallas de los machos por el harén más grande ―"selección intrasexual"―. En este último caso, el tamaño corporal grande y la musculatura proporcionan ventajas en el combate, mientras que en el primero, son otros rasgos masculinos, como el plumaje colorido y el complejo comportamiento de cortejo los que se seleccionan a favor para aumentar la atención de las hembras. Las ideas de Darwin en este sentido no fueron ampliamente aceptadas y los defensores de la teoría sintética (Dobzhansky, Mayr y Huxley) en gran medida ignoraron el concepto de selección sexual.

El estudio de la selección sexual sólo cobró impulso en la era postsíntesis. Se ha argumentado que Wallace (y no Darwin) propuso por primera vez que los machos con plumaje brillante demostraban de ese modo su buena salud y su alta calidad como parejas sexuales. De acuerdo con esta hipótesis de la «selección sexual de los buenos genes» la elección de pareja masculina por parte de las hembras ofrece una ventaja evolutiva. Esta perspectiva ha recibido apoyo empírico en las últimas décadas. Por ejemplo, se ha hallado una asociación, aunque pequeña, entre la supervivencia de la descendencia y los caracteres sexuales secundarios masculinos en un gran número de taxones, tales como aves, anfibios, peces e insectos). Además, las investigaciones con mirlos han proporcionado la primera evidencia empírica de que existe una correlación entre un carácter sexual secundario y un rasgo que incrementa la supervivencia ya que los machos con los más brillantes colores presentan un sistema inmune más fuerte. Así, la selección femenina podría promover la salud general de las poblaciones en esta especie. Estos y otros datos son coherentes con el concepto de que la elección de la hembra influye en los rasgos de los machos e, incluso, que puede ser beneficiosa para la especie en formas que no tienen ninguna relación directa con el éxito del apareamiento.

En el mismo contexto y desde la publicación del "Origen de las especies", se ha argumentado que el comportamiento altruista, los actos abnegados realizados en beneficio de los demás, es incompatible con el principio de la selección natural. Sin embargo, el comportamiento altruista, como el cuidado de las crías por los padres y el mutualismo, se ha observado y documentado en todo el reino animal, desde los invertebrados hasta en los mamíferos. Una de las formas más notorias de altruismo se produce en ciertos insectos eusociales, como las hormigas, abejas y avispas, que tienen una clase de trabajadoras estériles. La cuestión general de la evolución del altruismo, de la sociabilidad de ciertos insectos o de la existencia de abejas u hormigas obreras que no dejan descendientes ha sido contestada por la teoría de la aptitud inclusiva, también llamada "teoría de selección familiar". De acuerdo con el principio de Darwin/Wallace la selección natural actúa sobre las diferencias en el éxito reproductivo (ER) de cada individuo, donde ER es el número de descendientes vivos producidos por ese individuo durante toda la vida. Hamilton (1972) amplió esta idea e incluyó los efectos de ER de los familiares del individuo: la aptitud inclusiva es el ER de cada individuo, más el ER de sus familiares, cada uno devaluado por el correspondiente grado de parentesco. Numerosos estudios en una gran variedad de especies animales han demostrado que el altruismo no está en conflicto con la teoría evolutiva. Por esta razón, es necesario realizar una modificación y ampliación de la visión tradicional de que la selección opera sobre un solo organismo aislado en una población: el individuo aislado ya no parece tener una importancia central desde el punto de vista evolutivo, sino como parte de una compleja red familiar.

Cuando se define macroevolución como el proceso responsable del surgimiento de los taxones de rango superior, se está utilizando un lenguaje metafórico. De hecho, sólo nuevas especies «surgen», ya que la especie es el único taxón que posee estatus ontológico. La macroevolución da cuenta de la emergencia de discontinuidades morfológicas importantes entre las especies, razón por la cual se las clasifica como grupos marcadamente diferenciados, es decir, pertenecientes a unidades taxonómicas distintas y de alto rango. En los mecanismos que explican el surgimiento de estas discontinuidades que las diferentes concepciones y aproximaciones disciplinarias se contraponen.
El gradualismo es el modelo macroevolucionista ortodoxo. Explica la macroevolución como el producto de un cambio lento, de la acumulación de muchos pequeños cambios en el transcurso del tiempo. Este cambio gradual debería reflejarse en el registro fósil con la aparición de numerosas formas de transición entre los grupos de organismos. Sin embargo, el registro no es abundante en formas intermedias. Los gradualistas atribuyen esta discrepancia entre su modelo y las pruebas halladas a la imperfección del propio registro geológico (según expresiones de Darwin, el registro geológico es una narración de la que se han perdido algunos volúmenes y muchas páginas). El modelo del equilibrio puntuado ―propuesto en 1972 por N. Eldredge y S. J. Gould― sostiene en cambio que el registro fósil es un fiel reflejo de lo que en realidad ocurrió. Las especies aparecen repentinamente en los estratos geológicos, se las encuentra en ellos por 5 a 10 millones de años sin grandes cambios morfológicos y luego desaparecen abruptamente del registro, sustituidas por otra especie emparentada, pero distinta. Eldredge y Gould utilizan los términos estasis e interrupción, respectivamente, para designar estos períodos. Según su modelo, las interrupciones abruptas en el registro fósil de una especie reflejarían el momento en que ésta fue reemplazada por una pequeña población periférica ―en la cual el ritmo de evolución habría sido más rápido― que compitió con la especie originaria y terminó por sustituirla. De acuerdo con este patrón, la selección natural no sólo opera dentro de la población, sino también entre especies, y los cambios cualitativamente importantes en los organismos ocurrirían en períodos relativamente breves ―desde el punto de vista geológico― separados por largos períodos de equilibrio.

En biología evolutiva, un monstruo prometedor es un organismo con un fenotipo profundamente mutante que tiene el potencial para establecer un nuevo linaje evolutivo. El término se utiliza para describir un evento de especiación saltacional, el cual puede contribuir a la producción de nuevos grupos de organismos. La frase fue acuñada por el genetista alemán Richard Goldschmidt quien pensaba que los cambios pequeños y graduales, que explican satisfactoriamente los cambios microevolutivos, no pueden explicar la macroevolución. La relevancia evolutiva de los monstruos prometedores ha sido rechazada o puesta en duda por muchos científicos que defienden la Teoría sintética de evolución biológica.
En su obra "The material basis of evolution" (La base material de la evolución), Goldschmidt escribió que «el cambio desde una especie a otra no es un cambio que no involucra más y más cambios atomísticos, sino una modificación completa del patrón principal o del sistema de reacción principal en uno nuevo, el que, más tarde puede nuevamente producir variación intraespecífica por medio de micromutaciones».

La tesis de Goldschmidt fue universalmente rechazada y ampliamente ridiculizada dentro la comunidad científica, quien favorecía las explicaciones neodarwinianas de R. A. Fisher, J. B. S. Haldane y Sewall Wright.

No obstante, varias líneas de evidencia sugieren que los monstruos prometedores juegan un papel significativo durante el origen de innovaciones clave y de planes corporales noveles por evolución saltacional, más que por evolución gradual. Stephen Jay Gould expuso en 1977 que los genes o secuencias reguladoras ofrecían cierto apoyo a los postulados de Goldschmidt. De hecho, arguyó que los ejemplos de evolución rápida no minan el darwinismo ―como Goldscmidt suponía― pero tampoco merecen un descrédito inmediato, como muchos neodarwinistas pensaban. Gould insistió que la creencia de Charles Darwin en el gradualismo no fue jamás un componente esencial de su teoría de evolución por selección natural. Thomas Henry Huxley también advirtió a Darwin que había sobrecargado su trabajo con una innecesaria dificultad al adoptar sin reservas el principio "Natura non facit saltum". Huxley temía que ese supuesto podría desalentar a aquellos naturalistas que creían que los cataclismos y los grandes saltos evolutivos jugaban un papel significativo en la historia de la vida. En este sentido, Gould escribió:

Desde hace mucho tiempo los historiadores de la ciencia han señalado que una de las principales disciplinas biológicas, la biología del desarrollo (antes llamada embriología), no formó parte de la síntesis evolutiva, aunque esta rama de la biología fue discutida en detalle por Darwin. Ernst Mayr en su ensayo "What was the evolutionary synthesis?" («Qué fue la síntesis evolutiva?») describió que varios de los embriólogos del período en que se estaba construyendo la síntesis moderna tenían una postura contraria a la teoría evolutiva y señaló que: En las dos últimas décadas, sin embargo, esa falta se participación de la embriología dentro de la teoría sintética ha sido subsanada. De hecho, la biología del desarrollo y la biología evolutiva se han unido para formar una nueva rama de la investigación biológica llamada Biología evolutiva del desarrollo o, coloquialmente, «Evo-devo», que explora el modo en que han evolucionado los procesos del desarrollo y cómo, en última instancia, se han logrado los planes de desarrollo de las diversas partes del cuerpo de los organismos del pasado y de los organismos actuales.

El factor más importante responsable de la síntesis de la biología del desarrollo con la teoría de la evolución fue el descubrimiento de un grupo de genes reguladores llamado familia de genes homeóticos (genes HOX). Estos genes codifican proteínas de unión al ADN (factores de transcripción) que influyen profundamente en el desarrollo embrionario. Por ejemplo, la supresión de las extremidades abdominales de los insectos está determinada por los cambios funcionales en una proteína llamada Ultrabithorax, que es codificada por un gen Hox. Es importante destacar que la familia de genes Hox ha sido identificada en los artrópodos (insectos, crustáceos, quelicerados, miriápodos), cordados (peces, anfibios, reptiles, aves, mamíferos), y hay análogos entre las especies de plantas y hongos.
Los genes HOX desempeñan un papel muy importante en la morfogénesis de los embriones de los vertebrados, ya que proveen información regional a lo largo del eje anteroposterior del cuerpo. Esta familia de genes es homóloga tanto funcional como estructuralmente al complejo homeótico (HOM-C) de "Drosophila melanogaster". Sobre la base de la comparación de genes de varios taxones, se ha logrado reconstruir la evolución de los grupos de genes HOX en vertebrados. Los 39 genes que comprenden la familia de genes HOX en humanos y ratones, por ejemplo, están organizados en cuatro complejos genómicos localizados en diferentes cromosomas, HOXA en el brazo corto del cromosoma 7, HOXB en el 17, HOXC en el 12 y HOXD en el 2, y cada uno de ellos comprende de 9 a 11 genes acomodados en una secuencia homóloga a la que tienen en el genoma de "D. melanogaster".
Aunque el ancestro común del ratón y del humano vivió hace alrededor de 75 millones de años, la distribución y arquitectura de sus genes HOX son idénticas. Por lo tanto, la familia de genes HOX es muy antigua y aparentemente muy conservada, lo que tiene profundas implicaciones para la evolución de los patrones y procesos de desarrollo.

La microbiología fue prácticamente ignorada por las primeras teorías evolucionistas. Esto se debía a la escasez de rasgos morfológicos y la falta de un concepto de especie particularmente entre los procariotas. Ahora, los investigadores evolucionistas están aprovechando su mayor comprensión de la fisiología y ecología, ofrecida por la relativa facilidad de la genómica microbiana, para explorar la taxonomía y evolución de estos organismos.
Estos estudios están revelando niveles totalmente inesperados de diversidad entre los microbios.

Un resultado particularmente importante en los estudios sobre la evolución de los microbios fue el descubrimiento de la transferencia horizontal de genes en 1959 en Japón. Esta transferencia de material genético entre diferentes especies de bacterias ha jugado un papel importante en la propagación de la resistencia a los antibióticos. Más recientemente, a medida que se ha ampliado el conocimiento de los genomas, se ha sugerido que la transferencia horizontal de material genético ha jugado un papel importante en la evolución de todos los organismos. Estos altos niveles de transferencia horizontal de genes han llevado a cuestionar el árbol genealógico de los organismos. En efecto, como parte de la teoría endosimbiótica del origen de los orgánulos, la transferencia horizontal de genes fue un paso crítico en la evolución de eucariotas como los hongos, las plantas y los animales.

La evolución de los primeros eucariontes desde la condición antecedente procariota ha recibido una considerable atención por parte de los científicos. Este acontecimiento clave en la historia de la vida se produjo hace alrededor de 2000 a 1400 millones de años durante el Proterozoico temprano. Dos hipótesis mutuamente no excluyentes se han enunciado para explicar el origen de los eucariotas: la endosimbiosis y la autogénesis. La hipótesis (también llamada teoría) de la endosimbiosis indica que la evolución de las primeras células eucariotas es el resultado de la incorporación permanente de lo que alguna vez fueron células procariotas fisiológicamente diferentes y autónomas, dentro de una arquea como célula hospedadora. De acuerdo con este concepto, las mitocondrias han evolucionado de una cierta forma de antigua proteobacteria aerobia, mientras que los cloroplastos evolucionaron de alguna forma de procariota del tipo de las cianobacterias.

La explicación de estos procesos simbiogenéticos seguiría estos pasos: en un principio, un individuo entraría en contacto con una bacteria, al comienzo esa relación podría ser parasitaria, pero con el tiempo ambos individuos podrían llegar a una relación mutualista, el hospedador encontraría ventajas en las características y especialidades del hospedado. De no llegar a este punto la selección natural penalizaría esta relación, disminuyendo paulatinamente el número de estos individuos en el conjunto de la población; por el contrario, una relación fructífera, se vería favorecida por la selección natural y los individuos implicados proliferarían. Finalmente la estrecha relación de ambos se vería plasmada en la herencia genética del individuo resultante; este individuo portaría parte o el conjunto de los dos genomas originales.
En contraste, la hipótesis autogénica sostiene que las mitocondrias y los cloroplastos ―así como otros orgánulos eucariotas tales como el retículo endoplasmático― se desarrollaron como consecuencia de las presiones de selección para la especialización fisiológica dentro de una antigua célula procariota. Según esta hipótesis, la membrana de la célula hospedadora se habría invaginado para encapsular porciones fisiológicamente diferentes de la célula ancestral. Con el transcurso del tiempo, estas regiones unidas a la membrana se convirtieron en estructuras cada vez más especializadas hasta conformar los diferentes orgánulos que actualmente definen la célula eucariota. No obstante, numerosos hechos, tales como la estructura de la membrana, el tipo de reproducción, la secuencia de ADN y la susceptibilidad a los antibióticos de los cloroplastos y de las mitocondrias tienden a sustentar la hipótesis simbiogenética.

También existen formas de variación hereditaria que no están basadas en cambios de la información genética; pero sí en el proceso de decodificación de ella. El proceso que produce estas variaciones deja intacta la información genética y es con frecuencia reversible. Este proceso es llamado herencia epigenética que resulta de la trasmisión de secuencias de información no-ADN a través de la meiosis o mitosis; y puede incluir fenómenos como la metilación del ADN o la herencia estructural. Se sigue investigando si estos mecanismos permiten la producción de variaciones específicas beneficiosas en respuesta a señales ambientales. De ser éste el caso, algunas instancias de la evolución podrían ocurrir fuera del cuadro típicamente darwiniano, que evitaría cualquier conexión entre las señales ambientales y la producción de variaciones hereditarias; aunque recordando que indirectamente el origen del proceso en sí mismo, si estarían involucrados genes, como por ejemplo los genes de la enzima ADN-metiltransferasa, histonas, etc.

Al respecto, Richard Lenski, Profesor de la Michigan State University, lleva a cabo desde 1989 un experimento sobre evolución utilizando bacterias; debido a la rápida reproducción de estos microorganismos. En 1989, Lenski tomó de un congelador de –80 °C un vial que contenía una cepa de "Escherichia coli"; con el objetivo de observar si se producía alguna diferencia entre la cepa original y sus descendientes. En el experimento se establecieron subcultivos, los cuales se cultivan siempre en las mismas condiciones, y de los cuales cada 75 días se procede a tomar una muestra de cada uno de ellos y congelarla; procediendo del mismo modo para subcultivos de estos subcultivos. Cada 75 días transcurre unas 500 generaciones desde el comienzo del experimento, por lo que actualmente ya han pasado más de 40 000 generaciones desde la cepa original.

Tras 10 000 generaciones las bacterias ya mostraron bastantes diferencias con su ancestro. Las bacterias evolucionadas al compararlas con la cepa original, eran ahora más grandes y se dividían mucho más rápidamente en medio DM (medio de cultivo utilizado para el experimento). El "cambio evolutivo" más importante observado hasta el momento consistió en que uno de los subcultivos en la generación 31 500 de la "E. Coli" (que son citrato negativo) comenzaron a metabolizar el citrato, por lo que aprovechaban mucho mejor el medio DM y por ello pueden crecer más en este medio. Así, las "E. coli" de ese subcultivo evolucionaron en mutantes Citrato positivas.
Otro cambio evolutivo importante es que un subcultivo, después de pasadas 20 000 generaciones, sufrió un cambio en su velocidad de mutación, provocando que estas mutaciones se incrementaran y se acumularan en su genoma (fenotipo hipermutable). Al crecer en un medio ambiente constante, la mayor parte de las nuevas mutaciones fueron de carácter neutral (mutaciones neutrales), pero también se comenzó a notar un incremento de las mutaciones beneficiosas en los descendientes de este subcultivo.

Los resultados de Lenski propiciaron que se establecieran otros experimentos parecidos pero con diferentes condiciones: temperatura, otras fuentes de carbono, presencia de antibióticos; o con diferentes microorganismos: "Pseudomonas fluorescens, Myxococcus xanthus", e incluso levaduras; y en todos ellos se encontraban resultados parecidos. Los microorganismos cambiaban, evolucionaban y se adaptaban a las condiciones del cultivo.

Salvo excepciones, la evolución biológica es un proceso demasiado lento para ser observado directamente. Para ello se recurre a disciplinas como la Paleontología, Biología evolutiva o Filogenia, entre otras áreas, para lo observación y el estudio indirecto de la evolución. Por ello en 1990 el ecólogo Thomas Ray enfrentó el problema desde otra vertiente; consciente de que era imposible observar en un corto periodo de tiempo los cambios evolutivos más complejos de los organismos vivos, decidió crear una herramienta informática capaz de simularla. Este programa informático se denomina "Tierra".

"Tierra" es una simulación computarizada de la evolución biológica. La idea es simple: crear organismos digitales que vivan en un ambiente digital con recursos digitales. Los organismos son programas a los que se les confiere la característica principal de los organismos vivos: utilizar los recursos disponibles en su ambiente con el objetivo de reproducirse. El ambiente sería la memoria de la computadora y los recursos para proliferar el tiempo en el procesador. Por tanto estos organismos digitales competirán por un espacio en la memoria del ordenador y por un tiempo en el procesador que les capacita para la replicación.

En el siglo XIX, especialmente tras la publicación de "El origen de las especies", la idea de que la vida había evolucionado fue un tema de intenso debate académico centrado en las implicaciones filosóficas, sociales y religiosas de la evolución. Hoy en día, el hecho de que los organismos evolucionan es indiscutible en la literatura científica, y la síntesis evolutiva moderna tiene una amplia aceptación entre los científicos. Sin embargo, la evolución sigue siendo un concepto controvertido por algunos grupos religiosos.

Mientras que muchas religiones y grupos religiosos han reconciliado sus creencias con la evolución por medio de diversos conceptos de evolución teísta, hay muchos creacionistas que creen que la evolución se contradice con el mito de creación en su religión. Como fuera reconocido por el propio Darwin, el aspecto más controvertido de la biología evolutiva son sus implicaciones respecto a los orígenes del hombre. En algunos países ―notoriamente en los Estados Unidos― esta tensión entre la ciencia y la religión ha alimentado la controversia creación―evolución, un conflicto religioso que aún dura centrado en la política y la educación pública. Mientras que otros campos de la ciencia como la cosmología, y las ciencias de la Tierra también se contradicen con interpretaciones literales de muchos textos religiosos, la biología evolutiva se encuentra con una oposición significativamente mayor de muchos creyentes religiosos.

La evolución ha sido utilizada para apoyar posiciones filosóficas que promueven la discriminación y el racismo. Por ejemplo, las ideas eugenésicas de Francis Galton fueron desarrolladas para argumentar que el patrimonio génico humano debería ser mejorado a través de políticas de mejoramiento genético, incluyendo incentivos para que se reproduzcan aquellos que son considerados con «buenos» genes, y la esterilización forzosa, pruebas prenatales, contracepción e, incluso, la eliminación de las personas consideradas con «malos» genes. Otro ejemplo de una extensión de la teoría de la evolución que actualmente es considerada infundada es el darwinismo social, un término referido a la teoría malthusiana del partido Whig, desarrollada por Herbert Spencer en frases publicitarias tales como «la supervivencia del más apto» y por otras afirmaciones acerca de que la desigualdad social, el racismo y el imperialismo se encuentran justificados por la teoría evolutiva. Sin embargo, los científicos y filósofos contemporáneos consideran que estas ideas no se hallan implícitas en la teoría evolutiva ni están respaldadas por la información disponible.

A medida que se ha ido desarrollando la comprensión de los fenómenos evolutivos, ciertas posturas y creencias bien arraigadas se han visto revisadas, vulneradas o por lo menos cuestionadas. La aparición de la teoría evolutiva marcó un hito, no solo en su campo de pertinencia, al explicar los procesos que originan la diversidad del mundo vivo; sino también más allá del ámbito de las ciencias biológicas. Naturalmente, este concepto biológico choca con las explicaciones tradicionalmente creacionistas y fijistas de algunas posturas religiosas y místicas y ―de hecho― aspectos como el de la descendencia de un ancestro común, aún suscitan reacciones en algunas personas. El impacto más importante de la teoría evolutiva se da a nivel de la historia del pensamiento moderno y la relación de este con la sociedad. Este profundo impacto se debe, en definitiva, a la naturaleza no teleológica de los mecanismos evolutivos: la evolución no sigue un fin u objetivo. Las estructuras y especies no «aparecen» por necesidad ni por designio divino sino que a partir de la variedad de formas existentes solo las más adaptadas se conservan en el tiempo. Este mecanismo «ciego», independiente de un plan, de una voluntad divina o de una fuerza sobrenatural ha sido en consecuencia explorado en otras ramas del saber. La adopción de la perspectiva evolutiva para abordar problemas en otros campos se ha mostrado enriquecedora y muy vigente; sin embargo en el proceso también se han dado abusos ―atribuir un valor biológico a las diferencias culturales y cognitivas― o deformaciones de la misma ―como justificativo de posturas eugenéticas― las cuales han sido usadas como «"Argumentum ad consequentiam"» a través de la historia de las objeciones a la teoría de la evolución.

Antes de que la geología se convirtiera en una ciencia, a principios del siglo XIX, tanto las religiones occidentales como los científicos descontaban o condenaban de manera dogmática y casi unánime cualquier propuesta que implicara que la vida es el resultado de un proceso evolutivo. Sin embargo, a medida que la evidencia geológica empezó a acumularse en todo el mundo, un grupo de científicos comenzó a cuestionar si una interpretación literal de la creación relatada en la Biblia judeo-cristiana podía reconciliarse con sus descubrimientos (y sus implicaciones). Algunos geólogos religiosos, como Dean William Auckland en Inglaterra, Edward Hitchcock en Estados Unidos y Hugo Millar en Escocia siguieron justificando la evidencia geológica y fósil sólo en términos de un Diluvio universal; pero una vez que Charles Darwin publicara su "Origen de las especies" en 1859 la opinión científica comenzó a alejarse rápidamente de la interpretación literal de la Biblia. Este debate temprano acerca de la validez literal de la Biblia no se llevó a cabo tras puertas cerradas, y desestabilizó la opinión educativa en ambos continentes. Eventualmente, instigó una "contrarreforma" que tomó la forma de un renacimiento religioso en ambos continentes entre 1857 y 1860.

En los países o regiones en las que la mayoría de la población mantiene fuertes creencias religiosas, el creacionismo posee un atractivo mucho mayor que en los países donde la mayoría de la gente posee creencias seculares. Desde los años 1920 hasta el presente en los Estados Unidos, han ocurrido varios ataques religiosos a la enseñanza de la teoría evolutiva, particularmente por parte de cristianos fundamentalistas evangélicos o pentecostales. A pesar de las abrumadoras evidencias que avalan la teoría de la evolución, algunos grupos interpretan en la Biblia que un ser divino creó directamente a los seres humanos, y a cada una de las otras especies, como especies separadas y acabadas. Este punto de vista es comúnmente llamado creacionismo, y sigue siendo defendido por algunos grupos integristas religiosos, particularmente los protestantes estadounidenses; principalmente a través de una forma de creacionismo llamada diseño inteligente. Los «lobbies» religiosos-creacionistas desean excluir la enseñanza de la evolución biológica de la educación pública de ese país; aunque actualmente es un fenómeno más bien local, ya que la enseñanza de base en ciencias es obligatoria dentro de los currículos, las encuestas revelan una gran sensibilidad del público estadounidense a este mensaje, lo que no tiene equivalente en ninguna otra parte del mundo. Uno de los episodios más conocidos de este enfrentamiento se produjo a finales de 2005 en Kansas, donde el Consejo de Educación del Estado de Kansas (en inglés: "Kansas State Board of Education") decidió permitir que se enseñaran las doctrinas creacionistas como una alternativa de la teoría científica de la evolución. Tras esta decisión se produjo una fuerte respuesta ciudadana, que tuvo una de sus consecuencias más conocidas en la creación de una parodia de religión, el pastafarismo, una invención de Bobby Henderson, licenciado en Física de la Universidad Estatal de Oregón, para demostrar irónicamente que no corresponde y es equivocado enseñar el diseño inteligente como teoría científica. Posteriormente, el Consejo de Educación del Estado de Kansas revocó su decisión en agosto de 2006. Este conflicto educativo también ha afectado a otros países; por ejemplo, en el año 2005 en Italia hubo un intento de suspensión de la enseñanza de la teoría de la evolución.

En respuesta a la aceptación científica de la teoría de la evolución, muchos religiosos y filósofos han tratado de unificar los puntos de vista científico y religioso, ya sea de manera formal o informal; a través de un «creacionismo pro-evolución». Así por ejemplo algunos religiosos han adoptado un enfoque creacionista desde la evolución teísta o el creacionismo evolutivo, y defienden que Dios provee una chispa divina que inicia el proceso de la evolución, y (o) donde Dios creó el curso de la evolución.

A partir de 1950 la Iglesia católica tomó una posición neutral con respecto a la evolución con la encíclica "Humani generis" del papa Pío XII.
En ella se distingue entre el alma, tal como fue creada por Dios, y el cuerpo físico, cuyo desarrollo puede ser objeto de un estudio empírico:

Por otro lado, la encíclica no respalda ni rechaza la creencia general en la evolución debido a que se consideró que la evidencia en aquel momento no era convincente. Permite, sin embargo, la posibilidad de aceptarla en el futuro:

En 1996, Juan Pablo II afirmó que «la teoría de la evolución es más que una hipótesis» y recordó que «El Magisterio de la Iglesia está interesado directamente en la cuestión de la evolución, porque influye en la concepción del hombre». El papa Benedicto XVI ha afirmado que «existen muchas pruebas científicas en favor de la evolución, que se presenta como una realidad que debemos ver y que enriquece nuestro conocimiento de la vida y del ser como tal. Pero la doctrina de la evolución no responde a todos los interrogantes y sobre todo no responde al gran interrogante filosófico: ¿de dónde viene todo esto y cómo todo toma un camino que desemboca finalmente en el hombre?».

La reacción musulmana a la teoría de la evolución fue sumamente variada, desde aquellos que creían en una interpretación literal de la creación según el Corán, hasta la de muchos musulmanes educados que suscribieron a una versión de evolución teísta o guiada, en la que el Corán reforzaba más que contradecía a la ciencia. Esta última reacción se vio favorecida debido a que Al-Jahiz, un erudito musulmán del siglo IX, había propuesto un concepto similar al de la selección natural.

Sin embargo, la aceptación de la evolución sigue siendo baja en el mundo musulmán ya que figuras prominentes rechazan la teoría evolutiva como una negación de Dios y como poco fiable para explicar el origen de los seres humanos. Otras objeciones de los eruditos y escritores musulmanes reflejan en gran medida las presentadas en el mundo occidental.

Independientemente de su aceptación por las principales jerarquías religiosas, las mismas objeciones iniciales a la teoría de Darwin siguen siendo utilizadas en contra de la teoría evolutiva actual. Las ideas de que las especies cambian con el tiempo a través de procesos naturales y que distintas especies comparten sus ancestros parece contradecir el relato del Génesis de la Creación. Los creyentes en la infalibilidad bíblica atacaron al darwinismo como una herejía. La teología natural del siglo XIX se caracterizó por la analogía del relojero de William Paley, un argumento de diseño todavía utilizado por el movimiento creacionista. Cuando la teoría de Darwin se publicó, las ideas de la evolución teísta se presentaron de modo de indicar que la evolución es una causa secundaria abierta a la investigación científica, al tiempo que mantenían la creencia en Dios como causa primera, con un rol no especificado en la orientación de la evolución y en la creación de los seres humanos.

Richard Dawkins, en su obra "El gen egoísta" de 1976, hizo la siguiente afirmación:

La evolución biológica es un hecho aceptado desde el siglo XVIII, la teoría que la explica ha tratado de ser debatida científicamente. La teoría vigente que explica la evolución biológica, denominada teoría de la síntesis evolutiva moderna (o simplemente teoría sintética), es el modelo actualmente aceptado por la comunidad científica para describir los fenómenos evolutivos; y aunque no existe hoy una sólida teoría alternativa desarrollada, algunos científicos han reclamado la necesidad de realizar una reforma, ampliación o sustitución de la teoría sintética, con nuevos modelos capaces de integrar por ejemplo la biología del desarrollo o incorporar dentro de la teoría actual una serie de descubrimientos biológicos cuyo papel evolutivo se está debatiendo; tales como ciertos mecanismos hereditarios epigenéticos, la transferencia horizontal de genes, o propuestas como la existencia de múltiples niveles jerárquicos de selección o la plausibilidad de los fenómenos de asimilación genómica para explicar procesos macroevolutivos.

Los aspectos más criticados y debatidos de la teoría de la síntesis evolutiva moderna son:
el gradualismo, que ha obtenido como respuesta el modelo del equilibrio puntuado de Niles Eldredge y Stephen Jay Gould;
la preponderancia de la selección natural frente a los motivos puramente estocásticos;
la explicación al comportamiento del altruismo y
el reduccionismo geneticista que evitaría las implicaciones holísticas y las propiedades emergentes a cualquier sistema biológico complejo.
A pesar de lo indicado, sin embargo, hay que considerar que el actual consenso científico es que la teoría misma (en sus fundamentos) no ha sido rebatida en el campo de la biología evolutiva, siendo solo perfeccionada; y por ello se la sigue considerando como la «piedra angular de la biología moderna».

Entre otras hipótesis minoritarias, destaca la de la bióloga estadounidense Lynn Margulis, quién consideró que del mismo modo que las células eucariotas surgieron a través de la interacción simbiogenética de varias células procariotas, muchas otras características de los organismos y el fenómeno de especiación serían la consecuencia de interacciones simbiogenéticas similares. En su obra "Captando Genomas. Una teoría sobre el origen de las especies" Margulis argumentó que la simbiogénesis sería la fuerza principal en la evolución. Según su teoría, la adquisición y acumulación de mutaciones al azar no serían suficientes para explicar cómo se producen variaciones hereditarias, sino que ella postulo que las organelas, los organismos y las especies surgirían como el resultado de la simbiogénesis. Mientras que la síntesis evolutiva moderna hace hincapié en la "competencia" como la principal fuerza detrás de la evolución, Margulis postulo a la cooperación como el motor del cambio evolutivo. Argumenta que las bacterias, junto con otros microorganismos, ayudaron a crear las condiciones que se requieren para la vida, tales como el oxígeno. Margulis sostuvo que estos microorganismos constituirían un componente importante de la biomasa de la Tierra y que constituirían la principal razón por la que las condiciones actuales se mantienen. Afirmó, asimismo, que las bacterias son capaces de intercambiar genes con mayor rapidez y facilidad que los eucariotas, y debido a esto, son más versátiles, por lo que éstas serían las artífices de la complejidad de los seres vivos.

Igualmente, Máximo Sandín, por otra parte, rechazó vehementemente cualquiera de las versiones del darwinismo presentes en la actual teoría y propuso una hipótesis alternativa para explicar el hecho de la evolución. En primer lugar, justiprecia la obra de Lamarck, revisando las hipótesis o predicciones (conocidas como Lamarckismo) que fueran realizadas por este biólogo; y que actualmente según Sandín, se verían corroboradas por los hechos. Por ejemplo, Sandín formuló su hipóteis a partir del hecho que las circunstancias ambientales pueden condicionar, no sólo la expresión de la información genética (fenómenos epigenéticos, control del splicing alternativo, estrés genómico…), sino la dinámica del proceso de desarrollo embrionario, y postula que el cimiento fundamental de los ecosistemas es el equilibrio y no la competencia.
Conforme a sus ideas, él postula que se puede apreciar la tendencia de las formas orgánicas a una mayor complejidad, y ello sería por la existencia de unas «leyes» (es decir, no «el azar») que gobiernan la variabilidad de los organismos; y que la capacidad de estos cambios están, de alguna manera, inscritas en los organismos. Habida cuenta que el 98,5% del genoma humano, por ejemplo, está compuesto por secuencias repetidas con función reguladora así como una notable cantidad de virus endógenos; y a partir de ello, Sandín propone en su hipótesis que esa conformación del genoma no puede ser el resultado del azar y de la selección natural. Rechazando además la tesis del "ADN egoísta" de Dawkins, Sandín sugiere que ello se produciría por la presión del medio ambiente, lo que provocaría que ciertos virus se inserten en el genoma o determinadas secuencias génicas se modifiquen y, como consecuencia, se generen organismos completamente nuevos; con sustanciales diferencias con respecto a sus predecesores. Según esta teoría, entonces, el mecanismo fundamental del cambio evolutivo no es la selección natural ni la mutación aleatoria; sino solo la capacidad de integración de los virus (en genomas ya existentes (mediante la transferencia horizontalde sus genes). Además, Sandín señala que el medio ambiente, y no las mutaciones aleatorias, provocarían que determinados grupos de seres vivos asuman muevas características, las que ―por otro lado― no serían cambios graduales sino cambios bruscos, en episodios específicos y sin fases intermedias. Según Abdalla, la hipótesis sostenida por Sandín estaría fundamentada por una gran cantidad de datos científicos y abríría una nueva área de investigación en el campo de la biología.



En español:

En inglés:


</doc>
<doc id="3276" url="https://es.wikipedia.org/wiki?curid=3276" title="Evolución humana">
Evolución humana

La evolución humana u hominización es el proceso de evolución biológica de la especie humana desde sus ancestros hasta la actualidad. El estudio de dicho proceso requiere de un análisis interdisciplinario en el que se complementen conocimientos desde ciencias como la genética, la antropología física, la paleontología, la estratigrafía, la geocronología, la arqueología y la lingüística.

El término "humano", en este contexto, se refiere a los individuos de la especie "Homo sapiens". Evidencia morfológica, genética y molecular han determinado que la especie más cercana a "Homo sapiens" es el chimpancé ("Pan troglodytes"). De esta manera, el estudio específico de la evolución humana es el estudio del linaje, o clado, que incorpora a todas las especies más cercanas a los humanos modernos que a los chimpancé. Evidencia molecular y paleontológica han estimado que el ancestro común entre "Homo sapiens" y "Pan troglodytes", vivió en África entre 5 a 7 millones de años (Ma). A partir de esta divergencia, dentro del linaje hominino continuaron emergiendo nuevas especies, todas ellas extintas actualmente a excepción de "Homo sapiens".

La taxonomía se encarga de la clasificación de los organismos. Por ende, la definición de especie es un aspecto fundamental para clasificar especímenes como pertenecientes a distintas o mismas especies. En organismos vivos es posible definir especies bajo el criterio de la capacidad que tienen distintos individuos de reproducirse y tener descendencia fértil (definición de especie biológica). Sin embargo, el registro fósil plantea más problemas, ya que es imposible ver el potencial reproductivo entre organismos extintos. Esto hace que el definir especies en paleontología sea extremadamente complejo. Un supuesto para definir y nombrar especies basadas en el registro fósil es a partir de la morfología; bajo esta premisa esperamos que exista mayor variación morfológica entre especies que entre individuos de la misma especie.

En el estudio de la evolución humana, definir y nombrar especies es, como en toda disciplina paleontológica, no sólo un fenómeno científico pero también político. En ese sentido, podemos dividir a los paleontólogos en dos extremos de acuerdo a la cantidad de especies que están dispuestos a definir en el registro fósil: los agrupadores ("lumpers") tratarán de definir unas pocas especies, con mayor variación inter-específica (es decir, dentro de la especie), mientras que los divisores ("splitters") definirán nuevas especies cuando existe una pequeña diferencia morfológica entre especímenes. Obviamente estos son dos extremos de un fenómeno y la mayoría de los paleontólogos se ubicaran en algún punto medio.

Al analizar el genoma humano actual se ha descubierto que en su proceso evolutivo hay varios hechos que destacar. Así, se observa por ejemplo que el "Homo sapiens" comparte casi el 99 % de los genes con el chimpancé y con el bonobo. Para mayor precisión, el genoma de cualquier individuo de nuestra especie tiene una diferencia de sólo el 1,24% respecto al genoma de "Pan troglodytes" (chimpancés) y de 1,62% respecto al genoma de los gorilas.

El análisis genómico ha establecido el siguiente parentesco:

A partir del análisis genético, se ha postulado que la historia evolutiva humana, dentro de la genealogía humana se habría producido introgresión en varias ocasiones; Ejemplo de ello, el cromosoma Y actual más antiguo (cromosoma-Y A00), el cual se remontaría hasta los "Homo sapiens" arcaicos (hace unos 340 000 años aprox.).

También destaca el descubrimiento de la existencia de hibridación con otras especies homínidas más antiguas, tales como el "Homo neanderthalensis" (de un 1 % a un 4 % de genes neandertales por persona, principalmente en Europa), y con el homínido de Denisova (la población local que vive actualmente en Papúa Nueva Guinea, en el Sudeste Asiático, le debe al menos el 3 % de su genoma por persona a los homínidos de Denisova). Sin embargo, destaca que al analizar el porcentaje total de ADN del "Homo neanderthalensis" dentro de la población humana actual no africana (no dentro de un solo individuo actual), este porcentaje aumenta significativamente a un 20%; estando este genoma neandertal relacionado con genes que produjeron una "heterosis" a adaptaciones ambientales (como fenotipos de la piel), pero también implicado en enfermedades como la diabetes tipo 2, la enfermedad de Crohn, el lupus y la cirrosis biliar.

Igualmente destaca que los retrovirus endógenos humanos (HERV) ("Secuencia de ADN derivado de virus pertenecientes al grupo de los retrovirus") comprenden una parte significativa del genoma humano. Con aproximadamente 98000 fragmentos y elementos ERV, estos componen casi el 8% del genoma actual del ser humano, los cuales ha adquirido el ser humano en diferentes periodos temporales de su evolución.

Los primeros posibles homínidos bípedos (homininos) son "Sahelanthropus tchadiensis" (con una antigüedad de 7 millones de años y encontrado en el Chad, pero que genera dudas acerca de su adscripción a nuestra línea evolutiva), "Orrorin tugenensis" (con unos 6 millones de años y hallado en África Oriental) y "Ardipithecus" (entre 5,5-4,5 millones de años y encontrado en la misma región). Los fósiles de estos homínidos son escasos y fragmentarios y no hay acuerdo general sobre si eran totalmente bípedos. No obstante, tras el descubrimiento del esqueleto casi completo de un individuo de "Ardipithecus ramidus" apodado Ardi, se han podido resolver algunas dudas al respecto; así, la forma de la parte superior de la pelvis indica que era bípedo y que caminaba con la espalda recta, pero la forma del pie, con el dedo gordo dirigido hacia adentro (como en las manos) en vez de ser paralelo a los demás, indica que debía caminar apoyándose sobre la parte externa de los pies y que no podía recorrer grandes distancias.

Los primeros homínidos de los que se tiene la seguridad de que fueron completamente bípedos son los miembros del género "Australopithecus", de los que se han conservado esqueletos muy completos (como el de la famosa Lucy).

Este tipo de homininos prosperó en las sabanas arboladas del este de África entre 4 y 2,5 millones de años atrás con notable éxito ecológico, como lo demuestra la radiación que experimentó, con al menos cinco especies diferentes esparcidas desde Etiopía y el Chad hasta Sudáfrica.

Su desaparición se ha atribuido a la crisis climática que se inició hace unos 2,8 millones de años y que condujo a una desertificación de la sabana con la consiguiente expansión de los ecosistemas abiertos, esteparios. Como resultado de esta presión evolutiva, algunos "Australopithecus" se especializaron en la explotación de productos vegetales duros y de escaso valor nutritivo, desarrollando un impresionante aparato masticador, originando al "Paranthropus"; otros "Australopithecus" se hicieron paulatinamente más carnívoros, originando a los primeros "Homo".

No se sabe con certeza de qué especie proceden los primeros miembros del género "Homo"; se han propuesto "Australopithecus africanus", "A. afarensis" y "A. garhi", pero no hay un acuerdo general. También se ha sugerido que "Kenyanthropus platyops" pudo ser el antepasado de los primeros "Homo".

Clásicamente se consideran como pertenecientes al género "Homo" los homínidos capaces de elaborar herramientas de piedra. No obstante, esta visión ha sido puesta en duda; por ejemplo, se ha sugerido que "Australopithecus ghari" fue capaz de fabricar herramientas hace 2,5 millones de años. Las primeras herramientas eran muy simples y se encuadran en la industria lítica conocida como Olduvayense o Modo 1. Las más antiguas proceden de la región de Afar (Etiopía) y su antigüedad se estima en unos 2,6 millones de años, pero no existen fósiles de homínidos asociados a ellas.

De esta fase se han descrito dos especies, "Homo rudolfensis" y "Homo habilis", que habitaron África Oriental entre 2,5 y 1,8 millones de años atrás, que a veces se reúnen en una sola. El volumen craneal de estas especies oscila entre 650 y 800 cm³.

Esta es sin duda la etapa más confusa y compleja de la evolución humana. El sucesor cronológico de los citados "Homo rudolfensis" y "Homo habilis" es "Homo ergaster", cuyos fósiles más antiguos datan de hace aproximadamente 1,8 millones de años, y su volumen craneal oscila entre 850 y 880 cm³. Morfológicamente es muy similar a "Homo erectus" y en ocasiones se alude a él como «"Homo erectus" africano». Se supone que fue el primero de nuestros antepasados en abandonar África; se han hallado fósiles asimilables a "H. ergaster" (o tal vez a "Homo habilis") en Dmanisi (Georgia), datados en 1,8 millones de años de antigüedad y que se han denominado "Homo georgicus" que prueban la temprana salida de África de nuestros antepasados remotos.

Esta primera migración humana condujo a la diferenciación de dos linajes descendientes de "Homo ergaster": "Homo erectus" en Extremo Oriente (China, Java) y "Homo antecessor/Homo cepranensis" en Europa (España, Italia). Por su parte, los miembros de "H. ergaster" que permanecieron en África inventaron un modo nuevo de tallar la piedra, más elaborado, denominado Achelense o Modo 2 (hace 1,6 o 1,7 millones de años). Se ha especulado que los clanes poseedores de la nueva tecnología habrían ocupado los entornos más favorables desplazando a los tecnológicamente menos avanzados, que se vieron obligados a emigrar. Ciertamente sorprende el hecho que "H. antecessor" y "H. erectus" siguieran utilizando el primitivo Modo 1 (Olduvayense), cientos de miles de años después del descubrimiento del Achelense. Una explicación alternativa es que la migración se produjera antes de la aparición del Achelense.

Parece que el flujo genético entre las poblaciones africanas, asiáticas y europeas de esta época fue escaso o nulo. Parece que "Homo erectus" pobló Asia Oriental hasta hace solo unos 50 000 años (yacimientos del río Solo en Java) y que pudo diferenciar especies independientes en condiciones de aislamiento, como el caso del "Homo floresiensis" de la Isla de Flores (Indonesia), especie desaparecida hace 12 000 años, o el Hombre del ciervo rojo de China, desaparecido hace 11 000 años. Por su parte, en Europa se tiene constancia de la presencia humana desde hace casi 1 millón de años ("Homo antecessor"), pero se han hallado herramientas de piedra más antiguas no asociadas a restos fósiles en diversos lugares. La posición central de "H. antecessor" como antepasado común de "Homo neanderthalensis" y "Homo sapiens" ha sido descartada por los propios descubridores de los restos (Eudald Carbonell y Juan Luis Arsuaga).

Los últimos representantes de esta fase de nuestra evolución son "Homo heidelbergensis" en Europa, que supuestamente está en la línea evolutiva de los neandertales, y "Homo rhodesiensis" en África que sería el antepasado del hombre moderno.

Una visión más conservativa de esta etapa de la evolución humana reduce todas las especies mencionadas a una, "Homo erectus", que es considerada como una especie politípica de amplia dispersión con numerosas subespecies y poblaciones interfértiles genéticamente interconectadas.

La fase final de la evolución de la especie humana está presidida por tres especies humanas inteligentes, que durante un largo periodo convivieron y compitieron por los mismos recursos. Se trata del Hombre de Neanderthal ("Homo neanderthalensis"), la especie del homínido de Denisova y el hombre moderno ("Homo sapiens"). Son en realidad historias paralelas que, en un momento determinado, se cruzan.

El Hombre de Neanderthal surgió y evolucionó en Europa y Oriente Medio hace unos 230 000 años, presentando claras adaptaciones al clima frío de la época (complexión baja y fuerte, nariz ancha).

El homínido de Denisova vivió hace 40 000 años en los montes Altai y probablemente en otras áreas en las cuales también vivieron neandertales y sapiens. El análisis del ADN mitocondrial indica un ancestro femenino común con las otras dos especies hace aproximadamente un millón de años. La secuencia de su genoma ha revelado que habría compartido con los neandertales un ancestro hace unos 650 000 años y con los humanos modernos hace 800 000 años. Un molar descubierto presenta características morfológicas claramente diferentes a las de los neandertales y los humanos modernos.

Los fósiles más antiguos de "Homo sapiens" datan de hace unos 200 000 años (Etiopía). Hace unos 90 000 años llegó al Próximo Oriente donde se encontró con el Hombre de Neanderthal que huía hacia el sur de la glaciación que se abatía sobre Europa. "Homo sapiens" siguió su expansión y hace unos 45 000 llegó a Europa Occidental (Francia); paralelamente, el Hombre de Neanderthal se fue retirando, empujado por "H. sapiens", a la periferia de su área de distribución (Península ibérica, mesetas altas de Croacia), donde desapareció hace unos 28 000 años.

Aunque "H. neanderthalensis" ha sido considerado con frecuencia como subespecie de "Homo sapiens" ("H. sapiens neanderthalensis"), el análisis del genoma mitocondrial completo de fósiles de "H. neanderthalensis" sugieren que la diferencia existente es suficiente para considerarlos como dos especies diferentes, separadas desde hace 660 000 (± 140 000) años. (ver el apartado "Clasificación" en "Homo neanderthalensis").

Se tiene la casi plena certeza de que el Hombre de Neandertal no es ancestro directo del ser humano actual, sino una especie de línea evolutiva paralela derivada también del "Homo erectus/Homo ergaster" a través del eslabón conocido como "Homo heidelbergensis". El neandertal coexistió con el "Homo sapiens" y quizá terminó extinguido por la competencia con nuestra especie. Sin embargo, el análisis del genoma nuclear apunta a un aporte neandertal al acervo genético de los humanos modernos. Los euroasiáticos poseen entre el 1 y el 5 % de genes arcaicos por persona que se pueden atribuir a hibridación de Homo sapiens con Homo neandertales.

En cuanto al llamado Hombre de Cro-Magnon corresponde a las poblaciones de Europa Occidental de la actual especie "Homo sapiens".

Los parientes vivos más cercanos a nuestra especie son los grandes simios: el gorila, el chimpancé, el bonobo y el orangután.

Los fósiles más antiguos de "Homo sapiens" tienen una antigüedad de casi 200 000 años y proceden del sur de Etiopía (formación Kibish del río Omo), considerada como la cuna de la humanidad (véase Hombres de Kibish). A estos restos fósiles siguen en antigüedad los de "Homo sapiens idaltu", con unos 160 000 años.

Algunos datos de genética molecular concordantes con hallazgos paleontológicos, sostienen que todos los seres humanos descienden de una misma Eva mitocondrial o E.M., esto quiere decir que, según los rastreos del ADNmt - que sólo se transmite a través de las madres-, toda la humanidad actual tiene una antecesora común que habría vivido en el noreste de África, probablemente en Tanzania (dada la mayor diversidad genética allí) hace entre 150 000 y 230 000 años (ver haplogrupos de ADN mitocondrial humano).

Estudios de los haplogrupos del cromosoma Y humano, concluyen que por línea paterna hay una ascendencia que llega hasta el Adán cromosómico, el cual habría vivido en el África subsahariana entre hace 60 000 y 90 000 años.

Otros indicios derivados de muy recientes investigaciones sugieren que la de por sí exigua población de "Homo sapiens" hace unos 74 000 años se redujo al borde de la extinción al producirse el estallido del volcán Toba, según la Teoría de la catástrofe de Toba, volcán ubicado en la isla de Sumatra, cuyo estallido ha dejado como rastro el lago Toba. Tal erupción-estallido tuvo una fuerza 3000 veces superior a la erupción del Monte Santa Helena en 1980. Esto significó que gran parte del planeta se vio cubierto por nubes de ceniza volcánica que afectaron negativamente a las poblaciones de diversas especies incluidas la humana. Según esta hipótesis llamada entre la comunidad científica Catástrofe de Toba, la población de "Homo sapiens" (entonces toda en África; la primera migración fuera de África fue en torno al año 70 000 a. C.) se habría reducido a sólo alrededor de 1000 individuos. Si esto es cierto, significaría que el 'pool' genético de la especie se habría restringido de tal modo que se habría potenciado la unidad genética de la especie humana.

No todos están de acuerdo con esa datación. Después de analizar el ADN de personas de todas las regiones del mundo, el genetista Spencer Wells sostiene que todos los humanos que viven hoy descienden de un solo individuo que vivió en África hace unos 60 000 años.

Por todo lo antedicho queda demostrado el monogenismo de la especie humana y, consecuentemente, descartado el poligenismo, que servía de "argumento" a teorías racistas.

Junto a los hallazgos arqueológicos, los principales indicadores de la expansión del ser humano por el planeta son el ADN mitocondrial y el cromosoma Y, que son característicos de la descendencia por línea materna y paterna respectivamente.

Los humanos ya habrían comenzado a salir de África unos 90 000 años antes del presente; colonizando para esas fechas el Levante mediterráneo (Estos restos fósiles han sido atribuibles a tempranos "Homo sapiens", pero su relación real con los humanos modernos es muy discutible).

Australia y Nueva Guinea: la "Línea de Wallace" no significó para los "Homo sapiens" un límite insuperable para acceder a esta región. La llegada de humanos a Australia se data hace unos 50 000 años cuando pudieron fabricar rústicas almadías o balsas de juncos para atravesar el estrecho que separaba a Sahul de la región de la Sonda.

Europa: comenzó a ser colonizada hace sólo unos 40 000 años, se supone que durante milenios el desierto de Siria resultaba una barrera infranqueable desde África hacia Europa, por lo que habría resultado más practicable una migración costera desde las costas de Eritrea a las costas yemeníes y de allí al subcontinente indio. La expansión por Europa coincide con la extinción de su coetáneo de entonces, el hombre de Neandertal.

Oceanía: la colonización de estas islas más próximas a Eurasia se habría iniciado hace unos 50 000 años, pero la expansión por esta MUG (macro-unidad geográfica) fue muy lenta y gradual, y hace unos 5000 años pueblos austronesios comenzaron una efectiva expansión por Oceanía, aunque archipiélagos como el de Hawái y Nueva Zelanda no estaban aún poblados por seres humanos hace 2000 o 1500 años (esto requirió el desarrollo de una apropiada técnica naval y conocimientos suficientes de náutica).

América: la llegada del hombre a América, se habría iniciado hace unos 20 000 o, al menos, 15 000 años, aunque no hay consenso al respecto. Durante las glaciaciones el nivel de los océanos desciende al grado que el "Viejo Mundo" y el "Nuevo Mundo" forman un megacontinente unido por el Puente de Beringia.

Cuando los ancestros del "Homo sapiens" y otros muchos primates vivían en selvas comiendo frutos, bayas y hojas, abundantes en vitamina C, pudieron perder la capacidad genética, que tiene la mayoría de los animales, de sintetizar en su propio organismo tal vitamina. Tales pérdidas durante la evolución han implicado sutiles pero importantes determinaciones: cuando las selvas originales se redujeron o, por crecimiento demográfico, resultaron superpobladas, los primitivos homininos (y luego los humanos) se vieron forzados a recorrer importantes distancias, migrar, para obtener nuevas fuentes de nutrientes (por ejemplo de la citada vitamina C).

Todos los cambios reseñados han sucedido en un periodo relativamente breve (aunque se mida en millones de años), esto explica la susceptibilidad de nuestra especie a afecciones en la columna vertebral y en la circulación sanguínea y linfática.

La cerebración y la corticalización son temas que requieren, por sí solos, artículos propios, dado el alcance y la importancia de dichos procesos. Aquí importa comentar de lo mínimo indispensable para comprender la evolución humana.

La cerebración tanto como la corticalización son fenómenos biológicos muy anteriores a la aparición de los homínidos, sin embargo en estos, y en especial en "Homo sapiens", la cerebración y la corticalización adquieren un grado superlativo (hasta el punto que Theilard de Chardin enunció una curiosa teoría, la de la noósfera y noogénesis, esto es: teoría del pensar inteligente, que se basa en la evolución del cerebro).

El cerebro de "Homo sapiens", en relación a la masa corporal, es uno de los más grandes. Más llamativo es el consumo de energía metabólica (por ejemplo, la producida por la "combustión" de la glucosa) que requiere el cerebro: un 20% de toda la energía corporal, y aun cuando la longitud de los intestinos humanos evidencian los problemas que se le presentan.

En "Homo sapiens" el volumen oscila entre los 1200 a 1400 cm, el promedio global actual es de 1350 cm; sin embargo no basta un incremento del volumen, sino cómo se dispone; esto es: cómo está dispuesta la "estructura" del sistema nervioso central y del cerebro en particular. Por término medio, los "Homo neanderthalensis" pudieron haber tenido un cerebro de mayor tamaño que el de nuestra especie, pero la morfología de su cráneo demuestra que la estructura cerebral era muy diferente: con escasa frente, los neandertalenses tenían poco desarrollados los lóbulos frontales y, en especial, muy poco desarrollada la corteza prefrontal. El cráneo de "Homo sapiens" no sólo tiene una frente prominente sino que es también más alto en el occipucio (cráneo muy abovedado), esto permite el desarrollo de los lóbulos frontales. De todos los mamíferos, "Homo sapiens" es el único que tiene la faz ubicada bajo los lóbulos frontales.

Sin embargo, aun más importante para la evolución del encéfalo parecen haber sido las mutaciones en el posicionamiento del esfenoides.

Se ha hecho mención en el apartado dedicado a la aparición del lenguaje articulado de la importancia del gen FOXP2; dicho gen es el encargado del desarrollo de las áreas del lenguaje y de las áreas de síntesis (las áreas de síntesis se encuentran en la corteza cerebral de los lóbulos frontales). El aumento del cerebro y su especialización permitió la aparición de la llamada lateralización, o sea, una diferencia muy importante entre el hemisferio izquierdo y el hemisferio derecho del cerebro. El hemisferio izquierdo tiene desarrollado en su corteza áreas específicas que posibilitan el lenguaje simbólico basado en significantes acústicos: el área de Wernicke y el área de Broca.

Es casi seguro que ya hace 200 000 años los sujetos de la especie "Homo sapiens" tenían un potencial intelectual equivalente al de la actualidad, pero para que se activara tal potencial tardaron milenios: el primer registro de conducta artística conocido se data hace sólo unos 75 000 años, los primeros grafismos y expresiones netamente simbólicas fuera del lenguaje hablado se datan hace sólo entre 40 000 y 35 000 años. Las primeras escrituras (" memoria segunda" como bien les llamara Roland Barthes) datan de hace entre 5500 o 5000 años, en el Valle del Nilo o en la Mesopotamia asiática.

Se ha dicho, también líneas antes, que "Homo sapiens" mantiene características de estructura craneal "primitivas" ya que recuerdan a las de un chimpancé infantil;, en efecto, tal morfología es la que permite tener la frente sobre el rostro y los lóbulos frontales desarrollados.

La cabeza de "Homo sapiens", para contener tal cerebro, es muy grande; aún en el feto y en el neonato, razón principal por la cual los partos son difíciles, sumada a la disposición de la pelvis.

Una solución parcial a esto es la heterocronía: el neonato humano está muy incompletamente desarrollado en el momento del parto; puede decirse (con algo de metáfora) que la "gestación en el ser humano no se restringe a los ya de por sí prolongados nueve meses intrauterinos, sino que se prolonga extrauterinamente hasta, al menos, los cuatro primeros años"; en efecto, el infante está completamente desvalido durante años, tan es así que, que entre los 2 a 4 años es cuando tiene lo suficientemente desarrolladas las áreas visuales del cerebro como para tener una percepción visual de su propio ser (Estadio del espejo descubierto por Jacques Lacan en la década de 1930). Ahora bien, si "Homo sapiens" tarda mucho en poder tener una percepción plena de su imagen corporal es interesante saber que es uno de los pocos animales que se percibe al ver su imagen reflejada (sólo se nota esta capacidad en bonobos, chimpancés, y si acaso en gorilas, orangutanes, delfines y elefantes).

Tal es la prematuración de "Homo sapiens", que mientras un chimpancé neonato tiene una capacidad cerebral de un 65% de la de un chimpancé adulto, o la capacidad de "Australopithecus afarensis" era en el parto de un 50% respecto a la de su edad adulta, en "Homo sapiens" 'bebé' tal capacidad no supera al 25% de la capacidad que tendrá a los 45 años (a los 45 años aproximadamente es cuando se desarrolla totalmente el cerebro humano).

Pero no basta el desarrollo cronológico. Para que el cerebro humano se "despliegue" -por así decirlo- o desarrolle requiere de estimulación y afecto; de otro modo la organización de algunas de las áreas del cerebro puede quedar atrofiada.

Los Homininos, primates bípedos, habrían surgido hace unos 6 o 7 millones de años en África, cuando dicho continente se encontró afectado por una progresiva desecación que redujo las áreas de bosques y selvas. Como adaptación al bioma de sabana aparecieron primates capaces de caminar fácilmente de modo bípedo y mantenerse erguidos (East Side Story;). Más aún, en un medio cálido y con fuerte radiación ultravioleta e infrarroja algunas de las mejores soluciones adaptativas son la marcha bípeda y la progresiva reducción de la capa pilosa, lo que evita el excesivo recalentamiento del cuerpo. Hace 150 000 años el norte de África volvió a sufrir una intensa desertización lo cual significó otra gran presión evolutiva como para que se fijaran los rasgos principales de la especie "Homo sapiens".

Para lograr la postura y la marcha erecta han tenido que aparecer importantes modificaciones:






Es evidente que la gran cantidad de modificaciones anatómicas que condujeron del cuadrupedismo al bipedismo requirió una fuerte presión selectiva. Se ha discutido mucho sobre la eficacia e ineficacia de la marcha bípeda comparada con la cuadrúpeda. También se ha notado que ningún otro animal de los que se adaptaron a la sabana al final de Mioceno desarrolló una marcha bípeda. Hemos de tener en cuenta que partimos de homínidos con un tipo de desplazamiento cuadrúpedo poco eficaz para largos desplazamientos en terreno abierto: el modo en que se desplazan los chimpancés, apoyando la segunda falange de los dedos de las manos no puede compararse a la marcha cuadrúpeda de ningún otro mamífero. Los primeros homínidos de sabana probablemente se vieron obligados a desplazarse distancias considerables en campo abierto para alcanzar grupos de árboles situados a distancia. La marcha bípeda pudo ser muy eficaz en estas condiciones ya que:

Hace años se argumentó que la liberación de las manos por parte de los primeros homínidos bípedos les permitió elaborar armas de piedra para cazar, lo cual habría sido el principal motor de nuestra evolución. Hoy está claro que la liberación de las manos (que se produjo hace más de 4 millones de años) no está ligada a la fabricación de herramientas, que aconteció unos 2 millones de años después, y que los primeros homininos no eran cazadores y que a lo sumo comían carroña esporádicamente.

Pero la bipedestación trajo una desventaja en la reproducción, ya que el hecho de pasar del cuadrupedismo al bipedismo conllevó un cambio anatómico de las caderas, con gran reducción del canal del parto que hacia más difícil y doloroso el alumbramiento, tal como se demuestra cuando se compara la cadera de un chimpancé promedio con la de un "Australopithecus" como "Lucy", quienes además presentan un tamaño de cerebro similar.

La postura bípeda dejó libres los miembros superiores que ya no tienen que cumplir la función de patas (excepto en los niños muy pequeños) ni la de braquiación, es decir, el desplazamiento de rama en rama con los brazos, aun cuando la actual especie humana, de la cintura hacia arriba mantenga una complexión de tipo arborícola.

Esta liberación de los miembros superiores fue, en su inicio, una adaptación óptima al bioma de sabana; al marchar bípedamente y con los brazos libres, los ancestros del hombre podían recoger más fácilmente su comida; raíces, frutos, hojas, insectos, huevos, reptiles pequeños, roedores y carroña; en efecto, muchos indicios hacen suponer como probable que nuestros ancestros fueran en gran medida carroñeros y, dentro del carroñeo, practicaran la modalidad llamada cleptoparasitismo, esto es, robaban las presas recién cazadas por especies netamente carnívoras; para tal práctica, nuestros ancestros debían haber actuado en bandas, organizadamente.

Los miembros superiores, siempre en relación con otras especies, se han acortado. Estos miembros superiores al quedar liberados de funciones locomotoras, se han podido especializar en funciones netamente humanas. El pulgar oponible es una característica heredada de los primates más antiguos, pero si en éstos la función principal ha sido la de aferrarse a las ramas y en segundo lugar aprehender las frutas o insectos que servían de alimento, en la línea evolutiva que desemboca en nuestra especie la motilidad de la mano, y en particular de los dedos de ésta, se ha hecho gradualmente más precisa y delicada lo que ha facilitado la elaboración de artefactos; aún (junio de 2005) no se tiene conocimiento respecto al momento en que la línea evolutiva comenzó a crear artefactos, es seguro que hace ya más de 2 millones de años "Homo habilis/Homo rudolfensis "realizaba toscos instrumentos que utilizaba asiduamente (en todo caso, los chimpancés, en estado silvestre, confeccionan "herramientas" de piedra, madera y hueso muy rudimentarias). El desarrollo de la capacidad de pronación en la articulación de la muñeca también ha sido importantísimo para la capacidad de elaborar artefactos.

El humano hereda de los prosimios la visión estereoscópica y pancromática (la capacidad de ver una amplia tonalidad de los colores del espectro visible); los ojos en la parte delantera de la cabeza posibilitan la visión estereoscópica (en tres dimensiones), pero si esa característica surge en los prosimios como una adaptación para moverse mejor durante la noche o en ambientes umbríos como los de las junglas, en "Homo sapiens" tal función cobra otro valor; facilita la mirada a lontananza, el otear horizontes, en este aspecto la visión es bastante más aguda en los humanos que en los otros primates y en los prosimios. Esto facilitará el hecho por el cual "Homo sapiens" sea un ser altamente visual (por ejemplo las comunicaciones mediante la mímica), y facilitará asimismo "lo imaginario".

Pese al conjunto de modificaciones morfológicas antes reseñadas, desde el punto de vista de la anatomía comparada, llama la atención una cuestión: "Homo sapiens" es un animal relativamente poco especializado. En efecto, gran parte de las especies animales ha logrado algún tipo de especialización anatómica (por ejemplo los artiodáctilos poseen pezuñas que les permiten correr en las llanuras despejadas), pero las especializaciones, si suelen ser una óptima adaptación a un determinado bioma, conllevan el riesgo de la desaparición de la especie especializada y asociada a tal bioma si éste se modifica.

La ausencia de tales especializaciones anatómicas ha facilitado a los humanos una adaptabilidad inusitada entre las demás especies de vertebrados para adecuarse a muy diversas condiciones ambientales.

Más aún, aunque parezca paradójico, "Homo sapiens" tiene características neoténicas. En efecto, la estructura craneal de un "Homo sapiens" adulto se aproxima más a la de la cría de un chimpancé que a la de un chimpancé adulto: el rostro es achatado ("ortognato" o de "bajo índice facial") y es casi inexistente el "torus" supraorbitario (en la humanidad actual apenas se encuentran vestigios de "torus" en las poblaciones llamadas australoides). De otro modo se puede decir que los arcos superciliares de "Homo sapiens" son "infantiles", delicados, el rostro aplanado o ligeramente prognato.

"Homo sapiens" es, por su anatomía, un animal muy vulnerable si se encuentra en condiciones naturales.

Asociado al hecho por el cual morfológicamente el ser humano tenga características que le aproximan a las de un chimpancé "niño" se encuentra el 'ortognatismo' y esto quiere decir, entre otras cuestiones, que los dientes de "Homo sapiens" son relativamente pequeños y poco especializados, las mandíbulas, por esto, se ha abreviado y hecho más delicadas, falta además el diastema o espacio en donde encajan los colmillos. La debilidad de las mandíbulas humanas las hace casi totalmente inútiles para la defensa a mordiscos ante un predador y, asimismo, son muy deficientes para poder consumir gran parte del alimento en su estado natural, lo que es uno de los muchos déficits corporales que llevan al humano a vivir en una sociedad organizada.

Hablar de la aparición del lenguaje humano, lenguaje simbólico, por lógica parecería implicar que hay que hablar previamente de la "cerebración", y eso es bastante cierto, pero el lenguaje humano simbólico tiene sus antecedentes en momentos y cambios morfológicos que son previos a cambios importantes en la estructura del sistema nervioso central. Por ejemplo, los chimpancés pueden realizar un esbozo primario de lenguaje simbólico basándose en la mímica (de un modo semejante a un sistema muy simple de comunicación para mudos).

Ahora bien, el lenguaje simbólico por excelencia es el basado en los significantes acústicos, y para que una especie tenga la capacidad de articular sonidos discretos, se requieren más innovaciones morfológicas, algunas de ellas muy probablemente anteriores al desarrollo de un cerebro lo suficientemente complejo como para pensar de modo simbólico. En efecto, observemos la orofaringe y la laringe: en los mamíferos, a excepción del humano, la laringe se encuentra en la parte alta de la garganta, de modo que la epiglotis cierra la tráquea de un modo estanco al beber e ingerir comida. En cambio, en "Homo sapiens", la laringe se ubica más abajo, lo que permite a las cuerdas vocales la producción de sonidos más claramente diferenciados y variados, pero al no poder ocluir completamente la epiglotis, la respiración y la ingesta deben alternarse para que el sujeto no se ahogue. El acortamiento del prognatismo que se compensa con una elevación de la bóveda palatina facilitan el lenguaje oral. Otro elemento de relevante importancia es la posición y estructura del hioides, su gracilidad y motilidad permitirán un lenguaje oral lo suficientemente articulado.

Estudios realizados en la Sierra de Atapuerca (España) evidencian que "Homo antecessor", hace unos 800 000 años, ya tenía la capacidad, al menos en su aparato fonador, para emitir un lenguaje oral lo suficientemente articulado como para ser considerado simbólico, aunque la consuetudinaria fabricación de utensilios (por toscos que fueran) por parte del "Homo habilis" hace unos dos millones de años, sugiere que en éstos ya existía un lenguaje oral articulado muy rudimentario pero lo suficientemente eficaz como para transmitir la suficiente información o enseñanza para la confección de los toscos artefactos.

Además de todas las condiciones recién mencionadas, imprescindibles para la aparición de un lenguaje simbólico, se debe hacer mención de la aparición del gen FOXP2 que resulta básico para la posibilidad de tal lenguaje y del pensamiento simbólico, como se verá a continuación.

Se han hipotetizado diferentes posibilidades respecto a la evolución futura del ser humano, entre ellos:

Una línea del pensamiento que asegura que la especie humana ha dejado de evolucionar de la misma forma que el resto de los seres vivos, por diferentes motivos.
Sin embargo, existen también otras posturas que consideran que son precisamente los adelantos tecnológicos los que impulsan actualmente la evolución humana, aunque de manera artificial no darwiniana. Por una parte, se ha propuesto que el entorno actual favorece la reproducción de las personas inteligentes, independientemente de su fuerza física o su estado de salud. Además, es posible que la ingeniería genética humana permita seleccionar las características genéticas de la descendencia.

Por otra parte, también se ha propuesto que en el futuro la tecnología posibilite a las personas vivir como cyborg o incluso como seres digitales dentro de cuerpos o estructuras completamente artificiales.





</doc>
<doc id="3277" url="https://es.wikipedia.org/wiki?curid=3277" title="Homo">
Homo

Homo (del latín "homo", 'hombre', 'humano') es un género de primates homínidos de la tribu Hominini. Se caracteriza por ser bípedo, con pies no prensiles y su primer dedo alineado con los restantes, hipercefalización y una verticalización completa del cráneo. Agrupa a las especies consideradas humanas o que llevan el apelativo de "hombre", por lo que incluye al ser humano moderno ("Homo sapiens") y a sus más cercanos parientes. La antigüedad del género se estima en 2,5 millones de años ("Homo habilis"/"Homo rudolfensis"). Todas las especies, a excepción de "Homo sapiens", están extintas. Los supervivientes más recientes han sido "Homo neanderthalensis" en Europa —que se extinguió hace menos de 30 000 años—, "Homo floresiensis" en Indonesia —que, según las excavaciones realizadas por el Australian Research Council entre 2007 y 2014 y publicadas en marzo de 2016 en la revista "Nature", sobrevivió hasta hace poco más de 50 000 años atrás— y el llamado hombre de la cueva del ciervo en China —desaparecido hace cerca de 11 000 años—.

Entre las características que llevaron a separar "Homo habilis" del género "Australopithecus" destacan el tamaño del cráneo y, más importante aún, la capacidad de crear herramientas y conservarlas para un futuro uso.

La aparición del género "Homo" está sometida a varias interpretaciones. Las teorías ofrecidas por los expertos colocan a las diferentes especies "Homo" en una misma época, lo que hace difícil concretar la línea evolutiva. Por otro lado, estas interpretaciones son temporales y dependen de las investigaciones sobre los hallazgos fósiles encontrados hasta ahora, por lo cual, los nuevos descubrimientos producirán, inevitablemente, cambios en las teorías sobre la evolución humana desarrolladas sucesivamente. Aún así, se expone a continuación una versión más o menos aceptada por la mayoría de la comunidad académica.

Hasta ahora, las herramientas olduvayenses más antiguas atribuidas a "Homo", fueron encontradas en Gona (Etiopía) y datan de 2,6 a 2,5 millones de años. En Hadar (Etiopía) fue encontrado en 1994 un maxilar (AL 666-1), cuyas características indican que podría haber pertenecido a un "Homo" y que data de hace 2,33 millones de años.

Una de las teorías que explicarían el desarrollo del cerebro de "Homo", unido a la utilización de herramientas, es la teoría del tejido costoso, que sostiene que debido a una disminución de superficie de selva, los primeros "Homo" serían carroñeros y comerían el tuétano de los huesos, ayudándose de herramientas de piedra. Hay una relación inversa entre el grado de encefalización y el tamaño del aparato digestivo, por lo que este al acortarse, por una dieta más carnívora, pudo facilitar el aumento del tamaño del cerebro.

Cronológicamente, "Homo habilis" podría ser el primero de nuestros antepasados "Homo". Los restos más antiguos que con certeza pertenecen a esta especie, datan de hace unos 1,8 millones de años y su nombre, "hombre hábil", se debe a que se le adjudica cierto manejo en la elaboración de útiles de piedra. Se cree que convivió con los diferentes tipos de "Australopithecus" y que fue precisamente la presión ejercida por el género "Homo" lo que hizo desaparecer a los australopithecinos. Sin embargo, a pesar de la aparente superioridad tecnológica del "Homo habilis" sobre sus antecesores, las diferencias anatómicas eran relativamente escasas, aunque poseían un cerebro ligeramente más grande que los homininos anteriores.

Según las hipótesis tradicionales, el "H. habilis" evolucionó hace unos 1,5 millones de años hacia el "Homo erectus", especie que llegó a habitar gran parte del Viejo Mundo, desde África hasta China e Indonesia. Sin embargo se cree que tres especies de "Homo" habrían convivido entre hace 1,78 y 2,03 millones de años, Homo erectus, H. habilis y H. rudolfensis.

"H. erectus" comenzó a ser remplazado por formas arcaicas de "Homo sapiens" entre hace 400 y 250 mil años en distintas zonas geográficas. Este "Homo sapiens" arcaico, poseía un cerebro más grande aunque todavía mantenía similitudes físicas con el "Homo erectus".

Debido a los descubrimientos en la Sima de los Huesos en Atapuerca, en 1994, aparecen dos líneas evolutivas y a los estudios genéticos, hoy se tiene la perspectiva de dos líneas evolutivas. La primera desarrollada en el occidente de Asia y Europa desembocó en el "Homo heidelbergensis" y después de éste en el "Homo neanderthalensis"; y la segunda, desarrollada originalmente en el interior de África, devino en el "Homo rhodesiensis" o el "Homo helmei" y posteriormente en el "Homo sapiens". Esta teoría deja abierto el debate sobre el lugar de origen del "Homo erectus" y su relación con las especies "Homo ergaster" y "Homo georgicus".

La hipótesis de si las dos líneas son dos evolutivas es coherente con los estudios genéticos que sustentan la teoría del origen único en África del "Homo sapiens", y en cambio se opone a la hipótesis del origen multirregional, que supone la aparición simultánea del "H. sapiens" en Asia y África.

Tras el estudio del ADN mitocondrial y del genoma del homínido de Denisova, ha quedado claro además, que una tercera especie, diferente de "H. neandertalis" y de "H. sapiens", sobrevivió en Asia hasta hace 40 000 años. No se sabe aún cuál fue la relación de esta especie con "H. erectus" ni con fósiles de China y Java que antes se consideraban "sapiens" arcaicos. En cuanto a la especie "Homo floresiensis", que sobrevivió hasta hace unos 50.000 años, se consideraba inicialmente resultado de una adaptación especializada de "H. erectus" en un hábitat limitado, pero el estudio de sus extremidades indujo a pensar que no procedía de "Homo erectus", sino de algún otro homínido anterior, cuya dispersión en el sudeste de Asia aún no está documentada, o directamente de "Homo georgicus" o de una rama de "Homo habilis". También se ha documentado en Asia la presencia de los llamados Hombres de la cueva de los ciervos.


No todas las especies están plenamente aceptadas por la comunidad científica. Así "H. ergaster" es considerado por algunos autores como "H. erectus"; los escasos restos de "H. cepranensis" plantean numerosos interrogantes; "H. tsaichangensis" es el taxón asignado a una mandíbula fósil encontrada en Taiwán; se cuestiona si "H. georgicus" es una especie diferente o debe clasificarse como "H. erectus"; varios expertos consideran que "H. rhodesiensis" es una variante africana de "heidelbergensis"; "H. helmei" es una especie hipotética, a la que pertenecería un grupo de fósiles de transición hacia "H. sapiens"; se discute si "H. floresiensis" es una rama insular tardía de "H. habilis" o formas patológicas de "H. sapiens".
"Homo heildelbergensis" y "H. neanderthalensis" están muy emparentados y han sido considerados con frecuencia como subespecies de "H. sapiens", pero análisis de ADN mitocondrial de los fósiles del "H. neanderthalensis" sugieren que la diferencia existente es suficiente para denominarlos como dos especies diferentes. Y sobre el "Homínido de Denísova" y los "Hombres de la cueva de los ciervos", es posible que se traten de híbridos o subespecies de "H. sapiens".




</doc>
<doc id="3278" url="https://es.wikipedia.org/wiki?curid=3278" title="Homo erectus">
Homo erectus

Homo erectus es un homínido extinto, que vivió entre 2 millones de años y 70 000 años antes del presente —si se vincula su extinción a la teoría de la catástrofe de Toba— (Pleistoceno inferior y medio). Los "Homo erectus" clásicos habitaron en Asia oriental (China, Indonesia). En África se han hallado restos de fósiles afines que con frecuencia se incluyen en otra especie, "Homo ergaster"; también en Europa, diversos restos fósiles han sido clasificados como "Homo erectus", aunque la tendencia actual es la de reservar el nombre "Homo erectus" para los fósiles asiáticos.

Una característica principal de "Homo erectus" es la «forma de la bóveda craneal, [...] relativamente baja y angular», con un marcado toro supraorbitario, «una frente marcadamente huidiza, [...] y la anchura mayor en una posición muy baja». El volumen craneal, muy variable, fue aumentando a lo largo de su dilatada historia. Tenía una capacidad mayor que la del "Homo habilis" y que la del "Homo georgicus" encontrado en Dmanisi. Los primeros restos que se encontraron del Hombre de Java muestran una capacidad craneal de 850 cm, mientras que los que se encontraron posteriormente llegan a los 1100 cm. Poseía una fuerte mandíbula sin mentón, pero de dientes relativamente pequeños. Presentaba un mayor dimorfismo sexual que en el hombre moderno.

Era muy robusto y tenía una talla elevada, hasta 1,80 m de medida.

Producía industria lítica, principalmente Achelense, y probablemente dominaba el fuego.

Entre 1891 y 1892 el médico anatomista holandés Eugène Dubois creyó encontrar el «eslabón perdido» hipotetizado por Ernst Haeckel al descubrir algunos dientes sueltos, una calota craneal y un fémur —muy similar al del hombre moderno— en las excavaciones paleontológicas que realizaba en el río Solo cerca de Trinil, en el interior de la isla de Java (Indonesia). Dubois publicó estos hallazgos con el nombre de Pithecanthropus erectus (hombre-mono erguido) en 1894, pero más conocido popularmente como "El Hombre de Java" u "Hombre de Trinil". En la década de 1930 el paleontólogo alemán Ralpf von Koenigswald obtuvo nuevos fósiles, tanto de Trinil como de nuevas localidades como Sangiran (a unos 75 km), en total doce especímenes y, en 1938 von Koenigswald identificó claramente un magnífico cráneo de Sangiran, como ""Pithecanthropus"". No será hasta 1940 cuando Mayr atribuye todos estos restos al género "Homo" (Homo erectus erectus).

En China se encontraron otros yacimientos importantes de fósiles de esta especie como, por ejemplo, Lantian, Yuanmou, Yunxian y Hexian. Los investigadores también han encontrado gran número de utensilios fabricados por "H. erectus" en yacimientos como Nihewan y Bose, en China, y en otros lugares de antigüedad similar (al menos entre 1 millón y 250 000 años de antigüedad).

Luego se descubrieron, en Kenia el "Homo ergaster", que se puede considerar el "erectus" africano y probablemente la especie original. También se consideran relacionados con "H. erectus" en África, el cráneo KNM-ER 42700, de 1,55 millones años; y el Cráneo de Yaho (Chad) o "Tchadanthropus uxoris", de hace un millón de años.

En Dmanisi, República de Georgia, en el Cáucaso, fue descubierto el "Homo georgicus", que data de hace 1,8 millones de años, camino hacia el "erectus" de Extremo Oriente, pero relacionado descendiente del "Homo habilis", con lo cual se dibujó la ruta que siguieron los homínidos que dejaron África hasta dispersarse por Asia. Un diente encontrado en 2003 en la cueva Mohui (Guangxi, sur de China) que puede tener hasta 2 millones de años, así como los fósiles de Yuanmou (Yunnan, China, descubiertos en 1965) que datan de 1,7 millones de años, y el cráneo de Mojokerto (Java), que data de entre 1,8 y 1,49 millones de años, posiblemente estén relacionados con esta llegada temprana de "Homo" a Asia.
El conjunto de estos y otros hallazgos es clasificado actualmente dentro del género "Homo" y es designada la especie de los hombres de Java (hombre de Trinil) y Pekín, como "Homo erectus", que parece haber evolucionado en África como "Homo ergaster", a partir de las poblaciones anteriores de "Homo habilis", para a continuación dispersarse por gran parte de Asia desde hace unos 1,7 millones de años.

Fragmentos de un cráneo, identificado como perteneciente a un "H. erectus", fue encontrado en 2002 en Kocabaş, provincia de Denizli, Turquía. Este fósil data de hace 1,1 a 1,3 millones de años.

Fósiles de las épocas de los hombres de Java y Pekín fueron hallados entre 1936 y 1963 en Lantian, Shaanxi, China: los de Gongwangling datan de hace 800 000 - 750 000 años, aún con capacidad endocraneana de 780 cm³ y los de Chenjiawo, de una antigüedad de 530 000 años. También en 1994 en Tangshan, (Nanjing, Jiangsu), se encontró un cráneo de mujer de "Homo erectus" que data de 580 000-620 000 años antes del presente. 
Fósiles más recientes, clasificados como "H. erectus", han sido encontrados en Dali (Shaanxi, 1978), de hace 260 000-300 000 años, con capacidad endocraneana de 1120 cm³; en Jinniushan (Yingkou, Liaoning, 1974) de hasta 280 000 años y alta capacidad craneana; en Maba (Qujiang, Cantón, 1958) de 130 000 años, y en Dingcun (Xiangfen, Shaanxi) de 120 000-100 000 de antigüedad.

Los fósiles más recientes conocidos, atribuidos a la especie "H. erectus", proceden de la cuenca del río Solo, en Java, y fueron encontrados desde 1934 en Ngandong y en Sambungmacan (Sm-I con capacidad endocraneana de 1200 cm³), han sido datados entre 27 000 y 53 300 años antes del presente. Aunque la datación ha sido discutida y se afirma que los fósiles de Ngandong pueden tener más de 120 000 años, el hallazgo del homínido de Denisova y el estudio de su genoma concuerdan con la existencia simultánea con "H. sapiens", de otra especie de hominino en Asia, que la datación de los fósiles de Solo sugiere. De esta forma "H. erectus" habría sido una especie de gran éxito: se dispersó ampliamente y gozó de larga vida.

Desde el descubrimiento de "Homo erectus", los científicos se preguntan si esta especie era un antepasado directo de "Homo sapiens", debido a que las investigaciones hechas no eran suficientes para demostrarlo. Las últimas poblaciones de "H. erectus" —tales como las del río Solo en Java— pueden haber vivido hace solamente 50 000 años, simultáneamente con poblaciones de "H. sapiens", y se descarta que a partir de estas "últimas" poblaciones de "Homo erectus" haya evolucionado "H. sapiens".

Aunque poblaciones anteriores de "H. erectus" asiáticos podrían haber dado lugar a "H. sapiens", hoy se considera más probable que "Homo sapiens" hubiera evolucionado en África probablemente de poblaciones africanas de "H. erectus", luego los primeros "H. sapiens" habrían migrado desde el noreste de África hace menos de 100 000 años al Asia, donde tal vez se encontró con los últimos "H. erectus".

Una especie que posiblemente descienda tardíamente de "Homo erectus", es el pequeño "Homo floresiensis", aunque por el estudio de los huesos de la muñeca, los brazos y el hombro se considera más probable que descienda directamente de "Homo georgicus" o de "H. habilis".

En cuanto a la filogenia posible "Homo habilis" > "Homo erectus" aunque ésta aún es considerada posible, no parece que haya ocurrido de un modo "directo", sino, con más probabilidad, a través de un nexo de estas especies con "Homo rudolfensis". Lo concreto es que los hallazgos realizados en 2007 en Ileret, en la zona del lago Turkana, por Richard Leakey y Meave Leakey podrían indicar que los "H. habilis" vivieron en África hasta hace 1 440 000 años (cráneo KNM-ER 42703), lo cual confirmaría que ambas especies coexistieron por un lapso de por lo menos 500 000 años. Aunque hay autores como Erik Trinkaus que opinan que la convivencia no descarta que los "H. habilis" sean ancestros directos de los "H. erectus".

Un debate por resolver es si debe considerarse a "Homo ergaster" una especie diferente o si los fósiles clasificados como tal deben incluirse en "H. erectus". Su capacidad craneal oscila entre los 804 cm³ de KNM-ER 3883 y los 880 cm³ del niño de Nariokotome que data de 1,6 millones de años. Hay que tener en cuenta que un fósil africano posterior, que data de 1,55 millones de años, el cráneo KNM-ER 42700, por su morfología ha sido asignado a "H. erectus", a pesar registrar la baja capacidad craneal de 691 cm³, menor que la de "H. rudolfensis", que data de al menos 1,9 millones de años.

Se discute si la presencia de herramientas tecnología achelense o Modo 2 en África a partir de hace 1,65 millones de años y la ausencia del modo achelense por cientos de miles de años en los yacimientos en que se ha encontrado el "H. erectus" en Asia oriental y Java, es compatible con la identidad entre "H. ergaster" y "H. erectus" o si debe reconocerse una especie africana y otra asiática o euroasíatica.

En toda Eurasia los únicos yacimientos achelenses con una antigüedad cercana a la de los primeros yacimientos africanos, son el de Ubeidiya (Israel), que data de 1,3 a 1,4 millones de años y el de Attiramapakkam (Tamil Nadu, India), de hace más de un millón de años. En Asia oriental, la más antigua industria lítica similar a la achelense, fue encontrada en el sur de China, en Bose (Guangxi) y data de hace 803.000 ± 3000 años, mientras que en otros lugares de China y Java se mantiene exclusivamente el modo 1 olduvayense, con un relativo y sorprendente estancamiento tecnológico por muchísimo tiempo. 

Los fósiles asignados a "H. erectus" en África, sin embargo, se han multiplicado. Entre ellos se destacan: el Hombre de Buia (Eritrea), semejante a "H. ergaster", con antigüedad de más de un millón de años y capacidad craneal de cerca de 800 cm³; el cráneo de Daka (Etiopía) o BU-VP-2/66 de un millón de años de antigüedad y capacidad encefálica de 995 cm³; el cráneo descubierto en 1961 en Yaho (Angamma, Chad), de una edad estimada en un millón de años y que fue designado inicialmente como "Tchadanthropus uxoris"; tres mandíbulas y un parietal, descubiertos en Tighennif (Argelia), que datan de hace 800 000 años e inicialmente nombradas como "Atlathropus mauritanius"; y el cráneo OH 9 de Olduvai u Hombre de Chellean, de hace 1,15 millones de años y capacidad craneal de 1065 cm³, propuesto como prototipo de la especie "Homo louisleakeyi". Todos estos fósiles están asociados con herramientas achelenses.

Aunque los fósiles africanos de este conjunto comparten con "H. erectus" de Asia oriental un grueso toro supraorbital, cráneo alargado, así como la capacidad cerebral, presentan algunas características que los diferencian y apuntan hacia "H. sapiens": en el cráneo de Buia, por ejemplo, el tamaño y la colocación de los huesos parietales, con una posición alta de su parte más externa, lados curvados y más amplios en la parte superior. Algunos piensan que los cráneos de Daka y Buia podrían ser antepasados o relacionarse con el europeo "Homo antecessor", cuyo carácter de especie diferente de "H. erectus", también es discutida.


Hay menor consenso en la denominación de las siguientes subespecies:







</doc>
<doc id="3279" url="https://es.wikipedia.org/wiki?curid=3279" title="Homo antecessor">
Homo antecessor

Homo antecessor es una especie extinta perteneciente al género "Homo", considerada la especie homínida más antigua de Europa y probable ancestro de la línea "Homo heidelbergensis"-"H. neanderthalensis". Vivió hace unos 900 000 años (Calabriense, Pleistoceno temprano). Eran individuos altos, fuertes, con rostro de rasgos arcaicos y cerebro más pequeño que el del ser humano actual, y con menos circunvoluciones cerebrales.

Existe un intenso debate entre arqueólogos y antropólogos en torno a la clasificación taxonómica de "H. antecessor" y las relaciones de este con el resto de especies del género "Homo", debido a la falta de un cráneo adulto completo y que la mayoría de los especímenes conocidos de "H. antecessor" representan etapas juveniles. Sus descubridores, junto con el respaldo de otros expertos, consideran a "H. antecessor" como una especie diferente, sugiriendo que fue un vínculo evolutivo entre "H. ergaster" y "H. heidelbergensis", y que por tanto es también antepasado de "Homo neanderthalensis"; siendo a su vez el último ancestro común entre los humanos modernos y los neandertales. Por otra parte algunos científicos consideran que los restos de "H. antecessor" pertenecen a individuos de "H. heidelbergensis", que habitó Europa entre los 600,000 y los 250,000 años en el Pleistoceno.

La definición de esta especie es fruto de los más de ochenta restos hallados desde 1994 en el nivel TD6 del yacimiento de Gran Dolina en la sierra de Atapuerca, y que datan de hace al menos 900 000 años, según mediciones paleomagnéticas.

De acuerdo con sus descubridores, entre los caracteres anatómicos de estos homínidos cabe destacar un conjunto de rasgos muy primitivos en el aparato dental, que llevaron a establecer una relación entre éstos y los homínidos africanos del Pleistoceno Inferior. Una mandíbula muy bien conservada de una mujer "H. antecessor", de entre 15 y 16 años, recuperada del yacimiento de la Gran Dolina tiene similitudes muy claras con las del Hombre de Pekín ("Homo erectus"), lo que sugiere un origen asiático de "H. antecessor". Sin embargo, el patrón de desarrollo y erupción de los dientes es prácticamente idéntico al de las poblaciones modernas. 
La morfología facial es similar a la de "Homo sapiens", con orientación coronal y ligera inclinación hacia atrás de la placa infraorbital que determina la presencia de una fosa canina muy conspicua. El borde inferior de esta placa es horizontal y ligeramente arqueado. El arco superciliar es en doble arco y la capacidad encefálica, estimada a partir de un fragmento incompleto de hueso frontal, indica una cifra superior a los 1000 cm³. Mientras que "H. erectus" tiene un patrón de crecimiento facial que es similar al observado en los primeros "Homo" y los "Australopithecus", tanto en "H. antecessor" como "H. sapiens" predomina la resorción ósea durante el crecimiento facial. Las similitudes entre la anatomía subnasal de "H. antecessor" y "H. sapiens" sugieren que la "modernización" de la cara estaba ya claramente en marcha en "H. antecessor".

La morfología de la mandíbula recuerda a la de ciertos homínidos muy posteriores, del Pleistoceno Medio, de la especie "Homo heidelbergensis", como los de la Sima de los Huesos, también de Atapuerca. El esqueleto postcraneal indica una cierta gracilidad en comparación con la mayor robustez del Hombre de Neanderthal de la segunda mitad del Pleistoceno Medio.

La mayoría de individuos alcanzarían una altura de entre 160 y 185 centímetros, con un peso de entre 60 y 90 kilogramos.

En la actualidad, la validez de esta denominación como especie diferente es defendida por sus descubridores y otros expertos, que consideran que "H. antecessor" precede a "H. heidelbergensis" y por tanto es también antepasado de "H. neanderthalensis"; sin embargo, parte de la comunidad científica la considera una simple denominación, no específica, para referirse a restos encontrados en Atapuerca, que ellos asignan a la especie "H. heidelbergensis" o bien, la consideran una variedad de "Homo erectus/Homo ergaster".

En 1994 se descubrió en Ceprano (Italia) la parte superior del cráneo de un "Homo" de caracteres primitivos, datada entre 800 000 y 900 000 años de antigüedad, y para el que se propuso en 2003 la especie "Homo cepranensis". Sin embargo, las características filogenéticas, cronológicas, arqueológicas y geográficas de los restos hallados en Ceprano (Italia) han llevado a algunos autores a atribuirlos a "Homo antecessor". La comparación directa de ambos conjuntos de fósiles no es posible, pues pertenecen a distintas partes anatómicas o corresponden a individuos de edades diferentes, pero ambos conjuntos poseen en común el presentar rasgos intermedios entre las poblaciones de "Homo" primitivas de África y las más recientes de "Homo heidelbergensis" de Europa. Por otro lado, tanto la datación absoluta como la industria lítica de Ceprano son coherentes con las obtenidas del nivel TD6 de la Gran Dolina. Si realmente estos restos corresponden a la misma especie de "Homo", la denominación "H. antecessor" tiene prioridad nomenclatorial.

En 2010 se informó sobre el descubrimiento de herramientas en Happisburgh, Norfolk, Inglaterra, las cuales se cree fueron usadas por "H. antecessor" y datan de hace 780 000 años antes del presente.

En la playa de Happisburgh, en la misma formación geológica en la que han aparecido los restos anteriores, se han encontrado numerosas huellas de pisadas dejadas por al menos cinco individuos, un adulto y varios juveniles, sobre los sedimentos fangosos de un estuario. Las icnitas, datadas entre 1 000 000 y 780 000 años, se atribuyen tentativamente a "Homo antecessor" atendiendo a la edad similar de los restos de Atapuerca.

El arqueólogo Eudald Carbonell i Roura de la Universidad Rovira i Virgili en Tarragona, y el paleoantropólogo Juan Luis Arsuaga Ferreras de la Universidad Complutense de Madrid descubrieron restos de "Homo antecessor" en el sitio fosilífero de la Gran Dolina, en la Sierra de Atapuerca, en lo que ahora es el este de Burgos. Los restos de" H. antecessor" se han encontrado en el nivel 6 (TD6) del sitio Gran Dolina.

En 1994 y 1995 se descubrieron más de 80 fragmentos de hueso de seis individuos. El sitio también incluyó aproximadamente 200 herramientas de piedra y 300 huesos de animales. Se encontraron herramientas de piedra incluyendo un cuchillo tallado en piedra junto con los antiguos restos de homínidos. Todos estos restos databan de al menos 900,000 años de antigüedad. [20] Los restos mejor conservados son un maxilar superior y un hueso frontal de un individuo que murió a la edad de 10-11 años.

En marzo de 2008 se dieron a conocer nuevos restos que se atribuyeron inicialmente a "Homo antecessor", concretamente parte de una mandíbula de un individuo de unos 20 años y 32 herramientas de sílex de tipo olduvayense (modo 1), datados en 1,2 millones de años de antigüedad, haciendo retroceder considerablemente la presencia de homínidos en Europa. Los restos fueron hallados en 2007 en la Sima del Elefante, yacimiento situado a unos 200 metros de la Gran Dolina. Sin embargo el estudio detallado de la mandíbula, que presenta características en la dentición y la sínfisis que la aproximan a los "Homo" más antiguos de África y de Dmanisi (Georgia) pero con algún carácter derivado (parte interna de la sínfisis), han llevado posteriormente a identificar estos restos como "Homo" sp. (pertenecientes al género "Homo" pero sin precisar la especie), ya que probablemente pertenezcan a una nueva especie aún por definir, y cuyas relaciones filogenéticas, sin más datos, son aún imprecisas.

En el municipio de Atapuerca ha crecido significativamente a nivel económico, demográfico y social con el impacto generado con el yacimiento arqueológico y sus servicios asociados. El 15% de la población activa ya se dedica al turismo, esta "terciarización" de su economía ha revertido el despoblamiento y ha hecho que vuelva a crecer el número de habitantes, rejuveneciendo la población y situando la media de edad en 42 años. Directamente relacionado, la creación de empleo que se ha derivado de este tipo de acciones, ha tenido un impacto social positivo en la sociedad.




</doc>
<doc id="3282" url="https://es.wikipedia.org/wiki?curid=3282" title="Homínido">
Homínido

El término homínido está relacionado con la evolución humana y tiene los siguientes usos:



</doc>
<doc id="3285" url="https://es.wikipedia.org/wiki?curid=3285" title="Idioma catalán">
Idioma catalán

El idioma catalán ("català" es el autoglotónimo y la denominación oficial en la comunidad autónoma de Cataluña, de las Islas Baleares, Andorra, la ciudad italiana de Alguer y denominación tradicional en la región francesa del Rosellón) o idioma valenciano ("valencià" es el autoglotónimo y la denominación oficial en la Comunidad Valenciana y tradicional en la comarca murciana de El Carche) Es una lengua romance hablada por unos diez millones de personas (incluyendo hablantes no nativos) en Cataluña, la Comunidad Valenciana (excepto en algunas comarcas del interior), las Islas Baleares, Andorra, la Franja de Aragón, el Carche (una comarca de la Región de Murcia poblada por inmigrantes valencianos), el Rosellón, la ciudad sarda de Alguer y en pequeñas comunidades de todo el mundo (entre las cuales destaca la de Argentina, con 195 000 hablantes). Tiene unos diez millones de hablantes, de los cuales alrededor de la mitad son nativos; su dominio lingüístico, con una superficie de 68 730 km² y 13 529 127 habitantes (2009), incluye 1687 términos municipales. Como las otras lenguas romances, el catalán desciende del latín vulgar que hablaban los romanos que se establecieron en Hispania durante la Edad Antigua.

El grado de uso y de oficialidad del catalán varía mucho según el territorio, que va desde la nula oficialidad en Francia a ser el único idioma oficial en Andorra, pasando por la cooficialidad en tres comunidades autónomas españolas. Según un estudio del Instituto de Estadística de Cataluña en 2008, el idioma catalán es el segundo más usado habitualmente en Cataluña, tras el idioma castellano, que supera al catalán no solo como lengua habitual, sino también como lengua materna y de identificación, aunque el catalán es el más usado en cinco de las siete áreas funcionales de Cataluña y el 80 % de la población lo sabe hablar. Cada aspecto y contexto social del uso del idioma en Cataluña es estudiado por la Generalidad de Cataluña con el fin de fomentar su uso, donde es la lengua principal en la educación, en las administraciones públicas y en los medios de comunicación públicos; además, esta invierte anualmente en la promoción del catalán tanto en Cataluña como en otros territorios.

La lengua catalana se habla en cuatro países diferentes:
Una denominación que intenta englobar a toda esa área lingüística, no exenta de discusiones por el carácter ideológico que ha ido adquiriendo, es la de Países Catalanes, acuñada a finales del siglo XIX y popularizada por Joan Fuster en su obra "Nosaltres els valencians" («Nosotros los valencianos», 1962).

La característica sociolingüística más destacada del catalán es que en todos los territorios en los que se habla se encuentra en situación de bilingüismo social: con el francés en el Rosellón, con el italiano (más que con el sardo) en Alguer, y con el castellano en el resto de su ámbito lingüístico, incluyendo Andorra, donde es la única lengua oficial según la Constitución andorrana pero donde también se habla el castellano y el francés.

(% de la población de 15 y más años).

(% de la población de 15 y más años).

En Cataluña se hablan varias lenguas, siendo las principales el catalán y el castellano o español. De acuerdo con el Estatuto de Autonomía, ambos idiomas, junto con el occitano (en su variante aranesa), son oficiales. Además, se considera al catalán lengua propia de Cataluña, en tanto que el occitano se considera lengua propia del Valle de Arán. Generalmente los catalanes son bilingües y conocen las dos lenguas principales aunque difieren respecto al idioma que tienen por lengua materna. Según los datos de 2013, el 99,7 % de los catalanes sabe hablar castellano en tanto que el 80,4 % sabe hablar catalán. Además, el uso por cada hablante de uno u otro idioma depende con frecuencia del ámbito social en el que se exprese. Según los datos del Instituto de Estadística de Cataluña, el 36 % de los catalanes utiliza el catalán como lengua habitual, el 46 % el castellano, el 12 % ambas y el 0,03 % el aranés. El 6 % de la población residente de Cataluña utiliza habitualmente otras lenguas. El aranés es la lengua materna del 22,4 % de la población del Valle de Arán, la propia del 27,1 % y la habitual del 23,4 %.

En Cataluña se hablan los dos bloques principales de la lengua catalana. El oriental tiene como máximo exponente el dialecto central, que se habla en las comarcas del norte de Tarragona, Barcelona, y Gerona, en cuya región pirenaica se atisban rasgos de catalán septentrional. El occidental es el propio de las comarcas occidentales de Cataluña (provincia de Lérida y sur de las tarraconenses) y muestra rasgos similares al valenciano, con el que forma un continuo y en cuya intersección se encuentra el tortosino. El catalán es especialmente preponderante fuera del área metropolitana de Barcelona y del Campo de Tarragona. La Generalidad ha venido desarrollando legislación que promueve y protege el uso social del catalán. En 2008, la catalana era considerada la lengua materna del 35,4 % de los catalanes, la propia del 46 % y la de uso habitual del 47,6 % (los porcentajes incluyen también a los hablantes que consideran conjuntamente al castellano y al catalán como lengua materna, propia o de uso habitual).

El castellano que se habla en Cataluña tiene rasgos dispares, sin mostrar un dialecto específico. Algunos hablantes del castellano que son originarios de otras regiones de España muestran rasgos fonéticos y dialectales propios de su tierra de origen, mientras que otros neutralizaron esos rasgos, ya sea a voluntad, por contacto con catalanohablantes, por la influencia de los medios de comunicación, etc. Los catalanohablantes que hablan castellano muestran algunas influencias de su lengua materna y sus rasgos son, a veces, estereotipados como los propios de los catalanes al hablar en lengua castellana. El castellano no se considera lengua propia de Cataluña por su origen, puesto que fue traído por las grandes oleadas inmigratorias del siglo XX, sobre todo las de los años 60 y 70, procedentes del resto de España, sobre todo de Andalucía y Extremadura, y que en gran parte se concentraron en el área metropolitana de Barcelona. En 2008, el castellano era la lengua materna del 58,8 % de los catalanes, la propia del 55,3 % y la habitual del 57,9 % (los porcentajes incluyen también a los hablantes que consideran conjuntamente al castellano y al catalán como lengua materna, propia o de uso habitual).

La comunidad inmigrante o foránea instalada en Cataluña a menudo mantiene su lengua materna para comunicarse con sus familiares o hablantes de su mismo idioma que residan también en el territorio. Aparte del castellano hablado por los inmigrantes procedentes del resto de España y de Hispanoamérica, destacan sobre todo el árabe y el rumano, si bien su número se extiende considerablemente en ciudades que, como Barcelona, con habitantes de hasta 131 nacionalidades, muestra un amplio repertorio lingüístico, de los que además de los citados, destaca el bereber, el francés, el portugués, el alemán, el ruso, y el inglés. La encuesta estadística de usos lingüísticos de la "Generalidad" realizada en 2003 revelaba también la presencia importante de hablantes de gallego.

En Cataluña el factor más importante del bilingüismo social es la inmigración desde el resto de España en el siglo XX. Se ha calculado que, sin migraciones, la población de Cataluña habría pasado de unos 2 millones de personas en 1900 a 2,4 en 1980, en vez de los más de 6,1 millones censados en esa fecha (y superando los 7,4 millones en 2009); es decir, la población sin migración habría sido solamente el 39 % en 1980.

Actualmente, según el Instituto de Estadística de la Generalidad, el idioma catalán es el segundo más usado habitualmente en Cataluña, tras el idioma castellano, que supera al catalán no solo como lengua habitual, sino también como lengua materna y de identificación, aunque el catalán es el más usado en 5 de las 7 áreas funcionales de Cataluña. Según los datos del Instituto de Estadística de Cataluña (Idescat) para el año 2008, el catalán es la lengua habitual del 47,6 % de la población de Cataluña (un 35,6 % como lengua habitual y un 12 % bilingüe con el castellano). En términos absolutos, 2 933 300 personas tienen al catalán como idioma habitual ( como lengua habitual y 736 700 bilingüe con el castellano), frente a 3 566 700 (57,90 %) que tienen al castellano. Respecto a la encuesta anterior de Idescat se observa un aumento en cifras absolutas del uso habitual del catalán ( frente a 2 850 000 de 2003) pero un retroceso en valores relativos (47,6 % frente a 50,7 %). Se observa también un crecimiento, tanto en valores absolutos como relativos, de los habitantes de Cataluña que usan habitualmente tanto el castellano como el catalán (en valores absolutos se produce casi una triplicación, pasando de 265 400 a 736 700; en valores relativos, el crecimiento es del 4,7 % al 12 %), lo que se traduce en la disminución de las personas que utilizan habitualmente solo el catalán.

Los habitantes de Cataluña que tienen lengua materna al catalán son menos que aquellos que la usan de forma habitual. Según Idescat, en 2008, 2 186 000 personas (34,60 %) tenían al catalán como lengua materna (frente a 3 625 500, 58 %, que tienen al castellano). Estas cifras incluyen a 236 500 que también tienen como lengua materna al castellano. Se muestran fenómenos similares a los descritos con la lengua habitual en relación con los datos de 2003: estabilización de hablantes que tienen al catalán como lengua materna (2 177 800 en 2003 frente a los citados 2 186 000 de 2008), con retroceso en términos relativos (38,70 frente a 34,6 %); aumento de los hablantes que tienen como lenguas maternas al castellano y al catalán (se pasa de 141 600 a 236 500 hablantes; aumento del 2,5 al 3 % en términos relativos), con la consiguiente disminución del número de personas que consideran exclusivamente al catalán como lengua materna.

En sentido similar, los catalanes que consideran al catalán como lengua de identificación son menos (pero no de forma tan acusada) que los que la usan de forma habitual. Según los datos de Idescat, en 2008, 2 770 500 personas (49,3 %) tenían al catalán como lengua de identificación (por 3 410 300, 55,30 % que lo hacían con el castellano). Estas cifras incluyen a 542 800 personas que también se identifican con el castellano. Se muestran los mismos fenómenos que los relativos a lengua habitual y materna con respecto a los datos de 2003: ligero aumento del número de hablantes que se identifican con el catalán (2 770 500 en 2003 frente a los citados 2 770 500 de 2008), con retroceso en términos relativos (49,3 frente a 46 %); aumento de los hablantes que se identifican con el catalán y el castellano (se pasa de 278 600 a 542 800 hablantes; aumento del 5 al 8,8 % en términos relativos), con la simétrica disminución del número de personas que identifican exclusivamente con el catalán.

En cuanto al conocimiento escrito, según datos oficiales de 2007, el 56,3 % de la población catalana sabía escribir en catalán.

Se observa que el catalán se mantiene como lengua habitual en términos absolutos entre 1980 y 2008, aunque de manera lenta, en vez de retroceder como en la Comunidad Valenciana o Rosellón. El retroceso en términos relativos que se ha producido en el periodo 2003-2008 se debe a la importante llegada de inmigrantes a Cataluña, más de medio millón en dicho periodo, un 36 % de los cuales tiene al castellano como lengua materna. Otros estudios, como "La segunda generación en Barcelona: un estudio longitudinal" (marzo de 2009), aplicado al área metropolitana de Barcelona, señalan que aproximadamente el 80 % de los inmigrantes de la zona de estudio considerada prefiere utilizar el castellano, un porcentaje superior al de los que lo hablan por su origen. Los autores creen que es así por haberse instalado los inmigrantes en barrios donde el castellano es más usual.

Con respecto a la distribución territorial (datos de 2008), el uso del catalán (exclusivo, sin contar a quienes hablan también habitualmente en castellano) es predominante en las áreas funcionales de las Comarcas gerundenses (50,9 %), Tierras del Ebro (72,8 %), Poniente (64,4& %), Cataluña Central (56,7 %) y Alto Pirineo y Arán (60,1 %), donde el catalán como lengua habitual (exclusiva) es usado por más del 50 % de la población. Los grados menores de uso se dan en el Campo de Tarragona (33,1 %) y el Área Metropolitana de Barcelona (27,8 %). Respecto a los datos de 2003, se observa un retroceso porcentual de los hablantes habituales exclusivos de catalán en todas las áreas, que va del 8,8 % en Poniente al 16,5 del Campo de Tarragona.

La Generalidad de Cataluña ha llevado a cabo una labor de fomento y potenciación del uso del catalán como la lengua prioritaria en Cataluña. Tanto el estatuto de autonomía de 1979 como el de 2006 definen al catalán como lengua propia de Cataluña. El estatuto de 2006 indica además que:
La parte catalanoparlante de la Comunidad Valenciana tiene una realidad sociolingüística muy compleja y plural, con predominio del castellano en las zonas urbanas y con predominio del valenciano en las zonas rurales; la provincia de Castellón y el sur de la provincia de Valencia son las zonas donde más se habla el valenciano, y la provincia de Alicante y el área metropolitana de Valencia son las zonas donde menos se habla. 

En la Comunidad Valenciana existen dos lenguas de amplio uso y conocimiento entre la población autóctona: el valenciano y el castellano, declaradas como idiomas oficiales según el Estatuto de Autonomía. El valenciano está considerado como lengua propia, si bien el castellano es la lengua empleada por la mayor parte de la población y los medios de comunicación, pero ambas cuentan con una amplia tradición literaria y cultural. Asimismo, en la Comunidad Valenciana existen dos predominios lingüísticos oficiales territorialmente para el castellano y el valenciano, definidas por la Ley de uso y enseñanza del valenciano, basándose en la distribución lingüística del siglo XIX.

El predominio castellano se concentra básicamente en una franja interior central y occidental, y un exclave (Aspe y Monforte del Cid) en el extremo sur, comprendiendo en ella el 25 % del territorio y en la que reside el 13 % de la población. En dicho territorio se emplean unas variantes dialectales que son la churra y la murciana, si bien esta última no está consensuada por todos los lingüistas debido a las diferencias dialectales de la Vega Baja del Segura y Villena con la zona oriental de Murcia. El valenciano tiene en esta zona un grado de conocimiento limitado.

El predominio del valenciano se concentra en la costa y comarcas contiguas, abarca un 75 % del territorio y en ella reside el 87 % de la población. En esta área, el 36,4 % de la población afirma utilizarlo preferentemente en el hogar, según un sondeo del 2005, frente a un 54,5 % que usa preferentemente el castellano. Por zonas, el uso del valenciano en el hogar es predominante en las zonas de concentración urbana media o baja del área, mientras que el castellano lo es en las grandes concentraciones urbanas. El castellano que se habla en esta área es a grandes rasgos un estándar con algunos rasgos fonéticos y léxicos propios o influenciados por el valenciano.
En la parte de la Comunidad Valenciana donde es lengua propia, existe un proceso de sustitución lingüística del catalán (o valenciano) por el castellano. Este se ha completado casi del todo en la ciudad de Alicante y está muy avanzado en la de Valencia, aunque aún no es importante en áreas rurales. Hasta época reciente, muchos hablantes estaban en situación próxima a la diglosia, lo que significa que usaban el catalán solo en situaciones informales, mientras que en las situaciones institucionalizadas se usaba en exclusiva el castellano. Pero desde que se enseña en las escuelas ha aumentado mucho su conocimiento escrito, aunque en las últimas décadas ha retrocedido mucho su uso social. Además ha habido una importante inmigración desde otras partes de España, lo que ha contribuido al predominio estadístico del castellano en la comunidad.

El catalán es la lengua propia de las Islas Baleares (así definida en su Estatuto de Autonomía) y cooficial, junto al castellano, por serlo esta en todo el Estado. El caso balear es parecido al de Cataluña, ya que aquí el factor principal en la expansión del castellano ha sido la inmigración, en mucha mayor medida que la sustitución lingüística. 
La situación sociolingüística del catalán en las Islas Baleares es diferente según la isla y la zona, en Menorca y en la mayor parte de Mallorca, en la Parte Foránea, es donde más se habla el catalán, y en Palma y en Ibiza es donde menos se habla. Además, en las zonas turísticas, se hablan el inglés y el alemán. Aunque con menos impacto, el italiano es también un idioma frecuente, sobre todo en Formentera, que cuenta con un alto índice de turismo de esa nacionalidad.

De acuerdo con los datos del censo del Instituto de Estadística de les Illes Balears de 2001 y los datos sociolingüísticos del IEC de 2002, con respecto al catalán la población se distribuiría de la siguiente manera: sabe hablarlo el 74,6 %, lo entiende el 93,1 %, sabe leerlo el 79,6 %, sabe escribirlo el 46,9 %. Por su parte, según una encuesta realizada en 2003 por la Secretaría de Política Lingüística, de los 1 113 114 habitantes de Baleares lo entienden 749 100 (el 93,1 %), lo saben hablar 600 500 (el 74,6 %), y es la lengua habitual para 404 800 personas (el 45,7 %).

El catalán es la lengua propia y tradicional de este territorio llamado Franja Oriental de Aragón, es hablado por un sector significativo de la población (el 47,1 % de la población lo usa como lengua habitual según una encuesta del Instituto Aragonés de Estadística porcentaje que sube al 73,6 % según una encuesta realizada en las mismas fechas pero con un ámbito territorial menor por la Generalidad de Cataluña; ambas encuestas dan una cifra aproximada de 30 000 hablantes habituales de catalán), a pesar de que no es una lengua oficial y que tiene una presencia casi nula en las instituciones públicas, muy limitada en la enseñanza, donde solo es posible estudiarla como optativa, en la administración y en actos públicos en general.

El caso andorrano es parecido al de Cataluña, ya que aquí el factor principal en la expansión del castellano y del francés ha sido la inmigración en mucha mayor medida que la sustitución lingüística. Además es el único territorio donde el catalán es el único idioma oficial y es el único estado del mundo que lo tiene como idioma oficial.

Que un estado independiente lo tenga por idioma oficial le permite al catalán tener una cierta presencia en el ámbito internacional. El ingreso de Andorra en la ONU, el 28 de julio de 1993, permitió por primera vez en la historia el uso del catalán en una asamblea de esta organización. También Andorra llevó por primera vez la lengua catalana al Festival de la Canción de Eurovisión en 2004 con Marta Roure y la canción «Jugarem a estimar-nos».
El idioma oficial de Andorra es el catalán, aunque la realidad lingüística es el resultado de la gran transformación demográfica que ha vivido el país desde la segunda mitad del siglo XX: en 1940 las personas extranjeras residentes en el país representaban solo el 17 %; en 1989 representaban el 75,7 % —máximo histórico— y en 2007 son alrededor del 65 %. También suele oirse el francés, por la situación fronteriza del Principado. Recientemente ha habido un incremento significativo de la población de habla portuguesa.

De acuerdo con el Servicio de Política Lingüística del gobierno andorrano, el catalán es la lengua materna del 49,4 % de la población de nacionalidad andorrana, pero solo el 29,9 % de la población total lo utiliza. Por el contrario, el castellano es la lengua materna más extendida entre la población del Principado. A pesar del crecimiento de la población de nacionalidades andorrana y portuguesa, el 43,4 % declaró que el castellano es su lengua materna. El estudio muestra que en los últimos años se ha producido un deterioro de la posición de la lengua catalana en favor del castellano.

En cuanto a la alfabetización, el 100 % de los ciudadanos saben leer y escribir. El castellano es la lengua que ocupa el primer lugar respecto a la proporción de la población que aprendió a leer y escribir, seguida del francés, y en tercer lugar el catalán.

Según el Observatorio Social de Andorra del Instituto de Estudios Andorranos, los usos lingüísticos en Andorra son los siguientes:

En el Rosellón, como en la mayor parte de Francia, el proceso de sustitución lingüística del idioma local por el francés está muy avanzado, con el clásico patrón por el cual el idioma cambia primero en las ciudades y solo más tarde en el campo. Actualmente cerca de la mitad de la población entiende el catalán y entre un 20 % y un 30 % es capaz de hablarlo, pero su conocimiento escrito y su uso social es inferior al 10 %.

El real decreto francés de Luis XIV del 2 de abril de 1700, con fecha de aplicación de 1 de mayo del mismo año, prohibió drásticamente el uso de la lengua catalana en documentos oficiales, notariales y de otro tipo, bajo pena de invalidar el contenido. Desde entonces, el francés continúa siendo la única lengua oficial, y la única que se utiliza en la enseñanza pública.

Los últimos datos sociolingüísticos de los que dispone la Generalidad de Cataluña (2004) reflejan que el francés es la lengua mayoritaria en el Rosellón, con una presencia minoritaria del catalán. Habitualmente, a pesar del extendido ambiente de catalanidad, el 92 % de la población habla francés, el 3,5 % catalán, ambos idiomas un 1 % y el 3,5 % habla otras lenguas.

En cuanto a usos lingüísticos en diversos ámbitos cabe señalar que el 80.5 % de los nacidos en el Rosellón hablan únicamente francés en el ámbito familiar en contraposición con un 17,3 % en el que el catalán está presente. Además, el ámbito del uso del catalán se reduce cada vez más en las nuevas generaciones y en los inmigrantes. solo un 6.3 % de los estudiantes del Rosellón hablan en catalán entre ellos y un 0.5 % lo hace cuando va al médico. Sin embargo, la conciencia lingüística no ha disminuido y un 62.9 % de los habitantes del Rosellón cree que los niños deberían aprender catalán.

Asimismo, según las estadísticas oficiales, el 65,3 % de la población entiende el catalán, un 31,7 % lo sabe leer y un 10,6 % lo sabe escribir. Estos resultados se deben evaluar paralelamente con los deseos en relación a la lengua catalana. Así, un 57,9 % de la población querrían hablar catalán y un 62,9 % desearía que sus hijos aprendan catalán.

La ciudad de Alguer (Cerdeña, Italia) tiene una población de 43 831 habitantes (2009). La población de la ciudad fue sustituida por colonos catalanes de las comarcas del Penedès y del Camp de Tarragona tras un levantamiento popular contra el rey Pedro el Ceremonioso. A finales de 1354, la población quedó muy reducida por el hambre, después de medio año de asedio, y los resistentes alguereses fueron expulsados o esclavizados.

Por eso hasta hace relativamente poco la lengua mayoritaria de la ciudad era el catalán, en su variedad algueresa. Desde el fin de la Segunda Guerra Mundial, sin embargo, la inmigración de gente que habla sardo y la escuela, la televisión y los periódicos de habla italiana han hecho que menos familias lo hayan transmitido a los hijos. En 2004 los usos lingüísticos de la población de Alguer eran los siguientes:

Hasta hace relativamente poco, la mayoría de los habitantes de la zona hablaban alguerés, una variedad dialectal del catalán con influencias del sardo y el italiano. El catalán fue reemplazado por el castellano como lengua oficial durante el siglo XVII, y, en el siglo XVIII, por el italiano.

En 1990 un 60 % de la población local aún entendía el alguerés hablado aunque, desde hace un tiempo, pocas familias lo han transmitido a los hijos. Aun así, la mayoría de los alguereses de más de 30 años lo saben hablar y diferentes entidades promueven la lengua y la cultura, como por ejemplo Òmnium Cultural, el Centre María Montessori y la Obra Cultural de l'Alguer.

Los últimos datos sociolingüísticos de la Generalidad de Cataluña (2004) reflejan que para el 80.7 % de la población de Alguer la lengua vernácula es el italiano, la primera lengua del 59.8 % de la población y la habitual del 83.1 %. El catalán es la primera lengua para el 22.4 % de la población pero es menos de un 15 % quien la tiene como lengua habitual o la considera propia. La tercera lengua, el sardo, muestra un uso más bajo.

El Estado italiano, en virtud de la Norma en materia de tutela de las minorías lingüísticas históricas, prevé el uso de lenguas como el catalán en la administración pública, en el sistema educativo así como la puesta en marcha de trasmisiones radiotelevisivas por parte de la RAI siempre que el estatuto de lengua sujeta a tutela sea solicitado al consejo provincial por municipios en los que lo solicite el quince por ciento de la población. Anteriormente, el Consejo Regional de Cerdeña había reconocido la igualdad en dignidad de la lengua sarda con la italiana en toda la isla, así como con otras lenguas de ámbito más reducido, entre las que cita al catalán, en la ciudad de Alguer. La ciudad, por su parte, promulga su tutela y normalización en sus estatutos.

Debido a su origen catalán, los alguereses denominan a su ciudad Barceloneta, y existen vínculos culturales, fomentados por la Generalidad de Cataluña dentro de su programa de inversión en la extensión de las lengua y cultura catalanas por el mundo, entre Cataluña y Alguer. Entre sus tradiciones vivas destaca el "Cant de Sibil·la", que se canta en Nochebuena (como sucede en Mallorca).

En los últimos años ha habido un resurgimiento de la música cantada en la lengua local. Entre los más renombrados protagonistas de esta nueva ola destacan artistas como la cantante Franca Masu.

Al igual que las demás lenguas románicas de la Península, el catalán es notable por su uniformidad y las variantes dialectales no son demasiado divergentes ni comprometen la comprensión mutua. La división dialectal usada actualmente es la que Manuel Milá y Fontanals propuso ya en el año 1861: el bloque dialectal oriental (que incluye los dialectos central, insulares y de Francia) y el bloque dialectal occidental (que incluye el valenciano y el noroccidental). Pero incluso entre estos grandes grupos la diferencia es pequeña, y las discrepancias afectan más bien a la fonética (las vocales no acentuadas), que por tanto no se reflejan en la escritura, y a pequeñas variantes morfológicas y léxicas.

Los bloques dialectales no se pueden delimitar con exactitud porque entre uno y otro siempre hay una franja de transición, más o menos amplia (excepto en los insulares, obviamente). Además, ningún bloque es del todo uniforme: cualquiera de los que hay se pueden dividir en varios dialectos. Ateniéndose a ello, la lengua catalana se puede dividir en dos bloques dialectales y en subdialectos:

Existen dos estándares principales para la lengua catalana; el regulado por el Institut d'Estudis Catalans, el estándar general, que tiene como fundamento la ortografía establecida por Pompeu Fabra pero con las características del catalán central más aproximado al de Barcelona, no influenciado por el castellano, y el regulado por la Acadèmia Valenciana de la Llengua, estándar utilizado en la Comunidad Valenciana y el Carche, que parte de la tradición lexicográfica, literaria, y la realidad lingüística genuina valenciana, así como la normativización consolidada, a partir de las llamadas Normas de Castellón. 

El estándar del IEC, aparte de tener como base las características del catalán central, toma también características de otros dialectos considerándolos como estándar. Aun así, la diferencia más notable de ambos estándares es la acentuación de muchas "e" tónicas, por ejemplo: "francès" o "anglès" (IEC) / "francés" o "anglés" (AVL), "cafè" (IEC) / "café" (AVL), "conèixer" (IEC) / "conéixer", "comprèn" (IEC) / "comprén" (AVL) (inglés, francés, café, conocer, comprende). Eso es debido a la diferente pronunciación de algunas "e" tónicas, especialmente las Ē ("e" largas) y las Ǐ ("i" breves) tónicas del latín, en ambos bloques del catalán (en el bloque oriental se pronuncian y en el occidental se pronuncian [e]). A pesar de esto, el estándar de la AVL mantiene el acento abierto "è", sin pronunciarse abierto en el bloque occidental, en algunas palabras como son: "què", "València", "èter", "sèsam", "sèrie" y "època".

También hay otras divergencias como el uso de tl en algunas palabras por la AVL en vez de tll como en "ametla" / "ametlla" (almendra), "espatla" / "espatlla" (espalda) o "butla" / "butlla" (bula), el uso de los determinantes demostrativos elididos ("este", "eixe") al igual que los reforzados ("aquest", "aqueix") o el uso de muchas formas verbales comunes en el valenciano, y muchas de ellas extendidas por el bloque occidental, como las formas del subjuntivo o la escritura de los incoativos tanto con -ix- como con -eix- o el uso preferente del morfema -e de la 1ª persona singular del presente de indicativo en la 1ª conjugación (-ar), ya que las otras conjugaciones el morfema es -ø: ""jo compre"", ""jo tem"", ""jo dorm"".

En las Islas Baleares se usa el estándar del IEC adaptado al marco dialectal por la sección filológica de la Universidad de las Islas Baleares, el órgano consultivo del Gobierno balear. De esta manera, por ejemplo, el IEC indica que tanto es correcto escribir ""cantam"" como ""cantem"" (cantamos) y la Universidad determina que la forma preferente en las Islas tiene que ser ""cantam"" incluso en los ámbitos formales. Otra característica del estándar balear es la escritura de la 1ª persona del singular del presente de indicativo, donde no hay desinencia: ""jo cant"" (yo canto), ""jo tem"" (yo temo), ""jo dorm"" (yo duermo).

En Alguer, el IEC ha adaptado el estándar a la variedad algueresa. En este estándar se puede encontrar, entre otras características, el artículo "lo" de uso general, posesivos especiales "la mia", "lo sou / la sua", "lo tou / la tua", etc., uso de la "-v-" en el pretérito imperfecto en todas las conjugaciones: "cantava" (cantaba), "creixiva" (crecía), "llegiva" (leía); uso de muchas palabras de carácter arcaico de uso muy corriente en el alguerés: "manco" por "menys" (menos), "calqui u" por "algú" (alguien), "qual / quala" por "quin / quina" (cual), etc. y adaptaciones de los pronombres clíticos.

Es necesario diferenciar entre los certificados según el territorio en el que se obtienen, puesto que no son exactamente iguales, aunque son equivalentes.



Como en todas las lenguas romances, el cambio del latín vulgar al catalán fue gradual y no es posible determinar en qué momento se inicia su historia. Según Coromines, los cambios más radicales debieron producirse en los siglos VII y VIII, pero es difícil saberlo con precisión porque los textos se escribían exclusivamente en un latín artificioso, ajeno a la lengua de uso. Ya en el siglo IX y sobre todo en los siglos X y XI, aparecen palabras e incluso frases enteras intercaladas en algo que ya se puede denominar catalán, y documentos breves como el juramento feudal de 1028 o los "Greuges de Caboet" de 1080-1090, totalmente en catalán. De la primera mitad del siglo XII data la traducción del "Forum Iudicum", un fragmento de la cual se conserva en la biblioteca de la Abadía de Montserrat y que ya presenta características lingüísticas más modernas. Desde 1150 hay ya numerosos documentos escritos y hacia finales del siglo XII aparece el primer texto conocido de carácter literario, las "Homilías de Organyà", una colección de sermones. 

El catalán medieval de esta etapa, presenta muchas similitudes con la lengua occitana, con la que forma un continuo dialectal que se irá diferenciando con el tiempo hasta formar lenguas claramente diferenciadas, ya en el siglo XIII. El primer texto impreso en catalán, las "Obres e trobes en lahors de la Verge Maria", se publicó en 1474 en Valencia.

El catalán se formó en comunidades que poblaban ambos lados de los Pirineos (condados del Rosellón, Ampurias, Besalú, la Cerdaña, Urgell, Pallars y Ribagorza) y se extendió hacia el sur durante la Reconquista en varias fases: Barcelona y Tarragona, Lérida y Tortosa, el antiguo Reino de Valencia, las Islas Baleares y Alguer.

En cuanto al catalán como lengua extranjera, aunque no es una lengua muy difundida, cuenta con una larga tradición que se remonta a la Edad Media, a causa de la expansión medieval de la Corona aragonesa, y en su momento dejó huella especialmente en la Península itálica y en el vocabulario náutico mediterráneo. Actualmente, se enseña en varias universidades tanto en Europa como en los EE.UU. e Hispanoamérica, así como en numerosos centros catalanes de todo el mundo.

En todo el Mediterráneo, particularmente en el sur de Italia y las islas de mar Tirreno existen lenguas y dialectos que han sido influidos por la lengua catalana entre ellos están:

Asimismo, la influencia del catalán se dejó sentir en el suroeste andaluz debido a la emigración catalana a esas tierras, con registros aún conservados en la lengua viva del siglo XXI.

Con la democracia se recuperó la lengua catalana en el ámbito educativo. Sin embargo, la introducción del catalán en las aulas fue muy desigual según el territorio. Así, mientras que en Cataluña y en Baleares se ha adoptado un modelo lingüístico según el cual el catalán es la lengua vehicular principal, en la Comunidad Valenciana se ha seguido un modelo en el cual coexisten como vehiculares tanto el castellano como el valenciano.

En los territorios de lengua catalana existen diferentes medios de comunicación en catalán, los cuales conforman el llamado espacio catalán de comunicación. En el ámbito de la prensa hay que destacar la edición en catalán de La Vanguardia y El Periódico de Cataluña, los diarios editados solo en catalán El Punt Avui, Ara y L'Esportiu; la numerosa prensa comarcal en catalán (Segre, Regió7, Diari de Girona, El 9 Nou, etc.), las revistas en catalán (El Temps, Sàpiens, etc.) y los numerosos diarios digitales en catalán (VilaWeb, Racó Català, Nació Digital, Ara.cat, 324.cat, Diari de Balears, etc.). En cuanto a la radio destacan Catalunya Ràdio, IB3 Ràdio y RAC 1 como emisoras generalistas, Catalunya Informació como emisora de información 24 horas, y Catalunya Música, Sí Radio (desaparecida), Radio Flaixbac, Flash FM y RAC 105 como emisoras musicales. Finalmente por lo que respecta a la televisión hay que hacer mención de TV3, IB3 Televisió, 8tv y Canal 4 como canales generalistas y El 33, 3/24, Canal Super3, Esport3, Barça TV, RAC 105 TV y Estil 9 como canales temáticos.

El sistema de escritura también presenta ciertas características particulares. El catalán presenta una característica única, la escritura de la "-l-" geminada: "-l·l-" (como en "intel·ligent" –inteligente–). La otra característica es la "ny" [ɲ] (en castellano es equivalente a la "ñ") que se encuentra también en afaan oromo, aragonés, húngaro, quenya, valón, ladino, malayo, indonesio, ewe, gã, ganda, lingala, seSoto, swahili, zhuang y zulú. También cabe comentar la grafía "-ig" (pronunciada [t͡ʃ] si antes hay vocal y [it͡ʃ] si antes hay una consonante) representada en pocas palabras (como "faig" –hago–, "maig" –mayo–, "mig" –medio–, "desig" -deseo-, "puig" –monte–, "raig" –rayo–, "roig" –rojo–, "vaig" –voy–, "veig" –veo–) o la "t+"consonante"" para la representación de consonantes dobles con: "tm", "tn", "tl", "tll", o africación: "tg" y "tj" ("setmana", "cotna", "atles", "bitllet", "jutge", "platja").

El catalán tiene unas características lingüísticas específicas que lo diferencian de las lenguas románicas vecinas y se hicieron propias con la evolución local y peculiar del latín vulgar hasta lo que se conoce como lengua catalana. La lengua más cercana al catalán es el occitano, junto con el que forma el grupo occitanorromance. Se ha discutido si el catalán y el occitano deben considerarse una lengua galorromance o iberorromance, sin que pueda haya podido establecerse consenso. De un modo más conservador, se puede afirmar el catalán y el occitano son elaboraciones distintas de un mismo idioma, de un grupo románico central, el occitanorománico, la cuestión de si este grupo occitano-romance es parte del grupo iberorromance o galorromance o independiente permanece abierta.

Tipológicamente, el catalán, al igual que las otras lenguas romances occidentales, es una lengua flexiva fusionante con orden básico SVO y preferencia por la posición de núcleo sintáctico inicial ("regens ante rectum" o "núcleo-complemento").

Las siguientes características son algunas de las mutaciones del latín que se han ido haciendo durante la consolidación del catalán, aunque también se muestran otras características generales.

El catalán es una lengua flexiva fusionante, con una morfología similar al del resto de lenguas románicas occidentales. Los nombres, adjetivos y muchos determinantes tienen formas diferentes según su número y género gramaticales. Los pronombres personales además tienen formas distintas según el caso gramatical, aunque la distinción de género se reduce a los pronombres sujeto de tercera persona. El verbo tienen un sistema de flexión relativamente complejo, donde cada verbo pertenece a un tipo de conjugación (en catalán los verbos se agrupan usualmente en tres conjugaciones caracterizadas por la terminación del infinitivo). Todas características son compartidas por las lenguas románicas occidentales.

Algunas peculiaridades del catalán son:

El sistema vocálico está formado por 8 sonidos vocálicos o alófonos vocálicos diferentes:

Existen diferencias menores en cómo estos 8 alófonos se agrupan en fonemas. El catalán oriental estándar tiene 7 fonemas vocálicos en oposición , aunque en algunas variedades de Baleares los ocho alófonos anteriores están en oposición fonémica. También existen diferencias de realización alofónica entre los dialectos orientales y occidentales.

En catalán, cualquiera de los sonidos puede aparecer en sílaba tónica.
Sin embargo, en sílaba átona ocurren un buen número de neutralizaciones. En catalán oriental (central, balear, septentrional, alguerés), se dan las siguientes:
Mientras que en catalán occidental (noroccidental, valenciano), el sistema átono presenta menos reducciones presentando un sistema con 5 vocales átonas , en lugar de las 7 que pueden aparecer en sílaba tónica. Nótese que estas neutralizaciones tienen análogos en otras lenguas occitanorromances como el occitano.

A diferencia del castellano, suelen ser descendentes. Las pronunciaciones son según el dialecto central (Barcelona y alrededores). Ejemplos:


Los únicos diptongos ascendentes son aquellos del tipo "gu(a/o), gü(e/i)" y "qu(a/o), qü(e/i)":

Los triptongos se forman a partir de aquellos:

Para formar los hiatos, se añade diéresis sobre la "i" o la "u":


Diferencias en interdialectales:

La evolución típica del sistema vocálico tónico puede apreciarse en esta tabla:

El inventario de fonemas consonánticos del catalán, especificados mediante rasgos fonéticos binarios, se resume en el cuadro siguiente:

Como puede verse, usualmente se requieren diez rasgos fonéticos binarios para definir el inventario consonántico anterior de manera unívoca: [+/- dorsal], [+,- coronal], [+,- velar], [+/- sonorante], [+/- africada], [+/- nasal], [+/- lateral], [+/- vibrante múltiple] y [+/- sonora] (el resto de rasgos usados [+/- palatal] y [+/- labial] es redundante y puede deducirse de las combinaciones de los anteriores). Esta cantidad de rasgos es elevada si se compara con el mínimo número teórico de rasgos "abstractos" necesarios que es 5, ya que 2 = 32 > 23 (fonemas).

Las oclusivas devienen en sordas en posición final.

Las africadas devienen en sordas en posición final, pero la /ts/ y la // finales seguidas de vocal son sonoras (/dz/ y //.


Las fricativas en posición final se pronuncian sordas, pero al final de la palabra las /s/ y seguidas de vocal son sonoras.



Hay dos sonidos vibrantes en catalán.








La mayoría de palabras del catalán proceden del latín, aunque existen también una fracción apreciable de préstamos históricos de otras lenguas como: las lenguas germánicas como el gótico ("Ramon" 'Ramón', "espia" 'espía', "ganivet" 'cuchillo'... y los topónimos acabados en "-reny", como "Gisclareny") y más recientemente el inglés ("bar, web, revòlver..."); otras lenguas románicas como el francés ("brioix, garatge, fitxa..."), el italiano ("piano, macarró, pantà, pilot..."), el occitano ("espasa" 'espada', "beutat", "daurar", "aimia", el sufijo "-aire"...) y el del castellano ("bolero", "lloro", "burro"...")"; el árabe "(alcohol", "sucre", "alcova"...y muchos topónimos como" Benicàssim", "Albocàsser"...")", también el euskera ("esquerra" 'izquierda', "isard" 'gamuza, rebeco, sarrio', "estalviar"... 'ahorrar', y muchos topónimos como "Aran" y "Benavarri"...).




</doc>
<doc id="3302" url="https://es.wikipedia.org/wiki?curid=3302" title="20 de octubre">
20 de octubre

El 20 de octubre es el 293.º (ducentésimo nonagésimo tercer) día del año en el calendario gregoriano y el 294.º en los años bisiestos. Quedan 72 días para finalizar el año.








</doc>
<doc id="3303" url="https://es.wikipedia.org/wiki?curid=3303" title="19 de octubre">
19 de octubre

El 19 de octubre es el 292.º (ducentésimo nonagésimo segundo) día del año en el calendario gregoriano y el 293.º en los años bisiestos. Quedan 73 días para finalizar el año.






























</doc>
<doc id="3304" url="https://es.wikipedia.org/wiki?curid=3304" title="18 de octubre">
18 de octubre

El 18 de octubre es el 291.º (ducentésimo nonagésimo primer) día del año en el calendario gregoriano y el 292.º en los años bisiestos. Quedan 74 días para finalizar el año.








</doc>
<doc id="3305" url="https://es.wikipedia.org/wiki?curid=3305" title="17 de octubre">
17 de octubre

El 17 de octubre es el 290.º (ducentésimo nonagésimo) día del año en el calendario gregoriano y el 291.º en los años bisiestos. Quedan 75 días para finalizar el año.



































</doc>
<doc id="3306" url="https://es.wikipedia.org/wiki?curid=3306" title="16 de octubre">
16 de octubre

El 16 de octubre es el 289.º (ducentésimo octogésimo noveno) día del año en el calendario gregoriano y el 290.º en los años bisiestos. Quedan 76 días para finalizar el año.









</doc>
<doc id="3307" url="https://es.wikipedia.org/wiki?curid=3307" title="15 de octubre">
15 de octubre

El 15 de octubre es el 288.º (ducentésimo octogésimo octavo) día del año en el calendario gregoriano y el 289.º en los años bisiestos. Quedan 77 días para finalizar el año.










</doc>
<doc id="3308" url="https://es.wikipedia.org/wiki?curid=3308" title="Academia Ecuatoriana de la Lengua">
Academia Ecuatoriana de la Lengua

La Academia Ecuatoriana de la Lengua es una institución cultural ecuatoriana. Fue establecida en Quito el 15 de octubre de 1874 y pertenece a la Asociación de Academias de la Lengua Española. Su fin es científico y literario, en pro de la defensa del idioma, su espíritu y su unidad. Entre sus principales objetivos se encuentran la investigación lingüística, la evolución del idioma, la aparición y aceptación de neologismos y la atención a términos en desusos.

La historia se remonta de alguna forma, a la de la Real Academia Española, la cual fue fundada en 1713, por don Juan Manuel Fernández Pacheco con la finalidad de limpiar, fijar y dar esplendor a la lengua española.
A finales de 1870, la Academia Española concedió las respectivas autorizaciones para poder establecer instituciones correspondientes a ella en diferentes países que poseían el habla castellana, y así, el 4 de marzo de 1875, se logró instalar en Quito la Academia Ecuatoriana de la Lengua, que entre sus principales propósitos, se encontraba el de albergar en ella a los grupos intelectuales y literarios de todas las regiones del país.

En 2013, tras el fallecimiento de Renán Flores Jaramillo, Susana Cordero fue nombrada presidente de la institución, la primera mujer en ostentar el cargo.





</doc>
<doc id="3309" url="https://es.wikipedia.org/wiki?curid=3309" title="Quito">
Quito

Quito (oficialmente San Francisco de Quito) es la capital de la República de Ecuador, la más antigua de Sudamérica y de la Provincia de Pichincha, es la segunda con mayor , después de Guayaquil; cuenta con 1 607 734 habitantes (parroquias urbanas) y 2 239 191 habitantes en todo el Distrito Metropolitano. Además es la cabecera cantonal o distrital del Distrito Metropolitano de Quito. 

Está ubicada sobre la hoya de Guayllabamba, en las laderas occidentales del estratovolcán activo Pichincha, en la parte oriental de los Andes y su altitud es de 2850 msnm.
La ciudad está dividida en 32 parroquias urbanas, las cuales se subdividen en barrios.
Quito es el centro político, cultural y financiero de la República del Ecuador, alberga los principales organismos gubernamentales, administrativos, culturales, financieros, comerciales de la Nación, la mayoría de empresas transnacionales que trabajan en Ecuador tienen su matriz en la urbe.

La fecha de su primera fundación es incierta; los registros más antiguos se hallan en la hacienda del Inga alrededor del año 1030 a.C. El Inca Huayna Capac convirtió a Quito en una ciudad importante del norte del Tahuantinsuyo, territorio del imperio Inca, y durante varios lapsos de tiempo se movilizó entre esta y Tomebamba, esta última capital norteña del imperio. Sin embargo, se utiliza la conquista española de la ciudad, el 6 de diciembre de 1534, como su nacimiento y fecha de fundación. La Escuela quiteña es como se ha llamado al conjunto de manifestaciones artísticas y de artistas que se desarrolló en el territorio de la Real Audiencia de Quito. La Escuela Quiteña alcanzó su época de mayor esplendor entre los siglos XVII y XVIII, llegando a adquirir gran prestigio entre las otras colonias americanas e incluso en la corte española de Madrid. El 24 de mayo de 1822 el ejército independista comandado por el mariscal Antonio José de Sucre venció a las fuerzas realistas leales al rey de España quienes estaban bajo las órdenes de Melchor de Aymerich, en la denominada Batalla de Pichincha. Gracias a la victoria de las tropas grancolombinas, se consiguió la liberación de Quito y la independencia de las provincias pertenecientes a la Real Audiencia de Quito. El 13 de mayo de 1830 se crea la República del Ecuador, con Quito como capital tras separarse de la Gran Colombia.

Es la primera ciudad declarada, junto a Cracovia en Polonia, como Patrimonio Cultural de la Humanidad por la Unesco, el 8 de septiembre de 1978. En 2008, Quito fue nombrada sede de la Unión de Naciones Suramericanas (Unasur), siendo así el centro de reuniones oficiales de los países de América del Sur. En 2012, Quito ha sido evaluada dentro del concepto de ciudades mundiales o globales como una ciudad beta-, según el estudio de GaWC.

En la actualidad, el origen del nombre de la ciudad es debatido, ya que existen diferentes explicaciones sobre su creación.
Según la historiadora Anne Collin, este proviene de una mitológica tórtola de una antigua leyenda aborigen conocida en lengua quichua como quitus. La villa no cambiaría su nombre con la llegada del pueblo Inca y sufriría una pequeña modificación de la letra "u", por la "o" con los conquistadores españoles.

Otro de los posibles orígenes topónimos lo sugirió el historiador González Suárez quien afirmó que la ciudad tomó el nombre de su fundador, Quitumbe del monte "Umbe" del "herri"(pueblo) de Laukiniz : 

La teoría más aceptada relata que su denominación proviene de las lenguas tsa’fiki y cha’fiki, "Qui" -de quitsa-, que quiere decir mitad y "To" o "Tu", cuyo significado es tierra. Así la palabra Quito o Quitu, se traduce como: “Tierra en la mitad del Mundo”.

Aportes desde los pueblos y nacionalidades a la toponimia ¨kitu¨:

Desde el Ñawpay Rimay (oralidad vivencial) de los pueblos y nacionalidades en Ecuador, el asentamiento Quito obedece a la toponimia originaria como ¨kitukara¨ pues así se identifican los indígenas del lugar, reconocida por el CODENPE (Consejo de Desarrollo de las nacionalidades y pueblos del "Ecuador)" y la CONAIE, siendo uno de los 18 pueblos kichwas en Ecuador, por consiguiente las lenguas vivas que hasta la fecha están vigentes prevalecen sobre las interpretaciones de la historia oficial puesto que no sustentan la veracidad en razón de las teorías. Dicho de esta manera ¨kitukara¨ es un fonema compuesto de tres sufijos que demuestran la fusión de aquellos tiempos:

KI fonema del mundo lingüístico Chibcha, es decir, madre de las lenguas Tsa´fiki, Chapala y Awapi, su aproximación al castellano es: Tierra de la mitad, tierra equilabrada, tierra de dos caras.
TU fonema del mundo lingüístico Runashimi (kichwa, quechua o queshua), su aproximación al castellano es: Vector energético, rayo vital.
KARA fonema del mundo lingüístico Runashimi, su aproximación al castellano es: Piel, cáscara, pellejo, cuero, vaina.

Analizando las traducciones ¨kitu¨ se aproxima a Rayo vital en la tierra equilibrada y el fonema ¨kitukara¨ se aproxima en la Piel vital de la tierra equilibrada. Hay que considerar que los españoles en tiempos de conquista venían del paradigma del oscurantismo cristiano, los mismos que se encargaban de divinizar o satanizar bajo el concepto de Dios, cuyo enfoque derivó en que ¨kitu¨ se traduzca como DIOS. Se puede comprobar que en los pueblos de Abya Yala en ninguna lengua originaria de América existe traducción de Dios, y para el caso del kichwa con mucha razón puesto que el paradigma de Dios es propio de occidente.

Las investigaciones arqueológicas señalan que en el sector del Inga, una hacienda ubicada cerca del monte Ilaló, alrededor del año 1030 a.c. vivieron pueblos nómadas que se dedicaban a la caza, la pesca y la recolección de alimentos. Robert Bell, quien fue el primer científico que estudió esa zona, determinó que la roca obsidiana usada para la creación de herramientas databa del 7.080 a.C., posteriormente se estableció su edad en más de 12 milenios de antigüedad, como se lo reconoce actualmente. Debido a ello, por ahora, es considerado como el lugar con el asentamiento humano más antiguo del país. Este primer emplazamiento pertenece al período paleolítico ecuatoriano, el cual se caracterizó por el amplio uso de los habitantes de aquel material ígneo del que se han encontrado cerca de 80 mil piezas.

En el año 800 a.C. durante el período de desarrollo regional, la civilización de los cotocollaos (descubiertos por el Padre Porras en 1973) se estableció entre las montañas Casitagua y Pichincha. Esa sociedad fue sedentaria, basó su desarrollo tanto en la agricultura por el cultivo del maíz, la quinua, el chocho, la calabaza; por la cacería, siendo muy importante la presencia del venado, el conejo, y los camélidos; así como también por el comercio, el cual llegó a lugares distantes para la época como la región costera del país. Debido a ello la cerámica de los cotocollao compartió semejanzas, tanto en su decoración como en estilo, con las culturas Chorrera y Machalilla. Aproximadamente en el año 500 a.C. este pueblo desapareció a causa de las erupciones del volcán Pululahua.

El yacimiento arqueológico de Rumipamba (1500 a.C. hasta el 900 d.C.), una aldea y necrópolis ubicada en la parroquia homónima de la ciudad la cual fue abandonada en varias ocasiones debido a las erupciones de los volcanes Pululahua y Guagua Pichincha, es uno de los pocos vestigios remanentes que pertenecen a la cultura Quitu. Durante este período ( Integración) se da uno de los capítulos más controversiales de la historia de la urbe, El Reino de Quito, mencionado por el Padre jesuita Juan de Velasco en su «"Historia del Reyno de Quito"» publicada en el siglo XVIII.

En ella se habla de un supuesto Reino (palabra que se utilizaba en aquella época para definir al país de Quito por los españoles) conformado por las etnias Quitu y Caras, las cuales integraron un extenso territorio en la sierra central y norteña ecuatoriana. La historia fue negada aproximadamente un siglo atrás por el historiador Gonzales Suárez. Pese a las controversias, se sabe que una importante confederación como los Quitu, se asentaron en las laderas del Pichincha y habitaron la zona antes de la llegada de los incas.

La conquista Inca de esta región fue iniciada en el siglo XV por Túpac Inca Yupanqui, hijo de Pachacútec el fundador del Imperio incaico. Su hijo, Huayna Cápac, fue el primer soberano nacido en el actual territorio ecuatoriano y el que estableció su residencia en tierras cañaris en Tomebamba, la actual ciudad de Cuenca. Conquistaría el territorio de los Quitus, y luego mediante cruentas guerras libradas en los territorios caranguis (actualmente Pichincha e Imbabura) lograría su victoria definitiva, la batalla definitiva se dio en la laguna de Yaguarcocha («lago de sangre», en quichua). La importancia de la ciudad fue estratégica tanto en lo militar como en lo económico, así pues desde Quito el emperador Huayna Capac conquistó a los Caranquis. Algunos estudiosos apuntan a que Atahualpa nació en Caranqui aproximadamente en el año 1500 d.C.

Cuando llegaron los españoles al Tahuantinsuyo, el imperio inca estaba sumergido en una guerra civil provocada por la pugna de poder entre Atahualpa y su hermano Huáscar. El primero defendía su hegemonía desde Quito, el segundo desde Cuzco. Atahualpa y su ejército vencieron a Huáscar en las cercanías del río Apurimac. Le dio a elegir a Huáscar: vivir y quedarse con el imperio de Cuzco, o morir. Huáscar, indignado por la derrota, acepto la muerte. Pero en el año de 1533, luego de pacificar el imperio, Atahualpa aceptó una reunión con Francisco Pizarro, en la cual fue capturado y días después asesinado por orden del español.

La conquista de los Andes septentrionales fue motivada principalmente por el rumor de que en Quito se encontraba el tesoro de Atahualpa. Se formaron dos expediciones, la de Pedro de Alvarado, desde Guatemala, y la de Sebastián de Belalcázar procedente del sur. Fue este último el que consiguió llegar primero y quien, el 6 de diciembre de 1534, fundó la ciudad de San Francisco de Quito en las faldas orientales del volcán Pichincha. La ciudad se encontraba sobre cenizas, ya que días antes había sido incendiada por el general inca Rumiñahui con el objeto de que los españoles no encontraran nada al llegar. En agosto, la villa había sido fundada por Diego de Almagro cerca de la ciudad de Riobamba con el nombre de Santiago de Quito.

La urbe fue establecida con aproximadamente doscientos habitantes. Inmediatamente se señalaron los límites, se estableció el cabildo, se repartieron solares y se delimitaron áreas comunales. La fundación de la ciudad en este sitio parece haber respondido más que nada a razones estratégicas. A pesar de su topografía accidentada, su ubicación en una meseta presentaba ventajas sobre los valles aledaños, más propicios para el desarrollo urbano. Este último factor fue también el que primó en la determinación del lugar por parte de los pueblos originarios. En el ámbito arquitectónico, empezaron a construirse los primeros monumentos de la villa, destacándose el inicio de la construcción de la iglesia de San Francisco, en 1536.
Aproximadamente, siete años después de la fundación de Quito, Francisco de Orellana partiendo desde esa ciudad junto a numerosos indígenas, en busca del tesoro de Atahualpa, descubrió el río Amazonas el 12 de febrero de 1542. Debido a este suceso histórico, se creó la célebre frase: "Es Gloria de Quito el Descubrimiento del Río Amazonas". El 8 de enero de 1545, el Papa Alejandro Farnesio (Paulo III) fundó la Diócesis de San Francisco de Quito con la finalidad de mejorar el proceso de evangelización a los indígenas, que era difícil por la extensión del territorio.

Debido a los problemas de comunicación y transporte, así como también por la explosión demográfica, el cabildo de la ciudad solicitó al rey Felipe II la creación de la Audiencia y Presidencia de Quito. El 29 de agosto de 1563, él firmó la cédula real que dio nacimiento a esta. La jurisdicción estableció sus límites geográficos, que abarcaban una superficie cinco veces mayor que la de la actual República del Ecuador.
El Virrey Pedro Mesía de la Cerda otorgó el título de Presidente interino de Quito con fecha de 17 de mayo de 1766 a Juan Antonio Zelaya y Vergara, que durante este período ejerció sus responsabilidades en calidad de Duque de Quito como comandante general militar y político de dicha provincia

Enriquecida por la explotación minera y la producción textil, pudo construir templos barrocos y neomudéjares adaptados con originalidad al ambiente local y los ornamentó con gran profusión de pinturas y tallas, de innegable valor didáctico religioso. Fue la época de la afamada Escuela Quiteña, obra del mestizaje indio y español.

Los geodésicos franceses del sistema decimal introdujeron en Quito el espíritu racionalista moderno y usaron la magnífica biblioteca de la Universidad Jesuita de San Gregorio. Quito alimentó la extraordinaria empresa de las misiones de Jaén y Mainas. En Quito nació y vivió Mariana de Jesús, santa y patriota. De esta ciudad salió el más ilustre de los precursores de la independencia americana, el mestizo Xavier Chusig quien cambió su nombre a Eugenio de Santa Cruz y Espejo para evitar la discriminación. Espejo fue el fundador del primer periódico de Quito. También hay otras historias como la de Manuela Sáenz, la primera mujer enrolada al ejército bolivariano quien se convirtió en la fiel compañera y novia del libertador Simón Bolívar.

Algunos de los sucesos internacionales como la Declaración de Independencia de los Estados Unidos en 1776 de Gran Bretaña y la Revolución francesa de 1789, sirvieron de ejemplo a los criollos al mostrarles que un sistema de gobierno autónomo o incluso independiente era posible. Las influencias de varios acontecimientos locales tales como la visita de los geodésicos franceses quienes impulsaron las ideas de la ilustración en la urbe, el alto índice de empobrecimiento de la Audiencia y los crecientes sentimientos nacionalistas, estimulados por el interés de los criollos de todo el continente por obtener el poder, fueron también algunas de las causas principales, que motivaron el inicio del proceso revolucionario que dio fin al colonialismo español en la ciudad.

Durante la cena de Navidad, el 25 de diciembre de 1808 en la hacienda Chillo Compañía, propiedad de Juan Pío Montúfar y Larrea II Marqués de Selva Alegre, se celebró una reunión conocida como «La conspiración de Chillo» o «La Conjura Navideña» que discutió el establecimiento de una Junta Autónoma que se encargaría de gobernar la Presidencia de Quito. A ella asistieron Juan de Dios Morales, José Riofrío, Juan Pablo Arenas, Manuel Quiroga, Nicolás de La Peña, Francisco Javier de Ascázubi y el capitán Juan de Salinas y Zenitagoya.

Meses después el complot fue descubierto por el entonces presidente de la Real Audiencia de Quito Manuel Urríez, conde Ruiz de Castilla, debido a que Salinas comentó a Andrés Torresano, sacerdote del convento de La Mercéd, el tema de la reunión. Fue apresado el primero de marzo al igual que sus compañeros Juan Pío Montufar, el día cinco y Juan de Dios Morales el seis. Pocos días después todos fueron liberados debido a que las pruebas indagatorias fueron sustraídas.

El 8 de agosto se reunieron en el hogar del Dr. Francisco Javier de Ascázubi, donde se tomó la decisión de integrar la junta el día 10. El 9 de agosto, este grupo de ilustrados criollos, se reunió nuevamente en la residencia de Manuela Cañizares. El 10 de agosto de 1809, se firmó el acta que cesó en sus funciones al entonces presidente de la Real Audiencia de Quito, conde Ruiz de Castilla, e instauraron en la ciudad la Primera Junta Autónoma de Gobierno, con autoridades que respetaban la autoridad del rey español.

El rechazo de adhesión a la junta de Guayaquil, Cuenca, Popayán, Pasto, Barbacoa y Panamá así como el débil interés que poseían algunos de sus miembros, entre ellos el presidente, Juan Pío Montufar, ocasionó que el 5 de octubre se declarase una contrarrevolución y que el 24 del mismo mes se firmara la capitulación. Después de estos hechos, cientos de personas entre criollos y rebeldes fueron encarceladas en el Cuartel de Quito, lugar en el que entre el 2 y el 10 de agosto de 1810 fueron asesinados alrededor de 300 de ellos, lo que significó en aquel momento la muerte del 1 por ciento de la población de la urbe. Una masacre de iguales características hoy, representaría cerca de 17 mil víctimas. El poder vuelve a manos del Conde Ruiz de Castilla. Los virreyes de Lima y Bogotá envían tropas para sitiar la ciudad. En 1812 llega como Comisionado Regio de España Carlos Montúfar, hijo del Marqués de Selva Alegre para pacificar a los sublevados, pero lo que ocurrió fue que éste se unió a la lucha que se estaba disputando, esto ocasionó que en 1815 pierda la vida.

El 9 de octubre de 1820, Guayaquil declaró su independencia de España, marcando el inicio del proceso de emancipación. El 24 de mayo de 1822 en la denominada Batalla de Pichincha, la División Protectora de Quito creado en Guayaquil y bajo el mando del Mariscal Antonio José de Sucre ingresó a la ciudad desde Chillogallo, al sur. Luego en la madrugada tuvieron que subir las laderas del Volcán PIchincha, la compañía estuvo formada de 1701 hombres en enero de 1822, luego de los cuales las personas reclutadas eran de Guayaquil y región de la Sierra terminó el ejército en la madrugada del 24 de mayo siendo 2.900, con 200 colombianos de la Alta Magdalena en la vanguardia y británicos de Albión en la protección del tren de municiones. En la madrugada hubo una fuerte llovizna en la subida por las laderas del volcán y a pesar de los esfuerzos de los soldados fue difícil la subida por la razón que los senderos se convirtieron en ciénagas, luego bordearon el flanco occidental de las faldas del Pichincha, para rodear a las fuerzas realistas a cargo de Melchor de Aymerich. Se dio un disputado combate, en el cual el ejército grancolombino consiguió la liberación de la ciudad y la independencia de las provincias pertenecientes a la Real Audiencia. El 25 de mayo Melchor de Aymerich capituló y la antigua Presidencia de Quito pasó a formar parte de la Gran Colombia.

Luego de la Batalla de Pichincha, en junio de 1822 llega el Libertador Simón Bolívar para anexionar los territorios de la antigua audiencia a la República de la Gran Colombia, conformada entonces por los actuales Colombia, Ecuador, Panamá y Venezuela, con capital en la ciudad de Santa Fe de Bogotá. La Real Audiencia se transforma en Departamento del Sur o Presidencia de Quito. El 25 de junio de 1824 se funda la provincia de Pichincha, teniendo a Quito como capital. El 18 de marzo de 1826 se inaugura la Universidad Central del Ecuador.

El 13 de mayo de 1830 se crea la República del Ecuador con Quito como capital, tras separarse de la Gran Colombia. El general venezolano Juan José Flores asume el cargo de primer Presidente. El 27 de agosto de 1869 se funda la Escuela Politécnica Nacional otra de las grandes universidades del Ecuador.

Otros hechos históricos que describen a Quito en la historia de este país son: el asesinato del Presidente del Ecuador Gabriel García Moreno el 6 de agosto de 1875,; el asesinato del Presidente Eloy Alfaro cuyo cuerpo inerte fue arrastrado por las calles quiteñas e incinerado posteriormente en el Parque El Ejido el 28 de enero de 1912; la Revolución Juliana de 1925 para rescatar el estado de manos de la plutocracia bancaria; entre otros.

El 25 de junio de 1908 llegó por primera vez el tren de vapor a la Estación de Chimbacalle ubicada al sur de Quito. La obra de construcción del Ferrocarril Transandino entre Guayaquil y Quito había sido iniciada por el Presidente Gabriel García Moreno, y fue terminada en época del Presidente Eloy Alfaro. Un clavo de oro colocado en el último riel de la mencionada estación por América Alfaro, la hija del Presidente Alfaro, selló la obra de infraestructura más grande del Ecuador en aquella época. La llegada del ferrocarril de vapor a la ciudad, produjo la necesidad de crear un medio de transporte urbano que operase entre la Estación de Chimbacalle encima del cerro al lado sur de la ciudad, y el centro comercial en el otro lado del Río Machángara. La Quito Tramways Company fue organizada en 1910 en Wilmington (Delaware) - Estados Unidos, y fue controlada por la Ecuadorian Corporation Ltda. de Londres. La QTC empezó la construcción de una línea de tranvías eléctricos en 1911 y ordenó cuatro carros de dos ejes a la J. G. Brill en Philadelphia el 17 de febrero de 1914. La nueva línea, entre la estación del ferrocarril y el centro de la ciudad, fue inaugurada el 8 de octubre de 1914. La QTC mandó hacer a la Brill dos carros de cuatro ejes en 1915 y dos más de dos ejes en el año siguiente. La trocha de las líneas de tranvía de Quito, como del ferrocarril de vapor, era de 1.067 mm (42 pulgadas).

Durante 34 años la QTC operó los ocho mismos tranvías en dos servicios: de la estación Chimbacalle al Cementerio San Diego, y de Chimbacalle hasta la Avenida Colón, cerca al Palacio de La Circasiana. El depósito de tranvías se ubicó en la Avenida 18 de Septiembre y Jorge Washington. En 1921 una empresa ecuatoriana, Compañía Nacional de Tranvías, construyó une línea de tranvías en las Avenidas 10 de Agosto y la Prensa entre la Avenida Colón y la aldea de Cotocollao. Ya que la QTC poseía los derechos exclusivos de tracción eléctrica en la ciudad, los vehículos de la CNT tenían que ser accionados por motores de gasolina. La CNT importó los chasis y la parte mecánica de sus carros de la Allgemeine Elektricitäts-Gesellschaft (AEG) en Alemania, pero montó las carrocerías en Ecuador. La línea de Cotocollao, con trocha también de 1.067 mm (42 pulgadas), abrió el 22 de junio de 1923. Alrededor del 1926 los inversores ecuatorianos reorganizaron la CNT y adquirió la QTC. Los nuevos dueños cerraron la línea de gasolina de Cotocollao en 1928 hasta 1935 y las dos líneas de tranvía eléctrico aproximadamente en 1948.

En la década de 1930, las clases altas del centro de la ciudad se desplazaron al norte. Surgieron barrios residenciales dentro del esquema de "ciudad jardín". Los espacios del centro fueron ocupados por inmigrantes de las provincias vecinas. La parte antigua de la ciudad pudo por consiguiente conservar su traza original y su arquitectura colonial enriquecida con los nuevos aportes de los siglos XIX y XX. Hacia la mitad del siglo XX, el espacio urbano estaba ya socialmente estratificado.

El 5 de julio de 1941, estalla un conflicto con Perú lo que produce la Guerra peruano-ecuatoriana, durante los días de guerra con el Perú los ferrocarriles se dirigían al sur del país llevando jóvenes soldados voluntarios para hacer frente al enemigo peruano.

El 6 de agosto de 1960 se inaugura el Aeropuerto Internacional Mariscal Sucre. El 28 de marzo de 1976 en la loma de El Panecillo, el décimo primer arzobispo de Quito Pablo Muñoz Vega, inauguró la Virgen del Panecillo. Esta es una estatua hecha de aluminio, copiada a partir de un original de Bernardo de Legarda. El 18 de septiembre de 1978, Quito fue declarada como el Primer Patrimonio Cultural de la Humanidad por la UNESCO, con el objetivo de conservar sus conventos coloniales, iglesias y el centro histórico en general. Desde hace algunos años, el Municipio de Quito ha emprendido en un plan de salvamento arquitectónico y social del Centro Histórico.

A partir de la década del 70, Quito se modernizó gracias al boom petrolero en Ecuador. Se convirtió en la capital petrolera y en el segundo centro bancario y financiero del país. Su modernidad se aprecia en la arquitectura del sector norte de la ciudad. Uno de los exponentes de este desarrollo es la Torre CFN, que con sus 23 pisos es el edificio más alto de la ciudad.

La extensión de la ciudad hacia el norte y el sur comenzó durante los años 1980, cuando la principal área turística ubicada en el centro norte de la ciudad (Quito moderno) comenzó a crecer. Actualmente es la ciudad más desarrollada del Ecuador.

El 5 de marzo de 1987, se produjo un terremoto de, aproximadamente, magnitud 7 en la escala sismológica de Richter y cuyo epicentro se localizó a 80 km de Quito. El temblor causó daños en varias edificaciones de la ciudad. El 27 de diciembre de 1993, se promulga la Ley de Régimen para el Distrito Metropolitano de Quito. El 17 de diciembre de 1995, el Municipio de Quito inaugura la primera línea de trolebuses en la ciudad y en el Ecuador con el nombre de Trolebús de Quito o Trole.

A partir del año 2000, la ciudad había alcanzado 1.397,698 habitantes. Desde el año 2002 se inició la recuperación del centro histórico y del casco colonial, y se remodeló el Aeropuerto Internacional Mariscal Sucre al Norte de la Capital del País hasta el cierre de aquel antiguo aeropuerto al convertir en el Parque Bicentenario en el año 2013.

Los días 29 y 30 de noviembre de 2002, se llevaron a cabo los actos de inauguración de La Capilla del Hombre, un museo que contiene las mejores obras del maestro Oswaldo Guayasamín quien fue un destacado artista y sin duda alguna el mejor pintor ecuatoriano de la época moderna.

Entre 2003 y 2004 se construyó la línea de buses ecológicos MetrobusQ que atraviesa la ciudad de norte a sur, también se ampliaron las avenidas y se construyeron pasos deprimidos y reformas geométricas con la finalidad de dar mayor fluidez al tránsito. Para 2005 se terminó la recuperación del sector de La Mariscal, antiguamente una zona roja, creando una gran cantidad de restaurantes, calles peatonales, piletas, cafés, bares y lugares especialmente adaptados para la presentación de conciertos musicales. El 2 de julio de 2005, se inauguró una moderna línea de teleféricos en Cruz Loma con el nombre de "TelefériQo" (derivado de teleférico + Quito). La capital del país.

A través del FONSAL (Fondo de Salvamento) se han realizado proyectos de viveros, forestación, reforestación, puentes, túneles, nuevas autopistas, campañas contra incendios, construcción de escuelas públicas, restauración del centro histórico, construcción y restauración de museos y parques. Actualmente el Centro Histórico de Quito concentra la mayor parte de los visitantes a la ciudad, siendo este uno de los más interesantes atractivos de la ciudad, sino el más. Quito comparte el título de patrimonio cultural de la humanidad dentro del Ecuador junto con Cuenca, las cuales resultan ser las dos ciudades más importantes de la región sierra.

En 2010 arrancan los estudios para la construcción de su tren metropolitano (Metro).

Quito ha acumulado varios títulos durante su historia; entre ellos se destacan “Luz de América,” por ser la primera ciudad en América hispana en buscar un gobierno autónomo, “Patrimonio Cultural de la Humanidad”, primera ciudad en ser nombrada por la UNESCO en 1978 y “Capital Iberoamericana de la Cultura en el año 2004”, designada por la Unión de Ciudades Capitales Iberoamericanas (UCCI). En cada una de estas trascendentales designaciones, Quito fue un referente mundial de belleza arquitectónica, histórica, cultural y artística. Quito actualmente es la Capital Americana de la Cultura 2011

El 20 de febrero de 2013 se inaugura el Aeropuerto Internacional Mariscal Sucre situado en Tababela.

La ciudad y el distrito se encuentran ubicados, principalmente sobre el valle de Quito, que forma parte de la Hoya de Guayllabamba, la cual está emplazada en las faldas orientales del estratovolcán activo Pichincha, en la Cordillera Occidental de los Andes septentrionales de Ecuador, a una altitud promedio de 2850 msnm. La urbe está delimitada por el volcán Casitagua por el norte, la falla geológica EC-31 (conocida como "Falla de Quito-Ilumbisi" o "Falla de Quito") por el este, las faldas orientales del Pichincha por el oeste y por el Volcán Atacazo por el sur. Sus dimensiones aproximadas son de 50 km de longitud en dirección sur-norte y 8 km de ancho de este a oeste.

Centenas de millones de años atrás, durante el período Paleozoico, se desarrollaron las bases de lo que serían Los Andes cuando empezó la subducción de la Placa de Nazca bajo la Placa Sudamericana, la que pertenecía al antiguo continente de Gondwana. La mayor actividad telúrica se registró durante el período cuaternario en la época del pleistoceno, la cual formó el paisaje accidentado de la ciudad. En el plioceno, se presentaron varios eventos de considerable importancia en el país y el continente, sin embargo, la ciudad no fue influenciada mayoritariamente por estos. Pese a ello, existieron algunos acontecimientos durante ese período como se evidencia en algunas zonas del Este de la urbe.

Más adelante la morfología del distrito continuó transformándose, los períodos glaciares cubrieron la región con hielo, el cual se derritió progresivamente debido al cambio climático natural que experimentó el planeta después de la última glaciación, así como también a una serie de erupciones volcánicas que provocaron el derretimiento glaciar. Esto formó varias ciénagas y lagunas en todo el territorio, algunas de las cuales eran alimentadas por ríos formados por el deshielo de los picos nevados. Se ha registrado que los habitantes de aquel período establecieron sus hogares cerca de uno de los lagos más grandes del distrito llamado iñaquito, que desapareció debido al drenado que realizaron los españoles a su llegada, con el fin de ocupar los terrenos para usarlos como zonas comunes.

Actualmente, Quito se ubica en el valle cuyo terreno irregular tiene una altitud que oscila entre los 2800 msnm en los lugares llanos y los 3100 msnm en los barrios más elevados. Algunas estribaciones desprendidas de la cordillera de los Andes han formado un paisaje enclaustrado, dividido en su parte central por el cerro de El Panecillo (3035 m.s.n.m). Al este por las lomas de Puengasí, Guanguiltagua e Itchimbía. Así como también, la principal cadena montañosa perteneciente al volcán Pichincha, el que se encuentra emplazado en la Cordillera de los Andes, encierra a la urbe hacia el oeste con sus tres diferentes elevaciones, Guagua Pichincha (4794 m.s.n.m), Rucu Pichincha (4.698 msnm) y Cóndor Guachana. Debido a ello la ciudad posee una forma alargada, cuyo ancho no supera los 8 km, mientras que el distrito ocupa el valle de 12.000 km². El punto más bajo se encuentra a 2680 msnm en El Condado; mientras que el más alto es la cima de La Libertad a 3400 msnm.

El clima de la ciudad corresponde al clima subtropical de tierras altas, con muchas características continentalizadas que van desde climas áridos y templados hasta húmedos y fríos; Quito se divide en 3 zonas; sur, centro, y norte; donde el sur es el lugar más frío de la ciudad porque es la zona más alta, el centro es caliente; donde se dan siempre las temperaturas más altas, y el norte es templado. El clima de Quito se divide en 2 estaciones o etapas; el invierno con un período de lluvias prolongado con mucha prevalencia de fenómenos atmosféricos y climáticos como el granizo, las temperaturas suelen bajar drásticamente hasta ubicarse incluso en los 0°C, han existido casos esporádicos de nevadas a las afueras de la ciudad en páramos situados a 4000 msnm, la última de ellas registrada en enero de 2018. La estación seca de cuatro meses es la temporada donde se presentan las temperaturas más altas. Quito siempre tiene un clima templado con temperaturas que van desde los 10 a los 27 °C.

Debido a que está a 2850 metros de altura y a que está ubicada en un valle cerca de la línea ecuatorial, Quito mantiene condiciones primaverales todo el año. De junio a septiembre las temperaturas suelen ser más cálidas, sobre todo durante la tarde, mientras que el resto del año la temperatura suele ser templada. La población de Cumbayá en el Valle de Tumbaco es el lugar más cálido de la ciudad así como la mayoría de los valles que rodean a la ciudad con temperaturas que alcanzan los 30 °C al mediodía.

Debido a su posición geográfica, la ciudad de Quito recibe niveles extremos de radiación solar todo el año, siendo uno de los lugares de la tierra que más la recibe, llegando a recibir hasta 24 UVI (Índice Ultra Violeta) 

El principal eje en cuanto a gobernabilidad es la Ciudad de Quito, práctica centralizada que poco a poco y con últimas administraciones ha cambiado gracias a un pensamiento pro descentralización. En Quito se concentran las sedes de las 5 funciones del Estado Ecuatoriano, sedes de Ministerios y de otros organismos gubernamentales.

Después de su fundación, el 6 de diciembre de 1534 la ciudad se convirtió en la capital del actual Ecuador, al ser designada como sede de la Real Audiencia de Quito. En 1717 es suprimida debido a una guerra que enfrentó España con la Cuádruple Alianza y se restableció el 5 de noviembre de 1723. Durante esta época colonial, el Presidente de la Audiencia de Quito residió en el Palacio de Carondelet. Así como también desde aquel período, el cabildo ha utilizado el mismo edificio como su sede principal.

Luego de la independencia del país en 1822, la ciudad perdió su capitalidad al integrarse en la Gran Colombia hasta el año de 1830, cuando esta se separa junto con Cuenca y Guayaquil con los cuales formó el Ecuador. Ese año, se establecieron las instituciones políticas. Los organismos que representan a las cinco funciones del Estado fueron circunscritos en Quito desde esa época hasta la actualidad.

Quito es la cabecera del Distrito Metropolitano de Quito. La administración de la ciudad se ejerce a través del Municipio del Distrito Metropolitano de Quito conformado por un Concejo Metropolitano el cual está integrado por 21 concejales y es presidido por el Alcalde Metropolitano, todos estos componentes son elegidos para ejercer estas funciones durante un período de cinco años, mediante sufragio universal. En las últimas elecciones municipales llevadas a cabo en 2014, fue electo Mauricio Rodas, del movimiento SUMA, como Alcalde del Distrito Metropolitano de Quito, cuya posesión se realizó el 14 de mayo del mismo año.
Algunas de las principales competencias que tiene a cargo el Gobierno Metropolitano de Quito son, las del orden urbanístico de la ciudad, promoción cultural, prestación de servicios públicos, las disposiciones tributarias competentes de la urbe, la reglamentación del transporte público y privado, del uso de los bienes públicos, la aprobación del presupuesto general de la ciudad, la fijación de los límites urbanos, distritales y parroquiales.

Quito está dividido en Zonas Metropolitanas conocidas como Administraciones Zonales cuyas funciones son el descentralizar los organismos institucionales, así como también mejorar el sistema de gestión participativa. Cada una es dirigida por un administrador zonal designado por el alcalde, el cual es responsable de ejecutar las competencias de la urbe en su Zona. Actualmente existen nueve Zonas Metropolitanas, las se fraccionan en parroquias, 32 urbanas (ciudad), 33 rurales y suburbanas.

Hasta el momento la ciudad se ha ido extendiendo hacia los valles productivos (tendencia que sigue manteniendo), por lo que ha crecido sin mayor control que el de concentrar sus actividades en la zona centro norte, lo que la convierte en una ciudad congestionada. Además la ciudad cuenta con unos 500.000 automóviles (el parque automotor más grande de todo Ecuador)

Los barrios de Quito son la división política, y en ocasiones administrativa, más pequeña de la ciudad y el Distrito Metropolitano de Quito. Los habitantes de la urbe tradicionalmente la han dividido en cuatro grandes segmentos, los cuales abarcan en gran medida su territorio, estos son: «"el norte"», conformado en su límite septentrional por las parroquias de Cárcelen y el Condado y en el austral por Belisario Quevedo y Mariscal Sucre; «"el centro"», compuesto por Centro histórico, San Juan e Itchimbia; «"el sur"», en el cual en su extremo norte están halladas Magdalena, Chilibulo y Puengasí y en su borde meridional Guamaní y Turubamba; y los «"valles"», que pese a no formar parte de la ciudad de San Francisco de Quito, componen el Distrito Metropolitano.

Las parroquias urbanas que conforman esta división no oficial, suelen subdividirse en barrios. A su vez, estos -por encontrarse a diferente altitud- pueden adquirir el término de bajo o alto según corresponda, sin ser por ello renombrados; así, un barrio como el Batán, dependiendo el lugar al que se refiera el interlocutor, puede ser calificado como «alto» o «bajo» (esta práctica solo se utiliza en los lugares cuyas construcciones estén sobre laderas. Generalmente los habitantes suelen guiarse por esta práctica y por ello los límites geográficos de un determinado barrio están sujetos a la opinión que cada ciudadano tiene sobre él, ya que actualmente no existe una demarcación específica determinada por el Municipio de la ciudad.

Toda la ciudad y el distrito metropolitano se encuentran entre los estilos Contemporáneo y Colonial, que se están reconstruyendo constantemente día a día.
La estructura moderna se mezcla con la colonial donde los residentes nacionales y los visitantes extranjeros encuentran siempre un lugar para trabajar, gozar y recordar. Además, la ciudad está rodeada por los volcanes Pichincha, Antisana, Cotopaxi, y Cayambe que conforman el contorno andino. Quito está colmada de significados que la identifican y definen, ocupa laderas o baja a los valles, serpentea a través de callejones y se abre en amplias avenidas; zigzaguea, sorteando colinas y quebradas. Por esta belleza física, sus tradiciones, rincones de misticismo y leyendas vigentes, es considerada "Relicario del Arte en América". Estas fueron las características principales para que, en septiembre de 1978, Quito fuera declarada por la UNESCO "Patrimonio Cultural de la Humanidad".

En los últimos años el turismo ha crecido en la ciudad y ha significado un nuevo rubro en los ingresos de capital. La mayoría de extranjeros que visitan Quito proceden de los Estados Unidos y pertenecen a una población generalmente joven, cuyo promedio de edad no alcanza los 28 años. También visitan la ciudad europeos provenientes de Italia, Francia o Alemania. Hace unos años la mayoría de turistas que se quedaban en la ciudad unos días lo hacían porque su destino principal era el Archipiélago de Galápagos pero en la actualidad eso ha cambiado. Dentro de las principales anotaciones que destacan los turistas están la amabilidad de la gente quiteña, la majestuosidad de un paisaje lleno de volcanes nevados, y el frío húmedo de montaña, que extraña mucho a los extranjeros pues se acompaña de una sensación térmica muy baja sobre todo por las noches.

En el año 2008 Quito, se inició en la sección de viajes del New York Times. Este diario publicó su tradicional lista de las 53 ciudades que los norteamericanos podrían visitar y por primera vez Quito fue incluida. El sitio web de ese conocido diario presentó a Quito como uno de los mejores y más recomendables destinos turísticos del mundo. “El crecimiento turístico fue del 12% anual, cuando el promedio antes del 2002 era de tan solo del 8%”, sostiene una publicación del Cabildo. La capital vive sus fiestas y, heredera de una rica historia y cultura, muestra su legado a quienes la recorren.

Por otra parte, en el turno de Quito en los medios de comunicación, “Pasaporte hacia América Latina con Samantha Brown” que se estrenó en el verano y continúa en el aire fue tal vez el mayor logro en televisión para la ciudad. Samantha Brown dijo del Ecuador que era “el lugar perfecto para visitar cuando se quiere descubrir América del Sur”. La ciudad también recibió a un equipo de las cadenas estadounidenses CBS y NBC recientemente. Además ha recibido reseñas tremendamente entusiastas de publicaciones como Condé Nast Traveller, Frommer’s ‘Top Destinations for 2008’ (Los mejores destinos para 2008 de Frommer’s), la revista Nexos de American Airlines, la revista Geographical, el diario San Francisco Chronicle, y la revista alemana Merian entre otros. Quito se ubicó en el top 14 en la selección de las nuevas siete ciudades maravillas del mundo, entre más de 1200 ciudades alrededor del planeta.

La ciudad de Quito, al ser además de la capital de Ecuador es un importante centro turístico y cuenta con una variada lista de establecimientos hoteleros de todo tipo, desde casas rústicas hasta grandes y modernos hoteles. La mayoría de hoteles se encuentran concentrados en el centro norte y centro histórico de la ciudad, zonas que concentran los principales atractivos turísticos. Los hoteles en el centro histórico de la ciudad, tienen en su mayoría un ambiente más familiar y rústico, esto debido a que se encuentran en el cásco histórico de la ciudad, zona donde predominan las actividades culturales. Los hoteles del norte y centro norte de la ciudad, suelen ser más modernos y grandes, promocionándose como destino para ejecutivos y empresarios principalmente.

Pero Quito ofrece una variedad de alojamientos en la ciudad para todos los gustos y bolsillos, como los famosos "hostales bed & breakfast" de la zona de La Mariscal al igual que en el centro histórico de la urbe, en donde los turistas pueden alquilar una habitación a precios asequibles y a la vez estar cerca de los centros nocturnos más frecuentados de la ciudad.

Entre de los atractivos turísticos de la ciudad, y a una altura de 3.016m sobre el nivel del mar está la loma de El Panecillo, en la que
se encuentra la estatua de la "Virgen de Quito", inaugurada el 28 de marzo de 1976, siendo uno de los miradores más visitados por su visión de toda la ciudad

Novedoso es el teleférico, o como es conocido localmente, el "TelefériQo" (con "q" de "Quito"), que permite acceder a Cruz Loma (a 4.200 msnm), una pendiente al este del Pichincha, poseedor de un ecosistema de páramo andino. Fuera de la ciudad, al norte, en la parroquia San Antonio del cantón Quito, se encuentra el monumento de la línea ecuatorial, en medio de una verdadera zona comercial conocida como la Ciudad Mitad del Mundo y administrada por el Consejo Provincial de Pichincha.

En la zona de Guayllabamba, se encuentra el Zoológico de Quito, que alberga pumas, jaguares, osos, monos, leones, venados, cóndores, guacamayos, caimanes y canguros, entre otros. Los valles de Los Chillos y Tumbaco tienen también atractivos, tanto para los habitantes de Quito como para los visitantes. En los mencionados valles, se puede encontrar un clima cálido muy bondadoso para la salud, además de platos de comida típica como hornado, yaguarlocro, fritada, etc.

Para los que busquen actividades de aventura, deportes extremos y adrenalina en Quito está el parque extremo más grande del Ecuador, Nayón Xtreme Valley ubicado en la parroquia de Nayón en San Pedro del Valle, a tan solo 10 minutos del norte de Quito y del Valle de Cumbayá. Es un parque eco-turístico de 5 hectáreas donde se practican deportes como el Canopy, el Paintball y Rapel. También se encuentran caminatas por la naturaleza, pistas de bicicletas y juegos para todas las edades. Un paraíso de la naturaleza con paisajes únicos y un clima privilegiado dentro de la ciudad.

La ciudad de Quito cuenta con más de 450 establecimientos gastronómicos (entre restaurantes, bares y cafeterías), los que ofrecen una gran diversidad de estilos culinarios. Desde los establecimientos reconocidos por su comida típica ecuatoriana hasta los sabores de las altas cocinas francesa, italiana o argentina. Para los turistas que llegan a la ciudad, existe una gran herramienta que los puede ayudar a encontrar el lugar ideal donde ir a comer, tomar un trago o un café; ahí podrán encontrar establecimientos gastronómicos por tipo de comida, precio promedio, ubicación y/o ambiente.

Quito posee el centro histórico más grande, menos alterado y mejor preservado de América. Fue, junto al centro histórico de Cracovia en Polonia, los primeros en ser declarados Patrimonio Cultural de la Humanidad por la Unesco, el 08 de septiembre de 1978. El Centro Histórico de Quito se encuentra ubicado en el centro sur de la capital sobre una superficie de trescientas veinte hectáreas, y es considerado uno de los más importantes conjuntos históricos de América Latina. Tiene alrededor de 130 edificaciones monumentales (donde se aloja una gran diversidad de arte pictórico y escultórico, principalmente de carácter religioso inspirado en un multifacética gama de escuelas y estilos) y cinco mil inmuebles registrados en el inventario municipal de bienes patrimoniales. Lamentablemente durante varios años estuvo algo descuidado, lo que contribuyó al deterioro de muchas de sus edificaciones y a que sus calles circundantes se vieran invadidas por el comercio informal. Esto no solamente le restó belleza sino que contribuyó a que el sector sea poco seguro. Pero ahora todo ha cambiado gracias al programa que la Empresa de Desarrollo del Centro Histórico está impulsando para devolver a este lugar maravilloso su esplendor de años pasados.

Caminar por el Centro Histórico de Quito en la actualidad, ahora que ha sido recuperado casi por completo, se ha convertido nuevamente en una experiencia muy placentera. La Empresa de Desarrollo del Centro Histórico, es la encargada de la restauración y conservación de iglesias, calles y plazas de este lugar. Se han implementado varios sitios turísticos que invitan a propios y extraños a visitar el Centro Histórico en un viaje al pasado, que no solo es turístico sino didáctico. Para este efecto, los miembros de la Policía Municipal han sido capacitados para servir de guías en los mencionados recorridos. Por la noche, cuando las luces encienden la ciudad, es posible dar un paseo por el centro en un coche tirado por caballos al más puro estilo colonial. En la Plaza de la Independencia (también llamada Plaza Grande) se encuentra el Palacio de Carondelet, que es la sede de la Presidencia de la República.

Esta monumental basílica es la obra más importante de la arquitectura neogótica ecuatoriana y una de las más representativas del continente americano, siendo a su vez la más grande en tierras del nuevo mundo. Se ubica en el sector céntrico de la ciudad de Quito, en las calles Carchi y Venezuela junto al Convento de los padres Oblatos. Este templo religioso fue edificado para rememorar la consagración del Estado Ecuatoriano al Sagrado Corazón de Jesús, celebrada durante la presidencia de Gabriel García Moreno en 1873. Tiene 115 m de altura lo que convierte a esta iglesia en el edificio más alto de la ciudad con una relación de 40 pisos y está conformada por 24 capillas internas que representan a las provincias del Ecuador. Este santuario fue inaugurado y bendecido por el Papa Juan Pablo II en su visita al Ecuador el 18 de enero de 1985. La Basílica, tanto por su estructura como estilo, es comparada con dos de las grandes catedrales de todo el mundo: la Catedral de San Patricio ubicada en Nueva York y la Catedral de Notre Dame de París.

Un detalle que distingue a la obra es la sustitución de las clásicas gárgolas por reptiles y anfibios propios de la fauna ecuatoriana; además están dispuestos rosetones pétreos que representan a la flora del Ecuador. En el punto más alto de la torre principal se puede observar la ciudad y las montañas que la rodean. La nave central del templo tiene 140 m de largo, 35 de ancho y 30 de alto donde están dispuestas 14 imágenes de bronce que representan 11 apóstoles y 3 evangelistas. A lo largo de la historia de su construcción fueron varios los aportes realizados para que esta obra se lleve a cabo. Los padres Oblatos donaron el terreno donde se erige la Basílica; para proseguir con la construcción se aceptaron donaciones de creyentes quienes proporcionaron piedras a cambio de grabar sus nombres en las mismas. En 1985, el Estado implantó un impuesto por las compras de la sal para continuar con la edificación y se logró terminar la construcción luego que varias generaciones de pica pedreros dedicaran sus vidas para edificar cada pared del recinto. Otro de los atractivos de la Basílica del Voto Nacional es el panteón de jefes de estado del Ecuador.

La Catedral Metropolitana y Primada del Ecuador, por su ubicación en el corazón de la ciudad histórica y su condición de templo mayor de la urbe, es uno de los símbolos religiosos de mayor valor espiritual para la comunidad católica de la ciudad. Este templo inició su edificación en 1562, diecisiete años después de que el obispado de Quito fuera creado (1545). La construcción de la iglesia culminó en 1806, por obra del Presidente de la Audiencia el Barón Héctor de Carondelet. Posteriormente uno de los acontecimientos que se suscitó en este templo fue la muerte del Obispo de Quito de la época, José Ignacio Checa y Barba, quien en la misa del Viernes Santo del 30 de marzo de 1877 fue envenenado con estricnina disuelta en el vino de consagrar. En esta iglesia se encuentran sepultados los restos del Mariscal Antonio José de Sucre. Además los de varios expresidentes de la República, así como también los de obispos y sacerdotes. La Catedral está ubicada en la calle Espejo, en el costado sur de la Plaza de la Independencia.

La Iglesia de La Compañía inició su construcción en 1605, demoró 160 años en ser edificada. Para 1765 se finalizó la obra con la construcción de la fachada del templo. Esta fue hecha por indígenas quienes cuidadosamente plasmaron el estilo barroco en uno de los ejemplos más completos del arte en América. Para 1767 la iglesia fue cerrada a causa de la expulsión de los jesuitas del Ecuador. Cuarenta años más tarde en 1807, fue reabierta por el fraile chileno Camilo Henríquez, de la orden de la Buenamuerte, quien posteriormente formó parte en las luchas de independencia de su país.

Esta iglesia está inspirada en la Iglesia del Gesù de Roma, Italia. Las columnas son una copia de las hechas por Bernini en la basílica de San Pedro de Ciudad del Vaticano. En el interior, cuenta con bellísimos retablos y púlpitos cubiertos con pan de oro. En el retablo del altar mayor, obra de Legarda, se ha retomado como principal motivo de composición las columnas salomónicas de la fachada y las cornisas que se estiran al centro en arco, y se ha hecho culminar el conjunto, abigarrado y deslumbrante, en una corona sostenida por ángeles. La iglesia está ubicada entre las calles García Moreno y Antonio José de Sucre. 140 años después del terremoto que destruyó la torre-campanario de la Iglesia de la Compañía de Jesús, el Municipio de Quito empezó la re construcción de la torre campanario de 45 metros, la cual tendrá las mismas características con las que contaba antes.

San Francisco, es el más grande de los conjuntos arquitectónicos existentes en los centros históricos de las ciudades de América Latina. La construcción de la iglesia se inició en 1550, en terrenos aledaños a la plaza donde los indígenas realizaban los trueques de productos. La obra estuvo a cargo del franciscano flamenco Jodoco Rique. La iglesia, concluida definitivamente hacia 1680 es el resultado armonioso de influencias mudéjares, manieristas y barrocas. Los frailes franciscanos fueron los primeros que se establecieron en Quito. Atractivos como el altar mayor del templo, las capillas laterales y el púlpito son de excepcional belleza. En el altar mayor se encuentra la imagen de la Virgen de Quito, tallada por Bernardo de Legarda, maestro de la escuela quiteña.

Cuenta la leyenda que un indio apellidado Cantuña se comprometió a construir el atrio de este templo; el tiempo de entrega era corto y Cantuña no iba a finalizar la obra en el plazo acordado, de tal manera que al verse perdido hizo un pacto con el Diablo. Este a cambio, le pidió su alma y Cantuña aceptó. Los diablillos comenzaron la construcción que demoró una noche. Cumplida la obra, Cantuña rezó a la Virgen para que le salvara de ser llevado por el demonio, y cuando Lucifer vino a buscar el alma de Cantuña, descubrió que faltaba una piedra por colocar y por tal motivo el pacto quedó anulado. De esta forma salvó su alma.
Hay rumores de queCantuña era el familia de Atahualpa, el heredo la riqueza de él y con eso se construyó la iglesia, ya que él se encuentra enterrado en la iglesia pero la iglesia predominante en ese tiempo oculto la historia y la convirtió en leyenda para atraer a la gente, es una teoría que falta de peobar pero si hay fundamentos.

El templo está localizado en la intersección de las calles Benalcázar, Bolívar, Sucre y Cuenca. Se encuentra una cuadra más adelante de la iglesia de La Compañía.

En tiempos de la Colonia, la iglesia de El Sagrario constituyó uno de los mayores baluartes arquitectónicos de Quito. La construcción, de estilo renacentista italiano y edificada a finales del siglo XVII, cuenta con una mampara que posee acabados, esculturas y decoraciones que la caracterizan por su enorme belleza. Esta estructura fue construida por Bernardo de Legarda. Su bóveda central desemboca en una soberbia cúpula decorada con pinturas al fresco de escenas de la Biblia protagonizadas por arcángeles, obra de Francisco Albán. El retablo del altar mayor fue dorado por Legarda. Está ubicada sobre la calle García Moreno, junto a la Catedral.

Aunque llegaron a Quito en 1541, recién en el año 1580 los dominicos comenzaron a construir su templo, con planos y dirección de Francisco Becerra. La obra total concluyó en la primera mitad del siglo XVII. En el interior del templo se encuentran valiosas estructuras, como el altar mayor neogótico que fue colocado a finales del siglo XIX por dominicos italianos. El techo de la iglesia de estilo mudéjar, cuenta con pinturas de mártires de la Orden de Santo Domingo. La cubierta de la nave central está compuesta por una armadura apeinazada de par y nudillo, recubierta en el interior por piezas de lacería. En el museo situado al lado norte del claustro bajo, se encuentran estupendas piezas de los grandes escultores quiteños tales como: el Santo Domingo de Guzmán del Padre Carlos, el San Juan de Dios de Caspicara, y el Santo Tomás de Aquino de Legarda. Una de las joyas barrocas del siglo XVIII que se cuida celosamente es la Capilla de Nuestra Señora del Rosario, la cual constituye un baluarte de la arquitectura de Quito. Esta capilla fue construida junto a la iglesia, del lado del evangelio. En ella se fundó la más importante cofradía de la ciudad de Quito.

El Panecillo es una elevación natural de 3.000 metros sobre el nivel del mar, enclavada en el corazón mismo de la ciudad. Por su ubicación se ha convertido en el más importante mirador natural de la ciudad, desde el que se puede apreciar la disposición urbana de la capital ecuatoriana, desde su centro histórico y hacia los extremos norte y sur. El Panecillo está coronado por una escultura gigante de aluminio de la «Virgen de Quito», creada por el español Agustín de la Herrán Matorras, el cual se basó en la obra compuesta por Bernardo de Legarda, uno de los más importantes representantes de la Escuela quiteña. Compuesta por siete mil piezas diferentes, esta es la mayor representación de aluminio en todo el mundo. La obra, inaugurada el 28 de marzo de 1975, es una réplica de la escultura de 30 centímetros realizada en el siglo XVIII por el escultor quiteño Bernardo de Legarda, la misma que reposa en el altar mayor de la iglesia de San Francisco, y que está considerada como la obra cumbre de la escultura de la escuela quiteña colonial.

Debido a encontrarse a 2800 metros sobre el nivel del mar y por encontrarse en la zona ecuatorial, Quito es una ciudad de varios contrastes, es una ciudad de altura pero con varios pisos climáticos dentro de la misma y a sus alrededores; en la mañana hasta cerca del atardecer el clima va de tibio-muy caliente-tibio esto debido a encontrarse en la zona tropical, hasta llegar a la noche en que se pone el clima frío y en ocasiones muy frío, esto debido a que el clima se ve modificado por la cadena montañosa llamada "los Andes" y cuyos habitantes visten de acuerdo al clima que se presente, desde forma muy ligera hasta de forma abrigada. Hasta principios de los años sesenta el sombrero era pieza fundamental del guardarropa quiteño de antaño. En ocasiones el clima en la ciudad se comporta en forma desconcertante, el mismo día puede presentarse muy caluroso y a las pocas horas llover muy fuerte para luego tornarse nuevamente soleado o aún más extraño y risible para el extranjero recién llegado, en ciertas partes de la ciudad llueve mientras que en otra se observa totalmente iluminada por el sol. La ropa de abrigo aún es hoy de uso generalizado, sobre todo por las noches, pero en el día se puede ver a los habitantes de la ciudad que llegan a sudar por el calor y llevar ropa de veraneo, pero de cualquier forma el vestuario es acorde a los tiempos actuales de forma occidental moderna; la Tº promedio tanto en el día con 25 °C-26 °C, como en la noche que transcurren a 15 °C-17 °C y a la madrugada transcurren entre 10 °C-12 °C.
La vida nocturna de la ciudad gira alrededor de la Plaza El Quinde más conocida como "Plaza Foch", en el sector de La Mariscal. Son numerosas las terrazas al aire libre, restaurantes, cafés, bares, discotecas y karaokes que abren sus puertas cuando las galerías de arte, librerías y tiendas de artesanías del sector las cierran. Otra nueva opción para la tertulia, sobre todo para el público adulto y de mediana edad, se encuentra en el centro histórico de la ciudad, específicamente en el remozado barrio de La Ronda.

La zona de farra según la jerga de los la capitalinos es conocida como La Mariscal. En ella se concentran alojamientos para mochileros y extranjeros de todo el mundo, restaurantes de varios tipos para igual variedad de presupuestos que los se que encuentran por el resto de la ciudad; los bares, cafeterías, cybers, tiendas de libros y souvenirs y algunas discotecas que cierran sus puertas a altas horas de la mañana. Debido a su variada oferta, se puede andar toda la noche en la Mariscal. Los restaurantes de la zona ofrecen comida italiana, peruana, mongola, ecuatoriana, argentina, francesa, tapas españolas, o de cualquier rincón del mundo. Por precios convenientes se puede comer muy bien en varios de ellos, también los hay de mayor presupuesto para quien quiera proporcionarse un lujo o vaya con compañía a quien quiera impresionar. También se puede encontrar locales pequeños que sirven comida rápida barata junto con cervezas de precio muy cómodo.

El centro neurálgico de la zona es la Plaza Foch donde se concentran varios restaurantes y bares con terrazas que se llenan desde las horas de la tarde. En cuanto a la pura farra, se encuentra fundamentalmente en el sector de "La Mariscal" en cuanto a localización y oferta lúdico-festiva. Se puede observar multitud de bares con terrazas en el primer piso, discotecas, etc, con diversidad de ofertas para atraer a las personas que lo deseen, mucha gente joven local y extranjera yendo y viniendo o tomando en la calle medio a escondidas y mucho control policial para evitar potenciales riesgos.

En sus discotecas se pueden ver diestros y algunos principiantes locales y extranjeros intentando bailar cualquier ritmo o pasos latinos, ganándose a opinión de los turistas un lugar privilegiado entre las mejores ciudades farreras en Sudamérica. La farra comienza a ser muy movida desde el jueves.

Quito ha sido sede de conciertos importantes a nivel nacional, esto se debe a las bajas tasas que se pagan por realizar eventos de gran envergadura, y además que la ciudad ofrece todo lo necesario para la realización de los mismos.

Entre los principales artistas presentados en la ciudad son: Guns N' Roses, Aerosmith, Iron Maiden, Justin Bieber, Miley Cyrus, Jonas Brothers, Metallica, Simple Plan, Bon Jovi, Korn, Cypress Hill, Enrique Bunbury, Ozzy Osbourne, Maná, Shakira, Ricky Martin, Marc Anthony, Chayanne, Elton John, Big Time Rush, David Guetta, Paul McCartney, Lacrimosa, 30 Seconds To Mars, Therion, Joaquín Sabina, Kiss, Miguel Bosé, Roxette, entre otros. Además de la realización del certamen de Miss Universo en el año 2004.

Las fiestas de Quito son unas de las fiestas ciudadanas y populares, más importantes a nivel nacional. Esta se caracteriza por la presencia de: las bandas de pueblo, tarimas para todo tipo de expresión artística en muchos puntos de la ciudad. Se celebra desde fines del mes de noviembre donde se empieza a sentir en el ambiente un aíre festivo y culminan el 6 de diciembre, día de la fundación española de la ciudad. A esta vienen visitantes de todo el país y muchos extranjeros;
el 5 de diciembre la ciudad se paraliza producto del despliegue de algarabía, color, alegría incontenible y fiesta total. También se efectúan en la ciudad conciertos de diverso tipo de música, con muchos artistas locales e internacionales, con bailes generales callejeros, desfiles de varias expresiones culturales locales e invitados de todo el mundo, y ferias gastronómicas.

Destacan también la presencia de chivas (vehículos representativos de la cultura costeña, desprovisto de ventanas y puertas) que sirven para realizar city-tours, las cuales transportan a gente que baila al son de una banda de pueblo. Dichos vehículos son autorizados por el Ayuntamiento a circular por la ciudad en forma temporal previa revisión mecánica y de seguridad.

Parte importante de las fiestas de la ciudad, es la elección de la Reina de Quito, con lo que dan inicio oficialmente las festividades. La reina juega un papel muy importante porque trabaja por la ciudad en sus necesidades más puntuales fijadas de antemano por consenso y es la ayuda social que brinda a los sectores más desprotegidos de la capital.

Tras ser elegida Capital Americana de la Cultura en el 2011, el municipio de la ciudad llevó a cabo una elección para elegir las 7 maravillas de Quito, de un total de 35 sitios, de los cuales fueron elegidos los siguientes.

Su población es de 1`639.853 habitantes en el área urbana y de 2'239.191 en todo el Distrito.

Quito es la , con 2'239.191 habitantes, siendo la mayoría mujeres (51,50%); en su distrito metropolitano.

Aunque para datos reales de población de la ciudad que se obtiene con la aglomeración urbana o la Conurbación de Quito, notablemente visible por las localidades suburbanas de la ciudad, esto es considerando incluso las parroquias de Conocoto, Amaguaña, Cumbayá, Nayón, Zámbiza, Llano Chico, Calderón, Pomasqui, San Antonio, Tumbaco, Guangopolo, Puembo, Alangasi, La Merced,y Sangolquí dan una población real de la ciudad de Quito en 2.495.043 habitantes.

La población étnica de la ciudad es marcado por un aspecto diverso, en el hecho de las diversas etnias que conviven en la misma ciudad, mayormente conviven personas de raza mestiza junto a la blanca, esta última ha significado un enorme incremento junto a la asiática y la árabe desde 2003 aunque disminuyó en 2009, se ha mantenido como una de las más crecientes.

Según las cifras presentadas por el Instituto Nacional de Estadística y Censos Ecuador INEC en el censo realizado en 2010, la composición etnográfica del cantón Quito es:

El sur de la ciudad se caracteriza por ser una zona de alta inmigración nacional en continuo crecimiento, renovación y alta actividad comercial. En la parroquia de Chillogallo, una de las más grandes y densas de la capital, los negocios proliferan en todas partes, especialmente los de todo tipo de servicios. Uno puede degustar desde el tradicional pollo asado, hasta platos típicos de todo el Ecuador como: la guatita, el ceviche, seco de chivo o la fritada. Aquí se ubica uno de los parques industriales más grandes de la ciudad, la Estación de Trenes de Chimbacalle, el nuevo terminal terrestre de la ciudad y el "mall" más grande del Ecuador entre otras cosas destacables. Es importante resaltar que en el sur de la ciudad se encuentran las zonas verdes y los parques urbanos más grandes de la ciudad y del país, como el parque "Las Cuadras" de 24 hectáreas, "El Parque Metropolitano del Sur" de 672 hectáreas, y muchos otros.

En el centro de la ciudad, las calles son estrechas por tratarse el sitio donde nació la ciudad en sus albores, razón por lo cual se restringe el acceso a los vehículos durante los fines de semana, pero es atendida diariamente por el servicio de transporte público de trolebús. Este hermoso espacio urbano de la época colonial es muy llamativa y considerada como "la joya de la corona
", sobre todo sus grandes iglesias, conventos, museos. Aquí también se encuentra ubicada la casa presidencial conocida como el Palacio de Carondelet.
El municipio de Quito ha desarrollado un importante plan de restauración de la parte colonial de la ciudad también llamado "El Centro Histórico" especialmente por tratarse de una zona turística por excelencia con una variada riqueza social-cultural-arquitectónica y manteniendo esa atmósfera de antaño que sus habitantes han sabido conservar.

La zona limítrofe del norte con el centro histórico ha desarrollado una serie de edificaciones y torres elevadas, la más alta de las cuales es la Basílica del Voto Nacional de estilo gótico, con una altura de 36 plantas, y con un mirador excepcional de Quito. Los edificios como la Torre CFN, la Torre Corpei, la Torre Diez de Agosto, el Edificio Benalcázar Mil o la Torre Consejo Provincial de Pichincha son algunas de las construcciones quiteñas que sobrepasan las veinte plantas esto especialmente por las "ordenanzas municipales que limitan la altura de construcción en toda la ciudad", esto por que el aeropuerto internacional se encontraba enclavado dentro de la ciudad hasta febrero de 2013.

Muchos de los barrios del norte de la ciudad son de carácter residencial donde las nuevas generaciones, los más pudientes y hombres de negocios escogieron para vivir, es también donde se encuentra ubicado el centro financiero, bancario, sede de muchas empresas multinacionales y embajadas. Las casas matrices de muchos de los principales bancos que operan en Ecuador se encuentran ubicadas en esta parte de la ciudad, así como otras entidades de trascendental importancia como la Bolsa de Valores de Quito, el Banco Central del Ecuador, el Servicio de Rentas Internas, la Superintendencia de Bancos, entre otras.

Aquí es donde se puede apreciar lo más representativo de la arquitectura ecuatoriana actual, representada en muchas edificaciones levantadas para el funcionamiento de la banca, el comercio, la diversión, compras, etc. La mayoría de estas se encuentran ubicadas alrededor de un parque urbano muy conocido como "La Carolina" de 67 hectáreas. En este sector de la ciudad, es donde se puede apreciar claramente la fuerza del turismo con el cual se ve beneficiada, muchos de los transeúntes son de origen extranjero, pudiendo apreciarse los más disimiles puntos de origen de los ilustres visitantes.

Y es precisamente esta parte de Quito, donde se ha consolidado por la fuerza del propio turismo y vigor de la vida joven hace mucho tiempo, una gran "zona rosa" en la que se concentran múltiples bares, cafés, discotecas, casinos, karaokes, restaurantes, hoteles, etc, etc, conocida comúnmente como "La Mariscal", en la cual sus habitantes nativos, extranjeros residentes y turistas de todo el mundo disfrutan de múltiples terrazas al aire libre, para disfrutar de la ciudad, sus muchas y variadas manifestaciones artísticas, mientras se degusta de la gastronomía o simplemente se toma una copa.

La última y más notable zona de expansión de la ciudad se sitúa en las regiones suburbanas que en su casi totalidad se hallan ya fusionadas con el área administrativa urbana de la ciudad pero forman parte real y física de la ciudad de Quito, esta se desarrolla mayormente alrededor de los valles de los Chillos, Tumbaco, la meseta de Calderón; así como el valle de Pomasqui (mitad del mundo), las comunidades de Amaguaña, Puembo, entre otros y hasta el área urbana de Sangolqui que es un área administrativa separada, pero fuertemente ligada y dependiente de Quito. En todas estas regiones, se caracterizan por ser zonas residenciales de las personas que trabajan en otras zonas de la ciudad principalmente, pero albergan también centros comerciales, universidades, parques, instituciones, industrias, entre otros.

El área de mayor expansión de la construcción moderna se ha dado en la Av. República de El Salvador y sus alrededores, donde se conjugan una infinidad de edificios vanguardistas que en su interior albergan importantes empresas y departamentos de vivienda lujosos. A lo largo de la avenida se puede encontrar el moderno Ministerio de Salud, Edificio Twin Towers, City Group, Hotel Sheraton, el Hotel Dann Carlton y Le Parc Hotel; todos con un promedio de 15 pisos. También se puede apreciar la moderna Plaza Kendo que alberga restaurantes de comida nacional e internacional.

El Aeropuerto Internacional Mariscal Sucre que sirve a la ciudad de Quito fue inaugurado el 20 de febrero de 2013 tras 7 años de construcción como remplazo del Antiguo Aeropuerto Internacional Mariscal Sucre, inaugurado en los años 60 y que por estar ubicado en medio de la ciudad tenía un alto riesgo para la misma. El Aeropuerto de Quito tiene una pista de 4.100 metros, la torre de control más alta de Latinoamérica, de 41 metros de alto y un terminal de carga con varios cuartos fríos para la exportación de flores y otros productos perecederos. Está ubicado en la parroquia Tababela al oriente del Distrito Metropolitano.

El acceso al Aeropuerto es a través del Conector Alpachaca, que se conecta con la vía E28C también conocida como Interoceánica, otro acceso es la E35 vía que es usada para el transporte de carga. Al momento se construyeron 2 vías de acceso más, la Ruta de Integración a los Valles o "Ruta VIVA" a cargo del Municipio de Quito que fue inaugurada el 12 de diciembre de 2014 y la Ruta Collas-Nuevo Aeropuerto a cargo del Gobierno Nacional que fue inaugurada el 1 de agosto de 2014.

Se conforma por el Metro de Quito, el sistema Metrobus-Q y la red de autobuses.

Para el año 2019, se prevé la inauguración de la primera línea de Metro de Quito, la misma que operará en integración con la Red Integrada de Transporte Público de Quito. Con la inauguración del Metro con 15 estaciones de norte a sur, el servicio de transporte público de Quito tendrá un gran salto cualitativo.

En el último trimestre del 2015 se convocará al concurso internacional para escoger quién construirá la obra que deberá iniciar en noviembre del mismo año. Pero, para tanto, ya se conoce cómo será construida, según los estudios, de los asesores de Metro Madrid. Las estructuras serán edificadas con el sistema ‘cut and cover’, que consiste en la colocación de pantallas de hormigón formando un cuadrado. Para ello se utilizarán las máquinas ‘pantalladoras’, las cuales hacen perforaciones de aproximadamente 1 metro de ancho y de 3 metros de longitud, además se construirá también con Tuneladoras, y solo en el Centro Histórico se utilizará el método tradicional para evitar daños al patrimonio del mismo.

El Sistema Metrobus-Q está constituido por 5 líneas (llamadas corredores) con buses brt de gran capacidad tanto en el área urbana de la ciudad. La red se complementa con un sistema de paradas prestablecidas, estaciones de transferencia y terminales. Se está planificando un nuevo corredor brt para el norte de la ciudad, Corredores transversales para conectar las zonas del oeste con las del este y Corredores para las Zonas Metropolitanas (por ejemplo: Cumbayá, Tumbaco, el Valle de los Chillos, Calacalí, Mitad del Mundo) Los nuevos corredores deberán estar listos antes del 2019 año en el que se inaugura el Metro de Quito. El Sistema Metrobus-Q tiene también rutas alimentadoras con buses convencionales.

El trole constituye el eje central del Sistema Integrado y ha sido uno de los proyectos más exitosos de los últimos años en Quito. El 17 de diciembre de 1995 fue inaugurada la primera etapa de este moderno medio de transporte, en el tramo comprendido entre la Estación Sur El Recreo y la calle Esmeraldas en el centro de la ciudad, más las líneas alimentadoras que funcionan en la Estación Sur. En esta etapa se transportó un promedio de 50.000 pasajeros. El 19 de marzo de 1996 entró en servicio la segunda etapa, desde El Recreo al sur hasta la avenida Colón al norte. Se operó con 32 unidades y el promedio de usuarios que se transportó fue de 90.000 pasajeros. El 21 de abril se inauguró la tercera etapa, desde la Estación Sur de El Recreo hasta la Estación Norte de La Y, con la operación de 54 vehículos.

El promedio de usuarios que se transportó inicialmente fue de 120.000 pasajeros. Finalmente en el año 2000 se inaugura la extensión Morán Valverde hacia el sur, para lo cual arribaron nuevas unidades con algunos cambios en los diseños de ingeniería.

Al año 2008, con la Inauguración del Terminal Interprovincial de Quitumbe, se inauguró también una pequeña extensión sur del trolebús, que hace que este sistema de transporte se conecte con el transporte interprovincial.

Actualmente El trole tiene en funcionamiento 5 circuitos troncales: el C1 entre la Estación Norte La Y la Estación Sur El Recreo, el C2 entre la Estación Norte La Y la Estación Quitumbe, el C4 entre la parada La Colón y la Estación Quitumbe, el C5 entre la Estación Carcelén y la parada El Ejido, y el CM entre la Estación Quitumbe y la Estación Sur El Recreo.

La Ecovía fue inaugurada en el año 2000. El sistema recorre aproximadamente 9 km de la ciudad, desde la terminal La Marín en el centro de Quito hasta la Estación de Transferencia Río Coca en el norte. La troncal es operada por 90 buses articulados que funcionan a base de Diésel. También hay 75 buses alimentadores que operan desde la Estación Río Coca hacia algunos barrios del Distrito Metropolitano tales como: La Luz, Monteserrín, Zámbiza, Agua Clara, Nayon, Llano Chico, Carapungo Etape E, 6 De Julio, Comité del Pueblo, La Bota y San Juan de Cumbayá.

El Corredor Central Norte fue inaugurado en 2005 en la alcaldía de Paco Moncayo, recorre la ciudad principalmente a través de las avenidas Vásquez de Cepeda, De La Prensa y América. Conecta a la ciudad desde el norte en la Estación La Ofelia hasta el Playón de la Marín en el centro de la ciudad. Es el único corredor operado por empresas de transporte privado, esto ha causado que el corredor no esté en óptimas condiciones. En el 2011 la alcaldía de Quito tuvo que reconstruir la mayoría de sus paradas, puesto que estaban inutilizables.

Primero empezó a operar desde la Ofelia hasta el Seminario Mayor, desde esta estación se extendió hasta el Playón de la Marín; también se creó una extensión desde La Ofelia hasta Carcelén que nunca funcionó.

El Corredor Sur Oriental fue inaugurado el 26 de octubre del año 2010 con una operación provisional que estaba siendo realizada por treinta buses tipo en el circuito de la Estación Marín-Central hasta la Terminal Interprovincial de Quitumbe. A partir del mes de mayo de 2011, con la llegada de 80 buses articulados adquiridos por la municipalidad, funcionan dos circuitos que recorren la ciudad de sur a norte desde la Terminal Interprovincial de Quitumbe hasta la Estación Marín Central y desde la Terminal de Transferencia Capulí, localizada en el Sur de la ciudad hasta la parada "De las Universidades". localizada en la Parroquia La Floresta, localizada en el Centro Norte de la Ciudad.

El Corredor Sur Occidental fue inaugurado en 2012 en la Alcaldía de Augusto Barrera, recorre todo el lado noroccidental de la ciudad a través de la avenida Mariscal Sucre, conocida popularmente como "La Occidental" cruzando por los túneles de San Juan, San Roque y San Diego. Conecta a la ciudad desde el sur en el terminal Terrestre de Quitumbe hasta la estación del Seminario Mayor en el centro-norte de la ciudad.

El inicio de operación de este corredor fue polémico por la gran congestión que creó sobre todo en la zona de los túneles, así como la falta de 3 paradas y a causa de iniciar las operación con buses tipo en vez de buses articulados por lo que se formaban largas filas de buses esperando llegar a las paradas y acumulación de pasajeros en los buses.

La Red Convencional de Transporte de Quito está conformada por 135 líneas de transporte público operadas por 2.624 buses urbanos, que de acuerdo a las ordenanzas del municipio no pueden tener más de 10 años de servicio. Estas líneas y flotas actuales se encuentran en proceso de reestructuración, en la medida del avance de la Red Integrada de Transporte.

En la ciudad de Quito existen tres clases de transporte urbano: los buses tipo, que constituyen la mayoría; los buses especiales, que sirven a los sistemas integrados de transporte; y los buses interparroquiales, que unen el área urbana con sus distritos rurales. Es fácil diferenciarlos de acuerdo al color que ostentan: azul para los buses tipo, rojo para los especiales y verde para los interparroquiales.

Quito cuenta con dos terminales terrestres que comunican a la ciudad con el resto del país, uno de gran envergadura en el sur de la ciudad llamado "Quitumbe", por el sector en el que se ubica, y que recibe y embarca pasajeros con destino a las provincias del centro y sur del país. Y el Terminal Terrestre de Carcelén más pequeño, que hace lo mismo con los buses que se desplazan desde y hacia las provincias del norte, con mayor concurrencia desde y hacia la Villa de Ibarra.

Quito cuenta con más de 60 kilómetros de Ciclovías, la primera Ciclovía implementada en 2004 (llamada Ciclo-Q) recorre los parques lineales del Sur de Quito, el Centro Histórico y la Avenida Amazonas hasta la estación La Y del Trole también ese año se implementó la Ciclovía "Interuniversitaria" que a través de la Avenida Carrión conecta de este a oesta la Universidad Central del Ecuador con las Universidades Católica, Salesiana y la Escuela Politécnica Nacional.

En 2012 con el inicio de operaciones del sistema BiciQ se dio una gran ampliación de la red de Ciclovías; en las siguientes avenidas: Av. de la Prensa, Av. Gerónimo Carrión, Av. Diego de Almagro, Av. Luis Cordero, Av. Antonio de Ulloa, Av. Veracruz, Av. Atahualpa, Blvd. Naciones Unidas y Av. Mariana de Jesús.

Quito también cuenta con ciclovías recreativas en los Parques Lineales del Sur, Parque El Ejido, Parque La Alameda, Parque La Carolina, Parque Itchimbía, Parque Metropolitano Guanguiltagüa, y el Chaquiñan de Cumbayá-Tumbaco que es un sendero ecológico de 28 kilómetros creado sobre una línea férrea abandonada. Además la Universidad Católica cuenta con una red interna de ciclovías, denominada CicloPuce.

En 2012 la Alcaldía de Quito implementó un sistema de alquiler de bicicletas públicas denominado "BiciQ". El sistema consta de 425 bicicletas de un diseño único, distribuidas en 25 estaciones, ubicadas estratégicamente en lugares cercanos a los puntos de mayor afluencia, atracción o interés comercial, bancario, turístico o estudiantil, para acceder al sistema los usuarios deben registrarse en el sitio web www.biciq.gob.ec y tras pagar 25$ por año y firmar un contrato de buen uso se le otorga un carné de usuario, que sirve para hacer uso de las bicicletas de 7 de la mañana a 7 de la noche los 365 días de cada año.

El perímetro de aplicación del sistema es en el denominado "Hipercentro" entre el Centro Histórico y el sector de "La Y" y se estudia expandir el sistema hacia el Sur y el Norte de la ciudad. Cada Bicicleta se puede usar por 45 minutos y debe ser entregada en cualquier estación, si el usuario ya cumplió ese tiempo y no ha llegado a su destino debe esperar 10 minutos antes de poder acceder nuevamente al sistema.

Debido a la geografía de la ciudad, la cual se extiende de norte a sur teniendo aproximadamente 50 km de largo y solo 8 km de ancho, la gran mayoría de avenidas importantes de Quito se extienden de norte a sur. La avenida más larga que cruza la ciudad de norte a sur es el Eje Longitudinal Avenida 10 de Agosto (que se transforma en la Avenida Galo Plaza al norte y Avenida Vicente Maldonado al sur). La avenida que cruza la ciudad de norte a sur del lado occidental, es la Avenida Occidental Mariscal Sucre y la autopista que cruza la ciudad de norte a sur del lado oriental es la Autopista Corredor Periférico Oriental Simón Bolívar.

Se encuentran en proyección y ejecución, algunas avenidas transversales, tanto en el centro, sur y norte de la ciudad con conectividad a los dos valles orientales de Quito, con las que la ciudad crecerá efectivamente hacia sus conurbaciones, y entonces el área de la ciudad, triplicará la actual. En estos mismos valles, ya se encuentran en construcción, avenidas, autovías, autopistas, intercambiadores, etc., que preparan a la ciudad, al "Gran Quito", al reto de crecer en el próximo lustro hacia esas latitudes, estimulado por el Nuevo aeropuerto de Quito, que está ubicado en el valle de Tumbaco.
La ciudad de Quito, junto a todo el Distrito Metropolitano de Quito, cuenta con varios intercambiadores de tráfico que facilitan el tránsito. Entre los más importantes se encuentra el intercambiador de El Trébol, ubicado en el centro de la ciudad, que conecta al centro con el sur y el valle de los Chillos. Lastimosamente, en 2008, una crecida en el río Machángara que cruza por debajo del intercambiador, destruyó una de las "hojas" de El Trébol, el cual actualmente funciona como un gran redondel; posteriormente fue reconstruido. También el intercambiador de Miravalle, por el que cruzan las avenidas Nueva Oriental e Interoceánica, conectando al centro norte de la ciudad con el valle de Tumbaco, el norte y sur de la ciudad. También el Intercambiador de Carcelén, que conecta a la ciudad con la carretera Panamericana Norte. El intercambiador de las avenidas 10 de Agosto, Eloy Alfaro y Francisco de Orellana en el centro-norte de la ciudad, conectando los sectores de La Mariscal e Iñaquito. Uno de los intercambiadores más importantes de la zona urbana es La Y, en el que se conectan las avenidas América, 10 de agosto, La Prensa, Brasil y Gaspar de Villarroel.

De acuerdo a recomendaciones de la Organización Mundial de la Salud (OMS), cada territorio debe contar con al menos 9 metros cuadrados de espacios verdes por habitante. En el país solamente 10 cantones superan esa recomendación, Mera (Pastaza), Huamboya (Morona), el Distrito Metropolitano de Quito, Mocha (Tungurahua), El Pan (Azuay), Pablo Sexto (Morona Santiago), Sigchos (Cotopaxi), Paute (Azuay), Quero (Tungurahua), Saquisilí (Cotopaxi).

De acuerdo al estudio del Instituto Nacional de Estadísticas y Censos de 2010, Quito tiene 20,4 metros cuadrados de áreas verdes por habitante, la tercera más alta del país. Los parques que se encuentran en el Distrito Metropolitano de Quito además de ofrecer amplios espacios destinados a la práctica deportiva y a la recreación, ayudan a mejorar la calidad de vida de todos los habitantes en medio del sistema urbano, y con el objetivo principal convertirse en una ciudad verde.

El parque se encuentra enmarcado por las avenidas Shyris, Eloy Alfaro, Amazonas y Naciones Unidas. Con 67 hectáreas de terreno es uno de los parques urbanos en medio de la ciudad, más grandes del Distrito Metropolitano, del país y de América del Sur. Durante la semana acoge a deportistas habituales que disfrutan del ambiente tranquilo que ofrece el lugar por las mañanas, mientras que los fines de semana recibe aproximadamente a 50.000 personas. Posee una amplia infraestructura que incluye: canchas de fútbol, baloncesto, tenis, voleibol, trayecto atlético, circuito de bicicross, pista de patinaje, pista para acrobacias en bicicleta, perímetro de juegos infantiles, áreas de ejercitación deportiva, centro de exposiciones, restaurantes, centros de socialización, cinco lotes para estacionamiento de vehículos, etc.
El parque La Carolina ubicado en el sector de Iñaquito, nació como producto de la expropiación municipal a la hacienda La Carolina en 1939. El diseño moderno fue realizado por la Dirección de Planificación del Municipio en 1976 y está siendo remodelado nuevamente en la actualidad. El Papa Juan Pablo II llevó a cabo en este parque una multitudinaria misa durante su visita a Ecuador en 1985. Para conmemorar este evento, se construyó una cruz gigante en el sitio donde se efectuó la ceremonia.

Otros sitios de atracción ubicados dentro del parque son:









Está localizado en la zona norte del Distrito, cercado por las calles Guanguiltagua, Arroyo Delgado y Analuisa. Con una extensión de 557 ha, es el principal pulmón de la ciudad de Quito. El parque se encuentra ubicado a 2.890 a una máxima de 2.980 msnm y registra una temperatura media de 11 °C. Rodeado de árboles y obras de arte gigantescas, los visitantes pueden disfrutar de la naturaleza respirando aire puro a pocos metros de la ciudad. En la quebrada Ashintaco ubicada en el sector nororiental del parque, se puede observar las más de diez especies de colibríes y setenta especies de aves que anidan en el lugar, algunas de las cuales están en peligro de extinción. El parque ofrece también un camino de piedra y diferentes senderos para los amantes del ciclismo de montaña y también el Downhill. Cada fin de semana llegan al Parque Metropolitano de Quito aproximadamente entre 20 y 30 mil personas para acampar, hacer picnic, y muchas otras actividades. El parque resulta un mirador natural por su vista excepcional hacia el oeste (la ciudad propiamente dicha) y al Este (su prolongación en los valles, especialmente a Cumbaya).

Es un parque ubicado donde se asentó el antiguo Aeropuerto Mariscal Sucre y fue abierto al público el sábado 27 de abril de 2013, es el segundo parque más grande de la ciudad (después del Parque Metropolitano).

En los próximos meses se iniciará la fase de arborización que incluye la siembra de 2.800 especies nativas de árboles, en lo que corresponde a la primera etapa. El 89% del área del parque será verde y será un pulmón para la ciudad. Además de crear bosques, se conformarán humedales que alberguen variadas especies de fauna y flora silvestre. Este medio natural se complementará con varias fuentes de agua para la recreación de los visitantes. En el parque se establecerán viveros temporales, arborización permanente, jardines ornamentales, caminerías, canchas, pista atlética, juegos infantiles, grafismo temporal, accesos y estacionamientos.

El parque está delimitado por las avenidas Patria, 6 de diciembre, Tarqui y la calle Guayaquil. El Ejido marca la división entre el Quito antiguo y el Quito moderno, en él habitan alrededor de 1.470 especies de plantas nativas como el cholán, el aliso, el chamburo, las palmeras y los guabos.

En este parque todos los días se juegan partidos de ecuavoley que atraen la atención del público. Además las personas se congregan para disfrutar de los tradicionales juegos populares de los cocos cuyo objetivo consiste en sacar a éstos (bolas grandes de metal) del interior de un círculo trazado en la tierra y se debe eliminar de un pepo (golpe) a los adversarios.

También los fines de semana, y cobijados por la imponente Puerta de La Circasiana (un arco de piedra de 8 metros de alto con grabados renacentistas que en tiempos pasados fue la puerta de entrada al Palacio del mismo nombre), se realizan exposiciones culturales en las que se puede adquirir obras de arte, joyas en plata, ponchos, sacos, chalecos, entre otras novedades.

Está ubicado en el centro de la ciudad, dentro de un triángulo comprendido por la avenida Gran Colombia y las Calles Sodiro y Guayaquil. La Alameda es el parque más antiguo de Quito, era conocido antes por los indios como chuquihuada (que en quichua significa punta de lanza). Se encuentra en la parroquia San Blas y ocupa 6 ha. Este es un sitio que guarda muchas nostalgias y recuerdos, allí funcionó hasta inicios del siglo pasado la Escuela de Bellas Artes de Quito en medio de un ambiente casi místico que marcó el arte de la época. También se encuentra el Observatorio Astronómico de Quito construido en 1864 durante la presidencia de Gabriel García Moreno. En su época fue el mejor equipado de Sudamérica y utiliza aún los instrumentos de observación de ese entonces.

Actualmente los visitantes acuden al parque a descansar en el lugar, o utilizan pequeños botes para navegar en el pequeño lago. En La Alameda todavía se pueden encontrar a fotógrafos que retratan a los visitantes utilizando cámaras de antigua tecnología. En el acceso sur del parque, se inauguró el 24 de julio de 1935 el monumento al Libertador de América Simón Bolívar, acto que constituyó un verdadero acontecimiento político, social y cultural del país. Aquí se puede encontrar árboles de dimensiones importantes que han resistido al tiempo y a la invasión del cemento. Investigadores botánicos registraron una variedad importante de especies nativas y extranjeras como la acacia, la palmera, el cedro, el fresno, el pumamaqui, el yaloman, el arrayán, el eucalipto y la magnolia.

Este parque se encuentra ubicado al sur de la ciudad de Quito, en la Avenida Cardenal de la Torre en el sector de Solanda. Tiene una extensión aproximada de 20 ha, y ofrece varias distracciones para sus visitantes tales como ciclovía, ruta peatonal, áreas recreativas y de deporte, laberintos, esculturas, pileta, espejo de agua, mobiliario, baterías sanitarias, etc.

Se ubica en la cima y en las laderas de la loma Itchimbía la cual se encuentra en el límite oriental del Centro Histórico de Quito, rodeada por el río Machángara y los barrios de El Dorado, La Tola y San Blas. El parque está a 2900 msnm, y es considerado como un mirador natural por la amplia visibilidad que se tiene de la ciudad desde sus cuatro puntos cardinales.

El espacio recreacional cuenta con una ciclovía, una ruta peatonal, una plazoleta y un parqueadero para 150 vehículos. Cada año entre los meses de julio y agosto se celebrá el Festival de Cometas (papalotes), en el que se divisan estos milenarios juguetes chinos surcar los cielos, favorecidos por los fuertes vientes del verano capitalino.

En el interior del parque, y como punto principal del mismo, se encuentra el Centro Cultural Itchimbía, popularmente conocido como Palacio de Cristal; una enorme construcción de hierro y cristal que en tiempos republicanos (finales del siglo XIX y comienzos del XX) sirvió para albergar un popular mercado del Centro Histórico. El diseño original le pertenece al ingeniero francés Gustave Eiffel (el mismo que diseñó la Torre Eiffel - París); el edificio fue desmantelado de su ubicación original, recuperado en talleres especializados del municipio y llevado pieza por pieza para rearmarlo en la cima de milenaria colina del Itchimbía para albergar exposiciones de todo tipo a lo largo del año.

También se desarrolla el festival más grande de música al aire libre denominado Quito Fest, en el mes de agosto de cada año.

Está ubicado en la esquina suroccidental de las avenidas 10 de Agosto y Colón, y forma parte del Palacio de La Circasiana, un palacete levantado por Manuel Jijón y Larrea a finales del siglo XIX, que fue la primera obra civil neorenacentista de proporciones monumentales erigida en Quito. En las inmediaciones del palacete, Jijón edificó dos inmuebles más: una librería dotada con 40 000 títulos y un edificio museo, los que continúan sirviendo a la ciudad.

En la década de 1930, se podía apreciar a la estructura en todo su esplendor. Toda la casa estaba rodeada de jardines bien cuidados, un gran muro exterior y una portada en arco, que la aislaban de toda la ciudad.

La Puerta de La Circasiana ("La despedida de los centauros") fue una de las puertas de ingreso a la hacienda Chillo Jijón y fue cedida por la familia a la ciudad. Hoy, este arco constituye uno de los atractivos del Parque El Ejido. La propiedad es administrada ahora por el Fondo de Salvamento (Fonsal), el cual convirtió al ingreso de la residencia en un hermoso parque adornado c
on las figuras fantásticas de unicornios.

Está delimitado por las avenidas 12 de Octubre, 6 de diciembre y Tarqui, además se encuentra junto a la Casa de la Cultura Ecuatoriana. Muchos consideran este pequeño espacio verde como parte de su vecino: El Ejido, aunque en realidad se trata de un lugar diseñado bastante tiempo después.

En los terrenos del actual parque de El Arbolito se encontraba hasta mediados del siglo XX el único estadio de fútbol de la capital ecuatoriana. Al construirse el Estadio Olímpico Atahualpa en Iñaquito el predio fue cedido al municipio y este lo convirtió en el espacio recreacional que admiramos en la actualidad.

Este parque es también conocido por ser el punto de encuentro de las comunidades indígenas ecuatorianas en sus marchas hasta el Palacio de Carondelet. En las fiestas celebradas por la Fundación de Quito cada diciembre, el lugar se transforma en un gran patio de "Comidas típicas del Ecuador".

Tiene una extensión de 48 hectáreas y sus principales usuarios son los habitantes del Valle de Los Chillos Conocoto, Alangasí, Guangopolo, Pintag, Amaguaña y La Merced. El Parque cuenta con senderos y ciclovías.

Tiene una extensión 12 hectáreas. 250.000 habitantes de la parroquia de Puengasí, de los barrios de San José de Monjas, Jardín del Valle, Alma Lojana son sus usuarios.

El beneficio más importante para la comunidad es la recuperación del relleno de la quebrada Cuscungo como un espacio verde reforestado, con áreas deportivas, caminerías, ciclovía y mobiliario para el disfrute de los habitantes de Quito. El parque cuenta con senderos y ciclovías con una extensión de 1,6 kilómetros aproximadamente.

Cuenta con 320 hectáreas y está ubicado en la parroquia de Chillogallo. Atraviesa varios sectores del sur de la ciudad desde La Magdalena, cruzando por la Mena Dos hasta Lloa. Se ubica a 4,5 kilómetros aproximadamente desde la Mariscal Sucre, ingresando por la Mena Dos por la Vía a Lloa.

Posee una superficie de 620 hectáreas y está ubicado al sur de la ciudad en la parroquia Quitumbe, entre la avenida Simón Bolívar, sector el Troje, a 7 kilómetros aproximadamente del intercambiador de la Simón Bolívar y autopista General Rumiñahui. A este parque acuden los vecinos de Santa Rosa, Quitumbe, Chillogallo, Amaguaña, el Valle de Los Chillos, Guamaní, Guajaló, Músculos y Rieles, Buenaventura, San Juan de Turubamba, Cuapichu, Cataguango, entre otros.

Posee 24 hectáreas y se ubica al sur de la ciudad, en la avenida Rumichaca y calle Matilde Álvarez, sector de Quitumbe. A este sitio acuden vecinos de los barrios del sur de la ciudad como Solanda, Quitumbe, Guajaló, Oriente Quiteño, Registro Civil, Santa Rita, Las Cuadras y Chillogallo.

También cabe mencionar los nombres de otros parques urbanos importantes que forman parte de la ciudad de Quito: Parque Inglés, Parque de la Mujer y el Niño, Parque Julio Andrade, Parque Lineal del Machángara que va paralelo al río de su mismo nombre, YAKU - Museo-Parque del Agua, Parque Monteserrín, La Moya, Parque de La Magdalena, entre otros.

La ciudad de Quito cuenta con la mayor cantidad de museos del país: superan los 60, convirtiéndola en el eje fundamental de la cultura de Ecuador estos atraen a muchos turistas y trae igualmente una economía a Quito, los de arte y cultura que abundan en el Centro Histórico los museos interactivos al Sur y Centro de la ciudad, de pintura como el De Oswaldo Guayasamin En el Norte de Quito. Entre los más representativos, tenemos:

El Municipio del Distrito Metropolitano de Quito contribuye con este museo al desarrollo cultural en el ambiente del Centro Histórico de Quito. Este sitio muestra la historia desde la cotidianidad de los ciudadanos que la han vivido. Aparte de diferentes obras y objetos de exhibición, se pueden apreciar los testimonios de la vida cotidiana. Con los permanentes cambios de las exposiciones, hay la posibilidad de involucrarse con las actividades realizadas dentro de los históricos murales del Hospital de la Misericordia de Nuestro Señor Jesucristo, hoy con el nombre Hospital San Juan de Dios.

El Museo de la Ciudad está ubicado en el edificio más antiguo de Quito, que data de 1565. El Museo de la Ciudad se puede visitar entre el martes y domingos de 09:30 a 17:30. Los tours pueden ser organizados.

Aunque pertenece a la colección del Banco Central del Ecuador, este museo se encuentra albergado en el "Edificio de los Espejos" de la Casa de la Cultura Ecuatoriana, junto al Palacio Benjamín Carrión. Exhibe las más importantes piezas de arte ecuatoriano en sus salas de Arqueología, de Oro, de Arte Colonial, de Arte de la República y de Arte Contemporáneo. Cuenta además con varias Salas Temporales, abiertas con arte ecuatoriano y universal con muestras individuales y colectivas de carácter itinerante.

El museo se divide en diferentes áreas, la primera sería la Galería Arqueológico donde se exhiben artefactos que datan de 11.000 a. C. Tribunal de Oro, en representación de los grupos indígenas que adoraban al sol una, al hacerlo, utilizaron el oro para crear máscaras, adornos en el pecho, y figuras para representar el sol. Galería de Arte Colonial, que contiene piezas desde 1534 hasta 1820 y el republicano Galería de Arte, se puede ver esta transición. El Museo de Instrumentos Musicales se encuentra en el Museo Nacional del Banco Central del Ecuador.

Sin duda uno de los museos más importantes dentro de la ciudad es La Capilla del Hombre, obra cumbre del maestro plástico quiteño Oswaldo Guayasamín, y localizado en el sector de Bellavista.

El Museo Nacional de Medicina del Ecuador es un museo dedicado a la historia de la Medicina ubicado en Quito y fundado por el doctor Eduardo Estrella, quien fue un médico e investigador ecuatoriano. Estrella estudió medicina en la Universidad Central del Ecuador. Hizo sus estudios especializados en psiquiatría en la Universidad de Navarra, Pamplona, España. Estrella, más tarde presidió la Facultad de Medicina de la Universidad Central del Ecuador.

El libro más famoso de Dr. Estrella es "Flora Huayaquilensis" que relata una expedición que se había perdido por más de 200 años en los archivos del Real Jardín Botánico de Madrid. En 1985 Dr. Eduardo Estrella encontró evidencia de que Juan José Tafalla Navascués había recorrido el territorio de la Audiencia de Quito y que sus artículos, no publicados, eran la primera descripción sistemática de la flora del Ecuador.

Clasificación del Museo Nacional de Medicina del Ecuador, historia museo de la medicina en Quito


El Templo de la Patria es un complejo monumental construido en honor a los soldados que lucharon en la Batalla de Pichincha (1822); levantado en el lugar exacto donde se libró aquella gesta, con la que se selló definitivamente la independencia de los territorios que hoy conforman el Ecuador.
El Museo Templo de la Patria alberga además fascinantes leyendas, como la de Abdón Calderón, el héroe adolescente, nacido en Cuenca, que ofreció su vida por la libertad de su patria. El complejo consta del museo, los jardines, un teatro al aire libre, pequeños monumentos conmemorativos, un mural del maestro plástico lojano Eduardo Kingman y la tribuna en la que se realiza el desfile militar anual.

Cada 24 de mayo se realiza en los exteriores de este museo una parada militar para conmemorar la proeza libertaria, comandanda por Antonio José de Sucre, el Gran Mariscal de Ayacucho. Es además un sitio privilegiado desde el cual se puede divisar la totalidad del Centro Histórico de Quito y parte de la zona norte de la ciudad.

Este sitio contiene 17 salas donde se recrean 21 etnias de la Costa, Sierra y Amazonía del país. También hay una sala especial para las especies emblemáticas de la fauna nacional. En exhibición se distingue a un cóndor, a varios roedores y primates menores, todos embalsamados. Una sala final está dedicada al desarrollo de foros y proyecciones audiovisuales para el público estudiantil que acude a este complejo antropológico. La carta estética para este Museo la puso el talento escultórico de Galo Tobar. Este maestro supo capturar la mirada, la expresión y las posiciones heriáticas de pescadores montubios, agricultores serranos y sabios y guerreros orientales. Y dichos personajes aparecen en la recreación de sus ambientes originales. Para este efecto, cañas de bambú, bahareque, troncos de árboles endémicos y hojas de palmera han sido introducidas al lugar. El resto de la vegetación está hecha en papel.

Aspecto importante en esta muestra antropológica está en la apreciación de la vida cotidiana de los pobladores primigenios del país. La vestimenta es un punto de referencia hacia el entendimiento de las relaciones entre los seres humanos y sus entornos. El caso de los vecinos de la Amazonía es el más didáctico. En los atuendos de jíbaros y cofanes se aprecia que estas comunidades vieron en las fibras vegetales y las semillas la "tela" para sus diseños. Luego, los objetos utilitarios que complementan los ambientes dan cuenta de cómo los pobladores han intervenido en la naturaleza para su provecho. Las redes y los machetes dan cuenta de la dieta marina del habitante costero. Las vasijas recuerdan los procesos de almacenaje de los granos que cosechan los serranos. Y los pequeños tiestos son testimonio de la molienda del ají y de la mezcla de yerbas sagradas dentro de los rituales de curación en el oriente.

En esta misma temática, resulta buen atractivo las ambientaciones de las labores pastoriles de la gente de Simiátug, en el norte de la provincia de Bolívar; o las cholas cuencanas dedicadas a la alfarería y al tejido de sombreros de paja toquilla; o la fabricación de canoas con los árboles de los cayapas. Las alegorías festivas y el mundo mítico de Ecuador aquí también tienen su espacio. Una escultura presenta al personaje tradicional de la celebración sincrética del Corpus Christi. En su imagen conjugan la estética andina y la judeo-española. Espejos, plumas, encajes e íconos cristianos conforman el traje con el cual un danzante agradecerá a los dioses de la naturaleza por la fecundidad de la tierra.

Ubicado en la Escuela Politécnica Nacional, mantiene la colección paleontológica más antigua y más importante del país, fósiles colectados por Alejandro Humboldt, Teodoro Wolf, Franz Spillmann, Robert Hoffstetter, entre otros, y que forman parte del patrimonio Paleontológico del país.

Otros museos importantes dentro del Distrito son el museo de cera Mena Caamaño, en pleno casco histórico; el museo etnográfico Ciudad Mitad del Mundo, ubicado en el complejo turístico del mismo nombre en el norte de la ciudad; los museos históricos María Augusta Urrutia y Casa de Sucre, ambos en el centro histórico; el museo Numismático del Banco Central, junto a la iglesia de La Compañía, también en el centro histórico de la urbe; el museo de la Biblioteca Aurelio Espinosa Pólit, en la parroquia de Cotocollao, al norte de la ciudad; y el museo ecológico - recreacional Yaku, en la parte occidental y que se destaca por su invitación a conocer y preservar las bondades del agua. Además existen dos zonas arqueológicas emplazadas en medio de la ciudad moderna: Ciudad Metrópoli (ruinas de la ciudad de los Quitus y de los Incas), y la Necrópolis de La Florida (cementerio de los quitus).

La ciudad y sus alrededores cuentan con varias universidades de pregrado y postgrado. Desde su fundación Quito ha sido la capital académica y universitaria del país, la fundación de universidades de congregaciones católicas como la Santo Tomás de Aquino y la San Gregorio definieron el rumbo de lo que hoy son la Pontifica Universidad Católica del Ecuador y la Universidad Central del Ecuador. Así mismo durante la presidencia del ilustre conservador Gabriel García Moreno se fundó la Escuela Politécnica Nacional, mientras que otras instituciones de educación superior se fundaron posteriormente, tal es el caso de la Escuela Politécnica del Ejército, la Universidad de las Américas (parte de la Red Laureate), la Universidad San Francisco, la [[Universidad Politécnica Salesiana|Politécnica Salesiana]y la Universidad de Los Hemisferios (UDH)].

Quito posee a tres de las cinco universidades con la mayor red de investigación y desarrollo del país, la [[Pontificia Universidad Católica del Ecuador]], la [[Universidad San Francisco de Quito]] y la [[Escuela Politécnica Nacional]]; ubicándose en el primero, tercero y cuarto puesto respectivamente. El mayor campus universitario se encuentra ubicado dentro de la Universidad Central del Ecuador. Únicamente tres universidades de la ciudad son públicas ([[Universidad Central del Ecuador|UCE]], [[Escuela Politécnica Nacional|EPN]] y [[Universidad de las Fuerzas Armadas - ESPE|ESPE]]), el resto de universidades son privadas y dependiendo de cada una sus matrículas y pensiones suelen ser holgadas o elevadas como es el caso de la [[Universidad San Francisco de Quito|USFQ]].

Quito posee casi un 78% de la oferta curricular nacional, pues en conjunto entre todas las universidades posee estudios en [[Medicina]], [[Arquitectura]], [[Ingeniería]]s, [[Comercio]], [[Economía]], [[Jurisprudencia]], [[Ciencias Físicas]] y [[Matemáticas]], [[Tecnologías de la información y la comunicación|Tecnologías]], [[Química|Ciencias Químicas]], [[Arte]], [[Educación]], [[Filosofía]], [[Teología]], [[Sociología]], [[Deporte]]s, [[Cine]], [[Música]], [[Electrónica]], [[Robótica]], [[Antropología]], [[Veterinaria]], [[Ciencias ambientales|Ciencias Ambientales]], [[Agronomía|Ciencias Agrícolas]], [[Petróleo|Minas y Petróleos]], [[Odontología]], [[Diseño]], [[Comunicación]], [[Idioma]]s, [[Gastronomía]], [[Hotelería]], [[Historia]], [[Psicología]], [[Geología]], [[Política]], [[Artes liberales|Artes Liberales]] entre muchas otras. El resto de la oferta curricular nacional resulta estar relativamente cerca de la ciudad pues se concentra mayormente en la [[Ciudad del conocimiento Yachay|Ciudad del Conocimiento]] en la provincia limítrofe de [[Imbabura (provincia)|Imbabura]], donde se ofertan carreras como [[Nanociencia]]s, [[Polímero]]s, [[Energía renovable|Energías Renovables]], [[Biomedicina]], [[Sistema digital|Ciencias Digitales]], [[Sostenibilidad]], entre otras; carreras como las de [[Aeronáutica]], [[Ingeniería naval|Ingeniería Naval]] y [[Ingeniería forestal|Ciencias Forestales]] se encuentran algo lejanas, en [[Guayaquil]] y [[Tena (Ecuador)|Tena]] respectivamente; y [[Biología]] e [[Evolución biológica|Investigación evolutiva]] en [[Islas Galápagos|Galápagos]].

La infraestructura universitaria en Quito es por lo general óptima, con ciertas excepciones, tal es el caso de la [[Universidad Central del Ecuador]] la cual se halla severamente en deterioro por falta de inversión en la misma, algunas facultades de esta universidad datan desde hace más de cincuenta años en los cuales no ha existido una remodelación o intervención oportuna. Sin embargo el total del resto de universidades de la ciudad cuentan con campus bastante bien conservados, algunos de ellos muy modernos y vanguardistas, eclécticos y clásicos. La universidad que cuenta con mayor cantidad de campus es la [[Universidad de las Américas (Ecuador)|Universidad de las Américas]] con cuatro (Granados, Queri, Colón y Udlapark). A lo largo de la Avenida 12 de Octubre e Isabel la Católica se hallan los campus de cuatro universidades ([[Escuela Politécnica Nacional|EPN]], [[Pontificia Universidad Católica del Ecuador|PUCE]], [[Universidad Andina Simón Bolívar|UASB]] y [[Universidad Politécnica Salesiana|UPS-Q]]).

Tal como cualquier capital, la ciudad de Quito tiene una variada oferta en cuanto a gustos y temas se refiere. En esta ciudad se puede encontrar desde tiendas de diseñadores internacionales hasta tiendas artesanales locales. En la capital podrémos encontrar una gran variedad de personas de diferentes partes del país que por varios motivos han emigrado de sus ciudades natales, también una significativa cantidad de extranjeros provenientes de todos los lugares del mundo.
Quito se ha convertido en una ciudad que acoge a miles de inmigrantes provenientes principalmente de Colombia, Cuba, Perú, recientemente Venezuela, y aunque en menor cantidad, en la ciudad también viven ciudadanos europeos y norteamericanos. Quito es uno de los principales puntos turísticos del país y de toda Latinoamérica.

Quito cuenta con varios lugares llenos de cultura donde se aglomeran ciudadanos de todo el país y del mundo. Entre las principales tenemos a la Mariscal, donde se asientan varios lugares de ocio y diversión que atienden de lunes a domingo. En esta zona se encuentran varios negocios de comida internacional, bares, discotecas, karaokes, restaurantes y un sin número de ofertas de encuentro y diversión nocturna.
Otra zona muy concurrida por Quiteños y extranjeros es el centro histórico, ya que aquí se encuentran museos e iglesias de interés lo que lo convierte en un lugar muy transitado y un punto referencial del turismo y de la cultura Quiteña.

Exiten algunos paseos urbanos en la ciudad. Los más importantes son el bulevar de la Avenida Amazonas, de la avenida Naciones Unidas y el área de la Mariscal. Existen ciertos proyectos planteados por el Municipio del Distrito Metropolitano de Quito a fin de regenerar la avenida Colón para darle el mismo estilo urbanístico de las ya mencionadas Amazonas y Naciones Unidas.

La ciudad actualmente presenta una concentración de empresas y oficinas de negocios en cinco ubicaciones principales: El Ejido, La Whymper, La Coruña, La Carolina y 12 de Octubre. Que vendrían a ser los cuatro centros financieros de la ciudad. Estos se encuentran todos en la zona norte.

Según las estadísticas del (2001), los siguientes son los datos de la actividad económica de la capital de Ecuador:


Quito, capital de la provincia de [[Provincia de Pichincha|Pichincha]] y del Ecuador, es la ciudad que más aporta al [[PIB]] Nacional y la segunda con mayor [[Renta per cápita]] luego de Cuenca. Quito es la de mayor grado de recaudación de impuestos en el Ecuador por concepto de gravámenes según el Servicio de Rentas Internas (S.R.I.), superando el 57% nacional al año 2009, siendo en la actualidad la región económica más importante del país, según el último "estudio" realizado por el Banco Central del Ecuador, en el año 2006, el aporte fue del 18,6% al PIB, generando 4106 millones de dólares, sin embargo su valor de adjudicación permite que este [[PIB]] sea aún mayor llegando a adquirir en términos reales el 27% del [[Pib]] país gracias a las aportaciones de la producción petrolera y predial. Actualizado: al 2009 el PIB de Quito fue de 57,650 millones de dólares aproximadamente por concepto de producción (19% de aportación), 4112 millones de dólares por concepto de adjudicación (8% de adjudicación) y 14762 millones de dólares por concepto total de [[PIB]] (27% procedente del 8% adjudicado, 19% producido). En 2010, el PIB de Quito subió a 95.996 millones de dólares, convirtiéndose en la ciudad que más aporta al PIB de país.

[[Archivo:Panoramic View - Quito, Ecuador - South America02.jpg|miniaturadeimagen|300px|Avenida 12 de Octubre y su zona comercial.]]

Tabla: Datos económicos de la ciudad de Quito para el año 2009
La ciudad es sede de las más importantes compañías nacionales y de la casi totalidad de compañías multinacionales asentadas en el país; también es la sede de las oficinas centrales de las más importantes industrias que funcionan en el país. La actividad financiera y bancaria se concentra en el centro norte de la ciudad en los alrededores del parque de "La Carolina". Quito también es una ciudad muy agitada en el ámbito comercial, destacando grandes y modernos centros comerciales, malls, tiendas de textiles, artesanías y souvenirs, cadenas de grandes supermercados, ferreterías, farmacias, etc. La actividad económica es muy variada, aquí que es donde se concentra la mayor parte del accionar de la industria automotriz especialmente en el ensamblado para consumo nacional y exportación, la mayor actividad de construcción de todo el país, es la primera exportadora nacional de flores, madera, productos no tradicionales como el palmito y espárragos y varios más provenientes de sus valles y del mismo distrito; la actividad comercial es muy variada y la ciudad concentra la mayor cantidad de empresas dedicadas a esta actividad a nivel nacional. Es la segunda ciudad que más remesas recibe según estadísticas del Banco Central del Ecuador en el 2008. Y por último y lo más importante, la actividad turística que es la que más atrae a la ciudad y en la cual pretende concentrar y dedicar sus mayores esfuerzos. A partir de una gran inversión destinada a la regeneración urbana del Centro Histórico y otros lugares turísticos que empezó desde el 2001 por parte del Municipio del Distrito Metropolitano de Quito, el rubro turismo viene a ser una importantísima fuente de ingresos para la ciudad.

[[Archivo:MIRANDO A QUITO DESDE LAS ALTURAS (37628605982).jpg|thumb|right|300px|[[Estadio Olímpico Atahualpa]], principal escenario deportivo de la ciudad.]]
La "Concentración Deportiva de Pichincha" es el organismo rector del deporte en toda la [[Provincia de Pichincha]] y por ende en Quito se ejerce su autoridad de control. El deporte más popular en la ciudad, al igual que en todo el país, es el [[fútbol]], siendo el deporte con mayor convocatoria.

La competencia atlética de carácter popular más grande del país es la [[Quito Últimas Noticias 15K]] y se organiza en esta ciudad desde [[1960]]. En su edición 2013 contó con un total de 22000 atletas inscritos.

La ciudad es sede de dos de los equipos más populares del país: [[Liga Deportiva Universitaria de Quito|Liga de Quito]] y el [[Club Deportivo El Nacional]], ambos equipos participan en la [[Serie A de Ecuador|Primera División del Fútbol Nacional Ecuatoriano]].

[[Liga Deportiva Universitaria de Quito|Liga de Quito]] apodado como "Rey de Copas", el equipo con mayores logros internacionalmente, tiene 10 títulos nacionales y 4 títulos internacionales, ha conseguido Copa Libertadores ([[Copa Libertadores 2008|edición de 2008]]), dos [[Recopa Sudamericana]] ([[Recopa Sudamericana 2009|edición de 2009]], y [[Recopa Sudamericana 2010|edición de 2010]]) y una [[Copa Sudamericana]] ([[Copa Sudamericana 2009|edición de 2009]]), en el 2008 fue subcampeón del [[Copa Mundial de Clubes de la FIFA 2008|mundial de clubes]], y desde el año 2008 es el equipo ecuatoriano mejor ubicado según el Ranking de la [[Confederación Sudamericana de Fútbol|CONMEBOL]]. 

El [[Club Deportivo El Nacional]] es el tercer club con más títulos de campeón nacional, teniendo 13. Se caracterizado porque su plantilla de jugadores la componen únicamente pertenecen deportistas de nacionalidad ecuatoriana. Ha sido tricampeón y lo ha hecho dos ocasiones, en 1976, [[1977]], 1978 y en [[1982]], [[1983]], 1984. Otros equipos tradicionales quiteños son [[Sociedad Deportiva Aucas]], [[Club Deportivo América]], y [[Sociedad Deportivo Quito]], que juega en la [[Segunda Categoría de Ecuador|Segunda Categoría]], el primero tiene su sede en el [[Estadio Gonzalo Pozo Ripalda]] y los otros dos en el [[Estadio Olímpico Atahualpa]].

Los equipos profesionales de [[fútbol]] de la primera división A ecuatoriana que tienen a Quito, capital del país, como sede son:





[[Categoría:Quito| ]]
__FORZAR_TDC__

</doc>
<doc id="3311" url="https://es.wikipedia.org/wiki?curid=3311" title="Ecuador">
Ecuador

Ecuador, oficialmente denominado República del Ecuador, es un país americano ubicado en América del Sur. Limita al norte con Colombia, al sur y al este con Perú y al oeste con el océano Pacífico, el cual lo separa de las Islas Galápagos, ubicadas a 1000 kilómetros, desde la Puntilla de Santa Elena a la Isla San Cristóbal. Una sección volcánica de la cordillera de los Andes divide el territorio de norte a sur, dejando a su flanco occidental el golfo de Guayaquil y una llanura boscosa, y al oriente, la Amazonía. Ecuador ocupa un área de 283 561 km², por lo que se trata del del subcontinente, aunque para dar una perspectiva, su extensión es mayor a la del Reino Unido. Es el de América, con algo más de 16 millones de habitantes, el más densamente poblado de América del Sur y el quinto en el continente.

Ecuador es una reciente potencia energética basada en energías ecosustentables. Además, se trata del país con una de las más altas concentraciones de ríos por km en el mundo, uno de los países de mayor diversidad por km por ende, uno de los países con mayor biodiversidad del mundo. Es el primer país del planeta en tener los Derechos de la Naturaleza garantizados en su Constitución,

La capital del país es Quito y su ciudad más poblada es Guayaquil. La lengua oficial es el español, hablado por un 99% de la población, junto a otras trece lenguas indígenas reconocidas, incluyendo kichwa y shuar. Para 2016, el Índice de Desarrollo Humano de Ecuador es catalogado como "alto", ubicándose en el puesto 88 a nivel mundial y 10 a nivel de América Latina, tras Perú y delante de Colombia. Con un PBI PPA de 172 100 millones de dólares, la economía ecuatoriana ocupa el puesto número 59 a nivel mundial y el séptimo de Sudamérica. El país es un importante exportador de petróleo, consta como el principal exportador de banano a nivel mundial y uno de los principales exportadores de flores, camarones y cacao. Ecuador recibió en 2014 aproximadamente , lo cual posiciona al país como uno de los referentes regionales en recepción de turismo internacional.

Los primeros asentamientos humanos en el territorio se remontan a 12 000 años a. C. (El Inga, Chobshi, Cubilán, Las Vegas). El Imperio incaico conquistó parcialmente la región a mediados del siglo XV, y en 1543 comenzó la conquista española, tras la que permaneció como colonia durante casi trescientos años. La época independentista tuvo sus orígenes en 1809 e inició el proceso emancipador comprendido desde 1820 hasta 1822. Después de la definitiva independencia del dominio español, parte del territorio se integró rápidamente en la Gran Colombia, mientras el territorio del litoral permaneció independiente hasta la anexión "manu militari" por Simón Bolívar. En 1830 los territorios colombianos del sur se separaron y se creó la nación ecuatoriana. Desde los inicios de la república existió inestabilidad política, lo que condujo al origen de varias revoluciones a lo largo del siglo XIX y conflictos limítrofes con Colombia. El siglo XX estuvo marcado por los conflictos limítrofes con el Perú y la conformación de gobiernos militares. En 1979, el país volvió al sistema democrático, aunque la inestabilidad política de 1996 a 2006 llevó a una crisis económica, política y social que condujo a la dolarización de su economía y al derrocamiento de tres presidentes antes de terminar su mandato.

La primera referencia que se tiene de este país con relación a la línea ecuatorial está registrada en "Noticias Secretas de América" en 1826, una publicación donde se recopila estudios realizados durante el transcurso del siglo XVIII, incluyendo los de la misión geodésica francesa, en esta obra se menciona por primera vez a "las tierras de Ecuador" como jurisdicción de la Real Audiencia de Quito. El nombre hace alusión a la línea ecuatorial de la Tierra que pasa sobre la ciudad de Quito y que cruza el territorio Nacional de este a oeste. El nombre fue adoptado por la Gran Colombia en 1819 para identificar los territorios del distrito del sur, que luego en 1824 se subdividirían en Guayaquil, Azuay y Ecuador que comprendía Quito. El 13 de mayo de 1830 el distrito del sur se separó completamente de la Gran Colombia, formando un solo gobierno comandado en ese momento por el general Juan José Flores, las tempranas presiones regionalistas de los delegados de Guayaquil y Cuenca en la Primera Constituyente, motivaron que el nombre de "República de Quito" fuera desechado por el de "República de Ecuador", a pesar de que durante toda la época de la colonia española el territorio fue conocido como la Real Audiencia de Quito.

Los primeros registros de asentamientos humanos conocidos en el actual territorio ecuatoriano son de hace aproximadamente 13 500 años. Del Paleoindio se hallan vestigios en El Inga, Cultura Las Vegas, Chobsi, Cubilán y pinturas rupestres amazónicas.

La época precolombina del país comprende los períodos Paleoindio, Formativo, de Desarrollo Regional y de Integración o Periodo Incaico.
Durante el periodo formativo se descubrió el uso de la cerámica, siendo la cultura Valdivia una de las candidatas a poseer la alfarería más antigua de América. También en este periodo se domesticaron un gran número de especies vegetales, probablemente debido a la diversidad biológica y climática de la región; entre ellas, cabe mencionar: piña, papaya, zapallo, maní, tomate, tomate de árbol, naranjilla, ají, cacao, entre otras. La agricultura alcanzó un alto nivel de desarrollo: las zonas secas del país muestran abundantes restos de obras dedicadas a la recolección e infiltración de agua, conocidas como albarradas, que alteran el paisaje; las laderas de montañas en muchas regiones del país tienen restos de andenerías; mientras que en las zonas bajas y húmedas, en las vegas de ríos y orillas de lagos, se encuentran restos de camellones o grandes camas de cultivo con riego por inundación. Este último sistema es especialmente interesante por su dimensión en las cuencas de los ríos del litoral, como el río Guayas, con miles de hectáreas dedicadas al cultivo en camellones de gran tamaño.

La cultura Manteña, ubicada en la parte central del litoral ecuatoriano, controló una amplia ruta de comercio marítimo, que se extendió desde el actual Chile hasta México, basada en la navegación de cabotaje con grandes balsas impulsadas por velas. En el litoral norte, la cultura La Tolita produjo una metalurgia ornamental de alto nivel, principalmente en oro, plata y aleación de platino. La alfarería de las culturas Bahía y Jama-Coaque es recargada de detalles, y recuerda un tanto a la asiática, dando lugar a teorías de intercambio cultural transoceánico que no han podido probarse. Los pueblos de la sierra norte construyeron complejos funerarios y astronómicos como el de Cochasquí.

El territorio de Ecuador formó parte del Imperio Inca del Norte hasta la conquista española en 1533. A la llegada de los incas, se estima que habitaban en el territorio del actual Ecuador más de 36 nacionalidades, entre las cuales algunas de las más numerosas eran: Pastos, Caranquis, Imbayas, Paltas, Puruháes, Panzaleos, Cañaris, Hambatus. La influencia incaica se hizo sentir especialmente en el callejón interandino del sur y centro del país, que formaron parte del Tahuantinsuyo; la región norte se mantuvo parcialmente independiente hasta la llegada de los españoles, y tiene una de las mayores presencias de fortalezas o en el imperio Inca; mientras que las regiones de la costa y la Amazonía mantuvieron su independencia. Durante el imperio Icario, se construyeron algunos asentamientos con evidente influencia cuzqueña, siendo de los más importantes Ingapirca (aún se conserva buena parte de los restos arqueológicos) y Tumipampa (Tomebamba) (la ciudad de Cuenca fue fundada sobre la última aunque se conservan sus ruinas en algunos sectores).

En 1534, el capitán español Sebastián de Benalcázar conquistó las tierras ecuatorianas. Este, una vez tomada Quito, la refundó como ciudad española el 6 de diciembre de 1534, bautizándola como San Francisco de Quito en honor a Francisco Pizarro. Quito fue capital de la Presidencia de Quito y de la Real Audiencia de Quito, que formaba parte del Virreinato del Perú. Los españoles utilizaron los asentamientos urbanos indígenas y varios elementos de la estructura social autóctona como base de las nuevas ciudades mestizas y para colonizar los territorios que ocuparon.

Los indígenas los superaban en número, pero los españoles tenían una mayor fuerza militar, gracias a lo cual sometieron a las poblaciones indígenas, obligándoles a abandonar los valles templados de la Sierra y ubicarse en los páramos altos. Los incas, además de estar enfrentados entre sí en guerras internas, desconocían las armas de fuego. Se dice que muchos indígenas pensaron que los españoles que montaban sus caballos eran seres de cuatro patas y comparaban el sonido de los cañones con el de los truenos. Sin embargo, no pasó mucho tiempo hasta que los indígenas empezaran a defenderse, a pesar de su desventaja.

Quito fue el principal asiento español en la zona, y de ella partieron las expediciones que permitieron el descubrimiento del río Amazonas, y la fundación del resto de ciudades ecuatorianas. En 1739, Ecuador se integró en el Virreinato de Nueva Granada junto con Caracas, Panamá y Santa Fe de Bogotá. Las relaciones entre la población autóctona y los recién llegados se rigieron por instituciones jurídicas como la Mita y la Encomienda, esta última aprobada por las Leyes de Burgos en 1512 para la defensa de los indios. Enfermedades como el sarampión diezmaron la población indígena.

Esto hizo que para el trabajo forzado se trajera población africana negra, en calidad de esclavos, lo que contribuyó al mestizaje del Ecuador. Gran parte de la población negra en el país se encuentra en la actual Esmeraldas. Se dice que un barco de esclavos naufragó frente a las costas esmeraldeñas y una gran cantidad de esclavos quedaron ahí con dos españoles supervivientes que murieron al poco tiempo. Durante la época colonial se desarrollaron las artes, especialmente la arquitectura, pintura y escultura. En la Colonia se destaca la Escuela Quiteña, como un espacio de alta producción artística, famosa hasta la actualidad, por artistas como Miguel de Santiago, Caspicara y Bernardo de Legarda, entre otros.
Los primeros movimientos empezaron en 1809 con la rebelión de los Criollos contra el dominio español conocida como Primer Grito de Independencia Americana. Aunque hay otros precursores como Eugenio Espejo, sabio criollo de origen mestizo que lanzó las primeras proclamas por escrito en la publicación «"El Nuevo Luciano de Quito"». Los sublevados formaron una Junta de Gobierno provisional el 10 de agosto de 1809 en Quito, pero los participantes acabaron siendo encarcelados y asesinados en la Matanza del 2 de agosto de 1810. En esa fecha los sublevados no propugnaban la independencia sino que hablaban de una República Monárquica o una Monarquía Republicana, sino cambiar las autoridades «afrancesadas» en Quito, manteniendo fidelidad al cautivo rey Fernando VII. Parte de la historiografía del Ecuador considera este suceso como el Primer Grito de Independencia Hispanoamericana y el inicio del proceso de emancipación de la región.Terminado el dominio francés y con la negativa del rey de España, Fernando VII, de acatar la Constitución de Cádiz, se desencadenaron una oleada de movimientos independentistas en la América Española.

Guayaquil fue el primer territorio libre de la Audiencia con su proceso independentista que tuvo lugar el 9 de octubre de 1820, con el propósito de romper los lazos coloniales que existían entre el territorio de la Provincia de Guayaquil y el Imperio español, y que dio paso al surgimiento de la Provincia Libre de Guayaquil. La independencia de Guayaquil marcó el comienzo de la guerra de independencia de la Real Audiencia de Quito como parte de las guerras emancipadoras de Hispanoamérica. Entre los factores más influyentes para su desencadenamiento se puede determinar la voluntad de los criollos, los cuales ya poseían un alto estatus social y económico, de obtener el poder político.

Es así como la antigua Presidencia y Audiencia de Quito consigue escindirse de la metrópoli en la batalla de Pichincha del 24 de mayo de 1822, gracias al triunfo del mariscal Antonio José de Sucre, lugarteniente de Simón Bolívar, con ayuda del Ejército Protector de Quito, formado por las tropas independentistas guayaquileñas ideadas por el poeta José Joaquín de Olmedo. El territorio de Guayaquil (que se había separado de España el 9 de octubre de 1820 y mantenía un gobierno propio) pasó a formar parte de la Gran Colombia bajo el nombre de Distrito del Sur junto a los territorios de Quito y Cuenca. El colapso de la nueva república dio lugar a la formación de los estados soberanos de Nueva Granada (actuales Colombia y Panamá), Venezuela y Ecuador en 1830. Cuando en 1822 el ejército independentista, comandado por Antonio José de Sucre, venció a las fuerzas realistas en la Batalla de Pichincha, los territorios formaron parte de la Gran Colombia, pero la gran rivalidad entre su presidente, Simón Bolívar, y su vicepresidente, Francisco de Paula Santander, ocasionó la disgregación de la Gran Colombia. Desde 1830, año del fin de la Gran Colombia, las naciones de: Ecuador, Colombia, Venezuela y Panamá mantuvieron su nexo político y económico ya que continuaron como estados federados durante cinco años más.

La República del Ecuador vio la luz el 13 de mayo de 1830 cuando se separó de la Gran Colombia. Juan José Flores fue quien tomó las riendas del nuevo estado convirtiéndose en su primer presidente. Flores inició la organización del país tomándole cinco años de mandato interrumpido de 1834 hasta 1839 (periodo durante el cual el país acumulo una cuantiosa deuda externa principalmente debido a la adquisición de material bélico); año en que Vicente Rocafuerte asumió la curul presidencial. Cuando el venezolano fue elegido para un tercer período presidencial, los grupos de poder de la costa iniciaron un levantamiento popular con el fin de abatir el militarismo extranjero el 6 de marzo de 1845, la llamada "revolución marcista".

El presidente al cargo fue José Joaquín de Olmedo, gran pensador guayaquileño que se mantuvo en el poder solo hasta que el legislativo llamó a elecciones y se nombró a Vicente Ramón Roca como tercer presidente constitucional del Ecuador. De 1845 a 1859 se vivió un período de gobiernos liberales hasta que una nueva revuelta llevó a nuevas elecciones en donde la figura de Gabriel García Moreno llegó a la política nacional en 1859, tras la re-unificación del país luego de la batalla de Guayaquil, hasta 1875, año en que es asesinado. Los gobiernos de García Moreno son criticados desde la esquina liberal anti-Católica como un régimen autoritario, represivo y dictatorial vinculado al clero católico. Gabriel García Moreno fomentó una política de construcción de obras públicas como carreteras, el ferrocarril, escuelas, colegios, universidades y hospitales. Todo este período fue de represión contra los liberales ecuatorianos. García Moreno, cuando se encontraba en su tercera presidencia, fue asesinado en el balcón del palacio de Carondelet en Quito por manos de un grupo de liberales radicales, en el que destaca Faustino Lemus Rayo. Según la versión conservadora su asesinato fue por intereses políticos de los liberales que querían llegar al poder, mientras que para los liberales se trató de un acto "patriótico". Además se conoce que su muerte fue una conspiración liderada por la Francmasonería en rechazo a su piedad Católica y protagonismo en la consagración del Ecuador al Sagrado Corazón de Jesús. Tras la muerte de Don Gabriel García Moreno, los dos partidos políticos del Ecuador trataron de unificar su pensamiento hacia lo que se denominaría el progresismo a una suerte de conservadurismo liberal. Antonio Borrero Cortázar fue el primer presidente fruto de esta unificación, pero apenas duró un año en el poder y sería seguido por Ignacio de Veintemilla y la posterior aparición del revolucionario progresista General Eloy Alfaro Delgado.

El momento más importante de la historia del progresismo sería cuando en el período del presidente Luis Cordero se realizó la conocida Venta de la Bandera lo que desencadenó en la revuelta militar que dio origen al liberalismo ecuatoriano con Eloy Alfaro como presidente constitucional. El periodo liberal dura desde el 5 de junio de 1895 hasta el 9 de julio de 1925 con los gobiernos del General Eloy Alfaro, General Leonidas Plaza Gutiérrez, Dr. Alfredo Baquerizo Moreno y Dr. José Luis Tamayo. Es de destacar el asesinato del General Eloy Alfaro y sus acompañantes el 28 de enero de 1912 cometido por el populacho en la ciudad de Quito, después de ser sacados de la cárcel donde habían sido conducidos tras las derrotas en los combates de Huigra, Naranjito y Yaguachi. En la constitución de 1897 se estableció la libertad ante la ley, la libertad de pensamiento, la abolición de la pena de muerte para los delitos políticos y la garantía absoluta a la vida. Además se suprimió la participación de un eclesiástico en el consejo de Estado y se aceptó la libertad de cultos. 
En la constitución de 1906, se suprimió a la religión católica como religión oficial, se estableció la educación laica, se separó la Iglesia del Estado, se reconoció la libertad de conciencia en todas sus expresiones y se prohibió que los religiosos sean legisladores, además se aprobó el divorcio. Cabe destacar la cruel matanza del 15 de noviembre de 1922 cometida contra el pueblo guayaquileño que pedía pan durante el gobierno del Dr. José Luis Tamayo y que quedó impune, narrada magistralmente por el escritor Joaquín Gallegos Lara en su obra "Las cruces sobre el agua". Tras el golpe de estado del 9 de julio de 1925 hubo un Gobierno Plural Civil-Militar en el cual cada integrante de la junta gobernaba por una semana, gobernando así hasta marzo de 1926 cuando la junta cesó su actividad y se nombró como presidente a Isidro Ayora quien expidió una nueva constitución, la número trece. En su gobierno se creó el Banco Central del Ecuador, Banco de Fomento, Superintendencia de Bancos, Caja de Pensiones, Dirección Nacional de Aduanas, entre otras dependencias estatales; en 1930 se produjo una deflación general y Ayora se vio presionado a renunciar, dejándole el poder al ministro de Gobierno, Luis Larrea Alba, quien asumió el poder el 24 de agosto de 1931 y ante la negativa del congreso para adquirir poderes plenos, decidió disolverlo y el pueblo reaccionó frente a la dictadura y así él entregó el poder al presidente del Senado, Alfredo Baquerizo Moreno, quien convocó comicios presidenciales para octubre, y tras una serie de problemas de gobiernos, Velasco Ibarra se posesiona en 1934. Sería hacia 1941 justo y en curso de la segunda guerra mundial que el Perú invade la provincia de El Oro lo que desencadenaría la Guerra peruano-ecuatoriana durante el gobierno del liberal Dr. Carlos Arroyo del Río y que concluyó con el Protocolo de Río de Janeiro firmado el 29 de enero de 1942 cuando el sur del Ecuador se encontraba invadido. La gran figura de la política ecuatoriana desde mediados de los años 30 hasta inicios de los 70 fue el Dr. José María Velasco Ibarra quien fue Presidente del Ecuador en 5 ocasiones pero solo pudo culminar su tercer mandato. Fue Presidente en 1934-1935, 1944-1947, 1952-1956, 1960-1961 y 1968-1972.

Hacia comienzos de 1972, el Ecuador era un país sumido en el caos , con un presidente convertido en dictador civil, elecciones generales próximas a celebrarse y actores políticos cuyas futuras acciones eran impredecibles. Finalmente las fuerzas armadas decidieron intervenir, tomarse el poder e interrumpir el incipiente sistema constitucional en el que el país estaba inserto desde 1968. En febrero de 1972, hubo un golpe de Estado que tomó por sorpresa a la opinión pública y a la comunidad internacional. El derrocamiento de Velasco Ibarra sucedió en Guayaquil y fue ejecutado materialmente, y sin que se disparara ni una sola bala, por un oficial de la Armada llamado Jorge Queirolo Gómez, pero llevó al general Guillermo Rodríguez Lara al poder, quien se proclamó "nacionalista" y "revolucionario", lo que devino en una serie de nacionalizaciones, las que pueden ser evaluadas más o menos críticamente pero que, para el momento en cuestión, resolvían los temas básicos del sistema productivo y social del Ecuador. Así, el Gobierno creó en 1972 CEPE, la Corporación Estatal Petrolera Ecuatoriana (actualmente Petroecuador) y emprende el camino hacia la adquisición, paso a paso, de las acciones mayoritarias del Consorcio Texaco - Gulf (Gulf Oil Corporation) - CEPE.

El Ecuador da signos de querer adquirir autonomía nacional en el manejo del petróleo: en 1973 el Ecuador ingresa en el organismo más importante a nivel mundial de los países oferentes de petróleo, la Organización de Países Exportadores de Petróleo (OPEP). En 1974 adquiere el 25 % de las acciones del Consorcio que operaba en Ecuador. En 1976 asciende al 62 %, hasta que finalmente, adquiere la totalidad de las acciones. Con este tránsito, el Estado ecuatoriano pasa a ser el propietario del petróleo.

El gobierno de Rodríguez Lara también creó el Instituto Ecuatoriano de Electrificación (INECEL) (actualmente Corporación Eléctrica del Ecuador CELEC) y un sistema para asegurar el aprovisionamiento de víveres básicos para los sectores populares: Empresa Nacional de Productos Vitales (Emprovit), que expedía esos productos a precios accesibles. También creó el Instituto Ecuatoriano de Telecomunicaciones (IETEL).

El 1 de septiembre de 1975, se produjo un intento de golpe de Estado dirigido por el general Raúl González Alvear, en el cual miembros de la Brigada Patria y del batallón de paracaidistas se enfrentaron a la escolta presidencial que se atrincheró en el palacio de Carondelet, en una balacera que duró más de cuatro horas y que dejó un saldo de 22 muertos y más de 80 heridos y que produjo serios daños a la fachada del palacio de Gobierno. Luego de estos hechos, el general González se fue exiliado a Chile y el general Rodríguez promulgó un decreto-mordaza para proteger el "prestigio" de las Fuerzas Armadas de los comentarios de los medios de comunicación. A pesar de todo esto, la situación del general Rodríguez se hizo insostenible y el Consejo de Generales de las Fuerzas Armadas le pidió su renuncia, acto que se concretó en enero de 1976. El Gobierno quedó en manos de un triunvirato militar, presidido por el almirante Alfredo Poveda Burbano (Armada) e integrado por los generales Guillermo Durán Arcentales (Ejército) y Luis Leoro Franco (Fuerza Aérea). Durante este gobierno tuvo lugar la matanza del ingenio de Aztra en La Troncal el 18 de octubre de 1977 contra los trabajadores que habían iniciado una huelga, matanza que quedó impune. Su ministro de Gobierno, el entonces coronel Richelieu Levoyer, estructuró el Plan de Retorno a la Democracia que, en sus partes sustantivas, consistía en la formación de una nueva asamblea constituyente convocada por la Junta Militar, la que redactó una nueva Constitución y organizó un referéndum que tuvo efecto en enero de 1978, con el que el pueblo ecuatoriano eligió por simple mayoría entre la Constitución de 1945 reformada y la nueva Constitución. La última parte del plan de la Junta Militar fue convocar a elecciones generales, en las cuales participaron 17 partidos políticos aprobados por el Régimen.

Tras una segunda vuelta, que se realizó con mucha diferencia de tiempo de la primera, resultó elegido Jaime Roldós Aguilera, candidato del partido populista Concentración de Fuerzas Populares (CFP). Jaime Roldós gobernó de manera independiente y en abierta pugna con Assad Bucaram, que durante el primer año de su mandato ostentó el cargo de presidente del congreso. Roldós tuvo que afrontar otro conflicto fronterizo con Perú en 1981, que amenazaba con convertirse en una guerra abierta que, al final, no aconteció. Al estrellarse en misteriosas circunstancias el avión en que viajaba el 24 de mayo de 1981 (todavía se investiga el accidente), el poder pasó al vicepresidente constitucional en funciones el Dr. Osvaldo Hurtado Larrea, de tendencia Social-demócrata, al que sucedió en 1984 el socialcristiano conservador León Febres-Cordero. Sus medidas de austeridad por problemas con el petróleo y sus políticas represivas, que aunque eliminaron incipientes grupos guerrilleros como Alfaro Vive Carajo, provocaron un descontento social, que dio la victoria en 1988 al socialdemócrata Rodrigo Borja Cevallos, en cuyo mandato tuvo lugar un movimiento indígena que logró la distribución de 1 700 000 hectáreas a las comunidades autóctonas. Borja también impulsó la alfabetización y la educación bilingüe.El conservador Sixto Durán Ballén propició desde 1992 una política neoliberal con privatizaciones y ajustes cuestionados por la mayoría del Congreso, y provocó el abandono de la OPEP, mientras el país aumentaba la producción petrolera. Otro conflicto con Perú conocido a nivel mundial como la Guerra del Cenepa que terminó ese mismo año en 1995 con el Acuerdo de Itamaraty y, en 1998, bajo la Presidencia del democristiano Jamil Mahuad, con la firma definitiva de la paz en Brasilia que le dio a Ecuador acceso al Amazonas al concederle el Perú derechos de libre navegación fluvial, así como dos zonas francas y dos parques naturales en la zona de conflicto. De acuerdo con este protocolo, el Ecuador renunció a sus pretensiones históricas de anexar Tumbes, Jaén y Maynas; y los reconoció como territorios peruanos. Quedó así zanjada la disputa limítrofe que desde 1960 había sido enunciada por José María Velasco Ibarra. Este acuerdo tuvo provisiones para la colocación definitiva de los hitos fronterizos en cooperación con la misión de observadores de la OEA (MOMEP).

La normalidad institucional se vio resquebrajada en 1997 cuando el Congreso, en medio de manifestaciones populares en contra del Ejecutivo, destituyó por «incapacidad mental» al presidente populista Abdalá Bucaram, quien se había posesionado en agosto de 1996. En su reemplazo, el Congreso designó como Presidente Interino a Fabián Alarcón, hasta ese momento Presidente del Congreso Nacional (pese a que constitucionalmente le correspondía asumir la presidencia a la vicepresidente Rosalía Arteaga, quien se posesionó simbólicamente por unas horas). Tras una Asamblea Nacional Constituyente en 1998, la cual tuvo el mandato de revisar y modificar la Constitución de 1979, se realizaron elecciones generales en las que fue elegido presidente Jamil Mahuad Witt, de Democracia Popular. Mahuad fue depuesto en enero de 2000, en medio de una grave crisis económica ocasionada por la quiebra masiva del sistema financiero ecuatoriano, la caída de los precios internacionales del petróleo y la vinculación del gobierno de Mahuad con la banca corrupta cuya cabeza más visible fue Fernando Aspiazu, quien el 26 de agosto de 2002 fue condenado a ocho años de prisión por el delito de peculado. Todo ello provocó una huelga general, movilizaciones indígenas y un intento de golpe de estado que duró cuatro horas. Como resultado de la crisis económica durante este gobierno más de 2 millones de ecuatorianos tuvieron que migrar hacia otros países teniendo como resultado la separación de innumerables familias.

El vicepresidente Gustavo Noboa, a quien correspondía la sucesión conforme a la Constitución, asumió la Presidencia y estableció en abril un acuerdo con el FMI (Fondo Monetario Internacional) para acceder a créditos por un valor cercano a los 800 millones de dólares para continuar y fortalecer la dolarización, aplicando medidas de ajuste en diversos sectores de la economía. Además, centró sus esfuerzos en la construcción de un gran oleoducto de crudos pesados (OCP) desde la Amazonia hasta la costa del Océano Pacífico, para que la exportación de crudo se duplique a partir de 2003. El coronel retirado Lucio Gutiérrez ganó las elecciones de noviembre de 2002 al frente del Partido Sociedad Patriótica (PSP), una agrupación populista de centro-izquierda, que actuó en alianza con movimientos indígenas y de extrema izquierda. Gutiérrez obtuvo el 55 % de los votos en la segunda vuelta electoral. Fue destituido por el Congreso en abril de 2005, en medio de revueltas en Quito (a cuyos participantes Gutiérrez denostó como «forajidos», en la llamada «rebelión de los forajidos»), sucediéndose en el cargo el vicepresidente Alfredo Palacio, quien hasta entonces tenía poca figuración en el plano político.

En noviembre de 2006, Rafael Correa fue elegido presidente para el período 2007-2011. El margen electoral fue el tercero más alto en el actual período constitucional y democrático (1979-2007), superado únicamente por las elecciones de Jaime Roldós (1979) y Sixto Durán Ballén (1992). El 15 de abril de 2007 se eligió a la Asamblea Constituyente, la que redactó una nueva Carta Magna, vigente desde octubre de 2008. Debido a la vigencia de una nueva Constitución, se tuvo que llamar a elecciones generales para designar a las autoridades, siendo así como el presidente Correa en 2009 fue reelegido en su cargo, pese al escándalo mediático que se generó por denuncias de peculado y blanqueo de capitales, que involucraban a miembros de su gabinete ministerial de deporte. Dicho cargo lo desempeñó desde el 10 de agosto del mismo año hasta el 24 de mayo de 2013 con una nueva victoria en las urnas, esta vez en medio del escándalo de falsas afiliaciones a partidos políticos y que involucraban a toda la clase política nacional. Manteniéndose así, en su puesto como presidente hasta el 2017.
El 30 de septiembre de 2010 se realizó una paralización de actividades por una parte de varios miembros Policía Nacional del Ecuador y la Fuerza Aérea Ecuatoriana. Sin embargo el resto de las Fuerzas Armadas de Ecuador, apoyaron al régimen actual. La crisis fue declarada por el gobierno como intento de golpe de Estado y fue superada al final del mismo día, con la salida del presidente Correa del Hospital de la Policía Nacional rescatado por el Ejército de Ecuador y el GOE (Grupo de Operaciones Especiales de la Policía), frustrando las intenciones de los amotinados. El incidente resultó con un saldo de 5 muertos y 274 heridos.
La elección presidencial del 17 de febrero de 2013 dio como resultado la reelección de Rafael Correa con su binomio Jorge Glas con el 57,17 % de votos válidos, frente a su inmediato contrincante, el candidato de derecha Guillermo Lasso con el 22,66 % de votos válidos, Los demás candidatos no obtuvieron una votación que supere al 7 % en ningún caso.

En elecciones parlamentarias, el movimiento de izquierda Alianza PAÍS dirigido por Rafael Correa obtuvo 100 de los 137 escaños legislativos, que junto a su aliado movimiento político AVANZA que obtuvo 5 escaños, deja a la oposición liderada por el movimiento político de derecha CREO siendo Guillermo Lasso su principal figura, con 11 escaños ganados, junto a su aliado Partido Social Cristiano que obtuvo 6 escaños; los restantes 15 escaños se reparten entre otras tendencias no aliadas ideológicamente a ninguno de los anteriores. Esta asamblea tomó posesión el 15 de mayo de 2013, mientras que el binomio presidencial lo hizo el 24 de mayo de 2013.

"Artículo principal:" Manifestaciones en Ecuador de 2015

En Agosto de 2015, partió una marcha indígena desde la provincia de Zamora Chinchipe hacia la ciudad de Quito que culminó con varios días de protesta en la capital. La motivación fue el descontento de un sector de la población ante proyectos de medidas económicas como la ley de plusvalía y la ley de herencias, así como enmiendas a la constitución que permitirían la reelección indefinida en cargos de elección popular. El saldo fue de 58 detenidos y 106 heridos, entre policías y civiles.

Para 2016 el índice de aprobación al mandato de Rafael Correa se había mantenido, esto después de las políticas económicas y sociales impulsadas por el gobierno por la crisis económica mundial y la devaluación del petróleo, medidas tributarias y de aranceles para evitar la salida de divisas del país. Esto provocó un incremento mínimo en la inflación a nivel nacional, pero especialmente mayor en provincias fronterizas, además los índices de desempleo aumentaron levemente y el subempleo de igual forma. Sin embargo casi a la par del malestar de una parte de la población con el oficialismo surgieron movimientos de apoyo hacia el gobierno e incluso se creó un movimiento que buscaba la reelección indefinida, que recolectó las suficientes firmas para una consulta popular, pero el mismo presidente rechazó esta intención.
Tras el terremoto de Ecuador de 2016, sucedido el 16 de abril, el gobierno estableció algunas medidas económica que fueron aprobadas por la Asamblea Nacional, como el alza por un año de dos puntos al IVA (del 12% al 14%), la donación obligatoria de un día de sueldo por parte de los ciudadanos empleados que generaran ingresos mayores a $1000, y donaciones exclusivas en una ocasión a personas que posean un capital mayor a un millón de dólares. Especial revuelo mediático causó la negativa de 63 asambleístas de donar el 10% de sus salarios para las víctimas del terremoto, en su mayoría del bloque de Alianza País.

Varias consecuencias se desataron desde el inicio de la crisis económica, pero la principal fue la salida de capital del país a paraísos fiscales y la salida de divisas en la frontera colombo-ecuatoriana, pues en las provincias de la frontera norte existe un cruce comercial a ciudades como Ipiales y Tumaco donde el peso colombiano se encuentra devaluado ampliamente en relación al dólar.

Para 2017 el gobierno de Ecuador y otros entes públicos como el Municipio de Quito, se han visto envueltos en una maraña de acusaciones, que fueron desde el caso Odebrecht, donde personas ligadas al municipio de Quito y el vicepresidente de la nación, Jorge Glas dirigió el ministerio encargado de obras públicas y sectores estratégicos durante el tiempo en que Odebrecht afirma haber depositado sobornos para contratación de obras; también han existido quejas de grupos opositores al régimen por la supuesta falta de licitación y el supuesto otorgamiento de obras a dedo por parte del oficialismo en megaproyectos como la Refinería del Pacífico, además se asegura de la sobrevaluación de los proyectos como las hidroeléctricas en la Sierra y las carreteras y proyectos urbanos. Esto en medio de también desavenencias y divergencias con algunos sectores sociales y políticos de Ecuador por el apoyo del gobierno ecuatoriano al régimen de Nicolás Maduro en Venezuela, el otorgamiento de la Condecoración Manuela Sáenz por la Asamblea Nacional a Cristina Fernández de Kirchner mientras se halla enjuiciada por 419 denuncias de corrupción en Argentina, así como la expulsión de la opositora venezolana Lilian Tintori del país al querer inmiscuirse en la campaña política del Ecuador; todo esto en medio de una campaña electoral rumbo al balotaje para definir al nuevo presidente de Ecuador.

El 19 de febrero de 2017 se llevó a cabo la elección presidencial para designar al presidente y vicepresidente para periodo 2017-2021, los candidatos Lenín Moreno con 39.36% y Guillermo Lasso con 28.09% pasaron a balotaje ya que no se alcanzó el 40% sobre el 10% de ventaja al candidato en segundo lugar. En la primera vuelta también se efectuaron las elecciones legislativas en que se renovaron a los representantes al Parlamento Andino y los Asambleístas para el mismo período, en las cuales, el Alianza País perdió 26 curules; CREO ganó 24 curules más, PSC ganó 9 curules más. Además se realizó una consulta popular sobre la opinión de los ecuatorianos en el tema de los funcionarios públicos que tengan cuentas y empresas en paraísos fiscales.

El balotaje se realizó el 2 de abril del mismo año, en esta elección Lenín Moreno, candidato del movimiento oficialista Alianza PAIS, resultó presidente electo ganando la contienda con el 51.16% de los votos contra Guillermo Lasso, candidato de la alianza del Movimiento CREO y el Movimiento SUMA, quien obtuvo el 48.84%. El presidente electo tomó posesión de sus funciones el 24 de mayo de 2017.

El actual Estado ecuatoriano está conformado por cinco funciones estatales: Función Ejecutiva, Función Legislativa, Función Judicial, Función Electoral y Función de Transparencia y Control Social.

La Función Legislativa se ejerce por la Asamblea Nacional únicamente, que tiene su sede en la ciudad de Quito en el "Palacio Legislativo", y está conformada por 137 asambleístas, repartidos en diez comisiones, elegidos para un período de cuatro años. Quince asambleístas elegidos en circunscripción nacional, dos asambleístas elegidos por cada provincia, y uno más por cada doscientos mil habitantes o fracción que supere los ciento cincuenta mil, de acuerdo al último censo nacional de la población. Ajeno a lo anterior, la ley determinará la elección de asambleístas de regiones, de distritos metropolitanos, y además de la circunscripción del exterior.

La Función Ejecutiva está delegada al Presidente de la República, actualmente ejercida por Lenín Moreno. Está acompañado de su Vicepresidenta, actualmente Maria Alejandra Vicuña, elegido para un periodo de tres años (cuatro, normalmente). Es el Jefe de Estado y de Gobierno, es responsable de la administración pública.

El Presidente nombra a Secretarios Nacionales, Ministros coordinadores, Ministros de Estado y Servidores públicos. Define la política exterior, designa al Canciller de la República, así como también embajadores y cónsules. Ejerce la máxima autoridad sobre las Fuerzas Armadas de Ecuador y la Policía Nacional de Ecuador, nombrando a sus autoridades, Según la Constitución de la república. Son atribuciones y deberes de la Presidenta o Presidente de la República, además de los que determine la ley: Ejercer la máxima autoridad de las Fuerzas Armadas y de la Policía Nacional y designar a los integrantes del alto mando militar y policial.

La Función Judicial del País está conformada por el Consejo de la Judicatura como su ente principal y por Corte Nacional de Justicia, las Cortes Provinciales, los juzgados y tribunales, y los juzgados de paz. La representación jurídica la hace el Consejo de la Judicatura, sin perjuicio de la representación institucional que tiene la Corte Nacional de Justicia.

La Corte Nacional de Justicia está integrada por 21 jueces elegidos para un término de nueve años. Serán renovados por tercios cada tres años, conforme lo estipulado en el Código Orgánico de la Función Judicial. Estos son elegidos por el Consejo de la Judicatura conforme a un procedimiento de oposición y méritos. No son susceptibles de reelección.

Como organismos independientes de la Función Judicial están la Fiscalía General del Estado y la Defensoría Pública. Como organismos auxiliares están: el servicio notarial, los martilladores judiciales y los depositarios judiciales. Igualmente hay un régimen especial de justicia indígena.

La Constitución de 2008 elevó al rango de función del Estado a la institucionalidad electoral, cuyo mandato es garantizar el ejercicio de los derechos políticos de la ciudadanía y promover el fortalecimiento de la democracia, mediante la organización de procesos electorales y el apoyo a las organizaciones políticas y sociales; asegurando una participación equitativa, igualitaria, paritaria, intercultural, libre, democrática y justa para elegir y ser elegidos.

Constitucionalmente, la Función Electoral está conformada por dos órganos separados, bajo principios de autonomía e independencia: el Consejo Nacional Electoral, que administra y ejecuta los procesos electorales; y, el Tribunal Contencioso Electoral, encargado de la administración de la justicia electoral.

La Función Electoral, tiene su sede en la ciudad de Quito, el Consejo Nacional Electoral está conformado por 5 consejeros y el Tribunal Contencioso Electoral por 5 jueces, seleccionados en ambos casos a través de concurso público de oposición y méritos, ejerciendo sus funciones por 6 años con renovaciones parciales cada 3 años.

La Función de Transparencia y Control Social está conformada por: Consejo de Participación Ciudadana y Control Social, Defensoría del Pueblo, Contraloría General del Estado, y las Superintendencias. Sus autoridades ejercerán sus puestos durante cinco años. Este poder se encarga de promover planes de transparencia y control público, así como también planes para diseñar mecanismos para combatir la corrupción, como también designar a ciertas autoridades del país, y ser el mecanismo regulador de rendición de cuentas del país.

El territorio del Ecuador se divide en: Parroquias (urbanas o rurales), las cuales conforman los Cantones, estos las Provincias, y estas a su vez las Regiones Administrativas. Cada una de estas entidades y los Distritos Metropolitanos tienen un Gobierno Autónomo Descentralizado, encargado de ejecutar políticas dentro de su ámbito.

Las parroquias son las divisiones de cuarto nivel en Ecuador, siendo más de un millar en total. Son entidades similares a los municipios o comunas en otros países, diferenciadas a su vez en urbanas y rurales. Las parroquias están en manos de un Gobierno o Junta Parroquial de cinco vocales elegidos por sufragio universal, que es presidida por el vocal que alcanza la votación más alta, llamado Presidente de la Junta Parroquial.Entre ellas son:

Los cantones son las unidades territoriales de tercer nivel en Ecuador, siendo 221 en total. Al frente de estos existe un Gobierno Municipal, compuesto por un Alcalde y un Concejo integrado por concejales urbanos y rurales, electos todos por sufragio universal.

La República del Ecuador se divide en 24 provincias que son las unidades territoriales de segundo nivel. Las provincias eligen un Prefecto y Viceprefecto Provincial, quienes ejercen el gobierno local junto con un Gobierno Provincial integrado por todos los alcaldes de los cantones que componen la provincia.
La SENPLADES (Secretaría Nacional de Planificación y Desarrollo del Ecuador) conformó distintos niveles administrativos de planificación: zonas, distritos y circuitos a nivel nacional; que permitirán una mejor identificación de necesidades y soluciones efectivas para la prestación de servicios públicos en el territorio. Esta conformación no implica eliminar las provincias, cantones o parroquias.

Las zonas están conformadas por provincias, de acuerdo a una proximidad geográfica, cultural y económica. Hay siete zonas de planificación, dos distritos metropolitanos y el Régimen especial de Galápagos. Cada zona está constituida por distritos y estos a su vez por circuitos. Desde este nivel se coordina estratégicamente las entidades del sector público, a través de la gestión de la planificación para el diseño de políticas en el área de su jurisdicción.

El distrito es la unidad básica de planificación y prestación de servicios públicos. Coincide con el cantón o unión de cantones. Se han conformado 140 distritos en el país. Cada distrito tiene un promedio de 90 000 habitantes. Sin embargo, para cantones cuya población es muy alta como Quito, Guayaquil, Cuenca, Ambato y Santo Domingo se establecen distritos dentro de ellos.

El circuito es la localidad donde el conjunto de servicios públicos de calidad están al alcance de la ciudadanía, está conformada por la presencia de varios establecimientos en un territorio dentro de un distrito. Corresponde a una parroquia o conjunto de parroquias, existen 1134 circuitos con un promedio de 11 000 habitantes.

Las zonas, distritos y circuitos son niveles desconcentrados para la administración y planificación de los servicios públicos de algunos ministerios de la Función Ejecutiva. Fueron conformados respetando la división política administrativa, es decir corresponde a una nueva forma de planificación en el territorio más no a nuevos niveles de gobierno. Por lo tanto, los niveles de gobierno conservan autonomía y gobernabilidad a nivel de las provincias, cantones y parroquias.

Los niveles de planificación buscan contar con una oferta ideal de servicios en el territorio sustentado en un Estado planificado, desconcentrado, articulado, equitativo, con mayor cobertura y calidad de servicios públicos., compuestas por dos o más provincias contiguas, con el fin de descentralizar las funciones administrativas de la capital, Quito.
A la misma vez que por constitución se intenta llevar a las mismas a un sistema de autonomías mediante la elección por sufragio universal de Gobernadores Regionales y un cuerpo de Consejeros, con el objeto de atender políticas de desarrollo complementario entre provincias, enfocado a áreas turísticas, de inversión, comercio, etc. En Ecuador existen siete zonas, conformadas cada una por las siguientes provincias:

Además:
8: Distrito Metropolitano de Guayaquil: 
9: Distrito Metropolitano de Quito: 
10: Régimen Especial de Galápagos

La defensa del país está a cargo de las Fuerzas Armadas del Ecuador que son parte de la fuerza pública y responsable de la integridad y la soberanía del territorio nacional cuentan con un número de 167.910 efectivos activos y 185.000 en reservas dentro de las Fuerzas Terrestres, la Armada y a la Fuerza Aérea .Poseen armamento comprado a Reino Unido, Chile, Francia, Estados Unidos, Sudáfrica y Brasil.

Se trata de la participación en el sector social y el desarrollo económico del país y la prestación de asistencia en el mantenimiento del orden interno. Sus tareas incluyen la lucha contra el crimen organizado, tráfico de drogas, y operaciones de lucha contra la migración ilegal. Aplica programas de desarrollo social como la disposición de los profesores para las escuelas rurales a través de un acuerdo con el Ministerio de Educación.

La protección ambiental es también una prioridad, se implementaron varios programas como: Nacional de Forestación y Ornato, Lonely Tree, Vigilancia Verde, Fire Plan Ecuador Forestal y Reserva Militar Arenillas. Las Fuerzas Armadas son una parte esencial de la infraestructura de los países y por lo tanto muy apreciada por la sociedad. El territorio ecuatoriano está dividido en cinco regiones militares denominadas «Fuerzas de Tarea Conjunta», cuatro en el territorio continental, y el quinto es la Base Naval de la zona insular (incluida las Islas Galápagos). Los Territorios de ultramar incluyen también la Base Pedro Vicente Maldonado en la Antártica.

Además el Ecuador es el país que más gasta en defensa en relación a su PIB de entre todos los países de la UNASUR, destinando el 2,74 % de su PIB a las Fuerzas Armadas. El ministro de Defensa, Fernando Cordero, justificó esto diciendo que el gasto se dirige al mantenimiento del personal.

Ecuador se encuentra sobre la línea ecuatorial terrestre por lo cual su territorio se encuentra en ambos hemisferios. Comprende dos espacios distantes entre sí: el territorio continental al noroeste de América del Sur con algunas islas adyacentes a la costa y, el archipiélago o provincia insular de Galápagos, que se encuentra a 1000 kilómetros de distancia del litoral ecuatoriano en el Océano Pacífico.

Las principales unidades del relieve ecuatoriano son la llanura costera al norte del Golfo de Guayaquil, la sección de la Cordillera de los Andes en el centro del país y un extenso sector de la llanura amazónica ubicado al oriente del país.

Hacia el suroeste se ubica el golfo de Guayaquil, donde desemboca el río Guayas en el Océano Pacífico. Muy cerca de Quito, la capital, sobre la Cordillera de los Andes, se alza el Cotopaxi, el volcán activo más alto del mundo.

El punto más alto del Ecuador es el volcán Chimborazo, con 6315 msnm y cuya cima es el lugar más lejano al núcleo de la tierra debido a la silueta elíptica del planeta. En la zona estatal de Ecuador hay 22 de más de 3.500 metros de altitud que están todas en los Andes. Se extienden sobre las tres cadenas montañosas Cordillera Central, Cordillera Occidental e Interandino y 9 de ellas son volcanes activos o potencialmente activos. 

Ecuador es el país con más ríos por metro cuadrado del mundo. La cordillera andina es el divortium aquarum entre la cuenca hidrográfica del río Amazonas, que discurre hacia el este, y del Pacífico, que incluye de norte a sur los ríos: Mataje, Santiago, Esmeraldas, Chone, Guayas, Jubones y Puyango-Tumbes.

El país tiene un clima muy variado, pues aunque su posición latitudinal le propicia características tropicales, aquel tipo de clima solo es apreciable en un tercio del territorio (la Costa Norte en las provincias de Esmeraldas, Santo Domingo y Los Ríos; y la Región Amazónica), en los dos tercios del país restantes existen otros climas definidos, como el subtropical templado húmedo y seco, el continental subtropical, el mediterráneo, tropical de tierras altas, tropical de sabana, de montaña, bioma oceánico y desértico, la calidad del aire es muy buena por la presencia de grandes bosques naturales, parques nacionales y la selva amazónica, el 20 % del país es reserva ecológica.
Debido a la presencia de la cordillera de los Andes y según la influencia del mar, el Ecuador continental se halla climatológicamente fragmentado en diversos sectores. Además, a causa de su ubicación ecuatorial, cada zona climática presenta solo dos estaciones definidas: la húmeda y la seca, llamadas erróneamente «invierno» y «verano» respectivamente, al igual que ocurre en otras regiones del globo donde por sus emplazamientos próximos a la línea ecuatorial, no ocurren verdaderos inviernos y veranos.

Tanto en la Costa como en el Oriente del país, la temperatura oscila entre los 20 °C y 33 °C, mientras que en la sierra, esta suele estar entre los 3 °C y 26 °C por la altura de las ciudades. La estación húmeda se extiende entre diciembre y mayo en la costa, entre noviembre a abril en la sierra y de enero a septiembre en la Amazonía. Las islas Galápagos tienen un clima más bien templado y su temperatura oscila entre 17°C y 32°C, aproximadamente.
Estas estaciones húmedas y secas causan en cada región del país diferentes estaciones climáticas. Son muy variables las temperaturas por la altura de la sierra, la región amazónica, la costa del país y la región insular.

Así, de enero a marzo es principalmente estación seca, con la mayor temporada de playa en toda la región litoral o costa ecuatoriana, así como en la amazonía; en esos mismos meses en la sierra es temporada húmeda, con la mayoría de días nublados y frescos.

Del modo contrario, de julio a septiembre en la amazonía y en la región costa o litoral, es temporada húmeda, si bien algunas playas de clima más moderado siguen siendo disfrutadas (mayormente en la provincia de Esmeraldas) por los turistas, otras son claramente frías (como Salinas) en comparación con otras épocas del año y también reciben turistas de la sierra y países vecinos. En la sierra, en esos mismos meses el país tiene una estación seca, con días calurosos y mucho sol.

La capital del país, Quito posee temperaturas primaverales casi todo el año, aunque durante la estación húmeda (de Octubre a Marzo) las temperaturas suelen bajar incluso hasta los 0°C. Un extremo opuesto es la populosa ciudad de Guayaquil donde las temperaturas suelen incluso llegar a los 40°C durante la época seca (en especial en Julio y Agosto).

Debido al calentamiento global fenómenos poco usuales en estas latitudes se han presentado con mayor frecuencia, por ejemplo han existido nevadas recurrentes en Papallacta, provincia del Napo ubicada a cerca de 3.000 m.s.n.m.; Aunque son algo usuales las olas de frío en la Sierra durante la época lluviosa a últimas fechas se han exprimentado temperaturas bajo cero por tiempo prolongado en ciudades como Quito, Ambato, Cuenca, Riobamba y Latacunga.

Ecuador es uno de los países mejor preservados ambientalmente en el mundo, además su situación geográfica y latitudinal, así como su condición climática y orográfica moldearon al Ecuador como el país con más especies animales de la región, y uno de los 10 países con mayor endemismo a nivel mundial. Para comprender su fauna debemos recalcar que Ecuador alberga cuatro regiones que marcan mucho la faunística, así por ejemplo la región amazónica concentra especies únicas de anfibios, mamíferos como monos capuchinos, mono aulladores, monos araña, monos barbudos, pantera negra, osos, guantas, jaguares, tigrillos, ocelotes, pumas, capibaras, tapir, y otros; así como una infinidad de reptiles como caimanes, cocodrilos, boas, anacondas, serpientes venenosas, entre otras, así como aves tales como el tucán, ibis, pericos, guacamayos, aves canoras, águila, cóndor.

Ecuador posee una rica fauna y flora por lo que se encuentra dentro de la lista de países megadiversos. En efecto, el bioma de selva o bosque tropical se extiende por la mayor parte de su territorio, mientras que en el occidente, adyacente a la costa, se encuentra también el bioma del bosque seco y de los manglares. La fauna del Ecuador es muy extensa con una gran variación de especies e innumerables tipos de especies tropicales como los guacamayos, tucanes, tortugas, ranas, etc.

Al norte de la provincia de Esmeraldas en un lugar conocido como Majagual, se encuentran los manglares más altos del mundo.

En las alturas cordilleranas, se hallan dispersos además los bosques y los páramos andinos. El occidente forma parte del Chocó biogeográfico y el Oriente, de la Amazonia.

Las islas Galápagos poseen una gran variedad de especies endémicas, las cuales en su momento fueron estudiadas por el célebre naturalista inglés Charles Darwin, lo cual le permitió desarrollar su teoría de la evolución por selección natural. Las islas han ganado fama a nivel mundial debido a la particularidad de su fauna, especialmente de las tortugas conocidas como «Galápagos».

En 1986 el mar que rodea a las islas fue declarado reserva marina. Unesco incluyó a Galápagos en la lista de Patrimonio de la Humanidad en 1978, y en diciembre de 2001 se amplió esta declaración para la reserva marina.

Está además, el Parque nacional Yasuní; el término Yasuní, sin conocer su origen lingüístico, significa «tierra sagrada» como es interpretado de manera general por comunidades de la zona; el parque se extiende sobre un área de 9820 kilómetros cuadrados en las provincias de Pastaza y Orellana entre el río Napo y el río Curaray en plena cuenca amazónica a unos 250 kilómetros al sureste de Quito.

El parque, fundamentalmente selvático, fue designado por la Unesco en 1989 como una reserva de la biosfera y es parte del territorio donde se encuentra ubicado el pueblo Huaorani y los tagaeri y taromenane, grupos no contactados.

Según un reciente estudio, el Parque nacional Yasuní y la zona ampliada subyacente se considera la zona más biodiversa del planeta por su riqueza en anfibios, aves, mamíferos y plantas. Este parque cuenta con más especies de animales por hectárea que toda Europa junta.

El Chimborazo es el volcán y montaña más alta de Ecuador y el punto más alejado del centro de la Tierra, es decir el punto más cercano al espacio exterior, razón por la cual es llamado como «el punto más cercano al Sol», debido a que el diámetro terrestre en la latitud ecuatorial es mayor que en la latitud del Everest (aproximadamente 28º al norte). Su última erupción conocida se cree que se produjo alrededor del 550 dC. Está situado en los Andes centrales, 150 km al sudoeste de Quito y 20 km al noreste de Riobamba.

Los datos generados por el INEC informan que para febrero de (2015) habitan 16.178.162 personas en Ecuador. Ecuador es el sexto país con más aceleración de envejecimiento de Latinoamérica tras Costa Rica, Chile, Argentina, Cuba y Uruguay, después de que censos en 2010 arrojaran resultados de que más del 17 % de la población pasa de los 65 años, y más del 40 % de la población es adulta-media, y de que la edad promedio sea de entre los 27,3 y los 35,8 años, aunque todavía es un país con una población joven.

La esperanza de vida en Ecuador bordea los 73,5 años para los varones y los 79 años para las mujeres, en 2011 Ecuador fue el cuarto país con más longevos de América tras Chile, Costa Rica y Canadá, y por delante de Estados Unidos y Argentina, siendo especial lo ocurrido en el Valle de Vilcabamba ubicado en la provincia de Loja a tan solo 30 km de la ciudad de Loja ; En Ecuador hay 100 niñas nacidas en relación a los 93 niños. Así mismo, en lo referente al sexo de la población adulta, se puede establecer que alrededor del 49,4 % se encuentra compuesta por hombres, y un 50,6 % por mujeres. Estas cifras varían aún más a favor de las mujeres en las provincias de la sierra central ecuatoriana.

El 63 % de la población reside en zonas urbanas y el 37 %, en rurales. Cabe destacar que, dada la alta densidad poblacional y su extensión territorial reducida, la concentración de poblados es alta, por lo que las ciudades y poblados rurales se encuentran muy cerca unos de otros. La población rural ecuatoriana es la tercera población rural con mejor calidad de vida de Latinoamérica tras Uruguay y Cuba, se evidencia que la población ecuatoriana esta mayormente radicada en la zonas urbanas del Ecuador, el 44 % de la población urbana está radicada en las 15 ciudades más grandes del país de las cuales Guayaquil y Quito bordean el 60 % de la población urbana, eso resulta por factores como la expansión urbana a centros poblados rurales y el mejoramiento de estándares de vida en el sector rural en la última década, donde se ha dotado de infraestructura de calidad en centros de salud, educación, vialidad, mejoramiento de producción agro industrial, servicios básicos, bajo costo de vida, etc. La migración de población rural a las zonas urbanas es un fenómeno retroactivo de Ecuador a la tendencia sufrida en el resto de Latinoamérica.

La natalidad del país bajó recientemente a 1,8 hijos por mujer en el censo de 2010, con lo que se determinó una tendencia a un acelerado envejecimiento de la población, típico comportamiento de un país con una economía creciente. Aun con estos datos del INEC, Ecuador es el séptimo país más poblado de Sudamérica, y el octavo más poblado de 33 países en Latinoamérica. Es el país más densamente poblado en Sudamérica con 54 habitantes por kilómetro cuadrado.

Quito, ciudad capital que cuenta con 1.607.734 habitantes en perímetro urbano y 2.385.111 habitantes en su área metropolitana, siendo la sede de gobierno donde se concentran los poderes del estado, es la primera ciudad en ser declarada patrimonio de la Humanidad por la Unesco, porque también es el centro cultural del país. En 2018, será el cantón más poblado del país.

Guayaquil, la más poblada de Ecuador, con 2.278.691; habitantes en su área urbana y 3.113.725 en su área metropolitana. Polo de concentración de las ciudades del centro del país, es además, el puerto principal de Ecuador por donde se comercializan aproximadamente el 70 % de las importaciones y exportaciones del país.

Según los proyecciones entregadas por el INEC para 2013, uno de cada tres ecuatorianos vive en Quito o Guayaquil; ambas ciudades sumadas engloban una población de más de 5,8 millones de habitantes. Otras ciudades importantes son Cuenca, Machala, Santo Domingo, Ambato, Portoviejo, Manta y Loja.

El Censo del 2010 cuestionó a los ecuatorianos mayores de 15 años sobre su autoidentificación, dando como resultado un 71.9 % de personas que se identificaron como mestizas, 7.4 % montubias, 7.8 % afroecuatorianas, 7.1 % indígenas, 7.0 % blancas y un 0.4 % en otras. Esto presentó un cambio frente a lo visto en 2001, en el cual se dieron los datos que siguen: mestizo 77.4 %, indígena 6.8 %, afrodescendiente 5 %, blanco 10.5 % y otros 0.3 %.,

Los estudios genéticos que se han realizado por la Universidad de las Américas y conducida por un grupo de expertos genetistas graduados en la Universidad Autónoma de Madrid sobre la población ecuatoriana han mostrado una composición algo típica para el norte de Sudamérica, dividiendo a la población en tres grandes grupos: mestizos, afrodescendientes y nativos, se determinó la presencia de los tres componentes en los tres grupos antes mencionados, variando únicamente la proporción de la misma. El estudio también reveló las posibles anomalías, ventajas y predisposiciones que existen en los genes de la mayor parte de los ecuatorianos, entre ellas un alto grado de inmunidad a la malaria en la población afro, e índices de alta tolerancia a la lactosa en mestizos. La población eurodescendiente no entró en el análisis suponiendo que su genética es netamente caucásica.

El idioma oficial es el castellano con sus peculiaridades y modismos propios de cada zona o región. Junto al kichwa y el shuar son idiomas oficiales de relación intercultural según lo establece la constitución del Ecuador en el artículo 2 del capítulo primero de los elementos constitutivos del estado. El wao terero, tsáfiqui y «demás idiomas ancestrales son de uso oficial para los pueblos indígenas, en los términos que fija la ley».

Según el censo de 2001, el 94 % de la población habla español, el 4,8 % habla alguna lengua nativa conjuntamente con el español y el 1,1 % hablan solo una lengua nativa. De las 13 lenguas nativas que fueron contabilizadas por el mencionado Censo, el quichua, hablado por el 4,1 % de la población, es la más difundida. La segunda lengua nativa es el shuar, hablado por el 0,55 % de la población. En la mayoría de los institutos públicos del país se imparte el inglés de una manera neutral.
El español es el idioma que se habla mayoritariamente en el Ecuador, a pesar de que este tiene diferencias dependiendo de muchos factores, el más importante de estos factores es la región. En el país hay tres variantes principales que son:

A pesar de que estas sean las principales variantes del español en el país, hay muchos otros factores que influyen en el habla de una persona, siendo estos la etnia, la clase social o si se habita en el campo o la ciudad. Debido a que la costa y la región andina son las dos regiones más habitadas del país, sus dialectos son los más relevantes del país, ambos muy diferentes el uno del otro. Existen muchos modismos propios de cada provincia o región, así como otros que son entendidos y utilizados en todo el país.

Esta variante del español se encuentra clasificada dentro del dialecto del español ecuatorial, el cual se extiende desde las zonas costeras del Pacífico sur colombiano hasta la norte costa del Perú, cruzando el litoral ecuatoriano. El centro lingüístico influyente de esta región dialectal es la ciudad portuaria de Guayaquil.

La característica más destacable en este, es la aspiración de la letra "s" al final de las palabras o cuando va precedida de otra consonante, siendo muchas veces pronunciada como "j "suave o como una "h" inglesa [h].

Esta variante del español hablada a lo largo de la región costeña del país, así como las planicies colindantes al oeste de la cordillera de los Andes, representa una zona de transición entre las variedades caribeñas del norte de Colombia y Venezuela, y los dialectos ribereños del centro y sur del Perú; por lo que existen ciertas características compartidas con ambos que, a oídos de un foráneo, hacen difícil la identificación del dialecto costeño ecuatoriano.

Así, los dialectos de la Costa rigen y fijan el "foco fonemático de transición tonal-acentual" del español americano que se expande geográficamente desde la "entonación semigrave caribeña" y "mesoamericana" al norte –ya que la gravedad concreta la poseen las variantes del castellano peninsular o europeo– hacia la "intensa agudeza" localizadas al sur, propia de las tonalidades peruanas, chilenas y bolivianas.

Además, la variante ha incorporado dentro de su léxico, una serie de palabras compartidas con el resto de variedades dialectales del Ecuador, y que son entendidas solo dentro del país. En su mayoría son palabras provenientes del Español andino del Ecuador, con influencias del quichua, a pesar de que el idioma quechua no tuvo presencia histórica en el litoral ecuatoriano. Este es el caso de la palabra "ñaño" (hermano), muy extendida en todo el país y con origen quichua.

Uno de los acentos más relevantes de la región es el de la ciudad de Guayaquil, la ciudad más grande del país. Por ser una urbe grande y que a lo largo de su historia ha ido creciendo demográficamente gracias a la migración, en la misma urbe existen muchas diferencias dialectuales, principalmente asociadas a la clase social, etnia y el nivel de escolarización. Entre las clases más escolarizadas, se tiende a corregir el acento hacia un castellano más estándar, así como incorporan a su léxico palabras foráneas, especialmente provenientes del inglés. En las clases menos escolarizadas, existen otras variedades dialectuales. Existe un grupo de personas que tienden a tener una entonación más parecida a la de un campesino costeño, pronunciando una "s" sorda alargada, mientras que otro grupo de personas tienden a tener una entonación más fuerte, que se suele conocer como "callejera", en la que la letra "s" es pronunciada numerosas veces como /"sh"/, además de contar con una serie de palabras propias, que no siempre son entendías por el resto de hablantes de la región.

Fuera de la ciudad de Guayaquil, se tiende a hablar un mismo dialecto en todas las provincias de la costa (con excepción de la provincia de Esmeraldas) con ligeras variaciones locales. Una variante importante es el dialecto hablado por los "montubios" (campesinos de la costa ecuatoriana), los cuales tienden a acentuar la primera sílaba de la mayoría de las palabras.

En la provincia de Manabí, existen una serie de dialectos también diferenciados que lo hacen fácilmente identificable como una variedad aparte del acento costeño ecuatoriano.

La provincia de Esmeraldas, por otro lado, presenta una notoria variante muy diferente a la del resto de la región, con un fuerte componente africano, y que se asemeja mucho al de región costeña fronteriza con Colombia. Debido a que esta región tiene una mayoría de habitantes afrodescendientes, este dialecto tiende a ser un poco más fuerte y con características propias tanto de léxico, como de entonación.

En el altiplano ecuatoriano suele hablarse una variante del español que muchos extranjeros suelen comparar con la del español chilango mexicano. En esta región encontramos cuatro principales variantes, siendo estas el español pastuso, el español andino central, el morlaco y el lojano. El español hablado en los Andes ecuatorianos tiende a tener muchos modismos tomados del idioma kichwa, idioma de los indígenas nativos de esta región. Palabras como "ñaño/a" o "taita" son utilizadas por personas de cualquier etnia o clase social en esta área -taita es un préstamo lingüístico bien acogido en los Andes; Corominas dice de Taita:"(...) es imposible suponer origen americano a esta palabra castellana heredada del latín..."-
El voseo es relativamente usado en esta parte del país, usado solo para conversaciones informales entre amigos o familiares.

En el Carchi, provincia andina fronteriza con Colombia se habla una variación muy especial, semejante a la del departamento colombiano de Nariño. Desde las provincias de Imbabura Pichincha y el Distrito Metropolitano de Quito suele hablarse un español con muchos préstamos colombianos, ibéricos, así como también muchos anglicismos; la forma de hablar aquí difiere mucho dependiendo al estrato social al que se pertenezca. Desde Cotopaxi hasta los límites de la Sierra Central (provincia de Chimborazo) se habla un español con aportes más bien procedentes del quichua y con ciertas influencias de la Costa. En la región norte, una gran variante es encontrada dentro del valle del Chota en la provincia de Imbabura, valle habitado por afrodecendientes, este dialecto es diferente al común dialecto andino, pero a su vez diferente del dialecto hablado por los afrodecendientes en la costa.

En las provincias de Cañar y Azuay se habla una variante bastante peculiar, el dialecto "morlaco". Este se caracteriza por su "cantado" o entonación característica de la zona, siendo muchas sílabas acentuadas en otras no correspondientes. Así mismo, la letra "r" tiende a ser muy arrastrada, sonando más como una "sh". Así mismo, esta zona tiene muchos modismos propios solo utilizados en esta zona, como la palabra "gara".
La provincia de Loja también tiene su variante, con una entonación bastante neutral.

Las ciudades de Quito e Ibarra tienen un caso especial, ya que el habla de estas ciudades no es compatible con los dialectos de la sierra y del resto del país, siendo enormemente similar al español del norte y centro de Chile, y con rasgos traídos por los españoles, que suman una importante parte de la población en ambas ciudades, los rasgos del español rioplatense, el voseo, ciertas donaciones quichuas y una infinidad de anglicismos es lo que representa al español de estas dos ciudades del norte del Ecuador.

En la región amazónica del país se habla una variante del español parecida al dialecto andino ecuatoriano. En el norte tiene influencia de los quijos, ellos tutean y conjugan el verbo en usted. En la provincia de Morona Santiago, se halla una entremezcla entre el español serrano indígena quichua de la sierra centro y el español morlaco (referente al de Azuay y Cañar). En la provincia de Zamora Chinchipe tiene una influencia considerable del acento lojano con una ligera variación de los dialectos nativos locales.

En las islas Galápagos se habla un dialecto sumamente parecido al de la costa del Ecuador continental. No se presentan grandes variaciones de importancia, al ser esta una región poco habitada en comparación al resto de regiones del país. También se conocen los dialectos de los grupos que existen en ese lugar.

En 2012 el Instituto Ecuatoriano de Estadística y Censos (INEC) realizó un censo (solo en las principales ciudades) en el que el 91,95 % de los encuestados respondió que tiene una religión, el 7,94 % se autodefinió como ateo y el 0,11 % se identificó como agnóstico. Dentro del grupo que profesa una religión el 80,40 % se autodefinió como católico y un 11,30 % como evangélica. Otras religiones mencionadas por los encuestados fueron: Mormonismo (1,42 %), Testigos de Jehová (1,29 %), Budismo (0,29 %) y Espiritismo (0,12 %).

Del total de personas que respondieron pertenecer a una religión el 39,7 % a su vez se consideraron como personas que creen en Dios pero no asisten a las celebraciones eucarísticas y el restante 60,3 % respondieron si ir regularmente a estas, siendo así uno de los países clave para el catolicismo en América.

Según el centro de estadística Pewforum (Estados Unidos) la afiliación religiosa en Ecuador es la siguiente:

En el año 2011 la investigación realizada por el Latinobarómetro, ha entrevistado en Ecuador 1.189 personas. (El número hace referencia a las personas que han contestado a la entrevista).
De las cuales, 1.005 se han declarado católicos, 149 afiliados a otras religiones, 33 no afiliados a una religión, 2 ateos y agnósticos.
De 1.189, 1.060 son practicantes. De los cuales, 624 son practicantes o muy practicantes.
Por lo tanto, en porcentaje, la afiliación religiosa en Ecuador es la siguiente:


Durante 1998 a 2002, Ecuador atravesó una grave crisis económica, política y financiera, la misma que fue acentuada por el fenómeno de la dolarización, que provocó que el Sucre se devaluara a niveles nunca antes vistos, provocando su desaparición y que el país adoptara como moneda válida al dólar de Estados Unidos.

Esta medida afectó directamente a los sectores más vulnerables de la sociedad, provocando el crecimiento de los niveles de pobreza e indigencia en el país, disminuyendo a niveles mínimos su poder adquisitivo, presentándose además una serie de fenómenos económicos que contrajeron la economía a nivel nacional, incrementando el desempleo en el país, ocasionando que el ingreso familiar no pueda cubrir ni la canasta básica.

Bajo este panorama desalentador, gran parte de los ecuatorianos al no tener un ingreso fijo que les permita satisfacer sus necesidades básicas, optaron por ofertar su fuerza laboral en el extranjero, puesto que en ciertos países se alcanzaban ofertas de trabajo y niveles de remuneración sensiblemente más elevados que los que se podría obtener en Ecuador, por esta causa varios países de Europa (principalmente España) y Estados Unidos, comenzaron a captar personal para realizar trabajos pesados, pero que representaban una esperanza para quienes atravesaban problemas económicos, por lo tanto pese a representar muchos esfuerzos e inclusive ingresar como ilegales a otros países, se endeudaron para viajar y arriesgaron lo poco que tenían con la finalidad de alcanzar mejores ingresos económicos que les permita cubrir las necesidades básicas de su familia, pero sobre todo con el afán de obtener una remuneración más digna, para mejorar su nivel de vida y el de sus hijos; pero en muchos casos el costo de esto fue la descomposición de familias.

Durante 2007 hasta la actualidad, el repunte de la economía a niveles de hasta el 8 % anual, los factores de la crisis económica mundial del primer mundo, el conflicto militar colombiano, reformas migratorias y políticas internacionales de integración en Ecuador, modificaron completamente el panorama migratorio, atrayendo ahora una gran cantidad de inmigrantes.

A principios del siglo XX, eran muy pocos los ecuatorianos que dejaban el país para asentarse en otras latitudes.

Entre las décadas de 1910 y 1920, debido al "boom" de la cascarilla y el caucho en el Oriente, así como por el del cacao en la Costa, los antiguos terratenientes y la clase emergente de nuevos potentados comenzaron a enviar a sus hijos a estudiar en el exterior, particularmente en Francia. Esa emigración era selectiva, eventual y dio como resultado que quienes iban a Europa o a Estados Unidos no representarán mucho en las estadísticas de esos países.

Fue a partir de la década de 1950 en que se comenzó a registrar la salida de personas de bajos ingresos económicos que viajaban principalmente a Estados Unidos para «mejorar su situación económica», por necesidad, y algunas de ingresos medios que iban «a probar suerte», más por aventura que por necesidad.

En la década de los 70, en los destinos de los emigrantes se incluyeron países como Australia y Canadá. Personas de la Sierra centro, así como de Azuay y Cañar, veían reducidos sus grupos familiares. Esta migración aún no afectaba las estadísticas locales.

Ya entrados los años 80, la movilización humana hacia el exterior comenzó a adquirir dimensiones que ya incidían en lo económico y en los social. La influencia de modas, modos y costumbres comenzó a calar sobre todo en los hogares de clases media baja y baja.

Luego de la aguda crisis económica y financiera de 1999, se estima que más de tres millones de ecuatorianos (20 % del total de la población proyectada a 2005) abandonaron el país con rumbo a diferentes destinos, dirigiéndose la mayoría hacia Estados Unidos, España e Italia (a estos tres destinos fueron como mano de obra principalmente). También hubo emigración a otros países como Venezuela (en la década de los 80 y 90), Chile (con una buena cantidad de profesionales médicos o ligados a esta área), Canadá (profesionales técnicos) y, en menor grado y por diversos motivos, hacia Israel, Bélgica, México y el Reino Unido. La emigración ha continuado a lo largo de los primeros años del siglo XXI. No se conoce con exactitud cuántos ecuatorianos han emigrado ni tampoco existen estadísticas exactas sobre el número de ellos que reside en cada país, aunque extraoficialmente se calcula que solo en España viven casi 600 000 ecuatorianos.

En enero de 2007, el Gobierno nacional creó la Secretaría Nacional del Migrante (Senami), encargada de definir la política pública sobre movilidad humana (migración, emigración, inmigración, refugio, etc.), cuyas líneas se registraron en la Constitución de 2008. Ecuador desarrolló entonces importantes temas de movilidad humana, entre los que se incluyeron varios principios de la Declaración de Derechos Humanos que habían estado soslayados. Asimismo, se creó la Red Social Virtual de las Personas Migrantes y en enero de 2017 se emitió oficialmente la Ley de Movilidad Humana que vela por los Derechos de los migrantes dentro y fuera del país. Esta última fue reconocida por el ACNUR como ejemplo de protección integral para todas las personas en movimiento, posicionado al Ecuador como referente en la promulgación de Derechos Humanos de los migrantes.
Luego de 2002 la emigración se ha ido reduciendo año tras año con la estabilidad económica y se redujo más fuertemente con el inicio de la crisis del primer mundo en 2007 y el desarrollo significativo de la economía nacional y que desde el 2010 se ha limitado prácticamente a migración por razones de becas estudiantiles y meramente turísticas o comerciales.

Pero a pesar de una larga migración desde Ecuador, este ha recibido inmigrantes durante su historia y actualmente recibe a decenas de miles de personas de diferentes países que tuvieron que abandonar sus países de origen por diferentes causas.

En un inicio, durante la colonia, la inmigración se centró en ciudadanos europeos atraídos por la explotación agrícola, luego durante inicios de la era republicana, la inmigración se reflejó en árabes cristianos y europeos (principalmente españoles, belgas, neerlandeses, italianos y franceses) atraídos debido a crisis económicas y escapando de guerras, entre otras.

Después, durante el siglo XX la inmigración fue mayormente de otros países latinoamericanos que llegaron debido a guerras civiles, así como crisis económicas, dictaduras, entre otras, principalmente argentinos, chilenos y uruguayos; así como inmigrantes que huían de la segunda guerra mundial entre otras.

En la actualidad y más a partir del año 2002, el flujo de migrantes hacia Ecuador ha tenido un acelerado e importante flujo. La inmigración se centró principalmente en colombianos que han huido por el conflicto interno que tiene ese país. Se argumenta que cerca de 350.000 colombianos viven en Ecuador.

El mejor estándar de vida y el dólar estadounidense que se valúa en más de un 40 % a 60 % que la moneda de su país de origen es otra razón para la inmigración proveniente de Cuba, Haití, Bolivia, Perú, China y en menor cantidad, de otros países americanos; que llegan al Ecuador principalmente en busca de trabajos atraídos por los puestos abandonados por los migrantes ecuatorianos y una buena bonanza económica, productiva y social que ha ofrecido el país, sobre todo a partir de 2005 con leyes que protegen a los migrantes y la participación que se les ha dado en el ámbito social y político y que tras la crisis desatada en Europa y Norteamérica, que incluso ha causado la vuelta a casa de muchas decenas de miles de ecuatorianos con facilidades para su retorno, de establecerse con un empleo y condiciones adecuadas para su residencia.

Algunas personalidades extranjeras de televisión y deportes se establecieron firmemente debido al alto reconocimiento que han recibido por parte de los ecuatorianos en sus áreas de desenvolvimiento que ha merecido en muchos de los casos a solicitar naturalizaciones, conformar familias y crear inversiones de trabajo en el país. A partir del 2007 se aplicó una política para el retorno de migrantes ecuatorianos que establecía facilidades para que estos regresen al país con todo su mobiliario, materiales y accesorios de trabajo y hasta un vehículo familiar con exoneraciones totales de impuestos y aduanas. Así como accesibilidad a vivienda y créditos financieros, llevados de la mano con un auge económico y de posibilidades de empleo, lo que produjo un importante flujo de retorno de nacionales que cada vez ha aumentado considerablemente y que se acentuó aún más a raíz de la crisis económica que soporta Europa y Norteamérica.
De la misma manera varias políticas importantes en inmigración enfocada, resulta de atraer profesionales de calidad, con fines de investigación para universidades y escuelas politécnicas, cobertura en escasez y mejora en docencia universitaria, especialidades médicas para hospitales públicos, etc. que ha representado un fuerte flujo con atractivos incentivos que ha ayudado no solo al retorno de profesionales ecuatorianos de calidad, sino a la llegada de extranjeros para los mismos.

El último grupo de inmigrantes que denota en ser un reflejo de un alto valor de Ecuador a nivel internacional recae en los europeos, norteamericanos y demás latinoamericanos que han llegado al país por motivos de inversión y comodidad, atraídos por los paisajes naturales y la variedad de climas y especies, así estos han establecido sus residencias en sitios turísticos como localidades amazónicas; la ciudad de Baños de Agua Santa en la provincia de Tungurahua; ciudades como Cuenca, Guayaquil, Quito, Loja, Bahía de Caráquez, Salinas, etc; las ciudades, pueblos y villas en Imbabura (Otavalo, Cotacachi e Ibarra); balnearios de la Ruta del Sol y de la Ruta del Spondyllus, las islas Galápagos, parques nacionales entre otras.

Muchos también lo han elegido como destino de retiro para personas de la tercera edad y tratamientos médicos debido a las ventajas económicas y el coste que representa vivir en Ecuador frente al que tocarían afrontar en sus países de origen, además de la tranquilidad, accesibilidad y comodidad que este país ofrece, calificado en su excelencia por varios análisis, estudios y reportajes internacionales que lo ubican en primer lugar en esta categoría, por ello solo en Cotacachi, para el 2007 más de 13 000 jubilados y veteranos alemanes y suizos llegaron a quedarse definitivamente.

La economía de Ecuador es la de América Latina y experimentó un crecimiento promedio del 4,6 % entre 2000 y 2006. En enero de 2009, el Banco Central del Ecuador (BCE) situó la previsión de crecimiento de 2010 en un 6,88 %. El PIB se duplicó entre 1999 y 2007, alcanzando los 65.490 millones de dólares según el BCE. La inflación al consumidor hasta enero de 2008 estuvo situada alrededor del 1,14 %, el más alto registrado en el último año, según el INEC. La tasa mensual de desempleo se mantuvo en alrededor de 6 y 8 por ciento desde diciembre de 2007 hasta septiembre de 2008, sin embargo, ésta subió a alrededor de 9 % en octubre y volvió a bajar en noviembre de 2008 a 8 %. Se calcula que alrededor de 9 millones de ecuatorianos tienen una ocupación económica y unos 1,01 millones de habitantes están inactivos.

En 1998, el 10 % de la población más rica tenía el 42,5 % de la renta, mientras que el 10 % de la población más pobre solamente contaba con el 0,6 % de la renta. Durante el mismo año, el 7,6 % del gasto en salud pública fue a parar al 20 % de la población pobre, mientras que el 20 % de la población rica recibió el 38,1 % de este mismo gasto. La tasa de pobreza extrema ha disminuido significativamente entre 1999 y 2010.
En 2001 se estimó en un 40 % de la población, mientras que para 2010 la cifra bajó a un 16,5 % del total de la población. Esto se explica en gran parte por la emigración, así como la estabilidad económica lograda tras la dolarización. Las tasas de pobreza eran más elevadas para las poblaciones indígenas, afro-descendientes y rurales, alcanzando al 44 % de la población nativa

El petróleo representa el 40 % de las exportaciones y contribuye a mantener una balanza comercial positiva. Desde finales de los años 60, la explotación del petróleo elevó la producción y sus reservas se calculan en 4.036 millones de barriles
La balanza comercial total para enero de 2010 alcanzó un superávit de casi 5000 millones de dólares, una cifra gigantesca comparada con el superávit de 2007, que alcanzó un superávit de 5,7 millones de dólares, el superávit tuvo una disminución de alrededor de 425 millones comparado con el de 2006. Esta circunstancia se dio ya que importaciones, crecieron más rápido que las exportaciones. La balanza comercial petrolera generó una cifra positiva de 3.295 millones de dólares en 2008; mientras la no petrolera fue negativa por un monto de 2.842 millones de dólares. Esto permitió un déficit comercial, sin considerar el petróleo, de un 19 % en relación al año pasado. La balanza comercial con Estados Unidos, Chile, la Unión Europea y los países europeos que son socios de Ecuador, Bolivia, Perú, Brasil, es positiva México, Argentina, Colombia, Asia, es negativa.

En el sector agrícola, Ecuador es un importante exportador de bananas (primer lugar a nivel mundial en su producción y exportación), de flores, y el octavo productor mundial de cacao. Es significativa también su producción de camarón, caña de azúcar, arroz, algodón, maíz, palmitos y café. Su riqueza maderera comprende grandes extensiones de eucalipto en todo el país, así como manglar. Pinos y cedros son plantados en la región de la Sierra; nogales y romerillo; y madera de balsa, en la cuenca del río Guayas. Por otra parte, la industria se concentra principalmente en Guayaquil, el mayor centro industrial del país, y en Quito donde en los últimos años la industria ha crecido considerablemente, es también el mayor centro empresarial de país. La producción industrial está dirigida principalmente al mercado interno. Pese a lo anterior, existe una limitada exportación de productos elaborados o procesados industrialmente. Entre estos destacan los alimentos enlatados, licores, joyas, muebles y más.

Ecuador ha negociado tratados bilaterales con otros países, además de pertenecer a la Comunidad Andina de Naciones, y ser miembro asociado de Mercosur. También es miembro de la Organización Mundial del Comercio (OMC), además del Banco Interamericano de Desarrollo (BID), Banco Mundial, Fondo Monetario Internacional (FMI), Corporación Andina de Fomento (CAF), y otros organismos multilaterales. En abril de 2007, Ecuador pagó por completo su deuda con el FMI terminando así una etapa de intervencionismo de este organismo en el país. En 2007, se creó la Unión de Naciones Suramericanas (UNASUR), con sede permanente en Quito. También se ha sido parte de la creación del Banco del Sur, junto con seis otras naciones suramericanas. Ecuador realizó negociaciones para la firma de un Tratado de Libre Comercio con Estados Unidos, pero con la elección del Presidente Correa estas negociaciones fueron suspendidas.

El sistema público financiero del Ecuador está conformado por el Banco Central del Ecuador (BCE), el BanEcuador B.P., el Banco del Estado, la Corporación Financiera Nacional, el Banco Ecuatoriano de la Vivienda (BEV) y el Instituto Ecuatoriano de Crédito Educativo y Becas.

Actualmente Ecuador ha prohibido el uso de Bitcoin. El Banco Central de Estado emitió el siguiente mensaje:

""El Banco Central del Ecuador informa a la ciudadanía que el bitcoin no es un medio de pago autorizado para su uso en el país. El bitcoin es una criptomoneda que no tiene respaldo, pues sustenta su valor en la especulación. Las transacciones financieras realizadas a través del bitcoin no están controladas, supervisadas ni reguladas por ninguna entidad del Ecuador, razón por la que su uso representa un riesgo financiero para quienes lo utilizan."

"Es importante señalar que no está prohibida la compra y venta de criptomonedas -como el bitcoin- a través de Internet; sin embargo, se recalca que bitcoin no es una moneda de curso legal y no está autorizada como un medio de pago de bienes y servicios en el Ecuador, conforme lo establece el artículo 94 del Código Orgánico Monetario y Financiero.""

La prohibición viene dada por la intención de que el Estado cree su propia moneda virtual.

A partir de 2007, con una economía superada por la crisis económica, una serie de reformas políticas económicas han ayudado a encaminar a la economía ecuatoriana a un desarrollo sostenido, considerable y enfocado a lograr una estabilidad financiera, política y social; basada en la tendencia tomada por la región latinoamericana que ayudó a no verse afectada por la crisis mundial del primer mundo en 2010.

Impuestos enfocados a cambiar hábitos de consumo; desarrollo de sectores estratégicos y prioritarios; construcción, mejoramiento de sectores claves; desarrollo de la industria interna; políticas claras de comercio, competitividad, inversión estratégica, mejoramiento laboral, etc., han ayudado a lograr un crecimiento económico destacado que alcanza por encima del 8 % anual en 2011, esto reflejado en una clara disminución de la mendicidad y pobreza extrema, estabilidad de la clase media, disminución de la brecha de las clases sociales, creación de puestos de trabajos, aumento del comercio interno, entre otros.

Todo esto acompañado con nuevos mercados internacionales y de cooperación, principalmente con países asiáticos y latinoamericanos. Un ejemplo es la solicitud formal de convertirse en miembro pleno del Mercosur, creación de nuevas embajadas en Asia, fortalecimiento de organismos como la CELAC, CAN, ALBA, UNASUR, etc; e implementación de proyectos como el eje Manta-Manaos como alternativa al Canal de Panamá y el proyecto alterno de navegabilidad por los ríos Morona - Amazonas.
En las zonas rurales, en las que vive más o menos el 40 % de la población del país, se estima que el 40 % de los habitantes de dicha fracción subsiste en condiciones de necesidad. Gran parte de los casos fueron producto de no haber sido considerados por décadas al momento de hacer inversión en educación y de obras de infraestructura, como carecer de tierras adecuadas, regadíos suficientes y falta de vías de acceso en buen estado. Aunque se ha notado un progreso notable en revertir esta situación en los últimos años, desarrollando significativas inversiones en educación e infraestructura, junto con créditos de los de las cuales ha mejorado la vida del campesinado ecuatoriano y ha logrado detener la migración hacia las ciudades lo cual se reflejó en el censo de 2010 donde la población rural llegó al 37,23 % frente al 62,77 % de urbana.

Los indicadores más destacados en 2012 han sido la reducción a niveles más bajos que ha tenido el país en su historia en cuanto a desempleo que bajó a situarse en el 4,6 %, al mismo tiempo que el subempleo se situó en el 39,66 %, según datos del INEC; esto tomando en cuenta que la mayoría de la población ecuatoriana está en edad de trabajar. Le sigue otros datos como haber bajado al 16,3 % de pobreza por ingreso. Y una estimación de crecimiento económico por encima de la media latinoamericana del 5,4 % para 2012 según la CEPAL y una previsión para este año de un 5,3 %, centrado mayormente en el sector no petrolero.
En el “Índice de Competitividad Global 2017-2018” el Ecuador se encuentra en el puesto 97 con una diferencia de 6 puntos en contraste a la edición pasada. Según el mismo, uno de los factores problemáticos que resaltan en el país es la inestabilidad política que resulta ser un factor importante puesto que influye a las decisiones de posibles inversores extranjeros, este en el reporte anterior se establecía en quinto lugar, ahora se halla liderando la lista con un cambio de mandatario cuyos primeros actos han generado en una parte del pueblo ecuatoriano incertidumbre y en la otra gran parte, un respiro a la administración previa. El país enfrenta aun desafíos como la falta de capacitación en la administración de sus recursos pero su progreso sigue siendo notable.

Algunos de sus fuertes en la actualidad son la infraestructura, le tamaño del mercado, la educación superior y como índice principal, la salud y educación primaria que ha mejorado unos siete puntos en comparación con el “Índice de Competitividad Global 2016-2017” el cual se ubica en el puesto 68.
Si bien, el alza que mostró el Ecuador con su PIB desde el 2000 al 2014 ha revelado una baja desde ese año en adelante, para el 2018 el FMI prevé una mejora del PIB del país que lo ubicará en una cifra ligeramente positiva, con 0,6% y con una tenue mejora reduciendo la inflación del 2017 de 0,8% a 0,7% en 2018.

El país cuenta con una amplia gama de áreas de explotación, minera, petrolífera, agropecuaria, acuicultura y avícola, que ha sabido ser aprovechada a lo largo de su historia y que hoy en la actualidad se ha desarrollado en un ámbito más objetivo, focalizado, ordenado y orientado a un conservacionismo ambiental (p. ej: La reserva del Yasuní) y rentabilidad a corto y largo plazo. Independizándose cada vez más de la dependencia internacional o de proveedores privados para sustentar las necesidades generales o prioritarias en el desarrollo nacional.

El país cuenta con potencial para la industria en una gran variedad de sectores, como por ejemplo el petróleo. La producción interna de materias primas textiles y manufacturadas; la minería; la industria química, petroquímica; así como la petrolera y gasífera, por disponer de la principal entrada de esta industria; generación eléctrica debido a su altísimo potencial hidráulico, solar y eólico en varios sectores del país; la elaboración de productos a base de la fundición de materiales o cristales; producción agroindustrial y de alimentos procesados; producción farmacéutica, entre otros.
Los proyectos de mayor relevancia actualmente en desarrollo es la refinería del Pacífico, ubicada en Manta que será una de las mayores en la región, con lo que permitirá a Ecuador pasar de ser importador de derivados del petróleo a exportador de los mismos; los diversos proyectos hidroeléctricos entre el más destacable hasta el momento en construcción el Coca Codo Sinclair que generará cerca del 40 % de la demanda que tiene actualmente el país que junto a los demás proyectos permitirá a Ecuador ser unos de los principales exportadores de energía eléctrica en el continente; la minería a gran escala que firmó su primer contrato en la historia en marzo de 2012, permitirá ser una de las industrias de explotación de mayores ingresos junto con el petrolífero, pues se han encontrado varios yacimientos de diversos metales en Azuay, Morona Santiago, El Oro y Zamora Chinchipe, en este último, ya en marcha el mencionado contrato con más de 5.000 millones de dólares de ingresos netos al Estado ecuatoriano, fuera de otros.

La rehabilitación y reapertura después de décadas de abandono del ferrocarril ecuatoriano en toda su trayectoria, considerado uno de los más aventureros hermosos y diversos; iniciado en el gobierno de Gabriel García Moreno y considerado uno de los más complicados de elaborarse en la época por las condiciones de los terrenos donde fue construido.

La vialidad del Ecuador en los últimos años ha sufrido un enorme desarrollo, con proyecciones ya en desarrollo bastante considerables, como la de implementar vías rápidas o autopistas a lo largo de su territorio. Actualmente cuenta con casi su completa red vial asfaltada y con señalética y seguridades modernas para los mismos, todos los proyectos enfocados a vías de 6 carriles. Las vías de mayor importancia son la Panamericana (actualmente en ampliación de 4 a 6 carriles desde Rumichaca hasta Ambato, la conclusión de 4 carriles en todo el tramo de Ambato a Riobamba y la laguna de Colta, ampliación en la provincia de Cañar, mejoramiento entre Azogues y Cuenca y la ya en funcionamiento hasta Loja). La Ruta del Espondilus y/o Ruta del Sol (orientada a viajar por toda la línea costera ecuatoriana); la troncal amazónica (que cruza de norte a sur toda la Amazonía ecuatoriana, enlazando la mayoría y más importantes ciudades de la misma); Otro proyecto de gran importancia en desarrollo es la carretera Manta - Tena; la autopista Guayaquil - Salinas; la carretera Aloag - Santo Domingo; Riobamba - Macas (olvidada durante más de 40 años, y que atraviesa por el parque nacional Sangay), el complejo de puentes Unidad Nacional en Guayaquil, el puente sobre el río Napo en Francisco de Orellana; el puente sobre el río Esmeraldas en dicha ciudad del mismo nombre; y quizá la más rescatable de todas, el puente Bahía - San Vicente, siendo el mayor en la costa del Pacífico latinoamericano. Al igual que una amplia red vial de caminos vecinales asfaltados y de hormigón que han ayudado a mejorar sustancialmente el comercio y el desarrollo.
Los aeropuertos internacionales de Quito y Guayaquil han sufrido un alto aumento que ha requerido su modernización, que en el caso de Guayaquil y su Aeropuerto Internacional José Joaquín de Olmedo - Guayaquil implicó la construcción de una nueva terminal aérea de 53 mil metros cuadrados, considerada de las mejores de Latinoamérica y una de las mejores del mundo, terminal guayaquileña que es usada por más de cuatro millones de pasajeros anuales (aunque está en marcha la construcción del nuevo aeropuerto intercontinental en Daular en la vía a la costa, que contará con 3 pistas para vuelos simultáneos en un área de 2020 hectáreas); y en Quito se inauguró el 20 de febrero de 2013 el Aeropuerto Internacional Mariscal Sucre, ubicado en la localidad de Tababela, distante 25 kilómetros del Centro Histórico de la capital, su pista tiene 4.098 metros de longitud y su torre de control posee 41 metros de alto. Tiene espacio suficiente para una segunda pista, que será construida en el futuro. En una segunda etapa, prevista para 2023, se ampliará la terminal en 20 mil metros cuadrados más. Por el largo de su pista, puede recibir a los aviones más grandes de la actualidad, como el Boeing 747 o el Airbus A380. La superficie del Aeropuerto es de 1500 hectáreas, el área construida es de 70 hectáreas, la terminal de pasajeros tiene 38 mil metros cuadrados de superficie y se estima que cinco millones de personas lo usarán al año.

También entró a funcionar el restaurado y mejorado aeropuerto de Cotopaxi en Latacunga, orientado principalmente como aeropuerto de carga internacional, pero que funciona también para transporte interno de pasajeros. Actualmente se halla en desarrollo la implementación del aeropuerto internacional en Manta. Todos estos con proyecto de aeropuertos intercontinentales. Pero Ecuador también cuenta con varios aeropuertos de transporte interno. Entre los más destacables: Francisco de Orellana, Nueva Loja, Tulcán, Esmeraldas, Loja (Ciudad de Catamayo), Santa Rosa, Shell, Salinas, Tena, entre otros. En la Amazonía donde sirve la Fuerza Aérea para habitantes de sitios inaccesibles sus pistas y flotas han sido modernizados en los últimos años. Uno de los desarrollos destacables en aeronavegación ecuatoriana es la implementación de aeropuertos binacionales como el de Tulcán y Esmeraldas con vuelos a Colombia, y en desarrollo aun el de Santa Rosa y Loja con vuelos a Perú.

Los puertos marítimos son un notable punto en el comercio y turismo. El más importante, por donde pasa el 70 % de la exportación e importación del país, es el puerto de Guayaquil, ubicado al sur de la ciudad costera, al que también llegan cruceros con pasajeros de distintos países; además, la modernización en los últimos años ha permitido que puertos como el de Manta lleguen cruceros de gran calado. Otro puerto de gran importancia es el de Posorja en el Golfo de Guayaquil, mayormente de carga. Puerto Bolívar en Machala es principalmente para la exportación agrícola como banano, camarón, cacao, etc. El puerto de Esmeraldas principalmente para la exportación industrial de petróleo, gas y sus derivados.

En 2010 Ecuador tenía una tasa de alfabetización de 99,78 % (el quinto más alto de Latinoamérica), después de intensas campañas públicas para eliminar el analfabetismo. Con la implementación de la educación gratuita, las instituciones educativas públicas tanto escolares como secundarias no requieren cobro de aranceles ni pensiones de educación, y en sectores rurales y urbanos marginales. En el caso de la educación gratuita en las universidades e institutos superiores, se aplica bajo responsabilidad académica que exonera únicamente los créditos que no reprueba cada estudiante, así como servicios académicos como internet.

La educación superior ha llevado a cabo una fuerte evaluación de calidad, que categorizó en cinco niveles en un ámbito general de evaluación a las 68 universidades dedicadas a carreras de tercer nivel principalmente, que determinó los reconocimientos y deficiencias de cada institución. Solo 11 lograron la categoría A y 26 quedaron colocadas en categoría E. Bajo una política educativa, leyes y normas que ha dado a todas plazos cortos de reestructuración académica. En abril de 2012 se procedió con la primera etapa de depuración universitaria que suspendió a 14 universidades por no contar con exigencias mínimas y básicas para continuar funcionando después de tres años de su notificación. Ocho quedaron con fuertes limitaciones y sanciones. Tres quedaron elevadas a una categoría D y una quedó pendiente debido a que se trataba de una educación diferente y de mayor estudio. En la actualidad existen 54 universidades y escuelas politécnicas entre públicas y privadas, y tres universidades exclusivas de educación de cuarto nivel.

Otro aspecto clave ha sido el aumento por miles de becas y créditos, así como la amplitud a nuevos créditos a áreas claves de desarrollo, tanto para carreras de pregrado como posgrado, dentro y fuera del país, así como la convalidación de títulos extranjeros de manera ágil, con bajos costos de tramitación ayudado por el reconocimiento en ciertos casos directo de los títulos emitidos por varias centenas de universidades extranjeras. Por último, cabe destacar la creación de la Universidad Pedagógica en Azogues, la Universidad de Bellas Artes en Guayaquil, la Universidad Regional Amazónica en el Tena y la Universidad de Investigación Ciencia y Tecnología del Ecuador en Imbabura.

Actualmente el método de asignación de cupos en las diferentes carreras por institución educativa superior establece un examen general de aptitud a todos los postulantes sin límite de edad ni intentos, donde seleccionan en una jerarquía de 10 preferencias, la carrera y la universidad en la que desearían estudiar, después de obtener un puntaje, la lista se depura a fin de mostrar si el estudiante obtuvo el mínimo requerido exigido por cada universidad para la carrera que eligió, los mayores puntajes calificados como sobresalientes obtienen becas ilimitadas por el Estado ecuatoriano para cualquier carrera en cualquier universidad (siempre que esta universidad también lo apruebe) a nivel mundial con la condición y garantía de regresar al país una vez terminada su carrera. Los demás que alcanzan los puntajes de su listado se les asigna el cupo previa consulta al estudiante, y quienes no alcanzaron a ninguno de los cupos de su listado, se les asigna cupos de acuerdo a las aptitudes que la evaluación psicológica arrojó pues obviamente no concordaban con las aspiraciones que inicialmente pensó seguir, preguntándole, dándole la opción al estudiante de aceptar alguno de dichos cupos o esperar a la siguiente evaluación para con mejor preparación aprobar su aspiración. Finalmente si los postulantes exceden el número de cupos que posee cada carrera a nivel nacional, los interesados deberán esperar un periodo académico para poder obtener un cupo, si esta vez el número de postulantes excediera, nuevamente tendrán que postergar su ingreso a la universidad.

La literatura ecuatoriana se ha caracterizado por ser esencialmente costumbrista y, en general, muy ligada a los sucesos exclusivamente nacionales, con narraciones que permiten vislumbrar cómo es y se desenvuelve la vida del ciudadano común y corriente. Ecuador no ha dado literatos cuyos libros se vendan masivamente a nivel mundial. Pese a lo anterior, algunos escritores ecuatorianos han logrado ser medianamente conocidos en el contexto internacional, especialmente en los países hispanohablantes o iberoamericanos. Entre estos tenemos a Jorge Icaza, Alejandro Carrión Aguirre, Juan Montalvo, José de la Cuadra, Pedro Jorge Vera, Pablo Palacio, Demetrio Aguilera Malta, Alfredo Pareja Díez Canseco, Numa Pompilio Llona, Adalberto Ortiz, Medardo Ángel Silva, César Dávila Andrade, Luis Costales, Alfonso Rumazo González, José Martínez Queirolo, Jorge Enrique Adoum, Carlos Carrera Barreto, Hugo Mayo, Jorge Carrera Andrade, Arturo Borja, Ernesto Noboa y Caamaño. Uno de los aspectos más interesantes de las letras ecuatorianas, es que estas han producido una cantidad notable de buena narrativa, con autores que lograron fotografiar la idiosincrasia criolla y plasmarla en sus relatos. Nadie podría decir, pese a la crudeza de su contenido, que por ejemplo las novelas de Jorge Icaza no son un retrato muy hábilmente fabricado de las horribles penurias del indígena de la sierra ecuatoriana. Icaza traslada al lector al escenario que describe e incluso utiliza el mismo lenguaje que tienen los protagonistas en la vida real.

Pero la literatura ecuatoriana no se limita únicamente a Icaza y el indigenismo. También existen otros grandes expositores de la misma, como Alfredo Pareja Díez Canseco, quien destacó más que nada como novelista. Este, en contraposición a Jorge Icaza, creó novelas esencialmente urbanas, en las que aflora la denuncia social. También fue un gran historiador. Si seguimos en la senda de los novela dedicada a la denuncia social, es imprescindible nombrar a Joaquín Gallegos Lara, cuya obra, aunque breve, es magistral al aludir a los problemas que agobian a la clase obrera y la brutal explotación que esta sufre a manos de empresarios inescrupulosos. En "Las cruces sobre el agua" narra la peor masacre obrera ocurrida en la historia del Ecuador (1922). Demetrio Aguilera Malta, en cambio, fue más que nada un novelista costumbrista aunque también muy multifacético. En sus escritos describió al "montubio", el típico campesino mestizo de la costa ecuatoriana. Entre las mujeres que escriben está Alicia Yánez Cossío, dueña de una considerable producción narrativa, en la que se incluye la novela ""Sé que vienen a matarme"", una polémica novela acerca del dictador Gabriel García Moreno y sus excesos mientras era presidente de Ecuador. La poetisa Karina Gálvez, nominada en 2011 a la 1.  Medalla Internacional a la Paz y a la Cultura "Presidente Salvador Allende", autora del poema "La Batalla del Pichincha" , ha dado a conocer la poesía ecuatoriana a nivel internacional.

En la literatura contemporánea podemos encontrar varios ensayistas importantes como Agustín Cueva y Bolívar Echeverría; narradores como Javier Vásconez, Eliécer Cárdenas, Huilo Ruales, Santiago Páez, Abdón Ubidia, Marco Antonio Rodríguez, Humberto Salvador, Pablo Palacio, Leonardo Valencia, Gabriela Alemán, Iván Egüez, Jorge Luis Cáceres, Eduardo Varas, Miguel Antonio Chávez; o poetas como Ernesto Carrión, Alexis Naranjo, Hugo Mayo, Iván Carvajal Aguirre, Kleber Franco, Iván Oñate, Julio Pazos Barrera, Humberto Vinueza, Javier Ponce, Fernando Nieto Cadena, Jorge Martillo, Edwin Madrid, Paco Benavides, Xavier Oquendo Troncoso, Marcos Rivadeneira Silva, entre otros.

La práctica artística contemporánea en el Ecuador cuenta con talentosos exponentes que han madurado proyectos y propuestas. Las políticas culturales institucionales del país han provocado que la escena artística contemporánea del país no logre operar con la contundencia ni la eficiencia que se requiere para ocupar un espacio competitivo a nivel internacional. Sin embargo entre los artistas contemporáneos del Ecuador encontramos figuras que han logrado autónomamente estabilidad y competencia en la creación y exhibición de obras, así como el reconocimiento local e internacional: Oswaldo Guayasamín, Gonzalo Endara, Eduardo Kingman, Camilo Egas, Oswaldo Viteri, Carlos Rosero, entre otros.

Es un país con atributos en las artesanías. Esto se da, por una parte debido a su legendaria tradición de productos de uso cotidiano, pasando por la cerámica y los usos que se le dieron, además de los metales y la cestería, y por la otra gracias a una enorme cultura productora de textiles e instrumentos musicales.

En el campo de la dramaturgia casi no ha habido exponentes relevantes o que hayan alcanzado un alto grado de difusión, especialmente a nivel internacional. Sin duda el mejor, más prolífico y conocido es el guayaquileño José Martínez Queirolo, cuyas obras se han representado en Estados Unidos y Europa, a la vez que han sido traducidas a otros idiomas. También se lo conoce como autor de numerosos cuentos, entre los que también hay algunos creados para niños. Además es un destacado actor y dirige su propia compañía de teatro. Ganó el Premio Nacional de Cultura "Eugenio Espejo" en 2001.

El costumbrismo en el teatro ecuatoriano es un subgénero iniciado a principios del siglo XX cuyos principales exponentes fueron Ernesto Albán Mosquera con su personaje Evaristo Corral y Wigberto Dueñas Peña con el Indio Mariano. Es un género eminentemente satírico, con una alta carga de crítica socio-política.

La producción cinematográfica de Ecuador incluye cortos y documentales hechos a lo largo del siglo XX. Pese a la calidad o el valor histórico de algunas de esas aportaciones culturales, el cine ecuatoriano solo ha comenzado a tener repercusión internacional en el siglo XXI. La producción de largometrajes fue en el siglo pasado una limitante en lo referente a cantidad, en gran parte debido a los costos que conllevaba producir una película. Pese a eso en la última década la producción cinematográfica ha aumentado en una escalada importante, debido a las facilidades y garantías que ofrece el país bajo leyes e iniciativas para su producción y difusión, así como exoneraciones de impuestos.

Varias cintas han llegado a proyectarse comercialmente y con éxito, tal es el caso de "Crónicas", "Que tan lejos", "Con mi corazón en Yambo", "A tus espaldas", "Mono con gallinas", "Mejor no hablar de ciertas cosas" o "En el nombre de la hija", algunos de ellos con reconocimientos en festivales internacionales de cine.

Ecuador posee una diversidad de estilos musicales tanto autóctonos como populares y de influencia extranjera. Entre los géneros musicales locales se destacan ritmos mestizos como el pasacalle, el pasillo, el yaraví, el albazo, el bolero, el requinto; ritmos afros como la bomba, la marimba, salsa, guaracha, mambo; ritmos indígenas como el sanjuanito música Folclórica andina. De influencia extranjera géneros como el pop, el rock, el merengue, la salsa, la cumbia, el vallenato, la bachata, el ska, la música electrónica, el dance, el reggae, el heavy metal, el punk, el hip hop, el reggaeton y el k-pop.

El más destacado músico, Luis Humberto Salgado (Cayambe, 1903-Quito, 1977), fue un compositor de música clásica de Ecuador, autor de varías sinfonías, óperas, ballets y conciertos. Compuso nueve sinfonías, cuatro óperas, una ópera-ballet, siete conciertos, operetas y varias piezas de música popular ecuatoriana, sobre todo sanjuanitos y pasacalles, algunos con innovaciones notables como la Segunda sinfonía (Sintética no. 1), la Sexta sinfonía (para cuerdas y timbales), las sonatas 2 y 3 para piano y el Sanjuanito futurista.
Otro compositor de renombre, fue el lojano Carlos Miguel Agustín Vaca, cuyas aportaciones musicales fueron destacadas, ya que compuso el himno de numerosas ciudades y cantones dentro de Loja. De la música tradicional ecuatoriana extrajo motivos para sus óperas así como también ritmos y armonías para sus sinfonías. Pese a los esfuerzos de la Orquesta Sinfónica de Ecuador y del director Álvaro Manzano, buena parte de su música está aún por interpretarse y casi la totalidad por grabarse.

En el ámbito operistico, tenemos a Marlon Valverde, tenor ecuatoriano, quién encarnó al Libertador Simón Bolívar, en la ópera Manuela y Simón

También destacan artistas como: Carlota Jaramillo, Julio Jaramillo, Las Hermanas Mendoza Suasti, Polibio Mayorga, Dúo Benítez-Valencia, Hermanos Miño Naranjo, Paulina Tamayo, Fresia Saavedra, Segundo Rosero, Anita Lucía Proaño, Soledad Morales, Segundo Bautista, Juan Fernando Velasco, entre otros. En el subgénero de música popular como la technocumbia, se destacan también muchos cantantes como: Sharon La hechicera, Gerardo Morán, María de los ángeles, Jaime Enrique Aymara, Delfín Quishpe, entre otros.

Dentro de la música urbana y particularmente en el Rock destacan varios grupos como Anima Inside, Sudakaya y Rocola Bacalao, quiénes se han destacado por su trabajo profesional ofreciendo conciertos a nivel internacional, o bandas de pop rock como Verde70, Equilivre y Tranzas banda que llegó a posicionar a sus canciones entre las más sonadas en países como Colombia, Bolivia y México.

Gerardo Mejía, conocido en los 90 simplemente como Gerardo, también es uno de los artistas ecuatorianos con mayor repercusión internacional, siendo uno de los primeros latinos en triunfar en el mercado anglo, donde llegó a ubicar su single debut en el Top 10 de Billboard Hot 100. Paulina Aguirre, cantante de música cristiana ha sido galardonada con una Gaviota en el Festival de Viña del Mar y Juan Fernando Velasco, son los únicos en ganar un Grammy Latino. Otra artista femenina reconocida es Mirella Cesa, quien ha conseguido sonar internacionalmente, ganó una gaviota de plata en el festival Viña del Mar 2018 y aparece en los listados latinos de Billboard en Estados Unidos.

En la actualidad 7 de cada 10 ecuatorianos usan internet regularmente para diversos fines: desde correo electrónico, comercio electrónico, prácticas laborales, ocio, educación, información, entre otros. Se calcula que con la reducción y ampliación de accesibilidad que se ha dado en los últimos años, el 75 % de los hogares posee internet. La fibra óptica cubre las 24 provincias del país y el auge de la conexión Wi-Fi mayormente por entidades públicas.

Si bien la penetración de internet es comparable con la mayoría de los países latinoamericanos, Ecuador cuenta con un número interesante de usuarios de redes sociales, y páginas web especializadas. Según Facebook, a noviembre de 2012, Ecuador posee 5.077.060 de usuarios, lo cual la ubica en la posición 35 a nivel mundial de países con más usuarios de esta red social.

Si bien la telefonía fija se mantiene aún en el país con un crecimiento periódico, esta ha sido desplazada muy notablemente por la telefonía celular, tanto por la enorme cobertura que ofrece y la fácil accesibilidad.

Actualmente se determinó que existen más líneas de telefonía celular que habitantes en Ecuador, fenómeno que se aclara por los usuarios que optaron por tener dos líneas en su poder de diferentes operadoras para reducción de costes en llamadas y mensajes, por la descontrolada venta de líneas que en su mayoría dejaron de ser utilizadas y no han sido deshabilitadas, así como otras; en la actualidad tratan mediante políticas de estado, restructurarlas, registrarlas por usuario entre otras. Fuera de ello se determina que cada ecuatoriano por lo general posee un celular a partir de los 14 años más allá muchas veces de su estatus económico, tomando en cuenta ciertas excepciones y la menor presencia que obviamente se da en el sector rural frente al urbano.

En el país, existen cuatro operadoras de telefonía fija, CNT, ETAPA (públicas), TVCABLE y Claro (privadas) y cuatro operadoras de telefonía celular, Movistar, Claro y Tuenti (privadas) y CNT (pública).

En Ecuador existen dos bandas para la recepción de imágenes, la UHF y la VHF, y según donde se ubique la recepción de imagen, se sintoniza el canal, ya que en un principio el Estado no dispuso de un control de franquicia para comprar el derecho que otorgue la privacidad y que otro tipo de recepción no ocupe su espacio en cualquier otra ciudad o provincia del país.

Actualmente existen varios medios para la dotación del servicio televisivo, sea cobertura por antenas analógicas de aire de carácter abierto y gratuito, así como también las de pago mediante proveedores de televisión por cable o por antenas satelitales, tanto públicas como privadas.

La mayoría de ciudades y poblaciones disponen de al menos un canal comunitario que puede ser de frecuencia VHF (canales primarios) o UHF (canales secundarios). Algunos de sintonía nacional, entre los canales que poseen señal de sintonía internacional están: Ecuavisa - "Posee señal internacional", RedTeleSistema (RTS), Teleamazonas, que posee señal en Estados Unidos a través de los canales 839 y 842 del Dish Pack Latino, Ecuador TV Televisión Pública, TC Televisión, Gama TV, Canal Uno -, "Posee señal internacional", Telerama, RTU, Radio y Televisión Unidas, Latele, Televicentro, CiudadColorada - "Portal internacional" Teleamazonas HD, Ecuavisa HD, Oromar HD, TC HD y Canal Uno HD son canales nacionales de Ecuador que están en full HD.

El Ecuador, debido a la modernización de la época, estableció para la implementación de la televisión y radio digitales, la norma ISDB-Tb, con fecha 26 de marzo de 2010. El anuncio lo hizo el Superintendente de Telecomunicaciones, Fabián Jaramillo. Así, Ecuador se convierte en el sexto país en adoptar el standart ISDB-Tb. Siendo el estándar más optado en la región latinoamericana.

El apagón analógico se estableció para el 31 de diciembre de 2018 a nivel nacional, el cual se llevará en tres etapas, el primer apagón para las ciudades con más de 500 mil habitantes para el 31 de diciembre de 2016, el segundo para las ciudades con más de 200 mil habitantes para el 31 de diciembre de 2017 y el tercero a nivel nacional para el 31 de diciembre de 2018.

Los diarios de mayor circulación nacional son: Diario Extra (Guayaquil), El Telégrafo (Guayaquil), El Universo (Guayaquil), El Comercio (Quito), La Hora (resto del país). Muchas de las capitales de provincias disponen de 2 a 3 periódicos locales.

A continuación una lista de medios escritos:

El Ministerio del Deporte de Ecuador es el organismo rector de la actividad física y el deporte en el país.

La chaza, es el deporte nacional de Ecuador, sin embargo, el deporte más popular en Ecuador es el fútbol. El campeonato nacional de este deporte consta de 2 series profesionales la serie A y la serie B, en la serie A participan 12 equipos en un torneo cuya modalidad se modifica año a año.

El mayor representante del atletismo ecuatoriano es Jefferson Pérez, quien ha obtenido tres medallas de oro a nivel mundial en la modalidad de marcha atlética (París en 2003, Helsinki en 2005 y Osaka en 2007), además de una medalla de oro en los Juegos Olímpicos de Atlanta 1996 y una de plata en los Juegos Olímpicos de Pekín 2008. Por otra parte, Rolando Vera Rodas es el único fondista no brasileño que ha ganado por cuatro veces consecutivas (1986-1989) la Corrida Internacional de San Silvestre en Brasil. Glenda Morejón se constituye como una promesa del deporte de marcha atlética ya que obtuvo la medalla de oro en el Mundial Sub 18 de Atletismo que se desarrolló en Nairobi, Kenia en el año 2017.

Liga de Quito apodado como "Rey de Copas", el equipo con mayores logros internacionalmente, tiene 10 títulos nacionales y 4 títulos internacionales, ha conseguido Copa Libertadores (edición de 2008), dos Recopa Sudamericana (edición de 2009, y edición de 2010) y una Copa Sudamericana (edición de 2009), en el 2008 fue subcampeón del mundial de clubes, y desde el año 2008 es el equipo ecuatoriano mejor ubicado según el Ranking de la CONMEBOL. 

Barcelona Sporting Club es el club con más títulos nacionales, teniendo 15, hasta 2016. Fue el líder de Ecuador en el Ranking de equipos Conmebol por país, también ha sido subcampeón de la Copa Libertadores en dos ocasiones (edición 1990 y edición 1998), siendo el primer equipo ecuatoriano en haber llegado a una final de la Copa Libertadores, además nunca ha descendido. Barcelona Sporting Club posee un estadio, originalmente llamado "Estadio Monumental Isidro Romero Carbo". Reglamentariamente de más de 60 000 espectadores, siendo uno de los estadios más grandes de América y del mundo.

El Club Sport Emelec, apodado "Ballet Azul" o "Millonarios", fue el primer campeón nacional y además es el único equipo del país campeón en todas las décadas que se han disputado campeonatos, siendo campeón 14 ocasiones hasta el 2017, consiguiendo su último título en este mismo año (2017) siendo el actual campeón del fútbol ecuatoriano y primer tricampeón en representación del Guayas (en 2013, 2014 y 2015, respectivamente), convirtiéndolo en el segundo club con más títulos nacionales. En 2001 fue finalista de la Copa Merconorte, que luego se transformaría en la Copa Sudamericana.
El Club Deportivo El Nacional es el tercer club con más títulos de campeón nacional, teniendo 13. Se caracterizado porque su plantilla de jugadores la componen únicamente pertenecen deportistas de nacionalidad ecuatoriana. Ha sido tricampeón y lo ha hecho dos ocasiones, en 1976, 1977, 1978 y en 1982, 1983, 1984.

Hay otros clubes tradicionales, como América de Quito que fue el primer club de Ecuador campeón de un torneo internacional al ganar la Copa Ganadores de Copa en 1971. Deportivo Quito ha sido campeón de Ecuador 5 veces, Deportivo Cuenca, Club Deportivo Everest y Centro Deportivo Olmedo lo han sido una vez.

En cuanto a la selección nacional, esta se clasificó para tres mundiales, el de Japón-Corea 2002, Alemania 2006 llegando a octavos de final en este donde cayó eliminada por por 1 a 0, quedando en el puesto 12 entre 32 selecciones; y la Copa de Brasil 2014 siendo la última a la que se clasificó hasta la actualidad. Un ecuatoriano destacado en este deporte es Alberto Spencer, máximo goleador de todos los tiempos de la Copa Libertadores con 54 goles en su cuenta. Por otro lado Iván Hurtado es el jugador con más partidos disputados en la Selección de fútbol en 167 ocasiones y Agustín Delgado es el máximo goleador con 31 tantos y en Mundiales con 3 anotaciones (1 a México en 2002, 1 a Polonia y Costa Rica en 2006). Cabe también acotar que por esta selección se han destacado otros jugadores como Édison Méndez, Álex Aguinaga, Ulises de la Cruz, Antonio Valencia, Christian Benítez incluso Jaime Iván Kaviedes quien en el año de 1998 fue máximo goleador del mundo, marcando 43 goles en 39 partidos, siendo además el jugador con más cantidad de goles en una temporada de la denominada Serie A de Ecuador que responde al 'Campeonato Nacional de Primera División'. En abril de 2013 la selección ecuatoriana de fútbol se ubicó en el top 10 del ranking mundial de selecciones de fútbol del mundo, ubicada en el puesto 10.

Lo más destacado de los últimos años de la selección ecuatoriana (a nivel de categorías inferiores, sub 18 para ser exactos) ha sido la medalla de oro en Juegos Panamericanos lograda en el año 2007.

En cuanto al tenis, Pancho Segura fue considerado en 1952 como el Jugador número Uno del mundo por la Professional Lawn Tennis Association. También en esta disciplina, Andrés Gómez ganó el Roland Garros de 1990 y el Torneo de Forest Hills, fue tercero en el mundo; el tenista Nicolás Lapentti fue sexto en el mundo.

El Ministerio de información y Turismo fue creado el 10 de agosto de 1992, al inicio del gobierno de Sixto Durán Ballén, quien visualizó al turismo como una actividad fundamental para el desarrollo económico y social de los pueblos. Frente al crecimiento del sector turístico, en junio de 1994, se tomó la decisión de separar al turismo de la información, para que se dedique exclusivamente a impulsar y fortalecer esta actividad.

Ecuador es un país con una vasta riqueza natural. La diversidad de sus cuatro regiones ha dado lugar a miles de especies de flora y fauna. Cuenta con alrededor de 1640 clases de pájaros. Las especies de mariposas bordean las 4.500, los reptiles 345, los anfibios 358 y los mamíferos 258, entre otras. No en vano el Ecuador está considerado como uno de los 17 países donde está concentrada la mayor biodiversidad del planeta, siendo además el mayor país con diversidad por km2 del mundo. La mayor parte de su fauna y flora vive en 26 áreas protegidas por el Estado. Asimismo, posee una amplia gama de culturas. Desde 2007 con el gobierno de Rafael Correa se transformó la marca turística "Ecuador Ama la Vida" con la que se vendería la promoción turística de la nación. Enfocada en considerarla como un país amable y respetuoso con la naturaleza, la biodiversidad natural y la diversidad cultural de los pueblos. Y para ello se desarrollan medios de explotarlas junto con la economía privada.

El país posee dos ciudades patrimonio cultural de la humanidad: Quito y Cuenca, así como dos patrimonios naturales de la humanidad: las islas Galápagos y el parque nacional Sangay además de una reserva mundial de la biosfera, como es el macizo del Cajas. En lo cultural, es reconocido el sombrero de paja toquilla y la cultura del pueblo indígena zapara. Los sitios más concurridos por turistas nacionales y extranjeros tienen distintos matices debido a las diversas actividades turísticas que ofrece el país.

Entre los principales destinos turísticos se destacan:

A pesar de ser un país pequeño, la gastronomía de Ecuador es bastante variada, debido a la existencia en el país de cuatro regiones naturales diferenciadas –costa, sierra, oriente y región insular– con costumbres y tradiciones diferentes. Los distintos platos típicos y los ingredientes principales varían en función de estas condiciones naturales.

El pescado que suele comerse en la costa ecuatoriana es conseguido de las aguas del Océano Pacífico o de las innumerables ríos navegables de la zona. Entre los principales pescados se encuentran el picudo, albacora, dorado, camotillo, chame, corvina y la trucha. Algunos de los platos populares con pescado son: sopa marinera, ceviches de pescado, corviches, bollos, cazuelas, estofado de pescado con maní, encocado, etc.
Un plato típico de la costa se llama Encebollado de pescado.
Ecuador es un principal país exportador de plátano, por lo que este representa un importante elemento en la gastronomía, en especial en la costa ecuatoriana. Existen tres principales variedades de plátano, siendo las tres más importantes: el plátano verde, el plátano maduro y el guineo. Los plátanos verdes y maduros deben cocinarse antes de ser ingeridos. El plátano verde (simplemente llamado 'verde') suele comerse frito, asado o hervido. El plátano maduro (o simplemente, 'maduro') suele comerse frito, asado o hervido de igual manera, y tiene un sabor más dulce y una consistencia más suave. El "guineo", es el nombre típico de la banana ecuatoriana; suele comerse crudo como una fruta cualquiera, aunque también hay una plétora de postres preparados a base del guineo. Existe, además, un tipo de guineo en miniatura, que se conoce con el nombre de 'orito'.

Las verduras están presentes en diferentes formas, el arroz, el plátano verde o maduro, la yuca, o la salsa de maní (cacahuate) tostado y molido. El maíz se suele comer en las muy populares tortillas de maíz conocidas como bonitísimas, cocinan los choclos (elotes) en agua y sal, las mazamorras y los comen con queso fresco. Igual los frijoles, que acompañan a muchos de sus platos. El Puré de papas o lo sirven de base para platos como los llapingachos que son tortillas de papa o los locros.

Se suele comer carne de vaca, cerdo, cordero, cabra, pato, pavo y pollo. Algunos platos se combinan con verduras como el seco de chivo o el seco de gallina que son trozos de carne, tipo estofado, servida con arroz. Dentro de los platos exóticos se tiene el cuy, que suele comerse asado en las celebraciones de ciertas partes andinas del país. La carne de chancho ("cerdo, lechón, cachorro"), que se come en varios lugares del Ecuador, participa en la elaboración de diversos platos, algunos de ellos como fritada, hornado, chugchucaras.

Es de destacar de la cocina de Ecuador, los caldos (conocidos como "sopas" o "locros") que suelen prepararse con verduras muy diversas y carne de gallina, son frecuentemente servidos en los mercados callejeros como desayuno. Algunos de ellos son muy populares como el yaguarlocro que es una sopa de papas que lleva como ingrediente borrego y una salsa.

Aunque la mayoría de las veces se ingiere fruta como postre la cocina ecuatoriana tiene postres como:
Los aguardientes de caña son muy populares en las provincias de Loja, Manabí y Tungurahua, donde se elaboran diversas bebidas alcohólicas como el llamado currincho, el tardón, el guarapo, el quemado, el norteño, o canelazo que es a base de canela y la famosa Caña. Ciertas bebidas de estas provincias llevan aderezos de frutos y productos nativos como la miel y la grosella y muchos cítricos como la naranja, mandarina, limón e injertos de estas frutas con otras, incluso se mezclan con agua de flores nativas y hojas de otros frutos medicinales y exóticos como la hierba luisa o la manzanilla. Otra bebida es el pájaro azul, que es aguardiente de caña de azúcar mezclado con las cáscara de la mandarina, que al hervir le da ese color azulado. En especial la chicha que es un preparado a base de yuca, mote o maíz.
También existen bebidas caseras como es el caso del pisco sour, el cual se puede preparar con amargo de Angostura, jarabe de goma y jugo de limón y una bebida muy conocida y sin alcohol es el coctel de San Francisco.




</doc>
<doc id="3313" url="https://es.wikipedia.org/wiki?curid=3313" title="Islas Galápagos">
Islas Galápagos

Las islas Galápagos (también islas de los Galápagos y oficialmente archipiélago de Colón o archipiélago de Galápagos) constituyen un archipiélago del océano Pacífico ubicado a 972 km de la costa de Ecuador. Está conformado por trece islas grandes con una superficie mayor a 10 km², seis islas medianas con una superficie de 1 km² a 10 km² y otros 215 islotes de tamaño pequeño, además de promontorios rocosos de pocos metros cuadrados, distribuidos alrededor de la línea del ecuador terrestre, que conjuntamente con el Archipiélago Malayo, son los únicos archipiélagos del planeta que tienen tierras tanto en el hemisferio norte como en el hemisferio sur.

Las Islas Galápagos fueron declaradas Patrimonio de la Humanidad en 1978 por la Unesco. El archipiélago tiene como mayor fuente de ingresos el turismo y recibe 200 000 turistas al año. También se ha desarrollado el turismo ecológico con el fin de preservar las especies. La región fue el hábitat del Solitario George, el último espécimen de la especie tortuga gigante de Pinta, extinta el 24 de junio del 2012. Las islas también son hábitat de especies como tortugas marinas, delfines, tiburones, tiburones martillo, ballenas, arrecifes de coral, fragatas, iguanas, lagartos, cormoranes, albatros, leones marinos y pingüinos. Al igual que la masa continental de Ecuador, el archipiélago es atravesado por la línea ecuatorial, en su mayor parte por el norte de la isla Isabela. Galápagos es el segundo archipiélago con mayor actividad volcánica del planeta, superado únicamente por Hawái. Entra en la categoría de los puntos calientes; los volcanes más activos son Cerro Azul, Sierra Negra, Marchena y volcán La Cumbre en la Isla Fernandina, que es el más activo del archipiélago y uno de los más activos del mundo.

Las Galápagos son conocidas por sus numerosas especies endémicas y por los estudios de Charles Darwin que le llevaron a establecer su teoría de la evolución por la selección natural. Son llamadas, turísticamente, las Islas Encantadas, denominación que se ganó el archipiélago en el siglo XVI por su grandiosa biodiversidad de flora y fauna, heredando el nombre por generaciones.

Se estima que la formación de la primera isla tuvo lugar hace más de cinco millones de años, como resultado de la actividad tectónica. Las islas más recientes, llamadas Isabela y Fernandina, están todavía en proceso de formación, habiéndose registrado la erupción volcánica más reciente en 2009.

Administrativamente, Galápagos constituye una provincia de Ecuador, conformada por tres cantones que a su vez son islas, a saber: San Cristóbal, Santa Cruz e Isabela. El 12 de febrero de 1832, bajo la presidencia de Juan José Flores, las islas Galápagos fueron anexadas a Ecuador. Desde el 18 de febrero de 1973 constituyen una provincia de este país.

Las islas se formaron hace 5 millones de años como resultado de actividad tectónica en el fondo marino. Esta isla es muy joven. La actividad volcánica actual aún sigue expandiendo al archipiélago. El archipiélago es uno de los grupos volcánicos más activos del mundo. Muchas de las islas son solamente las puntas de algunos volcanes y muestran un avanzado estado de erosión. Islas como Baltra y North Seymour emergieron del océano por una gran actividad tectónica.

Un estudio realizado en el año 1952 por los historiadores Thor Heyerdahl y Arne Skjolsvold reveló que se encontraron cerámicas de algunos pueblos (posiblemente incas) de antes de la llegada de los españoles. Sin embargo, no se han encontrado tumbas, vasijas y ninguna construcción antigua que revele asentamientos antes de la colonización.

Las islas Galápagos fueron descubiertas por casualidad el 10 de marzo de 1535, cuando el barco del obispo de Panamá fray Tomás de Berlanga se desvió de su destino a Perú, donde cumpliría un encargo del rey español Carlos V para arbitrar en una disputa entre Francisco Pizarro y sus subordinados tras la conquista del imperio incaico.

Los primeros mapas en incluir las islas fueron realizados por los cartógrafos Abraham Ortelius y Mercator alrededor de 1570. Las islas estaban descritas como "Insulae de los Galopegos" (Islas de las Tortugas).

Las Galápagos fueron utilizadas por piratas ingleses como escondite en sus viajes de pillaje a los galeones españoles que llevaban oro y plata de América hacia España. El primer pirata registrado que visitó las islas fue el inglés Richard Hawkins, en 1593. Desde entonces y hasta 1816 muchos piratas llegaron al archipiélago.

Recién descubiertas las islas se encontraban deshabitadas y los barcos que pasaban junto a su ubicación coincidían cuando el archipiélago era tapado por la niebla. Diversos acontecimientos las llevaron a ser conocidas como las islas Encantadas e incluso algunos navegantes españoles afirmaban que no existían y solo eran espejismos.

La primera misión científica que visitó las Islas Galápagos fue la expedición Malaspina, una expedición española dirigida por Alejandro Malaspina que llegó en 1790. Sin embargo, los registros de la expedición nunca fueron publicados.

En el siglo XVII se empieza a poblar la zona cuando el navegante James Colnett describe al lugar como unas islas ricas en flora y fauna. Esto atrajo a los primeros colonos, en su mayoría ingleses, con interés por las ballenas, cachalotes, leones marinos y principalmente por los galápagos. El descubrimiento de la grasa de los cachalotes también atrajo a muchos balleneros lo que condujo a que se creara una oficina de correos improvisada, donde los barcos dejaban y recogían cartas. Colnett también dibujó las primeras cartas de navegación de las Galápagos.

En octubre de 1831 José de Villamil envió una comisión exploradora al archipiélago de las Galápagos con el fin de averiguar sobre la existencia de orchilla, planta utilizada en tinturar los tejidos y que se exportaba a México. El 14 de noviembre se constituyó la "Sociedad Colonizadora del Archipiélago de las Galápagos" y denunció como terrenos baldíos a la isla Charles, después denominada Floreana.

El 20 de enero de 1832 salió una expedición a las Galápagos al mando del Coronel Ignacio Hernández y Ecuador las anexó el 12 de febrero de 1832 bajo el gobierno del General Juan José Flores, bautizándolas como archipiélago de Colón.

A bordo de la nave Beagle la expedición británica al mando del capitán Robert FitzRoy llegó a Galápagos el 15 de septiembre de 1835 para realizar trabajos de sondeos y cartografía, dentro de una lista de lugares aislados difícilmente visitados por navegantes como Valparaíso, Callao, islas Galápagos, Tahití, Nueva Zelanda, Australia, Cabo Buena Esperanza y regresó a Falmouth el 2 de octubre de 1836. El capitán y otros a bordo, incluyendo el joven naturalista Charles Darwin, realizaron un estudio científico de la geología y biología en cuatro de las islas antes de continuar su expedición alrededor del mundo. El barco recorrió las islas durante cinco semanas, pero Darwin estuvo en tierra solo dos semanas. Investigó a los animales y plantas propios de la región; junto con todos los estudios de su viaje, le permitió a Darwin formular la teoría del origen de las especies.

La Unesco declaró a las islas Galápagos como Patrimonio Natural de la Humanidad en 1979 y, seis años más tarde como Reserva de la Biosfera (1985). En el 2007 la Unesco declaró a las islas Galápagos como Patrimonio de la Humanidad en riesgo medioambiental y estuvo incluida en la Lista del Patrimonio de la Humanidad en peligro hasta 2010.

El archipiélago se conoce por una variedad de nombres; en Ecuador comúnmente se conocen por sus nombres en español, que además son los oficiales, usando los antiguos nombres en inglés solo con fines históricos. El nombre oficial de las islas es "Archipiélago de Colón", mientras que administrativamente se conoce al territorio como "Provincia de Galápagos". La denominación más conocida y común es "Islas Galápagos". La primera carta de navegación de las islas, aunque rústica, fue realizada por el bucanero Ambrose Cowley en 1684, y en dicha carta bautizó las islas con los nombres de algunos de sus amigos piratas y de algunos nobles ingleses que apoyaban la causa de los corsarios.

Las siguientes son las islas de más de un kilómetro cuadrado de superficie:

Los siguientes son los islotes con entre una y cien hectáreas de superficie. Existen multitud de otros islotes, rocas y promontorios aún más pequeños:

Llamada así en honor a la Reina Isabel I de Castilla que patrocinó el viaje de Colón (su nombre en inglés honra al Duque de Albemarle). Es la mayor isla del archipiélago, con una superficie de 4588 km² y ocupa el 58% de la zona terrestre de las islas. La forma de la isla se debe a la fusión de cinco grandes volcanes (Cerro Azul, Sierra Negra, Salcedo, Darwin y Wolf) en una sola masa. Tiene una población de aproximadamente de 2200 habitantes. El punto más alto es el volcán Wolf, que alcanza 1.707 metros de altitud. En esta isla se pueden observar pingüinos, iguanas marinas, cormoranes no voladores, piqueros de patas azules, pelícanos, así como abundantes zayapas y tintoreras. En las faldas y calderas de los seis volcanes de Isabela, se pueden observar tortugas gigantes e iguanas terrestres, así como pinzones, palomas, halcones y una interesante vegetación. El tercer mayor asentamiento humano del archipiélago y su mayor puerto, conocido como Puerto Villamil o Albemarle, está ubicado en el extremo sur de la isla.

Llamada así en honor a la Cruz de Cristo (su nombre en inglés se debe al barco militar HMS Indefatigable). Tiene una superficie de 986 km² y una altitud máxima de 864 metros. En Santa Cruz está localizado el mayor asentamiento humano del archipiélago, en el poblado de Puerto Ayora. La Estación Científica Charles Darwin y las oficinas centrales del Servicio del Parque nacional Galápagos están ubicadas aquí. En el SPNG opera un centro de crianza de tortugas donde estos quelonios son preparados para su reintroducción en su hábitat natural. La "parte alta" de Santa Cruz tiene una exuberante vegetación y es conocida por los tubos lávicos. Una gran población de tortugas habita esta región. Caleta Tortuga Negra es un área rodeada de manglar donde tortugas marinas, rayas y pequeños tiburones la utilizan como lugar de apareamiento. Cerro Dragón, conocido por su laguna de flamencos y sus iguanas terrestres, también se encuentra en esta isla. Tiene zonas para practicar el buceo y el surf.

Bahía Tortuga está situada en la isla de isla Santa Cruz. Puerto Ayora esta alrededor de 20 minutos a pie. Hay un pequeño camino de 2.500 metros de largo y se debe iniciar y cerrar sesión en la oficina del Parque nacional Galápagos.

Llamada así en honor al Rey Fernando el Católico, quien patrocinó el viaje de Colón (su nombre en inglés homenajea a Sir John Narborough). Fernandina tiene una superficie de 642 km² y una altura máxima de 1494 metros. Es la más reciente y más occidental de las islas del archipiélago. El 11 de abril de 2009 se inició un nuevo proceso eruptivo que formó una nube de ceniza y vapor de agua con flujos piroclásticos que descendieron por las laderas del volcán, hasta llegar al mar. Punta Espinoza es una estrecha franja de tierra donde se reúnen centenares de iguanas marinas en grandes grupos. En esta isla habita el cormorán no volador, así como pingüinos y lobos peleteros. También se encuentran áreas de manglar. Diversos tipos de flujos de lava pueden observarse aquí.

Llamada así en honor al santo patrón de España, es conocida también como San Salvador en honor a la primera isla del Caribe descubierta por Colón. Tiene una superficie de 585 km² y una altura máxima de 907 metros. Aquí se encuentran iguanas marinas, lobos peleteros, leones marinos, tortugas terrestres y marinas, delfines y tiburónes. Una gran cantidad de animales domésticos, que fueron introducidos con la llegada de pobladores a la isla, han causado un gran daño a la flora y fauna endémicas. En esta isla se observan con frecuencia pinzones de Darwin y halcones de Galápagos. En la bahía Sullivan existe un flujo reciente de lava pahoehoe.

Llamada así en honor a Cristóbal mártir (su nombre en inglés es en memoria del Conde de Chatham). Es la capital de la provincia y tiene una superficie de 558 km² y una altura máxima de 730 metros. En la mitad sur de la isla, dentro de un cráter en la sierra de San Cristóbal, se encuentra la laguna El Junco, que es el mayor lago de agua dulce del archipiélago. Alberga una gran población de aves y cerca de allí está La Galapaguera, una estación de refugio y cría de tortugas gigantes. Cerca de la ciudad de Puerto Baquerizo Moreno, está el Cerro Tijeretas, una colonia de anidación para fragatas, y a unos diez minutos en autobús, se encuentra La Lobería, una colonia de lobos marinos. En la parte alta de la isla se encuentra la Estación Biológica San Cristóbal, dedicada a la conservación de los bosques del Ecuador. También hay excursiones en barco a sitios cercanos de buceo. "León Dormido" representa los restos de un cono de lava, ahora dividida en dos. "Isla Lobos" es un lugar de anidación de piqueros de patas azules.

Llamada así en honor al primer presidente del Ecuador, Juan José Flores, en cuya administración se tomó posesión del archipiélago (su nombre en inglés es el del rey Carlos II de Inglaterra). También se la conoce como Santa María en honor a una de las carabelas de Colón. Tiene una superficie de 173 km² y una altitud máxima de 640 metros. Entre diciembre y mayo, flamencos rosados y tortugas marinas anidan en esta isla. Aquí se puede encontrar una pequeña población de pingüinos de Galápagos y el endémico sinsonte de Floreana. Se pueden observar interesantes formaciones de coral en la denominada "Corona del Diablo", que es un cono volcánico sumergido.

Llamada así en honor de fray Antonio de Marchena. Tiene una superficie de 130 km² y una altitud máxima de 343,5 metros. Aunque no hay sitios para visitar en esta isla, es posible bucear en las aguas alrededor. Posee gran variedad de flora y fauna como los flamencos y leones marinos. También se pueden observar las tortugas gigantes. Marchena tiene una caldera volcánica con forma elíptica de aproximadamente 7 km de largo por 6 km de ancho, clasificada como grande dentro de la gama de tamaños de las calderas.

Llamada así por la primera ciudad de América (su nombre en inglés honra al vizconde Samuel Hood). Con sus 60 km² es una de las islas menores que conforman el archipiélago de las Galápagos, y es la más antigua de todas, ya que cuenta con alrededor de 3,5 millones de años de existencia. Aunque está deshabitada, en ella viven varias especies animales de interés, como el endémico sinsonte de Española, el piquero de patas azules, la tórtola de las Galápagos, la gaviota de cola bifurcada, la iguana marina y la lagartija de lava. Entre los visitantes, son especialmente populares la Bahía Gardner, que tiene una playa reconocida por su belleza, y Punta Suárez, de interés por el avistamiento de aves.

Llamada así en honor a una de las carabelas de Colón (su nombre en inglés está dedicado al Conde de Abingdon). Es la isla más septentrional de las Galápagos y la novena más grande del archipiélago. Tiene una superficie de 60 km² y una altitud máxima de 780 metros. Aquí se pueden observar gaviotas de cola bifurcada, iguanas marinas, lobos peleteros y gavilanes de las Galápagos. De esta isla era originaria la famosa tortuga, el «Solitario George», último ejemplar conocido de la especie "Chelonoidis abingdonii". También se encuentra aquí uno de los volcanes más activos.

Se desconoce el origen de su nombre, en inglés debe su nombre al marino británico Lord Hugh Seymour. Tiene una superficie de 27 km² y una altitud máxima de 100 metros. Alberga el principal aeropuerto del archipiélago, que fue construido durante la Segunda Guerra Mundial por la Marina de los Estados Unidos para «patrullar» el Canal de Panamá. En esta isla se reintrodujeron iguanas terrestres después de que esta especie nativa fuera totalmente eliminada por los soldados de Estados Unidos aquí acantonados. A lo largo de la isla aún se encuentran los vestigios de los cuarteles de los soldados. Algunos de ellos después de haberse retirado regresan en calidad de turistas. En la isla hay mucha flora silvestre desértica, mayormente poblada de cactus. Desde el aeropuerto al cual llegan los aviones desde el Ecuador continental, cada 10 minutos salen buses sin costo de las aerolíneas, desde el aeropuerto hacia el canal y el puerto. Existen decenas de puntos para practicar surf, snorkel o buceo, con el permiso previo de la Armada del Ecuador. Existe un segundo aeropuerto fuera de servicio que también data de la Segunda Guerra Mundial. Entre la Isla Baltra y la Isla Santa Cruz, se encuentra el Canal de Itabaca, utilizado por taxis acuáticos que llevan a las personas entre las islas. Los barcos operan fuera de la costa para llevar a la gente a otras islas de las Galápagos.

Llamada así en honor a las Capitulaciones de Santa Fe, en las que se otorgó a Cristóbal Colón los títulos de Almirante Mayor de la Mar Océana, Virrey y Gobernador General de las tierras que descubriera (su nombre en inglés es en honor al Almirante Samuel Barrington). Tiene una superficie de 24 km² y una altitud máxima de 259 metros. Santa Fe tiene un bosque de cactus "Opuntia" que son los más grandes del archipiélago, y de palo santo. Tiene una vistosa laguna color turquesa y aguas tranquilas donde se puede realizar snorkel con leones marinos. Sus precipicios costeros son el hogar de gaviotas de cola bifurcada, petreles y otros pájaros tropicales. La iguana terrestre de Santa Fe, endémica de la isla, habita en gran número al igual que la lagartija de lava.

Llamada así en honor a los hermanos Pinzón, capitanes de las carabelas la Pinta y la Niña en la primera expedición de Cristóbal Colón (su nombre en inglés recuerda al Vizconde de Duncan). Tiene una superficie de 18 km² y una altitud máxima de 458 metros. No tiene sitios para visitar y se requiere de un permiso especial de las autoridades para ingresar. Las principales especies forestales están en la isla, en la zona húmeda, se encuentra una especie única del llamado árbol margarita. Pinzón es el hogar de lobos marinos, tortugas gigantes, iguanas marinas y delfines, además de otras especies endémicas.

Llamada así en honor a la ciudad de Génova (Italia), el probable lugar de nacimiento de Colón. Tiene una superficie de 14 km² y una altitud máxima de 76 metros. La isla con forma de herradura tiene una caldera volcánica cuya pared se ha derrumbado, formando la Gran Bahía Darwin, rodeada de acantilados. El lago Arcturus, lleno de agua salada, se encuentra en el centro, y los sedimentos dentro de este lago del cráter tienen menos de 6.000 años de antigüedad. Aunque no hay erupciones históricas se conocen de Genovesa, hay flujos de lava muy jóvenes en los flancos del volcán. Es conocida como "Isla de los Pájaros", a causa de las grandes y variadas colonias de aves que anidan aquí. Hay una gran cantidad de fragatas, gaviotas de cola bifurcada, gaviotas de lava, petreles, pájaros tropicales, pinzones de Darwin y sinsontes de las Galápagos. El sitio denominado "El Barranco" constituye una magnífica meseta para observación de estas aves, especialmente de piqueros enmascarados y de patas rojas. También hay un gran bosque de palo santo.

Llamado así por el Monasterio de La Rábida donde Colón dejó a su hijo durante su viaje de descubrimiento a América (su nombre en inglés se debe al Almirante Jervis). Tiene una superficie de 4,9 km² y una altitud máxima de 367 metros. El alto contenido de hierro de la lava de Rábida ocasiona que la isla tenga un característico color rojizo. El paisaje está lleno de pequeños cráteres volcánicos a lo largo de las laderas y acantilados afilados. Ocasionalmente se pueden observar flamencos y lobos marinos en una laguna de agua salada cerca de la playa, donde pelícanos y piqueros construyen sus nidos. Se han registrado nueve especies de pinzones en esta isla. La rica fauna atrae a un sinnúmero de turistas de cruceros.

Llamada así en honor del noble inglés Lord Hugh Seymour. Tiene una superficie de 1,9 km² y una altitud máxima de 28 metros. Toda la isla está cubierta de vegetación baja y tupida, y tiene una pista para visitantes de aproximadamente 2 km de longitud que cruza la vía de la isla y permite explorar la costa rocosa. En esta isla se halla una gran población de piqueros de patas azules y gaviotas de cola bifurcada. También hay un sinnúmero de iguanas terrestres, que fueron introducidas de la isla Baltra, y que sirvieron para repoblar con esta especie la isla. Asimismo es posible observar una gran cantidad de fragatas y leones marinos con sus crías.

Llamada así en honor al geólogo alemán Theodor Wolf. Tiene una superficie de 1,3 km² y una altura máxima de 100 metros. Se encuentra alejada del grupo principal de islas y no tiene población permanente, por lo que no es accesible para visitar en tierra, no obstante, es un lugar popular para bucear. Anteriormente era conocida como isla Wenman. Aquí habitan focas peleteras, iguanas marinas y tortugas verdes. Entre las aves que se encuentran en esta isla están la fragata, el piquero de patas rojas y el pinzón vampiro. La vida marina de la isla incluye tiburones martillo, tiburones de Galápagos y ocasionalmente tiburones ballena, así como también delfines, mantarrayas y otros peces pelágicos.

Isla Tortuga se encuentra a 2 km al sur de la Isla Isabela. Tiene una superficie de 1,3 km² y una altura máxima de 100 metros. Esta isla es una antigua caldera volcánica, de la cual sólo la mitad permanece fuera del agua. El sitio de buceo está en el lado noreste de la isla, y desciende gradualmente fuera de la vista en las profundidades del Pacífico. Entre los 20 y 30 metros, se pueden observar a tiburones martillo, tiburones de Galápagos y rayas águila. También tiburones punta negra de arrecife frecuentan la zona, dando vueltas atrás y adelante, solos o en pareja. Es uno de los principales sitios de anidación de aves marinas de Galápagos. La posibilidad de avistamientos de tortugas marinas, mantarrayas y leones marinos, es también un incentivo para hacer turismo en esta isla.

Llamada así en honor a Sir Bartholomew Sulivan de la Marina Británica. Tiene una superficie de 1,2 km² y una altitud máxima de 114 metros. Esta isla ofrece algunos de los paisajes más bellos del archipiélago. La isla se compone de un volcán extinto y una variedad de formaciones volcánicas negras rojas, anaranjadas, verdes y brillantes. Los cactus de lava de Galápagos colonizan los nuevos campos de lava. En esta isla se encuentra el afamado Pináculo, que es uno de los lugares más representativos del archipiélago. Aquí se puede bucear y hacer snorkel con los pingüinos, lobos marinos, tiburones punta blanca de arrecife y otros peces tropicales. Estacionalmente, Bartolomé es el sitio de apareamiento y anidación de la tortuga verde. La bahía es también un excelente lugar para ir a nadar. Las bahías gemelas están separadas por un estrecho istmo.

Llamada así en honor a Charles Darwin, quien hiciera famosas las islas a nivel mundial. Tiene una superficie de 1,1 km² y una altura máxima de 168 metros. Esta isla no está abierta para visitas en tierra, los únicos visitantes son los que vienen a bucear. La vida marina en Darwin es diversa, las aguas de la isla atraen tiburones ballena de junio a noviembre, así como también a tiburones martillo, tiburones de Galápagos, tiburones sedosos y tiburones punta negra. Además se pueden encontrar focas peleteras, lobos marinos, delfines y ballenas. En la isla existe una gran población de aves, que incluye fragatas, piqueros de patas rojas, gaviotas de cola bifurcada y el pinzón vampiro.

El archipiélago forma la Provincia de Galápagos, cuya capital es Puerto Baquerizo Moreno. Se conforma por tres cantones:


Las Galápagos fueron declaradas parque nacional en 1959, protegiendo así el 97,5% de la superficie terrestre del archipiélago. El área restante es ocupada por asentamientos humanos que ya existían al tiempo de la declaratoria. Para entonces, aproximadamente 1000 a 2000 personas vivían en cuatro islas. En 1972 un censo determinó que 3488 personas vivían en Galápagos, pero en la década de 1980 este número se había incrementado notablemente a más de 20 000 habitantes.

En 1986 el mar que rodea a las islas fue declarado reserva marina. Unesco incluyó a Galápagos en la lista de Patrimonio de la Humanidad en 1978, y en diciembre de 2001 se amplió esta declaración para la reserva marina.

En el año 2007, fueron incluidas en la Lista del Patrimonio de la Humanidad en peligro, debido al turismo masivo y las especies invasoras. El 29 de julio de 2010, las Islas Galápagos fueron retiradas de la lista de patrimonios en peligro de extinción por el Comité de Patrimonios de la Unesco.

El archipiélago tiene diferentes figuras internacionales que se han aplicado para tratar de garantizar la conservación de Galápagos; entre ellas: Reserva de Patrimonio Natural de la Humanidad, Sitio Ramsar, Santuario de Ballenas, reserva de Biósfera, etc.
La Estrategia Mundial para la Conservación de la Naturaleza identifica a Galápagos como una provinciaI Biogeográfica prioritaria para el establecimiento de áreas protegidas.
A nivel nacional las figuras de Parque nacional y Reserva Marina, reflejan el compromiso asumido por el Gobierno Ecuatoriano de conservar este importante legado para las futuras generaciones de galapagueños, ecuatorianos y para la humanidad en general.

Las especies endémicas de singular importancia que habitan las islas incluyen:







</doc>
<doc id="3323" url="https://es.wikipedia.org/wiki?curid=3323" title="Astronauta">
Astronauta

Astronauta, cosmonauta o taikonauta es el término que designa a todo el personal de un objeto espacial, a la tripulación de una nave espacial e incluso «a toda persona que se encuentre en la luna».

A los viajeros espaciales de la Unión Soviética o entrenados allí y, actualmente, en Rusia, se les denomina normalmente cosmonautas —que proviene del término ruso "kosmonavt" (космонавт), que a su vez deriva de las palabras griegas "kosmos" (κοσμος, universo) y "nautes" (ναύτης, navegante)—. De manera similar, a aquellos de la República Popular China o entrenados ahí se les llama "taikonautas" —neologismo formado a partir del término chino 太空 ("tàikōng", espacio) y del griego ναύτης ("nautes", navegante) en semejanza con "astronauta" y "cosmonauta" que derivan del griego; la palabra oficial china que designa a un astronauta es 宇航員 (yǔhángyuán) pero el término taikonauta fue propuesto por Chiew Lee Yih en mayo de 1998 en Internet y se aceptó rápidamente en el mundo anglosajón—.

Actualmente diferentes agencias especiales buscan llevar astronautas a Marte.

La primera persona en salir al espacio en toda la Historia fue el cosmonauta Yuri Gagarin al ser lanzado el 12 de abril de 1961 a bordo de la nave Vostok 1. La primera mujer en volar al espacio fue Valentina Tereshkova, la cual salió al espacio el 16 de junio de 1963 a bordo de la Vostok 6. German Titov, cosmonauta soviético, fue el segundo hombre en órbita terrestre después de Gagarin.

En el marco del programa Intercosmos, también fueron al espacio cosmonautas del Bloque del Este y otros países aliados de la Unión Soviética, como Cuba. También Francia y la India, que no eran estados socialistas, participaron de Intercosmos.

Durante el programa Apolo, (1961-1975), los Estados Unidos enviaron un total de 30 misiones tripuladas: seis en el proyecto Mercury, 10 en el programa Gemini, 11 en el programa Apolo, tres en el programa Skylab y uno en el programa de pruebas Apolo-Soyuz. Estas 30 misiones proporcionaron 71 oportunidades de vuelo individual: seis en el Mercury, 20 en el Gemini, 33 en el Apolo, nueve en el Skylab, y tres en el Apolo-Soyuz. Estos puestos fueron cubiertos por 43 personas. De entre ellos, cuatro hicieron un total de cuatro vuelos, tres hicieron un total de tres vuelos, 10 un par de vuelos, y los restantes 26 volaron solo una vez. Algunos de ellos hicieron vuelos adicionales con el transbordador espacial.

De los 31 vuelos de la era Apolo, tres fueron suborbitales y nueve consistieron en misiones lunares. Los restantes 20 fueron vuelos orbitales terrestres. Los nueve vuelos lunares proporcionaron la oportunidad de realizar este tipo de vuelos a 24 personas. Sólo tres personas volaron dos veces a la Luna. Los 6 alunizajes que se produjeron con éxito llevaron a 12 personas a la Luna. Ninguno alunizó dos veces, aunque dos de ellos ya habían volado a la Luna al menos una vez, cinco de ellos habían hecho ya vuelos no lunares y cinco no tenían ningún tipo de experiencia en vuelos espaciales.

Todos los vuelos del programa Mercury y tres del programa Gemini tenían una tripulación formada por novatos, igual que uno de los vuelos del programa Skylab. Sin embargo todas las misiones del programa Apolo incluían al menos un astronauta veterano. Solo dos vuelos, las misiones lunares y las pruebas incluían una tripulación formada solamente por veteranos.

El primer grupo de astronautas estadounidenses se seleccionó en abril de 1959, para el proyecto Mercury de la NASA. Este grupo, que fue conocido como los "Mercury Seven" ("los Siete del Mercurio"), estaba compuesto por Scott Carpenter, Gordon Cooper, John Glenn, Gus Grissom, Wally Schirra, Alan Shepard y Deke Slayton. Todos eran pilotos de pruebas militares, un requisito que dictó el presidente Eisenhower para simplificar el proceso de selección.

Los siete miembros del primer grupo de astronautas fueron al espacio al final, aunque uno, Deke Slayton, no voló en una misión "Mercurio" por razones médicas. Finalmente, participaría en la misión Apolo-Soyuz. Cada uno de los otros seis viajaron al espacio en una misión Mercurio. Para dos de ellos, Scott Carpenter y John Glenn, la misión Mercurio fue su único vuelo en la Era Apolo Glenn, posteriormente, fue al espacio en la Lanzadera espacial. Tres de ellos, Gus Grissom, Gordon Cooper y Wally Schirra, también volaron en una misión durante el programa Gemini. Alan Shepard no voló en misiones Gemini debido a razones médicas, pero, más tarde, saldría al espacio en una misión Apolo. Fue el único astronauta del programa Mercurio que fue a la Luna. Wally Schirra también voló en el Apolo, además de en el Mercurio y en el Gemini, y fue el único astronauta que voló en los tres tipos de naves espaciales. Gus Grissom fue incluido en la tripulación del primer lanzamiento del Apolo, el Apolo 1, pero murió en un incendio en la plataforma de lanzamiento durante su entrenamiento.

La NASA seleccionó un segundo grupo de astronautas en septiembre de 1962. Este grupo estaba formado por Neil Armstrong, Frank Borman, Charles Conrad, Jim McDivitt, Jim Lovell, Elliott See, Tom Stafford, Ed White y John Young. Todos ellos participaron en misiones del programa Gemini excepto Elliott See, que murió en un accidente de vuelo mientras se preparaba para su viaje en el Gemini. Todos los demás volaron, también, en el Apolo, salvo Ed White, que murió en un incendio en la plataforma de lanzamiento durante su entrenamiento para el primer vuelo del Apolo. Tres de este grupo: McDivitt, Borman y Armstrong, realizaron un solo vuelo en el Gemini y en el Apolo. Cuatro de los otros: Young, Lovell, Stafford y Conrad, efectuaron dos vuelos cada uno en el Gemini y, al menos un vuelo en el Apolo. Young y Lovell volaron dos veces, cada uno, en el Apolo. Conrad y Stafford también realizaron segundos vuelos en la nave Apolo, Conrad en el Skylab y Stafford en la misión Apolo-Soyuz. Seis de este grupo: Borman, Lovell, Stafford, Young, Armstrong y Conrad, viajaron a la Luna. Lovell y Young fueron dos veces a la Luna. Armstrong, Conrad y Young caminaron por la Luna. John Young también voló, posteriormente, en la Lanzadera espacial.
Cinco miembros del tercer grupo de astronautas, que la NASA seleccionó en octubre de 1963, también realizaron misiones durante el programa Gemini. Fueron: Buzz Aldrin, Eugene A. Cernan, Michael Collins, Richard Gordon y David Scott. Cada uno hizo un solo vuelo en la misión Gemini, y al menos, otro en el programa Apolo. Scott y Cernan salieron al espacio una segunda vez en otra misión Apolo. Todos los integrantes de este grupo fueron a la Luna, de ellos, Cernan fue dos veces. Aldrin, Scott y Cernan caminaron por la Luna, en las misiones Apolo 11, Apolo 15 y Apolo 17, respectivamente.

De los 30 vuelos de la era Apolo, tres fueron suborbitales y nueve consistieron en misiones lunares. Los restantes 20 fueron vuelos orbitales terrestres. Los nueve vuelos lunares proporcionaron la oportunidad de realizar este tipo de vuelos a 24 personas. Solo tres personas volaron dos veces a la Luna. Los 6 alunizajes que se produjeron con éxito llevaron a 12 personas a la Luna. Ninguno alunizó dos veces, sin embargo dos de ellos ya habían volado a la Luna al menos una vez, cinco de ellos habían hecho ya vuelos no lunares, y cinco no tenían ningún tipo de experiencia en vuelos espaciales.

Siendo entonces, Neil Armstrong, el primer astronauta y el primer ser humano en la historia en pisar la Luna el 21 de julio de 1969, en la misión Apolo 11. Fue el mayor acontecimiento logrado por una agencia espacial.

El primer taikonauta de la historia fue Yang Liwei al salir al espacio en la nave Shenzhou 5 en octubre de 2003. Los taikonautas Fei Junlong y Haisheng fueron los siguientes en salir al espacio en la Shenzhou 6 en octubre de 2005. En 2012 China envió al espacio a la primera mujer "taikonauta", Liu Yang.

El éxito de una misión espacial implica que los astronautas cuenten con una técnica fiable, una serie de conocimientos especializados, una buena forma física y cierta estabilidad psíquica.

Entre las secuelas fisiológicas más comunes tras las estancias extraterrestres se encuentran los trastornos del sueño, la debilitación del sistema inmunitario, algunas atrofias musculares, la erosión de huesos y la carga radiactiva, que provoca que, a mayor tiempo en el espacio, más aumente la tasa de mutación de los cromosomas del ser humano y, por tanto, el riesgo de cáncer.

La ingravidez repentina es la causa de la mayor parte de los problemas físicos en el espacio: mareos, falta de apetito, náuseas y vómitos, los cuales solo empiezan a remitir de 2 a 4 días después. Con todo, a largo plazo se presentan otros problemas derivados de la falta de gravedad; el más importante es la destrucción de masa muscular, que empieza a producirse apenas dos semanas después del inicio del vuelo, debe contrarrestarse con un intensivo ejercicio físico por parte de los astronautas, es por esto que actualmente en la estación espacial internacional los astronautas realizan 2 horas de ejercicio, además que les ayuda a la parte emocional.

Otro problema frecuente es la hinchazón de la cara (en inglés, Puffy Face) en los primeros días en el espacio, debido a un exceso de sangre proveniente de los miembros superiores. Las repercusiones en la rigidez facial pueden ocasionar problemas de entendimiento con otros compañeros de misión.

En cuanto a las funciones cognitivas básicas (percepción, memoria y pensamiento lógico) se mantiene estables. Sin embargo, por lo que respecta al área psicomotora son perceptibles determinadas pérdidas funcionales: algunos movimientos voluntarios se ralentizan y se vuelven imprecisos, y la ejecución de tareas simultáneas se hacen más difíciles.

Psíquicamente, el aislamiento durante las misiones puede provocar un estado de "astenia", sobre todo a partir de la mitad de la misión: pasividad en aumento, fallos de atención, sensación de agotamiento, irritabilidad, depresión, etc. Debido a esto, en la Estación Espacial Internacional se viene desde hace tiempo aplicando el llamado "Human Behavior Performance Program" con el objeto de combatir el aburrimiento y el aislamiento social. Entre los métodos utilizados se encuentran el poner a disposición de los astronautas películas, discos, páginas personales para relacionarse con la vida en la tierra, videoconferencias familiares y una conferencia psicológica privada cada dos semanas con un psicólogo en Tierra.

En el nivel colectivo, las condiciones especiales de la vida en el espacio pueden provocar tensiones y conflictos. Además, las diferencias culturales pueden también generar problemas en el grupo.



</doc>
<doc id="3324" url="https://es.wikipedia.org/wiki?curid=3324" title="14 de octubre">
14 de octubre

El 14 de octubre es el 287.º (ducentésimo octogésimo séptimo) día del año en el calendario gregoriano y el 288.º en los años bisiestos. Quedan 78 días para finalizar el año.











</doc>
<doc id="3325" url="https://es.wikipedia.org/wiki?curid=3325" title="13 de octubre">
13 de octubre

El 13 de octubre es el 286.º (ducentésimo octogésimo sexto) día del año en el calendario gregoriano y el 287.º en los años bisiestos. Quedan 79 días para finalizar el año.










</doc>
<doc id="3326" url="https://es.wikipedia.org/wiki?curid=3326" title="12 de octubre">
12 de octubre

El 12 de octubre es el 285.º (ducentésimo octagésimo quinto) día del año en el calendario gregoriano y el 286.º en los años bisiestos. Quedan 80 días para finalizar el año.








</doc>
<doc id="3327" url="https://es.wikipedia.org/wiki?curid=3327" title="11 de octubre">
11 de octubre

El 11 de octubre es el 284.º (ducentésimo octogésimo cuarto) día del año en el calendario gregoriano y el 285.º en los años bisiestos. Quedan 81 días para finalizar el año.
















</doc>
<doc id="3328" url="https://es.wikipedia.org/wiki?curid=3328" title="10 de octubre">
10 de octubre

El 10 de octubre es el 283.º (ducentésimo octogésimo tercer) día del año en el calendario gregoriano y el 284.º en los años bisiestos. Quedan 82 días para finalizar el año.









</doc>
<doc id="3329" url="https://es.wikipedia.org/wiki?curid=3329" title="Ludwig von Mises">
Ludwig von Mises

Ludwig Heinrich Edler von Mises (Lemberg; 29 de septiembre de 1881-Nueva York, 10 de octubre de 1973) fue un economista austríaco de origen judío, historiador, filósofo y escritor liberal que tuvo una influencia significativa en el moderno movimiento libertario en pro del mercado libre y en la Escuela Austríaca.

Planteó lo perjudicial del poder e intervención gubernamentales en la economía que, según su teoría, por lo general llevan a un resultado distinto al natural y por esto muchas veces perjudicial para la sociedad, ya que generan caos en el largo plazo.

Nació en Lemberg, capital de Galitzia en el antiguo Imperio austrohúngaro (actualmente Ucrania) hijo de Arthur von Mises (ingeniero de ferrocarriles y funcionario público) y Adele Landau von Mises. Su hermano menor, Richard von Mises fue un célebre físico. La familia Mises se mudó a Viena siendo Ludwig niño. En 1892 ingresó en el "Akademisches Gymnasium", donde recibió una formación humanista. Fue compañero de Hans Kelsen. Desde temprana edad Mises se interesó por la historia y la política. Después de graduarse en 1900, comenzó estudios de derecho y administración pública en la Universidad de Viena.

Bajo la dirección de Carl Grünberg, Mises comenzó siendo un exponente de la llamada Escuela Histórica de Administración Pública, que daba mayor importancia a la búsqueda de datos que al análisis teórico. Pero en otoño de 1903 leyó la obra "Principios de Economía Política" de Carl Menger, texto fundador de la escuela económica austríaca. El libro le llevó a buscar un enfoque más teórico, y en los años sucesivos profundizó sus estudios de teoría económica, especialmente en el seminario de Eugen von Böhm-Bawerk, ex ministro de Hacienda y prócer de la Escuela Austríaca.

Mises se graduó en febrero de 1906. Ingresó como funcionario en el ministerio de hacienda austríaco, pero después de unos pocos meses abandonó harto de la excesiva burocracia. Durante los dos años siguientes trabajó como pasante en un bufete y dio clases de economía. En 1909, empezó a trabajar en la Cámara de Comercio e Industria de Viena, donde permaneció los siguientes veinticinco años. La Cámara era una organización cuasiestatal y a través de sus publicaciones Mises ejerció una influencia considerable en la política austríaca.

En 1912 publicó "La teoría del dinero y el crédito", obra en la que aplicaba la teoría de valor de Carl Menger al dinero y presentaba una nueva teoría de la coyuntura económica en la que las crisis eran provocadas por la distribución inadecuada de los recursos debido a la inflación. Demostró que la cantidad de dinero en la economía no era neutral y que su aumento tenía efectos redistributivos.

Durante la I Guerra Mundial sirvió como oficial en la artillería austrohúngara y fue asesor económico en el Ministerio de la Guerra. Sus experiencias bélicas le sirvieron para desarrollar sus teorías sobre el intervencionismo estatal. El último año de la guerra recibió el prestigioso nombramiento de profesor extraordinario en la Universidad de Viena.

Después de la guerra participó como adjunto en el gobierno austríaco ocupándose de asuntos financieros con el extranjero. Su principal logro durante esta época fue disuadir a su antiguo amigo Otto Bauer, líder del partido socialista, de intentar un golpe de estado bolchevique. También escribió un libro explicando el colapso del Imperio Austrohúngaro. En "Nación, Estado y Economía" (1919) afirmaba que el imperialismo germano era consecuencia de la aplicación del poder del Estado para resolver los problemas de las comunidades multiculturales de Alemania y Austria.

Posteriormente publica "El socialismo: un análisis económico y sociológico" (1922) donde afirmó que el sistema comunista no podía ser eficiente ya que le faltaba el mecanismo de precios que hacía que la distribución de los recursos fuera la adecuada, como sucedía en el sistema capitalista. Este libro tuvo gran influencia muchos años después (debido a las dificultades que tuvo su difusión y su tardía traducción al inglés) al advertir y predecir con mucha antelación el fracaso del socialismo evitando valoraciones éticas y morales.

Durante los años veinte, desde su puesto, Mises luchó con éxito contra la inflación y utilizó su influencia para imponer las reformas monetarias y financieras que experimentó Austria en 1922. 
No pudo impedir, no obstante, el constante aumento de la reglamentación estatal que, en su opinión, dilapidaba la hacienda pública. Esto le llevó a postular la teoría de que el intervencionismo estatal era totalmente contraproducente. Excluyó como soluciones las posibles terceras vías y defendió el laissez-faire como único remedio. En 1927, publicó una concisa presentación de su política filosófica utilitaria en "El liberalismo".

A finales de los años veinte publicó una serie de artículos sobre el carácter epistemológico de la ciencia económica. Mises afirmó que la ciencia económica no podía ser refutada ni comprobada a través de los datos observables. La Economía era una ciencia en la que predominaban los juicios "a priori" al igual que las matemáticas, la lógica o la geometría. No obstante, opinaba que la Economía era parte de una ciencia social más amplia, la Praxeología. 

En 1934 deja Austria y se traslada a Ginebra para empezar un nuevo periodo docente. Desde entonces y hasta 1940 se haría cargo de la cátedra de Relaciones económicas internacionales en la Universidad de Ginebra.

En 1940 tuvo que huir de Europa por temor a ser apresado por los nazis. Se instaló en Nueva York y se naturalizó estadounidense en 1946. A partir de 1945 fue profesor visitante en la Universidad de Nueva York hasta 1969, sin llegar nunca a adquirir una plaza interina dentro de esta institución. Fue en esta época cuando retoma la investigación económica y publica "La acción humana" (1949).
Mises fue, con su renovación del liberalismo clásico a través de la Escuela Austríaca de Economía, uno de los principales mentores espirituales del liberalismo libertario y su obra "La acción humana" (1949) ejerció gran influencia en intelectuales de raigambre austríaca como Friedrich Hayek, Murray Rothbard, Hans Sennholz, George Reisman, Ralph Raico, Leonard Liggio, Tibor Machan, Peter Boettke, Roger Garrison, Manuel Ayau y Joseph Keckeissen. Pero también fue vital para economistas no pertenecientes a su escuela (la mayoría de ellos Premios Nobel) y pensadores de muy diferentes áreas: Max Weber, Joseph Schumpeter, Oskar Lange, Henry Simons, Lionel Robbins, Maurice Allais, Milton Friedman, John Hicks y la lista sigue hasta el actual economista experimental Vernon Smith entre tantos otros. Von Mises murió en 1973 en el hospital St. Vincent de Nueva York.

La praxeología es para Mises el método para estudiar las ciencias sociales; siendo el equivalente su estudio al de las ciencias experimentales pero sin la capacidad de realizar experimentos. La praxeología hace referencia al estudio de cómo la mente humana estructura el pensamiento de modo que, conociendo dicha estructura, podemos deducir a priori los postulados que guían las decisiones individuales de cada sujeto. Este método sería similar al usado en matemáticas y en lógica.
Para Mises no es posible un estudio a través de la experiencia por la imposibilidad de que existan constantes en las relaciones entre variables. 
En la obra “La Acción Humana”, Mises critica el método matemático y la observación de datos como estudio de la economía, de hecho, considera que dichos métodos pueden ser usados en el análisis de la historia económica, pero no son válidos para entender o predecir el comportamiento humano. Ello se debe a que las simplificaciones y los problemas técnicos en la recogida de datos modifican de tal modo la relación entre las variables que puede alterar la relación y causalidad que, a priori y basándonos en hipótesis deductivas, persigue el ser humano para alcanzar su fin. Dicho de otro modo, todo estudio de la economía a través de datos empíricos es un estudio de hechos pasados y por tanto no sirve para deducir una pauta de comportamiento en los individuos.
Dentro del enfoque praxeológico, las hipótesis que se realizan sobre la acción humana llevan vinculadas factores como el valor de la operación, la riqueza, los términos de intercambio, precios y costes y también su valoración subjetiva ligada a la escala valorativa del sujeto, importancia relativa, escasez, etc. De este modo puede extraerse una acción humana que corresponda de forma racional a la maximización del sujeto de su bienestar individual.
La consecuencia de aplicar este sistema a las ciencias sociales es que el estudio de la “economía” no solo mide las relaciones humanas mesurables, sino aquellas que no pueden medirse en términos monetarios, pero presentan relaciones de intercambio, siendo dicho conjunto conocido como “acción humana” y representando este concepto un tramo de acción acotado, que posteriormente formando fenómenos complejos sea el que defina el comportamiento del individuo. Este conjunto nuevo de conocimiento abarca todas las disciplinas de las ciencias sociales, similar a la sociología pero sin el carácter historicista que Mises le atribuye a esta última.
Otra consecuencia es que para la Escuela Austriaca, todas las demostraciones empíricas no sirven para nada, ya que se basan en el estudio de datos, pero esta relación puede cambiar. Mises no contempla una categoría de ciencia donde sus leyes sean mutables y por ello todos los axiomas demostrados matemáticamente no tienen la consideración de economía, en todo caso podría considerarse estudio de la historia económica. El autor considera que las acciones extraídas de la praxeología son inmutables y por tanto leyes humanas y que dichas leyes humanas no dependen del tiempo ni de otros factores, por lo que pueden observarse o no dentro de la realidad y del estudio de datos dependiendo de la complejidad de los fenómenos.

La teoría austriaca del ciclo económico hace énfasis en que toda producción requiere tiempo. También transcurre tiempo entre el inicio de la producción y el consumo, por lo que se hace evidente la importancia que estos autores le daban al ahorro en relación con el tiempo en el que se invierte y la duración de dicha inversión. Esta relación será crucial en la forma en que los tipos de interés provocan cambios en la estructura de consumo de bienes de consumo frente a bienes de capital.
Este ahorro procede de los ingresos no gastados de los agentes y, a través de los bancos, financia el proceso empresarial. Esta relación puede ser modificada por la autoridad monetaria con el fin de aumentar la actividad económica, produce una disminución del tipo de interés, lo que a priori consigue su objetivo. Sin embargo, esa nueva actividad económica financiada con bajos tipos de interés son realmente actividades que no se llevarían a cabo en situaciones normales; son actividades especulativas y que generan poco o ningún valor añadido.
Esta nueva situación solo puede mantenerse en el tiempo si se mantienen los tipos de interés anormalmente bajos, lo que conduciría a una situación cada vez más complicada ya que nuevos recursos irán a parar a estas actividades especulativas. Por otro lado, si cesa la política de tipos de interés bajo, se producirá una pérdida de valor debido a que no es posible retirar todos los recursos asignados a los sectores poco productivos. 
Otro efecto producido por el aumento de la oferta monetaria es el aumento de la inflación, que cambia la relación entre los bienes de inversión y de consumo y añade problemas al sostenimiento de la política de tipos de interés bajo.

La teoría austriaca del dinero se presenta en la obra “Teoría del dinero y el crédito (1912). A partir de las aportaciones de Menger sobre la utilidad marginal, Mises aplica un modelo de oferta y demanda para explicar el origen del valor del dinero. 
El elemento central que compone la teoría del valor misiana es que el cambio objetivo (poder adquisitivo) del dinero es el que genera las peculiaridades que tiene, ya que sin capacidad de poder adquisitivo no se haría uso del dinero. Es este elemento el determinante de la demanda y dependerá del valor subjetivo que cada ciudadano tenga del dinero. 
Por otro lado, el valor subjetivo de cada individuo depende del valor subjetivo del resto de bienes económicos en relación con el dinero. Para los autores austríacos estos dos tipos de valores están relacionados a través del teorema de la regresión monetaria.
La demanda de dinero en el día “D” se basa en el poder adquisitivo que poseía en el día “D-1”. Este mismo poder adquisitivo surge por la intersección de la oferta del dinero en el día “D-1” y su demanda que, basándonos en el valor subjetivo de los individuos, situamos nuevamente en el valor determinado por el poder adquisitivo un día antes, en el día “D-2”. Esta regresión puede desarrollarse ad infinitum pero carecería entonces de sentido. Para el autor, esta dinámica empieza en el momento en el que el oro se usaba únicamente como bien, pero su uso como medio de cambio (en palabras misianas su valor de cambio objetivo) aún no existía. En el momento en el que en un pequeño grupo se estandariza el uso de oro como medio de cambio para evitar la ineficiencia de los intercambios del trueque, este posee ya valor de cambio objetivo y nace la dinámica de mercado que produce que este medio se extienda en el resto de regiones.
En cuanto a la oferta monetaria de este sistema, para Mises debe estar basada en el patrón oro. Dado que el valor que posee el dinero está basado en una cantidad de oro, este valor no se devalúa.
Otra característica que define a la escuela austriaca es que no aceptan la existencia de un término que resuma el sistema general de precios. Para los autores de esta escuela, los precios de todos los bienes, incluido el dinero, se puede expresar en infinidad de relaciones de intercambio respecto al resto de bienes que dependen del valor subjetivo que otorgue cada individuo a dichos bienes. De este modo, cualquier “resumen” de los precios en un solo valor o conjunto de valores únicamente hacer desaparecer los matices generados por un fenómeno complejo en la determinación de precios por un valor más comprensible pero carente de importancia.

Con la publicación de “El cálculo económico en la comunidad socialista” (1920), Mises empieza una crítica al sistema socialista que complementa su animadversión por la intervención estatal. La tesis principal presentada en esta obra es que en un sistema donde los precios muestran una limitada relación de intercambio, la información que aporta a los agentes es también limitada.
En una economía libre, el cálculo monetario permite apreciar a los agentes las potencialidades económicas. En una economía donde la única fuente de rentabilidad es satisfacer las necesidades del consumidor, los empresarios buscaran cubrir dichas necesidades de la manera más barata posible, garantizando la eficiencia y el progreso. 
Si el Estado interviene en la economía, incentiva procesos productivos ineficientes y modifica la relación entre precios relativos y la utilidad relativa que obtienen los agentes económicos.
En una economía socialista los precios no solo perjudican al mercado de bienes de consumo, donde las preferencias individuales son modificadas como hemos comentado por la intervención estatal, si no que los medios de producción y el mercado de bienes de producción óptimos para producir el bien final deseado no se realiza por el proceso de mercado y por tanto, no se conocerá la eficiencia de dichos métodos.
En resumidas cuentas, el desconocimiento de los agentes de los costes que suponen sus acciones lleva a que la producción y el intercambio se realicen sobre la base de criterios no económicos y por tantos ineficientes.
Desde el punto de vista de la política monetaria, los órganos gubernamentales tienen a incentivar la economía mediante tipos de interés bajos, lo que provoca inversiones improductivas y conduce a una situación que se desarrolla en la “Teoría austriaca del ciclo económico”.

Ludwig von Mises publicó a lo largo de su vida más de doscientos ensayos en los que trató diversos temas, como la aplicación del método positivo en economía, el estatismo y la educación. 
Una de las aportaciones más destacables es su negativa a aceptar el equilibrio general de los clásicos y los neoclásicos (tomando como referencia el modelo de Walras). En economía es imposible determinar un equilibrio general donde todas las variables puedan determinarse de forma simultánea. Para Mises el protagonista de la economía es el emprendedor, de manera que tendrá éxito siempre que los precios generados en el mercado cubran sus pérdidas, de este modo irán arruinándose los empresarios menos competitivos favoreciendo la innovación y el progreso. Todo este proceso de mercado hace que los condicionantes de la oferta y de la demanda produzcan cambios en el mercado de forma continua, por lo que un modelo estático no puede plasmar la realidad.

Por intermediación de Henry Hazlitt (periodista económico autor del librito clásico "La economía en una lección"), Ludwig von Mises conoció a la célebre filósofa y escritora objetivista Ayn Rand, de quien ya había leído "El manantial", habiéndole gustado mucho.
Ayn Rand, que ya tenía una cierta notoriedad pública en Estados Unidos, comenzó una campaña concertada para dar a conocer la obra de von Mises: publicó reseñas de sus libros, lo citó en sus artículos y discursos, asistió a varias conferencias de Mises y, en fin, recomendó su lectura a sus seguidores. Varios economistas han afirmado que es en parte gracias a los esfuerzos de Ayn Rand el que la obra de Mises haya llegado a un público tan amplio. (Fuente: "The Passion of Ayn Rand", de Barbara Branden)

Selección de alguna de sus obras:






</doc>
<doc id="3330" url="https://es.wikipedia.org/wiki?curid=3330" title="29 de septiembre">
29 de septiembre

El 29 de septiembre es el 272.º (ducentésimo septuagésimo segundo) día del año en el calendario gregoriano y el 273.º en los años bisiestos. Quedan 93 días para finalizar el año.









</doc>
<doc id="3331" url="https://es.wikipedia.org/wiki?curid=3331" title="30 de septiembre">
30 de septiembre

El 30 de septiembre es el 273.º (ducentésimo septuagésimo tercer) día del año en el calendario gregoriano y el 274.º en los años bisiestos. Quedan 92 días para finalizar el año.


















</doc>
<doc id="3332" url="https://es.wikipedia.org/wiki?curid=3332" title="1 de octubre">
1 de octubre

El 1 de octubre es el 274.º (ducentésimo septuagésimo cuarto) día del año en el calendario gregoriano y el 275.º en los años bisiestos. Quedan 91 días para finalizar el año.








</doc>
<doc id="3334" url="https://es.wikipedia.org/wiki?curid=3334" title="Juan de Garay">
Juan de Garay

Juan de Garay (1528 - 1583) fue un hidalgo, explorador, conquistador y gobernante colonial español que tuvo un importante papel en la organización de la parte atlántica de Sudamérica.

Juan de Garay destacó por su actuación en la gobernación del Río de la Plata y del Paraguay por haber sido el fundador de la ciudad de Santa Fe en 1573 en su primera ubicación, por lo cual fue asignado al año siguiente como su teniente de gobernador, para convertirse en 1577 en el teniente de gobernador general de Asunción. En 1580 fundó la ciudad de Buenos Aires, con el nombre de "Ciudad de la Trinidad", en el lugar donde en 1536 Pedro de Mendoza había fundado un fuerte con el nombre de " Real de Nuestra Señora Santa María del Buen Ayre".

Juan de Garay habría nacido en 1528 en un lugar por determinar del nordeste de la ya unificada Corona de España. Su lugar exacto de nacimiento es polémico: mientras unas fuentes señalan a la ciudad vizcaína de Orduña (actual País Vasco), otras apuntan al municipio burgalés de Junta de Villalba de Losa (actual Castilla y León). Si bien ambas localidades son vecinas, la de Losa fue y sigue siendo una zona castellana, por lo cual hay que tener en cuenta que él mismo se definía como «vizcaíno» y así lo expresaría su descendencia. No se ha encontrado la fe de bautismo de Garay ni en Losa ni en Orduña.

No hay apenas referencias a la infancia de Juan de Garay. Si en cuanto al lugar de nacimiento hay dudas, también las hay en cuanto al año. No se sabe a ciencia cierta cuándo nació realmente y podría situarse entre diciembre de 1527 y enero de 1529, y muchas veces aparece el año 1528, más aceptado por ser el promedio de ambas fechas y por contener la de su antroponimia que sería el 24 de junio, ya que su nombre no lo heredaba de su progenitor y ni del de su padrastro, como tampoco del de sus abuelos paternos ni maternos, como era costumbre dentro de los linajes nobles.

Su madre era Lucía de Mendieta y Zárate (n. Orduña, ca. 1512) y su padre fue el noble Clemente López de Ochandiano y Hunciano (n. Orduña, ca. 1491), un hijo de los hidalgos Diego López de Ochandiano y de María de Hunciano, pero sería criado por su tío materno, el licenciado Pedro Ortiz de Zárate hasta que su madre se uniera en matrimonio con Martín de Garay quien lo reconoció como su hijo, dándole su apellido, aunque Juan de Garay ostentaría el blasón de Ochandiano de su verdadero progenitor: ""grifo con bordura cargada con ocho aspas"".

La versión que apoya a Orduña como el lugar de nacimiento de Garay dice que el día 7 de octubre de 1535, debido a un fuerte incendio de esta localidad, la familia de Garay debió trasladarse al vecino pueblo de Villalba de Losa, en donde su tío Pedro y su esposa Catalina Uribe y Salazar eran propietarios de otras casas, ya que su palacio en aquella ciudad se había incendiado.

En el año 1543, cuando Garay contaba con unos 15 o 16 años de edad, acompañó a su familia materna al gran Virreinato del Perú, ya que su tío Pedro Ortiz de Zárate y Mendieta (Orduña, ca. 1485 - Lima, 1547) había sido nombrado oidor de la Real Audiencia de Lima con el nuevo virrey Blasco Núñez Vela quien portaba las famosas ordenanzas del emperador Carlos V, conocidas como Leyes Nuevas, que había sancionado en Barcelona el 20 de noviembre de 1542 con el objetivo de mejorar el trato y calidad de vida de los aborígenes sometidos en América y además mandaba quitar las encomiendas a los que habían participado en el bando pizarrista durante la guerra civil peruana.

Juan de Garay y su familia de parte materna: sus tíos Pedro y Catalina, además de sus primos —el primogénito Pedro Ortiz de Zárate, Ana de Salazar y el menor Francisco de Uribe— zarparon hacia América el 3 de noviembre de 1543 desde el puerto de Sanlúcar de Barrameda.

Hicieron escala en las islas Canarias, cruzaron el océano Atlántico y el mar Caribe para llegar el 10 de enero del siguiente año al puerto indiano de Nombre de Dios en Centroamérica, y posteriormente por tierra pasaron a la ciudad de Panamá. Por diferentes motivos, los Ortiz de Zárate retrasaron su llegada a Sudamérica que lo harían a través del océano Pacífico, y entraron en Lima el 10 de septiembre de 1544.

Además de su tío que ocuparía el cargo de oidor, compondrían la recién fundada real audiencia: Diego Vásquez de Cépeda, Juan Álvarez y Juan Lissón de Tejada. La futura rigidez en el gobierno del virrey Núñez Vela por disposición imperial generó enfrentamientos, que llevaron a una nueva guerra civil con los partidarios de Gonzalo Pizarro. El joven Garay fue fiel a su tío que estaba de parte del virrey y más adelante participaría activamente contra Pizarro.

En marzo de 1547 murió su tío materno Ortiz de Zárate, después de recibir la visita de su yerno Blas de Soto —un hermano uterino de Gonzalo Pizarro— que se había casado con su única hija, Ana de Salazar.

En aquellos enfrentamientos civiles Juan de Garay había conocido en su morada al vascongado Martín de Robles quien al fallecimiento del tío de aquel, se aposentó unos días en casa del difunto ya que el desamparado Garay con tan solo diecinueve años de edad no sabría qué hacer, por lo cual, lo convenció para empuñar las armas contra los insurrectos, y posteriormente se transformaría en un excelente soldado.

Juan de Garay hizo la campaña de La Gasca, en la que participaba el capitán Robles, hasta la batalla de Jaquijahuana o del valle de Sacsahuana del 9 de abril de 1548, a 25 km del Cuzco.

En 1549, Juan de Garay formó parte de la expedición de Juan Núñez de Prado que había sido nombrado gobernador del Tucumán —en la actual Argentina— enviado por Pedro de la Gasca quien era el presidente de la Real Audiencia de Lima, siendo el virrey peruano y marqués de Cañete, Antonio Hurtado de Mendoza.

Núñez de Prado quien antes de gobernador fuera el alcalde de Potosí, fundó en el año 1550 la ciudad de «El Barco I» —el primer asentamiento de la actual ciudad argentina de Santiago del Estero, y en donde diez años después Zurita fundara la ciudad de Cañete— pero a finales de mayo o junio de 1551 hizo mudarla al noroeste, en los valles del río Calchaquí —probablemente cercana al actual pueblo salteño de San Carlos— y se la conocería como «El Barco II», esto ocurrió por problemas jurisdiccionales con el gobernador de Chile, Pedro de Valdivia, que había enviado a su segundo Francisco de Villagra a resolver la situación.

En junio de 1552 los oidores de la Audiencia limeña ordenaron nuevamente su traslado hacia la región de los Juríes, por lo cual fue asentada sobre la margen del río Dulce —actual provincia argentina de Santiago del Estero— y de esta forma pasó a conocerse como «El Barco III», aunque en 1553, Núñez de Prado y algunos de sus hombres fueron apresados por Francisco de Aguirre, en la citada población, ya que seguía presentando problemas de jurisdicción, entonces este último resolvería volver a trasladarla a su actual emplazamiento, rebautizándola con el nombre de «Santiago del Estero del Nuevo Maestrazgo».

Juan de Garay no fue detenido por Aguirre aunque también pasó a Chile pero como proveedor del ejército. Núñez de Prado, estando preso en esas tierras, apeló ante el virrey del Perú y por mandato de los oidores de la Audiencia fue enviado a Lima, en donde fue juzgado, y luego de darle la razón, se lo liberó y confirmó mediante real provisión del 13 de febrero de 1555, en el cargo de gobernador del Tucumán, del que Aguirre lo había destituido, pero en una nueva expedición en que Garay volvió a acompañarlo, siguió participando en la misma aún después de desaparecer aquel de la escena política y de la historia.

En 1556 Garay se mudó a Potosí, cuatro años antes de la Primera Guerra Calchaquí, y se relacionó de nuevo estrechamente con sus parientes que residían allí, especialmente con su otro tío materno Juan Ortiz de Zárate.

Desde esta última fecha, Garay centraba sus actividades en la provincia de Charcas —actual Bolivia— en donde el hijo del virrey y futuro gobernador de Chile, García Hurtado de Mendoza, le encomendó trazar un camino a la costa del océano Pacífico que permitiera un comercio más activo de la «Villa Imperial», y para tal fin, estableció el puerto de Arica, villa que había sido fundada oficialmente el 25 de abril de 1541 por Lucas Martínez Vegaso, y se desempeñó nuevamente con el cargo de proveedor del ejército.

En 1557, luego de la muerte del capitán Martín Robles, Garay se integró con el grado de capitán a la expedición de conquista de Andrés Manso para poblar los territorios más allá de la «Villa de La Plata», y de esta forma asistiría a la fundación de la villa Santo Domingo de la Nueva Rioja, sobre la orilla izquierda del río Condorillos o Parapetí y cerca de los Bañados del Izozog, la cual duraría unos siete años antes de ser destruida por los aborígenes chiriguanos. En esta oportunidad fue que Garay conocería a Ñuflo de Chaves que por problemas jurisdiccionales con Manso, retrasaría su labor de conquista.

Hacia 1558 había retornado a casarse a la ciudad de Asunción del Paraguay, luego de confesar tener un hijo homónimo de tres años con una manceba aborigen, y en donde nació su primera hija: María de Garay, en el año 1559.

En cuanto a Chaves, este con sus 158 soldados se dirigió a la comarca de los aborígenes chiquitos, para fundar el 1 de agosto de 1559 una nueva ciudad que se llamaría Nueva Asunción o «La Barranca», en la orilla derecha del río Guapay que solo duraría unos cinco años antes de ser destruida por los chiriguanos, y sus habitantes trasladados a una nueva villa a 170 km al este, en la que hoy se conoce como Santa Cruz la Vieja.

Luego de la confrontación con los hombres de Manso en donde se encontraba Garay, aquel y Chaves habían marchado hacia Lima a finales del citado año, para reclamar sus derechos respectivos ante el virrey Andrés Hurtado de Mendoza quien en 1560 nombró a su hijo García Hurtado de Mendoza como administrador de la nueva comarca incorporada a la que llamaría gobernación de Moxos —que incluía a la futura gobernación de Santa Cruz de la Sierra y la de Chiquitos— a la vez que este nombrara como su lugarteniente a Ñuflo de Chávez quien se transformaría en el gobernador interino ya que aquel estaba residiendo en Chile.

Andrés Manso no se conformó con el dictamen virreinal, por lo que resistió a dichas órdenes, siendo apresado y enviado a la villa de La Plata, aunque por poco tiempo ya que se fugaría ayudado por el alcalde y prepararía una rebelión con veinte compañeros, pero Garay no adhirió a su causa porque se mantuvo leal a la disposición del virrey Mendoza.

El 26 de febrero de 1561 Garay participó junto a Ñuflo de Chaves en la fundación de la primera Santa Cruz de la Sierra —estaba situada a 14 leguas o bien a unos 56 km oeste sudoeste de la actual San José— ubicada originalmente en los Llanos de Chiquitos, de la que fue regidor de su cabildo y tuvo asignada una encomienda de indios.

En esta nueva ciudad citada tuvo con su esposa por lo menos dos hijos cruceños: Jerónima y su homónimo Juan de Garay ""el Legítimo"". A mediados del año 1568 partió otra vez hacia la Asunción, llevando consigo a su esposa y sus tres hijos legítimos, además de su hijo natural de unos 13 años de edad, por lo que tuvieron que atravesar el Chaco Boreal, el belicoso territorio de los guaycurúes.

En 1567 el tío materno de Juan de Garay, el capitán Juan Ortiz de Zárate, fue nombrado adelantado interino por el virrey del Perú —ya que desde el 19 de octubre de 1564 le había sido asignado el cargo de sexto gobernador del Río de la Plata y del Paraguay luego de destituir a Francisco Ortiz de Vergara— y por lo cual Zárate hizo ocupar el cargo de lugarteniente asunceño a Felipe de Cáceres quien a su vez, nombró capitán a Juan de Garay, pidiéndole que "traiga a gentes a la provincia de Paraguay".

Juan de Garay había partido hacia Asunción con su familia y llegó luego de unos cuatro meses, el 11 de diciembre de 1568, y en el transcurso del viaje lo nombraron el 8 de diciembre como alguacil mayor del Río de la Plata.

Bajo este cargo, a finales del año 1569, dio la orden al capitán Ruy Díaz de Melgarejo para fundar una ciudad que se llamaría Villa Rica del Espíritu Santo con el objetivo de afianzar las posesiones españolas en las zonas deslindadas por el Tratado de Tordesillas, además de sospechar de la existencia de una importante mina de oro. Dicho capitán partió en esa fecha desde Asunción hacia el Guayrá y una vez en Ciudad Real salió con 40 hombres para erigirla, y el 14 de mayo de 1570 a 60 leguas de la misma, la fundó en su primer emplazamiento provisional que estaría entre las nacientes de los ríos Piquiri e Ivaí y en donde mandó construir una iglesia y una fortaleza para luego trazar el poblado, y aunque las minas auríferas no fueran halladas, sí descubrieron una de hierro.

El 3 de abril de 1573 Martín Suárez de Toledo, como gobernador interino del Río de la Plata y del Paraguay, le encargó a Garay una expedición por el río Paraná que tenía como finalidad fundar una urbe que facilitara a la ciudad de Asunción la salida al mar y la comunicación con la metrópoli.De esta forma, se organizó una expedición integrada por 80 mancebos de la tierra, en un bergantín, embarcaciones menores y caballos, con 75 nativos guaraníes y 9 españoles. Se componía de dos grupos, uno por el Paraná que mandaba el propio Juan de Garay y otro por tierra a cargo de Francisco de Sierra que recorrería la margen izquierda del río, evitando así los bosques del Chaco y llevando las carretas, el ganado, los caballos y otros elementos necesarios para la fundación.

Garay salió de Asunción el 14 de abril de 1573 aunque el que iba por tierra lo hizo meses antes. Además, el gobernador Suárez de Toledo le había encargado la escolta de la carabela "San Cristóbal de la Buenaventura" capitaneada por Ruy Díaz de Melgarejo y su segundo el capitán Espinosa en donde llevaría preso a España a Felipe de Cáceres que había sido depuesto por el obispo Pedro Fernández de la Torre que también viajaba para formular oficialmente la acusación ante la Corte.

Tal como indica el poder de Suárez de Toledo, Juan de Garay llevaba: 

Los dos grupos se encontraron en un lugar llamado «La Punta del Yeso», justo enfrente de la actual Cayastá, avanzando juntos por el río San Javier, entonces llamado río de la Quiloazas.

Garay decidió desembarcar muy pronto y eligió la orilla sudoeste del río (donde hoy se encuentran las ruinas de Santa Fe la Vieja, a 5 km de Cayastá) construyendo un pequeño asentamiento allí. Desde ese lugar partió una pequeña expedición de exploración para encontrar un lugar más apropiado. Durante estas exploraciones de búsqueda coincidió con Jerónimo Luis de Cabrera que también estaba explorando el Paraná e intentando erigir una ciudad para apoyar la recién fundada Córdoba. Como resultado de este encuentro Juan de Garay decidió dar la categoría de ciudad al pequeño asentamiento, al cual regresó el 30 de septiembre.

La expresión «Abrir puertas a la tierra», que hizo suya Juan de Garay, fue la máxima de toda la administración española en esa parte de América. Con ella se quería indicar la necesidad de fundar ciudades para romper el aislamiento de Asunción hacia los dos lados, uno río abajo abriéndola al mar y conectándola con la metrópoli, y hacia el Alto Perú, centro político y económico de la época.

El 15 de noviembre de 1573, Juan de Garay fundó oficialmente la ciudad de Santa Fe en su primer emplazamiento. Según recogió el escribano Pedro E. Espinosa: ""Juan de Garay, en pie, junto al «palo rollo», símbolo de la justicia y el poder real"", y realizó la fundación con las siguientes palabras:

Los miembros del cabildo de la nueva ciudad fueron designados por el propio Garay. Entre las opciones de Suárez de Toledo para la ubicación de la urbe estuvo, incluso, la de hacerla en Banda Oriental, ya sea a orilla de los ríos San Juan o San Salvador, o de lo contrario en la isla San Gabriel.

El 23 de junio de 1576 la ciudad pasaría a llamarse «Santa Fe de Luyando» por orden del gobernador interino del Río de la Plata y del Paraguay, Diego Ortiz de Zárate y Mendieta, un primo de Garay. Fue así que el nombre fue cambiando hasta retornar al original de «Santa Fe de la Vera Cruz» pero esta vez en el nuevo emplazamiento.

Respecto a esta primera ubicación de la ciudad, solo duraría unos 80 años, por lo que se la conoce como «Santa Fe la Vieja», ya que luego se la mudaría unos kilómetros hacia el sur por motivos de seguridad, a causa de los ataques de los guaycurúes. El traslado duró diez años, ya que comenzó el 5 de octubre de 1650 y terminó hacia diciembre de 1660.

Elegidos los miembros del cabildo santafecino, nombraron de común acuerdo a Juan de Garay el 12 de marzo de 1574, como teniente de gobernador de Santa Fe, cargo confirmado por el adelantado Juan Ortiz de Zárate el 7 de junio del mismo año.

En el mes de mayo de este último año, Garay acompañaría al adelantado Juan Ortiz de Zárate a la Banda Oriental y cerca de la desembocadura del río San Salvador fundaría a la Ciudad Zaratina —en las cercanías de la actual ciudad uruguaya de Dolores— con pobladores de Santa Fe pero duraría hasta su abandono tres años después.

Una vez que Juan Ortiz de Zárate confirmara ante el rey el título de adelantado el 10 de julio de 1569 y retornara al Río de la Plata el 17 de octubre de 1572,en noviembre de 1573 arribó con una armada a la isla San Gabriel en donde levantó un fortín pero al quedar aislado por la resistencia de los aborígenes charrúas, Garay iría en su auxilio en mayo del siguiente año y luego de derrotarlos en la batalla de San Salvador, Zárate podría pasar a tierra firme para ocupar el cargo de gobernador recién en el mismo mes de 1574 hasta 1576, fecha en que fallecería.

El adelantado había designado para que lo sucediera a quien se casara con su hija Juana Ortiz de Zárate y Yupanqui. Mientras eligiese a los candidatos para desposarla, designó a su sobrino Diego Ortiz de Zárate y Mendieta para que ocupara interinamente el gobierno, hecho que se consumaría desde 1576 hasta 1577. Este, a su vez, designó como alcalde de Asunción a Luis de Osorio.

En enero de este último año, luego que Garay resolviera el tema de la despoblación de la «Ciudad Zaratina» cuyos pobladores huían hacia Santa Fe, Córdoba y Tucumán, se dirigió hacia el Perú ya que había sido nombrado tutor de Juana de Zárate, por lo cual tuvo que pasar por esta primera ciudad nombrada y luego por Santiago del Estero. Al dirigirse al Tucumán, Gonzalo de Abreu y Figueroa que había salido de Córdoba, lo obligó a ayudarle con la fundación de la «Ciudad de San Clemente de la Nueva Sevilla» sobre las ruinas de la anterior Córdoba de Calchaquí —fundada en marzo de 1559 por Juan Pérez de Zurita y destruida en 1560 durante la primera guerra con aborígenes lugareños, cuya ubicación actual es la localidad de Chicoana— que duró pocos días y luego de varias luchas contra los calchaquíes, la refundó pero en otro lugar —al sudeste de la actual Rosario de Lerma— pero también sería arrasada. Por tercera y última lo intentaría aunque también terminaría destruida. En marzo del mismo año, Garay pudo desprenderse de esta obligación para poder seguir con su viaje, hasta llegar a Lima.

Cuando Mendieta por sus excesos fuera depuesto en Santa Fe y tuviera que ausentarse para viajar a Charcas, designó en el mismo día 3 de mayo de 1577 a Osorio como su lugarteniente, haciéndose cargo del gobierno interino del Río de la Plata y del Paraguay, desde esa fecha hasta la llegada del nuevo adelantado. Durante la gestión de este último se despobló definitivamente la «Ciudad Zaratina de San Salvador», el 20 de julio del mismo año y ya ocupando nuevamente su cargo de alcalde mayor de Asunción mandaría una tropa de 30 arcabuceros a sofocar la rebelión de los guaraníes del norte asunceno que estaban liderados por el cacique mesiánico Oberá pero sin mayores resultados.

Finalmente el elegido para casarse con Juana de Zárate fue Juan Torres de Vera y Aragón, consumándose en secreto a mediados de 1577 y por lo cual este sería nombrado como el cuarto adelantado, y Juan de Garay su albacea, aunque no pudiera ocupar inmediatamente el cargo de gobernador del Río de la Plata y del Paraguay, dado que fue perseguido y apresado por el virrey Francisco de Toledo que estaba contrariado por no haber logrado casar a su candidato matrimonial. Garay logró huir de Lima y se refugió en Santa Fe, y finalmente Torres de Vera asumiría en el cargo el 3 de diciembre del mismo año. Durante su gobierno la ya citada «Ciudad Zaratina de San Salvador» que fue atacada por los charrúas, Garay fue en su auxilio, valiéndole el ascenso a teniente de gobernador general de todas las provincias del Río de la Plata con sede en Asunción y también lo nombró como su lugarteniente.

La impopularidad del adelantado hizo que el procurador de Asunción, Juan Caballero Bazán, llevara las reclamaciones a la Audiencia de Charcas que fueran recibidas con favorable acogida. De esta forma, el 15 de septiembre del siguiente año sería apresado en Charcas y quedaría Osorio como su reemplazante.

Luis Osorio que había sido nombrado lugarteniente de Mendieta cuando fue a Charcas, entregó inmediatamente el gobierno a Juan de Garay el 15 de septiembre de 1578, por alegar que este tenía más derecho porque había sido nombrado lugarteniente por ambos adelantados cuando fundara y defendiera la «Ciudad Zaratina de San Salvador».

Una vez gobernador, Garay dirigió una campaña a la región del Jejuy o Jejuí y a la zona de los ñuaras o Itatín —actual estado brasileño de Mato Grosso del Sur— en 1579 y luego de derrotar al cacique Oberá, decidió fundar una ciudad en la región cercana a la laguna de Xarayes, Jarayes o Gran Pantanal del alto río Paraguay. Para concretar su objetivo designó al teniente de gobernador del Guayrá, el capitán Ruy Díaz de Melgarejo, que partió de Asunción en 1580 con 60 soldados y hacia la latitud 19°S, sobre la orilla del este o diestra del río Mbotetey o Miranda que es afluente oriental del Paraguay, fundaría la primera ciudad de Santiago de Jerez aunque sería abandonada al poco tiempo por sus habitantes, ya que carecía de minas la región, además de no haber tráfico comercial y estar asediada continuamente por los guaycurúes.

En enero de 1580 Juan de Garay comenzó los preparativos de la segunda fundación de Buenos Aires. Se pretendía poblar la nueva ciudad con gente de Asunción, para lo cual se promulgó un bando ofreciendo tierras y otras mercedes. Se apuntaron 200 familias guaraníes y 76 de colonos. Se llevó todo lo necesario por el río en la carabela "Cristóbal Colón" y dos bergantines entre otras naves menores, expedición que salió el 9 de marzo del mismo año. Además de los colonos iban 39 soldados. Una parte del convoy fue por tierra y partió un mes antes.

El domingo 29 de mayo de 1580, Juan de Garay llegó a la boca del Riachuelo. Desembarcó justo en el lugar donde años antes lo había hecho el adelantado Pedro de Mendoza e instaló un campamento; la columna que viajaba por tierra llegó un mes después. Para el miércoles 11 de junio ya se había levantado un pequeño asentamiento, algo más hacia al norte de la fundación anterior, que dio base a la nueva ciudad de Buenos Aires. Ese día se celebraron las ceremonias fundacionales. Es importante recalcar una parte del acta fundacional:
El acta fundacional de la nueva urbe, llama a ésta «Ciudad de Trinidad», en recuerdo de su llegada que tuvo lugar el domingo de la Santísima Trinidad. El puerto de la misma recibió el nombre de «Santa María de los Buenos Aires». Ortiz de Zárate había denominado oficialmente a la región como «Nueva Vizcaya», en honor a su tierra natal.

Se plantó el «árbol de justicia» o símbolo de la ciudad, y tal como se acostumbraba y era obligatorio en tales casos, blandió la espada en las cuatro direcciones y dio un tajo a la tierra para señalar la posesión, y repartieron tierras entre los 63 pobladores que lo acompañaban, algunos presentes en la primera fundación.

Fueron nombrados alcaldes Rodrigo Ortiz de Zárate y Gonzalo Martel de Guzmán y se formó el Cabildo con seis regidores, siendo uno de ellos el general Alonso de Escobar, a la vez que se asignó el escudo de armas de la nueva ciudad, cuadrado blanco con águila negra coronada, con las alas totalmente desplegadas, sosteniendo la cruz roja de Calatrava en su pata derecha. También se asignaron encomiendas. Todo ello quedó registrado en el acta del acontecimiento redactada por el escribano Pedro de Jerez y tres testigos.

La nueva fundación fue atacada por los indígenas, mandados por su jefe Tabobá, pero Garay fue advertido del ataque por Cristóbal de Altamirano, que estaba prisionero de aquel, lo cual sirvió para organizar la defensa. En ese ataque el procurador Juan Fernández de Enciso dio muerte a Tabobá.

En octubre del citado año Garay volvió a Santa Fe y regresó a Buenos Aires en febrero del siguiente. A mediados de 1581 fue por tierra hasta cabo Corrientes —donde hoy se asienta la ciudad de Mar del Plata— en busca de la mítica «Ciudad de los Césares», regresando en enero de 1582, de donde retornó a Santa Fe y luego a Asunción, ciudad adonde comenzaría a ver que la nueva urbe podría desplazar su capitalidad.

En marzo de 1583, Juan de Garay acompañó a Sotomayor San Juan en el trayecto de Buenos Aires a Santa Fe. El convoy de botes estaba compuesto por 40 hombres, un franciscano y algunas mujeres. El 20 de marzo se desorientaron (entre las numerosas islas y lagunas del río Paraná) y entraron en una laguna desconocida, por lo cual Garay, algunos de sus hombres, el monje franciscano y dos mujeres decidieron pasar la noche en tierra, a fin de no dormir incómodamente a bordo de la pequeña embarcación.
Su campamento fue atacado por los indios del lugar, que mataron a Garay, al franciscano, a una de las mujeres y a 12 de sus 40 soldados.

Si bien no está documentado el lugar exacto de aquellos hechos, existen varias hipótesis sobre su ubicación:








Se desconoce la etnia de los guerreros («cuarenta indios que habitaban por allí») que mataron a Garay. Los distintos historiadores mencionan a

Fruto de la unión de Juan Garay con una manceba aborigen —una hija del cacique cautivo de la etnia chiriguano o Avá guaraní— había tenido un hijo mestizo e ilegítimo que sería criado con costumbres europeas:


Años más tarde, hacia 1558, Juan de Garay Ochandiano y Mendieta Zárate se unió en matrimonio en la ciudad de Asunción con Isabel de Becerra y Mendoza (Cáceres de Extremadura, ca. 1535 - Santa Fe la Vieja, ca. 1608).

Isabel era hija del capitán Francisco de Becerra (n. Cáceres, 1511 - costa Mbiaza, 1553) e Isabel de Contreras Mendoza (n. Medellín, ca. 1518), y hermana de Elvira de Becerra y Contreras Mendoza, quien se enlazó con el capitán Ruy Díaz Melgarejo teniente de gobernador del Guayrá de 1575 a 1585 y fundador de Ciudad Real del Guayrá en 1556, de Villa Rica del Espíritu Santo en 1570 y Santiago de Jerez del Itatín en 1580.

La citada familia de Isabel había arribado a la América del Sur en 1550 en el bergantín "La Concepción" del capitán Juan de Salazar Espinosa quien fuera el fundador de la ciudad de Asunción en 1537, y dirigido por Mencia Calderón Ocampo, la cual acompañada por el hidalgo Fernando de Trejo y Carvajal fundarían en 1553 en la costa atlántica, el efímero poblado español de San Francisco de Mbiaza —poco más de un siglo después resurgiría como una villa portuguesa— y en donde nacería el futuro obispo del Tucumán, Hernando de Trejo y Sanabria.

Antes del matrimonio, Juan de Garay le había confesado a su futura esposa de la existencia de su homónimo hijo natural de unos tres años de edad, quien por respuesta obtuviera que ella misma ""desearía criarlo a la usanza castellana"". Juan de Garay e Isabel Becerra concibieron a seis hijos legítimos documentados:










</doc>
<doc id="3337" url="https://es.wikipedia.org/wiki?curid=3337" title="Química orgánica">
Química orgánica

La química orgánica es la rama de la química que estudia una clase numerosa de moléculas que en su gran mayoría contienen carbono formando enlaces covalentes: carbono-carbono o carbono-hidrógeno y otros heteroátomos, también conocidos como compuestos orgánicos. 
Debido a la omnipresencia del carbono en los compuestos que esta rama de la química estudia, esta disciplina también es llamada química del carbono.

La química orgánica se constituyó o se instituyó como disciplina en los años treinta. El desarrollo de nuevos métodos de análisis de las sustancias de origen animal y vegetal, basados en el empleo de disolventes, como el éter o el alcohol, permitió el aislamiento de un gran número de sustancias orgánicas que recibieron el nombre de ""principios inmediatos"".
La aparición de la química orgánica se asocia a menudo al descubrimiento, en 1828, por el químico alemán Friedrich Wöhler, de que la sustancia inorgánica cianato de amonio podía convertirse en urea, una sustancia orgánica que se encuentra en la orina de muchos animales. Antes de este descubrimiento, los químicos creían que para sintetizar sustancias orgánicas, era necesaria la intervención de lo que llamaban ‘la fuerza vital’, es decir, los organismos vivos. El experimento de Wöhler rompió la barrera entre sustancias orgánicas e inorgánicas. Los químicos modernos consideran compuestos orgánicos a aquellos que contienen carbono e hidrógeno, y otros elementos (que pueden ser uno o más), siendo los más comunes: oxígeno, nitrógeno, azufre y los halógenos.

En 1856, "sir" William Henry Perkin, mientras trataba de estudiar la quinina, accidentalmente fabricó el primer colorante orgánico ahora conocido como malva de Perkin.

La diferencia entre la química orgánica y la química biológica es que en la química biológica las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es sólo testigo de su presente, sin pasado y sin evolución histórica.






La tarea de presentar la química orgánica de manera sistemática y global se realizó mediante una publicación surgida en Alemania, fundada por el químico Friedrich Konrad Beilstein (1838-1906). Su "Handbuch der organischen Chemie" (Manual de la química orgánica) comenzó a publicarse en Hamburgo en 1880 y consistió en dos volúmenes que recogían información de unos quince mil compuestos orgánicos conocidos. Cuando la "Deutsche chemische Gesellschaft" (Sociedad Alemana de Química) trató de elaborar la cuarta re-edición, en la segunda década del siglo XX, la cifra de compuestos orgánicos se había multiplicado por diez. Treinta y siete volúmenes fueron necesarios para la edición básica, que aparecieron entre 1916 y 1937. Un suplemento de 27 volúmenes se publicó en 1938, recogiendo información aparecida entre 1910 y 1919. En la actualidad, se está editando el "Fünftes Ergänzungswerk" (quinta serie complementaria), que recoge la documentación publicada entre 1960 y 1979. Para ofrecer con más prontitud sus últimos trabajos, el "Beilstein Institut" ha creado el servicio "Beilstein On line", que funciona desde 1988. Recientemente, se ha comenzado a editar periódicamente un CD-ROM, "Beilstein Current Facts in Chemistry", que selecciona la información química procedente de importantes revistas. Actualmente, la citada información está disponible a través de internet.

La gran cantidad de compuestos orgánicos que existen tiene su explicación en las características del átomo de carbono, que tiene cuatro electrones en su capa de valencia: según la regla del octeto necesita ocho para completarla, por lo que forma cuatro enlaces (valencia = 4) con otros átomos. Esta especial configuración electrónica da lugar a una variedad de posibilidades de hibridación orbital del átomo de carbono (hibridación química).

La molécula orgánica más sencilla que existe es el metano. En esta molécula, el carbono presenta hibridación sp3, con los átomos de hidrógeno formando un tetraedro.

El carbono forma enlaces covalentes con facilidad para alcanzar una configuración estable, estos enlaces los forma con facilidad con otros carbonos, lo que permite formar frecuentemente cadenas abiertas (lineales o ramificadas) y cerradas (anillos).

La clasificación de los compuestos orgánicos puede realizarse de diversas maneras: atendiendo a su origen (natural o sintético), a su estructura (p. ej.: alifático o aromático), a su funcionalidad (p. ejm.:alcoholes o cetonas), o a su peso molecular (p. ej.: monómeros o polímeros).

La clasificación por el origen suele englobarse en dos tipos: natural o sintético. Aunque en muchos casos el origen natural se asocia a el presente en los seres vivos no siempre ha de ser así, ya que la síntesis de moléculas orgánicas cuya química y estructura se basa en el carbono, también se sintetizan "ex-vivo", es decir en ambientes inertes, como por ejemplo el ácido fórmico en el cometa Halle-Bopp.

Los compuestos orgánicos presentes en los seres vivos o "biosintetizados" constituyen una gran familia de compuestos orgánicos. Su estudio tiene interés en medicina, farmacia, perfumería, cocina y muchos otros campos más.

Los carbohidratos están compuestos fundamentalmente de carbono (C), oxígeno (O) e hidrógeno (H). Son a menudo llamados "azúcares" pero esta nomenclatura no es del todo correcta. Tienen una gran presencia en el reino vegetal (fructosa, celulosa, almidón, alginatos), pero también en el animal (glucógeno, glucosa).
Se suelen clasificar según su grado de polimerización en:

Los lípidos son un conjunto de moléculas orgánicas, la mayoría biomoléculas, compuestas principalmente por carbono e hidrógeno y en menor medida oxígeno, aunque también pueden contener fósforo, azufre y nitrógeno. Tienen como característica principal el ser hidrófobas (insolubles en agua) y solubles en disolventes orgánicos como la bencina, el benceno y el cloroformo. En el uso coloquial, a los lípidos se les llama incorrectamente grasas, ya que las grasas son sólo un tipo de lípidos procedentes de animales. Los lípidos cumplen funciones diversas en los organismos vivientes, entre ellas la de reserva energética (como los triglicéridos), la estructural (como los fosfolípidos de las bicapas) y la reguladora (como las hormonas esteroides).

Las proteínas son polipéptidos, es decir están formados por la polimerización de péptidos, y estos por la unión de aminoácidos. Pueden considerarse así "poliamidas naturales" ya que el enlace peptídico es análogo al enlace amida. Comprenden una familia muy importante de moléculas en los seres vivos pero en especial en el reino animal. Ejemplos de proteínas son el colágeno, las fibroínas, o la seda de araña.

Los ácidos nucleicos son polímeros formados por la repetición de monómeros denominados nucleótidos, unidos mediante enlaces fosfodiéster. Se forman, así, largas cadenas; algunas moléculas de ácidos nucleicos llegan a alcanzar pesos moleculares gigantescos, con millones de nucleótidos encadenados. Están formados por la moléculas de carbono, hidrógeno, oxígeno, nitrógeno y fosfato.Los ácidos nucleicos almacenan la información genética de los organismos vivos y son los responsables de la transmisión hereditaria. Existen dos tipos básicos, el ADN y el ARN. 

Las moléculas pequeñas son compuestos orgánicos de peso molecular moderado (generalmente se consideran "pequeñas" aquellas con peso molecular menor a 1000 g/mol) y que aparecen en pequeñas cantidades en los seres vivos pero no por ello su importancia es menor. A ellas pertenecen distintos grupos de hormonas como la testosterona, el estrógeno u otros grupos como los alcaloides. Las moléculas pequeñas tienen gran interés en la industria farmacéutica por su relevancia en el campo de la medicina.

Son compuestos orgánicos que han sido sintetizados sin la intervención de ningún ser vivo, en ambientes extracelulares y extravirales.

El petróleo es una sustancia clasificada como mineral en la cual se presentan una gran cantidad de compuestos orgánicos. Muchos de ellos, como el benceno, son empleados por el hombre tal cual, pero muchos otros son tratados o derivados para conseguir una gran cantidad de compuestos orgánicos, como por ejemplo los monómeros para la síntesis de materiales poliméricos o plásticos.

En el año 2000 el ácido fórmico, un compuesto orgánico sencillo, también fue hallado en la cola del cometa Hale-Bopp. Puesto que la síntesis orgánica de estas moléculas es inviable bajo las condiciones espaciales, este hallazgo parece sugerir que a la formación del sistema solar debió anteceder un periodo de calentamiento durante su colapso final.

Desde la síntesis de Wöhler de la urea un altísimo número de compuestos orgánicos han sido sintetizados químicamente para beneficio humano. Estos incluyen fármacos, desodorantes, perfumes, detergentes, jabones, fibras téxtiles sintéticas, materiales plásticos, polímeros en general, o colorantes orgánicos.

El compuesto más simple es el metano, un átomo de carbono con cuatro de hidrógeno (valencia = 1), pero también puede darse la unión carbono-carbono, formando cadenas de distintos tipos, ya que pueden darse enlaces simples, dobles o triples. Cuando el resto de enlaces de estas cadenas son con hidrógeno, se habla de hidrocarburos, que pueden ser:

Los radicales son fragmentos de cadenas de carbonos que cuelgan de la cadena principal. Su nomenclatura se hace con la raíz correspondiente (en el caso de un carbono "met"-, dos carbonos et-, tres carbonos prop-, cuatro carbonos but-, cinco carbonos pent-, seis carbonos hex-, y así sucesivamente...) y el sufijo -il. Además, se indica con un número, colocado delante, la posición que ocupan. El compuesto más simple que se puede hacer con radicales es el "2-metilpropano". En caso de que haya más de un radical, se nombrarán por orden alfabético de las raíces. Por ejemplo, el "2-etil, 5-metil, 8-butil, 10-docoseno".

Los compuestos orgánicos también pueden contener otros elementos, también otros grupos de átomos además del carbono e hidrógeno, llamados grupos funcionales. Un ejemplo es el grupo hidroxilo, que forma los alcoholes: un átomo de oxígeno enlazado a uno de hidrógeno (-OH), al que le queda una valencia libre. Asimismo también existen funciones alqueno (dobles enlaces), éteres, ésteres, aldehídos, cetonas, carboxílicos, carbamoilos, azo, nitro o sulfóxido, entre otros.

Son cadenas de carbonos con uno o varios átomos de oxígeno.
Pueden ser:

El grupo –OH es muy polar y, lo que es más importante, es capaz de establecer puentes de hidrógeno: con sus moléculas compañeras o con otras moléculas neutras.





Son compuestos que contienen un ciclo saturado. Un ejemplo de estos son los norbornanos, que en realidad son compuestos bicíclicos, los terpenos, u hormonas como el estrógeno, progesterona, testosterona u otras biomoléculas como el colesterol.

Los compuestos aromáticos tienen estructuras cíclicas insaturadas. El benceno es el claro ejemplo de un compuesto aromático, entre cuyos derivados están el tolueno, el fenol o el ácido benzoico. En general se define un compuesto aromático aquel que tiene anillos que cumplen la regla de Hückel, es decir que tienen 4"n"+2 electrones en orbitales π (n=0,1,2...). A los compuestos orgánicos que tienen otro grupo distinto al carbono en sus cilos (normalmente N, O u S) se denominan compuestos aromáticos heterocíclicos. Así los compuestos aromáticos se suelen dividir en:

Ya que el carbono puede enlazarse de diferentes maneras, una cadena puede tener diferentes configuraciones de enlace dando lugar a los llamados isómeros, moléculas tienen la misma fórmula química pero distintas estructuras y propiedades.
Existen distintos tipos de isomería: isomería de cadena, isomería de función, tautomería, estereoisomería, y estereoisomería configuracional.
El ejemplo mostrado a la izquierda es un caso de isometría de cadena en la que el compuesto con fórmula CH puede ser un ciclo (ciclohexano) o un alqueno lineal, el 1-hexeno. Un ejemplo de isomería de función sería el caso del propanal y la acetona, ambos con fórmula CHO.

Los compuestos orgánicos pueden dividirse de manera muy general en:


Una de las principales relaciones entre la química orgánica y la biología es el estudio de la síntesis y estructura de moléculas orgánicas de importancia en los procesos moleculares realizados por los organismos vivos, es decir en el metabolismo. La bioquímica es el campo interdisciplinar científico que estudia los seres vivos, y ya que éstos usan compuestos que contienen carbono, la química orgánica es imprescindible para comprender los procesos metabólicos.

En términos biológicos la química orgánica es de gran importancia sobre todo en un contexto celular y esto lo podemos ejemplificar con moléculas como los carbohidratos, presentes desde la membrana plasmática así como en la estructura química del ADN, los libidos quienes son la base principal de la membrana plasmática, las proteínas que ayudan a dar sostén a un organismo o sus funciones como enzimas y el ADN, molécula encargada de resguardar la información genética de los organismos vivos.



</doc>
<doc id="3341" url="https://es.wikipedia.org/wiki?curid=3341" title="León (España)">
León (España)

León (pronunciado: ) (en asturleonés, Llión) es un municipio y ciudad española ubicada en el noroeste de la península ibérica, capital de la provincia homónima, en la comunidad autónoma de Castilla y León. León contaba en enero de 2015 con 127.817 habitantes repartidos en una superficie de 39,03 km², y un área metropolitana de 231623 habitantes según el mapa de áreas funcionales de la Junta de Castilla y León (otros proyectos dan cifras diferentes), distribuidos en quince municipios, siendo así la segunda más poblada de la comunidad.

Nacida como campamento militar romano de la "Legio VI Victrix" hacia 29 a. C., su carácter de ciudad campamental se consolidó con el asentamiento definitivo de la "Legio VII Gemina" a partir del año 74. Tras su parcial despoblación con motivo de la conquista musulmana de la península, León recibió un nuevo impulso como parte del Reino de Asturias. En 910 comenzó una de sus etapas históricas más destacadas al convertirse en cabeza del Reino de León, participando activamente en la Reconquista contra los musulmanes, llegando a ser uno de los reinos fundamentales en la configuración del Reino de España. La ciudad albergó las primeras Cortes de la historia de Europa en 1188, bajo el reinado de Alfonso IX, gracias a lo cual en 2011 fue proclamada por la Junta de Castilla y León como Cuna del Parlamentarismo. Desde la Baja Edad Media la ciudad dejó de tener la importancia de antaño, en parte debido a la pérdida de su independencia tras la unión del reino leonés a la Corona castellana, definitiva desde 1301.

Sumida en un período de estancamiento durante la Edad Moderna, en la Guerra de la Independencia fue una de las primeras ciudades en sublevarse de toda España, y años después del fin de la misma, en 1833, adquiriría su rango de capital provincial. La llegada del siglo XX trajo consigo el Plan de Ensanche, que acrecentó la expansión urbanística que venía experimentando desde finales del siglo XIX, cuando la ciudad se convirtió en un importante nudo de comunicaciones del noroeste con motivo del auge de la minería del carbón y de la llegada del ferrocarril.

Su patrimonio histórico y monumental, así como diversas celebraciones que tienen lugar a lo largo del año, entre las que destaca la Semana Santa, y su situación como paso obligado del Camino de Santiago, considerado Patrimonio de la Humanidad por la UNESCO, la convierten en una ciudad receptora de turismo nacional e internacional. Entre sus monumentos más representativos se encuentran la Catedral, el mejor ejemplo del gótico clásico de estilo francés en España, la Basílica de San Isidoro, una de las iglesias románicas más importantes de España, tumba de los reyes de León medievales y considerada como "La Capilla Sixtina del Arte Románico", el Monasterio de San Marcos, primer ejemplo de la arquitectura plateresca y renacentista española, el palacio de Los Guzmanes, el palacio de los Condes de Luna, la iglesia del Mercado o del Camino la Antigua, la iglesia de Palat del Rey, la Casa de las Carnicerías y la Casa Botines, de estilo modernista y realizada por el genial arquitecto catalán Antoni Gaudí; todos ellos declarados Bien de Interés Cultural. Ejemplo destacado de arquitectura moderna, y uno de los museos de la ciudad, es el MUSAC, de Mansilla + Tuñón Arquitectos.

León dispone de una red desarrollada de carreteras y ferrocarril, además de contar con un aeropuerto con vuelos nacionales e internacionales (de momento solo a París) que tras las obras de ampliación, inauguradas en octubre de 2010, cuenta con una nueva terminal y es capaz de atender al triple de pasajeros que hasta entonces.

En 2013 se están llevando a cabo distintos proyectos en la ciudad tales como la línea de alta velocidad AVE, la reconversión del Feve en tranvía y el palacio de congresos, entre otros. El proyecto del tranvía suscitó varias críticas por parte del Partido Popular, entonces en la oposición municipal, que alegaba que su desarrollo en una ciudad como León era un proyecto faraónico y de dudosa viabilidad y anunciaron que bajo su gobierno no desarrollarían tal proyecto.

La Universidad de León, fundada en 1979 como escisión de la Universidad de Oviedo, contaba en el curso 2006-07 con 13.217 alumnos; tiene su sede en la ciudad y está catalogada, a partir de criterios como la demanda universitaria, los recursos humanos o los planes de estudio, como la 2.ª universidad de Castilla y León, tras la Universidad de Salamanca, y la 30.ª de España. Desde el 4 de mayo de 2010, la ciudad alberga la segunda sede de la Universidad de Washington en Europa, tras su sede de Roma, con capacidad para 500 alumnos interesados en el aprendizaje del español. La ciudad cuenta también con una sede del Instituto Confucio desde 2011.

El origen del nombre de la ciudad proviene de la palabra latina "legio", que hace referencia a la legión que fundó la ciudad en su actual emplazamiento. Esta tesis, comúnmente aceptada, se refuerza con el todavía válido gentilicio legionense para referirse a los habitantes de la ciudad. La evolución de "Legio" a "León" se explica fácilmente, pues en latín clásico, la "gi" se pronuncia como si fuese una "gui", por lo que la pronunciación de "Legio" sería "Leguio", algo que acabó derivando en el "Leio" o "Leionem", que a su vez acabaron en el nombre actual de León.

El escudo de León está compuesto por un campo de plata en el que figura un león rampante de púrpura, linguado, uñado, armado de gules y coronado de oro. Aparece timbrado con una corona abierta de oro (la forma de la antigua corona real, usada hasta el siglo XVI). En el escudo de la ciudad de León aparece representada una corona marquesal en vez de la antigua real y el león no figura coronado.

Al producirse la unión en el año 1230 de las Coronas de León y Castilla con Fernando III el Santo se dispuso que en el escudo del rey los elementos heráldicos castellanos (un castillo almenado de oro sobre un campo de gules) y leoneses formaran un escudo cuarteado. Es de destacar que en los cuartelados no había sitio para dos leones, hasta aquel momento pasantes, por lo que se les situó como rampantes para ocupar por completo los cuarteles que les correspondían. Esta es la disposición que ha llegado a la actualidad. El uso de la corona sobre la cabeza del león no apareció documentado hasta el reinado de Sancho IV de Castilla y León (1284-1295).

Actualmente el escudo de León es el símbolo de la provincia y, acompañado por adornos exteriores, de la ciudad de León.

La ciudad de León está ubicada en una terraza fluvial en la confluencia de los ríos Bernesga y Torío, a una altitud de 840 msnm. Situada aproximadamente en el centro de la provincia, se encuentra en un lugar estratégico del Noroeste peninsular, ya que es paso obligado para ir a Galicia y a Asturias. Sus coordenadas son .

Su término municipal limita al norte con Sariegos y con Villaquilambre, al este con Valdefresno, al sur con Santovenia de la Valdoncina, Onzonilla y Villaturiel, y al oeste con San Andrés del Rabanedo y Valverde de la Virgen. El territorio del término municipal está representado en la hoja 161 del Mapa Topográfico Nacional.


Situado en la transición del Páramo Leonés a la Cordillera Cantábrica, su ubicación en la confluencia de dos ríos hace que la capital leonesa se asiente en una zona predominantemente llana, si bien según se aleja del núcleo urbano el terreno se eleva, encontrándose por el norte con el Monte de San Isidro y por el este con los altos en los que se encuentra Golpejar de la Sobarriba. En el término municipal se encuentran los vértices geodésicos de Valenciano, a una altitud de 938 msnm, y de San Isidro, a una altitud de 939 metros. En centro de la ciudad se encuentra a una altitud de 837 metros, mientras que la altitud del municipio varía desde los 800 metros en el último tramo en la localidad del río Bernesga hasta los 944 metros en el norte del municipio.

León está bañada por los ríos Bernesga, que recorre la ciudad por el oeste, y el Torío, que la delimita por el este, situándose la mayor parte del núcleo urbano entre los dos cauces. A su paso por la ciudad, se encuentran canalizados y adecuados para el paseante, con jardines y paseos peatonales. La confluencia de ambos se sitúa a la altura del polígono de La Lastra, donde el Torío vierte sus aguas en el Bernesga.

Sobre el río, y en el centro de la ciudad, se encuentra el Aula de Interpretación de las Energías Renovables de León, perteneciente al Ayuntamiento de León. Es un aula destinada a enseñar a sus visitantes las soluciones complementarias y alternativas que proporcionan las energías renovables al sistema energético actual, pretendiendo ser un referente en ese aspecto en la comunidad autónoma de Castilla y León.

Se trata de un edificio situado en los márgenes del río Bernesga junto al Puente de los Leones, construido tras un acuerdo alcanzado por el EREN y el Ayuntamiento de León. El Aula posee un espacio de exposiciones sobre el medio ambiente y cuenta con una instalación solar térmica, una instalación solar fotovoltaica y una minicentral hidroeléctrica. La electricidad generada por estas tres últimas se incorpora a la red eléctrica general para su posterior utilización, siendo capaz de dar luz a 1.100 familias.

El clima de León es oceánico mediterráneo de tipo Csb de acuerdo a la clasificación climática de Köppen.

Las precipitaciones están repartidas, como es habitual en los climas mediterráneos, de forma muy irregular a lo largo del año, con mínimos en la época estival y máximos durante primavera y otoño. La precipitación media anual es de 556 mm. La ciudad disfruta al año de 2624 horas de sol al año y de 78 de lluvia, además de 16 de tormenta.

Las temperaturas son frescas, con una media anual de 11,1 °C según los datos de la estación meteorológica de La Virgen del Camino, con inviernos fríos, siendo frecuentes las heladas (74 días de helada de media al año). La nieve hace acto de presencia en la capital leonesa durante 16 días de media al año, si bien las grandes nevadas no son frecuentes salvo en fechas como diciembre de 2009, cuando la ciudad y parte de la provincia se colapsaron debido a un temporal de frío y nieve durante el cual se registraron temperaturas mínimas históricas en algunos lugares y obligó a la UME a intervenir para hacer frente a las complicaciones derivadas del mismo. El verano es caluroso, suavizado por la altitud de la ciudad, con temperaturas máximas que rondan los 27 °C.

A continuación se muestran los datos del observatorio meteorológico de la AEMET situado en el Aeropuerto de León a 916 msnm, en el municipio de Valverde de la Virgen muy cerca de la ciudad de León. El periodo de referencia es 1981-2010 también para las extremas.

La ciudad de León surge hacia 29 a. C. como campamento militar romano de la Legio VI Victrix, en la terraza fluvial entre los ríos Bernesga y Torío, cerca de la ciudad astur de Lancia, con motivo de las llamadas guerras cántabras. A finales del siglo I, a partir de 74, el campamento es ocupado por la Legio VII Gemina, fundada por Galba, la cual permanecerá en León hasta aproximadamente principios del siglo V. Fue la única legión asentada en Hispania hasta la caída del Imperio Romano de Occidente (476), por lo que durante todo este tiempo León fue la capital militar de la Península. La ciudad perteneció al Convento Asturicense, con capital en Asturica Augusta, el cual formó parte de la provincia Tarraconense hasta el siglo III, cuando, con la creación de la provincia de Gallaecia, fue integrado en ésta. El trazado campamental romano original aún puede observarse en la actualidad, puesto que se conservan gran parte de las murallas que lo rodeaban en los siglos III y IV. Por los restos arqueológicos se sabe que contaba con unas termas (con ruinas aún visibles bajo la catedral) e incluso un anfiteatro con capacidad para 5000 espectadores a extramuros, actualmente enterrado bajo la calle Cascalerías. Alrededor del campamento fue creándose un núcleo civil paralelo, la cannaba, en la que se asentaban todas las personas que se encargaban de cubrir las necesidades de los soldados.

Tras el período romano, la ciudad formó parte del Reino suevo y posteriormente del Reino visigodo. Entre los siglos VI y VIII la escasez de evidencias arqueológicas proyectan una imagen carente de vitalidad urbana, con una clara reducción del espacio habitado, pero el descubrimiento de cerámicas adscribibles al periodo omeya cordobés en la zona de Puerta Obispo nos indica que la ciudad no fue abandonada completamente, sino que conservó cierta población estable.

León fue conquistada por el Califato Omeya en el año 712 pero reconquistada 754 por Alfonso I aunque durante casi un siglo, estaría deshabitada. Alrededor de 846, un grupo de mozárabes intentó repoblar la ciudad con población cristiana, ya que hasta entonces había permanecido en "estado latente", en el centro de la línea de combates; sin embargo, un ataque omeya acabó con aquella iniciativa. En 853 Ordoño I incorporó la ciudad al Reino de Asturias, repoblándola con éxito. Con Ordoño II, que ocupó el trono (914 - 924) tras la muerte de su hermano García I, la ciudad se convierte en capital del reino astur, iniciando el Reino de León.

Durante la existencia del reino, la ciudad de León fue creciendo y evolucionando en su desarrollo. En esta cuestión jugó un destacado papel el Camino de Santiago, quizás la más importante vía de circulación de gentes, ideas, cultura y arte del Medievo.

En el siglo XII, el geógrafo y viajero árabe Edrisi escribió lo siguiente sobre León: ""Allí se practica un comercio muy provechoso. Sus habitantes son ahorradores y prudentes"". Tenemos también noticia de León a través de diversos códices, entre ellos el "Codex Calixtinus", manuscrito que, entre otras cosas, contiene información sobre la ruta que los peregrinos seguían hacia Santiago de Compostela. Con todo ello, la ciudad conoció el desarrollo de nuevos barrios, en ocasiones extramuros de una ciudad que ya se quedaba pequeña, y casi siempre a la vera del camino de los peregrinos, que accedían a la ciudad por la llamada "Puerta Moneda".

La ciudad de León fue sede regia desde la fundación del reino con García I, a principios del siglo X, hasta la unión con Castilla en 1230, momento en que la capitalidad del reino unificado fue itinerante. En el siglo X destacarían reyes como Ordoño II, que fijó la capitalidad y consagró la primera catedral en las antiguas termas romanas (bajo la actual catedral), y su hijo Ramiro II, que construyó el primer palacio en Palat de Rey y llevó a cabo victoriosas campañas contra los musulmanes, al igual que su padre. La segunda mitad del siglo es de luchas civiles en León, reyes débiles con problemas con la nobleza, y de ataques y contraataques musulmanes a la ciudad, incluyendo uno de Almanzor, que causó graves destrozos. La recuperación y reordenamiento de la capital llegó con Alfonso V a inicios del siglo XI, así como el comienzo de la victoria cristiana en la península. Avanzado el siglo, hay un cambio de dinastía, destacando a Fernando I como rey iniciador de la basílica de San Isidoro y de su panteón real. Su sucesor Alfonso VI pasó a los anales de historia por el avance en la reconquista y, sobre todo, por su relación política con el Cid. En el siglo XII, y tras el paso de la primera reina, Urraca I, destaca su hijo Alfonso VII, que avanzó notablemente la reconquista y llegó a coronarse emperador de toda Hispania en la antigua catedral leonesa. A su muerte, dividió los reinos de León y Castilla entre sus hijos; Fernando II reinó en León, destacando la reconquista de Extremadura. Su sucesor y último rey privativo de León fue Alfonso IX, que convocó las primeras cortes de Europa, con participación de todos los estamentos sociales, en la basílica de San Isidoro en 1188. A su muerte, en 1230, la corona leonesa y la castellana recaen en un mismo cetro, Fernando III el Santo, perdiendo León la capitalidad fija.En el siglo XIII la ciudad aún tuvo gran prosperidad comercial y crecimiento. A mediados de siglo, Alfonso X el Sabio ordenó el derribo de la vieja catedral y la construcción de la actual, de estilo gótico.
Durante el siglo XIV, León experimentó una crisis económica que vino acentuada por una serie de acontecimientos climáticos en toda Europa que mermaron las cosechas, produciendo hambrunas y endeudamiento de los campesinos. Estas circunstancias fueron agravadas con la llegada de la peste a León entre los años 1349 y 1350, la cual provocó una gran mortandad en la zona, despoblando pueblos y mermando, según fuentes de la época, en más de un cuarto la población de la zona. A esta serie de fatalidades se le unieron una inestabilidad política en toda la Corona castellana que produjo continuas tensiones que a menudo desembocaron en conflictos armados.

Con la llegada del Siglo XV, las cosas comenzaron a mejorar, observándose un incremento notable en la población en la edificación de nuevas casas, reconstrucción de las anteriores y ensanche de los arrabales. Se hablaba en estos años de hacer una cerca que comprendiese el arrabal de la parte oriental de la ciudad, abarcando las iglesias de San Lorenzo, San Pedro de los Huertos y San Salvador del Nido de la Cigüeña. Así, la ciudad de León, a finales de siglo, contaba con una población entre los cuatro y cinco mil habitantes.

En el siglo XVI, la Guerra de las Comunidades contra Carlos I en León destacó por un insólito fervor comunero en el cabildo catedralicio y en los barrios extramuros. En la órbita local, las dos familias dominantes de aquella época, los Guzmanes, por parte de los comuneros, y los Quiñones, por parte del rey, hicieron de la guerra la excusa perfecta para resolver sus diferencias.

En los siglos XVII y XVIII, León vivió un estancamiento de su población, similar al de las ciudades de la meseta Norte. En estos años, el incremento poblacional en la ciudad no se debió a un incremento de la actividad industrial o comercial, sino al empuje de la agricultura de las zonas rurales que rodeaban la ciudad. Es por ello que la ciudad, con 5.500 habitantes, era, junto con Zamora, una de las ciudades menos pobladas de la meseta norte. Solo a finales del siglo XVIII, la Ilustración le dio algo de empuje a la ciudad, con la construcción de nuevas fuentes y equipamiento público, y la creación de una de las Sociedades Económicas de Amigos del País en la ciudad.


En los días previos al estallido de la Guerra de la Independencia, en concreto el 24 de abril de 1808, tuvo lugar en León, al mismo tiempo que una serie de incidentes acaecidos en otras ciudades españolas como Burgos, Toledo o Madrid, una manifestación popular en favor de Fernando VII ante el miedo de que Carlos IV, el cual contaba con el favor de los franceses, volviera a reinar, suponiendo, por tanto, un rechazo a Napoleón. El 26 de julio de ese mismo año la ciudad caería ante el general galo Jean-Baptiste Bessières. Retomado su dominio en junio de 1812, sólo volvió a manos francesas durante un breve período en 1813, pero acto seguido los franceses se replegaron totalmente, volviendo la ciudad a la normalidad.

En 1833 la ciudad adquirió el rango de capital de su provincia, la cual formaría parte, junto a Zamora y Salamanca, de la Región de León.

Entre finales de siglo y principios del XX, el desarrollo de la minería del carbón la convirtió en nudo comercial y de comunicaciones fundamental en todo el noroeste, con el desarrollo de diversas infraestructuras, entre las que destacan la construcción de su estación de ferrocarril (luego propiedad de Renfe y hoy, de Adif) para vías de Ancho Ibérico, y el trazado de una línea de Ferrocarril de vía estrecha, conocida como "El hullero", que, desde León, conectaba las principales zonas de extracción carbonífera con el núcleo industrial de Bilbao.


En 1904 se aprobó un Plan de Ensanche que tenía como eje principal la Gran Vía de San Marcos, la cual confluía en la Plaza de Santo Domingo. La Avenida Ordoño II unía esta plaza con la de Guzmán el Bueno, encargada de distribuir el tráfico de la estación de ferrocarril por las calles de Roma y República Argentina. A partir de estos grandes ejes se delimitaron manzanas de 100 metros de lado y una hectárea de superficie, solo variadas al noreste para conectar con el casco antiguo.


Tras la sublevación de julio de 1936, la mayor parte de la provincia quedó en manos de los sublevados. En León, la sublevación de la guarnición tuvo lugar el 20 de julio, una vez que la columna minera, que desde Asturias se dirigía a Madrid, hubo dejado la ciudad. La resistencia fue escasa y los cargos públicos del Frente Popular, entre ellos el alcalde Miguel Castaño, fueron arrestados, condenados a muerte y ejecutados.

En los 60 y 70 la ciudad comenzó a crecer, auspiciada por el auge de la minería y la industria. Esto originó una expansión urbana desorganizada en todas direcciones. La creación de barrios como San Mamés, San Esteban o El Ejido respondieron a esta expansión, que no solo se centró en la capital, sino que inició la andadura del Área metropolitana de León, con el desarrollo de pueblos como San Andrés del Rabanedo.
En 1979 se celebraron de nuevo elecciones democráticas en la ciudad de León, en las que se hizo con el triunfo el PSOE por un error de conteo de los votos, siendo finalmente el verdadero ganador el UCD, con Juan Morano a la cabeza, que gobernó hasta 1987. En este año se produjo el "Pacto Cívico", impulsado por José Luis Díez Villarig, por el cual sacó del gobierno a Juan Morano durante dos años, tras los cuales volvería al gobierno municipal por el PP, gobernando hasta 1995. Le sucedió en ese año Mario Amilivia, que gobernó ocho años, hasta 2003, logrando en su primer mandato, el del 1995, la primera y única mayoría absoluta que ha existido en el Ayuntamiento de León hasta las elecciones municipales de mayo de 2011.

Paralelo al desarrollo de estos actos, renació el leonesismo, movimiento cultural del siglo XIX recuperado para la reivindicación política, produciéndose la aprobación de mociones en favor de una autonomía leonesa por parte de municipios y la Diputación Provincial de León en 1983, así como manifestaciones en favor de la autonomía leonesa, con 20 000 personas en 1983 y 90 000 en 1984.

En la década de los noventa la ciudad inició su despegue definitivo, acentuándose con el inicio del nuevo siglo; barrios como Eras de Renueva, Pinilla, La Lastra o La Torre ampliaron aún más el núcleo urbano, el cual también ha visto levantarse singulares ejemplos de arquitectura moderna, como el Museo de Arte Contemporáneo de Castilla y León o el Auditorio Ciudad de León. En estos años el Área metropolitana dio un salto cualitativo, con nuevos planes urbanísticos que determinaron un gran área entre Villadangos del Páramo y Mansilla de las Mulas.


En las elecciones de 2003, el Partido Popular no logró la mayoría y, a diferencia de lo ocurrido en 1999, año en el que pactó con Unión del Pueblo Leonés, este partido decidió dar su apoyo a Francisco Fernández, del PSOE. El PSOE duraría un año en el gobierno municipal, pues una moción de censura y la ruptura del grupo municipal leonesista haría que Amilivia recuperase la alcaldía hasta 2007. En las elecciones de dicho año, el PSOE consiguió por primera vez en la historia de la democracia el mayor número de votos en las elecciones, no llegando aun así a la mayoría absoluta, teniendo que pactar con UPL. Desde el 2011 gobierna el Partido Popular con mayoría absoluta y el alcalde es Emilio Gutiérrez. En las elecciones de 2015, la fuerza política más votada fue el Partido Popular encabezada por Antonio Silván, quien gobierna en la actualidad con mayoría simple con apoyos puntuales de otras formaciones políticas.

El municipio de León cuenta con 125 317 habitantes según el censo de población de 2017 del INE, de los que 56 822 son varones y 68495 son mujeres. En cuanto a su distribución, 121 393 viven en León, 5250 en Armunia, 796 en Trobajo del Cerecedo y 378 en Oteruelo de la Valdoncina. En 1995, la población residente alcanzó la cifra más alta, 147 780 habitantes censados, y a partir de esa fecha se ha ido produciendo un descenso continuado del censo como consecuencia del envejecimiento de la población, la escasez de nacimientos y de la emigración de la población hacia los municipios del alfoz.


Esta estructura de la población es típica en el régimen demográfico moderno, con una evolución hacia un envejecimiento de la población y una disminución de la natalidad anual.


En 1860, la ciudad contaba con una población de 9866 habitantes, población que se incrementó rápidamente gracias a la mejora de las comunicaciones, en las que jugó un papel clave la llegada del ferrocarril a León en 1863. Así, la población creció un 58 % en apenas cuarenta años, hasta los 15 580 habitantes. Este crecimiento no se debió a un aumento de la natalidad o a una disminución del número de defunciones, sino al éxodo rural, que hizo que la mitad de las personas residentes en la ciudad hubiesen nacido en otro lugar.

Con el cambio de siglo, la ciudad comenzó un leve crecimiento, aumentando un 37 % en veinte años, hasta llegar a los 21 399 censados en 1920. Es a partir de este momento cuando se produjo el mayor crecimiento de la ciudad, duplicando el número de habitantes en el mismo período de veinte años hasta los 44 755. Como en décadas pasadas, este crecimiento se debió casi en exclusiva al éxodo rural. En el período entre 1940 y 1960, el crecimiento poblacional se moderó, debido principalmente a la continencia de la avalancha migratoria desde el medio rural, que redujo su aportación al crecimiento de la ciudad de un 97 % a un 25 %. La ciudad, con 73 483 habitantes representaba ya el 12 % del total provincial. En la década de 1960, acabada la época autárquica, el éxodo rural se intensificó, incrementando la población de la ciudad en un 62 % hasta 1975, fecha en la que la ciudad contaba con 115 176 habitantes.

A partir de 1975, la ciudad cambió la dinámica y aminoró su crecimiento a favor de un alfoz creciente donde comenzaron a despuntar pueblos como Trobajo del Camino. Este cambio de tendencia se confirmó a partir del año 1995, año en que la ciudad alcanzó su máximo histórico de 147 625 habitantes. A partir de este año, la población de la capital leonesa se fue reduciento de forma prácticamente ininterrumpida, con algunos años de leve recuperación, hasta los 135 119 habitantes del año 2008. Durante esos años, por el contrario, el área metropolitana de la ciudad experimentó un rápido crecimiento desde los apenas 31.974 habitantes con los que contaba en el año 1975 hasta los 69 256 habitantes con que contaba en 2008. Las razones hay que buscarlas en la falta de vivienda o un precio de ésta más elevado en la capital que en el área metropolitana.
</small>

Las entidades de población que componen el término municipal de León son las siguientes:

El colectivo inmigrante durante el año 2008 en la ciudad de León se cifró en 8280 personas, entre los que destacan los procedentes de América, con 3417 personas del total. Por países, los más numerosos son los de nacionalidad marroquí, integrando este colectivo 1418 personas, rumana con 1038 censados y los procedentes de Colombia con 1006, el resto de inmigrantes se reparte entre varias nacionalidades de todos los continentes.

En 1970, el área metropolitana de León contaba con una población total de 153 526 habitantes, población que disminuyó años después hasta los 150 104 de 1975. A partir de esta última fecha, el área urbana comenzó un rápido crecimiento que se prolongó hasta el año 1996, año en el que alcanzó los 190 648 habitantes. A partir de ese momento, hubo una pequeña caída en el número de habitantes del área debido a los efectos de la crisis del carbón, que atenazó las comarcas circundantes a León y a la propia ciudad. La población bajó hasta 183 611 habitantes en 2001. Es a partir de este año cuando comenzó un rápido crecimiento, que absorbió en su totalidad los municipios aledaños a León, crecimiento que todavía continúa, y que en 2008 supuso que el área tuviese 201 987 habitantes.

Una vez fue establecido el campamento romano en torno a los años 74-75, este se encargó del control, gestión y explotación de las minas de oro, de las cuales la más importante era la de Las Médulas. Su actividad atrajo a población civil que se asentó alrededor del campamento para satisfacer las necesidades de los soldados, asentándose en el recinto civil "canabae", que desarrollaba actividades como la artesanía o el comercio, que evolucionaron para no dar servicio tan solo a la legión sino también a la creciente población civil, crecimiento que atestigua la presencia de unas termas, de uso militar y civil.

Con la caída del Imperio romano, León entró en decadencia, el comercio y la artesanía pasaron a ser testimoniales y la población se redujo en gran medida, razón por la cual la ciudad pasó a ser un centro agrícola de poca importancia y un lugar de paso para los ganaderos de la zona. Con la llegada de los árabes, la ciudad se despobló definitivamente, sirviendo sus murallas como majada para los ganaderos de la zona.

No fue hasta el año 856 en el que Ordoño I repuebla la ciudad y reconstruye sus murallas, reactivando el comercio y la artesanía en la ciudad. Sin embargo el verdadero impulso lo dio Ordoño II al convertir a León en capital de su reino, haciendo que esta se convirtiera en uno de los principales centros urbanos de la España cristiana. Los avatares políticos fueron quitando protagonismo a León a lo largo de la historia, culminando esta pérdida de protagonismo en la unión definitiva con Castilla en el año 1230. Pese a ello, muchas de las instituciones del reino tuvieron continuidad después de esta unión.

Las malas comunicaciones con el resto del país hicieron que la ciudad mantuviese un aspecto rural y una población estable hasta comienzos del siglo XX. Es en ese siglo, cuando la ciudad inició una recuperación económica. Su condición de capital de provincia, y por ende, de centro urbano de referencia de la zona, así como la llegada del ferrocarril hizo que la ciudad se expandiera en todas direcciones con el ensanche y los barrios periféricos. La industria se asentó en un primer momento en los alrededores de la estación de ferrocarril desplazándose más tarde hacia el extrarradio y luego hacia los polígonos industriales habilitados en torno a la ciudad; no obstante la importancia de este sector nunca llegó a ser relevante en la estructura económica de la ciudad, en la que pesa más el sector servicios.

Durante la primera década del siglo XXI, la ciudad está viviendo una reactivación del sector industrial, motivado por su promoción como centro de transportes del noroeste con el aeropuerto y con las nuevas vías de alta capacidad, reactivación que se ve acompañada por el crecimiento de sectores económicos relacionados con el I+D. La ciudad no obstante, mantiene la lacra de la emigración de los jóvenes por la falta de trabajo en ciertos sectores, que sin embargo en términos globales está siendo amortizada por la inmigración y la reducción progresiva de esta emigración.

El sector primario en León se encuentra en vías de desaparición por la presión urbanizadora que la ciudad ejerce sobre los terrenos agrícolas todavía disponibles. No obstante, aún quedan remanentes de este antaño importante sector económico para la ciudad, en las vegas de los ríos Torío y Bernesga y en el alfoz, consistentes sobre todo en una modesta cabaña ganadera que hace uso de los pastos que rodean la ciudad y en pequeñas plantaciones de cultivos cerealistas, como la cebada y el trigo.

Es importante también citar la silvicultura, que se centra en las riberas de los ríos y utiliza el chopo, por su condición de especie de rápido crecimiento y aceptable calidad maderera. Por el contrario, la presencia de la acuicultura y la pesca es despreciable, en cuanto que de la primera apenas existen empresas y la actividad pesquera se centra solamente en la pesca deportiva en los ríos cercanos.

El sector secundario leonés se caracteriza por su debilidad y por inexistencia de grandes empresas que generen un entramado empresarial a su alrededor, basándose pues en pequeñas y medianas empresas. Los sectores en los que tradicionalmente se ha basado el entramado industrial de la ciudad son la metalúrgica de transformados metálicos, la industria química, de maquinaria, alimentaria, cerámica, del vidrio, del papel y artes gráficas y el textil. Es reseñable que la mayoría de las industrias de la ciudad se encuentran ubicadas fuera del término municipal de la ciudad, ubicadas en polígonos industriales que en su mayoría se encuentran conurbados con la ciudad.

Desde comienzos del siglo XXI y a consecuencia de la apertura de las grandes infraestructuras leonesas, tales como la A-66, la AP-71, la A-231 y el aeropuerto de León, inaugurado en 1999, la ciudad está experimentado cierto auge industrial, palpable en un aumento del suelo industrial disponible en el área metropolitana y en menor medida en el propio término municipal de León. La reactivación ha afectado también a las actividades relacionadas con la innovación y el desarrollo tecnológico, que tras el apoyo de las administraciones públicas con la implantación en la ciudad de varios centros tecnológicos como el Inteco y el superordenador Caléndula, perteneciente a la fundación de supercomputación de Castilla y León, además de la colaboración de la universidad con el impulso del sector, ha experimentado un desarrollo, con la llegada de varias empresas importantes del sector, como Hewlett-Packard, SAP, Telvent o Indra, entre otras.

El sector servicios leonés se encuentra diversificado, como corresponde a un centro urbano de cierta entidad, de este modo, la ciudad es el centro de referencia comercial de la provincia. Así, la ciudad cuenta con un sector comercial basado en su mayoría en un comercio tradicional, complementado en los últimos años con la apertura de grandes y medianas superficies en la ciudad, tales como Carrefour, El Corte Inglés, el E.Leclerc, Mercadona, entre otras, así como de centros comerciales, como Espacio León y León Plaza.

El turismo es también un factor clave en el sector servicios de la ciudad, pues la ciudad es visitada anualmente por más de 600 000 personas, animados por la presencia en la capital de un gran patrimonio monumental y de bellos espacios naturales en las inmediaciones de esta, así como de varias fiestas de gran afluencia y reconocido prestigio, entre las que sobresale la Semana Santa.

La Cámara Oficial de Comercio e Industria de León está presente en la ciudad desde el año 1907 por iniciativa de un grupo de comerciantes e industriales de la ciudad. Con sede en un edificio modernista de la avenida Padre Isla de la ciudad, la cámara se encarga de representar y defender los intereses generales del comercio y la industria de la provincia de León.

La ciudad es capital de la provincia de León, y por tanto están ubicados en ella los entes administrativos de ámbito provincial. Por parte del Gobierno de España se ubican la Subdelegación, y la Diputación Provincial. La Junta de Castilla y León, por su parte, gestiona las áreas de educación, sanidad y empleo.

Las primeras elecciones municipales democráticas tras la reinstauración de la democracia en España se celebraron en 1979. Dichas elecciones dieron en un principio el siguiente resultado: 10 concejales para la UCD, 10 concejales para el PSOE, 4 concejales para el PCE y 3 concejales para Coalición Democrática. De este modo, empezó gobernando la ciudad el PSOE, pero 3 meses después de las elecciones, se anularon varias mesas electorales. Aquel hecho dio lugar a que el PCE perdiera un concejal en favor de la UCD, que de esta manera, se alzaría con la alcaldía de la ciudad.

Desde ese año, han gobernado la ciudad: la UCD en sus primeros años y el equipo de gobierno formado por Juan Morano Masa tras la disolución de dicho partido hasta 1987, año en que el denominado "Pacto Cívico" (AP - PSOE - CDS) comenzara a gobernar la ciudad hasta 1989. Desde entonces ha gobernado en solitario el PP hasta 1999 y en coalición con la UPL durante la legislatura 1999-2003. El año 2003 comenzó a gobernar el PSOE en coalición con la UPL hasta el 2004, en que la ruptura de la coalición y una moción de censura llevaron al PP a gobernar la ciudad en minoría hasta el 2007, año en el que el PSOE volvió a gobernar en coalición con la UPL hasta el 2011, en que el PP consiguió la mayoría absoluta. En las elecciones municipales de 2015 el PP volvió a ser el partido más votado, pero perdió cinco concejales; este año se presentaron por vez primera el partido Ciudadanos y la agrupación Despierta León (Podemos) que obtuvieron respectivamente cuatro y dos concejales.

El Ayuntamiento regula asuntos como por ejemplo la planificación urbanística, los transportes, la recaudación de impuestos municipales, la gestión de la seguridad vial mediante la Policía Local y el mantenimiento de la vía pública (asfaltado, limpieza...) y de los jardines. También es el responsable de la construcción de equipamientos municipales como polideportivos, bibliotecas, centros de servicios sociales y viviendas de protección pública.

León está dividida administrativamente en barrios, algunos de los cuales, como La Sal, El Crucero o Barrio de Pinilla no pertenecen totalmente al municipio de León, ya que parte de ellos pertenecen a San Andrés del Rabanedo. En cambio, otros como Paraíso-Cantinas están totalmente dentro de este último municipio aunque forman parte de la ciudad.

Por otra parte, en el municipio, además de la cabecera, se encuentran las localidades de Armunia, Oteruelo de la Valdoncina y Trobajo del Cerecedo.

León es sede de la Audiencia Provincial y cabeza del Partido Judicial número 2 de la provincia de León, cuya demarcación comprende a la ciudad y a otros municipios de las comarcas limítrofes. El conjunto de organismos judiciales es el siguiente:


La ciudad es la sede de la diócesis de León, la cual abarca las zonas norte y este de la provincia de León, y es sufragánea de la archidiócesis de Oviedo. La ciudad posee además una mezquita musulmana en el barrio del Crucero.

Templo gótico dedicado a Santa María, fue comenzada a construir en el reinado de Alfonso X el Sabio a mediados del siglo XIII sobre la antigua catedral románica, que a su vez ocupaba los terrenos del Palacio Real que cediera Ordoño II para ello y que, a su vez, se asentaba sobre las termas romanas. De planta similar a la catedral francesa de Reims, tiene reducida su planta en 1/3 con respecto a esta. Una característica peculiar es que las torres aparecen separadas de la nave central mediante arbotantes. Su planta es de tres naves, con bóveda de crucería. Trabajaron en ella distintos arquitectos como el Maestro Simón, el Maestro Enrique y Juan Pérez (estos dos empleados por entonces también en la Catedral de Burgos) y el Maestro Jusquín. El cuerpo principal del edificio fue terminado a principios del siglo XIV junto al claustro y la torre norte, mientras que la torre sur fue terminada en el siglo XV, en estilo gótico flamígero. En los siglos XVII y XVIII sufrió modificaciones estéticas por parte de Juan de Náveda y Joaquín de Churriguera, elementos que provocaron daños al edificio y fueron retirados en el siglo XIX por Matías Laviña, Juan Madrazo, Demetrio de los Ríos, Juan Bautista Lázaro y Juan Crisóstomo Torbado, muchos de los cuales llevaron a cabo la intensa restauración decimonónica que salvó el templo de la ruina, además de devolverle su esencia gótica original. Lo más impresionante es su interior, destacando los más de 1.800 metros cuadrados de vidrieras de los siglos XIII al XVI, incluyendo tres grandes rosetones y vidrieras en la parte baja y el triforio, algunas de ellas añadidas en las restauraciones del siglo XIX.

Constituye uno de los ejemplos de arte románico más importantes de España y, sin duda, uno de los conjuntos más completos en este estilo, por cuanto que en él confluyen arquitectura, escultura y pintura, albergando en esta última técnica el Panteón Real, llamado por los expertos "Capilla Sixtina del Arte Románico". Impulsada su construcción por los reyes Fernando I y su esposa Doña Sancha en el siglo XI, originariamente fue un monasterio dedicado a San Juan Bautista, y se supone que anteriormente se asentaba en sus cimientos un templo romano. Con la muerte de San Isidoro, obispo de Sevilla, y con el traslado de sus restos a León, se cambió la titularidad del edificio. Albergó las primeras Cortes de la historia, las Cortes de León, celebradas en 1188.

Impulsada su construcción por los Reyes Católicos como sede de la Orden de Caballería de Santiago, puesto que de hecho fue erigido a orillas del río Bernesga y literalmente junto al puente medieval de San Marcos, por el que los peregrinos continuaban el Camino de Santiago, es hoy uno de los monumentos más importantes de León. De estilo plateresco, en su construcción participaron Juan de Orozco, que firmó los planos de la iglesia, Martín de Villarreal, autor de la fachada, y Juan de Badajoz el Mozo, a quien se debe el claustro y la sacristía. En el siglo XVIII se construyó el ala izquierda del edificio, respetando su arquitectura plateresca. Su historia ha estado llena de avatares y su uso original no duró mucho: tras ello ha sido cárcel (en ella encerró el Conde-Duque de Olivares a Francisco de Quevedo), cuartel, sede de los estudios veterinarios, origen de la Facultad de Veterinaria de la Universidad de León y del Instituto General y Técnico (uno de los tres primeros Institutos de Enseñanza Media creados en España por ley de 1845), fundado en 1846 (hoy IES Padre Isla), e incluso fue campo de concentración durante la Guerra Civil. Actualmente es un parador de cinco estrellas.

Obra de Antonio Gaudí, de estilo neogótico. Es de planta trapezoidal, flanqueada por cuatro torres rematadas en pináculos. Las ventanas tienen su inspiración en las ventanas del triforio de la catedral leonesa. En la portada hay una talla de San Jorge matando al dragón. El edificio fue concebido para el negocio de tejido en su planta baja y semisótano, destinándose las cuatro plantas restantes a viviendas de renta.

La construcción del edificio se debió a la iniciativa de unos comerciantes de tejidos de León, Simón Fernández Fernández y Mariano Andrés Luna, que estaban relacionados con industriales textiles catalanes, uno de los cuales, Eusebi Güell, recomendó a Gaudí como arquitecto para diseñar la nueva sede del negocio en la capital leonesa, ya que por entonces estaba construyendo cerca de León el Palacio Episcopal de Astorga. Gaudí delegó la dirección de las obras en el constructor Claudi Alsina i Bonafont, uno de sus ayudantes en varias obras en Barcelona, y contó con la colaboración de varios albañiles y artesanos catalanes.

Mandado construir por Juan Quiñones y Guzmán, obispo de Calahorra, se comenzó la obra en 1560 bajo la dirección de Rodrigo Gil de Hontañón. El edificio fue adquirido por la Diputación Provincial de León en 1882, teniendo ampliaciones en los años 1973 a 1976 por parte del arquitecto Felipe Moreno. De forma trapezoidal, los dos primeros cuerpos tienen vanos protegidos por rejería, siendo los balcones del superior adintelados, y el tercer cuerpo presenta una galería o paseador con arquillos entre pilastras corintias y gárgolas de grandes dimensiones. Tiene dos puertas del siglo XVI, una de ellas con una estructura de dos columnas jónicas, flanqueadas por dos soldados con los escudos de armas de la familia.

Antigua residencia de la familia Quiñones, condes de Luna, fue realizado en el siglo siglo XIV, época de la que se conserva el cuerpo central de la fachada. Está construido de piedra sillería y tiene cerca de once metros de ancho. La portada es gótica con dintel sobre modillones, un gran arco apuntado cobija el tímpano, y se encuadra en ancho molduraje. Se conserva también uno de los grandes torreones, de finales del siglo XVI, cuando se reformó el palacio al estilo renacentista. El edificio ha tenido diversos usos a través de la historia, además de su función original de residencia de los condes de Luna, como el ser sede del Tribunal de la Inquisición de la ciudad y vivienda particular, entre otros. Cedido al Ayuntamiento por la Fundación Octavio Álvarez Carballo, alberga la sede española de la Universidad de Washington así como la sede de la Fundación León Real.


La Plaza Mayor de León, ubicada en el corazón del casco antiguo, fue finalizada en 1677 según planos de Francisco del Piñal siguiendo el ejemplo de otras plazas mayores españolas, en particular la de Madrid. El edificio del Consistorio que preside la plaza es de estilo barroco y fue diseñado por el propio Francisco del Piñal como balcón para que la corporación municipal presidiera los eventos de la plaza.

Ubicado en la Plaza de San Marcelo, fue construido a finales del siglo XVI por Juan de Rivero para ser la sede del gobierno municipal, es de estilo renacentista y consta de tres alas y una escalera interior. Actualmente solo conserva algunas de las concejalías del ayuntamiento.

Se trata del templo más antiguo de la ciudad de León, fue fundada en el siglo X por Ramiro II de León y, como su nombre indica, se trata del templo del "Palat" (el Palacio) del rey. De su pasado como oratorio regio de la monarquía leonesa da buena cuenta la propia elección de la dedicación, San Salvador, recordando al templo mayor ovetense, o su uso como panteón de la monarquía, antes de la construcción del que sería el gran mausoleo isidoriano. Del templo original, prerrománico, pueden apreciarse hoy pocos restos, aunque ha sido recientemente restaurada y musealizada.

La Iglesia de Nuestra Señora del Mercado es una iglesia con planta basilical en forma de sepulcro, siendo más estrecha a los pies. Presenta una portada románica de arco ciego, dos ábsides también románicos decorados con bóveda de horno, capiteles y líneas de imposta con taqueado jaqués. Los pies del edificio se cierran con bóveda de crucería. La torre es obra de Felipe de Cajiga (1598), habiendo sido rematada por Fernando de Compostiza.


Arco que desde tiempos romanos era una de las puertas de entrada a la ciudad. Se situaba junto a una fortaleza que se conservó en la Edad Media. Actualmente, la fortaleza o castillo -sede del AHP de León- aún es visible, mientras que el arco de entrada fue reconstruido en el siglo XVIII. Está presidido por una estatua dedicada a Don Pelayo y es la única puerta de entrada a la ciudad que se conserva.


Se trata de un palacio de estilo barroco del siglo XVII que albergaba la residencia de los marqueses de Prado, señores de Valdetuéjar. Actualmente es el Hospital de Regla. Presenta una fachada barroca en la que se repiten los blasones de los Prado.

Embutido en el patio del colegio de las Teresianas se halla el único edificio civil del siglo XII de la ciudad. De planta cuadrada en ruinas, en una cara conserva puerta y ventanas románicas y una escalera interior de caracol. Se desconoce el servicio que se le dio en la Edad Media.


Construido en el siglo XVII, consta de grandes dimensiones, planta cuadrada y patio interior, y su fachada está recubierta de ladrillo ornamental. Actualmente es la sede del Recreo Industrial.


Edificado en la primera década del siglo XX por el arquitecto leonés Manuel de Cárdenas, su estilo arquitéctónico trata de no romper la armonía gótica catedralicia. Se observan también influencias de Gaudí en un edificio de gusto neogótico.

El edificio actual data del siglo XVII y aún conserva el blasón de esta familia, descendientes del emperador Alfonso VII y de Guzmán el Bueno.


De lo que fuera la iglesia dedicada al centurión romano Marcelo no queda más que una portada gótica. El templo actual es de estilo herreriano, terminado a principios del siglo XVII. Del exterior destaca la torre cuadrada de la iglesia, cubierta de característicos ladrillos, que se asoma a la plaza de Santo Domingo.


Situada en pleno corazón del Barrio Húmedo, se inició en estilo renacentista a finales del siglo XVI. La fachada, de corte clásico, consta de dos pisos. Actualmente está dedicada a oficinas bancarias.

De estilo neorrománico, fue realizada a finales del siglo XIX por Demetrio de los Ríos, uno de los restauradores principales de la Catedral de León. La portada imita la puerta del Perdón de la Basílica de San Isidoro de León. En su interior se conserva una escultura gótica del crucificado.

El Convento de las Concepcionistas, fundado en 1512 por Leonor de Quiñones, presenta una portada románica del antiguo edificio y corredores con pinturas mudéjares. Su iglesia es de una sola nave, con cabecero del siglo XVI, obra de Juan del Ribero. Se conservan en su interior mobiliario artístico, retablos barrocos, pintura y orfebrería.


Su construcción comenzó en el siglo XVIII pero no se vio totalmente terminado hasta 1936. Durante años fue la residencia oficial del . Posee una estructura cuadrada con un patio en el centro.

Destacable es también el "Castrum Iudeorum". Los primeros testimonios de presencia judía en la ciudad de León se remontan al siglo X; entonces se documenta la existencia de una próspera comunidad hebraica asentada en el cerro de la Mota, cercana a la actual pedanía de Puente Castro, sobre el curso del río Torío y a la vera del Camino de Santiago, circunstancia esta que favoreció su tradicional dedicación a actividades vinculadas al comercio y la banca. Sin embargo, tras el ataque que la aljama sufrió en el siglo XII a manos del rey de Castilla, sus moradores fueron obligados a abandonarla y asentarse en la ciudad de León, donde crearon una nueva aljama. En la actualidad, se están llevando a cabo una serie de investigaciones y estudios arqueológicos en torno a este yacimiento, dirigidas desde los departamentos de Historia y Patrimonio de la Universidad de León por Jorge Sánchez-Lafuente Pérez y José Luis Avello Álvarez.


La muralla romana de León tiene su origen en una primera fortificación militar de época augustea, en torno al siglo I a. C., y consistía en dos muros paralelos de madera rematados por un parapeto que estaban unidos por un entarimado. Pronto fueron sustituidos por una construida en piedra por la Legio VII en torno al siglo I, cuyos restos aún son visibles en la zona de San Isidoro. En torno a los siglos III y IV se construyó la que puede contemplarse hoy en día. Declarada Monumento Histórico Artístico en junio de 1931, aún quedan en pie muestras que encerraban el recinto de la ciudad en un cuadrilátero que fue rodeado de construcciones y más tarde deformado en las restauraciones de Alfonso V y Alfonso IX, con apertura de nuevas entradas a la ciudad. Está regularmente conservada desde la torre llamada de los Ponces (de origen también romano) hasta Puerta Castillo, y desde aquí hasta la torre de San Isidoro, en total casi la mitad del recinto, aunque con desigual estado de conservación. Se está procediendo actualmente a su restauración.

En cuanto a las cercas medievales, su origen data del siglo X, y fueron construidas para proteger la expansión de la ciudad extramuros de la muralla romana, que en la época romana se denominaba cannaba y que daría lugar al actual Barrio Húmedo. Las actuales murallas datan del siglo XIV y se conservan dos trazados importantes. En medio de su trazado se conserva parcialmente Puerta Moneda, antigua entrada al barrio judío de la ciudad.

Por Barrio Húmedo se conoce al distrito situado en el viejo León, una zona que abarca los alrededores de la Plaza Mayor y de la Plaza de San Martín o de las Tiendas, a la que van a desembocar un total de siete calles. Esta plaza fue el lugar en torno al cual se concentraban los artesanos, mercaderes y peregrinos de la ciudad de León y que hoy, desaparecidas esas actividades o desplazadas a otros lugares de la ciudad, han sido sustituidas por actividades hoteleras y de esparcimiento.

Su atractivo está en que desde la calle La Rúa hasta la calle Caño Badillo, el paisaje urbano del Barrio Húmedo se llena de bares, cafés y mesones que convierten la zona en la mayor ruta del "tapeo" y en escaparate de las especialidades gastronómicas de la ciudad y de la provincia. Esta circunstancia, unida a la estrechez de sus calles y sus plazas, forman el espacio más típico de la ciudad, caracterizado por su trazado medieval con irregularidades urbanísticas y que es destino obligado para los turistas que visitan la ciudad. El 22 de mayo de 1995 se terminó su peatonalización.

El Barrio Húmedo cuenta también con varios edificios y entornos urbanos destacables; además de las citadas Plaza Mayor y Plaza de San Martín, se encuentra la Plaza del Grano, conocida por conservar su viejo empedrado de canto rodado, estar rodeada de los últimos vestigios de arquitectura tradicional leonesa que quedan en la ciudad y por la iglesia del Mercado. Además de esto, en el barrio se encuentran la casa de las Carnicerías, el palacio de Don Gutierre y el palacio del Conde Luna.

La ciudad de León es una ciudad reconocida por su gran cantidad de zonas verdes, tanto es así que León es la ciudad española que más zonas verdes pone al servicio de sus ciudadanos. León cuenta con 2.196.542 m² de zonas verdes distribuidos por toda la ciudad. Este espacio se encuentra dividido entre numerosos parques, entre los que destacan por tamaño; el Parque del Chantre, Parque de Quevedo, el Jardín del Cid, el Jardín de San Francisco y el Parque de La Granja.

Es el pulmón verde más notable de la ciudad, paralelo al río, se extiende desde el convento de San Marcos hasta las inmediaciones de la plaza de toros, interrumpido por la plaza de Guzmán el bueno, que marca la línea divisoria entre el Paseo de la Condesa, aguas arriba, y el Paseo de Papalaguinda, aguas abajo.

Los orígenes de esta gran zona verde hay que buscarlos a principios del siglo XIX, cuando se planteó el ensanche para unir el casco histórico con el Bernesga, cuya unión definitiva se realizó a través de esta extensa zona verde. Hoy se encuentra jalonado de esculturas modernas y de quioscos de música, así como de escaleras para descender al río.

El parque está poblado por un buen número de diferentes especies de árboles, arbustos y aves. Por su gran presencia, destacan los ciruelos, arces blancos, cipreses, enebros, castaños de indias, olmos, encinas, hayas y glicinia, entre otras especies. Entre las aves, es común la presencia en el parque de currucas capirotadas, de golondrinas zapadoras, de verderones comunes y aguzanieves.



La escultura en León está protagonizada por obras que representan a ilustres personajes, a eventos y a la propia esencia de la ciudad a los que se les ha recordado de esta manera. Así, en el año 1789 se instaló la escultura de Neptuno, inicialmente en la plaza de la catedral, trasladándose a la Plaza mayor y más tarde al jardín de San Francisco, donde permanece actualmente. En la misma fecha se inaugura la fuente del mercado, en la Plaza del Grano. Guzmán el Bueno cuenta también con su propia escultura, presidiendo la plaza homónima, instalada en 1900, recibió numerosas críticas en su día, llegando a ser con el paso del tiempo una de las esculturas más emblemáticas de la ciudad. El escultor Julio del Campo, oriundo de la provincia, posee su propia escultura en el ensanche, inaugurada en 1917, también en el ensanche, se encuentra la escultura de la Inmaculada, inaugurada en los años 50.

Otra escultura de gran relevancia es La vieja negrilla, llegó por primera vez a la plaza de Santo Domingo en diciembre de 1997, donde permaneció diez años, hasta que un conductor ebrio se empotró contra ella en 2007. Amancio González modeló entonces la figura de nuevo y, gracias al patrocinio de Renfe, la nueva 'negrilla' se volvió a colocar en el mismo lugar en 2009. Con una diferencia: la segunda vez se hizo en bronce –la anterior era de hormigón–, "para que los niños pudieran subirse a ella y jugar, como hacía yo de pequeño con la vieja negrilla de mi pueblo", recuerda Amancio.

Son destacables también los cuatro leones, que adornan el puente de los leones, anteriormente del ferrocarril, instaladas en 1967, obra del autor Víctor de los Ríos y lejos de allí el Don Quijote en Sierra Morena, en la universidad, instalada en 1964 inicialmente en el Alto del Portillo por un encargo de Caja León y trasladada más tarde al campus universitario.

Entre las nuevas esculturas, instaladas desde finales del siglo XX en nuevos barrios en zonas remodeladas, como la avenida Ordoño II, caben destacar a escala humana el "monumento a la lucha leonesa", de Ángel Muñoz Alique y situado junto al Estadio Reino de León, el "Peregrino sentado en el Crucero" (1998), de Martín Vázquez de Acuña e instalado en la plaza de San Marcos tras su remodelación, el "Homenaje al Maestro Odón Alonso", de Ángel Muñiz Alique, junto al Auditorio Ciudad de León, "Antoni Gaudí sentado en un banco" (1998), de José Luis Fernández y enfrente de la obra de Antoní Gaudí la Casa Botines. Otros ejemplos son "Padre e Hijo" (1997), en la plaza de la Regla y obra de Jesús Trapote Medina y "Las Cabezadas", obra de José Luis Fernández.

El desarrollo urbanístico de la capital leonesa ha estado condicionado por su situación entre los ríos Bernesga y Torío. Entre ambos se situó el núcleo romano de la Legio VII y durante la Edad Media se expandió por el lado sur, estando rodeado todo él por una muralla. A principios del siglo XIX la ciudad seguía siendo ese pequeño núcleo urbano, articulado en torno a la Catedral y de marcado carácter rural.

La llegada del ferrocarril en 1863 se convirtió en el factor que provocó el crecimiento de la ciudad a partir de ese momento. La situación de la estación, en la margen derecha del Bernesga, y al oeste del casco antiguo, fue decisiva para el posterior desarrollo urbano que vivió su área circundante, ya que la ciudad se expandió principalmente hacia esa zona. A medida que avanzó el siglo, la ciudad vieja se reveló como un marco inadecuado para satisfacer las necesidades de la creciente población.

En 1904 se inició el ensanche de la ciudad en torno a su eje principal, la calle Ordoño II. Durante medio siglo supuso el lugar de asentamiento de la burguesía leonesa debido a que la legislación prohibía casas obreras e industrias en la zona. Entre 1910 y 1950 la llegada de inmigrantes a la ciudad fue continua, lo que provocó un problema pues la falta de vivienda distaba de satisfacer las necesidades de estos nuevos inquilinos; la solución fue la de iniciar la construcción de barrios obreros a las afueras, comenzando así la expansión suburbial de la ciudad.

A mediados de los años 1950 se iniciaron los proyectos para elaborar un Plan General de Ordenación Urbana (PGOU), aprobándose definitivamente en 1960. Gracias al mismo, se concluyen los barrios periféricos de la ciudad, algunos de ellos iniciados en los años 1920. En los años 1970 dicho PGOU había quedado superado, por lo que se hacía necesaria la implantación de un nuevo Plan: en 1975 se iniciaron los trámites, aunque finalmente sólo será una adaptación del Plan de 1960.

Tras el desarrollo a finales del siglo XX del barrio Eras de Renueva, dos son los nuevos espacios residenciales con los que contará la ciudad una vez acabados: La Lastra, junto a la confluencia de los dos ríos, y La Torre, junto a la universidad. Asimismo, desde finales del mismo siglo, la ciudad ha trasladado la mayor parte de su crecimiento fuera de los límites municipales, beneficiando a su área metropolitana, con municipios como San Andrés del Rabanedo, Villaquilambre o Valverde de la Virgen.

Si bien la llegada del ferrocarril fue un revulsivo para la ciudad, el soterramiento del mismo en la segunda década del siglo XXI supondrá otro tanto, pues a la desaparición de la barrera urbanística que significaba el tren hay que añadir el espacio liberado que se dedicará a diversos usos como zonas verdes, viviendas, equipamientos, etc.

La ciudad de León cuenta con numerosos centros de enseñanzas no universitarias. De carácter público, cuenta con 17 centros de educación infantil y primaria, uno de educación especial, 9 de educación secundaria y un centro específico de formación profesional. De carácter privado, la ciudad cuenta con 20 centros, dos de los cuales son de educación especial y uno de formación profesional.

En cuanto a las enseñanzas de régimen especial, León cuenta con una Escuela Oficial de Idiomas (en la que se imparten alemán, francés, inglés, italiano, portugués y español para extranjeros), una Escuela de Arte y de Conservación y Restauración de Bienes Culturales, dos Conservatorios de Música (uno de ellos de carácter privado) y un Centro de Educación de Personas Adultas (CEPA).

La ciudad también cuenta con la Universidad de León. Fue fundada en 1979, desgajándola de la Universidad de Oviedo, a partir de las diversas escuelas y facultades que, dependientes de aquella, existían en la ciudad de León, y sobre unos terrenos llamados Vegazana (de donde toma nombre el campus universitario) donados por la entonces Caja de Ahorros y Monte de Piedad de León. La universidad cuenta con dos campus, el ya citado de Vegazana, situado en la parte noreste de la ciudad, que está siendo objeto de una intensa ampliación para adaptarse a la normativa europea, y el de Ponferrada.

La Universidad de León cuenta con 8 facultades y 7 escuelas (3 de ellas adscritas), imparte más de 50 titulaciones, y posee 26 departamentos, 7 institutos universitarios y 4 centros tecnológicos. Además tiene un Centro de Idiomas donde se imparten 9 idiomas. En el curso 2011-12 contaba con 12.643 alumnos. Su rector es Juan Francisco García Marín.


El sistema sanitario de la ciudad de León se divide entre las prestaciones del sistema público de salud, gestionado por Sacyl (Sanidad Castilla y León), y las que realiza la medicina privada. La Ley 1/1993, de 6 de abril, de Ordenación del Sistema Sanitario, divide la atención sanitaria en tres niveles de atención: primaria, especializada y continuada.

La atención primaria en la provincia se divide en dos Áreas de Salud, El Bierzo y León. Esta última engloba 28 Zonas Básicas de Salud, correspondiendo a la capital 7 de las mismas. Para desarrollar esa atención primaria, León cuenta con 7 centros de salud, los de Eras de Renueva, La Palomera, El Crucero, Armunia, La Condesa y los dos de José Aguado.

Para la atención especializada, la ciudad cuenta con el Hospital de León, el cual lo conforman varios centros:

En cuanto a la sanidad privada, además de numerosas consultas particulares, existen 4 centros hospitalarios: la Clínica San Francisco, que cuenta con 94 camas y aglutina gran número de especialidades, la Clínica López Otazú, el Hospital de San Juan de Dios, perteneciente a la Orden Hospitalaria San Juan de Dios, que se encuentra en el límite municipal entre San Andrés del Rabanedo y León y cuenta con 234 camas y el Hospital de Nuestra Señora de Regla, con 120 camas y administrado por la Obra Hospitalaria Nuestra Señora de Regla, perteneciente al Obispado de León.


Los servicios sociales en la ciudad de León son gestionados por la Concejalía de Bienestar Social. Esta cuenta con una serie de programas sociales como Ayudas de Emergencia Social, Servicio de Apoyo a las Familias, Servicio de Ayuda a Domicilio, Servicio de Información y Orientación, Servicio de Teleasistencia y Minorías Étnicas.

Entre los medios que ofrece están el Hogar Municipal de Transeúntes, el Centro Municipal de Atención a Inmigrantes (CEMAI), el Centro Municipal de Acción Voluntaria y Cooperación (CAV) y 8 CEAS, los cuales dan cobertura a los distintos barrios y prestan los llamados Servicios Sociales Básicos, como por ejemplo Servicio de Apoyo a la Familia y Convivencia, Servicio de Atención a la Mujer o Servicio de Animación Comunitaria.


Del transporte de la energía eléctrica por todo el territorio nacional se ocupa la empresa Red Eléctrica de España. La distribución de la electricidad en León la realiza "Endesa-Distribución", del grupo Endesa. El consumo total de energía eléctrica durante el segundo trimestre de 2008 fue de 615.149 MWh, de los que 203.427 MWh correspondieron al consumo doméstico.


León y su provincia se abastecen de combustibles derivados del petróleo (gasolina y gasóleo) desde las instalaciones de almacenamiento que la Compañía Logística de Hidrocarburos (CLH) posee en la localidad de Vega de Infanzones, cercana a León.


El gas natural que se consume en León proviene, como en la mayor parte de España, principalmente de Argelia. Es transportado por una red básica en alta presión responsabilidad de Enagás, desde donde se distribuye a viviendas e industrias por las instalaciones de Gas Natural Castilla y León.


El Ente Regional de la Energía es un organismo público dependiente de la consejería de Economía y Empleo de la Junta de Castilla y León, creado el 3 de diciembre de 1996, sus funciones son las de asesorar en materia energética a las empresas de la comunidad autónoma de Castilla y León, promoviendo subvenciones a fin de mejorar la eficiencia energética en el sector empresarial y en las administraciones públicas.

Su sede se encuentra en un edificio vanguardista situado en el barrio de Eras de Renueva, enfrente del MUSAC y al lado del Tanatorio de SERFUNLE. Es la sede del organismo regional que se dedica a la planificación de la energía en la autonomía y servir de apoyo para las decisiones en el campo de la energía con la realización de estudios sobre la viabilidad e incidencia económica de dichas decisiones. En el EREN funcionan 21 metros cuadrados de paneles térmicos y una instalación fotovoltaica de 5 kWh, para auto abastecimiento energético del edificio.


El abastecimiento de agua a León lo realiza la entidad Aguas de León. Antiguamente, el agua se tomaba de los ríos Luna y Torío y de cinco perforaciones hechas en el área Bernesga-Torío, pero debido a las frecuentes restricciones estivales, se tomó la decisión de tomar agua del río Porma, corriente abajo del embalse Juan Benet. Éste se sitúa en la zona norte de la provincia, en el municipio de Boñar, y cuenta con una capacidad de 317 hm³.

A su llegada a León, la conducción de agua termina en la Estación de Tratamiento de Agua Potable (ETAP), que se encuentra en la localidad de Villavante. Por su parte, la depuración de las aguas residuales se lleva a cabo en la estación de depuración de aguas residuales (EDAR), situada junto al río Bernesga, en la localidad de Trobajo del Cerecedo. Esta estación sirve a la Mancomunidad Municipal para el Saneamiento Integral de León y su Alfoz (SALEAL) (integrada por los municipios de León, San Andrés del Rabanedo, Villaquilambre, Santovenia de la Valdoncina y Sariegos), la cual es titular del Servicio Público de Tratamiento y Depuración de Aguas Residuales.


Urbaser es la empresa responsable de la gestión de los residuos sólidos urbanos y la limpieza de las vías públicas de León. Entre otros servicios, la ciudad cuenta con recogida selectiva de residuos, dos puntos limpios fijos, un punto limpio móvil y servicio de recogida puerta a puerta.

León pertenece al Consorcio Provincial de Residuos (GERSUL), el cual gestiona los residuos urbanos generados en toda la provincia mediante su tratamiento en tres plantas de clasificación y un Centro de Tratamiento de Residuos (CTR), ubicado en San Román de la Vega, en el municipio de San Justo de la Vega.


La encargada del abastecimiento de la ciudad es la entidad Mercados Centrales de Abastecimiento de León (Mercaleón). Se creó el 29 de diciembre de 1989 como resultado de la colaboración del Ayuntamiento de León y la Empresa Nacional de Mercados Centrales de Abastecimiento S.A. (Mercasa) para la distribución al por mayor de productos perecederos en la ciudad de León y su área de influencia. Inició su actividad en abril de 1993, siendo una de las 23 unidades alimentarias de Mercasa, que a su vez depende de la Sociedad Estatal de Participaciones Industriales (SEPI) y del Ministerio de Medio Ambiente, Medio Rural y Marino.

Sus instalaciones, que cubren una superficie de 41.185 m², albergan a 32 empresas, de las cuales 17 son mayoristas (frutas, hortalizas y pescados) y el resto se dedica a tareas de distribución, logística o servicios a usuarios. Cuenta con un mercado de frutas y hortalizas, un mercado de pescados, un pabellón polivalente y servicios complementarios para facilitar el desarrollo de la actividad en el centro.

Su área de influencia no sólo se limita a León y su provincia, sino que se extiende incluso a otras provincias limítrofes como Lugo, Orense, Asturias, Zamora y Palencia, facilitado por su situación estratégica y por la mejora de las vías de comunicación en el noroeste peninsular.


El transporte urbano en León es gestionado por la empresa Alesa, filial del grupo ALSA. Presta servicio mediante una red de 14 líneas operadas con 52 autobuses, si bien las previsiones apuntan a la ampliación de las mismas con el objeto de ofrecer servicio al polígono industrial de Onzonilla y al aeropuerto.


La empresa FEVE mantiene en funcionamiento un servicio de Cercanías entre las localidades de León y Guardo, en la provincia de Palencia, aprovechando la línea de ferrocarril que discurre entre León y Bilbao. Este servicio atraviesa en su recorrido los municipios de León, Villaquilambre, Garrafe de Torío, Matallana de Torío, La Vecilla, Boñar, La Ercina Cistierna y Valderrueda.


Tras grandes cambios de un proyecto inicial que planteaba la creación de seis líneas de tranvía, el proyecto final planteaba dos en forma de Y. La primera de ellas discurriría entre el Área 17 y Puente Castro y la segunda, aprovechando la traza de Feve, discurriría entre la plaza de Santo Domingo y el límite municipal con Villaquilambre, con un ramal al complejo Hospitalario y la posibilidad de construcción de un segundo ramal al campus de Vegazana de la Universidad de León.

Presupuestado en 150 millones de euros, el tranvía leonés en su máximo desarrollo tendría un longitud de 9 kilómetros, sirviendo a una población de 130.000 personas a menos de 500 metros de cada parada, con una frecuencia de paso de 8 minutos en hora punta, y un uso en torno a los 9 millones de usuarios anuales. Sin embargo, el Partido Popular anunció que bajo su gobierno no desarrollaría tal proyecto por considerarlo innecesario.

La ciudad de León cuenta con una red de carril-bici en la que tradicionalmente los mayores itinerarios se reducían a las riberas del río Bernesga y del río Torío como elementos de esparcimiento, nunca de transporte de masas. Sin embargo, en los últimos años se ha mejorado la red con la construcción de nuevos itinerarios aprovechando los tramos inconexos anteriores.

Así, en 2007 se inició la construcción de un carril bici de 2,5 kilómetros, hoy inaugurado, paralelo a la ronda este y que recorre la periferia del campus universitario, viéndose prolongado poco después en 900 metros en el PAU de la Universidad. Se han construido otros itinerarios que conectan la universidad con distintos barrios de la ciudad y aprovechando la reforma de Fernández Ladreda un tramo de 800 metros. En construcción o proyectados se encuentran los itinerarios de conexión entre los paseos del Bernesga y el Torío en primer lugar y conexión de Eras de Renueva con el casco antiguo y San Andrés del Rabanedo en segundo.

Con el fin de expandir el uso de la bici por la ciudad. De este modo, se potenciará el préstamo de bicicletas, aumentando el número de puestos municipales destinados a tal fin de 4 a 16 y la creación de un 1.500 puestos de aparca-bicis, que han comenzado a instalarse en el campus universitario y que se expandirán a lo largo de 2009 por toda la ciudad, incidiendo con especial interés en los principales focos atractores de viajeros de la ciudad.


La ciudad de León es cruce de comunicaciones del noroeste de España, siendo lugar de paso hacia Asturias desde la meseta y hacia Galicia desde el noreste de España.Dentro de la red principal de comunicaciones, una nutrida red de autovías, autopistas y carreteras tiene origen en León o simplemente pasan por la ciudad. Cuenta con las siguientes vías de gran capacidad:


La estación de autobuses de León se encuentra en la Avenida Ingeniero Sáenz de Miera y enlaza la ciudad no sólo con diferentes puntos de la provincia y de la Comunidad, sino también con destinos nacionales e internacionales.

Entre las distintas compañías, el Grupo ALSA es uno de los que más servicios ofrece, enlazando León con múltiples destinos nacionales como por ejemplo La Coruña, Alicante, Barcelona, Bilbao, Gijón, Madrid, Valladolid, Málaga o Sevilla.


La ciudad de León es un centro de primer orden en el transporte ferroviario, con vías que en su mayor parte son una herencia del pasado minero de la provincia. Así la ciudad cuenta con dos estaciones de ferrocarril, la estación de León (ubicada en el barrio del Crucero), gestionada por Adif y sustituida de manera provisional por una nueva hasta la llegada de la alta velocidad, que mantiene líneas con Vigo, La Coruña, Madrid, Gijón, Barcelona o Alicante, y la estación de Matallana, ubicada en el centro de la ciudad (Avenida Padre Isla), gestionada por Feve y punto de partida del Transcantábrico y de regionales a Bilbao.

Al igual que otras ciudades españolas, León espera la llegada del Alta Velocidad, que según las previsiones del gobierno central se preveía para finales del año 2012.


El aeropuerto de León, que entró en servicio en 1999, es el único aeropuerto ubicado en la provincia y el más cercano al municipio, encontrándose entre Valverde de la Virgen y San Andrés del Rabanedo. Asimismo, las otras opciones más cercanas para el transporte aéreo son los aeropuertos de Valladolid y Asturias. En octubre de 2010 se inauguraron las obras de ampliación, centradas en la construcción de una nueva terminal y en la duplicación de la superficie de la plataforma.


En la ciudad pueden adquirirse los periódicos nacionales, regionales e internacionales de mayor difusión, algunos de los cuales incorporan una sección de información local o regional.

En cuanto a los periódicos locales, se editan, "Diario de León" y "La Nueva Crónica", el de mayor difusión es el "Diario de León", que en 2009 tenía una difusión media de 14.102 ejemplares, mientras que "La Crónica de León", tenía una difusión media de 7.058 ejemplares según la información que aporta la OJD. De manera gratuita se reparte el semanario Gente León, que se edita todos los viernes.


En la ciudad se pueden sintonizar todas las cadenas principales de radio que operan a nivel estatal y regional y en la ciudad disponen de emisoras locales que emiten espacios dedicados a la actualidad local en sus desconexiones en diferentes tramos horarios: Radio Nacional de España, Cadena SER, Onda Cero, COPE y Castilla y León esRadio. En FM se pueden sintonizar las emisoras eminentemente musicales y otras específicas dedicadas a la información deportiva, local, económica o religiosa.


Con la entrada en funcionamiento de la Televisión Digital Terrestre (TDT) se ha multiplicado el número de canales de televisión, tanto generalistas como temáticos y tanto gratis como plataformas de pago a los que pueden acceder los leoneses. A nivel autonómico funcionan en 2011 con desconexiones locales las emisoras CYLTV y La 8.


El uso creciente de dispositivos tecnológicos, desde los cuales se puede acceder a Internet, las zonas wifi libre que se van creando en la ciudad y la posibilidad que ofrece Internet de acceder a todo tipo de medios tanto prensa, radio y televisión han revolucionado el modo que tienen hoy día las personas de acceder a la información general y especializada. A nivel local cabe señalar la página web del Ayuntamiento donde se ofrece a los ciudadanos la información institucional más significativa que afecta a los leoneses, así como las versiones digitales de los periódicos locales.

El Ayuntamiento de León ha impulsado el conocimiento y uso de la lengua leonesa en la ciudad de León, tanto en enseñanza para adultos como con la creación de la asignatura "Llingua y Cultura Llïonesa" que se ofrece de manera optativa y extraescolar en los centros escolares.

En el curso 2008-2009 comenzó a impartirse la asignatura en 16 centros públicos y concertados de la ciudad de León, para niños de quinto y sexto curso de Educación primaria, con ochenta niños matriculados. El ayuntamiento, en colaboración con la Universidad de León, también ofrece cursos para adultos, habiéndose superado los cien matriculados.Los cursos de adultos se estructuran en seis niveles, llegándose en 2009 al quinto nivel.

El Ayuntamiento de León también realiza campañas de promoción del leonés y en leonés, ofreciendo algunas de sus concejalías información en leonés y castellano en los formularios públicos, y publicando las noticias en su página web en ambos idiomas.

El Museo de Arte Contemporáneo de Castilla y León fue inaugurado por los entonces Príncipes de Asturias, Felipe de Borbón y Letizia Ortiz, el 1 de abril de 2005, con un firme propósito: ser un Museo de Presente y convertirse en pieza fundamental en el desarrollo del Arte Contemporáneo, a nivel internacional. Este museo nace con un amplio sentido experimental a la hora de concebir y desarrollar proyectos y exposiciones a todos los niveles. El MUSAC se encuentra trabajando exclusivamente en el área temporal del presente, marcado por la memoria más cercana: el museo se inicia con la idea de desarrollar un nuevo comportamiento a la hora de abordar el arte del siglo XXI.

El Museo, se ha convertido en uno de los referentes internacionales en Arte Contemporáneo, superando su número de visitantes los 500.000, cifra muy superior a la de la ciudad de León, de los cuales, 51% son locales, un 28% del ámbito nacional, un 9% de Castilla y León, un 8% de la provincia y el 4% restante del extranjero.

Está localizado en Eras de Renueva, junto al edificio del EREN (Ente Regional de la Energía de Castilla y León) y es un edificio de nueva planta, obra del estudio madrileño Mansilla + Tuñón Arquitectos.

El MUSAC se une en la provincia al Museo de la Siderurgia y la Minería de Castilla y León para formar la Red de Museos Regionales de Castilla y León, en la que también se integran el Museo Etnográfico de Castilla y León, situado en Zamora, y el Museo de la Evolución Humana situado en Burgos.


El Museo de León es el más antiguo de la provincia y está dedicado a narrar su historia a través de la Arqueología, el Arte y la Etnografía. Inaugurado en 1869, aunque fundado a partir de la actividad de la Comisión Provincial de Monumentos de León en el contexto de la Desamortización decimonónica, desde 2007 se encuentra ubicado en el conocido como Edificio Pallarés, en el centro de la Ciudad. Asimismo, cuenta con dos anexos: la Villa romana de Navatejera, en el vecino municipio de Villaquilambre, y el antiguo convento de San Marcos, en la misma capital, que es asimismo la "sede histórica" del Museo.

La exposición permanente del Museo ofrece un itinerario por la historia del territorio provincial a través de algunas de sus realizaciones culturales más significativas y cualificadas. Está articulada en siete áreas de conocimiento en las que el desarrollo cronológico permite ofrecer otras reflexiones paralelas y recorridos alternativos. De este modo, el visitante puede recorrer la historia de León desde la Prehistoria hasta el mundo contemporáneo, pasando por la romanización, el final del mundo antiguo, la Edad Media y la Edad Moderna. Hay además otra sala que ofrece una panorámica sobre la ciudad de León, que incluye uno de los miradores más completos que existen sobre su perfil urbano histórico.


El Museo de la Real Colegiata de San Isidoro se destaca por el Panteón de los Reyes, el cual es denominado Capilla Sixtina del Románico por sus elaborados frescos. Otras piezas relevantes son la arqueta de San Isidoro, el cáliz de doña Urraca, del siglo XI, la Arqueta de los Marfiles y el Portapaz del Pantocrator, del mismo siglo, y la Arqueta de Limoges, entre otros.

El Museo Fundación Vela Zanetti, recoge una muestra muy significativa de la obra de este autor burgalés, aunque leonés de adopción.


Fue inaugurado el año 1981 y es el resultado de la fusión del antiguo museo catedralicio con el diocesano. Este último había sido creado por el obispo Almarcha el año 1945, aunque el mayor incremento de sus fondos se realizó a partir de la década de 1960.

En la actualidad constituye un conjunto único en su género, albergando piezas de todas las etapas de la historia del arte, desde la prehistoria hasta el siglo XX, todas ellas repartidas en diecisiete salas, en el entorno del claustro catedralicio. Se accede a él por una hermosa puerta de nogal, que según el profesor Merino Rubio, había sido hecha para la librería por Juan de Quirós, antes del año 1513; en su tímpano se narra la escena de la Anunciación, plenamente flamenca, sobre un espacio con arquerías góticas.

En la primera estancia se nos muestra la escalera plateresca de Juan de Badajoz el Mozo, que facilitaba la subida a la sala capitular. El soporte de sus tres cuerpos está profusamente decorado con labores menudas de bueráneos, "candelieri", medallones y otros temas del mejor Renacimiento. Se buscó como pretexto para colocar el escudo del obispo mecenas, Pedro Manuel, la pequeña tribuna que resalta sobre la balaustrada.


El Museo Sierra-Pambley muestra el retrato de la vida doméstica de una familia ilustrada del siglo XIX y el recorrido por la labor pedagógica de la Fundación Sierra-Pambley, fundada en 1885 por Francisco Fernández-Blanco y Segundo Sierra-Pambley en una reunión con los más notables miembros de la Institución Libre de Enseñanza en su casa de Villablino. El museo se encuentra dividido en dos partes claramente diferenciadas:




Ubicado en la Basílica de San Isidoro, cuenta con una colección de arqueología próximo-oriental que consta de cerca de mil piezas y una biblioteca de más de 10.000 volúmenes. Fue inaugurado por la reina Doña Sofía el día 11 de marzo de 2009, (la Reina, en la inauguración, pudo abrir una carta sumeria de 3.000 años de antigüedad que se había mantenido inédita) abriendo sus puertas al público el día 19 del mismo mes.


El Centro Leonés del Arte se encuentra en la Avenida de Independencia, nº 18. El complejo de edificios fue realizado por uno de los arquitectos más destacados de la primera mitad del siglo XX en León, Juan Crisóstomo Torbado (Galleguillos del Campo 1867- 1947). Es una obra de tipo neohistoricista que se construyó en 1927 para albergar el Instituto Provincial de Higiene, hasta que en el año 2006 se rehabilitó para albergar el Legado Caneja y convertirse en un centro expositivo y cultural de la Diputación de León.

La inauguración oficial del centro fue el 2 de febrero de 2007, con la presentación de dos muestras: “Legado Caneja” y “El paisaje en el coleccionismo leonés”.

La Casona de la Fundación Carriegos

La Casona es la sede de las actividades culturales de la Fundación Carriegos, ubicada en la avenida Suero de Quiñones. La casa-museo fue la vivienda particular del industrial Miguel Pérez Vázquez, importante ebanista leonés. Proyectada en 1925 por el arquitecto Manuel de Cárdenas, fue decorada en estilo modernista y art decó y amueblada según diseño y manufactura propia. Las dos plantas históricas de la casa conservan completo su ajuar original. Hoy el inmueble es un centro cultural que acoge el Aula Literaria "El fulgor de la memoria" dedicada a Victoriano Crémer que repasa la figura del poeta y periodista a través de una exposición permanente y una sala de exposiciones temporales que muestra el quehacer artístico más actual.

• "Casa-museo": ejemplo de vivienda burguesa anterior a la guerra civil española de 1936 cuyo estilo combina el historicismo neorrenacentista con otras estancias de estilo modernista y Art Decó (Galería, Vestíbulo, Sala de Música).

• "Exposición permanente": un conjunto de obras de Vela Zanetti y la Sala de Pintura con las obras ganadoras del Premio Carriegos de Pintura y Artes Visuales.

• "Aula Literaria El fulgor de la memoria de Victoriano Crémer": exposición sobre la vida y obra del poeta a través de su fondo documental, artículos y correspondencia, biblioteca y colección de arte.

• "Sala de exposiciones temporales": dedicada a la muestra del arte más actual.


El Auditorio Ciudad de León está situado en el barrio Eras de Renueva, junto al histórico convento de San Marcos y la Delegación del gobierno autonómico. El edificio es obra de Emilio Tuñón Álvarez y Luis Moreno Mansilla (Mansilla + Tuñón Arquitectos) y tiene una superficie construida de 9.000 metros cuadrados. Cuenta con tres salas, siendo la mayor para 1.128 personas, y las otras dos, más pequeñas, de 388 y 100 personas. Además, el auditorio cuenta con dos salas de exposiciones, retroproyectores, equipo de proyección y posibilidad de incorporar equipo multiconferencia.

El edificio supone un hito en la arquitectura de la ciudad, por ser de los primeros de arquitectura moderna. Su uso está en las artes escénicas y representaciones, aunque también acoge congresos de diverso tipo.


El Teatro Emperador, abierto al público en 1951 y obra del arquitecto Manuel de Cárdenas, fue uno de los teatros más destacados y bellos de León durante décadas. A finales del siglo XX, sin embargo, su uso para representaciones teatrales había quedado muy disminuido y su principal función era la de sala de cine. Finalmente, la empresa propietaria del inmueble clausuró el edificio en 2006 con una fuerte oposición de la ciudadanía leonesa. Tras un acuerdo inicial de compra por parte del Ayuntamiento de la ciudad, el edificio pasó a manos municipales, que tras las elecciones de 2007 transfirió el inmueble al Ministerio de Cultura, que proyectaba usarlo como sede del Centro Nacional de las Artes Escénicas y de las Músicas Históricas de España, organismo adscrito al INAEM. El proyecto nunca se puso en marcha y el edificio fue puesto a subasta en 2014.


El Teatro Trianón, situado en la avenida Ramón y Cajal es uno de los dos teatros que se conservan en la ciudad y el único con declaración de Bien de Interés Cultural. Esta medida de protección no ha evitado su deterioro hasta la situación actual. Además de su uso como escenario teatral, el pequeño edificio fue en tiempos sala de fiestas y de cine, e incluso, en su último uso, parque infantil. La maestría en el aprovechamiento del espacio a través de su estructura en chaflán o su decoración interior son algunos de sus aspectos más destacados.


La Plaza de Toros del Parque, actualmente también conocida como León Arena, fue construida en 1948 en el lugar de una plaza anterior de madera, edificada en 1912. Tiene dos pisos, 50 metros de diámetro, y un aforo de 11.300 localidades. Desde 2003, tras su conversión en plaza cubierta (cuando adquiere el nuevo nombre), acoge no sólo espectáculos taurinos durante las fiestas patronales, sino también conciertos nacionales e internacionales, eventos deportivos, ferias y congresos, exposiciones y grandes espectáculos.

Debido a su importancia como núcleo histórico y monumental, la ciudad de León forma parte de redes turísticas o culturales como la Red de Ciudades Catedralicias o la Red de Juderías de España.

Asimismo, por León pasa el Camino de Santiago, en concreto el Camino Francés, siendo el final de una de sus etapas. La ciudad cuenta con dos albergues, ambos abiertos todo el año, uno municipal y otro el de las Carbajalas (M.M. Benedictinas). La Asociación de Amigos del Camino de Santiago en León "Pulchra Leonina" se encarga de informar a peregrinos, defender y conservar el patrimonio cultural relacionado con el Camino, así como promocionar todo tipo de actividades culturales.

La Semana Santa en León es una fiesta declarada de Interés Turístico Internacional, señalada en el calendario festivo leonés como la más importante del año.

Durante los diez días que transcurren desde el Viernes de Dolores al Domingo de Pascua, un total de 16 cofradías y hermandades, integradas por decenas de miles de "papones" (término único y de gran personalidad que en León reciben los hermanos cofrades) a las que se unen la Junta Mayor de la Semana Santa de León y la iglesia parroquial de Nuestra Señora del Mercado y del Camino "La Antigua", recorren las calles de una ciudad atestada de gente como en ningún otro momento del año.

Entre sus acontecimientos más significativos está La Ronda, la cual es un acto singular y único, reflejo de la gran tradición que envuelve la Semana Santa leonesa. Parte a las 24.00 de la Plaza de San Marcelo, en pleno centro de la ciudad, donde lleva a cabo ante el antiguo Ayuntamiento el primero de sus "toques" oficiales, con el que llama al pueblo de León a la Procesión de los Pasos, auténtica recreación del Calvario, la cual arranca a las 7:30 y no acaba hasta las 16:00 horas.

A lo largo del año son numerosos los eventos culturales y festivos que tienen lugar en León. Cronológicamente, en el mes de enero tiene lugar el CiLe (Festival de Cine Digital de León). En febrero se celebran los carnavales, en la que tienen lugar multitud de actividades como la Gala de Elección de la Reina de Carnaval, el Festival Infantil, el Desfile del Martes de Carnaval o el Entierro de la Sardina.

En el mes de marzo tiene lugar el FIMA (Festival Internacional de Música Avanzada), cuya última edición se celebró en 2007. También en este año se celebró la Feria Leer León (Feria Internacional del Libro Infantil y Juvenil). Durante la Semana Santa se celebra el Entierro de Genarín, fiesta conmemorativa en honor de Genarín, pellejero muy conocido en León, atropellado por el primer camión de basura de la ciudad mientras hacía sus necesidades en la base del tercer cubo de la muralla, lugar donde se celebra todos los años el homenaje. A finales de abril se celebran Las Cabezadas, en las cuales la ciudad, representada por la Corporación Municipal, ofrece un cirio y dos hachas de cera en la Basílica de San Isidoro, enfrentándose dialécticamente con el Cabildo.

En junio (en 2001, 2002, 2009 y 2010, mientras que en 2011 tuvo lugar en octubre, durante las fiestas de San Froilán) se celebra Festival Celta Internacional Reino de León, en el que participan diversos grupos musicales representativos de la música celta. A finales del mismo mes tienen lugar las fiestas de San Juan y San Pedro, fiestas patronales de la capital leonesa; se trata de la fiesta grande de la ciudad y referente en el resto de la provincia. Parte de las mismas son la Calle Ancha Muestra de Teatro de Calle y el Festival Flamenco de León. También en época estival tiene lugar el Festival de Música Española.

En otoño se celebran dos acontecimientos musicales, el Festival Internacional de Órgano Catedral de León y el Campeonato de Bandas de Gaitas del País LLionés. El domingo previo al día de San Froilán (5 de octubre) tiene lugar una de las fiestas más tradicionales de cuantas tiene la ciudad de León, pues viene celebrándose desde la Edad Media. Son fechas en las que se puede asistir a la romería de la Virgen del Camino, disfrutar de los Carros Engalanados, contemplar una de las mayores concentraciones de Pendones que se da en la provincia y asistir a la lucha dialéctica que provoca el Tributo de las 100 Doncellas así como el Foro u Oferta de Las Cantaderas.

Por último, en el mes de diciembre, tiene lugar el Purple Weekend y el Festival Internacional Tiempo de Magia, durante las fiestas de Navidad.

La gastronomía de la ciudad es una composición de los diferentes platos típicos de la gastronomía provincial, adaptada al frío clima provincial mediante platos energéticamente ricos que permitían afrontar las tareas cotidianas durante los fríos inviernos leoneses.

El embutido es pieza clave en este aspecto, por lo que en la ciudad de León se pueden saborear productos como la cecina de León, la morcilla de León, el chorizo de León y el Botillo del Bierzo, entre otros. Platos de mayor consistencia como el cocido Maragato, la sopa de trucha, la trucha frita y fría y el lechazo asado también son muy relevantes en la gastronomía de la ciudad, compendio de la presente en el resto de la provincia. Todo ello sin olvidar los platos de legumbres y hortalizas provenientes de las huertas leonesas, tales como las alubias de La Bañeza, pimientos de El Bierzo y de Fresno de la Vega y puerros de Sahagún.

En la bebida, destacan los vinos, avalados por dos denominaciones de origen, Bierzo y Tierras de León. Acompañando a estos, la limonada es un producto muy típico que se bebe en Semana Santa, en la tradición de "matar judíos".

Sin embargo, las tapas son sin lugar a duda el mayor exponente de la gastronomía de la ciudad. Las tapas pueden ser de todo tipo, desde guisos y platos calientes pasando por fritos, arroces hasta la más ligera de platos fríos y sencillos. Una peculiaridad de la tapa leonesa es que se sirve gratuitamente junto a la bebida en cualquier bar de la ciudad, aunque sin duda el lugar donde el tapeo alcanza su máximo esplendor es en el Barrio Húmedo, donde la concentración de bares y el esmero de los propietarios de los mismos a la hora de preparar las tapas ha propiciado el ambiente idóneo para el tapeo.

En cuanto a la repostería, bebiendo en este caso también de la provincia, sobresalen las Mantecadas de Astorga, los Lazos de San Guillermo de Cistierna, los Imperiales de La Bañeza, los Nicanores de Boñar y las Rosquillas de San Froilán. Destaca también el arroz con leche y la leche frita.

El deporte en la ciudad de León es regulado por la Concejalía de Deportes, mostrando su apoyo tanto a los equipos profesionales como a los que empiezan. Así, el Ayuntamiento ofrece una serie de Escuelas Deportivas para formar a todos aquellos niños que quieran iniciarse en algunos de los deportes ofertados.


Además de los deportes que se practican en las instalaciones municipales y de los equipos escolares, en la ciudad destacan una serie de entidades, aunque la que más logros ha obtenido en su historia es el Club Balonmano Ademar León, ganador de una Liga ASOBAL, dos copas ASOBAL, una Copa del Rey y dos Recopas de Europa. El otro club de balonmano de la ciudad es el Club León Balonmano (Cleba), miliante de la Liga femenina.

En fútbol, la ciudad cuenta con la Cultural y Deportiva Leonesa que juega en segunda división, cuyo máximo logro ha sido la estancia en Primera División en la temporada 1955/56, el C. F. Atlético Pinilla, el Club Deportivo Ejido de León, equipo de Regional y que jugó varias temporadas en Tercera División de España, CD Bosco y el Club Ruta Leonesa Fútbol Sala (O.E. Ram León), militante de la División de Plata de la LNFS.

En cuanto a baloncesto, León está representada, en categoría masculina, por el Baloncesto León S.A.D., durante muchos años equipo de la Liga ACB, llegando incluso a jugar la Copa Korać en la temporada 1996/97, En categoría femenina la ciudad contó con el Club Baloncesto San José, cuyo máximo logro fue ser subcampeón de la Copa de la Reina en 2008, pero debido a una difícil situación derivada de problemas de diversa índole, el club desapareció como tal en julio de 2009, cerrándose así la trayectoria del club de baloncesto femenino más laureado de la capital leonesa.

Otras disciplinas tienen representación en la ciudad en clubes como Rugby León (Pasgon Play), Sprint Atletismo León, Club Voleibol León, C. D. León Curling, Club Natación León, Club Ajedrez Ciudad de León, Equipo Ciclista Diputación de León (Diputación de León/Deyser), el Grupo de Montaña Yordas y el Club Ritmo de gimnasia rítmica.

León cuenta con numerosos centros deportivos dependientes de la Concejalía de Deportes, en los cuales se pueden practicar multitud de actividades. Entre ellos dos polideportivos (La Palomera y Sáenz de Miera), cuatro pabellones (La Torre, San Esteban, Margarita Ramos y Gumersindo Azcárate), los estadios Reino de León e Hispánico, el Palacio de Deportes, el Campo Hípico "El Parque" y el Área Deportiva de Puente Castro. En agosto de 2010 se inauguró el Centro Especializado de Alto Rendimiento, perteneciente al Consejo Superior de Deportes y situado junto al campus de la Universidad de León. Dedicado casi exclusivamente al atletismo, está especializado en los lanzamientos y consta de dos zonas de lanzamiento, una exterior y otra interior, y un edificio central de oficinas, vestuarios, etc. También es el lugar de entrenamiento del Club Ritmo de gimnasia rítmica.


El estadio municipal Reino de León comenzó su construcción en 1999 y fue inaugurado el 20 de mayo de 2001, con el nombre de Nuevo Antonio Amilivia, con un partido entre el equipo de fútbol de la ciudad, la Cultural, contra el Xerez, que se saldó con una victoria a favor de los locales.

El estadio, ubicado junto a la avenida Sáenz de Miera, paralela al río Bernesga, tiene un aforo de 13.451 espectadores, con las dimensiones exigidas por la FIFA para albergar partidos de carácter internacional, 105 x 68 metros. El estadio, ligado al club de fútbol de la ciudad, se encuentra en manos del ayuntamiento y ha vivido, aparte de las celebraciones propias del equipo, otros eventos deportivos de cierta entidad como un partido clasificatorio para la Eurocopa 2004 entre España y Armenia.

En septiembre de 2008 el nombre Nuevo Antonio Amilivia fue sustituido por el de Reino de León por decisión del ayuntamiento de la ciudad.


El Palacio de los Deportes de León es un recinto deportivo inaugurado en 1970. Ubicado en la Avenida Ingeniero Sáenz de Miera, junto al río Bernesga, tiene un aforo de 6.500 espectadores. En él disputan sus partidos diversos equipos de la ciudad, entre los que destacan el Baloncesto León, el Club Balonmano Ademar y el Ruta Leonesa Fútbol Sala.
El área deportiva de Puente Castro es una instalación deportiva municipal del Ayuntamiento de León construida en 1998. Con un aforo de 4.600 espectadores, es lugar de juego del equipo filial de la Cultural y Deportiva Leonesa. Las instalaciones se completan con un campo de hockey, otro de rugby y otro de entrenamiento.


Entre los acontecimientos deportivos que tienen lugar en León a lo largo del año, destaca el Magistral de Ajedrez Ciudad de León, en el cual participan algunos de los mejores jugadores del mundo de ajedrez, y que se celebra desde 1988, siendo en 2009 su XXII edición. En 2009 se celebró la I Media Maratón "Ciudad de León", en la que participaron 1200 atletas.

Por otra parte, la ciudad ha acogido distintos eventos deportivos de carácter nacional e internacional. En los últimos 25 años, desde 1984, León ha sido en 14 ocasiones meta o salida de etapas de la Vuelta ciclista a España, siendo 27 el total de veces desde el inicio de la prueba. León fue sede de la Copa del Rey de Baloncesto en las temporadas 1969-70 (por entonces llamada Copa del Generalísimo) y 1996-97. Asimismo, en 2003, la Selección Española de Fútbol jugó un partido oficial clasificatorio para la Eurocopa 2004 contra Armenia. En 2001 la ciudad acogió el Campeonato de Europa de ajedrez, resultando vencedor Países Bajos, y en 2008 León fue sede del XXIX Campeonato del Mundo de Lucha de Brazos, en el cual la delegación leonesa fue la triunfadora con casi 20 medallas de oro, y dándose la peculiaridad de que la Federación Internacional reconocía al País Leonés como miembro de pleno derecho.


Entre los deportistas profesionales originarios de León destaca Manuel Martínez, lanzador de peso y campeón del mundo en Birmingham 2003, campeón de Europa en Viena 2002 y campeón en los Juegos Mediterráneos de Túnez 2001, así como campeón en la Universiada de Pekín 2001, campeón de Europa Sub-23 en Ostrava 1994 y campeón de Europa junior en San Sebastián 1993, además de campeón de España en 16 ocasiones entre 1993 y 2008.

También es de León la gimnasta rítmica Carolina Rodríguez, ganadora de la competición individual en los Juegos Mediterráneos de 2013 y olímpica tanto en conjuntos (Atenas 2004, donde obtuvo la 7ª plaza) como en individual (Londres 2012, donde acabó 14ª). Además, es la gimnasta individual que más veces ha sido campeona de España del concurso general contando todas las categorías, con 10 títulos (1 en alevín, 1 en infantil, 1 en primera categoría y 7 en categoría de honor).

La ciudad de León participa en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. A partir de esta iniciativa se han establecido lazos con las siguientes localidades:

A lo largo de la historia ha habido una serie de personajes, nacidos unos en León y otros vinculados con la ciudad, que han destacado en sus actividades profesionales. La siguiente lista no tiene carácter clasificatorio sino meramente ejemplar ya que habrá otras personas no mencionadas que merecerían ser también citadas.




</doc>
<doc id="3342" url="https://es.wikipedia.org/wiki?curid=3342" title="Excentricidad (matemática)">
Excentricidad (matemática)

En matemática y geometría la excentricidad, ε (épsilon) es un parámetro que determina el grado de desviación de una sección cónica con respecto a una circunferencia.

Este es un parámetro importante en la definición de elipse, hipérbola y parábola:

Para cualquier punto perteneciente a una sección cónica, la razón de su distancia a un punto fijo F (foco) y a una recta fija l (directriz) es siempre igual a una constante positiva llamada excentricidad ε.

La designación tradicional de la excentricidad es la letra griega ε llamada épsilon y es preferible no usar la letra e para designar la misma porque e se reserva para la base de los logaritmos naturales o neperianos. (véase número e).


Donde a es la longitud del semieje mayor en el caso de la elipse o semieje real en el caso de la hipérbola y b es la longitud del semieje menor en la elipse o semieje imaginario en la hipérbola.

Los cuerpos ligados gravitacionalmente entre sí describen órbitas en forma de elipse. La excentricidad de la órbita de un objeto se calcula de acuerdo con la fórmula anterior y expresa el grado de desviación con respecto a una órbita circular.

En el globo ocular, se llama excentricidad a la distancia desde cualquier punto de la retina a su centro. La resolución en la retina varía con la excentricidad ya que los conos se ubican principalmente en la zona de excentricidad 0°, que es el punto considerado como centro retiniano (llamado fóvea; zona de mayor poder resolutivo), y su densidad decrece con la excentricidad.


</doc>
<doc id="3343" url="https://es.wikipedia.org/wiki?curid=3343" title="Química inorgánica">
Química inorgánica

La química inorgánica se encarga del estudio integrado de la formación, composición, estructura y reacciones químicas de los elementos y compuestos inorgánicos (por ejemplo, ácido sulfúrico o carbonato cálcico); es decir, los que no poseen enlaces carbono-hidrógeno, porque éstos pertenecen al campo de la química orgánica. Dicha separación no es siempre clara, como por ejemplo en la química organometálica que es una superposición de ambas.

Antiguamente se definía como la química de la materia inorgánica, pero quedó obsoleta al desecharse la hipótesis de la fuerza vital, característica que se suponía propia de la materia viva que no podía ser creada y permitía la creación de las moléculas orgánicas. 

Se suelen clasificar los compuestos inorgánicos según su función en ácidos, bases, óxidos y sales, y los óxidos se les suele dividir en óxidos metálicos (óxidos básicos o anhídridos básicos) y óxidos no metálicos (óxidos ácidos o anhídridos ácidos).

El nombre tiene su origen en la época en la que todos los compuestos del carbono se obtenían de seres vivos; de ahí la química del carbono se denomina química orgánica. La química de compuestos sin carbono, fue, por ende, llamada química inorgánica.
Actualmente, se obtienen compuestos orgánicos en el laboratorio, de forma que la separación es artificial. Algunas de las sustancias con carbono que entran en el campo de la química inorgánica se incluye en uno de estos:


Los compuestos inorgánicos se dividen según su estructura en:

<references>



Apartados de interés de la química inorgánica incluyen:


Áreas de solapamiento con otros campos del conocimiento incluyen:


Hay muchos compuestos y sustancias inorgánicas de gran importancia, comercial y biológica. Entre ellos:




</doc>
<doc id="3346" url="https://es.wikipedia.org/wiki?curid=3346" title="Siglo XVIII">
Siglo XVIII

El siglo XVIII d. C. (siglo decimoctavo después de Cristo) o siglo XVIII e.c (siglo decimoctavo de la era común) comenzó el 1 de enero del año 1701 y terminó el 31 de diciembre de 1800. En la historia occidental, el siglo XVIII también es llamado el «Siglo de las Luces», debido al nacimiento del movimiento intelectual conocido como Ilustración. En ese marco, el siglo XVIII es fundamental para comprender el mundo moderno, pues muchos de los acontecimientos políticos, sociales, económicos, culturales e intelectuales de esos años han extendido su influencia hasta la actualidad.

De hecho, para la historia occidental es el último de los siglos de la Edad Moderna y el primero de la Edad Contemporánea, tomándose convencionalmente como momento de división entre ambas los años 1705 (máquina de vapor), 1751 ("L'Encyclopédie"), 1776 (Independencia de Estados Unidos) o, más comúnmente, el 1789 (Revolución francesa).

Tras el caos político y militar vivido en el siglo XVII, el siglo XVIII, no carente de conflictos, verá un notable desarrollo en las artes y las ciencias europeas de la mano de la Ilustración, un movimiento cultural caracterizado por la reafirmación del poder de la razón humana frente a la fe y la superstición. Las antiguas estructuras sociales, basadas en el feudalismo y el vasallaje, serán cuestionadas y acabarán por colapsar, al tiempo que, sobre todo en Inglaterra, se inicia la Revolución industrial y el despegue económico de Europa. Durante dicho siglo, la civilización europea occidental afianzará su predominio en el mundo y extenderá su influencia por todo el orbe.























</doc>
<doc id="3348" url="https://es.wikipedia.org/wiki?curid=3348" title="Catálogo Köchel">
Catálogo Köchel

El catálogo Köchel ("Köchel Verzeichnis", "Köchelverzeichnis" en alemán) fue creado por Ludwig von Köchel en 1862 y enumera las obras musicales compuestas por Wolfgang Amadeus Mozart (1756–1791). Cada una de las obras de Mozart está designada por un número precedido de la abreviatura "K." o "KV". El orden cronológico que Köchel pretendió dar al catálogo es realmente válido para la mayoría de las obras. Sin embargo, en la primera edición del catálogo aparecen algunas obras de otros autores atribuidas erróneamente a Mozart, y omite otras auténticas que aún no habían sido descubiertas.

El catálogo tuvo varias revisiones; en particular la 3.ª edición, de 1936, llevada a cabo por Alfred Einstein, reubicó una gran cantidad de obras en el lugar que se estimó correcto, con el expediente de agregar una letra al número original, para no alterar el número propio de Köchel.

En las décadas posteriores a la muerte de Mozart hubo varios intentos de catalogar sus composiciones, pero no se realizaron con éxito hasta que Ludwig von Köchel las listó en su catálogo en 1862. La página 551 del catálogo Köchel fue titulada «"Chronologisch-thematisches Verzeichnis sämtlicher Tonwerke Wolfgang Amadé Mozarts"» («Catálogo cronológico y temático completo de la obra musical de Wolfgang Amadé Mozart»). El catálogo incluyó los primeros pentagramas de cada obra a la manera de "íncipit".

Köchel intentó ordenar las obras por orden cronológico, pero las composiciones anteriores a 1784 sólo tienen dataciones estimadas. Desde que Köchel elaboró el listado se han encontrado numerosas piezas que se han atribuido a otros autores o se les ha asignado una fecha diferente, por lo que han sido necesarias tres revisiones posteriores. Estas revisiones, especialmente la tercera llevada a cabo por Alfred Einstein en 1937, y la sexta de Franz Giegling, Gerd Sievers y Alexander Weinmann en 1964, incorporan numerosas correcciones.

Aun así, los números dados por Köchel son una forma rápida de estimar cuándo compuso Mozart una obra en particular. Según la fórmula creada por Neal Zaslaw, para un número KV mayor que 100, se puede dividir por 25 y sumar 10, para estimar la edad de Mozart cuando compuso la obra; si se suma 1756 (el año de su nacimiento), se obtiene el año aproximado de la composición. Las letras fueron añadidas como nuevos números para mantener la numeración original del listado de Köchel mientras se reordenaba y revisaba la secuencia cronológica. Otros apéndices y suplementos al catálogo son marcados como "KV A(nhang)".




</doc>
<doc id="3349" url="https://es.wikipedia.org/wiki?curid=3349" title="Ludwig von Köchel">
Ludwig von Köchel

Ludwig Alois Friedrich Ritter von Köchel (Stein, Baja Austria,14 de enero de 1800 - Viena, Austria, 3 de junio de 1877) fue un escritor, compositor, botánico, editor y admirador de la obra musical de Wolfgang Amadeus Mozart, cuya obra catalogó a mediados del siglo XIX. 

Estudió Derecho en Viena y fue durante 15 años tutor de los cuatro hijos del archiduque Carlos de Austria-Teschen. Esta actividad le ganó el título de "Ritter" (caballero) y le permitió vivir de rentas el resto de sus días. Se dedicó en forma privada a la investigación; realizó campañas de recolección de especímenes botánicos en el norte de África, la península Ibérica, Gran Bretaña y Rusia, muy apreciadas por los científicos contemporáneos. Además se interesó por la geología y la mineralogía.

Su pasión por la música ("*1")—era miembro del Mozarteum de Salzburgo— lo llevó a sistematizar la lista de las composiciones de Mozart, que publicó en 1862 con el título de "Chronologisch-thematisches Verzeichnis sämtlicher Tonwerke Wolfgang Amadé Mozarts"
(«"Catálogo cronológico y temático de todas las obras musicales de Wolfgang Amadeus Mozart"»). Su apellido (y la inicial del mismo) quedaron desde entonces vinculados al célebre compositor; por ejemplo, el "Concierto para piano y orquesta n.º 23 en la mayor" de 1786 está identificado con el número "Köchel Verzeichnis" (abreviado KV o K.) 488. 

Von Köchel también financió en parte la primera edición completa de las obras de Mozart que publicó Breitkopf & Härtel desde 1877 hasta 1905. En ella se utilizó la clasificación en 24 categorías propuesta por Köchel.










</doc>
<doc id="3351" url="https://es.wikipedia.org/wiki?curid=3351" title="Resonancia">
Resonancia

El término resonancia se refiere a un conjunto de fenómenos relacionados con los movimientos periódicos o casi periódicos en que se produce reforzamiento de una oscilación al someter el sistema a oscilaciones de una frecuencia determinada. Más concretamente el término puede referirse a:



Otros contextos:

</doc>
<doc id="3352" url="https://es.wikipedia.org/wiki?curid=3352" title="Solubilidad">
Solubilidad

La solubilidad es la capacidad de una sustancia de disolverse en otra llamada solvente.Implícitamente se corresponde con la máxima cantidad de soluto que se puede disolver en una cantidad determinada de disolvente, a determinadas condiciones de temperatura, e incluso presión (en caso de un soluto gaseoso). Puede expresarse en unidades de concentración: molaridad, fracción molar, etc.

Si en una disolución no se puede disolver más soluto se dice que la disolución está saturada. Bajo ciertas condiciones la solubilidad puede sobrepasar ese máximo y pasa a denominarse solución sobresaturada.Por el contrario, si la disolución admite aún más soluto, se dice que se encuentra insaturada.

No todas las sustancias se disuelven en un mismo solvente. Por ejemplo, en el agua, se disuelve el alcohol y la sal, en tanto que el aceite y la gasolina no se disuelven. En la solubilidad, el carácter polar o apolar de la sustancia influye mucho, ya que, debido a este carácter, la sustancia será más o menos soluble; por ejemplo, los compuestos con más de un grupo funcional presentan gran polaridad por lo que no son solubles en éter etílico.

Entonces para que un compuesto sea soluble en éter etílico ha de tener escasa polaridad; es decir, tal compuesto no ha de tener más de un grupo polar. Los compuestos con menor solubilidad son los que presentan menor reactividad, como son: las parafinas, compuestos aromáticos y los derivados halogenados.

El término solubilidad se utiliza tanto para designar al fenómeno cualitativo del proceso de disolución como para expresar cuantitativamente la concentración de las soluciones. La solubilidad de una sustancia depende de la naturaleza del disolvente y del soluto, así como de la temperatura y la presión del sistema, es decir, de la tendencia del sistema a alcanzar el valor máximo de entropía. Al proceso de interacción entre las moléculas del disolvente y las partículas del soluto para formar agregados se le llama solvatación y si el solvente es agua, hidratación.

La solubilidad se define para fases específicas. Por ejemplo, la solubilidad de aragonito y calcita en el agua se espera que difieran, si bien ambos son polimorfos de carbonato de calcio y tienen la misma fórmula molecular.

La solubilidad de una sustancia en otra está determinada por el equilibrio de fuerzas intermoleculares entre el disolvente y el soluto, y la variación de entropía que acompaña a la solvatación. Factores como la temperatura y la presión influyen en este equilibrio, cambiando así la solubilidad.

La solubilidad también depende en gran medida de la presencia de otras sustancias disueltas en el disolvente como por ejemplo la existencia de complejos metálicos en los líquidos. La solubilidad dependerá también del exceso o defecto de algún ion común, con el soluto, en la solución; tal fenómeno es conocido como el efecto del ion común. En menor medida, la solubilidad dependerá de la fuerza iónica de las soluciones. Los dos últimos efectos mencionados pueden cuantificarse utilizando la ecuación de equilibrio de solubilidad.

Para un sólido que se disuelve en una reacción redox, la solubilidad se espera que dependa de las posibilidades (dentro del alcance de los potenciales en las que el sólido se mantiene la fase termodinámicamente estable). Por ejemplo, la solubilidad del oro en el agua a alta temperatura se observa que es casi de un orden de magnitud más alta cuando el potencial redox se controla mediante un tampón altamente oxidante redox FeO-FeO que con un tampón moderadamente oxidante Ni-NiO.

La solubilidad (metaestable) también depende del tamaño físico del grano de cristal o más estrictamente hablando, de la superficie específica (o molar) del soluto. Para evaluar la cuantificación, se debe ver la ecuación en el artículo sobre el equilibrio de solubilidad. Para cristales altamente defectuosos en su estructura, la solubilidad puede aumentar con el aumento del grado de desorden. Ambos efectos se producen debido a la dependencia de la solubilidad constante frente a la denominada energía libre de Gibbs asociada con el cristal. Los dos últimos efectos, aunque a menudo difíciles de medir, son de relevante importancia en la práctica pues proporcionan la fuerza motriz para determinar su grado de precipitación, ya que el tamaño de cristal crece de forma espontánea con el tiempo.

La solubilidad de un soluto en un determinado disolvente principalmente depende de la temperatura. Para muchos sólidos disueltos en el agua líquida, la solubilidad aumenta con la temperatura hasta 100 °C, aunque existen casos que presentan un comportamiento inverso. En el agua líquida a altas temperaturas la solubilidad de los solutos iónicos tiende a disminuir debido al cambio de las propiedades y la estructura del agua líquida, el reducir los resultados de la constante dieléctrica de un disolvente menos polar.

Los solutos gaseosos muestran un comportamiento más complejo con la temperatura. Al elevarse la temperatura, los gases generalmente se vuelven menos solubles en agua (el mínimo que está por debajo de 120 °C para la mayoría de gases), pero más solubles en disolventes orgánicos.

El gráfico muestra las curvas de solubilidad de algunas sales sólidas inorgánicas típicas. Muchas sales se comportan como el nitrato de bario y el arseniato ácido disódico, y muestran un gran aumento de la solubilidad con la temperatura. Algunos solutos (por ejemplo, cloruro de sodio (NaCl) en agua) exhiben una solubilidad bastante independiente de la temperatura. Unos pocos, como el sulfato de cerio (III) y el carbonato de litio, se vuelven menos solubles en agua a medida que aumenta la temperatura. Esta dependencia de la temperatura se refiere a veces como «retrógrada» o «solubilidad inversa». En ocasiones, se observa un patrón más complejo, como con sulfato de sodio, donde el cristal decahidrato menos soluble pierde agua de cristalización a 32 °C para formar una fase anhidra menos soluble. 

La solubilidad de los compuestos orgánicos casi siempre aumenta con la temperatura. La técnica de la recristalización, utilizado para la purificación de sólidos, depende de un soluto de diferentes solubilidades en un disolvente caliente y fría. Existen algunas excepciones, tales como determinadas ciclodextrinas.

La solubilidad de los gases varía no sólo con la temperatura sino además con la presión ejercida sobre el mismo. De esta manera, la cantidad de un soluto gaseoso que puede disolverse en un determinado solvente, aumenta al someterse a una presión parcial mayor (véase Ley de Henry). A nivel industrial, esto se puede observar en el envasado de bebidas gaseosas por ejemplo, donde se aumenta la solubilidad del dióxido de carbono ejerciendo una presión de alrededor de 4 atm. 



</doc>
<doc id="3353" url="https://es.wikipedia.org/wiki?curid=3353" title="Mozart (desambiguación)">
Mozart (desambiguación)

El apellido Mozart lo comparten varias personas pertenecientes a la :

Mozart también puede referirse a:

</doc>
<doc id="3354" url="https://es.wikipedia.org/wiki?curid=3354" title="Química">
Química

La química es la ciencia que estudia tanto la composición, como la estructura y las propiedades de la materia como los cambios que esta experimenta durante las reacciones químicas y su relación con la energía. Linus Pauling la define como la ciencia que estudia las sustancias, su estructura (tipos y formas de acomodo de los átomos), sus propiedades y las reacciones que las transforman en otras sustancias en referencia con el tiempo.

La química moderna se desarrolló a partir de la alquimia, una práctica protocientífica de carácter filosófico, que combinaba elementos de la química, la metalurgia, la física, la medicina, la biología, entre otras ciencias y artes. Esta fase termina al ocurrir la llamada Revolución de la química, basada en la ley de conservación de la materia y la teoría de la combustión por oxígeno postuladas por el científico francés Antoine Lavoisier.

Las disciplinas de la química se agrupan según la clase de materia bajo estudio o el tipo de estudio realizado. Entre éstas se encuentran la química inorgánica, que estudia la materia inorgánica; la química orgánica, que estudia la materia orgánica; la bioquímica, que estudia las sustancias existentes en organismos biológicos; la fisicoquímica que comprende los aspectos estructurales y energéticos de sistemas químicos a escalas macroscópica, molecular y atómica, y la química analítica, que analiza muestras de materia y trata de entender su composición y estructura mediante diversos estudios y reacciones.

La palabra "química" procede de la palabra «alquimia», el nombre de un antiguo conjunto de prácticas protocientíficas que abarcaba diversos elementos de la actual ciencia, además de otras disciplinas muy variadas como la metalurgia, la astronomía, la filosofía, el misticismo o la medicina. La alquimia, practicada al menos desde alrededor del año 330, además de buscar la fabricación de oro, estudiaba la composición de las aguas, la naturaleza del movimiento, del crecimiento, de la formación de los cuerpos y su descomposición, la conexión espiritual entre los cuerpos y los espíritus. Un alquimista solía ser llamado en lenguaje cotidiano «químico», y posteriormente (oficialmente, a partir de la publicación, en 1661, del libro "El químico escéptico", del químico irlandés Robert Boyle) se denominaría "química" al arte que practicaba.

A su vez, "alquimia" deriva de la palabra árabe "al-kīmīā" (الکیمیاء). En su origen, el término fue un préstamo tomado del griego, de las palabras χημία o χημεία ("khemia" y "khemeia", respectivamente). La primera podría tener origen egipcio. Muchos creen que "al-kīmīā" deriva de χημία, que a su vez deriva de la palabra "Chemi" o "Kimi" o "Kham", que es el nombre antiguo de Egipto en egipcio. Según esa hipótesis, "khemeia" podría ser "el arte egipcio". La otra alternativa es que "al-kīmīā" derivara de χημεία, que significa «fusionar». Una tercera hipótesis, con más adeptos en la actualidad, dice que "khemeia" deriva del griego "khumos", el jugo de una planta, y que vendría a significar "el arte de extraer jugos", y en este caso "jugo" podría ser un metal, y por tanto podría ser "el arte de la metalurgia"

La definición de "química" ha cambiado a través del tiempo, a medida que nuevos descubrimientos se han añadido a la funcionalidad de esta ciencia. El término "química", a vista del reconocido científico Robert Boyle, en 1661, se trataba del área que estudiaba los principios de los cuerpos mezclados.

En 1662, la química se definía como un arte científico por el cual se aprende a disolver cuerpos, obtener de ellos las diferentes sustancias de su composición y cómo unirlos después para alcanzar un nivel mayor de perfección. Esto según el químico Christopher Glaser.

La definición de 1730 para la palabra "química", usada por Georg Stahl, era el arte de entender el funcionamiento de las mezclas, compuestos o cuerpos hasta sus principios básicos, y luego volver a componer esos cuerpos a partir de esos mismos principios. 

En 1837, Jean-Baptiste Dumas consideró la palabra "química" para referirse a la ciencia que se preocupaba de las leyes y efectos de las fuerzas moleculares. Esta definición luego evolucionaría hasta que, en 1947, se le definió como la ciencia que se preocupaba de las sustancias: su estructura, sus propiedades y las reacciones que las transforman en otras substancias (caracterización dada por Linus Pauling).

Más recientemente, en 1988, la definición de química se amplió, para ser «el estudio de la materia y los cambios que implica», según palabras del profesor Raymond Chang.

La ubicuidad de la química en las ciencias naturales hace que sea considerada una de las ciencias básicas. La química es de gran importancia en muchos campos del conocimiento, como la ciencia de materiales, la biología, la farmacia, la medicina, la geología, la ingeniería y la astronomía, entre otros.

Los procesos naturales estudiados por la química involucran partículas fundamentales (electrones, protones y neutrones), partículas compuestas (núcleos atómicos, átomos y moléculas) o estructuras microscópicas como cristales y superficies. 

Desde el punto de vista microscópico, las partículas involucradas en una reacción química pueden considerarse un sistema cerrado que intercambia energía con su entorno. En procesos exotérmicos, el sistema libera energía a su entorno, mientras que un proceso endotérmico solamente puede ocurrir cuando el entorno aporta energía al sistema que reacciona. En la mayor parte de las reacciones químicas hay flujo de energía entre el sistema y su campo de influencia, por lo cual puede extenderse la definición de reacción química e involucrar la energía cinética (calor) como un reactivo o producto.

Aunque hay una gran variedad de ramas de la química, las principales divisiones son:

La gran importancia de los sistemas biológicos hace que en la actualidad gran parte del trabajo en química sea de naturaleza bioquímica. Entre los problemas más interesantes se encuentran, por ejemplo, el estudio del plegamiento de proteínas y la relación entre secuencia, estructura y función de proteínas.

Si hay una partícula importante y representativa en la química, es el electrón. Uno de los mayores logros de la química es haber llegado al entendimiento de la relación entre reactividad química y distribución electrónica de átomos, moléculas o sólidos. Los químicos han tomado los principios de la mecánica cuántica y sus soluciones fundamentales para sistemas de pocos electrones y han hecho aproximaciones matemáticas para sistemas más complejos. La idea de orbital atómico y molecular es una forma sistemática en la cual la formación de enlaces es comprensible y es la sofisticación de los modelos iniciales de puntos de Lewis. La naturaleza cuántica del electrón hace que la formación de enlaces sea entendible físicamente y no se recurra a creencias como las que los químicos utilizaron antes de la aparición de la mecánica cuántica. Aun así, se obtuvo gran entendimiento a partir de la idea de puntos de Lewis.

Las primeras civilizaciones, como los egipcios y los babilónicos, concentraron un conocimiento práctico en lo que concierne a las artes relacionadas con la metalurgia, cerámica y tintes, sin embargo, no desarrollaron teorías complejas sobre sus observaciones. 

Hipótesis básicas emergieron de la antigua Grecia con la teoría de los cuatro elementos propuesta por Aristóteles. Esta postulaba que el fuego, aire, tierra y agua, eran los elementos fundamentales por los cuales todo está formado como mezcla. Los atomicistas griegos datan del año 440 a. C., en manos de filósofos como Demócrito y Epicuro. En el año 50 antes de Cristo, el filósofo romano Lucrecio, expandió la teoría en su libro "De Rerum Natura" ("En la naturaleza de las cosas)". 

Al contrario del concepto moderno de atomicismo, esta teoría primitiva estaba enfocada más en la naturaleza filosófica de la naturaleza, con un interés menor por las observaciones empíricas y sin interés por los experimentos químicos.

En el mundo Helénico, la Alquimia en principio proliferó en combinación con la magia y el ocultismo como una forma de estudio de las substancias naturales para transmutarlas en oro y descubrir el elixir de la eterna juventud. La Alquimia fue descubierta y practicada ampliamente en el mundo árabe después de la conquista de los musulmanes, y desde ahí, fue difuminándose hacia todo el mundo medieval y la Europa Renacentista a través de las traducciones latinas.

Bajo la influencia de los nuevos métodos empíricos propuestos por "sir" Francis Bacon, Robert Boyle, Robert Hooke, John Mayow, entre otros, comenzaron a remodelarse las viejas tradiciones acientíficas en una disciplina científica. Boyle, en particular, es considerado como el padre fundador de la química debido a su trabajo más importante, «El Químico Escéptico» donde se hace la diferenciación entre las pretensiones subjetivas de la alquimia y los descubrimientos científicos empíricos de la nueva química. Él formuló la ley de Boyle, rechazó los «cuatro elementos» y propuso una alternativa mecánica de los átomos y las reacciones químicas las cuales podrían ser objeto de experimentación rigurosa, demostrándose o siendo rebatidas de manera científica.

La teoría del flogisto (una substancia que, suponían, producía toda combustión) fue propuesta por el alemán Georg Ernst Stahl en el siglo XVIII y solo fue rebatida hacia finales de siglo por el químico francés Antoine Lavoisier, quien dilucidó el principio de conservación de la masa y desarrolló un nuevo sistema de nomenclatura química utilizada para el día de hoy.

Antes del trabajo de Lavoisier, sin embargo, se han hecho muchos descubrimientos importantes, particularmente en lo que se refiere a lo relacionado con la naturaleza de "aire", que se descubrió, que se compone de muchos gases diferentes. El químico escocés Joseph Black (el primer químico experimental) y el holandés J. B. van Helmont descubrieron dióxido de carbono, o lo que Black llamaba "aire fijo" en 1754; Henry Cavendish descubre el hidrógeno y dilucida sus propiedades. Finalmente, Joseph Priestley e, independientemente, Carl Wilhelm Scheele aíslan oxígeno puro.

El científico inglés John Dalton propone en 1803 la teoría moderna de los átomos en su libro, "La teoría atómica", donde postula que todas las substancias están compuestas de "átomos" indivisibles de la materia y que los diferentes átomos tienen diferentes pesos atómicos.

El desarrollo de la teoría electroquímica de combinaciones químicas se produjo a principios del siglo XIX como el resultado del trabajo de dos científicos en particular, J. J. Berzelius y Humphry Davy, gracias a la invención, no hace mucho, de la pila voltaica por Alessandro Volta. Davy descubrió nueve elementos nuevos, incluyendo los metales alcalinos mediante la extracción de ellos a partir de sus óxidos con corriente eléctrica.

El británico William Prout propuso el ordenar a todos los elementos por su peso atómico, ya que todos los átomos tenían un peso que era un múltiplo exacto del peso atómico del hidrógeno. Newlands ideó una primitiva tabla de los elementos, que luego se convirtió en la tabla periódica moderna creada por el alemán Julius Lothar Meyer y el ruso Dmitri Mendeleev en 1860. Los gases inertes, más tarde llamados gases nobles, fueron descubiertos por William Ramsay en colaboración con lord Rayleigh al final del siglo, llenando por lo tanto la estructura básica de la tabla.

La química orgánica ha sido desarrollada por Justus von Liebig y otros luego de que Friedrich Wohler sintetizara urea, demostrando que los organismos vivos eran, en teoría, reducibles a terminología química Otros avances cruciales del siglo XIX fueron: la comprensión de los enlaces de valencia (Edward Frankland,1852) y la aplicación de la termodinámica a la química (J. W. Gibbs y Svante Arrhenius, 1870).

Llegado el siglo XX los fundamentos teóricos de la química fueron finalmente entendidos debido a una serie de descubrimientos que tuvieron éxito en comprobar la naturaleza de la estructura interna de los átomos. En 1897, J.J. Thomson, de la Universidad de Cambridge, descubrió el electrón y poco después el científico francés Becquerel, así como la pareja de Pierre y Marie Curie investigó el fenómeno de la radiactividad. En una serie de experimentos de dispersión, Ernest Rutherford, en la Universidad de Mánchester, descubrió la estructura interna del átomo y la existencia del protón, clasificando y explicando los diferentes tipos de radiactividad, y con éxito, transmuta el primer elemento mediante el bombardeo de nitrógeno con partículas alfa.

El trabajo de Rutheford en la estructura atómica fue mejorado por sus estudiantes, Niels Bohr y Henry Mosley. La teoría electrónica de los enlaces químicos y orbitales moleculares fue desarrollada por los científicos americanos Linus Pauling y Gilbert N. Lewis.

El año 2011 fue declarado por las Naciones Unidas como el Año Internacional de la Química. Esta iniciativa fue impulsada por la Unión Internacional de Química Pura y Aplicada, en conjunto con la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura. Se celebró por medio de las distintas sociedades de químicos, académicos e instituciones de todo el mundo y se basó en iniciativas individuales para organizar actividades locales y regionales.

El actual modelo de la estructura atómica es el modelo mecánico cuántico. La química tradicional comenzó con el estudio de las partículas elementales: átomos, moléculas, sustancias, metales, cristales y otros agregados de la materia. La materia podía ser estudiada en estados líquido, de gas o sólidos, ya sea de manera aislada o en combinación. Las interacciones, reacciones y transformaciones que se estudian en química son generalmente el resultado de las interacciones entre átomos, dando lugar a direccionamientos de los enlaces químicos que los mantienen unidos a otros átomos. Tales comportamientos son estudiados en un laboratorio de química.

En el laboratorio de química se suelen utilizar diversos materiales de cristalería. Sin embargo, la cristalería no es fundamental en la experimentación química ya que gran cantidad de experimentación científica (así sea en química aplicada o industrial) se realiza sin ella.

Una reacción química es la transformación de algunas sustancias en una o más sustancias diferentes. La base de tal transformación química es la reordenación de los electrones en los enlaces químicos entre los átomos. Se puede representar simbólicamente como una ecuación química, que por lo general implica átomos como la partícula central. El número de átomos a la izquierda y la derecha en la ecuación para una transformación química debe ser igual (cuando es desigual, la transformación, por definición, no es química, sino más bien una reacción nuclear o la desintegración radiactiva). El tipo de reacciones químicas que una sustancia puede experimentar y los cambios de energía que pueden acompañarla, son determinados por ciertas reglas básicas, conocidas como leyes químicas.

Las consideraciones energéticas y de entropía son variables importantes en casi todos los estudios químicos. Las sustancias químicas se clasifican sobre la base de su estructura, estado y composiciones químicas. Estas pueden ser analizadas usando herramientas del análisis químico, como por ejemplo, la espectroscopia y cromatografía. Los científicos dedicados a la investigación química se les suele llamar «químicos». La mayoría de los químicos se especializan en una o más áreas subdisciplinas. Varios conceptos son esenciales para el estudio de la química, y algunos de ellos son: 

En química, la materia se define como cualquier cosa que tenga masa en reposo, volumen y se componga de partículas. Las partículas que componen la materia también poseen masa en reposo, sin embargo, no todas las partículas tienen masa en reposo, un ejemplo es el fotón. La materia puede ser una sustancia química pura o una mezcla de sustancias.

El átomo es la unidad básica de la química. Se compone de un núcleo denso llamado núcleo atómico, el cual es rodeado por un espacio denominado «nube de electrones». El núcleo se compone de protones cargados positivamente y neutrones sin carga (ambos denominados nucleones). La nube de electrones son electrones que giran alrededor del núcleo cargados negativamente. 

En un átomo neutro, los electrones cargados negativamente equilibran la carga positiva de los protones. El núcleo es denso; La masa de un nucleón es 1836 veces mayor que la de un electrón, sin embargo, el radio de un átomo es aproximadamente 10000 veces mayor que la de su núcleo

El átomo es la entidad más pequeña que se debe considerar para conservar las propiedades químicas del elemento, tales como la electronegatividad, el potencial de ionización, los estados de oxidación preferidos, los números de coordinación y los tipos de enlaces que un átomo prefiere formar (metálicos, iónicos, covalentes, etc.).

Un elemento químico es una sustancia pura que se compone de un solo tipo de átomo, caracterizado por su número particular de protones en los núcleos de sus átomos, número conocido como «número atómico» y que es representado por el símbolo Z. El número másico es la suma del número de protones y neutrones en el núcleo. Aunque todos los núcleos de todos los átomos que pertenecen a un elemento tengan el mismo número atómico, no necesariamente deben tener el mismo número másico; átomos de un elemento que tienen diferentes números de masa se conocen como isótopos. Por ejemplo, todos los átomos con 6 protones en sus núcleos son átomos de carbono, pero los átomos de carbono pueden tener números másicos de 12 o 13.

La presentación estándar de los elementos químicos está en la tabla periódica, la cual ordena los elementos por número atómico. La tabla periódica se organiza en grupos (también llamados columnas) y períodos (o filas). La tabla periódica es útil para identificar tendencias periódicas.

Un compuesto es una sustancia química pura compuesta de más de un elemento. Las propiedades de un compuesto tienen poca similitud con las de sus elementos. La nomenclatura estándar de los compuestos es fijado por la Unión Internacional de Química Pura y Aplicada (IUPAC). Los compuestos orgánicos se nombran según el sistema de nomenclatura orgánica. Los compuestos inorgánicos se nombran según el sistema de nomenclatura inorgánica. Además, el Servicio de Resúmenes Químicos ha ideado un método para nombrar sustancias químicas. En este esquema cada sustancia química es identificable por un número conocido como número de registro CAS.

La química cubre un campo de estudios bastante amplio, por lo que en la práctica se estudia cada tema de manera particular. Las seis principales y más estudiadas ramas de la química son:


La diferencia entre la química orgánica y la química biológica es que en la química biológica las moléculas de ADN tienen una historia y, por ende, en su estructura nos hablan de su historia, del pasado en el que se han constituido, mientras que una molécula orgánica, creada hoy, es solo testigo de su presente, sin pasado y sin evolución histórica.

Además existen múltiples subdisciplinas que, por ser demasiado específicas o bien multidisciplinares, se estudian individualmente:


Hace aproximadamente 455 años solo se conocían doce elementos. A medida que fueron descubriendo más elementos, los científicos se dieron cuenta de que todos guardaban un orden preciso. Cuando los colocaron en una tabla ordenada en filas y columnas, vieron que los elementos de una misma columna tenían propiedades similares. Pero también aparecían espacios vacíos en la tabla para los elementos aún desconocidos. Estos espacios huecos llevaron al científico ruso Dmitri Mendeléyve a pronosticar la existencia del germanio, de número atómico 32, así como su color, su peso, su densidad y su punto de fusión. Su “predicción sobre otros elementos como —el galio y el escandio— también resultó muy atinada”, señala la obra Chemistry, libro de texto de química editado en 1995.

El origen de la teoría atómica se remonta a la escuela filosófica de los atomistas, en la Grecia antigua. Los fundamentos empíricos de la teoría atómica, de acuerdo con el método científico, se debe a un conjunto de trabajos hechos por Antoine Lavoisier, Louis Proust, Jeremias Benjamin Richter, John Dalton, Gay-Lussac, Berzelius y Amadeo Avogadro entre muchos otros, hacia principios del siglo XIX.

Los átomos son la fracción más pequeña de materia estudiados por la química, están constituidos por diferentes partículas, cargadas eléctricamente, los electrones, de carga negativa; los protones, de carga positiva; los neutrones, que, como su nombre indica, son neutros (sin carga); todos ellos aportan masa para contribuir al peso.

Los átomos son las partes más pequeñas de un elemento (como el carbono, el hierro o el oxígeno). Todos los átomos de un mismo elemento tienen la misma estructura electrónica (responsable ésta de la mayor parte de las características químicas), y pueden diferir en la cantidad de neutrones (isótopos). Las moléculas son las partes más pequeñas de una sustancia (como el azúcar), y se componen de átomos enlazados entre sí. Si tienen carga eléctrica, tanto átomos como moléculas se llaman iones: cationes si son positivos, aniones si son negativos.

El mol se usa como contador de unidades, como la docena (12) o el millar (1000), y equivale a formula_1. Se dice que 12 gramos de carbono o un gramo de hidrógeno o 56 gramos de hierro contienen aproximadamente un mol de átomos (la masa molar de un elemento está basada en la masa de un mol de dicho elemento). Se dice entonces que el mol es una unidad de cambio. El mol tiene relación directa con el número de Avogadro. El número de Avogadro fue estimado para el átomo de carbono por el químico y físico italiano Carlo Amedeo Avogadro, Conde de Quarequa e di Cerreto. Este valor, expuesto anteriormente, equivale al número de partículas presentes en 1 mol de dicha sustancia:

1 mol de glucosa equivale a formula_1 moléculas de glucosa.
1 mol de uranio equivale a formula_1 átomos de uranio.

Dentro de los átomos puede existir un núcleo atómico y uno o más electrones. Los electrones son muy importantes para las propiedades y las reacciones químicas. Dentro del núcleo se encuentran los neutrones y los protones. Los electrones se encuentran alrededor del núcleo.
También se dice que el átomo es la unidad básica de la materia con características propias. Está formado por un núcleo, donde se encuentran los protones.

Los enlaces son las uniones entre átomos para formar moléculas. Siempre que existe una molécula es porque ésta es más estable que los átomos que la forman por separado. A la diferencia de energía entre estos dos estados se le denomina energía de enlace.

Los átomos se combinan en proporciones fijas para generar moléculas concretas. Por ejemplo, dos átomos de hidrógeno se combinan con uno de oxígeno para dar una molécula de agua. Esta proporción fija se conoce como estequiometría. Sin embargo, el mismo número y tipo de átomos puede combinarse de diferente forma dando lugar a sustancias isómeras.

Para una descripción y comprensión detalladas de las reacciones químicas y de las propiedades físicas de las diferentes sustancias, es muy útil su descripción a través de orbitales, con ayuda de la química cuántica.

Un orbital atómico es una función matemática que describe la disposición de uno o dos electrones en un átomo. Un orbital molecular es el análogo en las moléculas.

En la teoría del orbital molecular la formación del enlace covalente se debe a una combinación matemática de orbitales atómicos (funciones de onda) que forman orbitales moleculares, llamados así porque pertenecen a toda la molécula y no a un átomo individual. Así como un orbital atómico (sea híbrido o no) describe una región del espacio que rodea a un átomo donde es probable que se encuentre un electrón, un orbital molecular describe también una región del espacio en una molécula donde es más factible que se hallen los electrones.

Al igual que un orbital atómico, un orbital molecular tiene un tamaño, una forma y una energía específicos. Por ejemplo, en la molécula de hidrógeno molecular se combinan dos orbitales atómicos, ocupado cada uno por un electrón. Hay dos formas en que puede presentarse la combinación de orbitales: aditiva y substractiva. La combinación aditiva produce la formación de un orbital molecular que tiene menor energía y que presenta una forma casi ovalada, mientras que la combinación substractiva conduce a la formación de un orbital molecular con mayor energía y que genera un nodo entre los núcleos.

Los orbitales son funciones matemáticas para describir procesos físicos: un orbital únicamente existe en el sentido matemático, como pueden existir una suma, una parábola o una raíz cuadrada. Los átomos y las moléculas son también idealizaciones y simplificaciones: un átomo y una molécula solo existen en el vacío, y en sentido estricto una molécula solo se descompone en átomos si se rompen todos sus enlaces.

En el "mundo real" únicamente existen los materiales y las sustancias. Si se confunden los objetos reales con los modelos teóricos que se usan para describirlos, es fácil caer en falacias lógicas.

En agua, y en otros disolventes (como la acetona o el alcohol), es posible disolver sustancias, de forma que quedan disgregadas en las moléculas o en los iones que las componen (las disoluciones son transparentes). Cuando se supera cierto límite, llamado solubilidad, la sustancia ya no se disuelve, y queda, bien como precipitado en el fondo del recipiente, bien como suspensión, flotando en pequeñas partículas (las suspensiones son opacas o traslúcidas).

Se denomina concentración a la medida de la cantidad de soluto por unidad de cantidad de disolvente.

La concentración de una disolución se puede expresar de diferentes formas, en función de la unidad empleada para determinar las cantidades de soluto y disolvente. Las más usuales son:


El pH es una escala logarítmica para describir la acidez de una disolución acuosa. Los ácidos, como por ejemplo el zumo de limón y el vinagre, tienen un pH bajo (inferior a 7). Las bases, como la sosa o el bicarbonato de sodio, tienen un pH alto (superior a 7).

El pH se calcula mediante la siguiente ecuación:

donde formula_5 es la actividad de iones hidrógeno en la solución, la que en soluciones diluidas es numéricamente igual a la molaridad de iones hidrógeno formula_6 que cede el ácido a la solución.


La IUPAC, un organismo internacional, mantiene unas reglas para la formulación y nomenclatura química. De esta forma, es posible referirse a los compuestos químicos de forma sistemática y sin equívocos.

Mediante el uso de fórmulas químicas es posible también expresar de forma sistemática las reacciones químicas, en forma de ecuación química.

Por ejemplo:





</doc>
<doc id="3356" url="https://es.wikipedia.org/wiki?curid=3356" title="Louis Braille">
Louis Braille

Louis Braille (; Coupvray, Sena y Marne, Francia, 4 de enero de 1809-París, Francia, 6 de enero de 1852) fue un pedagogo francés que diseñó un sistema de lectura y escritura para personas con discapacidad visual. Su sistema es conocido internacionalmente como sistema Braille y es usado tanto en la escritura como en la lectura y la notación musical.

Pese a su prematura discapacidad, Louis Braille destacó en sus estudios y recibió una beca para el . Todavía siendo un estudiante allí e inspirado por la criptografía militar de Charles Barbier, elaboró un código táctil específicamente diseñado para facilitar la lectura y la escritura de los alumnos con discapacidad visual de una forma mucho más rápida y eficaz en comparación con los métodos existentes en aquel momento.

A finales del siglo XVIII Francia vivió profundos cambios políticos, sociales y culturales. Durante la Revolución Francesa de 1789 empezaron a producirse una serie de transformaciones que a su vez iban a ser decisivas para las personas con discapacidad. El régimen antiguo empezó a estremecerse y a resquebrajarse de tal manera que se sucedieron nuevos cambios y aparecieron unas nuevas condiciones que resultaron favorables para que grupos que hasta entonces habían estado marginados de la sociedad, tuvieran acceso a la educación y a los derechos básicos de todos los ciudadanos. Hasta ese momento, la única atención que se había dado a las personas con discapacidad visual eran hospicios creados especialmente para ellos. A pesar de que a lo largo de la historia hubieran casos de personas con esta discapacidad que destacaran en el campo artístico, científico o incluso político, la mayoría fueron casos aislados de los que poco se conoce en la actualidad.

, un personaje erudito en el mundo de las letras que poseía cargos importantes en el ayuntamiento de París, en 1786 se interesó mucho por tratar de mejorar la situación de estas personas motivado a partir de una experiencia que él mismo describió. Haüy observó la penosa situación de un grupo de ciegos que, acogidos en el asilo (fundado en 1269 por Luis IX), tocaban música en la calle para ganarse, entre burlas y desprecios, alguna que otra limosna:

Haüy dedicó gran parte de su vida a la educación de estas personas. El encuentro en 1784 con la compositora y pianista Maria Theresia von Paradis, ciega desde los dos años de edad y que había aprendido por sí misma a leer textos y música palpando unos alfileres clavados en almohadones, reforzó en gran parte esta gran labor.

Haüy fundó en 1786 el Instituto de los Niños Ciegos, una de las primeras escuelas dedicadas a la educación de personas ciegas. Asimismo, empezó a diseñar un método de escritura en relieve que facilitaba el acceso a la lectura y escritura mediante la percepción táctil. Durante la Revolución Francesa, Haüy fue destituido como director de su Instituto y éste pasó a manos del estado y se llamó Instituto de los Trabajadores Ciegos hasta que finalmente pasó a ser la sede del Instituto Nacional de Jóvenes Ciegos.

Braille nació en Coupvray, un pequeño pueblo situado a unos 30 kilómetros al este de París. Él y sus tres hermanos mayores –Monique Catherine Josephine Braille (n. 1793), Louis-Simon Braille (n. 1795), y Marie Céline Braille (n. 1797)– vivían con su madre, Monique, y su padre, Simon-René, en tres hectáreas de tierra y unos viñedos en el campo. La familia Braille fue una familia humilde, tradicionalmente dedicada a la talabartería. Louis fue el hijo menor de una familia formada por padres ya mayores y hermanos también mayores. Todo ello determinó un marco familiar muy especial, sobre todo al tratarse de un niño que perdió la vista a muy temprana edad. Es posible, pues, que ese carácter afable, cálido, persistente, atento y observador se debiera en gran parte a ese marco familiar que siempre estuvo tan presente en los primeros años de su infancia. A pesar de ser una familia con poca formación cultural y de escasos recursos demostraron tener una gran tenacidad y destreza para que Louis se desarrollara tal y como lo hacían los otros niños a su edad. No deja de ser notable el hecho de que, dadas las circunstancias, no tomara una actitud sobre-protectora ante su hijo debido a la discapacidad.

Simón-René, el padre de Louis, le enseñó a leer mediante tachas de lapicero con las que formaba las letras sobre una madera o sobre un trozo de cuero. Louis recorría esas marcas con sus dedos hasta aprender letras y palabras enteras. En 1818 los Braille enviaron a su hijo a la escuela de la villa con la misma naturalidad que lo hicieron con sus otros tres hijos. A pesar de que inicialmente su aprendizaje fuera mediante transmisión oral, el maestro de la escuela Antonie Bécheret se sorprendió al observar que Louis pudiera poseer una actitud tan predispuesta hacia el aprendizaje. En sus primeros años de estudiante logró una beca para ingresar en el de París que le permitió emprender sus estudios, puesto que su familia no tenía recursos para hacerse cargo de los gastos. A partir de entonces, empezaría un largo camino tanto de alumno como de profesor en aquel instituto.

En el año 1812, Louis contaba con tan solo tres años de edad, y mientras jugaba en el taller de su padre tratando de imitarle, cogió un tranchete que este utilizaba para su trabajo y trató de cortar una correa con tal mala suerte que un pequeño accidente del cual no se tiene conocimiento exacto (un pedacito de cuero que le pudo saltar al ojo o bien la punta de la herramienta) le hirió el ojo derecho. La inflamación acabó por dañar también el ojo izquierdo, provocando una ceguera irreversible debida a una oftalmía simpática. Si dicha inflamación no es tratada a tiempo, la reacción autoinmune que se provoca en el ojo dañado acaba afectando al contralateral pudiendo causar una ceguera irreversible en ambos ojos. Teniendo presente el contexto y también la situación económica, era prácticamente imposible que no acabara afectando a ambos ojos y por eso Louis quedó con esta discapacidad a los cinco años de edad. 

Hay que tener presente que cuando la ceguera se produce antes de los cinco o seis años de edad, el niño no conserva prácticamente ninguna imagen visual clara, ni siquiera el recuerdo del rostro de sus familiares o el lugar en donde transcurrió su infancia. Además, el propio rostro pierde parte de su movilidad expresiva que surge como un efecto natural de la imitación espontánea de los niños en edades tempranas. A consecuencia de esto, existen algunas descripciones que todavía se conservan en las cuales algunos profesores de la Institución de Ciegos de París lo describen como una persona poco expresiva. Evidentemente, bajo esta apariencia exterior debido a la precocidad de su ceguera, existía una persona con unas grandes cualidades que poco a poco se irían descubriendo durante su estancia en el Instituto.

Inicialmente, la Institución Real de Jóvenes Ciegos de París estaba constituida por distintos edificios que en su mayor parte estaban viejos y realmente poco acondicionados para recibir estudiantes. En esos edificios un centenar de jóvenes estudiantes con discapacidad visual, además del personal de servicio, tenían que vivir y trabajar en una casa en la cual había una capilla, una biblioteca, una imprenta, unas aulas para las clases de instrumento y un salón para los ejercicios públicos; además de las habitaciones de los alumnos que ahí residían como internados. El comedor de los alumnos era una galería con una escalera a cada extremo y el taller principal (el telar) era un patio cubierto, con lo cual se privaba de luz a los pisos bajos contiguos. Los talleres restantes se separaban con una simple balaustrada y las habitaciones daban unas con otras. También había un cuarto de baño que dejaba entrever unas pésimas condiciones higiénicas, además que solían bañarse solamente una vez al mes. De hecho, los propios informes de los médicos de aquella época nos ilustran una realidad de apariencia lejana pero cierta. En una de las intervenciones de Pierre Henri menciona uno de los informes realizados por un médico el 14 de mayo de 1838, y dice así: «Ayer fui a visitar el establecimiento de Jóvenes Ciegos y puedo aseguraros que no hay la menor exageración en la descripción de aquel lugar [...] ya que, ciertamente, no hay descripción que pueda daros idea de aquel local estrecho, infecto y tenebroso; de aquellos pasillos partidos en dos para hacer verdaderos cuchitriles que allí llaman talleres o clases; de aquellas innumerables escaleras tortuosas y carcomidas que, lejos de estar preparadas para desgraciados que sólo pueden guiarse por el tacto, parecen un reto lanzado a la ceguera de aquellos niños [...].»
Y otras consideraciones de los propios alumnos que llegan a completar este testimonio:

«Hoy día los ciegos no pueden formarse una idea exacta de las muchas humillaciones de este género por los cuales era preciso pasar en aquella época en que los videntes encargados de nuestra educación, ignoraban aún hasta qué punto puede llegar nuestra destreza, por lo que nos exigían cosas muy superiores a las que un ciego, por experto que sea, puede practicar».

En resumen, son éstas las condiciones en las que vivieron el joven estudiante y sus compañeros durante más de 15 años. La instalación de la escuela, sin embargo, se trasladó en el año 1843 a un nuevo edificio de París y las condiciones mejoraron, pero posiblemente el estado de salud de Louis Braille y la enfermedad de tuberculosis que le acompañó desde temprana edad ya se habían originado en las viejas instalaciones.

Braille empezó a destacar primero como alumno y después como maestro ideando asimismo su sistema de lectura y escritura conocido actualmente como sistema braille. Inicialmente los estudiantes del Instituto aprendían a leer y a escribir mediante el sistema de Valentin Haüy que consistía en predisponer las letras en relieve, a pesar de que a la práctica fueran poco agradecidas para el tacto. A Louis, sin embargo, este sistema le permitió leer muchos de los libros impresos que estaban en la biblioteca del Instituto. También aprendió a escribir a lápiz con la intención de poderse comunicar con los videntes. Para ello, empleaba moldes que contenían las letras vacías por cuyos bordes había que deslizar el lápiz. Aprendió de ese mismo modo matemática y geografía. En el caso de la notación musical, durante muchos años se prescindió del relieve para incorporar la enseñanza musical mediante la transmisión oral y su memorización.

Desde su ingreso demostró su capacidad para desarrollarse en distintas áreas como: gramática, retórica, historia, geometría, álgebra y sobre todo música, tanto en teoría como en práctica (aprendió a tocar el órgano, violonchelo y el piano). En el instituto, enseñó más de una materia y realizó algunos manuales de historia y aritmética para sus alumnos. De hecho, no solamente enseñó a ciegos sino también a niños videntes puesto que en ese momento el instituto admitía un determinado número de videntes a los que se les enseñaba gratuitamente a cambio de una cierta cooperación que se prestaba a los jóvenes ciegos, como por ejemplo ayudarlos a leer, a redactar o guiarlos al andar.

Braille poseía una gran capacidad reflexiva y metódica, era cercano con sus alumnos y conseguía despertar su interés, comprenderlos y aconsejarlos en los momentos más difíciles. Posiblemente esta capacidad de síntesis se deriva de los complicados procedimientos de escritura e impresión, puesto que tal y como él mismo decía «hemos de procurar expresar el pensamiento con el menor número posible de palabras»."

A partir de 1835 y debido a los primeros síntomas de tuberculosis se fue retirando progresivamente de sus enseñanzas hasta quedar encargado únicamente de las clases de música. De hecho, la profesión mencionada que aparece en su testamento es la de "profesor de música". En el año 1840 recibió clases de los mejores maestros: Mme. Van der Burch en el piano; Bénazet para el violonchelo y Mangues para el órgano. Braille fue organista durante muchos años en la iglesia de San Nicolás de los Campos de París. En el órgano, dice Cotalt, «su ejecución era exacta, brillante y desenvuelta, y presentaba bastante bien el aire de toda su persona».

Louis Braille murió a la edad de 43 años de tuberculosis enfermedad que le había acompañado durante mucho tiempo y que probablemente se iniciara debido a las pésimas condiciones higiénicas existentes en el primer Instituto de Jóvenes Ciegos. El funeral se celebró en la capilla de la Institución Nacional y su cuerpo fue trasladado a su pueblo natal para ser enterrado en el pequeño cementerio de Coupray, al lado de su padre y su hermana que habían muerto años antes. Su ataúd se depositó allí el 10 de enero de 1852. En 1952 sus restos fueron trasladados al Panteón de París. Solo sus manos permanecieron enterrados en Coupray como un símbolo al sistema de lectura táctil que él inventó.

En 1825 Luis Braille ideó su sistema de puntos en relieve que aportaba a las personas ciegas una herramienta válida y eficaz para leer, escribir y facilitar el acceso a la educación, la cultura y la información.

Durante los primeros años del siglo XIX existía una gran preocupación por encontrar un sistema de lectura que se adecuara a las necesidades de las personas con discapacidad visuales.De hecho años antes, el sacerdote italiano Francesco Lana de Terzi en su libro Prodromo introdujo un alfabeto nuevo de invención propia para la gente ciega, basado en signos (guiones) que podían ser reconocidos por el tacto.

También Haüy ya había tratado de solucionar este problema reproduciendo las letras en altorrelieve, no obstante eso suponía una lenta y complicada tarea. En abril de 1821 Braille conoció el sistema de Barbier. Su creador, Charles Barbier, siempre había mostrado un especial interés y una gran dedicación al estudio y la experimentación de los sistemas de lectura y uno de sus objetivos primordiales era mejorar las comunicaciones del ejército Francés durante aquellos años. De esa manera generó un código cifrado que llamó «escritura nocturna» y que serviría para que los oficiales en campaña pudiesen redactar mensajes encriptados en la oscuridad y además poder descifrarlos con los dedos. 
Dicha escritura presenta una serie de virtudes que posteriormente serían tomadas y desarrolladas en el sistema de Louis Braille. La primera es el empleo del punto como el elemento clave para generar el código de lectura táctil (a diferencia del altorrelieve hasta ahora empleado) y la segunda, es el hecho de que no emplea la letra común sino que genera otras representaciones. A pesar de estas ventajas el sistema también tenía ciertos inconvenientes: en el sistema de Barbier no se representa el alfabeto sino grupos de sonidos de la lengua francesa. Además, la base constaba de un elevado número de puntos que dificultaba una rápida lectura mediante el tacto. La sonografía de Barbier se empleó en el instituto durante escasos años hasta quedar prácticamente desplaza por el sistema de Braille, a pesar de que inicialmente este último no fuera aceptado en el Instituto.

Posiblemente durante el año 1825, los alumnos más avanzados del Instituto que emplearon entusiasmados el sistema ideado por Barbier, comenzaron a reflexionar y discutir acerca de posibles mejorías de ese nuevo sistema. Es probable, pues, que entre todos tratasen de perfeccionar la sonografía dado el indiscutible interés que tenían. Este interés no consistía solamente en aprender un sistema que les permitiera agilizar su capacidad de lectura y escritura, sino también consistía en mostrar lo que eran capaces de realizar a una sociedad con tantísimos prejuicios frente a la comunidad invidente. Estos alumnos debían tener su alfabeto y posiblemente Braille fue el joven estudiante que acabó por encontrar la fórmula que le permitió perfeccionar ingeniosamente el sistema ideado por Charles Barbier. A diferencia de Barbier, fue posiblemente el hecho de que Braille fuera ciego el que dotaría a éste de una mayor «intuición psicológica» y trató de generar un signo hecho de puntos que pudiese formar una imagen bajo el dedo, convirtiendo en sintética la lectura táctil.

Antes de los 30 años, Louis había ideado un sistema que se adecuaba perfectamente a las características de la percepción táctil a nivel psicológico, estructural y fisiológico. El signo braille, compuesto por un máximo de seis puntos, se adapta perfectamente a la yema del dedo y esto produce que la persona lo pueda aprender en su totalidad, transmitido como una imagen al cerebro. Este sistema consta de 63 caracteres formados de uno a seis puntos y que al ser impresos en relieve en papel permiten la lectura mediante el tacto. Así mismo, los caracteres que integran el sistema, que Braille publicó en 1829 y 1837, están adaptados a la notación musical, facilitando así su comprensión. 

Inicialmente el sistema encontró una fuerte oposición e incluso se llegó a prohibir durante muchos años en aquel Instituto. Muchos maestros consideraron que dicho sistema, al ser distinto al empleado por los videntes, generaba aislamiento y segregación de cara al alumnado discapacitado. Esta argumentación no deja de parecer en muchas ocasiones una excusa para justificar que personas videntes (sobre todo profesores del Instituto) no emplearan su tiempo en aprender un código totalmente distinto de la escritura convencional. De hecho, fueron las personas ciegas las que defendieron e impulsaron el sistema, sin lugar a dudas los más indicados para decidir sobre esta cuestión.

Hay muchas anécdotas que han sobrevivido a aquellos años en los cuales se prohibió el sistema. Por ejemplo muchos alumnos y algunos profesores ciegos del Instituto lo emplearon de forma clandestina escribiendo cartas y copiando textos que luego serían mostrados a los demás y así sucesivamente. De ese modo e 1844, gracias a la presión ejercida por parte de esos grupos y coincidiendo con la inauguración del nuevo edificio del Instituto en el de París, el director reivindicó el sistema realizando un homenaje a su inventor. En 1853, un año después de la muerte de Braille, el sistema fue aceptado oficialmente por las Instituciones y por tanto su autor nunca llegó a ser reconocido oficialmente mientras vivía.

El sistema Braille, originado en Francia, utilizó muchos símbolos correspondientes a las 64 combinaciones de los seis puntos para representar acentos especiales correspondientes al francés. Al emplearse en otros idiomas, las combinaciones de puntos braille cambian de significado. Por ejemplo, el punto final y el signo de mayúscula cambian del español al inglés. Asimismo, Braille y su amigo Pierre Foucault llegaron a desarrollar una máquina de escribir para que fuera aún más fácil la producción de textos en Braille: el 

La música tuvo un especial lugar en la vida de Louis Braille. Durante toda su vida se dedicó a dar clases de música y también fue un instrumentista notable. Asimismo, ideó un sistema de notación musical en braille (Signografía Musical Braille, actualmente conocido como musicografía) para personas con discapacidad visual y estudió también las posibles formas de comunicación entre la escritura musical en tinta y en relieve con la intención de que pudieran ser empleados e intercambiados de forma recíproca entre personas videntes y discapacitadas. Debemos tener presente que durante la década del siglo XVIII e incluso principios del XIX era habitual identificar la ceguera con la mendicidad, cosa que provocó que durante muchos años se eliminara la enseñanza musical de los programas escolares para evitar su parentesco.

En 1952, un siglo después de su muerte, sus restos fueron trasladados a la capital francesa y enterrados en el Panteón de París.

Bélgica e Italia emitieron sendas monedas conmemorativas de 2 euros en 2009 para celebrar el 200.º aniversario de su nacimiento.




</doc>
<doc id="3359" url="https://es.wikipedia.org/wiki?curid=3359" title="Energía eólica">
Energía eólica

La energía eólica es la energía obtenida a partir del viento, es decir, la energía cinética generada por efecto de las corrientes de aire, y que es convertida en otras formas útiles de energía para las actividades humanas. El término «eólico» proviene del latín "aeolicus", es decir «perteneciente o relativo a Eolo», dios de los vientos en la mitología griega.

En la actualidad, la energía eólica es utilizada principalmente para producir electricidad mediante aerogeneradores conectados a las grandes redes de distribución de energía eléctrica. Los parques eólicos construidos en tierra suponen una fuente de energía cada vez más barata y competitiva, e incluso más barata en muchas regiones que otras fuentes de energía convencionales. Pequeñas instalaciones eólicas pueden, por ejemplo, proporcionar electricidad en regiones remotas y aisladas que no tienen acceso a la red eléctrica, al igual que la energía solar fotovoltaica. Las compañías eléctricas distribuidoras adquieren cada vez en mayor medida el excedente de electricidad producido por pequeñas instalaciones eólicas domésticas. El auge de la energía eólica ha provocado también la planificación y construcción de parques eólicos marinos —a menudo conocidos como parques eólicos "offshore" por su nombre en inglés—, situados cerca de las costas. La energía del viento es más estable y fuerte en el mar que en tierra, y los parques eólicos marinos tienen un impacto visual menor, pero sus costos de construcción y mantenimiento son considerablemente mayores.

A finales de 2014, la capacidad mundial instalada de energía eólica ascendía a 370 GW, generando alrededor del 5 % del consumo de electricidad mundial. Dinamarca genera más de un 25 % de su electricidad mediante energía eólica, y más de 80 países en todo el mundo la utilizan de forma creciente para proporcionar energía eléctrica en sus redes de distribución, aumentando su capacidad anualmente con tasas por encima del 20 %. En España la energía eólica produjo un 20,3 % del consumo eléctrico de la península en 2014, convirtiéndose en la segunda tecnología con mayor contribución a la cobertura de la demanda, muy cerca de la energía nuclear con un 22,0 %.

La energía eólica es un recurso abundante, renovable y limpio que ayuda a disminuir las emisiones de gases de efecto invernadero al reemplazar fuentes de energía a base de combustibles fósiles. El impacto ambiental de este tipo de energía es además, generalmente, menos problemático que el de otras fuentes de energía.

La energía del viento es bastante estable y predecible a escala anual, aunque presenta variaciones significativas a escalas de tiempo menores. Al incrementarse la proporción de energía eólica producida en una determinada región o país, se hace imprescindible establecer una serie de mejoras en la red eléctrica local. Diversas técnicas de control energético, como una mayor capacidad de almacenamiento de energía, una distribución geográfica amplia de los aerogeneradores, la disponibilidad de fuentes de energía de respaldo, la posibilidad de exportar o importar energía a regiones vecinas o la reducción de la demanda cuando la producción eólica es menor, pueden ayudar a mitigar en gran medida estos problemas. Además, son de extrema importancia las previsiones de producción eólica que permiten a los gestores de la red eléctrica estar preparados y anticiparse frente a las previsibles variaciones en la producción eólica que puedan tener lugar a corto plazo.

La energía del viento está relacionada con el movimiento de las masas de aire que se desplazan desde zonas de alta presión atmosférica hacia zonas adyacentes de menor presión, con velocidades proporcionales al gradiente de presión.

Los vientos se generan a causa del calentamiento no uniforme de la superficie terrestre debido a la radiación solar; entre el 1 y el 2% de la energía proveniente del Sol se convierte en viento. Durante el día, los continentes transfieren una mayor cantidad de energía solar al aire que las masas de agua, haciendo que este se caliente y se expanda, por lo que se vuelve menos denso y se eleva. El aire más frío y pesado que proviene de los mares, océanos y grandes lagos se pone en movimiento para ocupar el lugar dejado por el aire caliente.
Para poder aprovechar la energía eólica es importante conocer las variaciones diurnas, nocturnas y estacionales de los vientos, la variación de la velocidad del viento con la altura sobre el suelo, la entidad de las ráfagas en espacios de tiempo breves, y los valores máximos ocurridos en series históricas de datos con una duración mínima de 20 años. Para poder utilizar la energía del viento, es necesario que este alcance una velocidad mínima que depende del aerogenerador que se vaya a utilizar pero que suele empezar entre los 3 m/s (10 km/h) y los 4 m/s (14,4 km/h), velocidad llamada ""cut-in speed"", y que no supere los 25 m/s (90 km/h), velocidad llamada ""cut-out speed"".

La energía del viento se aprovecha mediante el uso de máquinas eólicas o aeromotores capaces de transformar la energía eólica en energía mecánica de rotación utilizable, ya sea para accionar directamente las máquinas operatrices o para la producción de energía eléctrica. En este último caso, el más ampliamente utilizado en la actualidad, el sistema de conversión —que comprende un generador eléctrico con sus sistemas de control y de conexión a la red— es conocido como aerogenerador. En estos la energía eólica mueve una hélice y mediante un sistema mecánico se hace girar el rotor de un generador, normalmente un alternador, que produce energía eléctrica. Para que su instalación resulte rentable, suelen agruparse en concentraciones denominadas parques eólicos.

Una turbina eólica o aerogenerador es una máquina que transforma la energía del viento en energía eléctrica aprovechable mediante unas aspas oblicuas unidas a un eje común. El eje giratorio puede conectarse a varios tipos de maquinaria para moler grano (molinos), bombear agua o generar electricidad. Cuando se usa para producir electricidad se le denomina generador de turbina de viento. Las máquinas movidas por el viento tienen un origen remoto, funcionando las más antiguas como molinos.

La energía eólica no es algo nuevo, es una de las energías más antiguas junto a la energía térmica. El viento como fuerza motriz se ha utilizado desde la antigüedad. Así, ha movido a barcos impulsados por velas o ha hecho funcionar la maquinaria de los molinos al mover sus aspas. Sin embargo, tras una época en la que se fue abandonando, a partir de los años ochenta del siglo XX este tipo de energía limpia experimentó un renacimiento.

La energía eólica crece de forma imparable ya en el siglo XXI, en algunos países más que en otros, pero sin duda alguna en España existe un gran crecimiento, siendo uno de los primeros países, por debajo de Alemania a nivel europeo o de Estados Unidos a escala mundial. El auge del aumento de parques eólicos se debe a las condiciones favorables de viento, sobre todo en Andalucía que ocupa un puesto principal, entre los que se puede destacar el Golfo de Cádiz, ya que el recurso de viento es excepcional.

La referencia más antigua que se tiene es un molino de viento que fue usado para hacer funcionar un órgano en el siglo I de la era común. Los primeros molinos de uso práctico fueron construidos en Sistán, Afganistán, en el siglo VII. Estos fueron molinos de eje vertical con hojas rectangulares. Aparatos hechos de 6 a 8 velas de molino cubiertos con telas fueron usados para moler trigo o extraer agua.

Los primeros molinos aparecieron en Europa en el siglo XII en Francia e Inglaterra y fueron extendiéndose por el continente. Eran unas estructuras de madera, conocidas como torres de molino, que se hacían girar a mano alrededor de un poste central para extender sus aspas al viento. El molino de torre se desarrolló en Francia a lo largo del siglo XIV. Consistía en una torre de piedra coronada por una estructura rotativa de madera que soportaba el eje del molino y la maquinaria superior del mismo.

Estos primeros ejemplares tenían una serie de características comunes. De la parte superior del molino sobresalía un eje horizontal. De este eje partían de cuatro a ocho aspas, con una longitud entre 3 y 9 metros. Las vigas de madera se cubrían con telas o planchas de madera. La energía generada por el giro del eje se transmitía, a través de un sistema de engranajes, a la maquinaria del molino emplazada en la base de la estructura.

Los molinos de eje horizontal fueron usados extensamente en Europa Occidental para moler trigo desde la década de 1180 en adelante. Basta recordar los famosos molinos de viento en las andanzas de Don Quijote. Todavía existen molinos de esa clase, por ejemplo, en Países Bajos.

En Estados Unidos, el desarrollo de molinos de bombeo, reconocibles por sus múltiples velas metálicas, fue el factor principal que permitió la agricultura y la ganadería en vastas áreas de Norteamérica, de otra manera imposible sin acceso fácil al agua. Estos molinos contribuyeron a la expansión del ferrocarril alrededor del mundo, cubriendo las necesidades de agua de las locomotoras a vapor.

Las turbinas eólicas modernas fueron desarrolladas a comienzos de la década de 1980, si bien continúan evolucionando los diseños.

La industria de la energía eólica en tiempos modernos comenzó en 1979 con la producción en serie de turbinas de viento por los fabricantes Kuriant, Vestas, Nordtank, y Bonus. Aquellas turbinas eran pequeñas para los estándares actuales, con capacidades de 20 a 30 kW cada una. Desde entonces, la talla de las turbinas ha crecido enormemente, y la producción se ha expandido a muchos sitios.

La energía eólica alcanzó la paridad de red (el punto en el que el costo de esta energía es igual o inferior al de otras fuentes de energía tradicionales) en algunas áreas de Europa y de Estados Unidos a mediados de la década de 2000. La caída de los costos continúa impulsando a la baja el costo normalizado de esta fuente de energía renovable: se estima que alcanzó la paridad de red de forma general en todo el continente europeo en torno al año 2010, y que alcanzará el mismo punto en todo Estados Unidos en 2016, debido a una reducción adicional de sus costos del 12 %.

La instalación de energía eólica requiere de una considerable inversión inicial, pero posteriormente no presenta gastos de combustible. El precio de la energía eólica es por ello mucho más estable que los precios de otras fuentes de energía fósil, mucho más volátiles. El costo marginal de la energía eólica, una vez que la planta ha sido construida y está en marcha, es generalmente inferior a 1 céntimo de dólar por kWh. Incluso, este costo se ha visto reducido con la mejora tecnológica de las turbinas más recientes. Existen en el mercado palas para aerogeneradores cada vez más largas y ligeras, a la vez que se realizan constantemente mejoras en el funcionamiento de la maquinaria de los propios aerogeneradores, incrementando la eficiencia de los mismos. Igualmente, los costos de inversión inicial y de mantenimiento de los parques eólicos han descendido.

En 2004, el costo de la energía eólica era una quinta parte del que presentaba en los años 1980, y los expertos consideran que la tendencia a la baja continuará en el futuro próximo, con la introducción en el mercado de nuevos aerogeneradores "multi-megavatio" cada vez más grandes y producidos en masa, capaces de producir hasta 8 megavatios de potencia por cada unidad. En 2012, los costos de capital de la energía eólica eran sustancialmente inferiores a los de 2008-2010, aunque todavía estaban por encima de los niveles de 2002, cuando alcanzaron un mínimo histórico. La bajada del resto de costos ha contribuido a alcanzar precios cada vez más competitivos. Un informe de 2011 de la Asociación Americana de la Energía Eólica ("American Wind Energy Association") afirmaba:

Otro informe de la Asociación Británica de la Energía Eólica estima un costo de generación medio para la eólica terrestre de 5-6 céntimos de dólar por kWh (2005). El costo por unidad de energía producida se estimaba en 2006 como comparable al costo de la energía producida en nuevas plantas de generación en Estados Unidos procedente del carbón y gas natural: el costo de la eólica se cifraba en $55,80 por MWh, el del carbón en $53,10/MWh y el del gas natural en $52,50. Otro informe gubernamental obtuvo resultados similares en comparación con el gas natural, en 2011 en Reino Unido. En agosto de 2011 licitaciones en Brasil y Uruguay para compra a 20 años presentaron costos inferiores a los $65 por MWh.

En febrero de 2013 "Bloomberg New Energy Finance" informó de que el costo de la generación de energía procedente de nuevos parques eólicos en Australia es menor que el procedente de nuevas plantas de gas o carbón. Al incluir en los cálculos el esquema de precios actual para los combustibles fósiles, sus estimaciones indicaban unos costos (en dólares australianos) de $80/MWh para nuevos parques eólicos, $143/MWh para nuevas plantas de carbón y $116/MWh para nuevas plantas de gas. Este modelo muestra además que «incluso sin una tasa sobre las emisiones de carbono (la manera más eficiente de reducir emisiones a gran escala) la energía eólica es un 14 % más barata que las nuevas plantas de carbón, y un 18 % más que las nuevas plantas de gas.»<ref name="bnef.com/2013/02/07/renewable-cheaper"></ref>

La industria eólica en Estados Unidos es actualmente capaz de producir mayor potencia a un costo menor gracias al uso de aerogeneradores cada vez más altos y con palas de mayor longitud, capturando de esta manera vientos mayores a alturas más elevadas. Esto ha abierto nuevas oportunidades, y en estados como Indiana, Míchigan y Ohio, el costo de la eólica procedente de aerogeneradores de entre 90 y 120 metros de altura puede competir con fuentes de energía convencionales como el carbón. Los precios han caído hasta incluso 4 céntimos por kWh en algunos casos, y las compañías distribuidoras están incrementando la cantidad de energía eólica en su modelo energético, al darse cuenta progresivamente de su competitividad.

El costo de la unidad de energía producida en instalaciones eólicas se deduce de un cálculo bastante complejo. Para su evaluación se deben tener en cuenta diversos factores, entre los cuales cabe destacar:

Existe una gran cantidad de aerogeneradores operando, con una capacidad total de 369597MW a finales de 2014, de los que Europa cuenta con el 36,3 %. China y Estados Unidos representan juntos casi el 50 % de la capacidad eólica global, mientras que los primeros cinco países (China, EE. UU., Alemania, España e India) representaron el 71,7 % de la capacidad eólica mundial en 2014.

Alemania, España, Estados Unidos, India y Dinamarca han realizado las mayores inversiones en generación de energía eólica. Dinamarca es, en términos relativos, la más destacada en cuanto a fabricación y utilización de turbinas eólicas, con el compromiso realizado en los años 1970 de llegar a obtener la mitad de la producción de energía del país mediante el viento. En 2014 generó el 39,1 % de su electricidad mediante aerogeneradores, mayor porcentaje que cualquier otro país, y el año anterior la energía eólica se consolidó como la fuente de energía más barata del país.

La siguiente tabla muestra la capacidad total de energía eólica instalada al final de cada año (en megavatios) en todo el mundo, detallado por países. Datos publicados por el "Global Wind Energy Council" (GWEC).

A finales de 2016, España tenía instalada una capacidad de energía eólica de 23 057 MW, lo que supone el 21,9 % de la capacidad del sistema eléctrico nacional, la segunda fuente de energía del país por detrás del ciclo combinado con 26 670 MW. Se sitúa así en cuarto lugar en el mundo en cuanto a potencia instalada, detrás de China, EE. UU. y Alemania. Ese mismo año la energía eólica produjo 47 695 GWh, el 18,0 % de la demanda eléctrica.

El 29 de enero de 2015, la energía eólica alcanzó un máximo de potencia instantánea con 17 553 MW, cubriendo un 45 % de la demanda.

Asimismo, está creciendo bastante el sector de la minieólica. Existe una normativa de fabricación de pequeños aerogeneradores, del Comité Electrotécnico Internacional CEI (Norma IEC-61400-2 Ed2) la cual define un aerogenerador de pequeña potencia como aquel cuya área barrida por su rotor es menor de 200 m². La potencia que corresponde a dicha área dependerá de la calidad del diseño del aerogenerador, existiendo de hasta 65 kW como máximo.

El Reino Unido cerró 2008 con 4015 MW eólicos instalados, lo que supone una presencia testimonial en su producción eléctrica. Sin embargo es uno de los países del mundo que más capacidad eólica tiene planificada, y ya ha otorgado concesiones para alcanzar los 32 000 MW eólicos marinos en sus costas:
Según la administración británica “la industria eólica marina es una de las claves de la ruta del Reino Unido hacia una economía baja en emisiones de CO y debería suponer un valor de unos 75 000 millones de libras (84 000 millones de euros) y sostener unos 70 000 empleos hasta 2020”.

Suecia cerró 2009 con 1021 MW eólicos instalados y tiene planes para alcanzar los 14 000 MW en el año 2020, de los cuales entre 2500 y 3000 MW serán marinos.

El desarrollo de la energía eólica en los países de Centroamérica y Sudamérica está en sus inicios, y la capacidad conjunta instalada en ellos, hasta finales de 2013, llega a los 4709 MW. El desglose de potencia instalada por países es el siguiente:


A finales de 2013, la potencia instalada acumulada por países del continente es la siguiente:

Debido a la falta de seguridad en la existencia de viento, la energía eólica no puede ser utilizada como única fuente de energía eléctrica.

Este problema podría solucionarse mediante dispositivos de almacenamiento de energía eléctrica, pero hasta el momento no existen sistemas lo suficientemente grandes como para almacenar cantidades considerables de energía de forma eficiente. Por lo tanto, para salvar los valles en la producción de energía eólica y evitar apagones generalizados, es indispensable un respaldo de las energías convencionales como centrales termoeléctricas de carbón, gas natural, petróleo o ciclo combinado o centrales hidroeléctricas reversibles, por ejemplo. Esto supone un inconveniente, puesto que cuando respaldan a la eólica, las centrales de carbón no pueden funcionar a su rendimiento óptimo, que se sitúa cerca del 90 % de su potencia. Tienen que quedarse muy por debajo de este porcentaje para poder subir sustancialmente su producción en el momento en que amaine el viento. Es por ello que, cuando funcionan en este modo, las centrales térmicas consumen más combustible por kWh producido. Además, al aumentar y disminuir su producción cada vez que cambia la velocidad del viento se produce un desgaste mayor de la maquinaría. Este problema del respaldo en España se va a tratar de solucionar mediante una interconexión con Francia que permita emplear el sistema europeo como colchón de la variabilidad eólica.

Además, la variabilidad en la producción de energía eólica tiene otras importantes consecuencias:

Aunque estos problemas parecen únicos a la energía eólica, son comunes a todas las energías de origen natural:



La microgeneración de energía eólica consiste en pequeños sistemas de generación de hasta 50 kW de potencia. En comunidades remotas y aislada, que tradicionalmente han utilizado generadores diésel, su uso supone una buena alternativa. También es empleada cada vez con más frecuencia por hogares que instalan estos sistemas para reducir o eliminar su dependencia de la red eléctrica por razones económicas, así como para reducir su impacto medioambiental y su huella de carbono. Este tipo de pequeñas turbinas se han venido usando desde hace varias décadas en áreas remotas junto a sistemas de almacenamiento mediante baterías.

Las pequeñas turbinas aerogeneradoras conectadas a la red eléctrica pueden utilizar también lo que se conoce como almacenamiento en la propia red, reemplazando la energía comprada de la red por energía producida localmente, cuando esto es posible. La energía sobrante producidad por los microgeneradores domésticos puede, en algunos países, ser vertida a la red para su venta a la compañía eléctrica, generando de esta manera un pequeño beneficio al propietario de la instalación que amortice la instalación.

Los sistemas desconectados de la red pueden adaptarse a la intermitencia del viento, utilizar baterías, sistemas fotovoltaicos o generadores diésel que complementen la energía producida por la turbina. Otros equipos, como pueden ser parquímetros, señales de tráfico iluminadas, alumbrado público, o sistemas de telecomunicaciones pueden ser también alimentados mediante un pequeño aerogenerador, generalmente junto a un sistema fotovoltaico que cargue unas pequeñas baterías, eliminando la necesidad de la conexión a la red.

La minieólica podría generar electricidad más barata que la de la red en algunas zonas rurales de Reino Unido, según un estudio de la organización Carbon Trust, publicado en 2010. Según ese informe, los mini aerogeneradores podrían llegar a generar 1,5  TWh de electricidad al año en Reino Unido, un 0,4 % del consumo total del país, evitando la emisión de 0,6 millones de toneladas de CO. Esta conclusión se basa en el supuesto de que el 10 % de las viviendas instalara miniturbinas eólicas a precios competitivos con aquellos de la red eléctrica, en torno a 12 peniques (unos 0,17 €) por kWh. Otro informe preparado en 2006 por "Energy Saving Trust", una organización dependiente del Gobierno de Reino Unido, dictaminó que la microgeneración (de diferente tipo: eólica, solar, etc.) podría proporcionar hasta el 30 % o 40 % de la demanda de electricidad en torno al año 2050.

La generación distribuida procedente de energías renovables se ha incrementado en los últimos años, como consecuencia de la mayor concienciación acerca de la influencia del ser humano en el cambio climático. Los equipos electrónicos requeridos para permitir la conexión de sistemas de generación renovable a la red eléctrica pueden además incluir otros sistemas de estabilidad de la red para asegurar y garantizar la calidad del suministro eléctrico.






</doc>
<doc id="3361" url="https://es.wikipedia.org/wiki?curid=3361" title="Mercurio (planeta)">
Mercurio (planeta)

Mercurio es el planeta del sistema solar más próximo al Sol y el más pequeño. Forma parte de los denominados planetas interiores o terrestres y carece de satélites naturales al igual que Venus. Se conocía muy poco sobre su superficie hasta que fue enviada la sonda planetaria "Mariner 10" y se hicieron observaciones con radar y radiotelescopios.

Antiguamente se pensaba que Mercurio siempre presentaba la misma cara al Sol, situación similar al caso de la Luna con la Tierra; es decir, que su periodo de rotación era igual a su periodo de traslación, ambos de 88 días. Sin embargo, en 1965 se mandaron impulsos de radar hacia Mercurio, con lo cual quedó definitivamente demostrado que su periodo de rotación era de 58,7 días, lo cual es 2/3 de su periodo de traslación. Esto no es coincidencia, y es una situación denominada resonancia orbital.

Al ser un planeta cuya órbita es interior a la de la Tierra, lo observamos pasar periódicamente delante del Sol, fenómeno que se denomina tránsito astronómico. Observaciones de su órbita a través de muchos años demostraron que el perihelio gira 43" de arco más por siglo de lo predicho por la mecánica clásica de Newton. Esta discrepancia llevó a un astrónomo francés, Urbain Le Verrier, a pensar que existía un planeta aún más cerca del Sol, al cual llamaron Vulcano, que perturbaba la órbita de Mercurio. Ahora se sabe que Vulcano no existe; la explicación correcta del comportamiento del perihelio de Mercurio se encuentra en la teoría general de la relatividad.

Mercurio es uno de los cuatro planetas rocosos o sólidos; es decir, tiene un cuerpo rocoso, como la Tierra. Este planeta es el más pequeño de los cuatro, con un diámetro de 4879km en el ecuador. Mercurio está formado aproximadamente por un 70 % de elementos metálicos y un 30 % de silicatos. La densidad de este planeta es la segunda más grande de todo el sistema solar, siendo su valor de 5430kg/m³, solo un poco menor que la densidad de la Tierra. La densidad de Mercurio se puede usar para deducir los detalles de su estructura interna. Mientras la alta densidad de la Tierra se explica considerablemente por la compresión gravitacional, particularmente en el núcleo, Mercurio es mucho más pequeño y sus regiones interiores no están tan comprimidas. Por tanto, para explicar esta gran densidad, el núcleo debe ocupar gran parte del planeta y además ser rico en hierro, material con una alta densidad. Los geólogos estiman que el núcleo de Mercurio ocupa un 42 % de su volumen total (el núcleo de la Tierra apenas ocupa un 17 %). Este núcleo estaría parcialmente fundido, lo que explicaría el campo magnético del planeta.

Rodeando el núcleo existe un manto de unos 600 km de grosor. La creencia generalizada entre los expertos es que en los principios de Mercurio un cuerpo de varios kilómetros de diámetro (un planetesimal) impactó contra él deshaciendo la mayor parte del manto original, dando como resultado un manto relativamente delgado comparado con el gran núcleo. (Otras teorías alternativas se discuten en la sección "Formación de Mercurio").
La corteza mercuriana mide en torno a los 100-200 km de espesor. Un hecho distintivo de la corteza de Mercurio son las visibles y numerosas líneas escarpadas o escarpes que se extienden varios miles de kilómetros a lo largo del planeta. Presumiblemente se formaron cuando el núcleo y el manto se enfriaron y contrajeron al tiempo que la corteza se estaba solidificando.

La superficie de Mercurio, como la de la Luna, presenta numerosos impactos de meteoritos que oscilan entre unos metros hasta miles de kilómetros. Algunos de los cráteres son relativamente recientes, de algunos millones de años de edad, y se caracterizan por la presencia de un pico central. Parece ser que los cráteres más antiguos han tenido una erosión muy fuerte, posiblemente debida a los grandes cambios de temperatura que en un día normal oscilan entre 623K (350 °C) por el día y 103K (–170 °C) por la noche.

Al igual que la Luna, Mercurio parece haber sufrido un período de intenso bombardeo de meteoritos de grandes dimensiones, hace unos 4000 millones de años. Durante este periodo de formación de cráteres, Mercurio recibió impactos en toda su superficie, facilitados por la práctica ausencia de atmósfera que pudiera desintegrar o frenar multitud de estas rocas. Durante este tiempo, Mercurio fue volcánicamente activo, formándose cuencas o depresiones con lava del interior del planeta y produciendo planicies lisas similares a los "mares" o "marías" de la Luna; una prueba de ello es el descubrimiento por parte de la sonda MESSENGER de posibles volcanes.

Las planicies o llanuras de Mercurio tienen dos distintas edades; las jóvenes llanuras están menos craterizadas y probablemente se formaron cuando los flujos de lava enterraron el terreno anterior. Un rasgo característico de la superficie de este planeta son los numerosos pliegues de compresión que entrecruzan las llanuras. Se piensa que, como el interior del planeta se enfrió, se contrajo y la superficie comenzó a deformarse. Estos pliegues se pueden apreciar por encima de cráteres y planicies, lo que indica que son mucho más recientes. La superficie mercuriana está significativamente flexada a causa de la fuerza de marea ejercida por el Sol. Las fuerzas de marea en Mercurio son un 17 % más fuertes que las ejercidas por la Luna en la Tierra.

Destacable en la geología de Mercurio es la cuenca de Caloris, un cráter de impacto que constituye una de las mayores depresiones meteóricas de todo el sistema solar; esta formación geológica tiene un diámetro aproximado de 1550 km (antes del sobrevuelo de la sonda MESSENGER se creía que su tamaño era de 1300 km). Contiene, además, una formación de origen desconocido no antes vista ni en el propio Mercurio ni en la Luna, y que consiste en aproximadamente un centenar de grietas estrechas y de suelo liso conocida como "La Araña"; en el centro de esta se encuentra un cráter, desconociéndose si dicho cráter está relacionado con su formación o no. Interesantemente, también el albedo de la cuenca de Caloris es superior al de los terrenos circundantes (al revés de lo que ocurre en la Luna). La razón de ello se está investigando.

Justo en el lado opuesto de esta inmensa formación geológica se encuentran unas colinas o cordilleras conocidas como Terreno Extraño, o "Weird Terrain". Una hipótesis sobre el origen de este complejo geomorfológico es que las ondas de choque generadas por el impacto que formó la cuenca de Caloris atravesaron toda la esfera planetaria convergiendo en las antípodas de dicha formación (180°), fracturando la superficie y formando esta cordillera.

Al igual que otros astros de nuestro sistema solar, como el más semejante en aspecto, la Luna, la superficie de Mercurio probablemente ha incurrido en los efectos de procesos de desgaste espaciales, o erosión espacial. El viento solar e impactos de micrometeoritos pueden oscurecer la superficie cambiando las propiedades reflectantes de ésta y el albedo general de todo el planeta.

A pesar de las temperaturas extremadamente altas que hay generalmente en su superficie, observaciones más detalladas sugieren la existencia de hielo en Mercurio. El fondo de varios cráteres muy profundos y oscuros cercanos a los polos que nunca han quedado expuestos directamente a la luz solar tienen una temperatura muy inferior a la media global. El hielo (de agua) es extremadamente reflectante al radar, y recientes observaciones revelan imágenes muy reflectantes en el radar cerca de los polos; el hielo no es la única causa posible de dichas regiones altamente reflectantes, pero sí la más probable. Se especula que el hielo tiene sólo unos metros de profundidad en estos cráteres, conteniendo alrededor de una tonelada de esta sustancia. El origen del agua helada en Mercurio no es conocido a ciencia cierta, pero se especula que o bien se congeló de agua del interior del planeta o vino de cometas que impactaron contra el suelo.

El estudio de la interacción de Mercurio con el viento solar ha puesto en evidencia la existencia de una magnetosfera en torno al planeta. El origen de este campo magnético no es conocido. En 2007 observaciones muy precisas realizadas desde la Tierra mediante radar, demostraron un bamboleo del eje de rotación compatible sólo con un núcleo del planeta parcialmente fundido. Un núcleo parcialmente fundido con materiales ferromagnéticos podría ser la causa de su campo magnético.

La intensidad del campo magnético es de 220nT.

La órbita de Mercurio es la más excéntrica de las de los planetas menores, con la distancia del planeta al Sol en un rango entre 46 y 70 millones de kilómetros. Tarda 88 días terrestres en dar una traslación completa. Presenta además una inclinación orbital (con respecto al plano de la eclíptica) de 7°.

En la imagen anexa se ilustran los efectos de la excentricidad, mostrando la órbita de Mercurio sobre una órbita circular que tiene el mismo semieje. La elevada velocidad del planeta cuando está cerca del perihelio hace que cubra esta mayor distancia en un intervalo de sólo cinco días. El tamaño de las esferas, inversamente proporcional a la distancia al Sol, es usado para ilustrar la distancia variable heliocéntrica. Esta distancia variable al Sol, combinada con la rotación planetaria de Mercurio de 3:2 alrededor de su eje, resulta en complejas variaciones de la temperatura de su superficie, pasando de los -185°C durante las noches hasta los 430 °C durante el día.

La oblicuidad de la eclíptica es de solo 0,01° (grados sexagesimales), unas 300 veces menos que la de Júpiter, que es el segundo planeta en esta estadística, con 3,1° (en la Tierra es de 23,5°). De esta forma, un observador en el ecuador de Mercurio durante el mediodía local nunca vería el Sol más que 0.01° al norte o al sur del cenit. Análogamente, en los polos el Sol nunca pasa 0.01° por encima del horizonte.

En Mercurio existe el fenómeno de los amaneceres dobles, donde el Sol sale, se detiene, se esconde nuevamente casi exactamente por donde salió y luego vuelve a salir para continuar su recorrido por el cielo; esto solo ocurre en algunos puntos de la superficie: por el mismo procedimiento, en el resto del planeta se observa que el Sol aparentemente se detenga en el cielo y realice un movimiento de giro. Esto se debe a que aproximadamente cuatro días antes del perihelio, la velocidad angular orbital de Mercurio iguala a su velocidad angular rotatoria, lo que hace que el movimiento aparente del Sol cese; justo en el perihelio, la velocidad angular orbital de Mercurio excede la velocidad angular rotatoria. De esta forma se explica este movimiento aparentemente retrógrado del Sol. Cuatro días después del perihelio, el Sol vuelve a tomar un movimiento aparentemente normal, pasando por estos puntos.

El avance del perihelio de Mercurio fue notado en el siglo XIX por la lenta precesión de la órbita del planeta alrededor del Sol, la cual no se explicaba completamente por las leyes de Newton ni por perturbaciones por planetas conocidos (trabajo muy notable del matemático francés Urbain Le Verrier). Se supuso entonces que otro planeta en una órbita más interior al Sol era el causante de estas perturbaciones (se consideraron otras teorías como un leve achatamiento de los polos solares). El éxito de la búsqueda de Neptuno a consecuencia de las perturbaciones orbitales de Urano hicieron poner mucha fe a los astrónomos para esta hipótesis. Este planeta desconocido se le denominaría planeta Vulcano. Sin embargo, a comienzos del siglo XX, la Teoría General de la Relatividad de Albert Einstein explicaba la precesión observada, descartando al inexistente planeta (véase órbita planetaria relativista). El efecto en el avance del perihelio mercuriano es muy pequeño: apenas de 42,98 arcosegundos por siglo, por lo que necesita 12 millones de órbitas para exceder una vuelta completa. Similar, pero con efectos mucho menores, a lo que opera para otros planetas, siendo 8,52 arcosegundos por siglo para Venus, 3,84 para la Tierra, 1,35 para Marte, y 10,05 para el asteroide Apolo (1566) Ícaro.
Durante muchos años se pensó que la misma cara de Mercurio miraba siempre hacia el Sol, de forma sincrónica, similar a como lo hace la Luna respecto a la Tierra. No fue hasta 1965 cuando observaciones por radio (ver Observación con Grandes Telescopios) descubrieron una resonancia orbital de 2:3, rotando tres veces cada dos años mercurianos; la excentricidad de la órbita de Mercurio hace esta resonancia estable en el perihelio, cuando la marea solar es más fuerte, el Sol está todavía en el cielo de Mercurio. La razón por la que los astrónomos pensaban que Mercurio giraba de manera sincrónica era que siempre que el planeta estaba en mejor posición para su observación, mostraba la misma cara. Ya que Mercurio gira en un 3:2 de resonancia orbital, un día solar (la duración entre dos tránsitos meridianos del Sol) son unos 176 días terrestres. Un día sideral es de unos 58,6 días terrestres.

Simulaciones orbitales indican que la excentricidad de la órbita de Mercurio varía caóticamente desde 0 (circular) a 0,47 a lo largo de millones de años. Esto da una idea para explicar la resonancia orbital mercuriana de 2:3, cuando lo más usual es 1:1, ya que esto es más razonable para un periodo con una excentricidad tan alta.

La magnitud aparente de Mercurio varía entre -2,0 (brillante como la estrella Sirio) y 5,5. La observación de Mercurio es complicada por su proximidad al Sol, perdido en el resplandor de la estrella madre durante un período muy grande. Mercurio solo se puede observar por un corto período durante el crepúsculo de la mañana o de la noche. El Telescopio Espacial Hubble no puede observar Mercurio, ya que por procedimientos de seguridad se evita un enfoque tan cercano al Sol.

Como la Luna, Mercurio exhibe fases vistas desde la Tierra, siendo "nueva" en conjunción inferior y "llena" en conjunción superior. El planeta deja de ser invisible en ambas ocasiones por la virtud de este ascenso y ubicación acuerdo con el Sol en cada caso. La primera y última fase ocurre en máxima elongación este y oeste, respectivamente, cuando la separación de Mercurio del rango del Sol es de 18,5° en el periastro y 28,3 en el apoastro. En máxima elongación oeste, Mercurio se eleva antes que el Sol y en la este después que el Sol.

Mercurio alcanza una conjunción inferior cada 116 días de media, pero este intervalo puede cambiar de 111 a 121 días por la excentricidad de la órbita del planeta. Este periodo de movimiento retrógrado visto desde la Tierra puede variar de 8 a 15 días en cualquier lado de la conjunción inferior. Esta larga variación de tiempo es consecuencia también de la elevada excentricidad orbital.

Mercurio es más fácil de ver desde el hemisferio sur de la Tierra que desde el hemisferio norte; esto se debe a que la máxima elongación del oeste posible de Mercurio siempre ocurre cuando es otoño en el hemisferio sur, mientras que la máxima elongación del este ocurre cuando es invierno en el hemisferio norte. En ambos casos, el ángulo de Mercurio incide de manera máxima con la eclíptica, permitiendo elevarse varias horas antes que el Sol y no se pone hasta varias horas después del ocaso en los países situados en latitudes templadas del hemisferio sur, como Chile, Argentina y Nueva Zelanda. Por contraste, en las latitudes templadas del hemisferio norte, Mercurio nunca está por encima del horizonte en más o menos a media noche. Como muchos otros planetas y estrellas brillantes, Mercurio puede ser visto durante un eclipse solar.
Además, Mercurio es más brillante visto desde la Tierra cuando se encuentra entre la fase creciente o la menguante y la llena. Aunque el planeta está más lejos en ese momento que cuando está creciente, el área iluminada visible mayor compensa esa mayor distancia. Justo al contrario que Venus, que aparece más brillante cuando está en cuarto creciente, porque está mucho más cerca de la Tierra.

El tránsito de Mercurio es el paso, observado desde la Tierra, de este planeta por delante del Sol. La alineación de estos tres astros (Sol, Mercurio y la Tierra) produce este particular efecto, sólo comparable con el tránsito de Venus. El hecho de que Mercurio esté en un plano diferente en la eclíptica que nuestro planeta (7° de diferencia) hace que sólo una vez cada varios años ocurra este fenómeno. Para que el tránsito se produzca, es necesario que la Tierra esté cerca de los nodos de la órbita. La Tierra atraviesa cada año la línea de los nodos de la órbita de Mercurio el 8-9 de mayo y el 10-11 de noviembre; si para esa fecha coincide una conjunción inferior habrá paso. Existe una cierta periodicidad en estos fenómenos aunque obedece a reglas complejas. Es claro que tiene que ser múltiplo del periodo sinódico. Mercurio suele transitar el disco solar un promedio de unas 13 veces al siglo en intervalos de 3, 7, 10 y 13 años.

Las primeras menciones conocidas de Mercurio, hechas por los sumerios, datan del tercer milenio a. C. Los babilonios (2000-500 a. C.) hicieron igualmente nuevas observaciones sobre el planeta, denominándolo como Nabu o Nebu, el mensajero de los dioses en su mitología.

Los observadores de la Antigua Grecia llamaron al planeta de dos maneras: "Apolo" cuando era visible en el cielo de la mañana y "Hermes" cuando lo era al anochecer. Sin embargo, los astrónomos griegos se dieron cuenta que se referían al mismo cuerpo celeste, siendo Pitágoras el primero en proponer la idea.

Las primeras observaciones con telescopio de Mercurio datan de Galileo en el siglo XVII. Aunque él observara las fases planetarias cuando miraba a Venus, su telescopio no era lo suficientemente potente para distinguir las fases de Mercurio. En 1631 Pierre Gassendi realizó las primeras observaciones del tránsito de Mercurio cruzando el Sol cuando vio el tránsito de Mercurio predicho por Johannes Kepler. En 1639 Giovanni Zupi usó un telescopio para descubrir que el planeta tenía una fase orbital similar a la de Venus y la Luna. La observación demostró de manera concluyente que Mercurio orbitaba alrededor del Sol.

Un hecho extraño en la astronomía es que un planeta pase delante de otro (ocultación), visto desde la Tierra. Mercurio y Venus se ocultan cada varios siglos, y el 28 de mayo de 1737 ocurrió el único e histórico registrado. El astrónomo que lo observó fue John Bevis en el Real Observatorio de Greenwich. La próxima ocultación ocurrirá en 2133.

En 1800 Johann Schröter pudo hacer algunas observaciones de la superficie, pero erróneamente estimó que el planeta tenía un periodo de rotación similar a la terrestre, de unas 24 horas. En la década de 1880 Giovanni Schiaparelli realizó un mapa de Mercurio más correcto, y sugirió que su rotación era de 88 días, igual que su período de traslación (Rotación síncrona).

La teoría por la cual la rotación de Mercurio era sincrónica se hizo extensamente establecida, y fue un giro de 180° cuando los astrónomos mediante observaciones de radio en los años 1960 cuestionaron la teoría. Si la misma cara de Mercurio estuviera dirigida siempre hacia el Sol, la parte en sombra estaría extremadamente fría, pero las mediciones de radio revelaron que estaba mucho más caliente de lo esperado. En 1965 se constató que definitivamente el periodo de rotación era de 59 días. El astrónomo italiano Giuseppe Colombo notó que este valor era sobre dos terceras partes del período orbital de Mercurio, y propuso una forma diferente de la fuerza de marea que hizo que los períodos orbitales y rotatorios del planeta se quedasen en 3:2 más bien que en 1:1 (resonancia orbital). Más tarde la "Mariner 10" lo confirmó.

Las observaciones por grandes telescopios en tierra no arrojaron mucha luz sobre este mundo difícil de ver, y no fue hasta la llegada de sondas espaciales que visitaron Mercurio cuando se descubrieron y confirmaron grandes e importantes propiedades del planeta. No obstante, recientes avances tecnológicos han llevado a observaciones mejoradas: en 2000, el telescopio de alta resolución del Observatorio Monte Wilson de 1500 mm proporcionó las primeras imágenes que resolvieron algunos rasgos superficiales sobre las regiones de Mercurio que no fueron fotografiadas durante las misiones del Mariner. Imágenes recientes apuntan al descubrimiento de una cuenca de impacto de doble anillo más largo que la "Cuenca de Caloris", en el hemisferio no fotografiado por la Mariner. Es informalmente conocido como "Cuenca de Shinakas".

Llegar hasta Mercurio desde la Tierra supone un significativo reto tecnológico, ya que la órbita del planeta está mucho más cerca que la terrestre del Sol. Una nave espacial con destino a Mercurio lanzada desde nuestro planeta deberá de recorrer unos 91 millones de kilómetros por los puntos de potencial gravitatorio del Sol. Comenzando desde la órbita terrestre a unos 30 km/s, el cambio de velocidad que la nave debe realizar para entrar en una órbita de transferencia, conocida como órbita de transferencia de Hohmann (en la que se usan dos impulsos del motor cohete) para pasar cerca de Mercurio es muy grande comparado con otras misiones planetarias.

Además, para conseguir entrar en una órbita estable el vehículo espacial debe confiar plenamente en sus motores de propulsión, puesto que el aerofrenado está descartado por la falta de atmósfera significativa en Mercurio. Un viaje a este planeta en realidad es más costoso en lo que a combustible se refiere por este hecho que hacia cualquier otro planeta del sistema solar.

La sonda "Mariner 10" (1974-1975), o "Mariner X", fue la primera nave en estudiar en profundidad el planeta Mercurio. Había visitado también Venus, utilizando la asistencia de trayectoria gravitacional de Venus para acelerar hacia el planeta.

Realizó tres sobrevuelos a Mercurio; el primero a una distancia de 703 km del planeta, el segundo a 48.069 km, y el tercero a 327 km. Mariner tomó en total diez mil imágenes de gran parte de la superficie del planeta. La misión finalizó el 24 de marzo de 1975 cuando se quedó sin combustible y no podía mantener control de orientación.

"MErcury Surface, Space ENvironment, GEochemistry and Ranging" (Superficie de Mercurio, Entorno Espacial, Geoquímica y Extensión) fue una sonda lanzada en agosto de 2004 para ponerse en órbita alrededor de Mercurio en marzo de 2011. Se esperaba que esta nave aumentara considerablemente el conocimiento científico sobre este planeta. Para ello, la nave había de orbitar Mercurio y hacer tres sobrevuelos –los días 14 de enero de 2008, 6 de octubre de 2008, y 29 de septiembre de 2009–. La misión estaba previsto que durase un año. El 18 de marzo de 2011 se produjo con éxito la inserción orbital de la sonda. Finalmente el fin de esta exitosa misión se produjo el 30 de abril de 2015, cuando la sonda se precipitó sobre la superficie del planeta produciéndose un impacto controlado.

Es una misión conjunta de la Agencia Espacial Europea (ESA) y de la Agencia Japonesa de Exploración Espacial (JAXA), que consiste en dos módulos orbitantes u orbitadores que realizarán una completa exploración de Mercurio. El primero de los orbitadores será el encargado de fotografiar y analizar el planeta y el segundo investigará la magnetosfera. Su lanzamiento está previsto en julio de 2018, la llegada al planeta en enero de 2024, y el final de la misión para un año más tarde, con una posible extensión de un año más.





</doc>
<doc id="3362" url="https://es.wikipedia.org/wiki?curid=3362" title="Mercurio (elemento)">
Mercurio (elemento)

El mercurio es un elemento químico con el símbolo Hg y número atómico 80. En la literatura antigua era designado comúnmente como plata líquida y también como azogue o hidrargiro. Elemento de aspecto plateado, metal pesado perteneciente al bloque D de la tabla periódica, el mercurio es el único elemento metálico que es líquido en condiciones estándar de laboratorio; el único otro elemento que es líquido bajo estas condiciones es el bromo (un no metal), aunque otros metales como el cesio, el galio, y el rubidio se funden a temperaturas ligeramente superiores.

El mercurio aparece en depósitos en todo el mundo, principalmente como cinabrio (sulfuro de mercurio). El pigmento rojo denominado bermellón se obtiene triturando cinabrio natural o sulfuro de mercurio obtenido por síntesis.

El mercurio se usa en termómetros, barómetros, manómetros, esfigmómetros, algunos tipos de válvulas como las de las bombas de vacío, interruptores de mercurio, lámparas fluorescentes y otros dispositivos, a pesar de que la preocupación sobre la toxicidad del elemento ha llevado a los termómetros y tensiómetros de mercurio a ser eliminados en gran medida en entornos clínicos en favor de otras alternativas, como los termómetros de vidrio que utilizan alcohol o galinstano, los termistores o los instrumentos electrónicos basados en la medición de la radiación infrarroja. Del mismo modo, manómetros mecánicos y sensores de calibradores de tensión electrónicos han sustituido a los esfigmomanómetros de mercurio. El mercurio se mantiene en uso en aplicaciones de investigación científica y en amalgamas odontológicas, todavía utilizadas en algunos países. También se utiliza en las luces fluorescentes, en las que la electricidad que atraviesa una lámpara conteniendo vapor de mercurio a baja presión produce radiación ultravioleta de onda corta, que a su vez provoca la fluorescencia del fósforo que recubre el tubo, produciendo luz visible.

El envenenamiento por mercurio puede resultar de la exposición a las formas solubles en agua del mercurio (como el cloruro mercúrico o el metilmercurio), por la inhalación de vapor de mercurio, o por la ingestión de cualquiera de sus formas.
El mercurio es un metal pesado plateado que a temperatura ambiente es un líquido inodoro. No es buen conductor del calor comparado con otros metales, aunque es buen conductor de la electricidad. Se alea fácilmente con muchos otros metales como el oro o la plata produciendo amalgamas, pero no con el hierro. Es insoluble en agua y soluble en ácido nítrico. Cuando aumenta su temperatura -por encima de los 40 °C-, produce vapores tóxicos y corrosivos, más pesados que el aire por lo que se evapora creando miles de partículas en el vapor que al enfriarse se depositan de nuevo. Es dañino por inhalación, ingestión y contacto: se trata de un producto muy irritante para la piel, ojos y vías respiratorias. Es incompatible con el ácido nítrico concentrado, el acetileno, el amoníaco, el cloro y los metales.

El mercurio es un elemento anómalo en varias de sus propiedades. Es un metal noble, ya que su potencial redox Hg/Hg es positivo (+0,85 V), frente al negativo del cadmio Cd (-0,40 V), su vecino inmediato de grupo. Es un metal singular con algo de parecido al cadmio, pero más semejante al oro y al talio. Es el único metal de transición líquido con una densidad tan elevada, 13,53 g/cm³; una columna de 76 cm define una atmósfera, mientras que con agua se necesita una columna de 10 m de altura. Su estado líquido en condiciones estándar indica que su enlace metálico es débil y se justifica por la poca participación de los electrones 6s² a la deslocalización electrónica en el sistema metálico (efectos relativistas).

Tiene la primera energía de ionización más alta de todos los metales (10,4375 eV) por la misma razón anterior. Además el Hg tiene muy baja entalpía de hidratación comparada con la del cinc Zn y la del cadmio Cd, con preferencia por la coordinación dos en los complejos de Hg (II), como el oro Au (I) isoelectrónico. Esto trae como consecuencia que los potenciales redox de aquellos sean negativos y el del mercurio sea noble (positivo).

La poca reactividad del mercurio en procesos oxidativos se justifica por los efectos relativistas sobre los electrones 6s² muy contraídos hacia el núcleo y por la fortaleza de su estructura electrónica de pseudogas noble.

También es el único elemento del grupo que presenta el estado +I, en forma de especie dinuclear Hg, aunque la tendencia general a estabilizar los estados de oxidación bajos sea la contraria en los grupos de transición: formación de compuestos de Hg (I) con racimos de pares Hg-Hg.
Esta rica covalencia también se puede apreciar en compuestos de Hg (II), donde se tiene que muchos de estos compuestos de Hg (II) son volátiles como el HgCl, sólido molecular con entidades Cl-Hg-Cl en sólido, vapor e incluso en disolución acuosa. También es destacable la resistencia de amidas, imidas y organometálicos de mercurio a la hidrólisis y al oxígeno del ambiente, lo que indica la gran fortaleza del enlace con el carbono Hg-C. También el azufre S y el fósforo P son átomos dadores adecuados: ligandos blandos efectivos para ácidos blandos como el Hg en estados de oxidación cero, I y II.

El estado de oxidación más alto del mercurio es el II debido a su configuración electrónica externa ds², y a que la suma de sus tres primeras energías de ionización es demasiado alta para que en condiciones estándar se generen estados de oxidación III o superiores. Sin embargo, en 2007 se ha descubierto que a bajísimas temperaturas, del orden de -260 °C (esto es, la temperatura media del espacio), existe en estado de oxidación IV, pudiendo asociarse con cuatro átomos de flúor y obteniéndose de tal modo ese grado de oxidación adicional. A esta forma se la denomina tetrafluoruro de mercurio (HgF); la estructura es plano-cuadrada, la de mayor estabilidad para una especie d procedente de un metal "5d". Este comportamiento es esperable, dado que el mercurio tiene mayor expansión relativista de sus orbitales 5d con relación a sus homólogos del grupo 12, con lo que frente al flúor, el elemento más oxidante de la tabla periódica, puede en condiciones extremas generar enlaces covalentes. La posibilidad de sintetizar este fluoruro de mercurio, HgF, fue predicha teóricamente en 1994 de acuerdo a modelos antes indicados. Por la misma razón se puede considerar la posibilidad del estado de oxidación III para este metal, y efectivamente se ha aislado una especie compleja, en un medio especial y por oxidación electroquímica, donde aparece el catión complejo,[Hg cyclam]; el cyclam es un ligando quelato que estabiliza al mercurio en este estado de oxidación raro (1,4,8,11-Tetraazaciclotetradecane= cyclam). Con todo esto, se debe concluir que el mercurio debe ser reconsiderado para ser incluido como un metal de transición, ya que genera especies con orbitales d internos que están vacíos, por lo que se tiene una energía favorable de "estabilización por el campo de los ligandos" (EECL).

El mercurio es un metal blanco plateado y pesado. En comparación con otros metales, es un mal conductor del calor, pero un buen conductor de la electricidad. Presenta un punto de solidificación de -38,83 °C y un punto de ebullición de 356,73 °C, ambos excepcionalmente bajos para un metal. Además, el punto de ebullición del mercurio de es el más bajo de cualquier metal. Una explicación completa de este hecho se adentra profundamente en el reino de la física cuántica, pero se puede resumir de la siguiente manera: el mercurio tiene una configuración electrónica única, en la que los electrones recubren todos los niveles disponibles 1s, 2s, 2p, 3s, 3p, 3d, 4s, 4p, 4d, 4f, 5s, 5p, 5d y 6s. Debido a que esta configuración resiste considerablemente a la liberación de un electrón, el mercurio se comporta de manera similar a los gases nobles, que forman enlaces débiles y por lo tanto se funden a bajas temperaturas. Tras la congelación, el volumen del mercurio disminuye en un 3,59% y su densidad cambia de 13,69 g/cm en estado líquido a 14,184 g/cm cuando se solidifica. El coeficiente de expansión volumétrico es de 181,59x10 a 0 °C, 181,71x10 a 20 °C y de 182,50x10 a 100 °C (por cada °C).

La estabilidad del orbital 6s es debida a la presencia del nivel 4f repleto. La capa f apantalla débilmente la carga nuclear efectiva, lo que aumenta la atracción debida a la fuerza de Coulomb entre el nivel 6s y el núcleo (ver "contracción lantánida"). La ausencia de un nivel interior "f" repleto es la razón de la temperatura de fusión algo más alta del cadmio y del zinc, aunque estos dos metales también funden fácilmente y, además, presentan puntos de ebullición inusualmente bajos.

Por otro lado, el oro, que ocupa un espacio a la izquierda del mercurio en la tabla periódica, tiene átomos con un electrón menos en la capa 6s que el mercurio. Esos electrones se liberan con mayor facilidad y son compartidos entre los átomos de oro, que forman un relativamente fuerte enlace metálico.

El mercurio no reacciona con la mayoría de los ácidos, tales como el ácido sulfúrico diluido, aunque los ácidos oxidantes como el ácido sulfúrico concentrado y el ácido nítrico o el agua regia lo disuelven para dar sulfato, nitrato, y cloruro. Como la plata, el mercurio reacciona con el ácido sulfhídrico atmosférico. Así mismo, reacciona con copos de azufre sólido, que se utilizan en los equipos para absorber el mercurio en caso de derrame (también se utilizan con este mismo propósito carbón activado y zinc en polvo).

El mercurio disuelve muchos otros metales como el oro y la plata para formar amalgamas. El hierro es una excepción, por lo que recipientes de hierro se han utilizado tradicionalmente para el comercio de mercurio. Varios otros elementos de la primera fila de los metales de transición (con la excepción del manganeso, el cobre y el zinc) son reacios a formar amalgamas. Otros elementos que no forman fácilmente amalgamas con el mercurio incluyen al platino. La "amalgama de sodio" es un agente reductor común en síntesis orgánica, y también se utiliza en las lámparas de lámparas de vapor de sodio de alta presión.

El mercurio se combina fácilmente con el aluminio para formar una "amalgama de aluminio" cuando los dos metales puros entran en contacto. Esta amalgama destruye la capa de óxido de aluminio que protege al aluminio metálico de oxidarse en profundidad (como le sucede al hierro ante el agua). Incluso pequeñas cantidades de mercurio pueden corroer gravemente el aluminio. Por esta razón, el mercurio no se permite a bordo de una aeronave bajo la mayoría de las circunstancias, debido al riesgo de la formación de una amalgama con partes de aluminio expuestas en la aeronave.

El ataque del mercurio sobre el aluminio es uno de los tipos más comunes de "fragilización por metal líquido".

Hay siete isotopos estables del mercurio, con siendo el más abundante (29,86%). Los radioisótopos más longevos son con un período de semidesintregración de 444 años, y con una vida media de 46,612 días. La mayor parte de los radioisótopos restantes tienen vidas medias que son de menos de un día. y son los núcleos activos más a menudo estudiados mediante resonancia magnética nuclear, teniendo espines de y respectivamente.

Hg es el símbolo químico moderno para representar abreviadamente al mercurio. Proviene de "hydrargyrum", una forma latinizada del término griego ὑδράργυρος ("hydrargyros"), que es una palabra compuesta que significa "agua-plata" (de ὑδρ- " hydr- ", la raíz de ὕδωρ, "agua", y ἄργυρος "argyros" "plata"), ya que es líquido como el agua y brillante como la plata. Comparte el nombre con el dios romano Mercurio, conocido por su velocidad y movilidad. Por el mismo motivo, también se asocia con el planeta Mercurio. El símbolo astrológico del planeta es asimismo el símbolo alquímico del metal; la palabra sánscrita para la alquimia es "Rasavātam", que significa literalmente "el camino de mercurio". El mercurio es el único metal para el que su nombre planetario alquímico se convirtió en su nombre común.

El mercurio se encuentra en tumbas del Antiguo Egipto que datan del 1500 a. C.

En China y el Tibet, el uso del mercurio era recomendado para prolongar la vida, curar fracturas y conservar la buena salud en general, aunque ahora se sabe que la exposición a los vapores de mercurio conduce a graves efectos adversos sobre la salud. El primer emperador de China, Qin Shi Huang (supuestamente enterrado en el denominado "Mausoleo de Qin Shi Huang", que contenía ríos de mercurio que fluyen reproduciendo un modelo de la tierra gobernada en el que se representaban los ríos de China) murió por beber una mezcla de mercurio y de jade en polvo recetado por los alquimistas de la Dinastía Qin (causándole fallo hepático, envenenamiento por mercurio y muerte cerebral) que pretendía darle vida eterna. Khumarawayh ibn Ahmad ibn Tulun, el segundo gobernante de Egipto tuluní (r. 884-896), conocido por su extravagancia y despilfarro según las crónicas de la época, construyó un recipiente lleno de mercurio, en el que se tendía sobre la parte superior de cojines llenos de aire y se balanceaba para dormir.

En noviembre de 2014, se descubrieron "grandes cantidades" de mercurio en una cámara de 18,2 m debajo del templo de 1800 años de antigüedad conocido como la "Pirámide de la Serpiente Emplumada", "la tercera pirámide más grande de Teotihuacán", México, junto con "estatuas de jade, jaguares vigilantes, una caja llena de conchas talladas y pelotas de goma".

En la Antigua Grecia se usaba el mercurio en ungüentos; las egipcias y romanas lo utilizaban en cosméticos. En Lamanai, una ciudad importante de la civilización maya, se encontró una balsa de mercurio bajo un marcador en una pista de juego de pelota. Hacia el año 500 el mercurio se utilizaba para hacer amalgamas (del latín medieval "amalgama", "aleación de mercurio") con otros metales.

Los alquimistas pensaron en el mercurio como la materia prima, a partir de la cual se formaron todos los metales. Creían que diferentes metales podrían ser producidos haciendo variar la calidad y cantidad de azufre contenido dentro del mercurio. El más puro de éstos era el oro, y el mercurio se usaba en los intentos de transmutación de los metales de base (o impuros) en oro, que era el objetivo de muchos alquimistas.

Las minas de Almadén (España), Monte Amiata (Italia) e Idrija (ahora Eslovenia) dominaron la producción de mercurio a partir de la apertura de la mina de Almadén hace 2500 años, hasta que aparecieron nuevos depósitos al final del siglo XIX.

El mercurio es un elemento extremadamente raro en la corteza terrestre, que tiene una abundancia media en peso de tan solo 0,08 partes por millón. Debido a que no se mezcla geoquímicamente con aquellos elementos que constituyen la mayoría de la masa de la corteza terrestre, los minerales de mercurio se encuentran extraordinariamente concentrados teniendo en cuenta la abundancia del elemento en la roca ordinaria. Los minerales más ricos de mercurio contienen hasta un 2,5% de mercurio en peso, e incluso los depósitos de concentrados más pobres contienen al menos el 0,1% de mercurio (12.000 veces la abundancia media en la corteza terrestre). Se encuentra ya sea como metal nativo (raro) o en forma de cinabrio, corderoíta, livingstonita y otros minerales, con el cinabrio (HgS) siendo la mena más abundante. Los yacimientos de mercurio se hallan por lo general en zonas de orogénesis reciente, donde las rocas de alta densidad se ven obligadas a surgir a la corteza de la Tierra, impulsadas por aguas termales o por la actividad de determinadas regiones volcánicas.
A partir de 1558, con la invención del proceso de patio para extraer la plata a partir de sus menas usando mercurio, este metal se convirtió en un recurso esencial en la economía de España y de sus colonias americanas. El mercurio se utilizó para extraer plata en las lucrativas minas de Nueva España y Perú. Inicialmente, las minas de la Corona Española en Almadén (localizadas en el sur del centro de España) suministraban todo el mercurio necesario en las colonias, hasta que fueron descubiertos nuevos yacimientos en el Nuevo Mundo, y más de 100.000 toneladas de mercurio fueron extraídos de la región de Huancavelica, Perú (especialmente de la mina Santa Bárbara), a lo largo de los tres siglos posteriores al descubrimiento de sus yacimientos en 1563. El proceso de patio primero y después el de pan amalgamación crearon una gran demanda de mercurio para el tratamiento de minerales de plata hasta finales del siglo XIX.

Antiguas minas en Italia, Estados Unidos y México, que una vez produjeron una gran proporción de la oferta mundial, han sido completamente agotadas o, en el caso de Eslovenia (Idrija) y de España (Almadén), debieron cerrar debido a la caída del precio del mercurio. La mina de McDermitt en el estado de Nevada, la última explotación de mercurio en los Estados Unidos, se cerró en 1992. El precio del mercurio ha sido muy volátil en los últimos años y en 2006 era de $650 por cada vasija de 76 libras (34,46 kg).

El mercurio se extrae por calentamiento del cinabrio en una corriente de aire y condensando el vapor. La ecuación para esta extracción es:

En 2005, China fue el principal productor de mercurio con casi dos tercios de la cuota mundial, seguida del Kirguistán. Se cree que otros países mantienen una producción no registrada de mercurio derivada de procesos de electrodeposición del cobre y por la recuperación de los efluentes.

Debido a la alta toxicidad del mercurio, tanto la extracción del cinabrio como el refinado del mercurio son causas peligrosas e históricas de envenenamiento. En China, el trabajo penitenciario fue utilizado por una empresa minera privada, en épocas tan recientes como la década de 1950, para explotar nuevas minas de cinabrio. Miles de prisioneros fueron utilizados por la empresa minera Luo Xi para excavar nuevas galerías. La salud de los mineros de las minas en explotación corre un alto riesgo.

La directiva de la Unión Europea disponiendo el uso obligatorio de lámparas fluorescentes compactas a partir del año 2012 ha alentado a China a reabrir sus minas de cinabrio para obtener el mercurio necesario para la fabricación de este tipo de bombillas. Los peligros ambientales han sido una preocupación, en particular en las ciudades sureñas de Foshan y Guangzhou, y en la provincia Guizhou, situada en el sudoeste del país.

Las plantas de procesamiento de las minas de mercurio abandonadas a menudo contienen acumulaciones de desechos muy peligrosas de cinabrio calcinado. El agua de escorrentía en estos lugares es una fuente reconocida de daños ecológicos. Antiguas minas de mercurio pueden ser adecuadas para la reutilización constructiva. Por ejemplo, en 1976 el Condado de Santa Clara (California) compró la histórica Mina "Almaden Quicksilver" y creó un parque del Condado, después de realizar un exhaustivo estudio de seguridad y un análisis ambiental de la propiedad.

La producción mundial de mercurio experimentó históricamente un crecimiento continuado (primero ligado principalmente a la minería del oro y de la plata en el Nuevo Mundo, y a partir de comienzos del siglo XX también relacionado con la producción industrial de cloro), con un progresivo descenso a partir de la década de 1980, cuando se empezaron a hacer patentes los riesgos ambientales que entraña su utilización indiscriminada. Por ejemplo, en la década de 1970 se estimaba que las minas de Almadén habían producido unas 200.000 tm de mercurio a lo largo de toda su vida útil, y que todavía albergaban otras 200.000 tm en su interior. Desde el final de la Segunda Guerra Mundial hasta la década de 1970, la producción mundial anual pasó de las 3.200 tm en 1948 a las 8.650 tm en 1965, estabilizándose durante una década en las 9.000-10.000 tm anuales.

A partir de la década de 1990, tanto por motivos económicos (el descenso del precio del metal obligó a cerrar muchas de las principales minas de los países occidentales, que ya no eran rentables), como ambientales (la producción se concentró en los países con menos restricciones legales en relación con el medio ambiente), China ha copado el mercado mundial, con más del 80% de la producción total en los primeros años del siglo XXI.

La producción mundial en el año 2013 fue del orden de 1.900 toneladas (prácticamente la quinta parte de su máximo histórico, registrado como ya se ha señalado en la década de 1970), con China en un destacado primer lugar (véase la tabla adjunta), siendo Kirguistán y Chile segundo y tercer productores, con porcentajes mucho menores.

El mercurio existe en dos estados de oxidación principales, I y II. Los estados de oxidación más altos son poco frecuentes (por ejemplo, el fluoruro de mercurio (IV), ), y sólo se han detectado bajo condiciones extraordinarias.

A diferencia de sus vecinos más ligeros, cadmio y zinc, el mercurio suele formar compuestos estables con simples enlaces metal-metal. La mayoría de los compuestos de mercurio (I) son diamagnéticos y cuentan con el catión dimérico, Hg. Los derivados estables incluyen el cloruro y el nitrato. El tratamiento de los compuestos complejos de Hg(I) con ligandos fuertes tales como sulfuro o cianuro, induce una desproporción a y la formación de mercurio elemental. El cloruro de mercurio (I), un sólido incoloro también conocido como "calomel", es realmente el compuesto con la fórmula HgCl, con la estructura Cl-Hg-Hg-Cl, un estándar en electroquímica que reacciona con el cloro para dar cloruro mercúrico HgCl, que se opone a la oxidación adicional. El hidruro de mercurio (I), un gas incoloro, tiene la fórmula HgH, que no contiene ningún enlace Hg-Hg.

Indicativas de su tendencia a adherirse a sí mismas son las formas de policationes de mercurio, que consisten en cadenas lineales con centros de mercurio, rematadas con una carga positiva. Un ejemplo es el .

El mercurio (II) es el estado de oxidación más común y por lo tanto el más frecuente en la naturaleza. Se conocen los cuatro haluros de mercurio, que forman complejos tetraédricos con otros ligandos, pero los haluros adoptan geometría de coordinación lineal, algo así como sucede con Ag. El más conocido es el cloruro de mercurio (II), una sustancia sólida de color blanco, fácilmente sublimable. HgCl forma complejos que son típicamente tetraédricos, por ejemplo el .

El óxido de mercurio (II), el óxido principal del mercurio, se forma cuando el metal está expuesto al aire durante largos períodos de tiempo a temperaturas elevadas. Los elementos se separan de nuevo si el óxido se calienta a cerca de 400 °C, como demostró Joseph Priestley en una de las primeras síntesis de oxígeno puro. Los hidróxidos de mercurio están mal caracterizados, como sucede con sus elementos vecinos oro y plata.

Siendo un metal "blando" a efectos de pH, los derivados de mercurio forman combinaciones muy estables con los calcógenos más pesados. La forma más abundante es el sulfuro de mercurio (II), HgS, que se produce en la naturaleza como el mineral cinabrio, utilizado como pigmento rojo brillante con el nombre de bermellón. Como el ZnS, el HgS cristaliza en dos formas, la forma rojiza cúbica y la negra con una configuración similar a la de la blenda. El seleniuro de mercurio (II) (HgSe) y el telururo de mercurio (II) (HgTe) son también conocidos, así como diversos derivados, como por ejemplo el telururo de mercurio y cadmio y el telururo de mercurio y zinc, que son semiconductores útiles como materiales detectores de infrarrojos.

Las sales de mercurio (II) forman una variedad de compuestos derivados del amoníaco. Estos incluyen la "base de Millon" (HgN), un polímero unidimensional (sales de ()), y un "precipitado blanco fusible" (el [Hg(NH)]Cl). Conocido como reactivo de Nessler, el tetraiodomercurato (II) de potasio () sigue siendo en ocasiones utilizado para la prueba del amoníaco, debido a su tendencia a formar la sal yoduro de la "base de Millon", de intenso color.

El fulminato de mercurio (II) es un detonator ampliamente utilizado en explosivos.

Estados de oxidación superiores a 2 en una especie no cargada son extremadamente raros, aunque un catión cíclico de mercurio (IV), con tres sustituyentes, puede ser un intermediario en reacciones de oximercuración. En 2007 se publicó un informe en el que se daba cuenta de la síntesis de un compuesto de mercurio (IV), el fluoruro de mercurio (IV). En la década de 1970 hubo una reclamación sobre la síntesis de un compuesto de mercurio (III), pero ahora se cree que es falsa.

Los compuestos orgánicos de mercurio son históricamente importantes en el desarrollo de la química, pero en el mundo occidental son de poco valor industrial. Las sales de mercurio (II) son un raro ejemplo de complejos metálicos simples que reaccionan directamente con los anillos aromáticos. Los compuestos organomercúricos son siempre divalentes y por lo general bidimensionales y de geometría lineal. A diferencia de los compuestos organocádmicos y los organozincados, los compuestos organomercuriales no reaccionan con agua. Por lo general tienen la fórmula HgR, que son a menudo volátiles, o HgRX, que a menudo son sólidos, donde R es arilo o alquilo y X es generalmente haluro o acetato. El metilmercurio, un término genérico para los compuestos con la fórmula CHHgX, forma una familia de compuestos peligrosos que se encuentran a menudo en el agua contaminada. Surgen por un proceso conocido como biometilación.

El mercurio (II) forma complejos con ligandos dadores de nitrógeno, fósforo y azufre, pero se resiste a formar complejos con los dadores de oxígeno; también genera complejos muy estables con bromo, iodo y cloro como corresponde a un catión blando. La estabilidad de los complejos de mercurio (II) es mayor que la de los otros dos elementos de su grupo, cinc y cadmio, porque además de enlaces σ con hibridaciones adecuadas del metal intervendrán enlaces π por la mayor expansión de los 5d del mercurio (efectos relativistas), que inyectan carga a los orbitales d vacíos de los ligandos: se creará un sistema resonante que es compatible con la asociación cuántica del subnivel lleno 5d, reforzando a la vez los enlaces M-L por retrodonación. Esto es inusual, puesto que los iones más pequeños forman normalmente los mejores complejos. No se conocen complejos con ligandos π, como CO, NO o alquenos.
Los complejos de cinc son incoloros, pero los de mercurio y en menor extensión los de cadmio, son coloreados debido a la transferencia de carga del metal al ligando (absorciones de transferencia de carga), y del ligando al metal que es más patente en el mercurio de acuerdo a lo indicado antes (expansión 5d>4d).

La mayoría de los complejos de Hg (II) son octaédricos distorsionados, con dos enlaces cortos y cuatro enlaces largos. El caso extremo de esta distorsión es la formación de solo 2 enlaces, ejemplo de esto son los compuestos Hg(CN) y Hg(SCN), y el complejo [Hg(NH)]Cl; este último contiene el ion lineal [HN-Hg-NH]. El mercurio (II) también forma complejos tetraédricos como [Hg(SCN)] y el K[HgI]. Este último es el denominado reactivo de Nessler para la determinación de amoníaco en disolución; se detectan concentraciones tan bajas como 1ppm y se forma un precipitado amarillo o marrón, [HgNI.HO] (unidades {HgN} que dan entorno tetraédrico de Hg para el N y lineal para el Hg (II), catión polimérico con estructura 3D de tipo cuprita, CuO, o bien anti-β-cristobalita.

Otros ejemplos de complejos de mercurio (II) donde se pueden apreciar diferentes entornos de coordinación, son:

El mercurio se utiliza principalmente para la fabricación de productos químicos industriales o para aplicaciones eléctricas y electrónicas. Se emplea en algunos termómetros, especialmente los que se usan para medir temperaturas elevadas. Una cantidad cada vez mayor se usa como mercurio gaseoso en lámparas fluorescentes, mientras que la mayoría de las otras aplicaciones se están eliminando lentamente debido a las regulaciones de salud y seguridad, siendo reemplazado en algunas aplicaciones por materiales menos tóxicos, pero considerablemente más caros, como la aleación Galinstano.

El mercurio y sus compuestos se han utilizado en medicina, aunque son mucho menos comunes en la actualidad de lo que lo eran antes, debido a que los efectos tóxicos del mercurio y de sus compuestos son mejor conocidos. La primera edición del Manual de Merck en 1899 incluía muchos compuestos de mercurio como medicamentos, tales como:

El mercurio es un ingrediente en amalgamas dentales. El "Thiomersal" (denominado "tiomersal" en los Estados Unidos) es un compuesto orgánico utilizado como conservador en vacunas, aunque este uso está en declive. El tiomersal se metaboliza en etilmercurio. Aunque se ha discutido ampliamente acerca de la seguridad del tiomersal ("véase: Controversia del tiomersal"), sugiriéndose que este conservante a base de mercurio podría provocar o desencadenar autismo en los niños, los estudios científicos no han mostrado evidencias que apoyen estas afirmaciones. Sin embargo, el tiomersal ha sido retirado, o reducido a pequeñas cantidades en todas las vacunas recomendadas para los niños de Estados Unidos hasta los 6 años de edad, con la excepción de la vacuna inactivada de la gripe.

Otro compuesto de mercurio, merbromina (mercurocromo), es un antiséptico tópico utilizado para pequeños cortes y raspaduras que todavía está en uso en algunos países.

El mercurio en la forma de uno de sus minerales más comunes, el cinabrio, se utiliza en diversas medicinas tradicionales, especialmente en la medicina china tradicional. Revisiones de su seguridad han encontrado que el cinabrio puede conducir al envenenamiento por mercurio significativo cuando se calienta, se consume en sobredosis, o tomado a largo plazo, y puede tener efectos adversos en dosis terapéuticas, aunque los efectos de las dosis terapéuticas suelen ser reversibles. Aunque esta forma de mercurio parece ser menos tóxica que otras formas, su uso en la medicina tradicional china aún no ha sido justificado, y la base terapéutica para su uso no está clara.

Hoy en día, el uso de mercurio en medicina ha disminuido considerablemente en todos los aspectos, especialmente en los países desarrollados. Los termómetros y los esfigmomanómetros que contienen mercurio se inventaron a principios del siglo XVIII y finales del XIX respectivamente. A principios del siglo XXI, su uso está disminuyendo y ha sido prohibido en algunos países por los propios estados y sus instituciones médicas. En 2002, el Senado de los Estados Unidos aprobó una legislación para eliminar gradualmente la venta de termómetros de mercurio. En 2003, los estados de Washington y de Maine se convirtieron en los primeros en prohibir los aparatos medidores de la presión arterial que utilizasen mercurio. Compuestos de mercurio todavía se pueden encontrar en algunos medicamentos de venta libre, incluyendo antisépticos tópicos, laxantes, pomadas para la dermatitis por pañal, colirios, y en aerosoles nasales. La FDA señala que ""sus datos son insuficientes para establecer el reconocimiento general de la seguridad y la eficacia"" de los ingredientes de mercurio en estos productos. El mercurio se sigue utilizando en algunos diuréticos, aunque ya existen sustitutos para la mayoría de usos terapéuticos.

El cloro se produce a partir del cloruro sódico (sal común, NaCl) utilizando electrólisis para separar el sodio metálico del gas cloro. Por lo general, la sal se disuelve en agua para producir una salmuera. Los subproductos de dicho proceso cloro-álcali son hidrógeno (H) e hidróxido de sodio (NaOH), lo que comúnmente se conoce como sosa cáustica. Con mucho, el mayor uso de mercurio a finales del siglo XX era en el proceso de celdas de mercurio (también denominado proceso Castner-Kellner), en el que se forma el sodio metálico como una amalgama en un cátodo hecho de mercurio. Este sodio se hace reaccionar con el agua para producir hidróxido de sodio. Muchas de las emisiones de mercurio industriales del siglo XX proceden de este proceso, aunque las plantas modernas afirmaban ser seguras en este aspecto. Después de alrededor de 1985, todas las nuevas instalaciones de producción de cloro-álcali que fueron construidas en los Estados Unidos utilizan tecnologías de ósmosis para producir cloro.

Algunos termómetros, especialmente los de altas temperaturas, contienen mercurio; aunque están desapareciendo gradualmente. En los Estados Unidos, la venta sin receta de los termómetros de mercurio está prohibida desde el año 2003.

El mercurio también se utiliza en los telescopios de espejo líquido.

Algunos telescopios de tránsito utilizan un recipiente con mercurio para formar un espejo plano y absolutamente horizontal, útil en la determinación de una referencia vertical o perpendicular absoluta. Espejos parabólicos horizontales cóncavos pueden formarse mediante la rotación de mercurio líquido en un recipiente cilíndrico: el líquido adopta de este modo forma parabólica, permitiendo la reflexión y el enfoque de la luz incidente. Estos telescopios son más baratos que los grandes telescopios de espejos convencionales hasta en un factor de 100, pero el espejo de mercurio líquido no se puede inclinar y siempre debe señalar hacia la vertical del lugar.

El mercurio líquido es una parte del popular electrodo de referencia secundaria (denominado electrodo de calomel) en electroquímica, como una alternativa al electrodo estándar de hidrógeno. El electrodo de calomelanos se utiliza para calcular el potencial del electrodo de las semiceldas. Por último, pero no menos importante, el punto triple del mercurio, -38.8344 °C, es un punto fijo utilizado como un estándar de temperatura para la Escala Internacional de Temperatura (ITS-90).

Los electrodos empleados en polarografía utilizan mercurio elemental. Este uso permite que un nuevo electrodo no contaminado esté disponible para cada medición o para cada nuevo experimento.

El mercurio gaseoso se utiliza en lámparas de vapor de mercurio, y lámparas fluorescentes y en algunos reclamos publicitarios del tipo "letrero de neón". Estas lámparas de baja presión emiten luz con líneas espectralmente muy estrechas, que se utilizan tradicionalmente en espectroscopia para la calibración de las posiciones espectrales. Se venden lámparas comerciales de calibración para este fin; analizar la luz de un fluorescente de techo en un espectrómetro es una práctica de calibración frecuente. El mercurio gaseoso también se encuentra en algunos tubos electrónicos, incluyendo ignitrones, tiratrones, y rectificadores de arco de mercurio. También se utiliza en las lámparas de atención médica especializada para el bronceado de la piel y desinfección. Se añade mercurio gaseoso a las lámparas de cátodo frío que contienen argón para aumentar la ionización y la conductividad eléctrica. Una lámpara rellenada de argón sin mercurio presentará manchas mates y dejará de iluminar correctamente. Los sistemas de iluminación que contienen mercurio pueden ser tratados térmicamente una sola vez. Cuando se añade vapor de mercurio a tubos llenos de neon, la luz producida presentará manchas rojas/azules inconsistentes, hasta que se complete el proceso térmico inicial; finalmente, se encenderá un solo color, mostrando finalmente un color azul apagado coherente.

El mercurio, en forma de tiomersal, es ampliamente utilizado en la fabricación de rímel. En 2008, Minnesota se convirtió en el primer estado en los Estados Unidos en prohibir el mercurio añadido intencionadamente en los cosméticos, lo que supone una norma más dura que la del gobierno federal.

Un estudio de la media geométrica de la concentración de mercurio en la orina, identificó una fuente no reconocida previamente de la exposición al mercurio inorgánico entre los residentes de Nueva York: los productos para el cuidado de la piel. Estudios basados en la biomonitorización de la población también mostraron que los niveles de concentración de mercurio son más altos entre los consumidores de pescado y marisco.

Un compuesto de mercurio llamado "fulminato de mercurio" se utilizaba principalmente en las cápsulas fulminantes como detonador de la carga de pólvora de los cartuchos que sirven de munición a las armas de fuego.

Muchas aplicaciones históricas hacen uso de las peculiares propiedades físicas del mercurio, sobre todo como un líquido denso y como un metal líquido:



Otras aplicaciones hacen uso de las propiedades químicas del mercurio:

El cloruro de mercurio (I) (también conocido como calomel o cloruro de mercurio) se ha utilizado en la medicina tradicional como diurético, desinfectante tópico, y laxante. El cloruro de mercurio (II) (también conocido como cloruro de mercurio o sublimado corrosivo) en tiempos se utilizó para tratar la sífilis (junto con otros compuestos de mercurio), aunque es tan tóxico que a veces los síntomas de su toxicidad se confunden con los de la sífilis que se creía tratar. También se utiliza como desinfectante. El ""Blue mass"", una pastilla o jarabe en el que el mercurio es el ingrediente principal, se recetó a lo largo del siglo XIX para numerosas enfermedades, como el estreñimiento, la depresión, la infertilidad y los dolores de cabeza. A principios del siglo XX, el mercurio se administró a niños pequeños como laxante y vermífugo, y se utilizó en polvo dental para lactantes. La merbromina, un organohaluro que contiene mercurio (a veces se vende como mercurocromo) sigue siendo ampliamente utilizado, pero ha sido prohibido en algunos países como los Estados Unidos.

El mercurio y la mayoría de sus compuestos son extremadamente tóxicos y deben ser manejados con cuidado; en los casos de derrames relacionados con el mercurio (por ejemplo, en el caso de rotura de termómetros o de tubos fluorescentes que contengan el metal o sus vapores), existen procedimientos de limpieza específicos para evitar la exposición y evitar su dispersión. Protocolos para fusionar físicamente las gotas más pequeñas depositadas sobre superficies duras para poder recogerlas con un cuentagotas, o bien para empujar suavemente el derrame hacia un recipiente desechable. Aspiradoras y escobas causan una mayor dispersión del mercurio y no deben utilizarse. Posteriormente se esparcen sobre el área afectada por el derrame escamas de azufre, zinc, o algún otro material en polvo que forme fácilmente una amalgama (aleación) con el mercurio a temperaturas ordinarias, antes de ser recogidos y depositados adecuadamente. La limpieza de superficies porosas y prendas de vestir no es eficaz para eliminar todos los rastros de mercurio y, por lo tanto, se aconseja a desechar este tipo de artículos cuando han estado expuestos a un derrame de mercurio.

El mercurio puede ser absorbido por la piel y las membranas mucosas y los vapores de mercurio puede ser inhalados accidentalmente, por lo que los contenedores de mercurio estén bien sellados para evitar derrames y evaporación. El calentamiento del mercurio o de sus compuestos, que pueden descomponerse cuando se calientan, debe llevarse a cabo con una ventilación adecuada a fin de minimizar la exposición al vapor de mercurio. Las formas más tóxicas de mercurio son sus compuestos orgánicos, como el dimetilmercurio y el metilmercurio. El mercurio puede causar tanto intoxicaciones crónicas como agudas, incluyendo el envenenamiento por mercurio.

La exposición crónica afecta principalmente al sistema nervioso central y a los riñones. La nefrotoxicidad se debe a la alta afinidad entre los iones mercúricos y los grupos sulfhidrilos (-SH) reducidos, los conjugados mercúricos con albúmina, L-cisteína, homocisteína y glutatión son las formas biológicamente importantes de Hg en circulación.

Tanto las formas orgánicas como inorgánicas del mercurio se captan, acumulan en la corteza renal, en el exterior de la médula externa, principalmente a lo largo de los tres segmentos del túbulo proximal, expresando así su toxicidad a nivel renal. Siendo las especies inorgánicas, las que poseen mayor relevancia nefrotóxica, por el contrario en el caso de las especies orgánicas se necesitan elevadas dosis y múltiples exposiciones para producir insuficiencia renal.

La parte más sensible de la nefrona a los efectos tóxicos ocasionados por estos compuestos, es el túbulo proximal, en concreto el segmento S3.

La nefrotoxicidad originada por dicho metal depende del tiempo de exposición, si la exposición es breve se produce una necrosis tubular aguda, sin embargo, si la exposición es a largo plazo, se produce glomerulonefritis.

Los índices de depósito pre-industriales de mercurio de la atmósfera pueden ser de aproximadamente 4 ng/(1 l de depósito de hielo). A pesar de que puede ser considerado un nivel natural de la exposición, las fuentes regionales o globales tienen efectos significativos. Las erupciones volcánicas pueden aumentar el nivel atmosférico entre 4 y 6 veces.

Las fuentes naturales, tales como los volcanes, son responsables de aproximadamente la mitad de las emisiones de mercurio a la atmósfera. La contaminación provocada por la actividad humana se puede dividir en los siguientes porcentajes estimados:
Los porcentajes anteriores son estimaciones de las emisiones de mercurio de origen humano a nivel mundial en el año 2000, con exclusión de la quema de biomasa, una fuente importante en algunas regiones.

La contaminación atmosférica reciente por mercurio en ambientes urbanos al aire libre se midió con valores de entre 0,01-0,02 mg/m. En 2001 se midieron y estudiaron los niveles de mercurio en 12 lugares del interior de viviendas elegidos para representar una sección transversal de las clases de construcción, la ubicación y las edades de los edificios en el área de Nueva York. Este estudio encontró concentraciones elevadas de mercurio en el interior de las viviendas significativamente más elevados que los registrados al aire libre, en un rango de entre 0,0065 y 0,523 mg/m. El promedio fue de 0,069 g/m.

El mercurio también entra en el medio ambiente a través de su eliminación inadecuada (por ejemplo, en los vertederos y en las incineradoras) de determinados productos que contienen mercurio, como: piezas de automóviles, baterías y pilas, bombillas fluorescentes, productos médicos, termómetros y termostatos. Debido a problemas de salud (véase más adelante), se está reduciendo progresivamente o eliminando el mercurio en estos productos. Por ejemplo, la cantidad de mercurio contenido en los termostatos vendidos en los Estados Unidos se redujo de 14,5 toneladas en 2004 a 3,9 toneladas en 2007.

La mayoría de los termómetros utilizan ahora alcohol tintado en lugar de mercurio, y los termómetros de la aleación galinstano son también una opción disponible. Los termómetros de mercurio se utilizan todavía de vez en cuando en el campo de la medicina, ya que son más precisos que los termómetros de alcohol, aunque frecuentemente, ambos están siendo reemplazados por los termómetros electrónicos y menos comúnmente por los ya citados termómetros de galinstano. Los termómetros de mercurio siguen siendo ampliamente utilizados para ciertas aplicaciones científicas debido a su mayor precisión y rango de trabajo.

Históricamente, una de las mayores emisiones se produjo en la planta industrial de Colex, una instalación dedicada a la separación de isótopos de litio situada en Oak Ridge, Tennessee. La planta operó en las décadas de 1950 y 1960. Los registros son incompletos y poco claro, pero las comisiones gubernamentales han estimado que se desconoce el paradero de unas novecientas toneladas de mercurio.

Un desastre industrial grave fue el vertido de compuestos de mercurio a la bahía de Minamata, en Japón. Se estima que más de 3.000 personas sufrieron varias deformidades severas, síntomas de intoxicación por mercurio o la muerte, en lo que se conoce como enfermedad de Minamata debido al envenenamiento por mercurio.

Más recientemente, en varias comunidades del estado de Querétaro, México, se ha descubierto la presencia de mercurio en alimentos de origen animal, vegetal y en el agua, y los niveles de contaminación por este elemento "exceden hasta en mil por ciento el máximo permitido, lo que implica graves riesgos para la salud".


Las emisiones de mercurio a la atmósfera se distribuyen globalmente y contaminan todos los ecosistemas. Como ya se ha señalado, el mercurio procede de actividades humanas (combustión del carbón, minería directa de mercurio, plata y oro) y actividades naturales (vulcanismo, por ejemplo). Las emisiones producen mayoritariamente Hg, con menor cantidad de Hg. El mercurio depositado puede ser re-emitido a la atmósfera mediante su intercambio entre el océano y el aire o la combustión de biomasa.

El mercurio almacenado en el hielo del monte Logan (5340 m sobre el nivel del mar; Yukon, Canadá) desde el año 1400 hasta 1998 ha sido medido con precisión. La mayoría de la acumulación de mercurio de origen antropogénico durante 600 años se produjo en el Monte Logan durante el siglo XX y especialmente entre 1940 y 1975. El incremento entre 1993 y 1998 (final del muestreo) puede reflejar el aumento de emisiones a la atmósfera por la combustión de carbón en Asia y la minería a pequeña escala de los países en desarrollo, que se ha estimado que continúa hasta la actualidad. La recolecta y estudio de nuevas muestras de hielo es urgente debido a la desaparición acelerada de los glaciares.

Debido a los efectos sobre la salud de la exposición al mercurio, sus usos industriales y comerciales son regulados en muchos países. La Organización Mundial de la Salud, la OSHA, y la NIOSH tratan al mercurio como un riesgo laboral, y se han establecido límites de exposición laboral específicos. Las emisiones y la eliminación del mercurio ambiental están regulados en los EE. UU. principalmente por la Agencia de Protección Ambiental.

Estudios epidemiológicos han constatado numerosos efectos nocivos del mercurio, como temblores, deterioro de habilidades cognitivas, y alteraciones del sueño en trabajadores con exposición crónica al vapor de mercurio, incluso a bajas concentraciones (en el rango de 0,7 a 42 mg/m. Un estudio ha demostrado que la exposición puntual (4-8 horas) a niveles de mercurio elemental calculados entre 1,1 y 44 mg/m dio lugar a dolor en el pecho, disnea, tos, hemoptisis, deterioro de la función pulmonar, y la evidencia de neumonitis. La exposición aguda intersticial al vapor de mercurio se ha demostrado que produce profundos efectos sobre el sistema nervioso central, incluyendo reacciones psicóticas caracterizadas por el delirio, alucinaciones y tendencia suicida. La exposición ocupacional se ha plasmado en un amplio alcance de perturbaciones funcionales, incluyendo eretismo, irritabilidad, nerviosismo, timidez excesiva, e insomnio. Con la exposición continuada, se desarrolla un ligero temblor, que puede transformarse en espasmos musculares violentos. El temblor inicialmente involucra a las manos, y luego se extiende a los párpados, los labios y la lengua. A largo plazo, la exposición de bajo nivel se ha asociado con síntomas más sutiles de eretismo, incluyendo fatiga, irritabilidad, pérdida de memoria, sueños vívidos y depresión.

Los efectos nocivos del mercurio pueden ser transmitidos de la madre al feto, e incluyen daño cerebral, retraso mental, falta de coordinación, ceguera, convulsiones e incapacidad para hablar. Los niños con envenenamiento por mercurio pueden desarrollar problemas en sus sistemas nervioso y digestivo y daños renales.

La investigación sobre el tratamiento de la intoxicación y el envenenamiento por mercurio es limitada. En la actualidad los fármacos disponibles para tratar la intoxicación mercurial aguda incluyen quelantes de N-acetil-D, L-penicilamina (PAN), Dimercaprol, ácido 2,3-dimercapto-1-propanosulfónico (DMPS), y ácido dimercaptosuccínico (DMSA). En un pequeño estudio incluyendo a 11 trabajadores de la construcción expuestos al mercurio elemental, los pacientes fueron tratados con DMSA y NAP. La terapia de quelación con ambos fármacos tuvo como resultado la movilización de una pequeña fracción del mercurio total corporal estimado. El DMSA fue capaz de aumentar la excreción de mercurio en un grado mayor que el NAP.

El pescado y el marisco tienen una tendencia natural a concentrar mercurio en sus cuerpos, a menudo en forma de metilmercurio, un compuesto orgánico altamente tóxico. Las especies de peces que forman parte de los niveles superiores de la cadena alimentaria, como tiburones, peces espada, caballas, atunes o albacoras contienen mayores concentraciones de mercurio que otros. Como el mercurio y el metilmercurio son solubles en grasa, se acumulan principalmente en las vísceras, aunque también se depositan en todo el tejido muscular. Cuando un pez es consumido por un depredador, el nivel de mercurio se acumula. Dado que los peces son poco eficientes en la depuración de la acumulación de metilmercurio, las concentraciones en sus tejidos aumentan con el tiempo. Por lo tanto, las especies que están más altas en la cadena trófica acumulan una carga corporal de mercurio que puede ser diez veces más alta que la de las especies que consumen. Este proceso se llama biomagnificación. Este tipo de envenenamiento por mercurio se produjo de esta manera en Minamata, Japón, dando lugar a la denominada enfermedad de Minamata.

Se transporta en estado líquido, código europeo del A.D.R.: [2809-80-8-8,Â66° c)]. Los contenedores deben cerrarse herméticamente. Se pueden emplear contenedores de acero, acero inoxidable, hierro, plásticos, vidrio o porcelana. Deben evitarse los contenedores de plomo, aluminio, cobre, estaño y cinc.

Almacenar en áreas frías, secas, bien ventiladas, alejadas de la radiación solar y de fuentes de calor y/o ignición, ya que a temperaturas mayores de 40 °C produce vapor. Debe estar alejado de ácido nítrico concentrado, acetileno y cloro. Debe almacenarse en recipientes irrompibles de materiales resistentes a la corrosión y que sean compatibles.

El mercurio puede amalgamarse accidentalmente con metales nobles como el oro, produciendo manchas sobre su superficie. Dado que el mercurio se evapora a unos 360 °C (de hecho, debe ser almacenado a una temperatura que no sobrepase los 40 °C para evitar la emanación de vapores), es posible eliminar una mancha (por ejemplo, de alguna joya) colocándola en la llama de un mechero y después puliéndola. Si la mancha es muy grande puede introducirse la joya en ácido nítrico concentrado o ácido sulfúrico concentrado (la joya debe ser de oro o platino, de lo contrario se disolverá). Los ácidos reaccionan con el mercurio, por lo que debe tenerse en cuenta que estas reacciones son exotérmicas y liberan vapores tóxicos.

De acuerdo a la legislación de la Unión Europea en el etiquetado deben incorporarse las : R 23 ("Tóxico por inhalación") y R 33 ("Peligro de efectos acumulativos"). También deben incorporarse las : S 1/2 ("Consérvese bajo llave y manténgase fuera del alcance de los niños"), S 7 ("Manténgase el recipiente bien cerrado") y S 45 ("En caso de accidente o malestar, acuda inmediatamente al médico (si es posible, muéstrele la etiqueta)").

Un total de 140 países acordaron en la el Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) con el objeto de evitar emisiones peligrosas.

En la Unión Europea, la directiva sobre la restricción del uso de ciertas sustancias peligrosas en aparatos eléctricos y electrónicos (véase RoHS) prohíbe el mercurio de ciertos productos eléctricos y electrónicos, y limita la cantidad de mercurio en otros productos a menos de 1000 ppm. También se han impuesto restricciones para la concentración de mercurio en los envases (el límite es de 100 ppm para suma de mercurio, plomo, cromo hexavalente y cadmio) y en las baterías (el límite es de 5 ppm). En julio de 2007, la Unión Europea prohibió también el mercurio en dispositivos de medición no eléctricos, tales como termómetros y barómetros. La prohibición sólo se aplica a nuevos dispositivos, y contiene excepciones para el sector de la atención sanitaria y un período de gracia de dos años para los fabricantes de barómetros.
Noruega promulgó una prohibición total del uso de mercurio en la fabricación e importación/exportación de productos de mercurio, el 1 de enero de 2008. En 2002, se constató que varios lagos en Noruega presentaban un mal estado debido a la contaminación por mercurio, con un exceso de 1 µg/g de mercurio en sus sedimentos.
En 2008, el Ministro de Desarrollo para el Medio Ambiente de Noruega, Erik Solheim, manifestó que: ""El mercurio es una de las toxinas ambientales más peligrosas. Alternativas satisfactorias al mercurio en los productos ya están disponibles, por lo que es apropiado introducir una prohibición"".

Los productos que contienen mercurio fueron prohibidos en Suecia en 2009.

En 2008, Dinamarca también prohibió la amalgama de mercurio dental, excepto para el relleno de la superficie de masticación de dientes permanentes, como los molares de adultos.

En los Estados Unidos, la Agencia de Protección Ambiental (EPA) se encarga de regular y gestionar la contaminación por mercurio. Varias leyes confieren a la EPA esta autoridad. Además, en la normativa recogida en el ""Mercury-Containing and Rechargeable Battery Management Act"", aprobada en 1996, se retira paulatinamente el uso del mercurio en las pilas, y se prevé la eliminación eficiente y rentable de los muchos tipos de baterías usadas. Los países de América del Norte contribuyeron aproximadamente con el 11% del total de las emisiones globales antropogénicas de mercurio en 1995.

La ""Clean Air Act"" (1990), aprobada en 1990, puso al mercurio en una lista de contaminantes tóxicos que necesitan ser controlados en la mayor medida posible. Por lo tanto, las industrias que liberan altas concentraciones de mercurio al medio ambiente han acordado en instalar el máximo alcanzable de las tecnologías de control (MACT). En marzo de 2005, la EPA promulgó una regulación que añadió las centrales eléctricas a la lista de fuentes que deben ser controladas e instituyó un sistema de "Comercio de derechos de emisión" nacional. Se dio de plazo hasta noviembre de 2006 para imponer controles más estrictos, pero después del desafío legal de varios estados, las regulaciones fueron derogadas por un tribunal federal de apelaciones el 8 de febrero de 2008. La norma no se considera suficiente para proteger la salud de las personas que viven cerca de las plantas de energía que queman carbón, dados los efectos negativos documentados en el "Informe al Congreso del Estudio de la EPA" de 1998. Sin embargo, nuevos datos publicados en 2015 mostraron que después de la introducción de controles más estrictos sobre el mercurio, éste se redujo drásticamente, lo que indica que la Ley de Aire Limpio surtió el efecto deseado.

La EPA anunció nuevas reglas para las plantas eléctricas de carbón el 22 de diciembre de 2011. Los hornos de cemento que queman residuos peligrosos se mantienen a un nivel de control menos estricto que las incineradoras estándar de residuos peligrosos, por lo que constituyen una fuente desproporcionada de la contaminación por mercurio.





</doc>
<doc id="3363" url="https://es.wikipedia.org/wiki?curid=3363" title="Químico">
Químico

Un químico es un científico especializado en la química. Los químicos estudian la composición de la materia y las propiedades que participan en su interacción, los productos resultantes, y la aplicación de estas propiedades en la vida del hombre como tal.

Son de especial interés para los químicos las propiedades de los compuestos, su reactividad, y su uso en campos como bioquímica, farmacología, industria cosmética, industria alimentaria, química de materiales, petroquímica, ingeniería, entre otras.

Los químicos utilizan sus conocimientos para aprender la composición y las propiedades de sustancias desconocidas; también para reproducir y sintetizar productos naturales en grandes cantidades y para crear nuevas sustancias artificiales mediante procesos rentables.

Los químicos pueden especializarse en diversas subdisciplinas de la química. Los metalúrgicos y los científicos de materiales deben compartir mucho de la educación y preparación seguidas por los químicos.

Los ingenieros químicos se relacionan con los procesos físicos necesarios para la producción industrial (calentamiento, refrigeración, mezcla, difusión, etc.) así como la separación y purificación de los productos, y trabajan con los químicos industriales y otros químicos en el desarrollo de nuevos procesos para estos productos.

La formación universitaria consiste, además de la formación en química, en una integración de ciencias auxiliares, donde se imparten asignaturas como física, matemáticas, química física, administración, legislación, bioquímica, entre otras.

Las licenciaturas que se relacionan con el campo de formación química son:

Los campos de especialización incluyen bioquímica, química orgánica, química inorgánica, química analítica, química teórica, química cuántica, química ambiental, química física, citoquímica y en algunos casos en microbiología y farmacia.

El campo laboral de un profesional de la química varía de acuerdo con el tipo de licenciatura cursada, su especialización, y los campos de aplicación de su rama. En general, el químico puede emplearse en cualquier proceso industrial donde se lleven a cabo reacciones de síntesis de compuestos de interés humano tales como: industria farmacéutica, industria petroquímica, industria cosmética, industria alimentaria, etc.

Los químicos especializados en ciencias de la salud humana, tales como químicos clínicos biólogos, químicos bacteriólogos parasitólogos, químicos farmacéuticos biólogos, farmacéuticos, bioquímicos, bioquímicos diagnósticos, pueden trabajar en la síntesis, producción, venta y regulación en materia legal de medicamentos, análisis clínicos, desarrollo, investigación, docencia y capacitación continua de personal.


A veces en español se hace un uso inadecuado del término químico como producto químico o sustancia química. Esto se debe a una mala traducción del inglés "chemical" como "químico", en vez de "producto químico" o "sustancia química". 
Está recogido en Wikipedia como un ejemplo típico de , aunque este error es frecuente incluso en los medios de comunicación.


</doc>

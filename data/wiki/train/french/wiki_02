<doc id="213" url="https://fr.wikipedia.org/wiki?curid=213" title="Amstrad CPC 464">
Amstrad CPC 464

L'Amstrad CPC 464 est un ordinateur personnel britannique de la gamme Amstrad CPC, à affichage couleurs ou monochrome vert, comportant de RAM, en ROM et utilisant le langage Locomotive BASIC 1.0, considéré par certains passionnés comme le meilleur BASIC ayant jamais 
existé.

Cet ordinateur, conçu pour l'utilisation familiale (il coûtait FF), répondait au lancement des ZX Spectrum, Oric 1 et Commodore 64 par Sinclair, Oric Corporation et Commodore. L'Amstrad CPC 464 sorti en septembre 1984 en France a connu un immense succès : il s'en vendait par mois pour un total d'un million d'exemplaires. Il a fait disparaître beaucoup d'ordinateurs et a peut-être marqué la fin d'une époque. Son succès fut tel que plus d'une dizaine de magazines spécialisés furent créés, dont le plus fameux, Amstrad Magazine. Pour la première fois, une seule fiche secteur était nécessaire, écran et unité centrale s'alimentaient directement sans adaptateur encombrant et deux fiches seulement reliaient les deux éléments entre eux, la mise en fonction était immédiate. C'est peu après le succès commercial du 464 qu'est apparu AMSDOS. CP/M qui était antérieur au 464 a été transposé sur cet ordinateur.

Il existe un projet de descendant de l'Amstrad CPC à base de eZ80 à près de , le CPCNG.

Il utilisait un processeur Zilog Z80A (8 bits) à 4 MHz et comportait un lecteur de cassettes intégré pour le stockage des données. On pouvait lui ajouter un lecteur de disquettes au format 3 pouces.

Le programme PROTEXT de traitement de texte n'était pas présent sur tous les CPC 464.

Z80A à . De par le gateArray les opcodes prenaient tous 4 cycles au minimum. Dès lors certains estiment la perte de performance moyenne à 15 %. Cette estimation reste très statistique mais relativement acceptée par la communauté.

 de RAM, extensibles à (des extensions à existent également, ce sont cependant des matériels non officiels).

 de ROM, extensibles à .

Le fait d'avoir un lecteur de cassette intégré n'avait rien d'anecdotique. En effet les débits depuis la cassette s'effectuaient sur le CPC 464 à 2000 bauds (2 kbit) en vitesse rapide ou à 1 000 bauds (1 kbit) en vitesse lente.

Il est à noter que ces vitesses étaient celles indiquées de base par le constructeur. Le jeu Bad Cat, de Rainbow Arts/Go! dans sa version pour Amstrad CPC cassette a été enregistré à 4000 bauds. Et l'Amstrad CPC lit les données à cette vitesse. 
César Nicolas Gonzalez, connu dans la communauté sous le pseudo de CNGSOFT a créé des versions compactées de jeux commerciaux, avec une vitesse d'enregistrement allant jusqu'à 6500 bauds. Un vrai CPC 464 est capable de gérer cette vitesse sans problème.

La machine est équipée d'un Motorola CRTC 6845 (ou clones), plus une puce spécifique à Amstrad, le Gate Array. Le CPC ne possède pas de mode texte en tant que tel. Les informations de modes texte qu'on retrouve régulièrement indiquent en fait le nombre de caractères (de huit pixels sur huit) que peut afficher le système dans les différents modes graphiques.

Le CPC standard possède une palette de 27 couleurs, constituées des trois teintes primaires (rouge, vert, bleu) auxquelles on applique les coefficients 0 ; 0,5 et 1. À l'origine le CPC était annoncé avec une palette de 32 couleurs. Malheureusement les 5 couleurs supplémentaires sont identiques à certaines teintes présentes parmi les 27 sus-nommées. Elles sont accessibles directement en Basic, mais n'ont aucun intérêt pratique.

Chose rare sous l'ère des 8 bits l'affichage est du full bitmap sans contrainte. Chaque pixel peut être adressé indépendamment et n'importe quelle couleurs de la palette définie (2, 4 ou 16 couleurs parmi les 27 selon le mode). 

Le CPC dispose de 4 modes graphiques de base utilisant 16 ko de mémoire, dont un non documenté :

Il était possible de créer des modes alternatifs via la programmation du CRTC, voire d'utiliser 32 ko de mémoire pour l'affichage. Cette astuce était essentiellement utilisée pour certains écrans d'accueil en fullScreen en 192x264 (25 ko). Néanmoins les versions CPC de Donkey Kong et d'Arkanoid, par exemple, utilisaient des modes alternatifs permettant du 128x256 en 16 couleurs (16 ko) leur donnant un aspect plus proche des bornes d'arcade à écrans verticaux dont ils étaient originaires.

Les développeurs de jeux d'arcade sur Amstrad ont souffert de la carence d'un mode tiles et sprites et d'un scrolling pixel par pixel et de mode graphique full bitmaps sans contrainte.

En effet, pour faire, par exemple un scrolling horizontal pixel par pixel à cinquante images par seconde en plein écran, il fallait que le Z80A fasse cinquante compositions d'écran de 16 kilooctets par seconde, soit 800 ko/s, sans compter les modifications d'image à faire pour positionner les « sprites » logiciels. En effet, bien que le CRTC permît d'effectuer des scrolling horizontaux hard, ceux-ci n'étaient disponibles que par incrément complet d'un octet, soit un pas de 2, 4 ou 8 pixels (mode 0, 1 ou 2) forçant bien des programmes à utiliser des scrolling soft pour éviter les saccades. C'est pour cette raison que la plupart des jeux d'arcade sur CPC présentent une surface jouable inférieure à leurs homologues sur d'autres plateformes. 

Il était par contre bien plus facile de gérer des scrolling verticaux en hard mais par incrément dont 1024 était un multiple plein. D'où de nombreux shoot verticaux dans des fenêtres de 128 pixels de large sur les 160 disponibles en mode 0 sur le CPC. Néanmoins, ici, le 128x200 avait aussi l'avantage de conserver l'aspect d'un shoot vertical.

À titre de comparaison, à la même époque, le mode Tiles & Sprite sur la Sega Master System (également à base de Z80A) avec scrolling hard pixel par pixel nécessitait, pour un scrolling pixel par pixel, 50/8 (scrolling hard) * 1 k (taille de la table de tiles) soit 6,2 ko par seconde à gérer par le Z80A.

Pour le C64, grâce à des caractères redéfinissables en 4 couleurs et d'un scrolling hard d'un pixel en mode "texte", on pouvait simuler des "tiles" cette opération ne nécessitait donc que de traiter que de 50*/8 (scrolling hard) * 10 k (taille d'un 160x200 en 4 couleur + 2 ko définissant les 4 couleurs utilisées par caractère) ⇒ 62,5 ko/s, ce qui, avec une capacité CPU deux fois inférieure au CPC, prenait tout de même cinquante fois moins de temps CPU et permettait une réelle fluidité. De plus, il disposait de huit sprites hard monochrome ou multicouleurs. 

Le ZX Spectrum, lui, devait rafraîchir 350 ko/s mais souffrait du Color Clash. 

Sur MSX, point de salut : les développeurs devaient se contenter de scrolling par pas de huit pixels et de sprite monochrome.

Pour les jeux « 3D » fil de fer (dont le majestueux Elite) voire en fractales (Rescue on Fractalus!), ces 800 ko/s (ici souvent du 320x200 en 4 couleurs) sont à comparer au 50x6,7 ko = 350 ko/s du mode 256x192 16 couleurs avec contrainte du ZX Spectrum. Ici le C64 devait traiter 50 x 10 ko = 500 ko/s de son mode graphique 320x200 16 couleurs avec contrainte du C64.

General Instruments AY-3-8912, 3 voies stéréo avec une fréquence de . Le même processeur sonore que les Oric, les MSX, les ZX Spectrum modèle 128 et successeurs, et l'Atari ST. L'AY-3-8912 possède aussi des ports d'entrées/sorties, qui sont utilisées sur CPC pour l'interrogation du clavier et du joystick.

AZERTY ou QWERTY suivant les régions, il existe aussi une version de 464 intégrant la touche « ñ » espagnole. Le clavier possède un pavé numérique.


Il n'y a pas d'interface RS-232, celle-ci est en revanche disponible séparément.




</doc>
<doc id="214" url="https://fr.wikipedia.org/wiki?curid=214" title="Amstrad CPC 6128">
Amstrad CPC 6128

L'Amstrad CPC 6128 est une évolution de l'Amstrad CPC 464 à l'intérieur de la gamme des Amstrad CPC. Il disposait du même processeur Z80 à 4 MHz, mais était doté de 128 ko de mémoire vive (dont 64 ko de mémoire paginée) au lieu de 64 ko, et de 48 ko de mémoire morte au lieu de 32 ko. En plus, il disposait d'un lecteur de disquette 3 pouces dont la capacité pouvait aller jusqu'à 178 ko par face, beaucoup plus rapide que le lecteur de cassette, et intégrait un langage BASIC en mémoire morte. Il était livré avec des disquettes comportant deux versions du système d'exploitation CP/M (CP/M 2.2 et CP/M 3+) et de nombreux utilitaires.

Dérivé du CPC 464, le CPC 6128 disposait en pratique de moins de mémoire vive accessible pour l'utilisateur BASIC que son prédécesseur. D'une part les 128 ko de RAM étaient en réalité constitués de 64 ko de mémoire paginée, d'autre part son système d'exploitation chargeait le pilote du lecteur de disquettes en plus du pilote du lecteur de cassettes.

En raison de son prix modique (avec un écran couleur, francs français (5990 francs a sa sortie), moins de € de 2015) et de sa capacité mémoire jugée énorme pour l'époque, le 6128 remporta un beau succès dans les hypermarchés, mais restait lié aux fameuses disquettes 3 pouces (et non 3 pouces 1/2) difficiles à se procurer. L'hebdomadaire "Hebdogiciel" annonça dans un numéro de la sortie imminente du CPC 5512, un 6128 équipé d'un lecteur de disquettes 5"1/4, accompagné d'une photo de ce prétendu nouveau modèle (en fait, un montage habile). Cela eut pour effet, selon le constructeur, de figer immédiatement les ventes du 6128 en France pendant une semaine… et de valoir à l'hebdomadaire en question un procès intenté par Amstrad.

L'Amstrad 6128 plus est sorti en 1990. C'est une évolution de l'Amstrad CPC 6128, celui-ci comporte quelques différences au niveau architecture :

Le 6128 plus partageait avec le 464 plus et la GX 4000 la même carte mère, mais en étant équipé de plus de RAM (128 Kio contre 64 Kio) et en étant équipé d'un lecteur de disquette 3 pouces, contre un lecteur de cassette sur le 464 plus.
La GX4000 étant quant à elle un 464 plus dépouillé de son clavier et lecteur de cassettes et se branchant directement sur une télévision ; une tentative commerciale ratée de rester sur le marché des machines orientées jeux en s'attaquant aux consoles vidéos de jeux de salon.



</doc>
<doc id="215" url="https://fr.wikipedia.org/wiki?curid=215" title="Amphétamine">
Amphétamine

L’amphétamine (DCI) est une substance sympathicomimétique aux effets anorexigènes et psychoanaleptiques. Dans la plupart des pays du monde, l'amphétamine est considérée comme un stupéfiant.

Le nom « amphétamine » trouve son origine de ses noms chimiques et est une abréviation dont l'étymologie est facile à comprendre : 
l'amphétamine, c'est une phénéthylamine à laquelle a été rajouté un groupement méthyle (-CH3) en position α (alpha) de sa chaîne, ce qui donne Alpha-Méthyl-Phénéthylamine ; c'est la version développée. Du nom « Alpha-Méthyl-PHÉnéThylAMINE » ne seront retenues par simplification que les lettres capitales, ce qui donne finalement : AMPHÉTAMINE. Par extension, un grand nombre de molécules similaires, ayant pour point commun un groupement méthyle (-CH3) en position α, sont appelées amphétamines.

La première synthèse d'amphétamines fut réalisée le par le chimiste roumain , qui lui donna le nom de "phénylisopropylamine" mais cette découverte tomba dans l'oubli.

En 1914, un chimiste allemand re-découvrit cette molécule et l'utilisa durant la guerre comme sérum de vérité.
Les recherches reprirent de nombreuses années plus tard et l'amphétamine fut à nouveau découverte lors de recherches d'un produit ayant des propriétés bronchodilatatrices. En 1932, après le rachat du brevet, elle fut lancée sur le marché par le laboratoire Smith, Kline & French sous le nom générique de "« »" et prescrite comme bronchodilatateur. En 1935, son action stimulante est constatée et utilisée pour des prescriptions concernant la narcolepsie.

Elle fut largement utilisée pendant la Seconde Guerre mondiale pour améliorer les performances et surtout l'endurance des soldats (par l'ensemble des belligérants) et c'est alors que les premiers excès seront constatés, avec des effets d'accoutumance.

Classée comme psychotrope par la Convention sur les substances psychotropes de 1971, elle fut progressivement déclarée illégale à mesure que les pays adaptaient leur législation, réduisant l'usage médical au traitement limité de quelques maladies.

Leur usage, désormais clandestin, concerne principalement l'augmentation des performances en sport et la résistance à la fatigue (lors de soirées festives ou lors de période de travail intense).

En France, parmi les personnes âgées de , 1,7 % déclarent avoir déjà consommé des amphétamines au cours de leur vie, mais seulement 0,2 % l’a fait au cours de l’année (usage actuel). Contrairement à toutes les autres drogues illicites, les femmes sont plus nombreuses à avoir expérimenté les amphétamines (1,5 % contre 0,8 % des hommes). Cette caractéristique des amphétamines correspond en grande partie à des usages ayant eu lieu à une époque où elles étaient encore présentes dans la pharmacopée et classiquement utilisées par certaines femmes dans la perspective de perdre du poids, grâce à leur puissant effet anorexigène.

Les « amphétamines » (au pluriel) sont un groupe de molécules apparentées à l'amphétamine de structure phényléthylamine.
On peut distinguer trois grands types de dérivés amphétaminiques selon leur effet principal, psychostimulant, hallucinogène ou anorexigène. En modifiant plus ou moins la molécule de phényléthylamine, il a été possible d'obtenir des produits dont l'un des effets (stimulant, hallucinogène ou anorexigène) est renforcé au détriment des autres. On a pu ainsi mettre au point des anorexigènes comme la fenfluramine qui ne présente pas d'effet psychostimulant ou des hallucinogènes puissants comme le STP.

Les trois types de dérivés peuvent donner lieu à de l'abus et connaissent des modes de consommation différents selon la nature des produits et les effets recherchés.

Jusqu'aux , le commerce illicite des amphétamines concernait essentiellement des dérivés psychostimulants. Depuis, des dérivés hallucinogènes s'y sont ajoutés, en particulier l'ecstasy qui fait l’objet d’une importante consommation. En outre, certains dérivés anorexigènes, dont l'effet psychostimulant n'est pas totalement absent, sont détournés de leur usage médical.

L'amphétamine possède deux énantiomères, le terme "amphétamine" désigne le mélange racémique (mélange 50/50 de D-amphétamine et de L-amphétamine). La D-amphétamine est dénommée Dexamphétamine (ou dextro-amphétamine), la L-amphétamine est nommée lévo-amphétamine.

Formule chimique : CHN
Masse molaire : /mol

La structure chimique de l'amphétamine ressemble à celle de stimulants naturels produits par le corps : les catécholamines dont l'adrénaline, la noradrénaline, la dopamine. L'amphétamine inhibe la recapture de la dopamine. Elle a aussi une action libératrice de la noradrénaline et de la dopamine, par action du transporteur vésiculaire VMAT2 (présynaptique). Ce phénomène serait la cause de la perturbation de la production de dopamine.

Elle est principalement employée en Amérique du Nord pour traiter les troubles de l'attention, la narcolepsie et parfois dans le traitement de l'obésité. Même si la forme pure est proscrite depuis 1959, elle reste utilisée sous forme de sulfate de dextroamphétamine.

Son utilisation comme anti-fatigue dans l'armée est connue.

L'amphétamine est utilisée comme drogue ou comme dopant, le plus souvent sous le nom de ". La drogue est aussi utilisée par certains pour une recherche de productivité accrue, lors de la réalisation d'un travail scolaire par exemple.

La drogue se présente généralement en poudre blanche, parfois colorée. On la trouve également en gélule, comprimé ou cristaux.

Le produit vendu clandestinement sous le nom de " peut contenir ou non des amphétamines (notamment amphétamine, dextroamphétamine, méthamphétamine), d'autres produits actifs aux effets similaires ou non dont des psychotropes ou même des excipients parfois dangereux, comme la plupart des drogues obtenues de façon illégale.

L'amphétamine agit en libérant de la dopamine dans le cerveau.
Elle bloque la recapture de la dopamine dans la synapse.
Elle inhibe l'activité de l'enzyme MAO (monoamine oxydase) à forte dose.
Elle agit dans le corps environ de après avoir été ingérée. Tout dépendant de la quantité prise et si elle est combinée avec d'autres stimulants, l'amphétamine peut s'avérer très dangereuse.
Elle traverse la barrière placentaire et cause de nombreux dégâts au fœtus.

L'amphétamine étant un produit psycho-actif, les effets recherchés peuvent parfois se transformer en "".

Il arrive parfois que les amphétamines, du fait de leur caractère stimulant, induisent des hallucinations à forte dose ; cet effet ne doit pas être confondu avec celui des psychostimulants.

La "descente" – fin des effets – est souvent très difficile et peut s'accompagner de :
La consommation d'amphétamines cause un effet d'indifférence ou un effet « sérum de vérité ».

L'usage régulier entraîne une accoutumance. L'arrêt brutal d'une consommation régulière entraîne un syndrome de sevrage.

Le sevrage des amphétamines peut résulter en une idée fixe qui peut amener à une irritation ou agressivité soudaine, ou un rattrapage de sommeil extrême (à ne pas confondre avec la fatigue chronique).

L'absorption de fortes doses peut entraîner une action hallucinogène.

Le mode de consommation entraîne aussi d'autres risques :

Les décès dus à la consommation d'amphétamine sont imputables à :





</doc>
<doc id="217" url="https://fr.wikipedia.org/wiki?curid=217" title="Amstrad">
Amstrad

Amstrad est une entreprise d'électronique grand public créée par Alan Michael Sugar au Royaume-Uni, et basée à Brentwood dans l'Essex, Angleterre. Le nom est une contraction de Alan Michael Sugar TRADing. Anciennement connue en France pour ses ordinateurs, elle appartient depuis 2007 à la British Sky Broadcasting (BSkyB).

Dans les années 1980, l'entreprise a lancé la gamme d'ordinateurs personnels "populaires" Amstrad CPC au Royaume-Uni, en France et en Allemagne, et aussi la gamme d'ordinateurs professionnels Amstrad PCW, qui fut principalement un interpréteur sous le système d'exploitation CP/M.

L'entreprise s'est adaptée avec le temps et a produit par la suite une gamme d'ordinateurs personnels bon marché tournant sous MS-DOS, dont le premier était l'Amstrad PC-1512.

Initialement montée pour distribuer du matériel électronique bon marché, Amstrad développe aujourd'hui des récepteurs satellite numériques pour BSkyB et Sky Italia.

Amstrad Ltd Co. était au départ un constructeur présent sur le marché de la Hi-Fi. Son PDG, Alan Michael Sugar n'est ni un informaticien ni un novateur, mais un entrepreneur ambitieux et talentueux, apôtre de la consommation de masse. En 1984, sans doute inspiré par les succès de Sir Clive Sinclair, il se lance sur le marché de la micro-informatique domestique en commercialisant une machine qui réunit tout le savoir-faire anglo-saxon en la matière (une architecture à base de Z80, sur un marché alors déjà ancien).

Les 8 bits sont alors commercialisés en grande surface depuis plusieurs années en Europe, mais Amstrad est la première marque qui s'implique vraiment sur ce créneau de distribution, notamment par un "marketing" agressif qui cible le grand public. Son succès rapide s'explique par l'essor que connaît alors la consommation de masse et par la multiplication des hypermarchés distribuant la marque. Parallèlement, A. M. Sugar est omniprésent dans la presse informatique.

Une stratégie identique gouverne le "packaging" : le premier modèle d'Amstrad, le CPC 464 pour Colour Personal Computer, réunit pour la première fois et à un prix abordable tout ce qui est nécessaire à l'utilisateur, avec une qualité supplémentaire : l'intégration. Cette dernière est jusque-là demeurée l'apanage des seules machines de luxe (Apple ou IBM). Le CPC 464, quant à lui, est fourni avec un moniteur monochrome (noir et vert) ou couleur, un lecteur de cassettes, un confortable clavier mécanique doté d'un pavé numérique, et un petit haut-parleur. Il se démarque de la concurrence par un "design" harmonieux et coloré qui doit beaucoup à l'Oric Atmos. Le marché visé est d'emblée européen mais c'est en Angleterre et en France que les Amstrad CPC vont s'imposer comme machines grand public par excellence.

Le constructeur sort rapidement deux nouveaux modèles équipés d'un lecteur de disquette 3" en lieu et place du lecteur de cassette : le CPC 664 au printemps 1985, au clavier bleu clair et blanc, sera remplacé dès l'automne suivant par le CPC 6128, au sobre clavier gris, doté de de RAM (sous forme de deux banks séparés, seuls étant adressables).

En matière de programmation, le langage BASIC des Amstrad conçu par Locomotive Software est des plus rapides, souple et puissant à la fois, avec un éditeur intégré, malheureusement « ligne à ligne ». Autre facteur de succès, la documentation est correctement traduite et de qualité. Bien fourni en logiciels maison dès l'origine (ils sont signés AmSoft), c'est le volume de ventes et le soutien des éditeurs de jeux anglo-saxons qui vont faire des CPC les rivaux du Commodore 64 en Europe.

Reprenant les principes qui ont fait le succès de sa gamme à usage domestique, Amstrad sort dans la deuxième moitié des années 1980 une gamme PCW, destinée à conquérir le marché à usage professionnel. Là aussi, le postulat est d'intégrer à un prix abordable un système d'exploitation éprouvé le CP/M et les outils de base (tableur, traitement de texte, imprimante, écran, clavier) à un prix forfaitaire abordable pour l'époque. Malgré l'arrivée des premiers PC compatibles, le PCW trouvera sa place avec 8 millions d'unités vendues.

Amstrad est l'un des derniers constructeurs de micros 8 bits encore en lice en 1987, mais sa politique orientée produits grand public à prix cassés ne lui permettra pas de dégager des bénéfices suffisamment importants pour s'imposer sur le marché des compatibles PC, qui explose à la fin des années 1980. La marque existe encore aujourd'hui. Le succès (relatif) de sa première gamme destinée aux professionnels, les PCW, s'explique plus par la notoriété de la gamme que par leur réel intérêt.

Dès lors, comprenant que les jours des micro-ordinateurs non compatibles PC sont comptés, Amstrad se lance sur ce créneau qu'il contribue à démocratiser avec quelques belles réussites dont la plus connue est le PC-1512. Au milieu des années 1990, le marché étant saturé, la marque doit renoncer à ses conquêtes et se replier sur son créneau originel : la Hi-Fi et la vidéo, puis les décodeurs satellites.

C'est cette dernière activité qui a motivé l'OPA de BSkyB en juillet 2007, à qui Amstrad écoulait deux tiers de ses ventes. Alan Sugar restera à la direction de l'entreprise jusqu'en 2008 et gardera 28 % de son capital.

À l'ENA, Alain Juppé a été surnommé Amstrad en référence à son intelligence.




</doc>
<doc id="245" url="https://fr.wikipedia.org/wiki?curid=245" title="Amsterdam">
Amsterdam

Amsterdam ( ; parfois surnommée "A'dam" par ses habitants) est la commune la plus peuplée et la capitale du Royaume des Pays-Bas, bien que le gouvernement ainsi que la plupart des institutions du pays siègent à La Haye. Sur la base des chiffres 2013, la commune d'Amsterdam compte près de appelés Amstellodamois, au cœur de la région d'Amsterdam qui regroupe environ . L'aire urbaine, qui rassemble plus de fait elle-même partie d'une conurbation appelée qui compte . Elle est située en province de Hollande-Septentrionale, mais n'en est cependant pas la capitale, cette dernière étant Haarlem.

Le nom de la commune vient de l'ancien nom néerlandais "Amstelredamme" évoquant les origines de la ville : la digue ("") sur l'Amstel. Ancien petit village de pêcheurs au , la ville connaît une très forte croissance au Moyen Âge au point de devenir l'un des principaux ports du monde durant le Siècle d'or néerlandais. Le quartier De Wallen est la partie la plus ancienne de la ville, qui se développe autour d'un réseau concentrique de canaux semi-circulaires reliés par des canaux perpendiculaires, formant une « toile d'araignée ». Au centre de la vieille ville, on trouve sur la place principale du Dam le palais royal d'Amsterdam, construit au , symbole de l'importance de la ville. Guillaume en fait sa résidence en 1815. Depuis juillet 2010, le quartier du , délimité par le Herengracht, Keizersgracht et Prinsengracht, figure sur la liste du Patrimoine mondial de l'UNESCO. Dans cette zone que se trouve le renommé béguinage d'Amsterdam, cour arborée et bordée d'habitations anciennes abritant en son sein une chapelle anglicane.

Amsterdam est l'un des centres économiques majeurs des Pays-Bas et l'un des principaux centres financiers d'Europe. Les sièges sociaux de plusieurs firmes multinationales (Philips, AkzoNobel, ING, TomTom) sont situés dans la ville et d'autres ont leurs bureaux européens basés à Amsterdam (Netflix, Uber, Tesla). La ville est également la capitale culturelle du pays, comme en témoigne la renommée de ses principaux musées concentrés autour du Museumplein : le musée d'État, le Stedelijk Museum et le musée Van Gogh sont parmi les plus visités au monde. Plusieurs classements placent Amsterdam parmi les métropoles mondiales offrant le meilleur confort de vie, le magazine américain "Forbes" la positionnant à la première place en 2016. Le majorité des déplacements en ville s'y effectue grâce aux quatorze lignes de tramway, à pied ou à vélo. La ville est aussi connue pour De Wallen, son principal « quartier rouge », ainsi que pour ses nombreux coffee shops possédant une licence leur permettant de commercialiser le cannabis, reflétant le progressisme politique des Pays-Bas.

Le toponyme du terme est seulement attesté avec sa graphie actuelle au . Le nom de la ville qui a connu une croissance urbaine assurée dès le s'est écrit de différentes façons par le passé : "Aemstelredam", "Aemstelredamme", "Amestelledamme" (1275), Amestelredamme (1285), "Amstelredam", et "Amstelredamme". Il existe une variante graphique "Amsteldam" attestée au et .

Le toponyme originel signifierait la digue ("dam" en néerlandais) de terre (« erd » ou « ered » son persistant intermédiaire du mot) sur une rivière nommée autrefois Amstel. Il existe selon Deroy et Mullon une autre hypothèse apparemment précise formulable sur l'installation portuaire à la faveur de cette digue située au sud-ouest de l'ancien golfe du Zuidersee. Elle segmente arbitrairement le toponyme en trois parts "Ame/stelle/dam", interprétant la première "Ame" en « cours d'eau ou rivière locale à eau vive », la seconde "Stelle" soit « une place portuaire, formé par une levée progressive de terre formant embarcadère ou un amas de terre de remblai, en partie creusé et aménagé, permettant le premier emplacement portuaire », la troisième "Dam" signalant toujours la digue en arrière, protégeant les habitations. Dans ce cadre hypothétique, la ville préserverait un nom signifiant approximativement la « digue du port fluvial ».

Les premières armoiries se composent « de gueules au pal cousu de sable chargé de trois flanchis d'argent ». Ce sont donc des armes à enquerre. Les origines du blason ne sont pas claires, mais les historiens considèrent qu'il s'agit des armoiries de la famille Persijn, qui était propriétaire d'une grande étendue de terres situées sur l'emplacement de la ville. Un certain Jan Persijn fut ainsi « seigneur de Amstelledamme » de 1280 à 1282 (on retrouve les mêmes couleurs et figures sur les blasons des villes d'Ouder-Amstel et Amstelveen qui furent, elles aussi, la propriété de la famille Persijn). Ces mêmes historiens estiment que la bande noire au centre du blason représente le fleuve Amstel (comme c'est le cas dans plusieurs autres villes néerlandaises, comme à Delft ou à Dordrecht, où la bande centrale stylise le cours d'eau principal de la ville). Les trois croix de saint André pourraient représenter les trois mots de la devise de la ville. Une tradition populaire voit pourtant dans ces trois croix les menaces pour la ville : eau, feu et peste.

En 1489, la petite ville commerçante acquiert le droit d'ajouter la couronne du Saint-Empire romain germanique à son blason. Il s'agit d'une faveur accordée par l'empereur Maximilien pour remercier les habitants de la ville du soutien qu'ils lui ont apporté. Cette même couronne est également visible (sous une forme stylisée plus proche de celle de Rodolphe II) au-dessus de la Westerkerk, l'une des églises les plus emblématiques de la ville. Sous le Premier Empire, Amsterdam fait partie des bonnes villes et est autorisée, à ce titre, à demander des armoiries au nouveau pouvoir : elles sont modifiées par l'ajout d'un « chef de gueules chargé de trois abeilles d'or », qui est la marque présente sur les blasons des bonnes villes de l'Empire.

La première mention du nom « » dans les documents historiques remonte à un acte de Florent V, comte de Hollande de 1256 à 1296. Ce document, baptisé « Exemption de taxes d'Amsterdam » () et daté du 27 octobre 1275 dispensait les quelques centaines d'habitants du « Barrage sur l'Amstel » du paiement des taxes sur le commerce de leurs produits à l'intérieur du Comté de Hollande et sur leur pont-barrage sur l'Amstel, construit vers 1270. Ces habitants sont désignés en latin en tant qu'« ' (littéralement, les personnes vivant près du barrage de l'Amstel). En l'espace de quelques années, ce mot évolue sous sa forme quasi finale d", comme en attestent des écrits de 1327. À cette époque, Amsterdam n'est rien de plus qu'un village de pêcheurs rattaché à l'évêché d'Utrecht. Cette exemption de péage donne alors un avantage compétitif aux Amstellodamois pour le commerce extérieur et permet à Amsterdam de devenir la première place commerciale de Hollande, et de poser les bases de sa richesse et de sa puissance futures.

Le bourg d'Amsterdam obtient le statut de ville en 1300 ou 1306, probablement par l'évêque d'Utrecht, Gui d'Avesnes, et devient une importante place commerciale au , grâce à son port qui se développe sur le Damrak, en aval du barrage originel. Le commerce, notamment avec l'Inde, reste toutefois dominé, dans un premier temps, par le port d'Anvers, confinant Amsterdam à commercer principalement avec les villes de la Ligue hanséatique.

En 1345, un miracle présumé qui se produit sur la Kalverstraat fait d'Amsterdam un important centre de pèlerinage jusqu'à la Réforme.

Avant 1385, l'Amstel sépare la ville d'Amsterdam en deux parties de taille à peu près égale : la « vieille ville » (') où se trouve la « vieille église » (), dont la construction avait débuté vers 1300, et la « Nouvelle ville » (') où se trouve la « nouvelle église » (), bâtie au début du . Afin de garantir sa protection, la ville se dote de canaux, complétés par une palissade (') composée d'un mur de terre surplombé par une palissade de bois. Lorsqu’après 1385, de nouveaux murs d'enceinte sont construits, le mur existant prend le nom de ' (avant-palissade) tandis que le nouveau est baptisé " (arrière-palissade), et ce à la fois dans les vieille et nouvelle villes. On voit encore aujourd'hui, dans le centre historique, quatre canaux/rues portant les noms de , , et (devenu ).

En 1421 et en 1452, la ville est ravagée par deux incendies majeurs. Le second détruit plus des trois quarts de la ville et l'empereur Charles Quint décrète en 1521 que les nouvelles habitations devront être construites en pierre plutôt qu'en bois. Restée théorique, l'interdiction devient définitive à partir de 1669. Presque toutes les habitations en bois de l'époque ont aujourd'hui disparu, à l'exception notable de la " (« Maison de bois ») du béguinage.

Au , la population se soulève contre le successeur de Charles Quint, le roi Philippe II d'Espagne. En effet, contrairement à Charles Quint dont la politique ferme restait très sensible aux évolutions sociales et religieuses au sein de ses provinces des Pays-Bas espagnols, Philippe II fait preuve d'intransigeance en matière religieuse et politique, ce qui génère de fortes crispations. La noblesse ainsi que les Protestants en sont les premières victimes. Une politique centralisatrice et absolutiste est ainsi mise en œuvre par le gouverneur du roi à Bruxelles, Ferdinand Alvare de Tolède, via notamment l'instauration du Conseil des troubles en 1568 ou la création d'une nouvelle taxe, l'année suivante, prélevant 10 % du montant de toutes les ventes de biens meubles, appelée le « dixième denier». En matière religieuse, le pouvoir décide d'avoir recours à l'Inquisition pour tenter d'enrayer la diffusion rapide du calvinisme, provoquant ainsi d'importantes persécutions religieuses. La révolte dégénère rapidement en guerre rangée – à laquelle Amsterdam se rallie à partir de 1578 - et conduit à l'indépendance des sept provinces septentrionales des Pays-Bas espagnols, sous le nom de Provinces-Unies. L'année 1578 est également marquée par le renversement du gouvernement catholique de la ville au cours de l'épisode de l'Alteratie, qui voit les Protestants prendre le pouvoir sans effusion de sang.

Sous l'impulsion du stathouder Guillaume le Taciturne, les Provinces-Unies deviennent un symbole de tolérance religieuse. Dans le contexte des guerres de religion qui ravagent d'autres pays d'Europe, nombreux sont ceux qui y cherchent alors un refuge pour vivre leur foi sans risquer de condamnation. Cette situation provoque l'immigration de familles juives depuis la péninsule Ibérique, de marchands protestants venus de Flandre ou encore de huguenots français. En particulier, de nombreuses et prospères familles, issues d'autres provinces encore sous contrôle espagnol, rejoignent Amsterdam pour y trouver la sécurité. En 1685, le revenu par habitant est ainsi quatre fois supérieur à celui de Paris, écart qui se creuse d'autant plus avec la deuxième vague d'exil de huguenots fuyant la France, à la suite de la révocation de l'Édit de Nantes en 1685. Parmi les réfugiés, on compte notamment des hommes de science tels que Comenius ou encore des philosophes tels que René Descartes. Par ailleurs, l'afflux d'imprimeurs flamands, provenant notamment d'Anvers, et la tolérance intellectuelle qui règne à Amsterdam contribuent à donner à la ville son statut de centre européen de la liberté de la presse.

Le est considéré comme l'âge d'or d'Amsterdam car elle devient à cette époque la ville la plus riche du monde. La reprise d'Anvers par les Espagnols en 1585, qui voit les bouches de l'Escaut bloquées par les Provinces-Unies se traduit par un afflux massif de bourgeois protestants qui apportent savoir-faire et capitaux. Amsterdam est alors au cœur d'un réseau mondial de commerce maritime avec les pays de la mer Baltique, l'Afrique, l'Amérique du Nord, le Brésil ou encore les Indes orientales. C'est ainsi que les marchands amstellodamois possèdent la majorité des actions de la première grande multinationale de l’Histoire, la Compagnie néerlandaise des Indes orientales, créée en 1602, mais également de sa rivale, la Compagnie néerlandaise des Indes occidentales (1621). Ces deux sociétés ont fait l'acquisition de plusieurs territoires outremer, par la suite devenus des colonies néerlandaises. Les bateaux revenant d'Indonésie chargés de précieuses épices font la richesse de la ville. Amsterdam rayonne à cette époque à travers toute l'Europe, tant au niveau artistique avec Rembrandt et Vermeer, que financier avec la création d'une « banque de change » initialement censée faciliter les échanges de monnaie, mais qui devient rapidement un pourvoyeur de fonds pour les particuliers et les entreprises, ainsi que de la première bourse de valeurs au monde en 1611. C'est également le cas en génie civil, avec la construction des célèbres canaux ou de l'hôtel de ville, achevé en 1655 sous la supervision de l'architecte Jacob van Campen, considéré par les Amstellodamois comme la huitième merveille du monde.

Cette période faste se traduit par un accroissement important de la population dans la première moitié du , accompagné d'une expansion significative de la ville. Le nombre d'habitants passe ainsi de à au cours du siècle, en dépit de plusieurs épidémies de peste (1623–1625, 1635–1636, 1655 et surtout 1664). Les deux premières expansions majeures de la ville ont lieu à la fin du , avec le « Premier Plan » (') (1566-1585) marqué par un développement en direction de l'est de la ville vers le , au-delà du , puis du « Deuxième Plan » (') (1585-1593) dans la foulée. Cependant, ces deux expansions ne permettent pas d'absorber la population croissante et de répondre aux besoins nouveaux créés par l'activité économique florissante de la ville. Un nouvel agrandissement significatif est ainsi approuvé par les États de Hollande et de Frise-Occidentale en 1609. Cependant, étant donné les coûts significatifs que le projet implique, et la nécessité de réaménager et rehausser les nouveaux quartiers, il est finalement décidé de réaliser l'élargissement en deux étapes. Le « Troisième plan » ("") est ainsi mis en place entre 1613 et 1625 et marque le développement de plusieurs quartiers situés à l'ouest de la vieille ville, comme le , les ou encore le . Mais le principal chantier du plan est la mise en place de la première partie du , entre les berges de l' et l'actuel , et d'un nouveau mur d'enceinte au niveau du . Les travaux de construction d'un nouveau port et du nouveau bastion débutent en 1611. Une fois celui-ci achevé en 1613, la destruction de l'ancienne muraille permet de commencer le creusement des canaux : le (1613), le (1614) puis le (1615).

En dehors des anciennes limites de la ville, de nouveaux quartiers émergent plus ou moins légalement. Alors qu'une partie de cette nouvelle « avant-ville » se retrouve dans l'enceinte des nouvelles fortifications, l'autre partie (correspondant au futur ) est volontairement laissée à l'extérieur, afin de réduire les coûts et de limiter le risque d'insurrection. Entre 1613 et 1620, la plupart des fossés sont transformés en canaux, et les chemins en routes. L’organisation des rues devient plus régulière et de nombreux immeubles sont construits. Alors que le sol est rehaussé dans la ceinture de canaux, celui du resté inchangé ; différence jamais réduite.

Le « Quatrième Plan » ('), rendu nécessaire par la pression démographique et le développement de zones illégales aux abords du mur d'enceinte, est marqué par l'achèvement du ' et l'agrandissement du port. L'aménagement des , entre 1652 et 1660, permet à la ville de se doter de chantiers navals et d'un port de premier plan. Le projet d'élargissement des limites de la ville est approuvé en 1660 et les travaux s'étalent sur dix ans, entre 1662 et 1672. Les marchands et bourgeois les plus fortunés s'installent alors sur les bords des canaux parallèles du , du et du . L'architecte Daniël Stalpaert joue un rôle important dans cette expansion de la ville en 1658. Pour la réaliser, Amsterdam a naturellement besoin de renforts en main-d'œuvre. Des ouvriers, provenant à la fois du pays, mais également de l'étranger, affluent dans la ville et s'installent dans des taudis situés en périphérie des canaux, notamment dans le quartier alors marécageux du Jordaan. Leur présence contraste avec la .

Après l'hégémonie du Siècle d'Or, le voit le déclin de la prospérité de la ville. Les guerres contre la France, entre 1672 et 1713, et la guerre de Succession d'Autriche ont entraîné le développement d'une dette très importante, atteignant 767 millions de florins en 1795, dont 450 rien que pour la Hollande. Les Néerlandais, qui étaient les principaux transporteurs des marchandises de l'Europe, voient leurs clients et leurs fournisseurs créer leurs propres flottes de commerce et passer de moins en moins par leur intermédiaire. Les Actes de navigation, votés en Angleterre à partir de 1651, interdisent l'accès aux ports et colonies britanniques pour les pavillons des autres nations. Ces dispositions visent particulièrement les Provinces-Unies.

Une autre cause du déclin de la puissance commerciale néerlandaise est l'obsolescence progressive de ses techniques. Le développement d'un vaste marché en Europe de l'Ouest rend nécessaire la construction de navires d'un plus fort tonnage, afin de transporter davantage de marchandises. Si les chantiers navals néerlandais lancent des navires plus importants au qu'au , ceux-ci sont pourtant dépassés par ceux de leurs concurrents, tant pour ce qui est de la taille que du niveau technique. Les retards accumulés par les Néerlandais ont également pour conséquence un ensablement des chenaux des ports de commerce, à commencer par ceux du Pampus et du Marsdiep qui permettent d'accéder à Amsterdam. Dans les années 1770, quarante jours sont nécessaires pour que le navire de la Compagnie des Indes orientales "" puisse accoster à Amsterdam. La place est affectée, par ricochet, par la terrible Famine au Bengale de 1770, dans la zone conquise par les Anglais en Inde, déclenchant , dont celle de la Banque Clifford d'Amsterdam et de ses alliés.

La Quatrième guerre anglo-néerlandaise, qui oppose les Provinces-Unies et leur allié, le royaume de France, au royaume de Grande-Bretagne, de 1780 à 1784, permet à la puissance britannique de reprendre de nombreuses concessions coloniales dans les Indes néerlandaises. Cette défaite, couplée aux difficultés de la période franco-batave, marque la fin de l'hégémonie d'Amsterdam en Europe. Onze ans après son arrivée au pouvoir en France en 1799, parvient à étendre son empire jusqu'aux Pays-Bas, qui sont annexés durant le Premier Empire en 1810. Amsterdam acquiert ainsi le statut de troisième ville de l'empire, aux côtés de Paris et Rome. Cette nouvelle annexion survient seulement quinze ans après la naissance de la République batave, issue des Provinces-Unies en 1795, puis après l'instauration du Royaume de Hollande par Napoléon en 1806. Ce contexte instable porte préjudice à la ville d'Amsterdam, touchée de plein fouet par le déclin du commerce et du transport maritime, consécutif à l'ensablement des voies d'accès maritimes à la ville, à la réduction des échanges avec les colonies. En outre, le conflit entre la France et l'Angleterre anéantit la majeure partie des échanges avec le Royaume-Uni, à la suite de l'instauration du blocus continental. Le frère de , Louis, imposé comme souverain du Royaume de Hollande de 1806 à 1810, décide de faire d'Amsterdam sa capitale lors de son arrivée à La Haye, le 23 juin 1806. Le 20 avril 1808, il déménage vers la capitale, et s'installe dans l'hôtel de ville dont il fait un palais royal. Le gouvernement l'accompagne. En dehors du déplacement du Rijksmuseum depuis La Haye, le mandat de Louis Bonaparte n'est pas marqué par d'autres faits majeurs pour la ville d'Amsterdam.

Après l'éviction des troupes françaises par les armées russe et prussienne en 1813, le nouveau monarque de la Maison d'Orange-Nassau choisit de nouveau La Haye comme lieu de résidence, et comme siège des États généraux du royaume des Pays-Bas. Amsterdam reste cependant la capitale du Royaume des Pays-Bas de 1815 à 1830, aux côtés de Bruxelles. Bénéficiant de la volonté de Guillaume d'en faire un centre économique de premier plan, Amsterdam se voit attribuer le monopole du commerce avec les colonies, après la Révolution belge de 1830. C'est dans l'optique de renforcer la puissance de son port que sont lancés les premiers projets majeurs de canaux, comme le Canal de la Hollande-Septentrionale, inauguré en 1825.

Avec l'explosion de la natalité durant plusieurs décennies, liée à un renouvellement des échanges, à l'émergence d'industries nouvelles et à l'apparition de nouvelles activités comme les services financiers, la population connaît une forte croissance, passant de en 1830 à en 1900. La ville n'est pas préparée à une telle augmentation, et se retrouve surpeuplée. Alors que les conditions de vie des classes les plus défavorisées de la population deviennent de plus en plus difficiles, les premières initiatives philanthropiques font leur apparition, notamment pour améliorer les conditions de logement et d'hygiène des ouvriers. Le médecin Samuel Sarphati en devient l'une des principales figures ; il joue un rôle important dans la création d'un système de gestion des déchets et, en 1847, obtient l'autorisation de collecter les ordures au travers d'une nouvelle entreprise, baptisée "Maatschappij ter bevordering van Landbouw en Landontginning". Cette dernière a pour objectif de collecter les déchets (déjections, carcasses d'animaux, etc.) mais pas de nettoyer les rues, que leur insalubrité rend parfois impraticables. En 1852, il crée la ' dans le but de promouvoir le commerce, l'industrie et l'agriculture, ce qui conduit notamment à la construction du . En 1855, il fonde la « Société de fabrication de farine et de pain » (') qui propose du pain à un prix 30 % inférieur à celui des boulangeries. Toutes ces initiatives contribuent à l'amélioration des conditions de vie dans la ville, notables à partir de 1870. En dépit de la dégradation des conditions de vie, la ville prospère à nouveau économiquement, et de plus en plus de gens déménagent vers la capitale pour y tenter leur chance.

La très forte industrialisation à partir des années 1860 marque une nouvelle période d'expansion avec la création de nombreuses constructions et infrastructures. C'est à cette époque que sont construits deux musées, d'abord un édifice entièrement nouveau pour le Rijksmuseum (1885), puis le Stedelijk Museum (1895), mais aussi la salle de concert du Concertgebouw (1888) et la gare centrale d'Amsterdam (1889). À la même période, une ligne de défense est édifiée autour d'Amsterdam, sous la forme d'un réseau unique de quarante-deux forts et de terres inondables, afin de défendre la ville contre des attaques. Pour répondre à l'arrivée massive de travailleurs, des centaines de logements ouvriers sont construits dans de nouveaux quartiers périphériques constituant le ' (« Ceinture du »), pendant populaire du "Grachtengordel". Ces quartiers, parmi lesquels figurent De Pijp, le Kinkerbuurt et le Dapperbuurt, sont principalement financés par des banquiers et des spéculateurs et constituent la première expansion majeure de la ville en dehors des frontières adoptées au . Alors qu'ils concentrent essentiellement des classes moyennes inférieures, les classes les plus pauvres s'installent dans le Jordaan et dans les Oostelijke Eilanden. L'émergence de ces quartiers populaires contribue au fort développement du socialisme dans les années 1880-1890, lorsque de vives tensions avec les autorités émergent à un rythme quasi hebdomadaire, notamment lors de la manifestation du ', pendant laquelle sont tués par l'armée. Les années 1890 sont notamment marquées par la création de syndicats par les employés du port de la ville, désireux d'améliorer leurs conditions de travail.

Après plusieurs décennies difficiles, la seconde moitié du est marquée par une nouvelle vie pour la ville, souvent considérée comme un second âge d'or. La construction du Canal de la Mer du Nord en 1876, qui supplante le canal de la Hollande-Septentrionale contribue à faciliter les liaisons avec les grands ports et les grandes métropoles d'Europe, ouvrant de nouveaux horizons commerciaux à la ville. Avec le développement de la ville, les anciens ports du Damrak et des Westelijke Eilanden ne sont plus adaptés à la croissance des échanges. Un nouveau complexe portuaire, construit sur de nouvelles îles artificielles est créé, et prend le nom d'Oostelijk Havengebied (les entrepôts historiques sont aujourd'hui reconvertis en logements). Celui-ci permet notamment d'accueillir les navires de marchandises desservant les Indes orientales néerlandaises, ainsi que des flux de population immigrée. À la fin du siècle, la rive nord de l'IJ est également aménagée pour accueillir des usines et des zones portuaires. Placée aux avant-postes des profonds développements économiques et sociaux de la deuxième moitié du , Amsterdam acquiert le statut incontesté de capitale du pays. Vers 1900, près de la moitié de la population active de la ville travaille dans l'industrie.

Peu de temps avant la Première Guerre mondiale, la ville continue à s'étendre et de nouvelles banlieues sont construites, notamment au travers du proposé par H. P. Berlage en 1915 et approuvé par la ville en 1917. Même si les Pays-Bas sont restés neutres dans le conflit, Amsterdam a subi une importante pénurie de nourriture et de combustible pour le chauffage. En 1917, ces pénuries déclenchent des émeutes, connues sous le nom d""' (littéralement, la « rébellion de la pomme de terre »), au cours desquelles neuf personnes sont tuées, et plus de cent blessées. À la suite de cette révolte, les magasins et les entrepôts sont pillés dans le but de trouver des provisions et des denrées alimentaires.

L'entre-deux-guerres est marqué par la volonté de mettre en place un nouveau plan d'agrandissement de la ville, le « Plan général d'élargissement » ("") ou « AUP », approuvé par la municipalité en 1935. Ce dernier, développé par l'architecte Cornelis van Eesteren, se concentre autour de quatre axes forts : habitations, travail et loisir, avec comme dénominateur commun le réseau de transport. Les architectes et urbanistes mettent ainsi en avant des espaces qui privilégient « la lumière, l'air et l'espace », ce qui contraste fortement avec les précédents projets, dont les immeubles constituaient l'élément structurant. En raison des difficultés économiques, le plan n'est finalement réalisé qu'au lendemain de la guerre.

Amsterdam mérite encore son surnom de "Venise hollandaise", avec les chalands du Singel autrefois observé par le philosophe et lunetier Baruch Spinoza, son urbanisme central et régulier au long des canaux, avec son habitat caractérisé par un couloir d'eau et des portes étroites, au point qu'il faille opérer tout déménagement conséquent par les fenêtres, avec ses lieux de rencontre tardive, où la bière et la "nostalgie des îles" permettent de freiner la dérive du vague à l'âme.

Amsterdam se cloisonne encore en quartiers, celui des Juifs, où s'activent les tailleurs diamantaires pour les commandes de leurs patrons, mais déjà en allant vers le Zeedijk des réprouvés, sans oublier les quartiers d'affaires près de la banque Amstel ou de la bourse, où s'échangent encore sous titres financiarisés du café, du quinquina, du cocotier, du thé, du caoutchouc, du poivre, des cigares et des ananas. S'imposent aussi les alignements rectilignes des quartiers bourgeois, dont les habitats sont marqués par l'idéal puritain, affichant d'emblée la hiérarchie par la naissance des (très) bonnes familles et les marques de désignation quasi-seigneuriale de leurs personnalités ou individualités, exigeant partout la netteté et la propreté, la sécurité et la tranquillité, tout en gardant leur bien-être intérieur, l'argent et les revenus du commerce discrets. Le pouvoir municipal participe de cette rigueur puritaine, comme le bourgmestre ancien interdisait la danse le dimanche, imposait le silence religieux au moment du benedicite.

Lors de la Seconde Guerre mondiale, l'Allemagne envahit et prend le contrôle des Pays-Bas le 10 mai 1940. Face à la politique de persécution et d'extermination du peuple juif menée par le régime allemand, certains citoyens d'Amsterdam tentent de résister en cachant certains d'entre eux, en dépit des risques que cela comporte. Au cours du conflit, plus de d'Amsterdam sont néanmoins déportés, réduisant presque à néant la communauté juive de la ville. Parmi les plus célèbres, on peut notamment citer la jeune Anne Frank cloîtrée pendant 25 mois avec sa famille et des amis au-dessus d'un magasin du centre d'Amsterdam, avant d'être déportée au camp de concentration de Bergen-Belsen. À la fin de la Seconde Guerre mondiale, toutes les communications avec le reste du pays sont coupées et la nourriture et le carburant s'épuisent dangereusement. De nombreux citoyens partent alors dans les campagnes à la recherche de nourriture. Pour rester en vie, certains habitants sont forcés de manger des chiens, des chats, des betteraves sucrières, ainsi que des bulbes de tulipes réduits en pâte. La plupart des arbres de la ville sont également coupés pour servir de combustible, de même que la plupart des meubles et des boiseries provenant des appartements des juifs morts en déportation.
Au lendemain de la guerre, de nombreux nouveaux quartiers, tels que Osdorp, Slotervaart, ou Geuzenveld-Slotermeer sont construits conformément à l'AUP. Ces quartiers sont conçus avec de nombreux jardins publics et de grands espaces ouverts, ce qui leur vaut le nom de « villes jardin » (""). Les nouveaux immeubles offrent également un confort de vie accru avec des pièces plus grandes et plus claires, des balcons et des jardins. À la suite de la guerre et des autres incidents qui ont émaillé le , une grande partie de la ville a besoin d'être restaurée ou rénovée. Alors que la société connait une évolution importante, des politiciens et d'autres personnalités influentes conçoivent des projets visant à dynamiser des parties importantes de la ville, notamment avec des immeubles commerciaux et de nouveaux axes routiers accessibles au plus grand nombre.

Les années 1960 et 1970 ramènent Amsterdam au premier plan de l'actualité, non seulement pour des raisons économiques ou commerciales après-guerre, mais aussi à cause de la tolérance de la ville envers l'usage des drogues douces, qui en fait une ville de prédilection pour la génération hippie. Amsterdam joue ainsi un rôle central dans l'émergence du mouvement contestataire Provo, initié dans les happenings de l'artiste Robert Jasper Grootveld, sur le Spui, à partir de 1964. Mais les émeutes et les affrontements avec la police se multiplient, et de nombreux squatters sont expulsés par la force. En 1980, alors que la reine Beatrix prête serment lors de son accession au trône, les protestataires, composés en majorité de membres du « mouvement des squatteurs », affrontent la police à l'extérieur de la Nieuwe Kerk, au cours des « émeutes du couronnement ».

Un projet de développement d'une voie express circulant au-dessus du métro est également envisagé pour faciliter le trafic entre la gare centrale d'Amsterdam et le reste de la ville. Les travaux de rénovation débutent dans les anciens quartiers juifs. Les rues les plus petites, telles que la ' sont élargies, et quasiment tous les immeubles qui s'y trouvaient sont démolis. Les tensions liées aux démolitions atteignent leur paroxysme lors des travaux sur Nieuwmarkt, qui donnent lieu à des émeutes (les « ' ») au cours desquelles les habitants expriment leur colère contre la politique de reconstruction de la ville.

En conséquence, les travaux de démolition sont stoppés, et l'autoroute planifiée n'est finalement pas construite, contrairement au métro qui est développé selon les plans. Il est inauguré en 1977, entre le nouveau quartier de Bijlmer (situé dans l'actuel arrondissement de Zuidoost) et le centre d'Amsterdam. En définitive, seules quelques rues du quartier sont réaménagées et élargies. La nouvelle mairie de la ville est inaugurée sur Waterlooplein, la place principale qui a été quasiment intégralement démolie. Dans le même temps, de grandes entreprises privées, telles que la ' (« Redéveloppement d'Amsterdam »), sont créées dans le but de réhabiliter et restaurer l'ensemble du centre. Bien que les résultats positifs de cette politique soient visibles aujourd'hui, des initiatives visant à continuer le développement du centre sont toujours menées. L'ensemble de la ville a globalement bénéficié de cette politique, au point d'acquérir le statut d'aire protégée. De nombreux immeubles sont élevés au rang de monuments nationaux ('), et en juillet 2010, le "Grachtengordel" (Herengracht, Keizersgracht, et Prinsengracht) est ajouté à la liste du Patrimoine mondial de l'UNESCO.

Au début du nouveau millénaire, des problèmes sociaux tels que la sécurité, la discrimination ethnique et la ségrégation entre les groupes religieux et sociaux commencent à se développer. Quarante-cinq pour cent de la population d'Amsterdam sont constitués d'allochtones, issus principalement d'Europe et de pays tels que le Suriname, le Maroc, la Turquie ou les Antilles néerlandaises. Amsterdam se caractérise cependant par son apparente tolérance sociale et sa diversité. De janvier 2001 à février 2010, le maire en poste, Job Cohen, et son adjoint à l'intégration Ahmed Aboutaleb, mènent une politique fondée sur le dialogue social et la tolérance, accompagnée de nouvelles mesures sévères contre ceux qui enfreignent la loi.

La ville s’affirme au début du comme une capitale culturelle incontournable en Europe, avec des chantiers dont la liste est longue. De nombreux musées ont fait l’objet de travaux de rénovation importants. Ainsi, le musée de la Marine est réinauguré avec une nouvelle scénographie en 2011, le Stedelijk s’est vu adjoindre un nouveau bâtiment contemporain surnommé en 2012, le Rijksmuseum a subi d’importants travaux et a été réinauguré par la reine Beatrix en 2013 (et visité par Barack Obama l'année suivante) et le musée Van Gogh construit en 1973 s’agrandit en 1999 et se dote d’une nouvelle entrée en 2014.

Le quartier résidentiel de l'IJburg, construit à l'est de la ville sur des îles artificiellement créées, est un modèle de quartier durable que la ville expérimente face à la montée des eaux, et au besoin d'espace à proximité du centre-ville. L'Amsterdam Science Park est un autre exemple de nouveau quartier développé : construits à la place d'anciennes friches ferroviaires, les bâtiments abritent des laboratoires de recherche et une partie du campus étudiant de l'université de la ville. Dans un même temps, plusieurs voix se font entendre pour une piétonisation complète du centre-ville et des quartiers construits au , ce qui pourrait être réalisé dans les années 2020.

Située à l'ouest des Pays-Bas, Amsterdam fait partie de la province de Hollande-Septentrionale et est située à proximité immédiate de celles d'Utrecht et du Flevoland. La rivière Amstel vient se jeter dans l'IJ et est intégrée à un réseau de canaux qui parsèment la ville. Cette dernière est située à deux mètres au-dessus du niveau de la mer. Les terres autour de la ville sont plates et formées de grands polders. Au sud-ouest de la ville se trouve l'Amsterdamse Bos, une forêt artificielle. Enfin, la ville est reliée à la mer du Nord par le long canal de la Mer du Nord qui dessert son port.

La ville d'Amsterdam a une superficie totale de carrés, dont de terres. La densité de population absolue est donc de par km, mais est en réalité de sur la base des terres habitables, avec une offre en logements de par kilomètre carré. Les parcs et les réserves naturelles forment environ 14 % de la superficie de la ville. Les espaces verts et récréatifs (parcs, jardins, terrains de sport) représentent à eux seuls 11,3 % de la surface totale, tandis que les bois et forêts en représentent 2,3 %.

Amsterdam possède un climat océanique (BFC dans la classification de Köppen) fortement influencé par la proximité de la mer du Nord à l'ouest et avec des vents d'ouest dominants. Amsterdam, ainsi que la plus grande partie de la province de Hollande-Septentrionale, se trouve dans une zone de rusticité de type 8b, correspondant à une moyenne de température comprise entre -9,4 et pour la température annuelle la plus basse atteinte au cours des vingt dernières années. Les gelées se produisent principalement lorsque le vent provient de l'est ou du nord-est depuis l'Europe continentale. Toutefois, du fait de sa proximité avec de grandes étendues d'eau et d'un effet significatif d'îlot de chaleur urbain, les températures nocturnes tombent rarement en dessous de , contre à Hilversum située à au sud-est d'Amsterdam.

Les températures estivales sont modérément chaudes, avec une moyenne de au mois d'août, et quelques pointes à qui se maintiennent rarement plus de 3 jours d'affilée. Le record pour ce qui est de l'écart de température annuelle va de à . Les précipitations à Amsterdam sont fréquentes avec en moyenne 187 jours de pluie par an, la majorité des épisodes pluvieux se manifestant sous la forme de bruine ou de brèves averses. La moyenne annuelle de précipitations est de . Le mauvais temps (nuage et pluie) est surtout fréquent dans la période froide, d'octobre à mars.

D'après les chiffres publiés par la ville en 2013, Amsterdam comptait , soit une hausse de 1,2 % par rapport à 2012 et de 7 % par rapport à 2008. Sur la base de ces mêmes chiffres, les autochtones ne représentaient que 49,4 % de la population, ce qui signifie que 50,6 % de la population est d'origine étrangère. Le bureau central de la statistique avance quant à lui le chiffre de en 2013.

Au et au , les immigrants non néerlandais étaient principalement des huguenots, des Flamands, des Juifs séfarades ainsi que des Westphaliens. Les huguenots affluèrent massivement à la suite de la révocation de l'Édit de Nantes en 1685, tandis que les protestants flamands émigrèrent à la suite de la Guerre de Quatre-Vingts Ans. Les Westphaliens émigrèrent quant à eux pour des motifs économiques dans des flux qui continuèrent au et au . Avant la Seconde Guerre mondiale, 10 % de la population d'Amsterdam étaient de confession juive.

Au , la première vague massive d'immigration arriva d'Indonésie à la suite de l'indépendance des Indes orientales néerlandaises au cours des années 1940 et 1950. Au cours des années 1960, de nombreux travailleurs immigrèrent en provenance de Turquie, du Maroc de l'Italie et de l'Espagne. La proclamation de l'indépendance du Suriname en 1975 attira également de nombreux immigrés qui s'installèrent pour la plupart dans le quartier de Bijlmer. D'autres immigrés, parmi lesquels des réfugiés demandeurs d'asile, mais aussi des immigrés illégaux affluèrent des Amériques d'Asie et d'Afrique. Au cours des années 1970 et 80, de nombreux « Amstellodamois de souche » déménagèrent vers des villes nouvelles telles qu' Almere et Purmerend ou vers Het Gooi, notamment à la suite du troisième plan d'aménagement du territoire proposé par le gouvernement. Ce dernier promouvait le développement de zones suburbaines, et proposait de nouveaux projets dits de « centres de croissance » ("). À la suite de cette politique, de nombreux jeunes actifs déménagèrent vers De Pijp et le Jordaan, délaissés par les plus vieux habitants de la ville.

À l'instar des autres grandes villes néerlandaises, Amsterdam est une ville multiculturelle dont la moitié de la population est d'origine étrangère. Sur la base des chiffres de 2013, les autochtones représentaient 49,4 % de la population. En outre, 34,9 % de la population totale et 52,6 % des jeunes de moins de 18 ans sont originaires de pays situés en dehors de l'OCDE. En 2009, la ville recensait 176 nationalités différentes, ce qui en faisait la ville la plus diversifiée au monde.

Au cours des dernières décennies, la nature de la démographie religieuse de la ville a été fortement modifiée par des afflux massifs d'immigrés en provenance des anciennes colonies. Les immigrés en provenance du Suriname ont ainsi introduit le mouvement des frères moraves, variante du luthéranisme et du protestantisme, de même que l'hindouisme. En outre, différents mouvements de l'islam issus de différentes parties du monde se sont également développés. L'islam constitue ainsi aujourd'hui la principale religion d'Amsterdam, en dehors du christianisme. Les importantes communautés ghanéenne et nigériane ont également mis en place plusieurs mouvements religieux nouveaux (parfois appelés « Églises Africaines »), organisés pour la plupart dans des garages dans le quartier de Bijlmer où la plupart des populations originaires de ces pays sont installées. En outre, un nombre important de mouvements religieux ont établi des congrégations, comme le bouddhisme, le confucianisme ou l'hindouisme. Un des lieux les plus visibles de l'immigration aux Pays-Bas est le Dappermarkt, marché situé dans le quartier indonésien ("), et réputé pour la variété et l'exotisme de ses produits.

En dépit de la réputation de tolérance des Néerlandais et des Amstellodamois en particulier, l’augmentation des flux d'immigration, et l'augmentation associée du nombre de religions, et de cultures à la suite de la Seconde Guerre mondiale, ont suscité des tensions sociales et ethniques à plusieurs occasions. L'assassinat du réalisateur Theo van Gogh par un extrémiste musulman en 2004 en constitue l'un des exemples les plus frappants. La suppression de plusieurs chaînes en arabe ou en turc des bouquets de base proposés pour les abonnements au câble constitue un autre exemple du changement de politique des Néerlandais envers certaines minorités.

Au cours des dernières années, des critiques se sont élevées contre les hommes politiques ayant pris la décision de mener une partie de leur campagne dans des langues « minoritaires ». En particulier, l'actuel maire de la ville Eberhard van der Laan avait vivement critiqué, alors qu'il était ministre de l'Intégration, les candidats ayant distribué des prospectus dans des langues autres que le néerlandais. Certains dépliants avaient même été saisis à cette occasion. Cette prise de position fit réagir les défenseurs du multiculturalisme, et valut à Van der Laan de nombreuses critiques, y compris au sein de son propre parti, le PvdA. Pour autant, à la même période, la ville d'Amsterdam a également lancé un programme complet et gratuit de cours de néerlandais destiné aux immigrés, conformément à sa politique d'intégration par l'assimilation.

Jusqu'au 30 avril 2010, la municipalité d'Amsterdam, étendue sur près de , était divisée en quinze arrondissements (") répartis sous la forme de deux couronnes autour de Centrum. Ces quinze arrondissements présentaient une population très inégalement répartie (chiffres de 2007) :

Depuis le , un conseil local gouverne chaque arrondissement dont le nombre est réduit à huit. Toutefois, la zone industrielle et portuaire de Westpoort fait figure d'exception, car cet arrondissement très faiblement peuplé conserve son intégrité à la suite de la réforme territoriale de 2010 et est placé directement sous le contrôle de la municipalité d'Amsterdam. À l'issue de la réforme, les nouveaux arrondissements sont (avec les populations respectives au janvier 2014) :

D'après le bureau central de la statistique, la ville d'Amsterdam s'intègre dans différents ensembles statistiques à géométrie variable : la municipalité d'Amsterdam, l'agglomération métropolitaine d'Amsterdam, le Grand Amsterdam et l'aire métropolitaine d'Amsterdam. La plus petite entité, la municipalité, s'étend sur pour une population de en 2013. La municipalité s'est ainsi étendue au cours du temps via l'absorption des villages voisins d'Amsterdam-Zuidoost, Buiksloot, Driemond, Durgerdam, Holysloot, Nieuwendam, 't Nopeind, Oud Osdorp, Ransdorp, Ruigoord, Schellingwoude, Het Schouw, Sloten, Sloterdijk et Zunderdorp. L'agglomération métropolitaine (") intègre, en plus d'Amsterdam, les villes de Zaanstad, Wormerland, Oostzaan, Diemen et Amstelveen, pour une population de en 2013. La zone du Grand Amsterdam (') est une région dite « COROP » qui inclut pour une population de en 2013. Bien que possédant une surface largement plus étendue, le Grand Amsterdam ne possède une population que modérément plus importante que l'agglomération du fait de la non-prise en compte de la municipalité de Zaanstad ( en 2011). Enfin, l'aire métropolitaine d'Amsterdam (') est la plus peuplée avec d'habitants. Par rapport au Grand Amsterdam, cette zone inclut les villes de Zaanstad, Wormerveer, Muiden, Abcoude, Haarlem, Almere et Lelystad mais exclut la ville de Graft-De Rijp. Il faut noter par ailleurs qu'Amsterdam fait également partie de la conurbation de Randstad qui comprend notamment les villes d'Utrecht, La Haye et Rotterdam, peuplée par près de d'habitants.

Si les décisions locales sont prises au niveau de chaque arrondissement, les projets d'envergure qui concernent toute la ville, les infrastructures notamment, sont du ressort de la municipalité ("). Depuis le , le Conseil municipal d'Amsterdam est dirigé par une coalition entre les travaillistes du PvdA, les libéraux du VVD et les écologistes de GroenLinks, qui totalise ainsi 30 des 45 sièges. Ville souvent présentée comme modèle pour sa bonne gestion, il y a plus de vélos que d'automobiles de fonction pour les fonctionnaires. Outre le bourgmestres, il existe un poste de vice-bourgmestres, et depuis 2016, de qui est chargé de réguler les politiques concernant le vélo.

Liste des bourgmestres qui se sont succédé à la mairie d'Amsterdam depuis la Libération de la ville :

Amsterdam est une ville globalement sûre, à l'image des Pays-Bas. Elle figure ainsi au sur le plan de la sécurité dans le classement mondial de la qualité de vie dans les villes réalisé par le cabinet de conseil Mercer en 2011. La ville possède également un taux de criminalité nettement inférieur à celui de la majorité des villes européennes. Cependant, selon le ', classement national réalisé par le Algemeen Dagblad sur la base de données collectées auprès de la police, elle est la ville la moins sûre des Pays-Bas, devant Rotterdam et Eindhoven. En 2012, le nombre de plaintes enregistrées par la police de la ville s'est élevé à , en hausse de 1,5 % par rapport à 2011. Les vols représentent la majorité des infractions commises dans la ville, avec environ 60 % des plaintes. Les vols de vélos/scooters et mobylettes (12,2 % du total), ainsi que les vols dans des véhicules motorisés (11,9 %) sont les délits les plus fréquemment relevés. En outre, le nombre de plaintes pour vol à la tire (10,3 % du total) a connu une forte augmentation entre 2011 et 2012, passant de à .

L'amélioration de la sécurité dans la ville, et la réduction de la criminalité constituent deux objectifs importants pour la municipalité. Dans le cadre du programme « Faire des choix pour la ville », approuvé par la coalition entre le PvdA, le VVD et GroenLinks en avril 2010, un volet consacré à la sécurité, le "Veiligheidsplan" a été développé pour la période 2012-2014. L'objectif de ce programme est de réduire la criminalité en ciblant plusieurs quartiers et types de crimes, parmi lesquels la prostitution et le trafic d'êtres humains, la discrimination et les crimes racistes, ou encore la violence domestique. En 2007, la mairie a également lancé un programme de réhabilitation de l'hypercentre baptisé « Project 1012 », dont deux des principales cibles sont la prostitution et la vente libre de drogues douces.

À l'instar du Bénin, de la Bolivie et de la Côte d'Ivoire, la capitale des Pays-Bas n'est pas le siège du gouvernement. En effet, le siège du gouvernement, le Parlement (Binnenhof), la Cour suprême et le palais du Roi ont toujours été situés dans la ville de La Haye, dans la province de Hollande-Méridionale, à l'exception d'une brève période entre 1808 et 1810 sous Louis Bonaparte. Par conséquent, les ambassades étrangères s'y trouvent également. Amsterdam ne doit son statut de capitale du pays qu'à une seule et unique mention dans la Constitution néerlandaise, juxtaposant le terme de « capitale » et « Amsterdam ». Ainsi, à l'article 32 du chapitre 2 de la Constitution, il est fait mention que le Roi (ou la Reine) prête serment et est couronné dans « la capitale d'Amsterdam » ('). Les précédentes versions de la constitution ne mentionnaient que « la ville d'Amsterdam » ('), sans aucune mention d'une quelconque capitale. En outre, Amsterdam n'est pas non plus la capitale de la province de Hollande-Septentrionale, rôle tenu par la ville d'Haarlem. Amsterdam reste néanmoins la plus grande ville des Pays-Bas, ainsi que le centre économique et touristique du pays.

La ville d'Amsterdam a développé un ensemble de partenariats et programmes de coopération avec plusieurs villes et pays au niveau mondial. L'objectif premier de ces partenariats est de renforcer le positionnement culturel et économique de la ville au travers d'un transfert de compétences et d'expertise. La coopération entre la municipalité et les villes partenaires s'adresse en premier lieu à trois grands groupes de pays :

Les principaux partenariats sont listés ci-dessous :

Amsterdam est la capitale financière et commerciale des Pays-Bas, et constitue la cinquième ville d'affaires d'Europe après Paris, Londres, Francfort et Bruxelles. La ville se classe également au cinquième rang du classement "" des meilleures villes européennes où s'implanter pour les entreprises, toujours derrière le même trio de tête, et juste derrière Barcelone. Les principales qualités de la ville qui ressortent du classement sont la diversité des langues parlées, ainsi que l'accès aux marchés et la qualité des infrastructures de transport, à la fois nationales et internationales. Beaucoup de grandes entreprises et banques néerlandaises y possèdent leur siège social, parmi lesquelles AkzoNobel, Heineken, le groupe ING, Ahold, TomTom, Delta Lloyd ou Philips. Le siège mondial de l'entreprise américaine KPMG, ainsi que celui de KLM sont situés dans la ville voisine d'Amstelveen, où de nombreuses entreprises non néerlandaises se sont également installées, pour bénéficier de loyers moins élevés et se rendre propriétaire de leur terrain, chose rendue difficile par les tarifs prohibitifs appliqués à Amsterdam.

Bien que de nombreuses petites entreprises soient toujours situées autour des anciens canaux, celles-ci se délocalisent de plus en plus à l'extérieur du centre-ville. Le nouveau quartier d'affaires Zuidas (« Axe du Sud ») est devenu le nouveau centre névralgique du secteur financier et juridique. Cinq des plus grands cabinets d'avocats et des cabinets de conseils des Pays-Bas y sont en effet installés, à l'instar de Boston Consulting Group ou d'Accenture. Il existe trois autres centres financiers secondaires. Le premier, situé au nord-ouest autour de la gare de Sloterdijk, accueille notamment le journal De Telegraaf, Deloitte, l'entreprise municipale de transports publics (') et les services du fisc néerlandais ('). Le deuxième est localisé autour de l'Amsterdam ArenA, au sud-est, alors que le troisième est centré autour de la gare d'Amsterdam Amstel avec notamment la Rembrandt Tower (le plus haut gratte-ciel d'Amsterdam) et le siège social de Philips. La Bourse d'Amsterdam (AEX) est également un centre névralgique de l'activité amstellodamoise, située en plein le centre-ville entre la gare centrale et le Dam. La plus ancienne bourse du monde, qui fait maintenant partie d'Euronext, est restée l'une des plus grandes bourses européennes.

Amsterdam est également une destination très prisée pour la tenue de congrès internationaux et de réunions d'affaires. En 2009, les hôtels et centres de congrès de la ville ont accueilli, selon le Bureau des Congrès d'Amsterdam, 515 réunions internationales de plus de quarante participants et d'une durée minimale de deux jours. Ouvert en 1961, le RAI Amsterdam, situé dans l'arrondissement de Zuid, accueille chaque année une cinquantaine de congrès internationaux et environ soixante-dix salons et expositions. Une douzaine de festivals complète la programmation. Au total, cela représente une fréquentation annuelle de l'ordre de 1,5 million d'entrées.

Amsterdam est régulièrement citée parmi les principaux centres économiques mondiaux, et parmi les villes les plus dynamiques et agréables à vivre.

Selon la classification des villes mondiales établie par le groupe de travail ' (GaWC) de Jon Beaverstock, Richard G. Smith et Peter J. Taylor en 1998, Amsterdam se classe parmi les « villes alpha ». Elle figure toujours dans cette catégorie dans la version actualisée de l'étude de 2010 aux côtés entre autres de Milan, Pékin, Los Angeles, Francfort et Moscou. Dans le ' réalisé par ' de Tokyo en 2012, Amsterdam figure au mondial d'un classement reposant sur six familles de critères distinctes (Économie, recherche et développement, rayonnement culturel, habitabilité, environnement et accessibilité). En 2012 également, Amsterdam se classe au mondial du ' de l"Economist Intelligence Unit" sur la base de la capacité à attirer les capitaux, les entreprises, les talents ainsi que les visiteurs. De même, le cabinet de conseil en stratégie A.T. Kearney fait figurer Amsterdam au de son "Global Cities Index" sur la base de cinq critères (activité économique, capital humain, échange d'informations, rayonnement culturel et engagement politique).

Sur le plan de la qualité de vie, Amsterdam figure au mondial de l'étude ' de l' derrière Hong Kong, et au du classement établi par le cabinet de conseil Mercer. Les deux études furent réalisées en 2012 et 2014 respectivement. Ce que mettent en avant les divers travaux sociologiques, c'est, au-delà de la richesse culturelle de la ville, et de ses atouts aquatiques naturels, l'engagement pris par ses habitants pour améliorer le cadre de vie de la communauté. Ainsi, à titre d'exemple, l'association citoyenne "Bankjescollectief", propose, chaque premier dimanche du mois en été, d'installer des bancs mobiles en bas des immeubles pour y créer un espace de voisinage. Cependant, certains habitants prennent eux-mêmes certaines initiatives, dont la plus connue reste, à Amsterdam, l'arrosage des plantes de la chaussée urbaine devant son logement.

Le Port d'Amsterdam est le deuxième des Pays-Bas, derrière celui de Rotterdam. Sur la base des chiffres 2010, il se classait au européen sur la base du tonnage de marchandises, derrière ceux d'Anvers et de Hambourg. Il est situé sur le canal de la Mer du Nord et sur les rives de l'IJ. Par le canal de la Mer du Nord il est relié à la mer du Nord, tandis que Den Helder est accessible par le canal de la Hollande-Septentrionale ; par l'IJ il est relié au Markermeer et à l'IJmeer et à la Rhénanie par le canal d'Amsterdam au Rhin. L'un des avantages de la localisation du port est que la zone portuaire n'est pas soumise aux marées, étant uniquement accessible via les écluses d'IJmuiden qui se situent à l'est du port d'IJmuiden (qui lui est soumis aux marées). Il se trouve à un niveau inférieur de deux mètres aux grandes marées.

Jadis grand port de mer pour les expéditions vers les Indes orientales ou occidentales, Amsterdam a vu ses entrepôts gorgés de marchandises coloniales se transformer en monuments historiques à Entrepotdok. Les tableaux accrochés dans les demeures de riches marchands ont rejoint les musées. Menacé par le voisinage du port géant de Rotterdam, Amsterdam a réagi en modernisant ses vétustes installations d'ancien port colonial.

Amsterdam est l'une des destinations touristiques les plus prisées d'Europe avec près de 5,3 millions de visiteurs ayant séjourné dans un hôtel ou une auberge en 2012, contre 4,6 millions en 2009. Il faut noter que ce chiffre n'inclut pas les quelque 16 millions de personnes qui ne visitent la ville qu'une journée sans y séjourner. Au total, le secteur du tourisme concentre quelque emplois (soit 9 % du total). Le nombre de visiteurs annuels est en constante augmentation depuis une dizaine d'années, ce qui s'explique principalement par l'afflux de visiteurs européens qui constituent à eux seuls 76 % des touristes. Au sein de cette catégorie, les Néerlandais (19 %), les Britanniques (13 %) et les Allemands (11 %) en constituent les principaux contingents. Sur la base de l'origine de la clientèle des hôtels, les Américains constituent le plus grand groupe de touristes non-européens avec 11 % des visiteurs.

Au juillet 2012, la ville compte 398 hôtels offrant plus de et plus de lits. Les deux tiers des hôtels sont localisés dans le centre-ville avec un taux d'occupation des chambres de l'ordre de 75 % en 2011, contre 72 % en 2010. Cela représente une forte hausse par rapport à 2009 (69 %), mais toujours moins que le record de 2006 (78 %). Ces chiffres doivent cependant être mis en regard avec la forte hausse de l'offre en hôtels, le nombre de chambres ayant augmenté de 8 % entre 2011 et 2012. Quatre campings situés dans l'enceinte de la ville, sur un total de 22 dans la région, attirent chaque année entre et .

La prostitution, symbolisée par le « quartier rouge » de De Wallen, ainsi que la vente libre de drogues douces et principalement de cannabis dans les coffee shops, sont deux images traditionnellement associées à la ville d'Amsterdam. La prostitution légale est limitée géographiquement aux « quartiers rouges », qui consistent en un réseau de ruelles contenant plusieurs centaines de cabines louées par des travailleuses du sexe. Celles-ci offrent leurs services derrière une porte vitrée généralement éclairée de rouge. Le quartier rouge le plus connu d'Amsterdam est "De Wallen", qui est devenu au fil des années une importante attraction touristique. Cependant, il est également possible de trouver des cabines dans le quartier du Spui et au sud du Singelgracht. En outre, Amsterdam n'est pas la seule ville des Pays-Bas dans laquelle il existe des quartiers rouges ; d'autres villes comme Rotterdam ou La Haye disposent également de leurs propres "". Premier coffee shop de la ville, le Bulldog ouvre ses portes en 1975. Le nom de « coffee shop » est alors utilisé pour désigner un endroit où il était possible d'acheter des boissons chaudes comme des cafés tout en pouvant fumer du cannabis. De nombreuses autres enseignes ouvrent leurs portes par la suite, avec une croissance exponentielle qui porte leur nombre à près des 550 adresses au début des années 1990. À la date de décembre 2012, Amsterdam comptait quelque 220 coffee shops, soit plus du tiers du nombre total aux Pays-Bas, qui est d'environ 650.

Au cours de l'été 2007, la mairie d'Amsterdam a lancé un programme de réhabilitation de l'hypercentre (c'est-à-dire la partie délimitée par le Singel), avec le double objectif d'y réduire la criminalité, et de mettre ses ressources en valeur. Ce programme, baptisé « Project 1012 », en référence au code postal de la vieille ville (') englobe une multitude d'initiatives et mises à jour des textes de loi. La réduction de la prostitution, à la fois dans le quartier rouge du Singel, et dans celui de De Wallen, autour du Oudezijds Achterburgwal et des rues attenantes, ainsi que celle du nombre de coffee shops, constitue l'un des principaux axes du programme. L'objectif est ainsi de réduire de 40 % le nombre de vitrines, qui était de 482 en 2007. En ce qui concerne les débits de drogue, la municipalité s'est fixé pour objectif de fermer 26 coffee shops, en ciblant les adresses clés pour la réhabilitation du quartier, ainsi que les principaux axes de circulation. Pour ce faire, la ville dispose de la possibilité de ne pas renouveler les licences des propriétaires, qui sont octroyées pour une durée de trois ans. Les dernières licences ayant été délivrées le septembre 2009, la fermeture des coffee shops ne sera donc possible qu'entre le septembre 2012 et le 31 août 2015. À l'échelle de l'ensemble de la ville, la municipalité espère en faire fermer 70, ce qui ramènerait leur nombre à environ 150. La politique de restriction de l'accès aux coffee shops lancée par le gouvernement en 2012, et qui consiste à contrôler si les consommateurs sont résidents du pays n'est pas appliquée à Amsterdam en date de septembre 2013. Les touristes étrangers peuvent donc y acheter librement des drogues douces. En novembre 2010, le maire de la ville Eberhard van der Laan s'était en outre opposé à l'introduction d'une carte d'accès aux coffee shops (le ') en expliquant qu'un tel système ne ferait que favoriser les trafics et la vente illégale dans les rues de la ville.

Les commerces d'Amsterdam vont des grands magasins comme De Bijenkorf fondé en 1870 ou la Maison de Bonneterie, un magasin de style parisien fondé en 1889, à de petites boutiques spécialisées. Les boutiques haut de gamme se trouvent principalement dans les rues Pieter Cornelisz Hooftstraat (souvent abrégée en « P.C Hooftstraat » ou « PC ») et Cornelis Schuytstraat, situées à proximité du Vondelpark. Deux des rues les plus animées d'Amsterdam sont l'étroite rue médiévale de Kalverstraat, située en plein cœur de la ville à proximité de la place du Dam et Nieuwendijk qui correspond à son prolongement au nord de la place. Parmi les zones commerçantes principales, les ' (littéralement « Neuf petites rues ») sont constituées de neuf ruelles étroites au sein du Grachtengordel, le système de canaux concentriques. Warmoesstraat, l'une des plus anciennes rues de la ville, est connue pour ses nombreux coffee shops, sex shops et pour être le centre névralgique de la communauté cuir de la ville. Les rues d'Haarlemmerdijk et d'Haarlemmerstraat ont pour leur part été désignées comme meilleures rues commerçantes des Pays-Bas en 2011. Alors que les ' sont majoritairement dominées par les boutiques de mode, "Haarlemmerstraat" et "Haarlemmerdijk" offrent une très grande variété de magasins : bonbons, lingerie, chaussures de sport, vêtements de mariage, décoration intérieure, livres, vélos, skatewear, charcuterie italienne, etc.

Le Bloemenmarkt est un marché aux fleurs permanent. Situé sur le Singel et s'étendant entre Muntplein et Koningsplein, il constitue l'unique marché aux fleurs flottant au monde. Les boutiques sont situées sur des bateaux arrimés au bord du canal, ce qui est un héritage de l'époque à laquelle tous les arbres et plantes devaient être acheminés quotidiennement depuis l'extérieur de la ville via les canaux. La ville dispose également d'un grand nombre de marchés en plein air tels que le marché Albert Cuyp, Westerstraat, Ten Kate et le Dappermarkt. Certains de ces marchés fonctionnent quotidiennement, comme les marchés Albert Cuyp et Dapper, très prisés des touristes et reconnus pour la variété et l’exotisme des produits qui y sont proposés. D'autres, comme le marché de Westerstraat, sont organisés sur une base hebdomadaire.

La ville d'Amsterdam est caractérisée par la présence de nombreuses brasseries allant des petits établissements artisanaux indépendants aux plus grands groupes multinationaux. Le groupe Heineken International, troisième brasseur au niveau mondial (en 2011, part de marché mondial en volume de 8,8 %) derrière InBev (18,3 %) et SABMiller (9,8 %) et qui commercialise plus de 250 marques de bières et cidres est ainsi basé dans la capitale néerlandaise, à proximité de sa brasserie historique qui a fermé ses portes en 1988 pour laisser place au Heineken Experience. La brasserie historique de la marque grand public Amstel se trouvait quant à elle sur le Mauritskade avant de déménager à Zoeterwoude.

Parmi les brasseries artisanales les plus populaires, la Brouwerij 't IJ, située à proximité du moulin de De Gooyer, propose une large gamme de bières biologiques. Elle brasse chaque année un volume supérieur à . Sur le même modèle, la Brouwerij De Prael, de plus petite taille, se destine en priorité aux amateurs de bières spéciales. Organisé chaque année au mois d'avril, le "Meibock Festival" permet aux amateurs de déguster les meilleures bières de printemps des Pays-Bas et des régions alentour.
Des marques de mode telles que G-Star, Gsus, BlueBlood, Iris van Herpen, 10Feet ou Warmenhoven & Venderbos, ainsi que des créateurs de mode comme Mart Visser, Viktor & Rolf, Sheila de Vries, Marlies Dekkers et Frans Molenaar sont basés à Amsterdam. Les agences de mannequins comme Elite, Touche et Tony Jones ont ouvert des succursales à Amsterdam. Il est à ce titre important de préciser que les top-modèles Yfke Sturm, Doutzen Kroes et Kim Noorda y ont commencé leur carrière.

Le point névralgique de la mode à Amsterdam est situé au World Fashion Center. Par ailleurs, des bâtiments du quartier rouge qui abritaient auparavant des maisons closes ont été convertis en ateliers pour les jeunes créateurs de mode tels qu'Eagle Fuel.

La ville d'Amsterdam possède l'un des plus grands patrimoines culturels et architecturaux d'Europe. Comme presque toute la ville, canaux compris, se trouve sous le niveau de la mer, les bâtiments anciens ou modernes sont posés sur pilotis qui s'appuient sur des couches de sable plus ou moins profondes. La majeure partie de la ville a été bâtie au Siècle d'or néerlandais, le long des nouveaux canaux concentriques qui ont été construits en grande partie grâce à la richesse accumulée par le commerce triangulaire. Jusqu'au , la ville s'ouvre sur son port et sur le Zuiderzee, dont elle est séparée par la construction de la grande gare centrale posée sur .

La ville des , presque entièrement préservée, témoigne d'un plan d'expansion urbain qui fut le plus grand et le plus homogène de son époque. Il constitue en cela un modèle de développement urbain à grande échelle, qui fut utilisé comme référence dans le monde entier jusqu'au .

Amsterdam est aujourd'hui considérée comme une référence en matière d'urbanisme. Cela tient au fait que la croissance de la ville s'est effectuée de manière continûment planifiée depuis le , ce qui demeure une exception en Europe. En particulier, la ville a échappé au développement urbain anarchique qui a accompagné la révolution industrielle dans de nombreux pays du Vieux Continent, en partie en raison du retard pris par les Pays-Bas dans ce processus. La demande en logements de la ville a ainsi connu une forte croissance dans le dernier quart du , à une période où l'urbanisme est devenu une préoccupation majeure. Par ailleurs, près de deux siècles se sont écoulés entre les plans d'agrandissement ambitieux du et la reprise démographique post révolution industrielle, ce qui a favorisé un développement harmonieux de la ville.

Amsterdam compte également sur son territoire 8 moulins traditionnels, le plus connu mais non visitable étant De Gooyer.

Amsterdam possède une riche histoire architecturale dont l'une des meilleures illustrations est le bâtiment le plus ancien de la ville, l'Oude Kerk (la « vieille église »), située au cœur du quartier de "De Wallen" et qui a été consacrée en 1306. Le plus vieux bâtiment en bois remonte à 1425 ; il s'agit de la "" (« maison de bois » en ancien néerlandais) qui se trouve dans le Begijnhof. Il s'agit de l'un des deux seuls bâtiments en bois encore présents à Amsterdam et de l'un des rares exemples toujours visibles d'architecture gothique. En effet, les bâtiments en bois, trop vulnérables aux flammes, ont été rasés au pour laisser place à des matériaux non inflammables. À cette même époque, de nombreux bâtiments sont construits dans le style architectural Renaissance. Les bâtiments de cette période sont très reconnaissables à leurs façades à pignons à redents, caractéristique de la Renaissance néerlandaise. Amsterdam développe même rapidement sa propre architecture Renaissance, qui repose sur les principes de l'architecte Hendrick de Keyser, à l'image de la Westerkerk, conçue selon ses plans. Au , l'architecture baroque devient très populaire, comme à travers toute l'Europe, grâce notamment aux architectes Jacob van Campen, Philips Vingboons et Daniel Stalpaert. Philips Vingboons conçoit notamment de splendides maisons de négociants à travers toute la ville.

Largement influencée par la culture française, l'architecture baroque se développe fortement tout au long du comme en témoigne le Palais Royal sur le Dam. Vers 1815, les architectes rompent avec le style baroque et commencent à construire des bâtiments de Style néo-gothique. À la fin du , le style Art nouveau devient à la mode et beaucoup d'architectes optent pour ce nouveau style très populaire. Du fait de la très forte expansion de la ville d'Amsterdam à cette époque, beaucoup de bâtiments arborent ce style à proximité du centre-ville ou autour de Museumplein. Le style Art déco, et sa variante locale de l'École d'Amsterdam, se développent au cours de la première moitié du , notamment dans le quartier du Rivierenbuurt. L'une des caractéristiques notables du style de l'École d'Amsterdam est l'utilisation de façades très décorées et fleuries, avec des fenêtres et des portes de formes irrégulières.

Le vieux centre-ville constitue donc un vaste creuset qui regroupe tous les styles architecturaux d'avant la fin du . Les styles Art déco et georgien se retrouvent quant à eux principalement à l'extérieur du centre-ville dans les quartiers construits au début du . La majorité des bâtiments historiques du centre-ville est constituée de bâtiments à pignons, dont les grandes maisons de marchands qui bordent les canaux constituent la meilleure illustration.

Les façades à pignons de formes différentes signent les architectures de chaque époque :

Le système de canaux d'Amsterdam est le résultat d'une politique d'urbanisme réfléchie. Au début du , lors de l'apogée de l'immigration, un plan complet est élaboré sur la base de quatre demi-cercles concentriques de canaux dont les extrémités émergent dans la baie de l'IJ. Les travaux s'inscrivent dans un programme de développement ambitieux impliquant l'assèchement de terrains marécageux. Trois canaux sont réservés au développement résidentiel : le Herengracht (« Canal des Seigneurs » en référence aux "Heren Regeerders van de stad Amsterdam", les seigneurs régnants sur Amsterdam), le Keizersgracht (littéralement le « Canal de l'Empereur ») et le Prinsengracht (le « Canal du Prince »). Construits au cours de l'Âge d'or néerlandais, ils forment ce que l'on appelle la « courbure d'or » ('). Le quatrième et le plus périphérique des canaux est le Singelgracht, dont il est rarement fait mention sur les cartes dans la mesure où il s'agit d'un terme générique pour tous les petits canaux périphériques. Ce canal ne doit pas être confondu avec le Singel, l'ancien canal qui ceinturait la ville médiévale et qui est situé plus au centre de la ville, dans ce qui constitue l'hypercentre (').

Les canaux ont longtemps été utilisés pour la défense militaire, la gestion de l'eau et le transport. Les défenses de la ville n'ont, semble-t-il, jamais pris la forme de superstructures de maçonnerie, telles qu'une muraille par exemple, mais étaient plutôt composées de douves et de digues en terre percées de quelques portes aux points de transit. Si les plans originaux des canaux ont été perdus, les historiens considèrent que l'arrangement en demi-cercles concentriques est davantage dû à des considérations pratiques et défensives, plutôt qu'à un but purement décoratif.

La construction du système de canaux, dans un premier temps jusqu'au Leidsegracht, commence dès 1613. Celle-ci se fait d'ouest en est, à la manière d'une toile d'araignée, et non de manière concentrique en partant du centre pour rejoindre l'extérieur. Les travaux de la dernière portion du canal entre le "Leidsegracht" et l'Amstel débutent entre 1658 et 1662, mais ne sont toujours pas totalement terminés en 1679. La partie orientale du réseau de canaux, correspondant à l'actuel Plantage ne voit cependant jamais le jour, et la ceinture de canaux ne rejoint pas directement la baie de l'IJ à l'est. À partir de la fin des travaux, des quartiers résidentiels y sont lentement édifiés. Au cours des siècles suivants, des parcs, des maisons pour personnes âgées, des théâtres et d'autres établissements publics s'y installent de manière quasi-anarchique. Au fil du temps, plusieurs canaux ont été comblés et ainsi transformés en rue ou en place, à l'instar de Nieuwezijds Voorburgwal ou du Spui.

Les canaux d'Amsterdam ont valu à la capitale des Pays-Bas son surnom de « Venise du Nord ». Ils s'étendent en effet sur plus de cent kilomètres, avec environ qui les traversent, reliant environ quatre-vingt-dix îles. Les quatre premiers canaux sont séparés par des bandes de terre de 80 à de largeur, tandis que la distance entre le quatrième et le cinquième peut aller jusqu'à environ (limite nord du quartier de Jordaan). Ces canaux sont également reliés par d'autres qui leur sont perpendiculaires, comme le Brouwersgracht, le Leidsegracht ou le Reguliersgracht.

Le , les canaux d'Amsterdam ont obtenu le label patrimoine mondial l'Unesco sous l'intitulé « Zone des canaux concentriques du à l'intérieur du Singelgracht ».

Après le développement des canaux en deux phases au , la ville ne croît quasiment pas au-delà de ses frontières en l'espace de deux siècles. Au cours du , Samuel Sarphati conçoit l'idée d'un développement calqué sur le plan du Paris et du Londres de l'époque. Il envisage ainsi de construire de nouvelles maisons, des bâtiments publics et un ensemble de rues immédiatement à l'extérieur du Grachtengordel. L'objectif principal reste néanmoins l'amélioration de la santé publique. Bien qu'elle ne connaisse pas une forte expansion à cette époque, Amsterdam voit l'érection de plusieurs des grands bâtiments publics encore existant à ce jour, comme le "Paleis voor Volksvlijt" (« Palais de l'Industrie »). À la suite de Sarphati, Van Niftrik et Kalff conçoivent au un anneau qui englobe tous les quartiers autour du centre de la ville, tout en conservant la propriété de toutes les terres qui séparent ce nouvel anneau de la limite de la ville du , pour mieux en contrôler le développement. Par la suite, la plupart de ces nouveaux quartiers construits voient s'installer la classe ouvrière de l'époque.

Le manque d'espace et l'entassement des habitants constituent deux freins majeurs au développement de la ville. Alors que les modèles développés en Europe visent à combiner rénovation des quartiers anciens et expansion périphérique, la priorité est donnée au second, en partie à cause de l'étendue du vieux centre, et du morcellement de l'espace par les canaux. La diversité et l'ancienneté des immeubles rendent quasiment impossible une « haussmannisation » du centre historique, sur le modèle de Bruxelles. Il est cependant décidé de gagner de l'espace sur les canaux en lançant des projets majeurs de comblement, comme sur le Spui, où il est également envisagé de développer les transports en commun. Ce processus sera maintenu jusqu'aux années 1950, le comblement du Rokin en constituant le dernier grand chantier. La fin du est marquée par la destruction de nombreuses habitations au profit de grands magasins comme De Bijenkorf, ou la construction de sièges d'entreprises comme celui de la Société de commerce néerlandaise.

En réponse à la surpopulation de la ville, deux plans sont conçus au début du , en rupture totale avec ce qu'Amsterdam a connu auparavant : le « "Plan Zuid" » conçu par l'architecte Hendrik Petrus Berlage, et le « "Plan Ouest" ». Ces plans prévoient le développement de nouveaux quartiers composés de grands ensembles de logements en s'assurant d'une certaine mixité sociale. Après la Seconde Guerre mondiale, de grands quartiers sont construits à l'ouest, au sud et au nord de la ville, afin de soulager la pénurie de logements et de fournir des logements à prix abordable avec toutes les commodités modernes. Ces nouveaux quartiers sont constitués de grands blocs d'habitation entrecoupés par des espaces verts et reliés à de larges routes pour favoriser la circulation automobile. Les banlieues de l'ouest de la ville construites à cette époque sont surnommées les "Westelijke Tuinsteden" (littéralement, les « banlieues occidentales »), alors que la zone située au sud-est de la ville est connue sous le nom de "Bijlmer". Témoins de la relance de la construction de logements, plus de la moitié des logements existants aujourd'hui dans la ville ont été bâtis après 1945.

La ville d'Amsterdam regorge de parcs, de grands espaces ouverts et de places. Les espaces verts représentent ainsi environ 12 % de la surface de la ville qui compte quelque à . Le Vondelpark, le plus célèbre parc de la ville, est situé dans l'arrondissement Oud-Zuid (littéralement « Vieux-Sud ») et tient son nom du célèbre auteur amstellodamois du , Joost van den Vondel. Chaque année, il attire environ de visiteurs. S'y trouvent notamment un théâtre de plein air, une aire de jeux, plusieurs établissements de restauration et des terrasses de café. Le Beatrixpark, du nom de la reine Beatrix, se situe dans l'arrondissement Zuid, au sud de la ville. Entre Amsterdam et la ville d'Amstelveen, se trouve l'Amsterdamse Bos (« Forêt d'Amsterdam »), la plus grande zone de loisirs de l'agglomération. Chaque année, près de de personnes visitent le parc dont les correspondent à environ trois fois la taille de Central Park à New York. Au sud de la ville, à proximité du moulin à vent de Rieker, on trouve l'Amstelpark, qui renferme une galerie d'art, une roseraie, un labyrinthe et des animaux. Le quartier de Plantage abrite non seulement l'Artis, un parc zoologique de plus de avec également un aquarium et un planétarium, mais également le Jardin botanique d'Amsterdam, un jardin botanique qui possède plusieurs serres tropicales dont une avec des papillons en liberté. D'autres parcs peuvent également être cités comme le Sarphatipark dans le quartier De Pijp, l'Oosterpark et le Flevopark dans l'arrondissement d'Oost, le Westerpark dans le quartier éponyme, ou le Rembrandtpark dans l'arrondissement d'Oud-West.

La ville dispose de quatre plages, la plage Nemo, Citybeach « "" » (Silodam), Blijburg, et Amsterdam-Noord.

De nombreux grands espaces ouverts sont également présents dans le centre-ville d'Amsterdam, au premier rang desquels on peut citer le Dam, grande place sur laquelle sont situés le palais royal et le "National Monument", ou encore Museumplein, une grande zone recouverte de pelouse où sont regroupés les musées du Rijksmuseum, le musée Van Gogh et le Stedelijk Museum. Parmi les autres grandes places d'Amsterdam, on peut citer également Rembrandtplein, Muntplein, Nieuwmarkt, Leidseplein, le Spui, Frederiksplein et Waterlooplein toutes situées au centre ville.

La ville d'Amsterdam est caractérisée par une multitude d'églises aussi bien catholiques que protestantes, qui témoignent de l'histoire religieuse de la ville et du pays. Symbole de la lutte entre les deux cultes à la suite de la Réformation, la Krijtberg (1642), ancienne église clandestine catholique de l'époque des Provinces-Unies, constitue l'une des nombreuses églises de ce type (les "Schuilkerken"), qui se développèrent alors que les cultes autres que le calvinisme étaient tolérés à condition qu'aucun signe extérieur ne soit apparent. Ons' Lieve Heer op Solder est également dans cette situation : construite entre 1661 et 1663 dans un grenier par un riche marchand catholique, elle était clandestine. Les autorités avaient eu vent de l'édifice religieux caché, mais appliquaient une politique de tolérance, puisqu'elle était hébergée dans une maison, et que les fidèles entraient discrètement par les ruelles pour y prier. Le bâtiment est aujourd'hui classé et devenu un musée.

La Oude Kerk (« vieille église »), construite en 1306 et ayant pour Saint-Patron Nicolas de Myre, est la plus ancienne église de la ville et constitue également l'un des plus anciens monuments d'Amsterdam. Initialement construite sous la forme d'une église romane catholique, elle devint une église calviniste à la suite de la Réformation en 1578. Elle fut construite sur un ancien cimetière, et continua à accueillir les corps de citoyens de la ville jusqu'en 1865. Au total, elle compte où sont enterrés parmi lesquels Jacob van Heemskerk, Frans Banning Cocq ou encore Saskia van Uylenburgh, la femme de Rembrandt. Elle se trouve aujourd'hui sur Oudekerksplein, en plein cœur du Red Light District.

Contrairement à ce que son nom pourrait laisser penser par opposition à la "Oude Kerk", la Nieuwe Kerk (« nouvelle église »), située sur le Dam fut bâtie seulement un siècle plus tard, et achevée en 1408. Construite dans un style gothique, elle est l'église nationale des Pays-Bas mais aussi un lieu majeur d'expositions. En particulier, elle est le lieu des investitures des souverains des Pays-Bas. Les reines Wilhelmine, Juliana, Béatrix et le roi Willem-Alexander y ont été intronisés. Le , y a été célébré le mariage de Willem-Alexander, prince d'Orange avec Máxima Zorreguieta Cerruti.

Située à proximité de la gare centrale d'Amsterdam, l'église Saint-Nicolas d'Amsterdam est la plus grande église catholique de la ville. Elle fut érigée entre 1884 et 1887 par l'architecte Adrianus Bleijs et constitue la troisième église de la ville à porter le nom de Saint-Nicolas. En outre, quatre églises datant du et désignées par un point cardinal, sont situées dans le centre de la ville. La Noorderkerk (« église du Nord »), construite spécialement pour les habitants du Jordaan, est de taille modeste. La Westerkerk (« église de l'Ouest »), située sur le Prinsengracht, constitue en revanche la plus grande église des Pays-Bas et est devenue l'un des symboles de la ville, notamment en raison de son architecture particulière, de la couronne de l'empereur Maximilien d’Autriche qui la recouvre et de son carillon ornant son clocher. La Zuiderkerk (« église du Sud »), située vers le Nieuwmarkt, fut quant à elle la première église de la ville à être construite spécialement pour le culte protestant entre 1603 et 1611. Enfin, la Oosterkerk (« église de l'Est »), également de taille modeste, n'est plus utilisée pour les services religieux depuis 1962.

Au cours de la dernière partie du , les "Rederijkerskamers" (« Chambres de rhétorique ») d'Amsterdam, à l'image de De Egelantier, organisent des concours entre les différentes chambres de lecture de poésie et de théâtre. La création de l'Académie en 1617 permet à Amsterdam de compter les cercles littéraires les plus réputés des Provinces-Unies au . En 1637, Amsterdam bâtit son premier théâtre, conçu par Jacob van Campen, où des spectacles de ballet sont donnés dès 1642. Au , le théâtre français devient populaire. Il y a peu de productions nationales d'opéra au cours du alors qu'Amsterdam est sous l'influence de la musique allemande. Le "Hollandse Opera" est construit en 1888 pour y remédier et promouvoir l'opéra néerlandais. À cette époque, la culture populaire est centrée autour du vaudeville et du music-hall autour de la zone Nes à Amsterdam. Le métronome, l'une des avancées les plus importantes de la musique classique européenne, est inventé en 1812 à Amsterdam par Dietrich Nikolaus Winkel. À la fin de ce siècle, le musée Rijksmuseum, Stedelijk et le Concertgebouw sont construits. Avec le arrivent le cinéma, la radio et la télévision. Bien que la plupart des studios soit situés à Hilversum et Aalsmeer, la programmation est largement influencée par Amsterdam où vivent beaucoup de gens qui travaillent dans l'industrie de la télévision.

Le zoo d'Amsterdam, surnommé "Artis", tient son nom de la Société royale de zoologie "Natura Artis Magistra" (« La Nature est maîtresse de l'Art »). C'est l'un des plus anciens du monde (le bâtiment principal date de 1838), avec celui de Londres (1828). Situé en plein centre-ville, son ambiance contraste fortement avec l'agitation urbaine environnante. Il comporte un aquarium (bâti en 1882), des musées zoologique et géologique, un planétarium ainsi qu'une bibliothèque.

La bibliothèque centrale d'Amsterdam possède des locaux récents : ils furent gagnés sur l'eau, près de la gare, dans le quartier de l'Oostelijk Havengebied. Elle est ouverte au public et gratuite. Le marché aux fleurs de la ville, présente différentes fleurs venant des champs néerlandais. Visité en masse par les touristes étrangers, qui achètent le plus souvent des bulbes à emporter, le marché possède également ses habitués, qui viennent y acheter des fleurs à bas coût.

Les musées les plus importants d'Amsterdam sont situés sur le Museumplein (la « place des Musées »). Cet espace est créé à la fin du sur le terrain de la précédente Exposition internationale et coloniale de 1883. La place est presque entièrement recouverte de pelouse, à l'exception de la partie nord, couverte de gravier et au centre de laquelle se trouve un long bassin rectangulaire qui se transforme en patinoire en hiver. L'organisation actuelle de la place remonte à 1999, date à laquelle elle est entièrement remodelée à l'occasion de la construction d'un grand parking souterrain.

Le nord de la place est bordé par le Rijksmuseum à l'architecture néogothique créée par Pierre Cuypers. Ce musée ouvre en 1885 et subit une importante rénovation entre 2003 et 2013, pour un montant de 375 millions d'euros. Le Rijksmuseum possède la plus grande et la plus importante collection d'art classique néerlandais. Sa collection se compose en effet de près d'un million d'œuvres de peintres et de sculpteurs hollandais, principalement du . Le musée est fréquemment associé au nom de Rembrandt, dont le travail et celui de ses élèves, est largement représenté dans les différentes galeries. La pièce maîtresse du musée reste probablement le chef-d'œuvre de Rembrandt, "La Ronde de nuit". Il abrite également les peintures d'artistes tels que Johannes Vermeer ("La Laitière", "La Ruelle"), Bartholomeus van der Helst, Frans Hals, Ferdinand Bol, Albert Cuyp, Jacob van Ruisdael et Paulus Potter. En dehors des peintures, la collection se compose également d'une grande variété d'œuvres d'art décoratif : de la faïence de Delft aux maisons de poupées géantes du .

Le nord-ouest du Museumplein accueille le musée Van Gogh, qui commémore le court séjour de Van Gogh à Amsterdam. Le musée est hébergé dans l'un des rares bâtiments modernes de ce quartier, conçu par Gerrit Rietveld, et accueille une collection permanente importante. Un nouveau bâtiment est adjoint au musée en 1999, « l'aile de la performance », pour accueillir les expositions temporaires. Cette aile du musée a été dessinée par l'architecte japonais Kisho Kurokawa. Le musée Van Gogh expose quelques-unes des plus célèbres toiles du maître hollandais, telles que "La Chambre de Van Gogh à Arles", "Les Mangeurs de pommes de terre" ou "Les Tournesols", faisant de ce musée le plus visité d'Amsterdam.

À côté du musée Van Gogh se trouve le plus important musée d'art moderne de la ville, le Stedelijk Museum. Construit à la même époque que la place, le bâtiment est inauguré en 1895. La collection permanente du musée se compose d’œuvres de Piet Mondriaan, Karel Appel ou encore Kazimir Malevich. Le musée a rouvert ses portes en septembre 2012, après d'importants travaux de rénovation, avec une nouvelle extension composite surnommée « la baignoire » en raison de sa forme.

La ville d'Amsterdam accueille de nombreux autres musées, de toutes tailles et de tous types. Dans le registre des musées historiques, le Nederlands Scheepvaartmuseum (« musée maritime néerlandais ») abrite la plus riche collection consacrée à la marine au monde. On y trouve des peintures, des maquettes, des armes ou encore des cartes de géographie maritime. L'Amsterdam Museum (anciennement, "Amsterdams Historisch Museum") est quant à lui entièrement dédié à l'histoire de la capitale néerlandaise à travers des œuvres d'arts et des documents divers. La Maison Anne Frank, où Anne Frank et sa famille se cachèrent des nazis avant sa déportation en août 1944, attire également des dizaines de milliers de touristes, à côté de la Westerkerk. Le musée historique juif, inauguré en 1987, occupe quant à lui quatre synagogues ashkénazes, tandis que le Bijbels Museum (musée biblique), situé sur le Herengracht, contient lui la première Bible imprimée en Hollande (1477). Le musée possède également des maquettes du temple de Salomon, d'Hérode et du tabernacle, et un grand nombre d'objets ainsi que des arbres mentionnés dans la Bible. Un autre musée, le Verzetsmuseum (« musée de la Résistance ») retrace la vie de la population néerlandaise sous l'occupation nazie. La synagogue portugaise d'Amsterdam, principal lieu de culte juif depuis plusieurs centenaires dans la ville, est désormais ouvert à la visite.

Parmi les autres musées de peinture remarquables, il est possible de citer la maison de Rembrandt, qui reconstitue la vie de l'artiste à travers ses œuvres, ainsi que l'Hermitage qui est la plus grosse dépendance étrangère du musée de l'Ermitage de Saint-Pétersbourg. Le Tropenmuseum (« musée des tropiques »), qui fait partie d'une entité plus large, l'Institut royal des Tropiques est consacré à l'ethnographie et à l'étude des cultures tropicales à travers le monde. Le musée du chat présente des dessins, peintures, gravures et autres œuvres dédiées à cet animal.

Dans le domaine des Arts visuels et du spectacle, le FOAM, musée de la photographie fonctionne principalement sur la base d'expositions temporaires. Le Nederlands Filmmuseum est quant à lui consacré au septième art.

Plusieurs musées à vocation plus touristique sont également très populaires. On peut ainsi citer le musée de Madame Tussauds où sont présentées les statues de cire de nombreuses personnalités comme Lénine, Michael Jackson, Pelé ou James Bond, le musée des sacs Hendrikje, le plus grand musée du monde consacré au sac ou encore le Heineken Experience, consacré à la marque de bière éponyme et situé dans l'ancienne brasserie. Le NEMO, musée scientifique pour enfants et adultes semblable à la Cité des Sciences française, fut conçu par l'architecte Renzo Piano et inauguré en 1997.

Enfin, bien que n'étant pas un musée, l'Institut néerlandais d'études militaires a ouvert l'accès à ses collections sur la Seconde Guerre mondiale au public. L'Académie royale néerlandaise des arts et des sciences, dont elle dépend, a également son siège à Amsterdam, dans la Trippenhuis.

Amsterdam possède un orchestre symphonique de renommée mondiale, l'Orchestre royal du Concertgebouw, qui évolue au sein du Concertgebouw situé sur Museumplein. L'acoustique de cette salle de concert est considérée par la critique comme l'une des meilleures du monde. Le bâtiment contient trois salles : la grande salle, la petite salle et la galerie des glaces. Près de huit cents concerts y sont produits chaque année, avec une fréquentation d'environ spectateurs. L'opéra d'Amsterdam, le Muziektheater, est quant à lui situé à côté de l'hôtel de ville au sein du même ensemble architectural surnommé « Stopera » (mot-valise issu de "Stadhuis", « hôtel de ville », et d'opéra). Cet immense complexe moderne, ouvert en 1986, se situe dans l'ancien quartier juif de Waterlooplein près de la rivière Amstel. Il héberge les troupes du Nederlandse Opera, du Nationale Ballet et du "Holland Symfonia". Ouvert en 2005, le Muziekgebouw aan 't IJ est une salle de concert située sur l'IJ, au nord de la gare centrale qui accueille principalement des représentations de musique contemporaine. Situé à proximité immédiate, le Bimhuis est plutôt dédié au jazz et à l'impro.

Le Heineken Music Hall est une salle de concert située près de l'Amsterdam ArenA qui accueille les grands concerts d'artistes de renommée internationale. Il accueille également de nombreux festivals de musique électronique tel que l'Amsterdam Music Festival, notamment avec les disc jockeys néerlandais Armin van Buuren ou Tiësto. Toujours à proximité de l'Amsterdam ArenA, le Ziggo Dome a ouvert ses portes en 2012, et accueille des artistes internationaux comme Pearl Jam, Madonna, Beyoncé ou encore Lady Gaga. Le Paradiso est une salle de spectacle et un centre culturel situés dans une ancienne église d'Amsterdam, bâtie entre 1879 et 1880 près du Leidseplein, l'un des centres touristiques et culturels de la ville. Également situé près du Leidseplein, le Melkweg est un autre lieu alternatif multi-disciplinaire, né d'une organisation indépendante en 1970. Tous deux offrent une programmation éclectique allant du rock indépendant au hip-hop, en passant par le R'n'B ou et d'autres genres populaires. Parmi les autres lieux de musique plus axés sur les sous-cultures, on peut notamment citer les salles OCCII, OT301, De Nieuwe Anita, Winston-Uni et Zaal 100. Chaque printemps, se déroule le festival "5 Days Off" qui est hébergé pendant cinq soirs au Paradiso et au Melkweg. Pendant l'été, plusieurs grands concerts se produisent en plein air tels que "A Day at the Park".

La ville d'Amsterdam accueille plusieurs lieux d'expression théâtrale. Bâtiment néo-Renaissance construit en 1894 sur Leidseplein, le Stadsschouwburg héberge la compagnie du "Toneelgroep Amsterdam". Alors que la plupart des pièces étaient jusqu'alors jouées dans la grande salle, le bâtiment a subi une importante phase de rénovation et d'expansion pour créer une salle de représentation supplémentaire qui est opérée conjointement avec le Melkweg. Le théâtre royal Carré, construit sur les rives de l'Amstel en 1887 dans le même style néo-Renaissance en vogue à l'époque, avait vocation initiale d'héberger un cirque permanent. Il est désormais l'hôte de spectacles de cabaret, de comédies musicales et de quelques concerts. Le Théâtre Tuschinski et la réouverture récente de la salle DeLaMar permet de compléter l'offre en ce qui concerne les pièces de théâtre et les comédies musicales.

Les Pays-Bas possèdent une forte tradition de cabaret, qui combine à la fois la musique, les contes, les commentaires, le théâtre et la comédie. Le genre du cabaret remonte aux années 1930 où des artistes comme Wim Kan, Wim Sonneveld et Toon Hermans ont été les pionniers de cette forme d'art aux Pays-Bas. On trouve ainsi, à Amsterdam, une académie des arts de la scène spécialement dédiée au cabaret, la "Kleinkunstacademie". Parmi les artistes populaires contemporains se trouvent, par exemple, Freek de Jonge, Herman Finkers, Hans Teeuwen, Herman van Veen, Youp van 't Hek, Theo Maassen, Najib Amhali, Raoul Heertje, Jörgen Raymann, Brigitte Kaandorp et Comedytrain.

Le journal "Het Parool", créé comme journal de Résistance pendant la Seconde Guerre mondiale est devenu un journal à tirage national, mais reste très centré sur Amsterdam. Le tirage quotidien est aujourd'hui de l'ordre de unités. Le journal hebdomadaire "De Groene Amsterdammer" (« L'amstellodamois vert ») est également très populaire. Le "Algemeen Handelsblad", dont est issu le "NRC Handelsblad" (à Rotterdam), fut également fondé à Amsterdam (où il est à nouveau implanté depuis décembre 2012) et de très nombreux journaux nationaux ont également leur siège dans la ville, comme "De Telegraaf", "De Volkskrant", "Trouw" ainsi que "Het Financieele Dagblad". Les journaux gratuits "Metro" en "Sp!ts" ainsi que la maison d'édition Elsevier, qui publie entre autres l'hebdomadaire du même nom y sont également implantés.

AT5 ("Amstel Televisie 5") est la chaîne de télévision locale. Elle fut fondée en 1992, et a révélé de nombreuses personnalités télévisées au niveau national, comme Sacha de Boer, Matthijs van Nieuwkerk et Fons van Westerloo. RTV Noord-Holland, SBS, Endemol, MTV, IDTV et plusieurs autres maisons de production ont également choisi Amsterdam pour implanter leurs quartiers généraux.

De nombreux programmes télévisés et radio nationaux sont (où ont été) enregistrés dans les studios Desmet Studio's (ainsi qu'au Studio Plantage jusqu'en 2012), tous deux situés dans le Plantage. La "Westergasfabriek" abrite également les enregistrements de nombreux programmes TV.

L'Amsterdam Internet Exchange (AMS-IX) est l'un des plus gros relais d'interconnexion internet des Pays-Bas, et même l'un des plus grands au niveau mondial.

L'Ajax Amsterdam est le principal club de football de la ville. C'est une équipe de la ligue néerlandaise de football, plusieurs fois vainqueur de la Ligue des champions de football (1971, 1972, 1973 et 1995), et deux fois vainqueur de la Coupe intercontinentale (1972 et 1995). Le club possède le meilleur palmarès néerlandais avec, en plus de ses titres européens, 30 championnats nationaux remportés à son actif. En 1996, ils abandonnent le vieux Stadion De Meer pour emménager dans le nouvel Amsterdam ArenA, au sud-est de la ville, à proximité de la gare Amsterdam Bijlmer ArenA. Le stade olympique, construit pour accueillir les Jeux olympiques d'été de 1928, subit une importante rénovation à la fin des années 1990 pour désormais accueillir des événements culturels ou sportif, à l'image du Marathon d'Amsterdam ou les Championnats d'Europe d'athlétisme 2016. En 1920, Amsterdam est l'hôte des épreuves de voile sur l'IJ, lors des Jeux olympiques d'été de 1920 qui se déroulent à Anvers.

Amsterdam possède deux franchises de football américain : les Amsterdam Crusaders et les "Amsterdam Panthers". Du temps où la NFL Europa existait encore (ligue majeure dissoute en 2007), la capitale néerlandaise était représentée sur la scène européenne par les Amsterdam Admirals. L'équipe de basket-ball des "MyGuide Amsterdam", basée au Sporthallen Zuid, évolue au sein du Championnat néerlandais. Le baseball est quant à lui représenté par l'équipe des Amsterdam Pirates au sein de la Ligue Majeure néerlandaise. En ce qui concerne le hockey sur glace, on peut signaler l'équipe des Amstel Tijgers Amsterdam qui joue sur la patinoire Jaap Eden, alors que le très populaire hockey sur gazon est représenté par trois équipes : Amsterdam, Pinoké et Hurley, qui s'affrontent au Wagener Stadium d'Amstelveen.

En plus du marathon d'Amsterdam se déroule chaque année la course "Dam to Dam", d'une longueur de (environ ), entre Amsterdam et Zaandam. Depuis 1999, la ville d'Amsterdam honore ses meilleurs sportifs par l"'Amsterdam Sports Awards". La première mouture de ce prix a été attribuée au boxeur Raymond Joval et à la milieu de terrain de hockey sur gazon, Carole Thate.

La vie nocturne d'Amsterdam est une des plus animées d'Europe. Les dizaines de boîtes de nuits ("clubs") branchées attirent beaucoup de jeunes de tous les Pays-Bas, ainsi que des touristes étrangers. Le "Melkweg", le "Paradiso", l’"Hôtel Arena", le "Jimmy Woo", le "Club More", le "Trouw", le "Powerzone" et l’"Escape" sont parmi les plus fameuses. On peut trouver ces clubs partout, mais les deux principaux points de concentration sont le "Rembrandtplein", le "Leidseplein" et leurs alentours. Amsterdam est aussi surtout connue pour son principal quartier chaud à "De Wallen" (le "Red Light District"), bardé de nombreux lieux de plaisirs tarifés (dans la rue "Oudezijds Achterburgwal") et les "Haschich Bars" ou "Coffee Shops" répartis un peu partout en ville, attirant de nombreux étrangers en quête de cannabis dans un cadre dépénalisé.

La ville d'Amsterdam est très dynamique dans le domaine des festivals, avec près de 140 festivals et événements qui s'y sont déroulés en 2008. Au premier rang des événements d'Amsterdam, on peut citer la fête nationale néerlandaise dénommée dorénavant "Koningsdag" (le « jour du Roi »), précédemment "Koninginedag" (le « jour de la Reine »), en raison du couronnement de Willem-Alexander le 30 avril 2013. La fête nationale correspond traditionnellement au jour de l'anniversaire du roi (ou de la reine) sauf si celui-ci tombe un dimanche, auquel cas, le "Koningsdag" a lieu la veille. Ainsi, le premier Koningsdag du règne de Willem-Alexander a-t-il eu lieu le 26 avril 2014, et non le 27 avril, jour de son anniversaire. À noter que, sous le précédent règne de Beatrix, la fête nationale ne coïncidait pas avec l'anniversaire de la reine. Lors de son accession au trône, le 30 avril 1980, la reine Béatrice décida en effet de conserver la date anniversaire de sa mère, la reine Juliana, le 30 avril au lieu de son propre anniversaire le 31 janvier, à la fois pour rendre hommage à sa mère mais également pour des raisons pratiques. Des festivités au plein cœur de l'hiver et donc dans le froid voire sous la neige aurait en effet probablement été moins propices aux festivités et à la liesse populaire. Chaque année, plusieurs centaines de milliers de personnes voyagent vers Amsterdam pour rendre hommage au roi (ou à la reine) avec les habitants de la ville. Des dizaines de milliers de personnes affluent alors vers la ville, que ce soit pour faire la fête en musique le long des canaux ou sur les concerts de rue, ou pour chiner dans les grandes braderies (les "freemarkets") aux quatre coins de la ville et notamment au Vondelpark.

Parmi les autres événements majeurs, la "Stille Omgang", une procession catholique silencieuse se déroulant à la nuit tombée, un soir de mars. Le "Holland Festival", dédié aux arts de la scène attire quant à lui chaque année des artistes du monde entier au mois de juin, tandis que la Gay Pride et son fameux défilé de bateaux sur les canaux de la capitale ont lieu au mois d'août. Le "Prinsengrachtconcert", dédié à la musique classique se tient également pendant le mois d'août sur Prinsengracht, de même que le "", qui ouvre chaque année la nouvelle saison culturelle avec des concerts, des récitals, des pièces de théâtre. Dans un autre registre, la Cannabis cup récompense au mois de novembre les meilleures variétés de cannabis.

Amsterdam est également une ville très dynamique sur la scène de la musique électronique. Chaque année, l'Amsterdam Dance Event ou ADE, organisé au mois d'octobre attire plus de visiteurs, dont touristes. Il s'agit de l'un des plus grands festivals en clubs au monde, et tous les genres de musique électronique y sont représentés. La ville accueille également la majorité des festivals techno Awakenings qui attirent chaque année des dizaines de milliers de visiteurs, à la fois dans des événements en plein air (à Spaarnwoude) ou en salle (au "Gashouder" du Westerpark). Parmi les autres principaux festivals organisés dans la ville, on trouve (le Premier ministre Mark Rutte y alla danser en 2011), , (généralistes), (house), ou encore Dekmantel et (deep house et techno). La ville est également l'une des premières villes néerlandaises à accueillir la musique gabber, dérivée de la scène house, au début des années 1990 ; le premier festival du genre internationalement reconnu, le Thunderdome, s'y est déroulé en 1992.

Amsterdam compte deux universités généralistes : l'université d'Amsterdam ("Universiteit van Amsterdam", ou UvA), institution laïque fondée en 1632, et l'université libre d'Amsterdam ("Vrije Universiteit", ou VU), institution d'origine protestante fondée en 1880. L'UvA est celle qui bénéficie du plus grand rayonnement international, ce qui lui a valu d'être classée au du classement mondial des universités publié par le quotidien britannique The Times en 2012, et au en 2013. La ville comprend également d'autres établissements d'enseignement supérieur consacrés à l'art, comme le conservatoire d’Amsterdam, la "Gerrit Rietveld Academie", la "Hogeschool van Amsterdam" ou la "Amsterdamse Hogeschool voor de Kunsten". L'Institut international d'histoire sociale d'Amsterdam est l'un des plus grands centres de documentation et de recherche en histoire sociale et en particulier sur l'histoire du mouvement ouvrier. Fondé au début du , le "Hortus Botanicus" est l'un des plus anciens jardins botaniques au monde avec de nombreux spécimens rares et anciens, dont le plant de café à l'origine de l'ensemble de la culture du café en Europe centrale et en Amérique du Sud. Enfin, la ville héberge également plusieurs facultés de politique et d'économie qui sont principalement à direction des étudiants étrangers.

Amsterdam dispose de cinq écoles secondaires privées (appelées "gymnasium"), le "Vossiusgymnasium", le "Barlaeusgymnasium", le "St. Ignatius Gymnasium", "Het Gymnasium" et le "Cygnus Gymnasium", où un programme classique inclut des cours de latin et de grec ancien. Bien que considéré par beaucoup comme un concept anachronique et élitiste jusqu'à très récemment encore, les gymnases ont récemment connu un regain d'intérêt conduisant à la création d'un quatrième, puis d'un cinquième lycée. La plupart des écoles secondaires d'Amsterdam proposent différents niveaux de scolarité au sein de la même école.

Quelques écoles primaires d'Amsterdam basent leur enseignement sur des méthodes pédagogiques particulières telles que la méthode Montessori. Le lycée le plus important basé sur cette pédagogie est le "Montessori Lyceum". Les autres lycées sont majoritairement basés sur des confessions religieuses, principalement catholiques ou protestantes, mais également des écoles islamiques et hébraïques, notamment dans la banlieue sud d'Amsterdam.

La circulation en voiture dans le centre-ville est très fortement découragée via des initiatives de la municipalité, telles que des frais de stationnement élevés ou de nombreuses rues fermées à la circulation ou à sens unique. Afin d'encourager les automobilistes à laisser leur véhicule à l'entrée de la ville, la municipalité a mis en place un système de stationnement incitatif composé de sept parcs relais regroupés sous l'appellation . Ces derniers permettent aux automobilistes de bénéficier de frais de stationnement très accommodants à condition d'emprunter les transports en commun (tram, métro) pour se rendre au centre-ville. La municipalité favorise également les initiatives d'autopartage et de covoiturage. Les moyens de transport en commun et de transport alternatif sont ainsi largement favorisés à Amsterdam.

La ville d'Amsterdam possède deux boulevards périphériques qui permettent de contourner la ville ou de traverser rapidement l'agglomération. Le périphérique extérieur de la ville est l'autoroute A10. D'une longueur de , il permet de relier les 18 voies urbaines principales aux grands axes autoroutiers du pays, et en particulier à l'A1 (qui dessert l'est des Pays-Bas), l'A2 (qui rejoint Utrecht, Bois-le-Duc, Eindhoven et Maastricht) et l'A4 qui dessert l'axe tracé entre Amsterdam, La Haye, Rotterdam et la Belgique. Le second périphérique de la ville est connu sous le nom de « Périphérique intérieur » ("Amsterdamse binnenring") ou S100, et est constitué d'un ensemble de trois quais qui délimitent l'arrondissement de Centrum le long du Singelgracht, le Nassaukade, le Stadhouderskade et le Mauritskade.

Initialement, lorsque les autoroutes furent imaginées en 1932, l'objectif était de faire d'Amsterdam le nœud central du réseau routier néerlandais, duquel partiraient les autoroutes A1 à A8. Cependant, le déclenchement de la Seconde Guerre mondiale et le changement de priorités ont fait que seules les autoroutes A1, A2 et A4 débutent dans la ville.

L'autoroute A3 vers Rotterdam est annulée en 1970 afin de conserver le Cœur-Vert au cœur du Randstad. L'autoroute A8, en direction du nord vers Zaandam, et le périphérique d'Amsterdam (A10), sont inaugurés respectivement en 1968 et 1974. Outre l'A1, l'A2, l'A4 et l'A8, deux autoroutes permettent de désengorger le trafic en direction de la Frise, au nord-est du pays, via la région du Flevoland (par l'A6) ou la Hollande-Septentrionale (par l'A7).

Le réseau de transports publics de la ville, géré par la GVB ("Gemeentelijk Vervoerbedrijf"), est très développé, combinant plusieurs modes de transport, à la fois ferroviaires (tramway et métro), routier (bus) ainsi que maritime et fluvial (ferries). Dans le centre, les tramways et les bus concentrent l'essentiel du trafic de passagers, tandis que les métros desservent les zones périphériques et la proche banlieue (Amstelveen et Diemen). Les liaisons en ferry, gratuites, permettent de traverser l'IJ et de relier l'arrondissement de Noord et les communes alentour au reste de la ville. Des bus régionaux et certains bus de banlieue sont, quant à eux, exploités par Connexxion et Arriva, et desservent entre autres l'aéroport d'Amsterdam-Schiphol. En 2013, la GVB a transporté environ 211 millions de passagers. Chaque jour, empruntent ses 56 lignes de bus, 16 lignes de tram, 4 lignes de métro et 5 liaisons maritimes.

La construction du réseau de métro, dont la première ligne a été mise en service en 1977, a été émaillée par plusieurs incidents et contestations de la part des habitants. Dans les années 1970, de nombreux bâtiments construits sur et autour de Nieuwmarkt sont détruits pour laisser place au projet de construction du métro (ainsi que d'une voie express), qui devait traverser le quartier. Ce projet provoque des désordres majeurs (connus sous le nom de "Nieuwmarktrellen") en 1975, ce qui conduit à l'abandon du projet de voie rapide. Le métro est cependant construit, et "Nieuwmarkt" en constitue aujourd'hui l'une des stations. Plus récemment, le projet de construction de la Noord/Zuidlijn (« Ligne Nord/Sud »), destinée à améliorer significativement les conditions de circulation dans le centre-ville et vers l'arrondissement nord, a été marqué par des incidents majeurs (effondrement de bâtiments sur Vijzelstraat, stations plus chères que prévu, inondations) qui ont repoussé de six ans sa date d'inauguration, de 2011 à 2017. De même, le coût total du projet pour la ville a plus que triplé, passant de 300 millions d'euros dans le plan initial à plus de 900. Le coût total de la ligne, initialement estimé à 1,46 milliard d'euros devrait ainsi dépasser les 3,1 milliards.

La carte à puces OV-chipkaart, lancée en 2005 et valable à la fois sur le réseau ferroviaire des Nederlandse Spoorwegen et sur les réseaux de transport en commun de plusieurs villes des Pays-Bas (Rotterdam, La Haye par exemple), est utilisable dans l'ensemble des transports en commun de la ville. Le bureau de la GVB, situé en face de la gare centrale, distribue gratuitement une carte du réseau de transport public.

Le vélo est le moyen de locomotion le plus populaire et le plus utilisé à Amsterdam. La ville offre ainsi de nombreuses infrastructures visant à faciliter les déplacements à bicyclette, telles que des couloirs spéciaux sur la majorité des rues, mais aussi une signalisation spécifique, permettant aux vélos (et plus généralement aux deux-roues) d'emprunter des voies à sens unique dans de nombreux endroits de la ville. La ville dispose en outre d'importantes infrastructures de garage, incluant d'immenses parkings surveillés dans certaines gares ("fietsenstallingen"), mais aussi des bateaux spécialement mis en place pour accueillir des vélos. D'après "I Amsterdam", au total, Amsterdam possède plus de de pistes cyclables. L'absence de relief favorise également l'usage de la bicyclette. Toutes les couches sociales utilisent ce moyen de transport, qui représente près de 38 % des voyages journaliers.

Selon une estimation de la municipalité datant de 2012, la ville comptait vélos. Selon cette même étude, environ 70 % des résidents trouvent que le vélo est un moyen de locomotion agréable pour s'y déplacer. Parmi les 30 % restants, seuls 11 % n'y prennent aucun plaisir, tandis que 19 % sont neutres. Parmi les principales raisons invoquées par les personnes enthousiastes à l'idée d'utiliser leur vélo, on retrouve en tête la facilité d'utilisation et la rapidité (50 %), suivies du fait qu'il permet de profiter de l'environnement urbain (19 %) et qu'il constitue un moyen de transport sain et bon pour la santé (17 %). La qualité des infrastructures et la gratuité n'arrivent qu'au quatrième et au cinquième rang (9 % et 6 % respectivement). Parmi les principaux points négatifs, l'étude cite le comportement asocial de certains usagers, la sécurité, ainsi que la gêne occasionnée par les scooters, également autorisés à circuler sur les voies dédiées au vélo. En outre, les résidents de la ville mentionnent également la difficulté qu'ils rencontrent parfois pour garer leur vélo, en particulier aux abords de la gare centrale d'Amsterdam. Au cours de l'année 2012, deux-roues ont été enlevés par la municipalité et conduits vers le dépôt du "Westelijk Havengebied". À titre de comparaison, ce chiffre n'était que de en 2011, et en 2010. Cette augmentation reflète le manque d'espaces de parking dans la ville, ce qui incite de plus en plus les gens à garer leur vélo dans des endroits non autorisés.

La surabondance de vélos dans la ville a également des répercussions négatives. Le vol et le trafic de vélos restent ainsi des problèmes endémiques, même si la tendance est à la baisse. En 2008, environ vols de vélos furent enregistrés contre en 2001. Selon les chiffres 2012, un peu plus de plaintes pour vols de vélos, scooters ou mobylettes ont été déposées.

Amsterdam et ses environs sont sillonnés par plus de 150 canaux, créant ainsi près de 90 mini-îles reliées par un réseau d'un millier de ponts. Pendant de nombreux siècles, ces canaux et voies d'eau ont été utilisés comme principales voies de transport à Amsterdam, notamment pour le transport d'eau, de charbon, de nourriture ou d'épices. Aujourd'hui, ces canaux ne sont adaptés qu'aux petites péniches, aux bateaux de plaisance et aux bateaux-mouches. Ils restent toutefois toujours utilisés par la société de messagerie DHL, dont le bateau livre des colis à travers toute la ville.

Trois ferries transportent gratuitement les piétons et les cyclistes sur l'IJ, entre la gare centrale d'Amsterdam et Amsterdam-Noord. Deux autres ferries payants permettent de parcourir l'IJ d'est en ouest, le long du port. Il est également possible d'utiliser des bateaux-taxis et des navettes fluviales, de louer des bateaux électriques ou encore d'effectuer une croisière fluviale sur les canaux de la ville. En sus de cela, le "Floating Dutchman", un bus également capable de naviguer sur l'eau, fait un circuit touristique dans le centre-ville.

La principale gare de la ville, pour ce qui est de la fréquentation et de la quantité de trains, est Amsterdam-Central qui dessert à la fois le reste du pays (trains locaux Intercity et Sprinter) ainsi que les liaisons internationales (Fyra, Thalys, Deutsche Bahn, etc.). La gare centrale est même la plus fréquentée du pays aux côtés de Utrecht-Central, avec passagers qui y transitent chaque jour. L'aéroport de Schiphol constitue également un nœud ferroviaire important où se côtoient lignes locales et internationales (liaison Amsterdam-Central-Amsterdam-Sud, Fyra, Thalys). La ligne "Amsterdam-Central-Schiphol" est ainsi la ligne de train la plus fréquentée du pays avec 5,6 millions de voyageurs par an. En outre, Amsterdam-Central est située sur les deux autres lignes les plus fréquentées du pays ("Utrecht-Central - Amsterdam-Central" et "Haarlem - Amsterdam-Central").

Les gares d'Amsterdam Sloterdijk (nord-ouest, passagers par jour), Amsterdam Zuid (sud), et Amsterdam Amstel (est) desservent quant à elles principalement des liaisons intérieures (La Haye, Leyde, Utrecht, etc.). Les gares de Lelylaan, Muiderpoort, Bijlmer ArenA ou encore RAI complètent la desserte « périphérique » de la ville.

L'aéroport de Schiphol, au sud-ouest de la ville, est situé à moins de 20 minutes en train de la gare centrale. En 2012, plus de 51 millions de passagers y ont transité (+2,6 % par rapport à 2011), ce qui le classe dans le top 16 des aéroports mondiaux et à la quatrième place en Europe, après Londres-Heathrow (), Paris-Charles-de-Gaulle () et Francfort-sur-le-Main (). La flotte de KLM, dont les avions volent vers près de 131 destinations dans 65 pays différents, est basée à l'aéroport de Schiphol.

Pour ce qui est du volume de marchandises, Schiphol figurait en 2012 au mondial avec un volume de marchandises de . Une fois encore, Schiphol figure au européen, derrière Paris-Charles-de-Gaulle (), Francfort-sur-le-Main () et Londres-Heathrow ().

L'action de nombreuses œuvres littéraires renommées se déroule totalement ou partiellement à Amsterdam. L'une des plus universellement reconnues est "Le Journal d'Anne Frank", livre composé du journal intime tenu par Anne Frank, une jeune juive allemande exilée aux Pays-Bas, lorsqu'elle se cache au-dessus d'un magasin situé près de Westerkerk, pendant vingt-cinq mois, avec sa famille et quatre amis, au cours de l'occupation des Pays-Bas par l'Allemagne nazie. Dans le roman "La Chute" d'Albert Camus (1956), l'histoire du principal protagoniste, Jean-Baptiste Clamence, se déroule à Amsterdam, où l'on apprend qu'il s'est exilé.

L'écrivain néerlandais Cees Nooteboom a également choisi Amsterdam comme décor de son roman "Rituels", paru en 1983, et qui raconte l'histoire de deux amis dont l'un a la fâcheuse tendance à violer la loi, tandis que l'autre s'y plie avec discipline. La ville est également le théâtre du roman "De hoogste tijd" de Harry Mulisch, paru en 1985, et qui dresse un portrait détaillé de la ville moderne en racontant l'histoire de l'acteur néerlandais classique Pierre de Vries. Dans Sur l'eau, paru en 1998, H. M. van den Brink raconte l'histoire de deux rameurs d'un club nautique de l'Amstel, Anton et David qui est juif ; ces derniers voient alors leur destin basculer au moment de l'invasion allemande.

Dans "Le Ministère de la Douleur", sorti en 2005, Dubravka Ugrešić dépeint les conditions de vie difficiles des immigrés d'Europe de l'Est dans l'un des quartiers pauvres d'Amsterdam, la ville ayant toujours constitué un centre d'accueil pour les réfugiés.

Amsterdam a servi de décor à de nombreux films et séries télévisées, à la fois néerlandais et internationaux. Parmi les principaux films néerlandais qui ont mis en scène la ville, on peut citer Le Choix du destin (' en néerlandais), réalisé par Paul Verhoeven et sorti en 1977. Le film, dans lequel joue entre autres Rutger Hauer, raconte l'histoire de six étudiants d'une université de Leyde, à l'approche de la Seconde Guerre mondiale. Insouciants au début du film, la guerre va changer leur vie, et alors que certains vont choisir la rébellion et résister à l'occupant, d'autres vont opter pour la collaboration. Ciske le filou, sorti en 1984 raconte l'histoire de Ciske, un enfant de 11 ans vivant dans un quartier pauvre d'Amsterdam dans les années 1930. Le film est inspiré d'un roman pour enfants ; l'acteur Danny de Munk y interprète une chanson, « Je me sens tellement seul », devenue un morceau classique pour la ville d'Amsterdam. Toujours dans les années 1980, le film L'Assaut, adaptation cinématographique du roman "" de Harry Mulisch, raconte l'histoire d'Anton Steenwijk qui essaie de comprendre les circonstances de la mort de sa famille dans une attaque allemande au cours de la Seconde Guerre mondiale. Bien que l'action se déroule principalement à Haarlem, le film, oscarisé en 1987, constitue une référence du cinéma néerlandais. Dans Amsterdamned, réalisé par Dick Maas et sorti en 1988, un dangereux plongeur sévit dans la ville, tuant sauvagement ses victimes à coups de couteau cranté. Le tueur utilise ainsi les canaux de la ville pour commettre ses crimes.

Amsterdam est également apparue dans plusieurs grosses productions hollywoodiennes et internationales. Dans Les diamants sont éternels, sorti en 1971, James Bond, interprété par Sean Connery, se rend dans la capitale néerlandaise pour y rencontrer Tiffany Case ; le film met principalement en scène les canaux de la ville, et en particulier le Magere Brug. Deux ans plus tard, dans Turkish Délices, la ville est le théâtre principal de la relation passionnée et tumultueuse d'Éric, sculpteur bohème, avec Olga, issue d'une famille conservatrice. Le film met ainsi en scène de nombreuses parties de la ville, comme le Dam et le Damrak, le Rokin, le Oudezijds Voorburgwal ou encore le Vondelpark. Dans le film "Pulp Fiction" de Quentin Tarantino, Vincent Vega (interprété par John Travolta) revient à Los Angeles après avoir passé trois ans à Amsterdam. La scène d'ouverture du film Ocean's Twelve, réalisé par Steven Soderbergh, montre l'équipe de Daniel Ocean en train d'organiser un casse à Amsterdam. Pour ce faire, les bandits vont jusqu'à faire descendre une maison de plusieurs centimètres en affaissant les pilotis sur lesquels elle est construite. La comédie américaine de 2005 Gigolo malgré lui se déroule à Amsterdam et montre les méfaits de la consommation de cannabis et la prostitution, mais aborde également la thématique du racisme.

Quelques scènes de Nos Etoiles Contraires, film racontant l'histoire peu ordinaire de deux adolescents atteints par le cancer, se déroulent aussi à Amsterdam.

La chanson "Amsterdam", interprétée par Jacques Brel, est l'une des chansons francophones consacrées à la ville les plus illustres. Le titre a souvent été repris, comme par le groupe Oi! orléanais "Komintern Sect", et notamment en anglais par Scott Walker, David Bowie et John Cale. Il a également été repris et modifié par le groupe Parabellum, qui en a fait une chanson contre l'usage des drogues. D'autres artistes francophones ont également chanté la « Venise du Nord » (surnom également donné à la ville de Bruges en Belgique), on peut ainsi citer Guy Béart ("À Amsterdam"), Maxime Le Forestier ("Petit Nuage sur Amsterdam"), Les Innocents ("Entre Amos et Amsterdam"), Graziella de Michele ("Vision d'Amsterdam"), Billy Ze Kick ("Bons Baisers d'Amsterdam") ou encore Oxmo Puccino ("Sur la Route d'Amsterdam"). Plus récemment, le groupe de rap Octobre rouge a également rendu hommage à la ville et plus particulièrement à son fameux quartier rouge dans le titre "Week end a Meda".

Dans un registre international, plusieurs chansons baptisées "Amsterdam" ont été interprétées par des artistes comme la chanteuse néerlandaise Maggie MacNeal, ou les groupes Coldplay, Van Halen, Peter Bjorn and John ou encore Mando Diao. Les chansons « ' » (« Aux canaux d'Amsterdam ») de Pieter Goemans ou encore « ' » du chanteur britannique Max Bygraves sont également devenues des classiques populaires.





</doc>
<doc id="246" url="https://fr.wikipedia.org/wiki?curid=246" title="Abréviations en informatique J">
Abréviations en informatique J



</doc>
<doc id="247" url="https://fr.wikipedia.org/wiki?curid=247" title="Aéronautique">
Aéronautique

L'aéronautique inclut les sciences et les technologies ayant pour but de construire et de faire évoluer un aéronef dans l'atmosphère terrestre. 

Les sciences incluent en particulier l'aérodynamique, une branche de la mécanique des fluides ; les technologies sont celles qui concernent la construction des aéronefs, leur propulsion ainsi que les servitudes. Les entreprises associées à ces technologies sont dans la catégorie .

Piloter un aéronef permet de le faire évoluer et de pratiquer une activité. Les activités principales sont liées à la composante aérienne des forces armées d'un pays, le transport aérien commercial ou à la pratique d'une activité de loisir ou de sport aérien. On y associe les organisations et les compagnies gérant ces activités.

Un aéronef est un engin qui, pour évoluer dans l'atmosphère, l'utilise pour sa sustentation. Les principaux aéronefs sont l'avion et l'hélicoptère. Les forces armées utilisent aussi des missiles et des drones dont certains sont assimilables à des aéronefs sans pilote humain à bord, en particulier les missiles de croisière et les drones d'observation.

Le cerf-volant, comme le parachute ne sont pas des aéronefs. Toutefois ce dernier est très lié à l'aéronautique par son utilisation comme moyen de sauvetage et son évolution récente en faisant un engin pilotable.

Les activités aériennes sont réglementées sous l'égide d'institutions le plus souvent étatiques à l'échelle mondiale comme l'AITA pour les compagnies aériennes, à l'échelle régionale comme Eurocontrol pour la gestion du trafic aérien dans la zone européenne ou à l'échelle nationale comme la DGAC pour l'aviation civile en France. Ces institutions organisent ou réglementent la formation dans les métiers de l'aéronautique en particulier lorsque la sécurité des vols est affectée : c'est bien sûr le cas pour les pilotes et le personnel navigant commercial mais aussi pour le personnel chargé de la maintenance et les contrôleurs aérien. Ces formations sont assurées par des écoles spécialisées.

La , la connaissance de l', la sont indispensables à l'aéronautique même si les bases ne lui sont pas spécifiques.

La liste des aéronefs est le point d'entrée principal où chaque aéronef est classé selon son constructeur.

Enfin on trouvera dans les articles de la catégorie « » et de la catégorie « » la relation des principaux événements intéressant l'aéronautique. Les biographies des aviateurs, des concepteurs et ingénieurs se retrouvent dans la catégorie « ».

L'astronautique concerne le déplacement et la navigation hors de l'atmosphère terrestre.

L'astronautique est le domaine des évolutions et de la navigation en dehors de l'atmosphère terrestre, éventuellement vers d'autres astres. Les engins utilisés traversent l'atmosphère mais doivent leur sustentation, et souvent leur pilotage, à un propulseur anaérobie.

L'aéronautique est le domaine des et de la navigation au sein de l'atmosphère terrestre et utilisant cette atmosphère pour sustenter un engin. Le plus souvent, mais pas obligatoirement, ces engins utilisent l'atmosphère pour assurer aussi le pilotage (gouvernes aérodynamiques) et la propulsion (aérobie).

L'aéronautique comporte deux classes d'engins :

Les principaux aérostats sont les ballons libres utilisés surtout pour des activités sportives ou de loisir et les dirigeables.

L'avion et l'hélicoptère sont des aérodynes avec pilote à bord. Leurs utilisations civiles ou militaires sont multiples.

Certains missiles, en particulier les missiles de croisière, et les drones sont des aérodynes sans pilote à bord. Ils sont soit guidés à partir du sol soit pré-programmés. Les missiles emportent une charge militaire et sont détruits en fin de mission ; les drones sont utilisés essentiellement pour le renseignement ou la surveillance et sont généralement utilisés par les forces armées, de police ou de douane.

Le parachute n'est pas un aéronef : il utilise l'atmosphère pour freiner sa descente sans effet de sustentation. Toutefois une nouvelle classe de parachute est apparue à la fin du comportant une voilure souple avec effet de sustentation ; ces engins se rapprochent des avions ultra-légers à voilure souple tels que les deltaplanes.

Les dictionnaires courants donnent des définitions quasi-équivalentes pour les deux termes : le domaine des machines permettant de naviguer dans l'atmosphère terrestre. Le terme « aviation » recouvrant plus particulièrement le domaine des avions, le terme « aéronautique » est donc plus général et doit être employé lorsque le sujet recouvre l'ensemble des aéronefs.

En anglais, le terme « "aviation" », bien plus usité dans cette langue que « "aeronautics" », recouvre quant à lui l'ensemble du domaine.

L'être humain aspire à voler depuis toujours. Si Léonard de Vinci, vers 1500, imagine des machines volantes, ce n'est qu'en 1783 que les premiers hommes vont pouvoir réaliser le vieux rêve d'Icare avec les montgolfières des frères Montgolfier, précédant de très peu les ballons à gaz de Jacques Charles. Ces engins sont tributaires du vent, l'aéronautique ne va vraiment prendre son essor qu'avec les ballons dirigeables, de Henri Giffard en 1852.

En parallèle au développement des plus légers que l'air, d'autres pionniers se tournent à la fin du vers le plus lourd que l'air, qui deviendra l'« avion ». La paternité des premiers vols planés comme celle des premiers vols motorisés est contestée pour des raisons de définition : certains essais de vol plané (s'ils ont réellement eu lieu) sont plus proches du parachute que du planeur et certains décollages motorisés nécessitaient une assistance au sol. De plus les sentiments chauvinistes ne sont pas exempt de certaines revendications.

Otto Lilienthal, en Allemagne, réussit plusieurs centaines de vols planés, et documentés, dans la dernière décennie du siècle. En 1890 et 1891 Clément Ader, en France, serait parvenu à faire décoller un avion équipé d'un moteur à vapeur devant témoins mais ses tentatives restent sans lendemain. Ce sont les frères Orville et Wilbur Wright, aux États-Unis, qui, à partir de 1903, peuvent non seulement faire décoller leur appareil mais parviennent à le contrôler sur des distances de plus en plus importantes atteignant en 1908. Ces vols sont documentés et font l'objet de démonstrations y compris en France.

La seconde voie explorée est celle de l'hélicoptère. À masse égale il nécessite une puissance nettement plus élevée que celle de l'avion pour assurer la sustentation. Pourtant dès 1907, Paul Cornu, en France, réussit le premier vol libre mais les progrès seront ensuite bien plus lents que ceux de l'avion.

Le premier conflit mondial qui survient à peine une décennie après les premiers vols voit le développement de l'avion en tant que moyen de renseignement sur les positions ennemies. Les avions s'équipent de mitrailleuses pour pouvoir abattre l'adversaire et l'empêcher d'accomplir sa mission. La construction aéronautique entre dans l'ère de la grande série puisque certains modèles sont construits en plusieurs milliers d'exemplaires.

La fin du conflit met sur le marché un grand nombre de pilotes et d'appareils. Les premières tentatives d'utilisation commerciale de l'avion apparaissent et des compagnies se forment pour transporter le courrier, puis des passagers, sur des lignes régulières. La navigation aérienne utilise les méthodes issues de la navigation maritime et nécessite donc que la visibilité soit bonne : le vol reste tributaire de la météorologie. La concurrence entre l'avion et le dirigeable pour le transport des passagers se développe au cours du premier tiers du et se termine tragiquement avec l'accident du dirigeable Zeppelin Hindenburg en 1937. C'est la fin de l'aérostation qui n'est plus qu'une discipline destinée au sport ou au loisir.

Les forces armées ont vu l'intérêt de l'avion pour le renseignement mais aussi pour le bombardement. La course à l'armement est lancée et les nouveaux appareils sont spécialisés : bombardiers, chasseurs, attaque au sol, etc. Il est tactiquement intéressant de voler de plus en plus vite, de plus en plus haut, de plus en plus loin. La course aux records en tous genres est lancée et c'est la période des exploits : traversée des mers, puis des océans ; survol des massifs montagneux ; croisières longue distance ; etc.

Le second conflit mondial est caractérisé par une utilisation massive de l'avion pour les missions de bombardement et, en corollaire, des chasseurs et intercepteurs chargés de les protéger ou de les détruire. Sur le plan technique c'est aussi l'apogée du moteur à piston. Le développement du réacteur, vers la fin du conflit, et l'apparition du radar vont permettre, la paix revenue, l'essor du transport aérien commercial.

De nouveau, à la fin du conflit, des pilotes entraînés et des avions se trouvent disponibles en grand nombre. Les progrès réalisés dans le domaine du radar permettent de suivre et de guider l'avion en vol sans visibilité. Les compagnies aériennes naissent et commencent à concurrencer les paquebots et les trains au moins pour le voyage en conditions luxueuses. La mise en service du Boeing 707 par la PanAm en 1958 marque le passage au transport aérien commercial de masse. La concurrence est vive entre les compagnies et s'intensifie encore avec la dérégulation lancée aux États-Unis en 1978. Les paquebots transocéaniques disparaissent et le train lui-même est concurrencé sur les trajets de durée supérieure à trois heures.

Sur le plan militaire, la « compétition » continue entre les États-Unis et l'URSS pendant la Guerre froide. Le mur du son est atteint puis largement dépassé, les bombardiers supersoniques volent à Mach 2 et les intercepteurs à plus de Mach 3. La nature du combat change avec les performances du radar de détection et l'utilisation des missiles air-air et sol-air pour empêcher la pénétration. L'accent se porte sur des performances nouvelles comme la furtivité et la pénétration basse-altitude, sous la couverture radar.

L'aéronautique est, depuis ses origines, une lutte pour l'allègement des structures et l'augmentation de la puissance. Ce n'est qu'en 1977 que le premier vol utilisant un « moteur humain » sera réalisé à bord du Gossamer Condor, un avion de moins de . À l'opposé l'Airbus A 380 est en service commercial depuis 2007, ses quatre réacteurs développent une poussée supérieure à et permettent de faire décoller plus de .

La pratique d'une activité aérienne est le plus souvent réglementée en raison de la nécessité de partager l'espace aérien entre les divers utilisateurs et en raison des risques ou inconvénients que la pratique de cette activité peut causer aux habitants ou à l'environnement.

Dans la plupart des pays, États-Unis et Europe en particulier, on distingue trois grandes classes d'activités :
Dans la majorité des cas un aéronef est conçu pour l'exercice d'une activité et configuré en conséquence. Les principaux types d'aéronef sont :
Dans la pratique la frontière entre activité et type d'aéronef n'est pas absolue. Un avion de ligne peut, par exemple, être utilisé comme avion d'affaire ou être utilisé par les forces armées pour le transport des autorités gouvernementales. La distinction entre activité de loisirs ou sportive est imparfaite.

Le développement d'un aéronef se fait en fonction de sa mission (terme utilisé par les forces armées) ou de son utilisation opérationnelle (terme utilisé dans les domaines civils). Cela conduit à des aéronefs de morphologie distinctes : l'aéronef est adapté à son activité principale.

Le nombre d'avions, de toutes catégories, dépasse largement le nombre d'hélicoptères en service. Le terme « aviation », de facto, recouvre l'ensemble des activités utilisant ces deux types d'aéronefs.

Plus de 900 compagnies aériennes proposent des vols réguliers chaque jour. La plus grande d'entre elles met en œuvre une flotte de plus de 400 appareils, les plus petites un seul. La flotte mondiale est estimée à plus de appareils en 2008. Les types d'avions utilisés sont :


Un avion (ou un hélicoptère) d'affaire est un appareil semblable à ceux utilisés pour le transport commercial de passagers mais n'accueillant que quelques passagers dans des conditions souvent luxueuses. Ils sont la propriété de grandes entreprises qui les mettent à disposition de leurs cadres ou bien sont utilisés par des compagnies qui proposent le transport à la demande, l'avion-taxi. C'est le cas des hélicoptères souvent utilisés pour joindre les grands aéroports à des héliports situés au centre des grandes métropoles ou vers des destinations de prestige.

Les avions utilisés sont :

Les avions de lutte contre l'incendie sont équipés d'un réservoir de soute pouvant contenir une grande quantité d'eau. Ils utilisent une écope pour récupérer l'eau en survolant un plan d'eau à très basse altitude.

La plupart des travaux aériens sont réalisés en utilisant des appareils existants modifiés pour pouvoir emporter les réservoirs ou les équipements nécessaires. Exemples :


L'hélicoptère est particulièrement adapté à certains travaux :

Ces avions sont le plus souvent des monomoteurs équipés d'un moteur à piston. Ils ne sont pas autorisés à pratiquer le vol sans visibilité et ne servent donc que pour les loisirs, l'apprentissage initial du pilotage, et plus généralement les activités ne nécessitant pas le respect d'un horaire.

Quelques hélicoptères légers entrent dans cette catégorie, mais le coût élevé de l'heure de vol, 3 à 4 fois celui d'un avion comparable, restreint la diffusion de cette passion.

La voltige utilise des avions monomoteurs semblables à ceux de l'aviation légère mais spécialement équipés pour cette activité : moteur puissant, alimentation en carburant permettant le vol sur le dos, etc.

Le vol à voile est une discipline sportive où le pilote utilise les courants d'air ascendants pour prolonger la durée du vol. Les avions utilisés sont des planeurs, des avions sans moteur, dont le décollage est assisté par un avion remorqueur ou un treuil. Les motoplaneurs sont équipés d'un moteur qui leur permet de rejoindre un aérodrome en cas de nécessité; selon la puissance et le type de moteur, il permet ou non le décollage autonome.

La réglementation aéronautique est contraignante et son application entraîne des surcoûts sur le prix des appareils qui deviennent de plus en plus sophistiqués, sur l'apprentissage du pilotage et sur les infrastructures. L'avion ultra-léger motorisé répond aux besoins de ceux qui veulent pratiquer le vol pour le plaisir, voire concevoir ou construire leur propre appareil, avec un minimum de contraintes. Si les premiers ULM ressemblaient souvent aux appareils des pionniers du , mais construits avec des matériaux modernes, aujourd'hui les meilleurs ULM "3 axes" ne se distinguent des avions proprement dits que par la règlementation particulière qui s'applique à leur construction, maintenance et licence de pilotage, en fonction de critères de poids et puissance du moteur notamment.

La mission de bombardement nécessite l'emport de charges lourdes. Le bombardier est le plus souvent un avion multimoteur doté d'un rayon d'action important. Les bombardiers stratégiques peuvent être capables de vitesses supérieures à M2 et d'atteindre les très hautes altitudes qui les mettent hors de portée de la défense sol-air « classique ».

Les missions de chasse et d'interception ont en commun de chercher à détruire les forces aériennes ennemies en vol. La chasse est plutôt destinée à la protection d'avions amis pendant l'exécution de leur mission, l'interception se fait à partir du sol. Dans tous les cas la mission nécessite des avions capables de performances élevées en vitesse, vitesse ascensionnelle, manœuvrabilité. Ils sont équipés d'armes air-air.

Ces deux missions peuvent être effectuées à partir de porte-avions.

La mission consiste à attaquer les mobiles ennemis au sol (ou en mer), en particulier les chars. Les aéronefs utilisés doivent être particulièrement maniables à basse altitude. Ils sont équipés d'armes air-sol (ou air-mer). L'hélicoptère est particulièrement adapté à cette mission à courte distance de la ligne de front.

La mission consiste à pénétrer les défenses ennemies pour détecter et identifier les cibles potentielles.

L'observation des mouvements ennemis est la première mission militaire confiée dès la fin du aux aérostiers. Les ballons captifs permettent l'observation au-delà de la ligne de front mais leur taille les rend vulnérables. Au cours de la Première Guerre mondiale la mission est assurée par les premiers avions. Des avions équipés d'appareils photographiques ou de caméras seront ensuite utilisés et le Lockheed U-2 spécifiquement construit par les États-Unis pour la surveillance stratégique des pays du bloc soviétique reste un épisode marquant de la Guerre froide. Les satellites d'observation ont pris le relais de la mission pour le renseignement stratégique alors que les drones sont de plus en plus utilisés pour le renseignement tactique.
Les missions de détection, de surveillance et de patrouille maritime nécessitent l'emport de moyens électroniques ou optiques et le maintien sur zone pendant une longue durée. Les avions doivent être capables d'une très longue autonomie et permettre à deux équipages de se relayer à bord. Ces missions peuvent être effectuées à partir d'un porte-avions, auquel cas elles sont assurées par des avions spécifiquement développés mais elles utilisent souvent des versions aménagées d'avions civils lorsque les appareils sont basés à terre.

La mission de ravitaillement en vol est effectuée par des avions de transport civils ou militaires spécialement aménagés : réservoirs de soute et perche de transfert de carburant.
La mission consiste à transporter du personnel ou du matériel sur un terrain proche de la ligne des opérations. En dehors de ses capacités d'emport, l'avion doit pouvoir être chargé et déchargé dans un temps très court, capable d'atterrir et de décoller sur des terrains courts et peu aménagés et éventuellement disposer de portes permettant le largage en vol du matériel ou le parachutage du personnel.

L'hélicoptère est particulièrement adapté à cette mission en terrain difficile. Il permet aussi la récupération de troupes précédemment déposées.

La mission d'école de pilotage de base peut être assurée avec le même type d'appareil que pour l'aviation civile. Ses caractéristiques doivent permettre au moniteur de rattraper les erreurs de pilotage, en particulier permettre de sortir d'une vrille ou d'un décrochage volontaires ou accidentels.

La transition vers les appareils monoplace du type chasseur ou intercepteur nécessite des avions biplaces avec des performances aussi approchantes que possible. On a alors recours soit à des avions spécialement développés pour cette mission, soit à des versions biplaces de l'avion « réel ».

Les patrouilles acrobatiques sont des formations destinées à sensibiliser le public aux métiers et au rôle des forces armées. Elles participent aux cérémonies nationales et à des actions de promotion. Les avions utilisés sont souvent des avions d'entraînement.

Le coût de développement des avions de haute performances étant très élevé et lorsque le nombre d'appareils à produire est relativement faible les constructeurs proposent des avions multi-missions. Ces avions sont équipés de pods et de rails d'armement interchangeables. En fonction de la mission l'avion emportera des réservoirs supplémentaires, des pods contenant des équipements électroniques ou optiques variés, des bombes ou des missiles divers.

Les avions embarqués à bord des porte-avions sont équipés d'une crosse d'appontage et d'ailes repliables.

Le domaine de vol des hélicoptères, plus restreint que celui des avions, les rend plus adaptables à l'exécution de missions multiples. Ils sont aussi capables de se poser sur des navires et donc d'assurer des missions de liaison, de détection ou d'attaque à leur profit.

L'aéronautique permettant le déplacement aérien et transfrontière de biens et personnes a très vite généré des entités chargées d'organiser cette activité sur le plan international afin de promouvoir des standards et des normes aussi bien au niveau des appareils qu'au niveau des équipages.

L'activité aéronautique est aussi une composante de l'économie d'un pays et de nombreuses écoles ont pour but de former les cadres des usines de construction aéronautique, de l'industrie du transport aérien ou du contrôle de la navigation aérienne.

Enfin, l'intérêt du grand public pour l'aéronautique a entraîné la création de nombreux musées qui lui sont dédiés ainsi que des salons et démonstrations aériennes.

Plus récemment, bien que - comme le transport maritime - non incluse dans le protocole de Kyoto, la contribution du transport aérien aux modifications climatiques est devenue un sujet de préoccupation international, en raison des émissions significatives de ces secteurs et en raison de leur forte croissance.

L'aérodynamique est une des applications de la mécanique des fluides. Les équations permettent de modéliser et d'expliquer pourquoi un aérodyne peut se sustenter et se déplacer dans l'atmosphère. La catégorie inclut aussi une présentation des différents éléments d'un aéronef qui permettent l'application pratique des théories de l'aérodynamique : ailes, empennage, volets, hélice et rotors, etc.

Les deux aéronefs les plus couramment utilisés sont, aujourd'hui, l'avion et l'hélicoptère. La catégorie présente les concepts de "plus légers que l'air" et de "plus lourds que l'air" qui ont marqué l'histoire de l'aéronautique. Tous les aéronefs d'aujourd'hui sont propulsés soit par des moteurs à pistons (aviation légère), soit par des turbopropulseurs et turbine à gaz (petits avions de transport, hélicoptères), soit par des turboréacteurs (gros avions de transport, aviation militaire). La catégorie présente ces moyens et d'autres moins courants.

Le déplacement d'un aéronef dans l'atmosphère fait appel à deux familles de technologies : celle permettant le pilotage, c'est-à-dire le contrôle de l'attitude de l'aéronef et, celle permettant le déplacement par rapport au sol, c'est-à-dire la navigation. Cette dernière catégorie n'est pas spécifique à l'aéronautique, nombre de technologies sont héritées de la navigation maritime complétées par les technologies les plus récentes, telle que la navigation par satellites (GPS), sont utilisées sur tous les types de mobiles. L'ensemble des technologies permettant à un aéronef de voler est regroupé dans la catégorie .

L'activité aéronautique est dépendante d'autres technologies telles que la connaissance de l'atmosphère terrestre et la météorologie et son anticipation.
Les entreprises du secteur aéronautique incluent :

La Catégorie rassemble les concepteurs, ingénieurs et techniciens, d'une part, et les pilotes ou membres d'équipage, d'autre part, qui ont marqué l'histoire de l'aéronautique.




</doc>
<doc id="248" url="https://fr.wikipedia.org/wiki?curid=248" title="Contrôle de la circulation aérienne">
Contrôle de la circulation aérienne

Le contrôle du trafic aérien (en anglais "Air Traffic Control" ou ATC), ou contrôle de la circulation aérienne, ou également appelé contrôle aérien, est l'un des trois types de services de la circulation aérienne. Il est rendu par les contrôleurs aériens aux aéronefs afin d'aider à l'exécution sûre, rapide et efficace des vols. Il consiste aussi à accélérer et ordonner la circulation aérienne. 

Le service de contrôle est assuré dans les buts suivants : 
Le contrôle aérien a d'abord été visuel, avant l'invention de la radio et du radar. Par la suite, le radar a amélioré la possibilité d'assister les vols de nuit et les jours de brume, mais le trafic aérien est encore fortement influencé par un rythme nycthéméral.

Pour assurer ces services, un organisme de contrôle est mis en place. Suivant le type de trafic et sa position, différents organismes assurent les services de contrôle, information et alerte : 



Ces organismes ont été différenciés car les compétences requises, les règles applicables, et les moyens techniques nécessaires ne sont pas les mêmes. Un centre de contrôle en route nécessite un radar, tandis que l'outil principal en contrôle d'aérodrome est la vue. En approche, tous les avions veulent aller au même endroit : la piste, on a ainsi un phénomène « d'entonnoir ». En route, les avions ont tous des provenances et destinations différentes, les problèmes sont donc pour beaucoup éparpillés et aléatoires. Ces différences, et d'autres, ont conduit à cette classification.

Le contrôle aérien est toujours lié à un espace aérien ou à un aérodrome, qui peuvent avoir le statut de « contrôlé » ou « non contrôlé ».

Tout l'espace aérien est par défaut non contrôlé. Dans cet espace aérien, on crée ensuite là où c'est nécessaire des portions d'espace aérien contrôlées. Outre ses limites géographiques, chaque portion est définie par sa classe d'espace aérien, définissant la nature des services rendus, et les conditions de vol pour pouvoir y pénétrer (visibilité minimale, distance minimale des nuages, vitesse de vol, emport d'un équipement radio, etc.). En général, plus le trafic est dense, plus les conditions pour y pénétrer sont contraignantes.

Ainsi l'existence d'un organisme de contrôle est liée à l'existence d'un espace aérien, et vice versa. Le contrôle n'existe que s'il y a un espace, et la compétence de l'organisme de contrôle est limitée à cet espace. De même, un espace aérien n'existe que dans les horaires de fonctionnement de l'organisme qui en est chargé. Lorsque le service de contrôle ferme (par exemple, de nuit sur un aérodrome), les espaces aériens et la circulation d'aérodromes associés sont déclassés comme « non contrôlés » (classe G) pendant cette période.

Un aérodrome est juste qualifié de « contrôlé » ou « non contrôlé ». Il n'y a pas de nuances dans le rendu du service de contrôle comme dans les espaces.

Un organisme de contrôle d'aérodrome ne peut exister que sur un aérodrome contrôlé. En dehors des permanences de l'organisme de contrôle, l'aérodrome devient non contrôlé. La zone de compétence du contrôle recouvre le sol, la piste d'atterrissage et l'espace aérien immédiatement adjacent à la piste.

Sur un aérodrome non contrôlé, deux cas peuvent se présenter. Soit un service AFIS est assuré, auquel cas un agent AFIS dans la tour de contrôle assure les services d'information et d'alerte (un agent AFIS ne fait pas de contrôle au sens réglementaire du terme), soit aucune permanence n'est assurée, auquel cas les pilotes font de l'auto-information, soit sur une fréquence attribuée à l'aérodrome, soit sur la « fréquence club » . Dans un tel cas, les pilotes discutent entre eux sur la fréquence pour se mettre d'accord sur les ordres de passage et s'informer mutuellement de leurs positions respectives.

La radiotéléphonie, appelée couramment « fréquence » ou « micro », est le principal outil des contrôleurs. Le contrôle aérien utilise principalement des radiocommunications VHF, mais aussi parfois, notamment pour le contrôle océanique, des fréquences HF qui ont une plus longue portée. Les militaires utilisent aussi les fréquences UHF. La bande de fréquences VHF réservée à la communication vocale en aéronautique s'étend de à (à titre de comparaison, la radio FM est émise entre 87,5 et , et la radionavigation aéronautique utilise, entre autres, la bande de 108 à ).

La fréquence est l'outil que le contrôleur utilise traditionnellement pour :

La caractéristique pratique de la radiotéléphonie de l'aviation civile est d'être une communication unilatérale : une seule station peut émettre à un moment donné. Si deux stations émettent en même temps, la fréquence est brouillée et on n'entend aucun des locuteurs.

Les échanges en radiotéléphonie sont codifiés. Tous les messages courants ont une forme canonique qui doit être utilisée. On appelle cela la phraséologie. Elle est étudiée pour que les messages soient :

À titre d'exemple, le chiffre neuf se prononce en anglais "niner" (avec un son « r » à la fin). En français, le chiffre un s'exprime « unité » pour plus de clarté. L'expression « affirmatif » est interdite, pour éviter les confusions. Elle est remplacée par « affirme ».

L'alphabet radio international est d'application, il permet une meilleure compréhension de chaque lettre au moyen de mots compréhensibles et prononçables dans le monde entier.

Enfin, le collationnement ("readback") est obligatoire pour la plupart des instructions : il s'agit de répéter l'instruction (ou du moins les éléments principaux) pour confirmer la bonne compréhension. Dans certains cas, le contrôleur doit encore confirmer avec le mot « correct ». Par exemple, pour autoriser un avion au décollage, le contrôleur lui communiquera l'instruction "cleared for take off" ou « autorisé au décollage », donnera la direction et la vitesse du vent et le numéro de la piste. Le pilote doit répéter l'instruction et, le cas échéant, la piste en service.

Les communications sont enregistrées et conservées pendant en général un mois. Il est strictement interdit de communiquer sur une fréquence d'aviation sans être détenteur d'une licence de radiotéléphonie restreinte (attribuée, entre autres, aux contrôleurs aériens, aux pilotes et au personnel au sol qui est amené à se déplacer sur les taxiways et pistes).

Bien souvent, afin d'obtenir la licence radio, le candidat doit prêter serment de ne jamais faire part au monde extérieur des communications entendues sur les fréquences. Or certains pays légalisent la vente et l'utilisation de scanners (appareils permettant l'écoute des fréquences), donc le caractère secret des communications n'est plus garanti.

Remarque sur l'utilisation des émetteurs : beaucoup de personnes pensent qu'il n'est pas possible d'identifier une personne émettant sur une fréquence d'aviation. Or les principaux aéroports sont bien souvent équipés de radiogoniomètres (aussi appelés gonio ou homer), qui permettent au bout de quelques secondes d'émission, d'avoir des informations sur la provenance du signal. Ce moyen sert, entre autres, à repérer un avion perdu (surtout en l'absence de radar) et lui donner une information (QDM) sur la position relative de l'aérodrome.

Les strips sont de petites « bandes de progression » ("strip" en anglais) en papier sur lesquelles sont inscrites les informations relatives aux vols pris en charge par le contrôle aérien.
À chaque phase de vol correspond donc des strips, où sont imprimés les détails connus du vol : indicatif d'appel en radio téléphonie, route, provenance, destination, type d'aéronef, niveau de vol ou altitude, waypoints de passage.

Le contrôleur utilise ensuite ce strip pour y inscrire les instructions qu'il donne à l'aéronef : changements de cap, d'altitude ou encore de vitesse, autorisations d'atterrissage ou de décollage, horaires de passage de certains points...

Par la suite, le strip est archivé et utilisé comme preuve pour facturer le service de contrôle aérien à la compagnie aérienne.

Certains systèmes modernes remplacent les strips en papier par des « strips électroniques » affichés à l'écran alors que d'autres systèmes de contrôle s'affranchissent totalement des strip (système "stripless"), les informations de chaque vol sont alors affichées directement dans le label de la piste sur l'écran (C'est le cas depuis 2016 pour la contrôle-en-route de Bordeaux en France).

Contrairement à ce que l'on pourrait penser, le radar n'existe pas dans tous les centres de contrôle. La plupart des petits aérodromes sont dépourvus d'écrans de visualisation radar.

Deux types de radar sont utilisés dans l'aviation civile :

Le contrôleur utilise le radar pour rendre trois services, appelés « services radar » :

Ces services radar ne peuvent être assurés qu'à des aéronefs identifiés radar. De plus, certains organismes ne peuvent assurer qu'une partie des services radar, à cause des performances des systèmes ou de la configuration de leur espace aérien. Par exemple, le guidage radar n'est pas recommandé en classe E. Dans ce cas, seules la surveillance et l'assistance radar sont fournies.

Pour plus d'information sur l'utilisation du radar en pratique, voir contrôle d'approche

En France, en circulation aérienne générale, il existe trois interfaces de contrôle:


Pour assurer le service du contrôle, un contrôleur utilise principalement les clairances (ou autorisation) : il s'agit d'instructions et d'autorisations accordées à un aéronef pour pouvoir circuler dans des conditions spécifiées. 

Le commandant de bord d'un aéronef peut toutefois décider de déroger à une clairance mais doit pouvoir justifier sa décision pour une raison de sécurité (par exemple, virage qu'il estime trop serré). Le non-respect d'une instruction du contrôle aérien peut mener à une infraction selon le Code de l'aviation civile

Les moyens qu'utilise le contrôleur pour prévenir les collisions sont : 



Le moyen utilisé pour prévenir les abordages dépend du régime de vol et de la classe de l'espace considéré.

Lorsque des secteurs de contrôle en route ou des aérodromes sont saturés en trafic aérien, cela peut engendrer des risques liés à l'augmentation de la charge de travail du contrôleur aérien, ou des circuits d'attente imposés à des aéronefs en vol. Ainsi, la sécurité du trafic peut s'en trouver compromise. C'est là qu'intervient le concept de la gestion des flux de trafic aérien (ATFM). En Europe, c'est Eurocontrol, via le « Network Manager » (CFMU, Central Flow Managing Unit) qui est chargé de veiller à ce qu'il n'y ait pas davantage de vols dans l'espace aérien que ce que le système peut accepter. Cet organisme, en coordination avec les centres de contrôle et les compagnies aériennes, veille à ce que la charge de trafic pour chaque secteur de contrôle ou aérodrome ne dépasse pas une limite qui mettrait la sécurité en jeu. 

Si cette limite est atteinte ou dépassée, la CFMU provoque un allègement de la charge de trafic : soit par un étalement de certains décollages grâce à l'établissement de créneaux horaires, ou bien en imposant des changements de route ou d'altitude de vol. Un certain nombre de vols sont ainsi retardés ou déroutés afin de ne pas surcharger les secteurs concernés. 

Outre le service du contrôle en espace aérien contrôlé et dans la circulation d'un aérodrome contrôlé, l'organisme de contrôle est chargé d'assurer ces deux types de service.


Les limites du système de strip papier ont commencé à apparaître avec la complexification des systèmes de visualisation radar, notamment l'apparition du filet de sauvegarde. Il s'agit d'une fonction qui permet de prédire la trajectoire des aéronefs, et d'afficher une alerte si le système prévoit que les aéronefs vont se rapprocher dangereusement dans les prochaines minutes. Le contrôleur prend alors une mesure corrective.

Ce système est un progrès énorme, mais présente des limites. Les calculs de trajectoires sont basés uniquement sur les informations radar, et ne prennent pas en compte les clairances données par le contrôleur. Parfois, une alerte se déclenche alors que le contrôleur a déjà pris les mesures pour la corriger. Dans ce cas, en plus du stress engendré par l'alarme, le contrôleur perd confiance dans le système, et risque d'ignorer l'alerte plus tard à un moment où elle était justifiée. Cette limite du système est donc un facteur de risque, et une étude a été menée pour chercher les solutions possibles. Une de ces solutions est le strip électronique.

Le problème vient du fait que l'ordinateur ne dispose pas d'informations suffisantes pour calculer les trajectoires de façon précise, sur un intervalle de temps de plusieurs minutes. Les informations que le contrôleur note sur ses strips seraient très utiles au système pour affiner ses prédictions. Il faut donc que le contrôleur renseigne ses strips, non plus sur papier, mais sur un ordinateur, afin que le filet de sauvegarde puisse aller y puiser les informations dont il a besoin. Les premiers projets ont consisté en un écran horizontal où sont dessinés des strips que le contrôleur peut remplir à l'aide d'un clavier et d'une souris. Un autre projet est une visualisation radar avec des menus déroulants permettant de changer le cap, la vitesse, l'altitude des avions.

Cependant parmi les contrôleurs, ce sujet suscite de la méfiance. En effet le strip papier est considéré comme le dernier recours quand les autres outils tombent en panne. Si la visualisation radar et le système de strip électronique tombent en panne en même temps, le contrôleur perd toutes ses informations, et est incapable de faire son travail. Également d'un point de vue plus psychologique, le strip a toujours été l'outil et le symbole des contrôleurs, et s'en détacher n'est pas facile.

Cependant les systèmes sans strip sont aujourd'hui les plus communs dans le monde et la méfiance initiale a fait place à une bonne acceptation du stripless. Les informations qui étaient disponibles sur les strip sont facilement introduites dans le système. Ainsi le système peut mettre à jour la situation et donner des alarmes le cas échéant. Le partage d'informations entre secteur est possible contrairement au strip papier qui n'est lisible que par les contrôleurs à proximité.

En France, le système stripless EEE (Environnement Électronique ERATO) a été mis en service à Brest fin 2015. Ce système est l'évolution du système ODS associé au serveur ERATO fournissant des aides au contrôle (En Route Air Traffic Organizer), développé en interne à la DGAC depuis les années 1990. Cependant, à l'horizon 2020, tous les centres de contrôle français se doteront du système 4-Flight, remplaçant du CAUTRA actuel, qui embarquera le serveur ERATO, et substituera le système de traitement des plans vols actuel, STPV, par CoFlight. Ce système permettra une gestion plus fine des trajectoires ainsi qu'un interfaçage complet avec le Data Link.

Créé initialement pour aider au contrôle du trafic au-dessus des espaces océaniques, ce système est en passe de devenir un nouvel outil capable de remplacer ou de seconder le radar et de compléter les communications vocales. Grâce à un équipement spécial à bord de l'avion, les données des calculateurs de bord (position, altitude, vitesse, météo) sont collectées, puis transmises à intervalles réguliers par satellite — au-dessus des océans — vers les équipements au sol. Une interface graphique permet de visualiser ces éléments et leur mise à jour sur un écran. Les images obtenues peuvent même être intégrées sur les écrans radar de dernière génération (moniteur graphique). Il s'agit aussi d'utiliser une messagerie électronique pour les dialogues entre pilotes et contrôleurs.

Services rendus : 

ADS ("Automatic Dependant Surveillance") : Surveillance automatique de la position réelle de l'avion. Pour l'instant on utilise principalement l'ADS-C (pour Contrat). Des contrats sont établis automatiquement entre le sol et les vols pour que les données soient envoyées à intervalles réguliers vers le contrôle aérien (ex. : toutes les 20 minutes). Si l'avion dévie de son profil de vol autorisé (route…), il passe dans un mode de surveillance plus serré qui permet de corriger sa position rapidement. Ce n'est malgré tout pas un remplacement d'un radar classique, principalement à cause du délai entre les mises à jour. Ce système est en service actuellement dans de nombreuses régions océaniques, par exemple au-dessus du Pacifique entre les FIR de la Nouvelle-Zélande, de Tahiti et celle d'Oakland aux États-Unis.

Le futur, c'est l'ADS-B (Broadcast : diffusion). Là, les délais d'envoi des informations par les vols seront très courts et permettront une surveillance accrue, identique à celle d'un radar classique. Ce système est en cours d'homologation par l'OACI. L'Australie le teste déjà conjointement avec un système radar pour pouvoir l'utiliser au-dessus des zones semi-désertiques où la mise en place de radars est soit trop onéreuse soit impossible. La France va bientôt équiper la zone de La Réunion et pourrait étendre ensuite ce système à toutes les zones impossibles à équiper en radar (archipels polynésiens, Guyane, etc.).

CPDLC ("Controller-Pilot Data Link Communications") : En plus de la partie surveillance automatique dévolue à l'ADS, le deuxième apport du DATA LINK est la possibilité de communication entre les pilotes et les contrôleurs par un système de messagerie. L'amélioration est spectaculaire particulièrement dans les zones transocéaniques, là où seule la HF est utilisable. Ces dialogues sont codifiés pour des raisons de sécurité : messages préformatés avec passage de paramètres (ex : autorisation de monter ou de descendre à tel ou tel niveau de vol), avec des procédures de bouclage pour s'assurer que l'information a bien été envoyée, reçue et suivie.

Citons encore la « surveillance enrichie » ou CAP ("Controller Access Parameters") qui est mise en œuvre en Europe continentale à l'aide de radars dits « mode S ». Les systèmes bord enverront automatiquement des informations de surveillance précises telles que : le cap magnétique, le taux de montée, la vitesse indiquée… Ces informations devenant disponibles pour les contrôleurs aériens, la surveillance des vols devient plus fine et la charge de communication réduite entre pilotes et contrôleurs.





</doc>
<doc id="249" url="https://fr.wikipedia.org/wiki?curid=249" title="Art nouveau">
Art nouveau

LArt nouveau est un mouvement artistique de la fin du qui s'appuie sur l'esthétique des lignes courbes.

Né en réaction contre les dérives de l’industrialisation à outrance et la reproduction sclérosante des anciens styles, c'est un mouvement soudain, rapide, qui connaît un développement international : Tiffany (d'après Louis Comfort Tiffany aux États-Unis), Jugendstil (en Allemagne), Sezessionstil (en Autriche), Nieuwe Kunst (aux Pays-Bas), Stile Liberty (en Italie), Modernismo (en Espagne), style sapin (en Suisse), Modern (en Russie). Le terme français « Art nouveau » s’est imposé au Royaume-Uni, en même temps que l’anglomanie en France a répandu la forme Modern Style au début du .

S'il comporte des nuances selon les pays, ses critères sont communs : l'Art nouveau se caractérise par l'inventivité, la présence de rythmes, couleurs, ornementations inspirés des arbres, des fleurs, des insectes, des animaux, et qui introduisent du sensible dans le décor quotidien. C'est aussi un art total en ce sens qu'il occupe tout l'espace disponible pour mettre en place un univers personnel considéré comme favorable à l’épanouissement de l'homme moderne à l'aube du . En France, l'Art nouveau était appelé « style nouille » par ses détracteurs, en raison de ses formes caractéristiques en arabesques, ou encore « style Guimard », à cause des bouches de métro parisiennes réalisées en 1900 par Hector Guimard.

Apparu au début des années 1890, on peut considérer qu’à partir de 1905, l'Art nouveau avait déjà donné le meilleur de lui-même et que son apogée est atteint. Avant la Première Guerre mondiale, ce mouvement évolua vers un style plus géométrique, caractéristique du mouvement artistique qui prendra la relève : l'Art déco (1910-1940).

Au , presque toutes les formes d'art s'inspirent du passé. L'imitation du gréco-romain cohabite avec celle des styles nationaux. Cependant, certains artistes ont espéré que le trouverait enfin un style qui lui soit propre. La prédominance des formes inspirées du passé est la raison fondamentale de l'apparition d’un Art nouveau.

La source est très ancienne et la thématique de l'Art nouveau se trouve déjà dans les textes des théoriciens révolutionnaires. Ainsi, Claude Nicolas Ledoux est l'un des premiers à poser cette question d’un art qui ne soit pas l’imitation de quelque chose, mais qui aille plus loin en créant quelque chose de totalement original pour une civilisation nouvelle. On l'aperçoit aussi dans les formes les plus inattendues comme avec le retour à l’historicisme qui n'est autre qu'un moyen d’évasion.

En étant l’un des premiers à dessiner une multitude de coquillages, fleurs, méduses, radiolaires, foraminifères, diatomées, etc., dans un but scientifique, Ernst Haeckel peut être considéré comme un autre précurseur de l’Art nouveau. Son travail a inspiré les grands lustres en forme de méduse de Constant Roux, pour le musée océanographique de Monaco. Les acteurs de l’Art nouveau feront souvent référence à cette réalisation, tant ce fut un choc pour eux, même si pour Haeckel il ne s'agissait que de copies du réel. De même, la porte monumentale de l'architecte français René Binet, à l'Exposition universelle de 1900, s'inspire du travail de Haeckel.

Les prémices de cet art sont perceptibles dans la dimension onirique perceptible chez les peintres préromantiques. Augustus Pugin, classé parmi les artistes de style néogothique, et qui vit en Angleterre vers 1830-1850, possède une invention qui préfigure l’extraordinaire saturation décorative de l’Art nouveau, la liberté des formes, la puissance de la couleur, la lutte entre architecture et décor qui est l’un des grands combats artistiques de la seconde moitié du . Par ailleurs, le préraphaélisme s'éveille dès 1850 aux courbes et aux couleurs, inspirées des maîtres italiens du ou de la Renaissance florentine (Botticelli) par réaction à la révolution industrielle.

Les fondements théoriques du mouvement Arts & Crafts, comme les thèses de William Morris, de John Ruskin, lequel influence Arthur Heygate Mackmurdo, ou de Charles Rennie Mackintosh qui réalise la Glasgow School of Art à Glasgow de 1897 à 1899, définissent un nouvel art décoratif au Royaume-Uni. Ils se prononcent contre les dérives de l'industrialisation et de l'assèchement créatif qu'elle entraîne, ils prônent un retour à l'esprit des guildes médiévales, à l'étude du motif naturel, à l'emploi de formes épurées : la régénération de la société ne se fera que par la vérité des formes qui l'entourent et dont elle use. Dans la foulée se développe un courant assez proche appelé esthétisme et qui marquera des artistes comme Oscar Wilde, Edward Burne-Jones à partir de 1874 ou Aubrey Beardsley en 1893.

En Espagne, et plus précisément en Catalogne, le mouvement porte le nom de modernisme catalan à la suite de l'exposition universelle de 1888 de Barcelone. Il se construit durant les années 1870 sur la conjonction de plusieurs facteurs : la rénovation artistique, en parallèle d’autres arts contemporains, la recherche de nouvelles expressions formelles et la volonté de se situer dans une modernité d’envergure européenne. Selon les mots de l’écrivain Joan Fuster, il a vocation à transformer .

Les prémices de l'Art nouveau se retrouvent dès 1871 dans les cours de la nouvelle École provinciale d'architecture de Barcelone, qui était alors dirigée par Elies Rogent i Amat (1821-1897) et dans les œuvres de Josep Domènech i Estapà — malgré lui, puisqu’il refusa explicitement ce mouvement. Cependant, il est classique de considérer qu'en Catalogne l'Art nouveau commence en 1888, lors de la première exposition universelle de Barcelone, occasion pour laquelle un grand nombre d'édifices modernistes furent construits. De cet évènement subsistent encore l'arc de Triomphe de Barcelone et le Château des trois dragons.

Ce mouvement présentait des similitudes conceptuelles et stylistiques avec diverses variantes de l’Art nouveau qui se développait en Europe à la même époque. Par contre, il se singularisait selon trois aspects : il se développait dans la continuité de la renaissance catalane (1833-1880) ; il apparait au moment où existait un pressant besoin d’évolution et de rénovation politique et sociale et, alors qu’au même moment, la plupart des villes de Catalogne s’agrandissaient à un rythme hors de toutes comparaisons depuis la Renaissance — Girone, Tarragone, Reus, Sabadell, Terrassa, Mataro et surtout Barcelone avec son plan Cerdà (lancé en 1859), qui offrait de terrains nus à l'imagination des architectes. En outre, et contrairement à d'autres pays d'Europe, il cherchait à créer un art national là où d'autres pays cherchaient à dépasser leurs frontières. Antoni Gaudí est le principal représentant des nouvelles tendances de ce mouvement, dès 1886 avec notamment le Palais Güell (1886-1890) orné de ferronneries et pinacles ouvragés, qui succède à sa période orientalisante initiée en 1883 (El Capricho, Casa Vicens) et précède le Collège Sainte-Thérèse de Barcelone (1888-1889) aux accents déjà modernes, puis le plein épanouissement de sa période naturaliste à la fin du siècle.

En France, le propos, plus ou moins moral, se veut plus rationnel, moins tourné vers le passé et moins fermé aux matériaux nouveaux : dans ses écrits théoriques marqués par le rationalisme ("Entretiens sur l'architecture", 1863), Eugène Viollet-le-Duc ne rejette pas le matériau moderne (le fer notamment), mais veut au contraire l'afficher en lui donnant une fonction ornementale et esthétique, à la manière des structures gothiques du Moyen Âge. Paradoxalement connu comme le chef de file français du mouvement néo-gothique, Viollet-le-Duc sera l'inspirateur de nombreux architectes de l'Art nouveau. Par ailleurs, certaines de ses œuvres décoratives, notamment ses fresques peintes au château de Roquetaillade (1850), sont de parfaits exemples du lien de filiation entre le mouvement néogothique et l'Art nouveau. Mais c'est à partir de 1892 à Bruxelles avec Victor Horta, Henry Van de Velde et Paul Hankar, puis en France que sont définis les principes formels d'une architecture spécifiquement dénommée « Art nouveau ».

À Bruxelles, où il existe un milieu d'avant-garde à la recherche de nouveauté capable de faire pièce à l'historicisme triomphant, un ensemble de mécènes et d'artistes connu sous le nom de groupe des vingt, organise à partir de 1884 des expositions regroupant des artistes refusés par les salons officiels. La plupart sont des admirateurs des stylisations novatrices de Mackmurdo et ils invitent de nombreux progressistes dans le domaine de la peinture. Ce groupe est peut être le premier à intégrer au sein d'une exposition de peinture et sculpture des objets d'art décoratif. Ce mouvement est très influencé par des penseurs et artistes anglais, tel William Morris, James Abbott McNeill Whistler ou Aubrey Beardsley ; et par l'art japonais. Ce groupe poursuit la même activité après 1894 sous le nom de La Libre Esthétique.

Le mouvement identifié en tant que tel est divisé en trois périodes, notamment par Paul Greenhalgh, une période d'apparition au grand public, très courte entre 1893 et 1895 ; une période où le mouvement s'étend rapidement et prend place dans tous les milieux culturels, entre 1895 et 1900 et enfin un moment où le mouvement se stabilise, commence à faire des bilans sur lui-même et essuie de sévères critiques, avant de s'effacer durant la Première Guerre mondiale.

Le mouvement en tant que tel nait et se développe dans toute l'Europe entre 1890 et 1895 avec une très grande rapidité. Il est ainsi très délicat d'identifier des initiateurs précis. Le fait que de très nombreuses disciplines s'emparent de ce nouveau catalogue de formes donne très rapidement l'impression aux contemporains qu'ils assistent à l'émergence d'un mouvement artistique à part entière englobant tous les aspects de la vie.

Paul Greenhalgh identifie la phase initiale du mouvment entre 1893 et 1895, autour de quatre évènements se déroulant surtout dans de grandes capitales, Londres, Bruxelles et Paris.

Le premier évènement est la publication dans le premier numéro de la revue "The Studio" des dessins de Aubrey Beardsley en 1893. Ce jeune illustrateur présente pour la première un style de dessin qui sera caractéristique de l'Art nouveau, et il devient instantanément le centre d'intérêt des avant-garde des deux côtés de l'Atlantique.

La même année, à Bruxelles, Victor Horta achève l'hôtel particulier de Emile Tassel, première réalisation architecturale Art nouveau aboutie. Horta exploite le premier la ligne courbe, symbole entre tous de ce mouvement. La fluidité des espaces fait écho aux courbes végétales qui investissent ferronneries, mosaïques, fresques et vitraux, éléments tant structures qu'ornements, dans la plus parfaite ligne d'Eugène Viollet-le-Duc. Horta conçoit un édifice inédit avec des meubles qui correspondent au rythme des murs et de l’architecture ; il dessine les motifs des tapis, conçoit les meubles : c'est la naissance d'un « Art total ».

L'année suivante, toujours dans la capitale belge, Henry Van de Velde publie un pamphlet "le Déblaiement d'Art" qui prend du recul sur les évolutions artistiques contemporaines et fustige avec fougue le monde de l'art institutionnalisé. Cette réflexion est la première intellectualisation de deux idées fortes de l'Art nouveau : la valeur des arts décoratifs aux côtés des arts dits nobles et l'importance de l'harmonie générale dans tout travail de décoration.

Le dernier évènement, qui clôt la phase initiale du mouvement est l'ouverture à Paris en 1895 du magasin et centre d'exposition la Maison de l'Art nouveau par Siegfried Bing qui popularise le style dans la capitale et le fait connaître au grand public.

À la fin du , les échanges artistiques s’ étant intensifiés, le mouvement se diffuse rapidement. Des albums et revues d’art et d’architecture sont abondamment illustrées et propagent les idées nouvelles, comme "L'Estampe originale" (1888-1895), "The Studio" (1893), "Jugend" (1896), "Art et décoration" (1897), etc. Le développement des moyens de communication permet aux architectes de voyager ; ainsi des connexions s'établissent entre Bruxelles et Paris : Hector Guimard sera très influencé au cours d’un voyage qu’il a fait en 1895 pour voir les architectures de Victor Horta, ce qui l’amènera à intégrer certaines de ses formes dans sa propre architecture. De même, des liens très étroits se tissent entre Vienne et Glasgow, et un architecte comme Otto Wagner recevra la visite de Charles Rennie Mackintosh.

L'expression « Art nouveau » est employée pour la première fois par Edmond Picard, en 1894, dans la revue belge "L'Art moderne", dans la lignée de "La Jeune Belgique", pour qualifier la production artistique d'Henry van de Velde.

Cependant, le nom a été inventé par Van de Velde avec Victor Horta, Paul Hankar et Gustave Serrurier-Bovy. Elle passe en France, lorsque, le , elle devient l'enseigne de la galerie d'art de Siegfried Bing, sise 22, rue de Provence à Paris, sous le nom maison de l'Art nouveau.

En Angleterre, ce mouvement est également connu sous le terme de "Arts and Crafts movements", même si les personnes qui emploient cette expression l'utilisent pour désigner un mouvement plus large.

En France, on utilise quelquefois, surtout au début, le terme "Modern Style" pour faire référence au rôle initiateur joué par l'Angleterre. Avec "Art nouveau", il existe les expressions "style 1900", "style Guimard", "style de Glasgow". Les personnes critiques envers ce courant artistique emploient volontiers les termes "style métro", "style Maxim's", "style ténia" ou "Yachting style", comme le nomme Edmond de Goncourt en comparant les présentations de Bing à l'exposition universelle de 1900 à des cabines de bateau.

En Allemagne, On emploie soit "Studio-stil" en référence à la revue "The Studio" qui a popularisé le mouvement soit "Jugendstil", du nom d'une autre revue défendant l'Art nouveau "Jugend". Les Allemands emploient également les termes "Belgischestil" ou "Veldeschstil" en référence à la Belgique ou à Henry Van de Velde. Apparraissent également outre-Rhin les expressions "Lilienstil", style lys ou "Wellenstil", style vague.

En Italie, en Espagne ou en Amérique latine, le terme de "style Liberty" est employé en référence aux magasins du même nom qui importent des produits de ce mouvements.

La phase d'extension et de maturité du mouvement se situe entre 1895 et 1900. Ce style se répend dans toute l'Europe, chaque ville ou pays adaptant le mouvement artistique à ses propres caractéristiques et considérations locales.

La Maison de l'Art nouveau de Bing est une des vitrines sur cette période de l'étendue de ce propose le mouvement. Il expose ainsi des vitraux de Tiffany, des réalisations de Van de Velde, de Beardsley, Lalique, Colonna, Gaillard ou De Feure.

Entre 1900 et 1914, l'Art nouveau s'est imposé et il commence à faire l'objet de débats, de discussion, de critiques. Dès 1900, de nombreux critiques d'art s'attaquent à ce mouvement. Ils reprochent notamment de laisser obstinament de côté l'un des principes des arts décoratifs qui veut que l'ornementation d'un objet doit être subordonné à sa fonction. Dès l'exposition universelle de 1900, Charles Genuys, critique à "La revue des arts décoratifs" soulève ce point entre autre. L’Art nouveau est également violemment attaqué par les mouvements nationalistes, à partir des années 1904-1905, où les associations d’extrême droite française condamnent notamment Hector Guimard. Ces mouvances n'hésitent pas à employer la même rhétorique que pour les juifs, accusant ces artistes d'être contre la nation et devant être éliminés.

Par ailleurs, les créateurs authentiques sont vite rattrapés par le succès d'une mode dont ils sont les inspirateurs, et qui triomphe à partir de l'exposition universelle en 1900, notamment dans une bimbeloterie envahissante qui ternit pendant longtemps la mémoire de l'Art nouveau. À partir de 1910, les salons des arts décoratifs sont inondés d'objets quelconques, reprenant des styles anciens et ne laissant plus de place aux objets art nouveau, que le public délaisse. De fait, la production d'objets Art nouveau après la Première guerre mondiale se poursuit avec un certain succès de nombreuses années, mais ceux-ci sont la plupart du temps de simples copies n'intégrant pas de nouveautés.

Le déclin de l'art nouveau se voit en partie par le déplacement d'une partie de ces créateurs s'éloignent vers d'autres styles (dès 1905-1906) qui, eux, se maintiennent en vie. De plus, comme les représentants les plus influents de ce courant sont dispersés dans toute l'Europe, ils n'ont pas pu élaborer de système formel, ni s'inscrire au sein d'une institution officielle qui aurait légitimé et porté le mouvement.

Toutefois, cette vision est l'héritière d'une historiographie qui a longtemps peu étudiée la fin de ce mouvement. La vulgate de l'histoire de l'art a longtemps considérée que les mouvements artistiques postérieurs ont rompu radicalement avec l'Art nouveau. Il ne faut toutefois pas omettre que de nombreux artistes pleinement membre du mouvement ont d'eux-mêmes et très progressivement fait évoluer leur pratique, et que les nouveaux artistes s'insèrent la plupart du temps volontairement dans la continuité des avant-gardes précédentes.

L'Art nouveau est un mouvement artistique d'une extrême richesse, qui n'est s'est pas déployé de la même manière selon les lieux, les moments et les techniques. Ce mouvement se reconnait toutefois à un certain nombre de caractéristiques communes, même si tous les artistes n'ont pas exploité les mêmes thèmes ni intégré les mêmes influences.
La grande variété inhérente au mouvement Art nouveau empêche d'isoler un nombre fini de thèmes explorés par les différents artistes mais certains d'entre eux sont fondamentaux : la femme, la nature, les lignes courbes et l'asymétrisme.

L'image de la femme est extrêmement présente au sein de la production artistique Art nouveau. Que ce soit en femme éthérée et mystérieuse, en femme symbole de la nature, en femme active et pleine de vie ou en femme fatale, matinée d'érotisme, ce thème est récurrent dans la grande majorité des tendances, des lieux et des mouvement internes.

L'image de la femme fatale est déjà très présente dans la littérature fin-de-siècle. Ainsi, le Nu féminin est traditionnellement limitée aux scènes mythologiques et il est très codifié, expurgeant ainsi tout érotisme. De nombreux artistes Art nouveau s'en emparent et l'utilisent en n'hésitant pas à rompre avec l'image académique de la femme. Ils réinterprètent ainsi les Salomé, les sphinx féminins et autres mythes similaires.

L'image de la femme est également importante dans le mouvement Art nouveau pour son aspect naturaliste. Un grand nombre d'artistes montrent les femmes actives, fortes et maîtresses de leur destin, là aussi à rebours des codes classiques des représentations réalistes de la femme. L'époque est celle de l'émergence des femmes de théâtre célèbres, de chanteuses à succès et de courtisanes. Les femmes artistes ont les faveurs des peintres et sculpteurs Art nouveau, qui voient dans ses femmes l'exemple des femmes fascinantes qu'ils se plaisent à imaginer et représenter.

En tant qu'objet scientifique en plein essor, la nature représente à la fin du XIXe siècle la modernité. Modèle de beauté parfaite, la nature est donc largement exploitée comme thème par le mouvement Art nouveau, mais en dépassant le naturalisme traditionnel. Si les artistes Art nouveau sont nombreux à sortir des ateliers pour aller voir la nature de plus près, ils s'emparent également largement de nombreuses publications scientifiques qui décrivent et représentent le plus précisément possible la faune et la flore pour non pas en reproduire l'image le plus fidèlement possible, mais pour en trouver une forme esthétique nouvelle. D'ailleurs, un certain nombre de créateurs Art nouveau ont fait des études scientifiques et publient dans des revues universitaires. Ainsi, cette idée de dépasser les représentations traditionnelles de la nature en exploitant avant tout les formes proposées par la faune et la flore apparait très tôt via le mouvement "Arts and crafts" et est théorisée par plusieurs figures du mouvement tels Owen Jones ou van de Velde. Les artistes s'emparèrent largement de l'ouvrage de Ernst Haeckel "Formes artistiques de la nature" qui, publié entre 1899 et 1904, est pour eux comme un immense répertoire de formes. Josef Maria Olbrich déclare ainsi : .

L'exploitation de la nature est pour nombre des premiers artistes Art nouveau également un rejet des thèmes traditionnels historicistes de l'art (scènes de guerre, portraits d'hommes célèbres, scènes religieuses, de la mythologie grecque ou romaine), tout autant que de leur forme.

Malgré la volonté affichée de rompre avec le passé, les artistes de ce mouvement ne rejettent pas entièrement les héritages du passé. En revanche, ils les mélangent avec d'autres influences, absentes des style qui les ont précédés.

Comme d'autres artistes inspirés par des civilisations lointaines et très différentes, les membres de l'Art nouveau ont été nombreux a être inspirés par l'art asiatique, japonais notamment, ou islamique. En cette fin de siècle, des images et des œuvres arrivent de ces contrées et surprennent les européens, qui s'emparent des formes et thèmes utilisés.

Dans la veine de la redécouverte des anciennes civilisations européennes, de nombreux artistes Art nouveau s'emparent des motifs et formes des images qui leur parviennent. Cela concerne surtout les civilisations celtiques ou Vikings.

Même si les artistes tenant de l'Art nouveau critiquent les excès de l'historicisme duquel ils veulent s'extraire, cela ne signifie pas qu'ils rejettent indifféremment les formes des styles antérieurs. Ainsi, il se retrouve de nombreux exemples, mêlés de manière plus ou moins complexe à leur propre style, de réemploi de motif classique, gothique ou même rococo dans leurs œuvres.

L'art symboliste a une influence importante sur de nombreux artistes Art nouveau, surtout en France. C'est ainsi que de nombreux postimpressionnistes, pointillistes ou membres du groupe Nabi se retrouvent pleinement dans la mouvance Art nouveau.

C'est à partir d'idées et d'idéaux communs que naquit l'aspiration à un style homogène qui trouverait son expression non pas dans l'uniformité, mais dans la diversité. L’Art nouveau contient l’acceptation des différences de genre et d’esprit entre les êtres, il procède d’une très grande générosité de pensée. Ainsi dans la même ville, Bruxelles, trois architectes de renom ont pu cohabiter : Paul Hankar, Henry van de Velde et Victor Horta. Plutôt que de s’enfermer dans un style, les artistes ont avant tout la volonté de trouver de nouvelles manières de s’exprimer.

L’Art nouveau apparaît un peu partout au même moment. L'historien Mario Praz parlera de « déflagration », « d'explosion de la jeunesse ». Ce courant est le fait d'une génération d'artistes, souvent jeunes (Hector Guimard a moins de trente ans lorsqu'il dessine le métro parisien), et qui sortent de leur tour d'ivoire pour prendre en main le décor de la vie. L'objectif est de rompre avec l'exploitation des styles du passé, afin de proposer une alternative à un historicisme officiel qui empêche le renouveau des formes. Le terme allemand "Jugendstil" signifie explicitement « style de la jeunesse ».

L’Art nouveau vient en réaction à l’obligation de faire ce qui est convenable, codifié. Ainsi, la lecture de la baronne Staffe, qui a écrit un traité des bonnes mœurs pour faire l’éducation des classes moyennes, permet de mieux comprendre la société de 1900 : tout y apparaît codifié, de la longueur du voile de deuil à la carte de visite en passant par le type de chapeau… Ces règles seront insupportables aux artistes de la mouvance Art nouveau, tout comme celui-ci paraîtra insupportable, en tant qu'art non convenu, dans lequel il est impossible de se repérer par rapport aux styles et aux conventions de l’époque. Dans l’Art nouveau, il y a liberté de jouer, de s’amuser, d’être non conventionnel : c'est un art sonore, joyeux, musical, ce n’est pas un art du silence, de l’austère.

Plus encore, la sensualité et l’érotisme de l’Art nouveau font scandale. S'il porte une charge érotique manifeste, la sensualité des formes végétales comme la sur-utilisation de l’image de la femme dans le répertoire ornemental sont intimement liés à ce sentiment de vie que les artistes cherchent à restituer dans le décor quotidien.

Réaliser l'unité de l'art et de la vie, tel était l'objectif déclaré de l'Art nouveau, qui estime qu’il faut un cadre de vie qui correspond aux exigences de l’homme moderne du début du . Un autre objectif est de réagir contre une dérive liée à l’industrialisation à outrance et dépourvue de toute capacité d’invention. Prendre la nature comme référence, c’est alors réagir contre le rationalisme du début de l’ère industrielle, sa froide efficacité et sa morale puritaine. Les motifs habituellement représentés sont des fleurs, des plantes, des arbres, des insectes ou des animaux, ce qui permettait non seulement de faire entrer le beau dans les habitations, mais aussi de faire prendre conscience de l'esthétique dans la nature. Si la référence à la nature est une constante, la façon dont ces artistes vont aborder les modèles naturels varie.

Émile Gallé est un artiste naturaliste qui s'inspire de la nature en la stylisant très peu, il utilise ses formes dans les décors et dans les dessins de ses meubles. D’autres artistes vont plus loin et restituent dans les formes qu’ils inventent le sentiment de la sève qui circule dans le monde végétal. Naissent ainsi des formes qui suggèrent plus un organisme en croissance qu’un modèle précis. C'est par exemple le cas de Guimard, de Gaudí et de certains artistes allemands, comme August Endell, qui partent de la nature pour évoluer vers un phénomène d’abstraction.

Les artistes vont créer des formes originales, inédites, inventer un vocabulaire nouveau tout en tenant compte de la possibilité de les reproduire industriellement. C'est une réaction à la fois contre une industrialisation mal pensée, tout en intégrant cette volonté de modernité. Avec l'utilisation des matériaux nouveaux et des moyens de production modernes, l'un des buts poursuivis, pour lequel il a échoué, était de s’adresser au plus grand nombre.

C'est dans cette optique que les anciens matériaux, comme le bois ou la pierre, ont été élégamment mariés avec les nouveaux, comme l'acier ou le verre. Pour chacun d'eux, des artistes ont poussé leurs recherches à l'extrême pour en tirer le meilleur parti. C'est ainsi que les pâtes de verre multicouches, les rampes d'escalier à entrelacs de ferronneries, les meubles aux ondulations de bois ont permis de mettre l'art à disposition de tous, pour un coût abordable, tout en gardant une volonté d'innovation formelle, inspirée de la nature. Cet art est tout de même lié à de nombreux mécènes et se propage dans un premier temps dans un milieu élitiste bourgeois.

Les clients sont nombreux pour les vases Gallé, dans les milieux mondains parisiens, entre 1896 et 1899. Mais, très vite, le succès populaire notamment dans le domaine de l’affiche, en fait quelque chose qui manque de classe et l’Art nouveau sera assez vite assimilé à l’émergence des classes moyennes. Très vite dévalué, puis mis en cause par les nationalistes, il devient totalement inexistant dans les milieux supérieurs en quelques années. Au contraire, dans les classes moyennes françaises, l’Art nouveau a une très longue durée, et se prolonge jusque dans les années 1920, comme en témoigne l’Exposition des Arts Décoratifs de 1925, où son influence est encore sensible.

Si le qui se profile se rêve nouveau et moderne, on se rend aussi compte que cette modernité risque de couper l'homme de la nature. Tout se passe comme si celle-ci risquait de s'échapper et que les artistes devaient essayer de la réintroduire le plus naturellement possible dans le cadre de vie. L’Art nouveau est un art essentiellement urbain, citadin qui trouve un écho dans des villes comme Barcelone, Glasgow, Vienne, Paris ou Bruxelles.

En France, l'Art nouveau se décline en deux écoles : Paris et Nancy.

À Paris, Samuel Bing, marchand d'art, ouvre en 1895 une galerie : la Maison de l'Art nouveau. Précurseur français du mouvement, qui sera baptisé, comme son magasin, l'Art nouveau, Bing expose des designers, tels Van de Velde, Colonna ou de Feure. À la même époque, la construction d'un immeuble, le Castel Béranger, rend célèbre, malgré les critiques, son architecte Hector Guimard ; le « style Guimard » est aujourd'hui indissociable des entrées du métro parisien, réalisées en fonte industrielle.

À Nancy, c'est autour d'Émile Gallé, verrier et ébéniste, qu'est créée en 1901 la fameuse École de Nancy. Par ce courant résolument novateur, Nancy s'affirme comme la capitale de l'Art nouveau en France. Des verriers, ébénistes, architectes ou ferronniers de renom en étaient membres. À titre d'illustration, un immeuble aujourd'hui monument historique, sis au 22, rue de la Commanderie, à Nancy, est le fruit de la collaboration entre l'ébéniste et ferronnier d'art Eugène Vallin, le verrier Jacques Grüber et l'architecte Georges Biet. Parmi les architectes nancéiens, citons encore Émile André, membre du comité directeur de l'école de Nancy avec, à son actif, une douzaine d'immeubles Art nouveau dans cette ville. De même Reims, ville reconstruite après la Première Guerre mondiale, peut-être considérée comme une ville de l’Art nouveau tardif. En Alsace-Moselle, on remarque la présence du Jugendstil (équivalent germanique de l'Art Nouveau) dans l'architecture, du fait de l'annexion allemande, notamment à Strasbourg et à Metz.

S'il existe des maisons de campagne d'inspiration Art nouveau, elles sont souvent commanditées par les mêmes personnes qui font construire leur hôtel particulier, ou hôtel de rapport, en plein cœur de la ville. L'Art nouveau inspire bien sûr l'architecture de nombreux immeubles parisiens, mais surtout celle, parfois très soignée, de nombreuses villas anciennes en meulière, construites pour la plupart au début du , et que l'on peut découvrir en banlieue parisienne, notamment dans les villes de banlieue du Val-de-Marne, de l'Essonne et de la Seine-Saint-Denis. Celles-ci se caractérisent par leurs audaces en fer forgé, leurs décors de briques et de faïence, leurs pignons et parfois leurs petites tours. C'est dans ces banlieues que des architectes français expérimentent de nouveaux matériaux et de nouveaux styles inaugurant l'Art nouveau qui, par opposition à l'académisme, se veut total.

En Catalogne, après l'explosion de l'exposition universelle, l'Art nouveau est surtout un fait bourgeois. Il fleurit sur l'avenue du passeig de Gràcia, à Barcelone, et dans les principales artères de l'Eixample, à la faveur de concours d'architectures organisés par la ville. Il conquiert rapidement tous les domaines et devient un art officiel avec les commandes publiques de bâtiments de grande dimensions (le palais de la musique catalane, l'hôpital de Sant Pau, le conservatoire de Barcelone) et pour l'aménagement urbain (des luminaires, places ou bancs). Pensé pour accueillir un quartier de la ville entièrement moderniste, le parc Güell resta cependant un des rares jardins publics Art nouveau, avec la fin de la vogue de cet art comme avant-garde, et le retrait des investisseurs. Propulsé par de riches industriel, l'Art nouveau devient rapidement — contre ses idéaux d'origine — un style industriel. L'usine textile Casaramora ou le cellier Güell sont des exemples de ce modernisme appliqué à l'industrie et aux exploitations agricoles. Cette architecture est également appliquée à l'art religieux (Sagrada Família, crypte de la Colonie Güell, cimetières), aux bâtiments scientifiques (observatoire Fabra), voire scolaires (école de la Sagrada Família, collège Sainte-Thérèse).

S'il est relativement polymorphe, l'Art nouveau concerne avant tout l’architecture et les arts du décor. Les connexions entre le mouvement et les arts dits nobles tels la peinture ou la scultpure sont plus éloignés et si des influences croisées apparraissent de manière évidentes, elles ne permettent pas de parler d'un style Art nouveau en peinture et sculpture.

Une partie des origines des réalisations Art nouveau en architecture vient des théories de Viollet-le-Duc qui, très tôt, utilise des formes nouvelles pour dépasser les styles anciens et surtout postule (sans le tester) la possibilité d'ériger des structures portantes en acier pour la recouvrir de maçonnerie. Cette nouvelle technique permet de penser différemment la construction des bâtiments par la suppression des ouvrages de renforcement obligatoire dans l'architecture traditionnelle tels les plafonds voûtés et les arcs-boutants. Cette idée est reprise lors de la période Art nouveau par tous les grands architectes du mouvement, Louis Sullivan, Victor Horta, Francis Jourdain ou Auguste Perret.

Le premier architecte véritablement Art nouveau est Victor Horta. Il emprunte résolument la voie de l'acier au sein de ces constructions ; mais contrairement à la norme adoptées par ces contemporains qui cachent cette matière pour laisser apparent les traditionnelles, il décide de montrer les structures en acier, de les intégrer hardiment à l'ensemble décoratif du bâti. Ce parti-pris à rebours des habitudes fait sensation et devient une marque de fabrique, qu'il porte au plus haut point avec la Maison du peuple commandée par le Parti ouvrier belge achevé en 1899. Mais le programme de l'architecture Art nouveau est tout entier contenu dans la première construction de Horta, l'hôtel Tassel. Édifié en 1892, cette construction surprend l'ensemble de la profession, car il porte l'architecture bien au-delà des arts décoratifs pour toucher à un domaine bien plus large.

L'hôtel Tassel a un retentissement important, bien au-delà des frontières belges. Ainsi à Paris de nombreux architectes sont conquis par cette nouveauté et s'en inspire plus ou moins largement. Le personnage emblématique de l'architecture Art nouveau dans la capitale française est Hector Guimard qui adjoint les courbes caractéristique du mouvement naissant à son propre style, déjà original. Toutefois, il est une exception car la plus grande partie des constructions Art nouveau parisienne est l'œuvre de professionnels peu célèbres, surtout pour des magasins et restaurants tel Maxim's ou la bijouterie de Georges Fouquet. En France, la principale ville où ce style se développe est Nancy où il s'insère dans le développement local d'un puissant mouvement artistique et industriel.

Le projet artistique de Victor Horta est très fréquemment utilisé de manière partielle, mêlé d'inspiration plus classique. Ainsi, Charles Plumet mélange des éléments Art nouveau à des bâtiments typés XVIIIe et Jules Aimé Lavirotte avec des immeubles somme toute classiques dans leur structure générale.

Les architectes qui reprennent le plus intégralement possible les fondamentaux Art nouveau sont peu nombreux. On peut citer en France Xavier Schoellkopf avec la maison de la chanteuse Yvette Guilbert.

L'Art nouveau en architecture est également le prétexte pour faire preuve d'une grande capacité d'invention, tout en dépassant les formes initiales. Ainsi, la villa Jika de Louis Majorelle édifiée par Henri Sauvage à Nancy est construite dans un mélange d'architecture médiévale fantasmée et de formes typiquement Art nouveau.

L'Art nouveau a été décliné selon la sensibilité de chaque pays.

L'Art nouveau a également laissé de nombreuses œuvres dans les villes de Nancy et Bruxelles qui furent des centres de développement de ce mouvement. Également, Rīga contient la plus grande concentration d'Art nouveau en Europe.

La conception du meuble de l'Art nouveau fait revivre l'artisanat : il est le style du concepteur individuel, remettant en son centre le travail de l'artiste et éloignant celui de la machine. L'innovation majeure dans le domaine de la décoration intérieure se situe dans la recherche d’unité. Toutefois, le style n’échappe pas à certains parallèles avec la tradition, en particulier gothique, rococo et baroque ; le gothique servit ainsi de modèle théorique, le rococo d’exemple dans l’application de l’asymétrie, et le baroque de source d’inspiration en matière de conception plastique des formes. De son côté, l’art coloré du Japon, par son traitement hautement linéaire des volumes, contribua également massivement à l’émancipation de l’Art nouveau de l’asservissement à la symétrie des ordres grecs.

Le bois prenait des formes étranges et le métal, à l’imitation des entrelacements fluides de la nature, devint tortueux. En effet, le style est très largement basé sur l’observation de la nature, non seulement en ce qui concerne l’ornement, mais aussi d’un point de vue structurel. Des lignes vitales, sensuelles et ondoyantes, irriguent la structure et en prennent possession. Chaises et tables semblent modelées dans une matière à la mollesse caractéristique. Partout où cela est possible, la ligne droite est bannie et les divisions structurelles sont cachées au bénéfice de la ligne continue et du mouvement. Les plus belles réussites de l’Art nouveau, au rythme linéaire marqué, relèvent d’une harmonie qui les rapproche de l’ébénisterie du .

C’est à Nancy que les affinités entre rococo et Art nouveau apparaissent de la manière la plus convaincante. Moins fascinant, mais faisant partie des personnalités artistiques les plus en vue de l’époque, Louis Majorelle (1859-1926) est le deuxième chef de file du courant Art nouveau à Nancy. Les travaux d’incrustation de Gallé étaient le point fort, en variant beaucoup les motifs, en allant du végétal aux inscriptions littéraires à contenu symbolique. Typique pour la production de ce maître est la transformation d’éléments structurels en tiges ou en branches se terminant en fleurs. Contrastant avec l’école de Nancy, l’Art nouveau parisien est plus léger, plus raffiné et austère. Les motifs d’inspiration naturelle présentent un degré de stylisation plus grand, parfois même une certaine abstraction, et apparaissent de manière marginale.

L’art de la joaillerie a été revitalisé par l’Art nouveau, la principale source d’inspiration étant la nature. Cette rénovation fut complétée par la virtuosité atteinte dans le travail de l’émail et des nouveaux matériaux, tels que l’opale et autres pierres semi-précieuses. L’intérêt généralisé porté à l’art japonais et l’enthousiasme grandissant pour les différentes techniques de la transformation du métal, jouèrent un rôle considérable dans les nouvelles approches artistiques et les thèmes d’ornementation.

Durant les deux siècles précédents, la joaillerie fine s’était centrée sur les pierres précieuses, particulièrement sur les diamants. La préoccupation du joaillier consistait principalement à former un cadre adapté, afin que la pierre resplendisse. Avec l’Art nouveau, un nouveau type de joaillerie voit le jour, motivé et dirigé par le concept du dessin artistique, ne donnant plus l’importance centrale du bijou à la pierre sertie.

Les joailliers de Paris et Bruxelles furent les principaux instigateurs de ce revirement, donnant un nouveau souffle qui se traduira rapidement par une large renommée du style Art nouveau. Les critiques français contemporains étaient unanimes : l’art de la joaillerie traversait une transformation radicale, et le joaillier et maître verrier René Lalique se trouvait en son centre. Lalique glorifia la nature dans ses créations, amplifiant son répertoire pour y intégrer des éléments peu conventionnels — citons les libellules et herbes — inspirés par les dessins de l’art japonais.

Les joailliers désiraient se démarquer tout en inscrivant ce nouveau style dans une tradition, puisant leur inspiration dans la Renaissance, pensons notamment aux bijoux en or émaillé et sculpté. Dans la majorité des créations émaillées, les pierres précieuses cédèrent leur place prédominante, les diamants étant relégués à un rôle subsidiaire en combinaison à des matériaux moins habituels comme le verre modelé, l’ivoire et la corne. La perception du métier de joaillier évolue, considéré par ses créations comme artiste et non plus comme artisan.

Ainsi, de nombreux éléments propres au mouvement Art nouveau sont expérimentés par des peintres avant-gardistes avant d'être repris par des artistes d'autres disciplines. Les caractéristiques les plus significatives communes à la peinture de l'époque et constitutives de l'Art nouveau sont :

Il n'existe donc pas réellement d'école de peinture Art nouveau, mais le mouvement est si protéiforme, il touche tant à tous les aspects des représentations graphiques qu'il a une influence sur un grand nombre d'artistes et d'écoles, quelle que soit leur orientation finale.

À la fin des années 1880, la recherche d'un dépassement de l'impressionnisme pousse de nombreux peintres a . Initiées par deux associations d'artistes novateurs, les Vingt de Bruxelles et la Société des artistes indépendants à Paris, les écoles de peinture européennes de la fin du siècle empruntent beaucoup au mouvement Art nouveau, que ce soit les symbolistes, les préraphaélites anglais, les expressionnistes allemands, les Nabis et les Fauves.

Louis Guingot, peintre résolument Art nouveau, mais méconnu, était membre du mouvement de l'École de Nancy. Il utilisait une technique de peinture à la colle très originale. Citons également Henri Bellery-Desfontaines, Jules Chéret, Georges de Feure, Victor Prouvé et Théophile Alexandre Steinlen, tous artistes peintres qui se dédièrent tout autant à la peinture, à la lithographie et à l'affiche, refusant la séparation entre arts nobles et arts mineurs : la peinture devient un élément du décor.

En Suisse, on peut aussi citer les noms d'André Evard et Charles L'Eplattenier.

Des couvertures de livres aux illustrations de revues, des affiches publicitaires aux panneaux décoratifs, de la typographie de presse aux cartes postales, l’Art nouveau a laissé sa trace.

Dans le cadre du renouveau de l'estampe dans les années 1880, soutenu notamment par Auguste Lepère, l'un des précurseurs du nouveau graphisme a été Jules Chéret, le fils d'un typographe, qui avait suivi des cours à la Petite École, future École nationale des arts décoratifs. L'apprenti typographe a développé une nouvelle technique, plus économique, pour la reproduction de la lithographie en couleurs, plus adaptée à la reproduction de masse de l'affiche publicitaire. En outre, il a amélioré la nature esthétique du manifeste, en lui fournissant des motifs décoratifs, le transformant en un art décoratif de forme autonome. Il a été appelé « le père de l'affiche Belle Époque », et a inspiré et encouragé d'autres artistes à explorer le genre.

Des nombreux auteurs qui s’y adonnèrent, le plus influent étant sans conteste le Tchèque Alfons Mucha. Ses créations gagnèrent une renommée internationale, grâce à la délicatesse de ses dessins qui incluaient le plus souvent la figure féminine comme figure centrale, enveloppée par des arabesques d’éléments naturels. Son style, principalement utilisé dans les œuvres à caractère commercial, fut imité par les illustrateurs de son époque. Ce fut, par exemple, le cas de Gaspar Camps, surnommé le Mucha catalan.

Aubrey Beardsley fut un des plus originaux artistes Art nouveau ; ses illustrations en noir et blanc projetant un style très personnel, malgré l’irrévérence érotique et la polémique issue des thèmes qu’il choisit d’illustrer. D’autres affichistes célèbres sont Privat Livemont, Koloman Moser, Charles Rennie Mackintosh, Eugène Grasset, Franz von Stuck ou encore Ramon Casas qui est un artiste du modernisme catalan.

En typographie, de nombreuses créations de caractères se font dans l’esprit de l’Art nouveau, avec, entre autres, Eugène Grasset, Ernest Lessieux et George Auriol (polices "Auriol", "Française légère") en France, Otto Weisert (police Arnold Böcklin, 1904) en Suisse…

Dans le domaine de la Verrerie, la France connait une révolution artistique dès les années 1880. Cette évolution importante s'ouvre au grand public via l'exposition "La pierre, le bois, la terre et le verre" qui a lieu à Paris en 1884. Cette exposition présente les deux pionniers du mouvement, Eugène Michel et Eugène Rousseau. Cette nouvelle vague est immédiatement rejointe par celui qui deviendra le symbole de la verrerie Art nouveau : Émile Gallé.

Émile Gallé révolutionne l'art de la verrerie durant ses vingt années d'activité, autant par l'immense inventivité des formes déployées que par le travail sur de nouvelles techniques et des combinaisons de techniques inédites. Il est connu ainsi pour ses « verreries parlantes », sur lesquelles il inscrit des vers. Son inspiration de prédilection est la nature, que ce soit via la botanique ou l'entomologie. Il bénéficie dès ses premières productions d'un immense succès critique et public. Rapidement, des imitateurs voient le jour et satisfont une demande croissante pour ce type d'objets décoratifs. Certains présentent de belles réussites artistiques, tels les frères Auguste et Antonin Daum, qui s'associent pour certaines réalisations avec Louis Majorelle, ou les frères Muller.

De très nombreuses sociétés s'engagent donc dans l'Art nouveau, certaines avec une certaine originalité, les plus nombreuses en produisant des copies à moindre coût. Parmi les sociétés dignes d'intérêt sont Schverer & Cie, H. A. Copillet & Cie, Legras & Cie ou les frères Pannier. Cette vague dure jusque dans les années 1930, s'éteignant donc bien après la mort de Gallé en 1904 et bien après la transformation de l'Art nouveau. Cet essor de la verrerie porte également le renouveau de la fabrication d'objets en pâte de verre, avec deux vagues d'artistes, les premiers entre les années 1890 et 1900 (Henry Cros, François-Émile Décorchemont ou Georges Depret) et les seconds durant les années 1910 et 1920 (Gabriel Argy-Rousseau, Jules-Paul Brateau, Albert-Louis Dammouse et Amalric Walter).

A l'étranger, la verrerie Art nouveau se développe largement en Bohême. Une des entreprise majeure de ce mouvement est la verrerie . ; elle est également connue pour des verres iridescents aux incrustations d'or, proche de la production de Tiffany, ou l'application aux vases d'anses aux formes graciles et grimpantes. Outre la verrerie Loetz, les quelques autres sociétés à travailler ce style ne le font que de manière superficielle et pour une petite part de leur production : Ludwig Moser und sohn, ou la Glasfabrik Blumenbach.

En Allemagne, le Jungendstil s'empare de la verrerie avec des motifs floraux chez ou issus de la mythologie germanique dans la verrerie .

En Scandinavie et en Russie, peu d'entreprises se lancent dans la fabrication d'objets de style art nouveau. En Suède, les entreprises Kosta et Orrefors, en Russie, la manufacture de verre de la cour tsariste, procèdent à quelques imitations Art nouveau, bien après les débuts du mouvement en Europe de l'ouest.

Les verriers du Royaume uni sont très peu réceptifs à la stylistique Art nouveau, préférant les motifs classiques ou mythologiques. Seules les sociétés Thomas Webb & Sons et Stevens & Williams, domiciliées à Stourbridge, osent itmidement quelques réalisations aux motifs floraux, tout en restant assez conventionnels.

En Amérique du Nord, la production d'objets en verre est dominée par Tiffany. Celui-ci, tout en réalisant toujours des gammes d'objets conventionnels, se tourne vers des thèmes floraux proches de l'Art nouveau européen. La mise au point d'un procédé de fabrication industriel nouveau lui permit de développer un commerce à destination des classes moyennes, étant ainsi en phase avec l'une des aspirations des artistes Art nouveau. On peut citer également la société (à Corning) et Philip Julius Handel (dans le Connecticut). L'immense succès de Tiffany incite de nombreuses compagnies à l'imiter, et à poursuivre même lorsque le mouvement s'essouffle dans les années 1920 et 1930.

Au sein de la monarchie Austro-hongroise, l'Art nouveau est dénommé mouvement sécessionnisme et est mené par Otto Wagner et ses élèves Olbricht et Hoffmann.

La première période de ce mouvement, entre 1895 et 1904, voit apparaître des bâtiments colorés, plein de courbes, fantaisistes et même facétieux. Par la suite, ils évoluent vers des formes plus épurées et un retour à la tradition. L'exemple le plus significatif de cette école est la maison d'Adolphe Stoclet, ré&alisé par Hoffmann entre 1904 et 1911, et qui à elle seule expose une grande partie du savoir-faire des artisans viennois.

Si Nancy et Paris concentrent à elles-deux la majorité de l'Art nouveau architectural en France, de nombreuses villes abritent plusieurs réalisations de cette époque et de ce style.

À Paris, comme ailleurs en France, mais en plus foisonnant, l'Art nouveau se développe à la suite de deux mouvements majeurs de la société française : l'esprit fin de siècle, esthétisant et décadent et dont les initiateurs sont les poètes Rimbaud, Verlaine, Baudelaire ou Gautier et le triomphe du modèle social bourgeois sous l'Empire et surtout la Troisième République. L'Art nouveau spécifiquement parisien est défini par les réalisations d'Eugène Gaillard et Georges de Feure, qui mettent en avant au-delà d'autres motifs les arabesques élégantes et la féminitté.

Le mouvement artistique Art nouveau s'exprime dans la capitale dans tous les arts et se déploie pleinement dans tous les aspects de la vie quotidienne, architecture, orfèvrerie, ébénisterie, arts visuels, ... De nombreux artistes tel Hector Guimard ne se limitent pas à tel ou tel aspects mais explorent leurs idées au travers des réalisations très variées.

Paris découvre l'Art nouveau essentiellement grâce aux efforts et au talent de dénicheur de Siegfried Bing. Celui-ci, mécène et revendeur passionné d'objets d'art ouvre en 1895 une galerie appelée Maison de l'Art nouveau qui fait connaître tout autant les productions d'un très grand nombre d'artiste du mouvement qu'il ne popularise le terme auprès du grand public. Bing investit également une forte somme pour aménager le pavillon de l'Exposition universelle, dont il confie la décoration de la façade à André Arfvidson, et qui lui assure, à lui tout autant qu'à l'Art nouveau, une très large renommée.

Mais c'est Nancy qui va constituer le plus bel ensemble d'Art nouveau français. La ville a accueilli à partir de 1871 de nombreux lorrains qui souhaitaient rester Français, après l'annexion d'une partie de la Lorraine par l'Empire allemand. L'Art nouveau y devient le moyen d'expression d'un régionalisme revendiqué. Émile Gallé, Daum Frères, Jacques Grüber et bien d'autres, créent l'École de Nancy.

Les prémisses de l'Art nouveau se retrouvent dans les serres royales de Laeken, construites à la demande du roi Léopold II. Mais c'est le Parti ouvrier belge qui lança véritablement l'Art nouveau en Belgique, en confiant la construction de la Maison du Peuple à Victor Horta, en 1897. Parmi les influences de Victor Horta, on peut nommer Paul Hankar et Gustave Serrurier-Bovy, inventeurs du style à membrures.

Pour Klaus-Jürgen Sembach, la maison de l'ingénieur Tassel incarne toute la complexité de l'Art nouveau : L'utilisation des structures d'acier permet d'assurer la transparence, concept central dans l'œuvre d'Horta, et donner une illusion d'espace dans une ville où les parcelles constructibles sont étroites.

L'artiste le plus célèbre de Bruxelles est Henry van de Velde, sans doute grâce à son talent dans le marketing personnel. Il commence sa carrière par la construction de sa propre maison, la villa Le Bloemenwerf, sans formation de design ou d'architecture.
En Suisse, sous l'impulsion de Charles L'Eplatenier, une variante locale de l'art nouveau s'attache à évoquer la végétation propre aux régions montagneuses du Jura. Il s'agit du style sapin visible dans la région de La Chaux-de-Fonds. Le musée des beaux-arts de cette ville conserve un important ensemble de meubles, peintures, ainsi que de créations horlogères. 

À l'opposé des autres tendances de l'Art nouveau en Europe, les artistes, en Catalogne et en Hongrie, cherchent à créer ou à mettre en valeur une architecture nationale réelle ou supposée. Lorsque Lluis Domènech i Montaner déclarait, en 1878 : 

L’Art nouveau en Catalogne est donc l’occasion comme l’écrit l’écrivain catalan Joan Fuster de créer « une culture nationale moderne. Elle s’exprime notamment à travers l’architecture, spécifique à l’art nouveau catalan et spectaculaire dans l’espace urbain comme à Barcelone où s’exprime « la libération des couleurs et des formes » :

En Hongrie, Ödön Lechner (1845-1914), s'inspirait de l'architecture indienne et syrienne, récupérait et intégrait les éléments et techniques de construction et de design traditionnels hongrois. Suivant un style différent, le Groupe des Jeunes (Fiatalok), qui incluait Károly Kós et Dezső Zrumeczky, s’inspira de ses méthodes et créa un autre style trouvant ses racines dans l'architecture de Transylvanie. Cette démarche fait clairement écho à la réutilisation du néomudéjar, puis à la récupération des techniques traditionnelles par les architectes catalans pour créer un art national.

Si dans l'un et l'autre des cas, ces démarches aboutirent à des tendances originales, d'autres artistes s'inspirèrent des autres mouvements.

L'Art nouveau est surtout un mouvement répandu en Europe, mais il existe aussi quelques développements aux États-Unis et en Tunisie.

Voici les principaux pôles et intervenants de l'Art nouveau à travers le monde :

















Les pays et villes faisant l'objet d'un article traitant de l'Art nouveau sont indiqués en gras.


Dans les grandes histoires de l’architecture européenne du , à partir des années 1930 et tout au long des années 1940-1950, les principaux historiens, à l'instar de Nikolaus Pevsner, Sigfried Giedion ou encore , ne prennent pas en considération l’Art nouveau. Ainsi, les premières versions du "Génie de l’architecture européenne", de Pevsner, ne mentionnent ni Guimard, ni Gaudí. En fait, ces auteurs peinent à situer l’Art nouveau dans une perspective historique et acceptent difficilement la remise en cause de l’affirmation d’une structure (acier, verre…) claire, franche et très affirmée.

Dans les années 1930, les surréalistes ont une part très active dans la réhabilitation de l’Art nouveau. Salvador Dalí publie un article dans la revue "Minotaure", organisme de diffusion de la pensée surréaliste, qui s'intitule « De la beauté terrifiante et comestible du Modern style ». Cet article est illustré par les photographes les plus modernes, comme Brassaï, à qui Dalí commande un reportage sur les entrées du métropolitain nocturne de Guimard. Un autre reportage est commandé à Man Ray pour les architectures de Gaudí. André Breton partageait cette appréhension de l’Art nouveau à la manière de Dalí qui évoque les « formes libidineuses de l’Art nouveau ». Mais surtout Dalí y voit un formidable moyen de lutte contre Le Corbusier, car l’Art nouveau présente une architecture onirique, érotique et beaucoup plus proche du rythme de l’homme.

À la même époque, Dalí découvre l'œuvre du peintre Clovis Trouille — il se présentait comme un « rescapé de 1900 » —, qui l'enthousiasme par son absence d'autocensure et ses références récurrentes à l'Art nouveau. C'est aussi au cours de ces années 1930 que le designer finlandais, Alvar Aalto, conçoit des formes sinueuses, libres et expressives, évocatrices des créations les plus abstraites de l'Art nouveau.

La chaise Escargot, de Carlo Bugatti, préfigure la chaise Floris de Günter Beltzig, ou encore la célèbre Panton Chair, créée en 1959 par le Danois Verner Panton, et devenue depuis un grand classique de la décoration contemporaine. Quant aux créations de Carlo Mollino, dans les années 1950, elles rappellent les ossatures du mobilier de Gaudí.

La parution des premiers grands ouvrages traitant de l’Art nouveau se fait à la fin des années 1950, avec Johnny Watser. Rétrospectivement, ce sont surtout les reproductions des affiches qui ont séduit et le matériel Art nouveau devient accessible aux gens qui font du design. Les motifs seront repris dans les années 1960 par les jeunes artistes graphistes designers. Deux dates expliquent cette connaissance : l'organisation en 1963, au Victoria and Albert Museum de Londres, d'une exposition Mucha et, en 1966, une exposition consacrée au dessinateur Aubrey Beardsley, deux évènements essentiels dans la redécouverte de l'Art nouveau.

En 1966, le sculpteur François-Xavier Lalanne renoue avec le projet de l'Art nouveau de saisir la nature pour améliorer le cadre de vie de l'homme moderne. Cette même année apparaissent à San Francisco les premières affiches psychédéliques dont les graphistes reprendront certains thèmes de l'Art nouveau tels que la chevelure, le paon ou les formes féminines.

Entre les années 1980 et 1990, le très nombreuses institutions muséales ont recherché et acquis des éléments art nouveau. Elles ont consacré à ce mouvement de nombreuses expositions et rétrospectives. Enfin, de nombreux ouvrages parus montrent l'intérêt que le public porte sur l'art nouveau sur cette période.

Les principaux bâtiments classés par l'Unesco comme patrimoine mondial se trouvent à Barcelone et Bruxelles.

La première ville abrite des monuments classés du modernisme catalan des architectes Antoni Gaudí et Lluís Domènech i Montaner. Pour le premier, il s'agit du parc Güell, du palais Güell, de la Casa Mila, de la Casa Vicens, du travail de Gaudí sur la façade de la Nativité et la crypte de la basilique de la Sagrada Familia, de la Casa Batlló et de la crypte de la Colonie Güell. Pour le second, il s'agit de l'hôpital de Sant Pau et du palais de la musique catalane.

À Bruxelles, ce sont des bâtiments de Victor Horta et Josef Hoffmann. Du premier, ce sont les quatre habitations majeures : l'hôtel Tassel, de l'hôtel Solvay, de l'hôtel van Eetvelde et de la maison Horta, maison-atelier de l'architecte, devenue musée Horta. Du second, c'est le palais Stoclet, réalisé entre 1905 et 1911 par l'architecte autrichien Josef Hoffmann, l'un des maîtres de la Sécession viennoise.








</doc>
<doc id="251" url="https://fr.wikipedia.org/wiki?curid=251" title="Aurélien Sauvageot">
Aurélien Sauvageot

Aurélien Sauvageot ( à Constantinople - à Aix-en-Provence), est un linguiste français fondateur des études finno-ougriennes en France.

Il est encore élève à l'École normale supérieure lorsque Antoine Meillet, qui règne alors en maître sur la linguistique française, lui intime l'ordre de se consacrer à l'étude des langues finno-ougriennes. Il part donc en pour la Suède, où il suit des cours sur les langues fenniques. En , il gagne la Finlande, où il séjourne jusqu'en octobre. En , il se rend à Budapest pour enseigner le français au Collège Eötvös et étudier le hongrois. Il reste en Hongrie presque huit ans. Enfin, en 1931, après avoir soutenu sa thèse de doctorat, il inaugure à l'École des langues orientales la première chaire de langues finno-ougriennes en France. En 1932 sort de presse son "Grand dictionnaire français-hongrois", qui sera suivi en 1937 de son pendant hongrois-français. Sous le régime de Vichy, Sauvageot est démis de ses fonctions en 1941 pour appartenance à la franc-maçonnerie, avant d'être rétabli à la demande expresse des ambassades de Finlande et de Hongrie dans sa chaire en . Entre-temps, il traduit pour gagner sa vie des ouvrages finlandais notamment sur la guerre avec l'Union soviétique.

En 1949, il publie son "Esquisse de la langue finnoise", description originale et personnelle très éloignée des grammaires traditionnelles. En 1951, il réitère l'expérience avec son "Esquisse de la langue hongroise" qui montre qu'elle dispose de mécanismes grammaticaux très réguliers. Dix ans plus tard, il publie "Les Anciens Finnois", initiation aux problèmes que posent les époques archaïques ayant précédé l'entrée des Finnois dans l'histoire. Parmi ses publications ultérieures sur les langues et les cultures finno-ougriennes, on peut citer son "Premier Livre de hongrois" (1965), son "Histoire de Finlande" (1968), "L'Édification de la langue hongroise" (1971), "L'Élaboration de la langue finnoise" (1973). Il est également l'auteur de nombreux ouvrages sur la langue française, notamment sur le français parlé et le français fondamental. On lui doit enfin des articles sur le tahitien, l'eskimo, le youkaguire ou les langues samoyèdes, ainsi que des traductions d'œuvres de la littérature hongroise.

Après 35 années d'enseignement, il se retire en 1967 à Aix-en-Provence. Il a 91 ans quand sort de presse son dernier livre publié de son vivant, "Souvenirs de ma vie hongroise". En 1992 paraît un ouvrage posthume qui résume ses idées sur la langue et la linguistique : "La Structure du langage".






Dans cet ouvrage, publié pour soutenir la Finlande en guerre avec l'Union soviétique (Guerre d'Hiver), Aurélien Sauvageot a rédigé "Un regard sur l'histoire de la Finlande" et "Le problème des langues" et traduit l"'Ordre du jour du maréchal Mannerheim" proclamé à l'occasion du traité de paix mettant fin à cette guerre.


</doc>
<doc id="252" url="https://fr.wikipedia.org/wiki?curid=252" title="Affixe">
Affixe

En morphologie, domaine de la linguistique, un affixe (du latin "ad-fixus" > "affixus", « (qui est) fixé contre ») est un morphème en théorie lié qui s'adjoint au radical ou au lexème d'un mot.
Des affixes peuvent se lexicaliser et donc devenir des morphèmes libres : c'est par exemple le cas pour le préfixe "ex-" dans une expression comme "mon ex", à savoir "mon ex-mari / -petit ami", etc.

Selon la norme ISO 4:1997, un affixe est un .

Elle distingue le préfixe, , du suffixe, .

Les affixes sont principalement de deux natures : les affixes grammaticaux et flexionnels et les affixes de dérivation.

Les affixes grammaticaux et flexionnels donnent naissance non à un nouveau lemme mais à une autre forme d'un même radical :

Les affixes de dérivation permettent de former, à partir d'un même radical, de nouveaux lemmes :

Selon leur place par rapport au radical, les affixes se subdivisent en plusieurs types :

Les affixes peuvent s'ajouter les uns aux autres ; un mot comme "anticonstitutionnellement", par exemple, s'analyse grossièrement ainsi :
D'autre part, le jeu de l'évolution phonétique fait parfois que le locuteur profane ne peut distinguer les morphèmes d'un mot donné : dans le verbe "pondre", par exemple, "po-" représente un ancien préfixe que, déjà en latin (dans "ponere"), les locuteurs ne savaient pas reconnaître comme tel. De fait, n'étant plus productif en latin, il ne l'est pas plus en français.

Il existe d'autres types de placements qui ne concernent plus vraiment une vision morphématique de la question mais considèrent que la flexion interne fait aussi partie des affixes :

Dans certaines langues, les affixes peuvent être reliés étymologiquement à des morphèmes autonomes comme des prépositions, c'est-à-dire étymologiquement des adverbes. C'est le cas dans nombre de langues indo-européennes. De sorte, il est parfois possible de leur rendre leur fonctionnement autonome en les séparant du radical : on parle alors d'une "tmèse" (du grec τμῆσις "tmêsis" « coupure ») qui est à ne pas confondre avec la figure de style du même nom et qui s'apparente à l'hyperbate. En grec ancien, la tmèse est assez rare et se limite surtout à des états anciens de la langue, quand la distinction entre affixe et adverbe n'était pas encore nette : ainsi, chez Sappho :
L'auteur utilise φέρεις... ἄπυ au lieu de ἀπύφερεις. Le préfixe ἀπύ- « (de) loin » est séparé du thème verbal φέρεις « tu transportes » et redevient grammaticalement mais non sémantiquement une préposition autonome. Un contresens ferait traduire par « tu mènes l'enfant loin de (ἀπύ-) la mère », en considérant que ἄπυ est une préposition ayant pour régime μάτερι « (à) la mère » ; on aurait dans ce cas ἀπὺ μάτερι « loin de sa mère » (noter la différence d'accentuation). Il faut donc faire de ἀπύ- un préfixe détaché du radical pour obtenir le sens, plus convaincant dans le reste de la phrase, de ἀπυφέρεις, c'est-à-dire « tu ramènes (de loin) ». Les tmèses sont aussi fréquentes chez Homère : τίθει... πάρα au lieu de παρατίθει « il place à côté, il offre ».

Ce cas de figure est cependant régulier dans certaines langues germaniques comme l'allemand où les « particules séparables » sont plus nombreuses que les inséparables (liste fermée : "zer-", "be-", "er-", "ge-", "miß-", "ent-", "emp-" et "ver-"). La position du préfixe, collé au verbe ou séparé, est régie par des règles strictes : an-ziehen" « serrer » mais "sie zieht die Schraube an « elle serre la vis », par opposition à "er-schlagen" « tuer » / "Kain erschlägt" Abel « Caïn tue Abel ».

La mobilité relative du préfixe dans certaines langues indo-européennes est un reliquat lointain d'une langue, l'indo-européen, dans laquelle les prépositions et les préfixes étaient d'anciens adverbes, même les préfixes mobiles en l'allemand ("umfáhren" « contourner (avec un véhicule)» ["er umfährt den Baum" synonyme à "er fährt um den Baum" « il contourne l'arbre », part.: umfahren] par opposition à "úmfahren" « renverser (avec un véhicule)» ["er fährt den Baum um" « il renverse l'arbre », part.: umgefahren]).
Poussé plus loin, le raisonnement permet de penser que les désinences flexionnelles elles aussi sont issues d'anciennes formes autonomes, ce que des langues très anciennes comme le grec d'Homère et le sanskrit du Rig Veda confirment en partie. Ainsi, la différence principale qui existe entre les langues agglutinantes et les langues flexionnelles se trouve réduite si l'on considère que l'existence de langues flexionnelles est peut-être le résultat de l'évolution d'états plus anciens, qui rejoignent le type agglutinant. Certaines désinences reconstruites de l'indo-européen montrent en effet des liens implicites avec d'autres types de suffixes : c'est le cas pour le celui de formation de mots féminins que l'on écrit "*-ih" (on lit « /i/ laryngale 2 ») et qui donne en grec des noms principalement féminins en -ια /ia/, équivalents aux noms latins surtout féminins en "-ia" et en sanskrit aux noms féminins en "-ī" (résultat phonétique attendu de "*-ih"). Ce suffixe devient dans les langues en question une désinence, celle de nominatif singulier féminin (sauf pour quelques cas). Étymologiquement, ce n'est qu'un suffixe de formation de noms dérivés d'un masculin indiquant la possession. 



</doc>
<doc id="254" url="https://fr.wikipedia.org/wiki?curid=254" title="Adessif">
Adessif

En linguistique, l'adessif est le cas grammatical exprimant la position en un lieu ouvert (par opposition à l'inessif) ou à proximité immédiate de quelque chose.

Exemples:
Il n'est pas utilisé pour exprimer la possession en hongrois (voir cet article) alors qu'en finnois, ce cas est aussi celui de la possession : "minulla on" ↔ "j'ai". On pourrait même dire que cette utilisation prime sur le sens positionnel figé dans l'étymologie du nom du cas, sans qu'on puisse clairement établir le sens d'une éventuelle dérivation. Cette relation entre deux interprétations (position superficielle et possession) se retrouve avec cohérence dans les cas voisins, allatif et ablatif.


</doc>
<doc id="255" url="https://fr.wikipedia.org/wiki?curid=255" title="Abessif">
Abessif

En linguistique, l'abessif est le cas grammatical exprimant l'absence d'une chose. On le désigne également sous le nom de caritif. Il correspond à la préposition française "sans".

Exemple : en estonien (suffixe "-ta") : "isa" « père » → "isata" « sans père ».

Le même exemple utilisant l'abessif en finnois serait "isättä", mais ce cas tombe en désuétude, remplacé par la préposition "ilman", régissant le partitif. L'abessif se maintient cependant dans certaines expressions bien implantées : "Mennä ulos pipotta / hatutta" (« sortir sans bonnet / sans chapeau »). Ce cas sert surtout en finnois moderne à la construction « sans + verbe »: "puhumatta" « sans parler ».

Il a aussi donné des expressions comme : "lukuunottamatta" (littéralement « sans prendre dans le nombre ») = « excepté ».


</doc>
<doc id="256" url="https://fr.wikipedia.org/wiki?curid=256" title="Allemand">
Allemand

L’allemand (en , ) est l'une des langues indo-européennes appartenant à la branche occidentale des langues germaniques. Du fait de ses nombreux dialectes, l'allemand constitue dans une certaine mesure une langue-toit ().

Son histoire, en tant que langue distincte des autres langues germaniques occidentales débute au Haut Moyen Âge, lors de la seconde mutation consonantique.

De nos jours, ses locuteurs, appelés « germanophones », se répartissent principalement, avec près de 100 millions de locuteurs, en Europe, ce qui fait de leur langue la plus parlée au sein de l'Union européenne (UE).

L'allemand n'est langue officielle d'aucun État fédéré des États-Unis malgré une assertion récurrente fondée sur une confusion historique.


Avec la première mutation consonantique ("erste germanische Lautverschiebung") aux environs du , naissait le germanique commun à partir d'un dialecte indo-européen. Cette transformation explique des différences entre les langues germaniques (plus l'arménien) et les autres langues indo-européennes. On peut, pour simplifier, présenter les faits ainsi :

On commence à parler de langue allemande (ou, en linguistique « haut-allemand ») lorsque les dialectes parlés dans le sud-ouest de l'Allemagne subirent la seconde mutation consonantique ("zweite germanische Lautverschiebung" ou "hochdeutsche Lautverschiebung", que l'on situe "grosso modo" vers le ), au cours de laquelle la langue commença à se différencier des dialectes du nord ("Niederdeutsch", bas-allemand).

Cette modification phonétique explique un certain nombre de différences entre l'allemand actuel et, par exemple, le néerlandais ou l'anglais:

pour résumer, *k / *p / *t ➜ "ch" / "pf" (ou "f") / "ts" (ou "s")

Les dialectes du nord qui n'ont pas ou peu subi cette seconde mutation phonétique sont qualifiés de bas-allemand. Cette appellation est jugée abusive par certains linguistes, notamment néerlandais (qui ne sont pas « allemands », du moins depuis les traités de Westphalie). Mais le terme « allemand » n'est ici qu'un terme linguistique, un peu comme « roman », « slave » ou « scandinave ».

Entre le et le eut lieu une diphtongaison dans les parlers du Sud-Ouest concernant l'articulation en deux phonèmes de "ei", "eu" et "au". Cela explique à nouveau certaines différences entre l'allemand standard et, par exemple, le néerlandais (les lettres dans les parenthèses expliquent la prononciation en utilisant la langue française):

Contrairement aux États voisins, les contrées germaniques sont restées morcelées ("Kleinstaaterei") au cours de l'ensemble du Moyen Âge contribuant au développement de dialectes très différents et parfois mutuellement inintelligibles. Un premier pas vers une langue interrégionale correspond au "Mittelhochdeutsch" poétique des poètes de cour vers le , bien que l'influence sur la langue vulgaire fut quasiment nulle, en raison de la faible alphabétisation. Aussi les régions germaniques restèrent-elles longtemps coupées en deux régions linguistiques : 

En 1521, Martin Luther traduisit le "Nouveau Testament" dans cet allemand standard en développement et en 1534, l'Ancien Testament. Bien que Luther ne fût pas, comme il fut considéré autrefois, le pionnier dans l'établissement d'une langue interrégionale — en élaboration depuis le — il n'en reste pas moins que la Réforme protestante contribua à implanter l'allemand standard dans les administrations et les écoles, y compris dans le nord de l'Allemagne, qui finit par l'adopter. En 1578, Johannes Clajus se basa sur la traduction de Luther pour rédiger une grammaire allemande.

Mais, jusqu'au début du , le "Hochdeutsch" resta une langue souvent écrite, que beaucoup d'Allemands, en particulier dans le sud, apprenaient comme une langue étrangère.

Avec la domination de l'Empire austro-hongrois en Europe centrale, l'allemand y devint la langue véhiculaire. En particulier, jusqu'au milieu du , les marchands et, plus généralement, les citadins y parlaient l'allemand, indépendamment de leur nationalité : Prague, Budapest, Presbourg, Agram et Laibach constituaient des îlots germanophones au milieu des campagnes qui avaient conservé leur langue vernaculaire.

Johann Christoph Adelung publia en 1781 le premier dictionnaire allemand exhaustif, initiative suivie par Jacob et Wilhelm Grimm en 1852. Le dictionnaire des frères Grimm, publié en seize tomes entre 1852 et 1860, reste le guide le plus complet du vocabulaire allemand. La normalisation progressive de l'orthographe fut achevée grâce au "Dictionnaire orthographique de la langue allemande" de Konrad Duden en 1880, qui fut, à des modifications mineures près, déclaré comme référence officielle dans la réforme de l'orthographe de 1901.

C'est une langue germanique de la branche ouest, proche, notamment, du néerlandais.



L'allemand s'écrit avec les 26 lettres de l'alphabet latin, trois voyelles surmontées d'un "Umlaut" (sorte de tréma) "ä", "ö" et "ü", et un symbole graphique spécial "ß", "Eszett" ou "scharfes S" (ligature de S long et de « s » ou « z »), utilisé en lieu et place de "ss" dans certains cas (principalement après une voyelle longue ou une diphtongue). La Suisse n'utilise plus le "ß" depuis les années 1930. Jusque dans les années 1940, l'allemand était imprimé en écriture gothique ("Fraktur") et écrit en sütterlin, qui sont différentes versions de l'alphabet latin.

L'orthographe allemande se déduit en général de la prononciation et d'un minimum de connaissances. Mais les fortes disparités régionales dans la prononciation peuvent rendre la tâche ardue. Les principales difficultés orthographiques de l'allemand résident dans :

Afin de supprimer une partie des difficultés décrites ci-dessus, les représentants allemands, suisses et autrichiens convinrent d'une réforme de l'orthographe. Elle est entrée en vigueur en 1998 en Allemagne et est devenue obligatoire à partir de la mi-2005. La dernière réforme datait de 1901 et portait entre autres sur la suppression du "h" dans "Thor" et sur l'ajout du "e" pour les voyelles longues et accentuées dans la conjugaison des verbes, par exemple "kritisirt" ➜ "kritisiert").

Les principaux changements concernent :

Cette réforme rencontre une forte critique en Allemagne. Le Land de Schleswig-Holstein a voté le retour à l'orthographe traditionnelle en 1998 (décision annulée pourtant par le parlement régional) et certains journaux et éditeurs ont depuis décidé de revenir à la graphie conventionnelle.

Contrairement à des langues telles que l'anglais, l'allemand standard ("Hochdeutsch") se prononce de manière assez conforme au texte écrit et contient très peu d'exceptions (les sons se prononcent souvent de la même façon), hormis pour les mots d'emprunt. Presque toutes les voyelles se prononcent clairement, voire longuement, même sans être suivies de lettre muette servant à insister sur la lettre précédente.

Toutefois, les francophones qui apprennent l'allemand rencontrent généralement quelques difficultés, listées ci-dessous.

Tous les sons n'y figurant pas se prononcent toujours de la même manière qu'en français (a, b, d, f, i, k, l, m, n, o, p, ph, q, r, t, x).

Lettres à Umlaut (le tréma français)


Les umlauts indiquent également l'accentuation. Ils marquent souvent le pluriel ou le diminutif (avec « -chen » et « -lein ») des mots.

Lorsque les Umlauts ne sont pas accessibles (clavier étranger, Internet…), ils sont représentés par « e » : ae pour ä, oe pour ö, ue pour ü.

En Alsace-Moselle, on remplace habituellement les umlauts : "Koenigshoffen, Haut-Koenigsbourg, Hœnheim" (dans ces exemples, c'est le ö qui est remplacé)



Il faut bien veiller à ne prononcer qu'un son et pas deux sons distincts pour les combinaisons de deux voyelles : par exemple, pour la combinaison [ei], il faudra prononcer ail (ou le [i] du mot anglais knife) et non le [aï] de na|ïf. Le son français [oi] en est l'exemple même : il ne se prononce pas directement [ou|a].


Notes :

L'allemand est une langue flexionnelle comportant des conjugaisons et des déclinaisons.

Le principe de la conjugaison allemande est assez proche du principe de la conjugaison française. Les différences notables sont :


En ce qui concerne la morphologie, les trois principaux types de verbes sont :

Parmi les verbes irréguliers se rangent également les auxiliaires de mode ("können", pouvoir ; "dürfen", avoir le droit; etc.), qui sont employés dans un nombre important de contextes différents.

La déclinaison allemande comporte quatre cas, le nominatif, l'accusatif, le datif et le génitif, auxquels se combinent trois genres grammaticaux, le masculin, le féminin et le neutre ainsi que deux nombres, le singulier et le pluriel.

Le porteur essentiel de la marque de déclinaison est le déterminant, secondé par l'adjectif épithète si le déterminant est absent ou bien sans désinence (marque de déclinaison).

Le nom porte également la marque de déclinaison au datif pluriel à tous les genres, au génitif singulier masculin ou neutre.

Les déclinaisons sont employées :

 Voir aussi grammaire allemande

L'allemand a pour particularité syntaxique principale de placer des éléments importants, soit en première position dans la phrase, soit en dernière position. L'inversion du verbe et du sujet a lieu quand un complément vient en tête de phrase ; 
« heute geht es ihm gut == aujourd'hui il va bien » ; le rejet est le renvoi du verbe en fin de phrase dans les subordonnées « ..., wenn er Wein trinkt = lorsqu'il boit du vin »

Autre exemple :

Er "nahm gestern trotz aller Schwierigkeiten "diese Maschine in Betrieb"."
Il a mis cette machine en service hier malgré toutes les difficultés.

Sont mis en valeur
Avant l'action et l'objet sont énumérées les circonstances. L'ordre de la phrase peut être modifié pour insister sur un des éléments, que l'on place alors en tête de phrase :

"Gestern nahm er trotz aller Schwierigkeiten diese Maschine in Betrieb."
C'est hier qu'il a mis cette machine en service malgré toutes les difficultés.

"Trotz aller Schwierigkeiten nahm er gestern diese Maschine in Betrieb."
Malgré toutes les difficultés, il a mis cette machine en service hier.

"Diese Maschine nahm er gestern trotz aller Schwierigkeiten in Betrieb."
C'est cette machine qu'il a mise en service hier malgré toutes les difficultés.

La langue allemande peut se passer d'article au génitif en juxtaposant deux éléments (déterminants + déterminé) - ou même beaucoup plus. L'allemand est même connu pour sa capacité à former des surcomposés de grande longueur que les Allemands eux-mêmes appellent par dérision "" « vers solitaires »...

Exemples : 

Certains des exemples ci-dessus sont fictifs (ils sont morphologiquement corrects, mais n'ont pas été employés de façon réelle). D'autre part, quand un surcomposé est très long ou peu courant, on peut le diviser par un trait d'union: "Mehrjahres-Programmvereinbarungen", « conventions-programmes pluriannuelles ».

La composition à multiples éléments ne se limite pas au couple objet possédé-possesseur (du type "Kapitänsmütze" « casquette de capitaine ») mais aussi à toutes sortes de relations :

En français, la possession marquée par « de » a plusieurs sens qui se rendent en allemand de trois manières distinctes :

Il faut savoir avant tout qu'en allemand, le premier mot dans un composé est, comme l'adjectif qui précède le sujet, moins mis en avant que s'il est placé "après" le sujet.

Prenons le titre du de la bande dessinée "Broussaille", « "La nuit du chat" ». Dans le titre (et dans l'histoire), l'élément (et le sujet) important est le chat, connu et recherché. C'est la nuit du chat, qui « appartient » au chat.
On va donc préférer la traduction "Die Nacht der Katze ("La nuit du chat) à Die Katzenacht ("La nuit à chats"). Dans cette dernière formulation, c'est l'élément nuit (Nacht) qui est visé.

Autre exemple plus rapproché de la syntaxe française : Dans « Nuits dans les jardins d'Espagne », la traduction correcte est "Nächte in den Gärten von Spanien" et non "Nächte in den spanischen Gärten". La traduction de "Nächte in den spanischen Gärten" est « Nuits dans les jardins espagnols ».

La langue allemande (ainsi que le peuple) a la particularité d'avoir des appellations très différentes d'une langue à l'autre (par exemple "German", "Deutsch", "alemán", "német", etc.). En effet, six racines différentes entrent en jeu :


En hébreu classique, les pays allemands sont connus sous l’appellation de "ashkenaz" (אשכנז), par généalogie populaire d'après Gen. 10:3. Pour l’hébreu moderne, voir plus haut.

Un nombre important de mots furent empruntés aux dialectes germaniques par le roman et l'ancien français (par ex. "heaume", "éperon", "cible", "fauteuil") ; seuls les mots d'origine plus récente sont encore discernables en tant qu'emprunts lexicaux (frichti, ersatz).

Exemples de phrases : 

L'allemand a toujours la possibilité sémantique de former de nouveaux mots par les procédés de composition et de dérivation.

Tout comme le français a créé le verbe "des pacsés/se pacser" à partir d'une abréviation administrative de l'état-civil (PACS), l'allemand peut adapter dans le langage courant des termes nouveaux adaptés à l'actualité.

Exemple :
Le mot "apprenti" s'est dit pendant des siècles "Lehrling" du verbe "lehren" « enseigner » signifiant donc « celui à qui l'on enseigne quelque chose », suivi du diminutif "-ling". Son maître était le "Meister".

La réforme administrative au début des années 1970 a remplacé le terme "Meister" par deux termes précisant que l'un enseigne effectivement ("der Ausbildende", gérondif de "ausbilden" « former ») et que l'autre a le droit et la responsabilité de la formation ("der Ausbilder" « le formateur »). L'apprenti devint logiquement "der Auszubildende" (c'est-à-dire celui qui doit être formé), en abrégé AZUBI (prononcé ATSOUBI). Le génie de la langue ajouta pour la forme féminine la terminaison habituelle -"in" et l'on prononça le tout "ATSOUBINE". Or, le terme "Biene" « abeille » désigne aussi une jolie fille ce qui transforma la sèche abréviation en un joli petit nom.

Prononciation; certaines lettres se prononcent différemment en Autriche. Le « R » a tendance à être roulé, un peu comme dans le Sud de l'Allemagne (Bavière).

D'une manière générale, dans la République démocratique allemande, la langue s'était enrichie de termes officiels, spécifiques au régime politique tout comme sous le régime national-socialiste. Dans le langage courant, de nombreux termes tournaient ces derniers en dérision. Par exemple, l'abréviation VEB (pour "Volkseigener Betrieb", usine propriété du peuple) devenait "Vaters ehemaliger Betrieb" (l'ancienne usine de Papa)...

De très nombreuses abréviations tirées de l'idéologie communiste avaient cours, les étudiants devaient tous suivre des cours de ML (marxisme-léninisme), 
On retrouve des néologismes ou de nouvelles expressions dans un nombre important de domaines, notamment : 



</doc>
<doc id="257" url="https://fr.wikipedia.org/wiki?curid=257" title="Allatif">
Allatif

En linguistique, l'allatif (du latin "allatum", participe passé de "affero" « apporter ») est un cas grammatical exprimant le lieu non clos vers lequel se produit un mouvement. L'allatif est complémentaire de l'ablatif en exprimant le mouvement en direction opposée, et l'adessif se situe entre les deux en exprimant la position en un lieu non clos sans mouvement.

Dans certaines langues, notamment les langues finno-ougriennes, l'allatif est un cas à proprement parler, utilisé régulièrement et de manière productive. D'autres langues, par exemple l'hébreu, n'y ont recours que dans quelques cas en tant que forme fossile, tandis que le mouvement vers un lieu est exprimé en général au moyen de prépositions sans flexion ni agglutination.

Exemples en finnois (suffixe "-lle") :
Dans l'acception possessive, c'est le cas de l'acquisition en finnois: "anna minulle" « donne-moi » ; c'est-à-dire dans d'autres langues le datif, qui n'existe pas en finnois : "Annan kirjan tytölle" « Je donne le livre vers la jeune fille » (littéralement) = « Je donne le livre à la jeune fille ».

En estonien (suffixe "-le") :

En hongrois (suffixe "-hoz/hez/höz") :

Il est à noter que l'allatif finnois correspond à un allatif ("-hoz") en hongrois quand il signifie « auprès de » (par exemple "menen Pekalle" «je rejoins Pekka») et à un sublatif ("-ra") quand il signifie « sur ».

Exemples en basque où le cas est utilisé régulièrement :

En lituanien, l'allatif est l'un des quatre cas de lieu dits secondaires, c'est-à-dire apparus plus tardivement et probablement sous influence finno-ougrienne : par exemple "miškop(i)" « dans la forêt », "jūrosp(i)" « dans la mer » (mouvement vers un lieu) ; aujourd'hui il n'est plus utilisé que dans des formes figées, reliques du passé de la langue, qui ont une valeur adverbiale, par exemple "vakarop" « le soir » (adverbe), "velniop" « au diable ! ».

En grec ancien, quelques mots comme : Ἀθῆναι "Athēnai" « Athènes » → Ἀθήναζε "Athēnaze" « à Athènes » (mouvement vers un lieu).

En hébreu, l'accusatif du proto-sémitique est devenu de fait le cas directif dans quelques formes fossiles : הבית "ha-báyt" « la maison » → הביתה "ha-baytáh" « à la maison » (mouvement vers un lieu).



</doc>
<doc id="258" url="https://fr.wikipedia.org/wiki?curid=258" title="Ablatif">
Ablatif

En linguistique, l’ablatif est un cas grammatical exprimant notamment un déplacement à partir d'un lieu (ouvert).

En basque, l'ablatif indique la provenance, l'origine, qui peut se traduire en français par « de ». Le cas grammatical de l'ablatif est nommé "nondik" et se fait par l'ajour de suffixe "-(e)tik", "-etatik" au pluriel :

"Bilbotik etorri naiz." (Je viens de Bilbao.)

L'ablatif finnois correspond à un ablatif hongrois lorsqu'il signifie "de" ou "à côté de" et à un délatif lorsqu'il signifie "du dessus de" : "putosin katolta", je suis tombé du toit ; "pihalta", de la cour.

Dans l'acception possessive, c'est le cas de la dépossession : "ota häneltä", prends-lui.

En latin, l'ablatif désigne le lieu d'origine (après des prépositions comme "ab", "ex", "de") : "de profundis", des profondeurs ou du fond de l'abîme. Il exprime aussi la provenance ou la matière dans laquelle est faite une chose.

Employé seul, il équivaut à l'instrumental disparu du proto-indo-européen et a une valeur de complément de moyen : "hostem gladio occidit", il tue l'ennemi par le glaive. Avec la préposition "cum", l'ablatif prend une valeur d'accompagnement.

Il a également une valeur de locatif, peu utilisé en latin (après la préposition "in"), et permet de désigner le lieu où l'on est (par opposition à l'accusatif, qui désigne le lieu où l'on va).

L'ablatif peut être employé également pour une localisation dans le temps :

"In "villa" Scipionis vidi balneolum angustum, tenebricosum ex "consuetudine antiqua... (Sénèque, Extrait des "Lettres à Lucilius", 86)."

Il peut aussi être utilisé comme ablatif absolu.



</doc>
<doc id="260" url="https://fr.wikipedia.org/wiki?curid=260" title="Alberta">
Alberta

L'Alberta est une province de l'Ouest du Canada, la sixième plus vaste du pays, avec et la quatrième la plus peuplée, avec près de 4,1 millions d'habitants en 2016. Elle fait partie de la région des prairies. Sa capitale est Edmonton alors que Calgary est sa plus grande ville. Les Albertains sont en très grande majorité des citadins et plus de la moitié d'entre eux vivent dans ces deux agglomérations. .

La partie Est de la province est occupée par les Grandes Plaines tandis que la partie Ouest voit s'élever les montagnes Rocheuses. Le mont Columbia, deuxième plus haut sommet des Rocheuses canadiennes est situé dans la province de l'Alberta comme l'intégralité des parcs nationaux de Banff et de Jasper. Elle abrite également une importante réserve de biosphère en Amérique du Nord, protégées au sein du parc national des Lacs-Waterton. Bordée par la Colombie-Britannique à l'ouest, la Saskatchewan à l'est, les Territoires du Nord-Ouest au nord, et par l'État américain du Montana au sud, l'Alberta, avant de faire partie du Canada, a d'abord été partagée en deux colonies britanniques, la terre de Rupert et le Territoire du Nord-Ouest. Créée depuis les actuels Territoires du Nord-Ouest, elle devient la du Canada le .

Aujourd'hui, elle attire de nombreux immigrants et fait partie des provinces conservatrices dominées par les Blue Tories. Son économie dynamique repose sur l'élevage, l'agriculture, les hydrocarbures, les industries pétrochimiques et de techniques de pointe. L'Alberta est la province la plus riche du pays en hydrocarbures, à ce titre, elle fournit 70 % du pétrole et du gaz naturel exploité sur le sol canadien.

L'Alberta, qui est devenu officiellement une province en 1905, doit son nom au Marquis de Lorne qui fut gouverneur du Canada entre 1878 et 1883. En effet, celui-ci a proposé le nom d"'Alberta" en l'honneur de sa femme, la princesse Louise Caroline Alberta, qui était la fille de la reine Victoria.

La province moderne d'Alberta jusqu'à la latitude 53°N a été pendant longtemps une partie de la terre de Rupert. Les Français furent les premiers colons au Nord-Ouest en 1731 où ils établirent des communautés sur les cours d'eau et les postes de traite (aujourd'hui, autour des lacs La Biche et Sainte-Anne ainsi que dans la région de Saint-Paul, Bonnyville et Athabasca). La Compagnie du Nord-Ouest de Montréal a occupé la partie nord du territoire d'Alberta avant que la Compagnie de la Baie d'Hudson ne prenne finalement possession du territoire.

Le premier explorateur européen en Alberta, Peter Pond, visita la région d'Athabasca au nom de la Compagnie du Nord-Ouest et il construisit Fort Athabasca près du lac la Biche en 1778. Roderick Mackenzie, cousin d'Alexander Mackenzie, construisit Fort Chipewyan près du lac Athabasca dix ans après, en 1788. Mackenzie suivit la rivière Saskatchewan Nord jusqu'à son point le plus au nord, près d'Edmonton, puis continuant à pied vers le nord, il atteignit la rivière Athabasca qu'il suivit jusqu’au lac Athabasca. Il a alors découvert le fleuve qui porte son nom, le fleuve Mackenzie. Il le suivit jusqu’à son embouchure dans l'océan Arctique. En retournant au lac Athabasca, il suivit la rivière de la Paix et, finalement, atteignit l'océan Pacifique. Il sera ainsi le premier Européen à traverser le continent au nord du Mexique.

La région d'Alberta a été créée comme une partie des Territoires du Nord-Ouest en 1875. Des privilèges additionnels et une législature locale ont été ajoutés en 1905 quand l'Alberta a été agrandi et a reçu le statut de province avec sa capitale à Edmonton. L'assemblée législative compte 83 membres.

L'Alberta a déjà été l'hôte des Jeux olympiques d'hiver en 1988.

L'évolution du territoire a été liée à son exploration et utilisation par les Européens à partir du .

L'Alberta est une province située à l'ouest du Canada et occupe une superficie de ². Elle est située entre la Colombie-Britannique à l'ouest, la Saskatchewan à l'est, le Montana au sud et les Territoires du Nord-Ouest au nord.

La province compte des dizaines de rivières et de lacs idéaux pour la natation, le ski nautique, la pêche et une gamme complète d'autres sports nautiques. Il y a une multitude de lacs, tous de moins de ². Il y a les deux lacs plus grands : le lac Athabasca, ² (dont une partie se trouve en Saskatchewan), et le Lesser Slave Lake, d'environ ².

La province possède un grand nombre de parcs naturels dont 5 parcs nationaux : Banff, Elk Island, Jasper, Lacs-Waterton et Wood Buffalo.

La frontière de l'Alberta s'étend sur du nord au sud, et sur environ de l'est à l'ouest. Il est normal que le climat change considérablement entre les parallèles de 49° et 60° nord et également entre 110° et 120° ouest. Le climat est également encore influencé par les différentes altitudes de la province.

Le nord de l'Alberta a beaucoup moins de jours sans gel que le sud, qui est presque un désert sans pluie en été. L'ouest de l'Alberta est protégé par les montagnes Rocheuses, aussi, en hiver, des vents chauds et secs provenant de l'ouest et appelés Chinook apportent des périodes de chaleur aux hivers au demeurant plutôt froids. L'est de l'Alberta est une prairie plate et sèche, où il peut faire très frais ( en hiver) ou très chaud (+ en été). Le centre et le sud de l'Alberta sont les endroits du Canada les plus sujets aux tornades en raison de la chaleur et des orages violents qui sont communs en été. La capitale de l'Alberta, Edmonton, est presque exactement au centre de la province, et la plus grande partie des réserves de pétrole de l'Alberta s'y trouve. Le sud de l'Alberta, là où est situé Calgary, est connu pour son ranching et l'élevage du bétail.

En général, l'Alberta a des hivers frais, avec une température d'environ pendant la journée, et des étés chauds, avec une moyenne d'environ .

L’Alberta dispose, en général, une bonne quantité de ressources en ce qui concerne l'eau. Tout d'abord, une grande quantité de ruisseaux traversent un grand nombre de vallées pour s'unir et ainsi former la Rivière Oldman ainsi que la Rivière Bow. Lorsque ces deux rivières se croisent, elles s'unissent pour continuer sous le nom de la Rivière Saskatchewan Sud pour continuer sur une distance de 1392 km. Plusieurs barrages installés sur la Rivière Saskatchewan Sud forment le Lac Diefenbaker, un immense réservoir qui fournit de l'hydroélectricité à l'ensemble du sud-ouest de la province de la Saskatchewan.

Dans le nord de la province, un ensemble de petit ruisseaux se rejoignent pour créer la La Biche, mieux connu sous le nom de "Red Deer River." Cette rivière rejoignera la "Rivière Saskatchewan Sud". Un peu plus loin, on aperçoit un peu plus au centre de la province la Rivière Saskatchewan Nord qui commence dans le Champ de glace Columbia, elle coule ensuite jusqu'à Rocky Mountain House où elle reçoit les eaux de la Clearwater River. La rivière traverse la ville d'Edmonton Beaucoup plus à l'est, hors de la province de l'Alberta, dans la région de Prince Albert, les rivières Saskatchewan Nord et Saskatchewan Sud s'unissent pour créer la Rivière Saskatchewan. Celle-ci continue son chemin jusqu'au Lac Winnipeg pour finalement se jeter dans la Baie d'Hudson.

Dans le nord de la province, l'ensemble des eaux convergent dans l'Océan Arctique. En effet, à partir du Mont Athabasca, la Rivière Athabasca se dirige vers le nord pour atteindre le Lac Athabasca. De plus, à partir des Montagnes Rocheuses, la Peace River se rendre jusqu'en Alberta pour se déverser dans un affluent du Lac Athabasca. L'eau est, à ce moment, transporter dans la Rivière des Esclaves jusqu'au Grand lac des Esclaves qui se trouve au Territoire du Nord-Ouest. Le cours d'eau devient ainsi le Fleuve Mackenzie qui termine sa course dans l'Océan Arctique.

Pour ce qui est des lacs, seul le Lac Athabasca a une certaine importance avec une superficie dont une certaine partie se trouve dans la province de la Saskatchewan. On peut aussi noter la présence du Petit lac des Esclaves qui a une superficie de . Outre ces deux plans d'eau, tous les autres lacs de l'Alberta ont une superficie inférieur à .

L'Alberta est un très grand territoire et détient un climat qui diffère selon les différents locations internes de la province. Tout comme l'ensemble des provinces du Canada, L’Alberta profite de 4 saisons qui se succèdent soit : l'hiver, le printemps, l'été et l'automne. Ces saisons sont différenciées principalement par la température, le temps d’ensoleillement et l'alternance de pluie et de neige selon ces saisons.

L'Alberta a d'ailleurs connu des records de températures impressionnant au cours de son histoire. En effet, le record de chaleur enregistré dans la province est de 43,3 °C à Bassano Dams le 21 juillet 1931. Complètement à l'opposé, 

L'Alberta est le plus grand producteur canadien de pétrole (l'Alberta possède la deuxième réserve mondiale de pétrole brut, derrière l'Arabie saoudite), de gaz naturel et de charbon. À Red Deer et à Edmonton, un grand nombre de compagnies fabriquent des produits de polyéthylène et de vinyle pour des clients du monde entier. Les raffineries de pétrole fournissent les matières premières pour une grande industrie pétrochimique à l'est d'Edmonton. Mais l'exportation pose problème car il n'y a pas d'accès du pétrole de l'Alberta à un port de mer. La compagnie canadienne Enbridge projette d'investir près de quatre milliards d'euros pour construire un oléoduc sur kilomètres de l'Alberta à Kitimat. Le double pipeline convoierait du pétrole vers l'ouest et du condensat - liquide qui sert à diluer l'épais pétrole brut - vers l'est. Les pétroliers géants, chargés de condensat ou d'au maximum 2,15 millions de barils de brut, devraient alors naviguer à travers un chapelet d'îles. Le projet de l'oléoduc est si important que le gouvernement fédéral a mis en place une commission mixte d'évaluation chargée de superviser le bilan environnemental et les modalités d'autorisation pendant deux ans. Elle devrait s'achever à la fin 2012. 
Les sables bitumineux de l'Athabasca ont des réserves de pétrole estimées à 2 trillions de barils. Avec l'amélioration des méthodes d'extraction, le bitume et l'huile synthétique sont produits à des coûts s'approchant de ceux des méthodes d'extractions pétrolières conventionnelles ; cette technique fut d'ailleurs développée en Alberta. Fort McMurray, une des villes les plus jeunes du Canada, a grandi entièrement en raison des grandes entreprises pétrolières multinationales. La région est aussi l'une des plus polluées du pays (par capital), avec un taux de cancer élevé, des pluies acides et une pollution des eaux souterraines et superficielles. L'extraction du pétrole est également coûteuse en énergie et nécessite de grands volumes d'eau. L'activité économique fait reculer la forêt et affecte la faune de cette partie de l'Alberta. Le boom pétrolier de l'Alberta a attiré des milliers de personnes en quête d'embauche immédiate et de salaire élevé. Mais, été comme hiver, les conditions de travail sont dures. Chaque soir, les ouvriers doivent dîner et dormir sur place, dans des préfabriqués qu'ils ne quittent qu'en fin de semaine.
Bien qu'Edmonton soit considéré comme le centre de raffinage de la province, la plupart des compagnies pétrolières ont leur siège social à Calgary.

Le bœuf et l'agriculture tiennent également des positions significatives dans l'économie de la province. Plus de 5 millions de têtes de bétail passent par la province à un moment ou un autre, et le bœuf d'Alberta a une renommée mondiale.

Avec l'appui du gouvernement provincial, plusieurs industries de pointe ont trouvé naissance en Alberta, notamment l'invention et le perfectionnement des systèmes d'affichage à cristaux liquides. D'une économie croissante, Alberta a plusieurs institutions financières gérant plusieurs fonds civils et privés.

Grâce à ses sources thermales très répandues, l'Alberta pourrait profiter de cette chance et utiliser la géothermie pour produire de l'électricité. Pour une utilisation à plus petite échelle comme pour les domiciles, il est également possible de profiter de l'occasion et utiliser la géothermie afin de climatiser, chauffer, chauffer l'eau chaude et purifier l'air de la maison.

L'Alberta est une démocratie parlementaire avec une Assemblée législative de 87 députés. Le lieutenant-gouverneur représente la reine et le cabinet est dirigée par le Premier ministre. La ville d'Edmonton est le siège du gouvernement albertain. Les revenus de la province proviennent principalement des ventes de pétrole, de gaz naturel, de bœuf, de bois et de blé. Ils incluent également des concessions du gouvernement fédéral, principalement pour les projets d'infrastructures. Les villes et les villages albertains ont leurs propres gouvernements municipaux qui travaillent en coopération avec le gouvernement provincial.

La politique de l'Alberta est bien plus conservatrice que celle des autres provinces canadiennes. L'Alberta est aussi la province la moins favorable envers l'interventionnisme économique. Par conséquent, elle est la province avec le niveau de taxation le plus bas au Canada. L'Alberta a traditionnellement eu trois partis politiques, les progressistes-conservateurs, les libéraux et le Nouveau Parti démocratique. Un quatrième parti, fortement conservateur, le parti du crédit social, était puissant pendant plusieurs décennies, mais a disparu de la carte politique quand les progressistes-conservateurs sont arrivés au pouvoir dans les années 1970. Pourtant, un autre parti politique est apparu lors de la dernière élection en Alberta, l'Alliance albertaine, par la suite devenu le Parti Wildrose, qui a fait élire 18 députés à la dernière élection avec 34 % des voix. Bien que les sondages donnaient ce parti gagnant aux élections, il n'a pas réussi à déloger les progressistes-conservateurs du pouvoir depuis 1973 en Alberta.

Le premier Premier Ministre de l'Alberta a été Alexander Cameron Rutherford, un libéral, de 1905 - 1910.

Actuellement et depuis le 24 mai 2015, la Première Ministre de l'Alberta est la néo-démocrate Rachel Notley.

Le premier lieutenant-gouverneur de l'Alberta est George H.V. Bulyea de 1905-1915.

Aujourd'hui et depuis 2015, Madame Lois E. Mitchell est lieutenant-gouverneure de la province.

Officiellement ouverte en 1912, l'Assemblée législative de l'Alberta est composée de 87 membres. 

C'est l'endroit où les membres se rencontrent pour discuter et débattre des politiques publiques en Alberta. 

Elle est ouverte 362 jours par année aux visiteurs.

L'Alberta est une province anglophone mais il existe une minorité francophone comprenant 4 % de la population gérée par l'ACFA. La province comprend 4 conseils scolaires dirigeant 37 écoles francophones. Les conseils scolaires sont le Conseil Franco-Sud (diviser dans 2 différents conseils. La première est la CSCFSA [Conseil Scolaire Catholique et Francophone du Sud de l'Alberta] et la CSSA [Conseil Scolaire du Sud de l'Alberta]), le Conseil Scolaire Centre-Est (CSCE), le Conseil Scolaire du Nord-Ouest (CSNO) et le Conseil Scolaire du Centre-Nord (CSCN).

Au Canada, l'éducation est de juridiction provinciale et l'Alberta a établi son propre système éducatif. Depuis 1905, le gouvernement albertain dirige les conseils scolaires laïcs et religieux, les universités, les collèges, les écoles techniques, les "charter schools" (écoles alternatives), les écoles privées et les écoles à la maison.

Les premières écoles albertaines furent des écoles de paroisses, ce qui signifie qu'elles étaient dirigées par le clergé, aussi bien catholique que protestant. Les élèves devaient payer un dû (dîme) afin d'assister aux cours.

Les premières écoles gratuites (donc publiques) ont été établies à Edmonton en 1881. À cette époque, aucune loi ne régissait ces établissements : les habitants élisaient des représentants qui dirigeaient et administraient l'école. Une taxe informelle basée sur la solidarité locale permettait à l'école de s'autosuffire.

Entre 1883 et 1905, une éducation publique se développe en Alberta, lancée dans les communautés par la population locale. Une école à vocation religieuse pouvait être créée subséquemment, sous certaines conditions. Ce système qui assurait l'éducation publique universelle et l'éducation religieuse conditionnelle a été officialisé en 1905 par la loi qui a créé l'Alberta ("Alberta Act"), par le gouvernement de Sir Wilfrid Laurier. 

On dénombre 42 regroupements d'écoles publiques ainsi que 17 regroupements scolaires privés. Seize de ces regroupements privés sont de confession catholique romaine et un (St-Albert) est de confession protestante. De plus, un district scolaire indépendant, Glen Avon, existe dans la région scolaire de St-Paul. La ville de Lloydminster chevauche la frontière entre l'Alberta et la Saskatchewan et tant les écoles publiques que les écoles privées suivent le système scolaire de la Saskatchewan.

En 1982, la Charte canadienne des droits et libertés amena l'émergence d'une éducation francophone en Alberta. Il existe cinq regroupements francophones, publics et privés, qui couvrent la province entière, mais ils n'ont l'obligation de créer une école francophone que lorsque la demande est suffisamment élevée.

Avant 1994, les regroupements scolaires albertains avaient le pouvoir de lever une taxe scolaire (foncière). En 1994, ce droit fut supprimé pour les regroupements publics, mais pas pour les regroupements privés. Le gouvernement provincial décide le taux de taxation, les autorités locales collectent la taxe puis la renvoient au gouvernement provincial. Le gouvernement redistribue cette taxe à travers la province aux regroupements publics, privés et francophones.

En plus de la taxe foncière, le gouvernement accorde des enveloppes à partir du "General Revenue Fund" afin de soutenir le projet éducatif "K - 12" qui vise à donner une scolarité de 12 ans à tous les jeunes Albertains.

Les "charter schools" ne demandent pas de frais de scolarité et reçoivent la même somme gouvernementale par élève qu'une école publique. Les écoles privées et les écoles à la maison reçoivent un certain financement, mais les parents défrayent une bonne partie des coûts.

Depuis 1994, tous les regroupements (publiques, privés et francophones) peuvent également permettre aux écoles de demander un montant pour les livres, le matériel spécialisé, les programmes et services particuliers, etc. Ces coûts vont de 20 $/an/élève à 750 $/an/élève.

À titre indicatif, en 2007, 29,7 % des revenus des commissions scolaires albertaines venaient des impôts locaux, 60 % du Fonds consolidé du gouvernement et 10,3 % d'autres sources.

L'Alberta compte environ élèves.

Tous les élèves albertains suivent le "Program of Studies" (programme d'études) et le curriculum approuvé par le ministère de l'Éducation. Tous les enseignants sont certifiés par le ministère, administrent aux élèves des tests d'aptitudes provinciaux et ont le pouvoir d'accorder les diplômes d'études secondaires.

La plus ancienne et la plus grande université albertaine est l'Université de l'Alberta, située à Edmonton. L'Université de Calgary, autrefois affiliée à l'Université de l'Alberta, est devenue autonome en 1966 et est maintenant la en importance dans la province. La Athabasca University est spécialisée dans la formation à distance. La quatrième université de la province est l'Université de Lethbridge.

Il existe 15 collèges et deux institutions techniques (Northern Alberta Institute of Technology et Southern Alberta Institute of Technology) financées par l'état.

Dans les dernières années, l'augmentation des frais de scolarité post-secondaire a engendré la controverse. En 2005, le premier ministre Ralph Klein a promis de geler les frais de scolarité et de chercher des solutions afin de réduire les coûts en éducation. Jusqu'à ce jour, aucun projet de loi n'a été proposé à cet effet.

On compte 786 360 de catholiques en Alberta pour 1 145 460 de protestants.

Les anglicans qui sont des protestants royalistes sont représentés par les diocèses "d'Athabasca, de Calgary, Rootenay et d'Edmonton" au sein de l'Église anglicane du Canada. 

Les catholiques pour leurs part sont représentés par "l'Assemblée des évêques catholiques de l'Ouest" au sein de l'Église catholique du Canada.





</doc>
<doc id="263" url="https://fr.wikipedia.org/wiki?curid=263" title="Asthme">
Asthme

L'asthme du grec ancien , "ásthma", via le latin "asthma" signifiant « respiration difficile », est une maladie du système respiratoire touchant les voies aériennes inférieures et notamment les bronches, définie comme étant une gêne respiratoire à l'inspiration. La maladie s'explique par quatre mécanismes caractéristiques :


Homère, dans l'Iliade, au chant XV, employa pour la première fois le mot (ἅσθμα, ατος (το) : courte respiration), pour désigner la dont souffrit Hector étendu dans la plaine.

Le mot ἅσθμα fut repris au sens de et d', par Eschyle, poète d'Eleusis dans , et par Platon dans . En terme médicale de ou d', il fut employé la première fois par Hippocrate dans les .

Le médecin et philosophe Juif Moïse Maïmonide fait également mention de cette maladie au .

Pour les Anglo-Saxons, le de John Floyer, publié en 1698 et qui se base, en partie, sur sa propre expérience, serait le premier manuel médical traitant intégralement de l'asthme.

Au début des années 1960, on a mis en cause les allergènes, acariens et autres. La chasse aux allergènes résultante (à l'aide de produits divers, possiblement allergènes pour la plupart) a aidé certains patients.

L'évolution de la corrélation entre asthme et allergie n'est pas toujours symétrique : cette corrélation a été démontrée en Grande-Bretagne, mais ni en Allemagne, ni en Italie, où la fréquence des allergies a augmenté mais pas celle de l'asthme. Ce qui tend à prouver que l'asthme aurait des causes intrinsèques comme une réaction auto-immune ou une prédisposition génétique, bien qu'il n'y ait à ce jour aucune étude le prouvant formellement.

Cependant, certaines études tendent à prouver que l'asthme est aussi fortement développé dans les endroits soumis à la pollution atmosphérique.
Au début des années 1990, il fut démontré que les particules de diesel dans l'air endommageaient le cœur et les poumons. Les fabricants ont donc changé les taux de compression des moteurs diesel et réduisant la taille des particules volatiles qui endommagent désormais les poumons.
Une étude néerlandaise précise la relation entre l'exposition aux polluants atmosphériques et l'augmentation du risque d'avoir un asthme chez les enfants.

Des études scientifiques montrent que la pollution atmosphérique est une des causes de l'asthme , notamment les COV (Composés Organiques Volatils) et les NOx (oxydes d'azote : monoxyde et dioxyde d'azote) présents aussi bien dans l'air que dans certains produits industriels de nettoyage ou de réparation.

Pour se prémunir de la pollution domestique, cause d'asthme et d'allergies, les médecins recommandent de ventiler régulièrement son appartement, ou d'utiliser un appareil pour purifier l'air des microparticules volatiles ou des résidus de produits polluants de nettoyage ou de rénovation, tels que le purificateur d'air. L'utilisation d'un échangeur d'air contribue aussi à réduire les maladies respiratoires chez les enfants.

Par ailleurs, l'UFC Que Choisir a récemment soulevé le problème de la pollution domestique et des impacts de la pollution dans les espaces intérieurs. S'ajoutent aux NOx, à la poussière et aux COV présents dans l'air, des produits industriels polluants utilisés pour la rénovation des moquettes encollées, polluants dont l'impact pour la santé est dénoncé par les associations de consommateurs, au même titre que les peintures industrielles, les colles et les produits nettoyants comme étant directement responsables de l'asthme chez les enfants et les personnes fragiles.

Stephen Holgate considère que la fumée de tabac est de loin la principale cause identifiée de l'asthme. Lui et son équipe ont prouvé que la fumée de tabac modifie les gènes de cellules pulmonaires de souris, et pourrait causer des changements génétiques dans les poumons des fœtus, les rendant ainsi vulnérables à l'asthme. La même chose pourrait être vraie pour les régimes alimentaires malsains et même pour le paracétamol (un antioxydant puissant qui a récemment été lié à l'asthme).

Le tabagisme aussi bien actif que passif peut être en cause.


Ces dernières années des chercheurs ont démontré que les voies respiratoires des patients souffrant d'asthme chronique sont altérées de façon permanente par la maladie ou — possiblement — se développent différemment dans l'utérus. Stephen Holgate, un des chercheurs sur l'asthme en Grande-Bretagne, a publié dans le journal "Nature" les résultats d'une recherche de cinq ans mettant en cause le gène "ADAM33". Ceci est le premier gène découvert pour l'asthme, et contrôlerait la façon dont le muscle se développe dans les voies respiratoires. Holgate pense également que les facteurs environnementaux pourraient influencer les choses bien plus tôt qu'on ne le pensait dans le développement de la maladie : ils pourraient influencer l'expression des gènes dans le développement du fœtus, contribuant ainsi à une modification génétique favorisant la maladie. Holgate et son équipe ont déjà démontré la modification du gène "ADAM33" par la fumée de tabac dans des cultures de tissu pulmonaire de souris. En octobre 2005, ils ont aussi publié un compte-rendu démontrant que, lorsque les tissus des voies respiratoires des asthmatiques sont inflammés, ils produisent une molécule appelée TNF alpha, ou "tumor necrosis factor alpha". Or le gène "ADAM33", impliqué dans l'asthme, se comporte de façon très similaire au gène "ADAM17" responsable de la production de TNF alpha. On trouve aussi cette molécule TNF alpha dans les tissus enflammés des patients souffrant d'autres maladies inflammatoires chroniques, comme la polyarthrite rhumatoïde ou la maladie de Crohn. Après six années de lutte auprès des industries pharmaceutiques pour réaliser ces essais, Holgate a réussi à les convaincre de réaliser une expérience qui va à l'encontre d'une tendance générale à prendre l'asthme pour une maladie allergénique. En octobre 2004, lui et son équipe ont injecté de l'etanercept à 15 volontaires, un récepteur soluble pour TNF alpha qui intercepte cette molécule et l'empêche de se lier avec les cellules des tissus et d'irriter les bronches. Les résultats sont pour l'instant très satisfaisants, avec des améliorations nettes et persistantes chez chacun des 15 volontaires. À cette date de novembre 2005, trois de ces patients n'ont utilisé aucun stéroïde depuis les 12 semaines d'injections hebdomadaires, un peu plus de douze mois auparavant.

D'autres gènes sont corrélés à la maladie asthmatique. Une mutation du gène codant la protéine YKL-40 (une chitinase) augmente ainsi sensiblement le risque de développer un asthme.

Selon l'OMS, près de 300 millions de personnes souffrent d'asthme dans le monde.
En France, une enquête nationale réalisée par la Caisse nationale d'assurance maladie (Cnam) pendant l'année 2007 sur tous les patients de 5 à 44 ans traités pour un asthme, retrouvait que personnes bénéficiaient d'un traitement régulier, avec trois prescriptions ou plus de médicaments antiasthmatiques. Sur ces patients, 27 %, la plupart, dans la tranche des 20-29 ans, avaient un asthme insuffisamment contrôlé, nécessitant au moins quatre fois par an de recourir à un médicament destiné uniquement à traiter la crise.
En France, il y a environ hospitalisations par an pour une crise d'asthme, et décès. Une cause allergique est retrouvée chez 70 à 80 % des adultes asthmatiques et chez 95 % des enfants atteints. Le coût de cette maladie est important pour la société car elle est responsable de journées d'hospitalisation et de 7 millions de journées d'arrêt de travail par an.

On note une expansion rapide de la maladie depuis 40 ans dans les pays développés. Il est surtout présent dans les pays connaissant une forte industrialisation, ou une industrialisation en développement rapide, il est par exemple peu connu en Afrique subsaharienne. En 1999, des chercheurs de l'International Study of Asthma and Allergies in Childhood ont quantifié quelques données concernant l'asthme, en mesurant le pourcentage d'asthme et d'allergies parmi les 13-14 ans dans différents pays :

La surveillance de l'évolution de l'incidence en France est effectuée par le réseau Sentinelles de l'INSERM.

À noter que des études récentes corrèlent l'augmentation de l'asthme avec le Bisphénol A
, interdit en France dans la fabrication des biberons depuis 2010.

Dans le monde, en 2016, on compte environ 334 millions de personnes souffrant d'asthme. Ce nombre, en constante augmentation, est souvent sous-estimé à cause de mauvais diagnostics.

Peu de temps après la réunification allemande en 1989, une étude est menée conjointement à Munich (RFA) et à Leipzig (RDA) sur la prévalence de l'asthme de plusieurs milliers d'enfants d'âge préscolaire. Il s'agissait de montrer l'effet de la pollution environnementale sur la prévalence de maladies respiratoires d'origine allergique (asthme et rhinite allergique), en particulier l'effet du dioxyde de soufre (). Les résultats obtenus sont les suivants :

Le résultat concernant la prévalence des bronchites était attendu, mais celui de l'asthme ne l'était pas. L'étude conclut, pour les troubles respiratoires à caractère allergique, que les déterminants sont surtout des . Plus tard, cette observation sera confirmée sur d'autres comparaisons de populations de cohortes génétiques équivalentes mais aux modes de vie éloignés, entre la Pologne, l'Estonie et la Suède. Ces écarts se comblent après la réunification et l'égalisation progressive des modes de vie entre la RFA et la RDA.

Les bronches ont notamment pour rôle de protéger les poumons des agents étrangers ou des agressions extérieures, notamment par la restriction du diamètre bronchique. L'asthme se manifeste par une réaction disproportionnée des bronches par rapport au milieu. Ainsi, les bronches d'un asthmatique sont inflammatoires et voient leur diamètre réduit. Le mucus produit en réaction à l'inflammation vient réduire encore le diamètre des bronches, rendant l'expiration difficile ; on parle d'obstruction bronchique expiratoire.
Les causes de l'inflammation et surtout ses conditions de manifestation permettent d'établir trois grands types d'asthmes: asthme chronique, asthme allergique et asthme d'effort.

Bien que chaque malade corresponde plus à l'un ou l'autre des profils d'asthmatiques, il ne s'agit que d'une manifestation générale de la maladie, il n'est pas rare qu'un asthmatique chronique connaisse des crises d'asthme allergique ou de l'asthme d'effort et inversement.

Dans tous les types d'asthme, on retrouve les symptômes suivants :


Seules l'intensité, la durée et les causes de ces symptômes varient d'un type à l'autre.

L'asthme est sensible à plusieurs facteurs : 

Il s'agit d'une hyperactivité chronique des bronches peu soumise aux agents extérieurs. L'inflammation est chronique, souvent d'installation lente et progressive.
Généralement présent depuis l'enfance, il peut se manifester dans les premières années de l'enfance par des crises d'asthme répétées ou des bronchites sifflantes chroniques. Dans ce cas, il s'agit d'une aggravation du syndrome asthmatique, qui prend un caractère chronique (alors qu'il existait jusqu'à cette aggravation un facteur déclencheur).

Du fait de l'installation lente et progressive de l'inflammation, celle-ci peut passer inaperçue, notamment parce que le malade a le temps de s'habituer à la gêne respiratoire et perd progressivement la notion de « normalité » respiratoire, jusqu'à ce que la gêne devienne trop envahissante dans la vie du malade.

Non traitée, cette forme d'asthme évolue généralement en insuffisance respiratoire.

Bien que les causes réelles restent à ce jour sujet à discussion, une hypothèse prédominante veut que cette forme d'asthme soit causée par une réaction auto-immune : c'est-à-dire que le système immunitaire du malade s'attaquerait à ses propres bronches, entretenant ainsi dans le temps l'inflammation.

En général caractérisé par la survenue d'une ou de plusieurs crises causées par une réaction excessive des bronches du malade à un agent extérieur (le plus souvent allergisant). Il s'agit de la forme d'asthme la plus grave sur le court terme, le degré de réaction bronchique pouvant être particulièrement important et parfois mortel.

La crise d'asthme allergique se manifeste par une obstruction soudaine et de progression rapide des voies bronchiques, le malade en crise s'étouffant par suffocation (l'impossibilité d'expirer correctement empêchant une nouvelle inspiration) et manque d'oxygène dans le sang (l'impossibilité d'expirer empêchant l'apport d'oxygène dû à l'inspiration, et saturant l'organisme en dioxyde de carbone).

Cette forme d'asthme peut évoluer en asthme chronique, notamment si l'exposition à l'allergène est constante et de longue durée.
Les facteurs déclenchants de cette forme commune d'asthme sont en général les allergènes inhalés comme les acariens, les poils d'animaux, les spores de moisissures et les pollens. L'asthmatique allergique, sans doute sensible à une combinaison de plusieurs de ces allergènes, présente également une rhinite allergique (rhume des foins) et/ou une conjonctivite allergique.

La crise d'asthme est toujours une urgence médicale engageant le pronostic vital et nécessite une prise en charge spécifique.

Il s'agit d'un asthme se manifestant par crise survenant pendant un effort physique. La cause est définie comme un effort traumatisant pour les bronches. C'est-à-dire un effort sollicitant particulièrement les bronches et/ou effectué dans des conditions rendant plus difficile le travail des bronches.
L'effort est typiquement un cardio-training (sollicitant le système cardiaque, en particulier donc la respiration). Les facteurs environnementaux aggravant cette forme d'asthme sont le froid, le vent et un milieu peu ventilé. Le froid et le vent favorisent l'inflammation bronchique, accentuant ainsi le risque de crise.

Ce type d'asthme est parfois isolé ou parfois associé à un asthme chronique ou allergique, devenant ainsi une complication du type d'asthme d'origine. Des crises d'asthme pourraient être facilitées par un stress intense. En effet, le stress a pour effet d'accélérer le rythme cardiaque et de développer un syndrome d'hyperventilation, facilitant ou aggravant l'asthme.

Les asthmes par crise sont également classés de la façon suivante :

L’asthme intermittent qui est défini arbitrairement par la survenue, au maximum, de deux crises brèves par semaine, et/ou deux épisodes nocturnes par mois, et un DEP (débit expiratoire de pointe ou Peak Flow) supérieur à 80 %.

L’asthme persistant qui est défini lorsqu'il existe plus de deux épisodes par semaine, et/ou plus de deux épisodes nocturnes par mois, avec retentissement sur les activités courantes. Il peut être léger, modéré ou sévère.

L’asthme aigu grave qui met en jeu le pronostic vital. Il nécessite une prise en charge urgente en milieu hospitalier (par exemple, en France environ personnes par an meurent d'asthme, soit 3,2 cas pour habitants). Cliniquement, il existe au moins un des signes suivants : 

Il convient d'en dissocier l’asthme du nourrisson, qui se définit par l'apparition d'au moins trois épisodes de sibilance avant l'âge de trois ans. Un asthme du nourrisson disparaît le plus souvent avant l'âge de cinq ans.

Chez les jeunes enfants (moins de cinq ans), l'exploration fonctionnelle respiratoire n'est pas possible ; le diagnostic repose donc exclusivement sur la clinique et l'évolution des symptômes :

→ Attention, l’existence de symptômes particuliers doit faire rechercher d’autres diagnostics que celui d’asthme (mucoviscidose, corps étrangers bronchiques, séquelles de viroses respiratoire, broncho-dysplasie, etc.) 
Le salbutamol en inhalateur ( Ventoline) est le médicament le plus utilisé pour traiter de l'asthme, en France. En effet, plus de 65 % des personnes atteintes de l'asthme utilisent ce bronchodilatateur. Pour traiter l'asthme chronique, l'ajout du corticostéroïde est nécessaire afin de contrer l'inflammation des bronches.

Le traitement par bêta-2 mimétiques à longue durée d’action peut être utilisé dans le traitement chronique de l’asthme. Ces médicaments sont pris tous les jours et toujours associés aux médicaments anti-inflammatoires, les corticostéroïdes inhalés (par exemple, Flixotide qui est du propionate de fluticasone).
Depuis quelques années sont apparus les antagonistes des récepteurs des leucotriènes (par exemple, montélukast, zafirlukast), permettant un traitement de fond de l’asthme.

Le traitement de première intention est un bêta-2 mimétique de courte durée d'action (par exemple, la Ventoline qui est du salbutamol). Ce bronchodilatateur permet de soulager au quotidien le malade et aurait un impact sur la balance bénéfices / risques du traitement. Si la consommation de bêta-2 mimétique dépasse un aérosol doseur par an (soit 2 utilisations par semaine), il convient d’entamer un traitement de fond.

Il est recommandé de surveiller les allergies et éventuellement les traiter par antihistaminique. Les personnes souffrant d'asthme d'origine allergique doivent éviter le contact avec les allergènes les plus fréquents : poils de chat, poussières, pollens

Utilisés par voie inhalée sont rares. Il peut s’agir de tremblements avec sensation d’excitation, de crampes musculaires ou de palpitations du cœur lorsque de grandes quantités de médicament sont inhalées. À fortes doses, ils peuvent entraîner des complications cardiaques avec des troubles du rythme chez des personnes âgées ou ayant une maladie cardiovasculaire. La balance Bénéfices / Risques des traitements est donc favorable.

La mesure thérapeutique dans la vie de tous les jours est l'administration d'un broncho-dilatateur le salbutamol ou la terbutaline, provoquant une « détente » des muscles bronchiques et la réouverture des bronches (bronchodilatation).

L'administration se fait essentiellement par inhalation : aérosols doseurs ou poudres. La technique d'utilisation des aérosols doseurs doit impérativement être connue par le patient pour une efficacité maximale.

L'utilisation de dispositifs spécifiques, « chambres d'inhalation », facilite grandement l'utilisation des aérosols doseurs, en particulier chez l'enfant mais également chez l'adulte. Toute crise qui ne cesse pas rapidement face à la médication doit être traitée comme une urgence médicale.

L’asthme aigu grave (AAG) est une urgence vitale. Du point de vue du malade, toute crise inhabituelle doit être considérée comme un possible AAG. Une crise est considérée comme grave si l'inhalation d'un bronchodilatateur n'a pas l'effet escompté et ne dilate pas les bronches. On note alors des difficultés à inspirer et à expirer alors qu'une simple crise d'asthme est caractérisée par la diminution du débit expiratoire (VEMS).

Un transfert médicalisé et une hospitalisation en urgence sont indispensables. Le traitement de première intention repose sur une oxygénothérapie à fort débit (6 à 8 litres/min), associée à la prise de bêta 2-stimulant d'action brève inhalé, à posologie élevée, et l'administration de corticoïde par voie orale ou intraveineuse14. En effet, le principal risque est ici une asphyxie.
Par ailleurs, comme dans plusieurs cas de ventilation difficile, l'hypercapnie permissive est une approche préconisée par plusieurs auteurs.

L’asthme étant une maladie chronique, un accompagnement personnalisé sur le long terme est nécessaire afin de mieux gérer sa maladie. Une éducation thérapeutique du patient peut permettre l’amélioration  de la qualité de sa prise en charge. L’éducation thérapeutique est une démarche qui comprend des activités éducatives d’information et d’apprentissage proposées et dispensées par des professionnels de santé : médecins généralistes et spécialistes, infirmières, kinésithérapeutes ou pharmaciens. Ces activités permettent d’acquérir les compétences utiles pour mieux comprendre et gérer l’asthme au quotidien et savoir comment réagir face à des situations difficiles.

Celle-ci est basée sur différents domaines de compétences que le patient doit acquérir :

Adaptée dans l'asthme par crise, elle repose sur les mesures suivantes avant de prévenir la survenue mais aussi l'intensité des crises : 

D'après plusieurs essais cliniques, une augmentation de la consommation de fruits, de légumes et de céréales permet d'arrêter la progression de l'asthme. Dans le cas d'un passage à un régime végétalien, 71 % des sujets ont rapporté une diminution significative des symptômes après 4 mois, et 92 % après un an, avec un arrêt ou une diminution drastique de leur traitement dans un essai clinique sans groupe de contrôle.

Les compléments d'antioxydants (vitamine A, C, E), de vitamine B8 ou d'acides gras polyinsaturés (oméga 3 et 6) n'ont en revanche aucun effet prouvé.

Les thérapies alternatives sont particulièrement utilisées dans le domaine de l'asthme : selon plusieurs études, environ 50 % des patients utilisent une forme de thérapie non conventionnelle. Toutefois, la plupart de ces traitements alternatifs n'ont pas démontré leur efficacité.

Par exemple, les données ne permettent pas de recommander l'utilisation de compléments en vitamine C dans le traitement de l'asthme. Plusieurs sources, dont les "National Institutes of Health" (NIH), déconseillent l'usage de l'acupuncture, qui ne semble pas apporter de bénéfices thérapeutiques ; il en va de même pour l'homéopathie. Les purificateurs d'air par ionisation n'améliorent pas non plus les symptômes asthmatiques. Enfin, les techniques de manipulation, telles que l'ostéopathie, la chiropraxie, ou la méthode de manipulation thoracique dite "méthode Gesret", sont également déconseillées puisqu'elles ne procurent aucun bénéfice objectif.

Voir l'équivalent asthme, pathologie associée s'exprimant par une hyperréactivité bronchique, provoquant toux spasmodiques et nocturnes, sans provoquer de diminution cliniquement décelable du volume d'air expiré.

Une crise d'asthme aiguë se caractérise par une exacerbation aiguë de la dyspnée, de la toux et du sifflement respiratoire, et s'accompagne d'une diminution (passagère) de la fonction pulmonaire. L'évaluation de la gravité d'une crise d'asthme peut se faire par l'évaluation de la fonction pulmonaire (débit expiratoire de pointe ou DEP, volume expiratoire maximal par seconde ou VEMS). L'évaluation clinique de la gravité de la crise est encore plus importante que l'évaluation de la fonction pulmonaire, entre autres parce que les résultats des mesures de la fonction pulmonaire pendant une crise d'asthme aiguë ne sont pas toujours fiables. En fonction de la gravité de la crise, il convient de décider si le patient peut être traité en première intention à domicile (avec hospitalisation en l'absence d'amélioration) ou s'il doit être hospitalisé immédiatement.

Les critères sur base desquels une crise d'asthme grave doit être suspectée et une hospitalisation immédiate envisagée sont les suivants :

Les signaux d'alarme suivants indiquent un épuisement et la nécessité d'une admission immédiate dans un service d'urgence :

Chez les patients suivants, qui ont un risque élevé de décès lié à l'asthme, une attention particulière s'impose, et une hospitalisation plus rapide est de rigueur :

La plupart des asthmatiques, comme d’autres personnes fragilisées, peuvent être victimes de complications exacerbées et graves en cas de grippe.
Le vaccin anti-grippe saisonnière leur est recommandé (à partir de 6 mois, pris en charge à 100 % pour les asthmatiques) en France par le Conseil supérieur d'hygiène publique de France, et par des organismes équivalents dans la plupart des autres pays industrialisés (Union européenne, États-Unis), Canada. La vaccination diminue chez l'asthmatique le risque d’hospitalisation et de besoin accru de médicaments.
Mais sur les trois millions de Français victimes d'asthme (surtout des enfants et adolescents), seuls 32 % se sont fait vacciner l’hiver 2006-2007 ; et moins d’un quart des moins de 65 ans ont été vaccinés (et 14 % seulement des moins de 15 ans), contre 77 % chez ceux de 65 ans et plus. L'allergie à l'œuf (rare et détectable par test cutané) est la seule contre-indication, si le sujet est indemne d'infection évolutive, de fièvre et/ou d'instabilité de l'asthme. Les asthmatiques vaccinés n’ont pas d’effet secondaire significatif ou particulier dans les quinze jours qui suivent (aucune modification de débit respiratoire, ni besoin accru de bronchodilateurs, ni augmentation des consultations médicales ou de consommation de corticoïdes).

Pour aider certains patients dont l'organisme répond mal aux traitements actuels ou qui souffrent d'effets secondaires la recherche de médicaments se poursuit 

Une piste explorée est un inhibiteur sélectif de la protéine Gq dit qui a donné de bons résultats en laboratoire chez la souris, le porc et chez l'homme ex-vivo, sans effets aigus sur la pression sanguine ni sur le rythme cardiaque ; il peut être délivré par inhalation et pourrait aussi aider à traiter d'autres maladies obstructives des voies respiratoires. Il doit encore être testé chez l'Homme. 




</doc>
<doc id="265" url="https://fr.wikipedia.org/wiki?curid=265" title="Arts visuels">
Arts visuels

On appelle arts visuels les arts qui produisent des objets perçus essentiellement par l'œil. La notion englobe les arts plastiques traditionnels (les anciens beaux-arts dégagés de la notion restrictive d'esthétique, comme du « beau »), auxquels s'ajoutent les techniques nouvelles : la photographie, le cinéma, l'art vidéo et l'art numérique, mais aussi les arts appliqués et les arts décoratifs (art textile, design, marqueterie...) et l'architecture.




</doc>
<doc id="266" url="https://fr.wikipedia.org/wiki?curid=266" title="Arvo Pärt">
Arvo Pärt

Arvo Pärt (), né le à Paide, en Estonie, est un compositeur estonien de musique contemporaine vivant à Tallinn. Il est souvent associé au mouvement de musique minimaliste qui s'est formé à partir des années 1960.

Arvo Pärt naît à Paide, ville située à environ au sud-est de Tallinn. Ses parents divorcent alors qu'il n'a que trois ans et sa mère l'emmène vivre chez son nouveau compagnon à Rakvere, au nord-est de l'Estonie. Là, entre sept et huit ans, il suit des cours de musique après l'école et apprend les bases du piano et de la théorie musicale. À la maison, il ne dispose que d'un vieux piano à queue dont seuls les registres extrêmes peuvent être joués convenablement ; cela le pousse à l'expérimentation et à inventer ses propres œuvres.

Adolescent, il écoute toutes sortes de musiques à la radio mais il est plus particulièrement intéressé par la musique symphonique. Il écoute notamment les programmes de la Radio finlandaise qui pouvaient être captés assez clairement dans le nord de l'Estonie. On raconte même qu'il tournait en rond sur la place de la ville alors que les concerts symphoniques y étaient diffusés via des haut-parleurs , à vélo, pour ne pas rester statique et ne pas éveiller les soupçons 

Bien que le piano soit son instrument de prédilection et qu'il en joue parfois en concert comme accompagnateur, il pratique aussi le hautbois dans l'orchestre de son école, les percussions dans un groupe de danse et chante dans le chœur de son école. Progressivement, il passe des improvisations au clavier à des compositions plus formelles qu'il commence à noter vers quatorze ou quinze ans. Vers ses 17 ans, il présente "Meloodia", pièce pour piano qu'il compose pour un concours de jeunes artistes. Sa pièce est remarquée mais, sans doute à cause d'un manque évident de racines ou d'influences estoniennes, il ne remporte aucun prix. Pärt se rappelle qu'elle était dans le style de Rachmaninov mais qu'elle n'avait rien de personnel.

Arvo Pärt entre en 1954 à l'École secondaire de musique de Tallinn et compte parmi ses professeurs Harri Otsa. Il y étudie la théorie musicale, la composition, le piano, la littérature musicale, l'analyse et la musique populaire. Cet apprentissage est interrompu après quelques mois seulement par le service militaire obligatoire au cours duquel il joue de la caisse claire et du hautbois dans la fanfare. Ces deux années sont vécues comme une souffrance et il contracte une maladie rénale qui compromettra sa santé pendant plus de dix ans. Il retourne à l'École secondaire de musique de Tallinn pour l'année académique 1956-57 avec Veljo Tormis pour professeur et assimile facilement toute idée nouvelle (dont le dodécaphonisme), particulièrement le peu de musique occidentale qu'il peut entendre. Il fait déjà preuve d'un talent évident et naturel pour la composition ; un de ses compagnons d'étude, Ave Hirvesoo, déclare même qu'il « semblait secouer sa manche et des notes en tombaient ».

Il entre au conservatoire de Tallinn à l'automne 1957 où il étudie avec Heino Eller. Les programmes obligatoires comportent également l'économie politique, l'histoire du Parti communiste et la « science de l'athéisme ». Parallèlement, il trouve un emploi d'ingénieur du son à la radio estonienne, poste qu'il occupe de 1958 à 1967. En 1962, l'une de ses compositions écrite pour chœur d'enfants et orchestre, "Notre jardin" (1959), le fait connaître dans toute l'Union soviétique et lui permet de remporter le Premier Prix des jeunes compositeurs de l'URSS. À cette époque il est quelque temps directeur musical du "Théâtre des Pionniers" de Tallinn et compose de la musique pour le théâtre, particulièrement des pièces pour les enfants et les marionnettes ("Quatre danses faciles pour le piano", "Cinq chansons enfantines") ; il reçoit également de nombreuses commandes de musiques de film. Quand il sort diplômé du conservatoire de Tallinn en 1963, sa carrière professionnelle de compositeur est déjà bien amorcée.

Au début des années 1960, il s'initie à la composition sérielle, dont relèvent ses deux premières symphonies ; cela lui attire immédiatement d'importantes inimitiés, la musique sérielle étant considérée comme un avatar de la décadence bourgeoise occidentale. Tout aussi incorrectes politiquement dans le contexte soviétique, ses compositions d'inspiration religieuse, ainsi que sa technique du collage un temps utilisée, limitent considérablement le rayonnement de son œuvre.

En 1968, en proie à une crise créatrice, et à la suite de la censure par le régime communiste de son œuvre "Credo", Arvo Pärt renonce au sérialisme et plus globalement à la composition elle-même, et ce durant une dizaine d’années, temps qu'il consacre à l'étude du plain-chant grégorien et à celle de compositeurs médiévaux français et flamands tels que Guillaume de Machaut, Ockeghem, Obrecht et Josquin des Prés. Ces études et réflexions aboutiront à l'écriture d'une pièce de style intermédiaire, la "Symphonie 3" (1971).

Son évolution stylistique est notable en 1976 avec la composition d'une pièce pour piano devenue célèbre, "Für Alina", qui marque une rupture avec ses premières œuvres et qui pose les jalons de son nouveau style, qualifié par lui-même de « style tintinnabuli ». L'auteur l'explique ainsi : « Je travaille avec très peu d'éléments - une ou deux voix seulement. Je construis à partir d'un matériau primitif - avec l'accord parfait, avec une tonalité spécifique. Les trois notes d'un accord parfait sont comme des cloches. C'est la raison pour laquelle je l'ai appelé "tintinnabulation" ». L'année suivante, Pärt écrira dans ce nouveau style trois de ses pièces les plus importantes et reconnues : "Fratres", "Cantus in Memoriam Benjamin Britten" et "Tabula rasa".

En 1980, accompagné de sa famille, il quitte son pays où il est en proie à la censure pour Vienne où il obtient la nationalité autrichienne. L'année suivante il part pour Berlin-Ouest. De fréquents séjours le conduisent près de Colchester dans l'Essex. Il revient ensuite en Estonie et vit désormais à Tallinn. Son succès jamais démenti dans tout l'Occident, et particulièrement aux États-Unis, a pour inconvénient de le ranger dans la catégorie des compositeurs « minimalistes mystiques », avec Henryk Górecki et John Tavener. En 1996, il devient membre de l'Académie américaine des arts et des lettres.

Créateur d'une musique épurée, d'inspiration profondément religieuse — il est de confession chrétienne orthodoxe, et les chants orthodoxes ainsi que les chants grégoriens ont influencé son style sur la modulation lente des sons —, associée par certains à la musique postmoderne, Arvo Pärt creuse à présent le sillon de son style tintinnabuli. Ses œuvres ont été jouées dans le monde entier et ont donné lieu à plus de 80 enregistrements, ainsi qu'à de très nombreuses utilisations pour l'illustration sonore de films et de spectacles de danse.

Elle se caractérise par l'écriture minimaliste de Pärt, une musique épurée qui donne une impression de simplicité. 

Le premier élément est l'utilisation de rythmes simples tels que « noire, blanche, noire, blanche » ou « blanche, noire, blanche, noire ». Le second élément est le fameux style tintinnabuli. Chez Arvo Pärt, cette écriture s'inspire donc du son de la clochette, lorsqu'un instrument - quel qu'il soit - articule son jeu entre trois notes principales, celle de l'accord parfait d'une gamme. Cette simplicité se retrouve également dans l'utilisation de notes récurrentes et d'une certaine stabilité de la gamme. Pärt, contrairement à beaucoup de compositeurs des époques baroque, classique et romantique, n'utilise donc pratiquement jamais de modulations.

Liste chronologique des œuvres complètes d'Arvo Pärt :


Dans les années 1960 et 1970, Arvo Pärt compose, sur commande, plusieurs musiques de film. Cette production est estimée à près de 40 bandes originales. Le style est imaginatif, mais manque d'unité. Ce travail n'a pour le compositeur qu'une fonction lucrative et reste sans rapport avec le travail de recherche qui l'occupe à la même époque.

Bien que la musique d'Arvo Pärt à partir de 1976 soit composée spécifiquement pour les concerts, le succès des enregistrements discographiques pousse de nombreux réalisateurs à utiliser, à partir des années 1990, des extraits de ses œuvres en leurs assignant une fonction critique et narrative importante. L'estimation actuelle regroupe une vingtaine de pièces, présentes dans plus d'une centaine de longs métrages. Parmi ses œuvres sont utilisées le plus fréquemment "Für Alina", "Fratres", "Cantus" et, plus particulièrement, "Spiegel im Spiegel" qui apparaît en surimposition d'une variété de thèmes, notamment la guerre, la maladie en phase terminale, le terrorisme, la compassion et le pardon.

D'une manière plus générale, y compris dans son utilisation au cinéma, l'œuvre d'Arvo Pärt peut être considérée comme l'agent esthétique de quelque chose qui est « inaccessible, oublié ou dépossédé ».

En 2015, le concert-spectacle "Adam's Passion", mis en scène par Bob Wilson, est joué dans une ancienne usine de sous-marins à Tallinn.

La plupart sont parus chez ECM dans la collection "New Series" et Harmonia Mundi :


Arvo Pärt est docteur honoris causa de plusieurs universités dans le monde dont celles de Sydney (1996), Tartu (1998), Durham (2003), Fribourg (2007), Liège (2009) et Saint Andrews (2010). 

Membre de l'Académie américaine des arts et des lettres (département musique) depuis 1996, il a reçu le prix Léonie-Sonning de la musique en 2008 et a été nommé chevalier de la Légion d'honneur en 2011. Il est également membre du Conseil pontifical pour la culture depuis 2011. En septembre 2013, il reçoit la distinction d'Archonte du patriarcat œcuménique de Constantinople.

Arvo Pärt a reçu en juillet 2014 le Praemium Imperiale dans la section "musique", attribué par l'Association japonaise des beaux-arts.





</doc>
<doc id="267" url="https://fr.wikipedia.org/wiki?curid=267" title="Akseli Gallen-Kallela">
Akseli Gallen-Kallela

Akseli Gallen-Kallela de son vrai nom Axel Waldemar Gallén (né le à Pori, en Finlande, et mort le à Stockholm, en Suède) est un peintre et graveur finlandais de la fin du et du début du .
Il fut l'un des artistes finlandais les plus connus internationalement.
Son œuvre est associée aux styles nationaliste romantique, symboliste et réaliste.

Axel Waldemar Gallén naît à Pori dans une famille suédophone. Axel est le troisième enfant du second mariage de son père avec Anna Mathilda (née Wahlroos en 1832). Il a 12 frères et sœurs.
Son père Peter Wilhelm Gallén (1817–1879) est employé comme caissier de la Banque de Finlande à Pori, il est aussi propriétaire à Tyrvää d'un élevage de chevaux et de deux autres terrains, soit en tout environ 150 hectares. Axel Waldemar passe sa jeunesse dans le domaine de Jaatsi. Ensuite, son père est successivement chef de police rurale, avocat, dans les années 1850, il met en place une bibliothèque, puis la Caisse d'épargne de Tyrvää et une vingtaine d'années plus tard la première école publique. Le père d'Axel meurt à l'âge de 62 ans alors qu'Axel a 14 ans. Sa mère Anna Mathilda vivra en bonne santé jusqu'à 90 ans.

Axel admire les agriculteurs de langue finnoise, il fait connaissance avec les paysans et apprend le finnois avec son père et avec les domestiques.
À l'automne 1876, Axel, avec son frère aîné Cleas Uno et son cadet Hugo Walter, sont envoyés au lycée normal suédois d’Helsinki. 
Axel ne se plaît pas au lycée, les cours de latin et de religion sont les plus ennuyeux. 
Il se passionne pour les exercices de dessin.

Déjà dans sa période scolaire, Axel n'accepte pas entre autres la théorie raciale de August Sohlman et de Peter Andreas Munch sur l'origine asiatique des finlandais auxquels les Suédois auraient apporté la culture. 
Au cours des années 1870, Axel commence à lire le Kalevala, ce que n'apprécie pas du tout sa mère Mathilda, qui est porteuse de la théorie suédoise du Scandinavisme et perçoit les Finlandais comme un peu grossiers et péquenots.

Dès 1878, Axel commence, après ses journées de lycée, à fréquenter l'école de dessin de l'Académie des beaux-arts d'Helsinki. L'automne 1879, son père, Peter Gallén, meurt brutalement. L'été suivant, à l'âge de 15 ans, Axel voyage pour la première fois en dehors des frontières du Grand-duché de Finlande pour Tallinn, où son demi-frère Peter Wilhelm Gallén est vétérinaire en chef.

Au printemps 1881, Axel peut enfin abandonner ses cours au lycée, ce que son père n'avait jamais accepté et ainsi il ne passera jamais son baccalauréat. La même année, il s'inscrit au cours de l'école de dessin de l'association artistique de Finlande. Il a d'abord comme professeur "Carl Jahn" et l'année suivante, il peut changer pour la classe modèle dont le professeur est Fredrik Ahlstedt. Il reçoit aussi des leçons particulières de "S. A. Keinänen". Selon ses dires, ce sont les enseignement d'Adolf von Becker qui l'ont le plus influencé.

Le décès de son père a mis la famille dans une situation économique telle qu'il cherche à financer ses études en faisant des agrandissements pour la librairie Edlund, et des illustrations de livres pour A. A. Granfelt. Ce faisant, il commence à s’intéresser de plus en plus au travail d'illustration.

En 1884, après ses études à Helsinki, il part étudier à Paris, à la fois à l'Académie Julian de 1884 à 1889 et en même temps de 1887 à 1889 à l'Atelier Cormon. En 1888, il exécute le portrait du peintre suédois Nils Forsberg qui vivait à Paris depuis 1868. Il fait des voyages d'études à Londres et à Berlin en 1895 et en Afrique de 1909 à 1911. En 1898, il participe à Saint-Pétersbourg à l' "exposition d’artistes russes et finlandais" et à l' "Exposition artistique internationale" organisée par l'association Mir Iskousstva en 1899.
À Paris, il habite avec Emil Wikström, Albert Edelfelt leur a donné son vieux poêle. Dans les milieux artistiques nordiques de l'époque, on trouve des peintres et des écrivains comme les Suédois August Strindberg et Ernst Josephson, les Norvégiens Bjørnstjerne Bjørnson, Edvard Munch, Frits Thaulow et Adam Dörnberger. Un de ses bons amis est le céramiste français Henry Dehaulme de Vallombreuse. Ils fréquentent les cafés de Montmartre et du Quartier latin. Wikström et Gallén tombent tous deux malades de la diphtérie au printemps 1886.

En 1890, Gallen-Kallela se marie avec Mary Helena Slöör. Le jeune couple fait son voyage de noce à Kuhmo et en Carélie. Il commence son célèbre triptyque "La Légende d'Aïno".
C’était l’époque de la , du carélianisme et les débuts du romantisme de la Carélie du Kalevala, qui de sa façon donnait réponse à la fuite de Gauguin vers le primitivisme. La tendance était au style Romantisme national dans tous les domaines artistiques.
Dans les années 1890, le couple aura trois enfants Marjatta, Kirsti et Jorma. Marjatta mourra de diphtérie à l'âge de quatre ans, ce qui marquera un tournant dans la carrière de l'artiste en 1895.
Dans ses années d’études à Paris, Gallen-Kallela peint la vie de bohème, mais peu à peu la nature, les paysages sauvages et la population de Finlande le rappellent. Et les thèmes mythiques du Kalevala le séduisent. 
Au tournant du siècle, Gallen-Kallela contribue fortement à la lutte contre la russification de la Finlande. Il contribue à l’essor d’un art national et en démontrant sa vitalité culturelle, il défend la légitimité de la Finlande à exister en tant que nation. À cette époque, les cercles artistiques finlandais sont majoritairement dans la mouvance du Parti jeune finnois (en ).
Dans le tableau "Symposion", il peint les rencontres de son cercle du Parti jeune finnois à l’hôtel Kämp. 
Dans ce cercle, on trouve entre autres Jean Sibelius, Eino Leino et Robert Kajanus. 
L'hiver 1905–1906, le révolutionnaire Maxime Gorki se cache à Helsinki dans l’atelier Pirtti de Gallen-Kalella, qui peindra son portrait pendant son séjour.

En 1907–1908, Akseli Gallen-Kallela voyage en Hongrie. En 1908, une exposition de près de 500 œuvres de Gallen-Kallela est organisée à Budapest. Dans cette ville est érigé un monument à la mémoire de Gallen-Kallela dans un parc de Buda proche du Danube.

En 1918, juste après l’Indépendance de la Finlande pendant la Guerre civile finlandaise, Gallen-Kallela rejoint les troupes blanches (en ) du Gouvernement de Pehr Evind Svinhufvud dirigées par Carl Gustaf Emil Mannerheim. Il est d’abord cartographe des Forces armées finlandaises puis s’installe au siège de l’armée blanche comme Aide de camp de Mannerheim.
À l’exception de la croix de Mannerheim, les médailles de l’ordre de la Croix de la Liberté sont créées à l’époque par Gallen-Kallela. Après la guerre, Gallen-Kallela est adjudant de Mannerheim et conçoit les uniformes de combat de l’armée finlandaise et les médailles de l’ordre de la Rose blanche.

En 1911–1913, Gallen-Kallela conçoit et construit son manoir Tarvaspää à Espoo. En 1961, il est ouvert au public sous le nom de musée Gallen-Kallela. En 1894-1895, il avait déjà conçu et construit un autre atelier nommé Kalela à Ruovesi, qu’il a habité avec sa famille de 1895 à 1900, l’été 1905 puis de 1915 à 1921. Kalela est aussi un musée.

En 1922, pendant qu’il travaille à Porvoo pour WSOY, il commence à travailler sur son projet de "Suur-Kalevalaa". Pendant plusieurs années, il recherche des ornements et une typographie. Mais c’est pendant un voyage au Mexique et en Amérique du Nord qu’il trouve comment illustrer en rendant l’esprit des poèmes du Kalevala. Il peut alors terminer son Suur-Kalevala de 75 pages.

En 1931, il est invité à Copenhague pour parler de son œuvre et pour rencontrer d’autres artistes nordiques. Sur le chemin du retour, il meurt d'une pneumonie, le 7 mars 1931 à Stockholm pendant son sommeil. Il est enterré au Cimetière de Hietaniemi à Helsinki.

Après avoir débuté par une peinture réaliste en puisant ses sujets dans la vie rurale, Akseli Gallen-Kallela se forge un style personnel d'inspiration néo-romantique, caractérisé notamment par des contours marqués et des couleurs vives. Il est célèbre pour ses grands tableaux illustrant des épisodes du "Kalevala", l'épopée nationale finlandaise ("La Défense du Sampo", 1896 ; "La Mère de Lemminkäinen", 1897).





</doc>
<doc id="268" url="https://fr.wikipedia.org/wiki?curid=268" title="America Online">
America Online

Après quinze années d'utilisation de la marque aux États-Unis, la société a décidé le de renommer la société AOL LLC afin d'adopter la marque AOL qui était jusqu'alors principalement utilisée pour communiquer sur ses services en Europe.

Le , Time Warner annonce sa séparation d'avec AOL, après neuf années de collaboration.

Le , la presse annonce que Verizon rachète AOL.

À la fin des années 1990, AOL fait partie avec ses compatriotes Amazon, Yahoo et eBay des sociétés devenues célèbres grâce à une bulle des capitalisations boursières des jeunes sociétés sans équivalent dans l'histoire, qui finit en krach, phénomène touchant aussi des nombreuses petites sociétés de biotechnologies et des Sociétés minières junior.

Le , AOL annonce qu'elle va se séparer de de ses employés dans les six prochains mois, soit le quart de son effectif, dont transferts d'emplois en Europe vers les sociétés qui reprennent ses activités de réseau d'accès à Internet.

Le , la société diffuse sur son site web, et à l'attention initiale d'un public académique spécialisé en techniques de recherche, les mots-clés utilisés par de ses abonnés américains dans le moteur de recherche AOL. Ces recherches couvrent une période s'étalant sur trois mois, soit environ d'entrées, non censurées ni filtrées. Le fichier d'environ est rapidement retiré du site lorsque AOL s'aperçoit des implications relatives à la vie privée et émet une lettre d'excuse. La liste, dont l'importance sur l'atteinte à la vie privée a été immédiatement réalisée par plusieurs internautes ainsi que par plusieurs sites majeurs d'information technique (Digg, Slashdot…) est disponible sur plusieurs sites miroirs ainsi que sur le réseau P2P BitTorrent. Les recherches, bien que rendues anonymes par AOL avant la publication de la liste, permettent d'identifier certains utilisateurs (par exemple plusieurs personnes ont recherché des informations sur elles-mêmes), et certains internautes ont utilisé des mots clés de recherche « douteux », voire hors la loi. Une plainte collective est déposée le pour violation de la , entre autres charges. Une plainte a aussi été déposé auprès de la Federal Trade Commission (Organisme de protection des droits du consommateur) par un groupe de défense des droits, l'Electronic Frontier Foundation, le 14 août 2006.

Le Time Warner annonce sa séparation avec AOL. Cette séparation est effective depuis le . Le AOL annonce avoir racheté le journal en ligne "The Huffington Post" pour de dollars.

En janvier 2014, AOL acquiert l'entreprise "Gravity", spécialisée dans la personnalisation de contenu internet, pour 83 millions de dollars.

En mai 2015, Verizon acquiert AOL, incluant donc ses activités dans la publicité mobile, Huffington Post, TechCrunch et Engadget, pour 4,4 milliards de dollars.

En juillet 2015, Microsoft annonce son retrait partiel du secteur de la publicité en ligne, au travers d'un partenariat avec AOL. Dans ce partenariat, les sites du groupe AOL utiliseront Bing comme moteur de recherche, en échange de quoi, AOL intègrera les activités de régie publicitaire de Microsoft, sauf pour le moteur de recherche Bing, en Allemagne, au Brésil, au Canada, en Espagne, aux États-Unis, en France, en Italie, au Japon et au Royaume-Uni. L'accord intègre également un transfert de emplois de Microsoft à AOL.

AOL faisait partie des premiers fournisseur d'accès à Internet (FAI) avec CompuServe.

Les filiales européennes (France, Royaume-Uni, Allemagne) étaient à l'origine issues d'une coentreprise entre America Online et Bertelsmann appelée AOL Europe, chargée de vendre en Europe les services et produits sous licence AOL. Ces produits comprennent notamment la connexion et l'accès à l'internet, la messagerie électronique, la messagerie instantanée et des contenus exclusifs accessibles via un logiciel propriétaire. Celui-ci avait la particularité d'être offert sur des CD distribués gratuitement dans les magazines informatiques, dans les boîtes aux lettres…

En 1998, Vivendi entre dans le capital d'AOL Europe à hauteur de 55 % via les groupes SFR-Cegetel et Canal+. La société devient la star du bas débit illimité en lançant une offre à (environ ) à partir du 21 août 2000. Il fallait s'abonner sur une période de deux ans pour bénéficier de l'offre à par mois ; sinon, le prix du forfait était de par mois. AOL proposait déjà une offre d'accès illimitée depuis le début de l'année 2000 (offre AOL Gold), pour par mois). L'offre à par mois a augmenté le nombre d'abonnés de façon significative, ce qui a provoqué des ralentissements et des problèmes de connexion dus au trop grand nombre d'utilisateurs sur les points d'accès. Les utilisateurs étaient d'ailleurs déconnectés à intervalles réguliers, afin de soulager la charge des serveurs. Le second problème étant que de nombreuses personnes prenant un abonnement illimité se sont retrouvées avec un abonnement limité, facturé à la minute. Cet incident a conduit à des factures élevées pour les premiers mois de certains clients (la cause étant une erreur de numérotation de connexion par les abonnés car seulement certains numéros étaient reconnus pour accéder à l'illimité) qui se sont très vite retirés de l'offre : AOL a d'autant plus terni son image après l'augmentation de ses prix, après deux ans, passant de 15 à . L'apogée de la société a donc été de courte durée sur le territoire hexagonal.

Finalement, le groupe français a pris beaucoup de retard dans le haut débit et ne s'est pas positionné à temps pour proposer un accès internet rapide en 2004, alors que de nombreux fournisseurs commençaient à proposer ces accès à des prix attractifs. Vivendi se désengage d'AOL France, mais malgré tout, AOL se modernise et propose des services associés (sécurité, antispam, antivirus, contrôle parental et autres) le tout inclus dans le prix du forfait.

AOL abandonne également peu à peu le modèle d'accès à ses services via ses logiciels propriétaires : de très nombreux clients se plaignaient en effet de devoir se connecter à internet via le logiciel AOL et non avec leur propre navigateur, se retrouvant bloqués. Cette obligation était aussi mal vue d'un point de vue légal que d'un point de vue pratique. C'est ainsi qu'apparaissent la possibilité de se connecter au réseau ADSL via n'importe quel modem routeur ou logiciel d'accès réseau à distance compatible PPPoE ou PPPoA, des accès aux services via le portail web (par exemple, webmail) et aux boîtes aux lettres AOL via tout logiciel compatible avec les protocoles SMTP et IMAP4, et la possibilité de configurer d'un logiciel de messagerie tiers pour l'accès aux boîtes aux lettres AOL. Mais hélas, il est déjà bien trop tard, et AOL se retrouve seul à imposer de telles restrictions à ses clients : le prix et la technique étant contre elle, la société perd de plus en plus de clients.

En 2005, AOL France propose une offre de téléphonie fixe, simple présélection sur la ligne téléphonique, sans offre illimitée. Fin 2005, AOL propose une Box ADSL et téléphone.

En , AOL a vendu son activité de fournisseur d'accès à Internet (FAI) au groupe Neuf Cegetel. AOL conserve ses activités de Média en France via sa filiale AOL France SNC, se recentrant sur le métier de fournisseur de contenu, par exemple la mise à disposition d'un portail « clé en main » à l'instar de ceux réalisés par AOL pour Neuf Cegetel et DartyBox, tout en continuant à proposer ses services de mail et messagerie instantanée.

La plupart des logiciels édités par AOL n'existent qu'en version pour les systèmes d'exploitation Windows et Mac, à quelques exceptions près, comme AIM pour GNU/Linux. Les utilisateurs d'autres systèmes d'exploitation, tels que les autres variantes d'Unix, peuvent néanmoins accéder à certaines fonctionnalités du service AOL telles que le courrier électronique via tout logiciel de messagerie traditionnel supportant les protocoles SMTP et IMAP, ou plus généralement via le webmail, AIM Express.

AOL propose AOL Explorer, surcouche à Internet Explorer.

En juin 2013, il a été révélé que AOL faisait partie du programme de surveillance PRISM de la National Security Agency.






</doc>
<doc id="269" url="https://fr.wikipedia.org/wiki?curid=269" title="Alphabet phonétique international">
Alphabet phonétique international

L'alphabet phonétique international (API) est un alphabet utilisé pour la transcription phonétique des sons du langage parlé. Contrairement aux nombreuses autres méthodes de transcription qui se limitent à des familles de langues, l'API est prévu pour couvrir l'ensemble des langues du monde. Développé par des phonéticiens français et britanniques sous les auspices de l'Association phonétique internationale, il a été publié pour la première fois en 1888. Sa dernière révision date de 2005, celle-ci comprend 107 lettres, 52 signes diacritiques et 4 caractères de prosodie.

L'API a été développé au départ par des professeurs de langue britanniques et français sous la direction de Paul Passy dans le cadre de l'Association phonétique internationale, fondée à Paris en 1886 sous le nom de "Dhi Fonètik Tîcerz' Asóciécon". La première version de l'API, publiée en 1888, était inspirée de l'alphabet romique d'Henry Sweet, lui-même élaboré à partir de l'alphabet phonotypique d'Isaac Pitman et Alexander John Ellis.

L'API a connu plusieurs révisions en 1900, 1932, 1938, 1947, 1951, 1989, 1993, 1996 et 2005.

La transcription phonétique en API consiste à découper la parole en segments sonores supposés insécables, et à employer un symbole unique pour chacun de ceux-ci, en évitant les multigrammes (combinaisons de lettres, comme le son "ch" du français, noté // phonologiquement, ou le "gli" italien, transcrit // phonologiquement).

Le nombre de caractères principaux de l’API est de 118, ce qui permet de couvrir les sons les plus fréquents. Ces caractères sont pour la plupart des lettres grecques ou latines ou des modifications de celles-ci : , , , (tirés de r) ; , (tirés de e). Les sons moins fréquents sont transcrits à partir des précédents en indiquant une modification du mode ou du point d'articulation par le biais d'un ou plusieurs signes diacritiques (au nombre de 76) sur le caractère principal : par exemple, le b du castillan "caber" (« tenir, rentrer dans ») est transcrit [] pour indiquer une spirante au lieu de la fricative bilabiale sonore [β]. Il existe également des symboles spéciaux pour noter des phénomènes suprasegmentaux, comme les tons mélodiques ou l'accent tonique : [], transcription de l'allemand "dulden" (« supporter, tolérer ») indique un accent tonique d'intensité sur la première syllabe (ˈ) et un n final vocalisé ( ).


La plupart du temps donc, les notations phonétiques exactes (indépendantes de la langue) sont rarement notées, au contraire des transcriptions phonologiques.

L'API possède des caractères principaux pour les voyelles orales les plus courantes qui sont classées selon : 

Ce tableau classe les voyelles selon les caractères ci-dessus, comme le triangle vocalique ou le trapèze vocalique.
Les autres voyelles sont transcrites à partir de celles-ci par adjonction d'un ou plusieurs diacritiques modifiant son articulation :

Par exemple, 

La quantité des voyelles est indiquée comme suit :

Notes:

Par exemple, "Pose cette rose !", phonologiquement //, est souvent réalisé en français parisien [].

L’amuïssement de voyelles phonémiques longues n'est pas noté phonétiquement : on utilise le symbole usuel en ôtant son signe d’allongement phonétique. En revanche les syllabes courtes sont notées phonologiquement par un accent bref et les voyelles amuïes sont soit supprimées de la notation phonémique soit marquées entre parenthèses.

La transcription des tonèmes suit le procédé ci-dessous. 

Notes :

L'API classe les consonnes selon trois critères :

Comme pour les voyelles, des diacritiques permettent d'indiquer une modification du point ou du mode d'articulation afin de transcrire des consonnes qui n'ont pas de symbole principal. 

Par exemple,

La quantité des consonnes (leur éventuelle gémination) est indiquée de la même manière que pour les voyelles. Le hongrois "Mit mondott?" (Qu'a-t-il/elle dit ?), phonologiquement /mit mon.dotː/, pourra être transcrit phonétiquement [].

Une consonne vocalisée, c'est-à-dire servant de sommet à une syllabe, comporte un trait vertical souscrit dans sa notation phonologique ; en revanche la vocalisation (par exemple un schwa bref) devrait être explicitée dans la notation phonétique, séparément de la consonne mentionnant l’articulation exacte :

Un point sépare les syllabes pertinentes dans la notation phonologique ; de même les mots restent séparés par des espaces. Ces deux signes phonologiques sont généralement omis des transcriptions phonétiques, sauf pour indiquer la présence effective d’une pause. Par exemple, l'allemand "Rindfleischetikettierungsüberwachungsaufgabenübertragungsgesetz" (loi sur le transfert de responsabilité de la surveillance de l'étiquetage de la viande bovine) se transcrit phonologiquement : 

Les syllabes accentuées sont précédées d’une courte barre verticale :
Les réalisations phonétiques des accents syllabiques peuvent varier suivant les langues et les locuteurs, entre la mutation de la consonne d'attaque, l’allongement ou la diphtongation de la voyelle au sommet de la syllabe, le changement de ton, la gémination ou la mutation de la consonne finale : ces réalisations possibles ne sont pas toujours distinguées clairement, et nombre de transcriptions phonétiques gardent la notation phonologique de ces accents avec les mêmes symboles.

La brève souscrite signale qu'un élément est à rattacher à la syllabe courante et ne constitue pas un nouvel élément syllabique. Par exemple : /po̯.ˈeta/, transcription phonologique du mot espagnol signifiant « poète ». (exemple tiré du "Handbook of the IPA", )

Le jeu de caractères Unicode permet d'écrire l'ensemble de l'API. Les symboles et diacritiques se situent dans les blocs de caractères suivants :
Certains caractères précomposés (avec diacritiques) sont accessibles dans les blocs suivants :





</doc>
<doc id="270" url="https://fr.wikipedia.org/wiki?curid=270" title="Anarchie">
Anarchie

L’anarchie (du grec / , composé de , préfixe privatif : absence de, et , pouvoir, hiérarchie, commandement) désigne l'état d'un milieu social sans gouvernement, la situation d’une société où il n’existe pas de chef, pas d’autorité unique, autrement dit où chaque sujet ne peut prétendre à un pouvoir sur l’autre. Il peut exister une organisation, un pouvoir politique ou même plusieurs, mais pas de domination unique ayant un caractère coercitif. L’anarchie peut, étymologiquement, également être expliquée comme le refus de tout principe premier, de toute cause première, et comme revendication de la multiplicité face à l’unicité.

Polysémique, le terme anarchie s'entend sous des acceptions, non seulement différentes, mais absolument contradictoires. Employé péjorativement, comme synonyme de désordre social dans le sens commun ou courant et qui se rapproche de l’anomie, il l'est aussi comme un but désirable à atteindre comme c’est le cas pour les anarchistes.

En 1840, Pierre-Joseph Proudhon est le premier à se réclamer anarchiste, c'est-à-dire, partisan de l’anarchie, entendu en son sens positif : « La liberté est anarchie, parce qu'elle n'admet pas le gouvernement de la volonté, mais seulement l'autorité de la loi, c'est-à-dire de la nécessité ». En 1987, Jacques Ellul précise : « plus le pouvoir de l'État et de la bureaucratie augmente, plus l'affirmation de l'anarchie est nécessaire, seule et dernière défense de l'individu, c'est-à-dire de l'homme ».

Pour les anarchistes, l’anarchie est l'ordre social absolu, grâce notamment à la socialisation des moyens de production : contrairement à l'idée de "possessions privées" capitalisées, elle suggère celle de "possessions individuelles" ne garantissant aucun droit de propriété, notamment celle touchant l'accumulation de biens "non utilisés". Cet ordre social s'appuie sur la liberté politique organisée autour du mandatement impératif, de l'autogestion, du fédéralisme libertaire et de la démocratie directe. L'anarchie est donc organisée et structurée : c'est l'ordre moins le pouvoir.

En 1850, Anselme Bellegarrigue publie "L'Anarchie, journal de l'ordre". Pour ses partisans, l’anarchie est donc organisée et structurée : c’est selon les mots d’Élisée Reclus .

Le mot "anarchie" est souvent employé avec une connotation péjorative.

Le dictionnaire des synonymes de référence du Centre de recherches interlangues sur la signification en contexte de l'université de Caen Basse-Normandie indique que parmi les 9 synonymes les plus proches, 7 relèvent du désordre ("désordre", "chaos", "confusion", "gâchis", "trouble", "émeute" et "pagaille") et deux des principes politiques de l'anarchisme ("égalité" et "liberté").

Cette proximité avec le champ lexical du désordre tient, dans les discours politiques dominants, d'une nécessité positive du principe fondamental d’autorité : dans ce sens "anarchie" sert à désigner une situation de désordre, de désorganisation, de chaos, sur la base de l’hypothèse implicite que l’ordre nécessiterait une hiérarchie. C'est ainsi que l'on trouve déjà dans le Littré (le mot est très peu usité avant le ) la définition de l’anarchie comme « absence de gouvernement, et par suite désordre et confusion ». Par extension ce sont toutes les formes de trouble et de désordre qui sont appelées anarchie ; c’est cette façon d’employer le mot qui prévaut dans l’usage courant, comme dans la plupart des dictionnaires.

En 1869, l"'Encyclopédie générale" rédigée sous la direction de Louis Asseline précise : « Pour les uns, c'est l'absence de gouvernement, d'autorité, de principe, de règle, et par conséquent c'est le désordre dans les esprits et dans les faits. Pour les autres, c'est l'élimination de l'autorité sous ses trois aspects politique, social et religieux, c'est la dissolution du gouvernement dans l'organisme naturel, c'est le contrat se substituant à la souveraineté, l'arbitrage au pouvoir judiciaire, c'est le travail non pas organisé par une force étrangère mais s'organisant lui-même, c'est le culte disparaissant en tant que fonction sociale et devenant adéquat aux manifestations individuelles de la libre conscience, ce sont les citoyens contractant librement non pas avec le gouvernement mais entre eux, c'est enfin la liberté, c'est l'ordre. »

Le poète Armand Robin (1912-1961) définit « l'anarchiste » comme celui qui est « purifié volontairement, par une révolution intérieure, de toute pensée et de tout comportement pouvant d'une façon quelconque impliquer domination sur d'autres consciences ».

Le mot correct pour une situation de désordre social, sans lois, sans règles, où les différends se régleraient par la seule violence physique (armée ou non), est l’anomie. L’anomie, néologisme durkheimien, est une dissolution des normes sociales, règles, lois, coutumes : cette situation peut être liée à une volonté de domination réciproque de plusieurs pouvoirs concurrents, à une réaction de désespoir ("L'anarchie est la formulation politique du désespoir", Léo Ferré) face à une société moribonde. 

À ce sujet, bien que "anomie" soit mieux adapté, le terme « anarchie » est utilisé systématiquement par les gouvernements pour indiquer une situation politique qu’ils ne maîtrisent pas (et qu’ils désireraient maîtriser), où leur pouvoir politique est en difficulté.

Anomie (en grec ἀνομία) a néanmoins un usage plus ancien, notamment dans le nouveau testament. Les exégètes lui donnent communément un sens similaire à celui de l'iniquité, notamment dans l'expression « mystère d'iniquité ».

De nombreux exemples historiques illustrent cette confusion avec l'anomie : il ne s’agit pas de situations qui puissent s’apparenter à l’anarchie au sens strict, auquel cas il n’y aurait plus de pouvoir, ni d’autorité, mais d’une désorganisation liée aux pouvoirs concurrents, d’une période politique troublée.

Ainsi, les historiens désignent par "Anarchie militaire" la période de 235 à 284 durant laquelle l'Empire romain subit la première grande crise de son histoire.

"The Anarchy" définit la guerre civile anglaise qui oppose deux concurrents au pouvoir, Mathilde l'Emperesse et Étienne de Blois entre 1135 et 1154.

Durant la Première Révolution anglaise (1642-1651), le mouvement des Niveleurs est stigmatisé par ses détracteurs comme « Switzerising anarchists ».

Lors de la Révolution française, pour Camille Desmoulins en 1789, « despotisme, anarchie, ou droit du plus fort, sont synonymes et emportent l'idée de l'absence des lois ». Tandis que pour le girondin, Jacques Pierre Brissot, la ligne des Enragés, qui revendiquent l'égalité civique, politique mais aussi sociale, mène à l'« anarchie ».

Bien souvent, le terme « anarchie » est utilisé pour décrire le chaos, les guerres civiles et les situations de désordre social.

Les anarchistes rejettent en général cette conception courante de l'anarchie utilisée par les médias et les pouvoirs politiques interprétée comme l’absence d’ordre, de règles et de structures organisées, bref : le chaos de l’anomie sociale. Pour eux, l'ordre naît de la liberté, tandis que les pouvoirs engendrent le désordre. Certains anarchistes useront du terme « acratie » (du grec « kratos », le pouvoir), donc littéralement « absence de pouvoir », plutôt que du terme « anarchie » qui leur semble devenu ambigu. De même, certains anarchistes auront plutôt tendance à utiliser le terme de « libertaire », inventé par Joseph Déjacque, en 1857, pour affirmer le caractère égalitaire et social de l'anarchisme naissant.

Par ailleurs, l'utilisation péjorative du terme provient des actions de certains anarchistes au tournant des et en Europe. À cette époque, les "illégalistes" qui ignorent les « lois » considérées comme illégitimes et les partisans de la "propagande par le fait" mettent en œuvre des moyens, y compris violents, dans le but de hâter l'avènement de l'anarchie. Concrètement, ces anarchistes illégalistes escroquent, volent et tuent "au nom de leur idéal", avec comme victimes des puissants (présidents, rois, princes, ministres, riches, compagnies d’assurances, etc.) ou des serviteurs de l’État (juges, douaniers, policiers, etc.). Quelle qu’ait été l’importance réelle de ce courant, il a énormément frappé les esprits. Ces actions provoquent la mise en place des lois anti-anarchistes (« lois scélérates ») à la fin du dans de nombreux pays et stigmatisent l’ensemble des anarchistes, tandis que les termes « anarchiste » ou « Ravachol » deviennent des injures.

L’usage du terme "libertaire" se répand en France avec l’interdiction des mots de l’anarchie, pour des raisons sociales et juridiques (être l’auteur de « "propagande anarchiste" » est resté passible de prison jusqu’en 1992).

Les anarchistes rejettent en général la conception courante de l’anarchie (utilisée dans le langage courant, par les médias et les pouvoirs politiques). Pour eux, au contraire, l’ordre naît de la liberté, tandis que les pouvoirs engendrent le désordre (voir termes historiques). Certains anarchistes useront du terme acratie, du grec / (le pouvoir) donc littéralement « absence de pouvoir », plutôt que du terme « anarchie », d’étymologie grecque lui aussi, qui leur semble devenu ambigu, porteur d’un aspect positif mais d’une trop grande connotation négative pour pouvoir être employé comme synonyme d’un objectif désirable. De même, certains anarchistes auront plutôt tendance à utiliser le terme de « libertaires » pour se désigner, ou indifféremment ceux de « fédéralistes », « anti-étatistes » ou « anti-autoritaires ».

Il est arrivé à Bakounine lui-même d’utiliser « anarchie » au sens de désordre, et l’on retrouve cette acception dans les écrits du Comité central de l’Internationale genevoise. Ces formulations ne se retrouvent toutefois plus chez les anarchistes actuels.

Cependant, les anarchistes utilisent encore le terme, porteur d’une histoire indissociable d’autres notions qui s’y rattachent comme l’anarchisme ou l’anarchie positive de Proudhon (qui est d’ailleurs le premier à donner un sens précis au mot anarchie, utilisé auparavant en guise d’insulte dans les milieux politiques sans avoir jamais été véritablement défini).

L’anarchie aux yeux des anarchistes n’est pas un chaos, mais la situation harmonieuse résultant de l’abolition de l’État et de toutes les formes de l’exploitation de l’humain par l’humain, « c'est l'ordre sans le pouvoir », « la plus haute expression de l'ordre » (Élisée Reclus). Fondée sur l’égalité entre les individus, l’association libre, bien souvent la fédération et l’autogestion, voire pour certains le collectivisme, l’anarchie est donc organisée, structurée, sans admettre pour autant, aux yeux des anarchistes anticapitalistes, de principe de supériorité quelconque de l'organisation sur l'individu. Au début du , ces principes rejoignent les valeurs propulsées par l'Internet : confiance et autonomie, et que certains libéraux suggèrent d'appliquer aux entreprises et aux administrations.

On peut noter que chez tous les anarchistes la qualité indispensable est la responsabilité individuelle (associée au droit naturel) qui permet d’agir dans l’intérêt personnel sans pour autant attenter à la liberté des autres. Les seuls mandatés le sont, par volontarisme et sans durée précise, dans un but et sur un mandat précis, et il n’existe ainsi nulle forme de domination ni de gouvernement.

 





Par ailleurs en Russie, la pensée libertaire était fortement présente lors de la Révolte de Kronstadt () et plus généralement dans les Soviets jusqu'à leur mise au pas par le parti bolchevique.






« L'anarchie est le plus haut degré de liberté et d'ordre auquel l'humanité puisse parvenir. » Pierre-Joseph Proudhon

« L'anarchie c'est l'ordre, et le gouvernement la guerre civile » Anselme Bellegarrigue, 1848

« L'anarchie est la plus haute expression de l'ordre. » Élisée Reclus 
Sur le sens d’ « anarchie » :







</doc>
<doc id="271" url="https://fr.wikipedia.org/wiki?curid=271" title="Astéroïde">
Astéroïde

Un astéroïde est une planète mineure (voir ci-contre le diagramme d'Euler des corps du système solaire) qui est composée de roches, de métaux et de glaces, et dont les dimensions varient de l'ordre du mètre (limite actuelle de détection) à plusieurs centaines de kilomètres.


En 1801, le premier astéroïde est découvert et nommé Cérès ; il est le plus grand du système solaire. Depuis, plus de astéroïdes du Système solaire ont été répertoriés (en 2015, date à laquelle une sonde spatiale a tourné pour la première fois autour de Cérès). Les premiers astéroïdes découverts ont une orbite située entre celles de Mars et de Jupiter, aussi cette zone est-elle appelée la ceinture principale d’astéroïdes. Une autre zone située au-delà de l’orbite de Neptune comporte une forte concentration d’astéroïdes : la ceinture de Kuiper.

La composition des astéroïdes de la ceinture de Kuiper est plus riche en glace et plus pauvre en métaux et en roche, ce qui les apparente à des noyaux cométaires. Contrairement aux comètes les astéroïdes sont inactifs, cependant quelques-uns ont été observés avec une activité cométaire.

On suppose que les astéroïdes sont des restes du disque protoplanétaire qui ne se sont pas regroupés en planètes.

Certains astéroïdes croisant l’orbite de la Terre (appelés géocroiseurs) sont considérés comme potentiellement dangereux, à cause du risque de collision, et sont surveillés par des systèmes automatisés.

Le premier astéroïde est découvert fortuitement par Giuseppe Piazzi, directeur de l’observatoire de Palerme. Le janvier 1801, alors qu’il mène des observations dans la constellation du Taureau afin d’établir un catalogue stellaire, il repère un nouvel astre. Le lendemain, il constate avec surprise que celui-ci s’est déplacé vers l’ouest. Il suit le déplacement de cet objet pendant plusieurs nuits. Son collègue, Carl Friedrich Gauss, utilise ces observations pour déterminer la distance exacte de cet objet inconnu à la Terre. Ses calculs situent l’astre entre les planètes Mars et Jupiter. Piazzi le nomme Cérès, du nom de la déesse romaine qui fait sortir la sève de la terre et qui fait pousser les jeunes pousses au printemps, et également déesse protectrice de la Sicile.

Selon la loi de Titius-Bode, formulée en 1766 par Johann Daniel Titius et divulguée par Johann Elert Bode, une planète aurait dû graviter entre Mars et Jupiter. Une campagne d’observation, initiée par Joseph Jérôme Lefrançois de Lalande en 1796, avait été lancée afin de la localiser. Piazzi, sans le vouloir, avait devancé ses collègues avec la découverte de Cérès sur l’orbite de l’hypothétique planète.

Entre 1802 et 1807, trois autres objets sont découverts sur des orbites voisines : Pallas, Junon et Vesta. Les quatre nouveaux corps sont alors considérés comme de véritables planètes. Le terme de "petites planètes" est généralement employé ; cependant dès 1802, William Herschel propose l’appellation d’"astéroïde", qui signifie littéralement « en forme d’étoile », à cause de leur aspect au télescope, différent de celui en forme de disque régulier des autres planètes. Avec, de plus, leur petite taille ou l’inclinaison orbitale élevée de Pallas, il s’agissait selon lui d’objets du Système solaire à distinguer des planètes.

Il faut attendre 1845 pour qu’une nouvelle "petite planète" soit découverte, Astrée, par Karl Ludwig Hencke. Dès lors, les découvertes ne cessent de se multiplier et l’appellation proposée par Herschel s’impose. En juillet 1868, cent astéroïdes sont connus. La millième découverte homologuée a lieu en novembre 1921 () et la dix-millième en octobre 1989 ((). En règle générale, l’ordre des dates de découverte diffère de l’ordre de numérotation des astéroïdes, car l’affectation d’un numéro se fait après une détermination suffisamment fiable de l’orbite de l’objet.

Heinrich Olbers, le découvreur de Pallas et Vesta, avait émis l’hypothèse que les astéroïdes étaient les fragments d’une planète détruite. Cet objet supposé fut même baptisé ultérieurement Phaéton. L’hypothèse la plus communément admise aujourd’hui considère les astéroïdes comme des résidus du Système solaire primitif n’ayant pu s’agglomérer jusqu'à former une planète, à cause notamment de l’influence gravitationnelle de Jupiter.
Ils sont donc considérés comme des reliques du Système solaire, leur étude plus poussée et leur exploration permettraient d’en savoir davantage sur la formation du Système solaire.

La majorité des découvertes d’astéroïdes se font dans la zone comprise entre Mars et Jupiter, et appelée la ceinture d’astéroïdes (ou ceinture principale). Mais d’autres sont découverts en dehors de cette zone, soit parce qu’ils possèdent une orbite qui les fait s’éloigner de la ceinture principale, soit parce qu’ils sont situés dans une toute autre zone du Système solaire (voir Principaux groupements).

L’étude des astéroïdes fut longtemps délaissée par les astronomes. Nous les connaissons depuis maintenant plus de deux cents ans, mais ils étaient considérés comme les rebuts du Système solaire. On sait maintenant que les astéroïdes sont une clé importante de la compréhension de la formation du Système solaire et c’est pour cette raison que les astronomes montrent un plus grand intérêt envers ces objets.

Jusqu’en 1998, les astéroïdes étaient découverts à l’aide d’un processus en quatre étapes . :

Depuis 1998, la plupart des astéroïdes sont découverts à l’aide de systèmes automatisés qui comprennent des caméras CCD et des ordinateurs reliés directement aux télescopes. Voici les principales équipes utilisant de tels systèmes, classées par le nombre de découvertes numérotées au 4 octobre 2015 :


En orbite autour de la Terre, le satellite de la NASA WISE a, quant à lui, découvert en 2010, dont 2007 numérotés au 4 octobre 2015.

Au 14 novembre 2016, le dénombre numérotés, dont , et non numérotés ; soit un total de connues.

Les astéroïdes sont presque impossibles à observer à l’œil nu. Ils sont bien plus petits que les planètes, et très peu lumineux. L’astéroïde en est l’exception, puisque c’est le seul qu’il soit parfois possible d’observer sans appareil optique. Sa luminosité n’étant toutefois pas très grande, il faut donc savoir où poser le regard.

Un astéroïde ressemble plus ou moins à une étoile qui brille dans le ciel nocturne. Le meilleur moyen pour partir à la chasse aux astéroïdes avec ses jumelles ou son télescope est d’observer le fond étoilé, plusieurs nuits d’affilée, et de détecter les points lumineux qui se déplacent par rapport au fond, qui, lui, paraît stable. Certains catalogues répertorient la position des astéroïdes, et il est alors plus facile de pointer le télescope au bon endroit.

Certains astéroïdes sont relativement bien connus, du fait de leur taille importante ou encore parce qu'ils sont géocroiseurs.

Dans les premières décennies du , les astéroïdes furent affublés d'un symbole astronomique ( pour Cérès, pour Pallas, pour Junon, etc.), à l'instar des planètes du Système solaire. Les astéroïdes étaient à cette époque considérés comme des planètes à part entière. En 1851, devant leur nombre grandissant, le spécialiste allemand Johann Franz Encke prit la décision de remplacer ces symboles par une numérotation.

En 1947, l'américain Paul Herget, directeur de l'observatoire de Cincinnati, est chargé par l'Union astronomique internationale de fonder le . Depuis, la désignation des planètes mineures est assurée par ce centre.

Quand l’orbite d’un astéroïde est confirmée, l’apparition reçoit une première désignation constituée de l’année de découverte suivie d’une lettre représentant la quinzaine durant laquelle s’est produite la découverte, et d’une seconde lettre indiquant l’ordre de découverte pendant cette quinzaine (la lettre I n’est pas utilisée). Si plus de sont découverts dans une quinzaine, on recommence l’alphabet en ajoutant un numéro qui indique combien de fois la seconde lettre est réutilisée (exemple : ).

L’astéroïde reçoit ensuite un numéro permanent, noté entre parenthèses, accompagnant la première désignation (exemple : ), puis parfois, et plus tard, un nom qui remplace la première désignation (exemple : ). Les premiers ont reçu les noms de personnages de la mythologie grecque ou romaine, à l’instar des planètes et de leurs satellites, d’autres mythologies ont ensuite été utilisées (nordique, celtique, égyptienne…) ainsi que des noms de lieux, des prénoms ou des diminutifs, des noms de personnages fictifs, d’artistes, de scientifiques, de personnalités des milieux les plus divers, des références à des événements historiques… Les sources d’inspirations pour nommer un astéroïde sont désormais très variées.

Ces dernières années, le rythme de découverte est tel que les astéroïdes sans noms sont majoritaires. Quelques groupes d’astéroïdes ont des noms ayant un thème commun. Par exemple, les centaures sont nommés d’après les Centaures de la mythologie et les Troyens sont nommés d’après les héros de la guerre de Troie. Au , sur numérotés, le dernier nommé était (301638) Kressin, et le premier astéroïde sans nom était .

Les premières images rapprochées d’un astéroïde sont l’œuvre de la sonde Galileo envoyée vers Gaspra en 1991 et Ida en 1993.

Lancée le par la NASA la sonde se met en orbite autour de l’un des plus gros astéroïdes géocroiseurs : Éros. Après avoir établi une cartographie complète de la surface de entre avril et octobre 2000, et bien que cela n'ait pas été prévu initialement, la sonde se pose en douceur sur l’astéroïde le . Son dernier signal est reçu le 28 février.

En 2003, la JAXA lance la sonde Hayabusa vers l’astéroïde Itokawa, avec pour objectif de s’y poser en douceur et d’en prélever des échantillons. Malgré plusieurs pannes et incidents, la sonde revient sur Terre le , sans que l’on sache si elle contient effectivement des échantillons. Finalement, le 16 novembre, la Jaxa annonce que l’analyse des particules récoltées par Hayabusa a confirmé leur origine extraterrestre. Le Japon devient ainsi le premier pays à s’être posé sur un astéroïde et en avoir rapporté des échantillons.

En 2012, Planetary Resources se constitue en vue de l'exploitation minière des astéroïdes, suivie en 2013 par la compagnie Deep Space Industries.

La ceinture dite "principale" , entre les orbites de Mars et Jupiter, distante de deux à quatre unités astronomiques du Soleil, est le principal groupement : plus de y ont été répertoriés à ce jour. L’influence du champ gravitationnel de Jupiter les a empêchés de former une planète.
Cette influence de Jupiter est également à l’origine des lacunes de Kirkwood, qui sont des orbites vidées par le phénomène de résonance orbitale.

Les astéroïdes géocroiseurs sont des astéroïdes dont l’orbite est relativement proche de celle de la Terre. Au 3 septembre 2011, on en dénombre .

Les Amors, dont fait partie, les Atens et les Apollons en sont les principaux groupes.

Seuls les Atens et les Apollons croisent l’orbite de la Terre, et l’intérêt grandissant qu’on leur porte est lié à la crainte de les voir entrer en collision avec celle-ci.
Ces croiseurs sont appelés ECA ou NEO en anglais.

L’agence spatiale européenne (ESA) a entamé en 2004 un projet à long terme de protection de la Terre contre les géocroiseurs. Voir Services publics dans le monde.

Les astéroïdes troyens sont situés sur l’orbite d’une planète, aux deux points de Lagrange, L et L. On en compte au 3 septembre 2011.

La quasi-totalité des Troyens sont sur l’orbite de Jupiter. Mars possède sept astéroïdes troyens, Neptune neuf, et la Terre un seul (, découvert en 2010 par le télescope spatial WISE).

À ce jour les autres planètes ne semblent pas en posséder, sans doute en raison de l’influence soit du Soleil, soit des planètes voisines susceptibles de perturber les points de Lagrange, ou n'ont pas encore été découverts.

La ceinture de Kuiper, située au-delà de l'orbite de Neptune, semble être potentiellement la plus grande concentration de petits corps du Système solaire. Au , transneptuniens sont dénombrés par le .

Le premier membre découvert de cette ceinture fut Pluton, longtemps le seul objet connu de cette zone. Son unicité et sa taille supposée similaire à celle de la Terre ont fait qu'il a longtemps été considéré comme planète. Néanmoins, la confirmation en 1978 de son compagnon, Charon, a permis de définitivement savoir que Pluton était bien plus petit qu'imaginé. Il faudra ensuite attendre 1992 pour qu'un autre objet de Kuiper soit découvert : ce sera (15760) Albion, classé par la suite dans la catégorie des cubewanos ou objets classiques de la ceinture de Kuiper. La découverte de ce corps attira l’attention des astronomes sur les objets «transneptuniens», leur laissant dire que comme prédit il devait en exister en grandes quantités. Aujourd'hui, plusieurs membres de la ceinture de Kuiper, de taille comparable à celle de Pluton ou de Charon sont connus.

Le plus grand objet identifié dans la ceinture de Kuiper est Pluton .

Cette ceinture serait la source de près de la moitié des comètes qui sillonnent le Système solaire.

En 2005 fut découvert un objet épars dont la taille était initialement estimée à près de . Cet objet, depuis lors nommé Éris et dont la taille a été aujourd'hui réévaluée à (soit seulement une vingtaine de kilomètres de plus que Pluton), a relancé le débat sur la démarcation entre les "gros objets" et les planètes du Système solaire. Ainsi, en août 2006, l’Union astronomique internationale décide de créer le statut de planète naine, aussitôt décerné à Pluton qui perd celui de planète, à , tous deux transneptuniens, et à , le plus gros astéroïde de la ceinture principale. D’autres objets de la ceinture de Kuiper sont candidats à ce nouveau statut.

Le nuage de Hills, parfois nommé nuage d'Oort interne, serait un disque de débris situé entre 100 à et à astronomiques du Soleil. Le nuage d’Oort (ˈɔrt), aussi appelé le nuage d’Öpik-Oort (ˈøpik), est un vaste ensemble sphérique hypothétique de corps situé à environ du Soleil (). Ces deux structures sont donc situées bien au-delà de l’orbite des planètes et de la ceinture de Kuiper. La limite externe du nuage d’Oort, qui formerait la frontière gravitationnelle du Système solaire, se situerait à plus d’un millier de fois la distance séparant le Soleil et Pluton, soit environ une année-lumière et le quart de la distance à Proxima du Centaure, l’étoile la plus proche du Soleil. Il n'est d'ailleurs pas exclu qu'il existe un continuum entre le nuage d'Oort "solaire" et une structure similaire autour du système Alpha Centauri.

Les Centaures sont des astéroïdes qui naviguent autour du Soleil entre les orbites des planètes géantes (au nombre de 319 au 3 septembre 2011, en incluant certains objets épars).
Le premier qui fut découvert est , en 1977.
On suppose généralement que ce sont des astéroïdes ou des comètes, provenant probablement de la ceinture de Kuiper, qui ont été éjectés de leurs propres orbites.

Outre les géocroiseurs, il existe d’autres regroupements d’astéroïdes, suivant qu’ils croisent l’orbite d’une autre planète du Système solaire. Certains astéroïdes peuvent faire partie de plusieurs regroupements. Il est à noter que si le terme de géocroiseur est répandu, ceux qui suivent sont très rarement employés.


Les avancées techniques aidant, dès 1980, le nombre d’objets découverts augmenta considérablement et des corps très massifs, de la taille de Pluton, alors considérée comme une planète, furent observés. Les scientifiques en vinrent alors à se demander comment différencier une planète d’un gros astéroïde.

Rappelons aussi que, selon la théorie de Laplace (astronome en 1796), les planètes et le Soleil seraient nées simultanément d’un nuage de gaz et de poussières en rotation. Issus de ce nuage, une multitude de planétoïdes seraient le résultat d’une histoire mouvementée, caractérisée par une succession de processus antinomiques d’accrétion et de collisions. Astéroïdes et planètes ayant été formés à partir de la même matrice protoplanétaire, on peut se demander sur quels critères physiques s’appuyer pour les différencier. La très grande majorité des astéroïdes est de forme très irrégulière, ce qui contraste avec les formes quasi-sphériques des planètes ; cependant, les très gros astéroïdes, tels Cérès, sont également quasi-sphériques. La nature de la surface n’entre pas non plus en compte dans la différenciation. La différenciation se fait essentiellement par la taille :

Un astéroïde se définit implicitement comme un corps n’excédant pas de diamètre et gravitant autour du Soleil. Ce diamètre correspond approximativement à celui de Cérès, le plus gros astéroïde de la ceinture principale.

Toutefois, de nouveaux objets découverts ont défrayé la chronique : , , , , et . Détectés soit sur des orbites similaires à Pluton, soit au-delà, ces objets ont des tailles comprises entre et et se situent à la frontière entre planètes et astéroïdes.

En août 2006, l’Union astronomique internationale, a revu la notion de planète et défini une nouvelle classe d’objets, les planètes naines. Ainsi, Pluton, Éris et Cérès furent classés dans la catégorie « planète naine », bien que Cérès continue à être également considérée comme un astéroïde.

Astéroïdes et comètes sont des petits corps du Système solaire. Les premiers ne présentent pas d’activités lorsqu’ils passent au périhélie (formation d’une chevelure ou d’une queue). Une minorité a cependant été observée avec une activité cométaire, comme le Centaure (2060) Chiron ou 133P/Elst-Pizarro dans la ceinture principale. Ces objets, qualifiés d'astéroïdes actifs, sont catalogués à la fois comme astéroïde et comme comète.

Les astéroïdes appartenant à la catégorie des damocloïdes sont des objets possédant une orbite à longue période et une forte excentricité tout comme les comètes périodiques. Il s’agit peut-être de noyaux cométaires devenus inactifs.

Selon une étude publiée dans la revue "Nature" en 2009, 20 % des objets de la ceinture principale seraient des noyaux cométaires. Ces noyaux, provenant de la ceinture de Kuiper, auraient été propulsés vers le Système solaire interne lors du grand bombardement tardif provoqué notamment par la migration de Neptune.

La composition des astéroïdes est évaluée d’après leur spectre optique mesurant la lumière réfléchie, qui correspond à la composition de leur surface. Celle des météorites est connue avec l'analyse des fragments retrouvés sur Terre.

Le système classique de classification spectrale des astéroïdes, élaboré en 1975, les classe selon un système basé sur leur couleur, leur albédo et leur spectre optique. Ces propriétés étaient censées correspondre à la composition de leur surface. Il faut noter, cependant, que certains types sont plus facilement détectables que d'autres. Ainsi, ce n'est pas parce que la proportion d'astéroïdes d'un type donné est plus importante qu'ils sont effectivement plus nombreux. Il existe des systèmes de classification plus récents, dont deux se démarquent : Tholen et SMASS.

À l'origine, la classification des astéroïdes se basait sur des suppositions au sujet de leur composition :

Ceci a porté à confusion, car le type spectral d'un astéroïde ne garantit pas sa composition.

Les astronomes doivent conventionnellement communiquer leurs observations d'astéroïdes nouveaux au Centre des planètes mineures . Le risque est identifié et fait l'objet d'une remédiation autant que possible : lire "stratégies de déviation des astéroïdes".

Lorsqu’un astéroïde ou un fragment d’astéroïde pénètre dans l’atmosphère de la Terre, les frottements avec cette dernière provoquent sa combustion. Si l’objet est assez volumineux, cette combustion n’est pas complète et il percute alors la surface de la Terre.

En 2010, plus de et comètes ont été détectés dans un rayon de de kilomètres autour du Soleil, assez près de notre planète pour que les astronomes les classent dans la catégorie des objets proches de la terre (, NEO) ou géocroiseurs. Ceux qui mesurent plus de de large et passent à moins de de kilomètres de l’orbite de la Terre sont considérés comme dangereux. Au 30 avril 2008, les astronomes avaient catalogué plus de célestes de ce type, dont , un astéroïde qui passera à de la terre en 2029. La probabilité qu’un de ces objets dangereux entre en collision avec la Terre est quasi nulle à l’échelle du temps humain, mais quasi certaine à l’échelle du temps cosmique, le phénomène d’accrétion n’étant nullement terminé. C’est la raison pour laquelle des observateurs surveillent constamment leur position — recalculant leur orbite et les risques d’impact qu’ils présentent — et scrutent les régions voisines de l’espace à la recherche de nouvelles menaces.

Par exemple l’observatoire de Remanzacco a signalé que le , à TU, un astéroïde d’un diamètre compris entre 5 et était passé à de la Terre. Cet événement se reproduit, en moyenne, une fois tous les six ans d’après la NASA.






Dans les années 2010, des projets d'exploitation minière des astéroïdes sont lancés par des sociétés privées du secteur spatial, Planetary Resources et Deep Space Industries. Les astéroïdes sont en effet riches en matériaux précieux, tels les métaux lourds et les terres rares, présents sur leur surface car ces corps sont trop petits pour avoir subi la différenciation planétaire : la valeur commerciale d'un km d'astéroïde, hors frais d'exploitation, est estimée à 5000 milliards d'euros. La NASA a également pour ambition de capturer un petit astéroïde (de 7 à de diamètre, avec un poids maximal de ) et de le mettre en orbite stable autour de la Lune. Les faisabilités et le coût de ces projets font l'objet de débats, seule la sonde Hayabusa ayant réussi en 2010 à ramener quelques poussières de l'astéroïde Itokawa.

Le 22 janvier 2014, l'Agence spatiale européenne a annoncé la première détection certaine de vapeur d'eau dans l'atmosphère de Cérès, le plus grand objet de la ceinture d'astéroïdes.
La détection a été réalisée par des observations en du télescope spatial Herschel.
La découverte est particulière parce qu'on s'attend à ce que les comètes, et non les astéroïdes, comportent des queues et des jets. Selon l'un des scientifiques, « la délimitation entre les comètes et les astéroïdes devient de plus en plus floue ».

Listes diverses
Les dix plus gros astéroïdes de la ceinture d'astéroïdes (+ diamètre ou plus grande dimension)
Astéroïdes visités par des sondes spatiales (+ dates des visites + noms des sondes + nature de la mission)


</doc>
<doc id="272" url="https://fr.wikipedia.org/wiki?curid=272" title="Albanais">
Albanais

L'albanais ("shqip" en albanais) est une langue qui constitue à elle seule une branche de la famille des langues indo-européennes

Il est parlé par sept millions de personnes environ, dont la moitié en Albanie.
L'albanais se subdivise en deux dialectes principaux : le guègue, au nord du fleuve Shkumbin, et le tosque, au sud.

On a longtemps considéré l'albanais comme une langue indo-européenne isolée, du fait que la langue antique dont il descend nous était inconnue et que tant sa phonologie que sa grammaire sont à un stade d'évolution atypique de l'indo-européen.

La plupart des linguistes considèrent aujourd'hui que l'albanais appartient à l'ensemble thraco-illyrien des langues indo-européennes. Cet ensemble est géographique plutôt que linguistique, mais l'albanais, langue satem, comprend des éléments issus des deux branches, illyrienne (« satem ») et thrace (« centum »), langues mortes très peu documentées qui ne permettent pas que l'on détermine avec précision sa position dans l'ensemble. L'existence d'un lexique commun à l'aroumain, au roumain (langues romanes orientales) et à l'albanais, ainsi que la toponymie côtière de l'Albanie, ont fait supposer une origine partiellement thrace (peut-être carpienne) des ancêtres des Albanais, qui auraient initialement évolué plus à l'est qu'aujourd'hui, dans les actuelles république de Macédoine et Serbie méridionale, au contact des aires linguistiques illyrienne et thrace.

Pour déterminer les liens que l'albanais entretient avec les autres langues indo-européennes, il a fallu reconstruire l'histoire de son phonétisme, afin d'isoler son fond lexical ancien des emprunts aux langues voisines. Sur cette base, on a pu clairement démontrer le caractère indo-européen particulier de l'albanais (voir notamment les travaux de Walter Porzig, Eqrem Çabej, Eric Hamp, Petro Zheji, etc.).

L'albanais a de nombreuses caractéristiques communes avec les langues géographiquement voisines, avec lesquelles il forme l'union linguistique balkanique.

Comme en grec, certains termes sont pré-indoeuropéens comme "kok" (« tête »), "sukë" (« colline »), "derr" (« cochon »), que le paléolinguiste et bascologue Michel Morvan rapproche du pré-occitan "kuk, suk" (« hauteur ») ou du basque "zerri" (« porc »).

Trois millions et demi d'albanophones vivent en Albanie. Les autres locuteurs se trouvent au Kosovo, en Serbie dans la vallée de Preševo, en Macédoine, en Turquie, au Monténégro, en Italie et en Grèce.

En Grèce, les Arvanites sont des albanophones chrétiens orthodoxes et parlaient un dialecte tosque, mais tous parlent le grec. En Turquie, on estime le nombre d'albanophones d’origine à près de 3 millions, mais la plupart d'entre eux parlent maintenant le turc. Il s'agit d'albanophones musulmans originaires de Macédoine, du Kosovo ou de la Grèce, qui ont été déplacés de force en Turquie après le traité de Lausanne et selon les dispositions de celui-ci. On les retrouve principalement à Istanbul, Bursa, Izmir et sur les côtes de la mer Égée.

On trouve également une communauté albanophone catholique répartie dans une quarantaine de villages en Italie du sud et en Sicile, les Arbëresh, qui descendent des Albanais émigrés au (à la suite de l'invasion des Ottomans dans la région balkanique).

Il est enfin parlé par quelques petits groupes en Bulgarie, en Roumanie, en Ukraine, ainsi que par une diaspora nombreuse aux États-Unis, en Suisse, en Allemagne et en Australie.

L'albanais est langue officielle en Albanie, au Kosovo et en Macédoine. En Italie, la langue et la culture albanaises sont protégées (statut de minorité linguistique).

L'albanais a été interdit durant l'occupation ottomane.

Les plus anciens textes conservés datent du ; des textes découverts en 1990 dans les archives du Vatican datent plus précisément de 1210 ; l'auteur est Theodor Shkodrani. La langue écrite standard actuelle, en caractères de l'alphabet latin, a été élaborée sur la base du dialecte tosque.

La transcription suit les usages de l'alphabet phonétique international.
Cet alphabet est utilisé officiellement depuis la normalisation de 1908. Il utilise des digrammes et deux diacritiques, le tréma ainsi que la cédille (on peut aussi compter l'accent circonflexe servant au guègue, souvent remplacé par un tilde dans des ouvrages de linguistique). Les digrammes et les lettres diacritées comptent pour des graphèmes indépendants et non comme des variantes (ce qui est le cas pour "é", "è", "ê" et "ë" en français, variantes de "e" pour le classement alphabétique). L'albanais était noté auparavant par divers alphabets originaux, comme l’écriture de Todhri, l'elbasan, le buthakukye et l'argyrokastron, le grec, le cyrillique ou un alphabet latin modifié différent de celui qui est utilisé de nos jours.

L'alphabet actuel est presque phonologique : dans l'absolu, toutes les lettres se lisent et toujours de la même manière, à l'exception du "e" caduc. On a donné dans le tableau ci-dessus les réalisations des lettres dans la prononciation standard. Il y a des variantes dialectales.

L'alphabet albanais compte 36 lettres : sept voyelles (A, E, Ë, I, O, U, Y) et vingt-neuf consonnes (B, C, Ç, D, Dh, F, G, Gj, H, J, K, L, Ll, M, N, Nj, O, P, Q, R, Rr, S, Sh, T, Th, U, V, X, Xh, Y, Z, Zh).

Si le guègue possède encore des voyelles nasalisées, notées par un circonflexe au dessus de la voyelle correspondante, le tosque les a perdues. La représentation du système vocalique albanais est alors assez simple.

La voyelle "ë" [ə] (comme le e de « je ») est souvent omise dans la prononciation lorsqu'elle est en position finale et atone après une seule consonne : [- accent tonique] > Ø / C_#.

La transcription des phonèmes de l'albanais selon la normalisation mise en place en 1908 peut sembler assez déroutante. En effet, plusieurs traditions orthographiques sont en jeu :

La palatalisation des consonnes est notée par "-j" subséquent ("j" seul notant ) : "gj" = (comparable au hongrois "gy" dans "magyar") et "nj" = (français "gn" dans "gnon"). Quand il faut représenter [gj] et [nj], on remplace le "j" par "i", afin d'éviter l'ambiguïté : [gja] s'écrit donc "gia", "gja" notant déjà [ɟa].

La sourde palatale (hongrois "ty") est rendue historiquement par "q".
La spirantisation peut être notée par un "-h" subséquent, ce qui est le cas pour "dh" (anglais "th" dans "then") et "th" (anglais "th" dans "thin"), mais pas pour "sh" (français "ch" dans "chien"), "xh" (français "dj" dans "Djibouti") ni "zh" (français "j" dans "je"). Dans ce cas, "-h" indique le caractère postalvéolaire des consonnes.

Les affriquées sifflantes sont rendues par "c", [ts] (français "ts" dans "tsar"), pour la sourde, et "x", [dz] (italien "z" dans "zero"), pour la sonore ; les affriquées chuintantes par "ç", (comme "tch" dans "tchèque"), et "xh" .

Il existe encore deux digrammes à retenir : "ll" (/l/ "sombre" de l'anglais dans "full") et "rr" (/r/ roulé à plusieurs battements comme en espagnol "perro"), qui s'opposent à "l" et "r" (/r/ battu bref comme en espagnol dans "pero").

On peut trouver une séquence "ng-" à l'initiale, qui n'est pas un digramme. Le jeu de la variation combinatoire fait qu'une telle séquence se prononce vraisemblablement [ŋg] (comme "ng" en anglais "finger").





</doc>
<doc id="273" url="https://fr.wikipedia.org/wiki?curid=273" title="Arménien">
Arménien

L'arménien est une langue qui constitue à elle seule une branche de la famille des langues indo-européennes, étant seule de cette famille à être plus agglutinante que flexionnelle. L'arménien classique (ou "Grabar" : en arménien Գրաբար, littéralement « langue écrite ») est attesté à partir du et véhicule une riche littérature théologique, historique, poétique, mystique et épique. Aujourd'hui coexistent l'arménien oriental, langue officielle de la république d'Arménie, parlée par les habitants de l'Arménie et par les communautés arméniennes d'Iran et de Russie, et l'arménien occidental, parlé par la diaspora arménienne. Le nombre total de locuteurs est évalué à plus de sept millions dont un peu plus de trois millions en Arménie.

L'arménien présente des ressemblances nombreuses avec le grec ancien (parallèles étymologiques, utilisation de l'augment, traitement particulier des laryngales de l'indo-européen, etc.), comme l'a souligné le linguiste français Antoine Meillet. D'autre part, les consonnes du proto-arménien ont connu la première mutation consonantique (loi de Grimm), ce qui le rapproche plus des langues germaniques pour sa physionomie phonologique.

L'arménien s'écrit au moyen d'un alphabet spécifique créé au .

La langue arménienne appartient à la famille des langues indo-européennes tout comme le français. Voici une courte liste de mots ayant une racine commune avec le latin et le grec.

Comme toutes les familles de langues, l'arménien est défini au sein de la famille indo-européenne par un certain nombre d'évolutions phonétiques intervenues dans la préhistoire de la langue. Parmi ces dernières, on pose ainsi en général :

Par la suite, on observe également une série de changements phonétiques survenus entre la naissance du proto-arménien et les premiers textes en arménien classique :

Enfin, l'arménien classique a naturellement connu une certaine évolution avant de donner naissance à l'arménien moderne. Au niveau phonétique, on note notamment :
Il est enfin à noter que la plupart des spécialistes de l'arménien classique, surtout en République d'Arménie, adoptent en permanence la prononciation moderne à la lecture des textes classiques ; ce qui n'est pas sans rappeler la prononciation italiénisante du latin en Italie ou la prononciation moderne du grec ancien en Grèce.

On trouvera ci-après quelques caractéristiques grammaticales générales de l'arménien.

L'ordre des mots est en général de type SVO (sujet - verbe - objet) mais reste assez libre. L'attribut se place entre le sujet et le verbe.

Le double-point [ : ] équivaut au point final du français, mais concerne aussi les phrases exclamatives ou interrogatives. Le point [ . ] équivaut au point-virgule ou au deux-points du français. La virgule [ , ] s'utilise comme en français. Le "bout" [ ' ] se place devant un mot ou un groupe de mots qu'il met en relief. Les signes d'interrogation et d'exclamation, qui ont des formes propres, se placent sur la dernière syllabe du mot concerné. Le "chécht" se place sur la dernière syllabe d'un mot mis en apostrophe ou en relief.

L'accent tonique se trouve toujours sur la dernière syllabe du mot, avant le "e" final éventuel.

Il n'y a pas de genre grammatical féminin ou masculin en arménien. La déclinaison des noms comprend 6 à 8 cas grammaticaux, selon les points de vue :

Seuls le cas direct et le datif peuvent avoir l'article défini en fin de mot ; l'article défini s'applique également aux noms propres.

Il existe sept types de déclinaisons, qui se partagent en deux catégories :

Deux noms ont une déclinaison particulière : "aghtchik" (« fille ») et "sér" (« amour »).

L'arménien utilise des prépositions, mais aussi un grand nombre de postpositions ; les unes et les autres régissent des cas particuliers.

L'adjectif ne s'accorde pas avec le nom.

Il existe trois groupes de verbes :
L'arménien oriental a fusionné les groupes I et II. Il n'utilise plus le suffixe [-il] ; ("khosil"), par exemple, devient donc ("khosél"). 

Le pronom personnel sujet n'est pas indispensable devant le verbe.

L'arménien connaît les modes personnels : indicatif, subjonctif, obligatif et impératif, plus l'infinitif, le participe (passé, présent et futur) et le concomitant, qui exprime une action accessoire à celle du verbe principal.

Les temps sont voisins de ceux du français. Il n'existe pas de passé antérieur ni de futur antérieur, mais on trouve un passé et un futur de probabilité. L'obligatif présente un passé et un parfait. Les six personnes sont les mêmes qu'en français.

Les temps composés se forment avec le verbe auxiliaire "ém" (« je suis »). L'auxiliaire suit normalement la base, mais il la précède si le verbe est négatif ou si l'on veut mettre en relief un terme de la phrase situé avant le verbe. Il existe deux autres verbes « être », l'un signifiant « être habituellement » et dont les formes complètent celles de "ém", l'autre signifiant « exister », « être (là) ».

Le causatif est marqué par un suffixe placé avant la terminaison de l'infinitif, et le passif par l'insertion d'un [v] entre le radical et la désinence.

Le verbe s'accorde en personne et en nombre avec le sujet ; dans les temps composés, c'est l'auxiliaire qui s'accorde.

D'une façon générale, l'arménien préfère le participe, l'infinitif ou le concomitant aux propositions relatives ou conjonctives.

En 1909, le linguiste arménien Hratchia Adjarian a proposé dans sa "Classification des dialectes arméniens" une répartition des dialectes arméniens en trois branches :


Les plus grandes figures de la linguistique arménienne (par ordre chronologique) :

M. Leroy et F. Mawet, "La place de l'arménien dans les langues indo-européennes", éd. Peeters, Louvain, 1986 .



</doc>
<doc id="274" url="https://fr.wikipedia.org/wiki?curid=274" title="Alfred de Musset">
Alfred de Musset

Alfred de Musset est un poète et dramaturge français de la période romantique, né le à Paris, où il est mort le . 

Lycéen brillant, il s’intéresse ensuite, entre autres, au droit et à la médecine, mais abandonne vite ses études supérieures pour se consacrer à la littérature à partir . Il fréquente les poètes du Cénacle de Charles Nodier et publie à "Contes d'Espagne et d'Italie", son premier recueil poétique. Il commence alors à mener une vie de « dandy débauché », marquée par sa liaison avec George Sand, tout en écrivant des pièces de théâtre : "À quoi rêvent les jeunes filles ?" , "Les Caprices de Marianne" , puis le drame romantique "Lorenzaccio", "Fantasio" et "On ne badine pas avec l'amour". Il publie parallèlement des poèmes tourmentés comme "" et "la Nuit de décembre" , puis "La Nuit d'août "(1836) "La Nuit d'octobre" (1837), et un roman autobiographique "La Confession d'un enfant du siècle" .

Dépressif et alcoolique, il écrit de moins en moins après l'âge de ; on peut cependant relever les poèmes "Tristesse", "Une soirée perdue" (1840), "Souvenir" et diverses nouvelles ("Histoire d'un merle blanc", 1842, le livre de chevet de Lucie Merle). Il reçoit la Légion d'honneur et est élu à l'Académie française . Il écrit des pièces de commande pour . Il meurt à et est enterré dans la discrétion au cimetière du Père-Lachaise.

Redécouvert au , notamment dans le cadre du TNP de Jean Vilar et Gérard Philipe, Alfred de Musset est désormais considéré comme un des grands écrivains romantiques français, dont le théâtre et la poésie lyrique montrent une sensibilité extrême, une interrogation sur la pureté et la débauche ("Gamiani ou Deux nuits d'excès", 1833), une exaltation de l'amour et une expression sincère de la douleur. Sincérité qui renvoie à sa vie tumultueuse, qu'illustre emblématiquement sa relation avec George Sand.

Né sous le Premier Empire, le 11 décembre 1810, dans la rue des Noyers (incorporée au boulevard Saint-Germain au milieu du ), Alfred de Musset appartient à une famille aristocratique, affectueuse et cultivée, lui ayant transmis le goût des lettres et des arts. Il prétend avoir pour arrière-grand-tante Jeanne d'Arc (son ancêtre Denis de Musset ayant épousé Catherine du Lys) et être cousin de la branche cousine de Joachim du Bellay. Une de ses arrière-grand-mères est Marguerite Angélique du Bellay, femme de Charles-Antoine de Musset.

Son père, Victor-Donatien de Musset-Pathay, est un haut fonctionnaire, chef de bureau au ministère de la Guerre, et un homme de lettres né le près de Vendôme; aristocrate libéral, il a épousé le 2 juillet 1801 Edmée-Claudette-Christine Guyot-des-Herbiers, née le , fille de Claude-Antoine Guyot-Des-Herbiers. Le couple a eu quatre enfants : Paul-Edme, né le , Louise-Jenny, née et morte en 1805, Alfred, né le et Charlotte-Amélie-Hermine, née le .

Son grand-père était poète, et son père était un spécialiste de Rousseau, dont il édita les œuvres. La figure de Rousseau joua en l'occurrence un rôle essentiel dans l'œuvre du poète. Il lui rendit hommage à plusieurs reprises, attaquant au contraire violemment Voltaire, l'adversaire de Rousseau. Son parrain, chez qui il passait des vacances dans la Sarthe au château de Cogners, était l'écrivain Musset de Cogners. L'histoire veut que lors d'un de ses séjours dans le château de son parrain, la vue qu'il avait depuis sa chambre sur le clocher de l’église de Cogners lui ait inspiré la très célèbre "Ballade à la Lune". Par ailleurs, il retranscrivit toute la fraîcheur du calme et de l'atmosphère de Cogners dans ses deux pièces de théâtre "On ne badine pas avec l'amour" et "Margot". En octobre 1819, alors qu'il n'a pas encore neuf ans, il est inscrit en classe de sixième au collège Henri-IV – on y trouve encore une statue du poète –, où il a pour condisciple et ami un prince du sang, le duc de Chartres, fils du duc d'Orléans, et obtient en 1827 le deuxième prix de dissertation latine au Concours général. Après son baccalauréat, il suit des études, vite abandonnées, de médecine, de droit et de peinture jusqu'en 1829, mais il s'intéresse surtout à la littérature. Le 31 août 1828 paraît à Dijon, dans "Le Provincial", le journal d'Aloysius Bertrand, "Un rêve", ballade signée « ADM ». La même année, il publie "L'Anglais mangeur d'opium", une traduction française peu fidèle des "Confessions d'un mangeur d'opium anglais" de Thomas de Quincey.

Grâce à Paul Foucher, beau-frère de Victor Hugo, il fréquente dès l'âge de 17 ans le « Cénacle », ainsi que le salon de Charles Nodier à la Bibliothèque de l'Arsenal. Il sympathise alors avec Sainte-Beuve et Vigny, et se refuse à aduler le « maître » Victor Hugo. Il moquera notamment les promenades nocturnes du « cénacle » sur les tours de Notre-Dame. Il commence alors à mener une vie de « dandy débauché ». En décembre 1830, il écrit sa première pièce de théâtre (seul ce genre littéraire apporte notoriété et beaucoup d'argent) : sa comédie "La Nuit Vénitienne" est un échec accablant (comédie arrêtée après deux représentations au théâtre de l'Odéon, notamment à cause des sifflets du public et du ridicule subi par la comédienne principale dont la robe est tachée par la peinture des décors pas encore sèche) qui le fait renoncer à la scène pour longtemps. Il choisit dès lors de publier des pièces dans la "Revue des deux Mondes", avant de les regrouper en volume sous le titre explicite "Un Spectacle dans un fauteuil". Il publie ainsi une comédie, "À quoi rêvent les jeunes filles ?" en 1832, puis "Les Caprices de Marianne" en 1833. Il écrit ensuite son chef-d'œuvre, un drame romantique, "Lorenzaccio" en 1834 (la pièce ne sera représentée qu'en 1896) après sa liaison houleuse avec George Sand et donne la même année "Fantasio" et "On ne badine pas avec l'amour". Il publie parallèlement des poèmes tourmentés comme "la Nuit de mai "et "la Nuit de décembre" en 1835, puis "La Nuit d'août "(1836) "La Nuit d'octobre" (1837), et un roman autobiographique "La Confession d'un enfant du siècle" en 1836. Il fait preuve d'une grande aisance d'écriture, se comportant comme un virtuose de la jeune poésie. Il publie en 1829 son premier recueil poétique, les "Contes d'Espagne et d'Italie", salués par Pouchkine. Il est d'ailleurs le seul poète français de son temps que le poète russe apprécie vraiment. En 1830, à 20 ans, sa notoriété littéraire naissante s'accompagne déjà d'une réputation sulfureuse alimentée par son côté dandy et ses débauches répétées dans la société des demi-mondaines parisiennes. La même année, la révolution et les journées des Trois Glorieuses donnent le trône au duc d'Orléans et son ancien condisciple, le duc de Chartres, devient prince royal. À l'âge de 22 ans, le 8 avril 1832, Musset est anéanti par la mort de son père, dont il était très proche, victime de l'épidémie de choléra. Cet événement va décider de la carrière littéraire que Musset choisit alors d'entamer. Musset tente sa chance au théâtre. Mais après l'échec de "La Nuit Vénitienne ou les noces de Laurette", comédie en un acte donnée le à l'Odéon, l'auteur dit , comme il l'écrit à Prosper Chalas. Cet éloignement durera dix-sept ans, jusqu'au succès d'Un Caprice, comédie en un acte donnée au Théâtre-Français le 27 novembre 1847. À cette époque, devenu alcoolique, il pouvait y revenir plus serein.

S'il refuse la scène, Musset n'en garde pas moins un goût très vif du théâtre. Il choisit de publier des pièces dans la "Revue des deux Mondes" avant de les regrouper en volume sous le titre explicite « Un Spectacle dans un fauteuil ». La première livraison, en décembre 1832 se compose de trois poèmes, d'un drame, "La Coupe et les Lèvres", d'une comédie, "À quoi rêvent les jeunes filles ?" et d'un conte oriental, "Namouna". Musset exprime déjà dans ce recueil la douloureuse morbidité qui lie débauche et pureté, dans son œuvre.

En novembre 1833, il part pour Venise, en compagnie de George Sand, dont il a fait la connaissance lors d'un dîner donné aux collaborateurs de la "Revue des deux Mondes" le 19 juin. Ce voyage lui inspire "Lorenzaccio", considéré comme le chef-d'œuvre du drame romantique, qu'il écrit en 1834. Mais Musset fréquente les grisettes pendant que George Sand est malade de la dysenterie et lorsqu'elle est guérie, Musset tombe malade à son tour, George Sand devenant alors la maîtresse de son médecin, Pietro Pagello. De retour à Paris le 12 avril 1834, il publie la deuxième livraison de son « Spectacle dans un fauteuil », comprenant "Les Caprices de Marianne", parue en revue en 1833, "Lorenzaccio", inédit, "André del Sarto" (1833), "Fantasio" (1834), "On ne badine pas avec l'Amour" (1834) et "La Nuit vénitienne", inédit depuis l'échec de l'Odéon (faux : pièce de théâtre antérieure à son voyage à Venise (jouée en 1830) rappelle son voyage à Venise est en 1833. Et c'est d'ailleurs cette pièce de théâtre qui a été l'échec qui l'amène à créer "Le théâtre dans un fauteuil""). "Le Chandelier" paraît dans la "Revue des deux Mondes" en 1835, "Il ne faut jurer de rien" en 1836 et "Un caprice" en 1837. Il écrit également des nouvelles en prose et "La Confession d'un enfant du siècle", autobiographie à peine déguisée dédiée à George Sand, dans laquelle il transpose les souffrances endurées.

De 1835 à 1837, Musset compose son chef-d'œuvre lyrique, "Les Nuits", rivales de celles d'Edward Young, James Hervey ou Novalis. Ces quatre poèmes : les Nuits de mai, d'août, d'octobre, de décembre – sont construits autour des thèmes imbriqués de la douleur, de l'amour et de l'inspiration. Très sentimentaux, ils sont désormais considérés comme l'une des œuvres les plus représentatives du romantisme français.

Après sa séparation définitive avec George Sand, en mars 1835, il tombe amoureux de Caroline Jaubert, l'épouse d'un juriste et la sœur d'Edmond d'Alton-Shée, pair de France et son ami, qu'il appelle la petite fée blonde et avec laquelle il a une liaison qui dure trois semaines, avant de reprendre fin 1835 ou début 1836. Hôte assidu de son salon, il en fera sa « marraine » et sa confidente, notamment tout au long de leur correspondance, qui s'étale sur vingt-deux années. C'est chez elle qu'il fait la connaissance, en mars 1837, d'Aimée-Irène d'Alton, sa cousine, avec laquelle il entame une liaison heureuse et durable. Elle lui propose même de se marier avec lui. Abandonnée par Musset pour Pauline Garcia, qui se refuse à lui, elle épousera son frère Paul le 23 mai 1861. Puis il rencontre, le 29 mai 1839, à la sortie du Théâtre-Français, Rachel, qui l'emmène souper chez elle, et avec laquelle il a une brève liaison en juin. En 1842, la princesse Christine de Belgiojoso, amie de Jaubert, lui inspire une passion malheureuse. De 1848 à 1850, il a une liaison avec Louise-Rosalie Ross, dite , qui avait découvert "Un caprice" dans une traduction russe de Alexandra Mikhaïlovna Karatiguine à Saint-Pétersbourg, et l'avait créé au théâtre Michel, le théâtre français de Saint-Pétersbourg, en 1843, où elle joue . Ensuite elle joue la pièce au Théâtre-Français en 1847. C'est grâce à cette pièce que Musset rencontre enfin le succès au théâtre, Théophile Gautier la qualifiant dans "La Presse" En 1852, Louise Colet, qui est la maîtresse de Flaubert, a, quelque, temps une liaison avec Musset.

Grâce à l'amitié du duc d'Orléans, il est nommé bibliothécaire du ministère de l'Intérieur le 19 octobre 1838. Le duc d'Orléans meurt accidentellement en 1842.

Après la Révolution française de 1848, ses liens avec la monarchie de Juillet lui valent d'être révoqué de ses fonctions par le nouveau ministre Ledru-Rollin, le 5 mai 1848. Puis, sous le Second Empire, il devient bibliothécaire du ministère de l'Instruction publique, avec des appointements de trois mille francs, le 18 mars 1853.

Nommé chevalier de la Légion d'honneur le 24 avril 1845, en même temps que Balzac, il est élu à l'Académie française le 12 février 1852 au siège du baron Dupaty, après deux échecs en 1848 et 1850. La réception a lieu le 27 mai suivant. Il fête le même jour sa nomination comme chancelier perpétuel au bordel et ses débordements alcooliques lui valent, de la part d'Eugène de Mirecourt, la formule de « chancelant perpétuel » au « verre qui tremble ». Ces crises convulsives, associées à des troubles neurologiques, font penser à une syphilis au stade tertiaire qu'il aurait contractée dans un bordel à 15 ans

De santé fragile (malformation cardiaque, voir le signe de Musset), mais surtout en proie à l'alcoolisme, à l'oisiveté et à la débauche, il meurt de la tuberculose le 2 mai 1857 à 3h15 du matin à son domicile du 6 rue du Mont-Thabor - Paris 1, quelque peu oublié. Cependant Lamartine, Mérimée, Vigny et Théophile Gautier assistent à ses obsèques en l'église Saint-Roch. On n’a révélé la mort de son fils à sa mère, qui était partie vivre chez sa fille Hermine à Angers, qu’après son enterrement.

En 1859, George Sand publie "Elle et Lui", roman épistolaire d'inspiration autobiographique. Elle y révèle en particulier l’héautoscopie dont souffrait le génie, forme de dépersonnalisation qui explique le caractère hallucinatoire de "La Nuit de décembre". Jugeant son frère calomnié par l'ensemble du roman, Paul de Musset lui réplique, six mois plus tard, en faisant paraître "Lui et Elle".

Le poète est inhumé à Paris, au cimetière du Père Lachaise, où son monument funéraire se dresse avenue principale.
Sur la pierre sont gravés les six octosyllabes de son élégie "Lucie" :

et sur la face arrière, le poème "Rappelle-toi" :






Redécouvert au , Alfred de Musset est désormais considéré comme un des grands écrivains romantiques français, dont le théâtre et la poésie lyrique montrent une sensibilité extrême, une interrogation sur la pureté et la débauche, une exaltation de l'amour et une expression sincère de la douleur. Sincérité qui renvoie à sa vie tumultueuse qu'illustre emblématiquement sa relation avec George Sand.

Son frère aîné Paul de Musset jouera un grand rôle dans la redécouverte de l'œuvre d'Alfred de Musset, par la rédaction de biographies et la réédition d'un grand nombre de ses œuvres, comme "La Mouche" ou "les Caprices de Marianne".

L'un des textes de son recueil "Poésies posthumes", intitulé "Nous venions de voir le taureau", a été mis en musique par Léo Delibes en 1874 sous le nom "Les Filles de Cadix".

Son drame "La Coupe et les Lèvres" a été à la base de l'opéra "Edgar" de Giacomo Puccini (1889).

En 1999, la liaison entre Alfred de Musset et George Sand a fait l'objet d'une adaptation cinématographique de Diane Kurys, "Les Enfants du Siècle".

Les œuvres de Musset ont fait l'objet de plusieurs adaptations cinématographiques : 





</doc>
<doc id="275" url="https://fr.wikipedia.org/wiki?curid=275" title="Aïkibudo">
Aïkibudo

L' est un art martial traditionnel d'origine japonaise ("budō") essentiellement basé sur des techniques de défense. Il a pour origine l'Aïkido-Yoseikan selon le Centre International de l'Aïkibudo.

Morihei Ueshiba, fondateur de l'Aïkidō, a fait évoluer sa vision de l'art martial tout au long de sa vie. L’aïkido moderne correspond à la forme la plus récente de son enseignement. Avant d'arriver à cette forme épurée, la forme de sa pratique et le nom de son école ont connu des changements. Ueshiba avait ainsi nommé son école "Daitōryū aikijūjutsu", en référence au "koryu" (école traditionnelle ancienne) d'où il tirait ses techniques, puis "aiki budō" (1930), qui deviendra ultérieurement "aikidō" (1942).

Certains de ses élèves créeront à leur tour leur propre style; l'un d'eux, Minoru Mochizuki viendra en France promouvoir l'aïkido d'alors. Par la suite, il le modifiera en fonction de ses recherches et développera le style "Aïkido-jujutsu du Yoseïkan". Il ralliera ainsi certains pratiquants français séduits par la pluralité des disciplines enseignées en son sein.

L'un d'eux, Alain Floquet, initié à l"'aiki jūjutsu", et pratiquant lui-même d'autres arts martiaux, décide de l'enseigner en France. À la recherche des origines des mouvements Aïki, il se verra présenté à divers professeurs célèbres dans divers arts martiaux (Daïto Ryu Aïkijujutsu, Katori shinto ryu…), en plus de l'Aïkido-jujutsu du Yoseïkan appelé aussi Yoseikan Aikido. Plusieurs années après, avec l'autorisation de ses professeurs, il synthétise son propre art, qui ne prend définitivement le nom d'aïkibudo qu'en 1980. L'aïkibudo ne se présente pas comme un concurrent de l’aïkido, mais comme une perception alternative de l'enseignement de Morihei Ueshiba basé en grande partie sur l'enseignement de Minoru Mochizuki.

Surtout développé et enseigné en France, l'aïkibudo est en développement et connait une croissance du nombre de ses pratiquants en Europe et à travers le monde.

Le terme aïkibudo est composé de quatre kanji signifiant approximativement :

"Aïkibudo" peut donc se traduire par « la voie de l'harmonie par la pratique martiale ».

Tout comme en Aïkidō, l'essentiel de la pratique consiste en des techniques de défense à mains nues, contre toutes frappes armées ou non, ou contre toutes saisies. Les mêmes principes qui forment la base des deux pratiques. On y trouve en outre en aïkibudo des variantes plus anciennes de ces techniques, ainsi que des formes issues d'autres écoles, par exemple des variantes des "sutemi waza" proposées par le maître Mochizuki et bien connues des judokas.

En outre, quelques armes sont étudiées, le "bokken" (sabre de bois d’entraînement), le "tanto" (couteau de bois), le "bō" (bâton long). Le pratiquant pourra également, dans le cadre de son étude, s'intéresser à d'autres armes traditionnelles telles le "tonfa" ou la "naginata" (hallebarde). La pratique des armes est issue du "kobudō".

L'assaillant et le défenseur sont dits « partenaires » et non « adversaires » ; ils échangent régulièrement leurs rôles, qui sont déterminés à l'avance. Chacun est amené à tour de rôle à subir les techniques (Uke) et à les appliquer (Tori). Il n'y a donc pas à proprement parler d'affrontement. Ni vainqueur, ni vaincu. L'une des conséquences est qu'il n'existe pas de compétition dans cet art martial.

Toutefois, absence de compétition ne signifie pas exclusion de tout travail spontané ni de travail en opposition ; si une partie de la pratique se fait en « partenariat » pour comprendre les techniques, un travail de randori permet de se confronter différents degrés d'incertitudes, première étape de l'acquisition de « réflexes combatifs » ; par la suite un travail de « kaeshi waza », permet de travailler et d'expérimenter les « contreprises » et les ripostes. Le « partenaire » devient alors « adversaire de travail », pour expérimenter sa progression personnelle et se tester soi-même.

D'une manière générale — même si dans certains clubs des ceintures de couleur sont attribuées — les "aïkibudokas" portent une ceinture (obi) soit blanche, soit noire. 

L'équivalent du changement de couleur de ceinture est un passage de grade "kyu", décerné par le professeur à l'issue d'un examen passé au sein du club. Le débutant, en ceinture blanche, passe successivement six grades kyu, du jusqu'au , qui correspond à la ceinture marron d'autres disciplines. À l'issue de cette progression, on prépare le grade de premier "dan", dont l'obtention autorise à porter la ceinture noire et le "hakama", et marque officiellement le passage de l'état de "débutant" à celui de "pratiquant". Les grades dan sont décernés par un jury fédéral après examen. Les pratiquants, portant la ceinture noire, sont appelés "yudansha".

Le professeur n`est pas celui qui dicte directement les commandements. Le professeur est devant ses élèves et son élève le plus avancé dicte les saluts aux autres élèves.



"Niveau


"Tsuki-Uchi-Waza (poing et frappes)"


"Keri-Waza (jambe)"


3 Kata existent pour les frappes :



Les chutes "ukemi" (réception du corps) sont en fait des roulades utilisées dans la pratique de l'Aïkido et l'Aïkibudo pour éviter de se blesser. On les appelle conformément : "brise-chutes".


Le programme de l'Aikibudo intègre également le maniement des armes, regroupé sous le terme Kobudo, qui signifie "art martial ancien". Le Kobudo de l'aikibudo est issu d'une école d'arme japonaise : le Tenshin Shoden Katori Shinto Ryu. La pratique des armes en Aikibudo comporte : 

Le Kobudo enseigné au sein de l'Aikibudo ne doit pas être confondu avec Le kobudo d'Okinawa qui est un art martial distinct. 

Le Daïto ryu est l'art secret du clan des Takeda. Art de guerre au départ enseigné uniquement par ce clan, ce sont des techniques de clé de bras et de jambes très efficaces et mortelles.

Le Daïto ryu est au programme de l'aïkibudo. Il fait partie de l'histoire martiale de Maître Alain Floquet qui l'a apprise de Maître Takeda Tokimune. 
Le pratiquant d'aïkibudo doit connaître un certain nombre de techniques de Daïto ryu pour son passage de grade du deuxième Dan pour lequel il présente les 10 techniques à genoux de la première série de cette école (Ikkajo).

Le Ki représente l'énergie, la source vitale de chaque individu, il est donné à la naissance de chaque être. 

L'Aikibudo comme tous les autres arts martiaux, se sert de l'énergie de l'adversaire (son Ki) pour la retourner contre lui. Mais pas seulement, chaque pratiquant tout au long de sa pratique se voir enrichir son propre ki, le développer ou plutôt le canaliser, c'est là que prend tout le sens de Ai et Ki l'harmonie de l'énergie.

Peu de pratiquants en prennent conscience, avant d'avoir atteint un certain niveau de pratique. Mais sa perception se fait de plus en plus grande avec le temps.




</doc>
<doc id="277" url="https://fr.wikipedia.org/wiki?curid=277" title="Agriculture durable">
Agriculture durable

L'agriculture durable (anciennement soutenable, traduction alternative de l'anglais "sustainable") est l'application à l'agriculture des principes du développement durable ou soutenable tels que reconnus par la communauté internationale à Rio de Janeiro en juin 1992. Il s'agit d'un système de production agricole qui vise à assurer une production pérenne de nourriture, de bois et de fibres en respectant les limites écologiques, économiques et sociales qui assurent la maintenance dans le temps de cette production.

L'agriculture durable vise notamment à réduire les impacts du secteur en matière environnementale (voir section environnement de l'article agriculture). C'est notamment une agriculture qui protège mieux la biodiversité, l'eau et les sols qui lui sont nécessaires et qui l'utilise mieux via les auxiliaires de l'agriculture et les services écosystémiques.

L'agriculture durable vise une amélioration de la pérennité du système, en créant plus de richesses pérennes par unité de production, sur une base plus équitable. Ces principes sont basés sur la reconnaissance du fait que les ressources naturelles ne sont pas infinies et qu'elles doivent être utilisées de façon judicieuse pour garantir durablement la rentabilité économique, le bien-être social, et le respect de l'environnement (les trois piliers du développement durable).

Concrètement et dans l'idéal (rien n'assurant qu'une agriculture respectant simultanément toutes ces qualités soit possible) :

Pour être durable, l'agriculture doit respecter quelques principes :

À ces principes de base, il faut ajouter la nécessité d'éviter les usages dispersifs des métaux en agriculture. L'étude de l'association des Centraliens sur la raréfaction des métaux recense un certain nombre d'usages dispersifs à éviter.

Le concept principal est celui d'une exploitation agricole constituée par un ensemble de sous-systèmes fonctionnant tous en interaction, un sous-système générant des entrées pour les autres, le système fonctionnant dans l'idéal en cycle fermé.

L'agriculture durable doit être intégrée par tous les agents économiques de la filière, de la fourche à la fourchette (du producteur au consommateur), en incluant les parties prenantes concernées par :

Le suivi en fonction des parcelles (agriculture de précision) nécessite l'utilisation de technologies de l'information, en particulier des systèmes d'information géographique.

Une filière doit être évaluée selon des critères normés communs à tous les agents économiques de la filière, en cohérence avec le cadre normatif des comptabilités nationales.



Types d'action (acteurs) :

Selon les principes de l'agriculture durable, la valorisation de la biomasse n'est pas réservée exclusivement à l'alimentation humaine. Les produits agricoles, ainsi que les déchets et résidus de l'activité agricole, peuvent avoir d'autres utilisations :

Assurer la sécurité sanitaire des aliments implique de mettre en place un suivi le long de toute la chaîne de production, « de la fourche à la fourchette ».

L'agriculture durable s'appuie sur des preuves et une traçabilité apportées par des certifications crédibles, établies par des certificateurs indépendants.

La mise en œuvre de filières intégrées d'agriculture durable met en jeu l'interopérabilité de systèmes hétérogènes, donc la cohérence et la qualité des données (voire leur sécurité), ce qui implique l'utilisation d'un cadre normatif global.

La normalisation relative aux denrées alimentaires est constituée par la série de normes ISO 22000 sur la
sécurité des denrées alimentaires.

La Politique agricole commune de l'Union européenne a fait l'objet de révisions en 1999. Le premier pilier sur le contrôle des marchés a été complété par un deuxième pilier : le développement rural, qui fait référence au développement durable, sur la filière forestière.

Le développement rural est décrit dans le règlement de développement rural (RDR) de la PAC qui peut financer des mesures agro-environnementales via les "États-membres". Une première version de ce règlement a été établie pour la période 2000-2006. Une seconde version (règlement de développement rural II) a été établie pour la période 2007-2013.

D'autre part, l'Union européenne a édicté des directives sur la sécurité alimentaire ("Paquet hygiène") qui concerne toute la filière agricole et agroalimentaire ("« de la fourche à la fourchette »").

Il existe aussi une série de normes internationales sur la traçabilité des denrées alimentaires : ISO 22000.

Voir aussi Traçabilité agroalimentaire

Le RDR européen se décline dans chaque États membres de l'Union européenne par un plan de développement rural national (PDRN).

La loi d'orientation agricole du 9 juillet 1999 définit un cadre contractuel innovant entre agriculteurs et pouvoirs publics, devant permettre de répondre aux nouvelles attentes de la société civile en termes de multifonctionnalité de l’agriculture et de développement durable.

Ce cadre est défini dans le Contrat Territorial d'Exploitation (CTE). Les CTE ont été modifiés par des Contrats d'Agriculture Durable (CAD) : le but est toujours de préserver les ressources naturelles en luttant pour la qualité des sols, de l'eau, de la biodiversité et des paysages (cf décret 2003-675 du 22 juillet 2003).

La conditionnalité soumet le versement de certaines aides de la politique agricole commune au respect d’exigences de base en matière d’environnement et de santé. La conditionnalité est mise en place depuis 2005. Elle garantit une agriculture plus durable et favorise ainsi une meilleure acceptation de la PAC par l'ensemble des citoyens. Ce dispositif soumet le versement de certaines aides communautaires au respect d'exigences en matière de bonnes conditions agricoles et environnementales (BCAE), de santé, et de protection animale.

Les nuisances et pollutions peuvent être mesurées dans le cadre des mesures de prévention des risques.

Il existe un modèle macroéconomique, développé par l'OCDE, qui sert de référence en Europe, pour la mesure de ces « pressions environnementales » : le modèle Pression-État-Réponse (PER).

On peut donc transformer les mesures enregistrées sur le terrain en indicateurs du modèle PER.

D'autre part, on peut agréger ces mesures par secteurs économiques, selon les nomenclatures officielles (NACE, secteurs institutionnels…).




L'agriculture durable ne doit donc pas être confondue avec l'agriculture raisonnée, un concept qui s'appuyait sur un référentiel national certifié par l'État jusqu'en 2013 et a été remplacé depuis par un dispositif de certification environnementale.

Le terme « agriculture soutenable », parfois rencontré, est une traduction du terme anglais "sustainable agriculture", qui a d'abord été utilisée bien qu'impropre parce que plus littérale.

De nombreux méthodes évaluatives font référence au concept d'indicateurs de durabilité à l'échelle de l'exploitation agricole. Cependant, ces méthodes n'intègrent parfois que la dimension environnementale, les dimensions sociales et économiques n'étant pas systématiquement prises en compte. On retrouve dans cette catégorie de nombreuses méthodes d'indicateurs agro-environnementaux ou d'autres outils développés pour une évaluation de la performance environnementale d'une exploitation agricole (Zahm, 2011).

Les outils d'évaluation prennent différentes formes dans leur construction et approches. Il s'agit :
En France, les travaux du projet PLAGE (2008-2014) ont permis d'identifier plusieurs outils d'évaluation de la durabilité selon les dimensions environnementale, sociale et économique.

Parmi ces différentes méthodes, la méthode IDEA (indicateur de durabilité des exploitations agricoles) est une méthode scientifique s'appuyant sur les travaux d'un comité scientifique. Elle est mobilisée depuis de nombreuses années à la fois dans l'enseignement agronomique (technique ou supérieur) pour son caractère pédagogique et transparent et par de très nombreux professionnels du conseil agricole pour accompagner des démarches de changement vers la transition écologique ou des démarches de responsabilité sociétale des entreprises. 

L'agriculture consomme un peu moins de 2 % de l'énergie en France, part voisine de sa contribution au PIB. La consommation concerne essentiellement les tracteurs et autres véhicules agricoles (fioul domestique, et dans une bien moindre mesure gazole et essence), le chauffage des élevages (électricité et gaz propane, butane), des séchoirs et des serres (fioul domestique et gaz propane, butane ou gaz de réseau). Les dépenses consacrées à l'énergie directe étaient de par exploitation agricole en moyenne en 2007.

Le ministère français de l'Agriculture a lancé à la suite du Grenelle Environnement un plan « Objectif Terres 2020 », dont l'objectif est de mettre en œuvre un nouveau modèle agricole français plus respectueux des exigences de développement durable.

Ce plan se décline actuellement dans une dimension énergétique en un plan de performance énergétique des exploitations agricoles.

La loi de modernisation de l'agriculture et de la pêche de 2010 a mis en place le plan régional d'agriculture durable. Le décret d'application de cette loi relatif au plan régional d'agriculture durable, du 16 mai 2011, a modifié le code rural (article D111-1).

L'agriculture fait l'objet du chapitre V (éclairages sectoriels), première section du plan national d'adaptation au changement climatique (). La recommandation précise :

Il existe des positions assez divergentes dans les pays développés sur l'attitude à adopter vis-à-vis de l'agriculture des pays du Sud :

Une solution pour sortir de ce dilemme consiste à inclure des correspondants des pays du Sud dans les réseaux de compétence.


Fédérations et réseaux

Économie et durabilité en agriculture

Union européenne

Sécurité alimentaire

Types d'agriculture comportant des caractéristiques de durabilité

Histoire de l'agriculture

Articles liés à la mondialisation

Syndicats agricoles français



</doc>
<doc id="279" url="https://fr.wikipedia.org/wiki?curid=279" title="Alchimie">
Alchimie

L'alchimie est une discipline qui peut se définir comme « un ensemble de pratiques et de spéculations en rapport avec la transmutation des métaux ». L'un des objectifs de l'alchimie est le grand œuvre, c'est-à-dire la réalisation de la pierre philosophale permettant la transmutation des métaux, principalement des métaux « vils », comme le plomb, en métaux nobles comme l'argent ou l'or. Un autre objectif classique de l'alchimie est la recherche de la panacée (médecine universelle) et la prolongation de la vie via un élixir de longue vie. La pratique de l'alchimie et les théories de la matière sur lesquelles elle se fonde, sont parfois accompagnées, notamment à partir de la Renaissance, de spéculations philosophiques, mystiques ou spirituelles.

Des pensées et des pratiques de type alchimique ont existé en Chine dès le et en Inde dès le . L'alchimie occidentale, quant à elle, commence dans l'Égypte gréco-romaine au début de notre ère, puis dans le monde arabo-musulman, d'où elle se transmet au Moyen Âge à l'Occident latin, où elle se développe à la Renaissance et jusqu'au début de l'époque moderne. Jusqu'à la fin du les mots alchimie et chimie sont synonymes et utilisés indifféremment. Ce n'est qu'au cours du qu'ils se distinguent et que l'alchimie connaît une phase de déclin, sans toutefois disparaître totalement, alors que la chimie moderne s'impose avec les travaux d'Antoine Lavoisier. 

L'étymologie du terme alchimie est discutée (""). Le mot « alchimie » viendrait de l'arabe , "al-kīmiyāﺀ" venant lui-même du grec ancien "khumeia / khêmeia". Le terme apparaît dans le vocabulaire français au , par le latin médiéval "alchemia". Les termes "alchimie" et "chimie" (en latin "alchemia" et "chemia", ou "alchymia" et "chymia") sont restés strictement synonymes jusqu'au début du , avec notamment l'ouvrage polémique d'Étienne-François Geoffroy, "Des supercheries concernant la pierre philosophale" (1722).

Différentes hypothèses ont été avancées pour l'origine du mot en arabe. Le mot arabe proviendrait du mot grec "khemeia", désignant également la chimie dans son acception moderne, ou bien du grec , "khymeia" désignant un mélange, une mixture. Le philologue Hermann Diels, dans son "Antike Technik" (1920) y voyait la « fusion », du grec ancien "khumeia / khêmeia", signifiant « art de fondre et d'allier les métaux ».

"Kimiya" pourrait également venir du mot copte kēme (ou son équivalent en dialecte bohaïrique, khēme), lui-même dérivant du grec kmỉ, correspondant au moyen égyptien "ḳm.t", désignant la terre noire, la terre alluvionnaire et par extension l'Égypte ().

Le fondateur légendaire de cette discipline est le personnage mythique d'Hermès Trismégiste, l'astrologie et l'alchimie étant les deux piliers essentiels dans l'édifice des connaissances secrètes placé sous sa caution prestigieuse.

Pour Michèle Mertens : 

Deux sources principales de textes de cette époque ont été conservées : deux recueils sur papyrus, conservés à Leyde et à Stockholm datés de 300 , dits papyri de Leyde et de Stockholm et un "corpus" constitué à l'époque byzantine.

François Daumas voit un lien entre la pensée égyptienne et l'alchimie gréco-égyptienne, à travers la notion de pierre, pierre à bâtir ou pierre philosophale. Garth Fowden, cependant, juge l'interprétation de Daumas trop optimiste : 

De nombreuses techniques artisanales sont connues dans l'Égypte hellénistique avant l'apparition de l'alchimie : fonte des métaux (seulement sept métaux sont connus de l'antiquité jusqu'à la renaissance : or, cuivre, argent, plomb, étain, fer et mercure), la fabrication d'alliage (bronze et laiton), différentes techniques de métallurgie et d'orfèvrerie, le travail du verre, la fabrication de gemmes artificielles, la fabrication de cosmétique.
Les plus anciens textes qu'on peut relier à l'alchimie sont les papyrus de Leyde et de Stockholm, écrits en grec et découverts en Égypte, et qui datent du . Ils contiennent 250 recettes techniques qu'on peut répartir en quatre catégories qui visent à donner aux métaux l'aspect de l'or ou de l'argent et à imiter la coûteuse pourpre et les pierres précieuses (émeraudes, perles...). Ces recettes sont claires dans la mesure où on parvient à en identifier aujourd'hui les ingrédients. Les papyrus recettes contiennent des tests de la pureté des métaux précieux et communs, ce qui indique que leurs auteurs sont parfaitement conscient de la différence entre l'imitation et l'original. Une de ces recettes par exemple, porte sur l'« eau de soufre », constituée d'un mélange chaux, de soufre et d'urine ou de vinaigre, que l'on chauffe. Elle permet de donner à l'argent l'aspect de l'or par l'action en surface de polysulfures de calcium.

Le plus ancien texte du "Corpus alchemicum graecum" est le "Physika kai mystika" ("φυσικά και μυστικά", Questions naturelles et secrètes), et que l'on peut dater du . Faussement au philosophe Démocrite d'Abdère du avant notre ère (on parle du Pseudo-Démocrite, Ce texte a souvent été considéré au , comme une version remaniée et interpolée d'un ouvrage plus ancien d'un auteur gréco-égyptien mal connu, Bolos de Mendès (entre -250 et -125) ; Les études plus récentes ont conduit à rejeter cette hypothèse. Synésius l'alchimiste, au , identifie le maître au mage Ostanès, et le temple à celui de Memphis. Le texte présente des recettes techniques très similaires à celles des papyrus, destinées à imiter l'or, l'argent, le pourpre et les pierres précieuses ; mais il présente des éléments qui deviendront caractéristiques des textes alchimiques :

Pour Didier Kahn c'est le premier traité d'alchimie connu, mais pour Lawrence Principe il appartient encore à la littérature technique des recettes. Comme l'indiquait Robert Halleux : .

Selon Lawrence Principe c'est vraisemblablement au cours du que l'idée, non plus d'imiter l'or et l'argent, mais de les fabriquer réellement émergea. Après le "Physika kai mystika" du pseudo-Démocrite, on dispose d'une série de citations ou de courts traités attribués à des personnages mythiques ou célèbres (Hermès, Isis, Moïse, Agathodémon, Jamblique, Marie la Juive, Cléopatre, Comarius, Ostanès, Pamménnès, Pibechius..., pour la plupart cités par Zosime de Panopolis , qui, vers 300, est le premier alchimiste pour lequel on dispose d'écrits et de détails biographiques substantiels.

. La légende dit qu'elle aurait initié le grand Zosime après un premier refus, prétextant qu'elle ne saurait initier un non juif à l'art divin. En revanche, avec Zosime de Panopolis (aussi nommé Zosime le panopolitain), la technique se double d'une mystique et d'une symbolique. Zosime reste le fondateur canonique de l'alchimie gréco-égyptienne. Il vivait à Alexandrie, mais aux environs de l'an 300. Il ne serait autre que le fameux Rosinus connu par des publications latines postérieures. Ses recettes alchimiques ainsi que ses principes feront autorité. 

Deux autres auteurs de cette période sont restés célèbres pour leurs commentaires ou leurs recettes : Olympiodore l'Alchimiste, qui est peut-être Olympiodore le Jeune (un recteur de l'école néoplatonicienne d'Alexandrie, en 541) et Synésius, qui est peut-être Synésios de Cyrène, ami et disciple de la philosophe néoplatonicienne Hypatie. Olympiodore le Jeune, au , sur l'analogie planètes-métaux, donne un système de correspondances, qui sera classique en alchimie : or-Soleil, argent-Lune, plomb-Saturne, électrum-Jupiter, fer-Mars, cuivre-Vénus, étain-Mercure.

L'alchimie est liée à la philosophie hermétique, que Françoise Bonardel définit comme . Il ne faut cependant pas confondre les deux, les textes philosophiques intitulés "Hermetica" ne parlant pas d'alchimie. Des textes, à la fois hermétiques et alchimiques, apparaissent dès le ou .

Les alchimistes alexandrins utilisaient quatre types de techniques pour « produire » de l'or, techniques consignées dans des recettes :

L'alchimie arabe naît en 685 quand, selon la légende, le prince Khâlid ibn al-Yazîd commande au moine Marianus (ou Morienus), élève de l'alchimiste Étienne d'Alexandrie (vers 620), la traduction en arabe de textes alchimiques grecs ou coptes.

Au VIII- apparaît le "Corpus Jabirianum", attribué à Jâbir ibn Hayyân. Jâbir ibn Hayyân, dit Geber (vers 770), pose comme première triade celle du corps, de l'âme et de l'esprit. Il insiste sur l'élixir comme remède et panacée, et l'élixir n'est pas seulement minéral. Geber pose aussi un septénaire, celui des sept métaux : or (Soleil), argent (Lune), cuivre (Vénus), étain (Jupiter), plomb (Saturne), fer (Mars), vif-argent (Mercure) ; un autre septénaire, celui des opérations : sublimation, distillation ascendante ou descendante (filtration), coupellation, incinération, fusion, bain-Marie, bain de sable. L’argyropée est une étape, non une chute : elle s’intègre dans l’œuvre. Les quatre éléments et les quatre qualités élémentaires sont autonomes. Dans toute substance des trois règnes il est possible d’augmenter, de diminuer la proportion, voire de faire disparaître le chaud, le froid, etc. et ainsi d'obtenir une tout autre substance.

On attribue à Geber la découverte de l'acide nitrique, obtenu en chauffant du salpêtre KNO en présence de sulfate de cuivre (CuSO⋅5HO) et d'alun (KAl(SO)⋅12HO), et de l'acide sulfurique (le vitriol), et l'eau régale. Il a également isolé l'antimoine et l'arsenic de leurs sulfures (stibine et orpiment/réalgar).

Un certain nombre de traités arabes médiévaux de magie, d’astrologie ou d’alchimie sont attribués à Balînâs Tûwânî (Apollonius de Tyane). Au (vers 825), en lien avec ce mage pythagoricien, le "Livre du secret de la Création. Kitâb sirr al-Khaleqa" donne en arabe le texte de la "Table d’émeraude", qui joue un rôle essentiel dans la tradition hermético-alchimique.
Râzî (860-923), appelé Rhazès en Occident, a laissé un "Livre des secrets. Kitâb al-asrâr" de grande influence. 

L'encyclopédie des Frères de la pureté (Ikhwân as-Safâ, 963) contient une section sur l'alchimie.

Le philosophe Algazel (Al-Ghazâlî 1058-1111) parle d'une alchimie de la félicité ("kimiyâ es-saddah"). Mais il est plutôt opposé à la pratique alchimique.

L'alchimie arabe, qui connaît son apogée entre le et le , va largement et rapidement se diffuser dans l'Occident chrétien sous la forme de traductions latines à partir du milieu du . L'une des tout premières est le "Morienus" : Robert de Chester, en 1144, traduit en latin un livre arabe de Morienus Romanus, le "Liber de compositione alchemiae quem edidit Morienus Romanus" qui dit : "Puisque votre monde latin ignore encore ce qu'est Alchymia et ce qu'est sa composition, je l'expliquerai dans ce livre. Alchymia est une substance corporelle composée d'une chose unique, ou due à une chose unique, rendue plus précieuse par la conjonction de la proximité et de l'effet." Vers la même époque Hugues de Santalla traduit le "Livre du secret de la création" attribué à Balinous (le nom arabe d'Apollonios de Tyane qui comprend la première version latine de la "Table d'émeraude"). Et le franciscain Gérard de Crémone (~1114-~1187) traduit le "Liber divinitatis de septuaginta" ("Livre des septantes") de Jabir Ibn Hayyan (dont la plupart des textes qui lui seront ensuite attribués sont des créations latines) et des textes faussement attribués à Rhazès.

Le passage du "Kitâb al-Shifâ’" (vers 1020), dans lequel Avicenne (Ibn Sīnā) s'oppose à l'alchimie, est traduit en latin sous le titre "De congelatione et conglutinatione lapidum" (De la congélation et de la conglutination de la pierre), par Alfred de Sareshel vers 1190. Mis en annexe du livre IV des "Météorologiques", dans lequel Aristote discute de la nature et de la formation des métaux, il sera attribué à ce dernier et influencera tant les alchimistes que leurs opposants. L’or est fait de Mercure et de Soufre combinés sous l’influence du Soleil. Une phrase célèbre retient les esprits : 

Cette vague de traductions se poursuit au et de nombreux textes arabes sont mis sous le nom d'autorités antiques, philosophes comme Socrate et Platon, Aristote, Galien, Zosime de Panopolis (latinisé en Rosinus, et lui effectivement alchimiste), ou figures mythiques comme Hermès Trismégiste, Apollonios de Tyane, Cléopâtre.

Avec ce corpus traduit de l'arabe, outre un certain nombre de termes techniques comme alambic ou athanor, l'alchimie latine va hériter de ses principales thématiques et problématiques : l'idée que les métaux se forment sous la Terre sous l'influence des planètes à partir de soufre et de mercure, et que l'alchimie vise à reproduire, accélérer ou parfaire ce processus ; l'analogie entre alchimie et médecine, sous la forme de l'élixir - la connotation religieuse, le dieu créateur étant vu comme le modèle de l'alchimiste - la question de la diffusion ou du secret de la connaissance alchimique. 

Plusieurs traditions sont représentées dans ces textes : des traités pratiques et clairs, parmi lesquels ceux issus de l'école de Geber et de Rhazès, et le "De anima in arte alchemia" attribué à Avicenne, qui reflètent une véritable recherche expérimentale, des traités de recettes reprenant la forme du "Secretum Secretorum" (attribué à Rhazès et traduit par Philippe de Tripoli vers 1243, et des textes allégoriques dont le "Morienus", la "Turba philosophorum" et la "Tabula Chemica" de Senior Zadith (Ibn Umail). Le Pseudo-Geber (Paul de Tarente, auteur de "La somme de perfection. Summa perfectionis", 1260), le Pseudo-Arnaud de Villeneuve ("Rosarius", av. 1332), Gérard Dorn ("Clavis totius philosophiae chymisticae", 1566) reprendront l'idée de mêler pratique et allégorie.

Vers 1210, le savant Michael Scot écrit plusieurs traités alchimiques : "Ars alchemiae", "Lumen luminum". Il est le premier à évoquer les vertus médicales de l’or potable ; Roger Bacon ("Opus majus", 1266 ; "Opus tertium", 1270), le Pseudo-Arnaud de Villeneuve ("Tractatus parabolicus", vers 1330), le paracelsien Gérard Dorn ("De Thesauro thesaurorum omnium", 1584) poursuivront dans ce sens.

Vers 1250, Albert le Grand admet la transmutation, il établit l’analogie entre la formation du fœtus et la génération des pierres et métaux. Il défend la théorie du soufre et du mercure. Il est sans doute l'auteur de "Alkimia" ou de "Alkimia minor", mais pas des autres traités, tels que "Semita recta", ou "Le composé des composés. Compositum de compositis". Thomas d'Aquin n'est pas alchimiste, quoiqu'on lui attribue le magnifique "L'aurore à son lever (Aurora consurgens)", qui présente l'alchimie comme une quête de régénération spirituelle, intérieure, qui date de 1320.

Roger Bacon s'est intéressé à l'alchimie dans son "Opus minus" (1267), dans son "Opus tertium", dans son commentaire au "Secret des secrets" (1275-1280) qu'il croit à tort d'Aristote ; mais "Le miroir d'alchimie (Speculum alchimiae)" date du s. : il est d'un Pseudo-Roger Bacon. Roger Bacon ("Opus majus", 1266) soutient que la médecine des métaux prolonge la vie et que l’alchimie, science pratique, justifie les sciences théoriques (et non plus l’inverse) : le premier, il voit le côté double (spéculatif et opératoire) de l'alchimie.

Pour le Pseudo-Roger Bacon : 

Les deux principes ou Substances étaient le Soufre et le Mercure, un troisième s'ajoute dès la "Somme de la perfection (Summa perfectionis)" (1260) : l'Arsenic. L'ouvrage est attribué à l'Arabe Geber (Jâbir ibn Hayyân), mais il est du Pseudo-Geber, ou Geber latin, Paul de Tarente.

Les auteurs les plus caractéristiques sont Arnaud de Villeneuve (1245-1313), Denis Zachaire, le Pseudo-Lulle (début du ), le chanoine George Ripley, le prétendu Bernard le Trévisan.

L'année 1330 est la date de "La nouvelle perle précieuse (Pretiosa margarita novella)", de Petrus Bonus, qui est un discours théologique. L'auteur distingue recherche scientifique et illumination divine. Il est le premier à faire une lecture alchimique des grands mythes antiques, comme la Toison d’or, Pan, les métamorphoses d'Ovide, Virgile, etc. ; il sera suivi par Augurelli, Pic de la Mirandole, Giovanni Bracesco + 1555, Dom Pernéty. Petrus Bonus soutient la théorie du mercure seul. Le premier, il compare la pierre philosophale au Christ : si le processus du Grand Œuvre correspond à la vie humaine (conception, gestation, naissance, croissance, mort), il correspond aussi aux mystères de la religion chrétienne (incarnation et passion du Christ, Jugement dernier, mystère de la Sainte-Trinité, etc.).

Vers 1350 Rupescissa (Jean de Roquetaillade) ("De consideratione quintae essentiae") assimile élixir et alcool, comme un cinquième Élément, une quintessence donc, qui peut prolonger la vie. Il dit que l’on peut extraire cette quintessence de toutes choses, du sang, des fruits, du bois, des fleurs, des plantes, des métaux. D’où certains remèdes. Il fait une alchimie distillatoire, car, pour lui, la quintessence est un distillat extrêmement puissant qui peut s’extraire de l’alcool distillé mille et une fois. Cette théorie de la quintessence introduit l’idée du « principe actif » possédant au centuple les mêmes propriétés que les simples, dont Galien avait détaillé les effets bénéfiques sur le plan humain.

L'Église catholique n'a jamais condamné pour hérésie l'alchimie en tant que telle. Les condamnations ne sont faites que dans des cadres limités : celle des faux-monnayeurs et des magiciens, la discipline interne aux ordres mendiants (franciscains et dominicains), et au la dénonciation des libertins. L'idée de cette condamnation n'apparaît qu'avec les occultistes du .

En 1273, 1287, 1289, 1323, 1356 et 1372, les chapitres généraux des dominicains intiment aux frères de remettre à leurs supérieurs les écrits d'alchimie ou (en 1321) de les détruire. En 1295, la législation des franciscains leur interdit de détenir, lire, écrire des livres d'alchimie. 

Élie de Cortone, Gérard de Crémone, Roger Bacon, Jean de Roquetaillade sont des franciscains.

Dans le "Tractatus parabolicus" du Pseudo-Arnaud de Villeneuve (milieu du s.), pour la première fois, l’image du Christ (sa vie, sa Passion, et sa résurrection) est comparée à la pierre philosophale. L'alchimie devient, dès lors, chrétienne. Le Pseudo-Lulle : "De même que Jésus-Christ a pris la nature humaine pour la délivrance et la rédemption du genre humain, prisonnier du péché par la suite de la désobéissance d'Adam, de même, dans notre art, ce qui est souillé criminellement par une chose est relevé, lavé et racheté de cette souillure autrement, et par la chose opposée." Toujours à la même époque (1350), Jean de Roquetaillade établit le lien entre Grand Œuvre et Passion du Christ.

Le poème "L'ordinaire d'alchimie" (1477) de Thomas Norton.

Denis Zachaire déclare avoir réussi à transmuter du mercure en or le jour de Pâques 1550 :

Quand Rodolphe II de Habsbourg est empereur (1576-1612), la capitale de l'alchimie est Prague. Les adeptes de l'époque y convergent : Heinrich Khunrath (auteur d'un admirable "Amphitheatrum sapientiae aeternae", 1602), Oswald Croll, Michael Maier (auteur, entre autres, de "Les Arcanes très Secrets", 1613, et de l’"Atalante fugitive", 1618). 

Le fameux ouvrage sur Nicolas Flamel, "Le livre des figures hiéroglyphiques", qui donne une interprétation alchimique de l'arche du cimetière des Innocents à Paris, n'a pas été écrit par Nicolas Flamel, qui ne fit jamais d'alchimie. Le livre est daté de 1399, mais il ne fut édité en 1612, il n'a pu être écrit que vers 1590, peut-être par l'écrivain François Béroalde de Verville (1558-1612). Il développe la notion d"'ars magna", une mutuelle délivrance de la matière et de l’esprit par la réalisation de l’œuvre, à la fois spirituelle et physique.

Paracelse, comme l'a montré un de ses éditeurs, Johann Huser, n'a rien écrit d'alchimique au sens courant du terme (transmutation des métaux, production d'or), puisqu'il se concentre sur l'utilisation médicale et l'aspect philosophique. Dans son "Opus paragranum" (1533), il substitue aux quatre Éléments les trois Substances ("tria prima") que sont le Soufre, le Mercure et (c'est Paracelse qui l'ajoute) le Sel ; il assimile le processus de digestion à l’alchimie, science des cuissons et des maturations. Cette approche spécifique qu'avait Paracelse de l'alchimie donnera naissance à la spagyrie.

Jean-Baptiste Van Helmont (Bruxelles 1579-1644) est d'abord diplômé en philosophie avant de chercher une autre voie dans l'astronomie, puis dans la médecine. C'est alors que, se penchant sur les mystères de l'alchimie, il tente la transmutation des métaux et découvre l'existence des gaz, ce qui le situe à l'orée de la science moderne . Il décrit plusieurs gaz dont le gaz carbonique. Ses œuvres ont été publiées par son fils François-Mercure sous le titre "Ortus medicinae, vel opera et opuscula omnia" .
Jean-Baptiste Van Helmont, un alchimiste précurseur de la chimie, voulait démontrer que la théorie des quatre éléments alchimiques n'était pas valable.

Van Helmont a fait pousser un jeune saule dans une caisse de bois contenant 90 de terre séchée au four. Il a couvert le pot d'un couvercle en fer étamé perforé de petits trous. Il exprime qu’il n'a pas tenu compte des chutes de feuilles et que de la poussière a pu s'envoler et se re-déposer. Après arrosage, durant cinq ans, avec de l’eau de pluie filtrée sur tamis ou de l'eau distillée si nécessaire, il a observé que le poids de l’arbre avait augmenté de 76 kg, tandis que celui de la terre n’avait diminué que de 57 g. La terre ayant quasiment le même poids, c’est donc l’eau qui s’est changée en bois, en écorces et en racines. Pour les alchimistes, l'élément alchimique "eau" était ainsi transmuté en élément "terre" .

Van Helmont expliquait que s'il provient de l'élément "eau", l'élément "terre" n’est pas élémentaire, donc que l'élément « terre » n'en était pas un et que la théorie des quatre éléments n'était pas valide .

Toutefois, ces quatre "éléments" correspondent à notre époque aux états de la matière (solide, liquide, gaz, plasma).

Avec Gérard Dorn ("Clavis totius philosophiae chymisticae", 1566), Jacques Gohory ("Compendium", 1568), Cesare Della Riviera ("Le monde magique des héros", 1603) . Elle se prolonge par certaines œuvres de Giordano Bruno ou de Jean d'Espagnet. Une correspondance s'établit entre les stades du Grand Œuvre et les étapes d’une transmutation spirituelle.

De grands alchimistes marquent encore cette époque dont le Basile Valentin, le Cosmopolite (Alexandre Seton ? Michel Sendivogius ?), l'Anglais Eyrénée Philalèthe (George Starkey). 

1616 : "Les noces chymiques de Christian Rosencreutz", de Jean Valentin Andreae. L'alchimie est ici spirituelle, allégorique, et surtout relève de la Rose-Croix. Michael Maier, médecin de l'empereur Rodolphe II du Saint-Empire, donne dans son livre Themis Aurea les règles d'or des médecins alchimistes de l'Ordre de la Rose Croix. 

En 1677 paraît à La Rochelle un livre singulier, dû à Jacob Saulat : "Mutus liber. Livre muet" : "toute la philosophie hermétique est représentée en figures hiéroglyphiques", en fait quinze planches, sans texte, qu'Eugène Canseliet éditera et commentera. Le livre semble tenir la rosée pour un élixir.

Robert Boyle qui croit à la possibilité de la transmutation des métaux, met en doute, dans "The Sceptical Chymist" (1661), la théorie des quatre éléments ainsi que celle des trois principes paracelsiens (soufre, mercure et sel), et introduits l'idée d'élément chimique comme élément indécomposable, et non transformable en un autre élément. 

De 1668 à 1675, Isaac Newton pratique l’alchimie. 

Le 31 janvier 1712, l'alchimiste Jean Trouin meurt embastillé sans avoir transformé le plomb en or comme il le prétendait.

En 1722, le médecin et naturaliste français Étienne-François Geoffroy, inventeur du concept d'affinité chimique ne croit pas à la transmutation, mais ne pense pas possible de démontrer son impossibilité :

En 1781, Sabine Stuart de Chevalier, une des rares femmes alchimistes, publie son "Discours Philosophique sur les Trois Principes, Animal, Végétal et Minéral, ou la Clef du Sanctuaire Philosophique." 

En 1783, Lavoisier décompose l'eau en oxygène et hydrogène. 

Le comte de Saint-Germain, célèbre en France entre 1750 et 1760, prétendait être immortel et capable de produire ou de purifier des pierres précieuses.

Au , les quelques alchimistes résiduels sont considérés comme des curiosités, vestiges d'une époque révolue.

Ceux qui pratiquent l'hyperchimie (Tiffereau, Lucas, Delobel, Jollivet-Castelot) veulent faire de l'alchimie de façon strictement chimique. Théodore Tiffereau fabrique de l'or à Mexico en 1847, et Gustave Itasse, un chimiste, découvre que cet or possède 

Certains francs-maçons français, (Jean-Marie Ragon 1781 - 1862, Oswald Wirth 1860-1943), s'inscrivant dans la lignée de certains de leurs prédécesseurs du (notamment le baron Tschoudy), lient étroitement l'alchimie mystique et la maçonnerie ésotérique.

En 1926 paraît un ouvrage intitulé "Le mystère des cathédrales", écrit par un inconnu usant d'un pseudonyme, un certain Fulcanelli. Ce même auteur fait publier quelques années après un autre ouvrage, "Les Demeures philosophales". Fulcanelli deviendra au cours du une légende. Canseliet, qui aurait été son élève, va venir souffler le chaud et le froid sur ce personnage, qui, selon la légende, aurait bénéficié du "don de Dieu", l'immortalité (il aurait été vu en Espagne âgé de 113 ans) : "Eh bien, quand je l'ai revu, il avait 113 ans, c'est-à-dire en 1952. J'avais à cette époque 53 ans. J'ai vu un homme sensiblement de mon âge. Attention, je précise, Fulcanelli en 1922 et même avant, c'était un beau vieillard, mais c'était un vieillard."

Sont également auteurs contemporains, Roger Caro, fondateur de l"'Église universelle de la nouvelle alliance", Kamala Jnana et Jean Clairefontaine, qui d'ailleurs ne constituent peut être qu'une seule et même personne Il faut préciser de Jean de clairefontaine n'est pas Roger CARO mais son Ami et mécène Maurice AUBERGER. Richard Caron fait état d'un regain d'intérêt notoire à partir du début . "On voit s'intéresser à l'alchimie non seulement des occultistes de tous horizons, mais également des écrivains, une certaine partie de la bourgeoisie qui fréquentait les salons littéraires, et particulièrement le milieu médical qui depuis la fin du siècle précédent a fait soutenir, dans ses facultés, un grand nombre de thèses en médecine."

Pour Fulcanelli, l'alchimie est "la science hermétique", "une chimie spiritualiste" qui "tente de pénétrer le mystérieux dynamisme qui préside" à la "transformation" des "corps naturels". L'archimie poursuit à peu près un des buts de l'alchimie ("la transmutation des métaux les uns dans les autres"), mais elle utilise "uniquement des matériaux et des moyens chimiques", elle se cantonne au "règne minéral". La spagyrie est "l'aïeule réelle de notre chimie". "Les souffleurs, eux, étaient de purs empiriques, qui essayaient de fabriquer de l'or en combinant ce qu'ils pouvaient connaître de l'alchimie (bien peu de chose!) et des secrets spagyriques."

En 1953 René Alleau publia aux éditions de Minuit un ouvrage fondamental : "Aspects de l'alchimie traditionnelle" avec une préface d'Eugène Canseliet. C'est d'ailleurs Alleau qui, en 1948, prononça une série de conférences sur l'alchimie auxquelles assista André Breton, et qui eurent un profond retentissement sur le chef de file des surréalistes. On doit au même auteur la collection Bibliothica hermetica.

En 1956 paraît pour la première fois en édition complète chez Denoël : "Le Message Retrouvé", du peintre Louis Cattiaux dont le témoignage alchimique, comme celui de sa "Physique et métaphysique de la peinture", est plus qu'évident. L'ouvrage sera réédité de très nombreuses fois dans sa langue française originale de même qu'en castillan, catalan, allemand, italien, portugais, anglais (en tout, plus de vingt éditions). Il a donné lieu à bien des commentaires alchimiques.

Dans : "Ces Hommes qui ont fait l'alchimie au XXe siècle", Geneviève Dubois donne la parole à, ou dresse la liste de nombreux alchimistes contemporains : Louis Cattiaux, Emmanuel d'Hooghvorst, José Gifreda, Henri Coton-Alvart, Henri La Croix Haute, Roger Caro, Alphonse Jobert, Pierre Dujols de Valois, Fulcanelli et Eugène Canseliet. 

Selon Serge Hutin : 
selon René Alleau (1953) 

La recherche des remèdes d'immortalité fait partie de la culture chinoise antique depuis la période des Royaumes combattants. Les souverains font confiance à la voie des magiciens et des immortels, et ces « magiciens » ont souvent des pratiques s'apparentant à l'alchimie. Sur un plan strictement historique, un savoir de type alchimique est établi, pour la Chine, à partir du avant l’ère chrétienne. On retrouve la trace, dans les Mémoires historiques de Sima Qian, d'un récit parlant de transmutation en or et d'allongement de la vie par des pratiques alchimiques lors du règne de Wu Di de la dynastie Han en 133 . On voit le magicien Li Shao-jun se rendre chez l'empereur et lui dire : "Si vous sacrifiez au fourneau, alors je vous enseignerai comment faire des vases en or jaune ; et dans ces vases vous pourrez boire et acquérir l'immortalité." "C'est probablement, dit J. Needham, le plus ancien document sur l'alchimie dans l'histoire du monde." À la lumière de travaux les plus récents sur l'origine de l'alchimie chinoise (Pregadio 2006, Campany 2002), les opinions de certains spécialistes français du de l'alchimie comme Serge Hutin paraissent complètement dépassées.

Un texte fondateur, bien qu'il soit plus un traité de cosmologie que d'alchimie, est le "Cantongqi" ("Tcheou-yi san-t'ong-ki. Triple concordance dans le livre des mutations des Tcheou"), attribué à Wei Boyang (Wei Po-yang), un Immortel légendaire situé en 142. Le premier traité alchimique chinois connu est le "Baopuzi neipian" écrit par Ge Hong (283-343 ). Les alchimistes chinois font une distinction entre "alchimie extérieure" ("waidan", "wai tan") et "alchimie intérieure" ("neidan", "nei tan"). L’alchimie extérieure, telle que pratiquée par Ge Hong par exemple, cède la place à l’alchimie intérieure qui domine dès la fin de la Dynastie Tang en 907. Les premières traces écrites de cette alchimie intérieure qui s'inscrit dans le cadre du taoïsme datent du .

L'alchimie dite "indienne" est hindouiste. Elle remonte à la période très ancienne des "Veda" ( millénaire av. J.-C.) et tire ses origines de l'Ayurveda. Cette connaissance alchimique est appelée "Rasâyana", qui signifie littéralement "voie du mercure". Le "Rasâyana "amène à la préparation d'un élixir de longue vie nommé "Ausadhi". 

L'Ayurveda est divisée en huit branches dont l'une est le "Rasâyana : "

Des rapprochement entre l'alchimie et les pratiques shivaïques et tantriques ont été effectués par plusieurs auteurs: Shiva, qui s'apparenterait au principe actif du soufre, féconde Çakti, qui s'apparenterait principe passif du mercure. Dans la tradition tantrique, le corps devient un "Siddha-rûpa", littéralement "corps de diamant-foudre" se rapprochant du concept de "corps de gloire" de l’"Ars Magna" en occident.

Malgré pléthore de sources archéologiques (anciennes et contemporaines) dont les "Veda "( millénaire ), les origines de l'alchimie hindoue ont trouvé maintes occasions d'êtres débattues. Il convient cependant de préciser qu'une vision ethnocentriste, "pro-occidentale ou coloniale", aurait pu influencer les partisans de la thèse d'une "origine importée ou acquise" de l'alchimie en Inde. 

Le sujet a été étudié par et Mircea Eliade.
"Robert. Eisler a suggéré l'hypothèse d'une alchimie mésopotamienne. En réalité, les tablettes dont Eisler faisait état sont soit des recettes de verrier, soit des rituels accompagnant les opérations de métallurgie". Les Mésopotamiens utilisent, dans leurs recettes pour fabriquer de la pâte de verre coloré, un langage secret, mais cela relève davantage du secret de métier que de la discipline de l'arcane.

Dès le en Babylonie et le en Assyrie il y a fabrication de gemmes de four (artificielles). Ce sont, à peu près, les mêmes recettes qu’on retrouvera à Alexandrie au : imitation des métaux précieux, coloration des pierres, production de la pourpre. 

L'étape mésopotamienne est un moment capital dans l'histoire de l'alchimie, car les métaux sont mis en correspondance avec les planètes. Ainsi se place le fondement ésotérique de l'alchimie, à savoir la mise en place de corrélations entre des niveaux différents de réalité dans un monde conçu sur base d'analogies (a est à b ce que c est à d).

La Lune est liée à la couleur argentée, au métal argent, aux dieux Sîn (dieu Lune) et Anum ; le Soleil est lié à la couleur dorée, au métal or, aux dieux Shamash (dieu Soleil) et Ellil ; Jupiter : bleu lapis, étain, Mardouk et Nin-ani ; Vénus : blanc, cuivre, Ishtar déesse de la fécondité et des combats) et Éa ; Mercure jaune-vert, vif-argent (?), Nabou (dieu de l'écriture) ; Saturne : noir, plomb (?), Nirurta ; Mars : brun-rouge, fer (?), Erra (Nergal).


L'alchimie s'est donné des buts distincts, qui parfois coexistent. Le but le plus emblématique de l'alchimie est la fabrication de la pierre philosophale, ou « grand œuvre », censée être capable de transmuter les métaux vils en or, ou en argent. D'autres buts de l'alchimie sont essentiellement thérapeutiques, la recherche de l'élixir d'immortalité et de la Panacée (médecine universelle), et expliquent l'importance de la médecine arabe dans le développement de l'alchimie. Derrière des textes hermétiques constitués de symboles cachant leur sens au profane, certains alchimistes s'intéressaient plutôt à la transmutation de l'âme, c'est-à-dire à l'éveil spirituel. On parle alors de "l'alchimie mystique". Plus radical encore, l'Ars Magna, une autre branche de l'alchimie, a pour objet la transmutation de l'alchimiste lui-même en une sorte de surhomme au pouvoir quasi illimité. Un autre but de l'alchimie, est la création d'un homme artificiel de petite taille, l'homoncule.

L'alchimiste oppose ou rend complémentaires alchimie pratique et alchimie spéculative. Roger Bacon, en 1270, dans son "Opus tertium", 12, distinguait ces deux types-ci d'alchimie : 

Le Grand Œuvre avait pour but d'obtenir la pierre philosophale. L'alchimie était censée opérer sur une "Materia prima", Première Matière, de façon à obtenir la pierre philosophale capable de réaliser la "projection", c'est-à-dire la transformation des métaux vils en or. Les alchimistes ont développé deux méthodes pour tenter d'obtenir la pierre philosophale: la "voie sèche" et la "voie humide". De façon classique la recherche de la pierre philosophale se faisait par la voie dite "voie humide", celle-ci est par exemple présentée par Zosime de Panopolis dès 300. La voie sèche est beaucoup plus récente et a peut-être été inventée par Basile Valentin, vers 1600. En 1718, Jean-Conrad Barchusen, professeur de chimie à Leyde, dans son "Elementa chemicae", développe cette voie. Selon Jacques Sadoul la voie sèche est la voie des hautes températures, difficile, tandis que la voie humide est la voie longue (trois ans), mais elle est moins dangereuse. Fulcanelli dit à ce propos « À l’inverse de la voie humide, dont les ustensiles de verre permettent le contrôle facile et l’observation juste, la voie sèche ne peut éclairer l’opérateur ».

Les phases classiques du travail alchimique sont au nombre de trois. Elles sont distinguées par la couleur que prend la matière au fur et à mesure. Elles correspondent aussi aux types de manipulation chimique : œuvre au noir calcination, œuvre au blanc lessivage et réduction, œuvre au rouge pour obtenir l'incandescence. On trouve ces phases dès Zosime de Panopolis. La phase blanche est parfois divisée en phase blanche lessivage et phase jaune réduction par certains auteurs alchimistes, qui admettent ainsi quatre phases (noir, blanc, jaune, rouge) pour l'ensemble au lieu de trois (noir, blanc, rouge).

Les Arabes sont les premiers à donner à la pierre philosophale des vertus médicinales et c'est par leur intermédiaire que le concept d'élixir est arrivé en Occident. Roger Bacon veut "prolonger la vie humaine".
La quête alchimique, de métallique aux origines, devient médicale au milieu du , avec le Pseudo-Arnaud de Villeneuve et Petrus Bonus. La notion de "médecine universelle" pour les pierres comme pour la santé vient du "Testamentum" du Pseudo-Lulle (1332). Johannes de Rupescissa (Jean de Roquetaillade) ajouta, vers 1352, la notion de quintessence, préparée à partir de l’ "aqua ardens" (alcool), distillée des milliers de fois ; il décrit l'extraction de la quintessence à partir du vin et explique que, conjointe à l'or, celle-ci conserve la vie et restaure la santé. Paracelse, en 1533, dans le "Liber Paragranum", va encore plus loin, en rejetant la transmutation comme but de l'alchimie, pour ne garder que les aspects thérapeutiques. Il a résumé ainsi sa pensée : "Beaucoup ont dit que l’objectif de l'alchimie était la fabrication de l’or et de l’argent. Pour moi, le but est tout autre, il consiste à rechercher la vertu et le pouvoir qui réside peut-être dans les médicaments." En un sens Paracelse fait donc de l'iatrochimie (médecine hermétique), plutôt que de l'alchimie proprement dite. Dès lors apparaît une opposition entre deux usages de la pierre philosophale, la production de l’or (chrysopée) ou la guérison des maladies (panacée). La iatrochimie (ou médecine hermétique) a eu "pour principal représentant François de Le Boë (Sylvius) et consistait à expliquer tous les actes vitaux, en santé ou en maladie, par des opérations chimiques : fermentation, distillation, volatilisation, alcalinités, effervescences." L'alchimie médicale a été étudiée par Alexander von Bernus. 

La légende veut que l'alchimiste Nicolas Flamel ait découvert l'élixir de jeunesse et l'ait utilisé sur lui-même et son épouse Pernelle. De même la légende du comte de Saint-Germain marqua l'alchimie, il aurait eu le souvenir de ses vies antérieures et une sagesse correspondante, ou aurait disposé d'un élixir de longue-vie lui ayant donné une vie longue de deux à quatre mille ans selon lui.

Aujourd'hui plusieurs laboratoires pharmaceutiques (Pekana, Phylak, Weleda…), revendiquant les remèdes spagyriques de Paracelse, de Rudolf Steiner, d'Alexander von Bernus, de Carl-Friedrich Zimpel, poursuivent cette tradition alchimique médicale.

L'alchimiste se présente comme un philosophe. Il prétend connaître non seulement les métaux, mais aussi les principes de la matière, le lien entre matière et esprit, les lois de transformation… Son ontologie repose sur la notion d'énergie, une énergie contradictoire, dynamique, une, unique, en métamorphoses. Il tire aussi une morale de ses travaux, l'éloge du travail et de la prière : "Prie et travaille ("Ora et labora")" (Khunrath). Il avance une grande méthode : l'analogie ("Tout ce qui est en bas est comme ce qui est en haut"). Sa notion-clef est celle d'origine, de retour, ou - comme le dit Pierre A. Riffard - de "réversion". L'alchimiste veut retourner à la matière première, rétablir les vertus primitives des choses, rendre pure et saine toute créature : faire nature, pourrait-on dire.

L'interprétation des buts poursuivis par l'alchimie est rendue plus difficile par les textes volontairement cryptiques laissés par les alchimistes. Cette difficulté d'interprétation a engendré de nombreuses thèses à propos du sens qu'il convenait de donner à l'alchimie. 

Les alchimistes se fondent sur une conception de la nature et de la matière première. Les théories s'opposent ou se combinent.

Depuis le , la théorie atomique a relégué l'alchimie au rang de pseudo-science. Paradoxalement, la physique nucléaire a montré que les transmutations de métaux sont possibles, reprenant d'ailleurs le terme, même si les théories alchimiques ont été réfutées.

Le laboratoire chimique doit énormément à l'alchimie, au point que certains positivistes (dont Marcellin Berthelot) ont qualifié l'alchimie de proto-chimie. 

Pourtant, l'objet de l'alchimie (la pierre philosophale et la transmutation des métaux) et celui de la chimie (l'étude de la composition, les réactions et les propriétés chimiques et physiques de la matière.) sont réellement distincts. D'autre part le rapport entre l'alchimie et les mythes locaux, et les constantes archétypiques universelles présentes dans la philosophie sous-jacente à l'alchimie la distinguent également de celle-ci. Plusieurs auteurs du qui ont étudié l'alchimie de manière approfondie la présentent comme une théologie, ou comme une philosophie de la Nature plutôt qu'une chimie naissante, à ce titre, certains anciens alchimistes se donnaient le titre de 'seuls véritables philosophes'.

L'interprétation de l'alchimie comme relevant uniquement d'une proto-chimie proviendrait essentiellement d'une erreur d'interprétation de Marcellin Berthelot au . Françoise Bonardel retient également l'hypothèse d'une simplification excessive opérée par certains historiens du .

Herbert Silberer, un disciple de Freud, est un précurseur de l'interprétation psychologique de l'alchimie.

La mise en évidence d'un symbole alchimique, similaire dans des civilisations éloignées dans le temps et dans l'espace, a conduit Carl Gustav Jung, très tôt, à valoriser l'alchimie comme processus psychologique. Il a particulièrement insisté sur l'intérêt psychologique ou spirituel ou même initiatique de l'alchimie. Elle aurait pour fonction « l'individuation », c'est-à-dire . Bernard Joly met en cause l'interprétation jungienne de l'alchimie qui la définissait comme un ensemble d'aspirations spirituelles.

Mircea Eliade, mythologue et historien des religions, défend dans "Forgerons et alchimistes" (1956) l'idée que l'alchimie, loin d'être l'ancêtre balbutiant de la chimie, représente un système de connaissances très complexe, dont l'origine se perd dans la nuit des temps, et commun à toutes les cultures (surtout asiatiques). Il développe l'idée, selon l'analogie du macrocosme et du microcosme, que les transformations physiques de la matière seraient les représentations des modalités des rites ancestraux, dans leur trame universelle : .

Gaston Bachelard, philosophe et historien des sciences, s'inspire des concepts jungiens pour établir une de la formation de la pensée. Dans "La Psychanalyse du feu", il tient l'alchimie pour une rêverie préscientifique, qui relève davantage de la poésie et de la philosophie que de la connaissance objective. Ses arguments sont que certains alchimistes, comme Nicolas de Locques et d'autres anonymes au , utilisent un vocabulaire sexuel pour désigner les vases, les cornues et l'ensemble des outils techniques utilisés en alchimie. Ainsi, la vision en partie inconsciente qu'ont les alchimistes du feu est une rêverie animiste et sexualisée, ils considèrent le feu comme une entité vivante et génératrice. Dans "La lumière sortant de soi-même des ténèbres" (1693), il est même fait mention d'un feu masculin, qui est agent, et d'un feu féminin, qui est caché, or en psychanalyse est un . Par conséquent, Bachelard peut écrire qu' et qu'elle est .

Déjà dans "La Formation de l'esprit scientifique", Bachelard tenait l'alchimie pour une discipline qui fait obstacle au progrès scientifique plus qu'elle n'y participe. Sa théorie historique repose de façon générale sur l'idée que l'homme est travaillé par des intuitions primitives, qui sont d'ordre affectif et inconscient, et qui poussent l'homme à se faire une représentation illusoire de la réalité. La connaissance scientifique se construirait alors en avec ces intuitions. En mathématisant le réel par exemple, nous passerions d'une rêverie vague et qualitative sur la matière à un savoir quantitatif et précis sur elle. L'alchimie serait plutôt une approche qualitative qui tend à substantialiser la matière. Bachelard écrit que . Ce rapport affectif à la nature est cependant inévitable en première approche selon l'auteur, qui ajoute que . Le sociologue Émile Durkheim écrit de même que l'alchimie, tout comme l'astrologie, repose sur des , c'est-à-dire des illusions subjectives qui répondent à des besoins pratiques de l'homme (la recherche de la pierre philosophale pour la richesse et la santé), et non sur des explications scientifiques qui auraient rompu avec ces illusions. 

Barbara Obrist et Bernard Joly contestent la lecture historique de Bachelard. Là où le philosophe cherche à établir une rupture entre l'esprit préscientifique et l'esprit scientifique, lorsque ce dernier surmonte la connaissance concrète et qualitative pour aller vers une connaissance abstraite et quantitative, Bernard Joly insiste plutôt sur la continuité voire l'indistinction entre l'alchimie ancienne et la chimie moderne. Il veut démontrer, en interprétant des textes d'Étienne-François Geoffroy et d'autres chimistes/alchimistes, que le fait que la transmutation des métaux soit un échec n'implique pas que ses pratiquants soient des rêveurs illusionnés. Au contraire, les alchimistes seraient des scientifiques au sens que prenait la science à leur époque, s'efforçant de connaître le monde objectivement et de construire des protocoles expérimentaux. Ce serait la physique cartésienne qui aurait tenté dès le de mettre un coup d'arrêt à la fois à l'alchimie et à la chimie non mécanistes, en les accusant d'être de fausses sciences pratiquées par des imposteurs.

Pour Joly, l'alchimie est une démarche essentiellement rationnelle (ce qui n'exclut pas que çà et là des imposteurs et des charlatans se soient servis de cette discipline). L'enjeu est de ne pas cantonner l'alchimie dans une sorte d'ésotérisme irrationnel, ésotérisme qui serait la possession exclusive d'« adeptes » et d'« initiés » s'immunisant contre les critiques faites à leur propre interprétation de l'alchimie.

En tant que connaissance ésotérique, les textes alchimiques possèdent la particularité d'être codés. Il s'agit d'un savoir qui n'est transmis que sous certaines conditions. Les codes employés par les anciens alchimistes étaient destinés à empêcher les profanes d'accéder à leurs connaissances. L'utilisation d'un langage poétique volontairement obscur, chargé d'allégories, de figures rhétoriques, de symboles et de polyphonie (voir langues des oiseaux) avait pour objet de réserver l'accès aux connaissances à ceux qui auraient les qualités intellectuelles pour déchiffrer les énigmes posées par les auteurs et la sagesse pour ne pas se laisser tromper par les pièges nombreux que ces textes recèlent.

Le même nom peut qualifier deux 'objets' ou 'sujets' totalement différents mais l'on peut aussi avoir plusieurs noms pour désigner le même objet. Ceci est particulièrement vrai pour le Mercure mais également pour d'autres termes. 

Presque tous les traités d'alchimie commencent au début du second œuvre et "omettent" de préciser quelle matière première utiliser et cette énigme de la matière première est sciemment recouverte par l'énigme du Mercure selon René Alleau. Fulcanelli, par exemple, s'emploie à multiplier les indications tout en restant cryptique. Synésius semble plutôt décrire la matière dans son état avancé. La matière aux mille noms, terme employé par Françoise Bonardel, demeure une énigme à double fond. Cet auteur résume la problématique ainsi: « Car si la force de l’alchimie réside bien dans le seul mercure des philosophes, comme le proclama très tôt Albert le Grand (1193-1280), c’est que la substance mercurielle, par excellence protéiforme, est alors envisagée soit comme une materia prima en qui sont latentes toutes les virtualités (dont celle du soufre), soit, après préparation, comme mercure double (ou hermaphrodite) en qui a été consommé et fixé l’union des 2 principes ».

Le symbole allégorique ne se recoupe pas avec le symbole chimique et, par exemple, le mercure alchimique n'est pas le mercure chimique. Voici quelques exemples de symboles : 

Pour l'alchimiste les quatre éléments ne représentent pas des composantes de la matière, en effet l'unicité de la matière est un des principes philosophiques de l'alchimie, mais plutôt des états de cette matière unique se rapprochant plus du concept physique d'état de la matière. Ces quatre éléments sont avec leurs symboles associés: 

Pour l'alchimiste les sept métaux sont liés aux planètes et aux astres:




À partir du va se développer une lecture alchimique de la Bible .



La lecture alchimique de la fable antique va se développer à la Renaissance.

Selon Serge Hutin, il existe une interprétation alchimique de la poésie au Moyen Âge, notamment du "Roman de la Rose" et de la "Divine Comédie". La Rose serait par exemple le symbole à la fois de la Grâce divine et de la Pierre philosophale.


Selon R. Halleux, « l'idée que des monuments ou des œuvres d'art contiennent un symbolisme alchimique n'est pas très ancienne. En 1612 paraît le "Livre des figures hiéroglyphiques" de Nicolas Flamel, qui se présente comme une explication alchimique des figures gravées par le célèbre adepte sur une arche du cimetière des Innocents à Paris. En 1636, un certain de Laborde interprète hermétiquement la statue de Saint Marcel au porche de Notre-Dame de Paris, et, en 1640, Esprit Gobineau de Montluisant écrit une "Explication très curieuse des énigmes et figures hiéroglyphiques physiques qui sont au grand porche de l'église cathédrale et métropolitaine de Notre-Dame de Paris". Cette tradition inspire les travaux d'hermétistes comme Cambriel, Fulcanelli, Canseliet qui prétendent reconnaître ainsi l'empreinte alchimique dans un certain nombre de monuments du Moyen Âge ou de la renaissance : Notre-Dame de Paris, chapelle Saint Thomas d'Aquin, Sainte Chapelle, cathédrale d'Amiens, palais de Jacques Cœur à Bourges, hôtel Lalemant à Bourges, croix de Hendaye, église Saint Trophime à Arles, château de Dampierre-sur-Boutonne, villa Palombara sur l'Esquilin à Rome, château du Plessis-Bourré, etc. Cette démarche aboutit à des résultats invraisemblables. »

Des travaux historiques solides ont paru, dont Jacques van Lennep, "Art et Alchimie. Étude de l'iconographie hermétique et de ses influences" (1966) et Alexander Roob, "Alchimie et Mystique" (Taschen, 2005).

Comme le dit Jacques Bergier, "l'alchimie est la seule pratique para-religieuse ayant enrichi véritablement notre connaissance du réel."

Marie la Juive (au début du ? à Alexandrie) a inventé le fameux "bain-marie", dispositif dans lequel la substance à faire chauffer est contenue dans un récipient lui-même placé dans un récipient rempli d'eau, ce qui permet d'obtenir une température constante et modérée.

Dans la ville d'Alexandrie, on trouve une importante corporation de parfumeurs, possédant des alambics ("ambikos") pour distiller des élixirs, des essences florales ; Zosime de Panopolis, vers 300, présente une illustration d'un alambic pour métaux, raffiné.

Geber (Jâbir ibn Hâyyan), mort vers 800, découvre divers corps chimiques : l'acide citrique (à la base de l'acidité du citron), l'acide acétique (à partir de vinaigre) et l'acide tartrique (à partir de résidus de vinification). Albert le Grand réussit à préparer la potasse caustique, il est le premier à décrire la composition chimique du cinabre, de la céruse et du minium. Le Pseudo-Arnaud de Villeneuve, vers 1330, ou Arnaud lui-même, découvre les trois acides sulfurique, muriatique et nitrique ; il compose le premier de l'alcool, et s'aperçoit même que cet alcool peut retenir quelques-uns des principes odorants et sapides des végétaux qui y macèrent, d'où sont venues les diverses eaux spiritueuses employées en médecine et pour la cosmétique. Le Pseudo-Raymond Lulle (vers 1330) prépare le bicarbonate de potassium. En 1352, Jean de Roquetaillade (Jean de Rupescissa) introduit de la notion de quintessence, obtenue par distillations successives de l"'aqua ardens" (l'alcool) ; cette idée d'un principe actif sera essentielle dans l'histoire de la médecine, car il introduit un grand nombre de médicaments chimiques, tels que la teinture d'antimoine, le calomel, le sublimé corrosif. 

Paracelse est un pionnier de l'utilisation en médecine des produits chimiques et des minéraux, dont le mercure contre la syphilis, l'arsenic contre le choléra. Il crée la médecine du travail, la toxicologie, la balnéothérapie. Vers 1526 il crée le mot "zinc" pour désigner l'élément chimique zinc, en se référant à l’aspect en pointe aiguë des cristaux obtenus par fusion et d’après le mot de vieil allemand "zinke" signifiant "pointe". 

Basile Valentin décrit vers 1600 l'acide sulfurique et l'acide chlorhydrique.

Jan Baptist Van Helmont, "précurseur de la chimie pneumatique" (Ferdinand Hoefer), révèle vers 1610, d’une façon scientifique, l’existence des "gaz", comme il les nomme, et en reconnaît plusieurs. Il identifie l’un d’eux, le "gaz sylvestre" (gaz carbonique), qui résulte de la combustion du charbon, ou de l’action du vinaigre sur certaines pierres, ou de la fermentation du jus de raisin. Pour Van Helmont, le gaz constitue l’ensemble des "exhalaisons" dont l’air est le réceptable.

Alchimiste à Hambourg, Hennig Brandt découvre le phosphore en 1669 en cherchant l'alkaest dans l'urine..

Isaac Newton s'intéresse aux pratiques alchimiques. Dans son "Optique" (1704), à la Question 31, il caractérise la chimie comme étant le lieu de forces attractives et de forces répulsives qui peuvent se manifester à courte distance. Cela lui permet d'expliquer le déplacement d'un métal dans un sel par un autre métal, et propose ce qui constitue la première échelle d'oxydoréduction des métaux. Il explique l'élasticité des gaz, la cohésion des liquides et des solides…

La création de la porcelaine en Occident revient, en 1708, à un alchimiste, Johann Friedrich Böttger, qui prétendait pouvoir fabriquer de l'or à partir de métaux non précieux. Böttger parvient à percer le secret de la pâte de porcelaine. 

La notion de transmutation a semblé absurde aux positivistes. Pourtant, Ernest Rutherford, en 1919, réalise la première transmutation artificielle : en bombardant de l'azote avec les rayons alpha du radium, il obtient de l'oxygène.

La franc-maçonnerie a repris de nombreux codes, symboles et rituels alchimiques.

L'alchimie est explicitement nommée et intégrée dans la poésie et la littérature par des auteurs symbolistes et surréalistes comme Stéphane Mallarmé, Joris-Karl Huysmans, Arthur Rimbaud, Maurice Maeterlinck et André Breton.

L'écrivain et théoricien littéraire Roger Laporte explique que Stéphane Mallarmé compare la quête artistique du « Livre » à la recherche du Grand Œuvre alchimique. Pour Laporte, il ne s'agit pas ici de l'alchimie au sens de la transmutation des métaux en or, mais de la fabrication d'une œuvre d'art, quitte à (l'expression est de Mallarmé). Mallarmé a été initié à l'alchimie et à la kabbale. , il se sert du symbolisme alchimique dans son écriture poétique. Cependant, Mallarmé critique l'alchimie comme pratique réelle et ne se sert du terme que de façon métaphorique : . La réalisation matérielle de l'or ne l'intéresse pas, elle n'est pour lui qu'une question d'économie politique. C'est l'or poétique et littéraire qu'il faut chercher, selon le poète français.

Le poète Arthur Rimbaud reprend la comparaison de la poésie à l'alchimie. Un poème du recueil "Une saison en enfer" s'intitule « Alchimie du verbe ». Michel Arouimi, spécialiste de l'œuvre de Rimbaud, parle d' et d', pour évoquer la façon dont Rimbaud marie les langues. Le jeune écrivain construit une poétique à partir de mélanges, entre le rythme et la violence par exemple.

L'écrivain surréaliste André Breton parle d' dans les "Manifestes du surréalisme". Il affirme qu'il faut prendre l'expression rimbaldienne « Alchimie du Verbe » au pied de la lettre. La poésie surréaliste se veut alors une transmutation spirituelle et intérieure, grâce à la faculté de l'imagination qui dépasse le rationalisme et s'élève au sens symbolique des choses. Selon Anna Balakian, .

En 1906, le catalogue de Fergusson recensait auteurs alchimistes et plus ouvrages, sur la base de la seule du docteur James Young. Serge Hutin précise (en 1951) qu'« il reste aussi un grand nombre de manuscrits inédits dans toutes les bibliothèques d'Europe ; un assez petit nombre seulement a été édité ». On estime le nombre des auteurs connus à plus de et le nombre des traités, écrits et études à plus de 100 000.

Pour une revue des travaux sur l'alchimie, voir Bernard Joly, « Bibliographie », "Revue d'histoire des sciences", vol. 49, , 1996, .














Dans "La Montagne sacrée" ("The Holy Mountain"), réalisé par Alejandro Jodorowsky en 1973, ce dernier joue le rôle d'un alchimiste sans nom précisé qui initie le héros à la sagesse.




</doc>
<doc id="281" url="https://fr.wikipedia.org/wiki?curid=281" title="Anastylose">
Anastylose

L'anastylose (du grec ancien , composé de : « de nouveau » et : « ériger ») est un terme archéologique qui désigne la technique de reconstruction d'un monument en ruines grâce à l'étude méthodique de l'ajustement des différents éléments qui composent son architecture. La reconstruction est faite en utilisant les fragments trouvés sur place avec des matériaux modernes, de couleur et de qualités différentes, de sorte que l'on puisse distinguer à l’œil nu l'ancien du moderne et préserver les pierres antiques de l'altération (par exemple en utilisant des matériaux légers). 

Cette technique doit être appliquée avec précautions parce qu'elle s'appuie sur des hypothèses. L'anastylose obéit au principe de réversibilité, c'est-à-dire qu'on puisse démonter la reconstitution en cas d'erreur.

Il peut aussi s’agir d’éléments reconstitués en matériaux contemporains pour présenter un détail de construction donnant l’échelle d’un édifice, comme cela a été le cas pour l'anastylose du site de Glanum à Saint-Rémy-de-Provence, Bouches-du-Rhône (reconstitution de la partie frontale et sud-est du plus petit des temples géminés). Si l’anastylose est assez souvent possible pour les monuments antiques en grand appareil, où chaque bloc avait une place définie, il est beaucoup plus difficile à réaliser pour des monuments aux pierres interchangeables comme les édifices médiévaux. L’anastylose partielle de ceux-ci n’est possible que pour les parties trouvées en connexion (généralement en fouilles).
Quand des éléments sont manquants, on peut avoir recours à des ajouts d'éléments modernes (ciment, plâtre, résine…)

La prudence est cependant de mise pour retenir la solution de l'anastylose, et dans tous les cas le choix de cette technique doit être précédé d'une étude scientifique préalable collégiale. En effet elle pose un certain nombre de questions :

L'un des premiers sites à avoir été restauré grâce à la technique de l'anastylose est le trésor des Athéniens, dans le sanctuaire oraculaire panhellénique d'Apollon Pythien à Delphes, par l'École française d'Athènes, à partir de 1901.

Exemples d'anastylose :


</doc>
<doc id="284" url="https://fr.wikipedia.org/wiki?curid=284" title="Andrew Wiles">
Andrew Wiles

Andrew John Wiles (né le à Cambridge, Angleterre) est un mathématicien britannique, professeur à l'université d'Oxford, en Angleterre. Il est célèbre pour avoir démontré le grand théorème de Fermat (1994). Il est lauréat du prix Abel 2016.

Après avoir obtenu son diplôme de bachelor au Merton College de l'université d'Oxford, il entre au Clare College en 1974 pour y préparer un "Ph.D." en mathématiques sur les lois de réciprocité et la conjecture de Birch et Swinnerton-Dyer, qu'il obtient en 1979. Il devient professeur à Princeton en 1981, poste qu'il conserva jusqu'en 2011. Il enseigne, entre-temps, à l'École normale supérieure entre 1985 et 1986 et à Oxford de 1988 et 1990. Il retourne finalement à Oxford en 2011.

En ce qui concerne la , l'odyssée commence en 1985, quand Kenneth Ribet, partant d'une idée de Gerhard Frey, démontre que ce théorème résulterait de la conjecture de Shimura-Taniyama-Weil qui affirme que toute courbe elliptique est paramétrable par une forme modulaire. Bien que moins familière que le théorème de Fermat, cette conjecture est plus significative, car elle touche au cœur de la théorie des nombres.

Cependant, personne n'a la moindre piste de travail pour la démontrer. Travaillant dans le plus grand secret pendant huit ans, et faisant part de ses idées et progrès à Nicholas Katz, un collègue de Princeton, Wiles démontre la conjecture de Shimura-Taniyama-Weil et, par conséquent, le théorème de Fermat. Comme toute démonstration de cette ampleur, elle est un tour de force riche en nouvelles idées.

Pour expliquer (par Wiles) et vérifier (par Katz), pas à pas, cette démonstration sans éveiller les soupçons, Wiles et Katz ont l'idée d'organiser un cours de doctorat intitulé "Calculs sur des courbes elliptiques", ouvert aux étudiants et professeurs. Peter Sarnak avait lui aussi été mis dans le secret. Wiles annonce donc trois conférences (les 21, 22 et 23 juin 1993) sans en donner l'objet, ce qu'il ne fait que lors de la dernière en précisant que le grand théorème de Fermat est un corollaire de ses principaux résultats.

Dans les mois qui suivent, le manuscrit de sa démonstration circule auprès d'un petit nombre de mathématiciens. Plusieurs critiques sont émises contre la démonstration que Wiles a présentée en 1993, presque toutes de l'ordre du détail et résolues rapidement, sauf une, qui met en évidence une lacune. Avec l'aide de Richard Taylor, Wiles réussit à contourner le problème soulevé, en octobre 1994. Son travail met ainsi fin à une recherche qui a duré plus de 300 ans.

Il est aussi l'auteur d'autres travaux importants en théorie des nombres. Avec John Coates (qui fut son directeur de thèse), il a obtenu plusieurs résultats sur la conjecture de Birch et Swinnerton-Dyer et a collaboré avec Barry Mazur sur les extensions cyclotomiques.

Récipiendaire dès 1988 du prix Whitehead pour ses résultats innovants dans le domaine des courbes elliptiques, il reçoit plusieurs prix pour sa preuve du dernier théorème de Fermat, dont le prix Schock en 1995, le prix Ostrowski en 1995, le prix Fermat en 1995, le prix Wolf en 1996, le prix Cole en 1997, le prix du Clay Mathematics Institute en 1999 et le prix Shaw en 2005. Ayant dépassé l'âge de quarante ans au moment de sa découverte, il n'a pas pu être honoré de la médaille Fields, mais a reçu une récompense officielle de l'Union mathématique internationale lors de son congrès de 1998.

Il est fait chevalier commandeur de l'Ordre de l'Empire britannique (KBE) en 2000.

En mars 2016, il reçoit le prix Abel « pour sa démonstration stupéfiante du dernier théorème de Fermat en utilisant la conjecture de modularité pour les courbes elliptiques semi-stables, ouvrant une ère nouvelle en théorie des nombres ».

En 2017, il reçoit la médaille Copley de la Royal Society.





</doc>
<doc id="289" url="https://fr.wikipedia.org/wiki?curid=289" title="Autorisation d'un produit phytopharmaceutique">
Autorisation d'un produit phytopharmaceutique

Dans l'Union européenne, l'autorisation d'un produit phytopharmaceutique est définie par la directive 91/414/CEE du 15 juillet 1991, comme suit : 

un acte administratif par lequel l'autorité compétente d'un État membre autorise, à la suite d'une demande déposée par un demandeur, la mise sur le marché d'un produit phytopharmaceutique sur son territoire ou une partie de celui-ci.

En France, le Ministère de l'agriculture et de la pêche délivre les autorisations de mise sur le marché (AMM) des produits phytopharmaceutiques. Depuis le juillet 2006, en application de la loi d'orientation agricole du 5 janvier 2006, l'Agence française de sécurité sanitaire des aliments est chargée de l’évaluation de la toxicologie et de l'efficacité des produits phytopharmaceutiques et des adjuvants, préalable à cette autorisation.




</doc>
<doc id="291" url="https://fr.wikipedia.org/wiki?curid=291" title="Alvar Aalto">
Alvar Aalto

Hugo Alvar Henrik Aalto (né le à Kuortane et mort le à Helsinki) est un architecte, dessinateur, urbaniste et designer finlandais, adepte du fonctionnalisme et de l'architecture organique. Son travail comprend également le mobilier, les textiles et la verrerie, ainsi que des sculptures et des peintures, bien qu'il n'ait jamais été considéré comme un artiste : il considère la peinture et la sculpture comme les . Le début de carrière d'Aalto va de pair avec la croissance économique rapide et l'industrialisation de la Finlande, au cours de la première moitié du et bon nombre de ses clients étaient des industriels, dont la famille Ahlström-Gullichsen. L'étendue de sa carrière, des années 1920 aux années 1970, se reflète dans les styles de son œuvre, allant du classicisme nordique des premières années, au modernisme rationnel de style international, dans les années 1930, en passant par un modernisme plus organique, à partir des années 1940. Cependant, ce qui caractérise toute sa carrière, c'est le souci du design en tant qu' œuvre d'art totale (en , qui fait qu'avec sa première femme Aino Aalto, il conçoit non seulement les bâtiments, mais aussi les surfaces intérieures, les meubles, lampes, ameublement et la verrerie. Ses créations de meubles sont considérées comme des meubles , dans le sens d'un souci de matériaux, en particulier, le bois, et de simplification mais aussi d'expérimentation technique, ce qui lui a valu des brevets pour divers procédés de fabrication, tels que le bois courbé. Le musée Alvar Aalto, conçu par Aalto lui-même, est situé dans ce qui est considéré comme sa ville natale de Jyväskylä.

Nombre de ses bâtiments s'intègrent de façon harmonieuse dans le paysage, avec lequel ils forment un tout architectural. Le bois et la brique constituent ses matériaux de prédilection. Alvar Aalto a conçu lui-même les meubles pour la plupart de ses bâtiments. On lui doit entre autres la Villa Mairea à Noormarkku, la Maison Finlandia à Helsinki et le campus de l'Université Technique d'Helsinki.

Hugo Alvar Henrik Aalto naît à Kuortane. Ses parents, Johan Henrik Aalto et Selly Matilda (née Hackstedt), sont respectivement arpenteur-géomètre finnophone et agente de poste suédophone. Alors qu’Aalto a 5 ans, sa famille déménage à Alajärvi, puis à Jyväskylä. Il y étudie jusqu’en 1916, et prend des cours de dessin auprès d’un artiste local du nom de Jonas Heiska.
De 1916 à 1921, il fait ses études d’architecte à l’Université Technique d'Helsinki, puis entre dans la vie active.

Hugo Alvar Henrik Aalto est le plus célèbre architecte et designer finlandais. Il est aussi l’un des plus importants pionniers du design organique. Dès son plus jeune âge, ce fils de géomètre-arpenteur a manifesté de l’intérêt et des dispositions pour les arts.

Il s’occupe d’abord de la conception d’expositions et fait de nombreux voyages en Europe centrale, en Italie et en Scandinavie. En 1923, il ouvre son cabinet d’architecture à Jyväskylä. L’année suivante il épouse sa consœur Aino Marsio, qui deviendra sa plus proche collaboratrice.

S'intéressant aux arts décoratifs, il crée des modèles d'objets usuels et de meubles en bois laminés et courbés. Il met au point plusieurs modèles de sièges emblématiques du nouveau design des pays nordiques. Néanmoins, c’est d’abord en qualité d’architecte qu’Aalto va s'imposer sur la scène internationale.
En 1929, il dessine l’immeuble du journal "Turun Sanomat" à Turku (son premier bâtiment fonctionnaliste), et deux ans plus tard il participe à la conception de l’exposition du de Turku, c'est son premier projet complet de style moderne présenté au public finlandais.

Ces débuts, déjà remarqués, seront suivis par de nombreuses réalisations architecturales largement saluées, la bibliothèque de Viipuri (maintenant Vyborg en Russie) (1927-1935), le sanatorium de Paimio (1929-1933) et le pavillon de la Finlande pour l’Exposition universelle de Paris en 1937 et de New York en 1939.
Il joua un important rôle d'urbaniste en Finlande après la guerre (plans d'aménagement et plans généraux de Rovaniemi, Nynäshamn et Imatra).

Dès la fin des années 1920, Alvar Aalto, architecte et designer, se démarque de ses contemporains Walter Gropius, Le Corbusier ou Marcel Breuer, dont le rationalisme renvoie à l’utilisation de matériaux industriels comme l’acier et le verre qu'il considère trop froids. Il propose une vision plus humaniste et plus proche de la nature : il fait alors du contreplaqué son matériau de prédilection. Dès 1927 le couple Aalto-Marsio mène, avec Otto Korhonen, directeur technique d’une fabrique de meubles de la région de Turku, des expérimentations sur le contreplaqué collé et courbé. Ces expériences vont conduire à ses sièges les plus innovants sur le plan technique, le fauteuil 41 (1931-1932) et le 31 (1931-1932), en porte-à-faux, l’un des contemporains, l’autre faisant partie de son projet de "Gesamtkunstwerk" (œuvre totale) pour le sanatorium de Paimio. Grâce à sa technique de cintrage du bois, qui va permettre, pour la première fois, d’ancrer les pieds directement sous l’assise sans faire appel à un quelconque châssis ou à une structure supplémentaire, Aalto conçoit, en 1933, des séries de sièges à piètements en L ("L-leg", 1932-1933), en Y ("Y-leg", 1946-1947) et en éventail ("fan-leg", 1954) dont le tabouret empilable à la demande de la bibliothèque de Viipuri.

À la fois fonctionnel et séduisant, ce design va immédiatement signaler à l’avant-garde internationale la nouvelle voie ouverte par l’usage de contreplaqué et l’émergence d’un vocabulaire de formes plus douces et chaleureuses. En 1933, ses créations, présentées à la boutique Fortnum et Mason de Londres connaissent un succès international. Pour répondre à l'afflux de commandes, le couple Aalto-Marsio fonde en 1935 la société Artek, qui édite également ses luminaires et créations de verre.

Aalto collabore aussi en indépendant avec les verres Riihimäki (1933) et Iittala (1936). Tout comme son mobilier et son architecture, ses créations en verre se caractérisent par leurs formes organiques. Le "Vase Savoy", aussi connu sous le nom "Vase Aalto", de 1936, est devenu un classique. Sa forme sinueuse de cet objet pourrait être aussi une allusion à son nom de famille : "aalto" signifie vague en finnois. Quoi qu’il en soit, les lignes rythmiques et asymétriques du "Savoy" expriment la quintessence de la nature et annoncent les formes fluides qui seront la marque du design finlandais d’après-guerre. Inspirée par la relation entre l’homme et la nature, la démarche globaliste et humaniste d’Aalto est le terreau philosophique sur lequel le design finlandais s’est développé et épanoui.
Adepte convaincu de la vocation humanisante du design, Aalto refusait non seulement les formes géométriques rigides mais aussi les tubes métalliques et autres matériaux artificiels, qu’il jugeait trop éloignés de la nature. Son travail fut particulièrement bien accueilli au Royaume-Uni et aux États-Unis dès les années 1930 et 1940. Ses idées de « père fondateur du design organique » ont beaucoup influencé des designers d’après-guerre comme Charles Eames et Ray Eames.

En 1952, Aalto épouse l’architecte Elissa Mäkiniemi, avec qui il collabore jusqu'à sa mort. Le musée d’art moderne de New York lui a consacré trois grandes expositions (en 1938, 1984 et 1997).
Il a réalisé une œuvre abondante et très diverse dans le domaine de l'architecture collective industrielle ou privée (dortoir du MIT à Cambridge, Massachusetts, 1947-1949 ; maison de la culture à Helsinki, 1955-1958, etc.). Plus constructeur que théoricien, il a pris des partis fonctionnels et utilisé des éléments standardisés, mais il a surtout fait preuve d'une extrême liberté formelle. Évitant le recours systématique aux orthogonales, il a souvent préféré les lignes courbes ou obliques en rapport avec un plan libre et asymétrique, engendrant un espace continu aux subtiles articulations. Enfin, il s'est surtout préoccupé d'adapter ses constructions à la spécificité du programme et de les d'harmoniser avec le site environnant. Sa démarche s'apparente à bien des égards à celle de Frank Lloyd Wright.







</doc>
<doc id="292" url="https://fr.wikipedia.org/wiki?curid=292" title="Anarcho-capitalisme">
Anarcho-capitalisme

L’anarcho-capitalisme est un courant de pensée politique inspiré par le libéralisme philosophique, selon laquelle l’existence de l’État est illégitime et inutile. Ce courant est une branche du libertarianisme, différent du minarchisme, qui soutient quant à lui l'existence d'un État minimal pour tous (« État veilleur »).

Il se sépare du libéralisme classique, lequel reconnaît la nécessité d’un État et ne vise qu’à limiter de façon stricte son domaine et ses modes d’intervention. Il se distingue aussi des courants historiques traditionnels (socialistes, fédéralistes, individualiste) de l’anarchisme par son acceptation sans limite de la propriété privée, et accessoirement, son appartenance aux libertariens, son absence de critique de la religion entre autres.

Les anarchistes traditionnels considèrent plutôt que l'anarcho-capitalisme appartient à l'aile droite des libéraux et pas au courant historique de l'anarchisme, en dépit de son rejet de l'État ; ceci du fait qu'il ne partage pas avec l'anarchisme historique son . Il en va de même pour la plupart des anarchistes individualistes.

Les anarcho-capitalistes appliquent de manière stricte les thèses du libéralisme pour en tirer une philosophie politique qu'ils jugent seule cohérente pour organiser la société. Un anarcho-capitaliste est aussi appelé un « anarcap »(ou "ancap"). Une société humaine organisée selon les principes de l'anarcho-capitalisme est appelée une « anarcapie ».

Comme le libéralisme classique, l'anarcho-capitalisme revendique un système où chaque être humain est pleinement propriétaire de lui-même, des fruits de son travail et de ce qu'il a obtenu par la coopération volontaire avec autrui, par échange ou par don. Tout être humain est aussi comptable de ses actes, tenu par les engagements qu'il prend, responsable des pertes de son travail et débiteur pour les torts qu'il a causés à des tiers non consentants.

Les anarcho-capitalistes considèrent que seules les interactions entre adultes consentants sont légitimes. Toute atteinte à la personne et à la propriété perpétrée sans consentement constitue dès lors une agression, et toute forme d'organisation coercitive est illégitime, y compris l'État et ses multiples succédanés.

Pour les anarcho-capitalistes, un État, comme toute autre organisation, ne saurait avoir de légitimité qu'auprès de ceux qui l'ont individuellement et volontairement accepté. En particulier, les contributions obligatoires (impôts directs et indirects, etc.) et les réglementations imposées (législation, décrets, mesures administratives, etc.) sont considérées comme illégitimes.

Pour les anarcho-capitalistes comme pour les libéraux classiques, la propriété privée est une composante essentielle de la liberté. Des anarchistes individualistes admettent la propriété privée mais sans la possibilité de la capitaliser.

La mise en commun du capital, la répartition des tâches et des responsabilités, la spécialisation des compétences et l'échange des services sont des moyens complémentaires de produire davantage de satisfactions. La garantie que ces moyens bénéficient au plus grand nombre est que chacun peut décider librement de participer ou de ne pas participer aux termes de l'accord, et d’en utiliser ou non les fruits. C'est le caractère volontaire d'un accord qui est garant tout à la fois et de sa légitimité et de son caractère bénéfique.

Cela n'empêche aucunement l'existence de communautés pratiquant un socialisme volontaire avec propriété commune, tant que celui-ci n'est pas coercitif, mais est un système d'échanges entre individus consentants ou entre organisations volontaires (une entreprise étant vue comme un « ensemble de contrats »).

Les anarcho-capitalistes récusent la nécessité de l'État pour garantir la propriété privée, voyant au contraire en lui le premier et le plus grand « criminel » contre la propriété privée. En revanche, ils font observer que, dans toute forme de propriété collective, une institution est nécessaire pour exercer les droits de propriété. Si tout est propriété collective, l’autorité de cette institution s’étend par définition à tout et à tous, et elle a de ce fait tous les attributs d’un État totalitaire, quel que soit le nom qu’on lui donne et quelles que soient ses modalités de fonctionnement. Par ailleurs, ils assimilent toute violation de liberté à une violation d'un droit de propriété (propriété de soi lors d'arrestations arbitraires, propriété de ses moyens de communication lors de censure, etc.). Pour ces deux raisons, ils accusent l’anarchisme socialiste – qui prétend combattre collectivement toute autre propriété que la propriété d'usage sur les biens – d’incohérence.

On peut distinguer au moins deux tendances anarcho-capitalistes :

Chaque tendance suit la même démarche, qui consiste à établir l'indissociabilité entre la propriété et la liberté, la désirabilité de ces droits, et la possibilité pratique de parvenir à une organisation de la société respectant ces droits. Mais elles montrent des disparités profondes dans l'application pratique de leurs principes, et ce même à l'intérieur de chacune de ces tendances.

L'approche jusnaturaliste remet en question la légitimité des droits de propriété en usage actuellement en les limitant aux seuls droits acquis conformément au « homesteading » développé à l'origine par John Locke, propose des méthodes pour réviser ces droits au cas par cas pour rétablir la légitime propriété au détriment des propriétaires actuels, et érige sa définition de la liberté et de la propriété comme universelle (le droit naturel est prioritaire sur le droit positif).

L'approche utilitariste part généralement de l'état actuel des droits de propriété sans chercher à établir de base absolue et universelle comme origine de ces droits, conservant la possibilité de justifier de manière utilitariste certaines violations de ces mêmes droits (la démarche utilitariste est alors prioritaire sur la liberté et la propriété).

Une conséquence de ces divergences est que la première approche ne permet pas l'application de n'importe quel contrat, mais seulement de ceux qui sont des échanges de titres de propriété valables. Ainsi, l'esclavage contractuel est impossible dans cette interprétation, car le libre-arbitre humain est inaliénable et inséparable du corps de l'individu. La seconde approche n'a pas ces limites, mais s'appuie sur la catallaxie pour faire émerger des règles communes acceptables par tous en matière de contrats.

Dans une situation où la violation des droits d'une personne permettrait assurément d'apporter plus en retour à une autre personne, l'approche utilitariste permet une telle violation, quand l'approche jusnaturaliste s'y oppose. Ainsi les utilitaristes peuvent justifier le sacrifice d'une personne pour en sauver plusieurs, mais pas les jusnaturalistes.

Les utilitaristes justifient donc le modèle de société anarcho-capitaliste par le fait que ce serait le plus efficace économiquement et par conséquent le plus désirable, tandis que les jusnaturalistes le justifient par le fait qu'il serait seul capable de respecter tous les droits fondamentaux des individus.

Une autre différence fondamentale se retrouve dans leurs conceptions de la justice :

Quel programme les anarcho-capitalistes proposent-ils concrètement ? Voici quelques idées provenant de deux figures emblématiques de l'anarcho-capitalisme : David Friedman (dans "Vers une société sans état") ; et Murray Rothbard (dans "L'Éthique de la Liberté") :


De nombreux projets d'utopies anarcho-capitalistes ont été formulés. Le plus célèbre et le plus avancé est celui dit de Seasteading, développé par le Seasteading Institute, qui vise à bâtir des îles dans les eaux internationales pour les affranchir du contrôle de tout État. L'institut a été fondé par Patri Friedman (fils de David Friedman) et est financé notamment par Peter Thiel.

Les anarchistes traditionnels considèrent que la position anarcho-capitaliste est incohérente et contrevient à la définition historique de l'anarchisme. Tout d'abord la propriété privée des moyens de production donnerait selon eux aux capitalistes un pouvoir, une autorité, de même nature que ceux d’un État. La propriété privée de l'outil de production doit donc être rejetée au nom des deux positions définissant historiquement l'anarchisme : la défense du principe d'égalité et le refus du principe d'autorité. Les anarcho-capitalistes défendent l'égalité en droit et non l'égalité sociale. Or cette égalité réelle ne pourrait, selon les anarchistes traditionnels être respectée dans une société qui accepte et valorise la propriété privée des moyens de production ; les anarchistes reprochent en outre aux anarcho-capitalistes utilitaristes d'accorder aux individus le droit de renoncer à leurs droits, c'est-à-dire de consentir librement à la domination, ce qui légitimerait l'esclavage initialement consenti.

De plus, pour les anarchistes, reprenant partiellement la théorie du contrat social, la propriété privée ne serait pas un droit naturel, mais une construction sociale qui exige l’action d’un État pour se maintenir. L'anarchiste Bob Black estime ainsi que le libre-échange ne peut exister sans État. 
Pour ces raisons, les anarchistes traditionnels refusent l’utilisation du mot « anarchisme » par les anarcho-capitalistes, pensée qu'ils déformeraient en la réduisant à l'anti-étatisme et à la défense de la liberté absolue oubliant au passage les valeurs anti-autoritaire, égalitaire et solidaire de l'anarchisme. Certains comme Noam Chomsky vont même jusqu'à dire que l'anarcho-capitalisme tend plus vers l'anomie que vers l'anarchie. Enfin, l'implication d'anarcho-capitalistes au sein du Parti libertarien, qui se présente aux élections présidentielles aux États-Unis, contredit une tradition anarchiste opposée à la participation réformiste dans le débat politique institutionnalisé, lui préférant l'abstentionnisme. Pour l'agoriste Samuel Edward Konkin III, la participation d'anarcho-capitalistes à un parti politique est en contradiction avec l'anarchisme . Ceci ne s'applique en revanche pas aux anarcho-capitalistes qui ne se reconnaissent pas dans le Parti libertarien, tel David Friedman, et aux agoristes, qui rejettent les moyens étatiques pour parvenir à l'anarchie.

Pour les anarcho-capitalistes, seule l'égalité des droits est possible et souhaitable, l'égalité des biens ne pouvant être réalisée que par des mesures qui s'opposent nécessairement à la liberté : par conséquent seule la hiérarchie de droit peut et doit être abolie, les hiérarchies de nature différente (hiérarchie sociale, culturelle, etc.) n'étant pas causées par la volonté humaine mais par la nature. Pour cette raison, ils considèrent que ce sont les anarchistes traditionnels qui sont incohérents et ne méritent pas le nom d'anarchistes, bien que ce soit leur courant de pensée qui ait inventé et défini le mot « anarchisme »

Il n'existe pas en France d'organisation ou structure anarcho-capitaliste d'ampleur comparable aux organisations anarchistes traditionnelles (comme la Fédération anarchiste, Alternative libertaire, l'Organisation communiste libertaire, la CNT ou la CNT-AIT) ; aux États-Unis, ce mouvement politique est en revanche relayé par quelques instituts tels que le Ludwig von Mises Institute. Ce mouvement se manifeste surtout sur Internet par le biais de sites personnels ou de blogs.

Enfin, alors que les anarchistes traditionnels sont opposés à l'anarcho-capitalisme, certains anarcho-capitalistes indiquent que dans une société qui fonctionnerait conformément aux principes anarcho-capitalistes, les individus qui voudraient vivre selon les principes anarchistes traditionnels pourraient le faire — Hans-Hermann Hoppe en montre des conséquences peu conciliantes. Il suffirait aux anarchistes traditionnels de refuser d'entrer dans toute organisation qui présenterait à leurs yeux un caractère hiérarchique, et pourraient également s'associer dans des organisations qui respecteraient les principes de l'anarchisme traditionnel. Voir panarchisme.






</doc>
<doc id="295" url="https://fr.wikipedia.org/wiki?curid=295" title="Amblyseius">
Amblyseius

Amblyseius est un genre d'acariens de la famille des Phytoseiidae, plus de 300 espèces sont connues.

Les espèces de ce genre sont prédatrices. Les formes mobiles ont pour proies principalement les acariens et les thysanoptères sur les arbres fruitiers, la vigne, les cultures légumières et les cultures ornementales. Plusieurs espèces sont donc utilisées comme agents de contrôle biologique de ces ravageurs.




</doc>
<doc id="297" url="https://fr.wikipedia.org/wiki?curid=297" title="Adalia">
Adalia

Le genre Adalia est constitué de coléoptères prédateurs, de la famille des coccinellidés, dont les larves et les adultes ont pour proies principalement des pucerons aussi bien sur les arbres fruitiers, les grandes cultures, les cultures légumières et les cultures ornementales que sur les plantes sauvages.

Selon :

Selon :

Selon :

Selon :



</doc>
<doc id="298" url="https://fr.wikipedia.org/wiki?curid=298" title="Aleochara">
Aleochara

Le genre Aleochara est constitué de coléoptères prédateurs de la famille des staphylinidés ayant pour proies principalement les diptères sur les arbres fruitiers, la vigne, les grandes cultures, les cultures légumières, et les cultures ornementales.

Selon :
Selon :

Selon :


</doc>
<doc id="299" url="https://fr.wikipedia.org/wiki?curid=299" title="Aphidolète">
Aphidolète

Aphidoletes, les Aphidolètes, est un genre d'insectes diptères prédateurs de la famille des cécidomyidés, dont les larves ont pour proies principalement les acariens et les pucerons sur les arbres fruitiers, la vigne, les grandes cultures, les cultures légumières et les cultures ornementales.





</doc>
<doc id="300" url="https://fr.wikipedia.org/wiki?curid=300" title="Anthocoris">
Anthocoris

Le genre Anthocoris comprend des insectes hétéroptères prédateurs de la famille des Anthocoridae dont les larves et les adultes ont pour proies principalement les psylles et les pucerons sur les arbres fruitiers, la vigne, les grandes cultures, les cultures légumières et les cultures ornementales.

Les anthocoris vivent dans la majorité des cas en prédateurs non spécifiques sur la partie aérienne des plantes. En général ubiquistes, certaines espèces tendent à préférer des plantes particulières. Rares différences d'écologie entre les nymphes et les adultes. La femelle hiberne, le mâle l'imite parfois. on peut trouver les œufs sous les épidermes des feuilles et des tiges des plantes hôtes. Selon l'espèce et le climat, on compte de 1 à 4 générations annuelles.

Ils se nourrissent d'aphides (pucerons…), de psylles, de psoques, d'œufs de divers insectes (lépidoptères, hyménoptères…) et de larves d'homoptères.



</doc>
<doc id="301" url="https://fr.wikipedia.org/wiki?curid=301" title="Atractotomus">
Atractotomus

Le genre Atractotomus comprend des insectes hétéroptères prédateurs de la famille des Miridae, dont les larves et les adultes ont pour proies principalement les acariens, les psylles, les pucerons et les thrips sur les arbres fruitiers, la vigne, et les cultures légumières.




</doc>
<doc id="302" url="https://fr.wikipedia.org/wiki?curid=302" title="Aruba">
Aruba

Aruba est une île néerlandaise de la mer des Caraïbes, située au large des côtes du Venezuela, faisant partie des Petites Antilles. L'île forme un État du Royaume des Pays-Bas à part entière depuis qu'elle s'est séparée des Antilles néerlandaises en 1986. En 2010, Aruba comptait , dont vivaient à Oranjestad, capitale de l'île.

L'île d'Aruba est située en mer des Caraïbes, au nord de l'État venezuelien de Falcón. Elle fait partie de l'archipel des Antilles. L'île se localise à au nord de la péninsule de Paraguaná, sur la côte septentrionale du Venezuela. 

Aruba possède peu de végétation tropicale et des plages de sable blanc qui font sa renommée auprès des touristes. Comme la métropole, Aruba est un pays plat dont le point culminant est le mont Jamanota à .

L'île est orientée nord-ouest sud-est sur une distance de . La superficie d'Aruba est de . Son littoral a une longueur de .

Les villes principales sont Oranjestad (capitale), Sint Nicolaas, Santa Cruz et Noord.

Aruba a un climat tropical chaud et très sec, voire semi-désertique, mais cependant rafraîchi par des vents venant de l'océan Atlantique. Les températures sont quasi constantes, autour de . Très sèche, elle ne comporte qu'une petite part de la flore tropicale que l'on peut trouver ailleurs dans les Caraïbes.

L'île reçoit entre 300 et d'eau par an, ce qui est très peu par rapport au reste des Antilles et la végétation n'y est pas très développée. On compte seulement 29 jours de pluie par an. Seul le mois d'octobre est arrosé, avec 9 jours de pluie, alors qu'on ne compte aucun jour de pluie en avril et mai. La température moyenne est de .

L'île d'Aruba est d'abord peuplée d'Amérindiens caiquetios, une tribu arawak venue de l'actuel Venezuela vers l'an 1000.
En 1499, l'explorateur espagnol Alonso de Ojeda accoste sur l'île. Celle-ci devient un refuge de pirates et de boucaniers espagnols, puis un immense ranch où les Espagnols introduisent chevaux, moutons, chèvres, cochons. Contrairement à ce qui a pu se passer ailleurs, les Espagnols n’exterminent pas les Arawaks mais leur permettent d'élever du bétail. Encore aujourd'hui, beaucoup d'Arubais ont des ancêtres amérindiens. Aruba reste sous contrôle espagnol jusqu'en 1636, date à laquelle le Royaume des Pays-Bas en fait une colonie.

Après la cession de l'île par les Espagnols, des Juifs marranes fuyant les persécutions dans leurs pays (Espagne et Portugal) viennent s'installer dans l'île. Aruba change à plusieurs reprises de statut : propriété de la Compagnie néerlandaise des Indes occidentales, colonie rattachée à la Guyane hollandaise et même, à deux reprises, les Néerlandais doivent cohabiter avec les Britanniques (1799-1802 et 1805-1816) sans qu'il apparaisse clairement qui détenait le pouvoir effectif à Aruba. Le gouverneur néerlandais entre 1642 et 1646 s'appelait Pieter Stuyvesant : il deviendra ultérieurement le gouverneur de la province néerlandaise de Nouvelle-Amsterdam jusqu'à son annexion par les Anglais en 1664 sous le nom de New York.

Pendant les guerres napoléoniennes, l'Empire britannique a pris le contrôle de l'île, entre 1799 et 1802, et entre 1804 et 1816, avant de la rendre aux Hollandais.

Lors de la Seconde Guerre mondiale, les Pays-Bas sont occupés par l'Allemagne nazie à partir du . Le lendemain, les Britanniques placent l'île sous leur protection avant de la laisser aux États-Unis du jusqu'à la libération des Pays-Bas en 1945.

Le , la Couronne néerlandaise accepte le principe de l'autodétermination pour Aruba. Les Antilles néerlandaises prennent leur autonomie le et Aruba fait partie de cet ensemble constitué des « Iles sous le vent » (Aruba, Bonaire et Curaçao situées près de la côte du Venezuela) et des « Iles du vent » (Saint-Martin, Saba et Saint-Eustache situées à l'est de Porto Rico). Une constitution est établie en avril 1955. Désormais, le Royaume des Pays-Bas est constitué de deux entités de droit égal que sont les Pays-Bas et les Antilles néerlandaises.

Pour commémorer l'accord de 1948, la date du 18 mars est choisie comme fête de l'île (on ne peut dire fête nationale, puisqu'il ne s'agit pas d'une nation). Depuis le , c'est le « jour du drapeau », date à laquelle sont adoptés en même temps le drapeau et l'hymne « Aruba Dushi Tera » (qui signifie "Aruba, terre précieuse").

Lors d'un référendum organisé en 1977, la population vote largement pour l'indépendance de l'île. Des négociations commencent. Le , des représentants de la Couronne néerlandaise, de chacune des îles des Antilles néerlandaises et des Pays-Bas acceptent au traîté de La Haye le principe de l'autonomie de l'île d'Aruba, autonomie vis-à-vis des Antilles néerlandaises et non du Royaume. L'autonomie est effective le . Le Royaume est alors constitué de trois entités. L'accord de 1983 prévoyait l'indépendance dix ans plus tard, en 1996, mais le gouvernement arubais a préféré demander en 1994 la suspension de cette clause.


Le gouverneur représente le chef de l'État néerlandais à Aruba. Depuis le , cette fonction est occupée par Alfonso Boekhoudt.

Le ministre-président est le chef du gouvernement d'Aruba. Depuis novembre 2017, cette fonction est occupée par Evelyn Wever-Croes, membre du Mouvement électoral du peuple.




Les États d'Aruba ("Staten van Aruba") est le Parlement de l'île. Il comprend 21 députés élus pour 4 ans. Depuis les élections du 22 septembre 2017, l'AVP et le MEP détiennent chacun 9 sièges, POR 2 et le RED 1.

La population d'Aruba est de habitants en 2006. Elle est composée 20,7 % de personnes de 0 à 14 ans, à 68,3 % de personnes de 15 à 64 ans et de 11 % de personnes de 65 ans ou plus. Sa densité est de 532 /km². En 2003, l'espérance de vie des hommes est 75,48 ans et celle des femmes est 82,34 ans. 

En 2003, le taux de croissance de la population est 0,55 %, son taux de natalité est de en 2001, le taux de mortalité est de la même année et le taux de mortalité infantile est en 2003.

Selon le Pew Research Center, en 2010, 91,9 % des habitants d'Aruba sont chrétiens, principalement catholiques (82,8 %) et dans une bien moindre mesure protestants (7,3 %).

Avant l'arrivée des Espagnols, Aruba cultivait essentiellement l'aloès mais on ne sait pas grand-chose de l'économie de cette époque.

En 1825, les Néerlandais découvrent de l'or. C'est le premier âge de prospérité de l'île avec l'ouverture de mines et l'afflux de chercheurs d'or.

En 1924, Aruba profite de sa position au sortir du golfe pétrolier du Venezuela et du lac Maracaibo pour ouvrir une raffinerie de pétrole, c'est le deuxième âge d'abondance pour Aruba. En 1985, la raffinerie Lago qui appartient à une filiale d'Exxon ferme : le gouvernement perd 30 % de ses recettes et entre en récession l'année suivante. En 1990, la raffinerie est rénovée puis achetée et rouverte par un autre consortium pétrolier américain, El Paso, mais en 2003 ce dernier indique son intention de vendre la raffinerie qui transforme barils par jour.

Troisième âge d'or : le tourisme. Avec son régime politique stable, son climat quasi idéal et ses plages, Aruba offre aux touristes américains, vénézuéliens et néerlandais une destination qui correspond à la demande d'une île « paradisiaque ». En 2001, le tourisme représente 35 % des emplois et 38 % du PIB de l'île.

Mais le gouvernement cherche d'autres ressources pour une île qui n'exporte que son pétrole raffiné. Il jette son dévolu sur les très rentables « services financiers off-shore » que l'on peut traduire en langage courant par "paradis fiscal".

Beaucoup de voisins antillais d'Aruba (Grenade, les Îles Caïmans, Antigua-et-Barbuda, etc.) ont déjà trouvé leur compte dans cet exercice. Mais la métropole et l'Union européenne exigent une plus grande transparence sur les transactions bancaires (en particulier via le GAFI). À partir du , les voyageurs avec plus de florins arubais (le taux du florin arubais est fixé par rapport au dollar américain à 1,79 florin par dollar) en espèces doivent déclarer cette somme aux douanes arubaises. Aruba possède sa propre banque centrale, ce qui lui offre une certaine latitude dans ses politiques économiques, mais a dû promettre à l'OCDE d'aligner son système bancaire d'ici 2006. Aruba essaie toujours de développer ce secteur, sachant que le domaine est immense et flou.

Après le , la fréquentation des Américains chute et les finances du pays s'en ressentent. Le pays entre en récession, son PIB se contractant de 1,2 % en 2001 et de 3,8 % en 2002 alors que lors du boom du tourisme (au début des années 1990), la croissance annuelle était de l'ordre de 5 %.

Le budget de l'État doit faire face à un gros déficit et à une balance commerciale négative : d'un côté les touristes viennent moins, ce qui crée un manque à gagner important, de l'autre les Arubais ont, au cours de cette période de forte croissance (jusqu'en 2001), obtenu de fortes revalorisations salariales que les employeurs ne peuvent que difficilement assurer.

Le chômage reste encore inexistant (0,6 %). Le FMI prévoit pour 2003 une reprise de l'économie arubaise et un taux de croissance de 4 %, mais toujours un problème de dette publique qui fin 2002 était estimée à 37 % du PIB.

Aruba essaie de développer d'autres secteurs de service : relancer les raffineries, mais alors le tourisme risque de pâtir de la pollution de cette industrie. Une des perspectives mises en avant par le gouvernement sont les télécommunications et le développement de zones franches.


Aruba a pour codes :





</doc>
<doc id="304" url="https://fr.wikipedia.org/wiki?curid=304" title="Accusatif">
Accusatif

En linguistique, l'accusatif (abréviation : ; du latin grammatical "accusativus", « qui marque l'aboutissement de l'action »), est un cas grammatical exprimant le complément d'objet direct (COD), c'est-à-dire l'actant qui subit l'action exercée par le sujet d'un verbe transitif direct actif, dit aussi "objet patient". Dans les langues ergatives, cette fonction peut être assumée par le cas absolutif.

Outre ce sens principal, l'accusatif peut également assurer différentes fonctions selon les langues.

En allemand, l'accusatif s'emploie principalement pour indiquer le complément d'objet direct et son attribut mais aussi après certaines prépositions ("bis, durch, für, gegen, ohne, per, pro, um, wider") ou avec la postposition "entlang" : den Fluss entlang = le long du fleuve.

Il sert aussi dans l'opposition entre les compléments de lieu locatifs et directifs après les prépositions « mixtes » pour exprimer une position ("an, auf, hinter, in, neben, über, unter, vor, zwischen"). L'accusatif s'emploie pour marquer le directif (le lieu où l'on va ; question "wohin ?"). Quant au datif, il exprime un locatif (lieu où on est ; question "wo ?").

Exemple :

Il est aussi employé pour désigner une durée :

La forme de l'accusatif est pareille à celle du nominatif sauf au masculin singulier.

L'anglais moderne n'a plus de déclinaisons sauf pour les pronoms : "whom" est l'accusatif de "who" (qui), "him" est l'accusatif de "he" (il, lui), et "her" de "she" (elle). Ces formes servant également de datif, on les range parfois sous la dénomination de cas oblique:

Toutefois, en anglais parlé, on a tendance à utiliser "who" ou "that" au lieu de "whom", ou à supprimer le pronom :

L'espéranto a fait le choix d'admettre le nominatif et l'accusatif comme seuls cas ; l'accusatif y est marqué par la désinence -n. Ceci peut paraître un peu compliqué pour les personnes dont la langue maternelle ignore les cas, mais cet accusatif est nécessaire à une bonne compréhension:


En espéranto, les adverbes dérivés peuvent se mettre à l'accusatif :

En espéranto, l'accusatif remplace souvent (certains disent : « toujours ») une préposition :

Le français conserve dans ses pronoms personnels des traces d'un accusatif (cas régime en ancien français) :

La distribution de ces formes toutefois ne reprend pas celle de l'accusatif puisqu'elles servent également d'attributs du sujet ("il l'est").

En latin, l'accusatif s'utilise aussi pour marquer l'attribut du complément d'objet direct et après certaines prépositions. Il sert à exprimer le lieu où on va et la durée et peut s'employer de façon exclamative :

Certains verbes admettent la construction du "double accusatif" :

En russe, comme dans la plupart des autres langues slaves, l'accusatif est le cas du complément d'objet direct ;


L'accusatif des langues slaves hésite souvent entre des formes semblables au nominatif ou au génitif. L'accusatif est généralement identique au nominatif pour les noms masculins inanimés ("билет" (billet)) et les noms neutres ; il se calque sur le génitif pour les noms masculins animés ("человек" (homme) devient à l'accusatif "человека"). Le féminin singulier a une forme propre : "роза" (rose) donnera "розу".

Cette règle est souvent compliquée par le remplacement de l'accusatif par le génitif dans un sens négatif et/ou partitif :

Il s'emploie également après certaines prépositions, telles que "в" et "на" (dans, à, sur), avec une idée de mouvement ou de direction :



</doc>
<doc id="307" url="https://fr.wikipedia.org/wiki?curid=307" title="Arthur Honegger">
Arthur Honegger

Arthur Honegger, né au Havre le et mort à Paris le , est un compositeur suisse.

Issu de parents de nationalité suisse et de religion protestante, Arthur Honegger nait au Havre où son père exerce la profession de négociant en café. Sa famille baigne dans l'univers musical. Sa mère joue du piano. Le jeune Arthur apprend donc le violon. Au duo mère-fils, se joint parfois un ami d'Arthur, également violoniste. Mais les œuvres pour deux violons et piano sont assez rares, et le jeune Arthur, qui admire Bach et Beethoven, est donc amené à composer pour cette formation des essais malhabiles. Il se lance également dans l'écriture d'un opéra et d'un oratorio.

En 1911, deux ans après s'être inscrit au Conservatoire de Zurich, Honegger le quitte pour le Conservatoire de Paris, dans lequel il étudie le violon et rencontre Darius Milhaud et Jacques Ibert. Il est élève de Charles-Marie Widor et Vincent d'Indy. En 1918, il quitte le Conservatoire en ayant déjà composé des mélodies, son premier quatuor et un poème symphonique, "Le Chant de Nigamon".
Très attaché au renouveau du répertoire, il est influencé par Igor Stravinsky, sur lequel il écrit un essai en 1939. Compositeur prolifique et désireux d'illustrer la transformation de la société, notamment par la technique ou le sport, Honegger écrit pour le théâtre, la radio et le cinéma aussi bien que pour la salle de concert : ballets, chansons, concertos, musique de chambre, musiques de films, opéras, oratorios, symphonies.

En 1921, il connaît le succès avec "le Roi David", pièce de René Morax, qu'il transforme en oratorio en 1924. Son œuvre la plus célèbre, créée en 1923, est "Pacific 231", premier de trois mouvements symphoniques et dédiée à la locomotive à vapeur éponyme. Les deux autres mouvements du triptyque s'intitulent "Rugby" et "Mouvement symphonique ".

Sa première symphonie date des années 1929-1930. Plus tard, durant l'Occupation, il compose ses "Trois Poèmes" (sur un texte de Claudel), ses "Trois Psaumes" et sa "Symphonie" pour orchestre à cordes et trompette "ad libitum". Composée en 1941, ses mouvements évoquent la mort, le deuil, puis la libération. En parallèle il enseigne la composition à l'École normale de musique de Paris où il aura parmi ses élèves Yves Ramette, futur auteur de six symphonies. Sa "Symphonie", intitulée "liturgique", son oratorio "Jeanne d'Arc au bûcher" (1938) — d'après un texte de Paul Claudel — et son dramatique "Roi David" (1921) soulignent la religiosité de ce compositeur protestant. Parmi ses œuvres qui ont le plus compté pour lui, il citait aussi "Antigone" (1926).

Sa symphonie n° 3 (dite « "Liturgique" », 1946) est très liée aux années difficiles que le monde venait de vivre du fait de la mondiale. Chacun des trois mouvements comporte un sous-titre d'origine liturgique. Elle est composée comme suit :

En 1925, Arthur Honegger a une liaison avec la chanteuse d'opéra Claire Croiza, de laquelle naît un fils, Jean-Claude. En mai 1926, il épouse la pianiste Andrée Vaurabourg (1894-1980) qu'il avait rencontrée au conservatoire de Paris en 1916 ; leur fille Pascale naît en 1932. Ils demeurent à Paris (tout en logeant dans des appartements séparés) durant la guerre, vivant notamment de commandes pour musique de film.

Sa quatrième symphonie est sous-titrée : "Deliciæ Basiliensis" ("Les Délices de Bâle"). La cinquième est dite "Symphonie di tre re" (« des trois ré », qui ponctuent chacun de ses trois mouvements).

En 1953, il est nommé membre étranger de l'Académie des beaux-arts et, l'année suivante, il est fait grand officier de la Légion d'honneur. Il est par ailleurs critique musical et professeur à l'École normale de musique de Paris. Il est également l'un des membres du groupe des Six, avec Georges Auric, Louis Durey, Darius Milhaud, Francis Poulenc et Germaine Tailleferre. Outre les Six, il a fréquenté Paul Claudel, Jean Cocteau, Max Jacob, Pierre Louÿs, Pablo Picasso, Erik Satie, Louis Jouvet et Paul Valéry, dont certains lui ont fourni des sujets pour ses œuvres.

Il est enterré avec son épouse au cimetière Saint-Vincent à Paris.

Arthur Honegger est un compositeur qui, au premier abord, paraît difficile à cerner à cause de la diversité de son œuvre, allant de la tonalité à l'atonalité (pour "Antigone") en passant par la polytonalité, utilisant tous les registres, du quatuor à cordes à l'opéra, et respectant autant les acquis du passé que les apports de ses contemporains. Toute sa vie, il a été marqué par la double influence germanique (Ludwig van Beethoven, Johann Sebastian Bach, Max Reger) et française (Claude Debussy, Florent Schmitt), ce qui contribue à situer son œuvre en marge des courants musicaux. Si l'on peut lui attribuer un style personnel, il n'est en revanche d'aucune école ; lui-même ayant rejeté, comme son confrère et ami Georges Enesco, les systèmes de classification trop stricts en musique.
La diversité de la musique d'Honegger reflète sa volonté de faire de la musique un moyen d'expression à vocation humaniste. Ainsi, il a souvent aspiré à une musique défaite de trop de formalisme, de trop de séduction et d'habitudes ("Cri du monde", 1931). La crainte d'une surmédiatisation de la musique se reconnaît dans sa recherche d'une musique authentique, capable de porter un message, parfois philosophique voire religieux ("Symphonie liturgique", 1945). Désireux de se renouveler à chaque œuvre, il a exploré différents genres et techniques en s'intéressant tout autant à l'harmonie de Claude Debussy, à la rythmique d'Igor Stravinsky, à la forme beethovenienne, au génie d'Arnold Schönberg (en excluant le sérialisme) et même à la musique électronique.

L'apparente simplicité de certains passages de sa musique doit être examinée dans le sens de l'objectivité. Il ne répugna pas à la complexité lorsque cela lui semblait nécessaire, comme dans "Horace Victorieux" (1926) ou dans ses symphonies. Comme d'autres artistes de son temps, tels Albert Camus, il cherche à émouvoir, notamment au travers d'œuvres religieuses, ce qui explique le succès de "Jeanne d'Arc au bûcher" (1935) entre autres.

Connu pour son humanisme, il a parfois émis des jugements sévères mais jamais durant son travail de critique. Au contraire, il a aidé les compositeurs des générations suivantes tels qu'Olivier Messiaen, dont il a confirmé après sa première écoute qu'il serait « l'un des plus grands compositeurs de son temps ».

Un catalogue des œuvres du compositeur a été établi par le musicologue Harry Halbreich. Cette nomenclature est figurée par la lettre H.







Il participe à l'écriture en 3 actes de l'opérette "Les aventures du roi Pausole", livret d'Albert Willemetz d'après le roman de Pierre Louÿs.
Albert Willemetz écrit des dialogues et des couplets extrêmement drôles. L'utilisation de l'alexandrin accentue le comique de ce vaudeville. Arthur Honegger joue à mélanger des styles musicaux sans pour autant céder à la mélodie facile.

Arthur Honegger fut aussi l'auteur d'oratorios. En 1907, il compose un "Oratorio du Calvaire". En 1924, il crée à Paris une version retravaillée en oratorio du "Roi David". Puis en 1927, il révise en oratorio le "Judith" de René Morax. "Cris du monde", oratorio sur un texte de R. Bizet d'après « Hymn to Solitude » de John Keats pour voix solistes, chœur d'enfants, chœur mixte, orchestre, est créé en 1930-1931. Deux nouveaux oratorios composés dans les années 1930 obtiennent un vif succès : la "Danse des Morts", basé sur des textes bibliques, et "Jeanne d'Arc au bûcher", oratorio dramatique sur un texte de Paul Claudel. À la suite de ces succès, il compose encore un oratorio dans les années 1940 : "Nicolas de Flue" sur un texte de Denis de Rougemont. Il est également l'auteur d'une cantate de Noël, pour baryton solo, voix d'enfants, chœur mixte, orgue et orchestre, en 1953.





Son portrait apparaît sur les billets de 20 francs suisses. Un autre de ses portraits a été réalisé en 1944 à Paris par Serge Ivanoff.

Un conservatoire lui est dédié au Havre.

Une Fondation Arthur Honegger a été créée en 1970 sous l'égide de la Fondation de France. Cette fondation soutient la création musicale en attribuant un prix international de musique. Ce prix a pour objet d'honorer soit un compositeur pour une œuvre particulière, soit un compositeur pour l'ensemble de son œuvre, soit une formation musicale de quatuor à cordes. La fondation Arthur Honegger a été créée à l'initiative de sa veuve afin de perpétuer sa mémoire et associer son nom à ceux d'autres créateurs.

Le réalisateur Georges Rouquier lui a consacré un court métrage ("Arthur Honegger", 1955).



</doc>
<doc id="309" url="https://fr.wikipedia.org/wiki?curid=309" title="Alain-Fournier">
Alain-Fournier

Alain-Fournier, pseudonyme dHenri-Alban Fournier, né le à La Chapelle-d'Angillon dans le Cher et tué au combat le à Saint-Remy-la-Calonne, est un écrivain français, dont l’œuvre la plus célèbre est "Le Grand Meaulnes" (1913).

Henri-Alban Fournier est né à La Chapelle-d'Angillon, chef-lieu de canton du département du Cher, à au nord de Bourges. Son père, Augustin Fournier (1861-1933), habituellement appelé Auguste, jeune instituteur, vient d'être nommé à Marçais, où le petit Henri vit ses cinq premières années. Sa mère, Marie-Albanie Barthe (1864-1928), est également institutrice. L'essentiel de son enfance se passe à Épineuil-le-Fleuriel, tout au sud du département. Il y sera, sept ans durant, l'élève de son père et aura pour compagne de jeux et de lectures sa sœur Isabelle (1889-1971). Dans une lettre à ses parents du , évoquant , il ajoute : . Les trois quarts des chapitres de son futur roman auront pour cadre et ses environs qui ressemblent à s’y méprendre au petit village de son enfance heureuse.

À douze ans, Henri part pour Paris, où il commence ses études secondaires au lycée Voltaire, récoltant presque tous les prix. Rêvant d’« être marin pour faire des voyages », il convainc ses parents, en septembre 1901, d’aller à Brest préparer le concours d’entrée à l’École navale : l’expérience sera trop rude et il y renonce quinze mois plus tard. C’est au lycée de Bourges qu’il prépare le baccalauréat ; il l’obtient, sans mention, en juillet 1903. Comme beaucoup de jeunes provinciaux, comme Péguy et Giraudoux avant lui, il va poursuivre des études supérieures de lettres au lycée Lakanal, à Sceaux – « l’internat des champs » –, puis au lycée Louis-le-Grand de Paris, où il prépare le concours d'entrée à l'École normale supérieure. C'est au lycée Lakanal qu'il rencontre Jacques Rivière avec lequel il se lie d'une amitié profonde. Celui-ci étant reparti à Bordeaux en 1905, il entretient avec lui une correspondance presque quotidienne qui sera publiée en 1928. Jacques Rivière épousera sa jeune sœur, Isabelle, en 1909.

Le , jour de l'Ascension – il a dix-huit ans –, il croise, à la sortie d'une exposition de peinture au Grand Palais, une grande et belle jeune fille, qui lui dira son nom dix jours plus tard : Yvonne de Quiévrecourt. Mais cet amour est impossible : Yvonne épousera, l'année suivante, un médecin de marine, Amédée Brochet, avec postérité. Bouleversé par cette brève rencontre, Fournier ne cessera, huit années durant, de penser à la jeune femme et de l’évoquer dans sa correspondance. Il s'en inspirera pour le personnage d’Yvonne de Galais dans "Le Grand Meaulnes".

Après son échec à l'oral de Normale en juillet 1907, il effectue son service militaire d'octobre 1907 à septembre 1909, d'abord à Vincennes et dans diverses casernes de Paris, de Vanves et de Laval, puis comme sous-lieutenant de réserve au d'infanterie à Mirande. Libéré à l'automne de 1909, il ne reprend pas ses études, mais est engagé comme chroniqueur littéraire à "Paris-Journal" en 1910. Il commence à publier quelques poèmes, essais, ou contes, qui connaissent quelque succès. Il rencontre alors plusieurs grands peintres et écrivains de son temps : Maurice Denis, André Gide, Paul Claudel, André Suarès et Jacques Copeau, et se lie d'une grande amitié avec Charles Péguy et Marguerite Audoux. Mais surtout il élabore lentement l'œuvre qui le rendra célèbre : "Le Grand Meaulnes", paru en novembre 1913 chez Émile-Paul. Ce roman manquera de peu le prix Goncourt, mais sera salué presque unanimement par la critique de l'époque.

Le 5 mai 1912, présenté par Charles Péguy, il devient secrétaire de Claude Casimir-Perier, fils de l'ancien président de la République et l'aide à mettre au point un gros ouvrage "Brest, port transatlantique" qui sera publié en avril 1914 chez Hachette. Il fréquente dès lors l'épouse de celui-ci, Pauline Benda, célèbre au théâtre sous le nom de Madame Simone et lui rend de multiples services. Simone révélera en 1957 la liaison passionnée, souvent orageuse, qu'elle a eue, à partir de juin 1913, avec le jeune écrivain, de neuf ans son cadet, dans son livre "Sous de nouveaux soleils" (Gallimard). Alain-Fournier est fréquemment reçu dans leur propriété de Trie-la-Ville, où sont également accueillis Charles Péguy ou Jean Cocteau. C'est sous les arbres du parc du château de Trie que Fournier écrira, en 1914, plusieurs chapitres de son second roman qu’il appelle alors « Colombe Blanchet », mais qu'il ne pourra achever avant la déclaration de guerre. La correspondance des deux amants a été publiée en 1992, présentée et annotée par Claude Sicard.

Durant cette même année 1913, qui voit, en juin, le début de sa liaison avec Pauline Benda-Perier – Madame Simone –, Fournier rencontre pour la seconde fois Yvonne de Quiévrecourt. Les chastes retrouvailles ont lieu au cours de l’été, sans doute du au 4 août 1913, à Rochefort-sur-Mer, où la jeune femme, mère de deux enfants, est de passage chez ses parents. Le jeune homme est bouleversé — des notes sur un petit carnet noir en témoignent — mais sa vie sentimentale a pris désormais irrévocablement une direction nouvelle. Il échangera encore quelques lettres avec Yvonne de Quiévrecourt, mais ne la reverra pas.

Lieutenant de réserve, mobilisé le 2 août 1914, Alain-Fournier part de Cambo dans le Pays basque, où il était en vacances avec Simone, pour rejoindre son régiment, le d'infanterie à Mirande ; il est affecté à la . Partis d'Auch en train jusqu'au camp de Suippes, ses hommes et lui rejoignent le front après une semaine de marche jusqu'aux environs d'Étain. Avec sa compagnie, il participe ensuite à plusieurs combats meurtriers autour de Verdun.

Le 22 septembre, un détachement de deux compagnies, la , commandée par le lieutenant Paul Marien et la , commandée par le lieutenant Fournier reçoit l'ordre d'effectuer une reconnaissance offensive sur les Hauts de Meuse, en direction de Dommartin-la-Montagne, à vingt-cinq kilomètres au sud-est de Verdun. Si l'on doit en croire les témoignages postérieurs, assez divergents, du sergent Zacharie Baqué et du soldat Laurent Angla, Fournier et ses hommes parviennent jusqu'à la Tranchée de Calonne où ils sont rejoints par le capitaine de Savinien Boubée de Gramont qui prend la direction des opérations et décide d'attaquer l'ennemi. Entendant des coups de feu, ils veulent rejoindre la de Marien qui s'est trouvée face à un poste de secours allemand et a ouvert le feu. Après avoir fait quelques prisonniers, ils sont pris à revers par une compagnie prussienne à la lisière du bois de Saint-Remy et décimés par la mitraille. Trois officiers (dont Alain-Fournier) et dix-huit de leurs hommes sont tués ou grièvement blessés, tandis que Marien et le reste du détachement parviennent à se replier. Sur le "Journal de marche et d'opérations" du R.I., trois officiers, un sergent et dix-huit soldats des et sont portés au .

S'il faut croire certaines sources, la patrouille dont Alain-Fournier faisait partie avait reçu l'ordre de , et avait obéi, ce que les Allemands auraient considéré comme un crime de guerre. Selon Gerd Krumeich, professeur à l’université de Düsseldorf, il est exact que la patrouille d'Alain-Fournier attaqua une ambulance allemande, mais il est difficile d'établir les faits précis.

Sa fiche militaire de décès publiée sur le site Mémoire des Hommes mentionne qu'il a été tué par l'ennemi le 26 septembre, à Vaux (Meuse), commune proche de la Tranchée de Calonne. Le bois de Saint Rémy se trouve entre la limite de cette commune et la Tranchée de Calonne (qui n'est pas une tranchée mais une route). Un monument dédié à Alain-Fournier s'élève à l'intersection entre cette route et le chemin menant de Vaux à Saint-Rémy-la-Calonne.

Fournier est mort sans avoir eu d'enfant.

La disparition du lieutenant Fournier, rapportée par la presse, impressionne fortement ses contemporains, bien qu'il n'ait été officiellement déclaré « mort pour la France » qu’en juin 1920. Il est ensuite décoré de la croix de guerre avec palme et nommé chevalier de la Légion d’honneur à titre posthume.

Le lieu exact de sa sépulture demeure inconnu pendant plus de trois quarts de siècle. Dès 1977, Michel Algrain enquête sur la localisation probable des derniers moments d'Alain-Fournier et parvient à coordonner des recherches. Son corps et ceux de ses vingt compagnons d'arme, originaires, pour la plupart, de la région de Mirande, sont retrouvés par Jean Louis, le , dans les bois près de Saint-Remy-la-Calonne. Ils avaient été enterrés dans une fosse commune creusée par l'armée allemande sur le lieu du combat. Après des fouilles archéologiques méthodiques et un examen approfondi des squelettes en laboratoire, ils sont ré-inhumés solennellement dans la nécropole nationale de Saint-Remy-la-Calonne.

La légende d'un écrivain mort pour la France en pleine jeunesse, après avoir écrit un seul roman, a sans doute contribué à assurer la fortune littéraire d'Alain-Fournier. Son nom figure sur les murs du Panthéon, à Paris, dans la liste des écrivains morts au champ d'honneur pendant la Première Guerre mondiale.

Alain-Fournier est généralement considéré comme l’auteur d’un seul livre : son roman "Le Grand Meaulnes" publié en 1913, alors qu’il avait vingt-sept ans, n’est pourtant pas son seul écrit. C’est d’abord par des poèmes en vers libres qu’Henri Fournier manifeste à partir de l’été 1904 – il a dix-sept ans – son désir de devenir écrivain. Quelques-uns de ces premiers poèmes et nouvelles ont été publiés de son vivant dans diverses revues, connaissant un certain succès ; avec la plupart des autres, ils furent rassemblés en 1924, par son beau-frère Jacques Rivière chez Gallimard, sous le titre "Miracles". Dès le 13 août 1905, au cours de son séjour à Londres, Henri Fournier déclarait, dans une lettre à son ami Jacques, former un autre projet, celui d’être romancier, à la manière de Dickens. Et sans doute peut-on dater de cette époque les toutes premières ébauches du "Grand Meaulnes".

Recueillis et classés méthodiquement par sa sœur Isabelle Rivière, les brouillons du roman ont été, avec tous les autres manuscrits de l’auteur, donnés en 2000 par Alain Rivière à la Ville de Bourges et ils sont aujourd’hui conservés à la Bibliothèque municipale de cette ville, qui envisage leur mise en ligne. Ils avaient été publiés intégralement en 1986 dans la collection des « Classiques Garnier », formant la dernière partie du volume, sous le titre « Dossier du Grand Meaulnes ». Cet ouvrage est épuisé depuis plusieurs années, mais les brouillons du roman ont été reproduits en 2010 dans le "Bulletin des amis de Jacques Rivière et d’Alain-Fournier". Avant que le roman n’atteigne à la forme définitive au début de 1913, Alain-Fournier est passé par maints tâtonnements au cours des huit années précédentes. Ses manuscrits en témoignent, composés de notes rapides, de plans, de fragments de journal ou de lettres, d’ébauches, de reprises.
Ni le manuscrit définitif du roman ni le dactylogramme ne sont parvenus jusqu’à nous ; il parut d’abord dans "La Nouvelle Revue Française" sur les cinq numéros publiés de juillet à novembre 1913, avant d’être publié par Émile-Paul à la fin d’octobre 1913, quelques jours avant la parution du premier volume de "À la Recherche du temps perdu" de Marcel Proust "Du côté de chez Swann" que Fournier semble ne jamais avoir lu.

C’est « l’aventure » d'Augustin Meaulnes, racontée par un narrateur d’abord anonyme, ensuite nommé François Seurel ; François et Augustin sont tous deux élèves du Cours supérieur à Sainte-Agathe, un village proche des bords du Cher, de Vierzon et de la Sologne. Lors d'une fugue, Augustin Meaulnes arrive par hasard dans un domaine mystérieux et ruiné où se déroule une , pleine d'enfants qui y font la loi. Le château est bruissant de jeux, de danses et de mascarades. Meaulnes apprend que cette fête est donnée à l'occasion des noces de Frantz de Galais. Au cours d’une promenade en barque sur l’étang, Meaulnes rencontre une jeune fille dont il tombe aussitôt amoureux. Il ne fait cependant que la croiser et apprendre son nom : Yvonne de Galais ; la fiancée attendue s'est enfuie, le mariage n'a pas lieu et la fête prend tristement fin. Revenu à sa vie d'écolier, Meaulnes n'a plus qu'une idée en tête : retrouver et la jeune fille qu'il aime. Ses recherches restent infructueuses, malgré l’aide d’un jeune bohémien qui s’avère être Frantz. Il s'en va étudier à Paris où il se lie à Valentine, qui n’est autre – il le découvrira trop tard – que l’ancienne fiancée de Frantz. De son côté, François Seurel, devenu instituteur, retrouve la jeune châtelaine et en apporte à Augustin. Après une et de longues fiançailles, le mariage a lieu, mais le jeune homme s’enfuit dès le lendemain pour accomplir la promesse faite à Frantz et réparer sa faute. Quand il revient au Domaine, sa jeune femme est morte en couches, et le narrateur imagine qu’il va repartir, avec sa petite fille .

Avant même l’achèvement du "Grand Meaulnes", Fournier avait entrepris l’écriture d’un second roman, qu’il voulait appeler « Colombe Blanchet », inspiré par les compagnonnages et l’atmosphère de sa période de garnison à Mirande : il espérait le terminer à la fin de 1914, mais la guerre l’en empêcha. Il nous en reste aujourd’hui sept chapitres inachevés et quelques esquisses et notes, qui ont été publiés en 1990.
Au mois de janvier 1914, Simone l’avait pressé d’écrire une pièce de théâtre, et il avait, en une nuit, jeté sur le papier, une ébauche de scénario en trois actes qu’il avait intitulée « La Maison dans la forêt », où passe le souvenir du conte "Boucles d'or et les Trois Ours" ; mais il abandonna bientôt ce projet pour reprendre celui de « Colombe Blanchet ».

Depuis son arrivée à Paris en 1898 jusqu’à sa mort, Alain-Fournier a entretenu une abondante correspondance, d’abord avec ses parents et sa sœur, puis avec ses condisciples du lycée Lakanal, Jacques Rivière surtout, qui deviendra son beau-frère – près de 370 lettres échangées en dix ans – et René Bichet – « le Petit B. » – le peintre André Lhote, Charles Péguy, son aîné de treize ans, et enfin Madame Simone, les trois dernières années. Elles ont été presque entièrement publiées par sa sœur et son neveu et couvrent huit volumes. La correspondance avec Jacques Rivière, en particulier, a nourri des générations de lecteurs et d’écrivains, de Simone de Beauvoir à Guy Debord, car elle donne un aperçu saisissant de la vie littéraire de la Belle Époque. Alain-Fournier fut également, trois ans durant, un chroniqueur littéraire très apprécié, dans "Paris-Journal" et dans d’autres revues de l’époque. Un choix de ses plus intéressants articles a été publié en 1990 par André Guyon sous le titre "Chroniques et critiques".


Voir plus haut la rubrique « Chronologie des publications ».






</doc>
<doc id="311" url="https://fr.wikipedia.org/wiki?curid=311" title="Antonin Artaud">
Antonin Artaud

Antonin Artaud, né Antoine Marie Joseph Paul Artaud, à Marseille, le et mort à Ivry-sur-Seine le , est un théoricien du théâtre, acteur, écrivain, essayiste, dessinateur et poète français.

La poésie, la mise en scène, la drogue, les pèlerinages, le dessin et la radio, chacune de ces activités a été un outil entre ses mains, .

Toute sa vie, il a lutté contre des douleurs physiques, diagnostiquées comme issues de syphilis héréditaire, avec des médicaments, des drogues. Cette omniprésence de la douleur influe sur ses relations comme sur sa création. Il subit aussi des séries d'électrochocs lors d'internements successifs, et il passe les dernières années de sa vie dans des hôpitaux psychiatriques, notamment celui de Rodez. Si ses déséquilibres mentaux ont rendu ses relations humaines difficiles, ils ont aussi contribué à alimenter sa création. Il y a d'un côté ses textes , de l'autre, selon Évelyne Grossmann, les textes fulgurants de ses débuts.

Inventeur du concept de « théâtre de la cruauté » dans "Le Théâtre et son double", Artaud a tenté de transformer radicalement la littérature et surtout le théâtre. S'il n'y est pas parvenu de son vivant, il a certainement influencé les générations de l'après Mai 68, en particulier le théâtre américain, et les situationnistes de la fin des années 1960 qui se réclamaient de son esprit révolutionnaire. Il a aussi influencé le théâtre anarchiste "Living Theatre", qui se réclame de lui dans la pièce "The Brig" où il met en pratique les théories d'Artaud.

Dans son œuvre immense, il fait délirer l'art (comme Gilles Deleuze, grand lecteur d'Artaud, fera délirer la théorie autour du corps sans organe). Son œuvre graphique est également importante. Il a fait l'objet d'un legs important au Centre national d'art et de culture Georges-Pompidou en 1994. Une partie de ses œuvres a été exposée en 2011.

Sur la question de la biographie, Florence de Mèredieu prévient que l'œuvre et la vie d'Artaud sont « un titanesque effort pour ruiner les balises et limites censées canaliser l'existence et l'être d'un individu. » Il se met en scène en continu, vivant comme à distance de lui-même. Il écrit « Antonin Artaud fut d'abord un modèle perverti, une esquisse essayée que j'ai reprise moi-même à un certain moment, pour rentrer chez moi habillé » Il va passer sa vie à perturber toutes les données de ce que l'on dénomme, dans nos sociétés un "état civil".

Antonin Artaud est né le à Marseille. Il est issu d'une famille bourgeoise aisée. Son père, Antoine-Roi Artaud, capitaine au long cours, et sa mère, Euphrasie Nalpas, sont cousins germains : ses deux grands-mères sont sœurs, toutes deux nées à Smyrne (Izmir - aujourd'hui en Turquie). L'une, Catherine Chilé, a été élevée à Marseille, où elle a épousé Marius Artaud, l'autre, Mariette Chilé, a grandi à Smyrne, où elle a épousé Louis Nalpas. Son oncle maternel, John Nalpas, rencontre la sœur de son père, Louise Artaud lors du mariage de leurs frères et sœurs, et ils se marient aussi. John et Louise s'installent à Marseille, les familles sont très proches, les enfants forment une tribu soudée. Antonin connaît à Marseille une petite enfance choyée dont il garde des souvenirs de tendresse et de chaleur.

Cette enfance est cependant perturbée par la maladie. Le premier trouble apparaît à l'âge de quatre ans et demi, lorsque l'enfant se plaint de maux de tête et qu'il "voit double". On pense à une méningite consécutive à une chute. Déjà, on préconise l'électricité pour le soigner. Son père se procure une machine qui transmet l'électricité par des électrodes fixées sur la tête. Cette machine est décrite dans le "Traité de thérapeutique des maladies nerveuses" du docteur Grasser. Bien que très différent des électrochocs, ce système relève de l'électrothérapie et l'enfant Artaud en a beaucoup souffert.

D'autres traumatismes suivront. À six ans, il aurait failli se noyer lors d'un séjour chez sa grand-mère de Smyrne Mais son premier grand choc vient de la mort d'une petite sœur âgée de sept mois, bousculée par un geste violent d'une bonne. Elle apparaît dans les écrits d'Antonin Artaud comme une de ses « filles de cœur » : 

Cependant, Antonin a aussi le sens du jeu et de la mise en scène. C'est à lui que l'on confie la mise en place de la crèche à Noël chaque année. Pour les enfants de la famille son talent de metteur en scène apparaît dans ses "tableaux vivants" : reproduction de tableaux célèbres, ou spectacles familiaux montés avec ses cousins. Souvent, les spectacles d'Antonin ont des « résonances macabres » : un enterrement au crépuscule, (Antonin tenant le rôle du cadavre). Une autre fois il invente une mise en scène pour effrayer son cousin Marcel Nalpas. C'était, selon le récit de sa sœur, une mise en scène macabre avec installation de têtes de mort et de bougies dans une chambre. Antonin fait ensuite entrer Marcel en déclamant un poème de Baudelaire. D'abord effrayé, Marcel a ensuite bien ri, avec Antonin. Dans ce "Théâtre de la cruauté", "Théâtre de la peur" Marie-Ange voit l'influence d'Edgar Poe.

Artaud a quatorze ans lorsqu'il fonde avec ses camarades du collège du Sacré-Cœur de Marseille, une petite revue où il publie ses premiers poèmes inspirés de Charles Baudelaire, d'Arthur Rimbaud ou Edgar Poe. Mais lors de sa dernière année de collège, en 1914, il est atteint de dépression, ne se présente pas au baccalauréat, et l'année suivante, sa famille le conduit à Montpellier pour consulter un spécialiste des maladies nerveuses. Il est envoyé au sanatorium de la Rouguière, en 1915 et 1916 et publie en février 1916 des poèmes dans "La Revue de Hollande". Le conseil de révision le déclare d'abord "bon pour le service" avant que l'armée le réforme provisoirement pour raisons de santé, puis définitivement en décembre 1917 grâce à l'intervention de son père.

L'année 1914 est un tournant dans la vie du jeune homme, à cause de la guerre, mais c'est aussi pour Antonin sa dernière année de collège. Il doit passer l'examen de philosophie, mais son état de santé ne le lui permet pas. Artaud est en état de dépression après avoir connu sa première expérience sexuelle, qu'il décrit comme dramatique, comme un traumatisme sur lequel il reviendra souvent dans ses écrits. Il a le sentiment qu'on lui a volé quelque chose. C'est ce qu'il exprime à Colette Allendy en 1947, peu avant sa mort.

Entre 1917 et 1919, il fait un certain nombre de séjours dans des lieux de cure et maisons de santé. Il peint, dessine, écrit. Plus tard, lors de son séjour à l'hôpital Henri-Rouselle pour une cure de désintoxication, il indique qu'il a commencé à prendre du Laudanum en 1919. 

En 1920, sur les conseils du docteur Dardel, sa famille confie Antonin Artaud au docteur Édouard Toulouse, directeur de l'asile de Villejuif, dont il devient le co-secrétaire pour la rédaction de sa revue "Demain". Le docteur l'encourage à écrire des poèmes, des articles, jusqu'à la disparition de la revue en 1922. En juin de cette même année 1920 Artaud qui s'intéresse au théâtre rencontre Lugné-Poë, et il quitte Villejuif pour s'installer dans une pension à Passy. Il s'intéresse aussi au mouvement Dada et découvre les œuvres d'André Breton, celles de Louis Aragon, Philippe Soupault.

Il rencontre Max Jacob qui l'oriente vers Charles Dullin. Dullin l'intègre dans sa compagnie en 1921. Là, il rencontre Génica Athanasiou dont il tombe amoureux et à laquelle il écrit un grand nombre de lettres réunies dans le recueil "Lettres à Génica Athanassiou" avec deux poèmes. Leur passion orageuse va durer . Jusqu'en 1922, Antonin Artaud publie poèmes, articles et comptes-rendus à plusieurs revues : "Action", "Cahiers de philosophie et d'art", "L'Ère nouvelle", revue de l'entente des gauches. L'aventure théâtrale 'Artaud commence en 1922 avec la première répétition des spectacles de l'atelier, où il joue "L'Avare" de Molière. Suivront d'autres rôles, toujours avec Dullin qui lui demande de dessiner les costumes et les décors de "Les Olives" de Lope de Rueda. Un exemplaire de ces dessins est conservé au Centre pompidou. Toute l'année 1922 est occupée par le théâtre et par les nombreux rôles que joue Artaud malgré sa santé défaillante et malgré les difficultés financières de la compagnie Il interprète notamment "Apoplexie" dans "La Mort de Souper" adaptation de la "Condamnation de Banquet" de Nicole de La Chesnaye.

En même temps, il produit aussi à la demande de Daniel-Henry Kahnweiler un recueil de tiré à et il fait la connaissance d'André Masson, de Michel Leiris, de Jean Dubuffet, de Georges Limbour. Sa correspondance témoigne de l'intérêt que lui portaient artistes et écrivains Elle occupe une très grande place dans le recueil de ses œuvres.

En 1923, il publie, à compte d'auteur et sous le pseudonyme d'Eno Dailor, le premier numéro de la revue "Bilboquet", une feuille composée d'une introduction et de deux poèmes : 

1923 est l'année où Artaud ajoute le cinéma aux modes l'expression qu'il cultive (peinture, littérature, théâtre). Le , le cinéaste René Clair lance une vaste enquête dans la revue "Théâtre et Comœdia illustré" car selon lui, peu de cinéastes savent tirer parti de « l'appareil de prise de vue. » Il se tourne alors vers des peintres, sculpteurs, écrivains, musiciens, en leur posant la double question : 1)« Quel genre de films aimez-vous ? », 2) « Quel genre de films aimeriez-vous que l'on créât ? ». Antonin Artaud répond qu'il aime le cinéma dans son ensemble car tout lui semble à créer, qu'il aime sa rapidité et le processus de redondance du cinématographe. Il aura par la suite l'occasion de tourner avec un grand nombre de réalisateurs parmi lesquels Carl Dreyer, G.W Pabst, Abel Gance. Le cinéma lui apparaît 

Le mois de mars 1923 est aussi celui de sa rupture avec Charles Dullin, au moment où l'Atelier crée "Huon de Bordeaux" mélodrame dans lequel Artaud a le rôle de Charlemagne. Mais il est en total désaccord avec le metteur en scène et l'auteur de la pièce sur la manière de jouer. Le 31 mars, le rôle est repris par un autre acteur : Ferréol. Interrogé par Jean Hort, Artaud aurait dit : 

Par l'intermédiaire de Madame Toulouse, Antonin est présenté à André de Lorde, auteur de Grand-Guignol, bibliothécaire de métier. André de Lorde a déjà mis en scène une adaptation d'une nouvelle d'Edgar Poe "Le Système du docteur Goudron et du professeur Plume" qui se déroule dans un asile d'aliénés. Et il a mis au point ce qu'il nomme le « Théâtre de la peur » et le « Théâtre de la mort », un style qui va inspirer Antonin Artaud pour le "Théâtre de la cruauté". Engagé par Jacques Hébertot, Artaud interprète le rôle du souffleur au Théâtre de la Comédie des Champs-Élysées dans la pièce de Luigi Pirandello : Six personnages en quête d'auteur montée par Georges Pitoëff, avec Michel Simon dans le rôle du directeur. Artaud et Simon ont en commun une grande admiration pour Alfred Jarry.

La correspondance d'Antonin Artaud avec Jacques Rivière, directeur de la "NRF", commence cette année-là, en mai-juin, alors qu'Artaud joue au théâtre Liliom de Ferenc Molnár mis en scène par Pitoëff. Une correspondance que Rivière publie plus tard. L'essentiel de sa formation théâtrale est due à Pitoëff sur lequel Artaud ne tarit pas d'éloges dans ses lettres aux Toulouse ou à Génica avec laquelle il vit « un an d'amour entier, un an d'amour absolu ».

Dans ses lettres à Génica, Antonin détaille tous les événements de sa vie quotidienne, même les plus infimes. Ces "Lettres à Génica" sont réunies en recueil, précédé de "Deux Poèmes à elle dédiés".

En 1946, Antonin Artaud décrit son entrée en littérature ainsi : 

Sa véritable entrée en littérature commence dans les années 1924-1925, période de ses premiers contacts avec la NRF et de sa "Correspondance avec Jacques Rivière" qui est publiée en 1924. Jacques Rivière a refusé les poèmes d'Artaud, et c'est à partir de ce refus que s'est établie cette correspondance entre les deux hommes. Cette première publication fait apparaître le rôle très particulier que l'écriture épistolaire joue dans toute l'œuvre d'Artaud. La critique littéraire s'accorde à trouver les poèmes refusés assez conventionnels, tandis que les lettres témoignent, par leur justesse de ton, de la sensibilité maladive d'Artaud que l'on retrouve même dans les plus courts billets et aussi dans ses lettres à Génica, et ses lettres au docteur Toulouse.

Dans ces années-là, si Artaud se plaint de la nécessité de prendre des substances chimiques, mais il défend aussi l'usage des drogues. C'est l'usage des drogues qui lui permet Dans les milieux de la littérature, mais aussi du théâtre et du cinéma, l'usage de l'opium est très répandu, vanté jusque dans les milieux surréalistes, le surréalisme se présentait lui-même comme une drogue dans la préface de "La Révolution Surréaliste" : .

Cette métaphore indique que c'est à la littérature de jouer le rôle de stupéfiant. Mais Artaud préfère se heurter au réel et il vante les mérites de la lucidité anormale que la drogue lui procure dans "L'Art et la mort". L'opium constitue pour lui un territoire de transition qui finit par dévorer tous ses territoires. Bien que Jean Cocteau ait avertit que , mais cela a tout pour plaire au grand anarchiste qu'est Artaud.

Dès 1924, il adhère au surréalisme, et tout en se lançant à l'assaut de le "république des lettres" il entame une carrière de théâtre et de cinéma.

Inspiré par les tableaux d'André Masson, il rédige son premier texte pour le de la revue "La Révolution surréaliste" paru en janvier 1925. C'est son admiration pour Masson qui le conduit à adhérer au mouvement surréaliste, en même temps que le peintre, le 15 octobre 1924. Artaud, qui n'a vécu ni l'expérience Dada, ni les premiers temps du surréalisme, est tout d'abord circonspect sur la théorie de "l'automatisme psychique" chère à André Breton. Son passage par le surréalisme va d'ailleurs moins influer sur son évolution littéraire, que ce qui reste, dans le groupe, de l'anarchisme de Dada. De 1924 à 1926, Artaud participe activement au mouvement avant d'en être exclu. La permanence de la Centrale du bureau de recherches surréalistes, créée le au 15 rue de Grenelle, est assurée par Pierre Naville et Benjamin Péret qui en sont les directeurs. Le dynamisme des textes de Artaud, sa véhémence, apportent un sang neuf à un mouvement qui s'étiole, et soutenu par Breton, il a pour mission de « chasser du surréalisme tout ce qui pourrait être ornemental ».

Après "l'Enquête sur le suicide" parue dans le de la revue, Artaud rédige une "adresse au Pape" dans le de la "Révolution surréaliste" () qu'il remanie en 1946 lors du projet de publication des œuvres intégrales d'Antonin Artaud, ainsi qu'une "Adresse au Dalï-Lama" qu'il remanie en 1946 toujours dans l'optique d'une publication d'œuvres complètes. D'autres textes sont encore publiés dans la revue. Mais le lien avec le collectif ira en s'amenuisant jusqu'à la rupture liée à l'adhésion des surréalistes au communisme. Des divergences sont déjà apparues dès le numéro un dans le groupe. Artaud a tenté de reprendre en main cette "Centrale Surréaliste" dont André Breton lui a confié la direction le . Cependant, au moment où Breton envisage l'adhésion au Parti communiste français Artaud quitte le groupe : 

À l'occasion de son départ, Aragon, Breton, Éluard, Benjamin Péret, Pierre Unik publient une brochure intitulée "Au Grand Jour", destinée à informer publiquement des exclusions de Artaud et Soupault du groupe surréaliste, et de l'adhésion des signataires au parti communiste. Artaud y est violemment pris à partie : Brochure à laquelle Artaud répond sans tarder en juin 1927 avec un texte intitulé "À la grande nuit ou le bluff surréaliste", en termes plus choisis mais non moins violents : 

Ayant quitté Dullin, Artaud rejoint la compagnie de Georges et Ludmilla Pitoëff installée à la Comédie des Champs-Élysées. Puis avec Roger Vitrac, Robert Aron et l'aide matérielle du René Allendy, psychiatre et psychanalyste, qui le soigne, il fonde le Théâtre Alfred Jarry en 1927. Il définit une conception nouvelle de l'art dramatique, publiée plus tard, en 1929-1930, dans une brochure intitulée "Théâtre Alfred Jarry et l'Hostilité publique", rédigée par Roger Vitrac en collaboration avec Antonin Artaud qui rappelle les objectifs du Théâtre Alfred Jarry , mais aussi de 

Le Théâtre Alfred Jarry présente quatre séries de spectacles : "Les Mystères de l'amour" de Vitrac, "Ventre brûlé ou la Mère folle" d'Artaud et "Gigogne" de Max Robur (pseudonyme de Robert Aron), "Le Songe" d'August Strindberg perturbé par les surréalistes (juin 1927), le troisième acte du "Partage de midi" de Paul Claudel joué contre la volonté de l'auteur qu'Artaud qualifie publiquement d'« infâme traître ». Il s'ensuit une brouille avec Jean Paulhan et la reconsidération des surréalistes (janvier 1928). "Victor ou les enfants au pouvoir" de Vitrac sera la dernière représentation (décembre 1928).

En 1971, Jean-Louis Barrault fait un rapprochement entre Alfred Jarry et Antonin Artaud : 

Dans sa biographie parue en 1972, Jean-Louis Barrault reconnaît tout ce qu'il doit à Artaud : 

De juillet à décembre 1929, Antonin Artaud et Roger Vitrac élaborent la brochure qui sera intitulée "Théâtre Alfred jarry et l'Hostilité publique", et il refuse de signer le "second manifeste du surréalisme" qui attaque Breton.
La brochure, qui parait en 1930, est un ensemble de photo montages, mis en scène par Artaud, photographiés par Eli Lotar. Roger Vitrac, Artaud et son amie Josett Lusson ont posé pour les photos. Artaud rédige deux projets de mise en scène, un pour "Sonate" de "Strinberg", l'autre pour "Le Coup de Trafalgar" de Roger Vitrac. Mais il décide de quitter le Théâtre Alfred Jarry. Il s'en explique dans une lettre à Jean Paulhan du : 

Mais Artaud, qui mène de front ses activités littéraires, cinématographiques et théâtrales, a déjà la tête ailleurs. En 1931, il assiste à un spectacle du Théâtre Balinais présenté dans le cadre de l'Exposition coloniale et fait part à Louis Jouvet de la forte impression ressentie : 

Poursuivant sa quête d'un théâtre du rêve et du grotesque, du risque et de la mise en danger, Artaud écrit successivement deux manifestes du "Théâtre de la Cruauté" : 

Sa première réalisation, "Les Cenci", jouée dans des décors et des costumes de Balthus, au théâtre des Folies-Wagram s'arrête faute de moyens financiers. La pièce est retirée de l'affiche après (1935). La critique est partagée et l'article élogieux de Pierre Jean Jouve dans la NRF arrivera trop tard. Artaud considère cela comme un « demi ratage » : 

Cette expérience marque la fin de l'aventure théâtrale d'Antonin Artaud qui envisage déjà de partir au Mexique pour « se CHERCHER » ainsi qu'il l'écrit à Jean Paulhan dans une lettre du 19 juillet 1935. Peu avant, il a assisté à la représentation du spectacle de Jean-Louis Barrault "Autour d'une mère" qui est l'adaptation du roman de William Faulkner " Tandis que j'agonise". Il écrit une note qui sera publiée dans le NRF du : 

Le , paraît un recueil de textes sous le titre "Le Théâtre et son double" comprenant "Le Théâtre et la peste", texte d'une conférence littéralement incarnée. Artaud y jouait sur scène les dernières convulsions d'un pestiféré Selon le récit d'Anaïs Nin, les gens eurent d'abord le souffle coupé, puis ils commencèrent à rire, puis un à un ils commencèrent à s'en aller.

Déçu par le théâtre qui ne lui propose que de petits rôles, Artaud espère du cinéma une carrière d'une autre envergure. Il s'adresse alors à son cousin Louis Nalpas, directeur artistique de la Société des Cinéromans, qui lui obtient un rôle dans "Surcouf, le roi des corsaires" de Luitz-Morat et dans "Fait divers", un court-métrage de Claude Autant-Lara, tourné en mars 1924, dans lequel il interprète « Monsieur 2 », l'amant étranglé au ralenti par le mari.

Toujours par l'intermédiaire de son cousin, Artaud rencontre Abel Gance avec qui il sympathise au grand étonnement de l'entourage du cinéaste, réputé d'accès difficile. Pour son film "Napoléon" en préparation, Abel Gance lui promet le rôle de Marat.

Artaud commence à écrire des scénarios dans lesquels il essaie de « rejoindre le cinéma avec la réalité intime du cerveau ». Ainsi "Dix-huit secondes" propose de dérouler sur l'écran les images qui défilent dans l'esprit d'un homme, frappé d'une « maladie bizarre », durant les dix-huit secondes précédant son suicide.

À la fin de l'année 1927, apprenant la préparation du film "La Chute de la maison Usher" de Jean Epstein, Artaud propose à Abel Gance de jouer le rôle de Roderick Usher : Après quelques essais, Artaud ne sera pas retenu .

La même année, Artaud justifie auprès des surréalistes sa participation au tournage du film de Léon Poirier, "Verdun, visions d'histoire", au motif que 
De la dizaine de scénarios écrits et proposés, un seul sera tourné : "La Coquille et le Clergyman" par Germaine Dulac. Artaud exprime ses objectifs : 

Engagé en même temps par Carl Theodor Dreyer pour son film "La Passion de Jeanne d'Arc", Artaud délaisse le rôle du clergyman qui lui était dévolu et ne suit que par intermittence la réalisation de "La Coquille". Le soir de la première projection au Studio des Ursulines, le , les surréalistes venus en groupe à la séance manifestent bruyamment leur désapprobation.

Dès lors, la magie du cinéma n'existe plus pour lui. Il poursuit malgré tout une carrière d'acteur, pour subvenir à ses besoins. L'avènement du parlant le détourne de cette à laquelle il oppose 

En 1933, dans un article paru dans le numéro spécial "Cinéma 83" "Les Cahiers jaunes" il écrit un éloge funèbre du cinéma : « La Vieillesse précoce du cinéma » 
En 1935, il apparaît deux ultimes fois dans "Lucrèce Borgia" d'Abel Gance et dans "Kœnigsmark" de Maurice Tourneur.

Antonin Artaud a tourné dans plus d'une vingtaine de films, sans jamais avoir obtenu le moindre premier rôle ni même un second rôle d'importance.

En 1936, Artaud part pour le Mexique. Il écrit qu'il s'est rendu à cheval chez les Tarahumaras . Il découvre le peyotl, substance dont Son initiation se fait au cours de la Danse du Peyotl, après de la douzième phase. 

De ce séjour dans la Sierra Tarahumara, on ne dispose que des témoignages d'Artaud et on n'a aucune certitude sur son initiation au rite du peyotl. On n'a pas non plus la certitude qu'il ait effectivement assisté aux danses des indiens, ou même qu'il soit réellement allé dans ce territoire d'accès difficile : s'est-il inspiré des récits d'explorateurs? En 1932, il avait déjà publié dans le magazine "Voilà" deux articles sur des régions où il n'était jamais allé :"Galapagos et les îles du bout du monde " et "L'Anour à Changaï". Pourtant selon J.M. Le Clézio la question de la véracité anthropologique des textes de Artaud n'a guère de sens : 

Outre le récit de son périple au Mexique, il y a encore beaucoup d'autres textes d'Antonin Artaud intitulés "Textes Mexicains", ainsi que les textes de trois conférences données à l'université de Mexico, reproduits dans l'édition Arbalète par Marc Barbezat en 1963. Le premier "Surréalisme et révolution" daté Mexico, , le deuxième " L'Homme contre le destin" daté Mexico , le troisième " Le Théâtre et les Dieux" daté Mexico .

Les trois conférences ont été réunies sous le titre " Messages révolutionnaires" qui est le titre qu'Artaud donna à ses textes dans la lettre adressée à Jean Paulhan le et qui comprennent d'autres textes de Artaud publiés au Mexique principalement dans "El Nacional", mais aussi dans "Revistas de revistas", notamment pour l'exposition de peintures de Maria Izquierdo et de sculptures d'Eleanor Boudin Les trois conférences ont été traduites en français parce que Artaud les avait fait parvenir à Jean Paulhan.

La conférence intitulée "Surréalisme et révolution" commence avec la présentation du tract du , au Grenier des Grands-Augustins rédigé par Georges Bataille. Artaud décrit ainsi le mouvement surréaliste et "Contre-Attaque": 

Et pour décrire son retrait du surréalisme il déclare: 

Parmi les très nombreux articles de Artaud publiés au Mexique, "L'anarchie sociale dans l'art" paru le sous le titre " La anarquía social del arte" dans "El Nacional" définit ainsi le rôle de l'artiste: 

Dès retour en France, il retrouve sa fiancée Cécile Schramme qu'il avait rencontrée en 1935 chez René Thomas. La jeune fille appartient à la bourgeoisie belge. Son père est directeur des tramways de Bruxelles et sa mère, une riche héritière flamande. Artaud contribue à organiser une exposition des gouaches de Maria Izqierdo en janvier-février 1937, mais dès le et jusqu'au , il entre en cure de désintoxication au Centre français de chirurgie, dont les frais seront réglés par Jean Paulhan . Cécile était devenue la compagne d'Antonin avant son départ, elle a partagé sa vie quotidienne à Montparnasse allant même jusqu'à l'accompagner dans sa prise de drogue.

Artaud prend contact avec les milieux littéraires bruxellois. Le il se rend à Bruxelles pour faire une conférence à la Maison de l'Art. Devant une salle comble de , il raconte son aventure mexicaine. Il y a ensuite trois témoignages différents : il est pris d'une crise et il quitte la salle en criant « Qui vous dit que je suis encore vivant ?  » Selon le témoignage de Marcel Lecomte, qui assistait à la conférence, Artaud se serait écrié : « en vous révélant cela je me suis tué. » D'autres témoins racontent qu'il serait arrivé sur scène en disant « Comme j'ai perdu mes notes, je vais vous parler des effets de la masturbation chez les jésuites. » En réalité, on ne sait pas avec certitude de quoi il parla : de son voyage au Mexique selon ceertains, de la pédérasttie selon lui). De toute façon, il fit scandale. Artaud est hébergé dans sa belle famille, jusque-là son beau-père se plaisait à lui faire visiter les hangars des tramways. Mais le scandale de la conférence met un terme aux projet de mariage avec Cécile. Leurs relations sont rompues le . 
Le 12 août 1937, Artaud embarque au Havre pour un périple irlandais, dans les Îles d'Aran. Le il débarque à Cobh, puis il séjourne dans le village de Kilronan, dans l'une des îles d'Aran. Financièrement totalement démuni, il demande des aides à Paulhan, à sa famille, au consulat de France. Il semble avoir quitté sans payer son logement chez un couple à Kilronan et dans un hôtel à Galway. Sa mère découvre aussi, lors de ses recherches, qu'il aurait été hébergé à l'asile de nuit Saint Vincent de Paul à Dublin où il est de retour le . Il avait écrit à sa famille qu'il était sur les traces de la culture celte 

Le 23 septembre 1937, Antonin Artaud est arrêté en Irlande à Dublin pour vagabondage et trouble de l'ordre public. Le 29, il est embarqué de force sur un paquebot américain faisant escale au Havre. Dès son arrivée, le lendemain, Artaud est remis directement aux autorités françaises qui le conduisent à l'Hôpital général, entravé dans une camisole de force. On le place dans le service des aliénés. Jugé violent, dangereux pour lui-même et pour les autres et souffrant d'hallucinations et d'idées de persécution comme l'indique le certificat du 13 octobre 1937, établi par le docteur R. avant le transfert aux Quatre-Mares : Il est transféré sous placement d'office à l'hôpital psychiatrique Les Quatre-Mares de Sotteville-lès-Rouen. Dans le certificat du , établi par le docteur U. de l'hôpital des Quatre-Mares, et reproduit, Artaud est encore présenté comme 

Le 8 novembre 1937, le préfet de la Seine-Inférieure déclare le sieur Antoine Artaud de sorte que Artaud est interné à l'asile des Quatre-Mares. On dispose de peu d'informations sur cet internement. L'hôpital a été détruit pendant la guerre. On ignore quel traitement lui a été appliqué. Une partie de son dossier aurait subsisté après la guerre et aurait fait l'objet de demandes qui n'auraient jamais abouti. Mais comme il était déclaré dangereux, il était isolé dans une cellule et condamné à l'immobilisation par une camisole de force. 

Sa famille et ses amis restés sans nouvelles s'inquiètent. Sa mère Euphrasie entreprend des recherches. Elle s'adresse tour à tour au docteur Allendy, à Jean Paulhan, à Robert Denoël. Elle finit par retrouver son fils en décembre 1937. Antonin, qui pourtant ne la reconnaît pas, donne des détails sur son aventure irlandaise. Un litige oppose alors la famille Artaud et les autorités irlandaises, Euphrasie accuse la police irlandaise, dont les méthodes seraient responsables de l'état d'Antonin, les autorités irlandaises réclament le paiement d'une dette laissée par Antonin.

Au mois de février 1938, Antonin adresse une lettre à dans laquelle il déclare être l'objet d'une méprise, dit qu'il écrit sur les conseils du docteur Germaine Morel médecin chef de l'asile d'aliénés de Sotteville-lès-Rouen. 

En avril 1938, les démarches de sa mère pour le faire transférer aboutissent. Artaud est admis au centre psychiatrique de Sainte-Anne où il reste onze mois sans que l'on connaisse les détails de ce séjour, à l'exception du certificat de quinzaine du , signé du docteur Nodet, qui indique : « Mégalomanie syncrétique : part en Irlande avec la canne de Confucius et la canne de St Patrick. Mémoire parfois rebelle. Toxicomanie depuis (héroïne, cocaïne, laudanum). Prétentions littéraires peut-être justifiées dans la limite où le délire peut servir d'inspiration. À maintenir. ». Artaud refuse toute visite y compris de sa famille. Il n'a cependant jamais cessé d'écrire, bien que l'on ne connaisse aucun texte de lui à cette époque, et malgré l'hypothétique déclaration de Jacques Lacan qui l'aurait déclaré « définitivement perdu pour la littérature », l'indication « graphorée » portée sur le certificat de transfert suivant donne une indication.

Le certificat du 22 février 1939, établi par le docteur Longuet de Sainte-Anne lors du transfert d'Antonin Artaud à l'hôpital de Ville-Évrard (près de Neuilly-sur-Marne, Seine-Saint-Denis) indique : À partir de cette date, il est interné à Ville-Evrard pour trois ans et onze mois. Considéré comme incurable, il ne reçoit aucun traitement. Mais il écrit de nombreuses lettres, et parmi celles-ci, une « Lettre à Adrienne Monnier », qui la fait publier dans "La Gazette des amis du livre" du , et qui reste le seul texte connu de Artaud pour la période 1938-1942. En réponse au reproche que lui fait Jean Paulhan, Adrienne Monnier répond que ce texte témoigne de la grande richesse imaginative que les psychiatres appellent « accès de délire ». Pendant cette période, Antonin Artaud remplit aussi des cahiers d'écoliers de "gris-gris", qui mélangent écriture et dessins. Dès 1940, la situation des internés dans les hôpitaux devient plus difficile du fait du rationnement. Sa mère et ses amis lui envoient des colis, mais ses lettres comportent toutes des appels pour qu'on lui envoie des aliments, et aussi à Genica Athanasiou, pour de l'héroïne

Début 1942, Antonin est dans un état inquiétant : il a faim, il est d'une maigreur effrayante, après avoir perdu dix kilos. Sa mère alerte alors ses amis et persuade Robert Desnos d'entreprendre des démarches auprès de Gaston Ferdière afin qu'Artaud soit transféré dans un autre hôpital.

La technique de l'électrochoc a été importée par des médecins allemands pendant la période d'occupation de la France. À l'époque où Artaud est interné à Ville-Évrard, le docteur Rondepierre et un radiologiste nommé Lapipe ont entrepris d'appliquer la technique de l'électrochoc. Ils font des essais sur des lapins, des porcs, puis sur des patients, la même année. En juillet 1941, ils présentent leurs résultats devant la Société Médico-psychologique. Artaud n'est pas encore soumis au traitement, mais tout se met en place. La mère d'Antonin, se souvenant des essais pratiqués sur l'enfant à l'électricité, demande au docteur Rondepierre s'il serait bon de faire appel à cette méthode pour son fils. Les éléments du dossier médical sont contradictoires sur ce point. Une lettre du docteur Menuau à la mère indique en 1942 « une tentative de traitement qui n'a pas modifié l'état du malade. » En contradiction totale avec une lettre, adressée à Gaston Ferdière par Euphrasie Artaud, dans laquelle le docteur dit qu'Antonin était trop faible pour supporter le traitement. L'usage de l'électrochoc a pourtant bien eu lieu, mais il s'est peut-être soldé par un coma prolongé, et pour cette raison Rondepierre a préféré taire l'incident ? En l'absence d'informations supplémentaires, cela reste une simple hypothèse.

En novembre 1942, Robert Desnos prend contact avec le docteur Gaston Ferdière, ami de longue date des surréalistes et médecin-chef de l'hôpital psychiatrique de Rodez (Aveyron), situé en zone « non-occupée » où la pénurie alimentaire semble moins sévère. Mais les hôpitaux psychiatriques subissent les mêmes, sinon de pires, restrictions que l'ensemble de la population. Les démarches aboutissent et Artaud sera transféré le . 

En décembre 1942, la santé d'Artaud s'est encore dégradée, il pèse entre . Desnos entreprend des démarches pour faire sortir un Antonin Ce n'est que le que Desnos et le docteur Ferdière obtiennent son transfert à Rodez, où on l'installe le pour trois ans, jusqu'au . Entre temps, Artaud fait un court séjour à l'hôpital de Chezal-Benoît où le certificat de vingt-quatre heures donne les observations suivantes : Le court séjour à l'hôpital psychiatrique agricole de Chezal-Benoît est une étape administrative obligatoire en raison de la ligne de démarcation. Artaud y séjourne du au . À Rodez, le docteur Gaston Ferdière est un des pionniers de l'Art-thérapie. Il va accorder immédiatement beaucoup d'attention à Antonin Artaud.

Au moment où Artaud arrive à Rodez, le , l'hôpital ne pratique pas encore l'électrochoc. Ce n'est que peu après son arrivée, en mai 1943 que l'appareil du docteur Delmas-Marsalet est livré à l'hôpital par les ateliers Solex. 

Ainsi, même à Rodez, la technique de l'électrochoc est employée, cette thérapie étant supposée d'une grande efficacité. Artaud subit une première série en juin 1943. Mais la deuxième séance provoque une fracture d'une vertèbre dorsale ce qui l'oblige à garder le lit pendant deux mois. Cela n'empêche pas les médecins de poursuivre le traitement dès le avec une série de d'électrochocs, dont ils se félicitent, jugeant qu'ils ont obtenu « moins de gesticulations et de confusion mentale. » Dans le cadre de l'Art-thérapie, Antonin Artaud avait écrit en septembre deux textes adaptés de Lewis Carroll : " Variations à propos d'un thème " et "Le Chevalier de Mate-Tapis". À partir du , Henri Parisot lui propose de publier chez Robert.J. Godet éditeur, un petit volume comprenant "Un voyage au Pays des Tarahumaras" qui était paru dans la NRF en 1937, et de l'augmenter. Artaud écrit "Le Rite du Peyotl chez les Tarahumaras". Dès le mois de janvier 1944, le docteur Ferdière donne à Artaud une chambre individuelle, où il écrit encore "Supplément au Voyages chez les Tarahumaras". L'artiste exécute aussi de petits dessins, écrit, adapte. Mais sa vie d'écrivain et d'artiste est mis en pointillé entre les séances d'électrochocs, qui reprennent dès le mois de juin 1944, du au . Antonin Artaud écrit au docteur Latrémolière le : 
Le , il envoie une lettre demandant à sa mère de faire interrompre le traitement à l'électrochoc . À chaque série de séances, il perd conscience pendant deux ou trois mois. Il dit avoir besoin de cette conscience pour vivre : . 

Dès janvier 1945, Artaud commence à faire de grands dessins en couleurs qu'il commente ainsi dans une lettre à Jean Paulhan du : Le mois suivant, il se met à travailler quotidiennement sur de petits cahiers d'écoliers où il écrit et dessine. Ce sont les "Cahiers de Rodez", mêlant écriture et dessins. À Rodez, en quinze mois, Artaud en réalise une centaine. Après les de Rodez, suivront les dits" du retour à Paris ". 

1945 est l'année de la renaissance créatrice de Artaud. Inlassablement, il écrit, le sujet de ses textes est toujours la question d'un "autre théâtre" à inventer. En regard de ses grands dessins, l'artiste rédige des commentaires . Evelyne Grossman y voit Deux ans plus tard, dans une lettre adressée à Marc Barbezat, Artaud écrit : 

Cette même année, "Les Tarahumaras" sont publiés par Henri Parisot dans la collection « L'Âge d'or » qu'il dirige aux éditions Fontaine sous le titre "Voyages au pays des Tarahumaras". Des écrits de Artaud sortent de l'hôpital malgré les protestations du docteur Ferdière qui protège les droits financiers et moraux d'Artaud au nom de la défense de biens des aliénés placés sous autorité administrative. Ce sont les "Lettres de Rodez" qui paraîtront l'année suivante, en avril 1946

En septembre 1945, Jean Dubuffet rend visite à Antonin Artaud. Il s'ensuivra avec Jean et madame Dubuffet une correspondance affective, d'autant plus que les recherches de Dubuffet le conduisent très souvent dans des asiles d'aliénés. En 1946, Dubuffet fait le portrait d'Artaud : "Antonin Artaud, cheveux épanouis ". Il fait part à Dubuffet et à Paulhan de son désir de sortir de l'hôpital. Dubuffet s'enquiert des possibilités de sorties. Peu avant, Artaud a lancé des appels à Raymond Queneau et Roger Blin pour qu'on vienne le chercher. Il dit avoir été libéré par le docteur Ferdière. Ferdière a en effet envisagé de le faire sortir mais il temporise car Artaud se déclare toujours la proie d'envoûtements, en particulier dans une lettre à Jean-Louis Barrault le .

Le , le de la revue "Les Quatre Vents" publie les "Lettres de Rodez". Ce même mois, les éditions Guy Lévis Mano (G.L.M) publient les lettres de Rodez à l'instigation de Henri Parisot.

Marthe Robert et Arthur Adamov rendent visite à Artaud le 26/. Le lendemain, Artaud demande, dans une lettre à Jean Paulhan, qu'on le fasse sortir de toute urgence : 

De retour à Paris, Marthe et Arthur très impressionnés par l'environnement de Artaud dans cet asile, considèrent qu'il est nécessaire qu'il revienne à Paris. Une vente aux enchères est organisée à son profit. Un « Comité de soutien des amis d'Antonin Artaud » présidé par Jean Paulhan, et dont Jean Dubuffet est secrétaire, regroupe Arthur Adamov, Balthus, Jean-Louis Barrault, André Gide, Pierre Loeb, Pablo Picasso et Henri Thomas. De son côté, Roger Blin s'emploie à organiser un gala au profit d'Artaud au Théâtre Sarah-Bernhardt

Les amis d'Artaud, Arthur Adamov, Marthe Robert et Jean Paulhan, obtiennent qu'il sorte de l'asile de Rodez. Le , Dubuffet, Marthe Robert, Henri Thomas l'accueillent à la gare et l'installent provisoirement dans une chambre individuelle à la maison de santé du docteur Delmas à Ivry, puis dans un petit pavillon dans le parc. Le de la même année, une séance d'hommage à Antonin Artaud est donnée au Théâtre Sarah-Bernardt, avec un discours d'André Breton en ouverture, et des textes de Artaud lus notamment par Adamov, Jean-Louis Barrault, Rober Blin, Alain Cuny, Jean Vilar (Le Pèse-Nerfs). Le , il enregistre à la radio" Les malades et les médecins", texte diffusé le , publié dans le de la revue "Les Quatre Vents". Le , la vente aux enchères de tableaux offerts par des artistes, et dont Pierre Brasseur est commissaire priseur, lui rapporte assez d'argent, ajoutée à la faible somme recueillie au théâtre Sarah-Bernardt et à ses droits d'auteur, pour vivre jusqu'à sa mort. 

Du 14 septembre au , Artaud séjourne à Sainte-Maxime en compagnie de Marthe Robert et Colette Thomas. Il écrit "L'Adresse au Dalaï Lama" et "L'Adresse au Pape" à l'auberge du Sans-Souci à Sainte-Maxime (Var). Il y termine aussi "Le Retour d'Artaud le Momo" et corrige des textes de 1925 pour les faire figurer dans ses œuvres complètes. Il retourne à Paris où il vivra encore deux ans.

Le 13 janvier 1947, devant une salle comble au Théâtre du Vieux-Colombier, Artaud fait un retour éclatant sur scène avec une conférence intitulée d'après l'affiche : "Histoire vécue d'Artaud-Momo", "Tête à tête par Antonin Artaud", "Le Retour d'Artaud le Momo Centre Mère et Patron Minet-La Culture indienne". Selon André Gide, 

Durant la période où il a été hébergé dans une clinique d'Ivry-sur-Seine, il était libre de ses mouvements. Le peintre Jean-Joseph Sanfourche (1929-2010) lui a rendu visite. Artaud a écrit sur plus de quatre cents cahiers d'écolier, dessiné des autoportraits et des portraits de ses amis à la mine de plomb et craies de couleurs. En novembre 1947, il enregistre pour la radio "Pour en finir avec le jugement de dieu" avec la participation de Maria Casarès, Paule Thévenin et Roger Blin. Programmé pour le , le directeur de la Radiodiffusion française, Wladimir Porché qui avait écouté l'enregistrement la veille, effrayé par le langage trop cru d'Artaud, décide d'interdire l'émission. Il allait en cela à l'encontre d'un verdict favorable à sa diffusion, rendu par un jury d'artistes et de journalistes réunis autour de Fernand Pouey. 

Le texte a fait l'objet d'une publication posthume en avril 1948. La même année, Artaud publie "Van Gogh le suicidé de la société", où il affirme que le peintre n'était pas fou et s'en prend violemment aux psychiatres.

Toujours souffrant, Artaud a repris sa consommation de drogues pour calmer ses douleurs. Il n'effectue pas un séjour de désintoxication, mais continue d'écrire, notamment à la demande de Michel de Ré qui veut fonder une revue théâtrale. Il lui donne entre autres le texte "Aliéner l'acteur". Il écrit également une "Lettre contre la Cabale" adressée à Jacques Prevel publiée en 1949 chez Aumont, et le , il signe deux contrats avec Marc Barbezat : pour "L'Arven et l'Aume" et pour "Les Tarahumaras".

Atteint d'un cancer du rectum diagnostiqué trop tard, Antonin Artaud meurt le matin du , probablement victime d'une surdose accidentelle d'hydrate de chloral, produit dont il connaissait mal l'usage. On l'a retrouvé recroquevillé au pied de son lit. Toutes ses affaires, ses notes, ses livres, ses cahiers, ses dessins accrochés aux murs, ses manuscrits, seront volés quelques heures plus tard.

Artaud était convenu, par contrat avec les éditions Gallimard du , de la publication de ses œuvres complètes (composées d'au moins quatre tomes), dont il avait lui-même inscrit la liste dans une lettre datée du à Gaston Gallimard. Publication qui fut menée pendant près de quarante ans par Paule Thévenin. Sur la dernière page de son dernier cahier de brouillon (cahier 406, feuillet 11), on a pu lire ses dernières phrases : 

Il est enterré civilement au cimetière Saint-Pierre à Marseille, où ses restes ont été transférés depuis le cimetière parisien d'Ivry en avril 1975.

L'esthétique d'Artaud se construit constamment en rapport au surréalisme, d'abord en s'en inspirant, puis en le rejetant (notamment sous la forme que lui donne André Breton).

André Breton, dans son premier "Manifeste du surréalisme" (1924), mentionne Artaud en passant, sans lui accorder une importance particulière. Le second "Manifeste" (1930) arrive après la rupture d'Artaud avec les surréalistes, et Breton lui adresse une critique sévère, quoique esthétiquement peu développée (ses griefs sont surtout d'ordre personnel). Il dénonce notamment le fait que l'« idéal en tant qu'homme de théâtre » d'« organiser des spectacles qui pussent rivaliser "en beauté" avec les rafles de police » était « naturellement celui de M. Artaud ».

Ce jugement qui paraissait irrévocable est corrigé par André Breton après l'hospitalisation d'Artaud : dans l"Avertissement pour la réédition du second manifeste" (1946), Breton dit n'avoir plus aucun tort à compter à Desnos et Artaud, à cause des « événements »(Desnos est mort en camp de concentration et Artaud passe plusieurs mois en psychiatrie à subir des électrochocs). Pure politesse peut-être ; reste que Breton, dans des entretiens publiés en 1952, reconnaît à Artaud une profonde influence sur la démarche surréaliste. Il dit également de lui qu'il était « en plus grand conflit que nous tous avec la vie ».

Pour Jean-Pierre Le Goff, la démarche surréaliste est essentiellement ambivalente, « marquée à ses deux pôles par les figures d'André Breton et d'Antonin Artaud ». Ces deux visions du surréalisme sont comme opposées et complémentaires à la fois. Breton cherchait essentiellement la beauté et l'émerveillement dans la vie, il souhaitait dompter au moyen de l'art « l'altérité inquiétante » de l'inconscient, centrant sa pensée sur la « dynamique positive de l'Eros » aboutissant à la révolution.

Artaud rompt avec cette vision de la poésie et de la vie, expliquant dans son texte « À la grande nuit ou le bluff surréaliste » qu'« ils [les surréalistes] aiment autant la vie que je la méprise ». La rage d'exister d'Artaud n'est pas caractérisée par la capacité de s'émerveiller, mais au contraire par la souffrance et l'angoisse incurables. Cela se ressent dans son esthétique littéraire : Artaud déclare dans "Le Pèse-nerfs" que « toute l'écriture est de la cochonnerie »
. En fait, il refuse violemment toute parenté avec la littérature et les littérateurs. Toujours dans "Le Pèse-Nerfs" il poursuit : Artaud s'éloigne ainsi irrémédiablement de tout platonisme en art : 

Le regard posé par Artaud sur Breton était ambivalent. En 1937, au moment où il écrit les "Nouvelles révélations de l'être", il appelle Breton « l'Ange Gabriel ». Il s'adresse à lui de la même façon dans les lettres qu'il lui écrit depuis l'Irlande. Mais Breton est aussi celui dont Artaud dira (à son ami Jacques Prevel), vers la fin de sa vie, à Paris : ("En compagnie d'Antonin Artaud", de J. Prevel).

Lors de l'exposition surréaliste à la galerie Maeght, en juillet 1947, André Breton lui avait demandé d'y participer. Le refus de Artaud dans une lettre à Breton datée du 28 février 1947, ne laisse aucun doute sur sa position vis à vis du surréalisme. Il écrit : 

Antonin Artaud a eu une profonde influence sur le théâtre, notamment le théâtre américain, mais aussi, sur les situationnistes de la fin des années 1960 qui se réclamaient de son esprit révolutionnaire.
Pierre Hahn rapporte qu'en mai 1968, au moment où les universités étaient occupées, la "Lettre aux recteurs des universités" de Artaud était affichée sur la porte d'entrée. Artaud y disait entre autres : « Je me suis rendu compte que l'heure est passée de réunir des gens dans un amphithéâtre même pour leur dire des vérités et qu'avec la société et son public, il n'y a pas d'autre langage que celui des bombes, des mitrailleuses et tout ce qui s'ensuit - Antonin Artaud cité par Pierre Hahn ». Artaud ne pouvait évidemment qu'attirer vers lui des révolutionnaires extrêmes comme le sont les situationnistes.

De même, le "théâtre de l'extrême" que fut le théâtre américain des années 1960, a pris au pied de la lettre les consignes données par Antonin Artaud dans le théâtre de la cruauté. Dans "The Brig" du Living Theatre, les acteurs sont enfermés dans des cages, humiliés, frappés, réduits aux "éléments passifs et neutres" dont parle Artaud : 

René Lalou rappelle que 

Christian Gilloux compare la réflexion d'Artaud sur ce que doit être le théâtre, avec l'interprétation qu'en a fait Peter Schumann dans le Bread and Puppet Theatre. La forme épurée, minutieusement façonnée, la lenteur des processions, ce jeu "artaudien" des Doubles que l'on retrouve dans le Bread and Puppet part de 

Le renouveau de la mise en scène par les auteurs du Nouveau Théâtre provient en grande partie de leur lecture d'Antonin Artaud et de la manière dont il a conçu l'écriture scénique.

Le 16 janvier 1948 Artaud reçoit le Prix Sainte-Beuve pour Van Gogh le suicidé de la société 

En 1973, le groupe de rock argentin , mené par Luis Alberto Spinetta, nomme son "Artaud" en référence au poète. Spinetta consacre l’oeuvre à Artaud après l’avoir bien lu. La thématique est une réponse au désespoir crée par sa lecture. L’album sera reconnu comme le meilleur album de rock argentin, dans une liste faite par le magazine Rolling Stone (Argentine) en 2007. 

En 1983, le groupe de batcave anglais Bauhaus consacre une chanson à l'écrivain dans son album "Burning From the Inside".’

En 1986, FR3 diffuse la conférence donnée par Artaud le 13 janvier 1947 au Vieux Colombier, .

En 2010, du 5 octobre au 6 novembre au Théâtre de l'Atelier, Carole Bouquet a lu les "Lettres à Génica et autres poèmes" de Artaud. Elle a réitéré sa performance à Rodez en 2011, invitée par "l'association Rodez Antonin Artaud".

E 2013, L'association "Rodez Antonin Artaud" créée par Mireille Larrouy, professeur de français a présenté une exposition : "Antonin Artaud, autoportraits".

En 2014, du 11 mars au 6 juillet 2014 le musée d'Orsay a présenté une exposition associant Vincent van Gogh et Antonin Artaud . Les œuvres de van Gogh était organisée dans un parcours qui mêlait une sélection de tableaux du peintre, des dessins et des lettres de Van Gogh avec des œuvres graphiques de Artaud. Le texte d'Antonin Artaud Van Gogh le suicidé de la société a été lu à cette occasion tous les soirs

En 2015, la Compagnie du Chêne Noir a repris la conférence du "Artaud le Momo", dans une mise en scène de Gérard Gelas au Théâtre des Mathurins sous le titre "Histoire vécue d'Artaud-Mômo" du au , avec Damien Remy dans le rôle d'Antonin Artaud.

Le monde de la chanson lui a également rendu hommage en l'évoquant ou en le citant. Serge Gainsbourg en 1984, Serge Gainsbourg lui consacre un couplet de sa chanson "Hmm, hmm, hmm" de l'album "Love on the beat", :
J'veux parler d'Antonin Artaud

Une piste de l'album "Folkfuck Folie", publié en 2007 par le groupe de black metal français Peste noire, est un « extrait radiophonique d'Antonin Artaud ».

Une chanson du triple album "Messina" de Damien Saez lui rend hommage en 2012. Intitulée "Les Fils D'Artaud", la chanson évoque l'auteur :
Aux enfants de Truffaut
Le 3 mars 1948, par testament olographe sur papier simple, Antonin Artaud écrit : Remis en cause par les héritiers, le travail de Paule Thévenin a donné lieu à une « affaire des manuscrits d'Antonin Artaud » dont Libération s'est fait l'écho en 1995. Parmi ces manuscrits se trouvaient les dessins d'Artaud que la Bibliothèque nationale de France a exposés en 2007 avec l'ensemble des manuscrits.

Les dessins d'Antonin Artaud ont été réunis par Paule Thévenin et Jacques Derrida dans "Antonin Artaud, dessins et portraits" paru le , réédité chez Gallimard en 2000. En 1994 Paule Thévenin a fait un important legs des dessins d'Antonin Artaud au Centre Pompidou, ce qui permet d' accéder à environ une quarantaine de ses œuvres que Jean Dubuffet, amateur de "l'art des fous" appréciait grandement lorsqu'il avait rendu visite à Artaud à Rodez.

On peut consulter en ligne une grande partie des œuvres graphique dont dispose le Centre national d'art et de culture Georges-Pompidou. À titre d'exemple, pour éviter un copié-collé intégral, sont donnés ci-dessous la plus ancienne et la plus récente accessibles en ligne:

Les dessins et peintures d'Artaud ont été exposés de son vivant par Pierre Loeb à la galerie Pierre du au sous le titre "Portraits et dessins par Antonin Artaud". C'est précisément Pierre Loeb qui avait conseillé au poète d'écrire sur van Gogh, après qu'Artaud, bouleversé par l'exposition Van Gogh du 2 février 1947, au musée de l'Orangerie lui eut fait part de ses impressions, rédigées très rapidement et publiées sous le titre "Van Gogh le suicidé de la société". À cette époque, chez Pierre Loeb, Hans Hartung a manifesté auprès du critique d'art Charles Estienne, le désire d'illustrer les textes de Artaud. Lorsqu'il apprend cela, Artaud réagit violemment dans une lettre adressée à « Mr. Archtung » auquel il explique crument qu'il ne saurait en être question. La lettre comporte une de ses formes noircies, brutes dont il a le secret, qui sont ses gris-gris. Beaucoup de ses œuvres sont conservées au Centre Pompidou, dont un autoportrait de décembre 1947 et un portrait de Henri Pichette




 ouvrages utilisés pour les sources




</doc>
<doc id="312" url="https://fr.wikipedia.org/wiki?curid=312" title="Ampoule">
Ampoule

Une ampoule fait référence à l'enveloppe transparente ou translucide d'une lampe électrique, par extension la lampe elle-même.

Elle peut aussi faire référence à :







</doc>
<doc id="315" url="https://fr.wikipedia.org/wiki?curid=315" title="Lampe à incandescence classique">
Lampe à incandescence classique

La lampe à incandescence classique, inventée en 1879 par Joseph Swan et améliorée par les travaux de Thomas Edison, produit de la lumière en portant à incandescence un filament de tungstène, le métal qui a le plus haut point de fusion (). À l'origine, un filament de carbone était utilisé, mais ce dernier en se sublimant puis en se condensant sur le verre de la lampe, opacifiait assez rapidement le verre.

En présence de dioxygène, le filament porté à haute température brûle instantanément, c'est la raison pour laquelle, ce type de lampe a été muni d’une enveloppe de verre qui permet d'isoler un milieu sans oxygène : l'ampoule, qui a donné son nom populaire au dispositif, est protégé par une enveloppe en verre, destiné à fabriquer de la lumière à partir d’électricité.

À l’intérieur de l’ampoule, on trouve soit un gaz caractéristique du type d’ampoule : gaz noble souvent du krypton ou de l’argon ; soit le vide.

Inéluctablement le filament surchauffé se vaporise et perd de la matière par sublimation, ensuite cette vapeur de métal se condense sur l’enveloppe plus froide. L’ampoule devient de plus en plus opaque et le filament devient plus fragile. Le filament finit par se rompre au bout de plusieurs centaines d’heures : pour une lampe classique, jusqu’à moins ou plus pour certaines lampes à usage spécial. 

La présence d'un gaz noble à l'intérieur de l'ampoule présente plusieurs avantages : certains atomes de tungstène devenus gazeux peuvent se déposer à nouveau sur le filament après un choc avec un atome de gaz noble, allongeant ainsi sa durée de vie. Le filament peut aussi être chauffé davantage. Enfin, cela limite le dépôt de tungstène sur la paroi de l'ampoule.

Dans les lampes actuelles, le filament de tungstène est enroulé en hélice, afin d’augmenter la longueur du filament, et donc la quantité de lumière visible produite.

La forme la plus classique de lampe à incandescence est l'ampoule « bulbe », mais on trouve également d'autres formes, dont celle de tube appelée linolite.

La luminosité d'une source est fonction du flux lumineux (en lumen) qu'elle émet. L'efficacité lumineuse mesure le rapport entre le flux lumineux (émis par la source) et la puissance électrique (en watt) qu'elle consomme ; l'efficacité lumineuse s'exprime donc en lumens par watt (lm/W).

Tant que les lampes à incandescences classiques étaient largement majoritaires, les consommateurs pouvaient comparer le flux lumineux des lampes sur la base de leur puissance électrique : ainsi on choisissait une lampe de pour un éclairage intense, pour un éclairage d'ambiance, et pour une veilleuse
Les différentes lampes utilisées comme alternatives aux lampes à incandescence classique ne présentant pas la même efficacité lumineuse, la comparaison des puissances électriques devenant absurde, il est indispensable d'utiliser le lumen comme unité de référence.

Le tableau ci-dessous reprend, de façon indicative car les valeurs varient légèrement d'un modèle à l'autre, la correspondance entre le flux lumineux et la puissance électrique d'une lampe à incandescence classique :

Des alternatives aux lampes à incandescence existent, avec une meilleure efficacité lumineuse, comme les lampes « fluocompactes » et les diodes électroluminescentes. Elles offrent aux industriels des prix assez élevés et des marges assez intéressantes pour qu’ils soient favorables à la substitution.

La production de lampes classiques a été, comme quantité d’autres produits, largement délocalisée : les pays « développés » n’ont plus d’industrie locale à protéger. La réduction de la consommation d’énergie est passée au premier plan, pour des raisons économiques (prix croissant de l’énergie) et écologiques (la production d’énergie est une composante majeure au niveau environnemental).

, soit 1,5 % de la consommation nationale d'électricité en 2010. En Europe, l'économie sera de l'ordre de 

Selon les régions ou pays, le coût environnemental de la production d'électricité comparé à l'émission de mercure des lampes fluocompactes, reste à étudier, afin de définir s'il présente un bilan environnemental positif. Dans les régions où l'électricité est produite au charbon, par exemple, la lampe fluocompacte peut être intéressante, car sa faible consommation se répercute par une importante diminution d'émanations (dues à la combustion du charbon). De tels désavantages ne peuvent être compensés que par une action volontaire des consommateurs qui rapporteraient leurs lampes usagées à un dépôt de recyclage dédié, à défaut de quoi, le mercure se retrouverait libéré dans la nature.

Certains sites spécialisés proposent de réparer ces ampoules.

Les États de l'Union européenne ont approuvé le l’arrêt progressif de la vente des lampes à incandescence de à partir du (puis les modèles de le et ceux de le ), leur abandon définitif devant intervenir le . Le passage à des méthodes d'éclairage moins dépensières en énergie permettrait d'économiser à l'échelle européenne l'équivalent de la consommation électrique de la Roumanie (soit environ de ménages) et de réduire ainsi les émissions de dioxyde de carbone de de tonnes par an


Une directive européenne impose que l'emballage des lampes fasse mention des informations suivantes:

La durée de vie des lampes est surévaluée. La des lampes LED varie d'un modèle à l'autre.

Le filament d'une ampoule à incandescence doit être chaud pour atteindre une bonne luminosité et un rendement élevé pour que l'électricité soit convertie en lumière visible plutôt qu'en chaleur. Mais en augmentant la température, on favorise la sublimation du filament et on accélère donc sa dégradation. De ces deux contraintes, est né un compromis entre une consommation d'électricité réduite et une durée de vie allongée, un compromis économique entre le coût de remplacement des ampoules et celui de l'électricité nécessaire pour les alimenter.

Par exemple, si on réduit la tension de 18 % (ou, inversement, si on conçoit la lampe pour supporter une tension supérieure à la tension disponible), on peut effectivement multiplier la durée de vie par 24. Mais, en contrepartie, la luminosité est diminuée de moitié, et il faut alors deux lampes pour obtenir le même éclairage. La consommation de chaque lampe est donc plus réduite, mais finalement, il faut 45 % d'énergie en plus pour obtenir la même luminosité. 

L'ampoule centenaire ou « ampoule de Livermore » est souvent citée comme preuve "" de la mise en œuvre de l'obsolescence programmée dans la fabrication des ampoules modernes. Cette lampe de à l'origine, à filament carbone, soufflée à la main, fabriquée à Shelby (Ohio), par la Shelby Electric Company à la fin des années 1890, brillerait depuis 1901 dans la caserne des pompiers de Livermore en Californie. N'ayant presque jamais été éteinte, elle serait la plus vieille lampe à incandescence encore en fonctionnement au monde. En fait, l'augmentation de la valeur de la résistance de son filament (en carbone) avec le temps explique sa durée de vie. Ce qui fait que, d'une valeur nominale de en début de vie, sa puissance consommée n'est plus que de (7 % de la valeur du début) et sa luminosité ne correspond plus qu'à 0,3 % de la valeur d'origine. Son rendement a donc été réduit par 24, ce qui revient à dire que le prix en électricité de la lumière produite est 24 fois plus élevé que la normale.

Techniquement, les équations reliant consommation, luminosité et durée de vie des ampoules peuvent être résumées comme suit : si la tension d'alimentation appliquée à l'ampoule est notée formula_1, la luminosité est proportionnelle à formula_2, la puissance électrique (la consommation d'énergie) est proportionnelle à formula_3 et la durée de vie est proportionnelle à formula_4. Ainsi, bien qu'une faible diminution de la tension augmente très fortement la durée de vie, elle augmente la puissance électrique consommée à luminosité constante. 

Créé le 23 décembre 1924, le cartel Phœbus regroupait les principaux fabricants mondiaux d'ampoules. Les industriels éditent une charte commune indiquant qu'il ne pourra plus être fabriqué d'ampoules ayant une durée de vie supérieure à . Ils se dotent pour cela d'une instance commune de vérification et de répression éventuelle au moyen d'amendes d'autant plus élevées que la vie constatée des ampoules est longue. 

En 1924, la durée de vie des ampoules était variable avec une moyenne de . En 1927, dans le monde entier, la durée de vie des ampoules des grandes marques était alignée sur . Cette situation provoque évidemment un plus grand renouvellement des ampoules par les consommateurs et le cartel Phœbus a été accusé d'avoir mis en place sur la lampe à incandescence classique le premier programme massif et mondial d'obsolescence programmée.

Les pratiques du cartel de Phœbus ont fait l'objet en 1951 d'un rapport de la commission anti-trust britannique. Si ce rapport dénonce une entente sur les prix qui aurait conduit le consommateur à payer plus cher ses lampes, il réfute par contre l'accusation d'une limitation de la durée de vie en défaveur du consommateur, montrant que cette durée de vie n'a pas été établie dans le but de réduire la concurrence, mais qu'elle résulte d'un compromis technique entre luminosité, consommation, couleur et durée de vie.

Dans les bandes dessinées et dessins animés, l'apparition d’une idée est souvent représentée par une lampe à incandescence qui s’allume au-dessus de la tête du personnage.

http://www.francelampes.com/lampe-a-incandescence-c-48.html


</doc>
<doc id="316" url="https://fr.wikipedia.org/wiki?curid=316" title="Lampe à incandescence halogène">
Lampe à incandescence halogène

La lampe à incandescence halogène produit de la lumière, comme une lampe à incandescence classique, en portant à incandescence un filament de tungstène, mais des gaz halogénés (iode et brome) à basse pression ont été introduits dans une ampoule en verre de quartz supportant les hautes températures et permettant la régénération du filament, au moins partiellement, ce qui augmente la durée de vie de l'ampoule.



Les principaux avantages et inconvénients par rapport aux ampoules classiques sont.


Ces dernières années, les lampes à halogène se sont multipliées dans les foyers :

Les lampes à halogènes sont également très utilisées dans le domaine de l’automobile et de la motocyclette. Leur dénomination commence par la lettre H :




</doc>
<doc id="317" url="https://fr.wikipedia.org/wiki?curid=317" title="Amplificateur électronique">
Amplificateur électronique

Un amplificateur électronique (ou amplificateur, ou ampli) est un système électronique augmentant la tension et/ou l’intensité d’un signal électrique. L’énergie nécessaire à l’amplification est tirée de l’alimentation électrique du système. Un amplificateur parfait ne déforme pas le signal d’entrée : sa sortie est une réplique exacte de l’entrée mais d’amplitude majorée.

C'est donc un quadripôle actif à base d'un ou plusieurs composants actifs (transistor, amplificateur opérationnel)

Les amplificateurs électroniques sont utilisés dans quasiment tous les circuits électroniques : ils permettent d’élever un signal électrique, comme la sortie d’un capteur, vers un niveau de tension exploitable par le reste du système. Ils permettent aussi d’augmenter la puissance maximale disponible que peut fournir un système afin d’alimenter une charge comme une antenne radioélectrique ou une enceinte électroacoustique.

Le premier amplificateur électronique fut réalisé en 1906 par l’inventeur américain Lee De Forest, à l’aide de la première version d’une de ses inventions : l’audion. En 1908, Lee De Forest perfectionna l’audion en lui rajoutant une électrode, donnant ainsi naissance à la première triode. La triode fut vite perfectionnée par l’ajout d’une (pour la tétrode) puis de deux grilles supplémentaires, palliant certains effets indésirables, notamment l’effet « dynatron » (zone où le tube présente une résistance négative). Ce tube pentode est ensuite rapidement adopté pour la plupart des amplificateurs à tubes, pour son meilleur rendement. Les amplificateurs à tubes sont aussi connus sous le nom d’amplificateurs à « lampes », en raison de la forme des tubes et de la lumière qu’ils émettent lorsqu’ils fonctionnent (voir photo ci-contre).
Depuis le début des années 1960, grâce à l’apparition des premiers transistors de puissance vraiment fiables et au coût réduit, la majorité des amplificateurs utilise des transistors. On préfère les transistors aux tubes dans la majorité des cas car ils sont moins encombrants, fonctionnent à des tensions plus faibles et sont immédiatement opérationnels une fois mis sous tension, contrairement aux tubes électroniques qui nécessitent une dizaine de secondes de chauffage.
Les tubes sont toujours utilisés dans des applications spécifiques comme les amplificateurs audio, surtout ceux destinés aux guitares électriques, et les applications de « très » forte puissance ou à haute fréquence comme pour les fours à micro-ondes, le chauffage par radiofréquence industriel, et l’amplification de puissance pour les émetteurs de radio et de télévision.

Dans le domaine des télécommunications spatiales demandant de fortes puissances, on utilise également des amplificateurs à klystron et des tubes à ondes progressives (ATOP). Il existe en outre des amplificateurs de type ("Solid State Power Amplifier") embarqués à bord des satellites.

Un amplificateur électronique utilise un ou plusieurs composants actifs (transistor ou tube électronique) afin d’augmenter la puissance électrique du signal présent en entrée. Les composants actifs utilisés dans les amplificateurs électroniques permettent de contrôler leur courant de sortie en fonction d’une grandeur électrique (courant ou tension), image du signal à amplifier. Le courant de sortie des composants actifs est directement tiré de l’alimentation de l’amplificateur. Suivant la façon dont ils sont implantés dans l’amplificateur, les composants actifs permettent ainsi d’augmenter la tension et/ou le courant du signal électrique d’entrée. Le principe de fonctionnement d’un amplificateur est présenté dans le schéma simplifié ci-contre. Ce schéma utilise un transistor bipolaire comme composant amplificateur, mais il peut être remplacé par un MOSFET ou un tube électronique. Le circuit de polarisation assurant le réglage de la tension au repos a été omis pour des raisons de simplification. Dans ce circuit, le courant produit par la tension d’entrée sera amplifié de β (avec β » 1) par le transistor. Ce courant amplifié traverse alors la résistance de sortie et l’on récupère en sortie la tension formula_1.
Avec formula_2 le courant d’entrée et formula_3 la valeur de la résistance.

Les amplificateurs peuvent être conçus pour augmenter la tension (amplificateur de tension), le courant (amplificateur suiveur) ou les deux (amplificateur de puissance) d’un signal. Les amplificateurs électroniques peuvent être alimentés par une tension simple (une alimentation positive ou négative, et le zéro) ou une tension symétrique (une alimentation positive, une négative et le zéro). L’alimentation peut aussi porter le nom de « bus » ou « rail ». On parle alors de bus positif ou négatif et de rail de tension positive ou négative.

Les amplificateurs sont souvent composés de plusieurs étages disposés en série afin d’augmenter le gain global. Chaque étage d’amplification est généralement différent des autres afin qu’il corresponde aux besoins spécifiques de l’étage considéré. On peut ainsi tirer avantage des points forts de chaque montage tout en minimisant leurs faiblesses.

Si l’on considère que l’alimentation d’un amplificateur est indépendante du signal d’entrée et de sortie de l’amplificateur, on peut représenter cet amplificateur par un quadripôle. Le formalisme des quadripôles permet d’obtenir une relation matricielle entre les courants et les tensions d’entrée et de sortie. Il a été introduit dans les années 1920 par le mathématicien allemand Franz Breisig. Dans le cas d’un amplificateur de tension, les grandeurs électriques sont définis par quatre paramètres : l’impédance d’entrée Ze, l’impédance de sortie Zs, le gain de transconductance G et le paramètre de réaction G. On a alors :

Pour un amplificateur parfait, G est nul (le courant de sortie n’influence pas l’entrée), Zs est également nul (la tension de sortie ne dépend pas du courant de sortie), et le gain G est constant. On a alors le gain de l’amplificateur :

En pratique ces conditions ne sont pas tout à fait respectées, entraînant de ce fait des caractéristiques altérées concernant la bande passante, le gain en puissance, le bruit dû au facteur température, ou encore la distorsion du signal. On évalue les performances d’un amplificateur en étudiant son rendement, sa linéarité, sa bande passante et le rapport signal sur bruit entre l’entrée et la sortie.

La « bande passante à » (décibel) d’un amplificateur est la gamme de fréquences où le gain en tension de l’amplificateur est supérieur au gain maximum moins trois décibels. Si on ne raisonne pas en décibel, cela correspond à la gamme de fréquences où le gain en tension est supérieur au gain maximum divisé par racine de deux, ce qui correspond à une division de la puissance fournie à la charge par deux. La bande passante est habituellement notée B ou BP. Occasionnellement on rencontre des bandes passantes plus larges, par exemple la bande passante à , gamme de fréquences où le gain en tension est supérieur à la moitié du gain maximum.

La linéarité d’un amplificateur correspond à sa capacité à garder constante la pente de la courbe donnant la tension de sortie en fonction de la tension d'entrée. Une limitation de linéarité vient de l’alimentation de l’amplificateur : la tension de sortie ne peut dépasser la tension d’alimentation de l’amplificateur. Lorsque cela arrive, on parle de saturation de l’amplificateur. La linéarité d’un amplificateur est aussi limitée par sa vitesse de balayage (ou "slew rate") qui représente la vitesse de variation maximale qu’il peut reproduire. Lorsque la variation du signal d’entrée d’un amplificateur est supérieure à sa vitesse de balayage, sa sortie est une droite de pente formula_6.

La vitesse de balayage est exprimée en V/µs.

Enfin, la caractéristique des éléments semiconducteurs n'est jamais totalement linéaire, et conduit à la distorsion harmonique. On réduit cette distorsion par la contre réaction (voir plus loin).

Un amplificateur doit fournir une tension de sortie ayant la même forme que le signal d'entrée, mais d'amplitude supérieure. Si la forme du signal de sortie (à l'amplitude près) est différente de la forme du signal d'entrée, on dit qu'il y a distorsion.

Cette distorsion a lieu si la bande passante de l'amplificateur n'est pas suffisante pour amplifier l'ensemble des fréquences (spectre) composant le signal. Cependant, si le signal d'entrée est sinusoïdal, le signal de sortie le sera également.

Cette distorsion est provoquée par un défaut de linéarité de l'amplificateur. Si le signal d'entrée est sinusoïdal, le signal de sortie ne l'est plus. Cette sinusoïde déformée peut être considérée comme la somme d'une sinusoïde pure (fondamentale) et de sinusoïdes de fréquences multiples de cette fondamentale (harmoniques). Le taux de distorsion harmonique sera fonction du rapport entre ces harmoniques et la fondamentale.

Le signal de sortie d'un amplificateur est composé généralement de plusieurs fréquences, qui devraient être amplifiées strictement en même temps. La forme d'un tel signal complexe ne sera plus conservée si le temps de propagation des fréquences qui le composent n'est pas le même. Ces retards sont peu audibles pour l'oreille. Cependant, si l'amplificateur doit amplifier des signaux numériques, cette distorsion devient très gênante et peut conduire à des erreurs sur les bits transmis et décodés. Pour cette raison, cette caractéristique est très importante pour les amplificateurs de signaux numériques.
On quantifie cette distorsion en précisant les différences de retard en fonction de la fréquence. Il est aussi possible de préciser la courbe du déphasage en fonction de la fréquence. Cette courbe doit être une droite pour ne pas avoir de distorsion de propagation de groupe.
Pour cette raison, les amplificateurs sans cette distorsion sont parfois qualifiés « à phase linéaire ».

Si des étages d'amplification sont non linéaires, on observera en plus de la distorsion harmonique, l'apparition de « fréquences parasites » qui sont des combinaisons linéaires des fréquences composant le signal à amplifier. Ce type de défaut est très gênant pour les amplificateurs traitant de signaux radioélectriques, car ces fréquences parasites peuvent perturber les liaisons radio (voir intermodulation).
Cette distorsion peut également être gênante pour les amplificateurs audio, car l'oreille pourra percevoir ces fréquences parasites qui sont surajoutées au signal.

En électronique, le bruit désigne les signaux aléatoires et non désirés, voire parasites, se superposant aux signaux utiles. Dans un amplificateur ces signaux parasites peuvent venir de son environnement ou des composants le constituant. Il existe cinq types de bruit en électronique : le bruit thermique, le bruit grenaille, le bruit de scintillation (« bruit flicker »), le bruit en créneaux et le bruit d'avalanche. Il est possible de réduire le bruit dans un amplificateur en s’attaquant directement à ses origines (voir ci-dessous) mais aussi en limitant le plus possible la bande passante de l’amplificateur, afin d’éliminer le bruit présent en dehors de ses fréquences de travail.

Le bruit thermique, également nommé "bruit de résistance", ou "bruit Johnson" ou "bruit de Johnson-Nyquist" est le bruit produit par l'agitation thermique des porteurs de charges, c’est-à-dire des électrons dans une résistance électrique en équilibre thermique. Le bruit thermique est un bruit blanc dont la densité spectrale de puissance dépend uniquement de la valeur de la résistance. Le bruit thermique peut être modélisé par une source de tension en série avec la résistance qui produit le bruit.
On caractérise le bruit thermique d'un amplificateur, par sa « résistance équivalente de bruit », ou, pour un amplificateur RF, par le facteur de bruit, qui dépend de la température de la source de signal.

Le bruit thermique a été mesuré pour la première fois en 1927 par le physicien John Bertrand Johnson aux Bell Labs. Son article "Thermal Agitation of Electricity in Conductors" montrait que des fluctuations statistiques se produisaient dans tous les conducteurs électriques, produisant une variation aléatoire de potentiel aux bornes de ce conducteur. Ce bruit thermique était donc identique pour toutes les résistances de la même valeur et n’était donc pas imputable à une fabrication médiocre. Johnson décrivit ses observations à son collègue Harry Nyquist qui fut capable d’en donner une explication théorique.

Le bruit grenaille a été mis en évidence en 1918 par Walter Schottky. Ce bruit apparaît dans les dispositifs où le nombre d’électrons est assez faible pour donner une fluctuation statistique détectable. En électronique, ce bruit apparaît dans les dispositifs à base de semi-conducteur (transistors, etc.) et les tubes électroniques. Le bruit grenaille est un bruit blanc dont la densité spectrale de puissance dépend uniquement de la valeur moyenne du courant traversant le composant bruyant.

"Note :" Le bruit thermique et le bruit grenaille sont tous les deux dus à des fluctuations quantiques, et certaines formulations permettent de les regrouper dans un seul et unique concept.

Le bruit de scintillation, également nommé "bruit en 1/f", "bruit en excès", bruit de "flicker" ou "bruit rose" est un bruit dont la densité spectrale de puissance est en 1/f. Cela signifie que plus la fréquence augmente, plus l’amplitude de ce bruit diminue. Ce type de bruit existe dans tous les composants actifs et a des origines très variées, comme des impuretés dans les matériaux ou des créations et recombinaisons parasites dues au courant de base d’un transistor. Ce bruit est toujours relatif à un courant continu. Il peut être réduit en améliorant les procédés de fabrication des semi-conducteurs et diminuant la consommation de l’amplificateur. Malheureusement, la réduction de la consommation d'un amplificateur passe par une augmentation de la valeur de certaines résistances ce qui va augmenter le bruit thermique.

Le bruit de scintillation se rencontre aussi avec les résistances au carbone, où il est désigné comme "bruit en excès" car il s’additionne au bruit thermique. Le bruit de scintillement étant proportionnel à la composante continue du courant, si le courant est faible, le bruit thermique prédominera quel que soit le type de résistance.

Le bruit en créneaux est également nommé "burst noise", ou "bruit popcorn", ou crépitement. Il a été découvert lors du développement de l’un des premiers amplificateurs opérationnels : le µA709. Il s’agit essentiellement de créneaux de tension (ou de courant) dont l’amplitude s’étend de moins d’un microvolt à plusieurs centaines de microvolts. L’intervalle entre les créneaux est de l’ordre de la milliseconde
Le bruit en créneaux, dans un amplificateur audio, produit des « pops » qui lui ont valu le nom de "bruit popcorn". L’apparition de ces « pops » est aléatoire : ils peuvent se manifester plusieurs fois par seconde puis disparaître pendant plusieurs minutes.

Les origines de ce bruit ne sont pas actuellement connues, mais il semble qu’elles soient liées à des imperfections dans les semi-conducteurs et à l’implant d’ions lourds. Les conditions les plus favorables à l’apparition de ce bruit semblent être de basses températures et la présence de résistances de forte valeur.

Le bruit d’avalanche a lieu dans les semi-conducteurs : le champ électrique accélère certains électrons au point de déloger d’autres électrons de valence et de créer des porteurs de charge supplémentaires. Ce bruit devient important pour les champs électriques élevés, au voisinage de l’effet d’avalanche.

On peut rencontrer d’autres types de bruits dans un amplificateur électronique. Ces bruits ne sont généralement pas dus à l’amplificateur lui-même mais à son environnement. On citera, par exemple, les bruits de quantification et d'échantillonnage engendrés par les convertisseurs numérique analogique et tous les bruits CEM attribués à la présence d’alimentations à découpage, d’émetteurs radio et de télévision et autres appareils sources d’interférences à proximité de l’amplificateur. La plupart de ces bruits peuvent être maîtrisés à l’aide d’un blindage et/ou d’un filtrage des signaux d’entrée et d’alimentation. Dans les cas les plus sensibles, il est parfois nécessaire d’avoir recours à de lourdes tables pour absorber les vibrations, des cages de Faraday, des chambres sourdes et des pièces climatisées.

Le rapport signal-bruit est un terme utilisé en ingénierie, en traitement du signal ou en théorie de l’information pour désigner le rapport entre la grandeur d’un signal (information utile, significative) et celle du bruit (information inutile, non significative). Comme de nombreux signaux ont une échelle dynamique élevée, les rapports signal-bruit sont souvent exprimés en décibels. Le rapport signal sur bruit désigne la qualité d’une transmission d’information par rapport aux parasites. On définit ainsi la qualité d’un amplificateur, quel que soit son type et la catégorie de signaux qu’il traite. Plus le rapport est élevé, moins l’appareil dénature le signal d’origine.

Il existe une grande quantité de classifications, elles découlent souvent des différentes caractéristiques du schéma d’un amplificateur. Toutes ces caractéristiques ont une influence sur les paramètres et les performances de l’amplificateur. La conception d’un amplificateur est toujours un compromis entre plusieurs facteurs comme le coût, la consommation énergétique, les imperfections des composants et, le besoin de rendre l’amplificateur compatible avec le générateur du signal d’entrée et la charge en sortie. Afin de décrire un amplificateur, on parle généralement de sa classe, de la méthode de couplage qui a été utilisée entre ces différents étages ainsi que la gamme de fréquences pour laquelle il est prévu.

Un amplificateur est généralement constitué de plusieurs étages d'amplification, chaque étage étant conçu autour d' « éléments actifs » (des transistors en général). Un élément actif n'est pas nécessairement polarisé de façon à amplifier le signal pendant 100 % du temps. Le système de lettres, ou classe, utilisé pour caractériser les amplificateurs assigne une lettre pour chaque schéma d’amplificateur électronique. Ces schémas sont caractérisés par la relation entre la forme du signal d’entrée et celui de sortie, mais aussi par la durée pendant laquelle un composant actif est utilisé lors de l’amplification d’un signal. Cette durée est mesurée en degrés d’un signal sinusoïdal test appliqué à l’entrée de l’amplificateur, 360 degrés représentant un cycle complet. En pratique la classe d’amplification est déterminée par la polarisation des composants (tubes, transistors bipolaires, transistors à effet de champ, etc.) de l’amplificateur, ou le calcul du point de repos.
Les circuits amplificateurs sont classés dans les catégories A, B, AB et C pour les amplificateurs analogiques, et D, E et F pour les amplificateurs à découpage. Pour les amplificateurs analogiques, chaque classe définit la proportion du signal d’entrée qui est utilisée par chaque composant actif pour arriver au signal amplifié (voir figure ci-contre), ce qui est aussi donné par l’angle de conduction "a" :

Les amplificateurs de classe AB se nomment ainsi car ils fonctionnent comme ceux de classe A pour les signaux de faible amplitude, puis ils passent progressivement en classe B au fur et à mesure que l’amplitude du signal augmente.

Il existe d’autres classes pour les amplificateurs analogiques : G et H. Ces classes ne se distinguent plus des autres grâces à leur angle de conduction mais grâce à leur rendement. La classe G a été introduite en 1976 par Hitachi. Les amplificateurs de classe G possèdent plusieurs bus de tensions différentes et passent de l’un à l’autre en fonction de la puissance demandée en sortie. Cela permet d’augmenter le rendement en diminuant la puissance « perdue » dans les transistors de sortie. Les amplificateurs de classe H sont similaires à ceux de classe G, à la différence près que la tension d’alimentation « suit », ou est modulée par le signal d’entrée.

À l’inverse des amplificateurs analogiques qui utilisent leurs composants actifs dans leur zone linéaire, les amplificateurs à découpage utilisent leurs composants actifs comme des interrupteurs en les amenant dans leur zone saturée. Quand ils sont utilisés ainsi, on peut distinguer deux modes de fonctionnement pour les composants actifs : passant (ou saturé) et bloqué. Quand un composant actif est bloqué, le courant qui le traverse est nul tandis que lorsqu’il est saturé, la chute de tension à ses bornes est faible. Dans chaque mode de fonctionnement, les pertes de puissances sont très faibles permettant ainsi aux amplificateurs à découpage d’avoir un fort rendement. Cette augmentation du rendement permet de demander moins de puissance à l’alimentation et d’utiliser des dissipateurs plus petits que pour un amplificateur analogique de puissance équivalente. C’est grâce à ces avantages en termes de rendement et de volume que les amplificateurs de classe D concurrencent les amplificateurs de classe AB dans beaucoup d’applications .

Les amplificateurs de classe E et F sont des amplificateurs à haut rendement qui sont optimisés pour n’amplifier qu’une faible gamme de fréquences. Ils sont généralement utilisés pour amplifier les fréquences radio. Le principe des amplificateurs de classe E a été publié pour la première fois en 1975 par Nathan O. Sokal et Alan D. Sokal. Les amplificateurs de classe F reprennent le même principe que les amplificateurs de classe E mais avec une charge accordée à une fréquence et à quelques-uns de ses harmoniques, tandis que la charge des amplificateurs de classe E n’est accordée que pour la fréquence fondamentale.

Les amplificateurs sont parfois classés par leur méthode de couplage entre l’entrée et la sortie ou entre les différents étages de l’amplificateur. Ces différentes méthodes incluent les couplages capacitif, inductif (transformateur) et le couplage direct.

Le couplage capacitif permet d'isoler la polarisation des étages entre eux, par contre il ne permet pas d'amplifier le continu. L’utilisation d’un couplage direct permet de se passer des condensateurs de liaisons et d'amplifier le continu, mais implique l’utilisation d’une alimentation symétrique. Le couplage inductif permet de réaliser une adaptation d'impédance entre les étages ou de réaliser un circuit résonant. La plupart des amplificateurs intégrés utilisent un couplage direct entre leurs étages.

On peut aussi décrire les amplificateurs en fonction de leur bande passante. Par exemple, les amplificateurs audio sont conçus pour amplifier les signaux à des fréquences sonores audibles (20 Hz à ) tandis que les amplificateurs d’ondes radio peuvent amplifier des fréquences allant bien au-delà des . Les amplificateurs d’ondes radio peuvent aussi être classés suivant la largeur de leur bande passante. On parle alors d’amplificateurs à bande étroite ("narrowband" en anglais) ou large bande ("wideband" en anglais). Les amplificateurs à bande étroite ne travaillent que sur une faible gamme de fréquences (par exemple de 450 à ) tandis que les amplificateurs large bande peuvent amplifier une grande gamme de fréquences. En général, les amplificateurs à bande étroite utilisent une charge accordée. Les charges accordées sont des filtres passe-bande : elles ne laissent passer qu’une seule fréquence ou une bande de fréquences et permettent d’utiliser des montages de classe E ou F qui sont intéressant car ils possèdent de forts rendements.

Une de ces classifications se réfère à « l’électrode reliée au zéro » : le schéma de l’étage amplificateur est alors décrit par l’électrode du composant actif qui est reliée au plus court au zéro. Ainsi, on parle d’amplificateur à émetteur commun, à "plaque commune" ou à "drain commun". Ces noms renseignent aussi sur le type de technologie utilisée. Par exemple, un amplificateur à émetteur commun utilisera un transistor bipolaire, celui à plaque commune un tube tandis qu’un amplificateur à drain commun utilisera un MOSFET ou un JFET. Quelle que soit l’électrode d’un composant actif, il existe certainement une application ayant amené à la création d’un montage où elle est reliée au zéro. Voir aussi : collecteur commun, base commune.

Une autre façon de classer les amplificateurs est d’utiliser la phase entre le signal d’entrée et celui de sortie. Un "amplificateur inverseur" produira un signal de sortie déphasé de 180 degrés par rapport au signal d’entrée, ou une image miroir de l’entrée si on visualise l’entrée et la sortie sur un oscilloscope. Un "amplificateur non-inverseur" produira quant à lui un signal de sortie ayant la même phase que l’entrée. Un montage émetteur suiveur (ou collecteur commun), est un type d’amplificateur dont le signal sur l’émetteur suit (même phase et même amplitude en tension) le signal d’entrée. Les montages qualifiés de « suiveur » sont des amplificateurs de courant : ils permettent d’obtenir un courant de sortie élevé tout en absorbant un courant d’entrée quasiment négligeable.

Cette description peut s’appliquer à un simple étage ou à un système complet.

Les amplificateurs peuvent aussi être classés par fonctions ou caractéristiques de sortie. Ces descriptions fonctionnelles s’appliquent souvent à un système complet et non à un étage unique.


La contre-réaction soustrait au signal d’entrée une image réduite du signal de sortie avant de l’amplifier. Son principal effet est de diminuer le gain du système. Cependant, les distorsions dues à l’amplificateur sont elles aussi soustraites au signal d’entrée. De cette façon, l’amplificateur amplifie une image réduite et inversée des distorsions. La contre-réaction permet aussi de compenser les dérives thermiques ou la non-linéarité des composants. Bien que les composants actifs soient considérés comme linéaires sur une partie de leur fonction de transfert, ils sont en réalité toujours non linéaires ; leur lois de comportement étant en puissance de deux. Le résultat de ces non-linéarités est une distorsion de l’amplification.

Le principe de la contre-réaction a été découvert par Harold Stephen Black le 2 août 1927. Cette idée lui serait venue alors qu’il se rendait à son travail aux laboratoires Bell
. Ses précédents travaux sur la réduction des distorsions dans les amplificateurs lui avaient déjà permis de découvrir les amplificateurs « "a priori" » ("feedforward" en anglais) qui modifient le signal à amplifier de façon à compenser les distorsions dues aux composants de puissance. Bien qu’ayant refait surface dans les années 1970 pour compenser les distorsions des amplificateurs BLU, dans les années 1920 la réalisation pratique des amplificateurs « "a priori" » s’avère difficile et ils ne fonctionnent pas très bien. En 1927, la demande de brevet de Black pour la contre-réaction fut accueillie comme une demande d’invention de mouvement perpétuel. Elle fut finalement acceptée neuf ans plus tard, en décembre 1931, après que Black et d’autres membres des laboratoires Bell aient développé la théorie relative à la contre-réaction.

Un amplificateur de conception soignée, ayant tous ses étages en boucle ouverte (sans contre-réaction), peut arriver à un taux de distorsion de l’ordre du « pour cent ». À l’aide de la contre-réaction, un taux de 0,001 % est courant. Le bruit, y compris les distorsions de croisement, peut être pratiquement éliminé.

C’est l’application qui dicte le taux de distorsion que l’on peut tolérer. Pour les applications de type hi-fi ou amplificateur d’instrumentation, le taux de distorsion doit être minimal, souvent moins de 1 %.

Alors que la contre-réaction semble être le remède à tous les maux d’un amplificateur, beaucoup pensent que c’est une mauvaise chose. Comme elle utilise une boucle, il lui faut un temps fini pour réagir à un signal d’entrée et pendant cette courte période, l’amplificateur est « hors de contrôle ». Un transitoire musical dont la durée est du même ordre de grandeur que cette période sera donc grossièrement distordu. Et cela, même si l’amplificateur possède un taux de distorsion faible en régime permanent. C’est essentiellement cela qui explique l’existence des « distorsions d’intermodulations transitoires » dans les amplificateurs. Ce sujet a été largement débattu à la fin des années 1970 et pendant une grande partie des années 1980

Ces arguments ont été sources de controverses pendant des années, et ont amené à prendre en compte ces phénomènes lors de la conception d’un amplificateur afin de les éliminer
. Dans les faits, la majorité des amplificateurs modernes utilisent de fortes contre-réactions, alors que les schémas utilisés pour les amplificateurs audio haut de gamme cherchent à la minimiser.

Quels que soient les mérites de ces arguments sur la façon dont elle modifie la distorsion, la contre-réaction modifie l’impédance de sortie de l’amplificateur et par conséquent, son facteur d'amortissement. En simplifiant, le facteur d’amortissement caractérise la faculté d’un amplificateur à contrôler une enceinte. Si tout se passe bien, plus la contre-réaction est forte, plus l’impédance de sortie est faible et plus le facteur d’amortissement est grand. Cela a un effet sur les performances en basses fréquences de beaucoup d’enceintes qui ont un rendu des basses irrégulier si le facteur d’amortissement de l’amplificateur est trop faible.

Le concept de contre-réaction est utilisé avec les amplificateurs opérationnels pour définir précisément le gain, la bande passante et beaucoup d’autres paramètres.

À des fins d’illustration, on utilisera cet exemple pratique d’amplificateur. Il peut servir de base à un amplificateur audio de puissance modérée. Son schéma, bien que sensiblement simplifié, est typique de ce que l’on retrouve dans un amplificateur moderne grâce à son push-pull de classe AB en sortie et à l’utilisation d’une contre-réaction. Il utilise des transistors bipolaires, mais il peut tout aussi bien être réalisé avec des transistors à effet de champ ou des tubes.

Le signal d’entrée est couplé à la base du transistor Q1 à travers le condensateur de liaison C1. Le condensateur permet au signal alternatif de passer, mais il bloque la tension continue due à la polarisation de Q1 par le pont diviseur R1-R2. Grâce à C1, aucun circuit antérieur n’est affecté par la tension de polarisation de Q1. Q1 et Q2 forment un amplificateur différentiel (un amplificateur différentiel multiplie par une constante (appelée gain en tension) la différence entre ses deux entrées). Le schéma utilisé ici pour faire un amplificateur différentiel est aussi connu sous le nom de paire différentielle. Cette configuration est utilisée pour implémenter facilement la contre-réaction, qui est fournie à Q2 grâce à R7 et R8. La contre-réaction dans l’amplificateur différentiel permet à l’amplificateur de comparer l’entrée à la sortie actuelle. Le signal amplifié par Q1 est envoyé directement au second étage, Q3, qui amplifie davantage le signal et fournit la tension continue de polarisation de l’étage de sortie (Q4 et Q5). R6 sert de charge à Q3. Un montage plus évolué utiliserait probablement une charge active, une source de courant constant par exemple. Jusqu’à présent, l’amplificateur travaille en classe A. La paire de sortie est câblée en "push-pull" de classe AB, aussi appelé paire complémentaire. Ils fournissent la majorité de l'amplification du courant et pilotent directement la charge à travers le condensateur de liaison C2 qui bloque la composante continue. Les diodes D1 et D2 fournissent une petite tension continue afin de polariser la paire de sortie, de sorte que la distorsion de chevauchement est minimisée.

Ce schéma est simple, mais c’est une bonne base pour la réalisation d’un véritable amplificateur car il stabilise automatiquement son point de fonctionnement grâce à sa boucle de contre-réaction, qui fonctionne du continu jusqu’au-delà de la bande audio. Un véritable amplificateur utiliserait probablement un circuit supplémentaire faisant baisser le gain au-delà de la bande de fréquences utile afin d’éviter la possibilité d’oscillations non désirées. De plus, l’utilisation de diodes fixes pour la polarisation peut poser des problèmes si les diodes ne sont pas thermiquement et électriquement assorties aux transistors de sortie. En effet, si les transistors deviennent trop passants, ils risquent de se détruire par emballement thermique. La solution traditionnelle pour stabiliser les composants de sortie est d'ajouter des résistances d’un ohm ou plus en série avec les émetteurs. Le calcul des résistances et des condensateurs du circuit se fait en fonction des composants actifs utilisés et de l’utilisation future de l’amplificateur.

On appelle amplificateur intégré un amplificateur se présentant sous la forme d’un circuit intégré. Un circuit intégré (CI ou puce électronique) est lui-même un type de composant constitué de plusieurs composants électroniques sous forme miniaturisée. Le circuit intégré permet de reproduire une ou plusieurs fonctions électroniques plus ou moins complexes, facilitant sa mise en œuvre. Ils contient principalement des transistors, des diodes, des résistances, des condensateurs, plus rarement des inductances car elles sont plus difficilement miniaturisables.

Le premier circuit intégré a été inventé par Jack Kilby en 1958, jetant ainsi les bases de l’informatique moderne. Pour la petite histoire Jack Kilby, qui venait de rejoindre la compagnie, a fait cette découverte alors que la plupart de ses collègues profitaient de vacances organisées par Texas Instruments. À l’époque, Kilby avait tout simplement relié entre eux différents transistors en les câblant à la main. Il ne fallut par la suite que quelques mois pour passer du stade de prototype à la production de masse de puces en silicium contenant plusieurs transistors. Cette découverte a valu à Kilby un prix Nobel de physique en 2000, alors que ce dernier siégeait toujours au directoire de Texas Instruments et détenait plus de 60 brevets à son nom.

Le plus connu des amplificateurs intégrés est l’amplificateur opérationnel (appelé « ampli-op » dans le jargon des électroniciens), mais il existe une multitude d’amplificateurs intégrés spécialement conçus pour une application précise. On citera par exemple, les amplificateurs d’instrumentations pour amplifier les signaux issus de capteurs, les « gainclones » en audio ou les "drivers" de lignes ADSL pour amplifier les signaux ADSL avant qu’ils ne soient envoyés sur une ligne téléphonique.

Les amplificateurs opérationnels (aussi dénommé ampli-op ou ampli op, AO, AOP, ALI, AIL ou encore CIL) ont été initialement conçus pour effectuer des opérations mathématiques en utilisant la tension comme image d’une autre grandeur. C’est le concept de base des calculateurs analogiques dans lesquels les amplificateurs opérationnels sont utilisés pour modéliser les opérations mathématiques de base (addition, soustraction, intégration, dérivation…). Cependant, un amplificateur opérationnel idéal est extrêmement souple d’utilisation et peut effectuer bien d’autres applications que les opérations mathématiques de base. En pratique, les amplificateurs opérationnels sont constitués de transistors, tubes électroniques ou de n’importe quels autres composants amplificateurs et ils sont implémentés dans des circuits discrets ou intégrés.

Les amplificateurs opérationnels ont été initialement développés à l’ère des tubes électroniques, ils étaient alors utilisés dans les calculateurs analogiques. Actuellement, les amplificateurs opérationnels sont disponibles sous forme de circuits intégrés, bien que des versions sous forme de composants discrets soient utilisés pour des applications spécifiques.

Le premier AOP intégré disponible en grande quantité, à la fin des années 1960, fut l’AOP bipolaire Fairchild μA709, créé par Bob Widlar en 1965 ; il a été rapidement remplacé par le μA741 qui offrait de meilleures performances tout en étant plus stable et plus simple à mettre en œuvre.
Le μA741 est encore fabriqué de nos jours, et est devenu omniprésent en électronique. Plusieurs fabricants produisent une version améliorée de cet AOP, reconnaissable grâce au « 741 » présent dans leur dénomination. Depuis, des circuits plus performants ont été développés, certains basés sur des JFET (fin des années 1970), ou sur des MOSFET (début des années 1980). La plupart de ces AOP modernes peuvent se substituer à un μA741, dans un circuit de conception ancienne, afin d’en améliorer les performances.

Les amplificateurs opérationnels sont disponibles sous des formats, brochages, et niveaux de tensions d’alimentation standardisés. Avec quelques composants externes, ils peuvent réaliser une grande variété de fonctionnalités utiles en traitement du signal. La plupart des AOP standard ne coûtent que quelques dizaines de centimes d’euros, mais un AOP discret ou intégré avec des caractéristiques non standard et de faible volume de production peut coûter plus de pièce.

Les principaux fabricants d’amplificateurs opérationnels sont : Analog Devices, Linear Technology, Maxim, National Semiconductor, STMicroelectronics et Texas Instruments.

Un amplificateur d’instrumentation est un dispositif électronique destiné au traitement de faibles signaux électriques. L’application typique est le traitement de signaux issus de capteurs de mesure. Son fonctionnement est basé sur le principe de l’amplification différentielle.

L’amplificateur d’instrumentation est généralement réalisé à partir d’un ou de plusieurs amplificateurs opérationnels, de telle manière qu’il améliore leurs caractéristiques intrinsèques : offset, dérive, bruit d’amplification, gain en boucle ouverte, taux de réjection du mode commun, impédance d’entrée.

Le gain idéal en mode commun de l’amplificateur d’instrumentation est minimisé.
Dans le circuit ci-contre, le gain en mode commun est causé par les différences de valeur entre les résistances portant le même nom et le gain en mode commun non nul des deux AOP d’entrées. La réalisation de résistances appairées en valeur est la principale contrainte de fabrication des circuits d’instrumentation.

Les amplificateurs d’instrumentation peuvent être réalisés avec plusieurs AOP et des résistances de précision, mais ils sont aussi disponibles sous forme de circuits intégrés dans les catalogues de plusieurs fabricants (dont Texas Instruments, Analog Devices, et Linear Technology). Un amplificateur d’instrumentation intégré contient généralement des résistances dont les valeurs ont été ajustées avec précision à l’aide d’un laser, et offre donc un excellent taux de réjection du mode commun.

Un amplificateur programmable désigne un amplificateur conçu pour que son gain soit programmable à distance, généralement via une liaison filaire (RS, GPIB ou autre), à la différence des amplificateurs classiques nécessitant un réglage manuel via une molette par exemple.





</doc>
<doc id="319" url="https://fr.wikipedia.org/wiki?curid=319" title="Liste de sigles en espagnol">
Liste de sigles en espagnol

Voici une liste de sigles utilisés en espagnol.

Vous trouverez à côté de chaque sigle le nom complet et une traduction en français



</doc>
<doc id="320" url="https://fr.wikipedia.org/wiki?curid=320" title="Alcatel">
Alcatel

Alcatel (acronyme dAlsacienne de constructions atomiques, de télécommunications et d’électronique") était une entreprise française spécialisée dans le secteur des télécommunications. Elle fusionne avec Lucent Technologies au mois de décembre 2006 pour devenir Alcatel-Lucent"'.

À l'origine une petite entreprise basée à Mulhouse et appartenant au groupe SACM qui concevait et fabriquait des équipements de télécommunications, elle fut absorbée en 1968 par la Compagnie Industrielle des Télécommunications (CIT), appartenant à la Compagnie générale d'électricité (CGE) ; création de CIT-Alcatel.

Entre 1968 et 1998, aucune entreprise ne portait le nom unique Alcatel, qui était alors associé à CIT. En 1998, pour des raisons d'image, la CGE prend le nom Alcatel (après Alcatel-Alsthom en 1991). CIT-Alcatel devient Alcatel-CIT.

Alcatel-CIT était un des leaders mondiaux dans la fourniture de commutateurs téléphoniques numériques (série E10), des câbles de transmission sous-marins, de l'infrastructure mobile (GSM, GPRS, UMTS), des applications de réseaux intelligents, des applications de Centre d'Appel, des applications vidéo (fixe et mobile) ainsi que des satellites et des charges embarquées. C'était aussi le leader mondial des marchés des réseaux optiques, des équipements d'accès DSL et des routeurs ATM et IP.

Alcatel fournissait aussi des services à tous ses clients depuis la conception de réseaux jusqu’à l'exploitation de ceux-ci en passant par la fabrication des équipements, le déploiement, l'intégration et l'installation.

En 2005, Alcatel était présent dans plus de 130 pays, avec un chiffre d'affaires de 13,1 milliards d'euros.

Le 31 mai 1898, la Compagnie générale d'électricité (CGE) est créée par Pierre Azaria et Paul Bizet avec Charles Herbault comme président. Pierre Azaria est alors administrateur délégué et Paul Bizet devient directeur général. L'ambition est de concurrencer des sociétés telles que AEG, Siemens ou General Electric.

En 1913, la Compagnie générale d'entreprises électriques (CGEE) est créée en tant que filiale de la CGE. En 1914, un nouveau siège social de la CGE est inauguré, au 54 rue La Boétie, Paris.

En 1925, la CGE absorbe la Compagnie générale des câbles de Lyon (la marque "Les Câbles de Lyon" étant préservée). En 1930, la CGE prend le contrôle de la Société des accumulateurs fixes et de traction (Saft).

En 1946, la Compagnie industrielle des téléphones (CIT) est créée.

En 1959, le centre des recherches de Marcoussis débute ses activités.

En 1965, CGE acquiert la Téléphonie industrielle et commerciale (Télic).

En 1966, les accords CGE-Hispano-Alsacienne sont signés. En 1968, la CGE prend le contrôle d'Alcatel.

En 1969, la CGE devient l'actionnaire majoritaire d'Alsthom, entreprise fabricant des locomotives et des moteurs créée en 1928 par le rapprochement de la Société alsacienne de constructions mécaniques et de la Compagnie Française Thomson-Houston.

En 1970, Ambroise Roux devient président de la CGE. Après 1982, il restera président d'honneur de la CGE jusqu’à son décès, en 1999. La même année, la CIT et Alcatel fusionnent, et le premier autocommutateur temporel est mis en service à Lannion (Côtes-d'Armor), le PLATON

En 1971, la CGE prend le contrôle de la Société générale d'entreprises (Génie civil, Bâtiment, Travaux industriels, Service électrique), aujourd'hui Vinci. La CGEE est renommée CGEE-Alsthom.

En 1972, Le train à grande vitesse TGV 001 (Alsthom), sorti en 1972, atteint 

En 1974, la CGE créée deux filiales, Electrobail, spécialisée dans les opérations de leasing, et la SLET, Société de location d'équipements téléphoniques privés.

En 1976, Alsthom absorbe les Chantiers de l'Atlantique, qui devient Alsthom-Atlantique. Alsthom-Atlantique étant contrôlé en totalité par la CGE. La Générale de services téléphoniques (GST) est créée, à la suite de la prise de contrôle de trois sociétés d'installation téléphonique.

En 1978, sa filiale GST fait des acquisitions.

En 1979, la société « la téléphonie industrielle et commerciale » est créée, elle est chargée de commercialiser les produits de Telic et de CIT Alcatel. 

En 1979, CGE prend une participation de 34 % au capital de Locatel, qui fournit au groupe CGE un réseau de points de vente.

En 1981, le TGV bat le record du monde de vitesse sur rail avec (rame ). En 1990, la rame portera ce record à .

En 1982, CGE est nationalisée. Ambroise Roux démissionne. Jean-Pierre Brunet devient président de la CGE. CGE prend le contrôle de la Sesa (SSII). Alcatel compte alors salariés.

En 1983, les activités de télécommunications publiques et de communication d'entreprise de Thomson-CSF sont regroupées au sein d'une holding Thomson Télécommunications passant sous le contrôle du groupe CGE.

En 1983, dans le cadre des accords CGE-Thomson, les sociétés Thomson Jeumont Câbles et Kabeltel sont acquises par les Câbles de Lyon.

En 1983, La Transac est cédée à Bull, alors que ses effectifs dépassent 1100 personnes.

En 1984, Georges Pébereau devient président de la CGE.

En 1985, Alsthom-Atlantique prend la dénomination Alsthom. CIT-Alcatel-Thomson Télécommunications fusionnent, la nouvelle société prenant le nom d'Alcatel.

En 1986, Pierre Suard accède à la présidence du groupe. CGE entre à hauteur de 40 % dans le capital de Framatome. Les Câbles de Lyon deviennent une filiale d'Alcatel.

En 1987, la CGE est privatisée. La Générale Occidentale de Jimmy Goldsmith est acquise, détenant notamment L'Express. Alsthom participe à l'équipement du réseau TGV Atlantique et prend la tête du consortium d'entreprises françaises, belges et anglaises chargées du réseau nord du TGV. La SSII Sesa est vendue au groupe Cap Gemini Sogeti

En 1988, "Alcatel NV", une société de droit néerlandais, est créée à la suite de l'accord conclu avec ITT Corporation qui cède ses activités télécommunications à la CGE. (filiale allemande, SEL : Standard Electrik Lorenz - filiale belge, Bell). Cette société choisit l'anglais comme langue de travail internationale.

En 1989, Alsthom fusionne avec la branche GEC Power Systems du groupe britannique General Electric Company, la nouvelle entité, GEC Alsthom, est une coentreprise franco-britannique, filiale commune de GEC et de la CGE. CGEE-Alsthom prend le nom de Cegelec.

En 1990, un accord entre CGE et Fiat est signé, Alcatel prend le contrôle de , spécialisée dans les systèmes de transmission, et Fiat devient majoritaire dans la CEAC. Les Câbles de Lyon acquiert les Câbleries de Dour (Belgique) et des activités câbles d'Ericsson aux États-Unis. Un accord sur la composition du capital de Framatome est signé, la CGE détenant 44,12 %.

En 1991, la Compagnie générale d'électricité prend la dénomination Alcatel Alsthom. Alcatel Alsthom achète la division systèmes de transmission du groupe américain Rockwell Technologies. Câbles de Lyon devient Alcatel Cable et rachète AEG Kabel.

En 1993, Alcatel-Alsthom acquiert STC Submarine Systems, une division de Northern Telecom Europe (qui deviendra Nortel).

En 1995, Serge Tchuruk devient Président-directeur général d'Alcatel Alsthom. Il entreprend de réorganiser la société pour la recentrer sur les télécommunications. Il vend l'Express à la Compagnie européenne de publications.

En 1998, Alcatel Alsthom décide de se recentrer sur ses métiers de base, à savoir les télécommunications. Elle décide avec GEC de vendre en bourse la majorité (52 %) du capital de GEC Alsthom, chacun en conservant 24 %. Celle-ci, ainsi indépendante, décide de prendre le nom d'Alstom, sans h. Alcatel Alsthom, quant à elle, se renomme Alcatel. Elle ne compte plus alors que salariés. La filiale d'électricité Cegelec est revendue à Alstom. Alcatel acquiert la société DSC, fortement implantée auprès des opérateurs américains

En 1999, Alcatel acquiert les sociétés américaines Xylan, Packet Engines, Assured Access et Internet Devices, spécialisées dans les réseaux et solutions pour l'Internet. Alcatel porte sa participation dans Thomson CSF à 25,3 % et réduit sa participation dans Framatome à 8,6 %.

En 2000, Alcatel acquiert la société canadienne Newbridge, leader mondial des réseaux en technologie ATM. Il vend l'activité modem DSL à Thomson Multimedia. Il acquiert la société américaine Genesyslab, leader mondial des centres de contact, et la société canadienne Innovative Fibers, leader mondial des filtres optiques en DWDM. Les activités câbles et composants sont filialisées et prennent la dénomination de Nexans.

En 2001, Alcatel cède de sa participation de 24 % dans Alstom. Alcatel met en bourse une large part des activités câbles et composants d'Alcatel qui deviendront Nexans, Alcatel conservant 20 % du capital. Le groupe acquiert 48,83 % d'Alcatel Space détenus par Thales portant ainsi la participation d'Alcatel à 100 %. Sa participation dans Thales est réduite à 20 %, via une cession de 4,2 % de sa participation dans Thales. Alcatel cède sa participation de 2,2 % dans Areva (ex Framatome).

En 2002, Alcatel finalise l'acquisition d'Astral Point Communications, société américaine spécialisée dans les systèmes métropolitains optiques SONET de prochaine génération. Il cède ses activités microélectroniques à STMicroelectronics. Il sort du capital de Thomson (ex TMM). À la suite de la stratégie « "fabless" » du Président en pleine bulle Internet, l'usine de Brest est vendue à Jabil Circuits après un audit économique et social mené par Cluny Finance (« "due diligences" »). Alcatel prend le contrôle de 50 % d'Alcatel Shanghai Bell, finalise l'acquisition de Telera, cède 10,3 millions de titres Thales, ramenant ainsi la participation d'Alcatel de 15,83 % à 9,7 %, et cède de 1,5 million de titres Nexans, ramenant la participation d'Alcatel de 20 % à 15 %.

En 2003, Alcatel, vend 50 % de sa participation dans Atlinks, un fabricant de téléphones résidentiels, à Thomson. Alcatel acquiert iMagicTV, fournisseur canadien d'applications et de services qui permettent au fournisseurs de service de créer, de distribuer et de gérer la télévision numérique et les services média sur les réseaux haut débit. Il acquiert également TiMetra société privée basée dans la Silicon Valley, qui produit des routeurs. Il vend sa division Composants Optiques à Avanex, la division Saft Batteries à Doughty Hanson. Alcatel et Draka créent le leader mondial du câble optique.

En 2004, Alcatel vend SAFT, une division du groupe spécialisée dans les batteries à Doughty Hanson. Alcatel et TCL Communication Technology Holdings Limited forment une "coentreprise" de téléphonie mobile. Cette nouvelle société est détenue à 55 % par TCL et 45 % par Alcatel. Alcatel et Draka ont combiné leur activité de fibres optiques et de câbles de communications. Draka détient 50,1 % et Alcatel 49,9 % de cette nouvelle société, Draka Comteq B.V. Alcatel a acquis la société américaine eDial, un leader dans la fourniture de services de conférence et de collaboration pour les sociétés et les compagnies de téléphonie. Alcatel a vendu 7,1 millions d'actions d'Avanex, ramenant sa participation sous 20 %. Alcatel a finalisé l'acquisition de la société américaine Spatial Communications (connu sous le nom de Spatial Wireless), un leader dans la fourniture des logiciels et des solutions de switchs mobiles multi-standard. En novembre, Alcatel rachète la société française Right Vision basée à Sophia Antipolis, leader dans le domaine des Internet Appliances. Le but de cette acquisition est la fourniture de solutions de convergence voix/données.

En 2005, les entreprises chargées du déploiement du système de localisation par satellite Galileo sont désignées le 27 juin 2005 : Alcatel, EADS, Finmeccanica et Thales. Alcatel et Finmeccanica créent le leader européen des satellites, et le mondial : Alcatel Alenia Space.

En avril 2006, Alcatel et l'Américain Lucent Technologies annoncent leur fusion, donnant naissance à un des leaders mondiaux des infrastructures de télécommunications. Au même moment, Thales acquiert l’activité spatiale et satellites d’Alcatel, contre une montée d’Alcatel à 21,6 % dans son capital, Thales récupère la participation des 2/3 d’Alcatel dans Alcatel Alenia Space et celle du 1/3 d’Alcatel dans Telespazio. Les parts complémentaires restent détenues par Finmeccanica.

Le décembre 2006, l'achat de Lucent Technologies par Alcatel devient effectif, sous le nom Alcatel-Lucent. Patricia Russo devient directrice générale du groupe, Serge Tchuruk devient président du conseil d'administration - président non exécutif.

Avant sa fusion avec Lucent en 2006, Alcatel possèdait plusieurs filiales : 'ALCATEL-CIT', 'ALCATEL-Optronics', 'ALCATEL-Cables', etc. 


Alcatel était coté dans quatre indices développement durable majeurs : Dow Jones Sustainability World (depuis septembre 2005), FTSE4Good (depuis 2002), ASPI Eurozone (depuis septembre 2005) et Ethibel (depuis janvier 2005). Le groupe a été évalué par ces indices en termes de gouvernance d'entreprise, de politique de ressources humaines, de responsabilité sociale dans la chaîne d'approvisionnement, de gestion de l'environnement, de réduction de la fracture numérique, de dialogue avec les parties prenantes et de citoyenneté d'entreprise. L'entrée dans ces indices est la reconnaissance de l'engagement de l'entreprise pour concilier développement économique et responsabilités sociales et environnementales et mettre en œuvre des plans d'action efficaces en accord avec les principes du Pacte Mondial des Nations unies.



Alcatel disposait dans ses statuts d'un système de limitation des droits de vote. Selon celui-ci, « "un actionnaire ne pourra exprimer en assemblée générale plus de 8 % des voix attachées aux actions présentes ou représentées lors du vote de toute résolution" », voire 16 % s'il dispose de droits de vote doubles.



</doc>
<doc id="322" url="https://fr.wikipedia.org/wiki?curid=322" title="Juillet 2003">
Juillet 2003















Mort de Compay Segundo à Havane ( Cuba )

Décès de Paulette Codazzi, fauchée sur son trottoir par un chauffard alcoolisé... A Maxey-Sur-Vaise (Meuse)

















</doc>
<doc id="323" url="https://fr.wikipedia.org/wiki?curid=323" title="Archaea">
Archaea

Les archées, ou Archaea (du grec ancien , « originel, primitif »), anciennement appelées archéobactéries, sont des microorganismes unicellulaires procaryotes, c'est-à-dire des êtres vivants constitués d'une cellule unique qui ne comprend ni noyau ni organites, à l'instar des bactéries. D'apparence souvent semblable à ces dernières, les archées ont longtemps été considérées comme des bactéries extrêmophiles particulières, jusqu'à ce que les recherches phylogénétiques sur les procaryotes, commencées en 1965<ref name="10.1016/0022-5193(65)90083-4">
..</ref>, à la publication en 1977 d'un arbre phylogénétique fondé sur les séquences des gènes d'ARN ribosomique des organismes étudiés, arbre dans lequel les procaryotes étaient scindés en deux domaines distincts, celui des bactéries et celui des "archées"<ref name="10.1007/BF01796092">
..</ref>. Cette vision s'est depuis largement imposée aux microbiologistes<ref name="10.1038/441289a">
.</ref> mais demeure contestée par certains scientifiques, tel Thomas Cavalier-Smith pour qui les archées, qu'il appelle "Archaebacteria", ne sont qu'un embranchement ("phylum") des "Unibacteria" dans le règne des bactéries.

Du point de vue de leur génétique, leur biochimie et leur biologie moléculaire, les archées sont des organismes aussi différents des bactéries que des eucaryotes. Les enzymes réalisant la réplication de l'ADN, la transcription de l'ADN en ARN ainsi que la traduction de l'ARN messager en protéines chez les archées sont apparentées à celles des eucaryotes et non à celles des bactéries, de même que la présence d'histones dans le matériel génétique des archées rapproche ces dernières des eucaryotes et les distingue des bactéries. Par ailleurs, les gènes des archées possèdent des introns et leur ARN messager subit des modifications post-transcriptionnelles, ce qui est le cas également chez les eucaryotes mais pas chez les bactéries. Par ailleurs, certaines archées possèdent des voies métaboliques qui n'existent ni chez les bactéries ni chez les eucaryotes, comme la méthanogenèse chez les archées méthanogènes, tandis que les archées dans leur ensemble sont dépourvues d'acide gras synthase, contrairement à la fois aux bactéries et aux eucaryotes : elles font un usage très limité des acides gras, et leur membrane plasmique est constituée essentiellement d'étherlipides, à la différence des bactéries et des eucaryotes. Un autre trait propre aux archées est la présence chez certaines d'entre elles d'une paroi cellulaire constituée de pseudopeptidoglycane, ou "pseudomuréine".

Les archées ont longtemps été vues comme des organismes essentiellement extrêmophiles présents notamment dans les sources hydrothermales océaniques, les sources chaudes volcaniques ou encore les lacs salés, mais on en a découvert depuis dans tout une variété de biotopes qui ne sont pas nécessairement extrêmes, tels que le sol, l'eau de mer, des marécages, la flore intestinale et même le nombril humain. Les archées seraient particulièrement nombreuses dans les océans, et celles faisant partie du plancton constitueraient l'un des groupes d'organismes les plus abondants de la Terre. Les archées interviennent par ailleurs de façon non négligeable dans le cycle du carbone et le cycle de l'azote. On ne connaît pas vraiment d'exemple d'archée pathogène ou parasite, mais elles sont souvent mutualistes ou commensales. Les archées méthanogènes de l'intestin humain et des ruminants participent ainsi favorablement à la digestion.

La taille et la forme des archées sont généralement semblables à celles des bactéries, bien que certaines espèces d’archées présentent une forme inhabituelle, comme "Haloquadratum walsbyi" dont la cellule est plate et carrée. En dépit de ces similitudes visuelles avec les bactéries, les archées s’en distinguent par certains caractères biochimiques, comme la constitution de la membrane cellulaire. De plus, elles présentent des gènes et des voies métaboliques semblables à ceux rencontrés chez les eucaryotes, notamment les enzymes impliquées dans le mécanisme de réplication de l'ADN, la transcription et la traduction. Les archées utilisent une plus grande variété de sources d’énergie que les eucaryotes : composé organique comme les sucres, l’ammoniac, les ions métalliques et même l’hydrogène gazeux comme nutriments. Les "Halobacteria" utilisent la lumière solaire comme source d’énergie, et certaines espèces d’archées peuvent fixer le carbone, cependant, à l’inverse des plantes et des cyanobactéries, il n’y a pas d’espèces d’archées connues capables de réaliser ces deux phénomènes. Les archées se reproduisent de manière asexuée et se divisent par fission binaire, fragmentation ou bourgeonnement. Par opposition aux bactéries et aux eucaryotes, aucune espèce d’archées identifiée à ce jour n’est capable de former des spores.

Les archées sont extrêmement diversifiées. Certaines sont connues pour leur capacité à vivre dans des conditions extrêmes et occupent des niches écologiques qu'elles sont souvent seules à occuper (pH proche de 0, température supérieure à , salinité élevée par exemple), mais il existe beaucoup d’archées vivant dans des biotopes plus courants et très variés comme le sol, les lacs, la mer ou l’intestin des animaux. Elles contribueraient jusqu'à 20 % du total de la biomasse. Ces procaryotes sont maintenant ainsi reconnus comme une part majeure du vivant sur Terre, ils peuvent jouer un rôle dans le cycle du carbone et le cycle de l'azote. Il n’y a pas d’exemple clairement reconnu d’archées pathogènes ou parasites, mais il existe des espèces mutualistes ou commensales. Par exemple, les archées méthanogènes du tractus intestinal de l’homme et des ruminants participent à la digestion des aliments. Les archées ont également une importance en technologie, avec par exemple l’utilisation des méthanogènes pour produire des biogaz ou leur participation au traitement des eaux usées. Par ailleurs, les enzymes des archées extrêmophiles, résistantes aux températures élevées et aux solvants organiques, sont exploitées en biotechnologie.

Au début du , les procaryotes étaient considérés comme un seul groupe d'organismes et classés en fonction de leur biochimie, de leur morphologie et du métabolisme. Par exemple, les microbiologistes essayaient de classer les micro-organismes sur la base des structures de leurs parois cellulaires, leurs formes, et les substances qu'ils consomment. Cependant, une nouvelle approche a été proposée en 1965 qui permet d’étudier les liens de parentés entre les procaryotes en utilisant les séquences des gènes de ces organismes. Cette approche, connue sous le nom de la phylogénétique, est la principale méthode utilisée aujourd'hui.

Les archées ont d'abord été classées comme un groupe distinct des procaryotes en 1977 par Carl Woese (professeur à l'université de l'Illinois à Urbana aux États-Unis) et George E. Fox dans les arbres phylogénétiques fondés sur les séquences de l’ARN ribosomique 16S (ARNr) des gènes. Ces deux groupes ont été initialement nommés les eubactéries et archaeobactéries et traités comme sous-règne ou règne. Woese fait valoir que ce groupe de procaryotes est fondamentalement différent des bactéries. Pour souligner cette différence, et pour insister sur le fait qu’ils composent, avec les eucaryotes, trois domaines bien distincts du vivant, ces deux domaines ont plus tard été renommés "Archaea" et "Bacteria". Le mot archées vient du grec ancien , qui signifie « choses anciennes ».

Dans un premier temps, seules les bactéries méthanogènes, isolées initialement par Carl Woese, ont été placées dans ce nouveau domaine et les archées ont été considérées comme des extrêmophiles qui n'existent que dans les habitats tels que les sources chaudes et les lacs salés : Woese découvre en 1979 les hyperhalophiles (genre "Halobacterium") puis les thermoacidophiles ("Thermoplasma acidophilum", "Sulfolobus acidocaldarius"), Karl Stetter isole en 1981 une archée anaérobie hyperthermophile, "Pyrococcus furiosus". À la fin du , les microbiologistes se sont rendu compte que les archées sont en fait un grand groupe diversifié d'organismes qui sont très répandus dans la nature et qui sont communs dans une diversité d’habitats, tels que les sols et les océans. Cette nouvelle appréciation de l’importance et de l'ubiquité des archées a été rendu possible grâce à la réaction en chaîne par polymérase pour détecter les procaryotes dans des échantillons d'eau ou de sol à partir de leurs acides nucléiques. Cela permet la détection et l'identification d'organismes qui ne peuvent pas être cultivés en laboratoire, ou dont la culture est difficile.

Ces organismes ont longtemps été regroupés sous le terme générique de "procaryotes" avec les bactéries. Pour les différencier, les microbiologistes avaient élaboré un système de comparaison et de classification fondé sur de petites différences visibles au microscope, ainsi que sur des différences physiologiques (capacité à se développer sur un certain milieu par exemple).

Dès qu'il a été question d'élucider les relations généalogiques entre les différents procaryotes, les biologistes ont dû se rendre à l'évidence : les différences nutritionnelles et phénotypiques ne permettraient pas de classer correctement les différents organismes. Au cours des , les biologistes ont pris conscience de l'existence irremplaçable d'information, au cœur même des cellules des êtres vivants, permettant de déterminer la phylogénie, l'ADN. Le gène identifié dans une cellule est le variant d'un gène qui a existé il y a de très nombreuses années. La comparaison gène à gène entre deux organismes permet donc de mesurer le temps écoulé depuis la divergence à partir de l'ancêtre commun.

Carl Woese s'est rendu compte que l'ARN ribosomique (ou ARNr, une des molécules contenues dans la cellule) des organismes qu'il étudiait permettait de mettre en évidence l'existence de deux groupes clairement séparés : les bactéries et les archéobactéries. Plus précisément, les ARNr des archées sont en fait aussi différents des ARNr des bactéries que de ceux des eucaryotes. Woese en a conclu qu'il ne fallait plus uniquement séparer en deux grands groupes le monde du vivant, en fonction de la présence ou de l'absence d'un noyau, mais plutôt en trois domaines primitifs : les bactéries, les archées et les eucaryotes.

Aujourd'hui (2008), de nombreuses études ont confirmé le caractère monophylétique de ce groupe. Ces microorganismes ressemblent par leur forme aux bactéries, mais d'un point de vue moléculaire, si certains de leurs traits les rapprochent des bactéries, d'autres les rapprochent plutôt des eucaryotes. Il n'est donc pas possible de voir les archées comme étant des ancêtres des bactéries.

Le classement des archées, et des procaryotes en général, est à la fois en évolution rapide et un domaine litigieux. Sur la base de critères uniquement métaboliques, les archées ont été divisées en quatre grands groupes selon qu'elles sont méthanogènes, halophiles, thermophiles ou sulfo-dépendantes.

Les systèmes de classifications actuels visent à organiser les archées en groupes d'organismes qui partagent des caractéristiques structurelles et des ancêtres communs. Ces classifications s'appuient fortement sur l'usage de la séquence des gènes de l'ARN ribosomique pour révéler les relations entre les organismes (phylogénétique moléculaire). La plupart des archées cultivables sont membres de deux principaux embranchements : "Euryarchaeota" et "Crenarchaeota". D'autres groupes ont été provisoirement créés. Par exemple, les espèces propres "Nanoarchaeum equitans", qui ont été découvertes en 2003, ont été classées dans un nouveau phylum : "Nanoarchaeota". Un nouveau phylum, "Korarchaeota", a également été proposé ; il contient un petit groupe d'espèces thermophiles inhabituelles qui partagent les caractéristiques des deux principaux embranchements, mais qui sont plus étroitement liées aux "Crenarchaeota". Récemment mises en évidence, d'autres espèces d'archées, tels que les "" (ARMAN), qui ont été découvertes en 2006, sont liées seulement de loin aux autres groupes antérieurement connus.
Le superphylum "TACK" a été proposé en 2011, pour regrouper les "Thaumarchaeota", "Aigarchaeota", "Crenarchaeota" et "Korarchaeota"

L'archée "Loki", identifiée en 2015 par son génome qualifié de "Candidatus" en nomenclature bactérienne, appartiendrait, du point de vue phylogénétique, à l'embranchement le plus proche des eucaryotes.
De nouvelles lignées d'archées, apparentées à "Loki", ont été identifiées dans les sédiments aquatiques par analyse métagénomique. Les archées "Odin", "Thor" et "Heimdall", formeraient, avec "Loki", le super-embranchement "Candidatus" "Asgard", proposé en 2017 d'après le nom du royaume des dieux de la mythologie nordique.

Bien que les fossiles connus de cellules procaryotes aient été datés de près de 3,5 milliards d'années, la plupart des procaryotes n'ont pas de morphologies distinctives et les formes des fossiles ne peuvent pas être utilisées pour les identifier comme étant des archées. Par contre, les fossiles chimiques, sous la forme des lipides caractéristiques des archées, donnent plus d'informations, car ces composés n’existent pas dans d'autres groupes d'organismes. Certaines publications ont suggéré que des lipides fossiles provenant de procaryotes ou d’eucaryotes étaient présents dans les schistes datant de 2,7 milliards d'années. Depuis, ces données ont toutefois été sujettes à question. Ces lipides ont également été détectés dans des roches datant du Précambrien. Les plus anciennes traces connues de ces lipides isopréniques proviennent des roches de la formation d'Isua à l'ouest du Groenland, qui comprennent des sédiments formés il y a  d'années et qui sont les plus anciens sur Terre.

Une "fossilisation expérimentale" est partie du principe que les premiers fossiles(> 3 Ga) se sont formés par silicification, c'est-à-dire via la précipitation de silice sur des structures cellulaires). On a fossilisé en laboratoire des souches différentes d'Archées ("Methanocaldococcus jannaschii "et" Pyrococcus abyssi") et de Bactéries ("Chloroflexus aurantiacus "et" Geobacillus" sp.) jugées proches des micro-organismes (thermophiles, anaérobies et autotrophes) qui ont colonisé la Terre primitive (et proches d'organismes qui auraient éventuellement pu avoir vécu sur la Planète Mars). Leur observation en microscopie électronique (MEB, MET, Cryo-MEB) a donné des indications morphologique utiles pour le repérage de vrais fossiles anciens (à ne pas confondre avec des structures prébiotiques de type sphères submicrométriques, tubules et éléments filamenteux ou d'apparence fibreuses possiblement issus d'une simple chimie organique) ; de même pour des analyses chimiques (GC, GC-MS, HPLC) ont apporté des données sur la dégradation/préservation de la matière organique durant ce processus de fossilisation par silicification. Ce travail a confirmé que certains micro-organismes qui ne se silicifient pas : ainsi l'archée "M. jannaschii" s'est rapidement lysée alors que "P. abyssi, Geobacillus sp." et "C. aurantiacus" se silicifiaient mais avec une intensité propre à chaque espèce. Certains de ces micro-organismes lors de la silicification tentent d'y survivre en produisant des EPS ou via un mécanisme de répulsion de la silice. Les fossiles déjà découverts ne sont donc pas nécessairement représentatif des espèces réellement présentes à l'époque (ni de leur nombre ou dominance).

Woese propose que les bactéries, les archées et les eucaryotes représentent chacune une lignée séparée qui aurait divergé à partir d’une colonie d'organismes ancestrale. D’autres biologistes, comme Gupta ou Cavalier-Smith, cependant, ont fait valoir que les archées et les eucaryotes proviennent d'un groupe de bactéries. Il est possible que le dernier ancêtre commun des bactéries et des archées soit un organisme thermophile, ce qui soulève la possibilité que la vie soit apparue dans des conditions de températures élevées. Cette hypothèse n’est toutefois pas approuvée par l’ensemble de la communauté scientifique. Par ailleurs, étant donné que les archées et les bactéries ne sont pas plus liées entre elles qu'elles ne le sont aux cellules eucaryotes, cela conduit à l'argument selon lequel le terme procaryote n'a pas de véritable signification évolutive et devrait être entièrement rejeté.

La relation entre les archées et les eucaryotes reste un problème important. En plus des similitudes dans la structure cellulaire et les mécanismes biochimiques qui sont discutées ci-après, de nombreux arbres phylogénétiques groupent les archées et les eucaryotes ensemble. Certaines des premières analyses ont même suggéré que la relation entre les eucaryotes et les archées de l’embranchement "Euryarchaeota" est plus proche que la relation entre les embranchements "Euryarchaeota" et "Crenarchaeota". Toutefois, il est maintenant considéré comme plus probable que l'ancêtre des eucaryotes a divergé tôt à partir de l’ancêtre commun avec les archées. La découverte de gènes provenant d’archées dans le génome de certaines bactéries, telles que "Thermotoga maritima", rend les relations entre organismes encore plus difficiles à déterminer, étant donné que le transfert horizontal de gènes a eu lieu. Les gènes archéens dans les génomes eucaryotes pourraient également provenir de transfert horizontal.

Certains auteurs suggèrent une théorie totalement différente, et non basée sur les axiomes courants d'ancêtre commun d'une lignée et de différenciation arborescente, la théorie endosymbiotique. Selon celle-ci, les eucaryotes se sont développés à partir d’une fusion entre des bactéries et des archées, fusion elle-même découlant de l'évolution d'une relation symbiotique. Cette théorie est aujourd'hui largement acceptée en raison de la variété de faits connus qui la soutiennent.

Les archées ont généralement un seul chromosome circulaire. Le plus grand génome archéen séquencé à ce jour est celui de "Methanosarcina acetivorans" avec paires de bases alors que le génome de "Nanoarchaeum equitans", le plus petit séquencé à ce jour fait un dixième de cette taille avec seulement . Il est estimé que le génome de "Nanoarchaeum equitans" comporte codant des protéines. Les éléments extrachromosomiques, appelés plasmides sont également présents chez les archées. Ces plasmides peuvent être transférés entre les cellules par contact physique, dans un processus qui pourrait être similaire à la conjugaison bactérienne.

La reproduction des archées a lieu de manière asexuée par division binaire, par fission multiple ou par fragmentation. La méiose ne se produit pas, tous les descendants ont le même matériel génétique. Après la réplication de l’ADN les chromosomes sont séparés et la cellule se divise. Les détails du cycle cellulaire des archées ont fait l'objet de quelques études dans le genre "Sulfolobus". Ce cycle a des caractères qui sont similaires à la fois des systèmes eucaryotes et bactériens. Selon les espèces d’archées, les chromosomes sont répliqués à partir de un ou plusieurs points de départ (origines de réplication) à l'aide d'ADN polymérases qui ressemblent aux enzymes équivalentes des eucaryotes. Toutefois, les protéines de la division cellulaire, tels que la protéine FtsZ, qui forme un anneau contractant autour de la cellule, et les composants de la cloison naissante dans le cœur de la cellule, sont similaires à leurs équivalents bactériens.

S’il existe des spores chez les bactéries et les eucaryotes, elles n’ont jamais été mises en évidence dans toutes les archées connues. Certaines espèces de "Haloarchaea" peuvent subir des modifications phénotypiques et croître avec différents types de cellules, incluant des parois épaisses. Ces structures qui sont résistantes aux permettent aux archées de survivre dans l'eau à de faibles concentrations en sel, mais ce ne sont pas des structures de reproduction et elles ne peuvent aider à la dispersion dans de nouveaux habitats.

Les archées sont très diverses, aussi bien d'un point de vue morphologique que physiologique. Ce sont des êtres unicellulaires avec une taille variant entre , mais certains se développent pour former des filaments ou des agrégats (filaments jusqu'à ). Elles peuvent être sphériques (coques), spirales, en forme de bâtonnet (bacilles), rectangulaires…

Elles font preuve d'une grande diversité de modes de reproduction, par fission binaire, bourgeonnement ou fragmentation.

D'un point de vue nutritionnel, elles se répartissent en de très nombreux groupes, depuis les chimiolithoautotrophes (tirant leur énergie de gradients chimiques d'origine non biologique) aux organotrophes.

D'un point de vue physiologique, elles peuvent être aérobies, anaérobies facultatives ou strictement anaérobies.

Les archées existent dans une large diversité d'habitats et sont une composante importante des écosystèmes de la planète. Elles peuvent contribuer jusqu'à 20 % de la biomasse totale sur la Terre. De nombreuses archées sont extrêmophiles, et les milieux extrêmes étaient initialement considérés comme leur niche écologique. En effet, certaines archées survivent à des températures élevées, souvent supérieures à , que l'on rencontre dans les geysers, les fumeurs noirs et des puits de pétrole. D'autres se trouvent dans des habitats très froids et d'autres en milieu très salé, acide ou dans l'eau alcaline. Toutefois, d'autres espèces d’archées sont mésophiles et poussent dans des conditions beaucoup plus douces, dans les marais, les eaux usées, les océans et les sols.

Les archées extrêmophiles sont membres des quatre principaux groupes physiologiques. Ce sont les halophiles, thermophiles, alcalophiles et acidophiles. Ces groupes n’ont pas de lien avec leur embranchement dans la classification phylogénétique. Néanmoins, ils sont un point de départ utile pour la classification.

Les halophiles, par exemple le genre "Halobacterium", vivent dans des environnements salins, tels que les lacs salés (Grand Lac Salé de l’Utah), le littoral marin, les marais salants, la mer Morte, avec des concentrations en sel jusqu'à 25 %. Les membres de l'ordre des "Halobacteriales" ("Haloferax", "Halobacterium", "Halococcus", "Halorubrum", "Natrinema", "Natronococcus"…) sont des exemples d’archées halophiles. Elles ont souvent une pigmentation rouge à jaune à cause des caroténoïdes et sont responsables de la coloration de certains lacs (Lac Magadi au Kenya par exemple). Les thermophiles se développent mieux à des températures supérieures à , dans des lieux tels que les sources d'eau chaude ; les archées hyperthermophiles sont définies comme celles qui se développent au mieux à une température supérieure à . "Pyrococcus", "Methanopyrus", "Thermococcus", "Sulfolobus", "Pyrodictium" sont des exemples d’archées hyperthermophiles. "Pyrobaculum" provient de réservoirs profonds de pétrole chaud. "Pyrolobus fumarii" est capable de se multiplier jusqu'à . Une étude récente a montré que la de "Methanopyrus kandleri" pousse à , ce qui est la température la plus élevée enregistrée à laquelle un organisme est encore capable de se développer. D’autres archées peuvent croître dans des conditions très acides ou alcalines. Par exemple, l'une des archées acidophiles les plus extrêmes est "Picrophilus torridus", qui croît à un pH de 0, ce qui équivaut à d'acide sulfurique.

Des études plus récentes ont montré que les archées existent non seulement dans les environnements mésophile et thermophile, mais également à basse température, parfois en grand nombre. Ainsi, les archées sont communes dans les environnements océaniques froids tels que les mers polaires. Les archées sont en fait présentes en grand nombre dans tous les océans du monde dans la communauté planctonique (dans le cadre du picoplancton). Bien que ces archées puissent représenter jusqu'à 40 % de la biomasse microbienne, presque aucune de ces espèces n’a été isolée et étudiée en culture pure. Par conséquent, notre compréhension du rôle des archées dans l'écologie des océans est rudimentaire, de sorte que leur influence sur les cycles biogéochimiques mondiaux reste largement inexplorée. Certaines "Crenarchaeota" marines sont capables de nitrification, suggérant que ces organismes jouent un rôle important dans le cycle de l'azote océanique, bien qu’elles puissent également utiliser d'autres sources énergétiques. Un grand nombre d’archées sont également présentes dans les sédiments qui recouvrent le fond de la mer et constitueraient la majorité des cellules vivantes à des profondeurs de plus d'un mètre dans ces sédiments.
Les archées méthanogènes (productrices de méthane) des marais sont responsables des gaz des marais (Poitevin par exemple). Beaucoup d’archées méthanogènes sont présentes dans le tube digestif des ruminants ("Methanomicrobium", "Methanosarcina"), des termites ou des humains. Des études portant sur la faune nombrilienne (les micro-organismes vivant dans le nombril humain) ont démontré la présence d'archées à cet endroit.

Jusqu'à aujourd'hui, il n'y a pas de démonstration claire qu'il existe des archées pathogènes, bien que des relations aient été proposées entre la présence d'archées méthanogènes et de maladies parodontales.

Bien qu’un grand nombre d’archées ne soient aujourd’hui pas cultivables en laboratoire, de nombreuses espèces peuvent être cultivées en utilisant des milieux de culture adaptés et en reproduisant au mieux les conditions environnementales de leurs habitats naturels. Les effets des archées présentes dans le nombril humain n'ont pas encore été étudiés.

Les archées sont semblables aux bactéries par beaucoup d’aspects de leur structure cellulaire et de leur métabolisme. Cependant, les mécanismes et les protéines impliqués dans les processus de réplication, de transcription et de traduction présentent des traits similaires à ceux rencontrés chez les eucaryotes. Les particularités des archées par rapport aux deux autres domaines du vivant (bactéries et eucaryotes) sont les suivantes :



</doc>
<doc id="324" url="https://fr.wikipedia.org/wiki?curid=324" title="AND">
AND

AND est un code, qui signifie :

And est une abréviation, qui signifie :

And peut désigner :


</doc>
<doc id="325" url="https://fr.wikipedia.org/wiki?curid=325" title="Ahmôsis II">
Ahmôsis II

Ahmôsis (ou Ahmès , Iâhmes ou encore Amasis d'après Manéthon) est un pharaon de la de la Basse époque égyptienne, régnant de -571 à -526.

Ahmôsis, général des mercenaires libyens (berbère), et d’origine libyenne lui-même, s'est couvert de gloire dans l'expédition contre les Kouchites organisée par Psammétique. Après l'expédition désastreuse que son prédécesseur et chef Apriès envoya à Cyrène pour limiter l'expansion grecque en Cyrénaïque, Amasis est envoyé par le pharaon pour calmer la foule ; mais celle-ci, au lieu de s'apaiser, convainc Ahmôsis de prendre le pouvoir et de chasser Apriès, ce qu'il fait en -570.
Parallèlement, c'est à cette époque que Nabuchodonosor, roi de Babylone, menaçait tout le Proche-Orient au cours de nombreuses campagnes qui l'opposaient à Apriès ; ayant déjà attaqué en vain l'Égypte par deux fois sous le règne de ce dernier (en -601 et -582), il reçoit le pharaon déchu à sa cour et le place à la tête d'une puissante armée pour réessayer de conquérir l'Égypte. Mais à la bataille finale, en -567, Ahmôsis écrase Apriès et celui-ci est lui-même tué.

Babylone ayant cependant conquis toute la Judée (de là date l'exil des Hébreux à Babylone), Amasis mène alors une politique étrangère radicalement opposée au roi babylonien. À la mort de Nabuchodonosor , il mène une campagne au Proche-Orient, et va même envahir Chypre, qu'Apriès avait attaquée la dernière année de son règne (-570) pour se replier en cas d'échec au Liban. C'est donc la première et seule fois avant les Ptolémées que Chypre appartient au pharaon égyptien.

Ahmôsis est alors le maître incontesté de l'Égypte, d'Éléphantine jusqu'au delta, mais avec une zone d'influence bien plus large, de Napata en Nubie - ce qu'il hérite des campagnes militaires de son prédécesseur Psammétique -, jusqu'à Byblos au Proche-Orient, sans oublier Chypre, du moins jusqu'en -545 : la renaissance saïte est alors à son apogée, et Ahmôsis a donc réussi à élever l'Égypte presque au niveau de ce qu'elle était au Nouvel Empire, en étant parti de peu de chose et dans un contexte défavorable.

Son long règne est propice à une intense activité architecturale. Dans le delta du Nil, outre à Saïs et son grand temple de la déesse Neith, dont Ahmôsis se déclare le fils dans sa titulature, il fait bâtir un temple à Athribis et accorde à Naucratis un statut particulier lui autorisant à fonder et à construire des temples. Il intervient également à Memphis et procède à l'enterrement d'un Apis en l'an 23 de son règne au Sérapéum de Saqqarah. Il fait reconstruire le sanctuaire d'Osiris en Abydos et édifie une chapelle dans l'enceinte d'Amon-Rê de Karnak conjointement avec sa fille Nitocris, qu'il fait adopter par Ânkhnesnéferibrê comme divine Adoratrice d'Amon. Au sud de la première cataracte des traces de son intervention sur l'île de Philæ suggèrent que dès l'époque saïte ce lieu sacré avait déjà reçu des monuments dédiés à la grande déesse Isis. Il est également réputé avoir fondé ou en tout cas agrandi le temple oraculaire de l'Amon de l'oasis de Siwa, sanctuaire dont la célébrité ira grandissante par la suite.

Ahmôsis entretient de bons rapports avec les Grecs. Allié à Cyrène, à Crésus de Lydie, à Polycrate de Samos, il envoie des offrandes à Delphes et finance la reconstruction du temple d'Apollon détruit par un incendie en -548, noue de nombreux contacts avec les cités grecques et accueille de nouveaux contingents ioniens et cariens. Ayant obligé Chypre à se soumettre à l’Égypte, il dispose aussi d'une flotte commerciale considérable. Son action philhellène ne se limite pas a des actions militaires ou commerciale car il est réputé avoir invité à la cour de grands penseurs, philosophes ou mathématiciens grecs tels Thalès de Milet et Pythagore.

S'opposant à l'hégémonie perse, il va jusqu'à s'allier à son mortel ennemi, Babylone, pour les contrer. Cette alliance est officialisée dans un traité entre Babylone, Pharaon et Crésus, roi de Lydie. Malgré ses efforts et son réseau d’alliances, même avec l’ancien rival babylonien, il ne peut contenir l’expansion perse et peu à peu tous ses appuis disparaissent, à commencer par Crésus, battu par Cyrus, roi des Perses, et finalement Babylone.

Pour alimenter cette subtile politique d'alliance, Amasis fait lever des impôts, notamment en prélevant une part des revenus du clergé ce qui lui attire une certaine animosité et la défiance d'une partie importante de la société égyptienne.

Quelques mois avant sa mort a lieu une bataille perdue par l'Égypte contre les Perses en -526, et un an plus tard est porté le coup fatal et final essuyé par son successeur.

Personnage haut en couleurs d'origine plébéienne il fut un souverain novateur et réformateur. Il conçoit un grand nombre de lois régissant le droit privé auxquelles on continua à se référer des siècles plus tard.

Psammétique lui succède de -526 à -525.

On sait par Hérodote que la tombe d'Amasis était située dans l'enceinte du grand temple de Neith à Saïs, où l'auteur grec l'aurait vue. Elle aurait été violée par les Perses à la suite de la victoire de Cambyse sur le fils d'Ahmôsis , Psammétique, victoire qui ouvre la première occupation du pays par l'empire achéménide.

De son viatique funéraire, seuls quelques débris d'oushebtis à son nom sont apparus sur le marché des antiquités ou ont été trouvés lors des fouilles sporadiques qui ont eu lieu sur le site de Saïs. L'une de ces statuettes funéraires fragmentaires est exposée au musée Petrie à Londres.

Le sarcophage du roi n'a jamais été retrouvé jusqu'à ce jour.




</doc>
<doc id="326" url="https://fr.wikipedia.org/wiki?curid=326" title="Ammout">
Ammout

Dans la mythologie égyptienne, Âmmout, , est la déesse qui, lors de la pesée du cœur, dévorait les âmes des humains jugés indignes de continuer à vivre dans l'au-delà.

Elle est représentée avec un corps d’hippopotame, une tête de crocodile et des pattes avant de lion. Après le règne d'Amenhotep IV (ou Akhénaton, 1353/52-1338) elle figura sur presque toutes les scènes peintes représentant le jugement du mort par le tribunal d'Osiris.

On la trouve attendant aux côtés de Thot et d'Osiris, dans la "salle du jugement des deux vérités", le verdict de la cérémonie de la "pesée du cœur", où le cœur du défunt est déposé dans une balance dont le contre-poids est la plume de la vérité (symbole de Maât). Si le cœur du défunt est plus lourd que la plume de Maât, c'est qu'il est chargé de trop de péchés, et Âmmout est chargée de dévorer son cœur, empêchant ainsi l'âme du coupable de retrouver son corps pour ressusciter dans le monde des morts.

Elle est associée au démon Babaï.


</doc>
<doc id="331" url="https://fr.wikipedia.org/wiki?curid=331" title="Aha">
Aha

Aha est un génie bénéfique de la mythologie égyptienne.

Ancêtre de Bès, il est représenté sous les traits d'un gnome au visage rond, ceint d'une crinière, aux oreilles de félin, aux membres longs et à la large queue.

Influent jusqu'au Moyen Empire, il est un génie protecteur des femmes enceintes et des enfants.


</doc>
<doc id="332" url="https://fr.wikipedia.org/wiki?curid=332" title="Amset">
Amset

Amset (ou Imseti ou Imsti ou Imset ou Mesti ou Mesta) est un génie anthropomorphe de la mythologie égyptienne.

C'est la divinité protectrice du foie des morts. Elle est représentée sous la forme d’un homme momifié. Son lieu de culte est associé à la ville de Bouto dans le delta du Nil. Il est l’un des quatre génies funéraires appelés « Les fils d'Horus » qui avaient pour mission de garder les viscères du corps du défunt.

À partir de la fin de la , les bouchons des vases canopes sont modelés à l’image des divinités qui les protègent. Le vase canope qui renferme le foie protégé par Amset, est coiffé d'un couvercle qui représente une tête humaine. Pour que le pouvoir s’accomplisse et qu’il protège les organes momifiés, ce génie doit être associé à une déesse et à un point cardinal. Pour Amset c’est le Sud et la déesse Isis.

"Les quatre fils d'Horus représentés sur les vases canopes" :



</doc>
<doc id="333" url="https://fr.wikipedia.org/wiki?curid=333" title="Aldous Huxley">
Aldous Huxley

Aldous Leonard Huxley, né le à Godalming (Royaume-Uni) et mort le à Los Angeles (États-Unis), est un écrivain, romancier et philosophe britannique, membre éminent de la famille Huxley. Il est diplômé du Balliol College de l'Université d'Oxford avec une mention très bien en littérature anglaise.

Auteur de près de cinquante ouvrages , il est surtout connu pour ses romans, dont "Le Meilleur des mondes" roman d’anticipation dystopique ; pour des ouvrages non romanesques, comme "Les Portes de la perception" qui retrace les expériences vécues lors de la prise de drogue psychédélique ; et pour un large éventail d'essais. Au début de sa carrière, Huxley a dirigé le magazine Oxford Poetry et publié des nouvelles et des poésies.

Au milieu de sa carrière et plus tard, il a publié des récits de voyage et des scénarios cinématographiques. Il a passé la dernière partie de sa vie aux États-Unis, vivant à Los Angeles de 1937 jusqu'à sa mort. En 1962, un an avant sa mort, il est élu Compagnon de littérature par la Royal Society of Literature.

Huxley était humaniste, pacifiste et satiriste. Il s'est également intéressé à des sujets spirituels tels que la parapsychologie et le mysticisme philosophique, en particulier l'universalisme. Vers la fin de sa vie, Huxley fut largement reconnu comme l'un des intellectuels prééminents de son temps. Il a été nominé sept fois pour le Prix Nobel de littérature.
Aldous Huxley est né le à Godalming, dans le Surrey (Royaume-Uni). Il est le fils de l'écrivain et de sa première épouse, Julia Arnold. Son grand-père, Thomas Henry Huxley, est un des plus importants naturalistes du , surnommé le « Bouledogue de Darwin ». Son frère Julian Huxley est un biologiste connu pour ses théories sur l'évolution. La famille de sa mère, quant à elle, est plutôt littéraire.

Huxley est un enfant fragile, mais fin d'esprit et doué intellectuellement. Son père, en plus d'être écrivain, exerce le métier d'herboriste, et Aldous commence à s'instruire dans le laboratoire botanique de son père, avant d'entrer à l'école Hillside, dont sa mère fut directrice jusqu'à ce qu'elle tombe gravement malade. À l'âge de neuf ans, il entre dans un internat. Dès lors, il est préparé à défendre ses idées.

Sa mère, Julia, meurt en 1908, alors qu'Aldous n'a que quatorze ans. Le même mois, sa sœur Roberta trouve la mort dans un accident dont les circonstances n'ont pas été relatées. Trois ans plus tard, Aldous contracte une maladie (keratitis punctata) qui endommage gravement sa vision pour deux ou trois ans. Son grand frère Trev se suicide en 1914. Quasiment aveugle, Aldous est déclaré inapte au service lors de la Première Guerre mondiale. Une fois rétabli (notamment grâce à la méthode Bates à laquelle il consacrera plus tard son ouvrage "L'Art de voir"), il étudie la littérature anglaise au Balliol College d'Oxford.

Il porte un intérêt grandissant à la littérature. Cet intérêt est avant tout d'ordre intellectuel. Ce n'est que bien plus tard () qu'il prend conscience de l'importance des sentiments dans son expression philosophique et littéraire.

Alors qu'il poursuit son éducation au Balliol College, Huxley n'est plus entretenu financièrement par son père et doit gagner sa vie. Il donne des cours de français à Eton College, où étudient Eric Blair (plus tard connu sous le nom du célèbre George Orwell) et Steven Runciman. C'est un professeur incompétent, incapable de discipline, mais il impressionne par son langage. Pendant une courte période en 1918, il est employé à l'intendance du ministère de l'Air, mais ne désire pas faire carrière dans l'administration (ni dans les affaires). Son besoin d'argent le conduit à mettre en application ses talents littéraires.

Il termine son premier roman (impublié) à l'âge de dix-sept ans et se tourne de façon décisive vers l'écriture à l'âge de vingt ans. Il publie alors des poèmes. Journaliste, critique musical et critique d'art, il voyage et fréquente l'intelligentsia européenne de l'époque. Musicien, ami du compositeur russe Igor Stravinsky, il rencontre aussi les surréalistes à Paris. Il écrira de nombreux essais littéraires sur ces thèmes. Profondément préoccupé par les bouleversements que connaît la civilisation occidentale, il écrit pendant les années 1930 de grands romans, sur les graves menaces que fait peser le mariage du pouvoir, du progrès technique et des dérives de la psychologie telles le béhaviorisme ("Le Meilleur des mondes"), contre la guerre et le nationalisme ("La Paix des profondeurs"). Adepte, comme de nombreux intellectuels et artistes anglo-saxons, de la méthode mise au point par Frederick Matthias Alexander, il fait apparaître celui-ci dans "La Paix des profondeurs".

Déjà reconnu comme satiriste et chroniqueur pendant la Première Guerre mondiale, Huxley passe la majeure partie de son temps à Garsington Manor, propriété de Lady Ottoline Morrell où se réunissent les membres du groupe de Bloomsbury tels que Bertrand Russell ou Alfred North Whitehead. Plus tard, dans "Jaune de Crome" (1921), il caricaturera la manière de vivre à Garsington. En 1919, il y fait la connaissance de Maria Nys, une réfugiée belge. Cette même année, John Middleton Murry, le second mari de la romancière Katherine Mansfield et proche ami de D.H. Lawrence, lui propose de rejoindre l'équipe rédactionnelle du magazine "Athenaeum" : Huxley accepte immédiatement cette offre et épouse rapidement Maria Nys. Ils auront ensemble un enfant, Matthew, qui deviendra épidémiologiste. Au début des années 1920, le couple part vivre avec leur jeune fils en Italie où Huxley rendra de fréquentes visites à son ami D. H. Lawrence. Après la mort de ce dernier, survenue en 1930, Huxley publiera sa correspondance (1932).

En 1926, il écrit un roman à fort succès "Contrepoint" (publié en 1928), où il donne une vision ironique de la "society". Certains de ses personnages se rapprochent des mondains de Balzac ou d'André Gide. Selon André Billy, .

En 1937, Huxley s'installe à Hollywood en Californie avec sa femme et son ami . Heard initie Huxley à la philosophie védanta et à la méditation. Il devient alors végétarien et commence à pratiquer le yoga. Dans son livre "La Fin et les Moyens" (1937), Huxley affirme que dans les civilisations modernes la plupart des individus s'accordent dans le même désir d'un monde de liberté, de paix et de justice, d'amour fraternel, mais ne sont pas capables de s'accorder sur la manière d’y parvenir. Ce livre enquête ensuite sur les raisons de la confusion et du désaccord, et sur les moyens d'y remédier.

Pendant la plus grande partie de sa vie, sa vue reste très basse (malgré la guérison partielle qui lui avait permis d’étudier à Oxford). Vers 1939, il entend parler de la méthode Bates pour l’amélioration de la vision naturelle, et d’un professeur, Margaret Corbett, qui peut lui apprendre cette méthode. Il révèle que sa vue s’est radicalement rétablie grâce à cette méthode dans "L'Art de voir", publié en 1942 aux États-Unis (1943 au Royaume-Uni). Il y déclare que pour la première fois depuis 25 ans, il a pu lire sans lunettes et sans effort.

À cette période, il gagne très bien sa vie en écrivant des scénarios pour Hollywood. Cet argent lui permet d'aider des Juifs, des écrivains et des artistes fuyant l'Allemagne nazie. Il écrit, notamment, l’adaptation à l’écran d"Orgueil et préjugés" (1940) et de "Jane Eyre" (1944).

Après la Seconde Guerre mondiale, Huxley demande la citoyenneté américaine, qui lui est refusée parce qu’il refuse d’envisager de prendre les armes pour défendre les États-Unis.

Par la suite, ses écrits sont fortement influencés par le mysticisme et par ses expériences hallucinatoires avec la mescaline, que lui fait connaître le psychiatre Humphry Osmond en 1953. Les expériences psychédéliques de Huxley sont racontées dans les essais : "Les Portes de la perception" et "Le Ciel et l'Enfer", dont les titres s'inspirent directement de l’œuvre du poète visionnaire William Blake, "Le Mariage du Ciel et de l'Enfer".

Selon Dick Huemer, Huxley a participé au début des années 1940 à la première des cinq réunions préliminaires à l'élaboration du scénario d"'Alice au pays des merveilles ("1951) et n'est jamais revenu. Pour John Grant, malgré le personnage la Chenille (qui peut rappeler les expériences d'Huxley en matière d'hallucinogènes), sa participation au film est inexistante.

L’épouse d'Aldous Huxley, Maria, meurt d’un cancer du sein en 1955 ; en 1956 il se remarie avec , elle-même auteur, et qui écrira une biographie de son mari. En 1960, on diagnostique chez lui un cancer de la gorge. Durant les années suivantes, sa santé se détériore. Trente ans après sa contre-utopie "Le Meilleur des mondes" il écrit le roman utopique "Île", et donne des cours sur les « potentialités de l’être humain » à l’Institut Esalen. En 1959, Huxley, qui était resté citoyen britannique, refuse le titre de "Knight Bachelor" que lui offre le gouvernement Macmillan.

Huxley est régulièrement invité à s’exprimer dans de prestigieuses universités américaines. Huxley développe des idées similaires à celles que J.B. Priestley, un écrivain qui lui était contemporain, développe dans son livre "Les Magiciens". :

Dans un autre de ses discours, prononcé à l'université de Californie à Los Angeles le 20 mars 1962, Huxley expose en détail sa vision d'une société totalitaire et en profite pour comparer la vision de George Orwell dans "1984" avec la sienne, qu'il juge bien plus efficace et durable. Il note également que certaines des techniques de contrôle des populations imaginées trente ans plus tôt étaient dorénavant disponibles ou sur le point de le devenir.

Sur son lit de mort, incapable de parler à cause d'un cancer de la gorge avancé, il demanda par écrit à son épouse : « LSD, , i.m. », il n'avait pas pris de psychoactif depuis près de deux ans et il faut savoir que le LSD est le plus proche équivalent existant du remède-moksha (psychédélique utilisé par les protagonistes de son roman "Île"). Elle lui fit une première injection de puis une seconde plus tard il mourut paisiblement, le .

L'annonce de sa mort par les médias fut éclipsée par celle de John F. Kennedy, survenue le même jour, tout comme celle de l'écrivain irlandais C. S. Lewis.

Militant pacifiste dans l'entre-deux guerres, il est membre de la .

À la fin des années 1930, initie Huxley à la philosophie védanta et à la méditation. Il devient alors végétarien et commence à pratiquer le yoga. En 1938, Huxley se lie d'amitié avec Jiddu Krishnamurti, dont il admirait les enseignements. Il devient en même temps un « védantiste » dans le cercle de , et il initie Christopher Isherwood à ce même cercle. Huxley publiera ,en 1948, une anthologie des valeurs communes à certaines religions : "La Philosophie éternelle", dans laquelle il discute des doctrines des grands courants mystiques.

Les écrits d'Huxley à partir de 1945 sont fortement influencés par le mysticisme et par ses expériences hallucinatoires avec la mescaline, que lui fait connaître le psychiatre Humphry Osmond en 1953. Il a décrit ces années où il s'est soumis aux psychotropes comme un paradis habituellement arrosé de bourbon. Il a été un des premiers à faire l'expérience des drogues psychédéliques sur lui-même, dans une quête d’illumination, et il est connu pour avoir pris de LSD sur son lit de mort. Les expériences psychédéliques de Huxley sont racontées dans les essais : "Les Portes de la perception" et "Le Ciel et l'Enfer", dont les titres s'inspirent directement de l’œuvre du poète visionnaire William Blake, "Le Mariage du Ciel et l’Enfer". Le titre du premier livre inspira plus tard à Jim Morrison et à son groupe le nom de « "The Doors" ». Ses écrits sur les expériences psychédéliques devinrent des classiques parmi les premiers hippies. À partir de cette époque, il fréquente beaucoup la région de Big Sur avec d'autres écrivains progressistes.

Par ses expériences avec les drogues, Huxley ne cherchait pas seulement une exaltation indéterminée, vague, mystérieuse et individuelle, mais cherchait plutôt à atteindre ce qu'on appelle parfois le « haut mysticisme » ; lui préférait le terme de "philosophie éternelle", qu'il donna à l’un de ses livres sur ce sujet.

Pendant les années cinquante, l’intérêt de Huxley pour le domaine de la recherche psychologique ne cesse de croître. Pendant presque un an, au début des années cinquante, Huxley et le psychiatre Milton Erickson consacrent beaucoup de temps à préparer une étude commune sur les différents états de conscience. Leur projet prend fin lorsqu'un incendie de broussailles détruit la maison de Huxley à Los Angeles et leurs carnets respectifs pour cette étude.

Ses idées furent à la base du Mouvement du potentiel humain.

Les idées de Huxley sur les rôles spécifiques de la science et de la technologie dans la société (tels qu'il les a décrits dans "Île") sont parentes de celles de penseurs britanniques et américains du , tels que Lewis Mumford, Gerald Heard (et, sous certains aspects, Buckminster Fuller et E.F. Schumacher). En France, son roman "Brave New World", traduit en 1932, a fortement influencé les « personnalistes gascons » Bernard Charbonneau et Jacques Ellul dans leur analyse du phénomène technique et du conformisme social (pour Charbonneau, il est un « romancier complet qui saisit l'individu dans la réalité de son environnement social »). C'est aussi par l'entremise d'Huxley que Jacques Ellul a pu faire paraître son ouvrage "La technique ou l'enjeu du siècle" en 1954 aux États-Unis.

Ces idées trouvèrent un écho dans les générations suivantes chez des personnes comme Stewart Brand.

Grâce à G. Heard, Huxley rencontra Huston Smith, qui devint plus tard un spécialiste reconnu et prolifique des religions. Les deux amis initient Smith au Védanta et à la pratique de la méditation. Plus tard, alors que Huxley était professeur invité au Massachusetts Institute of Technology, il présenta Smith à Timothy Leary, ce qui amena des épiphanies que Smith présenta dans son dernier livre, "Purification des Portes de la Perception".

Parmi les penseurs humanistes, Huxley fut considéré comme un intellectuel pour les intellectuels. Bien que les contraintes financières l’aient souvent amené à produire des articles et des livres en abondance, sa pensée et ses meilleurs écrits lui valent une haute estime. Ses œuvres ont été régulièrement inscrites dans la liste d’étude des cours de philosophie britannique moderne dans les lycées et universités d’Amérique. Il fut l’un des penseurs du honorés dans "Leaders of Modern Thought" (« Les Grands penseurs modernes ») des éditions Scribner (un volume de biographie et de critique littéraire par P. Thody, "Aldous Huxley").


Les dates correspondent à la première publication en langue originale.








</doc>
<doc id="341" url="https://fr.wikipedia.org/wiki?curid=341" title="Amenhotep III">
Amenhotep III

Amenhotep (né vers -1411/ -1403, et mort à Malqata vers -1353/-1352), ou Aménophis en grec ; Amāna-Ḥātpa en égyptien ancien, qui signifie "Amon est satisfait", est le neuvième pharaon de la (période du Nouvel Empire). Manéthon l’appelle Aménophis. Il règne trente-huit ans et sept mois, mais certains égyptologues pensent à une corégence avec son fils Amenhotep à la fin de sa vie.

On situe son règne aux alentours de -1391 / -1390 à -1353 / -1352.
Amenhotep est le fils de Thoutmôsis et de Moutemouia, une concubine de son père.

Son règne est une période de prospérité et de splendeur artistique sans précédent, alors que l’Égypte atteint l’apogée de sa puissance artistique et internationale. Lorsqu’il meurt, son fils lui succède en tant que Amenhotep , mais change ultérieurement son propre nom royal pour Akhénaton.

Amenhotep compte parmi les plus grands bâtisseurs de l’Égypte ancienne.

Amenhotep est le fils de Thoutmôsis et de la reine Moutemouia, laquelle assume la régence lorsqu'il monte sur le trône à l'âge de dix / douze ans.

Sa grande épouse royale est Tiyi, fille de Youya (Prophète de Min) et Touya (ou Tyouyou). Il l’associe étroitement au pouvoir et à la fin de sa vie, alors qu'il est très malade, la reine va l'aider énormément dans la gestion de l'État. Il épouse aussi, en l’an de son règne, Giloukhepa (ou Gilu-Hepa), la fille de l’empereur du Mitanni Shuttarna. Il épouse ensuite la sœur du roi de Babylone, Tarhoundaradou, la fille du roi d’Arzawa, Tadukhipa (ou Taduhepa), la fille de Tushratta, nouveau roi du Mitanni autour de l’an 36 de son règne, une fille de Kadashman-Enlil, roi de Babylone, une fille du dirigeant d’Ammia (en Syrie moderne) et enfin ses deux filles Iset et Satamon.

Tiyi lui donne sept enfants : le futur Amenhotep, Satamon, Iset, Henouttaneb, Nebetâh, Baketaton et Thoutmôsis dont l’existence est incertaine. Certains égyptologues pensent que Smenkhkarê (futur pharaon) pourrait être un fils qu'Amenhotep aurait eu avec Satamon.
Ses filles apparaissent souvent sur des statues et reliefs durant le règne de leur père et sont aussi représentées sur des objets plus petits – à l’exception de Nebetâh. Nebetâh est attestée une seule fois dans les sources historiques, sur un groupe de statues colossales faites de calcaire de Medinet Habou. Cette immense sculpture, qui mesure sept mètres de haut, montre Amenhotep et Tiyi assis l’un à côté de l’autre « avec trois de leurs filles se tenant debout en face du trône – Henouttaneb, la plus large et la mieux préservée, au centre ; Nebetâh à droite ; et une autre, dont le nom a été détruit, à gauche. ».

Amenhotep élève deux de ses filles – Satamon et Iset – au titre de grande épouse royale durant la dernière décennie de son règne. Il était courant qu’un pharaon épouse des femmes royales de différentes générations afin de solidifier les chances de succession. La déesse Hathor elle-même était liée à Rê d'abord en tant que mère, puis comme femme et fille du Dieu. Ainsi, les mariages d’Amenhotep à deux de ses filles ne sont pas invraisemblables. Des preuves que Satamon avait déjà été promue à ce titre en l’an 30 de son règne sont fournies par une inscription sur un vase découvert au palais royal de Malqata.

Amenhotep mène l’Égypte à l’apogée de sa puissance. Ce n’est pas un guerrier. Durant son long règne, une seule expédition militaire est attestée en Nubie en l’an 5 de son règne, pour réprimer une révolte.

Amenhotep va préférer maintenir la supériorité de l'Égypte par la diplomatie et il va chercher à renforcer les relations avec le puissant Mitanni. Des accords commerciaux sont pris avec Chypre : un important quota de bois et de cuivre est fixé pour l'importation en Égypte, ce qui apporte à l'île une exemption de droits de douane.

Quand en l'an 2 de son règne (-1406), il prend pour épouse Tiyi, qui devient la grande épouse royale, il commande une série de grands scarabées dont le verso relate l'événement et que l'on retrouvera disséminés dans tout l'empire.

Le scribe du roi, Amenhotep fils de Hapou, favori, directeur de tous les travaux du roi (architecte royal) est un « Premier ministre » de fait. Mérymosé devient vice-roi de Koush après Amenhotep.

Durant son règne, Bakenkhonsou est grand prêtre d'Amon.

La fin de son règne est marqué par une dégradation de la situation internationale. Les princes mitanniens de Syrie, pourtant alliés de l'Égypte, sont attaqués par une nouvelle puissance venue du Hatti, en la personne de l'empereur des Hittites, Soupilouliouma (-1382 / -1342). Amenhotep n'intervient pas pour venir à leur secours, malgré les appels des princes. L’Égypte, au contraire, signe un traité avec le Hatti. Le prince de Kadesh et le roi d'Amourrou (Liban) intriguent pour former une coalition de petits États : là encore, Amenhotep laisse faire. Ces négligences vont laisser à son fils un empire où le désordre s'est installé.

L’Égypte, grâce à l’or nubien, est la première puissance financière du monde. On assiste à un développement des grandes villes en Égypte, surtout celles qui sont résidences royales (Thèbes, Memphis).

Les Shardanes, peut-être originaires de Sardes, en Asie Mineure, sont employés comme mercenaires d’élite par Amenhotep .

Il entreprend de nombreux chantiers depuis la Nubie jusqu'au nord du pays. Il fait agrandir considérablement le complexe de Karnak en y faisant construire le temple de Louxor par son architecte Amenhotep fils de Hapou (qui, à l'instar d'Imhotep, l'architecte de Djéser, sera divinisé à titre posthume). L'élégance des formes architecturales et des proportions culmine alors (colonnes florales fasciculées à Louxor notamment).

Il fait également construire un immense château de millions d'années en face de Thèbes, dont il ne reste aujourd'hui que les colosses dit « de Memnon ».

On note sous son règne un raffinement des formes de l'art officiel (statuaire, relief, peinture).

L'ouverture du pays se poursuit sous le règne d'Amenhotep et un syncrétisme religieux s'opère entre les dieux d'Égypte et ceux d'Asie. Le roi du Mitanni envoie à Amenhotep la statue miraculeuse de la déesse Ishtar de Ninive.

On considère parfois qu'Amenhotep est l'un des initiateurs de l'art amarnien. Il est l’introducteur de la religion d’Aton qui va être suivie par son fils.

Sa tombe est située dans la Vallée des singes, un ouadi secondaire de la Vallée des rois. Elle fut découverte en 1898, dès les premières explorations de la vallée. Le tombeau, vidé de ses trésors dès l'Antiquité, a beaucoup souffert des nombreuses visites et du temps. Des parties de fresques ont été également prélevées, défigurant les parois du tombeau. Peu d'objets ont échappé au pillage en dehors de quelques oushebti exposés dans différents musées du monde.


</doc>
<doc id="342" url="https://fr.wikipedia.org/wiki?curid=342" title="Aviation civile">
Aviation civile

L'aviation civile désigne tout ce qui est relatif à l'aviation non-militaire. Cela englobe ainsi le transport civil de passagers et de marchandises et en France les questions de . 

Le trafic augmente régulièrement (5,5 milliards de passagers en 2013) avec Atlanta comme premier aéroport (l'aéroport de Paris-Charles-de-Gaulle étant au mondial). Elle traite aussi de l'aviation d'affaires, du vol libre, de la voltige aérienne, des rallyes aériens, du parachutisme, de l'aéromodélisme et des autorisations concernant les drones ou des vols de ballons et d'ULM, de l'industrie aérospatiale et aussi dans certains pays de l'industrie spatiale 

Au niveau international, c'est l'Organisation de l'aviation civile internationale (OACI), dépendant de l'Organisation des Nations unies (ONU), qui est chargée de l'élaboration de normes internationales pour le transport aérien civil.

Le trafic aérien régulier mondial civil tend à former des alliances (3 alliances principales en 2013, avec la Star Alliance (créée en 1997) comme leader ont assuré 76 % des PKT en 2013 (contre 75 % en 2012). les deux autres grandes alliances sont SkyTeam (créé en 2000) et Oneworld (créée en 1998).

Le trafic continue à croitre : Selon la DGAC, "" (5,2 % en Europe). 
Les attentats du 11 septembre 2001 puis la crise du SRAS et la crise de 2008 ont à peine influencé la courbe globale de croissance sur 15 ans. L'aviation (fortement détaxée et/ou subventionnée) compte aussi pour une part croissante aux émissions de gaz à effet de serre dans le secteur des transports. 

Il a atteint en 2013 à 3,1 milliards de passagers. Pour le trafic mesuré en "passagers kilomètres transportés" (PKT), il a 2013 atteint de PKT (+ 5,5 % par rapport à 2012). Et durant les 9 premiers mois 2014 le trafic aérien mondial a encore progressé de 2,7 % . Une partie de ces passagers ne font que changer d'avion dans l'aéroport : en Europe (27 % environ du trafic mondial), le trafic international représentait en 2014 3 fois le trafic intérieur européen.

En France, comme aux États-Unis les fonctions d'opérateur de navigation et de régulation/surveillance sont assurées par la même entité (DGAC en France), ce qui est peu commun dans les pays développés ; selon la DGAC, ceci rend .

En Europe, l'aviation civile s'organise peu à peu autour de l'objectif du Ciel unique européen selon un calendrier fixé par le Parlement européen et le Conseil. 

Elle a généralement des fonctions de et le plus souvent d' clairement dissociées. 
Selon la DGAC (2014) .
En 2013, le Royaume-Uni a connu le trafic le plus élevé avec de passagers (+3,1 % par rapport à 2012), et devant pays ayant compté plus plus de de voyageurs : Allemagne (171 millions de
passagers, soit +1,2 % par rapport à 2012), Espagne (157 millions de passagers, soit -1,5 %), la France (138 millions de passagers soit +1,6 %), et l’Italie (115 millions, soit -0,7 %).

Ryanair est pour l'Europe la première compagnie en termes de nombre de passagers (82 millions en 2013), devant Air France-KLM (78 millions), Lufthansa (), et easyJet (). 
Si le bilan est fait en termes de PKT, c'est Air France-KLM qui domine ( de PKT) devant
Lufthansa() puis British Airways (), mais les compagnies low cost progressent en termes de bénéfices (résultats d’exploitation : d’euros pour Ryanair et 591 millions d’euros pour easyJet en 2013, alors qu'Air France/KLM était limité à d’euros).
La FAA gère toute l'aviation civile, y compris le contrôle de la navigation aérienne (qui en Allemagne, Italie, Espagne au Royaume-Uni a été confiée à un organisme indépendant. Hormis pour ce qui relève de la sécurité des personnels et des équipements de la FAA, la sûreté des transports aux États-Unis reste néanmoins assurée par une administration spécifique (Transportation Security Administration ; TSA, qui dépend du Department of Homeland Security ; Elle inclut une entité dénommée Organisation du trafic aérien (ATO), et c'est la seule entité de ce type au monde à disposer en 2014 d'une structure consacrée au transport commercial spatial.

L'aviation civile est gérée par le département des transports (Department for Transport ou DfT) avec une
“public corporation” : la "Civil Aviation Authority" (CAA) et la " National air traffic services (NATS)" qui est l'opérateur de navigation aérienne. 

L'aviation civile allemande est gérée par le ministère fédéral des transports (Department for Transport ou DfT) aussi chargé des Infrastructures numériques (Bundesministerium für Verkehr und digitale Infrastruktur -BMVI-), en lien avec une "autorité fédérale de l’aviation" (Luftfahrt Bundesamt - LBA) et la DFS (Deutsche Flug Sicherung Gmbh), société pour la sécurité aérienne allemande qui est l'opérateur pour la navigation aérienne (société de droit privé détenue entièrement par l’État et financée par des redevances), mais les Lands ont aussi quelques responsabilités dans l'aviation civile

L'aviation civile relève - sous l'autorité du Ministère des transports - de l’ENAC, qui a statut d' “entreprise publique non économique” (proche de l'établissement public français, et qui agit en lien avec l’ENAV (Ente nazionale per l’assistenza al volo), société par actions financée par les redevances, chargée de la navigation aérienne.

C'est la Dirección General de Aviación Civil (DGAC) qui joue ce rôle sous tutelle de la direction du Ministerio de Fomento (Ministère du développement) chargé de la direction et la planification de la politique aéronautique civile, avec (depuis 2008) une "Agencia estatal de seguridad aérea" (AESA) sous le regard d'une autorité de surveillance (Agencia Estatal de Seguridad Aérea, AESA). L'opérateur national est ici l’AENA
(Aeropuertos Españoles y Navegación Aérea) qui est le premier exploitant dans les principaux aéroports du pays ( espagnols et ). Dans le cadre de la libéralisation/privatisation des services, le gouvernement a confié le contrôle aérien à des entreprises privées pour aéroportuaires. 

En France, c'est la Direction générale de l'Aviation civile (DGAC, en 2014 ) qui sous l'Egide et la tutelle du MEDDE est l'organisme public garant de la sécurité du transport aérien français. ' , cette direction est aussi dont pour '. 

Cette DGAC comprend 
Elle représente la France dans l'Organisation de l'Aviation civile internationale (OACI), l'Agence européenne de la sécurité aérienne, Eurocontrol. 

En 2013, 2,79 millions de vols ont été contrôlés (-0,7 % par rapport à 2012), ce qui fait de la France, pays très survolé, "

Il existe un « "observatoire de l'aviation civile" » (OAC) qui publie annuellement un document d'analyse et de données statistiques, fournissant notamment des tendances d'évolution à court terme : 

En 2013, les aéroports ont accueilli de passagers (+ 2,1 % par rapport à 2012) dont à Paris ( de passagers, soit + 1,7 %) et avec de passagers dans les régions (+ 3,6 %) et de passagers en Outre-mer... avec des situations contrastée selon les aéroports, mais qui ont globalement bénéficié d'une légère baisse des prix du kérosène.

Le nombre d'emplois directs (compagnies aériennes, entreprises d’assistance en escale, maintenance, gestionnaires d’aéroport) était en France en 2013 de environ, auxquels il faut ajouter les emplois du secteur de la sûreté aéroportuaire ( au 31 décembre 2012) . L'industrie aéronautique employait en outre fin 2013).




</doc>
<doc id="344" url="https://fr.wikipedia.org/wiki?curid=344" title="Auteur-compositeur-interprète">
Auteur-compositeur-interprète

Un auteur-compositeur-interprète (ACI) est une personne qui rédige les paroles et compose la musique de chansons qu'elle interprète.

L'ordre peut varier d'un artiste à l'autre, ou d'une œuvre à l'autre pour un même artiste ; soit il compose avant d'écrire le texte, soit après. Il peut même créer en parallèle texte et musique, voire les arranger pour une orchestration.

L'auteur-compositeur se réserve le droit de corriger, d'améliorer la partition manuscrite dont il est le créateur et seul détenteur, ce qui authentifie son style, sa carrure et son autonomie. Ce droit ne peut être détourné par des musiciens, ce qui authentifie son unicité. Il s'agit d'un droit d'auteur qui lui confère des droits moraux (inaliénables) et des droits patrimoniaux (courant en France 70 années après la date de son décès et les années de guerre visées par la jurisprudence).

En l'occurrence, une musique orchestrée, enregistrée ou non, n'est qu'un reflet des multiples possibilités qu'a pu prendre ou ne pas prendre la voie dans laquelle le compositeur a versé. L'œuvre sera protégée si elle est originale, si elle reflète la personnalité, les goûts et la fantaisie de son auteur, et ce indépendamment de ses qualités artistiques ou esthétiques.



</doc>
<doc id="345" url="https://fr.wikipedia.org/wiki?curid=345" title="Aménophis IV (homonymie)">
Aménophis IV (homonymie)



</doc>
<doc id="346" url="https://fr.wikipedia.org/wiki?curid=346" title="Animisme">
Animisme

L’animisme, (du latin "animus", originairement « esprit », puis « âme ») est la croyance en un esprit, une force vitale, qui anime les êtres vivants, les objets mais aussi les éléments naturels, comme les pierres ou le vent, ainsi qu'en des génies protecteurs. 

Ces âmes ou ces esprits mystiques, manifestations de défunts ou de divinités animales, peuvent agir sur le monde tangible, de manière bénéfique ou non. Il convient donc de leur vouer un culte. Ainsi défini, comme « croyance à l'âme et à une vie future et, corrélativement, croyance à des divinités directrices et des esprits subordonnés », l'animisme peut caractériser des sociétés extrêmement diverses, situées sur tous les continents.

À moins d'être redéfini dans le champ de l'anthropologie, par exemple à la manière de Philippe Descola, ou limité à un processus psychique, par exemple dans la psychanalyse ou dans la conception piagétienne, l'objet « animisme » ne correspond à aucune réalité religieuse se revendiquant comme telle. Il n'est qu'un objet créé historiquement pour distinguer des croyances et des pratiques n'entrant pas dans le cadre des paradigmes des religions dites universalistes.

Le médecin allemand Georg Stahl est à l’origine ("Theoria medica vera", 1707) d’une théorie médicale appelée "animisme", opposée au mécanisme et au vitalisme ; pour résumer à l’extrême, il s’agissait d’expliquer que l’âme avait une influence directe sur la santé. Une seule et même âme est à la fois principe de vie et principe de pensée.

L'animisme (Stahl) s'oppose alors au mécanisme (Démocrite, Descartes, Cabanis, Le Dantec) et se différencie du vitalisme (Platon, Paracelse, Paul-Joseph Barthez, Félix Ravaisson, Bergson, H. Driesch). L'animiste ne se contente pas de subordonner la matière à la vie, mais, qui plus est, il soumet la vie à la pensée. Les philosophes d'inspirations vitalistes considèrent au contraire l'activité intellectuelle comme fondamentalement subordonnée à la « vie ».

Edward Burnett Tylor (1832 - 1917) est le premier sociologue à avoir établi une théorie sur l’animisme, dans "Primitive Culture" (1871). Il fonde son analyse sur le sentiment, pour lui général dans les sociétés qu’il qualifiait alors de « primitives », que l’âme était distincte du corps car, lors des rêves, le dormeur semble atteindre un monde différent de celui où se trouve son corps.

C’est cette expérience qui aurait fondé la notion d’« âme ».

Par analogie et extension, des âmes auraient ainsi été prêtées (attribuées) à l’ensemble des éléments de la nature. Pour Tylor, l’animisme représentait le premier stade de religiosité humaine, celui des sociétés les plus primitives, et il devait être suivi par le fétichisme, puis le polythéisme et enfin, par le monothéisme, qui caractérisait la religion de sa propre société. 

La théorie de Tylor sur l’animisme eut un énorme succès. Le terme fut ensuite beaucoup repris, discuté et critiqué.

Les anthropologues ont notamment reproché à Tylor sa perspective évolutionniste (comme si toutes les sociétés devaient évoluer de la même manière vers un même but), sa perspective psychologique (il est difficile d’expliquer une notion telle que l’âme par une simple référence à une expérience de dormeur – ou alors, cette notion devrait prendre un sens identique dans toutes les sociétés, ce qui n’est pas le cas), ainsi que le caractère imprécis du terme animisme (tous les éléments de la nature ne sont pas partout perçus comme ayant une âme, attribuer un esprit ou une âme à un élément n’est pas la même chose, etc.).

En dehors de quelques anthropologues qui reprennent ce terme dans leur analyse en lui donnant une signification précise (tel Philippe Descola), le terme d’animisme n’est plus employé que de manière très vague, pour finalement désigner toutes les religions qui ne sont pas universalistes (c’est-à-dire les religions de la conversion, telles le christianisme, l’islam ou qui ne sont pas des religions de grands pays-civilisations (les religions chinoises, indiennes, etc.). Il est alors pris comme synonyme de « religion traditionnelle » (un terme qui ne signifie rien, en soi), ou d’autres termes à l’usage tout aussi vague, tels que le chamanisme.
En réalité la difficulté de définir clairement ces termes et circonvenir leur périmètre respectif procède essentiellement de leur éloignement des modes de pensées des sociétés modernes, issus d'une représentation du monde radicalement différente, que Philippe Descola qualifie de naturaliste.

Introduit à la fin du par l'anthropologue britannique Edward Burnett Tylor pour désigner les religions des sociétés qu'il nomme « primitives » ("Primitive Culture", 1871), le concept a connu un indéniable succès jusque dans les premières décennies du , devenant « l'un des termes de référence majeurs de l'histoire de l'ethnologie religieuse ». Cette ambitieuse tentative d'explication globale des croyances religieuses – une « doctrine de l'âme » – a perdu une large part de sa validité aujourd'hui et les travaux contemporains s'en écartent, notamment ceux de l'anthropologue français Philippe Descola qui ne voit pas dans l'animisme une religion, mais plutôt « une manière de concevoir le monde, et de l'organiser ».

Le terme lui-même, souvent entaché de connotations colonialistes, du moins perçues comme péjoratives, est employé avec circonspection, parfois remplacé par des expressions telles que « croyances populaires », « croyances indigènes », « religions traditionnelles ».
Par défaut ou par commodité, il est désormais utilisé dans le langage courant ou dans les statistiques, comme un mot fourre-tout désignant généralement l'ensemble de ce qui, ne relevant pas des grandes religions théistes s'appuyant sur des textes sacrés (christianisme, islam, bouddhisme…), est transmis par des traditions orales.

Parmi les anthropologues contemporains, Philippe Descola, dans une vision globalisante voire universaliste, a redéfini l'animisme dans un ouvrage remarqué, "Par-delà nature et culture" (2005). Il se place pour cela dans la situation de l'Homme s'identifiant au monde suivant deux perspectives complémentaires : celle de son « intériorité » et celle de sa « physicalité » vis-à-vis des autres, humains et non humains.

L'animisme correspondrait à la perception d'une identité commune des intériorités des existants, humains et non humains, et à celle d'une identité distincte entre leurs physicalités.
L'anthropologue décrit les trois autres « ontologies » qui suivent la perception d'une fusion ou d'une rupture entre intériorité et physicalité, et qui se nomment totémisme, analogisme et naturalisme ; les quatre modes (identité/rupture) * {intériorité/physicalité}) réunis auraient une vocation universelle, tout en revêtant diverses formes de cohabitation ou de dominance suivant les cultures (qu'elles soient archaïques, traditionnelles ou modernes).

Ainsi, pour Philippe Descola, l'animisme se caractérise ainsi. a) Ontologie. L'animisme repose sur cette affirmation : ressemblance des intériorités et différence des physicalités entre humains et non-humains (animaux, végétaux, esprits, objets). Les animaux, les plantes ont la même âme, intériorité (émotions, conscience, désirs, mémoire, aptitude à communiquer...) que les humains, ils ne s'en distinguent que par leurs corps et donc aussi par leurs mœurs, l'éthogramme, le mode de comportement spécialisé (, 190). Il y a, comme dans le totémisme, classification par prototype, c'est-à-dire à partir du modèle le plus représentatif, qui est, dans l'animisme, l'humain (p. 333). "De même que l'animisme est anthropogénique parce qu'il emprunte aux humains le minimum indispensable pour que des non-humains puissent être traités comme des humains, le totémisme est cosmogénique car il fait procéder de groupes d'attributs cosmiques préexistants à la nature et à la culture tout ce qui est nécessaire pour que l'on ne puisse jamais démêler les parts respectives de ces deux hypostases dans la vie des collectifs" (p. 368-369). b) Géographie. L'animisme se rencontre "en Amazonie, dans l'aire arctique et circumpolaire ou dans les forêts de l'Asie du Sud-Est" (), chez les Pygmées, les Dogon de Tireli au Mali, en Nouvelle-Calédonie. c) Notions. Métamorphose : les êtres ont la capacité de métamorphose, l'animal peut devenir homme et inversement (p. 192). Un chamane huaorani d'Amazonie peut devenir jaguar (p. 344). Perspectivisme : comme l'écrit Eduardo Viveiros de Castro, "les animaux (prédateurs) et les esprits voient les humains comme des animaux (des proies), tandis que les animaux (le gibier) voient les humains comme des esprits ou comme des animaux (prédateurs)." d) Religion. L'animisme ne consiste pas en croyances, mais en l'expérience qu'il y a des esprits avec lesquels on peut entrer en communication, par des rêves, par la parole... e) Sociabilité. Relations permanentes sur le registre de l'amitié, de l'alliance de mariage, de déférence vis-à-vis des anciens" (). Aussi, "sur tout son territoire on ne trouvera ni éleveurs exclusifs, ni castes d'artisans spécialisés, ni culte des ancêtres, ni lignages fonctionnant comme des personnes morales, ni démiurges créateurs, ni goût pour les patrimoines matériels, ni obsession de l'hérédité, ni flèche du temps, ni filiations démesurée, ni assemblées délibératives" (p. 538). f) Problèmes. Comment rendre compte de la forme non humaine des non-humains ? solution : la métamorphose ().

L’évocation d’une « religion animiste » est communément entendue :


Une fois de plus, ce terme relève du langage courant, il n’a pas de portée anthropologique. Il pêche à trois égards : d’abord parce qu’on peut mettre en doute que, pour leurs adeptes, ces éléments soient eux-mêmes doués d’une âme ; ensuite parce que les peuples concernés n’isolent pas la « religion » des autres aspects de leurs traditions ; enfin parce qu’il recouvre d’innombrables cultures, très différentes les unes des autres.

Dans beaucoup de religions sinon toutes, les éléments naturels occupent une place importante. On peut citer la vénération de fleuves, tel le Gange, dans l’hindouisme, ou la crue du Nil, divinisée sous le nom d’Hâpy dans l’Égypte ancienne ; celle du feu auquel pouvait être assimilé Vesta à Rome ; celle du chêne et du gui, sacrés chez les Celtes.

Les monothéismes abrahamiques s’appuient eux-mêmes sur des éléments naturels, objets de cultes antérieurs : la fête de Noël est celle du solstice d’hiver (septentrional), celle de la Saint-Jean du solstice d’été, la fête de Pessa'h ou de Pâques est attachée au calendrier lunaire, qui rythme également la liturgie musulmane, dont le ramadan, etc.

Dans les religions amérindiennes, les divinités sont associées à des éléments naturels, avec une grande importance accordée au soleil, à la lune, à la pluie… La Méso-Amérique ne comptait pas moins de 4 dieux du maïs : un pour le maïs blanc, un pour le jaune, un pour le rouge, un pour le noir. Dans l’aire inca, on pratiquait une offrande à Pachamama, la Terre mère.

Pour autant, aucune de ces religions ne rend un culte « aux pierres ou au vent ». Tous les peuples, depuis la préhistoire, savent dépendre pour leur survie d’éléments naturels : la terre, le soleil, l’eau… Mais ils ne les adorent pas eux-mêmes, ils attribuent leur puissance à des forces surnaturelles qui les commanderaient : ils les ont divinisés ou vénèrent les esprits ou les dieux qui les dirigent. S’il y a des éléments d’animisme dans la plupart des religions, il est jusqu’à preuve du contraire difficile d’exhiber des cas de religions essentiellement fondées sur le culte des éléments naturels eux-mêmes. Dans l’exemple des Celtes, la sacralisation du chêne et du gui n’implique pas qu’il leur ait été directement rendu un culte : aucune source sérieuse ne le mentionne. Ce n’est pas parce que l’hostie est sacrée que le pain azyme est l’objet d’un culte : à travers elle, c’est le Christ qui est vénéré…

Certains lieux présentant des caractères physiques impressionnants ont marqué tous les peuples qui les ont traversés. On en trouve un exemple frappant dans le nid d’aigle d’Erice (Sicile) :  Élymes, Phéniciens, Grecs, Romains, Arabes, chrétiens… en ont tous fait un lieu de culte. Aucun d’eux ne vénérait le rocher d’Erice : ils étaient convaincus que, perché au milieu du ciel, ce lieu était élu, qu’il offrait une voie d’accès privilégiée à leur(s) divinité(s).

L’interprétation « animiste » de religions « traditionnelles » comme celles de l’Afrique noire, repose sur une appréhension simpliste de cultes jugés « primitifs » et sur la conviction implicite de la supériorité des religions et des cultures des nouveaux venus. L’appellation d’animisme n’en reste pas moins et malheureusement généralisée.

Les voyageurs et les colons européens, observant des offrandes, des sacrifices et des rites devant des éléments naturels tels que le fleuve Saloum, la pierre d’Abeokuta, etc. en déduisaient que dans leur « pensée primitive » les Négro-Africains leur attribuaient une âme, d’où le terme d’animisme. La réalité est plutôt inverse : le culte est rendu à un esprit localisé à cet emplacement, parfois parce qu’il y est mort ou y est enterré, et où un autel lui est en général dressé. Ces esprits sont ceux d’ancêtres anonymes, d’ancêtres ayant joué un rôle historique, parfois d’ancêtres divinisés, ils peuvent être recueillis ou hébergés en des animaux de la brousse, être d’une autre nature, comme les djinnés (inspirés des djinns arabes)… Il est significatif que plusieurs langues d’Afrique de l’Ouest utilisent le terme désignant un esprit ancestral pour désigner également un serpent (sérère "o fangool"), un animal sauvage (wolof "rab"), un autel (mandinka "jálaŋ", maninka "boli", diola "bëcin") , etc.

On trouve un exemple du véritable sens de l’« animisme » dans la pratique encore très vivante en Afrique noire, consistant à réserver les premières gouttes d’une boisson (surtout alcoolisée) ou les premières parcelles de nourriture à la terre : ce n’est pas à la nature que cette offrande est rendue, mais aux ancêtres, dont le séjour est souterrain dans la vision africaine. Le même rite est largement pratiqué dans les régions marquées par une forte présence d’afrodescendants, comme la côte caraïbe de Colombie.

La persistance du "souffle vital" (contrairement au "corps" et à la "force vitale", éphémères) de l’ancêtre et son retour dans un nouveau-né (réincarnation partielle) sont centraux dans cette vision du monde qui englobe religion, mythe, magie, pouvoir, médecine… Elle a conduit certains auteurs, tel Louis-Vincent Thomas , à définir ces religions comme "vitalistes" plutôt que comme animistes  : elles sont avant tout des religions de la vie, dans lesquelles la force vitale occupe la place centrale, et la sexualité comme la fécondité y ont une portée religieuse. Le décès d’enfants en bas âge est attribué au renoncement de l’ancêtre, déçu par la vie terrestre qu’il retrouve et désireux de repartir au village des morts. Le sens de l’anthropophagie (symbolique, contrairement au cannibalisme) est l’appropriation de la "force vitale" (mais non du "souffle vital", qui ne peut l'être) de l’autre : l’esprit ou le sorcier anthropophage prend possession de sa victime pour absorber sa force vitale et augmenter la sienne .

Le terme de vitalisme, qui vise à restituer l’essence des religions africaines, ne fait pourtant pas l’unanimité, comme sont contestés tous ceux par lesquels on tente de remplacer celui d’animisme, soit parce qu’ils n’en rendent que partiellement compte (culte des ancêtres…), soit parce qu’ils ne sont pas signifiants (religions traditionnelles…). Quant à la tradition négro-africaine, elle n’analyse pas la religion isolément de la magie, du pouvoir, de la médecine… et n’éprouve donc pas le besoin de la nommer en tant que telle.

Une autre approximation consiste à opposer un animisme africain polythéiste, puisque vénérant d’innombrables esprits, aux religions monothéistes. En réalité, la plupart des religions africaines, sinon toutes, sont fondées sur la croyance en un Dieu suprême ou unique : Roog chez les Sérères, Amma chez les Dogons, Olodumare chez les Yoruba, etc. occupent cette position. Si le culte est rendu aux esprits, ancêtres ou orishas, et non au maître de l’univers, c’est que celui-ci est inaccessible et qu’il convient d’amadouer les puissances « intermédiaires » de l’au-delà pour intercéder auprès de lui .  Cette situation n’est pas si différente de la dévotion aux saints du catholicisme et a grandement facilité la syncrétisation entre saints et orishas ou saints et dans les cultes afro-américains, comme le candomblé.

Il y a plus qu’une parenté entre l’animisme et le totémisme, le chamanisme ou le shintoïsme : une interpénétration, tous comportant une part « d’animisme ».

Le totémisme est présent dans beaucoup de sociétés animistes ou chamaniques, dont il est un autre aspect de la vision du monde et de la culture. En Afrique de l’Ouest, chaque famille clanique a son animal totem, par exemple le lapin pour les Senn wolofs ou sérères ; cet animal est considéré comme un parent et ne peut être consommé par les membres du clan. La même interdiction existe en Australie ou en Amérique du Nord, où Claude Lévi-Strauss montre que le totémisme repose sur une analogie entre un groupe humain et une espèce naturelle : tel clan ou tel hameau s’apparente au raton laveur par son mode vie. Cette parenté existe parce que le totem est souvent assimilé à un ancêtre.

L’animisme africain et les chamanismes de Sibérie et des Amériques ont en commun la médiation entre les êtres humains avec des forces spirituelles (esprits de la nature, âmes des animaux sauvages, ancêtres…), généralement intercesseurs auprès de la ou des divinités. Dans les deux cas, des sacerdotes (chaman ou amérindiens, saltigui ou babalawo africains…) ont la connaissance ésotérique leur permettant d’entrer en contact avec l’autre monde. Le sacerdote, à l’aide de paroles rituelles et de plantes, voyage pour recueillir la volonté des esprits et leur soumettre les besoins humains : il recommandera les offrandes et rites qui lui permettront d’apporter la guérison, la pluie, la fécondité… L’intermédiation use dans certains cas du support de la transe qui peut être celle du sacerdote ou du disciple.

L’animisme africain et le shintoïsme japonais, d’ailleurs de lointaine origine chamanique, ont en commun le culte des ancêtres, dont les pratiques rituelles et les offrandes peuvent être assez proches de celles pratiquées en Afrique ou dans les religions afro-américaines.

Ce qu’il est convenu d’appeler animisme est présent dans d’autres civilisations. On peut citer de manière non exhaustive, avec souvent des caractères relevant de l’animisme et du chamanisme, les traditions :


Dans les années 1920, Jean Piaget utilise le mot à propos de la psychologie du développement chez l'enfant de 6 à 14 ans. « Animisme : tendance à concevoir les choses comme étant vivantes et douées d'intention ». Par exemple, l'enfant dit que la chaise contre laquelle il se cogne est « méchante », il croit que sa poupée est vivante. .

Selon la théorie de Piaget, durant le « stade 1 », à 6-7 ans, l'enfant confond vie et activité : le Soleil est vivant, puisqu'il éclaire. Durant le « stade 2 », vers 7-8 ans, l'enfant, plus précisément, assimile vie et mouvement : la table n'est pas vivante car elle ne bouge pas, mais le Soleil, oui, car il bouge. Durant le « stade 3 », vers 9-10 ans, l'enfant tient la vie pour le mouvement propre : la mouche est vivante car elle se meut elle-même, mais la bicyclette, non, car on la pousse. Enfin, durant le « stade 4 », vers 11-12 ans, l'enfant n'attribue la vie qu'aux plantes et aux animaux.

Dès 1932, cette théorie est contestée, par exemple par Johnson et Josey qui déclarent n'observer rien de tel.





</doc>
<doc id="347" url="https://fr.wikipedia.org/wiki?curid=347" title="Anthropologie religieuse">
Anthropologie religieuse

L'anthropologie religieuse est le domaine de l'anthropologie qui étudie le "fait religieux", c'est-à-dire non seulement les pratiques ou les rites mais aussi les corpus théologiques savants ou non (mythes, textes sacrés, doctrine) propres à chaque tradition religieuse.



Les faits religieux prétendent donner des clés pour comprendre le monde. Mais ce sens est produit par les hommes et les femmes, même s'ils considèrent que ce sens existait avant eux. Le fait religieux, s'il n'est pas mythique ou ésotérique, s'accorde avec des principes métaphysiques, ou philosophiques, qui, eux, ne peuvent pas faire l'économie de s'interroger sur l'existence d'un principe premier : idée de Bien (Platon), cause première (Aristote), existence de l'Être immuable (Parménide), présence de l'Un primordial (Plotin), Voie (taoïsme), pensée dialectique (Hegel), principe vital (Bergson) etc. La Bible prétend que Dieu a créé le monde "ex nihilo", c'est-à-dire de rien : ""Au commencement Dieu créa le ciel et la terre."" (Gn 1,1). Saint Jean soutient que ""Dieu est Amour"". (1 Jn 4,16)…

Le fait religieux, fondamentalement, c'est la relation de l'homme avec l'Être. Selon l'historien des religions Mircea Eliade, la religion, c'est-à-dire le rapport qu'a un homme avec "le sacré" ou ce qu'il prétend voir consciemment, ou inconsciemment, comme "tel", est consubstantiel à toute société humaine, aussi laïque, athée ou agnostique voudrait-elle se prétendre (les partis politiques sont à cet égard comparables à des sectes, – à des petites religions).

La religion implique donc :

En comparaison, on remarquera que l'hindouisme se révèle être une civilisation, et non une religion, les religions qui constituent la civilisation hindoue étant la multitude de sectes ou écoles de philosophie indienne ("sampradaya").



</doc>
<doc id="350" url="https://fr.wikipedia.org/wiki?curid=350" title="Alphabet phonétique de l'OTAN">
Alphabet phonétique de l'OTAN

L'alphabet phonétique de l'OTAN est le nom que l'on donne parfois à l'alphabet radio international qui a été normalisé par l'Union internationale des télécommunications. Il est utilisé notamment par l'OACI et l'OTAN mais aussi par les services de secours utilisant les fréquences radio tels que les pompiers, la police, la gendarmerie, la Croix-Rouge, la protection civile, la sécurité civile, les radioamateurs, etc. Il est issu des différents alphabets radio utilisés par les forces armées des États-Unis et a remplacé les alphabets radio nationaux. Il a été généralisé par l'OTAN, d'où son nom dans le langage courant.

Mis au point par des techniciens de 31 nations à la suite d'études très approfondies et de centaines de milliers d'essais, un nouveau code avait été mis en application depuis le mars 1956, dans l'aviation civile internationale qui avait fait enregistrer sur disque la prononciation correcte des mots. Lors des conférences de Genève en 1959, un accord a été réalisé pour l'emploi d'une table d'épellation officielle, valable dans les relations internationales .

Voici les codes et leur prononciation.
En aviation, le code « delta » est remplacé par « data », « dixie » ou « david » dans certains aéroports, dont l'Aéroport international Hartsfield-Jackson d'Atlanta, pour éviter la confusion avec l'indicatif d'appel « Delta », associé à la compagnie aérienne Delta Air Lines.

Le code « nine » peut être remplacé par le code « niner » pour éviter la confusion avec le mot allemand « "" », qui signifie « non » et qui se prononce exactement de la même manière. Quelquefois, le code « one » peut être remplacé par « unit », ce qui signifie « unité ». D'autre part, le code « five » peut être remplacé par « fife » pour éviter la confusion avec « fire » (« feu » en anglais avec un accent non rhotique ou le "r" final n'est pas prononcé), cela pouvant signifier un incendie.




</doc>
<doc id="351" url="https://fr.wikipedia.org/wiki?curid=351" title="ADSL">
ADSL

L’ADSL (de l'anglais ) est une technique de communication numérique (couche physique) de la famille xDSL. Elle permet d'utiliser une ligne téléphonique, une ligne spécialisée, ou encore une ligne RNIS (en anglais "ISDN" pour ""), pour transmettre et recevoir des données numériques de manière indépendante du service téléphonique conventionnel (c'est-à-dire analogique). À ce titre, cette méthode de communication diffère de celle utilisée lors de l'exploitation de modems dits « analogiques », dont les signaux sont échangés dans le cadre d'une communication téléphonique (similaire au fax, c'est-à-dire sur des fréquences vocales). La technologie ADSL est massivement mise en œuvre par les fournisseurs d'accès à Internet pour le support des accès dits « haut-débit ».

Le sigle ADSL de l'anglais , qui se traduit fonctionnellement par « [liaison] numérique [à débit] asymétrique [sur] ligne d'abonné ». La terminologie française officielle recommande l'expression , mais le sigle « ADSL » reste le plus largement utilisé dans le langage courant.

Comme son nom l'indique, la technologie ADSL fournit un débit asymétrique. Le flux de données est plus important dans un sens de transmission que dans l'autre. Contrairement à la technologie SDSL pour laquelle le débit est symétrique, donc équivalent en émission et en réception, le débit de données montant d'une communication ADSL (') est plus faible que le débit descendant ('), dans un rapport qui varie généralement entre 5 et 20.

Joseph Lechleider, ingénieur au laboratoire de recherche Bell, constata que lorsque les débits ascendants et descendants étaient identiques (SDSL), les parasites étaient trop nombreux et faisaient chuter les débits. Conservant en tête que le but premier était de fournir du contenu, il imagina donc un moyen de favoriser la vitesse de téléchargement aux dépens de la vitesse d’émission et rendit ainsi la connexion asymétrique. Le brevet fut déposé en 1988.

En France, le lancement commercial de l'ADSL a été effectué par France Telecom Interactive en 1999 ; l'ADSL a commencé à décliner au quatrième trimestre 2014 devant la concurrence de la fibre optique et du VDSL2 qui apportent un débit supérieur.

La ligne téléphonique qui relie le domicile d'un abonné à l'autocommutateur public qui dessert son quartier (le « central téléphonique ») est constituée d'une paire de fils de cuivre, en général continue entre ces deux points (la boucle locale). Les signaux utilisés pour la téléphonie classique (sonnerie, numérotation multifréquences, voix) occupent une bande de fréquences qui s'étend entre 25 et environ. Le principe de l'ADSL consiste à exploiter une autre bande de fréquence, située au-dessus de celle utilisée pour la téléphonie, pour échanger des données numériques en parallèle avec une éventuelle conversation téléphonique. Grâce à cette séparation dans le domaine fréquentiel, les signaux ADSL qui transportent les données et les signaux téléphoniques qui transportent la voix circulent donc simultanément sur la même ligne d'abonné sans interférer les uns avec les autres.

L'ADSL fait partie d'une famille de technologies semblables, regroupées sous le terme générique DSL ou xDSL. Les différents membres de cette famille se différencient par leur nature symétrique ou asymétrique, les débits offerts, les longueurs de ligne compatibles avec une qualité de service déterminée, etc. Parmi ces technologies, on peut citer le SDSL et les VDSL et VDSL2 ; toutefois, la méthode SDSL de transmission exploite la totalité de la bande passante de la ligne téléphonique, et ne permet donc plus le partage de celle-ci entre un service de téléphonie traditionnelle et la transmission SDSL.

L'ADSL nécessite l'installation d'équipements de communication dédiés à cette technologie aux deux extrémités de la ligne téléphonique (souvent dénommée « paire cuivrée » dans le jargon des télécommunications ou encore boucle locale). Dans les locaux de l'autocommutateur public, l'équipement qui traite les signaux ADSL d'un groupe d'abonnés s'appelle un DSLAM (pour ). Chez l'abonné, l'équipement qui effectue la même fonction est soit un modem ADSL, soit un routeur ADSL (qui n'est autre qu'un routeur classique muni d'un modem ADSL interne).

L'ADSL utilise un spectre de fréquences étendu sur un support physique qui n'était pas prévu pour cela à l'origine (la paire cuivrée). Cette technologie peut donc se révéler inexploitable sur des lignes d'abonnés qui présentent une trop forte atténuation pour les signaux de l'ADSL. C'est le cas lorsque la section de la paire cuivrée est trop faible ou lorsque la longueur de la ligne est trop importante. Le terme d'éligibilité technique est utilisé pour qualifier la compatibilité d'une ligne téléphonique donnée avec l'ADSL. Cette éligibilité peut être vérifiée sur les sites Web des fournisseurs d'accès.
Le signal ADSL transite sur la paire cuivrée téléphonique au même titre que le signal téléphonique, et la cohabitation de ces deux types de signaux requiert l'installation de filtres destinés à séparer les fréquences respectives des deux flux. Au niveau de l'autocommutateur public, ces filtres sont installés sous la forme d'armoires de filtrage qui regroupent plusieurs centaines, voire plusieurs milliers de cartes électroniques de filtrage. Chez l'abonné, la séparation des deux flux est réalisée au moyen d'un filtre ADSL placé entre la prise téléphonique et la fiche de connexion du téléphone.

Dans le cas où la ligne n'est utilisée que pour l'ADSL (cas des lignes en ADSL nu ou en dégroupage total) un filtre n'a aucune utilité.

Le signal à destination de l'ordinateur, arrive au modem lequel extrait les données numériques du signal ADSL. Ces données sont ensuite transmises à l'ordinateur, par l'intermédiaire d'un câble Ethernet, d'un câble USB ou encore grâce à une liaison Wi-Fi.

Les données numériques véhiculées par l'ADSL peuvent elles-mêmes servir de support à une communication téléphonique (VoIP), ou à la diffusion de programmes de télévision numérique (le plus souvent en MPEG-2, mais aussi en MPEG-4). On voit donc apparaître des familles de modems ADSL évolués qui permettent de traiter ces flux de manière native. En France, c'est le cas de nombreuses « » proposées par les fournisseurs d'accès à Internet : Freebox, Livebox, Neuf Box, Alice Box ou Darty Box. Ces modems spécialisés sont munis de connecteurs qui permettent d'y raccorder directement un poste téléphonique (connecteur RJ-11) ou un téléviseur (connecteur péritel ou HDMI). Pour des raisons de commodité de mise en œuvre et de limitation de la consommation électrique, ces matériels se présentent souvent sous la forme de deux boîtiers distincts. Dans ce cas, le premier boîtier assure généralement la communication ADSL proprement dite, ainsi que le support des fonctions de téléphonie et de transmission de données informatiques. Le second boîtier supporte les services multimédias comme la réception des chaînes de radio et de télévision numérique, l'enregistrement et la restitution des flux audio et vidéo sur disque dur embarqué ou externe, la restitution de programmes en différé, etc. La communication entre les deux boîtiers peut emprunter un câble Ethernet, une liaison Wi-Fi, ou encore un système de courants porteurs en ligne.

N.B. Le terme de sous-porteuse utilisé dans les chapitres ci-dessous est un abus de langage. Il s'agit en réalité de porteuses parce qu'elles ne modulent pas une porteuse principale. 

L'ADSL fait appel à la notion de sous-porteuses : la bande de fréquences comprise entre et environ est divisée en 255 intervalles de . À chaque intervalle est associée une sous-porteuse, qui donne un signal modulé. La n-ième sous-porteuse est donc matérialisée sous la forme d'un signal dont la fréquence de base vaut . Un modem ADSL peut donc être considéré comme la mise en parallèle d'un grand nombre de modems analogiques, chacun transmettant sur une fréquence différente : pour le premier, pour le second, pour le troisième, et ainsi de suite.

La sous-porteuse d'indice 0 n'est pas utilisée, car elle correspond à un signal de fréquence nulle. Les sous-porteuses d'indice 1 à 255 sont théoriquement utilisables pour transmettre des données. Toutefois, les sous-porteuses d'indice 1 à 6 ne sont en général pas exploitées en raison de la présence possible de signaux téléphoniques dans une gamme de fréquences proche des fréquences utilisées par ces sous-porteuses. Dans la pratique, lorsque l'ADSL est mis en œuvre sur une ligne téléphonique classique (analogique), les sous-porteuses d'indice 7 à 255 sont donc disponibles pour la communication ADSL proprement dite.

L'ADSL2 et le Re-ADSL () sont des évolutions de la technologie ADSL d'origine. Grâce à une technique de modulation/démodulation améliorée, ces nouveaux modes permettent d'obtenir une meilleure immunité de la communication aux perturbations et un fonctionnement acceptable sur des lignes qui auraient été trop longues pour supporter une transmission ADSL classique.

L'ADSL est considéré par les instances de normalisation comme une technologie destinée essentiellement au grand public mais aussi aux PME et aux TPE. Vis-à-vis des ressources disponibles sur Internet, cette catégorie de clients est en général davantage amenée à télécharger des informations (consultation d'un site Web, par exemple) qu'à envoyer des informations vers un site distant. Il a donc été décidé de favoriser le sens de la communication qui va du réseau vers l'abonné (descendant en français, en anglais), au détriment du sens qui va de l'abonné vers le réseau (montant en français, en anglais). C'est pour cela que l'ADSL est qualifié d’"asymétrique" : le nombre de sous-porteuses affectées au sens descendant est plus élevé que le nombre de sous-porteuses affectées au sens montant.

Quand l'ADSL est mis en œuvre sur une ligne téléphonique classique (analogique), les sous-porteuses 7 à 31 sont affectées au trafic émis par l'abonné vers le réseau. Les sous-porteuses 33 à 255, quant à elles, sont affectées au trafic reçu du réseau par l'abonné. En ADSL1, la sous-porteuse d'indice 64 sert de signal de référence (« porteuse pilote ») pour les deux sens de communication et n'est pas modulée. Dans les modes ADSL2 et supérieurs, le choix de la sous-porteuse utilisée comme pilote fait partie de la négociation préalable à la synchronisation.

Les modems ADSL les plus récents comportent des systèmes de traitement numérique basés sur une technique d'annulation d'écho qui permet, si nécessaire, d'utiliser aussi les sous-porteuses d'indice inférieur à 32 pour transporter des données de la voie descendante. Dans la pratique, ce mode de fonctionnement n'apporte qu'un gain de débit limité, au prix d'une plus grande sensibilité aux perturbations, et il ne semble donc pas promis à une large mise en œuvre.

Il existe deux variantes de l'ADSL que l'on utilise selon que la ligne d'abonné est une ligne téléphonique analogique (dans ce cas, on utilise l'ADSL annexe A) ou une ligne exploitée en RNIS (dans ce dernier cas, on utilise l'annexe B). Les paragraphes ci-dessus décrivent la répartition des sous-porteuses de l'ADSL annexe A. Comme la plage des fréquences utilisées par le RNIS est plus étendue que pour la téléphonie classique, et atteint la centaine de kiloHertz, l'annexe B prévoit de réserver davantage de sous-porteuses inutilisées dans le bas de bande, et décale d'autant la frontière entre les sous-porteuses utilisées pour le sens montant et celles utilisées pour le sens descendant. L'annexe B est utilisée en Allemagne, par exemple, où la plupart des lignes du réseau téléphonique public sont exploitées en RNIS.

On peut également signaler l'existence d'une autre annexe qui permet de disposer d'un plus grand nombre de sous-porteuses pour le sens montant (annexe M), au détriment du nombre de porteuses affectées au sens descendant. Cette option est relativement peu utilisée et n'est d'ailleurs pas autorisée sur le réseau public en France, car elle peut perturber par intermodulation une communication ADSL classique sur une ligne d'abonné adjacente (diaphonie, ou en anglais).

Chaque sous-porteuse est modulée en amplitude et en phase, au rythme de symboles par seconde (on notera toutefois que ce nombre n'est pas tout à fait exact, voir plus bas le paragraphe « supertrames »). Un symbole est un état de modulation qui peut représenter un plus ou moins grand nombre de bits d'information. La complexité de modulation de chaque sous-porteuse est choisie en fonction de la qualité de transmission observée sur la ligne pour cette sous-porteuse. En effet, les modulations complexes permettent de transporter un nombre de bits élevé, ce qui favorise le débit, mais ce type de modulation est plus difficile à décoder au niveau du récepteur et est donc plus sensible aux erreurs de transmission provoquées par d'éventuelles perturbations en ligne. Le niveau de modulation de chaque sous-porteuse peut donc être ajusté pour transporter entre 2 et d'information par symbole. Le nombre de bits affecté à chaque sous-porteuse est déterminé en début de connexion, après une phase de mesure de qualité de la ligne effectuée par échange de signaux de test entre les deux équipements ADSL qui établissent la communication.

Les informations transportées par l'ADSL dans chaque sens de communication sont organisées en trames d'une taille égale à la somme des bits véhiculés par l'ensemble des sous-porteuses affectées à ce sens de communication. En supposant par exemple que le sens descendant utilise 40 sous-porteuses et que chaque sous-porteuse transporte par symbole, la taille de la trame correspondante est de , soit . Chaque sous-porteuse étant modulée à raison de symboles par seconde, ce sont donc trames qui sont envoyées à chaque seconde, et avec les chiffres de notre exemple, le débit brut du sens descendant s'établit à × , soit .

Chaque trame contient des informations de service, des données utilisateur, et éventuellement des octets de redondance utilisés pour détecter et si possible corriger les erreurs. Ce mécanisme de détection et de correction d'erreurs, connu sous le nom de FEC (pour ) fait appel au code de Reed-Solomon. Chaque trame transporte donc des données qui ont été préalablement organisées sous la forme d'un ou plusieurs mot-code(s) ( en anglais) Reed-Solomon.

Pour des raisons de synchronisation, les trames ADSL sont regroupées en « trains » de 68 trames consécutives et complétées par une de contrôle qui contient des informations de service additionnelles plutôt que des données utilisateur. Ces groupes de 69 trames sont désignés sous le nom de « supertrames ».

La présence d'une trame de contrôle pour chaque groupe de 68 trames « prend de la place » en ligne et devrait normalement affecter le débit des données utiles. Pour éviter ce problème, la véritable rapidité de modulation de chaque sous-porteuse est non pas de symboles par seconde comme indiqué plus haut, mais de symboles par seconde (environ symboles par seconde), ce qui permet de transmettre exactement symboles « utiles » par seconde.

En supposant que la qualité de la ligne le permette, chaque sous-porteuse peut utiliser des symboles de , et transmet symboles par seconde. Pour le sens descendant, on dispose de 223 sous-porteuses pour transporter des données utilisateur (ce sont les sous-porteuses 32 à 255, moins la sous-porteuse pilote 64). Sans autre limitation, le débit maximum théorique du sens descendant s'établirait donc à [223 sous-porteuses × × symboles], soit .

Dans la pratique, ce débit est moindre pour deux raisons principales :

À cause de ces deux facteurs, le débit net maximum du sens descendant s'établit en général à une valeur intermédiaire, de l'ordre de quelques centaines à quelques milliers de kilobits par seconde. Par ailleurs, en fonction de l'abonnement souscrit (on parle de « paliers de débit »), le débit peut être volontairement limité par les équipements du fournisseur d'accès à Internet, indépendamment des possibilités techniques.

En ce qui concerne la limitation induite par le code Reed-Solomon, une modification ultérieure de la norme prévoit un format différent pour les mots-codes (tramage « S=1/2 »), qui permet de véhiculer presque deux fois plus de données utilisateur dans chaque mot-code. Lorsque ce format est mis en œuvre, le débit n'est plus limité que par le nombre de bits qui peuvent être transportés par l'ensemble des 223 sous-porteuses affectées au sens descendant. Le débit maximum théorique de l'ADSL s'établit alors à un peu plus de pour le sens descendant.

Ces valeurs ne s'appliquent pas à l'ADSL2+, qui utilise un nombre de sous-porteuses plus élevé.

Le même calcul s'applique aux 31 sous-porteuses disponibles pour transporter les données dans le sens montant, et les mêmes restrictions existent à cause de la structure des mots-codes Reed-Solomon. Le débit maximum du sens montant s'établit donc à (cette valeur est la même pour l'ADSL2+).

L’ADSL2+ est une évolution de l'ADSL qui utilise 511 sous-porteuses au lieu de 255, grâce à une extension de la bande de fréquences utilisées jusqu'à environ. Cette capacité accrue, associée à une structure de trame modifiée pour permettre le transport d'un plus grand nombre d'octets dans chaque trame, permet d'atteindre des débits de données de plus de dans le sens descendant. La capacité et le débit du sens montant restent inchangés par rapport à l'ADSL « classique ».

Toutefois, les performances accrues de l'ADSL2+ ne sont accessibles que dans de bonnes conditions :

Comme expliqué plus haut, chaque trame ADSL élémentaire transporte un mot-code Reed-Solomon, lequel comporte des informations de service, des octets de redondance utilisés pour détecter et si possible corriger les erreurs, et un espace destiné aux données de l'utilisateur.

Typiquement, les données utilisateur sont organisées sous la forme de cellules en protocole ATM, d'une longueur unitaire de , dont 48 sont disponibles pour les données utilisateur proprement dites. Les 5 premiers octets de chaque cellule ATM contiennent les références du circuit virtuel ATM, ainsi que des informations relatives à l'organisation des données utilisateur au sein d'un groupe de cellules consécutives. À l'émission, les données issues de l'équipement informatique de l'abonné (paquets IP, messages PPP ou encore trames PPPoE) sont automatiquement fragmentées en tronçons de et réparties dans autant de cellules ATM que nécessaire. À la réception, les données transportées par chaque cellule sont extraites, et le message d'origine est automatiquement reconstitué avant d'être délivré à l'équipement destinataire.

Les données redondantes transmises au sein de chaque trame ADSL permettent de détecter et, dans une certaine mesure, de corriger les erreurs de réception. Si l'erreur n'affecte que quelques bits dans la trame ADSL reçue, un mécanisme de correction d'erreur () incorporé au circuit de réception est en général capable de reconstruire les données abîmées. L'erreur est signalée dans les statistiques de réception sous la forme d'une « erreur FEC ». En revanche, si les données sont trop abîmées pour pouvoir être reconstituées, l'erreur est signalée sous la forme d'une « erreur CRC ». Dans certains cas, une erreur affecte l'en-tête d'une cellule ATM, et cette altération est détectée par le récepteur, qui la signale sous la forme d'une « erreur HEC ». Enfin, si le taux d'erreur est suffisamment grand, la structure de la trame ADSL elle-même peut être affectée au point que plus aucune donnée reçue n'est utilisable. On constate alors une perte de tramage (« erreur LOF ») qui peut aller jusqu'à la perte totale de synchronisation (« erreur LOS »). En présence de ce type d'erreur, le modem ADSL réagit le plus souvent en interrompant la communication et en entamant une nouvelle procédure de synchronisation depuis le début. C'est le phénomène connu sous le nom de « désynchronisation » par les internautes.

Le protocole ATM ne supporte pas nativement de système de correction des erreurs. Quand se produit une erreur suffisamment sévère pour que le dispositif de correction d'erreur natif de l'ADSL (FEC) ne puisse pas la corriger, les cellules ATM affectées par l'erreur sont supprimées en réception. Il manque donc un segment dans les données utilisateur reçues par le destinataire. En général, une couche de protocole de niveau supérieur (TCP par exemple) fait le nécessaire pour demander la retransmission de ce segment manquant.

Les modems ADSL maintiennent en permanence des statistiques sur la qualité de réception, qui est mesurée sur chaque sous-porteuse. Cette évaluation est faite au démarrage de la communication ADSL (phase de synchronisation) par l'intermédiaire de données de test émises sur chaque sous-porteuse et par l'échange d'informations entre les deux modems sur la qualité du signal reçu. Par la suite, les variations du rapport signal-sur-bruit de chaque sous-porteuse, représentatives de la qualité de réception, sont surveillées individuellement. Lorsqu'une sous-porteuse est affectée par des perturbations, le modem et l'équipement distant ont la possibilité d'échanger des requêtes qui leur permettent d'augmenter la puissance d'émission dévolue à cette sous-porteuse ou de réduire le nombre de bits transmis sur celle-ci, et de transférer la différence sur une sous-porteuse qui bénéficie de meilleures conditions de réception. Ce mécanisme est connu sous le nom de en anglais, et fonctionne de manière indépendante dans le sens montant et dans le sens descendant.

Le temps de latence constaté en réception dépend du mode de transport des cellules ATM au sein des trames ADSL. Il existe deux modes de transport qui affectent différemment les données :

Les utilisateurs qui ont besoin d'un temps de latence faible (joueurs en ligne, par exemple) exploitent donc le mode de transmission quand l'opérateur le propose, tandis que les utilisateurs qui recherchent une bonne fiabilité de transmission préfèrent le mode . En mode fast path, le temps de latence typique est de l'ordre de , alors qu'il se rapproche plutôt des en mode .

Comme illustré par le tableau d'exemples ci-dessous, le débit maximal dans le sens descendant dépend du mode de modulation utilisé (ADSL1 / ADSL2 / ADSL2+) et de l'atténuation totale subie par les signaux durant leur trajet sur la ligne de l'abonné. Cette atténuation totale dépend de la longueur et du diamètre de chaque tronçon de la ligne.

Exemples de valeurs d'atténuation et de débits en fonction de la longueur de la ligne :
En fonction de la distance, on constate que l'ADSL2+ procure des débits 1,5 à 2,5 fois plus importants que l'ADSL1 et l'ADSL2. Ce ratio doit toutefois être considéré avec prudence, car il ne tient pas compte d'une éventuelle utilisation du tramage « S = 1/2 » qui permettrait de dépasser les 8 Mbit/s en ADSL1 ou en ADSL2 dans les exemples les plus favorables cités dans le tableau.

On remarque également que le signal est beaucoup plus atténué par les câbles de diamètre 4/10 mm (environ 15 dB/km) que par ceux de 6/10 mm (environ 10,5 dB/km) .

L'ADSL peut parfois se révéler délicat à mettre en œuvre sur certaines lignes d'abonné. La bande de fréquences utilisée par les sous-porteuses de l'ADSL couvre en effet à peu près le domaine des fréquences radio correspondant aux « grandes ondes » et aux « ondes moyennes ». De nos jours, ces bandes de fréquences sont encore utilisées pour des émissions réalisées en modulation d'amplitude, lesquelles souffrent en général d'une réception de moins bonne qualité que celles réalisées en modulation de fréquence, avec des variations de signal parfois importantes, des craquements et sifflements qui résultent des perturbations extérieures. De ce point de vue, une communication ADSL peut être assimilée à une « transmission radio ondes moyennes sur ligne téléphonique » et elle est donc sujette aux mêmes distorsions et perturbations.

En fonction du trajet emprunté par une ligne d'abonné entre le domicile et l'autocommutateur public, il n'est pas rare que des perturbations ponctuelles ou permanentes affectent les signaux ADSL. Si elles sont d'une nature continue, ces perturbations sont détectées et évaluées par les équipements ADSL au moment de la synchronisation, et les sous-porteuses correspondantes sont délaissées au profit de sous-porteuses plus fiables.

Mais les perturbations les plus gênantes pour les communications ADSL sont celles que l'on classe dans la catégorie du « bruit impulsionnel », car elles sont trop rapides pour être prises en compte efficacement par le dispositif de redistribution des données entre les sous-porteuses. Ce type de perturbation résulte en général d'un défaut d'antiparasitage d'un dispositif électrique : moteur de deux-roues, moteur électrique de lave-linge, pompe de chaudière, gradateur de lampe halogène, four à micro-ondes, néon défectueux… Mais il existe parfois des causes plus inattendues : une pluie d'orage sur une ligne téléphonique aérienne entraîne également ce type de perturbation du fait de la charge électrique accumulée par les gouttes de pluie. D'autres perturbations peuvent être provoquées par une ligne téléphonique adjacente qui fonctionne dans des conditions anormales, par un mauvais fonctionnement de l'éclairage public des rues, ou encore par un filtre défectueux au niveau de l'armoire de brassage située dans le bâtiment de l'autocommutateur public. Ces perturbations peuvent affecter la communication en tout point du trajet physique de la ligne d'abonné, et être suffisamment gênantes pour entraîner des pertes de synchronisations répétées, suivies d'autant de tentatives de rétablissement de la connexion. Dans de telles conditions, la communication devient rapidement inexploitable.

Ces phénomènes très complexes, heureusement rares, sont souvent mal perçus par les abonnés, qui ne comprennent pas que leur fournisseur d'accès ne soit pas toujours en mesure de faire le nécessaire pour que leur abonnement ADSL fonctionne de manière satisfaisante. De ce point de vue, pour un faible pourcentage d'abonnés, l'ADSL reste une technologie dont la fiabilité est aléatoire, contrairement aux offres d'abonnement basées sur une transmission optique (FTTH) car cette technologie est beaucoup moins sensible aux perturbations électromagnétiques.

L'ADSL est en général associé à la notion d'accès Internet à haut débit. Toutefois, l'ADSL permet de transporter bien d'autres flux que le protocole TCP/IP. Il existe notamment des spécifications de transport de la téléphonie ou de la vidéo, segmentées en cellules ATM, directement dans l'ADSL. Dans la pratique, les services de téléphonie sur ADSL ou de diffusion de programmes de télévision via l'ADSL s'appuient tous sur une encapsulation des flux dans le protocole IP (parfois avec utilisation d'un circuit virtuel ATM dédié à chaque flux, pour des raisons de séparation et de qualité de service). La télévision sur ADSL, à présent très répandue, est donc de la vidéo sur IP. De la même manière, les offres de téléphonie sur ADSL proposées par les opérateurs sont implémentées par des protocoles de VoIP.

En dehors des particuliers, ces services intéressent également les entreprises, pour lesquels l'ADSL peut servir d'accès à un service de réseau privé virtuel (VPN) proposé par l'opérateur.

Les débits proposés par les fournisseurs d'accès à Internet sont en général exprimés en débits ATM. On a vu que les données utilisateur proprement dites sont transportées à raison de par cellule ATM, et que chaque cellule comporte un en-tête de . Il résulte de cette encapsulation que le débit constaté au niveau IP lors d'un transfert FTP, par exemple, est inférieur d'environ 20 % à la valeur de débit de l'abonnement.

Voici quelques valeurs de débit ATM pour les offres commerciales ADSL destinées au grand public en 2004 :

En Belgique, le débit montant se situe entre et , le débit descendant entre 1 et . Ces débits varient en fonction de l'offre, certains opérateurs proposant une version à pour environ et un débit plus élevé moyennant finances. Dans la grande majorité des offres, le volume utilisable par mois est limité (de pour les offres les moins chères à 10, 30 voire en moyenne pour les offres plus élevées). Cette limitation freine le déploiement des usages massivement consommateurs de bande passante, comme le pair-à-pair, la vidéo à la demande…

En France, le débit montant est typiquement compris entre et , le débit descendant peut atteindre (IP) sur de courtes distances du centrale (pour ADSL2+) Pour l'ADSL le débit va jusqu'à et l'ADSL2 jusqu'à . Les lignes d'environ plus de ne sont pas éligibles à l'ADSL2+ . Le ReADSL pour les lignes compris entre à permet d'avoir un débit entre à . En , le débit moyen en France est de selon le site Ariase, Le volume n'est pas facturé (ni même souvent les suppléments de débit utilisable sur la ligne au-delà de ), l'utilisateur dispose alors d'une connexion permanente forfaitaire pour 15 à (toutefois les débits et tarifs dépendent aussi de la présence d'offres de services combinés, dites , pouvant comprendre aussi la téléphonie sur IP, la visiophonie, l'accès aux bouquets TV numériques et vidéo à la demande, le relais de téléphonie mobile à domicile, la télésurveillance, etc.). Pratiquement tous les FAI (Fournisseur d'accès à Internet) proposent un modem-routeur gratuit ou en location à prix modique (inférieur à ) compatible avec l'offre (l'utilisation de ce modem est parfois obligatoire pour certains services comme la téléphonie ou la télévision), et permettant le partage de connexion Internet sur un réseau local Ethernet et/ou Wi-Fi.

En Suisse, le débit montant varie actuellement entre 100 et , alors que le débit descendant se situe entre et . Les offres les plus courantes proposent un débit /500 pour une somme forfaitaire d'environ CHF 49,00 par mois. De nouvelles offres à CHF 9,00 par mois proposent un débit descendant de comprenant ou une taxation horaire fixée à CHF 2,40 de l'heure, ou une taxation (au-delà des premiers ) liée au volume consommé qui est de CHF 0,19 le Mo, les deux modèles d'offre ayant une limite maximale de facturation située entre CHF 60,00 et 80,00. La clientèle visée par ces dernières offres se constitue des abonnés 56k. Récemment avec le dégroupage, certains FAI commencent à offrir des technologies concurrentes, par exemple VTX propose de l'ADSL2 et Swisscom, Green.ch eux offrent du VDSL.

Au Japon, l'ADSL peut atteindre des débits descendants de à la source et un débit supérieur à à de distance (pour environ ). Le débit montant est typiquement de constant jusqu'à environ de distance.

La barre des sur cuivre pourrait être atteinte avec la mise en œuvre de la technologie (DSM).

Au Cameroun, en 2009, Orange Cameroun commercialise des offres ADSL à des tarifs dégressifs: 128K descendants/64K montants au tarif de francs CFA mensuel (environ ); 512K descendants/128 K montants pour francs CFA mensuel (environ ).




</doc>
<doc id="352" url="https://fr.wikipedia.org/wiki?curid=352" title="Active Server Pages">
Active Server Pages

La dernière version livrée par Microsoft (la 3.0) date de l'an 2000. Microsoft l'a ensuite remplacée par ASP.NET.

C'est une suite de logiciels destinée à créer des sites web dynamiques. Elle nécessite pour fonctionner une plate-forme Windows avec IIS installé, ou encore une plate-forme Linux ou Unix avec une version modifiée d'Apache. ASP est une structure composée d'objets accessibles par deux langages principaux : le VBScript et le JScript. Il est possible d'utiliser d'autres langages comme le PerlScript, le REXX, ou encore le Python en ajoutant le moteur d'interprétation du langage adéquat à IIS.

À l'inverse de certains langages de programmation (C, C++), cette technologie n'utilise pas de langages compilés, mais des langages interprétés.

L'ASP possède sept vrais objets manipulables : les objets request, response, server, object context, application, session et error.

Il permet de lire tout ce qui a été renvoyé par le navigateur client, comme les formulaires ou les cookies. Il permet également d'obtenir des informations sur le serveur, sur le navigateur client, et de récupérer les cookies stockés sur la machine du visiteur. Il permet également de récupérer les données issues d'un formulaire utilisant les deux méthodes HTTP :

Inversement, il permet d'envoyer des informations au client, comme le fait d'écrire du texte dans une page ou d'écrire dans des Cookies.

Cet objet permet de créer et de gérer des connexions à des Bases de Données (nommé ADO), d'ouvrir des fichiers XML, Word, Excel… et en général de créer des objets et d'utiliser des composants installés sur le serveur.

Il permet de contrôler les transactions éventuelles avec le serveur de transaction Microsoft.

Il permet de stocker des variables globales à tous les visiteurs qui passent sur le site.

Il permet de stocker des variables uniquement accessibles à un seul visiteur du site.<br>Utilisé par exemple dans les sites possédant un panier pour stocker des articles.

Cet objet permet la gestion des erreurs.

L'ASP utilise COM (aussi appelé ActiveX) pour communiquer avec des ressources du poste serveur. Il renvoie ensuite de l'HTML au client via le protocole HTTP.

L'ASP est capable de se connecter à des bases de données, de lire des fichiers XML et possède des composants pour la gestion de l"'upload", du FTP…
Il peut lire et écrire des documents issus d'Office (Excel, Word…) en passant par le système COM (voir ci-dessus), si Office est installé sur le serveur. Du reste, d'autres langages (comme PHP) peuvent également utiliser la technologie COM, à condition de tourner également sur un serveur Windows où les produits Office sont installés.

Enfin, depuis la technologie .NET, l'ASP est devenu l'ASP.NET.



</doc>
<doc id="353" url="https://fr.wikipedia.org/wiki?curid=353" title="Agnosticisme">
Agnosticisme

L’agnosticisme ou pensée de l'interrogation est une attitude de pensée considérant la vérité de certaines propositions concernant notamment l'existence de Dieu ou des dieux comme inaccessible à l'intelligence humaine : l'agnosticisme n'est pas forcément incompatible avec l'athéisme ou le théisme même si certains agnostiques refusent de trancher. Si le degré de scepticisme varie selon les individus, les agnostiques s'accordent pour dire qu'il n'existe pas de preuve définitive en faveur de l'existence ou de l'inexistence du divin, et affirment l'impossibilité de se prononcer en ce qui concerne la connaissance et aussi, parfois, en ce qui concerne la croyance ou la non-croyance.

Si les agnostiques refusent de se prononcer quant à l'existence d'une intelligence supérieure, ils n'accordent, en revanche, ou du moins tendent à n'accorder, aucune transcendance et aucune valeur sacrée aux religions (prophète, messie, textes sacrés...) et à leurs institutions (clergé, rituels et prescriptions diverses...). Ceux-ci voient en effet les religions comme de pures constructions sociales et culturelles qui auraient surtout pour fonction historique d'assurer la cohésion et l'ordre dans les sociétés humaines traditionnelles via par exemple la menace de l'enfer, la promesse du paradis ou encore la notion de péché ou par le mécanisme du bouc émissaire. En d'autres termes, les religions, aux yeux d'un agnostique, seraient bien trop « humaines » de par leurs modes de fonctionnement et de par les dynamiques anthropologiques sur lesquelles elles se basent (soutien psychologique face à la mort, analogie très anthropocentrique d'un dieu bâtisseur de l'univers...) pour qu'elles aient un quelconque lien direct avec toute forme d'intelligence supérieure, tout en n'excluant pas non plus pour certains le fait que ce soit malgré tout possible. D'où cette interrogation constante propre à l'agnostique.

Les termes suivants sont proches, mais néanmoins distincts, de l'agnosticisme :





Ainsi le déisme est bien un théisme même s'il peut, pour certains, incorporer une certaine notion de scepticisme. Bertrand Russel et Richard Dawkins, tous les deux non-croyants, ne prétendent pas non plus que la question de l'inexistence (ou l'existence) est connaissable. Car si toute croyance n'est pas forcément une connaissance, toute connaissance est, notamment, une croyance (la connaissance est classiquement définie comme croyance vraie et justifiée). Comme d'autres, ils estiment ainsi l'agnosticisme compatible avec l'athéisme et même avec le théisme, puisque, notamment, là où l'agnostique ne se prononce pas c'est sur la connaissance de dieu. Reconnaître cette position ne nécessite d'ailleurs pas d'être agnostique et athée. Les agnostiques peuvent ainsi, bien que pas nécessairement, s'opposer aux croyants, considérant probable ou certaine et connaissable l'existence de telles divinités, ou s'opposer aux athées estimant que l'improbabilité ou l'impossibilité atteint le rang de connaissance. Ainsi certains agnostiques se disent athées, d'autres théistes alors que d'autres se disent ni l'un, ni l'autre. Si le degré de scepticisme varie selon les individus, il est premièrement intéressant de remarquer que ceux qui affirment connaître (gnosis ou γνῶσις signifiant l'action de connaître en grec ancien) peuvent, paradoxalement, en théorie avoir un niveau de doute plus élevé que l'agnostique étant donné que pour certains la certitude totale n'est pas nécessaire à la connaissance ou réciproquement pour certains agnostiques un niveau très élevé de certitude n'est pas suffisant pour qu'une croyance atteigne la valeur de connaissance. Et que deuxièmement, les agnostiques s'accordent pour dire qu'il n'existe pas de preuve définitive (assez pour qu'une croyance atteigne la valeur de connaissance) en faveur de l'existence ou de l'inexistence du divin, et affirment l'impossibilité de se prononcer en matière de connaissance et même parfois de croyance bien que le fait qu'une connaissance est un cas particulier de croyance soit classiquement admis en philosophie même après Edmund Gettier.

Le terme « agnosticisme » (parfois incorrectement écrit par une fausse étymologie), vient du grec "αγνωστικισμός, agnôstikismós", lui-même tiré de "agnôstos" (ignorant), la "gnôsis" étant la connaissance ; il désigne la privation de connaissance ou l'impossibilité de connaître ce qui dépasse l'expérience. Il s'agit donc d'une position plutôt épistémologique qui met éventuellement en question la légitimité de la métaphysique, de la révélation, de la divination, etc. L'agnosticisme n'est pas à confondre avec une opposition systématique et spécifique au gnosticisme, qui est une doctrine liée aux débuts du christianisme, mais elle a un sens beaucoup plus général. Antérieurement au christianisme, le mot « agnostique » désignait une personne qui n'avait pas été initiée à la , c'est-à-dire une croyance mystique en un .

Le mot a été forgé en 1869, dans une intention , par Thomas Henry Huxley (1825-1895) pour signifier : Il voulait que le terme fasse comprendre que la métaphysique est ; comme le pensait déjà le philosophe empiriste David Hume qui recommande, à la fin de son , de jeter aux flammes les livres de théologie ou de métaphysique scolastique.

C'est là la position de ce qui peut être nommé l’APP, ou Agnosticisme Provisoire en Pratique. L'APP estime que si un ou des dieux ont le monde, ils l'ont fait en cohérence avec les principes qui régissent , de manière que leur œuvre (donc, l'univers) soit par rapport à leur intention, et surtout, que d'éventuelles interventions divines postérieures à cette initiale (illustrées par exemple, par , , ) soient envisageables. De cette manière, on peut supposer que l'existence d'une divinité reste à la portée de notre Raison, et cette hypothétique existence constitue donc une question que la Science pourra éventuellement résoudre un jour, notamment par le moyen de l'étude de ses éventuelles interventions sur notre Terre. En attendant, les partisans de cet agnosticisme peuvent établir des probabilités sur l'existence de(s) dieu(x) en se basant sur les seuls éléments de preuves accessibles pour l'instant (récits, miracles, fossiles...), et en confrontant les arguments des diverses positions. L'APP prendra fin quand sera apportée à la question du divin une réponse scientifiquement irréfutable.
Cette phrase précédente synthétise le courant agnostique qu'est l'ADP, ou . Cette thèse s'appuie sur certains phénomènes et paradoxes que la Science, la logique se révèlent actuellement incapables d'expliquer, mais surtout sur l'idée que l'Humanité vivant sur la planète Terre ne représente qu'une part infime de l'univers, et même tellement infime qu'elle ne sera jamais en mesure de l'appréhender totalement et de prouver l'existence ou non de(s) dieu(x). Aussi, si un ou des dieux avaient créé le monde, ses intentions ne devraient guère se concentrer sur nous, et donc notre Esprit ne peut absolument pas être le reflet du sien ou des leurs. Plus on se rend compte de la complexité du monde dans lequel nous vivons, plus le(s) Créateur(s) supposés en être à l'origine doivent être complexes et puissants comparés à nous, et moins alors il devient probable que l'Humanité bénéficie d'une attention divine particulière (et encore moins donc un individu). La notion de preuves devient ainsi complètement ridicule : même si un croyant argue des miracles décrits dans ses textes sacrés (et même si un prophète en faisait la démonstration), il pourra toujours lui être objecté qu'il s'agit d'une technologie inconnue ou magie qui n'a pas nécessairement de lien avec une divinité. L'ADP insiste sur de l'Homme se croyant capable de répondre à la question de l'existence de(s) dieu(x). Pour les tenants les plus extrêmes de l'ADP, la question de l'existence de(s) dieu(x) est extra-rationnelle et ne peut donc faire l'objet d'une étude rationnelle et, pour ce motif même, elle ne peut point être discutée.

En pratique, les tenants de l'APP auront plutôt tendance à regarder les religions et leurs "témoignages" avec le même scepticisme que d'autres preuves plus scientifiques ; leurs convictions étant ouvertes à l'arrivée de toutes preuves infirmant ou confirmant l'existence de Dieu. En revanche, si les tenants de l'ADP partagent la même indécision quant à l'existence d'une existence supérieure (au-delà de l'aspect 'rationnel'), ils auront tendance à rejeter totalement et définitivement tout caractère sacré des religions (clergé, livres sacrés, miracles mis en avant dans la liturgie...). Et ce, à la fois parce qu'ils considèrent ces institutions comme de pures constructions sociales mais aussi parce que, pour eux, l'Univers est si immense, si complexe et nos capacités de perception et de compréhension si limitées, que postuler une intervention divine sous la forme de messie ou de prophète est une absurdité, qui devrait par elle-même nous rappeler le caractère humainement construit et non divinement révélé des religions. Il faut donc bien distinguer au sein du débat agnostique la question de l'existence d'une intelligence supérieure, de celle du caractère transcendantal ou non des religions et des institutions religieuses humaines. 

N.B. : Attention, le refus de se prononcer, dans l'ADP, n'implique pas une mise en équiprobabilité des hypothèses d'existence et d'inexistence de Dieu. On parlera plutôt dans ce cas d'un APP parfaitement neutre et impartial, car quand bien même il s'affirmerait aussi sceptique que l'ADP face à toute éventuelle preuve à venir sur la question, à partir du moment où sont fixées des probabilités, il fait l'hypothèse d'un dieu intra-rationnel, c'est-à-dire qui peut être appréhendé par la raison.

L'agnostique peut choisir par convention sociale de s'affilier, malgré tout, à une croyance religieuse, alors qu'il n'a ni la certitude de l'existence d'une intelligence supérieure ni le respect religieux du aux rituels et aux hommes d'Église ; mais cela lui évitera une éventuelle exclusion sociale, plus ou moins probable en fonction de la religiosité de son groupe social d'appartenance.

Certains agnostiques peuvent même se revendiquer ouvertement chrétiens ou musulmans dans une logique d'affirmation identitaire et culturelle sans rapport avec la croyance religieuse. Et ceci par cynisme politique ou encore par croyance en une supériorité civilisatrice de sa religion. L'agnosticisme n'est pas lui-même un système unifié, et donc sujet à interprétation dans sa pratique.

La conception philosophique même de l'agnosticisme fait qu'un agnostique ne peut pas éprouver de à l'égard d'un croyant. L'agnostique peut toutefois être quant à certains préceptes religieux, et quant aux actions des fidèles qui revendiquent . Mais la plupart des agnostiques y sont totalement indifférents. L'agnosticisme n'est donc pas antithéiste. À l'inverse, toute tentative de prosélytisme à leur égard est mal perçue car nul ne peut prétendre apporter la preuve de l'existence de Dieu (en l'état actuel des connaissances de l'Homme ou à jamais, selon les individus). Un croyant croit autant en Dieu qu'un agnostique assume sa conception philosophique, même si ce dernier la considère comme plus objective. En fait, l'attitude d'un agnostique est surtout fonction du de sa position. Un partisan de l'APP aura tendance à être plutôt tolérant et compréhensif, car il les arguments des croyants, et reconnaît, plus ou moins, la possibilité de leur position. Tandis qu'à l'opposé, un partisan de l'ADP tendra vers une attitude plus critique, considérant les arguments religieux comme intégralement infondés et irrecevables, et n'affiche donc au mieux que de l'indifférence, si ce n'est, parfois, du mépris. Les plus radicaux en appellent d'ailleurs à une certaine restriction de l'activité publique des institutions religieuses, car ils estiment qu'elles ne devraient pas être autorisées à véhiculer des théories cosmogoniques infondées (aujourd'hui, ou, à jamais) en les présentant comme . L'agnosticisme est donc souvent attaché au concept de laïcité ; et, sans être antireligieux, il reconnaît souvent sa conviction comme étant plus ou moins teintée d'anticléricalisme.

En réalité, il faut savoir que l'opposition entre croyants et agnostiques concerne davantage la question de l'intervention de Dieu dans les affaires humaines que celle de son existence. La plupart des religions affirment tenir leur savoir de révélations par leur dieu, ce qui en fait une , hors de portée de l'analyse scientifique. Or, un agnostique tient d'abord compte des informations apportées par les sciences (c'est-à-dire les connaissances démontrées ou prouvées) et, malgré la difficulté pour elle d'étudier le domaine religieux (en vertu du principe du NOMA), la Science apporte, chaque jour, d'importantes informations fiables sur la nature de notre environnement et nous enseigne à relativiser la place de l'homme dans l'univers. L'écart observé tend à devenir tellement grand qu'il discrédite l'hypothèse de l'ingérence des dieux dans les affaires humaines, et donc aussi la plupart des révélations dont se prévalent les religions. Il est envisageable que le(s) dieu(x) des religions puissent être des entités de nature supérieure, mais il est invraisemblable qu'ils aient créé l'Univers en s'intéressant d'aussi près à l'humanité de la manière que cela est décrit dans les Écrits religieux, qui font presque toujours référence d'une part à la et et, d'autre part, à des interventions ponctuelles et localisées de leur(s) dieu(x). Il y a donc un problème de disproportion dans les rapports dieu(x)/Hommes tels que décrits par les religions. Par conséquent, l'agnosticisme tend plutôt à considérer les religions comme des constructions sociales et culturelles, qui auraient surtout la fonction de permettre la cohésion sociale (le mot « religion » vient entre autres du latin « "religare" »=relier : Relier Dieu et les hommes, mais aussi les hommes entre eux). En l'absence de preuves établies scientifiquement, l'agnosticisme soutient qu'on ne peut prendre au sérieux les affirmations des religions comme des indices objectifs de l'existence de(s) dieu(x).

L'agnosticisme adopte ainsi une attitude de envers les religions, du moins tant qu'elles respectent les droits fondamentaux de la personne humaine. L'annulation des sacrements ou assimilés (telle la débaptisation dans le Christianisme) n’est nullement nécessaire aux agnostiques, ces derniers n'attachent pas d’importance aux divers rites religieux. Les fêtes religieuses, comme Pâques, Noël, Yom Kippour, ou l'Aïd el-Kebir peuvent être tout aussi bien célébrées. Elles sont perçues, tout simplement, comme des fêtes traditionnelles. De même, un agnostique peut se rendre à l'intérieur des édifices religieux si bon lui semble afin, par exemple, d'y contempler l'architecture, ou pour des raisons de convention sociale. Il n'y a aucune interdiction ou doctrine liée au fait d'être agnostique, puisque l'agnosticisme ne suit, par définition, aucun , si ce n'est . C'est pourquoi l'agnosticisme peut se concilier avec une certaine pratique religieuse ; chacun étant libre, à défaut de certitude scientifique, de suivre sa foi, comme il lui plaira. En cela, l'agnosticisme rejoint l'adage de Blaise Pascal : Il faut donc plutôt assimiler l'agnosticisme à un courant de pensée philosophique qu'à une religion.

Ces religions sont les premières visées par la pensée de l'agnosticisme. Ce sont leurs conceptions de Dieu que l'agnosticisme a d'abord étudiées et à partir desquelles il a construit sa pensée. Néanmoins, on remarquera que les controverses sont presque toutes restées limitées au christianisme.

La plupart des penseurs et exégètes juifs considèrent que l'on peut arriver à Dieu par sa seule raison, l'exemple type étant le patriarche Abraham. Cependant, selon la Bible, Dieu fait des miracles pour que l'on croie en lui et en son omnipotence, notamment dans l'Exode.

Les relations entre le christianisme et l'agnosticisme sont faites à la fois de confrontations et de tolérances. L'exemple le plus évocateur en étant le débat qui eut lieu entre les représentants des Églises et les défenseurs de la théorie de l'évolution de Charles Darwin. Après quelques débats passionnés, l'Église catholique reconnut la plausibilité de la théorie (en admettant qu'Adam et Ève pouvaient être des symboles) d'abord comme hypothèse, puis comme "davantage qu'une hypothèse". Il n'en existe pas moins dans quelques milieux protestants une persistance du créationnisme, entre autres chez les mouvements évangéliques américains, qui est en totale opposition avec la conception philosophique de l'agnosticisme. En effet, le créationnisme se propose de donner une valeur scientifique à des affirmations purement dogmatiques (comme la Création du monde en une semaine), relevant de la foi et non d'une démarche inductive et rationnelle, comme le relève la position agnostique. L'agnosticisme se montre alors d'autant plus critique que le se rend coupable de confusion entre foi et empirisme (rompant ainsi avec le NOMA).

Le Coran condamne les ainsi que les , nommés les , mais pas spécifiquement les agnostiques. Concrètement, l'histoire de la théologie musulmane est jalonnée de doutes : au , Burzoe, ministre du roi sassanide Khosro, exprime ses doutes concernant la vérité des religions de son époque, soupçonnant leurs enseignements d'être vides de sens, et considérant les croyants comme les victimes d'une illusion. Cette pensée a influencé très tôt l'Islam, initiant une tradition de libre-pensée et de littérature sceptique qui a conduit au scepticisme des missionnaires ismaëliens, ainsi qu'à celui de Al-Ghazâlî au . Cependant, le doute, en Islam, ne porte pas sur l'existence même de Dieu, mais sur la définition d'une pratique de son culte sur Terre. Si comme le pensent des théologiens de plusieurs religions et quelques philosophes comme Platon ou Plotin le sentiment du dieu unique est inné en la nature humaine, il n'y a pas besoin de preuve de son existence ; les révélations ne concernent alors que les modalités du culte à lui rendre, par gratitude d'abord, et accessoirement pour obtenir une éventuelle rédemption dans la vie éternelle.

Toutefois, il reste possible que, dans la recherche intellectuelle de Dieu, le doute quant à son existence même soit temporairement toléré par l'islam (soit dans le cadre de la pensée spéculative, soit dans un moment de désarroi), mais la condition reste d'aboutir, finalement, au monothéisme définitif, et donc la reconnaissance soumise à Allah par le biais des enseignements attribués à Mahomet son prophète. De ce fait, ce sont plus particulièrement l'athéisme et l'ADP, c'est-à-dire le refus inébranlable de reconnaître Allah, qui sont absolument condamnés. Mahomet a conseillé ses disciples (, compagnons) de ne pas se polariser sur des questions qui les dépassent. Selon le livre , Mahomet conseille à ses compagnons : . Concrètement, cette idée a été reprise par les théologiens musulmans rationalistes sous la forme d'un , qui affirme l'existence d'un fossé infranchissable entre Dieu et sa création qu'il transcende, rendant impossible toute prédiction ou connaissance à son sujet. En cela, ils rejoignent partiellement l'ADP, qui préconise de délaisser totalement les réflexions sur le divin. Cet agnosticisme partiel, en rappelant les limites conceptuelles humaines, s'accorde sur ce point avec les religions dites de la révélation. En effet, la révélation ne se définit pas comme un phénomène objectif librement observable par tous, mais comme une divine seulement adressée à une minorité très restreinte d'humains ayant la chance d'être élus pour recevoir des révélations inaccessibles humainement, par la grâce de Dieu. Hors de la révélation, personne n’a le droit dans cette optique d’affirmer - "a fortiori" imposer - quelque interprétation que ce soit concernant des questionnements dépassant en principe l'entendement humain ; de sorte que l'on ne pourrait rien attendre de l'analyse scientifique et rationnelle sur ces sujets.

Certaines traditions religieuses réfutent la croyance en un dieu créateur. Parmi les religions non théistes, sont encore pratiquées de nos jours le bouddhisme et le jaïnisme, tandis que le courant athée du Sâmkhya ne le serait plus. Ces trois points de vue non théistes partagent la même croyance dans la loi du karma, loi de causalité correspondant à ce qui pourrait être attribué à un créateur dans le contexte théiste. Ces religions sont plus difficiles à appréhender pour l'agnosticisme car elles ne font pas référence expressément à une ou des divinités. Or, si l'agnosticisme s'intéresse à toute religion, il se penche plus sur à travers les faits allégués par les religions que sur leurs contenus dogmatiques, d'où dérivent les rites et traditions (qui ont un aspect d'abord et surtout social). Cependant, les religions au sens large, ont toutes en commun de proposer une morale dont elles présentent souvent l'origine comme transcendante à l'Homme, c'est-à-dire divine. La vision agnostique consiste toujours à douter de la divinité de cette .

Le bouddhisme, même s'il ne vénère pas forcément de dieu, propose néanmoins une cosmogonie (une organisation du monde) et une vision de la vie après la mort, c'est-à-dire des affirmations fondées uniquement sur des , ce qui peut en faire une . Le concernant, l'agnosticisme s'intéresse surtout à la notion de karma, fondamentale dans la pensée bouddhiste, selon laquelle tous les êtres vivants sont pris dans un cycle de réincarnations perpétuelles, dans lequel ils ne peuvent que par la réalisation d'actions vertueuses, et cela, dans l'espoir d'échapper à ce cycle des réincarnations pour atteindre le Nirvāna. L'agnostique pourrait remarquer que si la progression des individus est conditionnée par la valeur de leurs actions, cela signifie qu'il existe (une entité transcendante) définissant le Bien et le Mal, et régulant les parcours des individus en fonction de la proportion de Bien et de Mal présents dans leurs actions. Le point de vue agnostique consiste à mettre en doute l'existence de ces entités transcendantes, et moins à contester l'idée de cycle de réincarnation (difficilement formalisable en l'absence de définition claire de l'identité là où la mémoire ne subsiste pas), ainsi que les idées de hiérarchisation et de progression des êtres telle qu'elles sont définies dans beaucoup de variantes du bouddhisme, compte tenu des éléments de connaissance actuelle (avec une plus ou moins grande perspective de recueillir de nouveaux éléments selon les points de vue). Quant à l'affiliation, elle reste affaire de convention sociale, comme indiqué plus haut.

L’agnosticisme est, à l’origine, en opposition aux religions, induisant le doute sur la connaissance de(s) dieu(x), dans le sens où il doute de son(leur) existence, avant de douter de son(leur) inexistence. Il va donc, dans un premier temps, dans le même sens que l’athéisme. Cependant, à partir du moment où l’athéisme affirme l’inexistence de(s) dieu(x), l’agnosticisme ne peut encore aujourd’hui, ou à jamais, le suivre, en l’absence de preuves suffisantes. Inversement, il ne peut pas suivre non plus les diverses formes de déismes, qui affirment l’existence d’un être suprême, d’un(de) dieu(x) indéfinissable(s), dans le sens où nulle personne ou nul mouvement religieux ne peut se prétendre être le dépositaire exclusif de sa volonté ; car eux aussi affirment sans preuves. Dans les deux cas, aucune certitude n’est établie, car il n’existe encore ou n’existera jamais aucun fait reconnu et établi scientifiquement qui permettrait de statuer sur la question. Quant aux démarches de purs raisonnements formulés par les deux partis, ils sont inutiles car impuissants à prouver quoi que ce soit, car ils ne relèvent que de la « Raison pure », et ne peuvent pas, de toute façon, surpasser la valeur, déjà insuffisante, de l’argument ontologique.

Cependant, cette position ne relève pas exclusivement de l'indifférence religieuse (comme c'est le cas de l'apathéisme), car l'agnosticisme reconnaît malgré tout l'impact que pourrait avoir l'existence d'une divinité, ne serait-ce qu'en termes d'eschatologie (ce sont peut-être l'Au-delà et l'éternité qui sont en jeu). Selon les degrés de scepticisme, les partisans restent plus ou moins attentifs à l'arrivée de tout nouvel élément sur la question.
Concrètement, du moins dans l'APP, il n'y a pas réellement d'agnostiques qui accordent personnellement une valeur égale aux deux hypothèses. On parlera d'agnostiques athées pour ceux qui penchent en faveur de l'inexistence de(s) dieu(x) et d'agnostique théistes pour ceux qui penchent en faveur de son(leur) existence.

Un reproche récurrent contre les agnostiques est que leur philosophie consiste à demeurer dans l'indécision, le compromis de l'entre-deux, le "mol oreiller du doute" mentionné par Montaigne dans ses "Essais". En refusant de prendre position sur un sujet aussi sensible que le divin, ils chercheraient à ne se fâcher avec personne. La plupart des religions y voient d'ailleurs un réservoir d'incroyants à convertir, tandis que les athées les considèrent comme coincés dans une progression inachevée vers l'athéisme, voire le qualifient d'« athéisme faible », comme le reste des irréligions, d'ailleurs. Cette raillerie contre une posture dite de l'« hésitation » simplifie évidemment le non-choix agnostique, qui se rapproche plutôt d'une forme de sagesse de la prudence face à l'ignorance. Selon l'expression de Bertrand Russell dans ses : .

Selon Friedrich Engels, dans l'introduction anglaise de Socialisme utopique et socialisme scientifique, l'agnosticisme est un matérialisme , c'est-à-dire que l'agnostique comme Thomas Huxley a honte de son matérialisme et rejette dès lors moralement le matérialisme, ainsi pour le lecteur anglais de la fin du , .

639 millions de personnes dans le monde se déclarent agnostiques d'après l"'Atlas des Religions" de La Vie et Le Monde et de l'étude "Atlas of Global Christianity 2010" réalisée par Todd M. Johnson et Kenneth R. Ross de l'Université d'Édimbourg. Un sondage de l'institut Harris Interactive, publié par le Financial Times, daté de décembre 2006, dénombre 32 % d'agnostiques en France soit autant que d'athées soit environ d'adeptes en France en 2013.

Protagoras est resté célèbre pour son agnosticisme avoué et un certain relativisme.

D'éminents artistes, intellectuels et scientifiques se sont revendiqués agnostiques. On peut par exemple citer : Blaise Cendrars, Charles Darwin, Émile Durkheim, Thomas Edison, Albert Einstein, Charlie Chaplin, Carl Sagan, Marie Curie, Thomas Henry Huxley, Claude Bernard, Émile Littré, Clarence Darrow.




</doc>
<doc id="354" url="https://fr.wikipedia.org/wiki?curid=354" title="Code Morse international">
Code Morse international

Le code Morse international ou l’alphabet Morse international, est un code permettant de transmettre un texte à l’aide de séries d’impulsions courtes et longues, qu’elles soient produites par des signes, une lumière, un son ou un geste.

Ce code est souvent attribué à Samuel Morse, cependant plusieurs contestent cette primauté, et tendent à attribuer la paternité du langage à son assistant, Alfred Vail.

Inventé en 1832 pour la télégraphie, ce codage de caractères assigne à chaque lettre, chiffre et signe de ponctuation une combinaison unique de signaux intermittents. Le code morse est considéré comme le précurseur des communications numériques.

Le morse est principalement utilisé par les militaires comme moyen de transmission, souvent chiffrée, ainsi que dans le civil pour certaines émissions à caractère automatique : radiobalises en aviation, indicatif d’appel des stations maritimes, des émetteurs internationaux (horloges atomiques), ou bien encore pour la signalisation maritime par certains transpondeurs radar et feux, dits « à lettre morse » (par exemple, la transmise par un tel feu sous la forme codice_1 signifie « eaux saines »). Le morse est également pratiqué par des amateurs comme de nombreux radioamateurs, scouts (morse sonore et lumineux), plongeurs ou alpinistes (morse lumineux), par des joueurs pour résoudre des énigmes, ainsi que comme sonnerie par défaut de réception de message pour les téléphones portables de marque Nokia (« SMS SMS » en morse).

Le code peut être transporté via un signal radio permanent que l’on allume et éteint (onde continue, généralement abrégé en CW, pour en anglais), ou une impulsion électrique à travers un câble télégraphique (de nos jours remplacé par d'autres moyens de communication numérique), ou encore un signal visuel (flash lumineux). L’idée qui préside à l’élaboration du code morse est de coder les caractères fréquents avec peu de signaux, et de coder en revanche sur des séquences plus longues les caractères qui reviennent plus rarement. Par exemple, le « e », lettre très fréquente, est codé par un simple point, le plus bref de tous les signes. Les lettres sont toutes codées sur quatre signaux au maximum, les chiffres sur cinq signaux. Les séquences plus longues correspondent à des symboles les plus rares : signes de ponctuation, symboles et caractères spéciaux.

Parallèlement au code morse, des abréviations commerciales plus élaborées ont été créées codant des phrases complètes en un seul mot (groupe de ). Les opérateurs de télégraphie conversaient alors en utilisant des mots tels que "BYOXO" ('), "LIOUY" (') et "AYYLU" (""). L’intention de ces codes était d’optimiser le coût des transmissions sur les câbles. Les radioamateurs utilisent toujours certains codes appelés et . Ils sont utilisés par les opérateurs afin de s’échanger des informations récurrentes, portant par exemple sur la qualité de la liaison, les changements de fréquences et les télégrammes.

Les premières liaisons radiotélégraphiques sans fil utilisant le code morse datent du début du . En 1903, la conférence de Berlin attribue la longueur d’onde de () au trafic en radiotélégraphie morse en mer et officialise en 1906 le signal SOS comme appel de détresse. Jusqu’en 1987, plusieurs conférences mondiales des radiocommunications définissent les bandes à utiliser pour les communications en télégraphie morse. 
Depuis le , dans le cadre du SMDSM 1999, les services maritimes côtiers et mobiles de France et de nombreux autres pays ont abandonné la veille radiotélégraphique obligatoire et cessé les émissions en morse, notamment sur la fréquence de (maritime et aéronautique) et sur la fréquence de , affectées au trafic de détresse ou d’appel en radiotélégraphie, depuis les , un système de satellites de télécommunication ayant pris le relais. À partir de ce moment, le trafic maritime radiotélégraphique et radiotéléphonique utilisant les ondes hertziennes commence à décliner lentement. Cependant, il existe encore à ce jour (2010) des fréquences internationales affectées par l’UIT à la diffusion de l’heure, de la météo marine ou aux communications maritimes en radiotélégraphie (parmi d’autres, à, ou à pouvant aussi être utilisé par l’Aviation civile). La bande des notamment reste utilisée par une vingtaine de pays dans le monde, parmi lesquels : l’Arabie saoudite, l’Argentine, l’Azerbaïdjan, le Cameroun, la Chine, la République du Congo, Djibouti, l’Érythrée, les États-Unis, l’Indonésie, l’Italie, l’Irlande, Oman, la Roumanie, la Fédération de Russie, les Samoa américaines et les Seychelles. À quelques exceptions près, la plupart des stations maritimes encore en activité n’émettent plus en morse que leur indicatif d’appel et éventuellement leur fréquence d’émission. Aujourd’hui, certaines fréquences destinées au trafic en CW de la marine marchande ont encore une affectation, même si elles ne sont plus utilisées que par quelques pays et très rarement.

Depuis le début du et l’invention de la lampe Aldis, les bateaux peuvent également communiquer en morse lumineux. Alors que la capacité à émettre de tels signaux reste exigée pour devenir officier de la marine marchande dans de nombreux pays, dont la France, cette pratique a tendance à devenir rare et ne se retrouve plus que dans la marine de guerre et chez certains plaisanciers.

Les premières liaisons radiotélégraphiques aéronautiques remontent au début du et ont cessé avant les , à une époque où les ballons dirigeables et les avions communiquaient en radiotélégraphie dans la bande aéronautique des (), en vol au-dessus des mers et des océans dans la bande marine des (), sur la longueur d’onde de radiogoniométrie de () et jusqu’en 1930 pour un échange de correspondances transcontinental radiotélégraphique au-dessus des océans dans la bande des ().
En vol une antenne pendante longue de à était déroulée pour établir les communications radiotélégraphiques sur ces longueurs d’ondes. À l’extrémité de l’antenne pendante un plomb de lest porte l’indicatif radio de l’aéronef.
Une autre antenne tendue le long de la coque de l’aéronef était pour établir ("à courte distance") les communications radiotélégraphiques en vol et au sol sur la longueur d’onde de () et dès 1930 pour établir les communications radios NVIS.

Les fréquences utilisées autrefois par l’aviation pour les communications (notamment celles voisines de ) sont aujourd’hui attribuées aux radiobalises de type NDB qui émettent des signaux radiotélégraphiques automatisés (indicatif composé de deux à trois lettres, transmis en morse à intervalles réguliers). L’aviation utilise également la sous-bande VHF pour d’autres types de radiobalises (systèmes VOR et ILS) qui transmettent également leurs indicatifs (de ) en morse. Pour ce qui est des communications radiotéléphoniques, elles s’effectuent de nos jours sur les bandes VHF pour le trafic local, et HF pour le trafic transcontinental ou transocéanique.

Dans certaines circonstances, la radiotélégraphie présente des avantages par rapport à la radiotéléphonie : par exemple, en cas de fort parasitage, il est plus aisé de reconnaître les signaux codés en morse que ceux, beaucoup plus complexes, transmis par la voix. Également, la radiotélégraphie s’avère être un moyen de communication plus discret que la radiotéléphonie qui demande de prononcer les mots hautement et clairement. Pour ces raisons, la plupart des armées dans le monde forment des officiers radio maîtrisant la télégraphie et disposent de fréquences réservées par l’UIT.

Il arrive également que les navires de guerre, s’ils sont suffisamment proches, utilisent le morse lumineux appelé le Scott pour communiquer à l’aide d’un projecteur, d'un feu de mâture visible sur tout l'horizon (FVTH) ou d'une Lampe Aldis. C’est par exemple le cas lorsqu’ils sont contraints d’observer une période de silence radio.

Les radioamateurs utilisent assez fréquemment le code morse pour les communications de loisir en radiotélégraphie et jouissent à cet effet de fréquences allouées par l’UIT.

Jusque dans les années 1990, pour obtenir la licence de radioamateur aux États-Unis (de la FCC), il fallait être capable d’envoyer encodés en morse par minute. La licence avec le plus de droits exigeait par minute. L’épreuve actuelle de lecture au son à l’examen (jusque dans les en France, uniquement pour la de radioamateurisme) requiert une vitesse minimum de par minute. Les opérateurs radio militaires et radioamateurs entraînés peuvent comprendre et enregistrer jusqu’à par minute.

Le Règlement des radiocommunications (RR) se compose de règles liées au service de radio amateur. Il est révisé tous les trois ans à la Conférence mondiale des radiocommunications (CMR). La révision de l’ du Règlement des radiocommunications à la Conférence de 2003, en particulier, a supprimé l’exigence de connaissance du code Morse à l’utilisation des fréquences inférieures à . Cela affecte la plupart des pays, mais certains (dont la Russie) continuent (en 2008) à l’exiger.




Deux types de code morse ont été utilisés, chacun avec ses particularités quant à la représentation des symboles de l’anglais écrit. Le code morse américain a été utilisé dans le système télégraphique à l’origine de la première télécommunication à longue distance. Le code morse international est le code le plus communément utilisé de nos jours .

C’est en 1838 que Friedrich Clemens Gerke crée un alphabet « morse » très proche de celui que nous connaissons actuellement. Il s'agit d'une modification du code morse originel, plus tard appelé code morse américain. Auparavant, certains espaces étaient plus longs que le point à l'intérieur même d'un caractère, ou le tiret pouvait être plus long, comme pour la lettre L. Gerke simplifie le code en n'utilisant plus que deux longueurs standards, le point et le tiret.

Deux types d’impulsions sont utilisés. Les impulsions courtes (notées « codice_2 », point) qui correspondent à une impulsion électrique de de temps et les longues (notées « codice_3 », trait) à une impulsion de de temps, les impulsions étant elles-mêmes séparées par de temps (l’unité de temps élémentaire étant alors voisine de la seconde pour la manipulation et l’interprétation humaine).

Alors que se développent de plus en plus de variantes du code Morse dans le monde, l'ITU adopte en 1865, comme code morse international, l'alphabet morse de Gerke avec quelques modifications. Il sera rapidement utilisé en Europe. Les compagnies de (radio)télégraphie américaines continueront à utiliser le code originel, qui sera alors appelé code morse américain.

Le code morse international est toujours utilisé aujourd’hui (certaines parties du spectre radio sont toujours réservées aux seules transmissions en morse). Utilisant un simple signal radio non modulé, il demande moins d’équipement pour envoyer et recevoir que d’autres formes de communications radio. Il peut être utilisé avec un bruit de fond important, un signal faible et demande très peu de bande passante.

On utilise deux symboles « positifs », appelés point et trait (ou « ti » et « taah »), et deux durées d’espacement, la coupure élémentaire entre signaux et l’espace séparant les mots. La durée totale d’émission d’un trait (y compris la coupure élémentaire entre signaux) détermine la vitesse à laquelle le message est envoyé, elle est utilisée en tant que cadence de référence. Un message simple serait écrit (où « ▄ » représente « ti » et « ▄▄▄ » représente « taah ») :

Voici la cadence du même message (« = » signifie « signal actif », « · » signifie « signal inactif », chacun ayant pour durée un « ti ») :
Conventions de cadence :

Les personnes familières du morse écriraient donc « code morse » ainsi : codice_4 et le prononceraient « "taahtitaahti taahtaahtaah taahtiti ti, taahtaah taahtaahtaah titaahti tititi ti" ».

Il existe d'autres formes de représentation, la représentation compressée, par exemple, qui associe au « ti » un point en bas, et au « taah » un point en haut ou encore le morse en dents de scie.

Les opérateurs composent des messages en morse à l’aide de "manipulateurs".



La vitesse de manipulation s’exprime en mots par minute, et varie d’une dizaine de mots par minute pour un débutant ou une identification d’émetteur compréhensible par tous, à par minute ou plus pour un manipulateur expert. Le record est détenu par Ted McElroy qui atteint le score de par minute au championnat mondial de 1939, à Asheville.

Il existe également des générateurs informatiques automatiques, qui sont généralement couplés avec des décodeurs automatiques.

Voici quelques tables récapitulant l’alphabet morse et quelques signes communément utilisés.

Note : le symbole « @ » a été ajouté en 2004. Il combine le A et le C en un seul caractère.

Abréviations et signaux divers à employer dans les radiocommunications du service mobile maritime.
Une erreur fréquente est de considérer le code de détresse international comme la succession des lettres « S O S » et de l’envoyer en tant que tel (=·=·=···===·===·===···=·=·=). La bonne façon de l’envoyer est en enchaînant les comme s’ils formaient une seule lettre (=·=·=·===·===·===·=·=·=).

Lorsque étendre l’alphabet morse à d’autres lettres ne suffit pas, on recourt à d’autres codes.

Ainsi, le code wabun est utilisé pour transmettre du texte en japonais. Les symboles représentent des kana syllabiques.

En Chine, un autre système était utilisé, le .

Cette méthode a été inventée par un psychologue allemand, Ludwig Koch, dans les . C'est une des méthodes permettant un apprentissage rapide du morse.

Cette méthode considère que:

La méthode Koch nécessite un ordinateur (équipé d'un logiciel spécifique) ou un professeur pour pouvoir écouter du code. En commençant tout de suite avec une vitesse supérieure à , elle permet d'apprendre à écouter du code morse correct, et non déformé par une vitesse faible. Elle permet aussi la reconnaissance des caractères par réflexe et sans phase de réflexion (ce qui est de toute façon impossible à une telle vitesse, et aux vitesses supérieures).

Dans les méthodes « traditionnelles », on apprend l'ensemble de l'alphabet et on pratique à une vitesse faible, par exemple, 5 mots/min. Avec la méthode Koch, on commence par reconnaître seulement , puis 3, puis 4, … mais une vitesse d'au moins . Cela évite les frustrations du « plateau des » des méthodes « traditionnelles ».

On utilise traditionnellement cet ordre pour les caractères : K, M, R, S, U, A, P, T, L, O, W, I, ".", N, J, E, F, 0, Y, ",", V, G, 5, "/", Q, 9, Z, H, 3, 8, B, "?", 4, 2, 7, C, 1, D, 6, X, <BT>, <SK>, <AR>

Donald R. "Russ" Farnsworth propose dans sa méthode d'utiliser la vitesse cible pour l'apprentissage (commencer tout de suite à , par exemple) mais avec des espaces inter-mots et inter-lettres plus élevés que requis par la vitesse cible. Elle donne ainsi plus de temps à la compréhension de chaque signe, tout en utilisant une vitesse élevée dès le départ pour la reconnaissance des signes.

On peut d'ailleurs combiner la méthode Farnsworth avec la méthode Koch : en commençant à , avec , avec des espaces triples par rapport à la normale, par exemple.

Il existe différents moyens mnémotechniques assez simples pour apprendre les de l’alphabet en morse mais vu qu’ils induisent des ralentissements dans la compréhension des messages, il n’est pas recommandé de les utiliser pour apprendre le morse à l’oreille.

Dans le tableau ci-dessous, un mot est affecté à chaque lettre de l’alphabet. Ces mots se trouvent dans les et du tableau. Au cas où plusieurs mots possibles sont affectés à une lettre, il suffit d’en choisir un. Le procédé mnémotechnique consiste simplement à apprendre une liste de correspondant aux de l’alphabet.

Chaque mot traduit le codage morse de la lettre qui lui est associée. Pour chaque syllabe du mot on a un ▄ ou un ▄▄▄. Le ▄▄▄ sera représenté pour une syllabe à consonance « o » ou « on » et le ▄ pour toutes les autres syllabes.

Par exemple, pour la , le mot « psychologie » (Psy/cho/lo/gue) a ses centrales en « o » (cho/lo), les autres n’ont pas de consonance en « o » ou en « on ». Le code de la est donc codice_5 avec longs pour les centrales et aux extrémités pour les syllabes restantes.

Un autre moyen est d’utiliser les mots de la dernière colonne du tableau. Pour chaque lettre des mots on a un "ti" ou un "ta". Une consonne représente un "ta" et une voyelle un "ti".

Il existe une règle différente pour les lettres composées uniquement de points ou de traits. Il faut retenir les mots mnémotechniques :
La position de la lettre dans ces mots renvoie au nombre de traits ou de points.

Par exemple, le S est codé par car la lettre est en dans le mot « EISH »

Le code morse est facilement mémorisable à l’aide des codes courts et longs remplacés par des syllabes. Le code long (codice_3) remplacé par une syllabe en « o ». Le code court (codice_2) remplacé par une des autres voyelles. Par exemple, A = codice_1 = Al/lO (une syllabe en « a » pour le codice_2 et une syllabe en « o » pour le codice_3).

Pour l’utilisation de la méthode consonne-voyelle, toute consonne remplace un trait (codice_3) alors que toute voyelle signifie un point (codice_2). L’idéal étant de trouver un mot correspondant qui comprend la lettre ou le son et l’on obtient ainsi :
Il est aussi simple de mémoriser le S et le O grâce au fameux signal SOS : trois brèves, trois longues, trois brèves (codice_13).

Pour les personnes qui ont plutôt une mémoire visuelle, il est également possible de retenir l’alphabet morse en utilisant un arbre binaire :

Les lettres sont regroupées par 2, celle de gauche représentant un (codice_2) et celle de droite un (codice_3). Un symbole (*) est mis quand il n’existe pas de lettre correspondant au code de l’emplacement. Dans cet arbre, le « CH » et les chiffres ne sont pas représentés (car réduisant la lisibilité de l’arbre et ayant peu d’intérêt), mais il ne tient qu’au lecteur de les ajouter pour obtenir un arbre complet. Cela ajouterait une ligne et remplacerait le symbole (*) correspondant à (codice_16).

Pour retenir cet arbre, on peut se servir des groupes de lettres et les retenir dans l’ordre des lignes : ET/IA/NM/SU/RW… avec pour chaque groupe un moyen. On peut trouver ses propres moyens à partir de choses côtoyées tous les jours et abrégées, pour plus de facilité à le mémoriser. Sinon on peut reprendre ceux-ci :

Certaines personnes retiennent ces groupes de lettres en apprenant une phrase. Par exemple : « Encore très irritée après nos manigances sexuelles, Ursuline réimplora Wendy de kidnapper Gérard ou Hervé, violeurs fanatiques et libérés, en promettant-jurant buter X, ce yankee zélé quadragénaire. » Ici, chaque première lettre de chaque mot doit être prise en compte ; les mots « et » et « en » ayant pour but de combler les « trous » après les lettres « F » et « L ».

Une fois l’arbre mémorisé, il suffit alors de le parcourir et à chaque intersection de regarder si on passe par la lettre de gauche (un point) ou celle de droite (un trait). Par exemple :

L’avantage de cet arbre est de fonctionner dans les deux sens de transcription de morse vers lettre (partir d’en haut en suivant un trajet et aboutir à la lettre) et de lettre vers morse (trouver la lettre dans l’arbre et en déduire le trajet, donc le code, en partant du haut) avec beaucoup de facilité.

D’autres moyens existent, qui font appel à des phrases ou à des expressions permettant d’ordonner les signes en fonction de leurs valeurs. Par exemple :








</doc>
<doc id="355" url="https://fr.wikipedia.org/wiki?curid=355" title="Australopithèque">
Australopithèque

Les Australopithèques sont un genre éteint d'hominines ayant vécu entre environ 4,2 millions d'années (Ma) et 2 millions d'années avant le présent. Le genre "Australopithecus" (du latin "australis", « du sud », et du grec ancien "πίθηκος", "píthēkos", « singe ») a été défini par Raymond Dart lors de la découverte d"'Australopithecus africanus" en Afrique du Sud en 1924.

Les Australopithèques présentent à la fois des caractères archaïques (cerveau peu volumineux) et des caractères évolués (denture proche de celle du genre "Homo"). Leur locomotion est généralement mixte, et associe une forme de bipédie à une capacité à grimper encore marquée. La lignée humaine, probablement issue d'une forme gracile ancienne d'Australopithèque (peut-être "Australopithecus anamensis" ), apparaît entre 2 et 3 Ma, avec les premiers représentants du genre "Homo", bipèdes terrestres stricts, qui se déployent d’abord dans le reste de l’Ancien Monde (ca 2 Ma).

Les découvertes successives d'ossements fossiles dans plusieurs pays d'Afrique, les progrès réalisés dans la lecture des formules chromosomiques et dans la biologie du développement, qui relie le programme génétique aux modifications de formes des espèces au cours de leur évolution, permettent une meilleure compréhension de l'évolution des grands singes et de l'homme.

L'orang-outan, le gorille, le chimpanzé, le bonobo, et l'homme ont cinq chromosomes identiques hérités de leur ancêtre commun. Il y a environ d'années, l'ancêtre de l'orang-outan évolua indépendamment en Asie. L'existence d'un ancêtre commun aux trois autres lignées est prouvée par la présence de 11 chromosomes communs et de 7 chromosomes mutés. Il y a environ d'années, les ancêtres des hominines et des panines se séparèrent pour donner naissance aux préhumains et aux préchimpanzés.

Les hominines anciens du Miocène supérieur ("Sahelanthropus tchadensis" dont l'âge est estimé à , "Orrorin tugenensis" âgé d'environ , et "Ardipithecus kadabba" âgé de , tous probablement bipèdes et associés à des milieux boisés) ont donné naissance vers aux Australopithèques, "Australopithecus anamensis" étant dans l'état actuel des connaissances le plus ancien.

Jusqu'à aujourd'hui, les découvertes de fossiles d'hominines pré-Homo ont eu lieu exclusivement en Afrique. Leurs traces ont été relevées principalement dans la Vallée du grand rift, de l'Éthiopie au Malawi, ainsi qu'en Afrique du Sud.

Les vestiges les plus nombreux proviennent de Tanzanie : sites d'Olduvai, Laetoli, explorés par Louis Leakey ; d'Éthiopie : vallée de l'Omo, bassin de l'Awash ; et du Kenya : Kanapoï, Lothagam, alentours du lac Turkana (Koobi Fora, Ileret, Allia Bay), explorés par Richard Leakey, ainsi que par des équipes kenyo-américaines.

La région de l'Afar éthiopien a été le cadre, depuis 1974, de quelques-unes des plus importantes découvertes, dont le squelette complet à 40 % d'un individu féminin de l'espèce "Australopithecus afarensis", auquel on a donné le surnom de Lucy. En 1979, on a mis au jour dans la même localité d'Hadar un gisement contenant des morceaux de crâne, des dents, des mandibules, ainsi que des os du bassin et des os longs d'australopithèques datant de 3,2 millions d'années.

Non loin de là, dans le bassin du moyen-Awash, des paléoanthropologues ont retrouvé des restes d'Ardipithèques encore plus anciens (de 5,8 à 4,4 millions d'années).

En 1994, les premiers ossements de "Little Foot" sont découverts en Afrique du Sud. Les fouilles permettent de reconstituer à 90 % le squelette d'une australopithèque datée de 3,67 millions d'années

Le nombre élevé de restes fossiles dont on dispose aujourd'hui a permis de reconstituer plusieurs individus presque complets. De plus, la présence de restes de faune et de traces d'installation sur les terrains cités précédemment a autorisé la première reconstitution du mode de vie de nos « cousins éloignés ». Un point sur lequel les chercheurs convergent est celui du statut des Australopithèques : étroitement apparentés (biologiquement) au genre "Homo", ils s'en sont différenciés en donnant naissance à une branche collatérale. Jusqu'à environ 2 millions d'années avant le présent, ils se ramifient en différentes espèces qui conservent une architecture générale du crâne assez primitive (et, sous certains aspects, encore simienne), et qui vont finalement laisser la place aux Paranthropes, plus spécialisés.

Les Australopithèques possédaient la station bipède, mais ne possédaient pas une bipédie complète : ils se déplaçaient encore par brachiation arboricole à l'occasion. Leur marche bipède a cependant été confirmée par la découverte en 1978 par Mary Leakey, près de Laetoli (plaine du Serengeti), en Tanzanie, d'une double série d'empreintes de pas conservées depuis 3,7 millions d'années.

La structure des mains des australopithèques est proche de celle des humains. L'articulation de la première phalange du pouce ne permet pas tous les mouvements d'une main d'homme moderne. Cette structure analogue indique cependant que les australopithèques étaient peut-être capables de façonner des outils rudimentaires. On a longtemps pensé que les australopithèques n'auraient jamais été aptes à travailler la pierre comme leurs contemporains "Homo habilis" ou "Homo rudolfensis".

Il existait d'autres différences morphologiques avec les premiers "Homo" connus, divergences dont la signification fonctionnelle est encore aujourd'hui un objet d'études. Avant tout, la capacité crânienne de l'australopithèque reste faible (environ , alors que celle d"Homo habilis" atteint environ ), mais elle est cependant proportionnée à la masse corporelle de ces hominines dont la taille était de l'ordre de . La structure du crâne, fort proche de celle d"Homo habilis", conserve toutefois quelques caractéristiques propres aux panines (chimpanzés).

Les restes fossiles semblent indiquer que le genre "Australopithèque" serait l'ancêtre du genre distinct d'hominines appelé Paranthrope (ou « australopithécinés robustes »), et probablement du genre "Homo" qui comprend les hommes modernes. L'intelligence des Australopithèques n'a sans doute pas dépassé celle des grands singes modernes, mais la stature bipède est le caractère clé qui distingue ce groupe des hominidés qui l'ont précédé. "Australopithecus" est la preuve que l'apparition de la bipédie a largement précédé celle d'un cerveau plus volumineux et plus complexe. On débat toujours pour savoir comment la bipédie est apparue il y a plus de 7 millions d'années (plusieurs théories s'affrontent toujours). La bipédie avait notamment pour avantages de libérer les mains pour pouvoir attraper des objets, tandis que les yeux pouvaient mieux examiner au-dessus des grandes herbes pour trouver des sources d'aliments possibles ou repérer des prédateurs.

Les changements radicaux dans la morphologie sont survenus avant la séparation entre "Australopithèques" et "Homo". La structure du bassin et des pieds les distinguent en effet à peine des hommes modernes. Les dents présentent aussi le même alignement avec des petites canines. Pourtant, l'évolution vers "Paranthropus" a donné naissance à une denture plus grande et plus robuste. Les Australopithèques devaient faire face à un défi particulier en vivant dans la savane. Ils étaient les primates les plus lents à se déplacer de leur temps et beaucoup d'entre eux ont fini au menu des carnivores africains (comme les lions, et "Dinofelis" aujourd'hui éteint).

Si les Australopithèques n'étaient peut-être pas plus capables d'utiliser des outils que les grands singes modernes, on s'est rendu compte cependant que les chimpanzés utilisent des instruments simples : ils ouvrent des noix avec des pierres et ils introduisent de petites branches dans les termitières. On a plus récemment fait la même découverte avec les gorilles. Certains chercheurs pensent qu"Australopithecus garhi" aurait déjà fabriqué des outils en pierre, antérieurement aux premiers représentants connus du genre "Homo", "Homo habilis" et "Homo rudolfensis". Les restes retrouvés d"Australopithecus garhi" étaient en effet accompagnés d'outils et de restes d'animaux découpés, ce qui suggère le début d'une fabrication d'outils. Cela a conduit beaucoup de scientifiques à penser qu"'Australopithecus garhi" était peut-être l'ancêtre du genre "Homo". Cependant les paléoanthropologues et les chercheurs attendent d'autres découvertes pour déterminer quel était le véritable ancêtre du genre "Homo".

Une découverte faite en 2009 en Éthiopie apporte une preuve indirecte de l'utilisation d'outils par "Australopithecus afarensis" ou l'un de ses contemporains, qui reporterait à 3,4 millions d'années l'âge des plus vieux outils en pierre.

Selon certains chercheurs, "Australopithecus anamensis" (de 4,2 à 3,9 Ma) pourrait être l'ancêtre commun des australopithèques et du genre "Homo".
Cette hypothèse est contestée par d'autres, qui estiment que la séparation entre les deux genres pourrait avoir eu lieu à une époque antérieure.


Les espèces du genre "Paranthropus" sont parfois classées en tant qu"'Australopithecus", selon les nomenclatures :


</doc>
<doc id="356" url="https://fr.wikipedia.org/wiki?curid=356" title="Antoine Parmentier">
Antoine Parmentier

Antoine Augustin Parmentier, né le à Montdidier et mort le à Paris, est un pharmacien militaire, agronome, nutritionniste et hygiéniste français.

Précurseur de la chimie alimentaire et de l'agrobiologie, il est surtout connu pour son action de promotion en faveur de la consommation de la pomme de terre dans l'alimentation humaine, mais aussi pour ses travaux sur l'hygiène alimentaire, l'utilisation de nouveaux aliments durant les fréquentes périodes de famine et comme pharmacien militaire (avec un rôle éminent dans l'organisation pharmaceutique du service de santé sous l'Empire).

Antoine Parmentier est né dans une famille bourgeoise : il est le cadet des cinq enfants de Jean-Baptiste Parmentier ( - ) qui tient une modeste boutique de marchand linger dans l'artère principale de la bourgade commerçante de Picardie, et de Marie-Euphrosine Millon ( - ), fille d'épicier. Son père ayant connu des revers de fortune, l'éducation de ses enfants est assurée par son épouse, aidée du curé de la paroisse, l’abbé Daugy qui leur inculque le latin, langue indispensable pour le métier de pharmacien. Il entre en 1750 à Montdidier comme commis à la pharmacie Frison qui vient d'être reprise par un lointain cousin, Paul-Félix Lendormy, cet apothicaire le formant à la pharmacie. En 1755, recommandé par Lendormy, il devient apprenti à la pharmacie Simmonet, rue Croix-des-Petits-Champs à Paris et est logé chez son maître d'apprentissage Jean-Antoine Simonnet, Picard comme lui.

N'ayant pas les ressources pour ouvrir sa propre officine, il décide de s'enrôler dans l'armée qui a besoin d'apothicaires. En mars 1757, il est engagé par Louis Claude Cadet de Gassicourt, apothicaire-major à l'Hôtel des Invalides, et affecté en tant que pharmacien de troisième classe dans les hôpitaux de l’armée de Hanovre dirigée par le maréchal d’Estrées au cours de la guerre de Sept Ans. Pierre Bayen, chef de cette branche de service, remarque son activité, son intelligence et « son dévouement passionné pour ses devoirs » : il devient son ami et appelle sur lui l’intérêt de Chamousset, intendant général des hôpitaux. Dans une épidémie de dysenterie qui ravage l’armée, il donne des preuves de ses capacités. Il tombe cinq fois entre les mains de l’ennemi mais, l'armée manquant cruellement d'apothicaires, il est systématiquement libéré lors d'échange de prisonnier. Grâce à ses deux protecteurs Bayen et Chamousset, le Lieutenant-général des armées le duc de Choiseul lui fait monter les grades : pharmacien de deuxième classe en janvier 1758, de première classe (soit aide-major) en 1760.

La chimie est alors particulièrement pratiquée en Allemagne et Parmentier s’y applique sous les yeux de , pharmacien célèbre de Francfort-sur-le-Main. Il aurait pu devenir son gendre et son successeur mais il ne veut pas renoncer à son pays. En 1763, de retour à Paris, il suit les cours de physique de l'abbé Nollet dont il devient le préparateur, de chimie de Rouelle et de botanique de Jussieu. Le 16 octobre 1766, il emporte au concours la place d’apothicaire adjoint de l’hôtel des Invalides. Il reçoit son brevet d'apothicaire-major de la pharmacie des Invalides le 18 juillet 1772. Il passe sa maîtrise d'apothicaire gagnant-maîtrise le 28 mai 1774. Cependant, un conflit naît avec les « sœurs grises » (nom des Filles de la Charité) qui veulent garder leurs prérogatives dans la gestion de l'infirmerie et de l'apothicairerie des Invalides. Voulant probablement sortir de cette impasse, Parmentier remet sa charge d'apothicaire-major le 29 juillet 1774. En contrepartie, Louis XVI fait de Parmentier un pensionnaire du roi aux Invalides. Il y garde gratuitement un appartement et peut désormais se consacrer entièrement à ses recherches. Sa sœur Marie-Suzanne (14 avril 1736-10 décembre 1809) devenue veuve s'installe avec lui dans cet appartement, « à charge pour elle de tenir la maison, d'assurer le secrétariat, de participer aux recherches », les deux formant dès lors un duo indissociable.

En 1779, il est nommé censeur royal. Du 6 juin 1779 jusqu'en mai 1781, il est apothicaire-major des hôpitaux militaires de la division du Havre et de Bretagne lors de la guerre maritime de la France et de l'Angleterre.

C’est au cours de la guerre de Sept Ans comme prisonnier militaire en Allemagne que Parmentier goûte la bouillie de pommes de terre, et qu’il reconnaît les avantages alimentaires de ce tubercule. À Hanovre, il découvre notamment sa culture en ligne qui augmente sa productivité. En Europe, en dehors de l'Allemagne, elle est cultivée en Italie dès le , en Alsace et en Lorraine au , en Savoie dès la fin du XVIIe, puis est adoptée dans le Midi, en Anjou et dans le Limousin. Elle a souvent le nom de "truffole" (ou apparenté), en rapport avec son aspect et son origine souterraine. Elle est cependant repoussée par le nord de la France, dont l'Ile-de-France, d'où vient Parmentier.

À la suite des famines survenues en France en 1769 et 1770, l’académie de Besançon propose en 1771, pour sujet de son prix, l’indication des substances alimentaires qui pourraient atténuer les calamités d’une disette. Parmentier établit, dans un Mémoire qui est couronné, qu’il était facile d’extraire de l’amidon d’un grand nombre de plantes, un principe nutritif plus ou moins abondant. À l’issue de la publication de son mémoire, l’Académie des Sciences, des Belles-Lettres et des Arts le récompense, malgré une interdiction du Parlement (qui a autorité sur la plus grande partie du nord de la France) de cultiver la pomme de terre datant de 1748.

En 1772, les membres de la Faculté de médecine de Paris planchent pendant de longues semaines sur le sujet et finissent par déclarer que la consommation de la pomme de terre ne présente pas de danger. Mais le terrain sur lequel il avait installé ses plantations près des Invalides appartient aux religieuses de l'Institution et, en opposition avec celles-ci, il doit bientôt renoncer à les cultiver.

Il rédige plusieurs mémoires pour promouvoir les vertus nutritionnelles de la pomme de terre pour l’homme, alors qu’elle était jusqu’ici abandonnée aux bestiaux ou aux « jours maigres » des communautés religieuses (tubercule souterrain, il est en effet classé au plus bas de « l'échelle des êtres »), et pour démonter les préjugés communs sur ce tubercule accusé de provoquer des maladies (fièvre, lèpre, peste ou écrouelles) et l’appauvrissement du sol. La "Manière de faire le pain de pommes de terre, sans mélange de farine" en 1779 fait suite aux tentatives précédentes de Joachim Faiguet de Villeneuve et de François Mustel (l'agronome rouennais ayant développé la culture en Normandie, il accuse à cette occasion Parmentier de plagier son mémoire) de faire un pain à base de farine de pomme de terre et de froment, d'orge ou de seigle, Parmentier reprenant ces expériences pour en fabriquer uniquement à base de farine de pomme de terre mais le procédé est difficilement exploitable car il prend six jours. Il poursuit sa « croisade parmentière » en obtenant du gouvernement deux arpents de terres dans la plaine des Sablons, champ militaire réputé incultivable, pour planter des tubercules de pomme de terre le 14 mai 1786. Faisant un bouquet de quelques-unes de celles-ci, il le présente à Versailles en compagnie du botaniste Philippe Victoire Levêque de Vilmorin le 24 août 1786, veille de la fête de saint Louis, au roi Louis XVI, qui place de suite une fleur à sa boutonnière et une dans la perruque de Marie-Antoinette. L’exemple du monarque (qui rend hommage au savant en déclarant « La France vous remerciera un jour d’avoir trouvé le pain des pauvres ») entraîne les courtisans et ceux-ci le reste de la France.

Parmentier va aussi promouvoir la pomme de terre en organisant des dîners où seront conviés des hôtes prestigieux, tels Benjamin Franklin ou Lavoisier assistant, le 29 octobre 1778, devant les fours de la boulangerie de l'hôtel des Invalides, à l'enfournement du pain à base de farine de pommes de terre. Le novembre, tous les invités se retrouvent à la table du gouverneur des Invalides pour tester le pain et une vingtaine de plats. Bien que le résultat gustatif se révèle médiocre, le "Journal de Paris" relate l'événement comme « la découverte la plus importante du siècle », et cette opération publicitaire est l'occasion pour Parmentier de publier "Le parfait boulanger ou traité complet sur la fabrication & le commerce du pain" et d'ouvrir son école de boulangerie en 1780.

Cependant certains se méfient encore, et Parmentier, selon la légende, utilise alors un stratagème pour vaincre les réticences : il fait garder le champ de la plaine des Sablons par des hommes en armes le jour, mais pas la nuit. La garde du champ augmente la valeur de la culture aux yeux du peuple parisien qui croit qu'il s'agit d'un mets de choix réservé à la table du roi et des plus hauts seigneurs et la nuit les vols de tubercules sont aisés. Le peuple parisien en profite donc pour « voler » des tubercules et la consommation se serait alors répandue. En réalité, les soldats surveillent de jour l’ensemble du terrain de manœuvres comme ils le font pour tout terrain militaire, et les lettres de Parmentier écrites à l'intendant révèlent ses craintes que les vols nocturnes des tubercules immatures (le manque de pluie et le sol ingrat ayant entraîné un retard sur la saison) nuisent à la promotion de la pomme de terre. L'année suivante, il renouvelle son expérience aux Sablons et, pour prévenir toute « dégénération » des semis, dans la plaine de Grenelle, ce qui se traduit par la publication le 27 juin 1787 du "Mémoire sur la culture des Pommes de terre à la plaine des Sablons et de Grenelle".

L'agronome français Henri Louis Duhamel du Monceau a cependant souligné, dès 1761, l'intérêt de ce tubercule lors de disette et, contrairement à ce qui est souvent écrit, popularisé avant Parmentier l'usage de la pomme de terre. En outre, Parmentier n'hésite pas à qualifier le chevalier Mustel de « premier Apôtre des pommes de terre en France, connu par d'excellents ouvrages ». Bien qu'elle fût considérée comme fade, farineuse et venteuse, la pomme de terre était en effet dans les campagnes bouillie avec du lard et du salé ou cuite sous la cendre pour accompagner les ragoûts et chez les personnes plus aisées, accommodée avec du beurre, avec de la viande, et ce bien avant la naissance de Parmentier. Parmentier a en fait permis, grâce à ses talents pour la promotion, à la reconnaissance royale et à son expérience de culture de la pomme de terre dans la plaine des Sablons, de défaire la pomme de terre de son image d'aliment de pauvre et d'introduire la consommation de ce tubercule chez les élites, faisant de la pomme de terre le « légume de la cabane et du château ».

Il se penche par ailleurs sur la châtaigne (1780), sur le maïs ou le blé de Turquie, en réponse à un sujet de l’Académie de Bordeaux (1785). Précurseur de la chimie alimentaire, il remplace la méthode du chauffage à la cornue qui détruisait les composants de l'aliment qu'on voulait justement analyser par une extraction plus douce employée précédemment par Claude de La Garaye. C’est un nutritionniste et un hygiéniste, traitant aussi des fécules, du pain ("Parfait boulanger, ou Traité complet sur la fabrication et le commerce du pain", 1778, in-8°), du sucre de raisin, s’intéressant aux produits laitiers (ainsi avec Nicolas Deyeux, en l’an VII). Il rédige plusieurs instructions lors de la disette de 1785.

Pour remédier à la pénurie de sucre de canne, il préconise l’emploi de sucres de raisins et d’autres végétaux sucrés. Il s’intéresse à la conservation des farines, du vin et des produits laitiers.

En 1772, en compagnie de Cadet de Vaux (ancien pharmacien des Invalides), il tente d’améliorer la qualité du pain distribué dans les hôpitaux et les prisons en imaginant une nouvelle méthode de panification. Il fonde avec Cadet de Vaux en 1780 une école de boulangerie rue de la Grande-Truanderie à Paris.

Parmentier s'occupe également de plusieurs sujets ayant trait à l'hygiène : sécurité sanitaire des exhumations, qualité de l'eau, qualité de l'air notamment dans les salles d'hôpitaux, préconisation de l'entretien et de la vidange régulière des fosses d'aisance.

Dans les premiers temps de la Révolution, le souvenir de ses travaux l’expose à une certaine défaveur, puis, en 1793, la pomme de terre trouve grâce devant les « niveleurs », qui la préconisent partout. Parmentier se tient d’abord à l’écart de l’administration, puis il est chargé de surveiller les salaisons destinées à la Marine, en s’occupant parallèlement de la préparation du biscuit de mer.

Il travaille aussi sur le maïs, l’opium et l’ergot de seigle. Il préconise la conservation de la viande par le froid. Il travaille également sur l’amélioration de la technique des conserves alimentaires par ébullition découverte par Nicolas Appert, en 1795 et publiée en 1810.

En 1793, il donne même les techniques à employer. C’est ainsi, que, grâce à lui, la première raffinerie de sucre de betterave mise en service par Delessert voit le jour en 1801.

En 1796, il est porté sur la liste de l’Institut, formé par le nouveau Directoire. Il est appelé sous le Consulat à la présidence du Conseil de salubrité du département de la Seine et à la place d’inspecteur général des Hospices et du service de santé (1805 et 1809) ; il rédige un "Code pharmaceutique" ( éd. en 1807). La Société d’agriculture l’envoie en Angleterre avec Jean-Baptiste Huzard pour rouvrir les communications scientifiques entre les deux pays.

Inspecteur général du service de santé de 1796 à 1813, il fait adopter la vaccination antivariolique par l’armée et s’occupe des conditions d’hygiène sur les navires de la Marine. Il est l’un des créateurs de l’École de boulangerie en France en 1800.
Il est pharmacien en chef de l'Armée des Côtes de l'Océan en 1803. Il devient le premier président de la Société de pharmacie de Paris, dès sa fondation en 1803, il la préside en 1804, 1807 et 1810.

Trop jeune pour participer à l'aventure des encyclopédistes et trop vieux pour prendre part au début du à la révolution des sciences, notamment à l'expédition scientifique de la campagne d'Égypte, Parmentier est cependant un scientifique à l’œuvre remarquable par sa diversité. Il participe, en outre, à la vie sociale en collaborant aux textes sur la réforme agraire, sources du code rural, proposés par la Société d’Agriculture à l’Assemblée nationale. Il est membre de la Société d’Agriculture de Paris en 1773. Il est élu à l’Académie des sciences le 13 décembre 1795 dans la section d’économie rurale.

N'ayant ni femme, ni enfant, il a consacré toute sa vie à ses recherches, mais il a le chagrin de perdre, en 1809, sa sœur Marie-Suzanne, confidente et collaboratrice qui lui a épargné la pénibilité d'un long célibat par ses soins affectueux. Ses amis du "Bulletin de pharmacie", tel Louis Claude Cadet de Gassicourt, le surnomment le « bourru bienfaisant » car, régulièrement sollicité par des pharmaciens pour obtenir une place ou une pension, il renâclait d'abord mais faisait finalement jouer ses relations pour les aider.

Il meurt d'une phtisie pulmonaire, rongé par la tuberculose, dans sa maison de la Folie-Genlis, 12 rue des Amandiers-Popincourt (correspond actuellement au 68 rue du chemin vert paris 11ème), le . Il est inhumé au cimetière du Père-Lachaise à Paris dans le caveau familial. Sa tombe est régulièrement entretenue par certaines sociétés de pharmaciens. Jusqu'au début du , cette tombe était ornée d'un potager où s'épanouissaient des plans de pommes de terre pour rendre hommage au grand vulgarisateur.







Antoine Parmentier a écrit 165 ouvrages d’agronomie dont la plupart consacrés à la culture de la pomme de terre.





</doc>
<doc id="360" url="https://fr.wikipedia.org/wiki?curid=360" title="Halobacteria">
Halobacteria

Les Halobacteria, ou halobactéries, sont une classe d'archées de l'embranchement ("phylum") des "Euryarchaeota". Ce sont des microorganismes chimioautotrophes qui se développent dans des milieux saturés ou quasiment saturés en sels dissous, tels que les marais salants. On les qualifie d'halophiles, bien que cet adjectif s'applique également à des organismes qui requièrent des salinités moins élevées. On les trouve dans tous les environnements humides riches en matières organiques et en sels.

Le nom "Halobacteria" a été attribué à ces organismes avant que le domaine des archées ne soit identifié, à une époque où tous les procaryotes étaient considérés être des bactéries. On les trouve parfois désignées sous le terme d’"haloarchées" pour les distinguer des bactéries halophiles.

Leur métabolisme peut être aérobie ou anaérobie. Leur membrane cellulaire présente une coloration pourpre caractéristique due à la bactériorhodopsine qui donne à leurs efflorescences des teintes rouges parfois violacées. La bactériorhodopsine capte la lumière du soleil afin de la convertir en énergie métabolique à travers la phosphorylation de l'ADP en ATP. Les halobactéries possèdent également un autre pigment, l'halorhodopsine, qui pompe les ions chlorure à travers la membrane cellulaire et génère un gradient de concentration contribuant également à la production d'énergie métabolique. Ces archées sont cependant incapables de fixer le carbone, contrairement aux organismes photosynthétiques.

Selon :

Selon :



</doc>
<doc id="361" url="https://fr.wikipedia.org/wiki?curid=361" title="Archées thermophiles">
Archées thermophiles

Les archées thermophiles (anciennement appelées "thermoacidophiles") sont des archées qui s'accommodent de la chaleur. On parle de thermophiles lorsqu'elles ont un optimum de croissance aux alentours de 60°C et d'hyperthermophiles lorsqu'elles se développent à plus de 80 °C, ces organismes sont dits chimiotrophes.

Par exemple :



</doc>
<doc id="363" url="https://fr.wikipedia.org/wiki?curid=363" title="Géographie de l'Arabie saoudite">
Géographie de l'Arabie saoudite

La géographie de l'Arabie saoudite, malgré la prépondérance du désert dans le pays, offre une grande diversité, ne serait-ce que dans les tailles, les formes et les couleurs des dunes sablonneuses couvrant les vastes étendues désertiques. La partie méridionale des monts Sarawat, (c'est-à-dire les monts de l'Asir - "Sarat-el Asir" - et ceux du Yémen - "Sarat-el-Yemen"), caractéristique du Sud-Ouest et très verdoyante, contraste fortement avec le reste du pays. Le Nadj escarpé d'origine sédimentaire au centre du pays n'est que le demi-frère des plateaux cristallins du Nadj de l'Ouest; quant aux aires couvertes de laves au Nord et à l'Ouest du pays, elles offrent la plus frappante disparité avec les immenses mers de sable du Nord, du Sud et de l'Est, tant d'un point de vue géomorphique que d'un point de vue culturel.

La péninsule doit la plupart de ses caractéristiques physiques à la séparation tectonique qui lui a donné naissance : le schisme de la péninsule arabique et de l'Afrique du Nord-Est le long du rift de la Mer Rouge au cours de l'Ère tertiaire.

Le premier tiers ouest du pays est un bloc cristallin massif de roches éruptives et métamorphiques, accompagnées de basaltes de formation plus récente à l'Ouest.

À l'Est de cette zone, des couches sédimentaires couvrent le reste de la péninsule jusqu'au sultanat d'Oman et la région d'al-Hajar, les strates s'affaissant en direction de l'Est, mais avec une succession d'affleurements plus jeunes dès que l'on s'approche du Golfe. En surface, les mers de sable et les dunes tapissent plus d'un tiers de la péninsule, joignant le "Nafud" ou Nefoud, au Nord, au Rub al-Khali au Sud par l'intermédiaire de l'arc de dunes de Dahna.

Dans les années 1930, le Rub al-Khali a été exploré par St. John Philby, l'un des premiers européens à le traverser et à le décrire. Ses relevés sont à l'origine du premier tracé de la frontière entre le Yémen et l'Arabie saoudite.

Jusque dans les années 1980, l'Arabie saoudite disposait encore de ressources aquifères en surface et souterraines formées il y a très longtemps et non renouvelables. Ces dernières années, ces ressources ont été utilisées abondamment tant à des fins agricoles que domestiques ; afin de répondre à la demande importante, l'eau consommée dans le pays provient désormais principalement de l’eau de mer dessalée. À la suite de la surconsommation de l'agriculture par rapport aux autres besoins du pays, la culture de céréales est abandonnée en 2016.. Dans Al-Hasa, de grandes fosses profondes sont constamment réapprovisionnées par des sources artésiennes grâce aux eaux souterraines du bassin versant est du Jabal Tuwayq. Ces puits permettent l'irrigation d'oasis vastes mais locales. Dans le Hedjaz et l'Asir, les puits sont abondants ; dans le Nejd et les grands déserts, des points d'eau sont relativement moins nombreux et éparpillés sur une immense zone.

La technologie moderne a localisé et augmenté la disponibilité d'une grande partie de l'eau souterraine ; la Saudi Arabian Oil Company a trouvé d'importantes réserves dans plusieurs régions du nord et de l'Arabie orientale. Le gouvernement saoudien, la Saudi Aramco, et l'Organisation des Nations unies pour l'alimentation et l'agriculture ont fait des efforts conjoints afin d'exploiter les ressources en eau souterraine de manière durable mais la surexploitation de ces ressources entre les années 1970 et 2000 font craindre un disparation de celles ci. Il faut creuser désormais à plus de de profondeur sous Al Safi pour trouver de l'eau.

La consommation d’eau et d’électricité en Arabie Saoudite est deux fois supérieure à la moyenne internationale et s’élève à environ 20 milliards de mètres cubes par jour en 2014, elle augmente de 5 % par an.




</doc>
<doc id="364" url="https://fr.wikipedia.org/wiki?curid=364" title="Industrie pétrolière de l'Arabie saoudite">
Industrie pétrolière de l'Arabie saoudite

L'industrie pétrolière de l'Arabie saoudite est issue des explorations des années 1930, qui permirent de découvrir les plus gros gisements de pétrole du monde. L'Arabie saoudite est actuellement le premier producteur mondial de pétrole (devant la Russie et les États-Unis). Le pays détient les deuxièmes plus grosses réserves mondiales (bien que leur montant exact soit sujet à caution, voir pic pétrolier). Entre 1973 et 2002, l'Arabie Saoudite a reçu de revenus pétroliers. Les investissements dans l'industrie font défaut et le pays vit majoritairement de la rente pétrolière.

L'Arabie saoudite détient les deuxièmes plus importantes réserves prouvées de pétrole au Monde. Elles totalisent , soit . Ces réserves représentent des réserves mondiales. Il s'agit d'un pétrole de qualité sensiblement meilleure que celui du Venezuela qui est le premier pays en termes de réserves.

L'Arabie saoudite reste le premier producteur mondial avec , devant la Russie () et les États-Unis ().

Les quantités de gaz naturel en Arabie saoudite apparaissent relativement modestes en proportion des réserves pétrolières. Le pays est tout de même classé à l'échelle mondiale, avec des réserves prouvées de , loin cependant des trois grands pays gaziers que sont l'Iran, la Russie et le Qatar. Ces réserves ne représentent que des réserves mondiales. 

L'Arabie saoudite produit , loin derrière les États-unis () et la Russie ().

Jusque dans les années 1930, sous les sables immobiles de l'est arabique, reposaient, insoupçonnées, les plus grandes réserves mondiales de cet or qu'on dit noir. Certes le roi Abdelaziz Al Saoud avait accordé une concession autorisant une holding britannique à la recherche de pétrole à explorer le désert, mais celle-ci n'ayant pas fait usage de ce droit avait perdu sa concession. En 1933, le roi, par l'intermédiaire de St. John Philby, attribua à la SOCAL (Standard Oil of California) les droits exclusifs de prospection et d'exploitation du pétrole dans la région Est de l'Arabie, ainsi que des droits spéciaux dans d'autres régions du royaume, ce pour une durée de 60 années, qui furent portées à 66 par la suite. Une nouvelle entité, la California Arabian Standard Oil Company (CASOC), détenue à 50 % par la Socal (qui devint par la suite Chevron) et (à partir de 1937) à 50 % par la Texas Company (future Texaco), devint propriétaire de la concession en 1934. En 1944, la Casoc fut renommée Arabian American Oil Company, mondialement connue sous son acronyme d'Aramco. En 1948, la Standard Oil Company of New Jersey (qui prit par la suite le nom d'Esso puis celui d'Exxon) et la Socony-Vacuum Oil Company (l'ancêtre de Mobil) rejoignirent le capital de l'Aramco. Les quatre compagnies, toutes américaines, restèrent jusqu'en 1973 les chevilles ouvrières du développement pétrolier en Arabie saoudite. 

La recherche de nouveaux gisements, qui se poursuit encore 55 ans après les premières découvertes, révéla bientôt que la Province de l'Est recelait les plus grands champs d'hydrocarbures du monde. Le premier segment de Ghawar, le plus vaste gisement du monde, fut découvert en 1948 ; Safaniya, le plus grand gisement offshore, en 1951. En 1991, 60 gisements exploitables avaient été mis au jour, dont 5 pour la seule année 1990. En 1989 et 1990, un total de 7 nouveaux gisements, d'un brut léger de qualité supérieure, ont été découverts au sud de Riyad, au cœur d'une région située en dehors des secteurs supposés à hydrocarbures.

Le forage débuta en avril 1935 dans la région de Dammam Dome, le long de la côte du golfe Persique, mais le premier puits ne commença à rendre qu'en mars 1938. Le premier baril embarqua en mai 1939 à Ras Tanura qui devint par la suite un des plus grands terminaux exportateurs de pétrole. En 1991 plus de 60 milliards de barils avaient été produits depuis 1938 rien que par l'Aramco, mais les réserves connues sont de 257,5 milliards de barils et susceptibles d'augmenter davantage à mesure que les gisements du Sud du Nadj seront circonscrits. Les réserves de gaz naturel non associé dépassaient en 1991 les 6,4 milliards de mètres cubes. Les programmes d'expansion en cours prévoyaient une augmentation de la production à 10 millions de barils par jour.

En 1973, l'Arabie saoudite s'arrogea 25 % des droits et des propriétés de l'Aramco. Cette réappropriation du patrimoine national conduisit le gouvernement à la prise de contrôle de l'Aramco dont elle acquit 60 % en 1974, puis 100 % en 1980. Officiellement renommée Saudi Arabian Oil Company ou Saoudi Aramco en 1988, l'Aramco continue de faire trembler l'industrie et de bouleverser les salles de change du monde entier en usant de son acronyme vieux de 50 ans. L'Aramco remplit aujourd'hui les fonctions d'opérateur pour la production du pays, et joue le rôle d'intermédiaire dans un certain nombre de projets de BTP ou d'ingénierie. La compagnie officiait d'ailleurs à ce titre dès 1949 en supervisant la construction de la ligne ferroviaire Dammam-Riyadh dont le gouvernement était maître d'œuvre, ou plus récemment en contrôlant le déroulement du Master Gas Plan. La puissance de l'Aramco s'est aussi manifestée par la conduite d'opérations en aval de l'extraction, par exemple l'établissement en 1988 d'une coentreprise avec Texaco destinée à raffiner, distribuer et commercialiser des produits dérivés du pétrole dans l'Est et la région du Golfe des États-Unis.

Bien que l'Aramco contrôle 95 % de la production d'Arabie saoudite, deux autres compagnies opèrent dans la moitié saoudite de la Zone Divisée, l'ancienne zone neutre entre le Koweït et l'Arabie saoudite. La Getty Oil Company, à capitaux américains, détient la concession pour la zone terrestre alors que l'Arabian Oil Company (AOC), de nationalité japonaise, exploite la concession offshore. Cette bande de territoire, sujet d'un litige entre les deux pays, fut mise en commun par l'Arabie saoudite et le Koweït en 1965, puis divisée en deux parties quasi-égales en 1970. Les deux pays convinrent également de partager équitablement les réserves pétrolières de la zone et de se diviser les revenus du pétrole. Les réserves connues pour toute la zone totalisaient en 1991 5 milliards de barils, et la production était en moyenne de 359 000 barils par jour entre 1985 et 1989, la part saoudite de cette production constituant entre 2 et 4 pour cent de la production totale de l'Arabie saoudite.

Seul opérateur pour la province de l'Est, l'Aramco n'a jamais eu besoin de forer et d'exploiter que le nombre optimum de puits ; après un demi siècle, plusieurs de ces puits requièrent une repressurisation artificielle par un système d'injection d'eau de source saumâtre non-potable, drainée et injectée dans les réservoirs à mesure que le brut en est extrait. Seuls 850 puits sont utilisés pour couvrir une production allant jusqu'à 9 millions de barils par jour, ce qui représente une moyenne de barils par jour chacun.

La manipulation, le transport et le traitement des matières pétrolières requiert un réseau complexe d'installations réparties dans tout l'Est arabique et reliées entre elles par plus de 21 000 km d'oléoducs. Chacune des 60 usines de séparation gaz-pétrole (Gas-oil separator plants ou GOSPs) dessert plusieurs puits dans un rayon d'action considérable, par l'intermédiaire d'une maille dense d'oléoducs de jonction, et embrase le ciel nu de ses cheminées de gaz incandescent. Les complexes stabilisateurs de Abqaiq et Juaymah adoucissent des flux de brut acide tandis qu'ailleurs les usines de traitement des gaz naturels produisent propane, butane et gaz de ville (méthane). À Ras Tanura, une raffinerie géante d'une capacité de 530 000 barils par jour, ouverte en 1941 et agrandie constamment depuis, traite une partie du brut avant son expédition. Ici opèrent des usines spécialisées dans les produits dérivés du pétrole - comme dans la ville nouvelle de Jubayl - alors que là, le long des côtes, de Ras Tanura à Juaymah en passant par Yanbu, s'étendent de vastes terminaux d'exportation. 

De grandes raffineries ont également été construites à Jubayl ( barils par jour), à Yanbu ( barils par jour pour l'export et barils par jour réservés à la consommation domestique). Les raffineries de Riyad ( barils par jour), de Jiddah ( barils par jour), Rabigh (332 000 barils par jour) et de Khafji ( barils par jour) portent à 8 le nombre total de raffineries en Arabie saoudite avec une capacité totale de 1,82 million de barils par jour.

Le long oléoduc trans-arabe (Tapline) ouvert en 1951, nécessitant des réparations constantes, perdit toute rentabilité durant les années 1970 et devint peu à peu inexploitable ; il fut définitivement fermé en 1990. Néanmoins, les Saoudiens n'abandonnèrent jamais l'idée d'une issue à l'Ouest, cette fois non pas pour les exportations normales, mais à des fins stratégiques, le détroit d'Ormuz partagé entre Oman et l'Iran étant un lieu de passage dangereux en cas de conflit entre États du Golfe et la navigation dans le Golfe présentant de réels dangers depuis la guerre Iran-Irak. Ces considérations stratégiques ont conduit durant les années 1980 à la construction, au prix de plusieurs milliards de dollars d'investissement, de trois vastes oléoducs joignant le port de Yanbu aux champs pétrolifères de l'Est : l'oléoduc des gaz naturels liquéfiés (NGL), de 1170 km de long et de 66 à 76 cm de diamètre, reliant l'Est à l'Ouest, fut mis en service en mars 1981 et transporte aujourd'hui 270000 barils par jour (équivalent pétrole) ; quant à l'oléoduc de brut d'un diamètre de 122 cm ouvert en juin 1981, ainsi que la voie d'évitement de 142 cm de diamètre qui lui fut adjointe en 1987, ils s'étendent tous deux sur de long. L'adjonction de pompes hydrauliques et l'expansion des capacités de stockage à chaque extrémité de ce système de transport du brut lui confèrent aujourd'hui une capacité de l'ordre de 5 millions de barils par jour - dix fois le débit de la Tapline et une capacité d'absorption de la moitié de la production de l'Aramco.

Ces audacieux systèmes d'acheminement qui traversent la péninsule, nouveaux transsibériens de l'énergie, dotent l'Arabie saoudite d'une route domestique totalement sure ; ils l'affranchissent en grande partie des menaces de blocus. Les Saoudites restent néanmoins sujets à une interdiction de leurs exportations depuis Yanbu si les deux détroits (Canal de Suez et Chatt-el-Arab) de la mer Rouge leur sont interdits.

À la fin des années 1970, l'Arabie saoudite mit en œuvre l'ambitieux plan de maîtrise du gaz ou Master Gas System. Le gaz autrefois brûlé lors de la séparation des hydrocarbures en fonction de leur densité est aujourd'hui en partie utilisé à la production de l'électricité, à la désalinisation de l'eau, la fabrication de verre, de ciments, d'engrais ou de chaux. Le gaz est aussi acheminé par conduits jusqu'aux nouvelles cités industrielles de Jubayl et Yanbu pour y servir de carburant aux raffineries, aux usines pétrochimiques, aux fabriques d'engrais ou aux hauts fourneaux, ou bien pour y être exporté vers les ports d'Europe, d'Amérique ou d'Asie. Conçu pour traiter 99 millions de mètres cubes de gaz par jour, le Master Gas System permet d'apporter l'équivalent de 750 000 barils de brut par jour à un monde assoiffé d'énergie.

L'Arabie saoudite fut un des membres fondateurs de l'OPEP et de l'OPAEP, et a joué un rôle de premier plan dans l'OPEP depuis ses tout débuts. Étant donné que la production de l'Arabie saoudite représente chaque jour une part plus importante de la production mondiale de pétrole, le pays a été appelé à jouer un rôle directeur et régulateur dans la fixation du prix du pétrole en faisant fluctuer sa production.



</doc>
<doc id="365" url="https://fr.wikipedia.org/wiki?curid=365" title="Économie de l'Arabie saoudite">
Économie de l'Arabie saoudite

L'économie de l'Arabie saoudite repose principalement sur son industrie pétrolière, qui a bouleversé l'histoire économique du pays. Ainsi, depuis 1938, Dhahran (ville située à l'est de l'Arabie Saoudite le long du golfe Persique) est devenue la capitale du pétrole arabe.

À l'opposé, l'agriculture de l'Arabie saoudite n'a cessé de décroître depuis les années 1960 avant de bénéficier d'aides gouvernementales.

En décembre 2005, au sommet de Hong Kong, le pays est devenu le de l'Organisation mondiale du commerce.

La découverte de pétrole en mars 1938 transforme le pays sur le plan économique.

L'Arabie saoudite est membre de l'OPEP. Entre 1977 et 1981, le revenu national provenant directement des revenus du pétrole dépasse à lui seul les 300 millions de dollars par jour, nourrissant de milliards de dollars la politique d'investissement des dirigeants du pays. Ces investissements se concrétisent en des plans quinquennaux, par l'intermédiaire de l'attribution de budgets d'État, culminant lors du second plan couvrant la période 1975-1980, où 195 milliards de dollars sont attribués et permettent la construction de 28 barrages, 4 ports, 175 000 nouvelles maisons, 24 000 km de routes et l'aéroport de Jeddah qui fut le plus grand du monde jusqu'à l'ouverture de l'aéroport de Riyad.

En 2013, les revenus des exportations pétrolières (pétrole brut et dérivés) du pays s'élevaient à 312,7 milliards de dollars selon l'OMC, tandis que le montant des exportations du secteur manufacturier se chiffraient a 56,4 milliards de dollars. Au fil des années la part des exportations de pétrole brut dans le PIB a baissé, passant de 34 % du PIB en 2000 à 21 % en 2012.

L'industrie pétrochimique est le économique du pays dans les exportations, faisant de l’Arabie saoudite le de produit pétrochimique du monde.

L'exploitation et l'exportation du pétrole ont fortement développé l'activité économique de la côte nord-est du pays, autour de Dammam, Khobar et Dhahran avec le port de Jubail, ainsi que la côte sur la Mer rouge (Djeddah, Yanbu).

Fort de la manne pétrolière, les dépenses publiques n'ont cessé d'augmenter dans les années 2000 notamment pour assurer « la cohésion sociale ».

La forte baisse des cours du pétrole en 2015, dans un contexte où les ventes de pétrole représentent encore 80 % des recettes, ne reste pas sans effets sur l'économie du pays. Ainsi, l'Arabie saoudite est contrainte à réaliser sa première émission de dette depuis 2007. Du côté des économies, le gouvernement envisage de réduire les subventions à l'énergie qui équivalent à 8 % du PIB, mesure peu populaire. Elle annonce ainsi un déficit à hauteur de plus de 20 % de son PIB. Si ce déséquilibre se poursuivait, les importantes réserves financières du pays pourraient être épuisées d'ici 2020.

Dans les années 1950, l'industrie manufacturière s'est développée par la conjonction de quatre facteurs : l'expansion et la diversification des activités pétrolières et de la main-d'œuvre industrielle, la croissance rapide de la population et de la demande intérieure, la disponibilité croissante du capital tant privé que gouvernemental, l'implantation de plus en plus fréquente de compagnies étrangères sous la forme de coentreprises avec des partenaires saoudiens - parfois avec le gouvernement.

Les décennies qui suivirent virent une augmentation du nombre, de la taille et de la diversité des unités de production : alimentaire industriel, papiers et matières plastiques, confection ou encore mobilier de bureau. Après 1970, vinrent s'ajouter des produits comme les peintures, les systèmes à air climatisé, les bâtiments préfabriqués en aluminium, les barres de fer et d'autres produits utilisant le pétrole ou le gaz en tant qu'énergie de base. Parmi ces derniers, les engrais, les produits de la pétrochimie et de la chimie minérale et les produits dérivés du soufre. Néanmoins, le pays manque toujours d'industries de pointes dans des domaines variés comme l'électronique, l'informatique ou l'optique.

Souffrant d'une insuffisance structurelle de main-d'œuvre locale qualifiée et d'expérience en matière technologique, l'Arabie saoudite s'est longtemps adressée à des contractants étrangers pour l'exécution de projets industriels le plus souvent livrés clefs en main. Pour nombre de ces projets, le contrat s'étendait au-delà de la livraison et comprenait la maintenance et même souvent l'exploitation par le constructeur. Cependant, les programmes d'éducation et de formation professionnelle à l'initiative des contractants étrangers et du gouvernement saoudien ont accru le nombre de techniciens qualifiés autochtones, le gouvernement ayant d'autre part mis en place en 1982 une politique de « saoudisation » des entreprises destinée à y augmenter le pourcentage de Saoudiens. Cette politique se manifeste par la nomination d'un président et d'un directeur général saoudien à l'Aramco.

Investissant les pétrodollars de la rente de l'or noir qui semblait ne jamais s'arrêter de croître, les Saoudiens construisirent dans les années 1970 deux cités industrielles et portuaires, Jubayl sur la côte du Golfe et Yanbu, au bord de la mer Rouge. Construites aux extrémités de la route du pétrole, ces nouvelles oasis développèrent en priorité les industries à forte consommation énergétique ou fabriquant des produits dérivés des hydrocarbures : outre les raffineries, la pétrochimie et la chimie lourde, les engrais, les plastiques bruts ou sous forme de produits manufacturés, l'acier et les produits en métal dominent l'activité. La construction entièrement intégrée de ces cités incluait, à proximité des sites industriels, la présence de services et de secteurs résidentiels à l'architecture ultramoderne.

À l'aube des années 1990, l'industrie représentait 9 % du PNB. Des mesures incitatives de la part du gouvernement attirèrent à la fois le capital local et les investisseurs étrangers. Une agence gouvernementale, la SABIC (Saudi Arabian Basic Industries Corporation) fut ouverte en 1976 afin de développer l'industrie lourde dans les domaines de la pétrochimie et de l'acier, investissant 37 milliards de dollars (bilan en 1993), usines ayant été créées.

L'agriculture en Arabie saoudite qui dans les années 1990 avait réussi a obtenir l'autonomie alimentaire du royaume verra sa production de blé disparaître pour limiter la surconsommation des faibles ressources hydrauliques du pays.

Un nouveau code d'investissement adopté en 2000 a créé la SAGIA ("Saudi General Investment Authority"), qui est dotée d'une compétence d'attribution très générale pour tout ce qui touche à l'investissement dans le royaume, y compris l'investissement étranger (à l’exception du secteur des hydrocarbures). La nouvelle institution instruit les demandes et délivre les licences d’investissement. La SAGIA a ouvert des « comprehensive service centers » - sortes de bureaux interministériels d'information et de liaison - chargés de faciliter les démarches administratives des investisseurs, notamment étrangers. Depuis sa création en avril 2000 et jusqu’à mi-juin 2004, la SAGIA a délivré plus de 2 400 licences d’investissements étrangers (3 378 licences délivrées au total si l’on tient compte de la période précédant la création de la SAGIA et donc l’intervention du ministère des finances) pour un montant global de plus de 18 milliards de dollars, mais les investissements tardent à se concrétiser.

Tout projet d’investissement est donc sujet à l’octroi d’une licence délivrée par la SAGIA. Le montant minimum d’investissement étranger est fixé à un niveau élevé dans le cadre du nouveau code : 6,7 millions de dollars dans le secteur agricole, 1,3 million de dollars dans le secteur industriel et 0,5 million de dollars dans les autres secteurs. Compte tenu du monopole de négoce réservé aux seuls citoyens saoudiens - qui interdit à tout ressortissant étranger d’être titulaire d’actions, par nature négociables, notamment dans le cadre d’une société anonyme - les sociétés étrangères ne peuvent, en pratique, que constituer des filiales de droit saoudien sous forme de Sàrl.

Malgré l’adoption d’un nouveau code plus attractif, on relève encore certaines restrictions en termes de participations étrangères : une liste « négative » exclut encore certains secteurs de l’investissement étranger. Adoptée le et révisée le , cette liste comprend, outre l'amont pétrolier (exploration, forage et production) dont on savait qu'il resterait fermé au capital étranger du fait de son exclusion du processus d'ouverture du secteur des hydrocarbures saoudien (« "Gas Initiative" »), et les activités diverses (investissements immobiliers et services aux pèlerins) pouvant être menées dans le périmètre des deux villes saintes de La Mecque et Médine (exclues pour des raisons religieuses) : 

La décision d’ouvrir le secteur de l’assurance aux investissements étrangers, une fois la loi-cadre publiée, a été prise en février 2003. Cette loi a été publiée au Journal officiel saoudien le 22 août et applicable depuis le 20 novembre 2003. Des restrictions continuent à être appliquées dans le secteur des services professionnels (participation étrangère limitée à 75 % du capital), de la banque (participation étrangère limitée à 40 % du capital), de la distribution.

L'Arabie saoudite s’intègre dans un ensemble régional – dans les secteurs ouverts, les investisseurs étrangers sont désormais traités de la même façon que les investisseurs nationaux pour l’accès aux aides et incitations publiques : prêts à taux nul du "Saudi Industrial Development Fund", exemption totale de droits de douane sur certains produits à l’importation et des taxes à l’exportation, exemption au moment du démarrage de l’activité de droit de douane sur les machines et matières premières non disponibles localement, mise à disposition de terrains à des tarifs préférentiels et mise en place de mesures de protection tarifaires pour les nouvelles productions. Des discriminations subsistent, notamment en matière fiscale.

Source : "Gulf Business", novembre 2011, page 63.

Au premier trimestre 2013, la population active est estimée selon les autorités saoudiennes à 11,286 millions de personnes (9,591 millions d'hommes, 1,695 millions de femmes). 80 % de celle-ci est composé d'étrangers. 

La population étant composée début 2013 de 7,5 à 8 millions de travailleurs étrangers selon des chiffres officiels. Le nombre des clandestins est estimé à deux millions sur 29 millions d'habitants. On suppute qu'ils constituent peut-être jusqu’à 40 % de la population du pays. 

L'État a pris la décision de limiter la proportion de travailleurs immigrés à 20 % de la population, la mise en application de cette décision a démarré en 2013. Un tel pourcentage ramènerait leur nombre aux environs de 4 millions. 

En novembre 2013, une vague d'expulsions d'immigrés clandestins a touché personnes depuis le début de l'année et l'on prévoit un million d'autres expulsions.

Divers projets immobiliers ou urbanistiques :




</doc>
<doc id="366" url="https://fr.wikipedia.org/wiki?curid=366" title="Histoire économique de l'Arabie saoudite">
Histoire économique de l'Arabie saoudite

L'économie de l'Arabie saoudite a connu une longue période de pauvreté "stable" jusqu'à ce que la découverte du pétrole en 1938 initialise un bouleversement littéralement fabuleux. 

Avant 1938, le royaume reculé et assez pauvre d'Arabie saoudite trouvait des moyens précaires de subsistance dans un pastoralisme à portée limitée, une agriculture d'oasis et les profits des pèlerinages à La Mecque dont vivait un petit commerce à l'état embryonnaire. 

L'exploitation des puits de pétrole par les compagnies occidentales, modeste mais en constante progression de 1945 aux années 1960, préfigura l'avenir du pays, lui faisant entrevoir ses possibilités de développement ; enfin, les années 1970 (les chocs pétroliers) virent l'explosion des revenus pétroliers et provoquèrent ce qui est probablement un exemple unique dans l'histoire : la transformation complète et radicale, tant économiquement que socialement, d'un si grand pays en un si petit laps de temps.

L'esclavage a été supprimé dans le royaume en 1962 mais la situation des sept millions d'immigrés (pour une population totale estimée à 23 millions en 2004) n'est guère enviable, et les femmes ont en outre un statut inférieur à celui des hommes

Entre 1977 et 1981, le revenu national provenant directement des revenus du pétrole dépasse à lui seul les 300 millions de dollars par jour, nourrissant de milliards de dollars la « machine à développer » conçue par les Al Saud. Concevant et menant à bien des milliers de projets, les architectes, les ingénieurs, les entrepreneurs, les éducateurs et les ouvriers venus de l'Europe de l'Ouest, d'Amérique du Nord, du Moyen-Orient, d'Asie du Sud-Est aident l'Arabie saoudite à construire de toutes pièces une nation développée au milieu du désert, depuis les infrastructures les plus élémentaires aux campus universitaires les plus sophistiqués, en passant par des complexes d'habitation à l'américaine avec leurs magasins de fast-food. Les investissements privés ont suivi l'impulsion forte et continue donnée par le gouvernement, et ont contribué pour une large part au développement des surfaces de vente et de nombreuses industries, et à une bonne part de l'expansion agricole.

Néanmoins, la majeure partie du développement s'est faite sous l'égide de plans quinquennaux, par l'intermédiaire de l'attribution de budgets d'État, culminant lors du second plan couvrant la période 1975-1980, où 195 milliards de dollars furent attribués et permirent la construction de 28 barrages, 4 ports, nouvelles maisons, de routes et l'aéroport de Jeddah qui fut le plus grand du monde jusqu'à l'ouverture de l'aéroport de Riyad. Avant et après cette période, des centaines de projets liés à la maîtrise de l'eau, servant au développement industriel ou à celui de l'infrastructure (comme la route d'un milliard de dollars desservant Bahreïn) furent menés à bien. 

Le quatrième plan - 1985-1990 - a vu son budget sérieusement réduit en raison de la récession qui suivit la baisse subite des revenus pétroliers à la suite du second choc de 1982. Les ventes de pétrole saoudien chutèrent de 9,6 millions de barils en 1980 à 3,3 millions en 1985 ; quant au prix du baril, il s'effondra de §35-40 en 1980 à §10-12 en 1986, ces deux événements se produisant à un moment où la valeur du dollar baissait elle aussi, en accentuant les effets. Ce triple choc réduisit les revenus pétroliers de 90 % et malgré une certaine diversification de l'économie, le Produit National Brut chuta de 75 % entre 1981 et 1988. Le cinquième plan quinquennal - 1990-1995 - prévoyait moins de 100 milliards de dollars en dépenses de développement, mais si la production de pétrole poursuivait sa lente reprise, ce montant était susceptible d'augmenter.

Tous ces plans ont en commun un programme de diversification de l'économie, conçu pour limiter la dépendance du royaume vis-à-vis des revenus du pétrole, de sorte qu'il soit moins à la merci d'un retournement de conjoncture en ce qui concerne les prix et les quantités consommées, et de manière à ce qu'à long terme, l'épuisement des ressources pétrolifères ne lui soit pas fatal. Les premiers plans privilégiaient des opérations coûteuses en énergie et en capital, le royaume possédant des réserves énormes de pétrole et les revenus de leur exploitation, mais manquant cruellement de main-d'œuvre à tous les niveaux. L'Arabie saoudite, en effet, avait une population inférieure à celle de Londres pour une surface équivalente à un quart des États-Unis. Une fois la main-d'œuvre attirée et fidélisée par des salaires attrayants et les investisseurs gagnés par la confiance, les priorités des plans ont été légèrement modifiées : le développement met désormais l'accent sur l'expansion et la diversification de l'industrie, en particulier en direction des secteurs non-pétrolifères et de ceux jusqu'à présent soumis à la loi de l'importation. L'agriculture est aujourd'hui également prioritaire, au même titre que la « saoudisation » de la main-d'œuvre et que l'éducation, la santé et l'ensemble des services publics.

Entre 1973 et 2002, la famille Al Saoud a reçu quelque 2000 milliards de dollars de revenus pétroliers. Les investissements dans l'industrie font défaut et le pays vit majoritairement de la rente pétrolière.



</doc>
<doc id="368" url="https://fr.wikipedia.org/wiki?curid=368" title="Politique en Arabie saoudite">
Politique en Arabie saoudite

L’Arabie saoudite est une monarchie absolue, où le roi est à la fois chef de l'État et chef du gouvernement. La loi fondamentale adoptée en 1992 définit le pays comme une monarchie gouvernée selon la charia par les descendants du roi Abdelaziz et dont la constitution est le Coran.

Le roi Abdelaziz ou Ibn Saoud, issu de la dynastie saoudienne et fondateur en 1932 du royaume d'Arabie Saoudite, choisit pour lui succéder l'aîné de ses fils, Saoud.

La règle de succession prévoit la transmission du pouvoir parmi les frères ou demi-frères du roi, par rang d'âge, l'héritier étant confirmé lors d'un conseil de famille. 

Cependant cette loi a l'inconvénient majeur de mettre en place rapidement une gérontocratie, ce qui peut être un frein majeur aux évolutions nécessaires d'un pays moderne. Ainsi, le roi Fahd (né en 1921) est monté sur le trône en 1982 à l'âge de 61 ans, et il est mort en 2005 à l'âge de 83 ans. Le prince héritier désigné Abdallah, né en 1921, accède au trône à l'âge de 82 ans, et à sa mort le 23 janvier 2015, son demi-frère Salmane accède au trône à l'âge de 79 ans. 

En 1992, le roi Fahd, conscient du problème, avait fait apporter une modification de la loi, dans le but de permettre le transfert du pouvoir au « plus apte » des petits-fils du roi Abdelaziz, c'est-à-dire à ses enfants ou à ceux de ses frères. L'avantage avancé était la désignation d'un roi plus jeune. L'inconvénient étant que ce mode de désignation allait monter les clans, issus des différentes épouses, les uns contre les autres. Considéré de par le nombre de ses princes, de par les postes clés qu'ils occupent, et de par la puissance de ses soutiens, le clan des princes issus de Hassa bint Ahmed Al Soudayri du puissant clan des Soudayri, semble le plus à même de remporter la mise, avec comme challengers, les princes issus de Fahda bint Assi Al Churaym du non moins puissant clan des Chammar, et comportant dans ses rangs, un atout majeur : le roi Abdallah, qui dirigeait déjà de facto le royaume saoudien depuis l'accident cérébral de son demi-frère le roi Fahd.

Le , le roi Abdallah crée le Conseil d'allégeance, comité de princes chargé après sa mort de choisir les futurs princes héritiers.

Le prince Moukrine, nommé prince héritier en janvier 2015, est à l'âge de 69 ans le plus jeune des fils encore vivants d'Ibn Séoud. En avril 2015 il est remplacé par son neveu Mohammed ben Nayef, 55 ans et premier de la génération des petits-fils à accéder à ce titre. 

Mais le 21 juin 2017, le roi Salmane parvient à évincer Mohammed ben Nayef au profit d'un de ses propres fils, Mohammed ben Salmane, préfigurant ainsi une succession directe de père en fils. Alors seulement âgé de 31 ans, contre 81 ans pour son père, le nouveau prince héritier pourrait être roi beaucoup plus jeune que ses prédécesseurs et régner pendant un demi-siècle. 

Le roi nomme les membres du conseil des ministres, chargés de le conseiller sur les lignes directrices de la politique du royaume.
Selon l’article 19 de la loi fondamentale : « Le pouvoir exécutif appartient au conseil des ministres» qui a pour tâche principale de déterminer les orientations de la « politique intérieure, extérieure, financière, économique, éducative et défensive de l’État », et suivant l’article 56 de la loi fondamentale le roi est également premier ministre. L’absence de tel poste fait clairement apparaître que le chef d’état, et lui seul, détient l’exercice effectif du pouvoir exécutif. Il cumule à ce titre les pouvoirs de chef d’État et de chef de gouvernement, disposant à sa guise de l’administration, de l’armée et de l’ensemble des organismes publics. En effet, le roi nomme, pour une durée de quatre ans sauf exceptions (loi fondamentale) les membres du cabinet, le premier ministre et les ministres chargés de le conseiller sur les lignes directrices de la politique du royaume. Il faut préciser que les postes clé du gouvernement saoudien tels que la défense, l’intérieur, le ministère des affaires étrangères ne sont pas sortis des mains de la famille royale. Il nomme les hauts fonctionnaires civils et militaires, des forces armées, de la garde nationale, de l'administration provinciale.

Le pouvoir du roi est limité par le cadre de la charia et des traditions saoudiennes. Il doit également recueillir un consensus au sein de la famille royale et parmi les chefs religieux, les oulémas. Ces derniers influencent le choix du prince héritier de la couronne et ils interviennent quand le roi se départ des principes de l'islam conservateur. Le roi est ainsi confronté à des contraintes pesant sur sa politique et ses décisions ponctuelles, en ce qui concerne la modernisation par exemple. Pourtant les pressions et les besoins croissants en faveur de la libéralisation à la fin des années 1980 et à partir de 1990 ont poussé le gouvernement à l'action dans ce domaine.

En mars 1992, le gouvernement du roi Fahd a annoncé une nouvelle constitution qui prévoit la création d'une Assemblée consultative de 60 membres qui peut présenter des projets de lois et contrôler les politiques mises en œuvre, la rédaction d'une Déclaration des Droits calquée sur le "Bill of Rights" britannique, et la pratique d'une approche plus libérale des relations politiques. Celle-ci a été créée en 1993. Ce nombre passe à 90 en juillet 1997, à 120 en mai 2001 puis à 150 en 2005. Ses membres sont nommés pour quatre ans par le roi. Comme son nom l’indique, son rôle est essentiellement consultatif et son pouvoir est extrêmement restreint.

Le , pour la première fois, des femmes saoudiennes ont été nommées membres de l'Assemblée consultative. 20 % des sièges leur seront désormais réservés, selon deux décrets royaux. Les femmes nommées sont des princesses, des universitaires et des militantes, mais leur conseil est limité aux domaines « féminins », tels la famille et les enfants. Thuraya Obaïd, qui a été secrétaire générale adjointe aux Nations unies, se trouve parmi ces femmes.

Les partis politiques et les syndicats ne sont pas autorisés. Pendant les années 1990, le Parti socialiste arabe d'action et le Parti communiste d'Arabie saoudite furent dissous et leurs membres libérés de prisons après leur engagement à ne pas poursuivre leurs activités politiques. Le Parti vert d'Arabie saoudite est la seule formation politique active dans le royaume, mais son existence n’est pas reconnue légalement.

Les premières élections, au niveau municipal uniquement, eurent lieu en 2005, et il est important de noter que deux femmes furent élues au Conseil de Jeddah. Une première dans le royaume. Le , le roi Abdallah accorde le droit de vote aux femmes à la suite des mouvements populaires qui ont eu lieu dans le cadre du Printemps arabe, avec une première mise en application lors des élections municipales de décembre 2015.





</doc>
<doc id="373" url="https://fr.wikipedia.org/wiki?curid=373" title="Liste de sigles en télécommunications">
Liste de sigles en télécommunications

Cette page présente quelques sigles utilisés couramment en télécommunication.





</doc>
<doc id="382" url="https://fr.wikipedia.org/wiki?curid=382" title="Blender">
Blender

Blender est un logiciel libre et gratuit de modélisation, d’animation et de rendu en 3D, créé en 1995. Il est actuellement développé par la Fondation Blender. 

Il dispose de fonctions avancées de modélisation, de sculpture 3D, de dépliage UV, de texturage, de rigging, d’armaturage, d’animation 3D, et de rendu. Il gère aussi le montage vidéo non linéaire, la composition, la création nodale de matériaux, la création d’applications 3D interactives ou de jeux vidéo grâce à son moteur de jeu intégré (le Blender Game Engine), ainsi que diverses simulations physiques telles que les particules, les corps rigides, les corps souples et les fluides. 

Disponible dans de nombreuses langues, Blender est également disponible sur plusieurs plates-formes telles que Microsoft Windows, Mac OS X, GNU/Linux, IRIX, Solaris, FreeBSD, SkyOS, MorphOS et Pocket PC. De plus, c’est un programme extensible (ajout de scripts) à l’aide du langage Python.

Produit à l’origine par la société néerlandaise "Not a Number Technologies" ("NaN") et par le studio d’animation "NeoGeo", Blender a d’abord été un "ray tracer" nommé "Traces" sur Amiga en 1989, la société de production se servait alors de comme modeleur. Le nom de "Blender" n’arriva que plus tard, inspiré de l’album Baby du groupe Yello, lorsque cela devint une suite d’animation complète .
La société NaN fut fondée en 1998 pour développer et distribuer le logiciel en tant que partagiciel, dont certaines fonctionnalités n’étaient accessibles que par l’intermédiaire d’une licence payante. Plus tard, son auteur Ton Roosendaal trouva des investisseurs pour l’aider à développer son produit et en faire un graticiel. Au début de l’année 2002, étant donné le peu de gains qu’ils en retiraient, les investisseurs décidèrent de retirer leur soutien à Ton Roosendaal, ce qui eut pour effet immédiat la faillite de la société NaN et la fermeture du site Internet de Blender. Néanmoins, la communauté d’utilisateurs de Blender se reforma rapidement au sein du forum de discussion du site Elysiun (devenu aujourd’hui BlenderArtists).

Le , Ton Roosendaal parvint à négocier le rachat des droits d’auteur de Blender contre , en vue de la création d’une fondation Blender et de la diffusion du logiciel sous la licence libre GPL. En moins d’une semaine, la communauté des utilisateurs a déjà rassemblé près de la moitié de la somme. Le 7 septembre est annoncée l’ouverture du code de Blender, supervisé par la Fondation Blender. C'est en fin de compte le 13 octobre 2002 que Blender est diffusé sous licence libre GPL.

La fondation commence alors à rendre plus modulaire le code pour faciliter son évolution. On sépare Blender en plusieurs bibliothèques afin d'ouvrir ses fonctionnalités à d’autres logiciels. Blender devient alors le composant standard d'un écosystème qui va en faire une référence.

Le sort la première version libre (2.26). Elle sera suivie le 14 mai d’un début d’internationalisation, permettant ainsi aux non anglophones de découvrir ce logiciel, ainsi que .

Blender dispose de nombreuses fonctionnalités et d’une gestion souple des fenêtres. Il pèse de 200 à selon les versions.

Son interface basée sur OpenGL s'écarte des habitudes classiques de Windows, MATE ou KDE, ce qui lui a valu (comme à GIMP) une réputation d’apprentissage long et difficile. Cette interface autorisant beaucoup de raccourcis clavier pour rendre le travail de production plus rapide, rend son utilisation moins intuitive que des cycles d'essais et d'erreurs sur des menus standards. Depuis son passage à l’Open Source, le logiciel a été amélioré notamment en ergonomie, par une réorganisation et l'ajout de quelques menus contextuels. L'interface revue et corrigée est intégrée à la version 2.50. Après quatre ans de développement, la Fondation Blender a sorti une version stable de Blender 2.5x en avril 2011 : Blender 2.57.
En octobre 2011 sort la version 2.60, qui marque l'aboutissement et la fin de développement de la série 2.5x. C'est en juin 2015 la version 2.74 qui est celle de référence, la 2.75 étant déjà annoncée avec le statut de "Relase Candidate".

La libération du code source lui a donné un élan important. Sa modularité et l’enthousiasme des équipes de développeurs y conduisent parfois à des fonctionnalités nouvelles importantes en quelques jours seulement. Les amateurs de Blender peuvent suivre son évolution à partir des compilations presque quotidiennes. Signe parmi d'autres de cette modularité : des professionnels du montage vidéo considèrent Blender comme le seul concurrent libre à la hauteur de logiciels commerciaux professionnels et onéreux, alors que les fonctionnalités de montage vidéo n'ont été qu'un simple ajout à Blender et ne sont qu'une de ses possibilités annexes.

De très nombreux tutoriaux en anglais sont disponibles sur le "Blender Cloud", certains gratuitement et d'autres disponibles avec un abonnement. Les fonds ramassés servent à financer le développement de Blender et des films libres de l'institut Blender. De nombreux tutoriaux sont également disponibles sur les sites de partage de vidéo comme YouTube.

Suzanne est la mascotte de Blender et le plus courant des « modèles d’essai » (comme la théière dans beaucoup de logiciels 3D). C’est un modèle composé de 507 sommets, arêtes et 500 faces. Suzanne est souvent utilisée pour tester les matériaux, les textures ou encore les réglages de l’éclairage.
Le plus grand concours de Blender décerne le Prix Suzanne (sous la forme d’une statuette de Suzanne).


Blender est compatible avec divers moteurs de rendu. Il possède de base un moteur dit "Hybride", le Blender Internal, qui est un mélange entre du scanline ainsi que du Raster scan. Il s'agit d'un moteur vieillissant, ne faisant plus l'objet de développement actuellement, malgré les nombreuses update mise en liste d'attente. La Fondation Blender travaille sur un nouveau moteur de rendu nommé "Cycles" de type Unbiasser qui permet d'utiliser notamment les calculs via GPU. Cycles est disponible à partir de la version 2.61, en complément du moteur Blender Internal.

À partir de la version 2.67, Freestyle est intégré dans Blender pour étendre les possibilités du moteur Blender Internal. Il s'agit d'un moteur de rendu non photo-réaliste dont la fonction est de générer des lignes en 2D sur un dessin 3D (contour des objets par exemple).

Le logiciel est fourni de base avec un exporteur vers POV-Ray, il nécessite l'installation de ce dernier en version 3.7 beta.

Beaucoup d'autres moteurs de rendu sont compatibles avec ce logiciel :

Blender ne demande pas pour son apprentissage une configuration minimale très puissante, contrairement à d'autres applications 3D. Seuls des effets avancés et des modèles comportant beaucoup de polygones demanderont un ordinateur puissant. La configuration minimale est :

Pour des utilisateurs avancés, le matériel suivant est recommandé :

Le mode GLSL de Gameblender exige une carte graphique supportant OpenGL GLSL.

Pour les versions 2.80 de Blender, une refonte totale de l'interface est en cours de développement. Un nouveau moteur de rendu est installé: EEVEE. Cela exige, au minimum, une carte graphique compatible OpenGL 3.3 et supérieure pour plus de performance et de fluidité dans la vue 3D.

Les recommandations actuelles pour une meilleure performance sont: 


Blender accepte des extensions en Python, mais n'est pas programmé lui-même dans ce langage pour des raisons de performance. Python est utile pour réaliser des travaux d'enchaînement qu'on ne veut plus faire à la main, à la manière d'un langage de script

L'affichage au lancement du message « "Compiled with python version 2.6.2" » signifie juste que cette version "inclut" la version 2.6.2 de l’interpréteur Python. Le code est presque entièrement écrit en C/C++ et compilé à l’aide d’un compilateur classique comme gcc. En 2004, Blender comporte plus de trois cent mille lignes de code dont (69,49 %) lignes en C ISO, (29,30 %) lignes en C++ et 3303 (1,01 %) lignes en python (autres : 673 (0,20 %)) .

La relation de longue date avec le langage Python a permis de constituer une importante base de greffons ("plugins") qui ont fait de Blender un véritable logiciel de conversion de formats. Ces outils ont été écrits par des auteurs aussi enthousiastes que bénévoles. Bon nombre d'entre eux, couverts par la licence GPL ou la Blender Artistic Licence, sont regroupés et distribués dans un paquet qui accompagne le logiciel. Ces scripts sont variés (création d'arbres, de coquillages, etc.).

Certains de ces formats, cependant nécessitent des plugins développés par des développeurs tiers qui doivent généralement être renouvelés régulièrement car le dialecte du Python utilisé évolue et les nouvelles versions ne sont pas toujours compatibles avec les versions antérieures. La liste des formats supportés par défaut, elle, varie selon les versions de Blender. Le format md2, par exemple, n'est plus supporté par défaut depuis la version 2.5.

Blender est aussi une sorte de phénomène social qui a donné naissance à un nombre considérable de productions : didacticiels, "plugins", images statiques, films d'animation. De fait, une licence spéciale a été créée, la Blender Artistic License.

Elle vise les didacticiels, les fichiers .blend d’exemple ainsi que les images et animations. Elle est plus limitative que la Blender Documentation License, mais est pensée pour protéger les droits des auteurs sur leurs didacticiels.

Les auteurs peuvent choisir la Blender Documentation License, moins limitative, mais aucune autre que ces deux licences ne sera acceptée pour les didacticiels sur le site de la Fondation Blender.

Le premier véritable projet professionnel dans lequel Blender a été utilisé était "Spider Man 2", où il a été essentiellement utilisé pour la création des animations et des aperçus lors de l’élaboration de la maquette du film.
"Vendredi ou un autre jour" est le premier long métrage utilisant Blender pour tous les effets spéciaux, réalisé sur des stations GNU/Linux. Il a gagné un prix au festival international du film de Locarno. Le film "Le Masque de la Mort Rouge" a également profité de toutes les capacités de Blender. Dernièrement les films "Home", "Oscar et la Dame rose", "Brendan et le Secret de Kells" et "Mr. Nobody" ont également profité de Blender. Les effets spéciaux de ces films ont été réalisés par l'entreprise belge Digital Graphics. La websérie "Le Visiteur du Futur" utilise Blender pour ses effets spéciaux.

En septembre 2005, des artistes et développeurs de Blender commencèrent à travailler sur un court métrage – "Elephants Dream" – en utilisant quasi exclusivement des logiciels libres. Cette initiative, connue sous le nom de "Orange Movie Project", avait pour but d'évaluer les capacités de Blender dans le milieu du cinéma professionnel.

Le résultat de ce travail, "Elephant Dreams", a été diffusé le en avant-première, puis distribué aux contributeurs financiers sous forme de DVD comprenant non seulement le film en haute-définition, mais également l'intégralité des sources : scènes, textures, etc.

"Plumíferos" est un projet de film commercial argentin entièrement créé sous Blender, de la société Manos Digitales Animation Studio. Il est actuellement disponible sur le site officiel et des bandes-annonces ont été présentées aux "Blender Conferences" de 2005 et 2006.
Le film a été fini en 2009 et est sorti sur les écrans argentins le .

"Big Buck Bunny" est un court métrage créé également par la Fondation Blender, laquelle, pour gérer ce projet et les suivants, a fondé l'Institut Blender. Ce court métrage est en production depuis le et s'inscrit dans la même logique que "Elephants Dream". Connu durant sa conception sous le nom de "Projet Pêche", ce projet a vu son nom officiel rendu public le .

Ce nouveau volet part sur un concept complètement différent du premier, en abandonnant le côté mystique pour aller vers du « drôle et doux » ("funny and furry" selon l'expression anglaise). Il est disponible en téléchargement depuis le .

Un nouveau projet de court métrage, "Sintel" (projet Durian), est annoncé par la Fondation Blender en mai 2009. Le thème se rapproche de la fantasy épique, il est un prétexte pour développer la création d'effets spéciaux avec Blender. Ce film a servi de support pour le développement de la nouvelle version majeure de Blender, la mouture 2.5, qui se caractérise par une refonte complète du logiciel.
Il est disponible sur le site officiel du projet depuis le . Durant toute la période de création, il a été possible de suivre le développement de l'ensemble du projet en temps réel sur le blog Durian.

"Tears of Steel" (produit sous le nom de Projet Mango) est réalisé par l'américain Ian Hubert. Il se focalise sur la réalisation d'effets spéciaux pour le cinéma, comme des outils pour la capture de mouvement ou un éclairage plus réaliste par exemple. L'action du film se déroule à Amsterdam et est de type science-fiction.

Il est officiellement sorti le , il est disponible (avec sous-titrage en français) sur le site officiel du projet.

Le dernier projet en cours de la Fondation Blender s'intitule "Gooseberry", il se profile comme le premier long métrage libre. Il sera réalisé par le français Mathieu Auvray qui dirigera 12 studios à travers le monde soit une équipe de 70 à 80 personnes en tout (développeurs compris).

La campagne de financement a débuté le avec pour objectif de rassembler abonnés au Blender Cloud. La campagne a été étendue le avec des objectifs revus à la baisse. Elle se finira le .

Grâce à un moteur de jeu intégré nommé « Blender Game Engine » (BGE), Blender peut être utilisé comme moteur de jeu et moteur 3D gérant le rendu, la logique du jeu, la simulation des interactions physiques et l'audio. La logique de l'application est contrôlée par des scripts Python et un système de briques graphique permettant de connecter ensemble des briques Capteurs, des briques Raisonnement et des briques Effecteurs. La simulation physique repose sur le moteur physique Bullet. Ce moteur apparaît dans la version 2.42.

La NASA a développé une application web interactive appelé "Experience Curiosity" pour célébrer le de atterrissage du rover Curiosity sur Mars. This Blend4Web-based app. L'application permet de contrôler le rover, ses caméras, son bras articulé et de reproduire les événements importants de la mission Mars Science Laboratory. L'application a été présentée au début de la section WebGL au SIGGRAPH 2015.

Deuxième projet de l'Institut Blender, "Yo Frankie!" est un jeu vidéo libre en 3D, sous licence Creative Commons. Le projet a débuté le et est sorti en DVD le et en téléchargement le .

Les personnages principaux du jeu sont basés sur ceux du projet de film d'animation "Big Buck Bunny". Il s'agit d'un jeu multiplate-forme tournant sous les systèmes GNU/Linux, Mac OS X et Microsoft Windows utilisant Blender pour la modélisation et l'animation, ainsi que le kit de développement Crystal Space et le langage de programmation Python.

L'objectif du projet a été de parvenir à réaliser un jeu libre d'un niveau équivalent aux meilleures productions commerciales non libres.

Le projet Apricot a stimulé le développement et l'optimisation du moteur de jeu de Blender.
De par sa nature opensource, des projets secondaires se basent sur blender. On pourra évoquer :





</doc>
<doc id="383" url="https://fr.wikipedia.org/wiki?curid=383" title="Biologie moléculaire">
Biologie moléculaire

La biologie moléculaire (parfois abrégée bio. mol.) est une discipline scientifique au croisement de la génétique, de la biochimie et de la physique, dont l'objet est la compréhension des mécanismes de fonctionnement de la cellule au niveau moléculaire. Le terme « biologie moléculaire », utilisé la première fois en 1938 par Warren Weaver, désigne également l'ensemble des techniques de manipulation d'acides nucléiques (ADN, ARN), appelées aussi techniques de génie génétique.

La biologie moléculaire est apparue au , à la suite de l'élaboration des lois de la génétique, la découverte des chromosomes et l'identification de l'ADN comme support chimique de l'information génétique.

La biologie moléculaire est apparue dans les années 1930, le terme n'ayant cependant été inventé qu'en 1938 par Warren Weaver. Warren Weaver était à l'époque directeur des Sciences Naturelles pour la Fondation Rockefeller et pensait que la biologie était sur le point de vivre une période de changements significatifs étant données les avancées récentes dans les domaines tels que la diffractométrie de rayons X. Il a donc investi des sommes importantes provenant de l'Institut Rockefeller dans les domaines biologiques.

Après la découverte de la structure en double hélice de l'ADN en 1953 par James Watson (1928-), Francis Crick (1916-2004), Maurice Wilkins (1916-2004) et Rosalind Franklin (1920-1958), la biologie moléculaire a connu d'importants développements pour devenir un outil incontournable de la biologie moderne à partir des années 1970.

Les chercheurs en biologie moléculaire utilisent des techniques spécifiques pour la biologie moléculaire (voir plus loin "Techniques de biologie moléculaire"), mais les combinent de plus en plus avec les techniques et les idées provenant de la génétique et de la biochimie. Il n'y a pas de frontière bien définie entre ces disciplines, bien qu'il y en ait eu à une certaine époque. La figure ci-contre illustre une vue possible de la relation entre les domaines :


L'essentiel du travail en biologie moléculaire est quantitatif, et récemment beaucoup de travaux ont été faits à l'intersection de la biologie moléculaire et de l'informatique, dans la bio-informatique et dans la biologie calculatoire. Depuis les années 2000, l'étude de la structure et de la fonction des gènes, la génétique moléculaire, fait partie des sous-domaines les plus saillants de la biologie moléculaire.

De plus en plus d'autres domaines de la biologie se concentrent sur les molécules, soit directement, en étudiant leurs interactions propres comme en biologie cellulaire et en biologie du développement, soit indirectement, quand les techniques de la biologie moléculaire sont utilisées pour déduire les attributs historiques des populations ou des espèces, comme dans les domaines de la biologie de l'évolution telles que la génétique des populations et la phylogénie. Il y a également une longue tradition d'étude des biomolécules « à partir du bas » en biophysique.

Depuis la fin des années 1950 et le début des années 1960, les biologistes moléculaires ont appris à caractériser, isoler et manipuler les composants moléculaires des cellules et des organismes. Ces composants incluent l'ADN, support de l'information génétique, l'ARN, proche de l'ADN dont les fonctions vont de la copie provisoire d'ADN jusqu'aux réelles fonctions structurelles et enzymatiques et qui est une partie fonctionnelle et structurelle de l'appareil traductionnel, et les protéines, molécules structurelles et enzymatiques les plus importantes des cellules.

Une des techniques les plus élémentaires en biologie moléculaire pour étudier le rôle des protéines est le clonage d'expressions. Dans cette technique, l'ADN codant la protéine qui nous intéresse est cloné en utilisant la réaction en chaîne par polymérase (PCR en anglais pour "Polymerase Chain Reaction") et/ou des enzymes de restriction dans un plasmide (qu'on appelle vecteur d'expression). Ce plasmide peut avoir des éléments de séquences promotrices spéciales pour diriger la production de la protéine en question et peut aussi avoir des marqueurs de résistance antibiotique pour aider à suivre le plasmide.

Ce plasmide peut être inséré dans des cellules, soit de bactérie, soit d'animal. Introduire de l'ADN dans des cellules bactériennes est appelé transformation, et cela peut être complété de plusieurs manières : électroporation, micro-injection, consommation passive et conjugaison. Introduire de l'ADN dans des cellules d'eucaryotes, telles que des cellules animales, est appelé transfection. Plusieurs techniques différentes de transfection sont disponibles : transfection calcium phosphate, transfection de liposomes ou lipofection, électroporation ou encore par réactifs de transfection propriétaires tels que le Fugene ou le Genecellin. L'ADN peut alors être introduit dans les cellules en utilisant des virus ou des bactéries pathogènes comme transporteurs. Dans de tels cas, la technique est appelée transduction virale/bactérienne, et les cellules sont dites transduites.

Dans les deux cas, le codage ADN pour la protéine qui nous intéresse est maintenant à l'intérieur d'une cellule, et la protéine peut maintenant s'exprimer. Une variété de systèmes, tels que des promoteurs inductibles et des facteurs spécifiques signalant les cellules, sont disponibles pour aider la protéine qui nous intéresse à s'exprimer à haut niveau. De grandes quantités de protéines peuvent alors être extraites de la cellule bactérienne ou eucaryote. La protéine peut être testée pour connaître son activité enzymatique dans une variété de situations, elle peut être cristallisée pour qu'on puisse étudier sa structure tertiaire, ou, dans l'industrie pharmaceutique, on peut étudier l'activité de nouveaux médicaments sur la protéine en question.

La réaction en chaîne par polymérase (PCR en anglais, pour "Polymerase Chain Reaction") est une technique extrêmement flexible de copie d'ADN. En gros, la PCR permet à une simple séquence d'ADN d'être copiée des millions de fois, ou d'être altérée par des moyens prédéterminés. Par exemple, la PCR peut être utilisée pour introduire des sites d'enzymes de restriction, ou pour muter (changer) des bases particulières de l'ADN. La PCR peut aussi être utilisée pour déterminer si un fragment particulier d'ADN se trouve dans une bibliothèque d'ADN complémentaires. La PCR a de nombreuses variations, comme la PCR à transcription inversée (RT-PCR en anglais pour "Reverse Transcription Polymerase Chain Reaction") pour l'amplification de l'ARN, et, plus récemment, la PCR temps réel (qPCR) qui permet des mesures quantitatives de molécules d'ADN et d'ARN.

L'électrophorèse est un des principaux outils de biologie moléculaire. Le principe de base est que l'ADN, l'ARN et les protéines peuvent être séparées par des champs électriques. Dans l'électrophorèse en gel d'agarose, l'ADN et l'ARN peuvent être séparés en fonction de leur taille en faisant circuler l'ADN à travers un gel d'agarose. Les protéines peuvent être séparées en fonction de leur poids en utilisant un gel SDS-PAGE. Les protéines peuvent aussi être séparées par leur charge électrique, en utilisant ce qu'on appelle un gel isoélectrique.

Nommé ainsi d'après le nom de son inventeur, le biologiste Edwin Southern, le Southern blot est une méthode pour sonder la présence d'une séquence précise d'ADN à l'intérieur d'un échantillon d'ADN. Des échantillons d'ADN avant ou après digestion par une enzyme de restriction sont séparés par électrophorèse et transférés sur une membrane par marquage via action capillaire. La membrane peut alors être testée en utilisant une sonde ADN marquée avec un complément de la séquence en question. À l'origine, la plupart des protocoles utilisaient des marqueurs radioactifs ; cependant, maintenant, il existe des possibilités de marquages non radioactifs. Le Southern blot est utilisé moins souvent dans les laboratoires, du fait que la PCR permet déjà de détecter des séquences ADN spécifiques à partir d'échantillons d'ADN. Cependant, ces marquages sont encore utilisés pour certaines applications, telles que la mesure du nombre de copies transgéniques dans les souris transgéniques, ou dans l'ingénierie de lignes de cellules souches embryonnaires à gènes invalidés.

Le northern blot est utilisé pour étudier les modèles d'expression d'un type spécifique de molécule d'ARN en comparaison relative avec un ensemble de différents échantillons d'ARN. C'est essentiellement une combinaison d'une dénaturation d'électrophorèse d'ARN, et d'un "blot". Dans ce processus, l'ARN est séparé en fonction de la taille, puis est transféré sur une membrane qui est alors sondée avec un complément marqué pour la séquence intéressante. Les résultats peuvent être visualisés d'une variété de façons selon le marquage utilisé ; cependant, la plupart conduisent à une révélation de bandes représentant la taille de l'ARN détecté dans l'échantillon. L'intensité de ces bandes est liée à la quantité d'ARN ciblé dans les échantillons analysés. Le procédé est utilisé généralement pour étudier quand et combien d'expressions de gènes se produisent en mesurant la quantité de cet ARN présent dans les différents échantillons. C'est un des outils les plus fondamentaux pour déterminer quand certains gènes s'expriment dans les tissus vivants.

Séparation des protéines par électrophorèse SDS-PAGE uniquement en fonction de leur poids (le SDS, ou sodium dodécylsulfate, dénature les structures tertiaire et quaternaire des protéines et les charge toutes négativement), puis transfert des protéines séparées sur membrane pour les rendre accessibles à divers marquages immunologiques ou autres.

Les anticorps pour la plupart des protéines peuvent être créés par injection de petites quantités de protéine cible dans les animaux tels que la souris, le lapin, le mouton ou l'âne (anticorps polyclonaux) ou produits dans une culture de cellules (anticorps monoclonaux). Ces anticorps peuvent être utilisés dans une variété de techniques analytiques et préparatives.

Dans le western blot (immunobuvardage), les protéines sont d'abord séparées en fonction de leur poids, dans un gel fin pris entre deux plaques de verre par une technique qu'on appelle SDS-PAGE (pour "Sodium Dodecyl Sulphate Poly-Acrylamide Gel Electrophoresis"). Les protéines dans le gel sont alors transférées sur un PVDF, nitrocellulose, nylon ou autre membrane de support. Cette membrane peut alors être sondée avec des solutions d'anticorps. Les anticorps qui s'attachent spécifiquement à la protéine en question peuvent alors être visualisés selon une variété de techniques, dont la colorimétrie, la chimiluminescence ou l'autoradiographie.

Des méthodes analogues de western blot peuvent aussi être utilisées pour marquer directement des protéines spécifiques dans des cellules et des sections de tissus. Cependant, ces méthodes de marquages immunologiques sont plutôt associées à la biologie cellulaire qu'à la biologie moléculaire.

Les termes "western" et "northern" sont des jeux de mots : les premiers "blots" étaient sur l'ADN, et comme ils ont été faits par Edwin Southern, ils ont pris le nom de "Southern" ("southern" veut dire « du sud » en anglais ; tandis que "western" signifie « de l'ouest » et "northern", « du nord »). Il est peu probable que Patricia Thomas, inventrice du "blot" ARN, qui est devenu le "northern blot", utilise vraiment ce terme. Pour pousser la plaisanterie plus loin, on peut trouver, dans la littérature , des références vers des "south-westerns" (« du sud-ouest ») (interactions protéine-ADN) et des "far-westerns" (du « far-ouest ») (interactions protéine-protéine).

Une puce à ADN, aussi appelée microarray, est une collection de milliers de puits microscopiques sur un support solide tel qu'une lame de microscope; chaque puits contient un grand nombre de fragments d'ADN identiques qui permet de mesurer l'expression d'un gène particulier par complémentarité de séquence avec ARN correspondant. Les puces permettent ainsi de connaitre le transcriptome, c'est-à-dire l'ensemble des gènes transcrit à un moment donné dans un groupe de cellules données.

Il y a plusieurs manières différentes de fabriquer des puces à ADN ; les plus courantes sont les puces à silicium, lames de microscope dont les taches ont 100 microns de diamètre, les puces qu'on peut adapter à ses besoins, et celles avec des taches plus grosses sur des membranes poreuses (macropuces).

Les puces peuvent aussi être fabriquées pour des molécules autres que l'ADN. Par exemple, une puce à anticorps peut être utilisée pour déterminer quelle protéine ou bactérie est présente dans un échantillon de sang.

Au fur et à mesure que de nouvelles procédures et de nouvelles technologies sont devenues disponibles, les anciennes sont rapidement abandonnées. Des exemples typiques sont les méthodes pour déterminer la taille des molécules d'ADN. Avant l'électrophorèse, avec agarose et polyacrylamide, on calculait la taille de l'ADN par sédimentation dans des gradients sucrés, une technologie lente et laborieuse nécessitant une instrumentation coûteuse ; et avant les gradients sucrés, on utilisait la viscométrie.

Indépendamment de leur intérêt historique, il est intéressant de connaître les anciennes techniques car cela peut être utile pour résoudre des problèmes particuliers.





</doc>

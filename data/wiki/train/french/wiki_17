<doc id="1823" url="https://fr.wikipedia.org/wiki?curid=1823" title="Letton">
Letton

Le letton (ou lette ; en letton : "latviešu [valoda]") est une langue qui appartient au groupe balte oriental de la famille indo-européenne.

Étroitement apparenté au lituanien, le letton s'est formé jusqu'au à partir des dialectes lette, curonien, le sémigalien et le sélonien aujourd'hui disparus.
Ces dialectes semblent avoir émergé au cours du Haut Moyen-Age avec l'affirmation des tribus du même nom et les contacts et influences mutuelles importantes avec les tribus finno-ougriennes voisines (Estoniens, Oeseliens, Lives à partir du Xe siècle). On trouve ainsi de nombreux fennicismes dans le letton ainsi qu'à l'inverse dans l'estonien un certain nombre de mots empruntés aux langues baltes. Plutôt qu'une hypothétique langue balto-slave, les chercheurs s'accordent à penser que ce sont aussi les multiples contacts avec les Slaves durant cette même période ainsi que la slavisation des territoires baltes plus à l'Est durant le Moyen-Age qui expliquent la présence cependant limitée d'un vocabulaire d'origine slave dans la langue lettonne (comme lituanienne). De manière ainsi caractéristique de nombreux mots lettons concernant le christianisme semblent d'origine slave, attestant des premiers contacts établis entre les tribus baltes et des missionnaires ou populations slaves christianisés dès le Xe siècle.

Avec les croisades allemandes et la conquête des territoires lettons, les dialectes lettons restent cantonnées au monde paysans et à partir du Xe siècle aux serfs, l'allemand étant la langue des élites. Cette christianisation par le fer et cette domination politique et culturelle des élites allemandes aboutiront à une influence importante de l'allemand sur le letton.
Il faudra attendre la Réforme et le souci d'utiliser les langues locales pour effectuer le travail évangélique pour voir apparaître les premiers textes écrits en lettons, souvant à l'usage des pasteurs allemands envoyés dans les campagnes. 

La disparition des anciennes tribus et la vie sous un même régime politique et social, les différentes migrations liées à celui-ci et aux différents conflis ont abouti à l'unification progressive de la nation lettonne et au rapprochement progressivement des dialectes, avec la mise au point progressive d'une langue commune qui va devenir le letton.

Seul le latgalien va connaître une évolution originale en raison du rattachement du territoire de Latgale (Livonie Polonaise) à la Pologne en 1629 et donc à une influence catholique et polonaise prolongée qui vont dissocier l'évolution de la région du reste du pays. De ce fait se maintiendra jusqu'à nos jours ce dialecte particulier.

Les plus anciens textes écrits en letton sont des hymnes traduits par Nicholas Ramm, un pasteur allemand à Rīga, recueil qui date de 1530.

Le passage sous domination russe au XVIIIe siècle a renforcé l'influence de la langue russe même si le mouvement de renouveau national a permis la codification d'une langue écrite et l'essort d'un mouvement littéraire qui vont profiter de la création de l'Etat letton en 1918.

Pendant l'occupation soviétique dans la seconde moitié du , des Russes ont immigré dans le pays sans apprendre le letton. Le russe dominait dans pratiquement toutes les sphères de la société, et le letton devenait de plus en plus marginalisé dans le pays. Cependant, le letton ne perdit pas pour autant son statut de langue officielle. Aujourd'hui, le letton est la langue maternelle de près de 60 % de la population du pays et de moins de 50 % dans les villes principales. Dans le processus pour l'indépendance du début des années 1990, la Lettonie, tout comme l'Estonie, proposa des lois pour prévenir l'extinction de la langue.

Le letton se divise en deux dialectes principaux : le haut letton (latgalien) et le letton occidental, qui représente la forme littéraire actuelle.

Le letton est la langue maternelle de 1,4 million de personnes en Lettonie où c'est la langue officielle et d'environ personnes à l'étranger.

Bien qu'on ait écrit en écriture gothique avant le , le letton utilise aujourd'hui l'alphabet latin enrichi de signes diacritiques (macron, háček, cédille). Son alphabet de 34 lettres ne contient pas "q", "w", "x" ni "y" (sauf dans des mots étrangers), mais y ajoute les lettres diacritées "ā", "č", "ē", "ģ", "ī", "ķ", "ļ", "ņ", "ŗ", "š", "ū" et "ž". Le phonème noté par "ŗ" s'étant amuï, la lettre est de nos jours généralement ignorée, du moins dans les publications de Lettonie propre. Le digraphe "ch" [x] a disparu également. Le "ō" n'est utilisé que dans le dialecte latgalien et on a cessé son utilisation au début des années 1940. Avant 1921, les cédilles sous les lettres ģ, ķ, ņ, ŗ pouvaient être rendues par des barres obliques avec les lettres ꞡ, ꞣ, ꞥ, ꞧ.

Les diphtongues "ai", "au", "ei", "ia", "iu", "ui" et "uo" s'écrivent respectivement "ai", "au", "ei", "ie", "iu", "ui" et "o".

Chaque phonème est noté par une lettre propre, à l'exception de "dz" et "dž", des deux sons écrits par les lettres "e" et "ē" (ouvert et fermé), et la voyelle orale "o" (longue ou brève) des mots étrangers n'est pas distinguée graphiquement de la diphtongue proprement lettonne "o" – laquelle peut être notée dans des ouvrages didactiques "uo". Il est donc facile de deviner la prononciation d'un mot en le lisant. L'accent de hauteur n'est pas mobile comme en lituanien, mais se place sur la première syllabe, à quelques exceptions près.

Décrire l'alphabet letton sans y intégrer les digrammes ("dz", "dž" et le "ch" aujourd'hui abandonné, ainsi que les diphtongues) montre la grande confusion qui règne dans les ouvrages et les esprits. À l'exception parfois de "ie" – qu'on peut trouver placée après "i" et "ī" –, les diphtongues ne sont pas distinguées dans les listes alphabétiques. On les trouve donc dans l'ordre purement alphabétique latin.


Le Letton possède deux genres (masculin et féminin) ainsi que 7 cas grammaticaux. Les adjectifs épithètes précèdent généralement le nom qu'ils modifient et s'accordent en genre, en nombre et en cas. De même, les adjectifs prennent un suffixe différent pour indiquer une interprétation définie ou indéfinie:

Déclinaison de "tas": celui-là, ce...là

Déclinaison de "šis": celui-ci, ce

La déclinaison du letton concerne les substantifs, les adjectifs, les adjectifs numéraux et les pronoms. Elle varie selon sept cas : nominatif, génitif, datif, accusatif, instrumental, locatif et vocatif.

Les noms peuvent être classés comme étant soit déclinables soit indéclinables. La plupart d'entre eux sont déclinables et appartiennent à l'une des six classes de déclinaison (trois pour les noms masculins et trois pour les noms féminins). Le cas instrumental est toujours identique à l'accusatif au singulier et au datif au pluriel. Il est utilisé en tant que cas indépendant (ie: sans utiliser de préposition) seulement dans des contextes très particuliers en Letton moderne.

Les trois déclinaisons du masculin se distinguent selon les propriétés suivantes :


Le paradigme complet des terminaisons pour ces trois déclinaisons est donné dans le tableau suivant :

La déclinaison décrit une palatalisation de la consonne radicale finale au génitif singulier ainsi qu'au pluriel (p → pj dans l'exemple ci-dessus). Les noms composés et les noms propres terminés par "-dis" ou "-tis" font exception à cette règle (ex. : Atis, gen. sing. Ata).

Une petite partie de ces noms de la déclinaison possède un nominatif singulier identique au génitif (la plupart d'entre eux se termine en "-ens"). Ceux-ci font partie de qu'on appelle les noms à consonne radicale (ex. : "akmens" "pierre", "asmens" "lame", "mēness" "lune", "rudens" "automne", "sāls" "sel", "ūdens" "eau" and "zibens" "éclair". La déclinaison du nom "suns" "chien" possède le génitif singulier régulier "suņa".




</doc>
<doc id="1827" url="https://fr.wikipedia.org/wiki?curid=1827" title="Liste de langues">
Liste de langues

Cette liste de langues rassemble les langues naturelles et langues construites, qu'elles soient vivantes ou mortes. Elle est arbitrairement limitée aux langues possédant un code ISO 639-3.

La propose une liste plus complète mais moins détaillée.




</doc>
<doc id="1829" url="https://fr.wikipedia.org/wiki?curid=1829" title="Légende">
Légende

Une légende (de l'adjectif verbal latin "legenda ", « qui doit être lue ») est, à l’origine, un récit mis par écrit pour être lu publiquement : généralement il s'agit d'une hagiographie lue dans les monastères, pendant les repas ; dans les églises, pour l’édification des fidèles lors de la fête d’un saint. Au s'opère un glissement de sens, la légende devenant un récit à caractère merveilleux où les faits historiques sont transformés par l'imagination populaire ou l'invention poétique. Cette évolution . Dans ce genre de littérature, la précision historique passe ainsi au second plan par rapport à l’intention spirituelle.

Dans la langue courante, le mot "légende" est aujourd'hui devenu synonyme de "mythe", et renvoie à quelque chose dont l'existence n'a jamais pu être prouvée.

Le légendaire désigne l'auteur, le compilateur de légendes mais aussi le recueil de légendes.

Certains auteurs distinguent encore le mythe de la légende. Selon eux, une légende tient de faits réels ; une histoire est racontée puis est transmise par oral d'où les modifications. On peut la définir comme un récit qui mêle le vrai et le faux. 

Le récit fictif dans les légendes, mythes, contes et fables, est le plus souvent d'origine orale et fait appel au merveilleux. Une légende est fortement liée à un élément clé, ceci est précisé et se concentre sur un lieu, un objet, un personnage, une histoire, etc. Au fil du temps, la légende devient un mythe pour les sociétés futures, car elle perd en précision et gagne en fantaisie et en amplification.

Aujourd'hui les notions de mythe, de légende, de conte et de fable sont souvent confondues.

Jean-Pierre Bayard, dans son "Histoire des légendes", énumère une dizaine de théories relatives aux origines des légendes. La théorie anthropologique, soutenue par Henri Gaidoz, Wilhelm Mannhardt et Edward Tylor, postule que les légendes proviendraient de pensées humaines primitives, de restes de religions et cultures élémentaires. La théorie astrale ou naturaliste considère les contes et les légendes étiologiques comme divinisant les grandes manifestations de la nature. La théorie mythologique est avancée par Grimm qui attribue la création des contes à l'enfance préhistorique de la patrie, Angelo De Gubernatis, à un naturalisme enfantin, Schelling, à la conscience individuelle du peuple qui ajoute aux légendes créées une signification religieuse. La théorie linguistique considère que les légendes sont issues de la transmission de récits entre plusieurs peuples qui empruntent les mots à d'autres cultures, les déforme, ce qui obscurcit le sens primitif originel et donne naissance à de nouveaux récits. 





</doc>
<doc id="1831" url="https://fr.wikipedia.org/wiki?curid=1831" title="Langues par famille">
Langues par famille

Une famille de langues est un groupe de langues, descendantes d’une langue-ancêtre commune (appelée proto-langue).

Le classement traditionnel ci-après combine cette logique « généalogique » avec un classement géographique (par exemple pour les langues caucasiennes ou le castillan). Par ailleurs :

Une encyclopédie numérique des langues est consultable sur le site du programme Sorosoro pour la préservation de la diversité linguistique.

Ceci est une liste des langues naturelles ou construites, vivantes ou mortes, classées par familles et par groupes. La notion de "famille génétique" de langues est décrite plus en détail dans l'article consacré à la linguistique comparée.

Les tentatives de certains linguistes, fréquentes depuis la deuxième moitié du , proposant de rassembler certaines familles de langue dans des super familles de langues (par exemple considérer que les langues altaïques et ouraliennes font partie d'une même famille dite « ouralo-altaïque ») n'ayant pas donné de résultats probants jusqu'à présent, seules les super familles dont la reconnaissance fait l'objet d'un large consensus, telles que les langues indo-européennes (qui ont été mises en évidence dès le) ou les langues austronésiennes, sont répertoriées.

Pour trouver rapidement une langue dans cette liste, faites Ctrl+F (ou "Édition > Rechercher" dans la barre d'outils de votre navigateur), et tapez le nom de la langue que vous cherchez.


Les groupes suivants sont classés à part :

"Cette liste n'est pas exhaustive."

Ce sont des langues en Inde autres que celles d'origine indo-européenne.






Les langues indo-européennes sont les plus répandues. Elles sont parlées en Europe, en Asie du Nord (Russie), en Iran, en Arménie, en Afghanistan, au Tadjikistan, au Pakistan et dans de nombreuses région de l'Inde, ainsi qu'en Amérique, Afrique et Océanie où leur apparition est toutefois plus récente.

Ce regroupement surtout géographique comprend un vaste ensemble de langues antiques parlées dans les Balkans et les régions limitrophes. On y rattache l'Albanais, seule langue vivante issue du groupe (du carpien selon les linguistes, mais politiquement, le régime d'Enver Hoxha a promu la théorie protochroniste d'une origine illyrienne, formulée par Zacharie Mayani). Plusieurs langues classées comme paléo-balkaniques auparavant ont été reclassées depuis dans d'autres familles ; restent les suivantes :








Certains linguistes rattachent le vénète à ce groupe.


Le ligure ancien † est vu par certains linguistes comme proche des langues celtiques.


Ce regroupement ne fait pas l'unanimité, certains linguistes considérant les langues baltes et slaves comme deux familles de langues bien distinctes.

Les langues chamito-sémitiques (nommées aussi "afro-asiatiques") sont parlées en Afrique septentrionale et saharienne ainsi qu'au Proche-Orient et au Moyen-Orient.





Le positionnement de ces langues est discuté. Elles sont vues, soit comme une sixième branche des langues afro-asiatiques, soit comme le groupe occidental des langues couchitiques.



Elles sont parlées en Afrique sub-saharienne : au Tchad, au Soudan, au Niger, au Mali, dans le Nord du Cameroun, en République centrafricaine, au Ghana, au Kenya, en Éthiopie et en Tanzanie.








Elles sont parlées en Afrique australe, notamment par les Bochimans et les Khoïkhoï.

Les langues tuu ont fait partie de cette catégorie, mais cette classification est aujourd'hui obsolète.

Le Caucase est une zone de très grande diversité linguistique, et le foyer de trois familles indépendantes de langues non représentées ailleurs et appelées langues caucasiennes au sens strict. Cette appellation n'implique pas de parenté génétique. Par ailleurs, des langues indo-européennes et altaïques sont également parlées dans le Caucase.




Elles sont parlées en Europe et en Asie du Nord.

Elles étaient parlées dans l'est de la Sibérie. Seul subsiste encore le ket.

Elles sont parlées dans le nord-est de la Sibérie et dans la presqu'île du Kamtchatka, dans l'extrême est de la Russie.

L'hypothèse de la famille altaïque reste très largement débattue et non consensuelle.

Elles regrouperaient un total de 558 millions de locuteurs de par le monde et sont essentiellement parlées au Proche et au Moyen-Orient, en Asie centrale et de l'Est et du Nord. Certains linguistes contestent qu'il s'agisse d'un groupe génétique. Depuis les années 1960 des chercheurs y rattachent généralement le coréen et les langues japoniques.

Certains réfutaient auparavant la filiation, d'une part par manque de fiabilité des relations dans le vocabulaire de base—mais celles-ci furent consolidées en 2004—et d'autre part à cause de la plus grande simplicité du japonais—fait qui a reçu une explication en 2008.


Parlées dans l'archipel japonais et les îles Ryūkyū. Certains linguistes les rapprochent des langues altaïques et les indices sont de plus en plus nombreux (voir l'entête de langues Altaïques).




Elles sont essentiellement parlées en Asie de l'Est et du Sud-Est. Selon les linguistes Joseph Greenberg et Merritt Ruhlen, elles constituent le groupe de langues parlées par le plus grand nombre de locuteurs au sein de la macro-famille linguistique des langues dené-caucasiennes.
Le terme de "langues sino-tibétaines" est sujet à controverse. L'origine de l'écriture tibétaine remonte à Songtsen Gampo (né vers 609-613~mort en 650) qui fut le du Tibet. Songtsen Gampo envoya en Inde des Tibétains pour y étudier le sanskrit. Le ministre Thonmi Sambhota créée l’écriture tibétaine à partir d'un alphabet de l'Inde du nord, probablement le Brahmi, ancêtre d'un certain nombre d'alphabets comme le devanāgarī, le Gujarati ou le Bengali.




Les langues tai-kadai sont classées en trois branches :

Également appelées miao-yao, elles sont parlées essentiellement en Asie du Sud-Est.
On distingue trois groupes :
Dans la classification chinoise établie par les linguistes Wang, Mao, Meng et Zheng, le hmong se répartit en branches :
Dans la classification chinoise établie par les linguistes Wang, Mao, Meng et Zheng, le mien se répartit en trois branches :

Elles sont parlées essentiellement en Asie du Sud-Est.

Il s'agit des langues parlées par une partie des populations aborigènes de la péninsule Malaise, appelées officiellement "Orang Asli" ("gens des origines" en malais).



Elles sont parlées à Taïwan, en Asie du Sud-Est, dans l'océan Pacifique et à Madagascar et Mayotte.

Il s'agit des langues des aborigènes de Taïwan.


Ces langues, très nombreuses, sont parlées en Nouvelle-Guinée et dans les îles proches. Leur classification interne, encore mal établie, ne fait pas l'objet d'un consensus. Une hypothèse propose de rassembler nombre de familles de langues papoues dans une famille de trans-nouvelle-guinée, mais l'inclusion de plusieurs familles reste controversée. Plusieurs langues papoues restent des langues isolées ou non classées.





















































Elles sont parlées dans les régions arctiques de l'Amérique du Nord et de la péninsule tchouktche.

Elles sont parlées en Amérique du Nord.

Elles sont parlées à l'Ouest des États-Unis et au Mexique.

Elles sont parlée dans le Sud-Ouest des États-Unis, notamment par les peuples pueblos. Une hypothèse les relie aux langues uto-aztèques.

Elles sont parlées en Amérique du Nord.

Le wiyot et yurok sont parfois rassemblés dans un groupe "ritwain". Cette hypothèse est contestée.

Elles sont parlées en Amérique du Nord.

Elles sont parlées en Amérique du Nord.

Elles sont parlées en Amérique du Nord.

L'appartenance des différentes familles de langues au groupe hokan n'est pas parfaitement prouvée.















L'appartenance des différentes familles de langues au groupe pénutien n'est pas parfaitement prouvée.






guambiano, totoro, awa pit, tsafiqui, cayapa

Elles sont parlées en Amérique du Sud, en Amazonie et rassemblent dix familles de langues.
Autres, à classer:









Ce sont des langues gestuelles dont la structure iconique et spatiale les distingue des langues orales sur le plan grammatical. La plupart sont utilisées pour la communication avec les personnes sourdes ou malentendantes, ces langues ont été classées par familles selon la langue des signes qui est leur « ancêtre » principal (il existe de nombreux cas de métissage avec les langues utilisées dans les mêmes régions géographique et des phénomènes de créolisation, faisant évoluer les langues des signes). Une classification a été établie par Anderson et Peterson en 1979, qui a été reprise par Henri Wittmann en 1991. Cette dernière propose la liste de familles suivante :
On peut ajouter à cette liste la famille de la langue des signes arabe dont les langues n'ont pas été traitées par Wittmann.

Il existe aussi plusieurs langues des signes « auxiliaires », qui sont utilisées par des personnes entendantes lorsque l'usage du langage parlé n'est pas possible (utilisation de mots tabous, vœu de silence, signaux militaires) ou même dans un but artistique et symbolique (mudrā).







</doc>
<doc id="1832" url="https://fr.wikipedia.org/wiki?curid=1832" title="Langage de haut niveau">
Langage de haut niveau

En programmation informatique, un langage de haut niveau est un langage de programmation orienté autour du problème à résoudre, qui permet d'écrire des programmes en utilisant des mots usuels des langues naturelles (très souvent de l'anglais) et des symboles mathématiques familiers. Un langage de haut niveau fait abstraction des caractéristiques techniques du matériel utilisé pour exécuter le programme, tels que les registres et les drapeaux du processeur.

Les langages de haut niveau sont plus proches des langues naturelles, ce qui facilite et vulgarise l'écriture des programmes. Ils sont généralement indépendants de la machine : le même programme pourra être utilisé tel quel sur plusieurs types d'ordinateurs — quoique les programmes puissent également être conçus pour un système d'exploitation en particulier.

Les langages de haut niveau sont apparus dans la seconde moitié des années 50 (Fortran en 1954, Lisp et Algol en 1958, COBOL en 1959). Ils ont permis d'écrire des programmes d'une manière plus familière, proche de l'anglais, et qui ne dépend pas du processeur qui sera utilisé.

En 2010, il existe plus de 200 langages de programmation de haut niveau.

Le terme « langage de haut niveau » n'implique pas que ce type de langage soit supérieur à un langage de bas niveau. La notion de profondeur désigne la distance du langage par rapport au travail de la machine. Le langage de haut niveau a un plus haut niveau d'abstraction que les langages machines. Plutôt que de s'occuper des registres, des accès mémoires et des piles, les langages de haut niveau s'occupent de concepts plus élaborés tels que les processus légers, verrous, objets, variables, tableaux, arithmétique complexe et expressions booléennes. De plus, ils n'ont en général pas la possibilité de s'occuper des détails liés à la machine tels que la gestion mémoire contrairement aux langages de bas niveau ou alors ces langages font appel à des fonctions préprogrammées (comme les opérateurs new et delete en C++). D'autres caractéristiques telles que des routines de manipulation de chaîne de caractères ou les concepts des langages objets peuvent être présentes.

De façon stéréotypée, les langages de haut niveau simplifient le travail du programmeur là où les langages de bas niveau permettent de produire un code plus efficace. La limite d'utilisation des langages haut niveau correspond aux situations où les ressources matérielles sont limitées. Les concepts de programmation haut niveau tels que les structures de données génériques, l'interprétation à l'exécution sont souvent lents à l'exécution et grands consommateurs mémoire. Pour cette raison, les codes qui ont besoin de s'exécuter rapidement et efficacement devraient être écrits en langage de bas niveau, même si la programmation en langage de haut niveau se ferait plus facilement.

Néanmoins, avec l'augmentation en complexité des architectures des microprocesseurs modernes et l'amélioration des compilateurs, on observe que fréquemment les langages de haut niveau produisent du code aussi efficace que ce qui peut être fait à la main par la plupart des programmeurs. De plus, une abstraction plus haute peut permettre des techniques plus puissantes produisant généralement de meilleurs résultats que leurs équivalents de bas niveau.



</doc>
<doc id="1833" url="https://fr.wikipedia.org/wiki?curid=1833" title="Langage machine">
Langage machine

Le langage machine, ou code machine, est la suite de bits qui est interprétée par le processeur d'un ordinateur exécutant un programme informatique. C'est le langage "natif" d'un processeur, c'est-à-dire le seul qu'il puisse traiter. Il est composé d'instructions et de données à traiter codées en binaire.

Chaque processeur possède son propre langage machine, dont un code machine qui ne peut s'exécuter que sur la machine pour laquelle il a été préparé. Si un processeur "A" est capable d'exécuter toutes les instructions du processeur "B", on dit que "A" est compatible avec "B". L'inverse n'est pas forcément vrai : "A" peut avoir des instructions supplémentaires que "B" ne connaît pas.

Le code machine est aujourd'hui généré automatiquement, généralement par le compilateur d'un langage de programmation ou par l'intermédiaire d'un bytecode.

Les « mots » d'un langage machine sont appelés "instructions". Chacune d'elles déclenche une commande de la part du processeur (par exemple : chercher une valeur dans la mémoire pour charger un registre, additionner deux registres, etc.).

Un processeur à architecture RISC ne reconnaît que peu d'instructions différentes, alors qu'un processeur à architecture CISC en possède un large éventail. Néanmoins certains processeurs CISC récents transforment en interne les instructions complexes en une suite d'instructions simples, qui sont alors exécutées.

Un programme est juste une longue séquence d'instructions qui sont exécutées par le processeur. Elles sont exécutées séquentiellement sauf quand une instruction de "saut" transfère l'exécution à une autre instruction que celle qui suit. Il existe également des sauts conditionnels qui sont soit exécutés (l'exécution continue à une autre adresse), soit ignorés (l'exécution continue à l'instruction suivante) selon certaines conditions.

Chaque instruction commence par un nombre appelé opcode (ou "code opération") qui détermine la nature de l'instruction.
Par exemple, pour les ordinateurs d'architecture x86, l'opcode codice_1 (en binaire codice_2) correspond à l'instruction "push" (ajouter une valeur en haut de la pile).
Par conséquent, l'instruction codice_3 (codice_4) correspond à "push 0x14" (ajouter la valeur hexadécimale codice_5 , ou 20 en décimal, en haut de la pile).

Certains processeurs codent toutes leurs instructions avec le même nombre de bits (par exemple : ARM, MIPS, PowerPC), tandis que chez d'autres la longueur de l'instruction dépend de l'opcode (exemple : x86). L'organisation des combinaisons de bits dépend largement du processeur. Le plus commun est la division en "champs". Un ou plusieurs champs spécifient l'opération exacte (par exemple "une addition"). Les autres champs indiquent le type des opérandes, leur localisation, ou une valeur littérale (les opérandes contenus dans une instruction sont appelés "immédiat").

Lorsque toutes les instructions ont la même taille elles sont également alignées en mémoire. Par exemple si toutes les instructions sont alignées sur 32 bits (), alors les deux bits de poids faibles de l'adresse mémoire de n'importe quelle instruction sont à zéro. Cela permet notamment une implémentation plus aisée du cache des prédictions de branchement bimodales.

En revanche le code machine prend moins de place en mémoire s'il ne possède pas de taille minimum, étant donné qu'on élimine les champs non utilisés.

Alors que le langage machine était le seul disponible à l'aube des ordinateurs, il est aujourd'hui très long et fastidieux de développer en binaire : il faut passer par au moins un langage intermédiaire.

De très nombreux langages de programmation sont transformés en langage machine lors de la compilation. Tous les programmes exécutables contiennent au moins une petite partie en langage machine.

Le langage le plus facile à convertir en code machine est l'assembleur car il possède quasiment les mêmes instructions. L'assembleur (ou langage assembleur) diffère d'une machine à une autre, bien que les instructions soient au bout du compte très semblables. Les langages de plus haut niveau sont convertis en assembleur pendant la compilation. Les langages utilisant une machine virtuelle passent par un bytecode qui est converti à la volée par la machine virtuelle.

Comme exemple spécifique, regardons l'architecture MIPS. Ses instructions ont toujours une longueur de 32 bits. Le type général de l'instruction est donné par les 6 bits de poids les plus forts (dans une représentation sur 32 bits, les 6 de gauche), qu'on appelle le champ "op".

Les instructions de type-J et de type-I sont pleinement spécifiées par le champ "op". Les instructions de type-R ont un champ supplémentaire, "fonct", pour déterminer la nature exacte de l'opération. Les champs de ces 3 types d'instructions sont :

"rs", "rt", et "rd" indiquent des opérandes de type registre ; "shamt" indique un décalage ("") ; et le champ "adresse" ou "immédiat" contient un opérande sous forme de valeur.

Par exemple, ajouter les registres 1 et 2 et placer le résultat dans le registre 6 est codé :

Charger une valeur depuis la cellule mémoire 68 cellules après celle pointée par le registre 3 dans le registre 8 :

"Sauter" à l'adresse codice_6 (la prochaine instruction à exécuter se trouve à l'adresse codice_6) :

Les processeurs de l'architecture ARM sont un cas particulier dans la mesure où toutes les instructions sont conditionnelles. Elles sont toutes d'une longueur de 32 bits, et leurs quatre premiers bits indiquent dans quelles conditions l'instruction doit être exécutée.



</doc>
<doc id="1835" url="https://fr.wikipedia.org/wiki?curid=1835" title="L'Ultime Commandement">
L'Ultime Commandement

L'Ultime Commandement (titre original : "The Last Command") est un roman de science-fiction écrit par Timothy Zahn. Publié aux États-Unis par Bantam Spectra en 1993, il a été traduit en français et publié par les éditions Presses de la Cité en 1994. Ce roman, se déroulant dans l'univers étendu de "Star Wars", est le troisième livre de la trilogie intitulée "La Croisade noire du Jedi fou". Il se déroule neuf ans après la bataille de Yavin.

Luke Skywalker et ses amis doivent trouver l'endroit où sont créés les clones-soldats que le Grand Amiral Thrawn, du côté obscur, utilise pour mettre les systèmes solaires à feu et à sang. Ils sont traqués par le Jedi fou qui cherche à devenir le maître de la galaxie.


Chaque volume a été adapté en comics par Dark Horse aux États-Unis. Dans les pays francophone, les deux premiers volumes sont d'abord parus chez Dark Horse France à raison de trois bandes dessinées (format européen) par volume. Ensuite la licence a été reprise par Delcourt qui a publié l'intégralité de la série en cinq volumes dans sa collection Contrebande (au format comics cette fois). Une édition intégrale est également parue chez Delcourt en 2012 intitulée "Le Cycle de Thrawn".




</doc>
<doc id="1836" url="https://fr.wikipedia.org/wiki?curid=1836" title="La Guerre du Bacta">
La Guerre du Bacta

La Guerre du Bacta (titre original : "The Bacta War") est un roman de science-fiction écrit par Michael A. Stackpole. Publié aux États-Unis par Bantam Spectra en 1997, il a été traduit en français et publié par les éditions Fleuve noir en 1999. Ce roman, se déroulant dans l'univers étendu de "Star Wars", est le quatrième livre de la série "Les X-Wings". Il se déroule sept ans après la bataille de Yavin.

Le super-destroyer Lusenkya a quitté Corusçant, et Ysanne Isard, la dirigeante de ce vaisseau-prison, est désignée à la tête du gouvernement de Typherria, la planète productrice du bacta, ce fluide précieux qui peut soigner presque tous les maux.

La désignation étant tout à fait légale, et de nombreuses autres planètes ayant d'anciens impériaux dans leurs rangs, la Nouvelle République ne veut pas intervenir, surtout quand un terrible virus frappe tous les non-humains, demandant des quantités importantes de bacta pour être soigné, alors qu'Isard distribue ses stocks au compte-goutte, faisant monter les prix.

L'escadron Rogue, Corran Horn et Wedge Antilles à sa tête, ne peut le supporter et démissionne pour combattre Cœur de Glace, se servant pour cela des millions de crédits qui devaient faire tomber Tycho Celchu pour espionnage. Mais les forces rebelles sont lourdement désavantagées, avec un seul escadron de pilotes d'ailes X, sans vaisseau, contre pas moins de 4 destroyers stellaires, dont le Lusenkya lui-même.




</doc>
<doc id="1838" url="https://fr.wikipedia.org/wiki?curid=1838" title="La Bataille des Jedi">
La Bataille des Jedi

La Bataille des Jedi (titre original : "Dark Force Rising") est un roman de science-fiction écrit par Timothy Zahn. Publié aux États-Unis par Bantam Spectra en 1992, il a été traduit en français et publié par les éditions Presses de la Cité en 1993. Ce roman, se déroulant dans l'univers étendu de "Star Wars", est le deuxième livre de la trilogie intitulée "La Croisade noire du Jedi fou". Il se déroule neuf ans après la bataille de Yavin.

Le Grand Amiral Thrawn, qui veut restaurer l'Empire, a perdu la première bataille. Mais il a passé un pacte avec le Jedi fou. Il fait accuser d'espionnage le chef militaire de la République. Et surtout... Dans un repli du continuum, la force sombre attend. Deux cents cuirassés en guerre, qui n'ont jamais servi. Depuis des dizaines d'années, ils sont tapis. Seuls, silencieux, effrayants. Ils vont décider du sort de la guerre.



Chaque volume a été adapté en comics par Dark Horse aux États-Unis. Dans les pays francophones, les deux premiers volumes sont d'abord parus chez Dark Horse France à raison de trois bandes dessinées (format européen) par volume. Ensuite la licence a été reprise par Delcourt qui a publié l'intégralité de la série en cinq volumes dans sa collection Contrebande (au format comics cette fois). Une édition intégrale est également parue chez Delcourt en 2012 intitulée "Le Cycle de Thrawn".




</doc>
<doc id="1839" url="https://fr.wikipedia.org/wiki?curid=1839" title="L'Héritier de l'Empire">
L'Héritier de l'Empire

L'Héritier de l'Empire (titre original : "Heir of the Empire") est un roman de science-fiction écrit par Timothy Zahn. Publié aux États-Unis par Bantam Spectra en 1991, il a été traduit en français et publié par les éditions Presses de la Cité en 1992. Ce roman, se déroulant dans l'univers étendu de "Star Wars", est le premier livre de la trilogie intitulée "La Croisade noire du Jedi fou". Il se déroule neuf ans après la bataille de Yavin.

L'histoire reprend quelques années après la fin de l'épisode VI, "". L'Empire galactique est renversé et devient rébellion avec le Grand Amiral Thrawn à sa tête, le seul non-humain à avoir été élevé au rang de Grand Amiral par l'Empereur, qui ne faisait généralement pas confiance aux autres espèces. La Nouvelle République est fondée avec Mon Mothma à sa tête ainsi que Leia Organa Solo en tant que diplomate. Luke Skywalker va tenter de fonder un nouvel Ordre Jedi.


Chaque volume a été adapté en comics par Dark Horse aux États-Unis. Dans les pays francophones, les deux premiers volumes sont d'abord parus chez Dark Horse France à raison de trois bandes dessinées (format européen) par volume. Ensuite la licence a été reprise par Delcourt qui a publié l'intégralité de la série en cinq volumes dans sa collection Contrebande (au format comics cette fois). Une édition intégrale est également parue chez Delcourt en 2012 intitulée "Le Cycle de Thrawn".




</doc>
<doc id="1842" url="https://fr.wikipedia.org/wiki?curid=1842" title="Liste de diffusion">
Liste de diffusion

Une liste de diffusion ou liste de distribution est une utilisation spécifique du courrier électronique qui permet le publipostage d'informations aux utilisateurs qui y sont inscrits. Celle-ci est gérée par un logiciel adéquat installé sur un serveur. On différencie les listes d'annonces destinées à envoyer des informations aux abonnés sans retour de leur part, des listes de discussions où toute personne inscrite peut envoyer un message ou y répondre.

Le principe est que l'auteur d'un courrier électronique envoie un message à une seule adresse, celle de la liste de diffusion, et que le serveur distribue celui-ci à tous les abonnés. 

Souvent, l'inscription à une liste de diffusion impose un enregistrement et celui-ci limite les consultations des archives par des opérations techniques plus ou moins contraignantes et peut poser des questions liées au respect de la vie privée. C'est par exemple le cas des listes gérées par Yahoo! Groups ou Google Groups.

Difficultés: l'utilisation de ces listes nécessite la mise en place d'identifiants de validation SPF et DKIM, dans les entrées DNS du gestionnaire du nom de domaine utilisé, pour réduire le risque d'être interprété comme SPAM.

Une liste de diffusion est gérée par un ou plusieurs administrateurs qui fixent les règles d'utilisation du service :
Ce fonctionnement est celui de la liste de distribution classique.

Il existe deux types principaux de listes :
L'utilisation du mot diffusion au lieu de distribution, n'est pas généralisé.







</doc>
<doc id="1843" url="https://fr.wikipedia.org/wiki?curid=1843" title="Le Spectre du passé (roman)">
Le Spectre du passé (roman)

Le Spectre du passé (titre original : "Specter of the Past") est un roman de science-fiction écrit par Timothy Zahn. Publié aux États-Unis par Bantam Spectra en 1997, il a été traduit en français et publié par les éditions Presses de la Cité en 1999. L'action se déroule dix-neuf ans après la bataille de Yavin et fait partie de l'univers étendu de "Star Wars". Ce roman possède une suite ayant pour titre "Vision du futur" qui compose avec lui le cycle appelé "La Main de Thrawn".





</doc>
<doc id="1846" url="https://fr.wikipedia.org/wiki?curid=1846" title="Les Trois Lois de la sexualité robotique">
Les Trois Lois de la sexualité robotique

Les Trois Lois de la sexualité robotique est une nouvelle de Roland C. Wagner datant de 1982 et pastichant les trois lois de la robotique d'Isaac Asimov.




</doc>
<doc id="1847" url="https://fr.wikipedia.org/wiki?curid=1847" title="Trois lois de la robotique">
Trois lois de la robotique

Les Trois lois de la robotique, formulées par l'écrivain de science-fiction Isaac Asimov, sont des règles auxquelles tous les robots positroniques qui apparaissent dans ses romans doivent obéir. 

Exposées pour la première fois dans sa nouvelle "Cercle vicieux" ("Runaround", 1942) mais annoncées dans quelques histoires plus anciennes, les lois sont :

Au cours du cycle des livres sur les robots, une loi zéro, qui prendra une importance considérable, sera instituée par deux robots, R. Giskard Reventlov et R. Daneel Olivaw, dans la nouvelle "Les Robots et l'Empire". Cette Loi zéro placera ou tentera de placer la sécurité de l'humanité avant celle d'un individu. Cependant, cette loi n'est pas codée au niveau matériel des cerveaux positroniques, à la différence des trois premières, et elle est une loi de type logiciel, puisque « déduite » par le robot R. Giskard Reventlov. 

D'après l’"Oxford English Dictionary", le premier passage dans la nouvelle d'Asimov nommée "Menteur !" qui mentionne la première loi est la plus ancienne mention enregistrée du mot « robotique ». Asimov n'en était pas conscient initialement ; il a supposé que le mot existait déjà, par analogie avec « mécanique » (comme positronique avec « électronique »), et d'autres termes similaires dénotant des branches de science appliquée.

Les trois lois forment un principe d'organisation et un thème unifiant l'œuvre de fiction d'Asimov, apparaissant dans son "Cycle des robots", et d'autres histoires reliées à celui-ci, comme dans son cycle de "Lucky Starr", fiction scientifiquement orientée pour jeune adulte. D'autres auteurs travaillant dans l'univers fictif d'Asimov les ont adoptées, et des références (souvent parodiques) apparaissent dans une bonne part de la science-fiction, et dans d'autres genres. Asimov considérait que ses lois devaient être universelles pour les robots. Aussi, assistant à la projection de "2001, l'Odyssée de l'espace", il quitta avec bruit la salle lorsque l'ordinateur HAL 9000 viola sa première loi en s'attaquant à des humains.

Dans la science-fiction des années 1930, la majorité des intelligences artificielles suivent le modèle de la créature de Frankenstein, ce qu'Asimov trouve pénible, voire insupportable : « des robots étaient créés et détruisaient leur créateur ; des robots étaient créés et détruisaient leur créateur ; des robots… etc. » Asimov nomme « complexe de Frankenstein » cette tendance à considérer les machines comme des créatures mortellement dangereuses.

Des récits de science-fiction suivent cependant la même inspiration qu'Asimov. En décembre 1938, Lester del Rey publie "Helen O'Loy", l'histoire d'un robot si semblable à une personne qu'elle tombe amoureuse de son créateur et devient sa femme idéale. Le mois suivant, Otto Binder publie une nouvelle nommée "I, Robot", mettant en scène un robot sympathique nommé Adam Link, une créature incomprise motivée par l'amour et l'honneur. C'est la première d'une série de dix histoires ; l'année d'après, "Adam Link's Vengeance" (1940) montre Adam pensant : « Un robot ne doit jamais tuer un être humain selon son propre libre arbitre. »

Le , Asimov assiste à un rassemblement de la Queens Science Fiction Society, où il rencontre Binder, dont Asimov a admiré l'histoire. Trois jours plus tard, Asimov commence à écrire , sa quatorzième histoire. Treize jours après, il propose "Robbie" à John W. Campbell, éditeur d"Astounding Science-Fiction". Campbell la rejette, disant qu'elle était trop ressemblante à "Helen O'Loy" de Del Rey. Frederik Pohl, éditeur du magazine "Astounding Stories", la publie dans son périodique l'année suivante. 

Asimov attribue les lois à John W. Campbell, au cours d'une conversation tenue le . Cependant, Campbell affirme qu'Asimov avait déjà les lois dans son esprit, et qu'elles avaient simplement besoin d'être formulées explicitement. Plusieurs années plus tard, un ami d'Asimov nommé Randall Garrett attribue les lois à une collaboration symbiotique entre les deux hommes, une suggestion qu'Asimov adopte avec enthousiasme. D'après ses écrits autobiographiques, Asimov inclut l'inaction à la première loi à cause d'un poème d'Arthur Hugh Clough nommé "Le Dernier Décalogue", qui contient les vers satiriques : « Tu ne tueras point, mais ne t'acharneras point non plus, légalement, à garder en vie. » 

Même si Asimov colle la création des Lois sur une seule date, leurs interventions dans sa littérature sont présentes sur une certaine période. Il écrit deux histoires de robots sans mention explicite des lois, "Robbie" et "Reason". Il y suppose cependant que les robots ont des garde-fous inhérents à leur nature. "Menteur !", sa troisième nouvelle sur les robots, fait pour la première fois mention de la première loi, mais pas des deux autres. Toutes les trois apparaissent finalement ensemble dans "Runaround". Quand ces histoires et plusieurs autres sont compilées dans l'anthologie "Les Robots", "Reason" et "Robbie" sont mises à jour pour que les Trois lois y apparaissent, bien que le matériel ajouté à "Reason" ne soit pas entièrement cohérent avec les lois telles que décrites ailleurs. En particulier, l'idée d'un robot protégeant les vies humaines sans même croire en l'existence des Êtres Humains est en désaccord avec le raisonnement d'Elijah Baley, décrit ci-après.

Pendant les années 1950, Asimov écrit une série de nouvelles de science-fiction expressément créée pour un public de jeunes adultes. Originellement, son éditeur attend des nouvelles qu'elles puissent être adaptées dans une série télévisée long métrage, quelque chose comme ce qu'avait été "The Lone Ranger" pour la radio. Ayant peur que ses histoires soient adaptées dans le programme « généralement déplorable » qu'il a vu inonder les canaux télévisés, Asimov décide de publier son cycle de "Lucky Starr" sous le pseudonyme Paul French. Quand les plans pour la télévision furent abandonnés, il décide d'abandonner ce nom ; il apporte les lois dans "Lucky Starr et les Lunes de Jupiter", .

Dans sa nouvelle "La Preuve" (""), Asimov expose, par le personnage Susan Calvin, une base morale derrière les lois. Calvin précise qu'il est naturel d'attendre des êtres humains qu'ils se restreignent de blesser d'autres humains (excepté dans des temps d'extrême coercition comme la guerre, ou pour sauver un plus grand nombre d'humains). Cela équivaut à la Première Loi pour un robot. De même, d'après Calvin, la société attend des individus qu'ils obéissent aux instructions des autorités reconnues : docteurs, enseignants, et ainsi de suite, ce qui est équivalent à la Seconde Loi de la robotique. Enfin, les humains sont en général enclins à éviter de se voir blessés eux-mêmes, ce qui est la Troisième Loi pour un robot. L'intrigue de "La Preuve" tourne autour de la question de la définition d'un être humain par rapport à un robot conçu spécialement pour sembler humain ; Calvin pense que si un tel individu obéit aux Lois, il serait un robot, ou .

Un autre personnage demande alors à Calvin si, après tout, les robots sont si différents des êtres humains. Elle répond : .

Dans un essai plus tardif, Asimov précise que les analogies des lois sont implicites dans la création de presque tous les outils :

Les histoires d'Asimov testent ses Lois dans une large variété de circonstances, proposant et rejetant des modifications. Un disciple de la Science-Fiction, James Gunn, écrit : « Les histoires de robots d'Asimov dans leur entier répondent de la meilleure manière à une analyse sur cette base : l'ambiguïté dans les Trois Lois et les manières par lesquelles Asimov a joué vingt et une variations sur le thème » (le nombre est précis depuis 1980). 

Tandis que l'ensemble des Lois procurait de l'inspiration à beaucoup d'histoires, Asimov introduisait de temps en temps des versions modifiées dans son œuvre. Comme l'exemple suivant le démontre, les Lois servent une fonction conceptuelle analogue au test de Turing, replaçant des questions brouillées comme « Qu'est-ce qui est humain ? » par des problèmes permettant une réflexion plus fructueuse.

Dans "Face aux Feux du Soleil", Asimov établit que la première loi était incomplète : il y montre qu'un robot est pleinement capable de blesser un être humain tant qu'il ignore que ses actions entraîneront une telle conséquence. L'exemple suivant est utilisé : un robot met du poison dans un verre de lait, car on lui a dit que le lait sera jeté plus tard ; ensuite, un second robot sert le lait à un homme, ne sachant pas qu'il est empoisonné.

Dans "Le Petit Robot perdu", plusieurs robots NS-2 ou « Nestor » sont créés avec seulement une partie de la Première Loi. On lit :
Cette modification est motivée par une difficulté pratique : des robots et des êtres humains travaillent au milieu de radiations fatales aux robots, mais supportables pour les hommes. Or les robots s'acharnent à aller « secourir » les humains au cœur des radiations, même en l'absence de danger immédiat ; ce faisant, ils nuisent au travail et se détruisent eux-mêmes.

Enlever la clause d'« inaction » de la Première Loi résout le problème, mais crée la possibilité d'un problème encore plus grand : un robot peut commencer une action dangereuse en sachant qu'il peut l'interrompre, puis décider de ne pas aller jusqu'au bout (lâcher une lourde charge sans la rattraper est l'exemple donné dans le texte).

La plupart du temps les défauts des lois sont corrigés par le fait que leur application dépend de la définition d'un humain ou de ce qu'est « faire du mal ». Ainsi, une situation où un robot empêcherait un humain de partir en guerre peut être empêchée si on n'apprend pas à un robot ce qu'est la guerre. Ainsi, la plupart des robots des nouvelles d'Asimov sont si basiques qu'ils ne réagissent selon la Première Loi que si le mal infligé aux êtres humains est vraiment évident et se passe devant leurs yeux. De plus, toujours dans les nouvelles d'Asimov, un roboticien doué sait donner à ses ordres (et donc à la Deuxième Loi) une importance si forte qu'elle atténue légèrement l'influence de la Première Loi, et peut même la dépasser. 

Les Solariens ont créé dans le roman "Les Robots et l'Empire" des robots régis par les Trois Lois de manière normale, mais pour lesquels le sens du mot « humain » est déformé. Selon leur programmation, seules les personnes parlant avec l'accent solarien sont humaines. De cette manière, ces robots n'auront aucun problème à agresser des humains non solariens (et certains sont même programmés spécifiquement pour cela). Au temps où se déroule "Terre et Fondation", les Solariens se sont modifiés génétiquement au point de devenir une espèce distincte de l'humanité, devenant hermaphrodites et capables de contrôler et de transformer diverses formes d'énergie (cinétique, chimique, thermique, lumineuse, électrique) au point d'alimenter en énergie tous les robots et machines de leurs immenses domaines. Les robots de Solaria continuent de respecter les Trois Lois en considérant les Solariens comme seuls humains, plutôt que les hommes normaux du reste de la Galaxie.

Asimov traite le problème des robots humanoïdes à plusieurs reprises. Le roman "Les Robots et l'Empire" et les nouvelles "Evidence" et "L'Incident du tricentenaire" décrivent des robots fabriqués pour tromper les hommes en leur faisant croire que les robots sont humains. D'un autre côté, "L'Homme bicentenaire" et "Pour que tu t'y intéresses" explorent la manière dont les robots peuvent changer leur interprétation des Trois lois à mesure qu'ils deviennent plus sophistiqués. (Gwendoline Butler écrit dans "Un cercueil pour le canari" : « Peut-être sommes-nous des robots. Des robots vivant la dernière Loi de la Robotique… Pour tendre à devenir des humains. »)

"Pour que tu t'y intéresses" ("That Thou art Mindful of Mind"), qui devait être pour Asimov l'« ultime » sonde dans les subtilités des lois, utilise finalement les trois lois pour conjurer le scénario à la « Frankenstein » que les hommes devaient résoudre. La nouvelle prend comme concept le développement grandissant des robots qui imitent les êtres vivants non humains, et sont par conséquent programmés pour agir selon les comportements de simples animaux qui ne nécessitent pas les Trois Lois. La présence d'une large palette de vie robotique qui sert le même but que la vie organique se termine avec deux robots humanoïdes concluant que la vie organique est une condition non nécessaire pour une vraie logique et une définition cohérente par elle-même d'« humanité », que comme ils sont les êtres pensants les plus avancés sur leur planète, ils sont les seuls vrais humains en vie, et que les Trois Lois s'appliquent seulement à eux-mêmes. L'histoire se termine sur une note sinistre : les robots hibernent et attendent le moment où ils vont conquérir la Terre et soumettre les humains biologiques s'y trouvant, une conséquence qu'ils considèrent comme le résultat inévitable des « Trois Lois de L'Humanique ».

Asimov arrive finalement à la conclusion qu'il faut ajouter aux Trois Lois une « Loi Zéro ». Le personnage robotique R. Daneel Olivaw est le premier à donner un nom à cette Loi, dans le roman "Les Robots et l'Empire" ; cependant, Susan Calvin articule ce concept dans la nouvelle "Conflit évitable".

Dans les scènes finales du roman "Les Robots et l'Empire", R. Giskard Reventlov est le premier robot à agir selon la Loi Zéro, cependant cela s'avère destructeur pour son cerveau positronique, car il n'est pas certain que son acte oriente l'humanité vers son bien absolu ou non. Giskard est télépathe, comme le robot Herbie dans la nouvelle "Menteur !", et il en vient à comprendre la Loi Zéro à travers sa compréhension d'un concept plus subtil de la "blessure" que la plupart des robots peuvent saisir. Cependant, à l'inverse de Herbie, Giskard saisit le concept philosophique de la Loi Zéro, ce qui lui permet de blesser des êtres humains si cela peut en quoi que ce soit l'aider dans son service du concept abstrait d'humanité. La Loi Zéro n'est jamais programmée dans le cerveau de Giskard, c'est en fait une règle qu'il tente de rationaliser par pure réflexion métaphysique ; il échoue, et donne ses capacités télépathiques à son successeur, R. Daneel Olivaw. Durant des milliers d'années, Daneel s'adapte lui-même pour être capable d'obéir pleinement à la Loi Zéro. 

Telle qu'il la formule, dans les livres "Terre et Fondation" et "Prélude à Fondation", la Loi Zéro se lit : 

Les Trois Lois sont donc modifiées de cette manière :

Le traducteur français Jacques Brécard incorpora le concept de Loi Zéro dans une des nouvelles d'Asimov avant même qu'Asimov lui-même ne l'explicite. Vers l'apogée des "Cavernes d'acier", Elijah Baley se fait un commentaire amer à lui-même, pensant que la Première Loi interdit à un robot de blesser un être humain, sauf si le robot en question est assez intelligent pour réaliser que ses actions sont faites pour le bien, à long terme, de l'homme (ce qui veut dire ici que dans "Les Cavernes d'acier" les pensées de Baley émergent dans une voie légèrement différente : « Un robot ne doit faire aucun tort à un homme, à moins qu'il trouve un moyen de prouver qu'en fin de compte le tort qu'il aura causé profite à l'humanité en général ! »).

Les conséquences de la Loi Zéro sont considérables : elle donne le droit aux robots de s’attaquer à des hommes si ceux-ci mettent l’humanité en danger. C'est justement le thème principal du film "I, Robot", où l'I.A. VIKI (mémoire centrale de la firme U.S. Robots) arrive à la conclusion logique que la plus grande menace pour l'homme est l'homme lui-même et décide d'enfreindre la Première Loi pour protéger l'humanité. Cependant, dans les romans, même programmés avec la Loi Zéro, les robots ne tueront aucun humain, essayant toujours de trouver le moyen de l'appliquer en intervenant le plus légèrement possible.

Les romans d'Asimov "Les Cavernes d'acier", "Face aux feux du soleil", "Les robots de l'aube" et "Les Robots et l'Empire" montrent que des robots trop basiques et surtout sans Loi Zéro finissent par avoir des effets néfastes sur l'humanité (et plus précisément sur les Spaciens) en empêchant toute forme de souffrance, la poussant ainsi à l'inaction et étouffant toute pensée artistique.

Ainsi, la conclusion du roman "Les Robots et l'Empire" en particulier est que seuls des robots très intelligents pour pouvoir correctement appliquer la Loi Zéro (en l'occurrence R. Daneel Olivaw et R. Giskard Reventlov) seront bénéfiques pour l'humanité, car ils ont connaissance de toutes les subtilités de l'esprit humain et du fait que tout n'y est pas bon ou mauvais. Ces robots peuvent comprendre des notions telles que le sacrifice (dont le but est censé être positif) ou la punition ("idem").

Gaïa, la planète dotée d'une intelligence collective dans les nouvelles de "Fondation", adopte, comme philosophie, une loi similaire à la Première : 

Par trois fois dans sa carrière d'écrivain de fiction, Asimov a fait le portrait de robots qui négligeaient complètement le système de valeur des Trois Lois, à l'inverse des robots Daneel et Giskard, qui avaient pour but de l'augmenter. Le premier cas, une histoire courte nommée "Première Loi", est souvent considéré comme insignifiant, invraisemblable ou même apocryphe. 

D'un autre côté, l'histoire courte nommée "Cal" (intégrée dans "Gold"), racontée à la première personne par un narrateur robotique, met en scène un robot qui néglige les Lois parce qu'il a trouvé quelque chose de bien plus important : il veut être écrivain. Humoristique, en partie autobiographique, et dans un style expérimental inhabituel, "Cal" a été vu comme une des histoires les plus fortes de "Gold". La troisième est une histoire courte dont le titre est "Sally", dans laquelle des voitures équipées de cerveaux positroniques sont apparemment capables de blesser et de tuer des humains, désobéissant à la Première Loi. Cependant, mis à part le concept de cerveau positronique, cette histoire ne se réfère pas aux autres histoires de robots, et ne peut donc pas vraiment être incluse dans la même continuité.

La nouvelle "Le Robot qui rêvait", reprise dans le recueil de même titre, dépeint un robot, LVX-1 ou « Elvex », qui entre dans un état d'inconscience et de rêve, dû à la structure fractale inhabituelle de son cerveau positronique. Dans son rêve, les deux premières Lois sont absentes, et la Troisième Loi dit : « Un robot doit protéger sa propre existence ».

La position d'Asimov sur la profondeur de l'empreinte des Lois évolua dans la chronologie de ses histoires : bien que dans ses premiers écrits elles n'aient été que des garde-fous précautionneusement conçus, dans les histoires plus récentes Asimov indiqua qu'elles étaient une partie inaliénable de la fondation mathématique soutenant le cerveau positronique : sans la théorie de base des Trois Lois, les scientifiques fictifs de l'univers d'Asimov seraient incapables de créer une unité cervicale viable. Dans "Le Petit Robot perdu", Susan Calvin considère que modifier les Lois est une idée terrible, mais faisable, tandis que, des siècles plus tard, le Gerrigel, dans "Les Cavernes d'acier", croit que c'est impossible.

Le Gerrigel utilise le mot « Asenion » pour décrire des robots programmés avec les Trois Lois. Dans les histoires d'Asimov, les robots étant des robots Asenion sont incapables de violer consciemment les Trois Lois, mais en principe un robot dans la science-fiction ou dans le monde réel pourrait être un non-Asenion. (. Asimov utilisa cette variation obscure de son nom pour s'insérer lui-même dans "Les Cavernes d'acier", d'une manière semblable à celle de Vladimir Nabokov apparaissant dans "Lolita" déguisé par l'anagramme « Vivian Darkbloom ».)

Comme les personnages dans les histoires le font souvent remarquer, les Lois telles qu'elles existent dans l'esprit d'un robot ne correspondent pas à la version écrite, verbale, habituellement citée par les humains, mais sont des concepts mathématiques abstraits sur lesquels l'entière conscience développante d'un robot est basée. Ainsi, les Lois sont comparables aux instincts basiques de l'homme sur la famille ou l'accouplement, et sont conséquemment plus proches de former la base d'une conscience propre d'un robot — un sens dont le but est basé sur le service à l'humanité, l'obéissance aux ordres des hommes et l'existence continue dans — plus que des limitations arbitraires de l'entourage d'un esprit indépendant sans cela. Ce concept est largement brouillé et peu clair dans les histoires les plus anciennes qui décrivent des robots très rudimentaires qui sont seulement programmés pour accomplir des tâches physiques sommaires, avec les Lois implantées comme des protections, mais dans l'ère des "Cavernes d'acier", où l'on voit des robots avec une intelligence de niveau humain, les Trois Lois sont devenues une vue éthique du monde sous-tendant les actions de tous les robots.

Les mouvances technophile et transhumaniste voient parfois les trois lois comme un idéal à venir. Des avancées significatives en matière d'intelligence artificielle seraient nécessaires pour que les robots obtiennent une intelligence. Toutefois, la complexité des robots a augmenté et l'avancée de la technologie suit une courbe exponentielle selon la singularité technologique. L'humanité a donc intérêt à élaborer des directives et des garanties pour leur fonctionnement.

Roger Clarke a écrit des analyses sur les complications dans l'application de ces lois, dans l'hypothèse que les systèmes soient un jour capables de les employer. Il a fait valoir que les lois de la robotique d'Asimov ont été un procédé littéraire très réussi. Peut-être ironiquement, ou peut-être parce que c'était approprié dans le domaine artistique, la somme des récits d'Asimov réfutent eux-mêmes l'affirmation avec laquelle il commence ces récits, c'est pourquoi Clarke conclut : .

En 2004, l'Institut Singularity a lancé une campagne Internet appelée 3 Lois dangereuses : "3 Laws Unsafe" (les 3 lois d'Asimov) pour sensibiliser aux questions de la problématique de l'intelligence artificielle et l'insuffisance des lois d'Asimov en particulier.

L'auteur de science-fiction Robert Sawyer déclare quant à lui : 
Il convient de noter que l'Essai de Sawyer néglige les questions de dommage involontaire ou inconscient, sujet traité dans des livres comme "Face aux feux du soleil".

À noter que les trois lois ne sont pas applicables pour les robots militaires qui peuvent avoir pour objectif de tuer des personnes humaines. Dans un tel cas, il est proposé que les robots puissent prendre des décisions éthiques.




</doc>
<doc id="1848" url="https://fr.wikipedia.org/wiki?curid=1848" title="Littérature française">
Littérature française

La littérature française comprend l'ensemble des œuvres écrites par des auteurs de nationalité française ou de langue française, elle peut également se référer à la littérature écrite par des citoyens français qui écrivent dans des langues de France telles que le basque, le breton, etc. 

La littérature écrite en langue française par les personnes d'autres pays tels que la Belgique, la Suisse, le Canada, le Sénégal, l'Algérie, le Maroc, etc. se réfère à la littérature francophone.

Son histoire commence en ancien français au Moyen Âge et se perpétue aujourd'hui.


Un des "Serments de Strasbourg" (842) est le premier texte complet connu rédigé en roman, l' « ancêtre » du français. Le premier texte conservé dans cette langue que l'on considère aujourd'hui comme « littéraire » est le "Séquence ou Cantilène de sainte Eulalie", probablement écrite entre 881 et 882 ; c'est une simple adaptation en 29 vers d'un poème latin à vocation religieuse et pédagogique.

Les premiers grands textes de la littérature française datent eux du milieu du Moyen Âge (), époque de développement de l'agriculture et d'expansion démographique après des périodes d'invasions, d'anarchie et d'épidémies.

Les chansons de geste sont de longs poèmes comportant des milliers de vers qui sont destinées à être chantées en public, geste signifiant ici exploits guerriers. Elles relatent, sous une forme épique mêlant légendes et faits historiques, des exploits guerriers passés, et mettent en valeur l'idéal chevaleresque. La plus ancienne et la plus connue est la "Chanson de Roland" qui a été écrite au ; elle raconte, en les idéalisant, les exploits de l'armée de Charlemagne.

La littérature courtoise, apparue au , a pour thème principal le culte de l'amour unique, parfait et souvent malheureux. Elle trouve son origine dans l'Antiquité, intègre des influences orientales dues au retour des Croisés, et s'inspire de légendes celtiques. Ainsi, la légende de Tristan et Iseult raconte l'histoire d'un amour absolu et impossible qui se termine par la mort tragique des amants ; ces poèmes étaient chantés à la cour des princes par les trouvères et les troubadours. Chrétien de Troyes (1135 ?–1190 ?) est sans doute le premier romancier de la littérature française ; ses romans comme "Yvain ou le Chevalier au lion", "Lancelot ou le Chevalier de la charrette" et "Perceval ou le Conte du Graal" sont typiques de ce genre littéraire. Le long poème "Le Roman de la Rose", "best-seller" datant du début du est l'un des derniers écrits portant sur le thème de l'amour courtois, et cela seulement dans son court début écrit par Guillaume de Lorris. Le reste du poème, continué par Jean de Meung contient au contraire des passages (dont celui de "La vieille") d'une étonnante misogynie, mêlée par ailleurs à des arguments articulés de "critique sociale".

Vers la même époque, le "Roman de Renart" est un ensemble de poèmes qui relatent les aventures d'animaux doués de raison. Le renard, l'ours, le loup, le coq, le chat, etc. ont chacun un trait de caractère humain : malhonnête, naïf, rusé… Les auteurs anonymes raillent dans ces poèmes les valeurs féodales et la morale courtoise.

Le poète parisien du Rutebeuf se fait gravement l'écho de la faiblesse humaine, de l'incertitude et de la pauvreté à l'opposé des valeurs courtoises.

Les premières chroniques historiques écrites en français sont des récits des croisades datant du . Certains de ces récits, comme ceux de Joinville retraçant la vie de saint Louis, ont aussi un but moral et idéalisent quelque peu les faits relatés. Ensuite la guerre de Cent Ans (1337–1453) est racontée par Jean Froissart (1337–1410 ?) dans deux livres appelés "Chroniques". Eustache Deschamps, le poète, témoigne de la société et des mentalités pendant la guerre de Cent Ans.

Après la guerre de Cent Ans, le poète François Villon (1431–1463 ?) traduit le trouble et la violence de cette époque. Orphelin d'origine noble et bon étudiant, il est ensuite condamné pour vol et meurtre. Son œuvre à la fois savante et populaire exprime une révolte contre les injustices de son temps.

Le théâtre religieux se développe tout au long du Moyen Âge, il met en scène les Mystères, c'est-à-dire les fêtes religieuses comme Noël, Pâques et l'Ascension ; au contraire des genres littéraires précédents plutôt aristocratiques, il s'adresse au plus grand nombre. À côté de ce théâtre religieux, un théâtre comique appelé farce apparaît au où il est durement combattu par les autorités religieuses.

Les principes de l'humanisme vont marquer profondément la littérature : retour aux textes anciens (grecs, latins et hébreux), désir de connaissance, épicurisme indiscutable, renouvellement des formes et des thèmes en se distinguant de la littérature médiévale.

La poésie compte comme auteur important Clément Marot, Jean de Sponde, Agrippa d'Aubigné, et les poètes de la Pléiade parmi lesquels figurent Ronsard et Du Bellay.

Les romans les plus marquants sont ceux de Rabelais et de Marguerite de Navarre.

Les "Essais" de Montaigne sont un important ouvrage situé entre la philosophie et l'autobiographie. Les "Essais" sont d'ailleurs une des premières autobiographies françaises et ouvrent ainsi la porte à Rousseau et tant d'autres. Le projet même des "Essais", à savoir se découvrir, mais aussi découvrir l'Homme, peut être rapproché de celui des "Confessions" de Jean-Jacques Rousseau qui cherche à peindre l'homme, avec ses qualités, mais aussi ses défauts.

Dès le début du , Honoré d’Urfé connaît un grand succès avec son roman précieux L'Astrée, roman d'aventures en partie autobiographique paru entre 1607 et 1633. C'est l'un des plus considérables succès du siècle, qui n'aura pas de postérité véritable dans le genre du roman pastoral, mais une influence considérable sur le roman, le théâtre (Molière), l'opéra et les mentalités.

Le compte deux grands courants littéraires tout à la fois concurrents mais aussi complémentaires : le classicisme et la littérature baroque. Concurrents car le classicisme en littérature s'imposera face au baroque mais aussi complémentaires car certains auteurs ont été influencés par les deux courants à la fois (comme Pierre Corneille). Mais dès la fin du siècle se dessine en littérature un courant de pensée qui annonce déjà les Lumières (avec La Bruyère par exemple).

Les grands noms de la littérature de cette époque sont : Corneille, Jean Racine, Molière, Pascal, La Rochefoucauld, La Fontaine, Nicolas Boileau, La Bruyère, Mme de La Fayette, Madame de Sévigné, Le Cardinal de Retz.

Le est appelé . Par cette métaphore le siècle cherche à consacrer, à travers l'esprit de la Renaissance et le cartésianisme du siècle précédent, le triomphe de la Raison sur les Ténèbres (l'obscurantisme et les préjugés). Les Lumières sont un phénomène européen, mais les philosophes français cristallisent le mieux les idées du siècle et donnent du relief à des nouvelles valeurs qui, au-delà de la Révolution française, marqueront durablement l'Europe et le monde. Les principaux philosophes francophones des Lumières sont Voltaire, Jean-Jacques Rousseau, Denis Diderot et Montesquieu.

Si le est important par le nombre de chefs-d’œuvre que la littérature française a engendrés, cette ère, remarquable dans l'histoire de la littérature française, reste difficile à appréhender ; et ce, en dépit de son caractère relativement récent. Pour de nombreux historiens de la littérature, le littéraire français demeure celui du romantisme, d'abord avec Chateaubriand, puis avec Victor Hugo, du réalisme avec Stendhal, Honoré de Balzac, Gustave Flaubert et du naturalisme avec Émile Zola.

Le romantisme et son foisonnement peuvent trouver partiellement leur cause dans certains points de vue. Certains mettent l’accent sur l’élan de liberté qu’a suscité la Révolution française, élan de liberté suivi d’un désordre, d’une confusion entraînée par l’instabilité, l’incertitude politique qui émane de la première moitié du siècle. Dans cette optique, on voit l’écrivain avec ses idéaux, manifestant son opposition à l’ordre politique et social. Pour d’autres, la place de la Révolution française et des troubles politiques qui s’ensuivront n’explique pas ou pas entièrement l’efflorescence du romantisme français, prenant pour preuve la naissance antérieure des romantismes anglais et allemand dans des pays qui ne furent pas secoués par la moindre révolution. Ils insistent plutôt sur l’influence qu’ont exercé l’étude et la lecture des littératures anglaise et allemande par les hommes de lettres français.

Le réalisme est une étiquette plus vague, accolée postérieurement aux écrivains à partir des définitions de Champfleury, Stendhal et Balzac se situant entre le romantisme et le réalisme. Gustave Lanson dont "l'Histoire de la littérature française" (1894) a longtemps fait autorité, a consacré de très importantes pages à Balzac où il définit la part de réalisme de son œuvre et la part de romantisme « Ainsi, par ses impuissances et par sa puissance, Balzac opérait dans le roman la séparation du romantisme et du réalisme. Il reste cependant dans son œuvre quelque chose d'énorme, une surabondance et une outrance qui en trahissent l'origine romantique. »

Le naturalisme, en revanche, procède d'une véritable démarche qu'Émile Zola a longuement explicitée.

La littérature française du a été profondément marquée par les crises historiques, politiques, morales et artistiques. Le courant littéraire qui a caractérisé ce siècle est le surréalisme, qui est surtout un renouveau de la poésie (André Breton, Robert Desnos, Paul Éluard...), mais aussi l'existentialisme (Gabriel Marcel, Jean-Paul Sartre), qui représente également une nouvelle philosophie ("L'existentialisme est un humanisme" de Jean-Paul Sartre). La source première chez les artistes de ce siècle est en rapport avec les conflits politiques de l'époque. La guerre est ainsi présente aussi bien dans la poésie que dans les romans.

Pour ce siècle, Marcel Proust apparaît comme le dernier grand auteur français. La seule comparaison est à chercher du côté de Louis-Ferdinand Céline, dans le rôle non négligeable qu'il a joué dans la remise en cause d'une narration trop policée et loin de la vie. Par une approche syntaxique au plus proche de la réalité de la rue, la création d'une novlangue mêlée à un argot fantaisiste, il s'est également illustré comme l'un des plus grands écrivains français de ce siècle et a marqué nombre d'écrivains, du père de San-Antonio en passant par les écrivains anglo-saxons (Burroughs, Miller, etc.).

En France, le Nouveau Roman, théorisé par Alain Robbe-Grillet dans "Pour un nouveau roman", ne concerne initialement que peu d'écrivains mais a inspiré ensuite toute une génération d'écrivains regroupés aujourd'hui autour des Éditions de Minuit, dont Jean Echenoz, Jean-Philippe Toussaint, Tanguy Viel, Christian Oster, Laurent Mauvignier ou Christian Gailly. Après cela, plus aucun mouvement au sens strict ne réussit à émerger. L'Oulipo, Ouvroir de littérature potentielle, auquel ont appartenu Queneau ou Perec (et aujourd'hui des auteurs comme Roubaud, Fournel, Jouet et Le Tellier) ne se conçoit en effet pas comme un mouvement, mais comme un groupe de travail. Il en va de même pour la Nouvelle fiction regroupant des romanciers tels que Hubert Haddad, Frédérick Tristan ou Georges-Olivier Châteaureynaud.

Aujourd'hui on a cru pouvoir rapprocher un certain nombre d'écrivains autour de la notion d'autofiction créée par Serge Doubrovsky. Pour autant, il est parfois difficile de rassembler sous une même étiquette une palette d'écrivains aux sensibilités, aux démarches artistiques et aux univers parfois antagonistes. Cette définition est aussi un argument mis en avant par les détracteurs d'une littérature trop nombriliste, germanopratine et qui, d'un point de vue strictement commercial, semble trouver peu d'échos à l'étranger.

Dans la continuité des romans de terroir du , dont l'une des représentante fut George Sand, la littérature de terroir français a continué de s'illustrer avec des auteurs comme Pierre-Jakez Hélias ("Le Cheval d'orgueil") et Henri Vincenot dans la seconde moitié du . Ce genre continue de se développer avec des auteurs comme Jean Anglade ou Jean-Paul Malaval.

Les mouvements littéraires les plus importants ont été :

La littérature française, en ce début , revient a des formes plus traditionnelles, contrastant avec le foisonnement et l'innovation formelles du . Même s'il est aujourd'hui mal-aisé de dégager ce que l'on pourrait appeler des courants littéraires, nous pouvons tout de même repérer des tendances. Nous ne prétendons pas à l'exhaustivité.

L'autofiction : Serge Doubrovsky, créateur de ce néologisme, considère Colette comme la pionnière de cette autofiction qui a connu, en ce début de siècle, un succès public et critique certain. Mais cette notion d'autofiction est loin d'être homogène "car la nature exacte de la synthèse [de l'autobiographie et de la fiction] est sujette à interprétation" pourtant il nous est possible de distinguer quelques types :
Le réalisme magique se distingue en ce début de , porté par des auteurs tels que Marie N'Diaye, Véronique Ovaldé ou Sylvie Germain.

La notion de minimalisme ou de roman ludique : "Au cours des années 1980, le terme de minimalisme est apparu puis s’est rapidement répandu pour désigner des auteurs ayant en partage un héritage (le Nouveau Roman)." Toute une génération d'écrivains regroupés aujourd'hui autour des Éditions de Minuit dont Jean Echenoz, Jean-Philippe Toussaint, Laurent Mauvignier ou Eric Chevillard. Même si l'idée de minimalisme fait encore débat, la notion de minimalisme se caractérise, selon Marc Dambre, par un "jeu citationnel", un "réenchantement sans illusion du monde", la "recherche d’un nouvel ordre narratif", la présence accrue du ludique, une mise à distance de l’incongru et une manière de prendre le mot au mot. Des caractéristiques, communes, qui ont poussé Olivier Bessard-Banquy à proposer la dénomination de Roman ludique pour regrouper ces auteurs, l'occasion de séparer minimalisme et littérature ludique : "Le goût du jeu est en effet chez eux bien plus marqué que la tentation du peu. […] C’est pourquoi l’étiquette du minimalisme — si tant est qu’elle ait un sens — doit être réservée à des ouvrages de peu, revendiquant à l’évidence une indigence absolue"

L'hypothèse du minimalisme positif : notion créée par Rémi Bertrand, dans son essai "Philippe Delerm et le minimalisme positif", désigne une « littérature articulée sur le bonheur au quotidien ». Une vision de l’écriture et de la vie apparaît, dès lors, de façon cohérente. Il s’agit de « préciser les conditions de possibilité d’une écriture du quotidien », de débarrasser « le quotidien et le bonheur des oripeaux de l’espérance » tout en fondant spontanément une éthique holistique du banal ; et ce, dans « une forme brève ». Sous cette bannière du minimalisme positif, Bertrand rassemble plusieurs auteurs : Philippe Delerm, Bobin, Jean-Pierre Ostende,Pierre Michon, Visage, tous « chantres des plaisirs simples ». Il faut tout de même noter que cette notion de minimalisme positif est contestée par une partie de la profession notamment Pierre Jourde : « réunir les auteurs du même type que Delerm en une sorte d’école, bref ériger cela en phénomène littéraire revient à encourager le développement actuel de la littérature de confort. »

La littérature écrite en langue française se retrouve dans de nombreux pays sur plusieurs continents.

Le prix Nobel de littérature a été désigné pour la première fois en 1901.




</doc>
<doc id="1849" url="https://fr.wikipedia.org/wiki?curid=1849" title="Littérature russe">
Littérature russe

La littérature en langue russe proprement dite naît au cours du , tout d'abord avec la poésie et le théâtre, mais très tôt naît une très riche tradition romanesque.

De grands auteurs russes apparaissent au d'abord avec le romantisme, au début du siècle qui voit l'éclosion d'une génération talentueuse avec surtout Alexandre Pouchkine, Mikhaïl Lermontov. La suite du « siècle d'or de la littérature russe » produit de grands romanciers comme Nicolas Gogol, Fiodor Dostoïevski, Ivan Tourgueniev, Léon Tolstoï ; la fin du siècle est marquée par la figure du dramaturge Anton Tchekhov.

Au tournant du , un nouvel élan littéraire est porté par la poésie symboliste puis futuriste, associé à une intense activité théorique mais il se heurte vite à la persécution soviétique. Le siècle est cependant riche de poètes comme Sergueï Essénine et Vladimir Maïakovski et de romanciers comme Maxime Gorki, Boris Pasternak, Mikhaïl Cholokhov ou Mikhaïl Boulgakov. La répression stalinienne frappe particulièrement de nombreux écrivains, comme Vassili Grossman, Varlam Chalamov ou Alexandre Soljenitsyne, qui dénoncent le système totalitaire soviétique.

Depuis la chute de l'Empire soviétique et la disparition du régime communiste, une nouvelle littérature russe naît progressivement dans les années 1990.

Immense différence avec l’Occident, il n’existe en Russie aucun document écrit avant le : le Codex de Novgorod semble le plus ancien document littéraire. Par ailleurs, le pays ne connaît pas de chevalerie en raison du joug tatar. La Russie ne connaît aucun texte non religieux avant le hormis des récits populaires, et pas d’université avant celle de Mikhaïl Lomonossov, créée au .

Après le schisme de 1054, l'opposition théologique et idéologique à l’Occident se traduit par le rejet de l’influence de Rome puis de l’Allemagne. La Russie se réfère plus volontiers à Byzance, dont Moscou récupère l'héritage après la chute de Constantinople prise par les Turcs en 1453. Moscou se voit comme la « troisième Rome » et reprend l’aigle bicéphale comme symbole.

Selon Stépan Chévyriov (1806-1864), si le mot actuel "Literatura" ("Литература") désigne la « littérature », il s'agit d'un emprunt du . "Slovesnost" ("Словесность") était l’ancien mot, qui signifie « art du mot ». Un art écrit, et surtout, un art oral. Ainsi, la « Литература » caractérise ce qui relève de l’écrit et « Словесность » ce qui relève du mot.

La littérature vieux-russe se constitue de rares ouvrages écrits en vieux-russe (à ne pas confondre avec le slavon d'Église) comme l'anonyme "Dit de la campagne d'Igor" (Слово о Полку Игореве). Les bylines, épopées orales, ont mélangé les traditions païennes et chrétiennes, dans lesquelles l'influence de la littérature byzantine se fait sentir. La littérature médiévale russe est écrite en slavon avec une très forte thématique religieuse. Le premier ouvrage en russe courant, l'autobiographie de l'archevêque Avvakoum, ne voit le jour que vers le milieu du .

Après un long joug mongol, le territoire russe est unifié autour de la Moscovie sous le règne d'Ivan le Terrible (1530-1584), premier « tsar de toutes les Russies ». À sa mort, il n’y a pas de successeur légitime. Le pouvoir échoit finalement à Boris Godounov. Son court règne ouvre le Temps des troubles (смутное время), pendant lesquels des boyards se succèdent au Kremlin. Le désordre politique s'accompagne d’une famine et d’une crise économique sans précédent, mais du point de vue culturel, ce temps chaotique est riche. Sous l'impulsion de la République des Deux Nations (Pologne et Lituanie), la Russie s'ouvre au monde extérieur.

L'incertitude s'achève en 1615, après l'élection d'un tsar en 1613 : Michel III Romanov, premier représentant de la longue dynastie Romanov. À la fin du , son fils Alexis, Алексей Михайлович « le très paisible », lui succède. Son règne est marqué par de nombreuses réformes et l'apparition du "Raskol". Sa deuxième femme, Natalia Narychkina (mère de Pierre le Grand) s’intéresse beaucoup à ce qui se passe en Europe et exerce une grande influence sur son mari. Elle introduit en particulier le théâtre occidental et met en place une troupe permanente.

Avec le règne de Pierre le Grand (1682-1725), la culture russe se sécularise et arrive progressivement à une littérature, une peinture et une musique russes au début du . Amorcé à la fin du , le changement prend corps au début , avec la création de Saint-Pétersbourg en 1703.

Dans un pays en grande partie analphabète, Pierre le Grand fonde le premier journal russe (gratuit), « Ведомости » ("Les Nouvelles"). Une réforme de l’alphabet simplifie les caractères cyrilliques, en s'inspirant de l'alphabet latin. En outre, de nombreuses écoles et institutions sont créées : l’Académie de Marine, l’École du Génie, l’École de Médecine de Moscou, l’Académie des Sciences, ainsi que le premier musée de Russie : la "Kunstkamera", située à côté du palais d'Hiver.

Cependant, Pierre le Grand ne nourrit pas un intérêt profond pour la littérature et l’art. C’est avant tout quelqu’un de pratique, comme l’attestent ses réformes sur les académies ou l’administration en général. De même, les premiers livres imprimés en caractères cybiles sont des guides pratiques concernant l’art militaire, ou encore un manuel de correspondance.

Sous Pierre le Grand, les chansons d’amour sont tolérées, ce qui est un changement essentiel. En effet, l’amour devient « autorisé », victime autrefois des désignations les plus dures. Ses chansons reprennent la tradition orale avec son système stylistique, ses images, et la poétique nouvelle du lyrisme européen.

On autorise également la littérature narrative : apparition de récits d’aventure, qui ne sont autres que des imitations de romans de chevalerie. Les « Повести » sont le plus souvent des adaptations au contexte russe de récits étrangers. On retrouve même des caractéristiques du conte populaire.
Le héros traditionnel de ces ouvrages est généralement un noble russe entreprenant, courageux, audacieux, et attiré par l’occident: idéal de l’homme nouveau, pour l’élite. Un exemple en est l’"Histoire du marin russe Vassili Kariotsky et de la belle reine Iraklia de la terre Florentine". L’aventure se passe à l’étranger, dans le ton du merveilleux (il devient roi de la terre florentine et épouse la reine). Le choix d'un marin comme héros dénote une certaine modernité (la flotte russe existe depuis très peu).

Cependant, tout le est marqué par l’hégémonie de la poésie.
Des auteurs comme Antioch Kantemir, Vasily Trediakovski, et Mikhaïl Lomonossov au début du forment la première vague littéraire russe. En poésie Gavril Derjavine, en prose Nikolaï Karamzine et Alexandre Radichtchev, au théâtre Alexandre Soumarokov et Denis Fonvizine défrichent des genres littéraires pour l'instant inexistants.

Le est une période déterminante marquée par une grande querelle concernant l'utilisation du vieux russe (ou slavon) ou du russe populaire comme langue littéraire. En 1743, Mikhaïl Lomonossov, futur fondateur de la première université à Moscou, écrit un traité de rhétorique en posant le dilemme du slavon. En 1745, le poète Vasily Trediakovski envisage la création d’une langue littéraire, mélange de la langue populaire et du slavon.

C'est finalement cette solution qu'adopte Lomonossov en 1755 avec la première "Grammaire russe". Il s’agit là de la première normalisation de la langue. La langue russe écrite emprunte par ailleurs à l'étranger de nombreux termes techniques. Le vocabulaire de la marine est ainsi emprunté au néerlandais, le vocabulaire militaire à l'allemand, et les termes conceptuels proviennent souvent du français.

On traduit par ailleurs de plus en plus de romans occidentaux qui suscitent réflexion, comme les romans de l’abbé Prévost (à l’origine du premier débat littéraire en Russie), Madame de Scudéry, Scarron ou Lesage.

Le débat prend une dimension importante. Différentes sensibilités s'affirment :

Les conservateurs : ils emportent dans un premier temps le débat. Le poète Alexandre Soumarokov ("Cyмapоков") explique que « lire des romans est une inutile et regrettable perte de temps ». Mikhaïl Kheraskov ("Xepaсков") ajoute qu'« on ne tire pas profit de la lecture des romans ».

Les opposants : Porochine (traducteur de l’abbé Prévost entre autres) argumente que les romans à l’européenne jouent un rôle social. Le succès grandissant des traductions entraîne l'apparition de romans russes qui auront beaucoup de succès.

Fédor Emine (1735 ?-1770) est le premier romancier en langue russe, bien que d'origine étrangère. Il combine les modèles d’intrigues les plus répandus en les russifiant, et use d'un style médiocre. Mais ses romans comblent l'attente du public russe naissant et rencontrent un réel succès. Dans ses romans, dont le plus connu est « "La fortune inconstante" » (1763), on trouve une sorte de mélange fantastique/réel, des amours difficiles, les poncifs du roman d’aventure, mais aussi des tableaux réalistes des mœurs de l’époques. Le fait que ces romans aient été écrits directement en russe explique pour une part leur succès. L’auteur affirme également que certaines aventures lui sont arrivées personnellement, rendant le lecteur enthousiaste.

Mais l’exemple de l'écrivain Mikhaïl Tchoulkov (1743 ?-1793) reflète bien un certain paradoxe : plutôt conservateur, il considère l’écriture de roman comme une activité insignifiante. Il est cependant intéressé par le fait que la langue d’Emine soit familière. Et il écrira lui-même dans le parler contemporain de Moscou. Ce fait révèle le besoin d'élaborer une langue écrite adaptée à la réalité de la Russie contemporaine.

Le romantisme, au début du voit l'éclosion d'une génération talentueuse avec Vassili Joukovski mais surtout Alexandre Pouchkine, Mikhaïl Lermontov et Fiodor Tiouttchev.
Ce siècle sera le siècle d'or de la littérature russe et plus particulièrement du roman avec Fiodor Dostoïevski, Nicolas Gogol, Ivan Gontcharov, Nicolaï Leskov, Mikhaïl Saltykov-Chtchédrine, Léon Tolstoï, Ivan Tourgueniev...

La littérature russe est beaucoup influencée dès cette époque par la littérature occidentale, comme le démontre Stefan Zweig dans son essai : "Trois maîtres", Dostoïevski, Balzac, Dickens" ainsi que Michel Cadot dans "La Russie entre Orient et Occident".

Parallèlement, les autres domaines littéraires se développent aussi avec le fabuliste Ivan Krylov, les poètes Evguéni Baratynski, Konstantin Batyouchkov, Alexandre Nekrassov, Alexis Tolstoï, Fiodor Tiouttchev et Afanassi Fet, le collectif satirique Kosma Prioutkov. Anton Tchekhov développe à la fois une œuvre théâtrale essentielle, mais aussi tout un registre d'histoires très courtes qui en fait un des auteurs russophone les plus marquants.


Le début du voit une vive activité dans le champ poétique avec l'éclosion de beaucoup de tendances telles le symbolisme puis l'acméisme et le futurisme russe. De nombreux poètes participent à ce nouvel âge d'or : Anna Akhmatova, Innocent Annenski, Andreï Biély, Alexandre Blok, Valéry Brioussov, Marina Tsvetaïeva, Sergueï Essénine, Nikolaï Goumilev, Daniil Harms, Vélimir Khlebnikov, Ossip Mandelstam, Vladimir Maïakovski, Boris Pasternak, Fiodor Sologoub ou Maximilien Volochine.

C'est également une période d'intense activité critique et théorique, avec le développement du formalisme russe.

Après la révolution d'Octobre, de nombreux écrivains russes s'exilent, notamment à Berlin, puis à Paris, où de nombreuses revues littéraires en russe sont éditées ("La Pensée russe"). En 1921, Nikolaï Goumilev, mari d'Akhmatova, est exécuté pour activités pro-impériales.

Mais avec le démarrage de la NEP, une relative liberté est accordée aux écrivains, et certains exilés choisissent de revenir en Russie (Victor Chklovski, Andreï Biély, et plus tard, Maxime Gorki).

La vie littéraire reprend tant bien que mal, malgré les tracasseries du pouvoir et la précarité de l'économie. Des groupes tels que les Frères de Saint-Sérapion ou le mouvement de l'Oberiou essaient de renouveler l'esthétique du roman ou de la poésie. Une certaine critique de la société trouve même droit de cité, comme dans les romans satiriques de Ilf et Petrov ou "L’Envie" de Iouri Olecha (1927). Mikhaïl Cholokhov publie "Le Don paisible" qui lui vaudra le prix Nobel de littérature en 1965.

L'arrivée au pouvoir suprême de Joseph Staline en 1930 marque la fin de la relative liberté accordée aux écrivains russes par le pouvoir bolchévique. Une esthétique officielle se met en place : le réalisme socialiste. Cette doctrine littéraire est simple, il s'agit d'utiliser le talent des écrivains pour vanter les mérites et les réussites du régime ainsi que pour expliquer la propagande officielle. Le régime s'occupe d'organiser la vie littéraire et l'orientation des thèmes via l'Union des écrivains qui relève directement du commissaire politique Andreï Jdanov. Toutefois la "Literatournaïa gazeta" (Gazette littéraire) garde une relative indépendance d'esprit.

Très rapidement, les écrivains réfractaires seront contraints à l'exil, à la prison, au camp de travail. Les poètes futuristes Vladimir Maïakovski et Marina Tsvetaïeva choisiront le suicide. Cette répression, associée à des conditions matérielles très dures dues à la Seconde Guerre mondiale conduiront à la disparition de la quasi-totalité du milieu littéraire russe. Dans le même temps, l'école de critique et de théorie littéraire russe est mise au pas. Roman Jakobson s'installe aux États-Unis, Victor Chklovski et Mikhaïl Bakhtine sont réduits au silence. Certains auteurs, pour contourner la censure, s'abritent derrière le genre du conte pour enfants (Daniil Harms) ou de la bibliographie historique (Iouri Tynianov). Mais la plupart des auteurs (Mikhaïl Boulgakov, Boris Pasternak, Andreï Platonov, Ossip Mandelstam, Iouri Olecha, Isaac Babel ou Vassili Grossman) continuent leur travail littéraire de manière parfois clandestine, en espérant être publiés de manière posthume ou à travers le régime des samizdat (publications artisanales clandestines).

Les auteurs en exil comme le prix Nobel Ivan Bounine, Alexandre Kouprine, Boris Zaïtsev, Ivan Chmeliov réussissent à vivre de leur travail, gardent leur liberté créatrice mais ne peuvent atteindre leur public que par samizdat.

Dans l'URSS d'après Staline, le socialisme réel reste le seul style littéraire autorisé mais les auteurs publiant sous samizdat ont plus de libertés. Surtout les auteurs peuvent vivre de leur travail et craignent moins la répression et l'internement. Les premiers récits concernant le goulag commencent à circuler en samizdat, tels ceux du prix Nobel Alexandre Soljenitsyne ou de Varlam Chalamov. Vénédict Erofeiev continue son travail de publication par samizdat.

Dans la période de déclin de l'union soviétique, les Russes émigrés reçoivent en Occident une reconnaissance assez forte tels le prix Nobel Joseph Brodsky ou le nouvelliste Sergueï Dovlatov. Leur œuvre n'est alors connue en URSS que par samizdat.

Il faudra attendre la politique de perestroïka entamée à la fin du des années 1980 pour que des écrivains dissidents soient officiellement publiés.

L'écrivain Vladimir Nabokov représente un cas à part : Russe de Saint-Pétersbourg, il devient américain par la suite. Il commence sa carrière en exil à Berlin et écrit en russe, puis en anglais.


À la fin du , la littérature russe doit passer une phase délicate : celle de la renaissance, par-delà le sel semé par des décennies de socialisme soviétique. Les besoins de cette période sont de deux types : former et découvrir de nouveaux talents et créer une économie de l'édition en Russie. Les maisons d'édition trouvent de l'argent pour se développer en vendant des romans de piètre qualité littéraire. Peu d'écrivains, comme Viktor Pelevine (1962-) ou Vladimir Sorokine sortent du lot. Les maisons d'édition publient peu des œuvres étouffées sous la période communiste ou connues par samizdat.

La poule aux œufs d'or de l'édition russe est, comme partout ailleurs, la littérature policière. Les polars empreints d'ironie de Daria Dontsova connaissent un grand succès. Les 50 romans policiers qu'elle a pour l'instant écrits se sont vendus à des millions d'exemplaires et sont traduits dans plusieurs pays européens.

Au début du , la demande du public russe s'est fortement accrue, en qualité comme en quantité. En conséquence, l'économie de l'édition russe est obligée de fournir ses clients en cherchant et rémunérant de nouveaux talents littéraires. Le nombre de maisons d'édition et les tirages augmentent.

Un certain nombre d'écrivains russes sont désormais populaires en Europe occidentale et en Amérique du Nord, telles Tatiana Tolstaïa ou Lïoudmila Oulitskaïa. Les polars de Boris Akounine avec son personnage fétiche Eraste Fandorine sont publiés en Europe et en Amérique du Nord. Alexandra Marinina, la plus grande écrivain de romans policiers en Russie a réussi à exporter ses livres en Europe et a bénéficié d'un grand succès en Allemagne.

La littérature plus traditionnelle trouve aussi un nouvel essor avec des auteurs venus de régions éloignées comme Nina Gorlanova de Perm avec ses histoires sur les difficultés quotidiennes et les joies de l'intelligentsia provinciale ou encore Youri Rytkhéou de la Tchoukotka qui raconte les problèmes identitaires des Tchouktches.

Des auteurs tels que Dmitri Gloukhovski ou Sergueï Loukianenko connaissent un succès avec leurs romans de science-fiction qui ont même été adaptés en films ou jeux vidéo.






</doc>
<doc id="1850" url="https://fr.wikipedia.org/wiki?curid=1850" title="Lettonie">
Lettonie

La Lettonie, en forme longue la République de Lettonie, en letton et , est un pays d'Europe du Nord et membre de l'Union européenne. Situé sur la rive orientale de la mer Baltique, c'est l'un des trois pays baltes et est bordé par la Lituanie au sud et par l'Estonie au nord. La Lettonie a aussi des frontières terrestres à l'est avec la Russie et au sud-est avec la Biélorussie. La Lettonie est un État membre de l'Union européenne depuis le mai 2004, et de la zone euro depuis le janvier 2014. Entre 1991 et 2011, la Lettonie a perdu plus de 23 % de sa population en raison d'un taux de fécondité (nombre d'enfants par femme) extrêmement faible et d'un solde migratoire négatif.

À partir du jusqu'au , la Lettonie, qui s'étendait en Livonie et en Courlande, était la possession des chevaliers prussiens de l'ordre de Livonie. Au , elle faisait partie de la Pologne et de la Suède depuis 1625. Le roi suédois Gustave II Adolphe fonda en 1632 l'université de Tartu (en allemand : Dorpat) ainsi qu'une cour d'appel à Tartu, tandis que le journal officiel du gouvernement suédois publiait l'une de ses éditions à Riga en letton. Au début de 1655, le roi suédois réclama des barons balto-allemands l'allégeance à la couronne suédoise.

Au , la Livonie et la Courlande font partie de l'Empire russe par le traité de Nystad : la Lettonie est composée du gouvernement de Courlande et d'une partie du gouvernement de Livonie. La domination traditionnelle des grands propriétaires germano-baltes et la langue allemande (langue administrative avec le russe jusqu'en 1917) ont cependant été conservées dans le pays.

Au cours de la guerre civile en Russie (1917-1922), la plupart des divisions militaires lettonnes (créées pendant la Première Guerre mondiale) combattirent contre l'Allemagne au côté des bolcheviks. Toutefois par la signature du traité de Brest-Litovsk, la Russie soviétique cède les États baltes à l'Empire allemand. Selon ce traité, la Lettonie aurait du être annexée par le Reich, mais la défaite allemande du lui permet de déclarer pour la première fois son indépendance, reconnue internationalement en 1919.

En 1940, durant la Seconde Guerre mondiale, elle est d'abord envahie, comme le prévoyaient les clauses secrètes du Pacte germano-soviétique (en même temps que les deux autres pays baltes), par l'URSS après un «plébiscite» organisé pour donner à l'annexion de la Lettonie l'apparence d'une légitimité. Toutefois, ni les États-Unis, ni le Parlement européen, ni la CEDH, ni le Conseil des droits de l'homme de l'ONU n'ont reconnu l'incorporation de la Lettonie parmi les 15 Républiques socialistes soviétiques ; de plus, la plupart des pays non-communistes membres de l'ONU ont continué à reconnaître "de jure" la Lettonie comme État souverain. Quelque furent déportés par les soviétiques et seule une minorité survécut au goulag ; ils furent remplacés après la guerre par des colons russes. Beaucoup de Lettons se réfugièrent dans la campagne ou en formant un « maquis » letton.

En 1941, la Lettonie est occupée par l'armée de la Wehrmacht, accueillie favorablement par une large majorité de la population en tant que libératrice après le régime de terreur du NKVD (il en sera de même sur tous les territoires soviétiques envahis durant les premières semaines de l'opération Barbarossa). Les maquisards lettons sont alors organisés en milices paysannes pour lutter contre les partisans des Soviétiques. Accusés en bloc d'avoir soutenu les Soviétiques, environ juifs lettons furent assassinés en Lettonie durant la Seconde Guerre mondiale, par des Einsatzgruppen allemands mais aussi par des unités paramilitaires et les forces de police lettones. D'autres Lettons ont choisi de rejoindre l'Armée rouge (cf. ).

L'Armée rouge a réoccupé à partir de 1944 la Lettonie, que l'URSS annexa sous le statut de république socialiste soviétique. À la fin de la guerre, un grand nombre de familles lettones trouvèrent refuge en Suède puis en Allemagne, aux États-Unis, au Canada et en Australie. 

Après l'occupation soviétique, la lutte armée par les maquisards lettons continua jusqu'à la mort de Joseph Staline en mars 1953. Pour priver la résistance lettone de ses approvisionnements, les Soviétiques lancèrent un programme de collectivisation forcée des fermes. En 1949, une seconde vague de déportations eut lieu : personnes furent déportées à Krasnoïarsk, Amur, Irkoutsk, Omsk, Tomsk et Novossibirsk en Sibérie (soit 2 % de la population lettone avant la guerre). En même temps, les autorités soviétiques transférèrent des milliers de Russes en Lettonie, dans le cadre d'un programme de russification du pays.

À la suite de la répression soviétique, la culture lettone fut plus diffusée après-guerre en dehors de Lettonie qu'en Lettonie-même.

Redevenue indépendante en 1991, comme la Lituanie et l'Estonie avant même l'effondrement total de l'Union soviétique, la Lettonie n'adhère pas à la CEI. La Lettonie accorde la nationalité et des passeports à la minorité russophone, qui constitue alors un tiers de la population, selon des lois qui furent examinées par une délégation du Conseil européen. Les conditions de naturalisation sont assez draconiennes et un nombre important de Russes sont privés de la citoyenneté et bénéficient d'un simple titre de séjour permanent. Ainsi, le pourcentage de Russes restant sans droits civiques s'établit à près de 14 % de la population totale en 2012.

Le pays a opté pour une alliance euro-atlantique et a finalement adhéré à l'OTAN en avril 2004, puis à l'Union européenne le "".

Le 3 juin 2015, Raimonds Vējonis, ancien ministre de l'environnement (2002-2011) puis de la défense (2014-2015), est élu Président de la Lettonie faisant de lui le premier chef d'État écologiste de l'Union européenne.

La Constitution date de 1922 et est restaurée en 1993, instaurant une république parlementaire. Le Parlement letton, la "Saeima", est unicaméral et comporte cent sièges : il est élu au suffrage universel direct tous les quatre ans.

Le président de la république est élu par les députés de la "Saeima" pour un mandat de quatre ans. Le vote se déroule à bulletin secret et à la majorité absolue (soit cinquante et une voix minimum sur cent). Son mandat est renouvelable une fois. Le président nomme le Premier ministre, qui forme avec son cabinet le pouvoir exécutif du gouvernement.

Enfin depuis 1996 une chargée de contrôler la constitutionnalité des lois a été mise en place.

La Lettonie compte parmi les États membres de l'Union européenne depuis le . Le pays dépose officiellement sa candidature pour l'adhésion aux Communautés européennes le et les négociations débutent en janvier 2000 à la suite du feu vert donné par le Conseil européen de Helsinki de décembre 1999.

Riga signe à Athènes le le traité d'adhésion aux côtés des autres pays candidats à l'adhésion (Chypre, l'Estonie, la Hongrie, la Lituanie, Malte, la Pologne, la République tchèque, la Slovaquie et la Slovénie). Le , un référendum sur la ratification par la Lettonie du traité d'adhésion à l'Union européenne donne 67 % de votes favorables contre 32,3 % d'opinions négatives, avec un taux de participation de 72,53 %. Le , la Lettonie entre dans l'Union.

L'entrée de la Lettonie dans la zone euro était prévue pour l'année 2008, mais n'a pas été possible en raison de l'importante crise financière de 2008 . Le 31 janvier 2013, la diète (parlement) de Lettonie, la Saeima, adopte une loi prévoyant l’adhésion du pays à l’euro le janvier 2014.

La Lettonie est divisée en quatre régions historiques qui ont aussi une valeur administrative secondaire : 

À compter du , la Lettonie est divisée en 110 municipalités ("novads" en letton) et 9 villes au statut spécial de la ville républicaine (en letton : ""). Les "novadi" peuvent être composées de villes et d'une ou plusieurs communes ("pagasti").

Jusqu'à cette réforme, la Lettonie était divisée en 7 villes républicaines et 26 districts (en letton : "rajons"), lesquels étaient subdivisés en "pagasti".

Chaque subdivision a une sphère d'influence sur les différents aspects du service public et perçoit une partie des impôts sur le revenu payés par les personnes enregistrées dans la subdivision.

La Lettonie comptait habitants au 1er janvier 2015, dont environ 60% de Lettons, une forte minorité russe (25%) et plusieurs autres minorités (15%). , la population de la Lettonie est en déclin depuis le début des années 1990.

Les pays baltes, dont l'Estonie, sont réputés être ou avoir été (de 2000 à 2006) le .

La monnaie officielle du pays est l'euro depuis le . Son ancienne devise, le lats, fut liée à l'euro dans le cadre du mécanisme de taux de change européen (MCE ) à partir du jusqu'à son remplacement au cours de pour 0,702804 LVL.

En 2009, la prévision de récession économique causée par la crise financière de 2008 est de 12 à 15 %. En décembre 2008, l'Union européenne et le FMI lui ont apporté une aide de 7,5 milliards d'euros, répartie sur trois ans et conditionnée à une réduction draconienne des dépenses de l'État.

L'entrée dans la zone euro était prévue pour l'année 2008, mais n'a pas été possible en raison de la crise financière de 2008 et de la trop forte inflation. Un nouvel objectif a été fixé pour 2014. Ce dernier a été atteint : la Lettonie a réussi à remplir les cinq critères du traité de Maastricht pour entrer le dans la zone euro et ainsi devenir son .

En décembre 2011, date de fin du plan d'aide financière sur trois ans au pays, la Lettonie n'a emprunté que 4,36 milliards d'euros sur les 7,5 prévus. En mai 2012, Standard & Poor's remonte la note financière de la Lettonie de BB+ à BBB-.

Les langues couramment utilisées en Lettonie sont le letton (officiel) et le russe.

Le live a officiellement disparu en 2013.

En septembre 2006, la Saeima a approuvé le projet de loi visant à faire adhérer le pays à l'Organisation internationale de la francophonie. Seul 1 % de la population maîtrise déjà le français, mais les personnes haut placées (dont l'ancienne présidente, longtemps professeur à l'université de Montréal au Canada, Vaira Vīķe-Freiberga) l'utilisent fréquemment, et une évolution grâce à l'enseignement reste donc prévue. La Lettonie est donc devenue observateur de l'organisme en 2008 lors du sommet qui se tint à Québec (Canada).

En 2012, un référendum a proposé plusieurs amendements à la constitution de la Lettonie pour faire du russe la deuxième langue officielle du pays, mais il a été refusé à 74,8 %.

La Lettonie est un pays de tradition luthérienne (70 % de la population en 1945). Mais par des récents sondages, il semblerait que la majorité de la population lettone ne pratique plus. Cependant, par les registres de naissance, il apparaît qu'à peu près les trois quarts de la population s'affilieraient à part équivalente (entre 20 et 25 %) aux trois religions chrétiennes suivantes : le protestantisme (église luthérienne), le catholicisme et l'orthodoxie.

Environ la moitié des Lettons soit a suivi les cours d'une école de musique, soit chante dans un chœur, soit sait jouer d'un instrument. L'Opéra national et l'Orchestre symphonique national sont fréquentés par une grande partie de la population. Les premiers opéras ont été organisés à Riga au — la première représentation en letton eut lieu en 1883.

Les festivals nationaux lettons du Chant et de la Danse ("Latviesu Dziesmu un Deju Svetki") sont d'importants événements dans la vie culturelle de la Lettonie et ont lieu tous les cinq ans depuis 1873. Les célébrations des chants et danses baltes organisés en Lettonie, en Lituanie et en Estonie ont été primés dans la liste des chefs-d’œuvre du patrimoine oral et immatériel de l’humanité par l'UNESCO. En 2014, Riga sera la capitale européenne de la culture conjointement avec Umeå en Suède et accueillera de nombreuses festivités culturelles

Festival de cinéma Arsenāls se déroulait à Riga au mois de septembre de 1986 à 2012.

Le festival letton de drame contemporain « Insight », en mars, célèbre les acteurs montants du drame contemporain.

Le festival cinématographique bisannuel Lielais Kristaps a été fondé en 1977 .

Le festival cinématographique bisannuel 2ANNAS a été fondé en 1996 .

Les noms de et sont originaires de la mythologie lettone et repris par les missionnaires allemands lors de la christianisation.

Autres jours importants :


La Lettonie a pour codes :





</doc>
<doc id="1851" url="https://fr.wikipedia.org/wiki?curid=1851" title="Langue agglutinante">
Langue agglutinante

Une langue agglutinante est, en typologie morphologique, une langue dont les traits grammaticaux sont marqués par l'assemblage d'éléments basiques ou morphèmes, chaque morphème correspondant à un trait et chaque trait étant noté par un morphème (dont la forme est quasi invariable). 

Les langues agglutinantes forment un sous-groupe des langues flexionnelles. Le terme de "langue agglutinante" a été créé en 1836 par le linguiste allemand Wilhelm von Humboldt. Il est formé à partir du verbe latin , signifiant « coller ensemble ». L'autre groupe des langues flexionnelles est celui des langues synthétiques.

Parmi les langues agglutinantes sont les langues dravidiennes (tamoul), les langues austronésiennes (indonésien, tagalog), les langues altaïques (turc, mongol), les langues ouraliennes (estonien, finnois, hongrois), le coréen, le japonais, le basque, le nahuatl, le géorgien, l'abkhaze, le swahili, le , le zoulou, le quechua, l'aymara. L'arménien est la seule langue indo-européenne agglutinante.

L'inuit est aussi une langue agglutinante mais appartient à un type de langue agglutinante particulier : les langues polysynthétiques. L'espéranto, comme certaines langues construites à visée internationale, possède également des mécanismes d'agglutination, bien que très sommaires.

Certaines langues de fiction, comme le klingon et le novlangue, sont fondées sur une syntaxe agglutinante.







 : une langue agglutinante est une langue dont la "flexion" est agglutinante (la déclinaison ou/et la conjugaison), mais ce caractère ne doit pas être confondu avec une grande capacité à créer des "mots composés".
Ainsi, l'allemand, malgré une forte capacité de composition ( = Société de navigation à vapeur du Danube), ne fait pas partie des langues agglutinantes. Le principe de la composition (nominale, verbale ou autre) se trouve dans un grand nombre de langues qui peuvent avoir un fonctionnement grammatical global très différent.



</doc>
<doc id="1852" url="https://fr.wikipedia.org/wiki?curid=1852" title="Langues par zone géographique">
Langues par zone géographique

Cette liste présente les langues naturelles classées en fonction de leur région d'origine ou des endroits où elles sont parlées (ou ont été parlées dans le cas des langues mortes).











"Article détaillé" : Langues en Inde


"Article détaillé" : Langues aux Philippines






Voir l'article langues régionales de France














</doc>
<doc id="1853" url="https://fr.wikipedia.org/wiki?curid=1853" title="Langues finno-ougriennes">
Langues finno-ougriennes

Les langues finno-ougriennes sont une famille de langues parlées en Europe, par les peuples finno-ougriens, sur une vaste aire géographique qui va de la mer Baltique et du nord de la Scandinavie jusqu'à l'Oural et au Don. Le nombre total de locuteurs de ces langues est estimé à 25 millions de personnes. Toutefois, le nombre de locuteurs varie très fortement selon les langues, allant de 14 millions pour le hongrois jusqu'à quelques locuteurs isolés pour le vote. Un certain nombre de langues finno-ougriennes ont disparu au cours du et plusieurs autres sont menacées d'extinction.

Les langues finno-ougriennes sont habituellement considérées comme formant l'une des deux grandes branches de la famille des langues ouraliennes, l'autre branche étant celle des langues samoyèdes. Cette dichotomie a été fortement remise en question ces dernières années, la branche samoyède étant désormais placée par certains au même niveau de ramification que les autres sous-familles. Avec l'effacement de cette dichotomie, le sens du terme « finno-ougrien » s'est élargi et le mot est de plus en plus souvent utilisé pour désigner l'ensemble de la famille ouralienne, y compris les langues samoyèdes. Depuis leur création dans les années 1960, les congrès mondiaux rassemblant tous les cinq ans les spécialistes des langues ouraliennes ont pour nom officiel « Congrès mondial des finno-ougristes ».

Selon la théorie la plus communément admise par les spécialistes, les langues ouraliennes sont issues d'une langue mère commune, le , qui aurait été parlée il y a au moins ans. L'une des grandes tâches de la linguistique finno-ougrienne a été pendant longtemps de reconstituer cette proto-langue et les diverses étapes de son évolution qui ont donné naissance aux langues actuelles. Les finno-ougristes ont également tenté de déterminer, en croisant différentes approches (linguistique, archéologie, paléobotanique, génétique des populations), le territoire où résidaient ses locuteurs.

Cette théorie de la proto-langue est contestée depuis la fin des années 1980 par quelques linguistes (Ago Künnap, , János Pusztay), qui ont tenté de la remplacer par l'idée que les langues finno-ougriennes actuelles seraient issues d'une ancienne "lingua franca".

La plupart des langues finno-ougriennes sont agglutinantes et recourent aux suffixes plutôt qu'aux prépositions. Elles sont généralement dépourvues de genre grammatical. Les adjectifs et pronoms possessifs sont rarement employés et la possession est exprimée au moyen des déclinaisons. Les langues qui ont évolué vers une forme flexionnelle emploient le génitif du pronom personnel, d'autres utilisent un suffixe pronominal, parfois combiné au génitif d'un pronom.

Le tableau ci-dessous indique les noms des nombres de "1" à "10" dans les principales langues finno-ougriennes et leur reconstitution en proto-finno-ougrien.
Une reconstruction possible pour les nombres "8" et "9" est *"kak+teksa", soit « dix moins deux » et *"yk+teksa" « dix moins un », où *"teksa" (cf. "deka") serait un emprunt indo-européen ; la différence entre /t/ et /d/ n'est pas phonémique, contrairement à l'indo-européen.

De façon générale, les langues ouraliennes se répartissent actuellement en sous-groupes bien caractérisés, mais les relations plus anciennes de ces sous-groupes sont peu claires, peu étudiées, et rendent difficile de les rassembler en branches plus larges.
La classification interne traditionnelle des langues finno-ougriennes est la suivante :

Les langues mordves sont plus proches des langues finno-sames que du mari.




</doc>
<doc id="1854" url="https://fr.wikipedia.org/wiki?curid=1854" title="Langues samoyèdes">
Langues samoyèdes

Les langues samoyèdes (parfois écrit samoïèdes) sont une famille de langues, en usage des deux côtés de l'Oural par une trentaine de milliers de personnes. Elles constituent traditionnellement une des deux branches de l'ensemble de langues ouraliennes, l'autre étant celle des langues finno-ougriennes. Toutefois, cette dichotomie primitive a été récemment remise en cause par certains linguistes, qui tiennent les langues samoyèdes pour une branche de même niveau que les subdivisions du finno-ougrien. De façon générale, les langues ouraliennes se répartissent actuellement en sous-groupes bien caractérisés, mais les relations plus anciennes de ces sous-groupes sont peu claires, peu étudiées, et rendent difficile de les rassembler en branches plus larges.

À un niveau taxinomique supérieur, on rapproche souvent des langues ouraliennes le youkaguir de l'est de la Sibérie.

Le terme de "samoyède" vient du russe самоед ("samoyed"), traduit par l'étymologie populaire comme signifiant « qui se mange soi-même » ("сам" → soi-même ; "ед" → manger), mais qui serait plutôt à rapprocher de l'auto-ethnonyme des sames : saamit.

Le nénètse de la toundra demeure la langue la plus répandue, et est même une langue officielle dans plusieurs régions autonomes (okrugs) de Russie.

La science qui étudie les langues samoyèdes s'appelle la samoyèdistique, et fut créée au , par entre autres les Finnois Matthias Alexander Castrén et Kai Donner ainsi que le germano-balte Franz Anton Schiefner. Elle ne peut généralement pas être étudiée séparément de l'ouralistique.

Les langues samoyèdes se transmettent le long des côtes arctiques de Russie, de la mer Blanche à la mer de Laptev, en passant par la Nouvelle-Zemble, la péninsule de Yamal, les embouchures des fleuves Ob et Ienisseï, et la péninsule de Taïmyr. Elles sont contiguës avec les langues ougriennes trans-ouraliennes et les langues permiennes cis-ouraliennes, mais sont séparées des langues fenniques par les Russes, à l'ouest, et du youkaguire par le peuple turc des Yakoutes, à l'est. Au , une ville importante de Samoyèdes se constitua à Mangazeïa, qui fut détruite au .


Notes : l'énètse et le yourak ne sont pas présentés par manque d'information.




</doc>
<doc id="1855" url="https://fr.wikipedia.org/wiki?curid=1855" title="Liste d'avions civils">
Liste d'avions civils









Société disparue 



Société disparue




Société disparue



Société disparue


"Avions actuels" :
"Avions en développement :"

Société disparue

Société disparue




Société disparue



Société disparue




Société disparue





Société disparue











financement



















Pour les avions actuellement en cours de production, voir aussi : aviation légère.


Anciennement Centre-Est Aéronautique et Avions Pierre Robin
Voir Robin














Les Jodels sont des avions qui ont été construits soit par des amateurs, soit par diverses sociétés. Ils sont tous regroupés ici :












Site des avions Piel
















Site des avions Piel



FERRASOFIAN WINGS
F200
F10
F1400
Voir Rutan Aircraft Factory









</doc>
<doc id="1856" url="https://fr.wikipedia.org/wiki?curid=1856" title="Locatif">
Locatif

En linguistique, le locatif est un cas grammatical exprimant la localisation dans l'espace (sans mouvement), le lieu où se déroule l'action exprimée par le verbe. Dans certaines langues, il peut se subdiviser en plusieurs cas spécifiques, selon que le lieu est fermé (inessif) ou ouvert (adessif, superessif).
Le locatif est un des huit cas indo-européens, présent en sanskrit. Il n'a pas totalement disparu en latin. Par exemple en latin, "domi", « à la maison », et "humi", « à terre » sont d'anciens locatifs. Cependant, généralement c'est l'ablatif précédé de la préposition "in" qui prend en charge la valeur de locatif, sauf pour les villes et les petites îles qui appartiennent à la première ou la deuxième déclinaisons.

En russe ancien, il existait un locatif qui fonctionnait seul (sans préposition), qui s’employait pour préciser la situation d'un procès aussi bien dans le temps que dans l'espace ; mais il s'est assez tôt transformé en prépositionnel, qui ne fonctionne plus qu'associé à certaines prépositions.

 ("Tu habites à Paris").

 (Tu habites à Paris).


</doc>
<doc id="1858" url="https://fr.wikipedia.org/wiki?curid=1858" title="Équations de Maxwell">
Équations de Maxwell

Les équations de Maxwell, aussi appelées équations de Maxwell-Lorentz, sont des lois fondamentales de la physique. Elles constituent les postulats de base de l'électromagnétisme, avec l'expression de la force électromagnétique de Lorentz.

Ces équations traduisent sous forme locale différents théorèmes (Gauss, Ampère, Faraday) qui régissaient l'électromagnétisme avant que Maxwell ne les réunisse sous forme d'équations intégrales. Elles donnent ainsi un cadre mathématique précis au concept fondamental de champ introduit en physique par Faraday dans les années 1830.

Ces équations montrent notamment qu'en régime stationnaire, les champs électrique et magnétique sont indépendants l'un de l'autre, alors qu'ils ne le sont pas en régime variable. Dans le cas le plus général, il faut donc parler du champ électromagnétique, la dichotomie électrique-magnétique étant une vue de l'esprit. Cet aspect trouve sa formulation définitive dans le formalisme covariant présenté dans la seconde partie de cet article : le champ électromagnétique y est représenté par un objet mathématique unique, le tenseur électromagnétique, dont certaines composantes s'identifient à celles du champ électrique et d'autres à celles du champ magnétique.

Les équations sont les suivantes :


Cette « correction » de Maxwell du théorème d'Ampère est particulièrement importante : elle signifie que la variation d'un champ magnétique crée un champ électrique et que la variation d'un champ électrique crée un champ magnétique. Par conséquent, ces équations permettent la circulation d'ondes électromagnétiques auto-entretenues, ou « rayonnement électromagnétique ».

La vitesse de propagation calculée pour les ondes électromagnétiques, qui pourrait être prédite par des expériences sur les charges et les courants, est exactement la vitesse de la lumière. En effet, la lumière est une forme de rayonnement électromagnétique (tout comme les rayons X, les ondes radio, etc.). Maxwell avait compris la relation entre le rayonnement électromagnétique et la lumière en 1864, unifiant ainsi deux domaines jusqu'ici disjoints : celui de l'électromagnétisme et celui de l'optique.

Vers 1865, Maxwell a réalisé une synthèse harmonieuse des diverses lois expérimentales découvertes par ses prédécesseurs (lois de l'électrostatique, du magnétisme, de l'induction…). Mais cette synthèse n'a été possible que parce que Maxwell a su dépasser les travaux de ses devanciers, en introduisant dans une équation un « chaînon manquant », appelé le courant de déplacement, dont la présence assure la cohérence de l'édifice unifié.

Maxwell a d'abord publié en 1865 sa théorie sous la forme de vingt équations à vingt inconnues, écrit à l'aide de quaternions. En 1873, dans l'ouvrage en deux volumes "", Maxwell a déjà réécrit sa théorie sous la forme de huit équations. Ce n'est que plus tard qu'Heaviside réécrivit ces équations sous la forme des quatre équations vectorielles aux dérivées partielles que l'on connaît maintenant.

Aujourd'hui, les quatre équations (vectorielles) de Maxwell se réduisent à seulement deux équations tensorielles, ou même à une seule équation multivectorielle en algèbre géométrique.

La synthèse de Maxwell a permis ultérieurement les deux plus grandes avancées de la physique moderne :


On présente ci-dessous la théorie microscopique fondamentale qui donne les équations de Maxwell-Lorentz dans le vide en présence de sources, qui peuvent être des charges ponctuelles et/ou leurs courants électriques microscopiques associés si ces charges sont en mouvement dans le référentiel d'étude.

La théorie "macroscopique" nécessitant l'introduction des champs "D" et "H" (et les équations de Maxwell associées) est discutée en détail dans Électrodynamique des milieux continus.

On note :


Dans cette équation, on utilisera l'opérateur nabla, noté : formula_9, valable en coordonnées cartésiennes uniquement. En particulier, formula_10.

Cette équation locale donne la divergence du champ électrique en fonction de la densité de la charge électrique :

Cette équation correspond à un « terme de source » : la densité de charge électrique est une source du champ électrique. Par exemple, pour une charge ponctuelle formula_11 fixée à l'origine formula_12, la loi de Coulomb donnant le champ électrostatique en un point formula_13 de l'espace, point repéré par le vecteur position formula_14 où formula_15 est le vecteur unitaire radial, s'écrit :
formula_16.
Ce champ électrostatique vérifie l'équation de Maxwell-Gauss pour la source statique :
formula_17.
où formula_18 est la distribution de Dirac dans l'espace à trois dimensions.

Le théorème de Gauss est la forme intégrale de l'équation de Maxwell-Gauss. Il affirme que le flux du champ électrique permanent à travers une surface de Gauss fermée formula_19, orientée selon la normale sortante, est égale au rapport de la charge formula_20 contenue dans le volume formula_21 délimité par la surface formula_19 et de la permittivité du vide :

On remarquera que l'équation de Maxwell-Gauss se retrouve facilement en appliquant le théorème d'Ostrogradski au théorème de Gauss et en prenant un volume infinitésimal.

Aussi appelée équation de Maxwell-Thomson.

Le flux du champ magnétique à travers une surface formula_24 "fermée" est toujours nul :

Cette équation est la forme intégrale de l'équation locale de Maxwell, et on passe de l'une à l'autre en appliquant le théorème d'Ostrogradski.

Cette équation locale est au champ magnétique ce que l'équation de Maxwell-Gauss est au champ électrique, à savoir une équation avec « terme de source », ici identiquement nul :

Elle traduit le fait expérimental suivant : il n'existe pas de monopôle magnétique. Un monopôle magnétique serait une source ponctuelle de champ magnétique, analogue de la charge électrique ponctuelle pour le champ électrique. Or, l'objet de base source d'un champ magnétique est l'aimant, qui se comporte comme un dipôle magnétique : un aimant possède en effet un pôle nord et un pôle sud. L'expérience fondamentale consistant à tenter de couper un aimant en deux donne naissance à deux aimants, et non un pôle nord et un pôle sud séparément.

L'analyse vectorielle montre que la divergence d'un rotationnel est toujours identiquement nulle pour tout champ quelconque formula_25 : 
formula_26.
Réciproquement, tout champ de vecteurs dont la divergence est identiquement nulle peut localement être exprimé sous la forme d'un rotationnel.

L'équation locale de conservation du flux magnétique permet donc de définir au moins localement un potentiel-vecteur formula_27 tel que :
formula_28.
Le problème important de l'unicité du potentiel-vecteur est discuté dans l'article "Invariance de jauge de la théorie".

Cette équation locale traduit le phénomène fondamental d'induction électromagnétique découvert par Faraday. 

Elle donne le rotationnel du champ électrique en fonction de la dérivée temporelle du champ magnétique :

Cela correspond à un « terme variationnel » : la variation du champ magnétique crée un champ électrique. Sa forme intégrale est donnée par :

formula_29 

et la loi de Faraday, respectivement:

où formula_30, est la force électromotrice d'induction dans un circuit électrique et formula_31 le flux magnétique à travers ce circuit.

L'analyse vectorielle montre que le rotationnel d'un gradient est toujours identiquement nul. Pour un champ scalaire quelconque formula_32 : 
formula_33. 
L'équation de Maxwell-Faraday couplée à l'existence locale d'un potentiel-vecteur formula_27 permettent de définir (au moins localement) le potentiel électrique formula_35 (scalaire) tel que :
formula_36. 
Le problème important de l'unicité du potentiel électrique est discuté dans "Invariance de jauge de la théorie".

Cette équation est héritée du théorème d'Ampère. Sous forme locale, elle s'écrit en termes du vecteur densité de courant formula_37 :

L'équation précédente peut se réécrire :

en introduisant le courant de déplacement de Maxwell :

La forme intégrale lie la circulation du champ magnétique sur un contour formula_38 fermé, et les courants qui traversent une surface formula_39 s'appuyant sur ce contour. C'est une conséquence directe du théorème de Green :

Prenons la divergence de l'équation de Maxwell-Ampère :

On peut écrire en permutant les dérivées spatiales et temporelles, puis en utilisant l'équation de Maxwell-Gauss :

On obtient finalement l'équation locale de conservation de la charge électrique :

La présence du terme de courant de déplacement, introduit par Maxwell, est essentielle à l'obtention de cette équation.

Prenons le rotationnel de l'équation de Maxwell-Faraday, compte tenu de Maxwell-Gauss et de Maxwell-Ampère :
formula_40, 

soit, en utilisant le fait que les dérivées spatiales et temporelle sont indépendantes

formula_41, 

ou, en réorganisant :

formula_42. 

Ceci montre que le champ électrique suit l'équation des ondes.

En prenant le rotationnel de l'équation de Maxwell-Ampère, compte tenu de Maxwell-Thomson et de Maxwell-Faraday, on retrouve le résultat équivalent :

formula_43. 

Ceci montre que le champ magnétique suit également l'équation d'ondes.

La vitesse de propagation de l'onde électro-magnétique formula_44 est donnée par :

formula_45.. 

L'analyse vectorielle montre que la divergence d'un rotationnel est toujours identiquement nulle : 

formula_46.

L'équation locale de conservation du flux magnétique permet donc de définir au moins localement un "potentiel-vecteur" formula_47 tel que :

L'analyse vectorielle nous dit également que 

formula_48.

Alors le potentiel-vecteur n'est pas défini de manière unique puisque la transformation suivante, avec formula_49 une fonction quelconque

formula_50

ne modifie pas la valeur du champ formula_51. Ceci est un exemple de transformation de jauge. Il faut donc imposer des conditions supplémentaires pour définir formula_47 de façon non-ambiguë. On appelle cela des conditions de jauge, par exemple la jauge de Coulomb ou encore la jauge de Lorenz.

Le lecteur notera qu'en physique classique, le potentiel-vecteur semble n'être qu'un outil mathématique commode pour analyser les solutions des équations de Maxwell, mais ne semble pas être une grandeur physique directement "mesurable". En 1959, dans le cadre de la physique quantique, Aharonov et Bohm ont démontré que le potentiel-vecteur avait un effet observable en mécanique quantique : c'est l'effet Aharonov-Bohm.

L'équation de Maxwell-Faraday couplée à l'existence locale d'un potentiel-vecteur formula_27 permettent de définir (au moins localement) le potentiel électrique formula_35 (scalaire) tel que :

Le potentiel formula_35 lui non plus n'est pas défini de façon unique mais la transformation de jauge associée et liée à celle de formula_27 est la suivante (on rappelle celle de formula_27 par souci de clarté) et l'on a 

formula_58.

Ces deux équations donnent l'invariance de jauge complète des équations de Maxwell.

On pose la condition de jauge de Lorenz (qui couple les 2 potentiels) : 

formula_59.

Prenons l'équation de Maxwell-Ampère, compte tenu de la condition de jauge de Lorenz et de l'expression de formula_60 en fonction des potentiels formula_35 et formula_27 :

formula_63,
formula_64.
On obtient l'équation de Poisson du potentiel vecteur :formula_65.

Idem pour le potentiel scalaire :

formula_66,soit

formula_67.

Pour simplifier, nous abrégeons « équations de Maxwell » en EM.

Les expressions des champs électriques et magnétiques peuvent être obtenues en intégrant sur tout l'espace les équations de Liénard-Wichert ou celles de Heaviside-Feynman.

Résolvons les équations de Maxwell dans l'espace .

Représentons des solutions par des lettres "Q", "R", … (ensembles des 6-vecteurs formés des six composantes du champ en tout point de coordonnées "x", "y", "z", "t"). Comme dans le vide les équations sont linéaires, "aQ + ßR +…", où "a", "ß", … sont des constantes réelles, est aussi une solution. En conséquence, l'ensemble des solutions des équations de Maxwell est un espace vectoriel réel.

Conformément à la définition introduite en acoustique, un "mode" est une direction de cet espace. Un système complet de solutions constitue une base dans cet espace nommé tantôt espace des solutions, tantôt espace des modes. Une solution particulière dans un mode est obtenue en multipliant un champ de ce mode posé comme champ d'amplitude unité, par une constante réelle, l'amplitude.

Avec un système d'unités convenable, l'énergie (à un moment donné) "W"("Q") d'une solution "Q" est l'intégrale étendue à tout l'espace, du carré de la norme du vecteur "Q" par rapport au produit scalaire usuel. Il faut faire attention au fait que l'énergie ne dépend pas linéairement de "Q". L'énergie de la somme de plusieurs solutions n'est donc pas, "a priori", la somme des énergies des différentes solutions prises séparément. Néanmoins, le procédé de Gram-Schmidt permet d'obtenir, à partir d'un système complet de solutions, un système complet de solutions orthogonales, ou encore système complet de modes orthogonaux. Dans de tels systèmes, les énergies sont indépendantes, c'est-à-dire que l'énergie d'une solution est égale à la somme des énergies de ses différentes composantes dans le système.

Planck a posé que l'énergie dans un mode monochromatique de fréquence ν se propageant dans un corps noir à la température T est . La valeur erronée de "K" donnée par Planck a été corrigée par Nernst en 1916 ; la valeur est facilement retrouvée car la thermodynamique impose que "w" tende vers "kT" lorsque "T" tend vers l'infini. Cette formule définit la température d'un mode. Cependant l'interprétation de cette formule est physiquement délicate car la définition d'une fréquence pure suppose une expérience de durée infinie.

On sait calculer les champs émis par des charges, par exemple le champ émis par un dipôle électrostatique oscillant. Pour se ramener au problème précédent, on utilise l'« astuce de Schwarzschild et Fokker ». Le champ émis par une source est nommé « champ retardé » . Dépouillé de la source, ce champ n'est "pas" solution des EM. Pour obtenir une solution identique dans le futur, il faut lui ajouter un « champ avancé » . Par cette définition, est solution des EM. Ainsi, en substituant le champ avancé à la source, on est ramené au problème linéaire d'un champ dans le vide et on peut définir des modes.

Un système physique possède, en général, des minimums d'énergie relatifs. En régime non évolutif (stationnaire), le système, excité par un champ électromagnétique de l'ordre de "h"/2 dans chaque mode qu'il est susceptible d'émettre (donc d'absorber), reste au voisinage d'un minimum d'énergie ; pour chaque mode monochromatique, son excitation l'amène à rayonner un champ en quadrature avec le champ incident, ce qui ne produit aucun échange d'énergie permanent, mais introduit un retard, la réfraction. Pour un champ plus intense, en particulier en raison d'une fluctuation favorable du champ, le système peut franchir un col de son diagramme d'énergie et absorber une énergie "h" cette absorption peut conduire à un niveau peu stable d'où le système peut évoluer rapidement vers d'autres niveaux, en une cascade plus ou moins radiative qui l'amène à un état stationnaire, stable.

Dans une théorie classique, aucun paradoxe ne peut être admis, en particulier le paradoxe d'Einstein, Podolsky et Rosen n'existe pas : supposons qu'un atome perde une énergie de résonance "h", par exemple par le rayonnement d'un dipôle. Le mode d'émission de ce dipôle n'est pas orthogonal aux modes d'émission (donc d'absorption) d'autres atomes dont l'amplitude peut être accrue ; 0, 1, 2, … atomes peuvent alors absorber "h", même si, en moyenne, un seul atome est excité ; les champs résiduels jouent le rôle d'un bain thermodynamique.

Il a été écrit que l'électron d'un atome d'hydrogène suivant une orbite de Bohr émet un champ, donc rayonne de l'énergie et devrait tomber sur le noyau. L'électron émet bien un champ, mais d'énergie très faible en raison de l'interférence du champ émis avec le champ résiduel ; cette énergie tombe à zéro si l'orbite est légèrement corrigée, de sorte que l'énergie de l'état stationnaire subit le décalage de Lamb.

L'étude de l'amorçage d'un laser semble indiquer que le champ du point zéro induit une émission deux fois plus intense qu'un champ d'intensité plus grande. Pour tenir compte de ce résultat, on peut introduire une « radiation de réaction », "ad hoc". La véritable explication est très simple : un atome est excité par un champ dans le mode qu'il peut émettre, dit sphérique ; au démarrage du laser, il existe dans ce mode une amplitude correspondant à "h"/2 ; le laser fonctionne sur un mode d'onde plane dont il faut prendre la composante sphérique pour exciter l'atome, ce qui divise l'énergie par deux.

Il n'existe pas de système électromagnétique isolé ; oublier que le champ minimum est le champ du point zéro conduit à des erreurs lorsqu'on détecte des champs faibles.

NB Cette partie suit les conventions de signe classiques de MTW

Cette partie adopte également la convention de sommation d'Einstein.

L'espace-temps de Minkowski (1908) est une variété différentielle "M" plate munie d'une métrique lorentzienne.

Soit un système de coordonnées quelconque formula_68 autour d'un évènement (point) formula_69 de l'espace-temps, et soient formula_70 une base locale de formula_71, espace tangent à la variété au point formula_72. Un vecteur tangent formula_73 s'écrit alors comme la combinaison linéaire :

Les formula_74 sont appelée les composantes "contravariantes" du vecteur w. Le tenseur métrique formula_75 est la forme bilinéaire symétrique :

Dans une base orthonormée d'un "référentiel inertiel", ses composantes covariantes formula_76 sont :

Ses composantes contravariantes formula_77 vérifient :

On obtient explicitement :

On utilisera ci-dessous les conventions usuelles suivantes :


Par exemple, les composantes contravariantes du 4-vecteur position s'écrivent dans un système de coordonnées orthonormales :

Le tenseur métrique définit pour chaque point formula_72 de l'espace-temps un pseudo-produit scalaire ("pseudo" au sens où l'hypothèse de positivité est retirée) dans l'espace formula_71 euclidien tangent à "M" au point formula_80. Si formula_81 et formula_82 sont deux vecteurs de formula_71, leur produit scalaire s'écrit :

En particulier, en prenant deux vecteurs de base, on obtient les composantes :

formula_74 désignant les composantes "contravariantes" du vecteur w, on peut définir de même ses composantes "covariantes" par :

Par exemple, les composantes covariantes du 4-vecteur position s'écrivent dans un système de coordonnées orthonormales :

On introduit l'opérateur différentiel "quadri-gradient" par ses composantes covariantes :

Ses composantes contravariantes s'écrivent :

L'opérateur invariant d'Alembertien s'écrit par exemple :

On introduit le quadrivecteur potentiel électromagnétique par ses composantes contravariantes :

où formula_35 est le scalaire potentiel électrique, et formula_86 le potentiel-vecteur magnétique. Ses composantes covariantes s'écrivent :

Les lois de transformation de jauge écrites précédemment sont donc résumées dans cette notation sous la forme

formula_87.

La condition de jauge de Lorenz s'écrit par exemple de façon covariante :

On introduit le "quadri-courant" électromagnétique par ses composantes contravariantes :

où formula_88 est le scalaire densité électrique de charge, et formula_37 le vecteur densité de courant. Ses composantes covariantes s'écrivent :

Le tenseur électromagnétique est le tenseur antisymétrique de rang deux défini à partir du quadri-potentiel par :

Ses composantes covariantes s'écrivent explicitement :

On obtient ses composantes contravariantes en écrivant :

La métrique étant diagonale dans un référentiel inertiel, on obtient alors les formules suivantes, "sans sommation sur les indices répétés" : 


soit explicitement :

Les équations de Maxwell se mettent sous forme relativiste covariante.

Puisque le tenseur de Maxwell est antisymétrique, cette dernière relation entraîne en particulier que "le quadri-courant est conservé" :

En écrivant explicitement le tenseur de Maxwell en termes du quadri-potentiel dans l'équation covariante avec terme de sources, on obtient pour le membre de gauche :

Dans la jauge de Lorenz formula_93, le second terme disparaît, et l'équation de Maxwell avec terme de sources se réduit à une équation de propagation pour le quadri-potentiel :

La solution de cette équation s'écrit de façon simple si l'on connaît une fonction de Green de l'équation de propagation, c'est-à-dire une fonction "G(x)" solution de l'équation aux dérivées partielles :

où formula_94 est la distribution de Dirac. On obtient alors le quadri-potentiel sous la forme d'un produit de convolution :

En électrodynamique classique, on utilise le plus souvent la fonction de Green "retardée" qui satisfait à l'hypothèse de causalité :



Accessible au niveau du premier cycle universitaire.





</doc>
<doc id="1861" url="https://fr.wikipedia.org/wiki?curid=1861" title="Littérature grecque">
Littérature grecque

La littérature grecque commence avec Homère () et elle se prolonge jusqu'à nos jours. En 28 siècles, la langue grecque a évolué et la littérature aussi. La littérature grecque antique, d'abord orale, est attestée par écrit à partir des deux épopées mythologiques que sont l’"Iliade" et l’"Odyssée" et se développe rapidement. Les ouvrages composés pendant la période classique, aux Ve et IV siècles avant J.-C., ont exercé une forte influence sur la littérature des siècles ultérieurs ainsi que sur la littérature latine. Après l'Antiquité, la littérature grecque se prolonge dans la littérature de l'empire byzantin, puis dans la littérature grecque moderne.

On distingue habituellement plusieurs périodes.

On ne possède pas d'œuvres grecques plus anciennes que l’"Iliade" ( ou ), mais il semble nécessaire qu'il y ait eu une longue évolution littéraire pré-homérique tant cette œuvre est achevée.

Jusqu'au début du : Homère, Hésiode, les poètes lyriques (Alcée, Sappho, Pindare, Anacréon, Archiloque de Paros, Tyrtée, Alcman, Sémonide d'Amorgos, etc.

Aux et , celle des grands poètes tragiques (Eschyle, Sophocle, Euripide), d'Aristophane, de Platon, mais aussi des sophistes et de certains « présocratiques » comme Leucippe et Démocrite), d'Aristote, des "orateurs attiques" (Lysias, Isocrate, Démosthène, etc.), d'Hérodote, de Thucydide, de Xénophon.

De la mort d'Alexandre le Grand au , marquée par la naissance de la critique philologique, le développement de l'épicurisme et du stoïcisme, et un goût prononcé pour la poésie (Callimaque de Cyrène, Apollonios de Rhodes, Théocrite). C'est aussi la période de rédaction de la Septante.

Du au , marquée par le déclin de la poésie, l'essor de l'histoire et de la géographie (Strabon, Denys d'Halicarnasse, Pausanias, Dion Cassius, Arrien, etc.) et l'enseignement institutionnalisé de la philosophie. Le voit aussi les débuts de la "Seconde Sophistique" et l'épanouissement du roman grec. Les deux auteurs emblématiques de la période sont Plutarque et Lucien.

Du au La conversion de l'empereur Constantin sonne le début d'une ère de profondes mutations, marquée par l'essor de la littérature chrétienne (œuvres patristiques d'Eusèbe de Césarée, Ignace d'Antioche, Clément d'Alexandrie) et le déclin de l'hellénisme païen (Julien, Libanios).

Jusqu'en 1453, date de la prise de Constantinople par les Turcs. On peut citer de cette période Procope de Césarée, Jean Zonaras, Constantin Porphyrogénète, Photios de Constantinople, Planude, Anne Comnène, Gemiste Pléthon.

Jusqu'au début du . Les œuvres de cette période sont souvent anonymes (chants et poèmes populaires).

Après la révolution de 1821, où la littérature grecque connaît un renouveau. Parmi les principaux auteurs de cette période (jusqu'au début ): Dionýsios Solomós, Andréas Kálvos, Kostís Palamás, Emmanuel Roïdis, Alexandros Papadiamandis, Ángelos Sikelianós, Kostas Karyotakis.

Au , les noms de Constantin Cavafy, Níkos Kazantzákis, Stratis Tsirkas, Dimitris P. Kraniotis, Yánnis Rítsos, Georges Séféris et Odysséas Elýtis sont particulièrement célèbres, ces deux derniers ayant été récompensés par le Prix Nobel de littérature. 

Parmi les auteurs contemporains, Dimitris Lyacos a consolidé sa réputation dans le récent contexte postmoderne.







 

</doc>
<doc id="1862" url="https://fr.wikipedia.org/wiki?curid=1862" title="Les Thanatonautes">
Les Thanatonautes

Les Thanatonautes est un roman de Bernard Werber, mêlant science et fantastique, publié en 1994 aux Éditions Albin Michel. Le mot "Thanatonaute" est une combinaison des racines grecques "Thanato" signifiant « la mort » et "naute" signifiant « navigateur » soit, littéralement, « navigateur de la mort » ou « explorateur de la mort. »

Le romancier a déclaré avoir produit quelques chapitres de son livre "Les Thanatonautes" par écriture automatique : 

Michael Pinson découvre la mort par son boucher monsieur Dupont, écrasé par une carcasse de bœuf charolais qui s'était inopinément décrochée. 
Autour des questions maladroites qu'il pose à sa mère, se développe une certaine curiosité à ce sujet. 
À la mort de son arrière-grand-mère Aglaé, une suggestion impromptue formulée par celui-ci provoque la colère de son aïeul, qui le gifle afin qu'il comprenne la gravité de la mort. 
Michael décide alors de se forcer à pleurer.
La troisième expérience de Michael au sujet de la mort, concerne la sienne, touché par un accident de voiture, il s'envole rapidement pour retomber ; il a alors sept ans. 

À huit ans, c'est au tour de l'oncle Norbert de décéder, à l'enterrement Michael a bien appris sa leçon, et pleure à chaudes larmes en pensant aux épinards en branche bouillies.. 
Là, il rencontre et fait la connaissance de Raoul Razorbak, qui deviendra son ami. Le cimetière du père Lachaise devient alors leur lieu de rendez-vous privilégié. Celui-ci raconte la passion de son père suicidé pour la mort : Francis Razorbak au travers de sa thèse, " la mort, cette inconnue". 
Toutefois Raoul est asocial, et se trouve être la bête noire de sa classe. Il lui fait visiter le service des mourants de l'hôpital Saint Louis afin de mieux comprendre la mort, dont ils découvrent un aspect peu ragoutant. 

Les deux amis se séparent, et Michael décide de faire des études de médecine, dans la branche anesthésie et réanimation. Les deux amis se retrouvent à l'âge de trente-deux ans. Michael est alors anesthésiste-réanimateur et Raoul, chercheur en biologie au CNRS et fait des recherches sur l'hibernation des marmottes.
Un autre thanatonaute fait son apparition dans l'histoire : le président français Lucinder, qui meurt un moment, après qu'une personne dans la foule lui a tiré dessus. Il vit alors une EMI, et est réanimé de justesse grâce à l'acharnement thérapeutique. À la suite de cette expérience, il décide de lancer un grand projet de recherche dans le domaine des NDE.
Michael (après quelques hésitations) et Raoul, avec l'aide de l'infirmière Amandine, s'adonnent alors des expériences sur les volontaires du centre pénitentiaire de Fleury-Merogis.
Après quelques échecs, ils finissent par tomber sur le bon candidat : Felix Kerboz. 

Mais le scandale médiatique de la thanatonautique éclate : on découvre tous les morts sacrifiés pour en arriver là. 
Afin d'évaluer la bonne foi de Lucinder, une expérience médiatisée est organisée en direct, il s'agit de faire revenir Felix Kerboz à la vie. Elle réussit de justesse, mais cela a permis au président de jouir d'une nouvelle renommée. 
La course pour la thanatonautique débute, il s'agit de repousser au plus loin l'exploration du continent décrit par Felix. Qu'y a-t-il derrière l'entonnoir? La carte et l'exploration du continent des morts deviennent alors une priorité, dans le monde entier. 
Un autre candidat est engagé, Jean Bresson, un cascadeur au sang froid, qui remplace rapidement Felix après sa mort, faisant la course avec l'anglais Bill Graham, un ex-trapéziste, puis d'autres venus le remplacer. 

Jean Bresson réussit alors à dépasser le premier mur somatique ; ce qu'il y découvre est terrible, après le territoire bleu, il y a le territoire noir, celui des ténèbres.
Du temps passe avant que la course ne reprenne, face à la peur que représente ce territoire. Stefania Chichelli la bouddhiste italienne fait son apparition. Elle n'utilise pas de produits chimiques pour voyager, mais la technique de la méditation. Finalement elle réussit à franchir la deuxième barrière comatique pour atteindre le troisième mur, de couleur rouge, c'est la zone des plaisirs sexuels.
Ce qui provoque un conflit entre les modernes et les conservateurs. À la suite d'une attaque ectoplasmique de fanatiques religieux que Michael arrive à repousser de justesse, il décide de se marier avec Rose tandis que Raoul se marie à Stefania. Une découverte technique radiophonique permet alors de mesurer l'avancée des thanatonautes dans l'espace et de situer le paradis dans les environs de sagittarius A, un trou noir. 

L'exploration du continent des morts continue, c'est un rabbin aveugle Freddy Meyer qui découvre ce qu'il y a derrière le troisième mur comatique, une plaine s'étendant sur plusieurs kilomètres, où des milliards d'ectoplasmes attendent. Tandis qu'ils explorent le continent des morts un attentat est perpétré par la secte des haschischins, des ectoplasmes pirates qui ont tenté de couper le cordon ombilical qui le reliait à son corps.
Se déclare alors une véritable guerre des religions sur le continent des morts. Freddy s'allie à des asiatiques pour former sa petite armée ectoplasmique qu'il nomme armée de l'alliance, tandis que les haschischins forment la coalition ; à la suite de la grande bataille du paradis qui profite à l'alliance, se mettent en place une paix durable entre les deux camps, et l'établissement de règles régissant le paradis. 

L'exploration continue ; après la zone d'attente, il y a la zone jaune, la connaissance absolue. 
Freddy met en place une nouvelle technique d'exploration qui consiste à attacher les cordons ombilicaux les uns aux autres afin d'en obtenir de plus solides. Après Moch 5, vient le sixième mur comatique, le vert, celui de la beauté idéale. À ce moment, la course pour la mort s'atténue tandis que s'accentuent les luttes intestines entre les partisans de la thanatonautique et les contradicteurs, qui agressent Rose, que Michael décide de sauver en expérimentant la NDE. Il finit par la rejoindre dans la montagne de la pesée où il rencontre les juges (des archanges), elle revient mais Freedy est assassiné par un haschischins. 

Michael retourne au où il discute avec un ange, Saint Jérome, qui lui apprend qu'il est un grand initié, dont la tache est de déceler la vérité, d’aider les âmes à s’élever spirituellement et de leur donner les informations concernant l'organisation du paradis. Il en profite pour discuter avec d'autres anges, qui lui apprennent la vérité sur son existence: la mère de Raoul a assassiné son père, et Michael n'est qu'un fils adoptif. 
Arrive alors le temps du show biz, et du spectacle ; des conférences sont organisées et un reporter ectoplasmique est envoyé au-delà afin de faire une interview de la pesée d'une âme. Suit alors une époque de gentillesse généralisée, de peur d'être mal noté à la pesée des âmes, rendant la société apathique et fataliste. Le commerce lié à la thanatonautique prend une allure disproportionnée, avec des comportements tels que des gens se suicident pour un oui ou un non, et une vague de vols de corps (par la multiplication des âmes errantes). Pour lutter contre cela, Stefania met en place une brigade du mal afin de contrebalancer la mièvrerie ambiante. Elle détruit le laboratoire des Buttes Chaumont avec son équipe et tue Raoul Razorbak, qui une fois arrivé à la montagne du jugement, demande à être réincarné en un pied de vigne
Les anges décident finalement d'intervenir dans notre monde en provoquant par le biais d'un accident d'avion, la mort des pionniers thanatonautes, et d'effacer la mémoire du grand public. 

À la fin de l'histoire, Michael Pinson et ses amis, considérés comme grands initiés, sont toutefois condamnés à être jugés et à suivre le cours traditionnel des réincarnations.

5 ans : il découvre la mort avec M. Dupont son boucher et son arrière-grand-mère Aglaé, ne comprenant pas la gravité de celle-ci, ses parents lui donnent une gifle ; désormais Michael se forcera à pleurer. 
7 ans : il se fait heurter par une voiture et meurt quelques instants. Première expérience personnelle de la mort. 
8 ans : il s'intéresse à la mort des autres à travers les médias. À l'enterrement de son oncle Norbert, il fait la rencontre de Raoul, qui va devenir son meilleur ami. Très cultivé il lui fait découvrir la mythologie égyptienne, il lui donne le courage de faire fuir les adeptes d'une secte sataniste, explore le service gériatrie d'un hôpital afin d'enquêter sur la mort. Trop originaux pour leurs camarades ils subissent leurs invectives.
Tandis que les amis se séparent pour un long terme, la mère de Michael découvre son journal intime que son frère Conrad va montrer à son école ce qui lui vaut moqueries et railleries. 
18 ans : il commence des études de médecine, avec comme spécialités l’anesthésie et la réanimation.
32 ans : il retrouve son ami Raoul au cimetière du Père-Lachaise qui lui a fait des études de biologie, pratiquant des expériences sur l'hibernation des marmottes. 
Par la suite, après quelques réticences il rejoint l'équipe des thanatonautes et s'adonne à des expériences sur des détenus ; après quelques sacrifices, sa première réussite, Felix Kerboz lui permet d'accéder au succès. Amoureux d'Amandine, il finit par se marier à la scientifique Rose Solal, qu'il doit récupérer dans le royaume des morts à la suite de son assassinat, qui est aussi sa première expérience thanatonautique. Il est alors considéré au même titre que ses amis comme grand initié ; son ami lui apprend alors qu'il est orphelin et qu'une autre femme l'attend ailleurs. L'expérience thanatonautique finit alors par dégénérer, les anges décident d'y mettre fin en provoquant un accident de Boeing 747 et en effaçant la mémoire de tous à ce sujet. Ses amis et lui meurent dans l'accident. 

Enfant, Raoul rencontre Michael Pinson au cimetière du Père-Lachaise à l'enterrement de son oncle Norbert alors qu'il est lui-même en train de se recueillir sur la tombe de son défunt père Francis Razorbak. Très cultivé, il lui fait découvrir la mythologie égyptienne ainsi que les joies de la lecture. Ils affrontent ensemble une secte de satanistes fréquentant le cimetière et décident d'explorer la mort en enquêtant au service gériatrique d'un hôpital. Son originalité lui vaut d'être le mouton noir de ses camarades. Par la suite, les deux amis se séparent alors une vingtaine d'années avant de se retrouver. Raoul est alors chercheur en biologie et pratique des expériences sur l'hibernation des marmottes. À la suite de la tentative d'assassinat du président Lucinder, il est engagé par celui-ci afin de mener des expériences sur les EMI sur les détenus de la prison de Fleury-Merogis.
Il va sacrifier beaucoup de vies humaines avant de réussir, avec l'aide réticente de son ami, et de la belle infirmière. À la suite d'un show télévisuel mondial dont l'enjeu n'est rien d'autre que l'avenir de la thanatonautique, il se marie à Amandine, tandis qu'il s'est amouraché de Stefania. Il découvre aussi que c'est sa mère qui a provoqué le suicide de son père. Par la suite, il quitte Stefania qui va rejoindre le mouvement sataniste et finit poignardé par Martinez, un ex-détenu revanchard. Arrivé dans le cycle des réincarnations, il décide d'être une grappe de raisin. 
Pour une étrange raison, il se trouve qu'il est en fait directement promu ange gardien dans l'ouvrage suivant (?)

Elle rencontre les deux héros au pénitencier de Fleury-Merogis afin de pratiquer des expériences d'EMI sur les détenus. Elle ressemble à Grace Kelly, et aime le sexe, elle a une attirance pour les thanatonautes qui prennent des risques, notamment Felix Kerboz, Raoul Razorbak et Freddy Meyer avec lequel elle finit par se marier. Michael l'aime secrètement, ce qui n'est pas réciproque. Elle finit dans l'accident du Boeing 747. 

Scientifique travaillant sur le projet eden, elle rejoint l'équipe des thanatonautes à mi chemin, et met au point un récepteur d'ondes qui lui permet de repérer les ondes émises par les ectoplasmes des thanatonautes. Rose et Michael terminent ensemble ; ce dernier ira la récupérer au paradis, à la montagne du jugement, elle donne par la suite naissance à Freddy junior au départ considéré comme sa réincarnation (à tort). Elle finit dans l'accident du Boeing 747. 

Ancien chorégraphe qui est devenu juif et rabbin à la suite d'un accident de deltaplane qui l'a rendu aveugle, Freddy Meyer a le sens de l'humour et met en place la technique des cordons tressés, qui permettra à nos héros d'arriver au bout du royaume des morts. C'est aussi un courageux aventurier et un stratège hors pair dans la bataille du paradis. Marié à Amandine, Il finit par être tué par le vieux de la montagne, à la suite de la mission de sauvetage de Rose Solal.

Chef de la secte des haschischin, et principal rival de Freddy Meyer, il organise un conflit d'ectoplasmes au paradis, il tue Freddy à la suite d'une embuscade fixée lors d'une opération de sauvetage (Rose), et finit lui-même tué par Amandine qui lui coupe le cordon ombilical. 

Italienne de Montpellier, de confession bouddhiste, elle est la première à réussir à traverser le mur des plaisirs grâce à sa technique de méditation. De forte corpulence (qu'elle a su dépasser par son intelligence et la méditation) elle sait lire dans les âmes, et a une certaine connaissance du royaume des morts, qu'elle tire du Bardo Thodol. Elle s'amourache de Raoul Razorbak qui finit par quitter Amandine, puis finit elle-même par le quitter pour rejoindre un mouvement sataniste, considérant que le monde est devenu trop mièvre ; son attitude la conduira à assassiner son ex-compagnon. C'est la seule à avoir été épargnée de l'accident du Boeing, les anges considérant qu'elle s'est repentie. 

Frère adoptif de Michael, il est aussi le petit chouchou de sa maman, et passe son temps à taquiner son frère et réussit dans la vie. À la suite de l'essor de la thanatonautique, il s'occupe plutôt de l'aspect mercantile et spéculatif de l'affaire. 

Détenu de la prison de Fleury Merogis, plus malin et solide que les autres, il est le premier rescapé des expériences, et le premier thanatonaute en herbe, et se met en couple avec Amandine. Mais mal dans sa peau, et alcoolique chronique il décide de quitter ce monde en lui demandant de lui pardonner. 

C'est le père de Raoul, à la base de son goût pour l'univers des morts, à cause de son mystérieux suicide alors qu'il rédigeait « la mort, cette inconnue », dont on retrouve des articles régulièrement disséminés au travers de l'ouvrage un peu comme "L'Encyclopédie du savoir relatif et absolu" d'Edmond Wells. C'est en discutant avec Satan que Raoul découvre qu'en réalité, le suicide de son père a été provoqué par sa mère. 

Reporter du petit thanatonaute illustré, doté d'une mémoire phénoménale, il est connu notamment pour avoir retranscrit la bataille du paradis, et avoir fait une interview d'une pesée d'une âme (l’interview de l’ectoplasme Donahue) dans la montagne du jugement. Il est aussi considéré comme un traitre par les archanges-juges et termine dans l'accident du Boeing.

Président de la République française dans les années 2060, victime d'une tentative d'assassinat à l'âge de 58 ans alors qu'il faisait un bain de foule à Versailles. Cette tentative lui permet de faire l'expérience d'une EMI, qui le décide à s'aventurer dans le domaine du paranormal en faisant pratiquer des expériences au sein de la prison de Fleury-Merogis par Raoul Razorbak, Michael Pinson, et Amandine Ballus. Ce n'est pas un explorateur, seul son mandat l'intéresse, il utilise d'ailleurs l'argument du paradis pour être réélu (un mensonge bien évidemment). Il finit par se suicider, las de son existence. 

Ministre de la recherche, Lucinder vient prendre conseil auprès de lui après sa tentative d'assassinat afin de réaliser des expériences sur les détenus du pénitencier de Fleury-Merogis, il finit par démissionner.

Procédé récurrent chez Bernard Werber, le livre alterne les passages consacrés au fil conducteur de l'histoire et ceux composés de textes sacrés extraits des mythologies, religions et cosmogonies du monde entier, regroupés sous le nom d'« Extrait de la thèse "La mort cette inconnue" de Francis Razorbak ».

Le livre est le premier volume du cycle des anges. "L'Empire des anges" vient compléter le cycle. "Le Cycle des Dieux" clôt la trilogie et est lui-même publié en trois parties : "Nous les dieux", "Le Souffle des dieux" et "Le Mystère des dieux".

Le roman s'intéresse au même thème que le film américain "L'Expérience interdite", sorti quatre ans plus tôt.

Le roman est adapté en bande dessinée, dont trois tomes sont sortis : 



</doc>
<doc id="1863" url="https://fr.wikipedia.org/wiki?curid=1863" title="Libanios">
Libanios

Libanios (en grec ancien / et Libanius pour les Romains) est un rhéteur de culture grecque de l'antiquité tardive (314–393).

Libanios est né en 314 à Antioche de Syrie, métropole de l'Orient pendant l'Antiquité tardive. Il est issu d’une famille curiale plutôt appauvrie. Il perd son père vers l’âge de onze ans et devient orphelin. Sa mère et ses deux oncles Panolbios et Phasganiois veillent sur sa jeunesse studieuse.
À l'âge de quatorze ans, il décide de vouer sa vie à l'étude et à la pratique de la littérature et de l'art oratoire. Rejetant l'enseignement de Zenobios d'Elusa (auquel il devait succéder comme sophiste d'Antioche après 354) parce qu'il le trouve de piètre qualité, il suit un parcours d'études atypique en se formant par lui-même tout en continuant de travailler chez un bon grammairien qui pourrait être Didymus Chalcenterus, puis va étudier à Athènes entre 336 et 340.

En 340, Nicoklès, un grammatiste de Sparte, lui offre un poste de professeur (sophiste) à Constantinople, mais il manque l'occasion d'obtenir ce poste et doit s'installer à son compte. Professeur libre, il est entretenu uniquement par ses élèves (jusqu'au nombre de quatre-vingts). Mais grâce à sa renommée grandissante, l'empereur décide de le garder à Constantinople par une nomination à un titre surnuméraire. Néanmoins ses rivaux profitent des émeutes entre ariens et nicéens et de la répression de 342 pour le chasser de la ville. Après un bref passage par Nicée, Libanios se réfugie alors à Nicomédie, de l'autre côté des détroits, où il devient un personnage célèbre par son art de la rhétorique. C'est une période heureuse pour lui et très productive. C'est à cette même époque qu'il aurait pu avoir dans son auditoire Basile de Césarée et que le futur empereur Julien se fit passer clandestinement ses cours. Rappelé à Constantinople par l'empereur vers 347/348, il s'y déplait et finit par rentrer dans sa ville natale d'Antioche en 354, d'où il ne semble guère avoir bougé jusqu'à sa mort.

Peu après son retour, il prend une concubine d’origine servile avec laquelle, il aura un fils Arabios (rebaptisé Cimon). Il acquiert rapidement une grande réputation de rhéteur dans la ville. De plus, il développe de très bons contacts avec les dirigeants municipaux ainsi qu'avec les fonctionnaires de la cour de l'empereur . L'empereur suivant, Julien, pour préparer une expédition contre la Perse, installe un temps son palais à Antioche. Mais à cause de son paganisme affiché et de sa rigueur morale, il entre en conflit avec la population de la ville. Ce qui n'est pas pour déplaire à Libanios qui entretiendra avec ce dernier une relation amicale. Mais la mort de l'empereur, à la suite de la Bataille de Ctesiphon (363), aura la double conséquence d'affecter personnellement Libianos et d'éloigner pour toujours l'idée d'un retour à l'empire païen d'Auguste, Trajan et Marc Aurèle. C'est vers cette époque qu'il dut avoir pour élève le futur évêque Amphiloque d'Iconium ; les auteurs chrétiens ultérieurs lui ajoutent Jean Chrysostome vers cette époque.

L'époque qui suit la mort de Julien, est plus difficile pour Libanios. La tentative de Coup d'État mené par Procope contre le nouvel empereur Valens vers 365, à laquelle bon nombre de cités de Syrie se sont associées, et surtout la conspiration menée par Théodore d'Antioche alors que Valens venait d'y établir sa capitale dans le cadre d'opérations militaires (371/372) ont entrainé des représailles sévères à l'égard des cités d'orient et la persécution de beaucoup d'intellectuels païens. Même si en raison de l'influence qu'il conserva à la cour, il ne fut pas directement touché par les persécutions, cette affaire le marqua, bien que ses écrits ne manifestent pas d'hostilité particulière à l'égard de cet empereur.

Après la catastrophe de la bataille d'Andrinople et la mort de Valens en 378, Libanios put de nouveau obtenir les faveurs de la cour de . Il interpelle ce dernier en faveur des sanctuaires paiens et pour dénoncer divers abus des puissants. Vers 383/384, il reçoit le titre de questeur honoraire. La date de sa mort reste l’objet de discussion ; sans doute entre 393 et 394.
Bien qu'il fût païen et grand admirateur et ami de l'empereur Julien, les auteurs chrétiens du siècle suivant (Socrate de Constantinople, Sozomène) lui ont attribué pour élèves Jean Chrysostome, Basile le Grand, Grégoire de Naziance et Grégoire de Nysse.

Libanios exerce le métier de professeur, dispensateur de la "Paideia" et de la tradition culturelle grecque classique ; seule culture noble à ses yeux. En particulier face à la perte d'importance de cette tradition dans la romanité, surtout dans l'empire occidental où la latinité s'affirme tant avec le christianisme et l'église.
" La Grèce vaincue, a conquis à son tour, son sauvage vainqueur et a apporté la civilisation au barbare latium.

Pour Libanios l'éloquence rhétorique n'est pas qu'une profession où il veut exceller, c'est un art de vivre, un élément fondamental de l'homme bien fait. En cela, il s'inscrit dans la tradition isocratique, cette tradition pédagogique de la rhétorique où " "L'art oratoire apprend à bien penser, à bien agir en même temps qu'à bien écrire." - Isocrate.". De la même manière, on peut aussi y trouver les racines de sa pensée réactionnaire et de son "nationalisme" hellénique:
"Nous appelons grecs ceux qui ont en commun avec nous la culture, plutôt que ceux qui ont le même sang.

Conscient de l'évolution de son siècle, il combat tous les adversaires à ses yeux de la culture grecque et de ses traditions païennes comme les empereurs Constantin et surtout , auteur d'une politique de répression contre le paganisme. Et soutient les hommes favorables à la réaction païenne tel l'empereur Julien

Il combat aussi l'évolution centralisatrice du pouvoir et l'interventionnisme croissant des empereurs dans la cité en ce , qui s'opposent à l'idéal libéral de la civilisation hellénique.

Il est l'auteur d'une œuvre immense, qui fit l'admiration de ses contemporains et qui servit de modèle pendant toute l'histoire de Byzance. Sa notoriété fut grande aussi en Europe pendant la Renaissance. Après une période de mise en réserve ou de défaveur, il fait de plus en plus parler de lui à la fin du et au début du .

Son œuvre est l’une des plus importantes que l’Antiquité nous ait transmises. Cela représente 11 volumes dans l'édition de Richard Forster :





</doc>
<doc id="1865" url="https://fr.wikipedia.org/wiki?curid=1865" title="Tirage photographique noir et blanc">
Tirage photographique noir et blanc

Le tirage photographique est une étape dans le processus de restitution d'une image présente sur une pellicule. Le but de cette étape est de transférer l'image (à partir d'une pellicule développée) sur du papier et de l'agrandir. Le tirage contact est une autre forme de tirage qui ne permet pas d'agrandissement.

Le tirage est assuré soit par le photographe lui-même, soit par un tireur.

Sur le plan professionnel, le tirage est assuré par un tireur dont le plus célèbre reste Pierre Gassmann, fondateur de Picto, tireur d'Henri Cartier-Bresson, qui a formé Georges Fèvre, François Duffort, Voja Mitrovic, etc. ; Claudine Sudre (Labo Nicole), etc. Aujourd'hui : Thomas Consani (Central-Dupon), tireur attitré de Marc Riboud , Diamantino Quintas (Diamantino Labo photo), Payram (Picto Bastille), Stéphane Cormier, etc.

Cet article développe particulièrement le processus de tirage dans le cas d'une pellicule noir et blanc (N&B).
On doit distinguer les tirages par contact des tirages par agrandisseur ; les tirages petits et moyens formats des tirages grands formats ; les tirages noir et blanc des tirages couleurs ; les tirages argentiques des tirages numériques ; etc.

Les étapes de l'exposition au fixage se déroulent en chambre noire sous lumière inactinique. C'est seulement après l'étape du fixage que le tirage peut être exposé à la lumière.

C'est l'étape où l'image présente sur le film est projetée sur le papier. C'est aussi l'étape où tout le sens artistique du photographe s'exprime : choix du cadrage, des masquages, des papiers, etc. Il ne faut surtout pas la réduire à une simple opération technique.

Pour qu'une exposition soit réussie, il faut considérer (entre autres) plusieurs facteurs :

Le révélateur est une solution liquide qui révèle l'image du papier photosensible. Pour de bons résultats reproductibles, la durée de cette opération doit être fixe (60 à généralement pour les papiers dits à « contraste variable »). En fonction du rendu général désiré, on fera varier l'exposition en jouant sur le temps d'exposition et sur le diaphragme de l'objectif de l'agrandisseur ainsi que sur le grade du papier, mais en aucun cas le temps de développement.

Un temps trop court dans le révélateur entraînera une image grisaillante et sans force (si l'image « monte » trop vite, elle est surexposée : on ne la sauvera pas en la sortant plus tôt du bain sauf si le filtre était suffisant). Avec une exposition correcte, une prolongation du temps est moins dévastatrice : une révélation trop longue aura tendance à rendre les blancs gris et ainsi gâcher le travail des grades, en perdant le contraste.

On peut aussi jouer sur la température du révélateur pour obtenir des images plus fortes. On peut aller jusqu'à sans problème.

Enfin, il ne faut pas trop se fier à ce qu'on voit sous la lampe de laboratoire. Souvent le rendu y est plus flatteur qu'en réalité. Ceci est particulièrement vrai avec une lampe rouge qui ne permet même pas de juger les contrastes.

Selon le couple papier/révélateur, la tonalité de l'image peut varier des tons froids (bleutés) à chauds (tirant sur le brun). Les papiers "bromures" sont des papiers neutres. Les "chlorobromures" sont plus chauds (développés dans un révélateur approprié).

Sortie du révélateur, l'image passe dans un bain d'arrêt acide (eau + acide acétique ou vinaigre). Il s'agit simplement d'arrêter la réaction de développement et d'économiser le bain de fixage. Le bain d'arrêt est court et dure généralement quelques dizaines de secondes.

Cette étape permet d'éliminer les grains d'argent encore sensibles (non développés) qui seraient toujours présents sur le papier.

Le fixage dans un fixateur « rapide » (hyposulfite d'ammonium) dure entre 30 et . L'usage d'un fixateur rapide présente l'avantage, lors de l'utilisation du papier baryté surtout, de réduire l'imprégnation de fixateur dans le support papier. Des résidus de fixateur entraîneraient une moins bonne conservation dans le temps (jaunissement de la photo).

Certains tireurs, préfèrent cependant l'utilisation de fixateurs traditionnels à l'hyposulfite de sodium, nécessitant un temps de fixage plus long (entre 3 et 5 min) mais qui apporteraient une meilleure conservation dans le temps.

Il est important que les feuilles ne se collent pas les unes aux autres et que le produit circule bien entre les images. Des photos qui jaunissent sont souvent le signe d'un mauvais fixage (toutefois ce jaunissement n'apparaît pas immédiatement mais plusieurs jours ou semaines après le traitement).

En cours de fixage, il est possible d'éclaircir certaines parties de l'image, pour renforcer "très localement" les hautes lumières. On utilise pour cela le "faiblisseur de Farmer" appliqué très rapidement au pinceau, et rincé immédiatement. Cette technique ne doit être appliquée qu'aux hautes lumières, pas dans les ombres. Elle donne de très bons résultats mais nécessite une bonne dextérité sous peine d'effacer totalement la partie travaillée.

Il est possible de modifier le rendu de l'image, tout en améliorant sa conservation, par le procédé de "virage" au cours duquel on substitue aux sels d'argent, des sels de métaux différents comme le sélénium, l'or voire le platine. Le virage le plus courant étant le virage au "monosulfure" qui donne à l'image un ton sépia. Ce produit (monosulfure de sodium) a le défaut de sentir l'œuf pourri.

Il est possible par virage d'obtenir des images de toutes les tonalités. On peut aussi choisir de ne virer qu'une partie de l'image, en protégeant les autres par un vernis.

Le lavage est important et doit être soigneux (surtout s'il s'agit de papiers barytés). Il peut se faire en cuvette (lavage par vidange) ou en cuve verticale (lavage continu).
Les papiers dits à contraste variable (ou encore résinés, plastifiés ou encore RC pour "" en anglais) sont lavés en en eau courante à plus de et en en eau courante à moins de .
Les papiers barytés nécessitent un temps de lavage plus long. Plus d'une heure pour un lavage sans auxiliaire. L'emploi d'un auxiliaire réduit le temps de lavage par 2.
Un lavage insuffisant provoquera une dégradation lente et progressive de la photo.

Avant d'être séchés, les tirages doivent être essorés. La méthode la plus simple est celle d'étendre le tirage sur une plaque de verre ou de "Plexiglas" et de passer sur ses deux faces une raclette, un rouleau-essoreur, un gant ou autre.

En fonction du papier utilisé, les techniques de séchage diffèrent. La méthode la plus rapide est l'utilisation d'une sécheuse électrique, mais ce type d'équipement est assez onéreux.

La repique est l'action qui consiste à corriger les défauts mineurs d'un inversible, d'un négatif ou d'un tirage : poussières, fils… 

Cette opération s'effectue à l'aide d'un pinceau très fin chargé d'une encre adaptée (appelée gris film), à l'aide de gouache ou encore, sur les papiers barytés, au crayon noir.

Cette opération, surtout réalisée par les professionnels, tendent à se populariser sur support numérique grâce à des logiciels d'édition et de retouche d'image (tels que GIMP et Photoshop). 

La retouche traditionnelles tendent à disparaître mais existent toujours dans des domaines tels que la restauration d'images photographiques.
Le tirage platine-palladium implique un autre procédé physico-chimique que l'argentique et une autre technique car on ne peut effectuer que des tirages par contact et, pour obtenir des agrandissements, il faut passer par un négatif intermédiaire.




</doc>
<doc id="1868" url="https://fr.wikipedia.org/wiki?curid=1868" title="Virage (photographie)">
Virage (photographie)

Le virage est un traitement chimique supplémentaire intervenant lors du développement d'un tirage photographique noir et blanc sur papier, dans le but de donner une couleur dominante à l'épreuve.

Le procédé consiste à remplacer partiellement l'argent de l'image par un composé de préférence métallique (plus stable).

On distingue les « virages directs » des « virages indirects » : les virages indirects nécessitent un premier bain de blanchiment, avant le virage proprement dit.

Aujourd'hui, avec les techniques de retouche numérique, l'effet de « virage » est à la portée de tout un chacun, dans les menus « Teinte » ou « Effets » des logiciels de traitement d'images, parfois même directement dans les réglages de l'appareil au moment de la prise de vue (cas du sépia).

Dans les formules indiquées, l'abréviation « q.s.p. » signifie « quantité suffisante pour ». En pratique, vu les mélanges utilisés, commencer avec les ¾ de la quantité de solvant (ici toujours de l'eau). Une fois la dissolution complète, ajouter de l'eau pour obtenir le volume de liquide indiqué.

Ce type de virage permet de retrouver la tonalité des épreuves sépia de la première moitié du .

Il existe des papiers permettant un tirage sépia direct (Ilford XP2 et Kodak Select 1).

En partant d'un tirage argentique noir et blanc, ce procédé nécessite deux solutions successives vendues dans le commerce. Il est possible de les préparer soi-même.

Ce procédé, relativement onéreux (utilisation du chlorure d'or), donne une teinte du noir pourpre au rouge sang aux parties de l'image.
Originellement, ce procédé visait également à améliorer la conservation des tirages.

Le choix des proportions des solutions A et B mélangées dans le bain de virage permet d'obtenir la coloration désirée.

Laver soigneusement à l'eau puis fixer (fixateur classique) et relaver.

Ce virage en un seul bain permet d'obtenir des tonalités bleues, du bleu foncé au bleu-vert en fonction de la durée du traitement et du type de papier.

Il faut toujours "ajouter l'acide dans l'eau" (jamais l'inverse : risque de projections) puis dissoudre le citrate de fer dans la solution obtenue.

Mélanger les solutions A et B en parties égales au moment de l'emploi (ce bain ne se conserve pas).

L'image est blanchie avec un bain de blanchiment auquel est ajouté du chlorure cuivrique qui formera du chlorure d'argent durant le blanchiment.
On lave l'image puis on l'expose à la lumière du jour. Elle est ensuite réduite par une des solutions ci-dessous suivant la coloration désirée.

Ce virage donne des tonalités du brun chaud au rouge clair selon la durée du traitement.
Remarques : 

Le virage au sélénium renforce le contraste et donne une tonalité violacée selon la durée du traitement et le type de papier. La tonalité peut aussi être brunâtre voire jaunâtre, ce qui donne aux photographies un côté archiviste et vieillot (pour ce genre de tonalité, la dilution doit être assez forte et le papier doit être du papier fibre).

Le procédé est assez simple. C'est le même procédé que pour le développement d'une photographie noir et blanc, c'est-à-dire à pour le révélateur, ensuite le virage et le temps varient selon l'effet voulu ainsi que la dilution du produit, puis pour le bain d'arrêt, dans le fixateur ensuite, un deuxième fixateur peut être employé si le type de papier est du papier fibre.

Note : le temps de tous les produits utilisés peut varier selon la dilution des produits et parfois même selon la température de l'eau.

Attention, les produits contenant du sélénium sont toxiques. Les gants sont de mise pour éviter, entre autres, une coloration brune des mains. Les vapeurs peuvent causer des nausées, des étourdissements ainsi que des maux de tête. Voir aussi Sélénium (maladie professionnelle).




</doc>
<doc id="1872" url="https://fr.wikipedia.org/wiki?curid=1872" title="Langage de la cryptologie">
Langage de la cryptologie

Comme toute science, la cryptologie possède son propre langage. Étant donnée la relative jeunesse de cette science, et le fait qu'une très grande partie des publications dans ce domaine sont en langue anglaise, le problème de la terminologie francophone se pose, parfois par manque de traduction, parfois par manque de traduction univoque ou élégante.

Heureusement, certains termes, de par l'utilisation systématique dont ils font l'objet, ne souffrent pas de discussion. Ainsi, on parle de "chiffrement" et non de cryptage. Ici, la raison est simple : "décryptage" a une signification et elle est totalement différente de "déchiffrement". Le "déchiffrement" est une opération effectuée par le destinataire légitime, pour retrouver le message qu'on lui a envoyé — que l'on appelle le "texte clair", le message chiffré étant parfois appelé "cryptogramme". Cette opération nécessite une donnée secrète que l'on appelle la "clé". Le "décryptage" consiste également à essayer de retrouver le texte clair, mais "sans connaître cette clé" : c'est donc l'opération d'un adversaire cherchant à prendre connaissance de la communication sans y avoir droit.

Mis à part les problèmes de terminologie, il y a des définitions qui peuvent sembler plus ou moins précises. Par exemple, on parle de l'algorithme de chiffrement RSA mais aussi du protocole d'échange de clés Diffie-Hellman ou encore de schéma de signature. La différence entre un "algorithme" et un "protocole" est une question d'interactivité : pour un algorithme, une seule personne est impliquée, celle qui fait les calculs ; pour un protocole, plusieurs entités interviennent, il y a échange d'informations. La notion de "schéma" est quant à elle moins précise, ce terme est généralement utilisé lorsque l'on souhaite mettre en évidence le fait que l'algorithme ou le protocole repose sur un algorithme plus élémentaire. Autrement dit on essaie de faire ressortir le caractère composé.

Le folklore des cryptologues veut que l'on utilise fréquemment les mêmes prénoms dans les explications des protocoles ou des attaques. Généralement, "Alice" et "Bob" (on trouve également "Bernard" en français) sont les deux personnes cherchant à communiquer de manière sûre (quand "Carole" ne se joint pas à eux) alors que "Ève" est l'espionne. Voir l'article Alice Oscar Bob Eve.

Pour finir, on se trouve parfois face à des définitions portant plusieurs noms mais recouvrant le même concept. Ainsi les preuves sans divulgation sont aussi désignées par les expressions "à apport de connaissance nulle" ou "sans apport de connaissance", la cryptographie à clé publique est également appelée "asymétrique", de par l'asymétrie existant entre la possibilité de chiffrer — accessible à tout le monde — et de déchiffrer — restreinte au seul destinataire. Sur le même principe, la cryptographie à clé secrète est dite symétrique. 


</doc>
<doc id="1873" url="https://fr.wikipedia.org/wiki?curid=1873" title="Lutte intégrée">
Lutte intégrée

Selon la FAO et l'OILB (Organisation Internationale de Lutte Biologique), la lutte intégrée ou protection intégrée est définie comme étant la « conception de la protection des cultures dont l'application fait intervenir un ensemble de méthodes satisfaisant les exigences à la fois écologiques, économiques et toxicologiques en réservant la priorité à la mise en œuvre délibérée des éléments naturels de limitation et en respectant les seuils de tolérance ».

En Europe, la lutte intégrée est définie par la directive communautaire 91/414/CEE du , comme suit :

« L'application rationnelle d'une combinaison de mesures biologiques, biotechnologiques, chimiques, physiques, culturales ou intéressant la sélection des végétaux dans laquelle l'emploi de produits chimiques phytopharmaceutiques est limité au strict nécessaire pour maintenir la présence des organismes nuisibles en dessous de seuil à partir duquel apparaissent des dommages ou une perte économiquement inacceptables. »

Le biocontrôle est un ensemble d’outils utilisables pour la protection intégrée. Il met en œuvre les mécanismes régissant les interactions entre les espèces dans le milieu naturel tels que : 

La définition de la lutte biologique par l’OILB est la suivante : "L’utilisation d’organismes vivants ou de leurs produits pour prévenir ou réduire les dégâts causés par les ravageurs aux productions végétales."

Le biocontrôle et la lutte biologique sont parfois utilisés comme des synonymes car les deux font appel à des auxiliaires naturels pour combattre un bioagresseur. Cependant contrairement au biocontrôle, la lutte biologique n’inclut pas l’utilisation des phéromones de synthèse ou de substances naturelles d’origine minérale et, de son côté, le biocontrôle n’intègre pas les vertébrés considérés comme un outil pour la lutte biologique.

La protection biologique intégrée résulte de la combinaison de la lutte biologique et de la protection intégrée. La protection biologique intégrée est donc une démarche de protection combinant toutes les techniques disponibles issues de méthodes de contrôle, si possible biologique. Elle comprend : la prophylaxie, l'observation et suivi des cultures, la lutte biologique, le biocontrôle, la lutte mécanique, l'aménagement de l’environnement, les pratiques culturales, les choix variétaux, les traitements chimiques compatibles…

L'Institut national de la recherche agronomique a mené pendant 10 ans un programme d’essai de systèmes de culture en protection intégrée, qui a montré que la flore adventice est maîtrisable par les techniques alternatives aux herbicides. L’expérimentation systémique de longue durée de Dijon Epoisses a été initiée en 2000. Les résultats obtenus au cours des 10 années d’essais indiquent que les leviers testés permettent de maîtriser de façon satisfaisante les infestations tout en réduisant de façon importante la dépendance aux herbicides et les impacts environnementaux associés.



</doc>
<doc id="1874" url="https://fr.wikipedia.org/wiki?curid=1874" title="Langue isolante">
Langue isolante

Une langue isolante ou analytique est en typologie morphologique une langue qui est extrêmement analytique, tous les mots restant invariables quelle que soit leur fonction syntaxique.

Les langues isolantes sont traditionnellement opposées aux langues agglutinantes, aux langues synthétiques (ou langues fusionnelles et aussi anciennement appelées langues flexionnelles) et surtout aux langues polysynthétiques.

Les langues isolantes sont fréquentes en Asie comme le chinois et le vietnamien.


</doc>
<doc id="1875" url="https://fr.wikipedia.org/wiki?curid=1875" title="Frise chronologique">
Frise chronologique

Une frise chronologique ou ligne du temps est une représentation linéaire d'évènements positionnés sur la flèche du temps ; elle associe des événements à leurs positions dans le temps le long d'une échelle graduée, ce en quoi elle se rapproche d'une chronologie.

On parle, par exemple, de la ligne du temps d'une civilisation ou de l'histoire des techniques pour représenter ses grands événements de part et d'autre d'une flèche qui part des temps les plus reculés et qui va vers le futur.




Un calendrier de l'évolution compte en millions d'années, alors que l'émission de particules dans un collisionneur est presque instantanée calendrier logarithmique utilisent une échelle logarithmique pour représenter le temps.

Il existe de nombreuses méthodes de visualisation pour les délais. 
Historiquement, elles ont d'abord été des représentations statiques, et généralement dessinées ou imprimées sur papier, plus ou moins bien mises en valeur par le design et la capacité de l'artiste ou illustrateur à rendre visible des données. La carte de Minard (1861) de l 'invasion de la Russie par Napoléon est un exemple d'un calendrier non standard qui utilise également la géographie dans le cadre de la visualisation.

Grâce à l'informatique, les échelles ne sont plus limitées par l'espace ni par d'autres limitations fonctionnelles antérieures. Les échelles de temps sont devenues numériques et interactives, généralement créées avec des logiciels pouvant intégrer des images, films, liens et de l'hypertexte.

Frise avec les articles de Wikipedia

Pour construire, coconstruire ou animer des flèches du temps, il existe maintenant de nombreux logiciels :




</doc>
<doc id="1878" url="https://fr.wikipedia.org/wiki?curid=1878" title="Le Droit à la paresse">
Le Droit à la paresse

Le Droit à la paresse, ouvrage de Paul Lafargue, paru en 1880 et puis en 1883 en nouvelle édition, est un manifeste social qui centre son propos sur la « valeur travail » et l'idée que les humains s'en font.

Texte classique, très riche historiquement — il propose une monographie sociale, économique et intellectuelle et analyse les structures mentales collectives du —, "Le Droit à la paresse" démythifie le travail et son statut de valeur. 

Dans l'introduction de son ouvrage, Paul Lafargue cite Adolphe Thiers : 
Pour lui, ce sont donc « les prêtres, les économistes, les moralistes » qui sont à l'origine de cet amour absurde du travail.

Dans ce premier chapitre, Lafargue s'étonne de « l'étrange folie » qu'est l'amour que la classe ouvrière porte au travail alors qu'il décrit celui-ci comme « la cause de toute dégénérescence intellectuelle, de toute déformation organique ».

Pourtant cet amour n'est pas universel : les sociétés primitives « que les missionnaires du commerce et les commerçants de la religion n'ont pas encore corrompues avec le christianisme, la syphilis et le dogme du travail » y échappent ainsi que les civilisations antiques dans lesquelles les philosophes considéraient le travail comme une « dégradation de l'homme libre ».

Dans ce chapitre, Lafargue s'attache à décrire les conditions de travail particulièrement difficiles de la classe ouvrière dans l'Europe capitaliste du . Il dénonce l'influence néfaste du progrès technique qui pourrait être bénéfique. Le machinisme selon lui devrait faire aboutir à une diminution du temps de travail, jusqu'à même des journées de travail de trois heures. Lafargue dénonce le fait qu'on fasse travailler plus de douze heures par jour de jeunes enfants en plus des femmes et des hommes. Il juge que le travail, dans les sociétés dites primitives ou en France sous l'Ancien Régime, étaient mieux organisées car on y prévoyait des jours fériés, chômés, bien plus que dans la société industrielle. Lafargue observe que les travailleurs s'appauvrissent alors qu'ils travaillent de plus en plus. Il juge que la Révolution de 1789 avec ses idéaux bourgeois de droits de l'Homme n'a pas arrangé grand chose, les bourgeois chrétiens, les propriétaires, se montrant par la suite propices à montrer leur charité chrétienne, mais ne défendant aucunement les "Droits de la paresse", primordiaux selon Lafargue. Lafargue pense que les esclaves et les forçats travaillaient moins d'heures par jour que les ouvriers.

Dans ce contexte de révolution industrielle et de progrès technique, la machine, au lieu de libérer l'humain du travail le plus pénible, entre en concurrence avec lui : 
Il en résulte une augmentation du temps de travail par la suppression des jours fériés et l'allongement des journées de travail, ce qui provoque une augmentation de la production.

Lafargue explique ironiquement que les bourgeois sont alors « contraints » d'arrêter de travailler et de surconsommer. Ils soustraient pour ce faire une quantité d'Hommes au travail productif pour les employer à leur service. La bourgeoisie « s'accommode » de ce mode de vie et ne peut plus envisager un retour en arrière. C'est alors que les prolétaires avec des mots d'ordres comme « Qui ne travaille pas, ne mange pas » se mirent en devoir d'imposer le travail à cette bourgeoisie oisive. Afin de mater ces soulèvements, les capitalistes « s'entourent de prétoriens, de policiers, de magistrats, de geôliers entretenus dans une improductivité laborieuse ».

Cette masse d'humains soustraits au travail productif ne suffit pas à écouler la surproduction, les capitalistes doivent donc chercher de nouveaux débouchés dans les colonies, diminuer la qualité des produits afin d'accélérer leur renouvellement (« Dans nos départements lainiers, on <nowiki>[...]</nowiki> fait des draps dits de renaissance, qui durent ce que durent les promesses électorales ») et créer de nouveaux besoins factices. Ces mesures ne suffisant toujours pas à écouler toute la surproduction, le recours au chômage est inévitable. 

Il convient donc de réduire le temps de travail et d'augmenter les salaires car c'est lorsqu'ils sont élevés que, pour les économiser, le capitaliste est contraint de développer le travail mécanique.

Pour sortir de la crise, il faut forcer les ouvriers à consommer leurs produits.

À la suite de cet afflux d'improductifs sur le marché du travail, celui-ci deviendra « débordant » et la seule solution serait de réduire drastiquement le temps de travail. Paul Lafargue propose trois heures par jour. Les hommes pourraient alors se consacrer aux loisirs.



À l'occasion du centenaire de la mort de Laura Marx et Paul Lafargue, à l'Université libre de Bruxelles, le 23 novembre 2011 : « Le droit à la paresse, nécessaire, urgent ?! ». Publication des actes de ce colloque en PDF





</doc>
<doc id="1879" url="https://fr.wikipedia.org/wiki?curid=1879" title="L'Abolition du travail">
L'Abolition du travail

L'Abolition du travail (ou "Travailler, moi ? Jamais !") de Bob Black, écrit en 1985, est un livre se présentant comme une manifeste pour une révolution ludique, ainsi qu'un pamphlet contre le travail, la misère et les nuisances du salariat.

Bob Black analyse et décrypte le temps travaillé comme un temps de servitude, de résignation qui tue le temps du plaisir et de la connaissance.





</doc>
<doc id="1881" url="https://fr.wikipedia.org/wiki?curid=1881" title="Groupe des Six (musique)">
Groupe des Six (musique)

Le groupe des Six, aussi nommé Les Six, est un groupe de compositeurs français réunissant entre 1916 et 1923 :
Georges Auric (1899-1983), Louis Durey (1888-1979), Arthur Honegger (1892-1955), Darius Milhaud (1892-1974), Francis Poulenc (1899-1963), et Germaine Tailleferre (1892-1983). Leur musique réagissait essentiellement contre l'impressionnisme et le wagnérisme. Ils étaient très influencés par les idées d'Erik Satie et de Jean Cocteau. Bien qu'ils aient écrit collectivement, chacun a conservé son style personnel de par la nature même des œuvres (mouvements ou morceaux séparés).

Le groupe se constitue, selon Jean Cocteau, vers 1916. Tout juste sortis du conservatoire, ils prennent l'habitude pendant la guerre de se réunir le samedi, le plus souvent chez Darius Milhaud, autour de Jean Cocteau. Après la guerre, les réunions continuent avec le renfort de quelques autres artistes, les pianistes Marcelle Meyer et Juliette Meerovitch, le chanteur russe Koubitsky, les peintres Émile Lejeune, Marie Laurencin, Irène Lagut et Valentine Gross, les écrivains Lucien Daudet et Raymond Radiguet.

Leur nom leur est donné par le critique et compositeur Henri Collet qui s'était invité, en référence au groupe des Cinq, dans deux articles parus dans la revue "Comœdia" les 16 et 23 janvier 1920 intitulés « Un ouvrage de Rimsky et un ouvrage de Cocteau : Les Cinq russes, les Six français et Erik Satie ». Les musiciens se produisent régulièrement ensemble jusqu'en Belgique.

En 1921, une certaine notoriété finit par peser sur le groupe réuni désormais au Bœuf sur le toit et Louis Durey s'en sépare.

La mort de Raymond Radiguet met fin aux réunions des « samedistes » en 1923.

Le groupe des Six a créé seulement deux œuvres collectives : un recueil pour le piano "Album des Six", et un ballet, "Les Mariés de la tour Eiffel".

Musiques composées par "Les Six" :

Quelques autres compositions des Six :



</doc>
<doc id="1885" url="https://fr.wikipedia.org/wiki?curid=1885" title="La Flamme et la Cendre">
La Flamme et la Cendre

La Flamme et la Cendre est un livre de Dominique Strauss-Kahn, publié en 2002 aux éditions Grasset. Ce livre retrace sa vision des racines du socialisme et son envie de le développer de manière moderne.


</doc>
<doc id="1886" url="https://fr.wikipedia.org/wiki?curid=1886" title="Lab">
Lab


LAB peut faire référence à :

</doc>
<doc id="1889" url="https://fr.wikipedia.org/wiki?curid=1889" title="Louis Malle">
Louis Malle

Louis Malle est un cinéaste français, né le à Thumeries (Nord) et mort le à Beverly Hills.

Né à Thumeries dans le Nord en 1932 au milieu d'une fratrie de trois frères et deux sœurs, Louis Malle est issu d'une grande famille d'industriels du sucre : son père, Pierre Malle (1897-1990), ancien officier de marine, est l'époux de la sœur de Ferdinand Béghin, Françoise (1900-1982). Les deux hommes sont directeurs de l'usine Béghin-Say de Thumeries.

Il grandit dans le milieu de la grande bourgeoisie et traverse l'Occupation dans différents internats catholiques dont celui qu'il évoque plus tard dans "Au revoir les enfants". Dès l'âge de 14 ans, il s'initie à la réalisation avec la caméra de son père. 

Il est élève à l'IEP de Paris de 1950 à 1952, mais c'est à ce moment que germe sa carrière de cinéaste. Il est reçu au concours de l'IDHEC en 1953.

Jacques-Yves Cousteau recherche alors un jeune assistant pour réaliser avec lui un documentaire sur les fonds marins. Parmi les jeunes étudiants que la direction de l'IDHEC lui propose, il choisit Malle. Plusieurs mois de travail sur la Calypso aboutissent au "Monde du Silence" (1955), récompensé par la Palme d'or à Cannes (premier film documentaire à en être lauréat et encore aujourd'hui le seul avec "Fahrenheit 9/11" de Michael Moore).

Sur le tournage, Louis Malle se crève les tympans lors d'une plongée et ne peut plus à l'avenir réaliser des travaux de ce type. Les projets qui suivent, films et documentaires, sont moins consensuels et volontiers provocateurs, optant pour des sujets plus critiques ou polémiques.

Il travaille par la suite avec Robert Bresson à la préparation d"'Un condamné à mort s'est échappé" et assiste à une partie du tournage. Il est profondément marqué par le travail de Bresson avec les « non-acteurs ».

C'est alors l'essor de la Nouvelle Vague mais Malle n'est jamais reconnu par ce mouvement même si son cinéma des débuts partage avec elle plusieurs caractéristiques. Malle suit son chemin seul, guidé par ses propres motivations.

Il réalise son premier long métrage de fiction à 25 ans, "Ascenseur pour l'échafaud" (1957), histoire d'assassinat avec Jeanne Moreau et Maurice Ronet qui joue sur les codes du film noir et remet en cause la dramaturgie du cinéma classique. La bande originale est réalisée par Miles Davis. Elle montre l'intérêt de Malle pour le jazz. Le film remporte le Prix Louis-Delluc en 1957.

Dans "Les Amants", une nouvelle fois interprétée par Jeanne Moreau, qui s'inspire lointainement de "Point de lendemain" de Vivant Denon, il s'attaque à l'hypocrisie de la société bourgeoise à travers le récit d'une relation adultère. Suivent l'adaptation légère, ludique et enthousiaste d'un roman de Raymond Queneau, "Zazie dans le métro" (1960), et, sur la suggestion de Roger Nimier, celle d'un récit de Pierre Drieu la Rochelle, "Le Feu follet" (1963), qui traite de la dépression et du suicide.

"Le Voleur" porte un regard cynique sur la bourgeoisie et les élites politiques, qui restent les cibles favorites de Louis Malle. "Le Voleur" du titre personnifie l'homme libre, extérieur à ce système empli de préjugés et sournois. Une acerbe critique sociale sourd dans la peinture psychologique des personnages.

Malle tourne par ailleurs plusieurs documentaires dont "Calcutta, l'Inde fantôme" en 1969.

De retour des Indes, Malle tourne un film vaguement inspiré de "Ma mère" de Georges Bataille, qui provoque un tollé : "Le Souffle au cœur". Il y évoque la relation incestueuse et romantique entre une mère et son fils. Ce thème est traité sans aucun jugement moral, ce qui sera une constante chez le réalisateur pour qui la vie s'apparente à une série de situations complexes. Il n'y a ni innocents ni coupables ou représentants du bien d'un côté et du mal de l'autre. Pour Malle, le spectateur doit être capable de se faire une opinion, sans condamner d'avance.

Trois ans plus tard, en 1974, "Lacombe Lucien" provoque une autre controverse. Le film décrit le progressif engagement d'un jeune homme désœuvré dans la collaboration après une tentative avortée d'entrer dans la Résistance. Là encore, Malle ne porte aucun jugement, et montre un individu dont l'engagement est essentiellement dû au hasard des circonstances. Même si une partie de la critique salue le film comme un chef-d'œuvre, une autre reproche au réalisateur de ne pas avoir vécu assez durement la guerre et juge son travail comme un affront à la mémoire des Résistants.

Cette polémique décide Malle à s'expatrier aux États-Unis. Il y tourne notamment à La Nouvelle-Orléans un drame en costumes sur la prostitution enfantine, "La Petite", avec la jeune Brooke Shields, puis part pour Hollywood réaliser "Atlantic City" (1980), avec Burt Lancaster, Susan Sarandon et Michel Piccoli, qui raconte les mésaventures d'un truand à la retraite et de sa voisine dans la ville des casinos de la Côte Est.

Lorsqu'il revient en France en 1987, c'est pour s'attacher au thème qui l'avait fait partir : l'Occupation. C'est alors la consécration de sa carrière avec "Au revoir les enfants". Dans un collège catholique, un garçon issu de la bourgeoisie découvre qu'un de ses camarades est juif. Une amitié, qui se construit entre les deux adolescents, ne peut empêcher une fin tragique.

Dans ce film, Louis Malle montre ce dont il se souvient de la guerre. L'histoire est en partie autobiographique, il a été témoin d'une situation similaire lors de son enfance, un jeune Juif avait été caché dans son internat puis découvert par la Gestapo et déporté. Il dira d'ailleurs que ce thème le hantait depuis toujours et que c'est cette histoire tragique qui l'avait amené au cinéma.

Le film reprend aussi certains éléments de ses précédents films polémiques : de "Lacombe Lucien" il reprend le collabo "malgré lui", du "Souffle au cœur" il reprend la relation fusionnelle entre la mère et le fils. Là encore il ne juge personne, il n'y a ni bons ni méchants mais une certaine fatalité. Cette œuvre, marquée par la fluidité de son récit et la sobriété de sa mise en scène, est considérée comme la plus émouvante et la plus personnelle de sa carrière. Elle reçoit un triomphe critique et public et obtient plusieurs récompenses en 1987 et 1988 : le Lion d'or à Venise, le Prix Louis-Delluc et sept Césars dont ceux du meilleur film et du meilleur réalisateur.

Suivent la comédie "Milou en mai" puis "Fatale" et l'adaptation de la pièce d'Anton Tchekhov "Vanya, 42e rue" (1994).

Il meurt d'un lymphome le à Los Angeles.

Au début des années 1960, il a une liaison avec le mannequin brésilien Vera Valdez.

Marié à Anne-Marie Deschodt de 1965 à 1967, Louis Malle se lie ensuite à l'actrice allemande Gila von Weitershausen dont il a, en 1971, un fils, Manuel Cuotemoc, à l'actrice franco-canadienne Alexandra Stewart qui lui donne, en 1974, une fille Justine Malle, puis, entre autres, à Susan Sarandon à la fin des années 1970.

Il épouse l'actrice Candice Bergen en 1980, leur fille, Chloé Malle, nait en 1985. Ils sont restés mariés jusqu'à sa mort en Californie, en 1995.

Sa cousine, Françoise Béghin (née en 1938), fille benjamine de son oncle maternel Ferdinand Béghin, est l'épouse de l'écrivain et académicien Jean d'Ormesson.

L'un de ses frères, Vincent Malle, a été producteur de cinéma.

Au cours de sa carrière, le réalisateur a alterné films de fiction pure et documentaires.

Son documentaire le plus connu est "Le Monde du silence", le premier vrai film sur la faune sous-marine. Co-réalisé avec Jacques-Yves Cousteau, ce long métrage remporte la Palme d'or au Festival de Cannes. Malle reste à ce jour le plus jeune lauréat. Ce film marque aussi sa première grande expérience professionnelle pour laquelle il devient scaphandrier.

Quinze ans plus tard, suivant l'exemple de Jean Renoir et Roberto Rossellini, Malle filme la vie des Indiens dans une série de documentaires tels que "L'Inde fantôme, réflexion sur un voyage" et "Calcutta", qui reçoit le Prix de la fraternité 1969.

Il décide ensuite de filmer les ouvriers français précaires de l'usine Citroën de Rennes dans "Humain trop humain", qui sort en 1973. Dans "Place de la République" en 1974 il donne la parole aux Parisiens. Il filme également la population pauvre des États-Unis dans "God's Country" (1985) et "La Poursuite du bonheur" ("And the Pursuit of Happiness", 1986). Il y retrace le parcours d'individus qu'il avait suivis une décennie plus tôt.














</doc>
<doc id="1890" url="https://fr.wikipedia.org/wiki?curid=1890" title="Chiffre de Vigenère">
Chiffre de Vigenère

Le chiffre de Vigenère est un système de chiffrement polyalphabétique, c’est un chiffrement par substitution, mais une même lettre du message clair peut, suivant sa position dans celui-ci, être remplacée par des lettres différentes, contrairement à un système de chiffrement monoalphabétique comme le chiffre de César (qu'il utilise cependant comme composant). Cette méthode résiste ainsi à l'analyse de fréquences, ce qui est un avantage décisif sur les chiffrements monoalphabétiques. Cependant le chiffre de Vigenère a été percé par le major prussien Friedrich Kasiski qui a publié sa méthode en 1863. Il n‘offre plus depuis cette époque aucune sécurité.

Il est nommé ainsi au en référence au diplomate du Blaise de Vigenère, qui le décrit (intégré à un chiffrement plus complexe) dans son "traité des chiffres" paru en 1586. On trouve en fait déjà une méthode de chiffrement analogue dans un court traité de Giovan Battista Bellaso paru en 1533.

Ce chiffrement introduit la notion de clé. Une clé se présente généralement sous la forme d'un mot ou d'une phrase. Pour pouvoir chiffrer notre texte, à chaque caractère nous utilisons une lettre de la clé pour effectuer la substitution. Évidemment, plus la clé sera longue et variée et mieux le texte sera chiffré. Il faut savoir qu'il y a eu une période où des passages entiers d'œuvres littéraires étaient utilisés pour chiffrer les plus grands secrets. Les deux correspondants n'avaient plus qu'à avoir en leurs mains un exemplaire du même livre pour s'assurer de la bonne compréhension des messages.

L'outil indispensable du chiffrement de Vigenère est la « table de Vigenère »
Table de Vigenère.

Pour chaque lettre en clair, on sélectionne la colonne correspondante et pour une lettre de la clé on sélectionne la ligne adéquate, puis au croisement de la ligne et de la colonne on trouve la lettre chiffrée. La lettre de la clé est à prendre dans l'ordre dans laquelle elle se présente et on répète la clé en boucle autant que nécessaire.

Le texte chiffré est alors :

Si on veut déchiffrer ce texte, on regarde pour chaque lettre de la clé répétée la ligne correspondante et on y cherche la lettre chiffrée. La première lettre de la colonne que l'on trouve ainsi est la lettre déchiffrée.
Mathématiquement, on identifie les lettres de l'alphabet aux nombres de 0 à 25 (A=0, B=1...). Les opérations de chiffrement et de déchiffrement sont, pour chaque lettre, celles du chiffre de César. En désignant la i lettre du texte clair par Texte[i], la i du chiffré par Chiffré[i], et la i lettre de la clé, répétée suffisamment de fois, par Clés[i],
elle se formalise par :
où x modulo 26 désigne le reste de la division entière de x par 26. Pour le chiffrement il suffit d'effectuer l'addition des deux lettres puis de soustraire 26 si le résultat dépasse 26. Pour le déchiffrement il suffit d'effectuer la soustraction et d'additionner 26 si le résultat est négatif. Le déchiffrement est aussi une opération identique à celle du chiffrement pour la clé obtenue par Clé'[i] = 26 - Clé[i].
Un , qui utilise une représentation circulaire de l'alphabet (après Z on a A), permet de réaliser directement cette opération.

Le chiffré d'un texte suffisamment long constitué uniquement de A donne la clé ( 0 + "x" = "x", soit A + Clés[i] = Clés[i] ).

Si l'on connait le nombre de symboles que comporte la clé, il devient possible de procéder par analyse de fréquences sur chacun des sous-textes déterminés en sélectionnant des lettres du message clair à intervalle la longueur de la clef (autant de sous-textes que la longueur de la clef). C'est l'attaque bien connue sur les chiffrements mono-alphabétiques.

Friedrich Kasiski publie en 1863 une méthode efficace pour déterminer la taille de la clef, le test de Kasiski, en repérant la répétition de certains motifs dans le message chiffré. Charles Babbage s'est intéressé au chiffrement de Vigenère une dizaine d'années auparavant. Il avait décrypté dans des cas particuliers des messages chiffrés par la méthode de Vigenère. Il n'a rien publié à ce sujet, mais on dispose de ses notes. On ne sait pas quelle méthode il a utilisée, il a pu exploiter des faiblesses de l'utilisation du chiffrement. Certains historiens pensent qu'il a pu découvrir la méthode de Kasiski, bien qu'il n'en ait pas laissé de trace écrite.

Des techniques statistiques fondées sur l'indice de coïncidence, découvertes au , s'avèrent encore plus efficaces pour casser le chiffre.
Le chiffre de Vigenère a été réinventé de nombreuses fois au cours des siècles et il a existé plusieurs variantes. Il n'est pas indispensable d'utiliser un décalage comme substitution alphabétique, n'importe quelle permutation des lettres de l'alphabet convient. L'avantage du chiffre de César est d'être entièrement déterminé par la lettre qui donne le décalage. Mais, avant Vigenère, Giovan Battista Bellaso avait proposé un tel système (repris par le physicien Giambattista della Porta qui s'en inspire sans citer Beloso), où chacun des correspondants dispose d'une même grille qui donne une suite de permutations de l'alphabet chacune associée à une ou plusieurs lettres. Chiffrement et déchiffrement demandent la grille et un mot clef. Les lettres du mot clef sont utilisées de la même façon que pour le chiffrement de Vigenère, mais indiquent l'une des permutations de la grille et non un décalage. "A priori", la connaissance de la grille ne permet pas à elle seule de déchiffrer le message, puisqu'il faut le mot clef. Cependant le chiffrement est susceptible des mêmes attaques que celui de Vigenère.

Le système a connu d'autres variantes comme le chiffre de Beaufort.







</doc>
<doc id="1893" url="https://fr.wikipedia.org/wiki?curid=1893" title="Lune">
Lune

La Lune est l'unique satellite de la Terre. Suivant la désignation systématique des satellites, la Lune est appelée ; cependant en pratique cette forme n'est pas utilisée. Elle est le cinquième plus grand satellite du Système solaire, avec un diamètre de . La distance moyenne séparant la Terre de la Lune est de .

La Lune est le premier et le seul objet non terrestre visité par l'Homme. Le premier à y avoir marché est l'astronaute américain Neil Armstrong le . Après lui, onze autres hommes ont foulé le sol de la Lune, tous membres du programme Apollo.

La longueur du demi grand axe entre la Lune et la Terre est de . Le diamètre moyen de la Lune est de . La force qu’exerce la Terre sur la Lune est d'environ .

Parmi les influences les plus connues, des plus réelles aux plus romantiques, citons :


Dans la représentation la plus simple, on peut dire que la Lune a une orbite elliptique autour du centre de la Terre (conformément aux lois de Kepler), qui lui-même tourne autour du Soleil. Pour être plus précis, on peut résoudre le problème à deux corps, ce qui permet de montrer que la Terre et la Lune orbitent en fait autour du barycentre du système double, qui lui-même tourne autour du Soleil, l’influence gravitationnelle perturbatrice du Soleil étant faible par rapport à leur interaction mutuelle. Comme ce barycentre se trouve à l’intérieur de la Terre, à environ de son centre, le mouvement de la Terre est généralement décrit comme une « oscillation », et le système Terre-Lune est donc le plus souvent considéré comme un système planète-satellite plutôt qu'une planète double, bien que ce dernier statut tende à devenir plus courant ces dernières années et a même été considéré ainsi (au moins pendant un temps) par l'Agence spatiale européenne.

La période de rotation de la Lune est la même que sa période orbitale et elle présente donc toujours le même hémisphère (nommé « face visible de la Lune ») à un observateur terrestre (l'autre hémisphère est donc appelé « face cachée de la Lune »). Cette rotation synchrone résulte des frottements qu’ont entraînés les marées causées par la Terre à la Lune, et qui ont progressivement amené la Lune à ralentir sa rotation sur elle-même, jusqu’à ce que la période de ce mouvement coïncide avec celle de la révolution de la Lune autour de la Terre. Actuellement les effets de marée de la Lune sur la Terre ralentissent la rotation de cette dernière et provoquent un léger éloignement des deux astres d'environ par année. Du fait de cet éloignement et du ralentissement qui fait que la durée du jour terrestre augmente de par an, la Lune à sa naissance orbitait à une distance 2 fois moindre qu'aujourd'hui et la Terre tournait alors sur elle-même en 6 heures.

Les points où l’orbite de la Lune croise l’écliptique (plan orbital de la Terre) s’appellent les « nœuds » lunaires : le nœud ascendant est celui où la Lune passe vers le nord de l’écliptique et le nœud descendant est celui où elle passe vers le sud.

Le plan de l’orbite lunaire est incliné en moyenne de par rapport à l’écliptique. Cette inclinaison varie entre et selon un cycle de 173 jours (la moitié d'une année draconitique).

Le plan de rotation de la Lune subit une précession d’une période de (). Cette précession est provoquée par la gravitation du Soleil et, dans une moindre mesure, par le bourrelet équatorial de la Terre.

Comme la Terre est elle-même inclinée de par rapport à l’écliptique, l’inclinaison du plan orbital lunaire par rapport à l’équateur terrestre varie entre et .

Enfin, l’inclinaison de la Terre varie de de part et d’autre de sa valeur moyenne, ce qu’on appelle la nutation, mise en évidence pour la première fois par James Bradley en 1748 (voir aussi Librations en latitude).

L’origine de la Lune est au cœur d’un débat scientifique. Plusieurs modèles de formation ont été historiquement évoqués:


Mais aucune de ces théories ne parvient à expliquer simultanément un certain nombre d'observations qui se sont accumulées durant le :


Trois hypothèses généralement acceptées forment aujourd'hui le cadre conceptuel de l'origine et de l'évolution de la Lune :

Une collision entre la Terre en formation (proto-Terre) et un objet de la taille de Mars dénommé Théia, aurait éjecté de la matière autour de la Terre, qui aurait fini par former la Lune que nous connaissons aujourd’hui. Cet impact est estimé à 42 millions d’années après la naissance du Système solaire, soit il y a , pendant la période d'intense bombardement initial ayant donné lieu à la formation des planètes telluriques. Il s'agit donc d'une sorte d'hybride entre la théorie de la fission et la théorie de l'accrétion, l'impact ayant éjecté de la matière de la Terre, et cette matière s'étant peu à peu agrégée pour former la Lune.

Des simulations publiées en août 2001 soutiennent cette hypothèse. Elle est aussi corroborée par la comparaison entre la composition de la Lune et celle de la Terre : on y retrouve les mêmes minéraux, mais dans des proportions différentes. Ce sont les substances les plus légères qui auraient été éjectées le plus facilement de la Terre lors de l’impact et que l’on retrouve en plus grande quantité sur la Lune. Le principal élément qui confirme cela est le Fe ; en effet, cet isotope du fer est présent sur Mars dans les mêmes proportions que le Fe, mais sur la Terre et la Lune, il existe en quantité très faible. Seulement, pour qu’il puisse s’évaporer, il faut qu’il soit chauffé à plus de pendant un temps important. La principale thèse pour expliquer cet échauffement est la collision Terre / Théia.

Ce modèle est aussi rendu vraisemblable par la découverte relativement récente de gigantesques bassins d'impact sur la Lune, comme le Bassin Pôle Sud-Aitken, créé par un impact géant. Ces astroblèmes géants montrent l'existence et la possibilité de tels impacts.

En 2012 l'analyse d'échantillons provenant des missions Apollo montre cependant que la Lune a la même composition isotopique du titane que la Terre, ce qui va à l'encontre de la théorie de l'impact géant. En 2017 une hypothèse alternative est proposée, celle d'une série d'impacts moins cataclysmiques : chaque impact forme un anneau de débris (formés principalement de matériaux terrestres) qui se rassemble en un petit satellite, que les effets de marée font ensuite s'éloigner ; ces petits satellites finissent par se rejoindre et fusionner tout à tour en un unique (gros) satellite, la Lune. Ce scénario serait plus compatible avec les contraintes de composition chimique et de moment cinétique, et nécessiterait des conditions moins particulières que celui de la collision Terre/Théia.

Cette hypothèse a été formulée peu après les premières analyses des roches retournées par Apollo 11. Elle permet d'expliquer notamment la présence abondante de plagioclases en surface, et la présence de KREEP (voir #Composition et structure interne).

À la suite de l'impact géant, une telle quantité d'énergie a été produite qu'il est probable que la surface de la Lune consistait alors en un vaste océan de magma, sur une profondeur de plusieurs centaines de kilomètres. La cristallisation et la différenciation de ce magma lors de son refroidissement ont formé la croûte et ses roches anorthosiques typiques, ainsi que le manteau lunaire tels que nous les connaissons aujourd'hui.

Toutefois, ce modèle n'explique pas toutes les caractéristiques observées de la composition de la surface. Un peu comme la physique newtonienne n'est pas fausse en première approximation, mais peut être complétée par la théorie de la relativité, ce modèle doit être amélioré pour expliquer certains détails. Notamment, on observe une forte dissymétrie entre la face cachée de la Lune, plus épaisse, où le thorium est rare en surface, et le relief plus exacerbé (ce qui a été démontré par le relevé topographique effectué par SELENE), et la face visible de la Lune où il existe de fortes concentrations en thorium et en KREEP, et où le relief est peu marqué, avec de vastes plaines lisses (dites « mers lunaires »). Même dans le cas de l'hypothèse de l'océan magmatique lunaire, des bassins profondément creusés comme le bassin Pôle Sud-Aitken auraient dû révéler des concentrations semblables sur les deux faces, et le relief aurait dû être plus homogène sur les deux faces de la Lune. Pour expliquer cette dichotomie géomorphologique et physicochimique, des planétologues ont proposé diverses explications : 

Cette hypothèse suppose que la surface de la Lune a été abondamment et violemment bombardée, il y a à peu près 4 milliards d'années, pendant environ 200 millions d'années, par un grand nombre de météorites ou comètes. Les plus grands cratères ou bassins lunaires proviendraient de cet épisode cataclysmique.

À l’exception de Mercure et de Vénus, toutes les planètes du Système solaire possèdent des satellites naturels qualifiés de lunes. Jupiter et Saturne, de leur côté, en possèdent respectivement 67 et 62, de tailles et de formes très variées, mais quelques uns de taille similaire à la Lune (Ganymède, Io, Callisto et Europe pour Jupiter, Titan pour Saturne, Triton pour Neptune). Dans les années 1970, on connaissait 32 lunes dans le Système solaire, on en distingue aujourd’hui plus de 140.

On considère aujourd’hui que la Lune est un corps différencié : sa structure en profondeur n’est pas homogène mais résulte d’un processus de refroidissement, de cristallisation du magma originel, et de migration du magma évolué. Cette différenciation a résulté en une croûte (en surface) et un noyau (en profondeur), entre lesquels se trouve le manteau. Cette structure ressemble fortement à ce que l'on trouve pour l'intérieur de la Terre, aux dimensions absolues et relatives près, et surtout à la différence essentielle que la Lune est désormais devenue très « froide »; et n’est plus active comme l’est encore la Terre (convection, tectonique, etc.).

Après sa formation, il y a environ 4,5 milliards d’années, la surface de la Lune était un océan de magma liquide. Les scientifiques pensent qu’un des types de roches lunaires présent en surface, la "norite riche en KREEP", (KREEP pour K-potassium, [terres rares], P-phosphore) représente l’ultime évolution de cet océan de magma. Cette « norite KREEP » est en effet très enrichie en ces éléments chimiques que l’on désigne par le terme « d’éléments incompatibles » : ce sont des éléments chimiques peu enclins à intégrer une structure cristalline et qui restent préférentiellement au sein d’un magma. Pour les chercheurs, les « norite KREEP » sont des marqueurs commodes, utiles pour mieux connaître l’histoire de la croûte lunaire, que ce soit son activité magmatique ou ses multiples collisions avec des comètes et d’autres corps célestes.

La croûte lunaire est composée d’une grande variété d’éléments : oxygène, silicium, magnésium, fer, titane, calcium, aluminium, potassium, uranium, thorium et hydrogène. Sous l’effet du bombardement par les rayons cosmiques, chaque élément émet vers l’espace un rayonnement, sous forme de photons gamma, rayonnement dont le spectre (distribution de l’intensité relative en fonction de la longueur d’onde) est propre à l’élément chimique. Quelques éléments sont radioactifs (uranium, thorium et potassium) et émettent leur propre rayonnement gamma. Cependant, quelles que soient les origines de ces rayonnements gamma, chaque élément émet un rayonnement unique, que l’on appelle une « signature spectrale », discernable par spectromètre. Depuis les missions américaines Clementine et Lunar Prospector, les scientifiques ont construit de nouvelles cartes d'abondances (dites "géochimiques") des éléments à la surface de la Lune.

La croûte lunaire est recouverte d’une couche poussiéreuse appelée régolithe. La croûte et le régolithe sont inégalement répartis sur la Lune. L’épaisseur de régolithe varie de dans les mers, jusqu’à sur les hauts plateaux. L’épaisseur de la croûte varie de selon les endroits. Au premier ordre on peut considérer que la croûte de la face visible est deux fois plus fine que celle de la face cachée. Les géophysiciens estiment aujourd’hui que l’épaisseur moyenne serait autour de 35-45 kilomètres sur la face visible alors que jusqu’aux années 2000 ils pensaient unanimement que celle-ci faisait d’épaisseur. La croûte de la face cachée atteint, elle, environ d’épaisseur maximum. Les scientifiques pensent qu’une telle asymétrie de l’épaisseur de la croûte lunaire pourrait expliquer pourquoi le centre de masse de la Lune est excentré. De même cela pourrait expliquer certaines hétérogénéités du terrain lunaire, comme la prédominance des surfaces volcaniques lisses (Maria) sur la face visible.

Par ailleurs, les innombrables impacts météoritiques qui ont ponctué l’histoire de la Lune ont fortement modifié sa surface, en creusant de profonds cratères dans la croûte. La croûte pourrait ainsi avoir totalement été excavée au centre des bassins d’impact les plus profonds. Cependant, même si certains modèles théoriques montrent que la croûte a entièrement disparu par endroits, les analyses géochimiques n’ont pour le moment pas confirmé la présence d’affleurements de roches caractéristiques du manteau. Parmi les grands bassins d’impact, le bassin Pôle Sud-Aitken, avec ses de diamètre, est le plus grand cratère d’impact connu à ce jour dans le Système solaire.

Selon les données disponibles à ce jour, le manteau est vraisemblablement homogène sur toute la Lune. Cependant, certaines hypothèses proposent que la face cachée comporterait un manteau légèrement différent de celui de la face visible, ce qui pourrait être à l’origine de la différence de croûte entre les deux hémisphères.

De la même manière, peu d’informations sont aujourd’hui disponibles pour contraindre la présence d’un noyau. Les données de télémétrie laser (Lunar Laser Ranging experiment) accumulées depuis les missions Luna et Apollo permettent toutefois aux scientifiques de penser qu’un petit noyau de de rayon est bien présent. Celui-ci est beaucoup moins dense que celui de la Terre (ne contient pas ou très peu de fer) et pourrait être partiellement fluide.

Comparé à celui de la Terre, la Lune a un champ magnétique très faible. Bien que l’on pense qu’une partie du magnétisme de la Lune est intrinsèque (comme pour une bande de la croûte lunaire appelé Rimae Sirsalis), la collision avec d’autres corps célestes pourrait avoir donné certaines des propriétés magnétiques de la Lune. En effet, une vieille question en science planétaire est de savoir si un corps du Système solaire privé d’atmosphère, tel que la Lune, peut obtenir du magnétisme à la suite d'impacts de comètes et d’astéroïdes. Des mesures magnétiques peuvent également fournir des informations sur la taille et la conductivité électrique du noyau lunaire, données qui aident les scientifiques à mieux comprendre les origines de la Lune. Par exemple, si le noyau contient plus d’éléments magnétiques (tels que le fer) que ceux qui existent sur la Terre, l’hypothèse de l’impact perd de la crédibilité.

La présence d'un champ magnétique global peu après la formation de la Lune est attestée par l'aimantation rémanente de ses roches les plus anciennes. L'étude détaillée d'un échantillon de troctolite vieux de montre un paléo-champ d'une intensité de donc très comparable à celle du champ magnétique terrestre aujourd'hui. Ce résultat confirme la présence d'une dynamo à cette époque, mais ne permet pas d'en connaître précisément le mécanisme (convection thermique ou solutale, notamment).

La Lune a une atmosphère très ténue. Une des sources de cette atmosphère est le dégazage, c’est-à-dire le dégagement de gaz, par exemple le radon, en provenance des profondeurs de la Lune. Une autre source importante est le gaz amené par le vent solaire, qui est brièvement capturé par la gravité lunaire.

La surface de la Lune n’est pas uniforme. Très rapidement, du fait de la relative facilité d’observation, les hommes purent distinguer de grandes taches sombres qu’ils prirent pour l’équivalent de leurs océans terrestres et auxquelles ils donnèrent le nom latin de "maria" (mers). En réalité, ces étendues de régolithe ont une concentration supérieure de basalte, d’origine volcanique, et sont très inégalement réparties sur la surface lunaire, leur grande majorité se situant sur la face visible, la face cachée n’en ayant que quelques-unes, et de taille beaucoup plus réduite. Le reste de la surface lunaire est constitué par de grands plateaux recouverts de régolithe moins dense en basalte et donc beaucoup plus réfléchissant. Autre relief ponctuant la géographie lunaire, les multiples cirques et cratères, créés par les impacts de météorites de tailles diverses.

"", la quasi absence d’atmosphère et une température supérieure à au soleil devrait rendre impossible la présence d’eau sur la Lune. Pourtant, les données recueillies par les sondes "Clementine" et "Lunar Prospector" à la fin des années 1990 montrent la présence de grandes zones riches en hydrogène, aux pôles sud et nord. Or l’hydrogène est un des constituants de l’eau avec l’oxygène. À la fin de sa mission, la sonde "Lunar Prospector" a même été précipitée dans le fond d’un cratère censé contenir de la glace d’eau. On pensait que l’écrasement dégagerait de la vapeur d’eau, détectable par les télescopes terrestres, apportant ainsi une preuve supplémentaire de la présence d’eau sur la Lune. Mais aucune molécule d’eau n’a été détectée pendant l’impact. Cependant, la probabilité d’en voir était très faible : la sonde étant petite, l’énergie dégagée lors de l’impact n’était pas forcément suffisante pour vaporiser de l’eau.

L’hypothèse actuellement la plus populaire au sujet de la provenance de cette eau propose une origine cométaire à l’eau lunaire et non une origine de l'impacteur Théia. Les comètes, de grosses boules de neige sale, en percutant la Lune il y a plusieurs milliards d’années, se seraient vaporisées, créant ainsi une atmosphère provisoire. La vapeur d’eau contenue dans cette atmosphère se serait condensée puis aurait givré sur le sol. La glace située au fond des cratères du pôle sud aurait pu se conserver pendant deux milliards d’années, le fond de ces cratères n’étant jamais exposé aux rayons du Soleil en raison de l’inclinaison très légère de l’axe de la Lune par rapport à l’écliptique (5,145°). De même au pôle nord, où l’eau glacée serait protégée par une couche de régolithe de d’épaisseur.

Les scientifiques estiment le volume d’eau présent sur la Lune à (un milliard de mètres cubes), une quantité suffisante pour rendre son exploitation intéressante par d’éventuels explorateurs. De l’hydrogène et de l’oxygène pourraient en être extraits par des stations alimentées par panneaux solaires ou par énergie nucléaire. Cela rendrait possible une colonisation permanente de la Lune. L'oxygène est en effet indispensable pour que de futurs explorateurs puissent respirer durant de longues périodes de présence, et l’hydrogène est un carburant pour les fusées. Or le transport régulier de l’hydrogène et de l’oxygène depuis la Terre est très coûteux.

En 2006, les relevés réalisés par le radiotélescope d’Arecibo braqués sur les cratères polaires constamment dans l’ombre montrent que la présence de glace d’eau est encore plus rare qu’escomptée.

L’équipe d’Alberto Saal de l’université Brown (États-Unis) a analysé, au spectromètre de masse, des échantillons de sphérules vitreuses de basalte lunaire ramenés par les missions Apollo 11, 15 et 17 entre 1969 et 1972. Elle y a trouvé la présence d’eau et a conclu que le magma lunaire contenait d’eau avant sa remontée, soit une proportion semblable à celle de la Terre il y a .

Le 17 juin 2009, la NASA a lancé deux sondes spatiales dont l'une des missions principales est de confirmer la présence d'eau dans les régions proches des pôles de la Lune, au fond des cratères plongés en permanence dans l'obscurité. Si cette présence était confirmée, l’eau pourrait être exploitée par les missions habitées.

La mission LCROSS a pour objectif de confirmer ou infirmer les informations faisant état de présence d’hydrogène et de glace dans ces lieux difficiles à explorer et encore largement méconnus. Jusqu'à présent, aucune trace d'eau n’a été trouvée dans les régions équatoriales explorées par les sondes automatiques ou les équipages des six missions Apollo.

Le 24 septembre 2009, la NASA a annoncé la présence d'eau proche de la surface de la Lune. Cette présence a été mise en évidence grâce aux données recueillies par la sonde spatiale Deep Impact (dont la mission étendue a été rebaptisée EPOXI), passée en juin 2009 à 6 millions de kilomètres de la Lune. Cette présence d'eau, et son cycle journalier (évaporation le jour, puis adsorption la nuit, l'eau évaporée étant repoussée vers la surface par le vent solaire résiduel), ont été corroborées par les données de l'instrument M3 de la sonde spatiale indienne "Chandrayaan"-1 et l'instrument VIMS de la sonde Cassini-Huygens. Les quantités d'eau ainsi mises en évidence sont très faibles : un demi-litre d'eau par élément de surface de la taille d'un terrain de football, selon les termes d'un des scientifiques auteurs de la découverte.

Le 13 novembre 2009, la NASA annonce qu'elle a découvert « des quantités significatives » d'eau à la surface de la Lune, à la suite de l'analyse des projections provenant de l'impact volontaire de la sonde LCROSS avec la Lune.

Une quantité équivalente à d'eau à l'état liquide a été trouvée dans le cratère. Toutefois, cette quantité rapportée à la masse de matière éjectée pourrait correspondre à une proportion d'eau très faible (peut-être plus faible que dans une roche terrestre).

Une nouvelle analyse du panache de poussières (provoqué par l'impact de la sonde LCROSS) tendait à démontrer, en juin 2010, la présence de molécules d'eau qui n'avaient pas été exposées à la lumière du soleil depuis des milliards d'années, ce qui suggérait alors l'existence d'une quantité d'eau bien plus importante que ne le laissaient présager toutes les précédentes estimations.

Toutefois, dès août 2010, une autre étude portant sur la contenance en chlore d'échantillons de sol lunaire (ramenés par la mission Apollo) relance l'hypothèse émise 40 ans plus tôt selon laquelle la Lune serait très sèche.

Et ce malgré les informations issues de l'impact de la sonde LCROSS.

Des observations réalisées par LADEE montrent la présence d'un nuage de poussières permanent en orbite autour de la Lune.

Avec une magnitude de -12,6 pendant la pleine lune, c'est le corps céleste le plus visible dans le ciel de la Terre, après le Soleil. Cette luminosité et sa proximité la rendent facilement observable, même à l’œil nu ou en plein jour. Des jumelles permettent de distinguer les mers et les plus gros cratères. De plus, de nombreux phénomènes observables, liés à son orbite caractéristique, la distinguent des autres astres.

Du fait de sa rotation synchrone, la Lune présente toujours quasiment la même partie de sa surface vue de la Terre : la face dite « visible ». Mais la moitié de la sphère éclairée par le soleil varie au cours des 29,53 jours d’un cycle synodique, et donc la portion éclairée de la face visible aussi. Ce phénomène donne naissance à ce que l’on appelle les phases lunaires, qui se succèdent au cours d’un cycle appelé « lunaison ». Ces lunaisons ont été ou sont encore utilisées par plusieurs cultures et civilisations pour construire leurs calendriers annuels (par exemple le monde musulman pour l'établissement de la succession des mois et des fêtes religieuses au sein de l'islam, avec quelques adaptations afin que les mois soient composés de 29 ou 30 jours). On parle alors de calendrier lunaire.

Vue de la Terre, la taille apparente de la Lune est presque égale à celle du Soleil, si bien que deux sortes d’éclipses solaires sont possibles selon l’éloignement de la Lune : totale et annulaire, selon que la Lune en étant plus proche cache totalement le Soleil, ou qu’elle soit plus loin et alors la bordure du Soleil reste visible, l’éclipse est alors annulaire. Les éclipses ne se produisent que rarement puisque le plan de la trajectoire de la Lune autour de la Terre est différent du plan de la trajectoire de la Terre autour du Soleil. Elles ont lieu uniquement quand un nœud coïncide avec la nouvelle Lune. Celle-ci couvre alors le Soleil, en tout ou en partie. La couronne solaire devient visible à l’œil nu lors d’une éclipse totale.

C’est alors la Terre qui sert de cache et son ombre portée sur la Lune est alors observée de nuit à partir de la Terre ; pour quelqu’un résidant sur la Lune, cela sera qualifié d’éclipse de Soleil produite par la Terre servant de cache. La Lune peut alors prendre une apparence orangée, dite de lune rousse.

Au fil du cycle lunaire, la déclinaison de la Lune varie : d’un jour au suivant, elle augmente pendant une moitié du cycle et elle décroît pendant l’autre moitié. En un point de l’hémisphère nord :

La Lune renvoie la lumière du Soleil. Son spectre lumineux est proche de ce dernier du fait de l'atmosphère quasi-nulle (raies de Fraunhofer). Toutefois, la roche poreuse à la surface absorbe une partie du rayonnement et renvoie la lumière sans direction privilégiée. On dit que la lumière est polarisée (voir polarisation de la lumière).

La Lune présentant toujours le même hémisphère à la Terre (sa rotation étant synchrone, c’est-à-dire sa période de révolution étant égale à sa période de rotation), on appelle "librations" les phénomènes permettant à un observateur à la surface de la Terre de voir plus de 50 % de la surface de la Lune.

Ces phénomènes peuvent prendre quatre formes : les librations en longitude, les librations en latitude, les librations parallactiques et les librations physiques.

L’ensemble de ces phénomènes de libration au cours de lunaisons successives permet d’observer environ 59 % de la surface lunaire depuis la surface terrestre. Toutefois, les zones supplémentaires ainsi offertes à l’observation sont très déformées par l’effet de perspective, et rendent difficile l’observation de ces régions depuis le sol. Seules les sondes automatiques, par un survol régulier, en permettent l’étude topologique précise.

Ces phénomènes transitoires de quelques dixièmes de milliseconde, de magnitude généralement de 5 à 10 (mais pouvant être 3), ne sont visibles qu'au télescope ou lunette associés à une caméra vidéo et sur la partie non éclairée de la Lune. Le flash lunaire provient de la chute de corps (provenant essentiellement d'essaims de météorites ou de comètes) de percutant la Lune à des vitesses de , ce qui fait fondre la roche en surface au point d'impact et projette des gouttelettes de roches liquides. L'éclair lumineux est produit par l'énergie dégagée lors de cet impact. Depuis cinq siècles, 570 phénomènes de flash lunaire ont été rapportés par 300 observateurs.

Le (jour de la capitulation du Japon), l'auteur de science-fiction Robert A. Heinlein, alors ingénieur civil pour le compte de la Marine américaine, transmet à sa hiérarchie un projet de mission lunaire qui, adopté par l', sera discuté (et rejeté) l'année suivante en réunion de cabinet à la Maison-Blanche.

Le premier objet fabriqué par l’homme à atteindre la Lune fut la sonde soviétique "Luna 2", qui s’y écrasa le à 21:02:24 Z. L'année 2009 marque l'anniversaire des premières photographies de la face cachée de la Lune envoyées de l'espace pour la première fois le lorsque la sonde automatique "Luna 3", également lancée par l’Union soviétique, passa derrière la Lune. "Luna 9" fut la première sonde à se poser sur la Lune (plutôt que de s’y écraser) ; elle retourna des photographies de la surface lunaire le . Le premier satellite artificiel de la Lune fut la sonde soviétique "Luna 10", lancée le . Le , Lunokhod 1 fut le premier véhicule robotisé à explorer sa surface.

Le , les membres de l’équipage d’Apollo 8 (Frank Borman, James Lovell, et William Anders) furent les premiers humains à apercevoir directement la face cachée de la Lune. Les premiers humains à se poser sur la Lune le firent le . Ce fut le point culminant de la course spatiale engagée entre les États-Unis et l’URSS, alors en pleine Guerre froide. Le premier astronaute à poser le pied sur la Lune fut Neil Armstrong, le capitaine de la mission Apollo 11, et le second, Buzz Aldrin, le même jour. Les derniers hommes à marcher sur le sol lunaire furent le scientifique Harrison Schmitt et finalement l’astronaute Eugene Cernan, lors de la mission Apollo 17 en . Au total au et jusqu'à nos jours, 24 hommes orbitèrent autour de la Lune et 12 d'entre eux marchèrent sur celle-ci.

À la fin des années 1990, les sondes Clémentine et Lunar Prospector ont trouvé des indices de présence d’eau sur la Lune.

La sonde européenne SMART-1 s’est insérée en orbite autour de la Lune avec succès le , elle doit trouver de l’eau et permettre de mieux déterminer l’origine de notre satellite (par calcul du taux de fer), grâce à une analyse étendue par des rayons X.

Récemment, l’agence spatiale chinoise (CNSA) a dévoilé son plan lunaire qui est fondé en 3 étapes :

Le programme constellation de 2008 visait à établir une base sur la Lune d'ici 2024.

Peu après, cette base lunaire devait être utilisée pour les décollages vers Mars et même plus loin encore. En effet ceux-ci seraient plus faciles du fait de la très faible gravitation (6 fois moins élevée que sur Terre).

Le , Washington annonce que ce projet est annulé en raison de contraintes budgétaires.

Bien qu’ils aient planté symboliquement à plusieurs reprises leur drapeau sur le sol lunaire, les Américains n’ont jamais émis de revendication territoriale sur aucune portion de surface de la Lune. Elle est considérée, grâce au traité de l'espace entré en vigueur le , comme un espace international au même titre que les eaux du même nom. Le traité exclut de plus toute utilisation militaire de l’espace, en particulier le déploiement d’armes non conventionnelles.

Le traité lunaire de 1979 n’ayant pas été ratifié par les grandes nations de l’exploration spatiale, l’appropriation dans des buts économiques et commerciaux par des privés reste dans le flou juridique, ce qui entraîne parfois des revendications des plus fantaisistes. Ainsi, en 1953, l’avocat chilien enregistra la propriété de la Lune en payant de l’époque. On a officialisé l’écriture le dans le Conservateur des Biens Racines de la ville de Talca.

La face cachée de la Lune a été explorée uniquement par photographie depuis des sondes spatiales, la première ayant été Luna 3, en 1959.

Le , un satellite de la NASA a révélé des photographies de la face cachée de la Lune. L'animation accélérée proposée par l'Agence Spatiale dure quelques secondes alors que le passage de la Lune devant la Terre dure en réalité près de cinq heures. Les clichés ont été pris par le satellite DSCOVR en orbite à 1,5 millions de kilomètres de la Terre en direction du Soleil.

Le mot "Lune" provient du latin . La forme latine "*luxna" rapproche "Luna" de "lux", « lumière » dont la racine serait "*leuk", mot signifiant être lumineux. Le mot « Lune » aurait été utilisé en France en 1080 pour « astre satellite de la Terre ».

La première définition littéraire de la Lune appartient aux "Hymnes homériques" où elle s'unit à Zeus et accouche de Pendée.

Les astronomes de l'Antiquité ont proposé différentes interprétations résumées notamment dans le chapitre "De la substance de la Lune" du Pseudo-Plutarque. En -450, Démocrite y voyait « des montagnes élevées et des vallées creuses ». Plutarque (46-125) pensait que « la Lune est une terre céleste », les zones sombres et régulières (les plaines) sont des dépressions remplies d’eau. Appelés "maria" (mot latin signifiant « mers » au pluriel), tandis que les hauts plateaux, de couleur claire furent baptisés "terrae" (« terres »), ces reliefs ne correspondaient pas à la conception du monde d'Aristote.

Pour Aristote, le monde supralunaire est parfait et donc la Lune est une sphère lisse et inaltérable. Le disciple d'Aristote Cléarque de Soles explique les taches lunaires par le fait que la Lune est un miroir poli qui réfléchit le paysage terrestre. Cette conception aristotélicienne subsiste jusqu'au Moyen Âge. Ainsi, sur certaines cartes médiévales terrestres sont reportées les taches lunaires : manuscrits du "De Natura Rerum" d'Isidore de Séville, représentation par le géographe Ibn Saïd de l'Afrique du Sud comme la Mare Orientale v. 1250. Cependant, cette théorie est invalidée par l'observation que la Lune se déplace devant la Terre, le visage de la Lune reste inchangé. D'autres savants imaginent alors que les taches sont des vapeurs condensées d'un nuage ou émanant de la Terre. Bien que Galilée ait tourné son télescope vers le ciel et prouvé la réalité de ces reliefs, cette conception de la sphère parfaite est retrouvée dans la Perse du et dans le folklore européen du .

Ces variations de teintes et de lumière à la surface de la Lune sont vues aussi comme des motifs que les hommes interprètent différemment suivant leur culture et leur imaginaire : les nœuds lunaires étaient la tête et la queue du dragon lapon ou du dragon oriental ; la Lune était associée aux animaux nocturnes : le chat (Mandingues en Afrique), le lapin ou le lièvre de jade compagnon de Chang'e. Certains y voient un buffle aux cornes lunaires, une vieille femme au fagot ou à béquilles (Lune descendante), le visage poupin de Jean de la Lune ou un visage de femme ou d’homme entre autres.

La Lune est très présente dans de nombreuses mythologies et croyances folkloriques, et a souvent été associée à des divinités féminines. Ainsi, la déesse grecque Séléné ("Luna" chez les Romains) a été associée à la Lune, avant d’être supplantée par Artémis (Diane chez les Romains). En revanche, la déesse japonaise Amaterasu est associée au Soleil et son frère, Tsukuyomi, est lui associé à la Lune, de même chez les Mésopotamiens, où le dieu Nanna (ou Sîn) est associé à la Lune. Cette inversion est également présente dans les mythologies nordiques et germaniques (scandinave, lettonne …), et c’est pourquoi J. R. R. Tolkien l’a reprise dans sa mythologie de la Terre du Milieu, faisant de Tilion le dieu de la Lune et d’Arien la déesse du Soleil.

La Lune est également présente dans la culture religieuse musulmane. Non seulement elle est à la base de l'édification du calendrier musulman qui est un calendrier lunaire, mais elle est aussi évoquée dans les différentes biographies religieuses de Mahomet puisqu'on lui prête l'exploit d'avoir fendu la Lune en deux.

Les connaissances empiriques des hommes sur l’agriculture ont toujours accordé une grande importance à la Lune, dans les diverses phases de développement des végétaux ou pour déterminer les moments propices aux semailles.

Le mot "lunatique" est dérivé de Luna par supposition ancienne en Europe que la Lune était liée au cycle menstruel de la femme (mais pas en Inde, où celui-ci est plus proche de 32 jours, voir article) ou de folie périodique. De même pour les légendes concernant les thérianthropes — tel le loup-garou — créatures mythiques qui tireraient leur force de la Lune et seraient capables de passer de leur forme humaine à leur forme bestiale pendant les nuits de pleine Lune.

Certains auteurs ont fait remarquer que, si la Lune n’avait pas constamment présenté la même face à la Terre, l’histoire de la pensée aurait été différente. En effet, la voyant tourner, il devenait évident d’y voir une sphère et non un disque. Une généralisation de cette constatation à d’autres objets célestes et en particulier à la représentation de la Terre aurait pu accélérer considérablement l’adoption de conceptions de l’univers non géocentriques.

La Lune a souvent fait rêver, notamment chez les amoureux qui considèrent souvent le clair de Lune comme très romantique. On appelle « lune de miel » un voyage en amoureux, en français comme en anglais ("").

Une chanson populaire française très connue s’appelle "Au clair de la lune".

Mais la Lune est également très présente dans les films d’horreur, tels que "Frankenstein" et "Freddy Krueger".

L’imaginaire a par ailleurs doté la Lune d’habitants, les "Sélénites". Ce nom vient du nom de la déesse grecque Séléné, qui était associée à la Lune.

La lumière de la Lune serait, selon une croyance, à l’origine du blanchissement du linge. Or les pigments sont principalement altérés par les rayons ultraviolets. La lumière de la Lune n’étant qu’une réflexion partielle de la lumière du Soleil, la quantité d’ultraviolet est très faible : environ plus faible que la lumière directe du Soleil. La lumière directe du Soleil est donc plus responsable du blanchissement du linge que la lumière de la pleine Lune.

Cependant, quand la Lune est bien visible la nuit, il y a moins de nuages pour réfléchir l'infrarouge émis par le sol terrestre. Donc le linge exposé se refroidit plus vite et condense plus de rosée. Or la rosée contient du peroxyde d'hydrogène qui peut oxyder les colorants organiques du linge. Ce peroxyde d'hydrogène est produit le jour par les rayons ultraviolets solaires en brisant des molécules d'eau.

Dans la mythologie hindoue, la lune est une entité masculine et se nomme Chandra, elle est représentée par un dieu masculin de la même désignation. Elle est aussi connue sous le nom de Soma, un dieu fameux dans le Rig-Véda. La lune est considérée comme une planète édénique des plus importantes où on y boit le soma, une drogue qui a le pouvoir de donner l'immortalité, dans le cadre de la manifestation cosmique, manifestation qui prend fin après des milliards d'années puis reprend à nouveau dans un cycle sans fin. Même Brahma, le démiurge meurt un jour ou l'autre Le soma est aussi la sève des plantes et la lune est réputée être responsable pour donner le suc et le goût aux plantes. Les êtres qui y vivent, selon les écrits religieux de l'Inde, n'ont évidemment pas de corps comme ceux des humains, constituée de matière brute comme l'eau et la terre. La Bhagavad-gita la mentionne à plusieurs reprises et elle fait partie des douze astres qui, en astrologie, influencent la santé et la destinée de l'être humains : ""D'entre les Adityas, je suis Vishnou, et d'entre les sources de lumière, le soleil radieux. Parmi les Maruts, je suis Marici, et parmi les astres de la nuit, la lune.""

En Unicode, plusieurs symboles existent :




</doc>
<doc id="1895" url="https://fr.wikipedia.org/wiki?curid=1895" title="Lee Scratch Perry">
Lee Scratch Perry

Lee « Scratch » Perry (de son vrai nom "Rainford Hugh Perry", surnommé entre autres The Neat little man, The Upsetter, Pipecock Jaxxton…) est un producteur et musicien jamaïcain né en 1936.

Tour-à-tour producteur, chanteur, danseur, compositeur, ingénieur du son, bricoleur de génie, porte-parole virulent et visionnaire, il reste un des rares artistes jamaïcains de sa génération encore en activité.

Cet homme passe pour un authentique schizophrène depuis le jour où il détruisit complètement son studio (Black Ark) en y mettant lui-même le feu en 1981. Exilé en Suisse, il y épouse une citoyenne helvétique. Aujourd'hui, Lee Perry donne dans le reggae international avec un son new roots plutôt numérique, et est désormais essentiellement connu, hors du milieu spécialisé et des érudits du genre, comme simple chanteur. Malgré ses facéties en live et son côté « décalé », le personnage actuel est tout autre que celui des années 1970. Alors qu'aujourd'hui il est d'abord et surtout un artiste de concert (comme peut le témoigner son passage au Zénith avec Pierpoljak et Max Romeo en avril 2006) et s'est éloigné des studios et de la culture jamaïcaine "stricto sensu", dans les années 1970, c'est d'abord pour son travail de studio et le rôle influent qu'il a joué et joue alors dans la musique jamaïcaine que lui est vouée une grande considération.

À cette époque, il est un aventurier, un bricoleur et expérimentateur qui crée de manière artisanale un son, une musique que personne n'a réussi à imiter, et suscite même pour ses travaux et son génie extravagant cette flatteuse comparaison :

« Cet homme est le Salvador Dalí jamaïcain » ("Bass Culture" de Lloyd Bradley).

Ses œuvres, ses techniques, le son qu'il a su obtenir de son studio (avec un simple 4 pistes au départ) ainsi que ses différentes collaborations avec tout le gotha de l'île ont fait de lui un pilier du son jamaïcain, du ska au reggae, musique qui va vibrer parallèlement à sa vie, qui va brutalement connaître la fin de son âge d'or le jour où il incendie son studio et détruit tout son matériel et ses bandes.

Pour les connaisseurs, Lee Perry est et reste une figure emblématique de la musique expérimentale même s'il n'est pas reconnu (ni même connu) parmi le monde de la musique savante. Sa musique instrumentale et les dubs qu'il pouvait créer, bien qu'expérimentaux, restaient rythmés contrairement à d'autres formes de musique expérimentale. Lee Perry a souvent été en avance sur ses contemporains et découvrit des univers musicaux, par le sample et d'autre audaces, qui ont eu du succès et une influence considérable. Lee Perry joua ainsi un rôle d'initiateur et de figure tutélaire pour le reggae et le dub mais aussi la plupart des musiques dites « actuelles » qui en découlent (dubstep...).

Rainford Hugh Perry est né le 20 mars 1936 à Kendal, un village pauvre de fermiers au nord ouest de la Jamaïque (paroisse de Hanover). Sa date de naissance exacte reste inconnue du fait de l'isolement de ce village où il grandit et commence à travailler très tôt dans les champs. Lee Perry gagne peu, connaît la misère et décide vers l'âge de 15 ans de partir tenter sa chance ailleurs. Il erre pendant près de 10 ans, s'installe a divers endroits, alternant entre travaux manuels, danse et jeu de dominos (le domino est un jeu très courant en Jamaïque et on y joue de l'argent). C'est lorsque les gens ont commencé à le voir danser qu'il reçut le surnom de "Neat Little man" ou "Little Perry" du fait de sa petite taille. Après maints exodes, un mariage qui ne dure pas longtemps, il arrive à Kingston à la fin des années 1950 avec l'ambition d'y faire une carrière de chanteur, voulant profiter du climat favorable à l'émergence d'une nouvelle musique jamaïcaine qui va secouer l'île, le ska, point de départ au début des années 1960 de la grande onde sismique jamaïcaine qui va déferler sur le monde. Ce climat d'émergence de la musique jamaïcaine est intimement lié à celui qui amènera la Jamaïque à son indépendance en 1962.

Lee Perry traîne dans la capitale Kingston qui connaît alors une croissance urbaine exponentielle causée par l'exode rural, et qui est à l'origine de nombreux ghettos (comme Trenchtown) dont les sound systems vont profiter pour se créer un public. Beaucoup de gens comme Lee Perry rêvaient à l'époque de faire une carrière dans la musique. Lee Perry est présenté à Clement Seymour Dodd dit "Coxsone" dans l'espoir de devenir chanteur, cependant ce dernier, trouvant qu'il n'avait pas de voix, refuse de lui enregistrer un disque. Mais vraisemblablement pris par une certaine amitié pour lui, Coxsone accepte quand même que Lee Perry travaille pour lui en tant qu'homme à tout faire, assistant Coxsone pour toute tâche en rapport avec le studio ou non, comme Lee Perry en témoigna : « Coxsone avait des gens avec qui il aimait bien être, qui l'assistaient et dont il voulait leur présence, j'étais l'un d'eux ». Il devient ainsi conducteur de session d'enregistrement, auditionneur, parolier et même compositeur de chansons.

À l'époque, la rivalité entre sound systems était forte au point que certains sound clashes ( "duels" entre deux sound systems face au public, les DJ passant des disques chacun leur tour) se terminaient souvent en pugilat, avec comme résultat final des blessés assez graves. Au-delà des sound clashes, la concurrence entre sound systems se faisait aussi par le vinyle en produisant des chansons dont le but était de dénoncer, d'attaquer l'autre. Lee Perry était la personne de Studio One qui écrivait ces chansons adressées aux concurrents comme Duke Reid. Il développe à cette époque ce talent de composer des attaques virulentes exprimées implicitement en musique qui se retrouvera plus tard sur les chansons "Small Axe" des Wailers et "White Belly Rat" par exemple. À plusieurs reprises, il retente de s'imposer comme chanteur à Studio One mais Coxsone refuse obstinément de lui accorder son ticket, bien que Lee Perry compose désormais des chansons beaucoup plus osées et formellement intéressantes. Il commence même à être un porte-parole du ghetto, à faire des chroniques sociales en chanson, à dénoncer la violence des politiciens et des mafieux qui contrôlent l'île, sans pour autant abandonner les chansons légères sur l'amour, le sexe et les petites anecdotes de la vie quotidienne. Pendant cette période Studio One, il collabore avec Clancy Eccles avec qui il noue une relation durable. Il va aussi y faire la connaissance des Wailers avec qui il compose et chante à diverses occasions (la chanson "Pussy Galore" par exemple). Il assiste également aux débuts de Toots and the Maytals, Lord Tanamo, Max Romeo et bien d'autres...

En 1965, il fait la connaissance de Pauline Morrisson avec qui il se mariera et qui jouera plus tard un rôle important dans la production des disques de Lee Perry (à leur rencontre, elle a 14 ans alors que lui en a 28). Cette même année, il enregistre à Studio One « "Chicken scratch" », où il figure enfin en tant que chanteur. C'est cette chanson qui rendit public son surnom qu'il portait bien avant et qui figure désormais sur tous ses disques. Cette chanson ainsi que quelques autres ont souvent été enregistrées discrètement en fin de session d'autres enregistrements considérés comme plus important par Coxsone, qui ne voulait pas entendre Lee Perry chanter. Exploité par Coxsone (comme la plupart des musiciens qui travaillaient pour lui) qui pille ses chansons et ne le crédite pas pour celles qu'il écrit pour d'autres, il est aussi mal payé. Il quitte Studio One en 1966.

Il se met alors à travailler avec et pour Prince Buster, chez qui il produira quelques 45 tours. Il travaille aussi chez Joe Gibbs, qui était le concurrent de Dodd, pour qui il produira en 1968 la chanson "I am the Upsetter" qui sera à l'origine du label qu'il va créer, « "Upsetter" » (chieur, emmerdeur, fouteur de merde), d'un nouveau surnom et quelques années plus tard du nom de son groupe : « The Upsetters ». Perry commence petit à petit à voler de ses propres ailes en produisant lui-même ses disques, bien qu'il ne possède pas encore son propre studio. C'est à cette époque qu'il produit et chante des chansons comme ""People Funny Boy"" ou " "You Crummy"" et collabore à certains titres avec Clancy Eccles.

Petite anecdote à propos de la chanson "People Funny Boy ": les pleurs d'enfant dans la chanson sont ceux de celui de Lee Perry qui, pour obtenir l'effet désiré, lui a mis une fessée.

1969 est l'année où Lee "Scratch" Perry apparaît réellement sur le devant de la scène. Lee Perry fait la connaissance des Hippy boys, parmi lesquels deux figures historiques du reggae et qui joueront un rôle crucial pour Lee Perry, les frères Barrett (Aston à la basse et Cartlon à la batterie). C'est avec eux et notamment Val Benett ou Alva Lewis à la guitare et des membres du Gladdy's all stars qu'il fonde le groupe son fameux groupe The Upsetters. Il y avait déjà eu deux groupes qui s'appelaient The Upsetters, le premier était un vieux groupe jamaïcain des années 1950, le deuxième était le Gladdy's all stars. Avec cet ensemble, il enregistre l'album instrumental Return of Django et signe un contrat chez Trojan Records. L'album se vend bien et le morceau éponyme "Return of Django" atteint le 5 des Charts anglais. Celui-ci, tout comme d'autres morceaux du même genre (le "Liquidator "des Harry J. All Stars), étaient fortement appréciés par les premiers skinheads anglais (on parle d'ailleurs de « skinhead reggae » pour cette période). On commence à l'époque à entendre le mot reggae, et l'origine du terme comme celle du genre est disputée ; Lee Perry fait partie de ces quelques protagonistes (avec notamment Frederick « Toots » Hibbert) que l'on crédite d'avoir "inventé" le reggae, du fait de l'avance qu'il avait prise au niveau musical par rapport à d'autres musiciens de l'île.

Fort du succès de "Return of Django", le groupe est invité à tourner en Angleterre, avec Lee Perry en chanteur et notamment les frères Barrett, qui commencent à devenir la section rythmique principale des Upsetters alors qu'à l'origine ils n'étaient que des "roues de secours" parmi les Hippy Boys. Lee Perry va ensuite enregistrer avec le groupe les albums "Many Moods of the Upsetters", "Scratch the Upsetter again" et "Eastwood Rides again". Scratch montre progressivement son intérêt pour le cinéma en faisant de plus en plus souvent référence aux western italiens ou aux films de Kung-fu, et plus tard à des films engagés (comme "Blackboard Jungle"). Cet intérêt pour le cinéma se retrouvera aussi dans les samples qu'il fera plus tard de dialogues de film. Scratch commence aussi à planter quelques graines qui aboutiront au dub en commençant à expérimenter les subtilités de la table de mixage.

Fin 1969, désemparés et cherchant à enregistrer, les Wailers s'adressent à Lee Perry qu'ils connaissaient déjà. Ce dernier n'accepte pas tout de suite car il voulait à l'époque n'enregistrer que des instrumentaux et n'avait pas besoin de chanteurs. Il les auditionne quand-même et comprend que quelqu'un les avait renvoyé car pendant l'audition Bob Marley était en train de chanter "My Cup" qui contenait une phrase qui résumait bien sa situation « Ma tasse est pleine et je ne sais pas quoi faire ». Perry les accepte donc et les Wailers commencent à enregistrer avec les frères Barrett qu'ils connaissaient, ayant déjà enregistré "Black Progress" ensemble. Ils commencent par enregistrer "My Cup", "Riding high", "Soul Rebel" et des anciens morceaux qu'ils avaient chantés à Studio One.

Mais Lee sent que Bob Marley, qui devenait le personnage prédominant du trio, ne faisait pas de son mieux, que les chansons qu'il écrivait n'étaient pas encore des plus captivantes, qu'il n'était pas encore « grand », et en conclut qu'il était "possédé" par un mauvais esprit ("duppy" en jamaïcain). Lee Perry, qui commençait à devenir de plus en plus extravagant, raconta plus tard qu'il confina alors Bob Marley quelque temps dans une pièce de sa maison pour qu'il acquière son génie (de la même manière qu'Aladdin), mais surtout il écrit pour lui la chanson" Duppy Conqueror", censée chasser ce mauvais esprit qui le possédait. Quoi qu'il en soit de son effet réel, la chanson fut en quelque sorte une étape franchie par Bob Marley et les Wailers et ils purent continuer d'enregistrer avec Scratch.

Leur collaboration durera jusqu'en 1971, s'avérant mutuellement enrichissante et particulièrement productive pour les Wailers, qui furent ainsi « prêts » pour conquérir le public mondial, Scratch les ayant aidés au maximum pour les faire progresser et leur permettre de donner leur meilleur. De son côté, Scratch profita de cette collaboration pour aller plus loin dans ses expérimentations sonores et musicales, remixant en dub tous les morceaux des Wailers qu'il pressait en 45 tours et s'adonnant à des arrangements beaucoup plus originaux. De nombreux connaisseurs du reggae considèrent cette période comme majeure dans l'histoire du genre ; selon eux, c'est non seulement celle pendant laquelle les Wailers offrirent leurs meilleurs chansons, mais aussi celle qui permit d'élaborer et affiner le son « reggae », annonçant un âge d'or qui n'allait pas tarder à advenir pour Lee Perry, Bob Marley mais aussi la musique jamaïcaine dans son ensemble. Bob Marley confia quelques années plus tard que Lee Perry était selon lui un génie. Perry en fit de même pour Bob Marley en le considérant comme le « meilleur musicien qu'il ait jamais connu ». Pour beaucoup d’observateurs, c'est avec cette collaboration et ce qu'elle apporta à chacun que Bob Marley et Lee "Scratch" Perry sont devenus et resteront à jamais considérés comme des monuments du reggae.

Mais cette collaboration devait toucher à sa fin à cause de conflits sur les droits d'auteur et des disques que Scratch a publiés sans le consentement des Wailers. Après avoir enregistré des œuvres majeures que sont les chansons compilées dans les albums Soul Rebels et Soul Revolution, les Wailers quittent Lee Perry en "s'emparant" au passage des frères Barrett, qui ne sont désormais plus membres des Upsetters et deviennent des membres permanents des Wailers.

Cette collaboration, bien que majeure, a tendance à faire oublier les différents travaux que Lee Perry a produits pendant la période 1970-1972. Il enregistra ainsi d'autres artistes comme Junior Byles ou Max Romeo, ainsi que divers DJ, poursuivant ses expériences et enregistrant entre autres les chansons "Justice to the people", "Kentucky Skank "ou "Bathroom Skank", chansons de plus en plus constellées de « bidouillages », bruitages divers et sons fantaisistes, un peu de la même manière que les Beatles de la dernière période.

Durant cette période, Lee Perry s'affirme bien qu'il ne possède pas encore son propre studio. Le son qu'il produit se rapproche un peu plus du son qui est connu comme le son "reggae roots". En 1972, il enregistre des classiques du "reggae roots" comme "Fever" chanté par Junior Byles. Ce son "roots" consiste en une guitare rythmique très sèche, une batterie à l'inverse accentuée sur les sons graves et une basse plus présente, des percussions africaines, des harmonies vocales mises en valeur différemment. Lee Perry y enregistre beaucoup de disques qui resteront quasiment inconnus hors de Jamaïque jusqu'à ce qu'on les réédite vers la fin des années 1990. Ces enregistrements obscurs n'en sont pas moins intéressants ; Lee Perry, ses chanteurs et ses musiciens y accomplissent en effet plusieurs tours de force : en 1972, sort par exemple "Cow Thief Skank", un morceau de hip-hop avant l'heure, en avance de quelques années sur les DJ de hip-hop. Il s'agit là véritablement d'un "cut" de différent rythmes, c'est-à-dire une succession continue d'extraits de chansons de Lee Perry, de la même manière que feront plus tard les Dj hip-hop avec des vinyles. De plus, Lee Perry a enregistré sur cette bande un toast (proto-rap) du DJ Charlie Ace, préfigurant ce que sera le genre une dizaine d'années plus tard. Suit "Cloak and Dagger" en 1973, tiré d'un titre d'un film de 1946, un album d'instrumentaux et de "versions" qui préfigure le dub, agrémenté de différents bruitages et effets sonores (klaxon, boîte à meuh), réalisé avec des musiciens comme le saxophoniste ténor Tommy McCook (Skatalites, Aggrovators, Revolutionnaries...) et les Upsetters (avec les frères Barrett).

En dehors de cet album, on remarquera d'autres morceaux originaux, étranges, avant-gardistes, bizarres ou totalement « déjantés » : "Bucky Skank" (avec le jeu de batterie bancal du batteur Tin Legs qui, selon les dires de ceux qui l'ont connu, était inexpérimenté), "Jungle Lion", "Black Ipa"….

Entre temps Lee Perry produit d'autres chansons peu diffusées mais néanmoins "cultes" du "roots reggae" comme "Curly Locks", chantée par Junior Byles, "To be a lover" (pastiche de "I forgot to be your lover" de William Bell) par George Faith et "Words" ("Words of my mouth") chanté par Sangie Davis. Cette dernière, chantée avec beaucoup de conviction et une rare puissance, connaîtra un long destin ; Lee Perry en fit un de ses classiques et la réutilisa une dizaine de fois en dub, instrumental, versions DJ….

Lee Perry reprend des morceaux, notamment ceux cités ci-dessus, les remixe, les retravaille, les distord, et produit ainsi l'album "Blackboard Jungle Dub". "Bucky Skank" devient selon David Katz « une cacophonie de cordes grinçantes, de bruits vocaux et de cuivres » avec des flûtes et des sirènes. "Fever", "Words" de Sangie Davis et "Dreamland", "Kaya et" "Keep on moving" des Wailers subissent aussi ces remixages. Scratch pousse ainsi les plaisanteries de "Cloak and Dagger" encore plus loin, augmente l'utilisation des effets et de la réverbération, et met encore plus en avant la basse et les percussions. Cet album a apparemment été réalisé avec l'aide de King Tubby, l'autre "inventeur" du dub, qui a toujours été généreux avec Scratch et l'a souvent aidé.

Néanmoins, cet album - comme tant d'autres de ses productions de l'époque 72-74 - est longtemps resté rare (voire introuvable) et les informations le concernant sont souvent obscures. De nombreux doutes sont émis quant à la contribution de King Tubby à l'album : une version des faits relate que celui-ci est la grande et unique rencontre entre King Tubby et Lee Perry, l'autre voit en cet album le chef-d'œuvre du génie solitaire de Lee Perry. Ce qui n'arrange rien, "Blackboard Jungle Dub" est sorti avec différentes pochettes et sous différents labels, et ne fut pressé en 1973 qu'à 300 exemplaires, dont seulement 100 pour le Royaume-Uni. Il existe donc plusieurs éditions de l'album, plus ou moins fiables, dont celle qui semble la plus complète est l'édition intitulée "Upsetters 14 Dub Blackboard Jungle", qui contient 14 plages alors que les autres n'en contiennent que 12 ; c'est d'ailleurs celle-ci qui est reconnue par David Katz, biographe de Lee Perry. Les autres versions sont sorties sous le nom "Blackboard Jungle Dub" avec, sur la pochette, soit un lion fumant un joint, soit un tableau noir, et l'ordre et le nom des plages ont été changés. Ces "fausses versions" constituent aussi en partie la compilation "Scratch Attack". Lesquelles ont apparemment été sorties quelques années après (1975, 1980 ou 1990 pour la version CD), mais c'est bel et bien l'édition "Upsetter 14 Dub Blackboard Jungle" avec ses noms de plage et son ordre qui a été retenue par Trojan Records pour le coffret "Dub Triptych" qui réédite entre autres cet album. Autre preuve de la rareté de ces enregistrements, c'est que cette réédition CD a été faite à partir d'un des vinyles de la première édition et non avec les bandes originales, qui n'ont pas été retrouvées (elles n'existent sûrement plus, peut-être détruites dans l'incendie de Black Ark).

On est certes encore loin du dub plus moderne qu'ont pu faire King Tubby ou Scientist plus tard, mais les bases sont bien là. Selon la critique actuelle, cet album est le premier véritable album de dub. Selon le magazine britannique The Wire, il y a dans cet album " une œuvre majeure de génie parmi toutes celles qui la suivirent dans tous les genres". Pour les admirateurs de Lee Perry, cet album est un des plus importants qu'il ait jamais réalisés ; c'est aussi une œuvre qui commence à être reconnue par d'autres et à se faire une place dans l'histoire de la musique expérimentale.

<nowiki>Durant ces années, si Lee Perry s'était effacé de la scène internationale naissante du reggae, ses productions, que l'on pourrait qualifier avec du recul d'</nowiki>"Underground", sont magistrales. Lee Perry fonde avec les chansons qu'il a produites alors les bases de la musique des années qui suivent, posant en quelque sorte les bases de son studio qui va bientôt voir le jour. Tandis que ses expérimentations de plus en plus osées et affirmées sont autant des nouvelles pistes à explorer, des chefs-d'œuvre précurseurs de nouveaux genres comme le dub et plus tard les musiques électroniques, le trip-hop ou le ragga qui en sont de lointains descendants, voient alors le jour.

En 1973, Lee Perry commença à souffrir de la pression d'avoir à compter sur des studios commerciaux pour son travail. La majeure partie de son œuvre avait été enregistrée au studio Randy's, ou chez Dynamic Sounds.
Quelques années plus tôt, lui et sa famille avaient déménagé à Washington Gardens, une banlieue chic de Kingston. C'est ici qu'il eut une sorte de révélation, dans un rêve qu'il fit lors d'une sieste dans le jardin de sa propriété. Il décida d'entreprendre la construction de son propre studio. 
Vers la fin de l'année 1973, il peint les mots "Black Ark" au-dessus de la porte, car il décida qu'il y établirait les 10 commandements du reggae, en référence religieuse à l'Arche d'alliance où Moïse plaça les Tables de la Loi. Ce qui pouvait ressembler à de la vanité excentrique fut plus tard confirmé par le caractère unique de la musique qui en sortit, à en croire l'engouement des artistes qui s'y pressaient.
La musique qu'il enregistra les cinq années suivantes marqua un tournant dans l'histoire du reggae.

Aux commandes de son propre studio, Perry porta ses compétences à un autre niveau, faisant de sa table de mixage un véritable instrument. Les expérimentations des années passées ont ouvert la voie à des nouveaux sons, plus complexes. Le spécialiste du reggae Steve Barrow dira plus tard que "le son du Black Ark était comme la signature d'un peintre sur sa toile", soulignant l'originalité de ces sons.
L'aura du Black Ark commençait à attirer parmi les plus grands artistes jamaïcains, des vétérans Heptones jusqu'aux petits nouveaux comme Jah Lion. Travaillant avec passion, il donnait souvent sa chance à des inconnus, leur offrant un premier essai, et enregistrait également des "has-been" en mal de reconnaissance. Même le fils prodigue Bob Marley retourne au Black Ark et enregistre plusieurs morceaux.
Alors que d'autres studios pensaient "productivité", Perry était heureux de prendre tout le temps nécessaire pour arriver à la bonne "vibe". Une session d'enregistrement au Black Ark faisait penser à une fête, Perry laissant les portes du studio en béton ouvertes, permettant aux curieux de venir y faire un tour, pendant qu'il dansait, tapait des mains, et criait son approbation lorsqu'il était satisfait du résultat.
Perry utilisait des méthodes excentriques, comme nettoyer les têtes des cassettes avec son t-shirt pour ensuite souffler de la fumée de ganja sur les bandes pendant qu'elles tournaient, s'assurant que la musique enregistrée au Black Ark ait un son brut, une qualité magique qui ne pourrait être surpassée.

Utilisant un équipement basique, Perry était capable d'utiliser un 4 pistes et de donner l'impression d'un 8 pistes ou plus en mixant plusieurs pistes sur une seule et en répétant le processus. Avec un matériel loin du dernier cri, il se débrouilla pour créer un certain nombre d'astuces qui étonnent encore les producteurs d'aujourd'hui. "Il n'y avait que 4 pistes sur la machine, " expliqua Lee Perry, énigmatique "mais j'en piochais 20 chez les extra-terrestres". Paul Douglas a mentionné, « Scratch avait un son particulier et tout le monde était fasciné par son son. Il avait cette façon de rassembler les choses; C'était juste son son et cela a influencé beaucoup de gens. Je suis même allé au Black Ark avec Eric Gale pour cet album Negril (album); je me souviens de moi-même et Val Douglas, nous avons posé quelques pistes là, Eric Gale a surdubé des choses dessus, mais honnêtement, je ne me souviens pas de ce qui leur est arrivé. » Lee Perry et son studio ont été formateurs en créant le sous-genre de reggae appelé dub.

Entre 1974 et 1979, beaucoup de pièces maîtresses de l'âge d'or du reggae sortirent du Black Ark. Notamment Max Romeo (War In A Babylon), The Upsetters (Super Ape), Junior Murvin (Police And Thieves), The Heptones (Party Time), et The Congos (Heart Of The Congos), ainsi que de nombreux singles.

Pendant ce temps, le climat politique en Jamaïque commençait à chauffer. Les deux principaux partis disposaient de "gunmen" pour faire régner l'ordre dans les rues de Kingston et asseoir leurs positions. De nombreuses chansons anti-violentes ont vu le jour, décrivant une prochaine apocalypse, comme "War Ina Babylon" de Max Romeo, "Cross Over" de Junior Murvin, et "City Too Hot", l'appel à la raison lancé par Lee Perry. Durant cette période, les productions Black Ark reflétèrent assez fidèlement ce climat et cette confusion.

Dans ce contexte tendu, le son du Black Ark commençait à être internationalement reconnu. Perry fixa un accord de distribution internationale avec Island Records, et attire bientôt l'attention du milieu rock, comme Paul McCartney, Robert Palmer, ou encore les Clash. Des journalistes commençaient à voyager en Jamaïque pour rencontrer l'Upsetter, dont le studio devint célèbre au-delà de l'île.

En dépit de la musique incroyable et des "vibes" magiques de l'arche noire, Vers la fin des années 1970, tout n'allait pas bien au royaume de Perry. Pique-assiettes et vagabonds ont commencé à lui taper sur les nerfs, et faire de la musique est devenu de plus en plus difficile.

Les sessions d'enregistrement-marathons carburées à la ganja et à l'alcool a commencé à se faire sentir sur le travail. Island Records avait considéré certains de ses plus grands enregistrements comme insortables. Le Black Ark est également devenu la cible de bandits locaux qui ont commencé à faire pression pour obtenir de l'argent de protection.

La relation de Perry avec sa femme a commencé à tomber en morceaux. Les demandes polies et moins polies n'ont pas réussi à faire sortir les "mauvaises herbes" de son jardin ; bientôt, Scratch s'est tourné vers des méthodes plus étranges pour se débarrasser des indésirables rude boys. Le Black Ark a bientôt atteint le point d'ébullition, et un point de non retour pour Perry.

L'arche noire a cessé de fonctionner dès 1979. Grillé physiquement, mentalement, et spirituellement, Lee Perry et son studio sont tombés en morceaux. Son épouse Pauline l'abandonne, prenant les enfants avec elle. Perry se retrouve sur une corde raide entre l'imagination et la réalité, et le départ de sa famille a semblé l'enfoncer un peu plus dans le chaos.

Une nouvelle et inquiétante personne a émergé, et tandis que Perry déclarait que c'était une démarche volontaire pour nettoyer la maison de gens qu'il ne voulait plus autour de lui, l'humeur de l'Upsetter était clairement sujet d'inquiétudes. Les visiteurs et les journalistes sont arrivés chez Lee Perry pour le trouver vénérant des bananes, vandalisant le studio, et débitant de longues et violentes diatribes. Des bobines de bandes étaient répandues sur le plancher, et les appareils d'enregistrement étaient presque inutilisables en raison des dommages de l'eau qui perfusait à travers le toit troué. Le studio jadis puissant était maintenant plus ou moins un dépotoir.

En avril 1979, Perry a reçu une visite de Henk Targowski, un imprésario et le propriétaire du Black Star Liner, une maison de production et de distribution basée en Hollande. Targowski a voulu distribuer la musique de Lee Perry, mais n'était pas préparé à la folie qu'il rencontrait au Black Ark. Avec quelques associés, Targowski décide de tenter une opération de sauvetage du studio, essayant de remettre le studio en état de marche. Financé par la Black Star Liner, le travail de reconstruction a progressé tout au long de l'année 1980, du nouvel équipement a été commandé et installé. Au printemps 1980, cependant, le projet de restauration a été abandonné, et l'équipe de la Black Star Liner quitte la Jamaïque pour de bon. Ce qui avait été soigneusement reconstruit fut vandalisé, démonté et détruit par Perry.

En 1981, avec sa vie et son studio en ruines, l'Upsetter quitte la Jamaïque et passe son temps à New York, et joue dans des concerts de groupes locaux de reggae. Une série de grands concerts a lieu, notamment avec les Clash à New York en juin 1982. Perry retourne alors en Jamaïque, et commence peu après à enregistrer un nouvel album, "Mystic Miracle Star". Il semble qu'après deux ans de confusion, Lee Perry a retrouvé la forme. Cependant, le désastre était juste au coin de la rue.

Un matin de 1983, le Black Ark Studio fut détruit. L'incendie a fait rage dans la structure en béton, la température à l'intérieur devint si intense que le toit a finalement été soufflé. Le studio, source d'une partie de la musique la plus puissante jamais enregistrée, tombe progressivement en ruines. "The Black Ark was too black and too dread" (La "Black Ark" était trop noire et trop effrayante), expliqua plus tard Perry. "Bien que je sois noir, je me dois de le brûler, pour sauver mon esprit. Il était trop noir. Il voulait m'avaler" La destruction ardente de l'arche noire est devenue un point-clé dans la légende Lee Perry. Lee Perry clama plusieurs fois qu'il avait brûlé le studio lui-même dans un accès de frustration, mais il est probable que nous ne sachions jamais la cause exacte du feu mais la destruction du Black Ark Studio fut complète.

Après avoir passé trois jours en prison, suspecté d'incendie, Perry n'a nulle part où aller. Sa vie en Jamaïque est en ruines, s'ensuit une période d'exil, en Angleterre majoritairement.
Tournant le dos à la production, il se concentre alors sur l'expression de son œuvre, qui semble être un support infini. Pendant cette période, son travail est irrégulier; collaborations douteuses et faux départs sont au rendez-vous. Sa relation fragile avec Island Records se brise lorsqu'il traite le chef d'Island Chris Blackwell de vampire et dit qu'il est responsable de la mort de Bob Marley.

Lee Perry travaille ensuite avec des groupes de Londres, et commence à se produire sur scène. Finalement, l'album voit le jour en 1986. L'opus, plein de surprises, sonne comme un témoignage de la situation de Perry : après des années de confusion, "the Upsetter was ready to upset again". L'année suivante, Perry fait équipe avec le producteur anglais Adrian Sherwood et donna naissance à "Time Boom X De Devil Dead". Travaillant avec le Dub Syndicate, "les Upsetters de Sherwood", "Time Boom" fut un retour en arrière à l'époque des jours de gloire du Black Ark pour Perry. Le son insufflé par Sherwood correspondait parfaitement à l'Upsetter. Fort de deux nouveaux albums, Perry est remis sur pieds pour de bon.

En 1989, Lee Perry cesse d'errer à droite à gauche et part s'installer en Suisse avec sa nouvelle femme Mireille Ruegg, une zurichoise qui devient le manager de Scratch. Loin de l'effervescence de la scène musicale jamaïcaine, Lee Perry devint un "family man" heureux.
Il a eu deux enfants avec Mireille, un garçon Gabriel, et une fille Shiva.
Vers le milieu des années 1990, il travailla sur un nouveau studio, basé dans la cave de sa propriété de Zurich: le "White Ark", son "laboratoire secret" où "aucun homme n'est entré avant".

20 ans après l'apogée du Black Ark, Lee Perry connaît une renaissance avec une nouvelle vague de fans de sa musique.
Fans et critiques redécouvrent la musique de Perry, notamment avec les Beastie Boys en 1996, et l'Upsetter profite d'un regain de popularité. Les compagnies d'enregistrement ne tardent pas à réagir à l'engouement du public, et une grande variété de collections produites par Lee Perry sont rééditées, ainsi que de nombreux albums. Cette vague donna naissance à l'Arkology en 1997, anthologie du Black Ark en 3CD préparée par Steve Barrow et David Katz, deux fans de Perry. 
À la surprise générale, il se produit dans deux concerts à guichets fermés en avril 1997 à San Francisco, ses premiers spectacles depuis plus de 15 ans aux États-Unis. Il apparaît également à New York, dans un concert pour le Tibet libre. Une grande tournée américaine et européenne s'ensuit, tout en sortant régulièrement des rééditions.

En juin 2000, David Katz publie une grande biographie de l'Upsetter: "People Funny Boy: The Genius Of Lee "Scratch" Perry".

Un long métrage documentaire est en production maintenant par Volker Schaner avec le titre "Vision of Paradise".

Lee a prêté sa voix aux créateurs du jeu vidéo GTA V pour animer une radio diffusant uniquement du reggae nommée Blue Ark.








</doc>
<doc id="1896" url="https://fr.wikipedia.org/wiki?curid=1896" title="Laurel Aitken">
Laurel Aitken

Laurel Aitken est un chanteur jamaïcain, bien que né à Cuba le 22 avril 1927 d'un père jamaïcain et d'une mère cubaine. Surnommé par les Anglais le "Godfather of Ska" (le parrain du Ska), Laurel Aitken était considéré comme l'un des grands noms de la scène ska et reggae. Il est mort d'une crise cardiaque à Leicester, le 17 juillet 2005, à l'âge de 78 ans.

Alors qu'il n'a que 11 ans, Laurel, son frère (le futur chanteur Bobby Aitken) ainsi que toute sa famille, quittent Cuba pour émigrer en Jamaïque.

Sa toute première expérience musicale est de chanter, entre autres, au sein d'un groupe de calypso qui accueille les touristes débarquant à Kingston de leur croisière en bateau. Il se fait rapidement remarquer et enregistre dès le milieu des années 1950 des morceaux de mento, de calypso et bientôt de ce qui va être appelé le ska.

En 1959, la radio nationale de Jamaïque, la "Jamaican Braodcasting Company" qui ne passait jusqu'à présent que des artistes des États-Unis, diffuse la chanson de Laurel Aitken "Boogie Rock". Deux semaines plus tard, le titre grimpe à la première place du hit-parade. 

En janvier 1960, il sort un single : "Little Sheila" et "Boogie In My Bones" sur R&B, le premier label de Chris Blackwell (futur fondateur d'Island Records). La carrière de Laurel est lancée et, comme bon nombre d'artistes des studios jamaïcains, il quitte son île pour rejoindre la riche Angleterre.

Commence alors une période intense d'enregistrement : au cours des années 1960, Laurel Aitken va sortir dix-sept 45 tours sur Blue Beat, quatre sur Dice, sept sur Doctor Bird, dix-huit sur Rio, vingt-trois sur Nu Beat, trois sur Kalypso, trois sur Rainbow, etc.
Son style musical évolue du ska vers le rocksteady et enfin vers le skinhead reggae à la fin des années 1960. 

C'est à cette époque que Laurel Aitken commence à toucher un public blanc (essentiellement les jeunes mods et skinheads, issus de la classe ouvrière) et non plus seulement les immigrés jamaïcains des ghettos londoniens. C'est aussi à cette période qu'il sort ses plus grands succès musicaux sur le label Pama Records : "Fire In My Wire", "Bartender", "Jesse James", "Landlords and Tenants", "Scandal In A Brixton Market" ou encore "Pussy Price".

À la fin des années 1970, il profite du renouveau du ska, avec la vague 2 tone emmenée par Madness et The Specials, pour sortir ce qui sera son unique tube en Angleterre, "Rudy Got Married," qu'il enregistre avec son groupe The Unitones et sort sur le label I Spy Records.

Après la retombée de la vague Two Tone, Laurel Aitken continue tranquillement sa carrière durant les années 1980, notamment en collaborant avec le groupe de ska traditionnel anglais The Potato 5. Il sort avec eux deux 45 tours, "Sally Brown "et "Sahara", puis un album intitulé "Floyd Lloyd & The Potato 5 Meet Laurel Aitken" qui sort sur Gaz's Rockin' Records, le label de Gary Mayall, fils du bluesman John Mayall et leader du groupe ska The Trojans.

En 1988, Laurel Aitken est une nouvelle fois porté par le renouveau de la scène ska (en particulier en Angleterre et en Allemagne), enchaînant les concerts avec sa propre formation The Pressure Tenants. Il apparait comme tête d'affiche aux côtés de groupes comme The Loafers ou The Bad Manners. En 1989 et 1990, en plus de ses concerts réguliers en Grande-Bretagne, Laurel entame une tournée avec les Bad Manners en France, aux Pays-Bas, en RFA, puis aux États-Unis et au Canada.
Le label anglais Unicorn Records, en profite pour rééditer sur disque quelques-uns de ses meilleurs hits, mais sort également ses nouvelles compositions telles que le Maxi-45 tours "Sally Brown", l'album "Ringo The Gringo" et enfin la vidéo de son concert: "Live At Gaz's Rockin' Blues".

Il continue à se produire en concert pratiquement jusqu'à sa mort, après avoir subi une crise cardiaque en 2004 et avant de succomber un an plus tard, à l'âge de 78 ans, d'une nouvelle attaque le à Leicester, Angleterre.




</doc>
<doc id="1900" url="https://fr.wikipedia.org/wiki?curid=1900" title="Lewis Henry Morgan">
Lewis Henry Morgan

Lewis Henry Morgan ( – ) est un anthropologue américain. Il fut le premier à mettre en place une étude des systèmes de la parenté, qui est un domaine élémentaire de l'anthropologie sociale et culturelle contemporaine. Il vécut parmi les Indiens iroquois et observa leur vie sociale et culturelle, faisant de sa propre expérience le matériau brut de sa réflexion.

Né en 1818 dans l’État de New-York. Après des études d’avocat, il devient d'abord conseiller juridique d’une compagnie de chemin de fer. 
L.H. Morgan se reconvertit ensuite à la politique. Membre du Parti républicain, il est élu député puis sénateur.
Il s'intéresse à l'anthropologie lorsqu'il rencontre un Indien Senca (peuple faisant partie de la Confédération Iroquoise) dans un club littéraire. Adopté par le "clan Faucon" à l’issue de son enquête sur la « ligue des Iroquois », il publie ensuite un essai sur le Gouvernement constitutionnel de six nations indiennes.

Il étudie ensuite le système de parenté iroquois, à partir de données recueillies chez les Indiens du Kansas, du Nebraska, du Missouri, et de la baie d'Hudson. Puis il tente une étude des systèmes de parenté à l'échelle de la planète, à l'aide d'un questionnaire envoyé dans les différentes ambassades, colonies, et missions évangéliques. Il en publie les résultats dans "Systems of Consanguinity and Affinity of the Human Family" (1871). 

Pour la première fois, une analyse scientifique de la parenté, une étude d'anthropologie sociale voyait le jour. C'est dans cette œuvre que Morgan entreprit de comparer les institutions sociales de l’antiquité occidentale classique et celles des peuples primitifs contemporains cherchant en celles-ci la clef de l’intelligibilité de celles-là. Dans "Ancient society (1877)", il développa la théorie évolutionniste dont il fut le plus fervent défenseur, en avançant l'idée que l'évolution de l'humanité suit un schéma unique, caractérisé par trois stades successifs : la sauvagerie, la barbarie et la civilisation. 

Ces théories furent reprises par Friedrich Engels, notamment dans son ouvrage "L'Origine de la famille, de la propriété privée et de l'État" publié en 1884. On peut également relever la note que Engels ajoute dans l'édition anglaise de 1888 du "Manifeste du parti communiste", qui limite la portée du principe selon lequel "l'histoire de toute société connue jusqu'ici a été l'histoire d'une lutte de classes": cela n'est vrai, écrit-il, "que de l'histoire "écrite"". Car "finalement, grâce à la découverte décisive de Morgan, qui a révélé la nature véritable de la "gens" et sa place dans la peuplade, la structure intérieure de cette société communiste primitive a été mise à nu dans sa forme typique. Avec la dissolution de ces communautés primitives, la société commence à se diviser en classes distinctes, et finalement antagonistes." ("Ibid".) Les rapports de production n'acquièrent donc une importance dominante (et finalement écrasante) dans l'infrastructure sociale qu'avec l'entrée dans l'histoire au sens strict, définie comme l'âge de l'écriture. Claude Lévi-Strauss s'appuie sur ces textes contre les marxistes trop orthodoxes qui s'irritent de voir l'ethnologie revaloriser les rapports de parenté contre les rapports de production ("Anthropologie structurale", chap. XVI, Paris, Plon, 2e éd. 1974, p. 396). L'œuvre de Morgan a donc joué un rôle majeur dans la théorie du communisme primitif comme première étape de la société humaine, ainsi que dans la transformation et la complexification de la théorie marxienne de l'histoire.

En 1844, Lewis H. Morgan s'établit comme avocat à Rochester, où il mourut le .

Dans sa théorie, Morgan distingue trois stades principaux dans l'évolution de toute société humaine :

Morgan accorda pour la première fois de l'importance aux études des relations de parenté pour une compréhension d'un système social complexe. Il montra la logique interne de ces rapports, et avança qu'ils constituaient les fondements des sociétés primitives, et par extension la source de l'histoire de l’humanité. Délaissant ses travaux sur les indiens des plaines, à la fin des années 1850, Lewis H. Morgan se consacra quasi exclusivement aux études sur la parenté. Il entreprit une enquête à l'échelle planétaire dans l'espoir d'une analyse comparative. Le contexte de la colonisation lui permit d'envoyer des questionnaires dans le monde entier : administrations coloniales, consulats et ambassades américaines, scientifiques, missionnaires. Il ne récolta pas autant de données qu'il aurait souhaité, seules quelques régions du globe avaient répondu à son appel. Ce fut très positif par exemple pour l'Europe et l'Asie, mais très peu concluant pour des régions comme l'Afrique en général, l'Amérique centrale et latine, ainsi que pour la Polynésie.

Il jugea cependant ces données suffisantes, et en s'appuyant sur les systèmes indiens bien connus de lui, il distingua l'existence de deux terminologies possibles sur la surface du globe :
On voit ici que (quels que soient les défauts possibles de cette théorie), Morgan distingue les sociétés les unes des autres à partir de leur "propre" structuration de parenté. Il est ainsi le fondateur d'un nouveau domaine scientifique qui est aujourd'hui considéré comme le champ classique de l'anthropologie sociale : l'étude de la parenté .

Il ajoutera ensuite l'hypothèse selon laquelle les sociétés primitives ont un mode d'organisation sociale fondé sur la parenté, à la différence des sociétés civilisées qui elles fonctionnent sur une base politique. Cependant, encore une fois, Morgan a le premier clairement posé une question essentielle, à savoir celle de la naissance du politique et de l'État. Cette question est ensuite reprise par Evans-Pritchard dans son ouvrage "Les Nuer". On considère ainsi Lewis Henri Morgan comme étant un des pionniers de l'anthropologie politique.

Le livre de Morgan sur le castor américain, "The American Beaver and His Works" (1868, Philadelphie), est un ouvrage qui a été une référence et qui est souvent cité comme point de départ de l'éthologie aux États-Unis. Il est basé sur des années d'observations personnelles, complétés de témoignages d'indiens, de coureurs des bois et de trappeurs (citant les auteurs de chaque témoignage important), et de photos faites par des photographes professionnels à la demande de Morgan. Ce dernier - lorsqu'il a vécu chez les indiens - a rapidement été fasciné par cette espèce-ingénieur et les relations complexes et parfois proches que les amérindiens entretenaient avec lui (en particulier, comme il le raconte dans son Journal Indien, quand il rencontre à Fort Union, une amérindienne qui allaitait un castor d'environ six semaines. 





</doc>
<doc id="1901" url="https://fr.wikipedia.org/wiki?curid=1901" title="Lucien Tesnière">
Lucien Tesnière

Lucien Tesnière, né le à Mont-Saint-Aignan (Seine-Inférieure) et mort le à Montpellier (Hérault), est un linguiste français, considéré comme le fondateur de la grammaire de dépendance.

Professeur à Strasbourg (1924), puis à Montpellier (1937), il a publié des travaux sur les langues slaves, mais il est surtout connu pour sa théorie syntaxique originale, exposée dans son livre posthume "Éléments de syntaxe structurale" (1959), où il propose une formalisation des structures syntaxiques de la phrase, en s'appuyant sur des exemples tirés d'un grand nombre de langues.

Le modèle de Tesnière se base sur la distinction entre l'ordre linéaire et l'ordre structural de la phrase. L'ordre linéaire, monodimensionnel, est réalisé en discours et observable, alors que l'ordre structural est pluridimensionnel et caché. Dès les premières pages de son livre "Éléments de syntaxe structurale", il cite la notion de "innere Sprachform" (Wilhelm von Humboldt) pour postuler qu'il existe une structure non-matérielle sous-tendant la structure visible d'un énoncé. Les transformationistes y reconnaîtront l'opposition entre structure profonde et structure de surface dans la grammaire générative.

Pour représenter l'ordre structural, Tesnière utilise une représentation graphique qu'il appelle "stemma". Le stemma sert à visualiser des relations verticales et horizontales au sein des constructions syntaxiques.

Les "stemmas" de Tesnière (on lit parfois le pluriel "stemmata") préfigurent les arbres syntaxiques de la grammaire générative, mais l'organisation des unités représentées correspond à des liens spécifiques qui ne figurent pas la structure de l'analyse en constituants immédiats.


La notion de dépendance n'est pas définie à proprement parler par Lucien Tesnière. On cite généralement la définition de Paul Garde:

La notion de "nœud" recouvre la notion de connexion. Le régissant et ses subordonnés constituent un nœud.

Selon Tesnière (1959: ch. 48), le verbe est le centre du «petit drame» qu'exprime la phrase. Selon cette conception, qui mêle sémantique et syntaxe, il est en relation avec des acteurs et des circonstances, qui sont exprimés respectivement par des "actants" et des "circonstants". On retiendra les définitions de Tesnière 

Le verbe est l'élément de niveau hiérarchique le plus élevé, il régit des compléments, qui régissent eux-mêmes des éléments subordonnés, y compris le sujet. Son intuition que le sujet (grammaire) n'est qu'un actant parmi d'autres résonne avec l'hypothèse d'un sujet à l'intérieur du SV (syntagme verbal) des théories générativistes.

Selon Tesnière, la nature de la dépendance liant les mots de la phrase est déterminée par la partie du discours à laquelle ils appartiennent. Il y aurait donc une sorte de déterminisme morphologique, les classes de mots gouvernant la syntaxe.

Tesnière décrit ainsi les mots qu'il dit «pleins», qui sont les seuls à pouvoir posséder des dépendants. Ces «espèces de mots» sont représentées par une lettre, qui correspond à leur terminaison en espéranto.

Un mot peut assumer une fonction qui n'est pas prévue par sa nature en subissant une translation. Ainsi, un nom peut être transféré en adverbe et assumer la fonction de circonstant. La translation est une opération à deux termes:

Par exemple dans la phrase "Il a plu pendant un an", le mot "an" (transférende) est un substantif qui est transféré en adverbe grâce au translatif "pendant". De ce fait, il peut assumer la fonction de circonstant.

Il existe deux types de translation :


Paul Garde, « Des parties du discours, notamment en russe », "Bulletin de la Société de linguistique de Paris", 76(1), 155–189.


</doc>
<doc id="1902" url="https://fr.wikipedia.org/wiki?curid=1902" title="Laurent Lafforgue">
Laurent Lafforgue

Laurent Lafforgue est un mathématicien français né le à Antony. Il a reçu la médaille Fields en 2002 pour avoir démontré une partie des conjectures de Langlands.

En 1984 et 1985, Laurent Lafforgue participe aux Olympiades internationales de mathématiques, et reçoit une médaille d'argent les deux années. Il est aussi lauréat du concours général. Ancien élève de l'École normale supérieure il effectue sa thèse sous la direction de Gérard Laumon dans l'équipe d'arithmétique et géométrie algébrique du laboratoire de mathématiques d'Orsay de l'université Paris-Sud (Paris XI). Dans ce même laboratoire, il travaille alors comme chargé de recherches, puis devient directeur de recherches au CNRS.

En 2000, il devient professeur de mathématiques à l'Institut des hautes études scientifiques (IHÉS).
En 2002, il reçoit, avec Vladimir Voevodsky, la médaille Fields au cours du international des mathématiciens qui se déroule cette année-là à Pékin. Il apporte une contribution exceptionnelle dans les domaines de la théorie des nombres et de la géométrie algébrique, en démontrant une partie des conjectures de Langlands.

Le mathématicien ukrainien Vladimir Drinfeld a établi le cas du groupe linéaire en deux variables sur les corps de fonctions des courbes en caractéristique positive.
Généralisant la méthode de Drinfeld, Laurent Lafforgue démontre le cas des groupes linéaires en un nombre quelconque de variables sur ces mêmes corps de fonctions. 

Laurent Lafforgue est membre de l'Académie des sciences, section mathématiques, depuis le .

En 2004, il commence à s'intéresser au système éducatif français et se rapproche du collectif "Sauver les lettres". Il cosigne avec Alain Connes et d'autres scientifiques le texte "Les savoirs fondamentaux au service de l’avenir scientifique et technique : comment les réenseigner" où ils expriment leur point de vue sur l'enseignement des mathématiques et du français à l'école primaire.

Nommé membre du Haut Conseil de l'éducation (HCE) par le président de la République de l’époque, Jacques Chirac, il en démissionne à la demande du président du HCE au lendemain de la première réunion de travail le . Selon Laurent Lafforgue, le motif invoqué aurait été le contenu virulent d'un courriel où il expose ses réticences concernant la qualification des experts du ministère de l'Éducation nationale nommés pour mener à bien le travail du HCE. Il est remplacé le par Antoine Compagnon, universitaire et historien de la littérature.

Il écrit avec Liliane Lurçat un livre intitulé "La Débâcle de l'école" et donne de nombreuses interviews et conférences sur l'éducation.

En 2016, il cofonde l'École professorale de Paris, établissement privé de formation des enseignants.

Laurent Lafforgue a deux frères, Thomas (cadet) qui est professeur en deuxième année de classe préparatoire (PC*) aux grandes écoles au lycée Louis-le-Grand, et Vincent (benjamin), mathématicien, directeur de recherche CNRS à l'université de Grenoble.

Laurent Lafforgue s'est engagé vigoureusement pour la promotion d'un enseignement classique dans l'école républicaine. 
Tout en se plaçant avant tout, par sa foi catholique, dans le cadre de l’Église catholique et de son enseignement universitaire établi au Moyen Âge, il promeut, de ses propres mots, , l'enseignement laïque et républicain, qui en est pour lui un héritier et dont il est, ainsi que ses parents et frères, personnellement issu. 
Dans ce cadre il dénonce le « reniement par l'école de ses principes ».

Récemment, il a contribué aux activités de la Fondation Lettres et Sciences, fondée en 2004 par Philippe Nemo et associée à la Fondation pour l'école dirigée par Anne Coffinier, laquelle se donne pour objet de favoriser l'ouverture d'écoles privées hors-contrat . Il enseigne par ailleurs les mathématiques à l’École professorale de Paris dont il est l’un des membres fondateurs.





</doc>
<doc id="1908" url="https://fr.wikipedia.org/wiki?curid=1908" title="Matrice">
Matrice

Matrice — du mot latin (), lui-même dérivé de , qui signifie « mère » — un élément qui fournit un appui ou une structure, et qui sert à entourer, à reproduire ou à construire.







</doc>
<doc id="1909" url="https://fr.wikipedia.org/wiki?curid=1909" title="Mykonos">
Mykonos

Mykonos (en grec / ) est une île du nord des Cyclades grecques dans la mer Égée méridionale. Elle se situe entre Tinos, Syros, Paros et Naxos. En 2001, elle comptait habitants, pour une superficie de , un littoral de et accueillait au moins touristes par an. Les villes les plus importantes sont Chora et Ano Mera. L'activité principale de l'île est le tourisme. Les habitants de Mykonos sont appelés les Mykoniates ou Mykoniens.

Mykonos se trouve à (4 heures de traversée en Ferry) au sud-est du port du Pirée.

Mykonos a une grossière forme de triangle irrégulier brisé de petites criques dont les plus importantes sont à Panormos (Nord), Tourlos (Ouest) et celle qui constitue le port de Chora (capitale de l'île). Les principaux promontoires de l'île sont Aligomantra, Armenistis, et Evri en face de Dragonisi. À moins de au sud ouest, se trouvent l'île historique de Délos et Rhénée. D'autres îlots dépendent administrativement de Mykonos : Ano Rematiaris - Kato Rematiaris - Kouneli - Prasso - Sfontili - Kromidi - Ag. Giorgos - Kavoura - Marmaronisi - Moles et .

L'île comprend une dizaine de villages:

Le sous-sol de l'île est principalement composé de granite. Il y a des roches sédimentaires au nord-est. Il y a des roches métamorphiques de type schiste bleu.
Il existe de nombreux ruisseaux intermittents mais pas de cours d'eau permanent. Il n'existe qu'une seule source, située près du monastère de la Panagia Tourliani ; la majeure partie de l'eau provient donc de puits.

Il existe deux modestes massifs montagneux, l'Anomeritis (est) et le Vorniotis (nord-ouest), comprenant chacun deux sommets : les deux plus hauts sommets de ces deux massifs sont appelés Profitis Ilias ("le prophète Élie").

Le climat de Mykonos est de type méditerranéen, tempéré chaud. Il se caractérise par une sécheresse estivale et des précipitations variables. Les températures moyennes sont de l'ordre de . Les deux caractéristiques du climat de l'île sont l'ensoleillement et le vent. Durant l'été, la température moyenne est de , tandis que l'hiver elle est de . Le nombre de jours de précipitations annuelles est de 71. L'hiver, l'île subit les vents violents du sud qui apportent souvent avec eux les orages électriques. Le plus célèbre est le Sirocco, qui chaque printemps apporte le sable des déserts et des pluies rouges. L'été, c'est le Meltemi qui domine en juillet et août, soufflant tout au long de la journée et réduisant sa vitesse au cours de la nuit. Les vents de tempête ne sont pas rares.

L'île manque de forêts et présente peu de plantation ou cultures. Celles existantes sont du blé, de l'orge, du raisin et des figuiers. Auparavant Mykonos était réputée pour la chasse aux cailles et aux tourterelles. Les résidents pratiquent la pêche, qui est loin de couvrir les besoins de l'île, et l'élevage.

Mykonos fait partie de la périphérie (région administrative) de l'Égée-Méridionale (dont le siège est Ermoúpoli, sur l'île de Syros), et dont elle constitue un des districts régionaux.

L'île forme un dème (municipalité) depuis 1997, à la suite de la fusion, dans le cadre du programme Kapodistrias, des deux circonscriptions pré-existantes qui sont devenus des districts municipaux :

La population de Mykonos a varié selon les époques, oscillant de 2000 et 5000 personnes, et s'est accrue par les migrations de Crète ou des plus proches îles : Naxos, Folegandros, Sikinos, Kimolos, après les famines et les épidémies, puis les conflits jusqu'à la fin du .

Évolution démographique

Selon la mythologie, l'île prend son nom du héros Mycon, fils du roi Aniou de Délos, ce dernier étant lui-même fils du dieu Apollon, et de la nymphe Rios (ou Royous), fille de Dionysos. Une autre légende veut que les géants qui ont été exterminés par Hercule pendant la guerre des géants soient enterrés sous les imposantes roches de Mykonos.

Le premier peuplement attesté de l’île remonte au Néolithique : il s’agit de tribus originaires de Carie. Puis les Phéniciens leur succèdent. Au début du , des Ioniens d'Athènes se sont installés et ont dominé l’île après en avoir expulsé les dominateurs précédents. Pendant les guerres médiques, Mykonos accueille le général perse Datis, de retour de la bataille de Marathon. Par la suite, Mykonos fait partie de la ligue de Délos, au sein de laquelle elle paie un tribut d’un talent et demi, réduit à un talent, par la suite. En raison de sa proximité avec Délos, l’île sert de base de ravitaillement à cette dernière. Le temple d’Apollon à Délos possède ainsi des terres au sud-ouest de Mykonos. L’île était plutôt pauvre bien qu’étant une île agricole. Les habitants étaient adorateurs de Dionysos, Déméter, Zeus, Apollon, Poséidon et Héraklès. Pline l'Ancien () précise à propos de l'île : « Mycone, avec le mont Dimaste, à pas de Délos ».

À partir de 1204, à la suite de la Quatrième croisade et de la chute de Constantinople, les conquérants latins se partagent l'empire Byzantin. La souveraineté sur les Cyclades revient aux Vénitiens, qui annoncent qu'ils cèdent l'administration des îles à celui étant capable de les gérer en leur nom. De nombreux aventuriers arment des flottes à leurs propres frais et les frères Andrea et Geremia Ghisi deviennent maîtres de Tinos et Mykonos. L'île passe ensuite sous administration vénitienne directe, comme Tinos, en 1390.

En 1537 l'île est attaquée par le grand amiral de Soliman le Magnifique, Barberousse et passe sous contrôle ottoman, étant rattachée au domaine du Kapudan Pacha. Pendant la période ottomane, l'île est pratiquement autonome, les impôts étant collectés par les autorités locales.

De nombreux habitants de l'île sont actifs pendant la révolution d'Orloff (1770-1774), au cours de laquelle Mykonos est occupée par les Russes. L'économie de l'île profite ensuite du traité entre les Ottomans et l'Empire russe.

Des entreprises de boulangerie et de meunerie s'y développent, grâce au blé d'Ukraine, transformé en pains et en biscuits, ensuite envoyés sur des navires.

L'île participe à la guerre d'indépendance grecque. Elle résiste à une expédition punitive ottomane en empêchant le débarquement d'un escadron de la flotte de l'Empire ottoman en 1822 grâce à son héroïne locale Mando Mavrogenis. Les Myconiens participent activement à la guerre, avec quatre navires armés, dont deux sont totalement équipés et fournis par Mando Mavrogenous : avant que la guerre soit finie, elle avait dépensé la quasi-totalité de sa considérable fortune familiale.

Durant les années 1950 et en raison du développement du tourisme la popularité de l'île a augmenté ainsi que sa population. Aujourd'hui, on estime que Mykonos compte résidents et qu'il y a résidents étrangers. Pendant les mois d'été durant la saison touristique l'île accueille plus de vacanciers, habitants et travailleurs à temps partiel. Plus de 80 % des emplois de l'île sont impliqués dans l'industrie du tourisme, le reste se répartit entre la pêche, l'agriculture et la construction.

Selon le dernier recensement fait en Grèce (2001) la population de l'île s'élevait à habitants.
L'activité principale de l'île est le tourisme. En octobre 2010, l'île a été élue destination européenne favorite par personnes lors d'une consultation pour le magazine Condé Nast Traveler.

Comparaison du poids du tourisme dans diverses îles des Cyclades
En 2010, un article paru dans la presse britannique révèle que le gouvernement Grec, pour faire face à la crise grecque, envisagerait de vendre une partie de l'île.

Les mines de l'île sont à l'abandon, les gisements ne sont plus exploités.

La vie de l'île a toujours été liée au port de Mykonos. L'augmentation du tourisme a conduit l'île, en 1994, à lancer des travaux qui ont duré jusqu'en 2002 : le vieux port n'était pas suffisant pour accueillir les ferries et les navires de croisière. De plus le port n'était pas protégé contre les vents du nord, rendant l'ancrage des navires difficile. En 2011, un appel d'offres est lancé pour la rénovation du port.

L'île de Mykonos est desservie par de nombreuses liaisons maritimes (ferry classiques et NGV) à partir des ports du Pirée, de Raffina ou Héraklion. Les liaisons au départ du Pirée sont quotidiennes et durent toute l’année et l’été ils se doublent ou se multiplient selon la période.

L'aéroport de Mykonos, construit en 1971, se trouve à 4 kilomètres au sud est de la ville de Mykonos et est desservi par des vols internationaux durant tout l'été. Hors saison estivale, cet aéroport est desservi par des vols nationaux. Il est doté d'une piste d'une longueur de .

Transport aérien
Le réseau routier de Mykonos est très correct. Des bus municipaux (KTEL) desservent pratiquement toutes les régions de l'île. Mykonos possède deux arrêts de bus centraux : le premier est localisé dans la partie sud de la ville de Mykonos (près de la place Fabrika) et il dessert principalement les plages du sud : Ornos, Agios Ioannis, Paranga, Psarou, Platis Gialos et Paradise. Le deuxième arrêt de bus est situé au nord de Hora (près du Musée archéologique) et dessert les plages de l'ouest, ainsi que les villages d'Ano Mera et de Tourlos. Par ailleurs, il existe un grand nombre de taxis. En complément de ces deux réseaux, il existe des navettes "caïques" - petites barques de pêcheurs - converties l’été en "taxis". Elles partent du vieux port et de Platis Gialos pour les plages suivantes : Super Paradise, Agrari, Elia, Paranga et Paradise.

L'église de la Panaghia Paraportiani : il s'agit d'un assemblage de 5 églises différentes. Elle se situe dans le quartier du kastro (château) sur une colline fortifiée par les Vénitiens, dont il reste encore quelques vestiges des remparts. C'est la plus importante des 400 églises et chapelles présentes à travers l'île. Son architecture typique est un mélange d'éléments populaires byzantins et occidentaux. Elle a été construite par un artisan anonyme au ou . Elle est surmontée d'un clocher et d'une coupole, entièrement chaulés, qui se dressent face à la mer.

Les plages sont nombreuses sur l'île : au sud de la marina, la petite plage de sable d'Agia Anna. Plus au sud à la sortie de Mykonos, s'étend la longue plage de sable de Magali Amnos. Au nord se trouvent les plages de Tourlos et d'Agios Stefanos. Pour rejoindre le sud de l'île, des caîques, permettent de se rendre à Paranga Beach, Paradise Beach, Super Paradise, les plages les plus célèbres de l'île.

Depuis les années 1970, Mykonos est une destination réputée pour la communauté homosexuelle. Il existe ainsi de nombreux sites dédiés à la communauté gay (hôtels, restaurants, bars, plages et clubs).

De nombreuses personnalités ont séjourné sur l'île, comme Aristote Onassis et Jackie Kennedy, Jean Seberg et Romain Gary, Richard Burton et Elizabeth Taylor, Jean-Paul Sartre et Simone de Beauvoir, Jean-Paul Gaultier ou encore Ana Beatriz Barros.

L'île de Mykonos se compose des villages suivants :
Mykonos : (7929) (entre parenthèses : nombre d'habitants)

Ano Mera (1391)


Presse :

Radios :

Les activités sportives les plus pratiquées sur l'île sont les sports nautiques (ski nautique, voile, planche à voile et plongée), mais aussi l'équitation. Mykonos a un grand nombre de clubs de sport.

Outre l'ensemble de la cuisine grecque, les spécialités de l'île sont le "Louza Loukaniko" (une sorte de saucisse) et le "Kopanisti" (un fromage frais de chèvre au poivre).





</doc>
<doc id="1910" url="https://fr.wikipedia.org/wiki?curid=1910" title="Ministère de l'Économie, du Commerce et de l'Industrie">
Ministère de l'Économie, du Commerce et de l'Industrie

Le , plus connu sous son acronyme anglais METI () est le , du commerce extérieur et de l’industrie du Japon, institué en 2001. Depuis 2016, le ministre est Hiroshige Sekō.

Il s’appelait initialement MITI (, « ministère du Commerce extérieur et de l'Industrie ») qui avait été fondé en 1949.

Ses principales missions :

Il est notamment le ministère de tutelle de :




</doc>
<doc id="1912" url="https://fr.wikipedia.org/wiki?curid=1912" title="Mécanique analytique">
Mécanique analytique

La mécanique analytique est une branche de la mécanique, dont elle constitue une formulation très mathématisée et de portée très générale. La mécanique analytique s'est avérée un outil très important en physique théorique. En particulier, la mécanique quantique emprunte énormément au formalisme de la mécanique analytique.

Contrairement à la mécanique d'Isaac Newton qui s'appuie sur le concept de point matériel, la mécanique analytique se penche sur les systèmes arbitrairement complexes, et étudie l'évolution de leurs degrés de libertés dans ce qu'on appelle un espace de configuration.

Les lois du mouvement sont quant à elles déduites d'un principe variationnel qui, appliqué à une grandeur appelée action, donne le principe de moindre action. En substance, le principe de moindre action énonce que parmi toutes les trajectoires possibles pour relier deux points de l'espace de configuration, celle qui est effectivement parcourue par le système est celle qui donne une valeur extrémale à l'action.

Étant donnés deux points A et B de l'espace de configuration, et une trajectoire formula_1 effectivement parcourue, la donnée d'un point C sur cette trajectoire fait apparaître deux trajectoires intermédiaires formula_2 et formula_3. Ces deux trajectoires étant effectivement parcourues, elles doivent nécessairement donner une valeur extrémale à l'action pour leurs points de départ et d'arrivée respectifs. La somme de ces deux actions sera alors extrémale par rapport à toutes les trajectoires possibles entre A et B passant par C, ce qui suggère que la somme de l'action entre A et C d'une part, et C et B d'autre part, est égale à l'action de la trajectoire correspondante entre A et B passant par C :

En appliquant ce raisonnement à une infinité de points répartis sur une trajectoire reliant les points A et B, on peut écrire l'action ainsi :
où "s" est la position du système dans l'espace de configuration, et d"s" est un élément infinitésimal de déplacement sur la trajectoire considérée. formula_6 est ce qu'on appelle le lagrangien, du nom du physicien français Joseph-Louis Lagrange. Il ne dépend pas de la trajectoire mais uniquement de la position dans l'espace de configuration.

En mécanique classique non relativiste, l'espace de configuration est constitué par les degrés de libertés du système et leurs dérivées par rapport au temps, de telle sorte que l'expression de l'action s'écrit le plus souvent :

où formula_7 la position du système dans l'espace de configuration, et formula_8 la dérivée de formula_7 par rapport au temps. Le fait que l'intégrale est calculée pour une trajectoire donnée entre deux points est ici implicite. L’expression exacte du lagrangien est quant à elle obtenue en général à l'aide de considérations sur les symétries observées par le système.

Les lois du mouvement sont ensuite obtenues à partir de ce qu'on appelle le calcul variationnel, qui permet de calculer un élément formula_10 infinitésimal de variation de l'action entre deux trajectoires infiniment proches.

formula_11.

Une intégration par parties permet alors de réécrire cette expression ainsi :

formula_12,

ce qui, dans le cas où par application du principe de moindre action on a formula_13, aboutit aux équations du mouvement dites d'Euler-Lagrange :
</math>.

Ces équations constituent la base de la mécanique lagrangienne.

Le principe de d'Alembert est parfois utilisé pour parvenir au même résultat, avec comme avantage qu'il permet d'obtenir une expression du Lagrangien :

où formula_14 et formula_15 sont respectivement l'énergie cinétique et l'énergie potentielle du système.

Une manière intuitive pour retrouver l'expression du Lagrangien consiste à considérer les écritures de l'impulsion et du champ de force dans l'espace cartésien :

formula_16.

Comme l'impulsion ne dépend pas de la position et le champ de force ne dépend pas de la vitesse, l'expression formula_17 permet de retrouver l'un et l'autre en dérivant respectivement par la vitesse et la position.

formula_18.

L'équation d'Euler-Lagrange s'avère alors être l'exact équivalent de la deuxième loi de Newton :
Le principal avantage de l'équation d'Euler-Lagrange par rapport à la première loi de Newton est qu'elle reste valable dans le système de coordonnées généralisées formula_19 et non uniquement dans le système de coordonnées cartésiennes formula_20.

L'expression formula_21 est appelée "impulsion généralisée". On doit à William Rowan Hamilton d'avoir introduit cette notion, et avec elle la transformée de Legendre du lagrangien, appelé Hamiltonien et noté formula_22, satisfaisant la relation :

En utilisant l'hamiltonien, les équations du mouvement deviennent :
formula_23
que l'on peut noter de façon plus compacte :
Ces équations, qui forment la base de la mécanique hamiltonienne, ont une interprétation géométrique immédiate : elles montrent que dans l"espace de phase" (et non plus maintenant dans l'espace de configuration), la trajectoire suivie est tangente aux surfaces égalisant l'hamiltonien. Autrement dit, l'hamiltonien est une "constante du mouvement", ce qui conforte son assimilation avec le concept d'énergie.

Les équations du mouvement de la mécanique hamiltonienne permettent d'exprimer la dérivée temporelle de n'importe quelle fonction formula_24 de formula_7 et formula_26 :

formula_27

Cette expression a amené le physicien français Siméon Denis Poisson à introduire l'opération dite crochet de Poisson, qui se calcule entre deux fonctions quelconques dans l'espace de phase :

ce qui permet de réécrire les lois du mouvement, pour n'importe quelle fonction A de l'espace de phase :

Il s'avère que le crochet de Poisson permet de donner une structure algébrique à la mécanique analytique. Cette structure, non commutative, est profondément analogue à l'algèbre des observables de la mécanique quantique. L'équation précédente est par exemple l'exacte analogue du théorème d'Ehrenfest.


Les outils permettant de traiter ces problèmes sont, entre autres :


</doc>
<doc id="1913" url="https://fr.wikipedia.org/wiki?curid=1913" title="Mécanique quantique">
Mécanique quantique

La mécanique quantique est la branche de la physique qui a pour objet d'étudier et de décrire les phénomènes fondamentaux à l'œuvre dans les systèmes physiques, plus particulièrement à l'échelle atomique et subatomique.

Elle fut développée au début du par une dizaine de physiciens européens, afin de résoudre différents problèmes que la physique classique échouait à expliquer, comme le rayonnement du corps noir, l'effet photo-électrique, ou l'existence des raies spectrales.

Au cours de ce développement, la mécanique quantique se révéla être très féconde en résultats et en applications diverses. Elle permit notamment d'élucider le mystère de la structure de l'atome, et plus globalement elle s'avéra être le cadre général de description du comportement des particules élémentaires, jusqu'à constituer le socle de la physique moderne.

L'expression physique quantique désigne quant à elle un corpus théorique un peu plus étendu, qui s'appuie sur la mécanique quantique pour décrire des phénomènes particuliers, notamment les interactions fondamentales.

La mécanique quantique comporte de profondes difficultés conceptuelles, et son interprétation physique ne fait pas l'unanimité dans la communauté scientifique. Parmi ces concepts, on peut citer la dualité onde corpuscule, la superposition quantique, l'intrication quantique ou encore la non-localité.

Globalement, la mécanique quantique se démarque de la physique classique par deux aspects : des règles différentes quant à l'additivité des probabilités, et l'existence de grandeurs physiques ne pouvant se manifester que par multiples de quantités fixes, appelés quanta, qui donnent leur nom à la théorie.

Dans la conception classique des lois de probabilités, lorsqu'un événement peut se produire de deux façons différentes incompatibles l'une avec l'autre, les probabilités s'additionnent. Tel n'est pas le cas en mécanique quantique, où la probabilité d'un évènement est liée à une amplitude de probabilité susceptible d'interférer, y compris de façon destructive.

Cette propriété est illustrée par l'expérience des fentes de Young, considérée notamment par Richard Feynman comme la plus emblématique du comportement quantique de la matière. Dans son cours de mécanique quantique, Feynman consacre un long chapitre à son analyse détaillée. Cette expérience illustre aussi le concept de dualité onde-corpuscule, à la base de l'interprétation standard de la théorie.

On considère actuellement qu'aux échelles macroscopiques, l'apparente non-observation de ce comportement probabiliste s'explique par un phénomène appelé décohérence. Cependant d'autres explications existent, mais aucune ne fait l'unanimité : elles relèvent essentiellement de différences dans l'interprétation de la mécanique quantique.

La mécanique quantique tire son nom de l'existence de grandeurs ne pouvant se manifester que par multiples de quantités fixes, souvent liées à la constante découverte par Max Planck. Ces grandeurs sont par exemple l'énergie ou le moment cinétique des particules.

L'illustration la plus manifeste et la plus riche en conséquences de ce phénomène se trouve probablement dans la structure de l'atome et plus précisément dans l'organisation des électrons autour du noyau. En effet, les électrons se répartissent en occupant les places laissées libres par les valeurs possibles des nombres quantiques liés à leur énergie et leur moment cinétique. Cette organisation permet d'expliquer le comportement chimique et spectroscopique des éléments naturels.

L'existence des quanta n'est pas une propriété fondamentale de la mécanique quantique, car elle peut être démontrée à partir d'autres considérations, notamment relatives à la règle sur l'additivité des probabilités mentionnée plus haut. Cependant, elle constitue certainement l'un des aspects les plus caractéristiques de la mécanique quantique, car c'est elle qui se manifeste le plus aisément dans les équations, et c'est historiquement par cet aspect que la mécanique quantique fut découverte.

C'est incontestablement la résolution du problème du rayonnement du corps noir qui a marqué le début de la mécanique quantique. Au début du , Max Planck résout en effet ce problème en faisant l'hypothèse que l'énergie des atomes ne peut s'échanger que par multiples de quantités proportionnelles à la fréquence du rayonnement, selon la formule désormais célèbre :

En confrontant son modèle aux données expérimentales, il obtient alors facilement une valeur numérique précise pour la constante "h", depuis appelée constante de Planck et reconnue par la suite comme l'une des trois constantes fondamentales.

Cette idée de grandeurs énergétiques ne pouvant s'échanger que de façon discrète inspirera alors de nombreux physiciens, comme Niels Bohr, qui s'en serviront notamment pour développer un modèle de la structure de l'atome. Plus généralement, ce fut le début de ce qu'on appela la théorie des quanta.

Peu de temps après la découverte de Planck, Albert Einstein, à la suite notamment de son analyse de l'effet photo-électrique, suggère que la quantité "h"ν est l'énergie d'une particule électromagnétique qui sera plus tard appelée photon. Cette réintroduction d'une conception corpusculaire de la lumière va inciter Louis de Broglie à proposer une relation analogue à celle de Planck, mais pour la quantité de mouvement :

où formula_2 est un vecteur d'onde. formula_3 est la constante de Planck dite réduite.

Ce faisant, il est l'instigateur de la dualité onde corpuscule qui incitera certains physiciens à rechercher une description ondulatoire de la matière. Parmi ceux-ci, Erwin Schrödinger y parvient et obtient une équation différentielle, portant désormais son nom, qui permet de décrire précisément l'évolution quantique d'une particule. Cette équation prouva rapidement sa pertinence dans sa description du modèle de l'atome d'hydrogène.

Parallèlement, Werner Heisenberg avait développé une approche radicalement différente, qui s'appuyait sur des calculs matriciels directement inspirés de la mécanique analytique classique.

Ces deux approches, ainsi que la confusion concernant le concept de dualité onde corpuscule, donnaient à la mécanique quantique naissante un besoin de clarification. Cette clarification intervint grâce aux travaux d'un physicien britannique, Paul Adrien Dirac.

Dans un livre publié en 1930, intitulé "Principes de la mécanique quantique", Dirac montre que les deux approches, celle de Schrödinger et Heisenberg, ne sont en fait que deux représentations d'une même algèbre linéaire. Dans cet ouvrage fondateur, Dirac extrait les lois proprement quantiques, en faisant abstraction des lois déjà imposées par la physique classique. Dirac donne alors une représentation axiomatique de la mécanique quantique, probablement inspirée des développements mathématiques de l'époque, notamment en ce qui concerne la géométrie projective.

Le travail de Dirac avait été précédé quelques années auparavant par celui réalisé par John Von Neumann, mais l'ouvrage de Von Neumann était beaucoup plus rigoureux sur le plan mathématique, de telle sorte qu'il plaisait surtout aux mathématiciens. Les physiciens lui ont préféré celui de Dirac et c'est donc essentiellement l'ouvrage de Dirac qui a laissé une postérité. Dans la préface d'une ré-édition de son livre, Von Neuman mentionne l'ouvrage de Dirac et le décrit comme , mais ajoute tout de même dans le paragraphe suivant que sa méthode .

Paul Dirac dégage les propriétés essentiellement quantiques des phénomènes physiques et les exprime à travers quelques postulats et concepts qui sont à la base de la mécanique quantique. Elles sont présentées ici d'une façon moins formelle, plus propice à une compréhension générale. L'article détaillé présente leur formulation de façon plus rigoureuse mais aussi plus abstraite.

En substance, un état quantique est ce qui quantifie ce que l'on peut savoir d'un système quantique. Il permet de calculer les probabilités et les valeurs moyennes mesurées des observables (position, quantité de mouvement etc.). Les états quantiques sont décrits mathématiquement par vecteur d'état dans un espace de Hilbert, représenté par une notation dédiée introduite par Dirac, dite notation bra-ket. Un état quantique s'écrit alors sous la forme formula_4. L'évolution dans le temps de ce vecteur d'état est décrit mathématiquement par la fonction d'onde formula_5, gouvernée par l'équation de Schrödinger.

Ces deux représentations concernent les états purs, c'est-à-dire les états de systèmes quantiques simples idéalisés et isolés, où chaque composante peut être quantifiée et observée. Pour les états mixtes, représentant les états quantiques en interaction complexe avec un environnement ou un appareil de mesure, où les composantes sont trop nombreuses ou inaccessibles à l'observation, l'état quantique est plutôt représenté par une matrice densité.

Dans le cas de la notation bra-ket, on exprime l'état quantique en fonction des états propres, c'est dire les états pour lesquels on est sûr que si on effectuait une mesure d'une observable, on obtiendrait à coup sûr une valeur donnée. On utilise en général pour ces états le même symbole que celui utilisé pour identifier cette valeur. Par exemple, lorsqu'on est sûr que si on effectuait cette mesure, le résultat serait une valeur formula_6, alors on note l'état formula_7. Il existe en général un certain nombre (voire une infinité) d'états propres pour une observable donnée. Par exemple, si on s'intéresse au spin d'une particule de spin 1/2, on obtient deux états propres de direction opposée : formula_8 et formula_9. Pour l'observable de position, on obtient une infinité d'états propres correspondant à chacune de positions possibles formula_10 ... formula_11.

Ces états propres sont des vecteurs orthogonaux de l'espace vectoriel de Hilbert, et en forment une base, liée à une observable donnée. Un état quantique quelconque est alors exprimé comme une combinaison linéaire de ces états propres, par exemple un état généralisé de spin 1/2 : formula_12, a et b étant des nombres complexes.

Deux états quantiques quelconques distincts ne sont pas forcément "distinguables", car il existe une probabilité que la mesure de deux états distincts donne la même valeur mesurée. Deux états quantiques sont dits "distinguables" lorsqu'il existe au moins un processus de mesure dans lequel on est absolument sûr que les deux états donnent des résultats différents.

Le plus important postulat de la mécanique quantique est probablement le principe de superposition. Selon ce principe, si un système physique peut se trouver dans un état formula_13, et si de même il peut se trouver dans un état formula_4, alors il peut aussi se trouver dans un état linéairement composé :

où formula_6 et formula_17 sont deux nombres complexes quelconques.

Autrement dit, l'ensemble des états possibles d'un système physique est un espace vectoriel (ou plus précisément un espace de Hilbert, comme mentionné plus haut), dont la dimension peut être quelconque.

Le point important est qu'un état superposé n'est pas un état traduisant une ignorance vis-à-vis de l'état « réel » du système, mais bien une indétermination intrinsèque au système, qui n'est ni dans l'état formula_13, ni dans l'état formula_4. Ce point souleva de nombreux questionnements dans la communauté scientifique. En particulier, le principe de superposition est à l'origine de ce qu'on appelle le problème de la mesure quantique, que Schrödinger popularisa en l'appliquant à un chat qui ne serait, selon le paradoxe de Schrödinger, ni mort, ni vivant.

Le principe de superposition fut aussi analysé et critiqué par Einstein qui, avec Boris Podolsky et Nathan Rosen, imagina une expérience, dite expérience EPR, afin de le mettre en défaut. Une expérience comparable fut menée à la fin du par Alain Aspect, qui confirma le principe de superposition.

La règle de Born, du nom du physicien Max Born, est une interprétation probabiliste des coefficients linéaires du principe de superposition. Elle est d'ailleurs souvent appelée interprétation probabiliste.

Cette règle peut être illustrée en considérant par exemple le chat de Schrödinger, évoqué plus haut, et dont l'état quantique peut être écrit ainsi :

Une expérience qui chercherait à déterminer si ce chat est mort ou vif ne donnerait aucun résultat avec certitude (dans le cas contraire le chat serait soit dans l'état formula_21, soit dans l'état formula_22). De façon simplifiée, il peut être dit que la règle de Born quantifie cette incertitude en stipulant que la probabilité de trouver le chat mort est égale au carré du module de formula_6, divisé par la somme des carrés des modules de formula_6 et formula_17.

Plus généralement, pour un système dont le vecteur d'état est une combinaison linéaire d'états distinguables formula_26, la probabilité pour que le résultat de la mesure définissant la distinguabilité soit le même que si le système avait été dans l'état formula_27 est :

où les formula_29 sont les coefficients linéaires du vecteur d'état.

Pour simplifier les calculs, les vecteurs d'états sont en général normalisés afin que le dénominateur soit égal à un. Cela n'affecte en rien les calculs de probabilités. En pratique, la règle de Born s'écrit donc le plus souvent :

ou encore :

La règle de Born est l'un des postulats de la mécanique quantique les plus difficiles à appréhender. Il fait aussi l'objet de controverses, ne serait-ce que parce que son statut axiomatique est mis en doute par au moins deux interprétations : l'interprétation des mondes multiples et l'interprétation transactionnelle. Selon ces deux interprétations, la règle de Born peut être déduite à partir de considérations mathématiques et physiques plus profondes.

Lorsque à la suite d'une expérience, on est sûr d'obtenir toujours le même résultat de mesure formula_6, on dit que le système physique considéré est dans l'état formula_7. Ceci ne signifie pas pour autant qu'on connait avec certitude le résultat d'une mesure effectuée avec un dispositif expérimental différent. En d'autres termes, la connaissance même totale de l'état d'un système ne garantit pas la connaissance parfaite de résultats de toute expérience faite sur lui.

Ainsi par exemple, si on mesure la position d'une particule dans l'état formula_35, on est sûr qu'on obtiendra formula_36, mais par contre il n'est a priori pas possible de savoir avec certitude ce que donnera le résultat de mesure d'impulsion, car sinon la particule serait aussi dans l'état formula_37, ce qui n'est pas le cas général et constitue donc une hypothèse "ad-hoc".

Plus généralement, si pour un certain processus de mesure "A" on note formula_38 tous les états de résultat de mesure parfaitement déterminés, alors en vertu du principe de superposition, toutes les combinaisons linéaires possibles sont aussi des états possibles pour certains systèmes :

Parmi ces combinaisons linéaires, certaines peuvent très bien être des états de mesure parfaitement déterminée pour un autre processus de mesure "B". La question est donc de savoir quel peut être le résultat de mesure de "A" pour ces états « propres » à "B".

L'interprétation probabiliste des coefficients linéaires suggère alors que le résultat de mesure, s'il n'est pas déterministe, sera tout de même statistiquement égal à l'espérance mathématique :

Cette expression est une forme sesquilinéaire des coefficients formula_41. Dans le sous-espace vectoriel généré par les formula_42, on peut donc écrire cette expression en utilisant un produit scalaire dans lequel la base formula_38 est orthonormée. C'est le choix de ce produit scalaire qui donne un sens à la notation bra-ket : les vecteurs bra, notés « vers la gauche », sont alors les éléments de l'espace dual de l'espace des états ket. On a alors la relation :

de telle sorte que l'expression de l'espérance mathématique peut s'écrire :

Le terme formula_46 suggère l'introduction de l'opérateur linéaire dont les vecteurs propres sont les formula_38 et dont les valeurs propres associées sont les formula_29, valeurs possibles des résultats de mesure. Cet opérateur formula_49 est ce qu'on appelle l'observable associé au processus de mesure "A". Ce n'est rien d'autre qu'un outil mathématique qui permet le calcul de l'espérance mathématique du résultat de mesure, espérance qui s'écrit alors :

L'intérêt d'une telle expression est qu'elle ne dépend plus explicitement de la base formula_38. On gagne ainsi en abstraction et on simplifie les calculs, un peu comme en géométrie analytique où il est souvent plus facile de manipuler les vecteurs avec leur notation abstraite formula_52 plutôt qu'avec leurs coordonnées formula_53 dans une base particulière.

À partir de considérations algébriques élémentaires, il est facile de se convaincre que l'observable formula_49 est un opérateur auto-adjoint qui peut s'écrire en fonction de ses vecteurs propres et valeurs propres ainsi :

Lorsqu'on dispose de suffisamment d'observables pour décrire tout résultat de mesure, on dit qu'on dispose d'un ensemble complet d'observables qui commutent, et c'est dans l'espace hermitien généré par les vecteurs propres de ces observables que l'on travaille.

Par construction, le produit scalaire dans l'espace formula_56 des états permet de calculer les probabilités de résultats de mesure. Il est alors facile de comprendre que les opérateurs linéaires qui conservent ce produit scalaire jouent un rôle très important en mécanique quantique. En algèbre linéaire, ces opérateurs qui conservent le produit scalaire sont appelés opérateurs unitaires. Ils ont comme propriété essentielle d'être l'inverse de leur adjoint :

Puisqu'il conserve le produit scalaire, un opérateur unitaire transforme formula_56 en un espace formula_59 physiquement indiscernable car donnant exactement les mêmes probabilités de mesure. Inversement, s'il existe un opérateur qui transforme formula_56 en un espace indiscernable, alors cet opérateur est unitaire.

La considération de l'ensemble de tous les opérateurs unitaires sur formula_56, ainsi que d'un sous-ensemble qui puisse être paramétré de façon continue par un scalaire μ, permet alors d'approcher formula_62 au premier ordre en μ :

où formula_64 est un opérateur linéaire "a priori" quelconque qui peut, sans perdre en généralité, être écrit sous la forme formula_65.

En écrivant la relation d'unitarité de formula_66, il vient, en restant au premier ordre :

formula_67

C'est-à-dire que formula_68 est auto-adjoint.

En somme, lorsqu'il existe un paramètre formula_69 qui transforme formula_56 de façon continue en un espace formula_71 physiquement indiscernable, alors il existe un opérateur unitaire formula_66 et une grandeur observable formula_68 tels que formula_66 transforme formula_56 en formula_71 et :

En assimilant formula_56 à formula_79, et en notant formula_80 le vecteur de formula_71 tel que formula_82, formula_83 apparait comme le taux d'accroissement de formula_80 pour une variation infinitésimale de μ au voisinage de zéro, de telle sorte qu'il peut être écrit :

où la dépendance de formula_86 en formula_69 est sous-entendue (formula_88).

Les considérations précédentes peuvent être utilisées pour introduire l'équation de Schrödinger d'un point de vue théorique, grâce à un principe de symétrie selon lequel les lois de la physique sont invariantes dans le temps. Une autre façon de dire cela est de dire qu'une expérience menée dans un espace d'états formula_89 est indiscernable d'une expérience identique menée dans un espace d'états formula_90. On peut donc appliquer les résultats précédents en prenant t (ou -t) pour formula_69 :

Le facteur formula_3 est ici réintroduit pour satisfaire aux contraintes dimensionnelles ignorées jusqu'alors. L'expression détaillée de l'observable formula_94, appelé hamiltonien par analogie avec la mécanique classique, est le plus souvent obtenue à l'aide du principe de correspondance.

Cette formulation de l'équation de Schrödinger est assez différente de la formulation historique, et à ce titre elle est parfois appelée "équation de Schrödinger généralisée et dépendante du temps".

Comme pour l'équation de Schrödinger, mais cette fois par application du principe selon lequel les lois de la physique sont invariantes dans l'espace, on introduit l'observable du moment linéaire (aussi appelée "impulsion") et ses trois composantes spatiales :

Le cas du moment cinétique (parfois appelé de façon plus explicite "moment angulaire") se traite de la même façon, mais pour les rotations dans l'espace.

Étant donnés deux opérateurs A et B, non nécessairement observables, on définit leur commutateur ainsi :

Cet opérateur joue un rôle très important en mécanique quantique. Par exemple, lorsqu'on s'intéresse à l'évolution de l'espérance mathématique d'une observable A pour un état formula_86 :

On obtient, en utilisant l'équation de Schrödinger et avec la notation formula_99 :

expression qui constitue le théorème d'Ehrenfest.

Le commutateur est analogue au crochet de Poisson de la mécanique classique. Il intervient aussi dans l'explication et la description du principe d'incertitude.

En pratique, l'état formula_86 est le plus souvent écrit dans une base formula_102 d'états de position spatiale parfaitement déterminée :

Ici l'intégration joue le rôle de la sommation utilisée plus haut notamment dans l'énoncé du principe de superposition, la différence étant qu'il s'agit d'une somme continue, c'est-à-dire de la somme d'une infinité de termes infiniment petits.

La fonction formula_104 est appelée « fonction d'onde » et c'est sur elle que se font l'essentiel des calculs obtenus à partir de l'équation de Schrödinger.

L'écriture de l'équation de Schrödinger non plus en fonction de formula_86 mais de la fonction d'onde se fait en remplaçant chaque terme de l'hamiltonien par les expressions correspondantes dépendant de la fonction d'onde. Par exemple, l'impulsion formula_106 s'écrit comme vu plus haut formula_107 où "T"("x") est l'opérateur unitaire de translation de longueur "x" dans l'espace, c'est-à-dire tel que :

Dès lors, il vient :

Par un changement de variable sous l'intégrale, et en se rappelant que l'équation est écrite au voisinage de "x" = 0, il découle :

Autrement dit, l'opérateur d'impulsion agit sur le vecteur d'état en donnant un vecteur dont les coordonnées dans la représentation spatiale sont les dérivées de la fonction d'onde (à un facteur formula_111 près ignoré ici). Ceci permet d'effectuer tous les calculs uniquement sur la fonction d'onde et ainsi de se ramener à la résolution d'une équation aux dérivées partielles, c'est-à-dire à l'équation de Schrödinger sous une forme plus proche de sa forme historique :

La règle de Born implique que le résultat d'une expérience peut être indéterminé même lorsque l'état du système est parfaitement déterminé. Cette indétermination est intrinsèque au système, et ce en un sens qui n'a pas d'équivalent classique. Cependant, une ignorance concernant l'état exact du système peut aussi justifier une description probabiliste au sens classique du terme, c'est-à-dire avec l'acceptation usuelle des lois de probabilités.

Ainsi, dans une base orthonormale d'états formula_113, même si l'état exact est inconnu, il est tout de même possible de lui attributer une distribution de probabilités formula_114, où formula_115 est la probabilité pour le système d'être dans l'état quantique formula_113. La question est alors de savoir comment rendre compte de ce type de probabilité dans les calculs.

L'étude du système se réduit à celle de la mesure des observables disponibles, qui elle-même se réduit à la mesure de leur valeur moyenne qui s'écrit, pour une observable formula_117 et si le système est dans l'état formula_113 :

Comme le système est dans un état inconnu, mais avec la distribution de probabilité formula_114, l'espérance mathématique devient :

Cette expression est en quelque sorte une double espérance mathématique, prenant en compte à la fois les probabilités quantiques et classiques. Les termes formula_119 sont en effet des espérances mathématiques, pour des distributions de probabilité associées au principe de superposition et à la règle de Born. L'expression formula_123 est quant à elle une espérance mathématique associée à une distribution de probabilité traduisant une ignorance vis-à-vis de l'état réel du système, c'est-à-dire une distribution de probabilité classique.

L'espérance mathématique peut alors s'écrire :

formula_124

L'expression formula_125 est ce qu'on appelle la "matrice densité" associée à la distribution de probabilités formula_115 dans la base formula_113. formula_128 est la trace.

La matrice densité n'est, à l'instar des observables, qu'un outil mathématique qui permet le calcul des espérances mathématiques des résultats de mesure, mais contrairement aux observables, la matrice densité incorpore la prise en compte d'une possible ignorance de l'état exact du système.

En mécanique quantique, il existe quelques problèmes et sujets d'études qui sont désormais très bien analysés, et qui s'avèrent très utiles pour la compréhension d'autres systèmes. Ils font partie intégrante du corpus théorique et sont traités en détail dans tous les manuels.

Les principes fondamentaux énoncés plus haut suffisent déjà à expliquer l'une des propriétés les plus importantes de la matière : la distinction entre bosons et fermions.

En effet, cette distinction découle essentiellement du caractère vectoriel de l'espace des états et de son interprétation probabiliste. Si on considère un système physique (ou plus simplement une particule) et que l'on note formula_129 son état, alors un système physique constitué de deux de ces particules s'écrira formula_130 en utilisant le produit tensoriel des deux vecteurs.

La question qui se pose alors est celle de savoir comment se comporte le système si, par la pensée, on intervertit les rôles joués par les deux particules. Autrement dit, on s'interroge sur la relation entre formula_131 et formula_132. Ces deux systèmes étant parfaitement analogues, lorsque les particules sont considérées indiscernables, elles doivent se comporter de la même façon. Leur répartition de probabilité est donc la même et elles sont donc reliées par un scalaire formula_6 :

formula_134

Or, si on intervertit à nouveau les particules, on doit nécessairement réobtenir le système initial, de telle sorte que :

formula_135

Même parmi les nombres complexes, il n'existe que deux racines carrées de l'unité : 1 et -1. Cela implique qu'il ne peut exister que deux types bien distincts de particules, celles pour lesquelles formula_136, les bosons, et celles pour lesquelles formula_137, les fermions (ces noms font référence aux physiciens qui ont découvert les statistiques associées : Satyendranath Bose et Enrico Fermi).

De cela il découle directement le principe d'exclusion de Pauli, auquel seuls les fermions obéissent. Considérons par exemple un fermion et imaginons deux particules de cette espèce dans exactement le même état formula_129.

On a :
formula_139
et donc :
formula_140

Autrement dit la probabilité pour que deux fermions soient dans le même état est toujours nulle. Une telle propriété est d'une importance considérable dans la nature. On lui doit par exemple en grande partie l'.

À l'inverse, les bosons ont tendance à se regrouper les uns avec les autres, car leurs amplitudes de probabilités interfèrent constructivement quand ils sont dans le même état. Ceci est la cause de nombreux phénomènes, comme l'émission stimulée, à la base du fonctionnement des lasers.

Des considérations comparables aux calculs effectués plus haut permettent de comprendre qu'un nombre pair de fermions se comportent comme des bosons. Ceci est la cause de phénomènes comme la supraconductivité, où les électrons forment des paires de Cooper. C'est aussi ce qui explique les différences de comportement entre les différents isotopes de l'hélium : dans un atome d'hélium 4 (He), chaque particule est présente en double (deux électrons, deux protons et deux neutrons, formant des paires de Cooper), ce qui fait de cet atome un boson. Ce qui n'est pas le cas dans l'atome d'hélium 3 (He), qui n'a qu'un neutron, ce qui fait de cet atome un fermion ; qui peut s'associer à un autre atome d'hélium 3 pour former un boson d'une paire de Cooper.

Le caractère bosonique ou fermionique des particules est lié à leur spin, par ce qu'on appelle le théorème spin-statistique.

Parmi les systèmes que l'on peut résoudre analytiquement en mécanique quantique, l'un d'entre eux a une importance particulière tant sur le plan historique que théorique. Il s'agit de l'oscillateur harmonique.

En mécanique classique, l'oscillateur harmonique est un système de grande importance car il constitue une bonne approximation de n'importe quel système stable autour d'une position d'équilibre. Dans un système d'unités adéquat, l'équation énergétique s'écrit :

formula_141

Où formula_142 et formula_143 sont respectivement l'impulsion et la position du mobile.

En mécanique quantique, l'équation est formellement la même, mais les grandeurs impliquées sont de nature différente. Au lieu d'être des scalaires réels dépendant du temps, l'impulsion et la position sont des opérateurs linéaires sur l'espace vectoriel des états. Ces grandeurs peuvent être manipulées de manière algébrique comme avec des scalaires normaux, à ceci près qu'il s'agit d'une algèbre non commutative. Il faut donc prêter attention aux commutateurs entre les opérateurs concernés. En l'occurrence, le commutateur entre formula_142 et formula_143 est :

formula_146

La résolution du système passe alors par une factorisation inspirée de l'identité remarquable formula_147. En se rappelant que formula_148, on introduit donc deux opérateurs (à un facteur de normalisation formula_149 près) :

formula_150

Pour des raisons qui apparaissent en cours de calcul (cf article détaillé), ces opérateurs sont appelés opérateurs respectivement de création et d'annihilation de quanta, ou encore opérateurs d'échelle. Ensuite, un raisonnement par récurrence permet de montrer le caractère quantifié des niveaux d'énergie possible, et de calculer leurs valeurs. Ces quanta sont l'analogue mécanique des photons, et à ce titre ils sont parfois appelés phonons.

Cette introduction d'opérateurs de création et d'annihilation est une technique assez emblématique de la physique quantique. On la retrouve par exemple dans la théorie du moment cinétique quantique ou en théorie quantique des champs.

L'un des systèmes les plus simples en mécanique quantique est la particule libre, dont l'énergie est réduite à sa composante cinétique. L'équation de Schrödinger s'écrit alors :

Les solutions sont de la forme :

L'effet tunnel désigne la propriété que possède un objet quantique de franchir une barrière de potentiel même si son énergie est inférieure à l'énergie minimale requise pour franchir cette barrière. C'est un effet purement quantique, qui ne peut pas s'expliquer par la mécanique classique. Pour une telle particule, la fonction d'onde, dont le carré du module représente la densité de probabilité de présence, ne s'annule pas au niveau de la barrière, mais s'atténue à l'intérieur de la barrière, pratiquement exponentiellement pour une barrière assez large. Si, à la sortie de la barrière de potentiel, la particule possède une probabilité de présence non nulle, elle peut traverser cette barrière. Cette probabilité dépend des états accessibles de part et d'autre de la barrière ainsi que de l'extension spatiale de la barrière.

Historiquement, le spin de l'électron est d'abord un phénomène expérimental observé notamment lors de l'expérience de Stern et Gerlach. En substance, il apparaît comme une sorte de très faible moment magnétique n'admettant que deux valeurs possibles, qui sont opposées et qui ne varient pas continûment selon l'axe de mesure. Il s'agit donc d'une grandeur qui ne respecte pas, du moins en apparence, les lois spatiales de la trigonométrie, tout en étant directionnelle. Ces observations assez curieuses n'ont pu être expliquées que par la mécanique quantique.

Le spin de l'électron est donc une grandeur "a priori" directionnelle qui ne peut prendre que deux valeurs de magnitude égales et de sens opposées. Les états quantiques correspondant sont alors en général notés formula_153 et formula_154. Ces états dépendent d'un axe d'observation particulier, traditionnellement placé verticalement, c'est-à-dire selon l'axe formula_155.

Avec un choix d'unités adéquat, cela signifie que pour un électron dans l'état formula_153, la mesure du moment magnétique de spin selon formula_155 donnera à coup sûr +1 comme résultat de mesure. De la même façon un électron dans l'état formula_154 donnera nécessairement -1 comme résultat de mesure selon ce même axe.

Dès lors, formula_153 et formula_154 forment la base d'un espace vectoriel de dimension deux, et l'observable associée à la mesure du spin selon l'axe formula_155 s'écrit alors, en représentation matricielle :

Par application du principe de superposition, toute superposition linéaire de formula_153 et formula_154 est aussi un état possible pour l'électron. Parmi ces combinaisons linéaires, il en est qui sont les vecteurs propres de deux matrices formula_166 et formula_167 :

formula_166, formula_167 et formula_171 forment avec la matrice unité ce qu'on appelle les matrices de Pauli.

La considération d'un vecteur unitaire formula_172 et de l'observable : formula_173 permet alors de faire apparaître la valeur moyenne suivante de formula_174 pour l'état formula_153 :

où formula_177 est l'angle éloignant formula_178 de l'axe formula_155.

Autrement dit, dès lors que formula_166 et formula_167 sont associés aux observables liées à la mesure du spin selon les axes formula_182 et formula_183, alors les règles de trigonométries apparaissent, mais avec une signification probabiliste. C'est là un résultat typique de la mécanique quantique.

Le spin de l'électron joue un rôle très important en mécanique quantique, d'une part parce que c'est un phénomène qui n'a pas d'équivalent classique, et d'autre part parce que c'est l'un des systèmes quantiques les plus simples dans la mesure où il n'a que deux états (ou plus précisément, que son espace vectoriel est de dimension deux). À ce titre il est souvent utilisé comme modèle d'étude pour des systèmes plus complexes, même lorsque le phénomène physique sous-jacent est complètement différent. L'exemple emblématique est le modèle d'Ising.

Richard Feynman, dans sa thèse en 1942, introduit la notion d'intégrale de chemin afin de présenter une nouvelle formulation de la mécanique quantique. Ces résultats ne seront publiés qu'en 1948 en raison de la seconde guerre mondiale. À terme, le but de cette approche serait de formuler une théorie de l'électrodynamique quantique en développant la quantification par intégrale de chemin. Si de nos jours on retient le formalisme Hamiltonien de la mécanique quantique pour traiter des problèmes classiques (au sens non relativiste), il s'avère que la formulation de Feynman est largement prédominante pour traiter les problèmes relativistes notamment en théorie quantique des champs, l'avantage provenant du fait que cette approche est non perturbative.

Par ailleurs, en 1953, Feynman appliqua son approche pour formuler la par intégrale de chemin (intégrale de Wiener, ) et tenta d'expliquer la transition lambda dans l'hélium superfluide.

La mécanique quantique est une théorie « non relativiste » : elle n'incorpore pas les principes de la relativité restreinte. En appliquant les règles de la quantification canonique à la relation de dispersion relativiste, on obtient l'équation de Klein-Gordon (1926). Les solutions de cette équation présentent toutefois de sérieuses difficultés d'interprétation dans le cadre d'une théorie censée décrire « une » seule particule : on ne peut notamment pas construire une « densité de probabilité de présence » partout positive, car l'équation contient une dérivée temporelle seconde. Dirac cherchera alors une autre équation relativiste du « premier ordre en temps », et obtiendra l'équation de Dirac, qui décrit très bien les fermions de spin un-demi comme l'électron.

La théorie quantique des champs permet d'interpréter toutes les équations quantiques relativistes sans difficulté.

L'équation de Dirac incorpore naturellement l'invariance de Lorentz avec la mécanique quantique, ainsi que l'interaction avec le champ électromagnétique mais qui est traité encore de façon classique (on parle d'approximation semi-classique). Elle constitue la mécanique quantique relativiste. Mais du fait précisément de cette interaction entre les particules et le champ, il est alors nécessaire, afin d'obtenir une description cohérente de l'ensemble, d'appliquer la procédure de quantification également au champ électromagnétique. Le résultat de cette procédure est l'électrodynamique quantique dans laquelle l'unité entre champ et particule est encore plus transparente puisque désormais la matière elle aussi est décrite par un champ. L'électrodynamique quantique est un exemple particulier de théorie quantique des champs.

D'autres théories quantique des champs ont été développées par la suite au fur et à mesure que les autres interactions fondamentales ont été découvertes (théorie électrofaible, puis chromodynamique quantique).

Les relations d'incertitude de Heisenberg traduisent l'impossibilité de préparer un état quantique correspondant à des valeurs précises de certains couples de grandeurs conjuguées. Ceci est lié au fait que les opérateurs quantiques associés à ces grandeurs classiques « ne commutent pas ».

Les inégalités de Heisenberg sont très fréquemment désignées par l'expression « principe d'incertitude ». "Stricto sensu", cette appellation est trompeuse : ces inégalités ne sont pas un principe car elles sont parfaitement démontrées grâce à l'analyse de Fourier, et elles ne concernent pas des incertitudes au sens commun du terme mais une indétermination intrinsèque, propre à la nature aléatoire de la mécanique quantique.

Considérons par exemple la position formula_184 et l'impulsion formula_185 d'une particule. En utilisant les règles de la quantification canonique, il est facile de vérifier que les opérateurs de position et d'impulsion vérifient :

La relation d'incertitude est définie à partir des écarts quadratiques moyens de grandeurs conjuguées. Dans le cas de la position formula_184 et de l'impulsion formula_185 d'une particule, elle s'écrit par exemple :

Plus l'état possède une distribution resserrée sur la position, plus sa distribution sur les valeurs de l'impulsion qui lui est associée est large. Cette propriété rappelle le cas des ondes, via un résultat de la transformée de Fourier, et exprime ici la dualité onde-corpuscule. Il est clair que ceci mène à une remise en cause de la notion classique de trajectoire comme chemin continu différentiable.

Il existe également une relation d'incertitude portant sur l'énergie d'une particule et la variable temps. Ainsi, la durée formula_190 nécessaire à la détection d'une particule d'énergie formula_191 à formula_192 près vérifie la relation :

Cependant, la dérivation de cette inégalité énergie-temps est assez différente de celle des inégalités position-impulsion.

En effet, si le hamiltonien est bien le générateur des translations dans le temps en mécanique hamiltonienne, indiquant que temps et énergie sont conjuguées, il n'existe pas d'opérateur temps en mécanique quantique (« théorème » de Pauli), c'est-à-dire qu'on ne peut pas construire d'opérateur formula_194 qui obéirait à une relation de commutation canonique avec l'opérateur hamiltonien formula_195 :

ceci pour une raison très fondamentale : la mécanique quantique a en effet été inventée pour que chaque système physique stable possède un « état fondamental d'énergie minimum ». L'argument de Pauli est le suivant : si l'opérateur temps existait, il posséderait un spectre continu. Or, l'opérateur temps, obéissant à la relation de commutation canonique, serait aussi le générateur des « translations en énergie ». Ceci entraîne alors que l'opérateur hamiltonien posséderait lui aussi un « spectre continu », en contradiction avec le fait que l'énergie de tout système physique stable se doit d'être "bornée inférieurement".

La notion d'intrication quantique intervient dès lors que deux systèmes formula_197 et formula_198 sont considérés dans leur ensemble comme formant un seul et unique système formula_199. Cette assertion peut être vérifiée par exemple dans le cas simple où les espaces d'état de formula_197 et formula_198 ont pour bases les vecteurs propres formula_202 et formula_203 de deux observables formula_117 et formula_205 agissant respectivement sur formula_197 et formula_198. 

formula_117 et formula_205 agissent nécessairement aussi sur formula_199 puisque formula_199 est constitué de la réunion de formula_197 et formula_198. On peut donc noter formula_214 le vecteur d'état de formula_199 tel que dans cet état la mesure de formula_117 donne à coup sûr formula_217 et la mesure de formula_205 donne à coup sûr formula_219.

D'après le principe de superposition, toutes les combinaisons linéaires des vecteurs d'état formula_214 sont des états possibles du système. Or, il existe formula_221 tels vecteurs, et donc l'espace vectoriel qu'ils engendrent est au moins de dimension formula_221. Dans le cas général, cette dimension est supérieure à formula_223, c'est-à-dire au nombre de degrés de libertés nécessaires pour décrire les systèmes formula_197 et formula_198 considérés séparément.

Il apparaît donc que dans le cas général la description complète des deux systèmes dans leur ensemble ne peut être réduite à celle des deux systèmes pris séparément. Autrement dit, il existe des états de formula_199 tels qu'il n'existe aucun état de formula_197 ni aucun état de formula_198, c'est-à-dire aucune combinaison linéaire des formula_202 ni aucune combinaison linéaire des formula_203 qui permettent d'obtenir les probabilités de résultats de mesure. De tels états de formula_199 sont alors dit "intriqués". Un tel exemple d'état intriqué est :

Deux systèmes ou deux particules peuvent être intriqués dès qu'il existe une interaction entre eux. En conséquence, les états intriqués sont la règle plutôt que l'exception. Une mesure effectuée sur l'une des particules changera son état quantique selon le postulat quantique de la mesure. Du fait de l'intrication, cette mesure aura un effet instantané sur l'état de l'autre particule, même si la ligne d'univers qui relie les deux évènements « "mesure 1" » et « "mesure 2" » de l'espace-temps est une courbe de genre espace ! Par suite, le fait que la mécanique quantique tolère l'existence d'états intriqués, états ayant effectivement été observés en laboratoire et dont le comportement est en accord avec celui prévu par la mécanique quantique (voir l'expérience d'Aspect), implique que la mécanique quantique est une théorie physique non locale. Néanmoins, il est incorrect d'assimiler ce changement d'état à une transmission d'information plus rapide que la vitesse de la lumière (et donc une violation de la théorie de la relativité). La raison est que le résultat de la mesure relatif à la première particule est toujours aléatoire, dans le cas des états intriqués comme dans le cas des états non intriqués. Il est donc impossible de « transmettre » quelque information que ce soit, puisque la modification de l'état de l'autre particule, pour immédiate qu'elle soit, conduit à un résultat de la mesure relatif à la seconde particule qui est toujours aussi aléatoire que celui relatif à la première particule. Les corrélations entre les mesures des deux particules, bien que très réelles et mises en évidence dans de nombreux laboratoires de par le monde, resteront indétectables tant que les résultats des mesures ne seront pas comparés, ce qui implique nécessairement un échange d'information classique, respectueux de la Relativité (voir aussi le Paradoxe EPR).

La téléportation quantique fait usage de l'intrication pour assurer le transfert de l'état quantique d'un système physique vers un autre système physique. Ce processus est le seul moyen connu de transférer parfaitement l'information quantique. Il ne peut dépasser la vitesse de la lumière et est également « désincarné », en ce sens qu'il n'y a pas de transfert de matière (contrairement à la téléportation fictive de Star Trek).

Cet état ne doit pas être confondu avec l'état de « superposition ». Un même objet quantique peut avoir deux (ou plus) états « superposés ». Par exemple un même photon peut être dans l'état « polarité longitudinale » et « polarité transversale » simultanément. Le chat de Schrödinger est simultanément dans l'état « mort » et « vivant ». Un photon qui passe une lame semi-réfléchissante est dans l'état superposé « photon transmis » et « photon réfléchi ». C'est uniquement lors de l'acte de mesure que l'objet quantique possédera un état déterminé.

Dans le formalisme de la physique quantique, un état d'intrication de « plusieurs objets quantiques » est représenté par un produit tensoriel des vecteurs d'état de chaque objet quantique. Un état de superposition ne concerne qu'« un seul objet quantique » (qui peut être une intrication), et est représenté par une combinaison linéaire des différentes possibilités d'états de celui-ci.

On ne peut déterminer l'état d'un système quantique qu'en l'observant, ce qui a pour effet de détruire l'état en question. Celui-ci peut en revanche, une fois connu, être en principe recréé ailleurs. En d'autres termes, la « duplication » n'est pas possible dans le monde quantique, seule l'est une « reconstruction en un autre endroit », voisine du concept de téléportation dans la science-fiction.

Élaborée théoriquement en 1993 par C.H. Bennett, G. Brassard, C. Crépeau, R. Jozsa, A. Peres, et W. Wootters dans l'article "Teleporting an unknown quantum state by dual classical and EPR channels", de la "Physical Review Letter", cette reconstruction a été réalisée expérimentalement en 1997, sur des photons, par l'équipe d'Anton Zeilinger à Innsbruck, et plus récemment sur des atomes d'hydrogène.

Ces « paradoxes » nous questionnent sur l'interprétation de la mécanique quantique, et révèlent dans certains cas à quel point notre intuition peut se révéler trompeuse dans ce domaine qui ne relève pas directement de l'expérience quotidienne de nos sens.

Ce paradoxe (1935) met en évidence les problèmes d'interprétation du postulat de réduction du paquet d'onde.

Ce paradoxe (1935) met en évidence la non-localité de la physique quantique, impliquée par les états intriqués.

Cette expérience peut être interprétée comme une démonstration que les résultats d'une expérience enregistrée à un instant T dépendent objectivement d'une action effectuée à un temps ultérieur T+t. Selon cette interprétation, la non-localité des états intriqués ne serait pas seulement spatiale, mais également temporelle.

Toutefois, la causalité n'est pas strictement violée car il n'est pas possible — pour des raisons fondamentales — de mettre en évidence, avant l'instant T+t, que l'état enregistré à l'instant T dépend d'un évènement ultérieur. Ce phénomène ne peut donc donner aucune information sur l'avenir.

Selon la mécanique quantique, des évènements qui « auraient pu se produire, mais qui ne se sont pas produits » influent sur les résultats de l'expérience.

Alors que les principes de la mécanique quantique s'appliquent a priori à tous les objets contenus dans l'univers (nous y compris), pourquoi continuons-nous à percevoir classiquement l'essentiel du monde macroscopique ? En particulier, pourquoi les superpositions quantiques ne sont-elles pas observables dans le monde macroscopique ? La théorie de la décohérence explique leurs disparitions très rapides en raison du couplage inévitable entre le système quantique considéré et son environnement.

Cette théorie a reçu une confirmation expérimentale avec les études portant sur des systèmes mésoscopiques pour lesquels le temps de décohérence n'est pas trop court pour rester mesurable, par exemple un système de quelques photons dans une cavité.



Accessibles au niveau d'un premier cycle universitaire.


Accessibles à partir du second cycle universitaire.


Accessible sans bagage physique préalable.







Il existe de nombreuses interprétations des effets de la mécanique quantique, certaines étant en contradiction totale avec d'autres. Faute de conséquences observables de ces interprétations, il n'est pas possible de trancher en faveur de l'une ou de l'autre de ces interprétations. Seule exception, l'école de Copenhague dont le principe est justement de refuser toute interprétation des phénomènes.










</doc>
<doc id="1917" url="https://fr.wikipedia.org/wiki?curid=1917" title="Maurice Leblanc">
Maurice Leblanc

Marie Émile Maurice Leblanc est un écrivain français né le , à Rouen, et mort le , à Perpignan. Auteur de nombreux romans policiers et d’aventures, il est le créateur du célèbre personnage d’Arsène Lupin, le "gentleman"-cambrioleur. Relégué au rang de « Conan Doyle français », Maurice Leblanc est un écrivain populaire qui a souffert de ne pas avoir la reconnaissance de ses confrères mais a toujours suscité un solide noyau d'amateurs et de quelques lupinologues.

On peut visiter la maison de Maurice Leblanc, le "Clos Lupin" à Étretat, dans la Seine-Maritime. L’aiguille d’Étretat forme d’ailleurs l’un des décors du roman "L'Aiguille creuse". La revue d'études lupiniennes "L'Aiguille preuve" est éditée annuellement par l'Association des Amis d'Arsène Lupin (AAAL) fondée en 1985 par le philosophe et essayiste François George.

Maurice Leblanc est le deuxième enfant d'Émile Leblanc, négociant armateur de trente-quatre ans, et de Mathilde Blanche, née Brohy, fille de riches teinturiers, âgée de vingt et un ans et qui fut accouchée par Achille Cléophas Flaubert, père de Gustave Flaubert. Il a pour sœur aînée Jehanne née en 1863, et pour sœur cadette la cantatrice Georgette Leblanc, qui fut l'interprète de Maurice Maeterlinck et sa compagne de 1895 à 1918. 

Pendant la guerre franco-allemande de 1870, son père l'envoie en Écosse où les paysages ont dû fertiliser son imagination. De retour, il achève ses études à Rouen. Le jeune Maurice reçoit sa première éducation dans une institution libre, la pension Patry, puis, de 1875 à 1882, fait ses études secondaires au lycée Corneille. Adolescent, il fréquente Gustave Flaubert et Guy de Maupassant. Refusant la carrière que son père lui destine dans une fabrique de cardes, il « monte à Paris », en 1888, pour écrire. D’abord journaliste, puis romancier et conteur (son premier roman "Une femme" très remarqué en 1893, "Des couples", "Voici des ailes", son unique pièce "La pitié" en 1902 est un échec, le faisant renoncer au théâtre), il éveille l’intérêt de Jules Renard et d'Alphonse Daudet, sans succès public. Il fréquente les grands noms de la littérature à Paris : Stéphane Mallarmé ou Alphonse Allais. En 1901, il publie "L'Enthousiasme", roman autobiographique. 

En 1905, Pierre Lafitte, directeur du mensuel "Je sais tout", lui commande une nouvelle sur le modèle du "Raffles" d'Ernest William Hornung et des aventures de Sherlock Holmes : "L'Arrestation d’Arsène Lupin" se révèle un grand succès public mais Maurice Leblanc souffre déjà de n'avoir pas la reconnaissance des gens de lettres. Deux ans plus tard, "Arsène Lupin, gentleman-cambrioleur" est publié en livre. La sortie d’"Arsène Lupin contre Herlock Sholmès" mécontente Conan Doyle, furieux de voir son détective Sherlock Holmes (« Herlock Sholmès ») et son faire-valoir Watson (« Wilson ») ridiculisés par des personnages parodiques créés par Maurice Leblanc. 
Maurice Leblanc reçoit la Légion d'honneur, le 17 janvier 1908, des mains du sous-secrétaire d’État aux Beaux-Arts, Étienne Dujardin-Beaumetz, député radical de l’Aude.

Radical-socialiste et libre-penseur, Leblanc s’embourgeoise avec l’âge et la Première Guerre mondiale. Il aurait déclaré : Dès 1910, il tente de tuer son héros dans "813", mais il le ressuscite dans "Le Bouchon de cristal", "Les Huit Coups de l'horloge"…

En 1918, Maurice Leblanc achète à Étretat une maison à colombages de facture anglo-normande où il y rédige 19 romans et 39 nouvelles. Devant l'occupation allemande, il quitte Le Clos Lupin et se réfugie en 1939 à Perpignan où il meurt d'une pneumonie. Exhumé du cimetière Saint-Martin de Perpignan en 1947, il est réinhumé, le 14 octobre de cette année-là, à Paris, au cimetière du Montparnasse, aux côtés de sa femme Marguerite et d'autres membres de sa famille (notamment son beau-frère René Renoult).

Fin 1888, Maurice Leblanc se décide à quitter Rouen pour Paris où il se marie le 10 janvier 1889 à Marie-Ernestine Lalanne (1865-1941) qui lui donne une fille, Louise Amélie Marie Leblanc (1889- 1974) qui n'aura aucune postérité de son mariage. Les deux jeunes gens s'aperçoivent vite qu'ils ne s'entendent pas et divorcent en 1895. L'écrivain tombe ensuite amoureux de Marguerite Wormser (1865-1950) qui a déjà un fils Claude Oulmann (1902-1994), lequel sera autorisé à porter le nom de Leblanc par décret. La procédure de divorce entamée par Marguerite contre son premier époux traînant en longueur, Maurice a des ennuis de santé et sombre dans la dépression. Ils ne se marient que le 31 janvier 1906.

Une Association des amis d’Arsène Lupin est fondée en 1985 par son ancien élève le philosophe François George.

Son œuvre a inspiré Gaston Leroux (créateur de Rouletabille), ainsi que Souvestre et Allain (créateurs de Fantômas). Les exploits d’Arsène Lupin se déroulaient dans la capitale et dans le pays de Caux, que Maurice Leblanc connaissait bien : collectionneur de cartes postales, il avait recensé pas moins de quatre cents manoirs entre Le Havre, Rouen et Dieppe. Les « lupinophiles » arpentent les lieux cités dans les intrigues de Leblanc en Normandie : Étretat et le trésor des rois de France, Tancarville, le passage souterrain de Jumièges devant mener au trésor médiéval des abbayes, etc. Selon les lupinophiles mythomanes, la piste des sept abbayes du pays de Caux reliées entre elles dessinerait la Grande Ourse et permetrait de retrouver l’étoile d’Alcor.

La série "Arsène Lupin" compte 17 romans et 39 nouvelles, ainsi que 5 pièces de théâtre, tous écrits de 1905 à 1941.







</doc>
<doc id="1921" url="https://fr.wikipedia.org/wiki?curid=1921" title="Meringue">
Meringue

La meringue est une pâtisserie très légère et très fine composée uniquement d'un mélange de blancs d'œufs et de sucre. Un peu d'acide peut être ajouté sous forme de citron ou d'acide tartrique.

Des variantes existent en changeant les quantités et le type de sucre mais surtout la façon de l'incorporer à la masse.

On découvre une recette de « biscuits de sucre en neige », dans le « Pasticier » de la Varenne, mais le nom de « meringue » apparaît en 1692 dans le livre de cuisine de François Massialot. D'après Alain Rey, l'origine du mot est controversée :

L'origine du mot « meringue » lui-même n'est pas connue avec certitude :





La meringue utilise la capacité du blanc d'œuf à enfermer des bulles d'air dans un réseau protéique (foisonnement). La dose de sucre est d'environ par blanc d'œuf en supposant un blanc d'un poids moyen de et sachant qu'un blanc peut dissoudre seulement jusqu'à de sucre.

Il y a trois modes différents de préparation se prêtant à différents usages :

La meringue française est la plus facile à faire. Elle se réalise en battant des blancs d'œufs avec du sucre semoule.

Les blancs sont battus seuls et lorsqu'ils sont bien mousseux, le sucre est alors incorporé tout en continuant à battre jusqu'à consistance bien ferme.

Pour ce type de meringues on préfère parler de séchage plutôt que de cuisson. Il y a deux façons différentes de procéder :

Pour des meringues bien blanches, le séchage se fait à basse température. La préparation est mise au four à pendant environ une minute pour des meringues petites à cœur tendre et jusqu'à plusieurs heures pour une préparation plus croustillante et surtout pour des grosses pièces. Dans le temps, les pâtissiers la cuisaient tout doucement toute une nuit. C'est la façon dite « à l'ancienne » ou « de Meiringen ».

Pour des meringues dorées et plus goûteuses, les meringues sont saisies à four chaud à une température de maximum pendant une vingtaine de minutes puis baissée autour de pour une durée pouvant aller jusqu'à deux heures et demies en fonction du poids du produit. La masse est légèrement moins blanche car le sucre effectue une légère caramélisation. Mise au point par Angelo Rime à Botterens en Suisse dans les années soixante, c'est la façon dite « de Botterens ».

La consistance croustillante de la meringue en fait une pâtisserie à part entière destinée surtout à être consommée telle quelle mais entre aussi dans la préparation de pâtisseries composées par exemple en fourrage sous forme entière ou en petits morceaux, rajoutant ainsi son croustillant bien spécial à d'autres pâtisseries. Les meringues peuvent être présentées couplées dos à dos avec une crème fouettée ou une crème glacée entre les deux. Normalement de couleur blanche, elles peuvent aisément être colorées pour créer par exemple des contrastes de couleurs avec d'autres ingrédients d'une pâtisserie composée. D'un goût plutôt neutre, elles se prêtent bien aussi à être aromatisées, le plus classiquement au cacao, à la vanille, à la noix de coco, au citron et décorées avec des amandes effilées.

Elles peuvent se conserver deux semaines en conditions optimales et jusqu'à trois mois au congélateur mais toujours bien au sec.

La meringue suisse se réalise en montant des blancs d'œufs en neige avec du sucre glace sur un bain-marie.

Les blancs sont d'abord battus seuls au bain-marie puis, lorsque la préparation devient mousseuse, le sucre glace est incorporé en continuant à battre. Lorsque le mélange prend consistance, le battage est finalisé hors du feu.

L'aspect fini est lisse et brillant.

Elle est cuite une quinzaine de minutes à four plus chaud que pour la meringue française, aux alentours de .

Sa consistance très ferme et moins friable que la préparation française permet de l'utiliser à la confection de décors. Les sujets sont formés sur plaque avant cuisson.

La durée de conservation peut aller jusqu'à trois semaines en bonne condition.

La meringue italienne se réalise en montant des blancs d'œufs en neige avec du sucre cuit. La meringue finale elle-même ne se cuit pas. Contrairement à ce que son nom peut laisser penser, elle est typiquement issue de la pâtisserie française.

Cuire préalablement le sucre au petit boulé (~). Battre les blancs jusqu'à ce qu'ils moussent puis verser le sucre en continuant à battre, et ce jusqu'à ce que l'appareil prenne en mousse serrée.

L'aspect fini est brillant et bien compact quoique très léger.

Elle sert à alléger d'autres préparations comme les mousses, les soufflés, la crème pâtissière. Elle sert de chemisage à d'autres pâtisseries comme tartes, entremets meringués ou omelette norvégienne. C'est aussi le principe de base de la confection des guimauves.

Pour un aspect plus appétissant elle peut être brièvement dorée à la salamandre, au chalumeau ou au tison. Elle ne se conserve pas plus d'une heure en condition de froid sec.

Lors de la préparation, le battage des blancs d'œuf rompt certaines liaisons hydrogène par cisaillement mécanique. C'est ce qui s'appelle une dénaturation des protéines. La pointe d'acide parfois ajoutée agit dans le même sens. Les protéines ont alors une structure leur permettant d'emprisonner des bulles d'air dans la masse. La difficulté est de ne pas battre plus que le temps nécessaire à l'obtention d'un appareil bien ferme. Au-delà, la pâte peut retomber car l'ovalbumine passe dans une troisième phase incapable de retenir les bulles. Une fois que cette phase est atteinte, il n'est plus possible de revenir en arrière et de les refaire monter car c'est la structure protéique elle-même qui a changé.

La moindre goutte de matière lipidique inhibe la formation de la structure alvéolaire si bien que, lors de la séparation des jaunes et des blancs d'œuf, une seule goutte de jaune - qui est un lipide - diminue la capacité de « montage » de moitié. C'est aussi pour cette raison que si l'aromatisation éventuelle se fait sous forme de concentré lipidique, elle n'est effectuée que juste à la fin du battage. Les bols en plastique sont souvent déconseillés car ils peuvent, par affinité, retenir en surface quelques traces de matières grasses même après lavage.

Le froid aide à la bonne tenue de la « pâte » c'est pourquoi le battage est parfois effectué en plaçant le récipient dans un « bain-marie » glacé.

Le sucre est indispensable à la bonne tenue de la meringue. Il n'est de fait pas possible de faire de meringues avec les édulcorants les plus courants. Au-delà de le sucre et les protéines interagissent pour amorcer une réaction de Maillard et donne au produit fini un aspect pouvant aller du gris terne à l'ambré.

Le blanc et le sucre sont deux matières assez hygroscopiques, c'est pourquoi il est indispensable pour la conservation d'abriter le produit fini de toute humidité en le plaçant dans une boite hermétique et un local sec.

En allemand, une meringue se nomme "Baiser" (sauf en Suisse) ; le mot est neutre, on dit « das Baiser » ou Küsschen. Les "Luxemburgerli" suisses se nommaient originellement des "Baisers de Mousse".



</doc>
<doc id="1922" url="https://fr.wikipedia.org/wiki?curid=1922" title="Liste d'ensembles de musique médiévale">
Liste d'ensembles de musique médiévale

La liste des ensembles de musique médiévale regroupe des formations de musique ancienne vocale, instrumentale ou mixte, spécialisés dans le répertoire de la musique médiévale profane ou religieuse. 














</doc>
<doc id="1923" url="https://fr.wikipedia.org/wiki?curid=1923" title="Malicorne (groupe)">
Malicorne (groupe)

Malicorne est un groupe français de musique folk (puis de folk rock et de rock progressif) formé à l'automne 1973 par le couple Gabriel Yacoub (guitare acoustique et électrique, épinette des Vosges, chant) et Marie Yacoub (dulcimer, bouzouki, vielle à roue, chant), Laurent Vercambre (violon, alto, bouzouki, psaltérion à archet, harmonium, mandoline, chant) et Hughes de Courson (guitare électrique, basse, cromorne, percussions, chant). Ils seront rejoints en 1976 par Olivier Zdrzalik-Kowalski (basse, claviers, chant).

Après une première séparation fin 1981, le groupe se reforme à l'été 1984 pour une tournée principalement nord-américaine avec de nouveaux musiciens accompagnant le couple fondateur, puis en 1986 avec d'autres musiciens accompagnateurs pour l'enregistrement d'un ultime album studio et pour une ultime tournée en 1987–1989.

Malicorne se reforme dans sa configuration originelle (dite "formation classique") à l'occasion d'un concert unique donné le 15 juillet 2010 dans le cadre du festival des Francofolies de La Rochelle.

Avec de nouveaux musiciens accompagnateurs, le couple fondateur Gabriel Yacoub et Marie Sauvet forme fin novembre 2011 un nouveau groupe sous le nom de « Gabriel et Marie de Malicorne » (renommé simplement « Malicorne » dès septembre 2012) qui se lance dans deux projets : la mise en place dès l'été 2012 d'une importante tournée de concerts en France (et à l'étranger – en Belgique en 2013, aux Pays-Bas en 2014 et en Suisse en 2015) intitulée "Almanach Tour" et l'enregistrement dès le printemps 2013 d'un nouvel album studio « qui sera constitué uniquement de titres inédits » à la publication initialement prévue pour 2013 mais sans cesse repoussée. 

Seulement une semaine avant l'événement, le groupe annonce qu'il donnera un ultime concert le à Paimpol en Bretagne.

Le projet de nouvel album est définitivement abandonné concomitamment au dernier concert donné à Paimpol.

Futur membre fondateur de Malicorne et véritable pilier du groupe, le chanteur et guitariste Gabriel Yacoub fait ses débuts sur scène en 1969 au sein du trio New Ragged Company aux côtés de Youra Marcus et Phil Fromont.

Fin 1971, à 20 ans à peine, Gabriel rejoint le groupe d'Alan Stivell à la guitare, au banjo, au dulcimer et au chant. Il participera à l'enregistrement de deux albums déterminants du harpiste breton "À l'Olympia" (1972) et "Chemins de terre" (1973) à l'origine du renouveau de la musique celtique en France. N'étant pas lui-même de culture bretonne, il décide à la fin de l'été 1973 de quitter la formation de Stivell afin d'interpréter des chansons issues du patrimoine francophone plutôt que de chanter d'une manière artificielle, voire factice, en breton ou en langues gaéliques.

Au printemps 1973, Gabriel et Marie Yacoub enregistrent (avec notamment Dan Ar Braz) l'album expérimental "Pierre de Grenoble" par lequel le couple revisite le répertoire traditionnel. Sorti en octobre 1973, le succès immédiat et inattendu de ce coup d'essai sera à l'origine du renouveau des musiques traditionnelles en France.

Interprétant d'une manière originale, personnelle et actuelle des airs et chansons empruntés au répertoire traditionnel, le groupe connait le succès tout au long des années 1970 avec la publication d'une dizaine d'albums.

À la fin de l'été 1973, Gabriel Yacoub quitte définitivement la formation d'Alan Stivell pour voler de ses propres ailes.

Avant même la sortie de leur premier album "Pierre de Grenoble", Gabriel et Marie Yacoub fondent Malicorne le 5 septembre 1973 en compagnie de deux autres musiciens Laurent Vercambre et Hugues de Courson.

Le nom du groupe provient du pur hasard : c'est en se rendant en Bretagne pour donner un concert fin juillet 1973 à Moëlan-sur-Mer à l'occasion du festival de Kertalg que le couple fondateur Gabriel et Marie Yacoub est amené à l'approche du Mans à passer par hasard à Malicorne-sur-Sarthe à la suite d'une déviation. Hughes qui accompagne alors le couple a l'idée de retenir ce nom pour le groupe, alors encore en gestation. Gabriel Yacoub raconte : « Une déviation nous a fait passer à Malicorne-sur-Sarthe. C'est dur de trouver un nom pour un groupe, et celui-là nous a paru très poétique, très évocateur ».

À peine formé, le groupe est rejoint en renfort par le bassiste Max Picout et se lance à l'automne 1973 dans une première tournée de quelques dates en Bretagne avec un tout premier concert à Quimper le 31 octobre puis trois autres concerts début novembre à Concarneau, Morlaix et Rennes.

Alors vendeur de disques, Daniel Bornet découvre l'album "Pierre de Grenoble" à sa sortie en octobre 1973. Un ami lui permet de rencontrer Gabriel Yacoub. Le groupe est alors en agence chez Michel Salou. De cette rencontre naîtra une véritable amitié et Daniel Bornet deviendra très rapidement l'agent historique et le manager du groupe jusqu'à sa dernière tournée à l'été 1989.

Au printemps 1974, la formation (réduite à nouveau à un quartet après le départ définitif du bassiste Max Picout) enregistre déjà aux studios Acousti son premier album "Malicorne 1" qui sera bien accueilli par le public à sa sortie en octobre. Fin 1974, le public fera également un accueil très chaleureux à la tournée française de promotion de l'album.

Au printemps 1975, le groupe enregistre déjà son second album "Malicorne 2" qui sort en novembre et connaît un succès plus grand encore, devenant disque d'or ( exemplaires vendus). Malicorne commence à se produire à l'étranger. Les Anglais découvrent ainsi ce groupe de folk music en juillet 1975 à l'occasion du Cambridge Folk Festival. À travers ses chansons soucieuses de l'écologie, Malicorne s'oppose .

Pierre Kerhervé remplace Hughes de Courson sur scène (à la basse, aux instruments à vent et au chant) de novembre 1975 à septembre 1976. 

Au printemps 1976, Malicorne est à nouveau aux studios Acousti pour enregistrer son troisième album, l'emblématique "Almanach". Publié en septembre 1976, l'album se présente comme un album-concept déclinant en douze titres traditionnels les , comme l'explique Gabriel Yacoub sur la pochette de l'album. "Almanach" décroche son premier Disque d'or en novembre 1977 pour exemplaires vendus.

Déjà, Laurent Vercambre et Hughes de Courson ne veulent plus participer au groupe. Ils quittent Malicorne en septembre 1976 mais reviendront à l'été 1977, riches d'expériences acquises hors du groupe qu'ils mettront à profit pour l'enregistrement de l'album suivant.

René Werneer remplace Laurent Vercambre sur scène au violon de septembre 1976 au printemps 1977 tandis qu'Olivier Zdrzalik-Kowalski remplace Hughes de Courson sur scène à partir de septembre 1976.

Le quatuor originel aura donc réalisé seul les trois premiers albums studio du groupe avant d'être rejoint durablement en septembre 1976 (principalement à la guitare basse et au chant) par Olivier Zdrzalik-Kowalski (présenté à Gabriel par Daniel Bornet qui le connaissait en tant que bassiste du groupe Komintern). Olivier participe à l'été 1977 au studio Normandie à l'enregistrement de "Malicorne 4", le quatrième album du groupe qui sort en octobre 1977. Il participera à l'enregistrement de tous les albums suivants et deviendra ainsi membre à part entière de Malicorne.

Claude Alvarez-Pereyre est, à son tour, amené à remplacer Laurent Vercambre sur scène au violon dans le courant de l'année 1977.

Fin 1977, Malicorne publie sa première compilation intitulée "Quintessence" comprenant "Martin", un titre jusque là inédit en album car auparavant disponible uniquement en single (sorti début 1975), qui deviendra l'un des titres les plus emblématiques du style et du son Malicorne.

À nouveau, Laurent Vercambre et Hugues de Courson ne veulent plus participer au groupe. Cette formation dite "classique" enregistre pourtant en 1978 son cinquième et dernier album studio, "L'Extraordinaire Tour de France d'Adélard Rousseau, dit Nivernais la clef des cœurs, Compagnon charpentier du devoir", second album-concept du groupe (après "Almanach").

En 1979, Malicorne publie "En public", son premier album live, enregistrement (partiel) de deux concerts donnés à Montréal au El Casino les 2 & 3 décembre 1978. Outre quelques-uns de leurs classiques issus des premiers albums et de "Pierre de Grenoble", ce live comporte quelques titres inédits en studio : un medley de reels, une suite de branles et « C'est le mai », une chanson en partie écrite par Gabriel.

Cette formation classique (avec ou sans Olivier Zdrzalik-Kowalski) aura publié au total 7 albums (parmi les plus marquants du groupe) dont la première compilation et le premier album live du groupe.

La publication en 1979 de l'album live "En public" coïncide avec le départ définitif de deux de ses membres originaux, Laurent Vercambre et Hughes de Courson, qui sont tous deux remplacés par Patrick Le Mercier au violon et à la guitare et par Jean-Pierre Arnoux à la batterie et aux percussions. 

L'anglais Brian Gulland aux vents et Dominique Regef à la vielle à roue et au violoncelle complètent la formation pour l'enregistrement de l'album "Le Bestiaire" (1979), troisième album-concept du groupe. Cette nouvelle formation (réduite à nouveau à un quintet avec le départ de Brian Gulland et Dominique Regef) s'éloignera du répertoire traditionnel en 1981 avec la publication de son second album "Balançoire en feu", fruit d'une collaboration inédite avec un membre extérieur au groupe, le parolier Étienne Roda-Gil, qui rencontrera un succès mitigé.

« [Sans] lassitude aucune mais plutôt l'impression d'avoir fait le tour du sujet », ayant « peur d'être moins créatifs » et conscients que leur « dernier album a dérouté [leurs] fans », les membres de Malicorne « d'un commun accord » décident d'arrêter : le groupe se sépare fin 1981 à l'issue d'une dernière tournée de concerts. Comme l'explique Gabriel Yacoub, « on a choisi de tirer notre révérence et puis chacun avait d'autres projets artistiques ou projets de vie. Marie voulait privilégier sa vie de famille et moi j'ai continué une carrière en solo qui m'a mené d'Europe aux USA en passant par le Canada en écrivant mes propres chansons mais sans renier la musique traditionnelle. »

Le groupe se reforme en 1984 le temps d'une tournée de concerts nord-américaine et de quelques dates en France puis à nouveau en 1986 autour de l'album "Les Cathédrales de l'industrie" et enfin en 1987–1989 pour une tournée d'adieu passant notamment en Belgique, en Angleterre et aux États-Unis (à laquelle Marie participera sur une partie seulement, limitée aux premiers concerts donnés fin 1987).

Le 15 juillet 2010, près de 29 ans après la première séparation du groupe et près de 22 ans après sa séparation définitive, sur l'insistance de Gérard Pont, le directeur des Francofolies de La Rochelle (« qui faisait le forcing depuis une quinzaine d’années pour programmer [le groupe] dans son festival »), Gabriel accepte de reformer Malicorne le temps d'un concert exceptionnel donné dans le cadre du festival réunissant sur scène pour la première fois en 31 ans les cinq membres de la formation classique soit les quatre membres fondateurs Gabriel Yacoub, Marie Sauvet, Laurent Vercambre et Hughes de Courson augmentés d'Olivier Zdrzalik-Kowalski (qui avait rejoint le groupe seulement en 1976). Cette formation classique est complétée par Yannick Hardouin (basse) et David Pouradier Duteil (batterie et percussions) et rejointe sur scène par de nombreux artistes invités à interpréter à leur façon les classiques du répertoire de Malicorne : Gilles Chabenat, J. P. Nataf, Le Quatuor, Michel Rivard, Karl Zéro, Tété, Claire Diterzi, Bensé et Jil Is Lucky en duo.

La publication en mars 2011 d'un album et d'un DVD tous deux intitulés "Concert exceptionnel aux Francofolies de la Rochelle" rend compte de cet évènement.

. Gabriel explique : « finalement, ce sera toujours Malicorne mais avec quatre nouveaux musiciens parmi lesquels de vieux complices comme Gilles Chabenat [à] la vielle » [ou Yannick Hardouin à la guitare basse] mais aussi l'arrivée dans le groupe d'un petit nouveau : l'accordéon diatonique « malicornisé » sous les doigts de Romain Personnat. »

Sur scène, Malicorne reprend « quelques "standards", histoire de ne pas décevoir ». Mais pas question pour le groupe « de se laisser enfermer dans le passé ou de s'imposer des limites. » Outre du « traditionnel [qu'il adapte] sans vergogne », le groupe « propose d'autres textes plus oniriques que [Gabriel écrit lui-même] en gardant « l'esprit Malicorne» ». À côté des airs du Berry où [Gabriel s'est] installé depuis longtemps, on retrouve « aussi diverses influences musicales allant de la Louisiane au Haut Moyen Âge en passant par Ravel, Debussy ou Fauré… »

En novembre 2011, 38 ans après que Gabriel et Marie Yacoub ont publié l'album "Pierre de Grenoble", le site web de Kerne Production (agence dirigée par Jean-Philippe Mauras jusqu'en 2013) annonce la formation d'un nouveau groupe sous le nom de « Gabriel et Marie de Malicorne » : outre le couple éponyme Gabriel Yacoub (chant, guitare, mandoloncelle) et Marie Sauvet (Marie de Malicorne sur scène) (chant, dulcimer, psaltérion), le groupe comptera Yannick Hardouin (claviers, basse acoustique, chant), Gilles Chabenat (vielle à roue électro-acoustique), tous deux collaborateurs de longue date de Gabriel (avec lequel ils jouent habituellement en trio), David Pouradier Duteil (percussions, chant) (déjà présent sur scène aux côtés de Malicorne lors du concert de La Rochelle) et un nouveau venu, Romain Personnat (accordéon diatonique, harmonium, chant).

Prenant modèle sur "Almanach", album publié en 1976, Kerne Production annonce deux projets :

En juillet 2013, Daniel Bornet reprend le management du groupe. Le "booking", jusque-là assuré par Kerne Production, est alors confié à Pyrprod.

Au cours de l'« Almanach Tour », le groupe se produit dans des salles plus ou moins grandes mais aussi en plein air lors de festivals d'été. Pendant les six premiers mois, sur huit dates, l'« Almanach Tour » passe pas moins de cinq fois par la Bretagne. Par la suite, la tournée se poursuit dans toute la France mais passe aussi dans trois pays limitrophes ou proches : en Belgique en 2013, aux Pays-Bas en 2014 et en Suisse en 2015.

Le 20 septembre 2012, la page Facebook de Malicorne annonce que le groupe de 6 musiciens Gabriel et Marie de Malicorne devient simplement « Malicorne », à la suite du fait que le couple éponyme s'est aperçu que le nouveau nom « ne marchait pas du tout », que « dans l'esprit des gens ça restait Malicorne ».

En mars 2013, Malicorne rentre en studio à Greneville-en-Beauce pour commencer l'enregistrement du nouvel album. Le groupe travaille cinq titres : "Soleillet de l'air en l'air", "Géant" (deux titres que le groupe s'était déjà essayé à interpréter sur scène à l'été 1984 lors de sa tournée américaine), le tout nouveau titre "Au bout du bois", une nouvelle version du classique "Pierre de Grenoble" et un cinquième titre (non dévoilé) laissé seulement à l'état de maquette.

Un musicien est présent sur scène lors du concert de Malicorne le 24 mai 2013 à Aubervilliers : collaborateur de longue date de Gabriel Yacoub, le guitariste Nicolaïvan Mingot avait déjà participé à l'enregistrement en mars 2013 des cinq premiers titres prévus pour le nouvel album. Malicorne joue à nouveau à seulement 6 musiciens au concert suivant le 6 juillet 2013 à l'occasion du Gooikoorts Festival qui a lieu en Belgique, mais au concert d'après, le 14 juillet 2013 au Château d'Ars, Nicolaïvan Mingot est de retour sur scène avec le groupe et, depuis lors, a intégré la formation.

Ancien membre fondateur du groupe, Laurent Vercambre, d'abord seul au violon et au nyckelharpa puis en fin de concert au sein du duo de nyckelharpas qu'il forme avec Eléonore Billy, rejoint Malicorne sur scène sur plusieurs titres le 14 juillet 2013 lors du festival du Château d'Ars. C'est alors la première fois que Laurent rejoue avec Malicorne depuis le concert du groupe (dans sa configuration classique) aux Francofolies de La Rochelle 3 ans plus tôt le 15 juillet 2010. Par la suite, Laurent Vercambre assure la première partie du concert parisien que Malicorne donne le 20 septembre 2014 au Trianon. 

Le 17 novembre 2014, Malicorne annonce sur sa page Facebook le retour permanent au sein du groupe en décembre 2014 de Laurent Vercambre au violon et au nyckel-harpa, en remplacement de Romain Personnat, avec un premier concert donné par la nouvelle formation le samedi 17 janvier 2015 à Dijon (Théâtre des Feuillants). Romain Personnat aura donc définitivement quitté le groupe après un dernier concert de l'année 2014 donné le 11 novembre à Coutances.

Le 24 juin 2015, Malicorne confirme que son nouvel album studio est toujours en préparation, confiant « prendre [son] temps pour qu'il soit de qualité » et annonce « souhaiter tout de même sortir [...] pour le mois d'octobre 2015 » deux disques : un EP de 4 titres « pour donner le ton de l'album » et un vinyle (en deux versions) d'un nouveau titre, "Les Cendres de Jeanne", une chanson écrite par Gabriel Yacoub sur le personnage de Jeanne d'Arc<ref name="Ouest-France Malicorne 15/02/2013">Ouest-France / Bretagne / Lorient / Quéven / Archives du vendredi 15-02-2013 / Article "Malicorne en concert aux sources de la chanson traditionnelle - Quéven" du vendredi 15 février 2013 par Gildas JAFFRÉ</ref>. Finalement, seul le vinyle sera publié le 10 décembre 2015 sous le titre "Jehanne", proposant deux versions différentes, l'une en face A intitulée "Les Cendres de Jeanne" (signée Gabriel Yacoub / Nicolaïvan Mingot) interprétée par Malicorne, l'autre en face B intitulée "Ghjuvanna" (signée Gabriel Yacoub / Nicolaïvan Mingot / Laurent Vercambre) interprétée par A Filetta, groupe corse ami de Malicorne.

Mi-septembre 2015, Malicorne annonce sur sa page Facebook le départ définitif de Laurent Vercambre après seulement neuf mois passé au sein du groupe. Ce départ a été décidé de façon concertée avec le groupe, Laurent Vercambre souhaitant se lancer dans d'autres projets musicaux. À l'origine, Laurent Vercambre devait annoncer son départ sur scène à l'occasion d'un dernier concert donné au sein de Malicorne le 25 septembre 2015 à Coucy-le-Château dans le cadre du « Historica - Pagan Festival » mais le concert avait été annulé à la suite de l'annulation de l'intégralité du festival par ses organisateurs. Malicorne redevient donc une formation à 6 musiciens, comme c'était le cas depuis le début de la tournée jusqu'à l'arrivée de Nicolaïvan Mingot au sein du groupe au printemps 2013.

Le groupe donne un ultime concert le lors du festival du chant de marin de Paimpol.

Le projet de nouvel album est définitivement abandonné concomitamment au dernier concert donné à Paimpol.








Les singles que Malicorne a publiés sont, pour la plupart, des singles promotionnels donc non destinés à la vente (sauf trois d'entre eux publiés en 1986, 1996 et 2015):





L'influence de Malicorne, dans la vague folk des années 1970 en France, fut importante, au côté de groupes et d'artistes plus traditionnels comme La Bamboche ou La Chifonnie. Pour "L'encyclopédie de la chanson française", le groupe est . Le groupe se caractérise par l'innovation constante, avec un important travail sur le son et le mariage réussi des instruments traditionnels (acoustiques) et modernes (électroacoustiques et électriques).

De source traditionnelle, la plupart des chansons de Malicorne traitent de sujets sombres tels que les amours déçues ("Le Deuil d'amour", "Le Mariage anglais", "Le Galant indiscret"), la guerre ("Pierre de Grenoble", "Le Prince d'Orange"), la cruauté ("L'Écolier assassin"), la pauvreté ("Alexandre / Danse bulgare") ou de tristes histoires d'épouvante ("L'Auberge sanglante"). La magie et les malédictions en tout genre tiennent également une place importante dans les albums, à l'image du "Bestiaire" (1979).

À côté des titres sérieux, trouve également des textes ou des mélodies plus légères voire comiques ("La Fille au cresson", "Couché tard, levé matin", "Ma chanson est dite", "Le Branle des chevaux" ou "Le Ballet des coqs").

Le travail des voix est l'un des piliers de ce groupe , qui a fait des chants et complaintes a cappella une de ses spécialités, notamment avec des titres comme "Le Prince d'Orange", "Marions les roses" ou "La Blanche Biche". Quasiment toutes leurs chansons a cappella peuvent être retrouvées sur la compilation "Vox" (1996).

De la guitare électrique au mandoloncelle en passant par le cromorne ou le violon, le groupe a marié les instruments traditionnels et électriques. Les mélodies sont généralement issues du répertoire traditionnel, mais souvent des compositions originales introduisent ou finissent des thèmes traditionnels.

Le groupe travaille également sur des sons novateurs et des bruitages qui créent une ambiance particulière et va jusqu’à créer des instruments nouveaux tels que l'orgue à voix qu'on peut entendre notamment sur le titre "La Blanche Biche" (1977). Sont utilisés également des instruments moins connus tels que le dulcimer, l'épinette, la vielle à roue, le nyckelharpa, la viole d'amour, l'orgue Elka, etc.

Les influences musicales du groupe sont principalement les musiques traditionnelles française et québécoise, la musique du Moyen Âge, la musique celtique mais aussi le rock et le jazz.

Certains disques ont un thème général ou un fil conducteur, faisant d'eux des albums-concept : 

Le disque "Almanach" (1976) reste à ce jour encore l'album le plus vendu par le groupe ( exemplaires).

Sur leur première compilation "Quintessence" (1977) figure un titre, "Martin" (paru originellement en single) uniquement disponible (en LP) sur ce disque.




</doc>
<doc id="1924" url="https://fr.wikipedia.org/wiki?curid=1924" title="MSX">
MSX

Le MSX était un standard de micro-ordinateurs à vocation domestique (grand public) d’origine japonaise, qui date des années 1980. Contrairement à la plupart des ordinateurs de l’époque, les MSX ont été produits par divers fabricants. Ils étaient compatibles entre eux, aussi bien pour le matériel que pour les logiciels. Plusieurs versions du standard se sont succédé.

Souvent interprété comme , le sigle "MSX" signifierait selon Kazuhiko Nishi, initiateur du projet. Le standard fut créé en 1983 et produit par de nombreuses sociétés nippones comme Canon, Casio, Panasonic, Sanyo, Sony, Toshiba ou Yashica. Le japonais Yamaha a notamment produit des MSX à vocation musicale dont une version MSX1 avec un processeur sonore huit voies et des prises MIDI. En Europe, Philips ou Schneider ont été présents sur la scène MSX.

Le standard MSX fut inventé à la suite d'un appel d'offres de la part du METI qui désirait que les ordinateurs soient compatibles entre eux (à l’époque chaque marque/modèle d'ordinateur avait son propre langage et son propre système d'exploitation). Microsoft répondit à l’appel pour développer les couches logicielles : langage de programmation interprété MSX-Basic intégré en standard dans une mémoire ROM et système d’exploitation . Plus tard, le MSX-DOS 2 fut développé par ASCII, ajoutant entre autres les notions de répertoires (et sous-répertoires), de partitions de disques durs SCSI…

Les MSX avaient des particularités "nationales" : la version commercialisée au Japon était équipée d'un clavier QWERTY/Kanji. En France, la plupart possédait un clavier AZERTY. Une version supportant la langue arabe a été commercialisée par la société Al Alamia. Cette déclinaison portait le nom de "Sakhr" (roche en arabe).

Les MSX2 de Sony possédaient déjà une souris et un bureau (interface) graphique.

Il y eut quatre générations de MSX : les MSX (ou MSX1), les MSX2, les MSX2+ et les . Ils furent très populaires au Japon et en Europe, surtout à l’époque du MSX1. Le MSX2+ fut commercialisé en Europe grâce à quelques rares importateurs en France, en Espagne et aux Pays-Bas. Le fut encore plus rare en Europe, car produit uniquement par la firme Panasonic et réservé exclusivement au marché japonais.

La norme MSX 1 fut définie à partir d’une machine existante, le Spectravideo 318 :


En 1985, la norme MSX-2 fut définie :


On peut citer comme célèbres : le Sony HB-F 700-F (deux cartouches et un lecteur de disquettes) ainsi que le Philips VG-8235 (deux cartouches et un lecteur de disquettes 360Ko). Mais aussi les MSX 2 Sony HB-G 900 et Philips NMS 8280. Ces deux ordinateurs étaient orientés vidéo (numérisation, retouche, titrage, etc).

NB : Il existait pour ce standard, une cartouche d’extension sonore à et une bibliothèque sonore, cette cartouche nommée FM-PAC était compatible avec plusieurs jeux MSX de différents éditeurs. On peut également citer le de Philips, cartouche avec prises Midi et connecteurs RCA, pour laquelle un clavier externe (piano) était disponible en option.

En 1988, la norme MSX2+ fut définie :


En 1990, la norme fut définie :



Les ordinateurs MSX font partie des plates-formes les plus émulées aujourd’hui. Des versions existent sur la plupart des plates-formes, y compris les téléphones mobiles.

Une grande partie des émulateurs MSX est basée sur le code du pionnier fMSX, un émulateur portable créé par le russe Marat Fayzullin. La licence du code source de fMSX n’étant pas libre, de nombreux émulateurs ont supprimé dans leurs dernières versions le code qui émule le Z80 de Fayzullin pour éviter tout problème légal. Il existe également une version pour Mac.

BlueMSX est considéré comme le meilleur émulateur MSX pour plate-forme Windows par les sites spécialisés. Il est disponible sur le site blueMSX.

OpenMSX est un émulateur multiplateforme (Unix, Windows, Wii, Dingoo A320…) . Sa rapidité, sa qualité d’émulation et ses possibilités de paramétrage le placent à un niveau équivalent à BlueMSX du point de vue de la qualité. Son principal avantage est qu’il n’est pas lié à un système d’exploitation. Il est disponible sur le site officiel OpenMSX.

En 2001, Kazuhiko Nishi, inventeur du standard MSX, a initié une renaissance du MSX autour d’un émulateur officiel appelé MSXPLAYer. C’est le seul émulateur MSX officiel. Tous les droits sont maintenus par l’association MSX.

Nintendo Japon a annoncé en 2006 et confirmé en que des jeux MSX seront disponibles sur la Console virtuelle de la Wii à partir de l’été 2007 au Japon.

En 2006, la société japonaise D4 Enterprise produit un ordinateur MSX très compact fonctionnant avec un FPGA qui émule le processeur d’origine (Z80) de façon semi-matérielle (un FPGA étant programmable) : le 1chipMSX.



</doc>
<doc id="1926" url="https://fr.wikipedia.org/wiki?curid=1926" title="Maine-et-Loire">
Maine-et-Loire

Le département de Maine-et-Loire est un département français du Val de Loire dans la région Pays de la Loire. Créé en 1790, ses limites reprennent en grande partie celles de l'ancienne province d'Anjou.

Avec , il est le de France par sa superficie, et le par sa population avec habitants appelés « Angevins » et « Angevines » en référence à la province d'Anjou. Département à grande dominance rurale et agricole, il a comme plus grand centre urbain la ville d'Angers, qui en est la préfecture, secondée par trois sous-préfectures de tailles plus modestes : Cholet, Saumur et Segré-en-Anjou Bleu (par ordre de population).

Le Maine-et-Loire fait partie du bassin de la Loire et de la région naturelle du Val de Loire inscrit depuis 2000 sur la liste du patrimoine mondial de l’UNESCO . Le territoire est traversé d'est en ouest par la Loire où se déversent de nombreux affluents faisant de lui un des départements les plus drainés de France. Son climat tempéré de type océanique, sa diversité géologique et ses nombreuses zones humides favorisent la biodiversité et en font la première région horticole de France.

Économiquement, il est le second pôle industriel des Pays de la Loire et un des premiers départements français en valeur et en diversité agricole, hébergeant notamment le vignoble le plus étendu du Val de Loire.

L'Insee et la Poste lui attribuent le code 49.

Le département a été créé à la Révolution française, le en application de la loi du , et correspond à la majeure partie de la province d’Anjou. Il existe une incertitude quant au nom de la rivière, « Maine » ou « Mayenne », et donc sur le nom du département à sa création. Ainsi, le décret de l’Assemblée du donne le nom de « Mayenne-et-Loire », comme la nomination par le roi des commissaires du département le et comme le décret du qui indique qu’Angers est le siège de l’administration du département. Mais la proclamation du roi du qui sanctionne le décret précédent parle de « Maine-et-Loire ». Le SPLAF indique que le département s’est d’abord appelé « Mayenne-et-Loire » pour changer de nom pour « Maine-et-Loire » le mais aucun décret national de changement de nom n’a été retrouvé à cette date, toutefois l’assemblée du département était réunie en séance ce jour-là et le choix du changement de nom a peut-être été fait au niveau local. Dans tous les cas, les deux noms ont cohabité dans les documents officiels de la création du département jusqu’à au moins la fin de l’an II (1794).

On dit et on écrit « "le" Maine-et-Loire » ; le nom du département a donc connu une masculinisation consacrée par l’usage alors que la règle aurait souhaité, la Maine et la Loire étant toutes deux féminines, que le nom « Maine-et-Loire » le fût également. Le masculin vient peut-être, par contamination, de la province du Maine (chef-lieu Le Mans), voisine. Comme tous les noms de départements formés de deux termes liés par « et », à l’inverse des autres noms de départements qui s’emploient avec l’article dans les compléments du nom et à la suite de la préposition « dans », on doit dire et écrire « département "de" Maine-et-Loire » ou « "en" Maine-et-Loire », et non « département "du" Maine-et-Loire » ou « "dans le" Maine-et-Loire », même si, là encore, « département "du" Maine-et-Loire » ou « "dans le" Maine-et-Loire » sont régulièrement utilisés.

Le Maine-et-Loire se situe à l'extrémité Ouest du bassin parisien et à l'extrémité Est du massif armoricain, la frontière partageant le département en deux au niveau d'Angers.

Le Maine-et-Loire est situé dans le Grand Ouest français, près de la façade atlantique. Depuis 1955, il fait partie de la région Pays de la Loire.

Le Maine-et-Loire est le deuxième des départements français en nombre de départements qui lui sont limitrophes avec huit départements voisins. Ces huit départements sont la Mayenne, la Loire-Atlantique, la Sarthe et la Vendée pour la région des Pays de la Loire, l’Indre-et-Loire pour le Centre-Val de Loire, les Deux-Sèvres et la Vienne pour la région Nouvelle Aquitaine et l’Ille-et-Vilaine pour la région Bretagne.

Le Maine-et-Loire était même, jusqu’au janvier 1968 et la création des nouveaux départements franciliens, le département ayant le plus de départements limitrophes : la Seine-et-Marne, qui en compte maintenant dix, n’en comptait alors que sept.

L’"Atlas des paysages de Maine-et-Loire" recense en tout 13 unités paysagères au sein du département de Maine-et-Loire, ainsi qu'une trentaine de sous-unités. Celles-ci sont définies selon leurs critères identitaires.

Les points les plus hauts se situent dans la partie sud du département : dans la région du chemillois, la Trottière à La Tourlandry, point culminant de Maine-et-Loire (), le puy de la Garde à Saint-Georges-des-Gardes (), Melay (), et dans la région du Vihiersois, Saint-Paul-du-Bois (), La Plaine () et Vihiers ().

Les points bas de l'Anjou sont tous positionnés sur la Loire. Au moment où celle-ci entre en Anjou, à Montsoreau, elle se trouve au-dessus du niveau de la mer. Elle perd peu à peu de la hauteur au fur et mesure qu'elle s'avance dans l'Anjou. Vers Beaufort-en-Vallée, dans la vallée de l'Authion, elle ne s'élève plus qu'à , puis 14 quand elle rencontre la Maine. Quand elle sort de l'Anjou, au niveau de Champtoceaux, elle s'élève à .

Le territoire angevin est un territoire de rencontre entre le Massif armoricain et le Bassin parisien. Les deux zones géologiques se chevauchent, divisant le territoire du nord au sud. Cependant, on reconnait trois structures géomorphologiques au sein de l'Anjou.

L'Anjou noir, situé à l'ouest de la région, à partir d’Angers et englobant les Mauges et le Segréen. Son sous-sol est majoritairement constitué de schistes et de grès. La moitié nord depuis Pouancé et jusqu'à Brissac-Quincé se caractérise par des bandes de schistes ardoisiers. Au sud de la Loire, dans les Mauges, le plateau de grès et de schistes se trouve percé de granite à l'est et l'ouest de Cholet.

L'Anjou blanc, à l’est, se confond avec le Saumurois et le Baugeois par ses sols de calcaire et de tuffeau. Ces terres blanches, résultant de l’altération de la craie (tuffeau), marquent l’extrémité Sud-Ouest du Bassin parisien.

La vallée de la Loire elle-même constitue un territoire géologique. Traversant l'Anjou d'est en ouest, elle y dépose de riches alluvions tout le long de son parcours.

Avec environ de cours d'eau, le Maine-et-Loire est un des départements les plus drainés de France. L'Anjou se trouve en totalité incluse dans le bassin hydrographique de la Loire qui traverse le territoire d'est en ouest. À l'est, à Montsoreau, en frontière avec l'Indre-et-Loire, la Loire reçoit les eaux de la Vienne, puis à Saumur, les eaux du Thouet grossies par la Dive qui drainent le sud-est, vers Montreuil-Bellay. Vers le centre du département, aux Ponts-de-Cé, la Loire reçoit les eaux de l'Authion, qui coule dans la Vallée angevine et se trouve grossie par le Lathan et la Couasnon qui drainent le Baugeois. Dans le nord et l'ouest du département, la Sarthe, grossie par le Loir, rejoint la Mayenne dont les affluents drainent tout le nord-ouest du département avec l'Oudon rejoint par la Verzée, l'Argos et l'Araize. Les zones humides comprises entre la Sarthe, la Mayenne et le Loir sont connues sous le nom de Basses vallées angevines et bénéficient d'un classement en zone de protection spéciale et font partie du réseau Natura 2000.

En se rejoignant, la Sarthe et la Mayenne forment la Maine, qui ne possède donc pas de source pour raison hydronymique. Elle ne parcourt que avant de se jeter dans la Loire au niveau de Bouchemaine. Plus loin, les rivières du Louet qui reçoit l'Aubance, et du Layon qui reçoit les rivières Lys, Hyrôme et Jeu, et qui drainent une partie des Mauges, se jettent dans la Loire à hauteur de Chalonnes-sur-Loire. Le fleuve recueille enfin les eaux de la Romme sur sa rive droite, et de l'Èvre sur sa rive gauche. Seules l'Erdre et une petite section de la Sèvre Nantaise qui reçoit la Moine traversent le département pour se jeter dans la Loire près de Nantes, dans le département voisin, la Loire-Atlantique.

Pendant toute la durée de son existence, l'Anjou aura à subir les crues répétitives de la Loire. Afin de contrer ce fléau, Henri II Plantagenêt décida en 1161 la construction d'une première levée de la Loire afin d'atténuer les crues et d'augmenter les terres cultivables.

Le Maine-et-Loire continue néanmoins de subir les crues de la Loire, parfois dévastatrices. Lors de la crue de 1856, les ardoisières de Trélazé sont inondées et l'économie de la région est dévastée. L’empereur Napoléon se rend à Trélazé pour promettre des aménagements. En 1911, la Loire est en crue, la Maine déborde et plusieurs quartiers d'Angers se retrouvent sous les eaux.

La LPO recense en Maine-et-Loire plus de 300 espèces d'oiseaux différents (plus de 170 espèces nicheuses). L'Anjou est avantagé par la présence de la Loire et de ses affluents et par de nombreuses zones humides et vallées inondables. De par sa position géographique, elle se trouve sur le trajet migratoire de plusieurs espèces. On trouve notamment de nombreuses espèces de canards outre le canard colvert (le canard siffleur, le canard chipeau, le canard souchet ou le canard pilet en zone d'hivernage). S'y trouvent d'autres espèces permanentes comme le grand cormoran, le vanneau huppé, le héron garde-bœufs ou le busard cendré. Des espèces rares peuvent s'y trouver comme le faucon pèlerin, le râle des genêts, la marouette ponctuée ou en limite de leur zone de nidification, comme la cigogne noire, le circaète Jean-le-Blanc, le balbuzard pêcheur, l'outarde canepetière, le moineau soulcie. En hivernage, on peut trouver entre autres espèces, la grande aigrette, l'œdicnème criard, le goéland pontique, le goéland brun…

Les mammifères présents en Maine-et-Loire sont pour la plupart des espèces communes. On trouve en revanche la présence du cerf dans plusieurs forêts dans l'Est du département. Parmi les espèces les plus rares se trouvent le castor, la loutre et la genette. Le ragondin est en revanche devenu commun. Le vison d'Europe aurait en revanche disparu de la région. Avec 18 espèces, les chauves-souris forment un tiers de l'ensemble des mammifères présents dans le département, notamment grâce aux nombreuses cavités présentes dans le tuffeau.

Pour les reptiles, on y trouve des orvets, des lézards verts, des vipères (aspic et péliade) et cinq espèces de couleuvres (la couleuvre vipérine, la couleuvre à collier, la couleuvre d'Esculape, la coronelle lisse et la couleuvre verte et jaune). La cistude était autrefois présente dans certains bras de la Loire. Si certains individus ont été observés, il n'y a aucune preuve d'existence d'une population viable. En revanche la tortue de Floride est bien présente (lac de Maine notamment) et le xénope lisse continue son invasion dans le Sud-Est du département.

Chez les invertébrés, la présence la plus notable est celle de 550 espèces d’araignées sur les 1 570 espèces présentes dans l'Hexagone, grâce aux différentes influences climatiques et aux voies de pénétration, notamment méridionales. On trouve également des moules d'eau douce des genres Corbicula et Unio, ainsi qu'une grande variété d'insectes (plus de 60 espèces d'Odonates), notamment dans les zones calcaires.

Le Maine-et-Loire possède un climat tempéré de type océanique. Le poète français Joachim du Bellay a vanté la « douceur angevine » qui lui manquait tant.

Au , la longueur totale du réseau routier du département de Maine-et-Loire est de , se répartissant en d'autoroutes, de routes nationales, de routes départementales et de voies communales. 
Il occupe ainsi le au niveau national sur les métropolitains quant à sa longueur et le quant à sa densité avec par km de territoire.


Pour les transports en commun :

Les lignes ferroviaires traversant le département sont les suivantes :

Pour le réseau aérien :

Les principales villes sont Angers, Cholet, Saumur, Doué-la-Fontaine, Segré-en-Anjou Bleu, Longué-Jumelles, Chemillé-en-Anjou, Beaupréau-en-Mauges, Sèvremoine, Chalonnes-sur-Loire, Beaufort-en-Anjou et Baugé-en-Anjou. En 2014, outre Angers (151 056 hab.), la communauté urbaine Angers Loire Métropole compte huit communes de plus de habitants : Trélazé (13 580 hab.), Avrillé (13 251 hab.), Les Ponts-de-Cé (12 338 hab.), Saint-Barthélemy-d'Anjou (9 318 hab.), Montreuil-Juigné (7 399 hab.), Verrières-en-Anjou (6 977 hab.), Bouchemaine (6 575 hab.) et Longuenée-en-Anjou (6 290 hab.).

L’Anjou faisait partie depuis le de la généralité de Tours avec la Touraine et le Maine. La généralité de Tours d’après le Règlement général du (États généraux) fut organisée avec un certain nombre de modifications qui laissaient présager le démantèlement des anciennes provinces royales. En effet, le 11 novembre 1789, l’Assemblée Constituante brusque les choses en ordonnant aux députés des anciennes provinces de se concerter afin de mettre en place un réseau de nouveaux départements d’environ 324 lieues carrées, soit actuels. Des réunions se tiennent aussitôt dans l’hôtel du duc de Choiseul-Praslin, député de la noblesse de la Sénéchaussée d’Angers. Une trentaine de députés (des trois provinces) présents envisagent de rétrocéder des territoires au Poitou et de subdiviser le domaine restant en quatre départements, autour des capitales traditionnelles, Tours, Angers et Le Mans, et autour de la ville de Laval qui récupérerait des terres du Maine et de l’Anjou.

Le 12 novembre 1789, vingt-cinq députés (des trois provinces) approuvent ce partage mais les deux représentants de Saumur, de Ferrières et Cigongne, se dissocient de cette décision. Les Saumurois plaident en faveur d’un département de Saumur situé au carrefour des trois provinces de l’Anjou, de la Touraine et du Poitou, avec Loudun pour le partage des pouvoirs. Ils accusent les représentants d’Angers de s’entendre avec leurs collègues du Maine et de Touraine pour le dépeçage de la sénéchaussée de Saumur. Ils les accusent également d’abandonner à la Touraine vingt-quatre paroisses anciennement angevines (autour de Château-la-Vallière et de Bourgueil). Le mécontentement grandit, la population de Bourgueil manifeste pour son maintien dans l’Anjou et se solidarise avec Saumur. Pendant ce temps, les représentants de Chinon, à l’instar de ceux de Saumur tentent également de créer leur propre département. Des dissensions apparaissent au sein du conseil municipal de Saumur. Certains représentants de la noblesse et du clergé approuvent le découpage proposé par Angers. En décembre de la même année, les Loudunais rompent leur accord avec Saumur. Le 14 janvier 1790, l’Assemblée Nationale décrète que « Saumur et le Saumurois feront partie du département de l’Anjou ».

Intégrée dans le département de « Mayenne-et-Loire » (futur « Maine-et-Loire »), Saumur tente de partager avec Angers la fonction de chef-lieu. Ayant perdu la partie, les représentants de Saumur proclament que l’alternance entre Angers et Saumur permet de déjouer les intrigues et les cabales qui naissent de la fixité. Le lundi 24 mai 1790, ils obtiennent cent quatre suffrages en faveur de l’alternance mais cinq cent trente-deux voix se prononcent en faveur d’un siège permanent à Angers. Le nouveau département est constitué. L’Assemblée Constituante entérine cette structure le 22 juin 1790 et le roi le 25 juin 1790. Afin de calmer la susceptibilité des Saumurois, les trente-six membres du nouveau conseil du département portent à leur présidence Gilles Blondé de Bagneux (ancien maire de Saumur). Ainsi, jusqu’en novembre 1791, le premier président du conseil général du département sera Saumurois.

En 1790, lors de la création des départements français, le Sud-Saumurois (sénéchaussée de Loudun et pays de Mirebeau dépendants du gouverneur de Saumur et partie méridionale de l'Anjou) est rattaché au département de la Vienne. En 1802, lors de la nomination des premiers préfets de France, c'est un Loudunais, Pierre Montault-Désilles qui devient premier préfet du département de Maine-et-Loire. La même année, son frère Charles Montault-Désilles, devient l'évêque du diocèse d'Angers.

Après la victoire des coalisés à la bataille de Waterloo (18 juin 1815), le département est occupé par les troupes prussiennes de juin 1815 à novembre 1818 (voir occupation de la France à la fin du Premier Empire).
En 1940, le Maine-et-Loire est envahi par l'Allemagne nazie.

La question d’un rattachement du département à une région administrative Val de Loire fait l’objet d’un débat récurrent.


Au , le Maine-et-Loire, profondément marqué par le souvenir des guerres de Vendée (dans les Mauges et la vallée du Layon) et de la Chouannerie (dans le nord du département) apparaît comme un bastion conservateur catholique, principalement légitimiste. Ainsi, en 1876 et en 1877, quatre légitimistes sont élus députés, à Cholet, Beaupréau et Angers. Segré et Saumur élisent un bonapartiste. Baugé nomme un républicain. En 1885, la liste d'Union conservatrice (bonapartistes et légitimistes) est élue entière avec environ 60 % des voix. Pour la droite, c'est l'un des meilleurs résultats de France. Les royalistes conservent quatre circonscriptions sur sept jusqu'en 1928, et la majorité absolue au conseil général jusqu'en 1925.

Le Maine-et-Loire est un département ancré depuis des décennies dans le courant de la droite modérée. Depuis la Libération, les députés furent d’abord gaullistes puis membre de l’UDF ou du RPR, pour être à l’unanimité membre de l’UMP lors de sa création en 2002. Pourtant, lors des élections législatives du , l’élection d’un député socialiste, Marc Goua, a montré que les inclinations politiques y changent. Ceci s’explique sociologiquement par le fait que la population du département est de plus en plus citadine. Cette tendance s'est effectivement accrue lors des élections sénatoriales de 2011 et lors élections législatives du puisque désormais la gauche angevine compte deux sénateurs sur quatre (1 PS, 1 EELV) et trois députés sur sept (2 PS, 1 DVG).



Le Maine-et-Loire est le second département industriel des Pays-de-la-Loire avec emplois dans l'industrie. L'agroalimentaire emploie personnes et la construction .

En 2008, le Maine-et-Loire comptait établissements se répartissant en 40 % pour l'industrie, 27 % pour les services, 22 % pour les commerces et 11 % pour la construction. L'économie angevine est globalement rurale, avec trois bassins urbains que sont Angers, Cholet et Saumur. On compte sur le territoire une trentaine de bassins économiques ruraux, dont près du quart ont une orientation économique diversifiés. L'activité industrielle est surreprésentée dans les Mauges qui se hissent à la des bassins industriels de France.

L'agriculture occupe 64 % du territoire (surface agricole utile). Environ sont consommés chaque année pour l'artificialisation des sols.

L'Anjou se place au cinquième rang des départements français pour la valeur de ses productions agricoles. Le département est le premier producteur horticole français, ainsi que le premier producteur français de champignon de Paris, de pommes, de cassis et de camomille. Il héberge également le troisième vignoble français par la superficie. La filière végétale représente emplois dans entreprises. Un pôle de d'excellence du végétal à vocation mondiale, Végépolys, a été labellisé.

Le tourisme en Maine-et-Loire représente un pan important de l'économie du département, avec près de 2,3 millions de visiteurs par an, dégageant un chiffre d'affaires direct et indirect de 1,5 milliard d'euros.

Se situant juste en arrière de la façade atlantique, deuxième destination touristique après la façade méditerranéenne, le Maine-et-Loire possède de nombreux atouts afin d'attirer les touristes. Département rural, il mise depuis plusieurs années sur le thème de la nature et du végétal, notamment mis en valeur par l'ouverture du parc Terra Botanica. 

En juin 2015, le département s'engage dans la promotion de l'art contemporain, et décide de signer avec Philippe Méaille un bail emphytéotique, d'une durée de 25 ans, concernant le château de Montsoreau. Le Château de Montsoreau-Musée d'art contemporain ouvre ses portes le 8 avril 2016. 

Son offre touristique inclut également la Loire, le vignoble angevin, les châteaux de la Loire, les musées, la tenture de l’Apocalypse, l’habitat troglodyte, etc.

Les habitants de Maine-et-Loire n'ont pas de nom officiel. La dénomination "Angevin(e)" reste cependant largement employée. En février 2012, le quotidien "Ouest-France l"ance un sondage pour choisir le nom des habitants de Maine-et-Loire avec une dizaine de propositions différentes. Le résultat place "Angevin(e)" loin devant les autres propositions avec un peu plus de 56 % des voix sur près de votants.

Histogramme 

En 2017, le département comprend 34 communes dont la population est supérieure à habitants.

La préfecture du département, Angers, est l'un des premiers pôles universitaires de France, avec près de étudiants pour environ habitants.


La chaîne France 3 émet un décrochage local avec France 3 Ouest, qui propose des émissions régionales France 3 Pays de la Loire (journaux télévisés 12/13 et 19/20). Trois chaînes câblées se sont également succédé à Angers, TV10 Angers (1988-2007), Angers 7 (2007-2010) et depuis février 2013 Angers Télé. À partir de 2010, la chaîne TLC émet depuis Cholet sur tout le Sud-Ouest du département ainsi que sur les départements adjacents de la Loire-Atlantique, de la Vendée et des Deux-Sèvres.

La presse écrite locale est principalement dominée par le groupe Ouest-France et ses éditions "Ouest-France" et "Le Courrier de l'Ouest" dont Angers est le siège. "Haut-Anjou" est diffusé dans le Nord de Maine-et-Loire, dans le territoire correspondant au Haut-Anjou historique qui incluait la Mayenne angevine. "L'Éclaireur" couvre également le Nord-Ouest du département autour de Pouancé.

Plusieurs radios nationales possèdent des antennes locales basées à Angers, comme Chérie FM, NRJ, Virgin Radio, Radio Nova ou RCF. Les radios régionales comprennent Hit West (Bretagne et Pays de la Loire), Vibration (Centre, Pays de la Loire, Bourgogne) et Alouette (Bretagne, Pays de la Loire, Poitou-Charentes). Les radios locales comprennent Radio Campus Angers (Angers), Radio G !, Oxygène Radio Haut-Anjou (Segréen) et Ouest FM.

Jusqu'en 2010, le festival international du scoop et du journalisme se déroulait à Angers.

En 1835, d'après Abel Hugo, la langue usitée dans les villes du département était le français, avec un accent un peu traînant. Les habitants des villages avaient encore à cette époque divers patois qui leur étaient propres, et qu'on ne comprenait plus à quelques lieues de distance.

Les paysans y tenaient beaucoup et s'en servaient uniquement entre eux. Cependant, presque tous entendaient aussi le français ; quelques-uns même le parlaient bien, mais ils n'osaient pas s'exprimer avec pureté, de peur que les voisins ne les plaisantent sur leur parler « noblat », expression qui était employée dans le pays pour désigner la langue française.




Quelques spécialités culinaires de la cuisine angevine : 

La superficie du vignoble angevin est de , dont sont en appellation d'origine contrôlée (AOC).

Les deux sites architecturaux les plus visités du département sont le château d'Angers et l'abbaye de Fontevraud. 

Seul château de la Loire construit dans le lit du fleuve, le château de Montsoreau est aujourd'hui un musée d'art contemporain. Il a été construit en 1450 par Jean II de Chambes, conseiller du roi Charles VII. Claude de France, Anne de Bretagne et Marie Stuart y ont séjourné.

Le département comporte

Les festivals se développent en France durant la seconde moitié du . En Maine-et-Loire, le festival d'art dramatique, le "festival d'Angers", naît au début des années 1950. Sa particularité sera au fil du temps d'utiliser plusieurs scènes du département. En 1975, il change de nom pour devenir le "festival d'Anjou".

Plusieurs festivals se développent sur le département à la fin du et au début du : le festival du scoop et du journalisme à Angers en 1985, le festival Premiers plans à Angers en 1989, Les Orientales à Saint-Florent-le-Vieil en 1990, Gypsy Swing Festival à Angers en 1992, le Festival estival de Trélazé en 1996, Tour de scènes à Angers en 1998, Les Z'éclectiques à Chemillé en 1998, les Accroche-Cœurs et le festival Angers-BD à Angers en 1999, le festival Terres à vins, Terres à livres à Savennières en 2005, le festival Tempo Rives à Angers en 2009, qui prend la suite du festival d'Angers l'été créé en 1980, Saveurs Jazz festival à Segré en 2010, le festival Bouche à oreille à Bouchemaine cette même année, etc.

Les plus importantes manifestations sont en 2011, le festival Premiers plans (Angers), le Mondial du Lion (Le Lion-d'Angers), le festival d'Anjou, les présentations publiques du Cadre noir (Saumur) et les journées de la rose (Doué-la-Fontaine). Deux ans plus tard, ce sont le festival Premiers plans, les présentations publiques du Cadre noir, le festival d'Anjou et les Ecuyers du temps (Saumur). Se déroulent également cette même année, le Festival estival de Trélazé, les Z'éclectiques, Les Orientales, Saveurs Jazz festival, les Accroche-Cœurs, le festival Tempo Rives, Foliklores, Angers-BD, ainsi que la foire Saint-Martin (Angers), le carnaval de Cholet et Anjou Vélo vintage.

L’indicatif téléphonique attribué au Maine-et-Loire est +33 241 (02 41). Le département partage cependant de nombreux indicatifs téléphoniques :





</doc>
<doc id="1927" url="https://fr.wikipedia.org/wiki?curid=1927" title="Manche (département)">
Manche (département)

La Manche est un département français de la région Normandie. Son nom provient du bras de mer qui le borde sur tout son pourtour nord et ouest, et le quart est. L'Insee et La Poste lui attribuent le code 50. Sa préfecture est Saint-Lô.

La Manche fait partie de la région Normandie. Elle est limitrophe des départements du Calvados, de l'Orne, de la Mayenne et d'Ille-et-Vilaine. Incluant la péninsule du Cotentin, le département est baigné par la Manche sur toute sa façade ouest, ainsi qu'au nord et au nord-est, sur de côtes.

Par la géologie, le département se rattache au Massif armoricain. La Manche se divise en terroirs, intégré essentiellement au bocage normand. On peut citer du nord-ouest au sud-est la Hague, le val de Saire, le bocage valognais, le Plain, le Coutançais, le Saint-Lois, l'Avranchin et le Mortainais.

La population est majoritairement rurale. En dehors des agglomérations cherbourgeoise et saint-loise, le territoire est maillé de petites villes et de gros bourgs commerçants.

Le département présente la particularité d'être le moins boisé de France.

Contrairement à la tradition française qui consiste à avoir toutes les compétences regroupées en un seul lieu, la Manche présente un polycentrisme dans son organisation des pouvoirs. Si Saint-Lô est avec le conseil départemental et la préfecture départementale la capitale politique et administrative, Coutances est avec le tribunal de grande instance, la maison d'arrêt et le pôle d'instruction la capitale judiciaire. Le siège du diocèse y est implanté, ce qui en fait également la capitale religieuse. Par ailleurs le Cotentin, première agglomération du département et la quatrième de la région avec son port et son industrie nucléaire, est considérée comme la capitale économique et industrielle de la Manche.

Lorsque le , l'Assemblée nationale constituante fixe par décret le nom de chacun des départements instaurés, la Manche désigne déjà depuis le milieu du , soit , la mer décrite plus haut.

Peuplée à l'origine des peuples celtes, les Unelles et les Abrincates, cette région est envahie par les Romains contre les troupes de Viridorix (-56). À l'époque mérovingienne, elle fait partie de la Neustrie. Le Cotentin connaît, au et début du , une immigration de Vikings venus de Norvège et ayant transité par les Hébrides et l'Irlande ; elle se distingue donc des autres régions du Nord de la Normandie dont l'immigration scandinave provenait du Danemark. L'Avranchin et le Cotentin sont concédés, par Charles le Chauve, au roi Salomon de Bretagne en 867, jusqu'à ce que les Normands la conquièrent en 933, au détriment de Vikings de Bretagne, commandés par Incon. La frontière avec la Bretagne est fixée à la Sélune. En 1008 ou 1009, elle est déplacée vers le Couesnon.

Au , le Cotentin voit naître la Maison de Hauteville, famille à l'origine de l'épopée normande du Sud de l'Italie et de la Sicile.

Au également, Geoffroy de Montbray, évêque de Coutances, proche de Guillaume le Conquérant, a considérablement œuvré pour le rayonnement du diocèse de Coutances. Nous lui devons notamment la cathédrale de Coutances, bâtie au , de style roman, et base de l'actuelle cathédrale qui date du début du , et le Parc médiéval de l'Évêque.

La région est occupée par les troupes de Philippe-Auguste et annexée au royaume de France en 1204, exceptées les îles Anglo-Normandes. Le futur département se partage alors entre deux pays, hérités des peuples gaulois, le Cotentin et l'Avranchin.

Au début de la guerre de Cent Ans, la plupart des terres de l'actuel département de la Manche reviennent au roi Charles II de Navarre, de par leur cession par Jean II au traité de Mantes en 1354. Ces terres subissent les conflits entre troupes françaises et navarraises de 1354 à 1378, date de leur occupation par les troupes de Charles V. Cherbourg est ensuite occupée par les Anglais de 1378 à 1393.

Durant l'Ancien Régime, la Manche est ensuite le théâtre de nombreuses guerres et révoltes , la Ligue du Bien public (1476), les Guerres de religion du entre Montgomery et Matignon, la révolte des va-nu-pieds en 1639, la révolte vendéenne (siège de Granville, 1793), et la chouannerie normande de 1793 à 1800. Les rivalités franco-anglaises entraînent au le développement des deux ports : le port militaire de Cherbourg, et l'activité corsaire à Granville.

Le département est créé à la Révolution française, le , en application de la loi du , à partir d'une partie de la province de Normandie et de la généralité de Caen. Son chef-lieu est d'abord fixé à Coutances, puis remplacé par Saint-Lô en 1796, bien que le tribunal d'instance soit toujours à Coutances de nos jours.

Après la victoire des coalisés à la bataille de Waterloo (18 juin 1815), le département est occupé par les troupes prussiennes de juin 1815 à novembre 1818 (voir occupation de la France à la fin du Premier Empire).

Le voit l'industrialisation du département qui garde cependant une identité profondément rurale. Le travail du fer se développe, et l'industrie agroalimentaire apparaît et s'exporte. En 1858 est inaugurée la ligne ferroviaire reliant Cherbourg à Paris.

Le port de Cherbourg atteint son apogée au début du comme point de départ des voyageurs pour l'Amérique. Il sert aussi au transport de troupes et de matériel pendant la Première Guerre mondiale.

Pendant la Seconde Guerre mondiale, l'armée allemande fortifie les côtes de la Manche avec le mur de l'Atlantique. En juin et juillet 1944, la bataille de Normandie se joue en partie dans le Cotentin, avec le débarquement à Utah Beach, la bataille des Haies et l'opération Cobra. Avec de nombreuses communes détruites à 80 % ou 90 % (comme Saint-Lô, dite la « capitale des ruines »), les années 1945-1960 voient le retour des populations et la reconstruction rapide du pays.

La ville de Coutances reprend temporairement le rôle de préfecture, sous Édouard Lebas, après la Seconde Guerre mondiale en raison de la destruction presque totale de Saint-Lô, et ce jusqu'à la reconstruction de cette dernière en 1953.

Entre 1956 et fin 2015, le département de la Manche est administrativement intégré à la région de programme de Basse-Normandie. La réunification de la Normandie intervient effectivement le .

Au , le territoire de la commune de Pont-Farcy est rattaché au département de la Manche, décision préalable à la fusion des communes de Tessy Bocage, située dans la Manche, et de Pont-Farcy, située dans le Calvados. Pont-Farcy rejoint de fait Saint-Lô Agglo.

Ce blason rappelle celui de la Normandie, province dont fait partie le département et la partie gauche symbolise la Manche qui le borde sur toute sa côte Ouest.

Avec trois façades maritimes en de côtes, le climat manchois est fortement océanique : les hivers sont doux, avec une température moyenne de janvier comprise entre et du Bocage vers le cap de la Hague, aux rares gelées, les étés tempérés, avec une température moyenne d'août de environ. Ainsi, la période de gel n'excède pas 6 jours sur les côtes, et dure jusqu'à 54 jours dans le Saint-Lois et le Mortainais. L'amplitude thermique journalière est entre sur la côte et dans les terres l'hiver, à l'été.

La pluviométrie est importante (entre 120 et 160 jours de précipitations supérieures à par an en moyenne), mais varie beaucoup en fonction des terroirs, entre sur la côte et dans le bocage du sud, fréquemment sous forme de crachin.

Les côtes ouest et nord bénéficient de l'influence adoucissante de la mer, permettant la naturalisation de beaucoup de plantes méditerranéennes ou exotiques (mimosas, palmiers, agaves, etc.), malgré une faible insolation (environ ).

Le vent marin souffle régulièrement sur la côte, ce qui participe avec les marées à des changements de temps rapides dans une journée. Les forts coups de vent ou les tempêtes sont courants.

La Manche est le premier département agricole français, principalement dans l'élevage (bovin, ovin, équin) et la culture de fruits (pommes) et légumes (carottes, poireaux, choux-fleurs). Cherbourg-Octeville est un port important (pêche, plaisance, trafic transmanche, commerce, militaire, construction navale). L'industrie nucléaire a pris une importance considérable. L'économie manchoise repose enfin sur le tourisme, essentiellement balnéaire et saisonnier.

Le département de la Manche est dans la moyenne nationale avec une population de , et une densité de 82 hab/km².

La seule grande ville du département est Cherbourg-en-Cotentin (), devant la préfecture, Saint-Lô (). L'agglomération cherbourgeoise compte plus de , elle représente à elle seule plus de 20 % de la population totale d'un département qui reste fortement rural (52 % de la population vit dans les campagnes, contre 20 % en France).

Au cours du , la population du département est passée de en1801, à un demi siècle plus tard, avant de retomber à en 1871, et en 1891. Cette baisse est imputable à l'exode rural et à un taux de mortalité supérieur à celui de la natalité. Gochet recense à la fin de ce siècle « environ et ».

De en 1968, la population a cru de 6,5 % pour atteindre en 1999, puis en 2007, soit une progression annuelle moyenne de 0,24 % ces sept dernières années. L'analyse de cette dernière période met en relief un vieillissement de la population, du fait de soldes naturel et migratoire positifs tandis que les jeunes quittent le département pour la formation et l'emploi. Du fait de l'étalement urbain, les villes de plus de 10 000 habitants perdent leur population au profit des territoires ruraux, particulièrement littoraux. L'Avranchin et le Coutançais présentent un solde positif, aux dépens du Mortanais et du Plain-Cotentin. Manque de logement et problème économique engendrent un flux migratoire négatif dans l'agglomération cherbourgeoise

Les habitants du département de la Manche sont appelés traditionnellement « les Manchots ». À la fin du , les médias ont commencé à utiliser aussi le terme « Manchois ». L’intelligentsia les avait avec le même mot précédés dès le . On se dit effectivement autant « Manchot » que « Manchois ». Tout dépend peut-être du milieu auquel on appartient. De souche profondément rurale, et toujours prêt à le montrer, on se dirait plutôt « Manchot ». Mais, venant d'autres milieux, on aurait tendance à s'affirmer « Manchois » pour ne pas se dévaloriser. 
À noter que les ouvrages tel que le Dictionnaire Larousse 2004 par exemple qualifient les habitants de la Manche comme étant des « Manchois ».


La Révolution avait provisoirement maintenu deux sièges épiscopaux à Avranches et Coutances ; l'Église catholique les fusionne en 1801, le département de la Manche correspondant au diocèse de Coutances et Avranches. L'évêque nommé du diocèse est monseigneur Laurent Le Boulc'h, qui a été ordonné le dimanche 27 octobre 2013 à la cathédrale de Coutances. Le Mont-Saint-Michel est un haut lieu de pèlerinage pour les catholiques. Il existe de nombreuses abbayes dans la Manche.

La religion catholique est largement majoritaire dans la Manche. Selon un sondage de l'IFOP pour "La Vie", en décembre 2006, plus de 75 % des Manchots se déclaraient proche du catholicisme.

Plus de 20 % des Manchois se disent sans religion. Les musulmans représentent moins de 1 % de la population.

Les protestants de la Manche dépendent du consistoire de Caen.

L'Église de Jésus-Christ des saints des derniers jours est représentée dans la Manche avec deux paroisses : la paroisse de Cherbourg et la paroisse de Coutances.

Cherbourg-en-Cotentin est le principal pôle culturel de la Manche, disposant d'une scène nationale, d'un centre régional d'art du cirque, de deux cinémas, dont un d'Art et d'essai.

Plusieurs festivals animent la saison culturelle, qu'ils soient musicaux (Chauffer dans la noirceur, "les Traversées Tatihou", Jazz sous les pommiers, Festival Papillons de nuit) ou cinématographiques (Cinémovida, Festival des cinémas d'Irlande et de Grande-Bretagne de Cherbourg-Octeville...). Le chanteur Allain Leprest originaire de Lestre, dans le nord de la Manche, est resté fidèle à son département et à sa mer d'origine qu'il a chantés par exemple dans "Il pleut sur la mer". L'un de ses derniers disques, "Parol' de Manchot", est une collaboration avec François Lemonnier, lui aussi du crû.

Le Réseau départemental des sites et musées de la Manche mis en place par le conseil départemental regroupe patrimoniaux et musées sur l'ensemble du département, dont 14 sont ouverts au public.

Le conseil départemental de la Manche lance officiellement le sitel internet wiki, "Wikimanche" au printemps 2007.

Terre rurale, le département a politiquement une tradition conservatrice, selon les mots d'Alexis de Tocqueville, à l'exception notable de Cherbourg, plus ouvrière et plus à gauche. Cependant, avec le temps, le département semble de plus en plus voter conformément aux vagues électorales nationales.

Après la guerre, le conseil général de la Manche est dominé par les élus indépendants sous la présidence d'Henri Cornat et Léon Jozeau-Marigné. Avec l'élection de Jean-François Le Grand puis la constitution de l'UMP, la plupart des conseillers se présentent sans étiquette, souvent des proches du parti majoritaire qui refusent d'être encartés. Aussi, la droite dispose actuellement d'une majorité confortable avec neuf UMP, douze divers droite et quinze sans étiquette. L'opposition est composée de , dont les conseillers des cantons de la communauté urbaine de Cherbourg, et de quatre divers gauche. L'assemblée compte également un élu du Modem, Olivier Beck.

L'assemblée reste dominée par les hommes âgés. La moyenne d'âge de l'assemblée est en 2008 de et demi, contre et 4 mois en 1994, et et demi en 2001, augmentation parallèle à celle de l'âge des candidats : en 2008 contre de en 2001, et près de 90 % des élus départementaux qui ont plus de . Conséquence, les retraités représentent plus d'un tiers des conseillers généraux, contre 17,3 % en 1994 et 19,2 % en 2001. Les professions médicales sont également sur-représentées, avec sept vétérinaires, deux médecins et un dentiste, en activité ou à la retraite. Parmi les actifs, on trouve donc en 2008 un quart de professions libérales, dont la part se réduit au profit des cadres (24,2 %), des chefs d'entreprise (18,2 %), des employés (15,2 %), des enseignants (12,1 %) et des agriculteurs (6,1 %), ces derniers qui n'étaient plus présents dans l'assemblée depuis la décennie 1990.

L'assemblée ne compte que quatre femmes, l'obligation de parité se traduisant par la relégation des femmes comme suppléante. La féminisation des édiles n'est pas plus forte parmi les autres élus. Depuis la défaite d'Anne Heinis aux sénatoriales de 2001, les Manchoises sont absentes des bancs parlementaires, et aucune ne dirige une commune de plus de habitants, pour lesquelles la loi du 31 janvier 2007 contraint à la stricte alternance hommes-femmes dans la constitution des listes électorales.

Le cumul des mandats est la règle au niveau départemental : 51,9 % des conseillers généraux sont maires, 11,5 % occupent des postes d'adjoints, 23,1 % sont conseillers municipaux. Trois des huit parlementaires de la Manche ne détiennent que ce mandat. Les législatives de 2007 ont en revanche vu le renouvellement de trois députés sur cinq, faisant passer la moyenne d'âge de 60 à 53 ans et demi.

Le second tour de l'élection présidentielle de 2012, confirme une poussée de la gauche dans les villes, 9 des communes donnant l'avantage à François Hollande, contrairement aux chef-lieux de cantons ruraux, Nicolas Sarkozy demeurant majoritaire de justesse au niveau départemental (50,1 %). Devenu ministre, Bernard Cazeneuve confirme l'implantation de la gauche dans le Cotentin en conservant son siège au premier tour malgré une circonscription redécoupée. Les députés UMP élus en 2007, Philippe Gosselin et Guénhaël Huet conservent leur siège, mais Stéphane Travert fait tomber Alain Cousin, en poste depuis 1988, faisant passer le PS d'un siège sur cinq en 2007, à sur 4.


Au 31 décembre 2015, il y a . Ce nombre a diminué au janvier 2016 à la suite de la création de nouvelles communes comme Cherbourg-en-Cotentin, Torigny-les-Villes...








</doc>
<doc id="1928" url="https://fr.wikipedia.org/wiki?curid=1928" title="Marne">
Marne

Marne est un nom commun ou un nom propre qui peut désigner









</doc>
<doc id="1929" url="https://fr.wikipedia.org/wiki?curid=1929" title="Mayenne (département)">
Mayenne (département)

La Mayenne ( ) est un département français de la région Pays de la Loire. Ses habitants sont les "Mayennais" et les "Mayennaises". L'Insee et la Poste lui attribuent le code 53. Son chef-lieu est Laval.

Le département a été constitué par le décret de l'Assemblée nationale du . Comme une soixantaine de départements en France, il s'identifie au nom d'un cours d'eau, en l'occurrence la Mayenne. Il correspond essentiellement au Bas-Maine, qui formait la moitié occidentale de la province du Maine. Le tiers méridional du département faisait toutefois partie de l'Anjou, et il est d'ailleurs appelé « Mayenne angevine ». L'Anjou se trouve également dans la région naturelle du Val de Loire.

L'histoire du département est notamment marquée par la période gallo-romaine, avec la cité de Noviodunum, par les conflits entre la France et l'Angleterre au Moyen Âge, puis par la Chouannerie. La canalisation de la Mayenne pendant la Renaissance permet un certain essor économique et jusqu'au , la culture et le tissage du lin ainsi que la métallurgie sont les principales activités départementales. Au , après un exode rural marqué, la Mayenne réoriente son économie vers la production laitière et l'agro-alimentaire.

Le département comptait et 261 communes en 2013. Malgré une croissance démographique modérée, la Mayenne demeure le département le moins peuplé des Pays de la Loire. Elle ne possède que trois villes de plus de : le chef-lieu, Laval, et les deux sous-préfectures, Château-Gontier et Mayenne.

La Mayenne se trouve au nord-ouest de la France, elle est sans accès à la mer, mais Landivy, située au nord-ouest du département, n'est qu'à une trentaine de kilomètres de la baie du mont Saint-Michel. Le département forme un rectangle régulier, orienté sur un axe nord-sud, c'est-à-dire selon le cours de la rivière Mayenne. À l'ouest, il est limitrophe de la région Bretagne avec l'Ille-et-Vilaine, et au nord de la Basse-Normandie, avec la Manche et l'Orne. À l'est, il est bordé par la Sarthe, et au sud, par le Maine-et-Loire, deux départements qui font également partie des Pays de la Loire. La Mayenne est au carrefour des routes reliant Paris à la Bretagne et Caen à Angers et Nantes.

La Mayenne possède dans son ensemble un relief peu marqué, c'est une région de transition entre la Bretagne, la Normandie et l'Anjou, des régions très différentes. Le département est surtout fait de collines arrondies et de vallées peu profondes. Néanmoins, il se trouve sur la partie orientale du massif armoricain et s'élève en moyenne à au-dessus du niveau de la mer. Son point culminant, le mont des Avaloirs, atteint . C'est le point le plus haut du massif ainsi que de tout le Grand Ouest français. Le mont se trouve au nord-est du département, près de l'Orne et de la Sarthe. D'autres sommets se répartissent dans le nord-est de la Mayenne, comme le mont Rochard, le Montaigu et le Tertre Ganne.

À cause d'un relief relativement uniforme, la Mayenne peut difficilement être divisée en régions naturelles distinctes. Six espaces ont toutefois été distingués par des organismes locaux comme la Direction régionale de l’environnement des Pays de la Loire :

Le département fait . Il est couvert à 80 % par des terres agricoles et à 7,9 % par les surfaces urbanisées. Les forêts occupent 7,1 % de la superficie départementale, les autres milieux naturels, 3,3 % et le bocage, 2,7 %. La Mayenne compte par ailleurs de surface agricole utile, dont 60 % de surfaces fourragères. Le bocage structure une grande partie du département, et les prairies naturelles sont aussi un élément caractéristique de la Mayenne. Les milieux naturels, comme les forêts et les landes, sont surtout présents dans le nord-est, autour du mont des Avaloirs et des Coëvrons. La surface agricole diminue progressivement, perdant en moyenne par an. Cette perte se fait au profit de l'urbanisation, qui gagne , le reste étant surtout composé de terres reboisées.

L'urbanisation des terres agricoles concerne à 58 % la construction de logements. Elle est très marquée depuis les années 1970, lorsque les agglomérations ont commencé à s'étendre de façon rapide. Suivent les zones d'activités et les infrastructures de transport comme les routes et les voies ferrées. La construction de la LGV Bretagne-Pays de la Loire a par exemple fait disparaître plus de de terres agricoles.

L'ensemble du territoire mayennais fait partie du massif armoricain, et son sous-sol s'est formé au Paléozoïque, notamment grâce à l'activité volcanique, puis a été recouvert par des formations tertiaires diverses. Une grande partie du département, environ , est ancienne, puisqu'elle est constituée de schistes précambriens. Ces schistes forment le sous-sol de la Mayenne angevine et de petits territoires épars, comme le sud de Laval et les environs d'Évron, de Chailland, de Bais, d'Ernée ou encore de Villaines-la-Juhel.

La moitié nord du département, avec le mont des Avaloirs, la forêt de Pail, la forêt de Charnie et les Coëvrons, est surtout constituée de grès du Silurien. Les schistes de Renazé, exploités jusqu'au , ainsi que ceux de Javron datent de la même époque. Les sous-sols du Silurien représentent . Le nord de la Mayenne est aussi partiellement formé d'anciens terrains éruptifs ayant laissé du gneiss et du granite. Ces terrains représentent .

Au centre, le département est marqué par le bassin sédimentaire de Laval datant du Carbonifère et du Dévonien. Il s'étend d'est en ouest de Sablé-sur-Sarthe à Saint-Pierre-la-Cour et sa faible largeur est comprise entre Montigné-le-Brillant et Louverné. Similaire au bassin de Châteaulin dans le Finistère, il forme une zone de faiblesse au sein du massif armoricain, il est comprimé entre les blocs rigides mancellien au nord et rennais au sud. Il est composé de schistes et de calcaires carbonifères formant des couches plissées. Le marbre et la chaux y a été abondamment exploitée, notamment à Saint-Berthevin, Argentré, Louverné, Grez-en-Bouère et Saint-Pierre-la-Cour. Le bassin regroupe environ .

Enfin, les vallées fluviales autour de Château-Gontier, Gorron, Ambrières, Mayenne, Craon, Meslay-du-Maine ou encore Évron contiennent des alluvions déposées pendant l'Ère tertiaire, notamment du gravier. Ces alluvions concernent environ .

Le département est presque entièrement compris dans le bassin versant de la Mayenne, vaste de . La Mayenne est une rivière longue de qui prend sa source sur le versant nord du mont des Avaloirs, dans l'Orne. Elle traverse le département du nord au sud, puis elle descend vers Angers, où, avec la Sarthe et le Loir, elle forme la Maine, un affluent de la Loire. Le bassin versant de la Mayenne s'étend donc dans l'Orne et le Maine-et-Loire, mais aussi dans la Manche et l'Ille-et-Vilaine. Le département de la Mayenne possède toutefois 71 % du bassin. La Mayenne possède un certain nombre d'affluents, comme l'Aisne, l'Aron, la Jouanne et l'Ouette sur la rive gauche, et la Varenne, la Colmont, l'Ernée et le Vicoin sur la rive droite. L'Oudon prend sa source dans le département mais se jette dans la Mayenne en Maine-et-Loire, près du Lion-d'Angers. Le bassin compte au total de cours d'eau.
Le bassin de la Mayenne se trouve principalement sur un sous-sol de schistes et de granites. On y trouve deux types d’aquifères : les aquifères d’interstices, situées dans les sols perméables constitués de sables, de grès
altérés ou de calcaires, et les aquifères de fissures et de fracturation, dans lesquelles l'eau circule dans les fissures non argilisées. Ce dernier type est le plus important et il forme un vaste réseau d'eaux souterraines. Chaque année, près de six millions de mètres cubes d'eau sont prélevés dans ces réserves, et le bassin possède un potentiel annuel estimé à environ douze millions de mètres cubes.

La Mayenne est navigable jusqu'à la ville de Mayenne, mais son intérêt économique se limite au tourisme fluvial (pêche, voile, randonnée sur les chemins de halage, etc). En outre, des petits barrages et 24 installations hydroélectriques construits sur la Mayenne fournissent près de 2 % de la consommation électrique du bassin versant, qui totalise .

Une mince bande de territoire, située le long de la limite orientale du département, fait partie du bassin versant de la Sarthe. La rivière y compte plusieurs affluents, comme le Merdereau, la Vaudelle, l'Orthe, la Vaige, la Taude et l'Erve, qui arrose les Coëvrons. Cette dernière a creusé les sols calcaires autour de Saulges, formant un paysage karstique comprenant un canyon et des grottes. L'extrémité nord-ouest de la Mayenne fait de son côté partie des bassins de la Sélune et du Couesnon, deux fleuves côtiers qui se jettent dans la baie du mont Saint-Michel. Enfin, quelques territoires de l'ouest de la Mayenne font partie du bassin de la Vilaine, qui y prend sa source.

La Mayenne, grâce à la proximité de la Manche et de l'océan Atlantique, possède un climat océanique. Le temps est souvent humide, avec des températures régulières et modérées. Les températures extrêmes, en hiver et en été, sont rares. Les précipitations sont étalées sur l'année et sont donc rarement abondantes. Le centre et le sud du département reçoivent d'eau par an. Le nord, au relief plus marqué, est aussi plus humide. Les régions autour de Landivy et de Pré-en-Pail reçoivent ainsi près de d'eau par an.

Données pour Laval :

La Mayenne compte de plantes, 63 espèces de mammifères, 280 espèces d’oiseaux, 16 espèces d’amphibiens, 11 espèces de reptiles et plusieurs milliers d'espèces d'insectes, encore peu étudiées. Le département compte plusieurs espaces naturels remarquables, comme des tourbières, des forêts et du bocage. Les tourbières se trouvent surtout dans le nord du département. On y trouve des espèces animales et végétales rares, comme des plantes carnivores (droséras et grassette), vénéneuses (fritillaire pintade) ou herbacées (linaigrette à feuilles étroites, potentille des marais). Les tourbières sont généralement entourées de bois d'aulnes et de frênes, au pied desquels poussent des fougères ou des plantes basses comme la dorine à feuilles opposées. La faune comprend notamment des libellules et des araignées.
Le département possède de grandes étendues de forêts, comme la forêt de Concise, la forêt de Mayenne ou celle de Charnie. Le nord de la Mayenne, et surtout les massifs du mont des Avaloirs et des Coëvrons, sont cependant plus boisés que le sud. La plupart des forêts mayennaises font moins d'un hectare ; la futaie et les espèces feuillues, notamment le chêne, dominent. Les forêts abritent le chevreuil, le petit sylvain, la salamandre tachetée, l'amanite tue-mouche ou encore le pic mar. Le bocage sert de refuge à un grand nombre d'espèces très variées : le blaireau européen, la chevêche d'Athéna, la reinette verte, l'ancolie commune, la couleuvre d'Esculape…
Les pelouses sèches, qui couvrent les sols calcaires ou gréseux, sont également riches en faune et en flore. Elles abritent la vipère aspic, l'azuré du serpolet, un papillon, l'ophrys abeille, une orchidée, ou l'œdipode turquoise, un criquet. Les landes, situées principalement dans le nord, sont peuplées d'ajoncs nains, de bruyères, d'argiopes, d'engoulevents ou de fauvettes. Les anciennes carrières sont le refuge de chauves-souris, de géraniums, de batraciens, ou encore de platanthères, qui sont des orchidées. Dans les villes et les villages, on trouve aussi le hérisson d'Europe, le choucas des tours, l'hirondelle de fenêtre ou encore la linaire cymbalère, qui pousse sur les vieux murs. Enfin, les rivières et les étangs sont le refuge de l'anguille, du triton crêté, de la loutre d'Europe, du martin-pêcheur, de la couleuvre à collier, de la gallinule poule d'eau, de la renoncule, de l'iris des marais, du sagittaire flèche d'eau, ou de l'isopyre faux-pigamon, une petite plante vénéneuse.

La périurbanisation, qui entraîne le mitage des terres agricoles et la disparition du bocage, est la principale menace qui pèse sur les paysages mayennais. Le département compte un site naturel classé, Ahuillé, ainsi que huit autres sites inscrits. Le parc naturel régional Normandie-Maine, qui s'étend aux confins de la Mayenne, de l'Orne et de la Sarthe, englobe plusieurs communes du nord-est du département, comme Ambrières-les-Vallées, Pré-en-Pail et Lassay-les-Châteaux. Ce parc recouvre également la plupart des sommets mayennais, comme le mont des Avaloirs. Le département est aussi soumis à des problèmes de pollution liés au traitement des déchets. L'usine Aprochim de Grez-en-Bouère a par exemple été accusée en 2011 d'avoir diffusé du PCB dans l'environnement. Plusieurs troupeaux de bovins avaient dû être abattus. En outre, la qualité de l'eau n'est pas toujours optimale, notamment pour les affluents en rive gauche de la Mayenne. Ces cours d'eau contiennent des quantités non négligeables de nitrates rejetés par l'industrie et l'agriculture, mais la situation tend à s'améliorer, et la Mayenne doit atteindre un bon état écologique avant 2021. Par ailleurs, les nombreux barrages construits sur les rivières sont des obstacles infranchissables pour les poissons migrateurs.

Au , la Mayenne possédait de routes, dont d'autoroutes, de routes nationales, de routes départementales et de voies communales. Le département occupait ainsi le au niveau national sur les métropolitains pour la longueur du réseau routier, et le pour la densité, avec de route par km de territoire.

La Mayenne est traversée par une seule autoroute, l'A81, qui relie Le Mans à Rennes et traverse le département d'est en ouest, en desservant Laval. L'A81 se trouve par ailleurs sur la route entre Paris et la Bretagne. Le nord du département est traversé par un axe parallèle, la RN 12, qui relie Paris à Brest en passant par Alençon. Elle dessert notamment Pré-en-Pail, la ville de Mayenne, Ernée et entre en Ille-et-Vilaine par Fougères.

Laval est le principal nœud routier mayennais, et de nombreuses routes partent en étoile depuis la ville. C'est notamment le cas de la D 31, qui dessert Ernée et Landivy et qui est partiellement en . La vallée de la Mayenne est également un axe important, puisqu'elle permet de relier Laval aux deux sous-préfectures, Château-Gontier et Mayenne, et traverse le département du nord au sud. Elle est empruntée par la RN 162, qui se poursuit jusqu'au Lion d'Angers, en Maine-et-Loire.

La Compagnie des chemins de fer de l'Ouest en 1855 ouvre la section Le Mans-Laval puis le est inauguré les 73 kilomètres restant reliant Laval à Rennes. L'événement suscite ce commentaire dans "l'Illustration" daté du : 

En 1959, le réseau ferroviaire mayennais totalisait de voies, huit lignes et 43 gares et haltes. La plupart des lignes a été déclassée au cours de la seconde moitié du et seule la ligne de Paris-Montparnasse à Brest sert encore au transport de voyageurs. Cette ligne est l'une des plus fréquentées de France et elle est empruntée par des TGV Atlantique ainsi que par des TER Pays de la Loire. Ces derniers desservent les quelques gares mayennaises, comme celle de Laval, d'Évron, de Montsûrs, de Port-Brillet ou de Saint-Pierre-la-Cour. Ils permettent aussi des liaisons avec des gares de la Sarthe et d'Ille-et-Vilaine, notamment Le Mans et Rennes. La gare de Laval est la seule à être desservie par des TGV, qui effectuent pour la plupart des liaisons entre Paris et des grandes gares bretonnes.

La LGV Atlantique, qui dessert le Grand Ouest, s'arrête avant Le Mans jusqu'en 2017, les TGV poursuivant leur trajet en Mayenne puis en Bretagne roulent alors sur une ligne classique. Afin de remédier à cette situation et de réduire les temps de trajet, Réseau ferré de France lance la construction de la LGV Bretagne-Pays de la Loire, qui s'étend du Mans jusqu'à Rennes; elle est inaugurée le . La Mayenne est traversée d'est en ouest par cette nouvelle LGV qui s'aligne parallèlement à la ligne classique de Paris-Montparnasse à Brest.

La Mayenne possède un petit aéroport, celui de Laval-Entrammes, qui ne propose aucun vol commercial régulier. L'aéroport proposant ce type de services le plus proche est celui de Rennes Saint-Jacques. Le transport fluvial, autrefois très important pour l'économie locale, a été abandonné au profit du transport routier, et la Mayenne, l'unique cours d'eau navigable, n'est plus utilisée que par les plaisanciers. Elle n'est d'ailleurs pas gérée par l'établissement public "Voies navigables de France".

La carence en lignes ferroviaires est compensée par un réseau départemental d'autocars, le réseau Pégase, qui dessert de nombreuses communes mayennaises ainsi que des villes situées dans des départements limitrophes, comme Châteaubriant et Sablé-sur-Sarthe. Par ailleurs, le réseau TER Pays de la Loire relie en autocar Laval et Château-Gontier à Nantes et Angers. L'agglomération de Laval possède enfin son propre réseau de bus, géré par les Transports urbains lavallois.

La Mayenne possède un réseau de pistes cyclables conséquent. Il englobe notamment d'anciennes voies ferrées reconverties en voies vertes ainsi qu'un chemin de halage de qui suit le cours de la Mayenne. À Laval, il existe un système de vélos en libre-service, appelé Vélitul.

Le département porte le nom de la rivière Mayenne selon un processus courant de transfert d'un hydronyme à un toponyme.

Le nom de la rivière est attesté sous les formes "Meduana" au , puis "Medena", "Mehena", "Maesne" et "Meenne". La forme "Mayenne" est mentionnée pour la première fois au .

Hans Krahe a vu dans "Meduana" un dérivé de la racine indo-européenne "*medh-" « du milieu ». Il s'agit plus probablement du mot indo-européen qui s'est perpétué en celtique sous la forme "*medu" « miel, hydromel ». Il se retrouve dans l'irlandais "meda" (génitif de "mid"), le gallois "medd" et le breton "mez" qui signifient tous « hydromel ». "Medu-" est identifié également dans les noms de rivières "Meduacus" (aujourd'hui la Brenta) en Cisalpine, la Mionnaz en Suisse (de "*Medu-ona").

Le second élément "-ana", appellatif ou suffixe est fréquemment attesté dans les noms de rivières. Il s'agit peut-être du gaulois "ana" « marais » attesté dans le glossaire de Vienne que l'on rencontrerait également dans le nom de la Seine, "Sequana".

"Mayenne" est aussi le nom de la deuxième ville du département. La Mayenne est l'un des département français, avec la Corrèze et le Doubs, qui partage son nom à la fois avec une rivière et avec une commune.

À Angers, la Mayenne rencontre la Sarthe et le Loir et les trois cours d'eau forment la Maine, dont le nom dérive de « Mayenne ». En revanche, la province du Maine, à laquelle faisaient partie les deux tiers du département, tient son nom de la tribu gauloise des Cénomans, tout comme Le Mans.

La région des Coëvrons, située dans l'est du département, est la plus riche en vestiges préhistoriques. Les traces les plus anciennes d'occupation humaine datent de 400 000 av. J.-C., soit pendant le Paléolithique inférieur. Les environs de Saulges semblent ensuite avoir été occupés par l'Homme de Néandertal, puisque des objets du Moustérien y ont été découverts. Ensuite, la région est régulièrement occupée au Paléolithique supérieur. La commune de Saulges, située dans un canyon calcaire, comprend une trentaine de grottes et d'abris sous roche. Deux de ces grottes, la grotte Margot et la Mayenne-Sciences, sont ornées. Le site de Saulges est surtout occupé pendant l'Aurignacien puis au Solutréen. Le Magdalénien et l'Azilien sont eux aussi représentés, mais une occupation gravettienne n'est pas totalement confirmée.
Au Néolithique, plusieurs mégalithes sont édifiés, comme les menhirs de Bouchamps-lès-Craon, dans le sud-ouest, et de Saint-Thomas-de-Courceriers, dans le nord, l'allée couverte de la Hamelinière et le dolmen des Erves à Sainte-Suzanne. Pendant la Protohistoire, la région est envahie par les Celtes et les deux-tiers du département sont occupés par les Aulerques Diablintes tandis que le sud est occupé par les Andécaves. Les Diablites ont laissé plusieurs traces d'occupation, dont l'oppidum de Moulay, l'un des plus grands oppida de Gaule. Il possédait deux rangées de remparts et couvrait . Il fut vraisemblablement la capitale des Diablintes du au .

Les Romains envahissent l'Ouest de la Gaule en 57 av. J.-C. Auguste place la Mayenne actuelle dans la Gaule lyonnaise et l'oppidum de Moulay est délaissé au profit d'une ville nouvelle, "Noviodunum" (actuelle Jublains). Les Gaulois avaient occupé le site depuis environ 450 av. J.-C. et y avaient fondé un sanctuaire en bois. Il est remplacé par un temple vers 66 ap. J.-C. Une vingtaine d'années plus tard, les Romains construisent un théâtre et Noviodunum devient une ville romaine à part entière. À la fin du , alors que les invasions barbares commencent, l'entrepôt fortifié est probablement transformé en forteresse. Les Romains laissent d'autres traces à travers le département, comme les thermes d'Entrammes, près de Laval, la forteresse-garnison du Rubricaire, près de Sainte-Gemmes-le-Robert sur les pentes du Mont Rochard. Ils construisent aussi la voie reliant Le Mans à Corseul, qui traversait la Mayenne d'est en ouest.

Pendant les invasions barbares, l'autorité romaine s'affaiblit et des révoltes éclatent régulièrement contre la pression fiscale. Noviodunum périclite et perd son statut de chef-lieu des Diablintes au . Son territoire est rattaché à celui des Cénomans, dont la capitale était Le Mans. Cette annexion est le premier acte fondateur de la province du Maine. Noviodunum demeure toutefois occupée jusqu'à l'époque carolingienne, puis disparait totalement après 900, lors de la naissance de la ville de Mayenne.

Les Francs s'installent durablement dans la région au où ils cohabitent avec les Gallo-romains avant d'être assimilés. Au même moment, l'Armorique connaît une importante immigration de Bretons, venus de Grande-Bretagne. Ceux-ci établissent des petits royaumes ennemis des Francs. Les guerres entre Francs et Bretons sont d'ailleurs fréquentes entre le et le , et la Mayenne actuelle est soumise à plusieurs attaques bretonnes. Pour sécuriser leur frontière, les Francs mettent en place la Marche de Bretagne, une zone tampon comprenant plusieurs forteresses et qui s'étale du Poitou à la Normandie.
La ville de Mayenne, qui fait partie intégrante de la Marche de Bretagne, apparaît dès le , avec la fondation du château, censé protéger un gué sur la Mayenne. Il est attaqué par les Bretons entre 840 et 870 puis il est reconstruit en pierres. La marche est alors contrôlée par Charles le Chauve, petit-fils de Charlemagne, qui donne probablement le château aux premiers comtes du Maine. De nombreux autres châteaux forts sont construits vers l'an mille, notamment ceux de Laval, Lassay, Château-Gontier et Sainte-Suzanne. Ils sont généralement l'œuvre de nouveaux seigneurs à qui les comtes du Maine délèguent des pouvoirs locaux. Ces seigneurs fondent de grandes familles comme les Scépeaux, les Mayenne, les Laval ou encore les Beauvau dans la partie angevine du département, qui appartenait alors aux comtes d'Anjou.
Au même moment, le bocage remplace peu à peu la forêt. La plupart des villes et villages actuels apparaissent. Des constructions à caractère religieux sont également entreprises, comme l'abbaye de la Roë et celle d'Évron. Ces abbayes attirent des moines venus d'Aquitaine et d'Auvergne qui participent activement au défrichement du territoire. Au , Béatrix de Gâvre, épouse de Guy IX de Laval, fait venir des tisserands du comté de Flandre dont elle est originaire. Le tissage du lin devient alors la principale activité économique de Laval et les marais alentours sont comblés pour cultiver la plante.

Le Maine est envahi au par Guillaume le Conquérant, qui attaque notamment Sainte-Suzanne et Mayenne. Le roi Philippe-Auguste rattache finalement le Maine au domaine royal en 1206, alors que la France s'oppose aux Plantagenêt qui possèdent notamment l'Angleterre, l'Anjou et l'Aquitaine. Lors de la guerre de Cent Ans, les Anglais occupent brièvement des places fortes du Maine comme Laval en 1428 et Mayenne de 1361 à 1364, puis de 1425 à 1448.

La guerre de Cent Ans laisse en Mayenne de nombreux villages ruinés. La situation économique est alors difficile, notamment parce qu'il y a de moins en moins de terres à défricher et que la population augmente rapidement. La culture du lin et, de manière plus anecdotique, celle du chanvre, commencées au Moyen Âge, deviennent alors des activités lucratives qui permet des exportations importantes. La culture et le tissage du lin demeurent des activités essentielles pour l'économie locale jusqu'au . Au , la production textile locale concernait par exemple de à .

François fait canaliser la Mayenne en 1536 et les travaux occasionnent un autre bouleversement. La rivière permet en effet d'importer les vins d'Anjou, de bonne qualité, et les vignes mayennaises sont arrachées et remplacées par des vergers.

L'industrialisation de la Mayenne, commencée avec le tissage du lin, est poursuivie au avec l'émergence des forges et des fours à chaux. Ces activités consomment une grande quantité de bois, et la forêt atteint ses limites actuelles au . Les forges industrielles apparaissent surtout au et elles produisent des poêles, des clous, du fer plié ou de la fonte moulée. Les forges les plus importantes se trouvent alors à Port-Brillet, Aron, Chailland, Moncor et au bord de l'Orthe. Elles emploient des centaines d'ouvriers. Ainsi, l'industrie concernait non seulement les villes, où se concentraient les tisserands, mais aussi les villages. Au , le territoire de la Mayenne peut être considéré dans son ensemble comme une région industrielle.

La Gabelle est à l'origine d'une importante économie parallèle dans le Bas-Maine. La Mayenne est pays de grande gabelle; elle est bordée au nord par le bailliage de Cotentin, pays de quart-bouillon, les salines après avoir reversé le quart de leur production à la ferme du roi vendaient un sel détaxé, et à l'ouest par la Bretagne, pays de franc-salé où aucune taxe n'est perçue sur le sel. le prix du sel d'un coté à l'autre pouvait aller de un à dix: 50 à 60 livres le minot en Mayenne contre 2 à 3 livres en Bretagne, Vitré ou Fougère par exemple à la Croixille. En 1760 une estimation évalue que mille personnes consomment un muid sept setiers sur les greniers du Maine pour vingt muids sur les dépôts de Vitré et de Fougères, le différentiel est le reflet du trafic. Si le Parlement de Bretagne ne répond que mollement aux injonctions de la ferme du roi, il s'oppose même à l'instruction des procédures et défend les privilèges de la province, la commission de Saumur, malgré l'étendue de sa juridiction recense la proportion de 40% des affaires traitées pour l'ouest de la Mayenne. Ce trafic favorise la complicité ou la solidarité entre les paysans pauvres, le bas-clergé, et la petite aristocratie; les manoirs comme les maisons simples de cette époque disposent souvent d'une cache pour le sel dans ou proche de la cheminée, les gabelous ayant un droit de perquisition. Un chef de bande de faux-sauniers, Jean Cottereau grâce à sa connaissance du terrain et son entrainement dans les escarmouches avec les gabelous devient un des chefs de la chouannerie sous le nom de Jean Chouan.

En 1789, au début de la Révolution française, les idées jacobines se répandent dans les milieux instruits, notamment chez les imprimeurs, les avocats, les commerçants et propriétaires aisés. Néanmoins, la Mayenne, comme une partie non négligeable de l'ouest de la France, est fortement royaliste et catholique.

La Mayenne, comme 82 autres départements, est créée le . Elle résulte de la partition du Maine en deux, le Haut-Maine, centré sur Le Mans, devient la Sarthe et le Bas-Maine, centré sur Laval, devient la Mayenne. L'Anjou, situé au sud, étant trop grand pour former un seul département, est diminué et devient le Maine-et-Loire (d'abord appelé « Mayenne-et-Loire »). Des parties de l'Anjou sont donc cédées à l'Indre-et-Loire, à la Vienne et aux Deux-Sèvres, et la frange nord est répartie entre la Mayenne et la Sarthe. La Sarthe reçoit la région de La Flèche, et la Mayenne celle de Craon et Château-Gontier. Le lin étant un trait marquant de l'économie mayennaise, sa limite méridionale de culture sert à déterminer la frontière entre la Mayenne et le Maine-et-Loire.

La constitution civile du clergé et l'instauration de la conscription mécontentent une grande part de la population. La Chouannerie, une guerre civile qui oppose royalistes et républicains à partir de 1792, naît en partie grâce à Jean Cottereau, dit Jean Chouan, originaire de Saint-Ouën-des-Toits. Il prend la tête du mouvement royaliste local et mène plusieurs batailles contre la Garde nationale, par exemple au Bourgneuf-la-Forêt. En 1793, la région connaît une certaine accalmie, mais au sud de la Loire, la guerre de Vendée est toujours en cours. Les royalistes vendéens, qui souhaitent obtenir du renfort d'Angleterre, tentent d'atteindre la côte normande et lancent la virée de Galerne. Avec l'aide des Chouans, ils traversent facilement la Mayenne, remportant des victoires à Laval, Entrammes ou Ernée, mais à Dol-de-Bretagne, les républicains mettent les Vendéens en déroute. Ils retraversent la Mayenne vers le sud puis ils perdent le Siège d'Angers et sont défaits par l'armée républicaine. Les Chouans poursuivent toutefois leur guérilla jusqu'à l'arrivée au pouvoir de Napoléon.

Après la victoire des coalisés à la bataille de Waterloo (18 juin 1815), le département est occupé par les troupes prussiennes de juin 1815 à novembre 1818. Avant la création des départements en 1790, certaines paroisses faisaient à la fois partie du Maine et de la Normandie, et elles avaient alors été coupées en deux, une partie rejoignant la Mayenne, l'autre l'Orne. Cette situation peu pratique persiste jusqu'en 1832 pour douze communes. Une loi puis une ordonnance font alors réunifier la plupart des communes, qui intègrent l'un des deux départements.

Au , la Mayenne connaît d'importants changements économiques, surtout après 1850. Alors que la France connaît dans son ensemble une industrialisation et une urbanisation massives, la Mayenne perd son industrie et ses villes ne s'étendent que très lentement. Le déclin de l'industrie mayennaise est principalement dû à l'agonie de la production textile. À la suite de l'effondrement des prix du lin, les tisserands préfèrent travailler du coton importé, et les agriculteurs mayennais cessent donc de cultiver du lin. Les tisserands sont installés dans les villes, et les campagnes perdent alors les liens qu'elles avaient longtemps entretenus avec le monde industriel. L'industrie textile, qui concernait l'ensemble du territoire au , est devenue citadine. Par ailleurs, malgré le passage du lin au coton, les tisserands mayennais n'ont pas renouvelé leurs méthodes de production. Ils travaillent encore chez-eux, et investissent dans le foncier plutôt que dans de nouvelles machines. Quelques filatures modernes sont néanmoins ouvertes par des entrepreneurs extérieurs à la Mayenne, par exemple à Laval et Fontaine-Daniel.

La métallurgie, très ancienne en Mayenne, a pratiquement disparu avant 1850, à cause de l'épuisement des gisements de fer. En 1848, il ne reste par exemple que cinq forges dans le département, et seule celle de Port-Brillet existe encore à la fin du siècle. De leur côté, les gisements de marbre, de grès, de granit et d'ardoises sont encore importants, et de nombreuses carrières voient le jour après l'ouverture de la voie ferrée Paris-Brest en 1856.

La production de chaux est une autre activité qui gagne en importance au , mais elle périclite vers 1890, à cause de l'arrivée des engrais chimiques. Les fours à chaux mayennais ont atteint leur apogée sous Napoléon III, et ils se trouvaient principalement autour de Laval. Il y avait par exemple 247 fours en Mayenne en 1872. Leur bref succès s'explique par la demande des agriculteurs de la région, qui avaient besoin de la chaux pour mettre en valeur les terrains pauvres. Grâce à elle, de grandes surfaces de lande ont disparu au profit du bocage et de l’élevage, ce qui a grandement contribué à transformer la Mayenne en département agricole. Cette transformation a aussi eu lieu grâce aux choix économiques des grandes fortunes mayennaises. Les principaux notables du département étaient alors de grands propriétaires fonciers conservateurs, qui préféraient sauvegarder les vieilles structures sociales et investir dans des activités sûres. En outre, les profits liés à l'agriculture ne servent pas aux aménagements d'intérêt économique, mais à l'embellisement des villes, à la reconstruction des églises paroissiales et à la construction de châteaux.
Pendant la guerre de 1940, les Allemands arrivent en Mayenne le et ils s'emparent des villes principales en moins d'un jour. Plusieurs villages, dont Chémeré-le-Roi et Villaines-la-Juhel, sont bombardés afin de faciliter l'avance des troupes. La Résistance commence ses actions en juin 1944 avec l'organisation d'un maquis à Lignières-Orgères. Au même moment, Laval est touchée par plusieurs bombardements alliés. Les Américains arrivent dans le nord de la Mayenne en août 1944 et doivent mener une bataille de quelques jours avant de prendre la ville de Mayenne le 12 août. Le reste du département est libéré avant le 8 août.

Pendant les années 1950, l'agriculture commence à se mécaniser. L'exode rural, important au début du , est cependant moins accentué que dans d'autres départements français. Ainsi, seulement 22 % des exploitations mayennaises disparaissent entre 1955 et 1980, contre 45 % en moyenne en France. Après 1970, les prairies fourragères diminuent au profit de la culture du maïs. Le bocage s'estompe lui aussi et l'élevage hors-sol devient important. Le retard industriel est partiellement rattrapé par l'implantation d'usines agro-alimentaires, notamment spécialisées dans la production laitière. La construction de l'autoroute A81 et de la LGV Atlantique intègrent le département au réseau national.

Il existe d'autres propositions de blasons départementaux, comme celle de 1950 de Robert Louis : "de gueules à la fasce ondée d'argent accompagnée en chef de six écussons d'or disposés 3-2-1 et en pointe d'un léopard d'or armé et lampassé d'azur". Ces armes reprennent celles de la ville de Mayenne et de Laval, la fasce suggérant le cours de la Mayenne. L'héraldiste avait aussi proposé dix ans plus tôt "de gueules à six écusson d'or disposés 3-2-1", qui est le blason de la ville de Mayenne.

Le conseil départemental de la Mayenne possède un logo ainsi qu'un drapeau qui le reprend sur fond blanc. Il représente un Pégase rouge et évoque la tradition équestre du département.

Pendant la Révolution, les Mayennais étaient majoritairement royalistes. Cette tendance n'était pas spécifique à la Mayenne, elle concernait tout le nord-ouest de la France. Ces sentiments royalistes ont perduré assez longtemps, et cela s'explique par la nature du royalisme mayennais : il s'agissait davantage d'un rejet du nouveau système que de la nostalgie de l'ancien. Le royalisme concernait toutes les couches de la population, mais il s'est atténué au cours du , notamment à cause de la mise en place de l'économie de marché, de l'exode rural et de la généralisation de l'enseignement. Il se transforme définitivement en conservatisme et en républicanisme modéré au début du . Dans le nord du département, un certain radicalisme laïc apparaît toutefois à la même époque, avant de disparaître au cours des années 1950. En 1965, avant la visite du général de Gaulle, le préfet de la Mayenne écrivait dans un rapport que .
Depuis les années 1950, l'électorat mayennais a surtout élu des députés issus de la droite centriste, appartenant par exemple au MRP, au CNI puis à l'UDF. L'UDR, le RPR puis l'UMP ont également eu plusieurs députés mayennais. La gauche s'est toutefois imposée à Laval dans les années 1970 et un premier député de gauche a été élu en 1986 dans la troisième circonscription. La première circonscription possède quant à elle un député socialiste depuis 2007.

Les deux sénateurs de la Mayenne sont membres de l'Union centriste. Il s'agit de François Zocchetto et Elisabeth Doineau. Les députés de la Mayenne sont Sylvie Pichot (PS), Guillaume Chevrollier (UMP) et Yannick Favennec (UDI).

À l’élection présidentielle de 2002, le premier tour a vu arriver en tête Jacques Chirac avec 25,82 %, suivi de Lionel Jospin avec 14,40 %, puis de Jean-Marie Le Pen avec 11,87 % et enfin François Bayrou avec 8,43 %. Au second tour, les électeurs ont voté à 88,59 % pour Jacques Chirac contre 11,41 % pour Jean-Marie Le Pen avec un taux d’abstention de 17,43 % (nationalement 82,21 % et 17,79 % ; abstention 20,29 %).

Au référendum sur le traité constitutionnel pour l’Europe du , les Mayennais ont voté pour la Constitution européenne, avec 52,37 % de Oui contre 47,63 % de Non avec un taux d’abstention de 28,48 % (France entière : Non à 54,67 % ; Oui à 45,33 %).

À l’élection présidentielle de 2007, le premier tour a vu se démarquer en tête Nicolas Sarkozy avec 32,12 %, suivi par François Bayrou avec 23,59 %, Ségolène Royal avec 22,63 %, puis Jean-Marie Le Pen avec 7,56 %, et Olivier Besancenot avec 4,11 %. Le second tour a vu arriver en tête Nicolas Sarkozy avec 55,45 % (résultat national : 53,06 %) contre 45,55 % pour Ségolène Royal (national : 46,94 %).

Enfin, à l’élection présidentielle de 2012, Nicolas Sarkozy est arrivé en tête au premier tour, avec 30,69 %, suivi par François Hollande avec 25,87 %, Marine Le Pen avec 14,77 %, puis François Bayrou avec 13,78 %, et Jean-Luc Mélenchon avec 8,30 %. Le second tour a vu arriver en tête Nicolas Sarkozy avec 53,07 % (résultat national : 48,36 %) contre 46,93 % pour François Hollande (national : 51,64 %).

Plusieurs personnalités qui ont contribué à la vie politique française sont liées à la Mayenne :

Le conseil départemental (anciennement "conseil général") est l'assemblée délibérante du département de la Mayenne, collectivité territoriale décentralisée. Son siège se trouve à Laval, rue Mazagran. Le conseil départemental comprend 32 conseillers généraux issus des 32 cantons de la Mayenne. Ces conseillers sont renouvelés par moitié tous les trois ans et se réunissent publiquement au moins une fois par trimestre. Les conseillers élisent un président ainsi qu'une commission permanente. Cette commission, composée du président du conseil et de 15 autres membres, délibère de dossiers délégués par le conseil. Les membres du conseil se réunissent aussi dans onze commissions spécialisées, qui concernent l'économie, la solidarité, les transports, l'environnement ou encore la culture. Le conseil départemental emploie environ , comme des archivistes, des assistantes sociales, des éclusiers, des mécaniciens, des bibliothécaires, des éducateurs spécialisés ou encore des cuisiniers.

Les principales compétences du conseil départemental sont :

En 2013, le budget primitif du conseil général voté le février a été arrêté à 312,263 millions d'euros. La plus grosse part est consacrée à l'action sociale avec soit près de 55 % du budget. (14 %) sont en outre attribués au développement des territoires, (9 %) à l'éducation et à l'enseignement, (7 %) à l'environnement, (6 %) à la voirie et aux transports, (4 %) à l'économie et l'emploi, (3 %) à la culture et (1 %) aux sports et à la jeunesse. L'argent provient essentiellement de la fiscalité indirecte, qui contribue à 32 %. Les dotations de l'État représentent 26 %, les recettes diverses 20 %, la fiscalité directe 15 %, et l'emprunt, 5 %. La dette départementale représentait en 2011, soit euros par habitant. Ce chiffre classe la Mayenne parmi les départements les moins endettés de France, la dette de la Sarthe représentant par exemple par habitants. La dette mayennaise a toutefois augmenté de 24,5 % entre 2001 et 2011 (la moyenne des départements français se situe à 55 %).

À l’issue des élections cantonales de 2011, neuf groupes sont actuellement constitués au sein de l’assemblée délibérante, six pour la majorité de droite totalisant vingt-six conseillers et trois pour l’opposition de gauche totalisant six conseillers. Le président du conseil est Olivier richefou qui a succédé à Jean Arthuis, membre de l'Alliance centriste, quand celui-ci a été élu député européen lors des dernières élections en 2014.
Le département de la Mayenne est composé de 261 communes, 33 cantons et 3 arrondissements. En raison d'un relief et d'un peuplement relativement uniformes, les divisions territoriales de la Mayenne présentent une certaine régularité. Ainsi, chaque arrondissement compte une dizaine de cantons et chaque canton regroupe en général une dizaine de communes. Par ailleurs, ce sont les trois seules villes de plus de qui sont chef-lieu d'arrondissement.

L'arrondissement de Château-Gontier regroupe les sept cantons les plus méridionaux, qui forment par ailleurs la Mayenne angevine : Bierné, Château-Gontier-Est, Château-Gontier-Ouest, Cossé-le-Vivien, Craon, Grez-en-Bouère et Saint-Aignan-sur-Roë. L'arrondissement totalise et .

L'arrondissement de Laval, qui correspond au centre du département, regroupe treize cantons : Argentré, Chailland, Évron, Loiron, Meslay-du-Maine, Montsûrs, Saint-Berthevin, Sainte-Suzanne et les cinq cantons lavallois (Laval-Est, Laval-Nord-Est, Laval Nord-Ouest, Laval-Saint-Nicolas et Laval-Sud-Ouest). Il totalise et .

L'arrondissement de Mayenne, qui correspond au tiers nord du département, compte douze cantons : Ambrières-les-Vallées, Bais, Couptrain, Ernée, Gorron, Le Horps, Landivy, Lassay-les-Châteaux, Mayenne-Est, Mayenne-Ouest, Pré-en-Pail et Villaines-la-Juhel. Il totalise et .

Du côté des intercommunalités, la Mayenne comptait début 2012 une communauté d'agglomération, Laval Agglo (20 communes), et 17 communautés de communes, dont une partagée avec la Sarthe (CC de Sablé-sur-Sarthe). Une seule commune, Saint-Georges-Buttavent, n'était rattachée à aucune intercommunalité. Laval Agglo totalise à elle seule , soit presque le tiers de la population départementale. En mars 2012, la préfecture a proposé un nouveau découpage des communautés, et leur nombre doit être réduit à onze avant 2014. Le Pays du Craonnais, la Région de Cossé-le-Vivien et Saint-Aignan-Renazé doivent fusionner pour former une seule communauté, tout comme Villaines-la-Juhel et les Avaloirs. Cinq autres communautés ont déjà fusionné fin 2012 pour former la communauté de communes des Coëvrons.

Les départements de la Mayenne, du Maine-et-Loire et de la Sarthe dépendent du ressort de la cour d'appel d'Angers pour l'ordre judiciaire. Le ressort couvre un territoire de comprenant plus de d'habitants et compte dont 5 sont situées en Mayenne : les tribunaux d'instance et de grande instance de Laval, installés dans le palais de justice de la ville, ainsi qu'un Conseil de prud'hommes, un tribunal de commerce et un tribunal des affaires de sécurité sociale, également situés à Laval. Le département est par ailleurs couvert par le tribunal administratif de Nantes, dont le ressort s'étend sur toute la région des Pays de la Loire. Les tribunaux d'instance de Mayenne et Château-Gontier ont été supprimés lors de la réforme de 2007 de la carte judiciaire. La Mayenne compte une maison d'arrêt, située à Laval.

En matière de police, la Mayenne dépend de la Direction interrégionale de la Police judiciaire (DIPJ) de Rennes (Ille-et-Vilaine), qui couvre l'ensemble des régions Bretagne, Pays de la Loire, Haute et Basse-Normandie. À cela s'ajoute la présence de la police municipale dans certaines communes et de la Direction départementale de la sécurité publique de la Mayenne (DDSP53), qui est la principale direction opérationnelle intégrée à la Direction Générale de la Police nationale, incluant notamment Police secours. La Mayenne possède aussi un groupement départemental de gendarmerie.

Le préfet de la Mayenne, c'est-à-dire le représentant de l'État dans le département, est M. Philippe Vignes depuis le 8 juillet 2013. Il remplace Corinne Orzechowski, nommée préfète hors cadre.

La Mayenne est le département le moins peuplé des Pays de la Loire et le deuxième plus petit dans le Grand Ouest après l'Orne voisine. Il est d'ailleurs moins peuplé que plusieurs départements de montagne, comme l'Ardèche, le Tarn ou encore la Savoie. En 2009, il se trouvait au national avec . Sa densité était alors de 59 habitants par km² lorsque la même année celle-ci s'élevait à 114 hab/km² en France métropolitaine. La population départementale était enfin estimée à en 2012. Elle est en hausse constante depuis 1962 () mais encore loin du record enregistré en 1861 ().

La Mayenne possède une croissance démographique régulière mais modérée, avec environ 0,7 % d'augmentation par an. Cette hausse est principalement due au solde naturel (0,4 %), le solde migratoire équivalent à 0,2 %. Ce dernier était par ailleurs légèrement négatif avant 1999. Le taux de natalité, qui atteignait dans les années 1960, est descendu à dans les années 2000, mais il demeure au-dessus de la moyenne française ().

Si ces tendances se poursuivent, la hausse de la population devrait se prolonger. Néanmoins, elle s'accompagnerait d'un vieillissement de la population et, à terme, le taux de croissance diminuerait (tout en restant malgré tout positif). Le solde migratoire supplanterait également le solde naturel. L'Insee prévoit une hausse de entre 2007 et 2040, avec un taux de croissance moyen de 0,4 %, en dessous de celui des Pays de la Loire (0,7 %).

Le faible solde migratoire mayennais est surtout lié aux flux entre la Mayenne et les départements limitrophes. En effet, ils sont généralement plus importants dans le sens de sortie. L'Ille-et-Vilaine est le premier département concerné. Entre 2007 et 2010, pour vers l'Ille-et-Vilaine, la Mayenne a reçu . Sur la même période, se sont installés en Maine-et-Loire, et ont rejoint la Mayenne. La Sarthe suit, avec un flux sortant de et un flux entrant de . La Mayenne entretient aussi des échanges avec la Loire-Atlantique, l'Orne, et de manière plus anecdotique avec la région parisienne, le Morbihan, le Calvados, la Manche et la Vendée. Le flux entre la Mayenne et ces départements a représenté moins de entre 2007 et 2010. Le flux entrant est supérieur au flux sortant uniquement pour la Sarthe, l'Orne et la région parisienne.

La Mayenne accueille peu d'immigrés. Ils étaient en 2006 et représentaient alors 2,3 % de la population départementale. La moyenne nationale se situait à 8,1 % et celle des Pays de la Loire à 2,7 %. La Mayenne est par ailleurs l'avant-dernier département de la région pour la proportion de personnes d'origine étrangère, derrière la Vendée (1,7 % en 2006). L'immigration vers la Mayenne et les Pays de la Loire en général est un phénomène récent. La région ne comptait que en 1968. L'immigration touche surtout les zones urbaines, Laval Agglomération comptant par exemple un tiers de la population immigrée du département. 9 % des immigrés viennent de Guinée et 50 % sont originaires de l'Union européenne, notamment du Royaume-Uni. Les Britanniques sont particulièrement présents dans le nord du département.

La Mayenne est un département principalement rural, structuré autour de l'axe Mayenne-Laval-Château-Gontier et des petites villes d'Évron, Ernée, Craon, Gorron
et Villaines-la-Juhel. Le nord et l'est sont les régions les plus touchées par le vieillissement de la population, tandis que les agglomérations de Laval, Mayenne et Château-Gontier ont une population relativement jeune, avec par exemple plus de trois habitants de moins de 25 ans pour un habitant de plus de 75 ans. Le nord et l'est sont aussi les régions les moins peuplées et les moins attractives. La densité de peuplement des Coëvrons se situe autour de 33 habitants par km, alors qu'elle atteint autour de Laval.

Les communes qui ont enregistré les plus fortes hausses de population entre 1999 et 2009 se trouvent principalement en deuxième couronne de Laval, où les terrains à construire sont plus nombreux et surtout moins chers. Tandis que la ville de Laval stagne autour des depuis les années 1970, des communes comme La Chapelle-Anthenaise, Châlons-du-Maine, Saint-Germain-le-Fouilloux et Louvigné ont vu leur population doubler en dix ans. Le département connaît de manière générale une forte rurbanisation, qui entraîne le mitage des terres agricoles et l'augmentation des déplacements en voiture.

Le département compte treize bassins de vie ruraux, centrés sur Château-Gontier, Évron, Mayenne, Cossé-le-Vivien, Craon, Ernée, Gorron, Landivy, Lassay-les-Châteaux, Meslay-du-Maine, Renazé, Villaines-la-Juhel et Pré-en-Pail. Certaines communes mayennaises appartiennent à des bassins de vie centrés sur des communes de départements voisins : Fougères, Vitré, La Guerche-de-Bretagne, Pouancé, Sablé-sur-Sarthe, Sillé-le-Guillaume, Alençon et La Ferté-Macé. La Mayenne comprend en outre trois aires urbaines, celles de Laval, Château-Gontier et Mayenne, qui regroupent . Quelques communes mayennaises font aussi partie des aires urbaines d'Alençon et de Sablé-sur-Sarthe. Enfin, la Mayenne compte douze unités urbaines, dont neuf ne comprennent qu'une seule commune : Ambrières-les-Vallées, Cossé-le-Vivien, Craon, Ernée, Évron, Gorron, Meslay-du-Maine, Renazé et Villaines-la-Juhel. Les autres sont celles de Laval (six communes), Mayenne et Château-Gontier (trois communes chacune).

49 % des Mayennais vivent dans une unité urbaine, un chiffre en dessous de la moyenne des Pays de la Loire, 69 %, et de celle de la France (hors Île-de-France), 73 %. Par ailleurs, 51 % de la population urbaine de Mayenne vit dans une ville-centre, c'est-à-dire une commune qui comprend au moins 50 % de la population de son unité urbaine. Ce chiffre est similaire à celui des Pays de la Loire, 49 %. En revanche, la part de population urbaine vivant dans des banlieues, c'est-à-dire des communes qui comprennent moins de 50 % de la population de leur unité urbaine, est inférieure en Mayenne (16 % dans le département, 27 % dans les Pays de la Loire). Enfin, 33 % des Mayennais vivent dans une ville isolée, c'est-à-dire une unité urbaine ne comptant qu'une seule commune (Pays de la Loire : 25 %).
Les principales communes du département étaient au :

Les pyramides des âges du département de la Mayenne, comparées sur les années 1999 et 2009, expriment le vieillissement de la population. La tranche des plus de 75 ans est ainsi passée de 8 à 11 % de la population totale en dix ans, tandis que la tranche des 15-29 ans a reculé de 20 à 17 %. La part des 45-59 ans a de son côté fortement augmenté, passant de 17 à 20 %, tandis que la part des 0-14 ans reste similaire.

Le nombre total de ménages mayennais est de en 2009. Le département compte une majorité de ménages avec famille, qui représentent 66,7 % du total. Les ménages composés d'une seule personne suivent avec 31,8 %, dont 17,6 % de femmes seules et 14,2 % d'hommes seuls. Ces ménages concernent surtout les plus de 65 ans, qui représentent 77 % du total, et dans une plus faible mesure les jeunes de 20-24 ans (21 %). Les ménages sans famille sont rares et ne représentent que 1,5 % du total des ménages. Les familles mayennaises sont plus traditionnelles que l'ensemble des familles françaises, avec une forte proportion de familles nombreuses (une sur quatre), surtout présentes dans les campagnes, notamment dans les Coëvrons et la Mayenne angevine. Les familles monoparentales, plutôt urbaines, sont minoritaires (une sur dix). Ces deux types de famille sont les plus sujettes à la précarité. Le pourcentage de divorcés chez les plus de 15 ans est faible : 5,1 %, et la majorité d'entre eux sont mariés : 53,2 %. Les célibataires représentent quant à eux 33,3 % des plus de 15 ans.

En 2009, le département de la Mayenne comptait s. Parmi ceux-ci, il y avait 87,1 % de résidences principales, 5,3 % de résidences secondaires et 7,6 % de logements vacants. Le pourcentage de résidences principales est légèrement au-dessus de la moyenne en France métropolitaine, établie à 83,5 % pour la même année.

En outre la même année, les maisons individuelles représentaient 79,9 % des logements. En France métropolitaine, ce taux était de 56 %.

64,9 % des occupants des résidences principales sont propriétaires, 33,8 % sont locataires et 1,3 % sont des personnes logées gratuitement. Les logements mayennais sont généralement grands : 45,7 % des résidences principales font 5 pièces ou plus, contre 31,2 % pour la France entière, et seulement 2,6 % d'entre eux ne font qu'une pièce (France entière : 6,2 %). Toujours en 2009, les maisons mayennaises comptaient en moyenne 4,8 pièces, et les appartements, 2,9 pièces. 97,1 % des logements comptaient une salle de bain avec une baignoire ou une douche.

En 2013, le prix moyen de l'immobilier à la vente au niveau départemental s'élevait à pour une maison. Les appartements étaient légèrement plus chers, avec une moyenne de . Il existe de fortes disparités entre secteurs, les prix étant les plus élevés dans l'agglomération lavalloise, et les plus bas dans le nord du département. Ainsi, à Lassay-les-Châteaux, le prix moyen par mètre carré pour une maison s'élevait à seulement , contre à Laval. La Mayenne est, avec l'Orne voisine, l'un des départements les moins chers dans l'Ouest pour l'immobilier.

En 2009, le département comptait sociaux. La Mayenne est le département des Pays de la Loire où la demande est la plus faible, et les logements sociaux y ont généralement un taux de vacance élevé, supérieur à 10 % dans le parc public pour certaines communes, notamment dans le nord-ouest.

Selon le recensement général de la population du janvier 2008, 5,3 % des logements disponibles dans le département étaient des résidences secondaires. Ce tableau indique les principales communes de la Mayenne dont les résidences secondaires et occasionnelles dépassent 10 % des logements totaux.

Sources :

Le Grand Ouest français est traditionnellement une région fortement attachée au Catholicisme. La déchristianisation amorcée au y a cependant fait fortement diminuer le nombre de pratiquants. La Mayenne, avec la Manche et les Deux-Sèvres, demeure toutefois l'un des départements les plus catholiques de France. Une enquête menée par l'IFOP en 2006 fait état dans ces départements de plus de 75 % de personnes se déclarant appartenir à la religion catholique (moyenne nationale hors Corse : 64 %). Ce chiffre regroupe aussi bien les pratiquants réguliers que les individus ayant uniquement un attachement culturel à l'Église. La Mayenne correspond au diocèse de Laval qui fait partie de la province ecclésiastique de Rennes.

Les autres religions sont pratiquement inexistantes. Le Protestantisme concerne moins de 1 % des Mayennais. Il a connu un certain succès dans le Bas-Maine au , puis s'est éteint après la révocation de l'édit de Nantes par Louis XIV et n'est redevenu visible qu'à la fin du . Le département ne compte qu'une seule église protestante, située à Laval. L'enquête de 2006 de l'IFOP donne également un très faible pourcentage de juifs et musulmans, inférieur à 1 %. La Mayenne ne compte pas de synagogue mais une mosquée et quelques salles de prière musulmanes. Le nombre de personnes sans religion est estimé entre 20 et 27 % de la population mayennaise (moyenne nationale : 27,6 %).

Comme dans le reste de la France, le français est largement majoritaire en Mayenne, et il côtoie parfois les langues liées à l'immigration. Le département possède aussi un parler qui lui est propre, le mayennais, également appelé "bas-mainiot" ou simplement "patois". Il s'agit d'une langue d'oïl, tout comme le gallo, le picard, le normand ou le poitevin. Il descend directement du latin populaire et a connu un développement parallèle à celui du français. Le mayennais n'est donc pas une déformation du français, ce sont deux parlers distincts, avec une origine commune et une histoire différente. Ses limites géographiques et linguistiques sont difficiles à définir, car il fait partie d'un continuum, c'est-à-dire un ensemble de parlers partageant les mêmes origines, qui se mélangent et se chevauchent sur plusieurs régions. Le mayennais est voisin du normand au nord, du gallo à l'ouest, de l'angevin au sud, et du sarthois (ou "haut-mainiot" et "manceau") à l'est. La définition géographique du mayennais varie selon les auteurs, certains incorporant le nord du département dans l'aire du normand, d'autres le rattachant à l'angevin, etc. Le mayennais est étudié par des universitaires depuis le mais il est de moins en moins pratiqué, notamment à cause de la généralisation du français et de la dépréciation dont il est victime. Certains termes mayennais sont toutefois employés par de nombreux habitants, ce qui a donné naissance à une variété régionale du français.

En matière d'éducation et d'enseignement, le département de la Mayenne appartient à la circonscription administrative de l'académie de Nantes qui regroupe également les départements de la Loire-Atlantique, du Maine-et-Loire, de la Sarthe et de la Vendée. La Mayenne compte 345 écoles maternelles et primaires, dont 237 sont publiques et 108 sont privées (soit 31 %). L'enseignement privé catholique est généralement beaucoup plus présent dans l'Ouest de la France que dans le reste du pays. La Mayenne possède aussi 42 collèges, dont 27 publics et 15 privés, 12 lycées généraux et technologiques, dont 6 publics et 6 privés, et 7 lycées professionnels, dont 4 publics et 3 privés. En 2012, ces établissements totalisaient élèves, dont dans le public et dans le privé.
Le taux d'élèves dans l'enseignement professionnel (24 %) et technologique (23 %), est plus important en Mayenne que dans la France entière (respectivement 19 % et 16 %). Le taux de réussite au baccalauréat est lui aussi généralement plus élevé en Mayenne, ainsi que dans l'ensemble des Pays de la Loire. Le département et la région avaient par exemple obtenu 91 % de réussite au bac général en 2011 (France entière : 88,2 %) et 89,5 % pour le bac professionnel (France entière : 83,6 %).

La Mayenne présente une forte proportion de personnes peu ou pas diplômée, et en 2009, seulement 17,4 % des Mayennais avaient un diplôme de l'enseignement supérieur. 26,7 % avaient alors un CAP ou un BEP, et 40,4 % n'avaient aucun diplôme ou bien seulement le BEPC ou le certificat d'études primaires. Les 14,5 % restant avaient un brevet professionnel ou un baccalauréat.

Quelques lycées offrent des formations en BTS ou en licence professionnelle, mais l'offre en enseignement supérieur est maigre et les étudiants mayennais partent le plus souvent étudier à Rennes, au Mans ou encore à Nantes. Laval possède cependant des antennes de l'Université du Maine, qui y maintient un IUT proposant des formations en biologie, en commerce et en informatique, ainsi qu'une faculté de droit. Ces deux antennes se trouvent sur le Centre universitaire de la Mayenne, qui regroupe aussi l'École d'ingénieurs du monde numérique (ESIEA) et l'École supérieure des techniques aéronautiques et de construction automobile (ESTACA).

L'Université de Nantes a aussi ouvert un IUFM, installé près du CNAM. Le Campus EC 53 regroupe de son côté l'enseignement supérieur catholique, avec l'UCO Laval, dépendante de l'Université catholique d'Angers. Laval compte aussi l'École supérieure de création interactive numérique (ESCIN), un centre de formation en alternance AFTEC, et deux écoles de commerce et de management, l'ESUP et Antaxia. La ville possède aussi quelques laboratoires de recherche, comme ceux de l'ESTACA, de l'ESIEA et de l'IUT. Ces laboratoires travaillent principalement sur la réalité virtuelle, la cryptologie la virologie, la biologie et l'électro-magnétique. La Mayenne comptait en 2012.

La Mayenne est couverte par de multiples médias audio-visuels, nationaux et locaux. Du côté de la télévision, la seule chaîne locale est France 3 Pays de la Loire. TV Mayenne.com est une chaîne de télévision diffusée par internet. Le principal émetteur du département est construit au sommet du mont Rochard. Il transmet les programmes de la TNT depuis 2006 et sert également aux radios France Culture, France Musique, France Bleu Mayenne et France Inter. En plus de France Bleu Mayenne, le département compte des bureaux de Radio Fidélité. France Bleu Mayenne est de loin la radio la plus écoutée, avec quotidiens.

La presse écrite locale compte deux journaux, Ouest-France, un quotidien qui couvre tout le Grand Ouest français, et le Courrier de la Mayenne.

En 2012, la Mayenne comptait de santé, dont infirmiers et infirmières diplômés d'État, généralistes, spécialistes, , 176 masseurs et kinésithérapeutes et 125 chirurgiens-dentistes. La densité de professionnels libéraux de santé pour le département s'élevait alors à 80 généralistes, 47 spécialistes et 93 infirmiers (IDE) pour . La Mayenne est le département des Pays de la Loire qui a la densité la plus faible de professionnels de santé, la région ayant par exemple 101 généralistes pour (France entière : 108). Les Pays de la Loire connaissent par ailleurs une baisse régulière des effectifs depuis plusieurs décennies. En Mayenne, les femmes ne représentent que 24 % des généralistes (Pays de la Loire : 28 %, France : 29 %).

Les niveaux de santé en Mayenne sont bons, avec par exemple une mortalité générale et une mortalité prématurée inférieures à la moyenne
des Pays de la Loire et de la France entière. Les admissions en maladie de longue durée, les cas de cancers, de diabète, de pathologies liées à une consommation excessive d'alcool et les chutes chez les plus de 65 ans sont aussi moins courants en Mayenne que dans l'ensemble de la région. Cependant, la Mayenne possède un nombre élevé de décès par accident de la route chez les femmes. Il existe aussi des disparités locales, et le centre du département est généralement le plus favorisé, tandis que la situation est moins bonne dans le nord.

Les sports les plus populaires en Mayenne sont le football, le cyclisme et l'équitation, Le jeu de palets traditionnel y est aussi pratiqué. Le département compte plusieurs infrastructures sportives importantes, comme le stade Francis-Le-Basser, qui compte plus de assises, le stade d'athlétisme de Laval, le golf de Changé, le circuit de moto-cross Raymond Demy et l'aérodrome d'Entrammes. La Mayenne possède aussi deux grands hippodromes, ceux de Laval et de Craon, où des courses hippiques ont régulièrement lieu. La tradition hippique mayennaise est aussi entretenue par de nombreux centres équestres et une dizaine d'éleveurs de chevaux. Le Centre d'entraînement régional de galop de l'ouest se trouve au sud du département, à cheval entre Senonnes en Mayenne et Pouancé en Maine-et-Loire. Les Boucles de la Mayenne est une course cycliste annuelle qui fait partie de l'UCI Europe Tour.

La vallée de la Mayenne compte de nombreux chemins de randonnée, notamment le chemin de halage, emprunté par les cavaliers, les cyclistes et les promeneurs. Le reste du département possède aussi un important maillage de voies vertes, aménagées à la place d'anciennes voies ferrées. Les sports nautiques peuvent être pratiqués dans quelques endroits, notamment près de Château-Gontier, où se trouve une base de ski nautique. La Mayenne, ses affluents ainsi que de nombreux étangs forment une vaste réserve de pêche.

La principale formation professionnelle mayennaise de football est le Stade lavallois Mayenne Football Club, club de football évoluant en Ligue 2. D'autres équipes incluent l'Union sportive changéenne, un club de football évoluant en championnat régional, les Francs Archers Laval, l'Union sportive lavalloise et l'ASPTT Laval. La Mayenne a vu naître quelques grands sportifs, comme les cyclistes François Pervis, pistard, huit fois champion du monde et deux records du monde, Freddy Bichot, Jacky Durand et Marc Madiot, le boxeur Jean-Claude Bouttier, l'athlète Manuela Montebrun et le jockey Olivier Peslier.

En 2011, la Mayenne comptait . La plupart d'entre elles, 63 %, étaient spécialisées dans les services, le commerce et les transports. Suivaient l'industrie (15 %), la construction (10 %), et les administrations et les établissements d'enseignement, de santé et d'action sociale (10 %). L'agriculture, qui représente 9 % des emplois et 10 % du PIB, est un secteur très important pour l'économie locale, mais elle est devancée par l'industrie, qui totalise 25 % des emplois. Les exportations mayennaises sont faibles et seules 2,5 % des entreprises locales exportent, contre 4 % des entreprises françaises. Les entreprises mayennaises sont aussi plus anciennes que la moyenne nationale, 42 % ont dix ans ou plus. En 2005, le PIB départemental s'élevait à , soit par habitants et par emploi. Ces deux derniers chiffres sont inférieurs à la moyenne nationale ( et ), mais supérieurs à ceux du Maine-et-Loire et de la Vendée. La même année, la valeur ajoutée provenait principalement des services, surtout marchands, (47 %), de l'industrie (21 %), des services administrés (20 %), de la construction (7,4 %) et de l'agriculture (7 %), une répartition similaire à celle des autres départements des Pays de la Loire.

En 2009, l'INSEE recensait en Mayenne foyers fiscaux, dont 50,6 % d'entre eux ont été imposables. Le revenu net total déclaré par tous les foyers fiscaux s'élevait pour cette année à (partagé à hauteur de 75 %( d'euros) par les foyers imposables et 25 % par les non-imposables). En outre le revenu net déclaré moyen s'élevait pour sa part à € par foyer fiscal ( € pour les foyers fiscaux imposables et € pour les non-imposables) et l'impôt moyen à . La même année en France, la part de foyers imposables était de 53,6 % et le revenu moyen de €, soit des chiffres sensiblement supérieurs à ceux constatés en Mayenne. Toujours en 2009, les revenus déclarés de la population savoyarde se sont répartis en 61,5 % de salaires, de 26 % de retraites, pensions et rentes, de 6,4 % de revenus non salariés et de 6,1 % d'autres revenus.

S'agissant du revenu selon la profession et catégorie socio-professionnelle (PCS) en 2009, l'INSEE mesure un revenu horaire en Mayenne de pour les cadres, pour les professions intermédiaires, pour les employés, pour les ouvriers qualifiés et pour les ouvriers non qualifiés.

Enfin, fin 2012, la Mayenne comptait plus de foyers bénéficiaires du Revenu de solidarité active (RSA). Ce nombre a augmenté de 8 % depuis 2010, avec le contexte de crise économique débuté en 2009.

Du côté de l’imposition sur le patrimoine, la Mayenne comptait en 2010 une seule commune de plus de possédant plus de 50 redevables de l'Impôt de solidarité sur la fortune (ISF) : Laval, avec 375 redevables et un montant moyen légèrement supérieur à la moyenne française ( à Laval et au niveau national).

En matière d'emploi, l'INSEE comptait en 2009 en Mayenne occupés, dont , soit 84,6 % du total. Le temps partiel représentait alors 15,6 % des actifs, et 46,6 % des actifs étaient des femmes.

En 2010, 33,9 % des emplois concernaient les services, les transports et le commerce. Suivaient l'administration publique, l'enseignement et l'action sociale, avec 28,1 % des emplois, l'industrie, 21,6 %, l'agriculture, 8,7 %, et la construction, 7,7 %. La grande majorité des emplois, 70 %, était alors à durée indéterminée. Les contrats à durée déterminée (CDD) ne représentaient que 5,5 % du total, les emplois en intérim, 2,6 %, et les stages et les contrats d'apprentissage, 3,1 %. Les emplois non salariés, 18,3 % du total, concernent surtout l'agriculture, 77,5 % des agriculteurs étant à leur compte.

En 1999, 51,3 % des Mayennais travaillaient hors de leur commune de résidence. En dix ans, la situation s'est amplifiée puisqu'en 2009, ce taux est passé à 60,6 %. Dans 51 % des cas, cette commune se situait elle aussi dans le département de la Mayenne.

La Mayenne connaît traditionnellement des taux de chômage faibles, en dessous des moyennes nationales et des Pays de la Loire. Néanmoins, la crise économique de 2009 a touché le département, et il a connu une hausse de 0,7 % entre janvier et novembre 2012 pour atteindre 6,4 % (Pays de la Loire : 8,3 %). Les disparitions d'emplois concernent surtout les missions d'intérim, les transports et l'industrie.

Au janvier 2011, l'Insee recensait en Mayenne , majoritairement anciennes avec 42 % d'entreprises de 10 ans ou plus et 14 % âgées de 6 à 9 ans. À l'inverse, les entreprises de moins d'un an ne concernaient que 12 % d'entre elles à cette période. Les entreprises mayennaises étaient alors les plus nombreuses dans le secteur du commerce, des transports et des services divers : soit 63,5 % du total. Suivaient la construction, avec 15,4, l'administration publique, l'enseignement, la santé et l'action sociale, 10,7 %, et l'industrie, 10,4 %.

Toujours en 2011, avec nouvelles entreprises, le taux de création moyen s'élevait donc à 12,1 %, en baisse par rapport à 2010, le plus fort étant de 12,9 % dans le secteur des transports, du commerce et des services. En 2009, la Mayenne comptait par ailleurs actifs, 69 % d'entre eux n'employant aucun salarié, 24 % employant de 1 à 9 salariés, 3 % de 10 à 19 salariés, 2 % de 20 à 49 salariés et 1,3 % employant 50 salariés ou plus (371 établissements).

La Mayenne compte plusieurs parcs d'activités, comme "Laval Mayenne Technopole" (unique technopole du département).

Ci-après sont listées les principales grandes entreprises dont le siège et/ou au moins un établissement sont situés en Mayenne. En gras sont indiquées les entreprises qui y ont leur siège.

L'agriculture est un secteur très important pour l'économie mayennaise. Le département se situe au français pour la viande bovine, au pour le porc et le lait et au pour les volailles. La Mayenne compte environ , dont certaines emploient plus de , notamment quatre entreprises laitières, sept abattoirs de volailles, un abattoir de porcs, une entreprise de découpe-transformation de viande et une usine de charcuterie-salaison. La production laitière domine largement le secteur et elle concerne ainsi que 40 % des exploitations. Par ailleurs, l'activité bovins en général occupe à elle-seule 30 % des actifs. Les autres activités importantes sont l'élevage hors-sol () et les ovins, les caprins et les autres herbivores (). La grande culture et la polyculture concernent chacune . Les races bovines les plus fréquentes en Mayenne sont la Rouge des prés (ou Maine-Anjou) pour la viande, et la Prim'Holstein et la Normande pour le lait.

Les activités agricoles mayennaises actuelles ont pour la plupart été adoptées récemment. Les élevages bovins, les pâtures et le bocage n'ont d'ailleurs rien de traditionnel. Jusqu'au , l'agriculture mayennaise était dominée par la culture du lin, et du chanvre dans une moindre mesure. Par ailleurs, la culture de la vigne était commune avant le et la canalisation de la Mayenne. Le vin d'Anjou, bien meilleur, pouvait désormais être importé facilement par bateau, et les vignes locales ont fait place à des vergers, dont certains existent encore. Ces vergers servaient notamment à produire du cidre. Au , les engrais chimiques permettent de revaloriser les terres les plus pauvres et le lin, devenu peu rentable, disparaît. Le bocage mayennais prend alors forme et l'élevage du bétail ainsi que la production de céréales s'imposent. Finalement, à la fin du , les agriculteurs mayennais se spécialisent définitivement dans l'élevage bovin et abandonnent le blé, qui se vendait mal. Enfin, au , le maïs en tant que plante fourragère se généralise et sa culture tend à uniformiser les paysages mayennais.
Le département de la Mayenne, tout comme les autres départements de France, est découpé depuis 1946 en quatre « petites régions agricoles » (PRA) au sein d'une région agricole française (RA). Les PRA furent déterminées en fonction d'une même vocation agricole dominante. Ces PRA sont numérotés par l'Insee.

Le secteur de l'industrie était en 2010 le en Mayenne avec , derrière le secteur des services et du commerce. Le Grand Ouest français s'est industrialisé plus tardivement et plus faiblement que des régions comme la Lorraine et le Nord-Pas-de-Calais, en privilégiant une grande diversité dans les activités. Il n'a d'ailleurs pas connu de désindustrialisation massive à la fin du . L'industrie mayennaise a toutefois tendance à diminuer au profit des services, perdant par exemple 1,3 % de ses emplois par an entre 1999 et 2006. Le département demeure toutefois au-dessus de la moyenne des Pays de la Loire, avec un quart d'emplois dans l'industrie, contre un cinquième pour la région entière.

L'industrie mayennaise a beaucoup changé au cours du . Vers 1900, les activités industrielles les plus communes étaient liées aux richesses du sous-sol (ardoise, marbre, antimoine, quartz aurifère, briqueteries) et aux productions agricoles (minoteries, brasserie, tanneries…). L'industrie textile, en déclin depuis la seconde moitié du , était surtout présente à Château-Gontier et Laval. Le tissage de la soie et du coton importés avait succédé à celui du lin cultivé localement et les tisseurs mayennais produisaient surtout de la toile à matelas. Toiles de Mayenne est une des dernières entreprises locales à perpétuer l'industrie textile mayennaise. Le département comprend aussi des ateliers de couture qui fournissent des grandes marques comme Hermès, Dior et Chanel. Le maroquinier Longchamp est par ailleurs implanté à Ernée et Château-Gontier.

L'industrie de la Mayenne s'appuie sur quelques grands groupes agroalimentaires comme Lactalis ou Bel qui exploitent les productions laitières. Le département compte aussi des usines de construction et de sous-traitance automobile, comme Gruau, Rapido et Valeo, des usines de plasturgie, de métallurgie et d'électronique ainsi que des imprimeries. La Mayenne fait en outre partie du territoire d'action de quatre pôles de compétitivité : "IDforCar" (sous-traitance automobile), "EMC2" (matériaux métalliques et composites), "Valorial" (agroalimentaire) et "Images et réseaux" (technologie numérique).

La Mayenne n'a pas autant d'arguments touristiques que les départements du littoral ou de montagne. Elle possède toutefois des atouts importants, comme des cours d'eau navigables, des paysages sauvages, un grand réseau de chemins de randonnée et quelques sites historiques et artistiques notoires. En 2010, le tourisme y a généré directs. Le secteur souffre néanmoins de la crise économique car le nombre de visiteurs et la durée des séjours diminuent.

Le département comptait 53 hôtels de tourisme en 2010. Ils totalisaient et 224 lits. Huit hôtels avaient une « une étoile », 27 en avaient deux et neuf en avaient trois. Ces hôtels trois étoiles se trouvent dans les trois plus grandes agglomérations (Laval, Mayenne et Château-Gontier), ainsi que dans les Coëvrons. Les hôtels mayennais ont enregistré en 2010, dont deux-tiers de clients d'affaires, avec un taux d'occupation moyenne de 53 % (Pays de la Loire : 54 %). La clientèle des hôtels mayennais est en grande majorité française et les étrangers ne représentaient que 8,3 % des nuitées en 2010 (15 % en 2008). Ils viennent surtout du Royaume-Uni, mais aussi d'Allemagne, de Belgique ou d'Italie.

Un restaurant est distingué d'une étoile au Guide Michelin, L'Éveil des Sens à Mayenne étoilé depuis février 2011.
La Mayenne comptait 19 établissements d'hôtellerie de plein air en 2010, dont 11 avec deux étoiles et 7 avec trois étoiles. Ils totalisaient ensemble nus, ou mobil-homes et . De mai à septembre 2010, ils ont enregistré , dont 14 % de nuitées étrangères (Britanniques, Néerlandais, Belges et Allemands, principalement). Les clients français sont surtout issus des Pays de la Loire et de régions limitrophes comme la Bretagne et la Basse-Normandie, mais aussi du Nord-Pas-de-Calais et de l'Île-de-France. Toujours en 2010, les ruraux du département ont enregistré .

Le site le plus fréquenté du département, le Refuge de l'Arche, un parc zoologique spécialisé dans l'accueil des animaux blessés ou abandonnés, a enregistré en 2010. Il était suivi par le musée de Jublains, les grottes de Saulges, le château de Sainte-Suzanne, le musée Robert-Tatin, le musée des sciences de Laval et le musée du château de Mayenne, des sites qui ont tous accueilli entre et . Le musée du Vieux-Château de Laval se distingue également avec environ la même année. Les groupes représentent 34 % des visites dans les sites payants mayennais, et les étrangers ne forment que 5 % du total.

La Mayenne a consommé tonnes d'équivalent pétrole en 2008. Les activités qui consomment le plus sont l'industrie (30 %) et les transports (23 %). Les bâtiments résidentiels consomment à eux-seuls 29 % de l'énergie dépensée en Mayenne. Le secteur tertiaire n'en consomme que 10 %, et l'agriculture, 8 %. Par ailleurs, 70 % de cette énergie est d'origine fossile, avec 54 % de pétrole et 16 % de gaz naturel, tandis que l'électricité représente 24 % de la consommation, et le bois, 8 %. Cette répartition est similaire à celles observées au niveau des Pays de la Loire et de la France entière, sauf pour le pétrole (France : 45 %, Pays de la Loire : 47 %). La consommation énergétique en Mayenne augmente régulièrement, principalement à cause de la construction de nouveaux logements. Entre 1990 et 2008, la consommation a ainsi augmenté de 22,5 %. Par ailleurs, plus de la moitié des logements mayennais ont été construits avant 1975, ils sont donc peu efficaces au niveau énergétique. Enfin, les Mayennais consomment beaucoup de carburants automobiles, à cause du caractère rural du département.

En 2010, le département ne produit que 6,5% de ses besoins électriques, mais ce pourcentage devrait passer à 28 % en 2020. 

Les centrales hydroélectriques sur la rivière Mayenne ne fournissaient que 0,3 % en 2007 de la consommation départementale. A l'initiative du Conseil Départemental et d'EDF, un renouvellement de 16 installations a été effectué entre 2007 et 2015, elles doivent désormais couvrir 1 % des besoins départementaux.

En 2010, les 22 éoliennes mayennaises fournissaient 4,7 % de la consommation. La Mayenne s'est fixé comme objectif d'avoir 100 éoliennes en service d'ici à 2020. Les zones à plus fort potentiel se trouvent principalement dans la moitié orientale et le Sud du département, autour de Saint-Cyr-en-Pail, Courcité, Vaiges, Saint-Pierre-sur-Erve, Saint-Denis-du-Maine, Azé, Cossé-le-Vivien, La Rouaudière. Au 1er janvier 2017, 12 parcs éoliens étaient raccordés au réseau haute tension pour une capacité de 99,5 MW. A cette même date, 20 autres parcs étaient en cours d'instruction ou autorisés mais non raccordés, ce qui représente une capacité supplémentaire potentielle de 211,8 MW. 

L'énergie solaire connait un développement limité dans le département du fait de faible ressource d'ensoleillement. La seule centrale photovoltaïque au sol en activité, d'une puissance de 2,4 MW, est située sur la commune de Fougerolles-du-Plessis accueille la seule centrale photovoltaïque au sol en activité en Mayenne. Plusieurs autres projets ont été portés ou autorisés, mais n'ont pas été mis en place.   

La Mayenne développe aussi les biocarburants, avec la méthanisation, qui doit fournir 3 % des besoins en 2020, et la cogénération issue des déchets, qui doit atteindre 2,5 %. 

L'entreprise espagnole Iberdrola avait annoncé la construction d'une centrale thermique à gaz en 2008, qui devait voir le jour à Villiers-Charlemagne. Le projet a cependant été abandonné en 2012 à cause de la crise économique.

La Mayenne compte neuf musées portant le label « Musée de France ». Laval possède à elle-seule deux de ces musées : celui du Vieux-Château, spécialisé dans l'art naïf et qui expose des toiles du Douanier Rousseau, de Séraphine de Senlis ou Lucien Le Guern, et le musée de sciences, qui se sert de son important fonds d'histoire naturelle pour proposer des expositions temporaires.

Le musée d'Art et d'Archéologie - Hôtel Fouquet, qui se trouve à Château-Gontier, expose des collections d'antiquités grecques et romaines, notamment des céramiques étrusques, des monnaies et des vases funéraires. Il possède aussi de la peinture italienne, hollandaise et française des , des livres anciens, des statues du Moyen Âge et un fonds d'art contemporain.
L'autre sous-préfecture du département, Mayenne, possède elle aussi un musée, le musée du château installé dans le château de Mayenne. Ses collections sont considérables et regroupent de nombreux artefacts découvert dans le château, fondé à l'époque carolingienne, ainsi que dans le reste du département. On peut notamment y voir des monnaies antiques et médiévales, des objets de la vie quotidienne au Moyen Âge (vaisselle, lampes à huile, outils, serrures, armes…), des sarcophages et des objets religieux. Les jeux de société médiévaux, incluant des pièces d'échecs et un tablier de jeu de tables, figurent parmi les pièces les mieux conservées au monde.
Le musée Robert-Tatin, situé à Cossé-le-Vivien, est un « environnement d'art » conçu de 1962 à 1983 par Robert Tatin, qui était peintre, architecte et céramiste. L'ensemble est composée de diverses constructions et statues réalisées en ciment peint, montrant des influences variées. Le site comprend par ailleurs des salles d'exposition où sont accrochées des toiles de l'artiste.

Le musée archéologique de Jublains, situé sur le site de la ville antique de Noviodunum, présente l'histoire du département pendant l'âge du fer et la période romaine. Les artefacts provenant de Noviodunum sont les plus nombreux et incluent un fragment d'aqueduc, des inscriptions, des décors peints, des objets de culte, etc.

Le musée de l'ardoise de Renazé est installé sur un vaste site minier abandonné en 1975. Il renferme surtout des outils, des machines et des photographies anciennes. Le petit musée municipal d'Ernée expose des collections d'archéologie s'étalant de la Préhistoire au Moyen Âge provenant de fouilles locales.

Le musée des tisserands d'Ambrières-les-Vallées retrace l'histoire du textile mayennais, centré sur le chanvre, il est installé dans trois authentiques maisons de tisserand du .

Le musée de prehistoire à Thorigné-en-Charnie installé près des Grottes de Saulges dans la vallée de l'Erve a ouvert en mars 2017.
Le département compte d'autres musées non classés, comme le Lactopôle de Laval, ouvert par l'entreprise Lactalis. Il présente les étapes de la production laitière ainsi que l'histoire de cette activité et le musée de l'auditoire de Sainte-Suzanne présente l'histoire des Coëvrons de l'âge du fer à la Chouannerie.

Non loin de Sainte-Suzanne, à Chammes, se trouve le Centre médiéval et culturel du Maine, aussi connu sous le nom de Ferté-Clairbois. Un petit château fort, une taverne, un jardin médiéval, des écuries et une chapelle y a été construits, et l'endroit sert à des reconstitutions historiques, notamment des tournois et des marchés.

A Ernée l'espace culturel Louis Derbré : atelier de fonderie, jardin de sculptures et théâtre de plein air, permet de voir quelques œuvres monumentales du sculpteur, d'autres sont installées dans la ville ou dans le département.

La Mayenne compte quelques grandes salles de spectacle. Le théâtre de Laval est la seule scène conventionnée du département. Il possède une grande salle de 583 places et propose surtout des représentations théâtrales, de la danse et de la musique classique. "Le Carré" de Château-Gontier est une scène nationale installée dans l'ancien couvent des Ursulines. L'institution gère aussi un espace d'art contemporain, qui occupe la chapelle du Genêteil. D'autres espaces du même genre se trouvent à Mayenne et Pontmain. Le théâtre de Mayenne est le dernier théâtre à l'italienne du département. Construit en 1889, il compte plus de . Plusieurs autres communes mayennaises possèdent aussi leur salle de spectacle.

L'Association départementale pour le développement de la musique et de la danse (ADDM 53) est un relais du ministère de la culture qui promeut et favorise la danse et la musique, par exemple en milieu scolaire et auprès des amateurs et des professionnels. Plusieurs communes ont par ailleurs un conservatoire de musique ou de danse, et le conseil général soutient l'Ensemble instrumental de la Mayenne, qui se produit régulièrement sur les scènes du département. Le 6PAR4 de Laval est une salle uniquement dédiée aux musiques actuelles.

Les archives départementales, situées à Laval, possèdent un grand nombre de fonds légués par des particuliers, surtout constitués de photographies anciennes. Elles ont aussi des collections de cartes postales, de monographies communales réalisées en 1899, des documents de l'abbaye Notre-Dame d'Évron, le chartrier du château de Fresnay, ou encore des archives de l'armée américaine. La bibliothèque départementale est installée à Saint-Berthevin, dans l'agglomération lavalloise. Elle renferme . La bibliothèque Albert Legendre de Laval possède un fonds patrimonial riche, qui comprend des incunables, des manuscrits, anciens, une généalogie des comtes de Laval et des œuvres d'Ambroise Paré et d'Alfred Jarry dans leur version originale.

Plusieurs festivals ont lieu tous les ans en Mayenne. La plupart sont dédiés aux musiques actuelles. Le festival Les 3 Éléphants se déroule à Laval en mai et propose de nombreux concerts payants et gratuits dans différents sites du centre-ville. En 2012, le festival a par exemple reçu en tête d'affiche Charlotte Gainsbourg, Baxter Dury, The Rapture ou encore C2C. Le festival Au Foin de la Rue se tient à Saint-Denis-de-Gastines et il a notamment reçu La Rue Kétanou, Emir Kusturica, Emily Loizeau et Wax Tailor. "Les Mouillotins Fanfare-On" est un festival qui a lieu tous les ans à Cuillé. En 2013, il reçoit Broussaï, Monsieur Roux et Les Fatals Picards. Le festival "Terra Incognita" se déroule à Carelles et il est spécialisé dans le rock et l'électro français et indépendants. L"'Été des 6 jeudis" propose six concerts de groupes différents dans des communes du nord du département.

Les "Nuits de la Mayenne", qui ont lieu en été dans de nombreux lieux historiques du département, proposent principalement du théâtre, mais aussi de la musique classique. Le "festival d’arts sacrés" d’Évron est de son côté spécialisé dans la musique religieuse. Le "Festival Ateliers Jazz de Meslay-Grez", qui a lieu à Meslay-du-Maine et dans des communes voisines, s'intéresse au jazz contemporain. Il a par exemple accueilli China Moses, Shai Maestro, Dhafer Youssef, Dee Dee Bridgewater et Archie Shepp.
Des spectacles de rue, des arts du cirque, des concerts d'artistes locaux ou encore des séances de cinéma en plein air sont aussi proposées à Laval et Mayenne en été. À la fin de l'année, la ville de Laval organise les "Lumières de Laval", une mise en lumière ambitieuse de la ville, renouvelée chaque année et accompagnée par un marché de Noël. Laval Virtual, aussi appelé « Rencontres internationales de la réalité virtuelle et des technologies convergentes » est un autre grand événement lavallois. Il a lieu au printemps et met en valeur l'industrie technologique de Laval, spécialisée dans la réalité virtuelle. Les trois premiers jours sont réservés aux professionnels, avec un salon, des conférences et des remises de prix, puis le week-end, Laval Virtual présente au public les dernières avancées technologiques dans le domaine.

Le département possède aussi un festival du cinéma étranger, "Les Reflets du Cinéma". Il présente pendant 15 jours des films étrangers selon des thématiques variant chaque année. Son président d'honneur est l'écrivain Jean-Loup Trassard. Le "Festival du premier roman" a pour vocation la découverte de nouveaux talents de la littérature. Il permet aux habitants de la Mayenne de faire partie des jurys qui évaluent les livres proposés et il propose divers cafés littéraires, tables rondes et séances dédicaces. Il a révélé des écrivains comme Sorj Chalandon, Laurent Gaudé, Jean-Christophe Rufin et Delphine de Vigan.

Les plus anciens exemples d'architecture civile remontent au Néolithique, avec par exemple le site d'Oisseau. L'oppidum de Moulay présente une organisation spatiale et des structures typiques de La Tène, et le site de Jublains contient les restes d'un forum, d'un théâtre et d'habitations romaines.

Des villes comme Château-Gontier et Laval comptent un certain nombre de maisons à encorbellement datant du Moyen Âge, ainsi que des hôtels particuliers construits du au . Ces hôtels, généralement construits en tuffeau, suivent les courants artistiques de leur époque de construction, allant de la Renaissance au néoclassicisme. Les villes et les bourgs comptent aussi souvent des maisons de tisserands. Petites, elles possèdent le plus souvent un perron en pierre ainsi qu'une cave humide semi-enterrée qui servait au tissage du lin et du chanvre dans une atmosphère suffisamment humide.
Les maisons rurales construites avant le étaient faites de schiste, de grès, de calcaire ou de granite. De plain pied, elles étaient couvertes de chaume ou d'ardoises et n'avaient en général qu'une seule porte et peu ou pas de fenêtres. Elles comptaient le plus souvent deux pièces (une salle commune et un cellier), un grenier ainsi qu'une étable et une porcherie attenantes.

Le chaume est interdit en 1844 afin de limiter les incendies, et, au cours du , les maisons sont progressivement améliorées : les sols en terre battue sont parfois carrelés, les greniers sont surélevés, et de nouveaux communs sont ajoutés, soit de l'autre côté de la cour, parallèlement à la maison, soit en prolongement de la maison elle-même. Les maisons de la Mayenne angevine, au sud du département, montrent des caractéristiques propres à l'Anjou, comme des linteaux sculptés ou jambages nervurés en tuffeau, datant du . Dans la vallée de l'Erve, la proximité de la Sarthe se voit dans l'usage de tuiles plates et de murs en grès roussart. Enfin, dans le nord-ouest, les toitures étaient souvent faites en bardeaux de châtaigner.

L'architecture du est illustrée par les maisons éclusières, les moulins à eau, les maisons de maître, quelques usines, comme des filatures et les ardoisières de Renazé, ainsi que par divers édifices publics construits dans les villes et les bourgs, notamment des hôtels de ville, des théâtres, des ponts et des immeubles d'habitation en tuffeau. Les courants du et l'architecture contemporaine sont peu visibles dans le département, et la Mayenne ne possède aucune œuvre majeure de cette époque, à l'exception du musée Robert-Tatin, comparable au Palais idéal du Facteur Cheval.

L'oppidum de Moulay, avec ses deux rangées de remparts ses est l'un des plus grands sites gaulois fortifiés de France. Le site de Jublains comprend de son côté une vaste forteresse de la fin du .

La Mayenne compte un grand nombre de châteaux et manoirs. Le plus ancien château fort du département est celui de Mayenne, de fondation carolingienne. Il a été réaménagé plusieurs fois au cours du Moyen Âge, mais l'importance des vestiges carolingiens en fait un site exceptionnel, classé site archéologique d'intérêt national.

Le château de Laval, qui domine la plus grande ville mayennaise, est un château fort du , redécoré au puis agrandi par une galerie à la Renaissance. Son donjon circulaire possède encore son hourd en bois d'origine. Le château de Lassay, qui date du , est une forteresse impressionnante rythmée par huit tours. Celui du Bois Thibault, situé à proximité, date de la même époque et remplace une construction disparue pendant la guerre de Cent Ans. Certaines villes possèdent encore une partie de leurs fortifications médiévales, comme Laval, Château-Gontier et Sainte-Suzanne.
La plupart des autres châteaux du département ont perdu tout caractère défensif après la Renaissance. C'est notamment le cas du château de Mausson, de celui de la Roche-Pichemer, de celui du Rocher, celui de Bourgon, celui de l'Escoublère, celui de Foulletorte ou encore de Sainte-Suzanne, qui compte toutefois un donjon du . Tous ces châteaux furent reconstruits sur une période allant du au et présentent des façades austères similaires à celles des châteaux bretons de la même époque. Ils sont ornés de fenêtres à meneaux, de grandes cheminées, de tourelles et de lucarnes à pignon. La plupart sont toujours habités.

Le néoclassicisme est peu représenté en Mayenne. Le château de Craon, datant de 1770, est toutefois représentatif des châteaux construits à cette époque, avec ses façades en tuffeau ornées de pilastres et de pots à feu. Le département compte enfin quelques demeures du , notamment de style néogothique, comme le château du Tertre d'Ambrières, dessiné par Eugène Viollet-le-Duc.

Un grand nombre d'églises paroissiales du Moyen Âge ont été détruites au pour faire place à de nouveaux édifices néogothiques plus grands. Cependant, le département compte encore plusieurs églises romanes du , L'Église Saint-Pierre de Saulges avec sa crypte pré-romane serait l'édifice religieux le plus ancien du département, les églises de Saulges et de Pritz sont en partie antérieures à l'an mil. Les exemples romans les plus représentatifs sont les églises d'Ambrières-les-Vallées, Bannes, Javron-les-Chapelles, Saint-Pierre-sur-Erve, Parné-sur-Roc, Saint-Martin de Laval et Saint-Jean Baptiste de Château-Gontier. Cette dernière église faisait à l'origine partie d'un prieuré fondé par des moines angevins. Elle fait soixante mètres de long et renferme des fresques illustrant des scènes de la Bible, comme la Fuite en Égypte et l'Annonciation. Des fresques du sont également visibles dans l'église Saint-Vigor de Neau, elles racontent la vie de Vigor de Bayeux; l'Église Saint-Pierre-le-Potier à Laval, l'Église Saint-Pierre de Varennes-Bourreau témoignent également de la richesse en peintures murales romanes du département. 
L'abbaye Notre-Dame d'Évron possède une grande basilique édifiée du au . Elle comprend un clocher carré, une crypte ainsi qu'une nef romane massive, Le chœur est gothique, des fortifications sont réalisées à la renaissance (pont-levis, fossés, remparts). L'abbaye de la Roë, première abbaye fondée par Robert d'Arbrissel en 1096, est affiliée à l'abbaye de Fontevraud, maison-mère de l’ordre de Fontevraud. Celle du Port-du-Salut, à Entrammes, date du . Ses moines trappistes sont à l'origine du fromage Port-Salut. L'abbaye de Clermont, à Olivet, date du et compte notamment un réfectoire voûté. Laval avait aussi un grand nombre d'établissements religieux avant la Révolution. Certains sont encore visibles à travers quelques vestiges, comme le couvent des Ursulines, dont la chapelle appartient au lycée Ambroise-Paré. L'abbaye de la Coudre, qui se trouve en limite sud de la ville, date de 1859 et elle est encore habitée par une communauté trappistine.

La cité de Pontmain, site d'Apparition mariale et de pèlerinage catholique, possède une grande basilique datant de 1880.

Les églises mayennaises sont souvent dotées de retables. Les plus vieux datent du , mais la plupart ont été réalisés aux . Leur présence dans les églises était alors pratiquement obligatoire en relation avec la Contre-Réforme; cette production constitue ce qui est considéré comme l'école des Retables lavallois. Ces retables ont souvent été conçus par des architectes et ils sont généralement décorés de marbre. Le bois fut toutefois privilégié vers 1680, grâce à sa plus grande souplesse. Le bois a aussi permis aux sculpteurs de suivre la mode, notamment le style rocaille. Au , la reconstruction des églises paroissiales a brièvement donné un second souffle à la production de retables. Cette production disparaît complètement à la fin du siècle, lorsque les styles roman et byzantin sont à la mode.

Le département est riche en croix et calvaires. ils font l'objet d'études par l'Abbé Angot puis par la société d'archéologie et d'histoire de la Mayenne, Alain Gueguen en inventorie plus de 6600 qu'il publie en 1993





</doc>
<doc id="1930" url="https://fr.wikipedia.org/wiki?curid=1930" title="Meurthe-et-Moselle">
Meurthe-et-Moselle

La Meurthe-et-Moselle est un département français qui se situe au cœur de la Lorraine. Il fait aujourd'hui partie de la région administrative Grand Est. La préfecture du département est Nancy. L'Insee et La Poste lui attribuent le code 54.

Le département de Meurthe-et-Moselle fut créé le , à partir des territoires des départements de la Meurthe et de la Moselle que le traité de Francfort avait laissés à la France.

Les arrondissements de la Meurthe (Lunéville, Nancy et Toul), restés français comme celui de Briey en Moselle, furent associés pour constituer le nouveau département de Meurthe-et-Moselle. Les autres arrondissements de la Meurthe, ceux de Château-Salins et de Sarrebourg, de même que le reste de la Moselle, furent quant à eux rattachés à l'Empire allemand jusqu'en 1918.

La limite actuelle entre les départements de Meurthe-et-Moselle et de la Moselle correspond précisément à la frontière franco-allemande entre 1871 et 1919. Cette limite servit à nouveau de frontière après l'annexion de fait des départements de la Moselle, du Bas-Rhin et du Haut-Rhin par le régime nazi entre 1940 et 1944.

La seule modification intervenue dans les limites du département fut le rattachement en 1997, pour des raisons de gestion administrative, de la petite commune de Han-devant-Pierrepont, qui appartenait auparavant à la Meuse.
Le département de Meurthe-et-Moselle est situé au centre de la Lorraine. Il est entouré par les départements de la Meuse, des Vosges, du Bas-Rhin et de la Moselle et sa frontière nord jouxte le Luxembourg et la Belgique.

Remodelé par les guerres franco-allemandes, le département a une forme inhabituelle : ses dimensions sont de du nord au sud, et entre 7 et d'est en ouest. Cette forme, dont la partie nord correspond à un "panhandle", est parfois comparée à celle d'une oie. Une autre particularité du découpage de ce département est le fait qu'une de ses communes, Othe, est enclavée dans un autre département, la Meuse.
La forêt recouvre 32 % du département. Elle a été fortement endommagée par la tempête de 1999.

La ville principale du département est sa préfecture, Nancy. Parmi les autres pôles urbains importants on peut citer Val-de-Briey, Longwy, Lunéville, Pont-à-Mousson, Toul et Villerupt.

Le département doit son nom aux deux principaux cours d'eau qui le traversent : la Moselle et la Meurthe. Parmi les autres rivières : la Chiers, la Crusnes, l'Orne, la Seille, le Madon, la Mortagne et la Vezouze. Le relief est modelé par les vastes plaines que ces cours d'eau ont creusé dans le plateau lorrain. Il cède sa place au massif des Vosges dans le sud-est.

Le point culminant est le Roc du Taurupt (), entre Bionville et Raon-sur-Plaine. Un autre relief historiquement important est la colline de Sion. Le point le plus bas est situé à Arnaville ().

Le climat meurthe-et-mosellan subit des influences océanique et continentale. Cela implique des températures très contrastées entre les saisons (gelées – canicules). Les précipitations sont cependant modérées et rarement violentes et les vents généralement faibles sans direction dominante. [données ?]

L'économie départementale a longtemps été liée à l'extraction minière (fer, sel et calcaire). Prospère jusque dans les années 1960, elle a commencé à souffrir de la crise de la sidérurgie à partir des années 1970, ce qui l'a contrainte à reconvertir son économie.

Au nord de Val-de-Briey, le Pays Haut est la région qui a le plus souffert de cette crise. Aujourd'hui encore le taux de chômage y est élevé et l'emploi transfrontalier vers la Belgique et le Luxembourg très développé. À titre d'exemple, 50 % de la population active de l'ancienne ville sidérurgique de Longwy travaille au Grand-Duché.
La région de Lunéville est également un territoire en difficulté.
L'agglomération nancéienne est, elle, au contraire, très dynamique avec une forte implication dans les services, la recherche et l'enseignement supérieur. Dans le Sud du département, le Saintois (le verger des Ducs de Lorraine) est resté, quant à lui, très rural.

En 2008, sur l'emprise d'une partie de l'ancienne base aérienne américaine de Chambley-Bussières est lancée l'installation d'un important pôle aéronautique pour la production de l'avion Skylander SK-105 (groupe GECI International) mais cette société ayant été mise en liquidation en 2013, le projet est arrêté puis définitivement abandonné.

La centrale photovoltaïque de Toul-Rosières, mise en service en 2012, est la plus importante de France.

Selon le recensement général de la population du janvier 2008, 1,6 % des logements disponibles dans le département étaient des résidences secondaires.

Ce tableau indique les principales communes de Meurthe-et-Moselle dont les résidences secondaires et occasionnelles dépassent 10 % des logements totaux en 2008. Il est à noter que le nombre de résidences secondaires à Pagny-sur-Moselle était dû à la présence de la base-vie pour la construction de la LGV Est européenne.
Sources : Insee, chiffres au 01/01/2008.

Issue de la guerre de 1870, la limite actuelle entre Meurthe-et-Moselle et Moselle ne marque pas une réelle frontière culturelle et les habitants des deux départements, hormis quelques enjeux d'aménagement du territoire, ou bien lors de rencontres sportives, se reconnaissent comme parties du même ensemble.

Près de Longwy, l'ancien bassin industriel du "Pays Haut" reste par exemple culturellement plus proche de la région de Thionville en Moselle que du sud du département.

Compte tenu de la manière dont le département a vu le jour (assemblage historique accidentel de deux morceaux d'anciens départements de l'époque de la Révolution française), les comportements politiques sont relativement différents selon que l'on se situe dans la partie nord ou la partie sud, selon qu'on est dans une ville industrielle ou dans la ville centre d'un pays rural.

Dans les faits, et jusqu'à aujourd'hui, l'arrondissement de Briey (Meurthe-et-Moselle nord) est largement acquis à la Gauche, dont l'influence provient en grande partie du monde ouvrier, dans la diversité de ses origines et de ses immigrations, dans celle de ses activités.

Mineurs ou sidérurgistes, ouvriers du bâtiment ou de la chimie, les habitants du nord choisissent le plus souvent leurs représentants parmi les partis de gauche.

L'agglomération de Nancy, où les activités industrielles ont eu un certain poids, est plus partagée, même si elle penche de plus en plus à gauche, à l'exception de la ville préfecture.

Dans la partie sud du département, longtemps influencée par l'Église, longtemps consacrée à l'implantation d'unités militaires d'importance, plus marquée par les activités rurales, la Droite a bénéficié d'une forme d'hégémonie durant de longues décennies.

Cette situation évolue toutefois, faisant du département de Meurthe-et-Moselle celui des départements lorrains le plus orienté à gauche et le seul dont le Conseil général est d'ailleurs présidé par la Gauche.

Dans les villages du Sud de Meurthe-et-Moselle, le Front national continue à faire de très bons scores, surtout aux élections présidentielles (y compris de 2007).





</doc>
<doc id="1931" url="https://fr.wikipedia.org/wiki?curid=1931" title="Meuse (département)">
Meuse (département)

La Meuse est un département français situé en Lorraine. Il fait aujourd'hui partie de la région administrative Grand Est. Le département doit son nom au fleuve qui la traverse du sud au nord, la Meuse. L'Insee et la Poste lui attribuent le .

La préfecture du département est Bar-le-Duc. Ses deux sous-préfectures sont Commercy au sud et Verdun au centre. Verdun est la ville la plus peuplée du département.

La Meuse fait partie de la région Grand Est. Elle est limitrophe des départements des Ardennes, de la Marne, de la Haute-Marne, des Vosges et de Meurthe-et-Moselle, ainsi que de la Belgique.

Les villes importantes sont : Verdun, Bar-le-Duc, Commercy, Saint-Mihiel, Ligny-en-Barrois, Étain, Montmédy, Stenay, Revigny-sur-Ornain et Vaucouleurs.

Les cours d'eau sont : la Meuse, l'Aire, la Chiers, l'Ornain, la Saulx, l'Orge, l'Oignon, la Vaise, l'Orne et l'Aisne dont la source est dans le département.

Les « côtes de Meuse », cuestas en bordure Est du Bassin parisien, sont la forme de relief la plus caractéristique du département. Les fronts, bien drainés, sont favorables à la culture des arbres fruitiers, particulièrement des mirabelles, et autrefois de la vigne. Le revers, plateau calcaire aux vallées bien marquées, est aujourd'hui entièrement occupé par des cultures céréalières.

Ces côtes dominent la plaine de la Woëvre, région au sol argileux et marécageux.

Le département est soumis à un climat à la fois océanique et continental qui se traduit par des saisons prononcées entrecoupées par des périodes intermédiaires au cours desquelles les températures et les précipitations restent moyennes.

Le volume des précipitations oscille autour de avec toutefois des nuances très fortes entre la région de la Woëvre (à l'est) qui reçoit moins d'eau (moins de ) que la région du Barrois (au centre) avec plus de de précipitations.

Par une loi du , cette entité administrative est primitivement appelée "département du Barrois" par l'Assemblée nationale ; le 26 février suivant, cette même assemblée change sa dénomination pour "département de la Meuse", en référence à l'une des principales rivières qui le traverse.

La Meuse est l'un des 83 départements créés à la Révolution française, à partir de la partie la plus occidentale de la province de Lorraine. Contrairement au département voisin de la Moselle, la Meuse n'a presque pas varié dans ses limites depuis sa création. On peut cependant citer le cas de la petite commune de Han-devant-Pierrepont, qui en a été détachée en 1997 pour être rattachée au département de Meurthe-et-Moselle.

Durant la guerre de 1870, l'ouest du département fut le théâtre des opérations qui aboutirent à la défaite de Sedan.

Ce département fut l'un des principaux théâtres de combat de la Première Guerre mondiale, particulièrement à Verdun en 1916. Après l'armistice, les dégâts étaient tels que de terres furent considérés Zone rouge (séquelles de guerre) dans ce seul département lors des premières évaluations. La loi du 24 avril 1923 définit finalement de surfaces qui furent l'objet d'expropriation et classement en forêt de guerre, où les séquelles des combats (cratères) sont encore très visibles de nos jours.

À la fin du , le laboratoire de Bure est construit dans ce département, pour la recherche sur le stockage des déchets radioactifs en couche géologique profonde.


La Meuse est un département agricole peu peuplé. La tendance démographique est à la baisse depuis les années 1970. La densité de population est très faible (seulement 31 habitants par kilomètre carré, contre une moyenne nationale de 100 habitants par kilomètre carré).

La commune la plus peuplée est Verdun, avec habitants en 2011 et la moins peuplée (si l'on exclut les six communes "Mortes pour la France" inhabitées) est Ornes, avec seulement 6 habitants en 2011.

Vers 1893, la langue française était à cette époque parlée et comprise par la grande majorité des habitants. Cependant, le patois lorrain était encore conservé dans beaucoup d'endroits.



Le tourisme de la région s'appuie sur deux éléments ; le tourisme vert d'une part (forêts d’Argonne, étangs de la Woëvre), et le patrimoine historique de la Première Guerre mondiale d'autre part (Verdun, Douaumont).

Les sites de la grande guerre en Meuse 1914-1918 :
La Meuse et plus particulièrement Verdun sont des symboles de la Première Guerre Mondiale. De 1914 à 1918, du Saillant de Saint-Mihiel à l'Argonne, des Eparges à Vauquois, toute la Meuse est en première ligne de la Grande Guerre. Au centre de ces champs de bataille, le Champ de Bataille de Verdun qui est le plus mondialement connu.
La bataille de Verdun est la bataille la plus meurtrière de l'histoire entre la France et l'Allemagne.

D'autres éléments touristiques sont à signaler :

Selon le recensement général de la population du janvier 2008, 5,2 % des logements disponibles dans le département étaient des résidences secondaires.

Ce tableau indique les principales communes de la Meuse dont les résidences secondaires et occasionnelles dépassent 10 % des logements totaux.

Sources :


Le projet "Meuse Énergies Nouvelles" s’articule autour trois axes :





</doc>
<doc id="1932" url="https://fr.wikipedia.org/wiki?curid=1932" title="Morbihan">
Morbihan

Le Morbihan est un département français situé en région Bretagne, qui doit son nom au golfe du Morbihan. Il correspond pour l'essentiel au royaume, devenu comté puis baillie de Broërec et plus anciennement à la cité des Vénètes. De tous les départements français de métropole, c'est le seul à avoir un nom fait entièrement de termes qui ne sont pas issus de la langue française. L'Insee et la Poste lui attribuent le code 56. Sa préfecture est Vannes.

Le département a été créé à la Révolution française, le en application de la loi du , à partir d'une partie de l'ancienne province de Bretagne : il est édifié pour les 4/5 de son étendue sur les terres de l'ancien diocèse de Vannes fondé au (moins deux petites parties au nord, une autre à l'est et une dernière à l'ouest), de l'extrême est de la Cornouaille, du sud de l'évêché de Saint-Brieuc, du sud-ouest de l'évêché de Saint-Malo et du nord-ouest de l'évêché de Nantes.

Il est créé à partir de la circonscription du présidial de Vannes à laquelle on a retranché la sénéchaussée de Quimperlé et la moitié nord de celle de Ploërmel et ajouté la sénéchaussée de Gourin (à peu près).

Les communes de la Cornouaille morbihannaise (l’ancienne sénéchaussée de Gourin) qui n'avaient pas choisi d'être annexées au département du Morbihan ont régulièrement réclamé de rejoindre le Finistère sans succès. Des pétitions circuleront à plusieurs reprises mais seule la commune de Locunolé réussira à obtenir gain de cause en 1847.

Les concepteurs des départements ont choisi de ne pas reprendre les noms portés par les circonscriptions antérieures pour en éradiquer les identités, cultures et particularismes, afin qu'il n'y ait plus ni Angevin, ni Corses, ni Alsaciens, ni Breton , mais seulement des Français. Les noms des départements sont choisis à partir de particularités géographiques, notamment des noms de fleuves, de mers ou de montagnes. On songea à nommer ce département « les Côtes du sud », par opposition aux Côtes-du-Nord, mais la présence de plusieurs golfes appelés "mor bihan" (« petite mer » en français) par les habitants, à Gâvres et au sud de Vannes, lui a fait préférer ce vocable géographique.

De 1791 à 1793, les neuf districts (Auray, Le Faouët, Hennebont, Josselin, Pontivy, Ploërmel, La Roche-Bernard, Rochefort et Vannes) du département du Morbihan fournirent quatre bataillons de volontaires nationaux.

Les et de volontaires du Morbihan furent envoyés pour combattre la révolution haïtienne et participèrent à la bataille du Cap-français

Le département du Morbihan fait partie en 1919 de la économique ou région de Nantes (départements 37-44-49-53-56-72-85), mais aussi de la région touristique de Bretagne (22-29-35-56). Plus tard, le Morbihan fait partie des régions « Bretagne » créées successivement en 1941, 1944 et 1956-72-88 (l'actuelle région Bretagne) et regroupant toutes les départements du Finistère, des Côtes-d'Armor et d'Ille-et-Vilaine.

Le nom du département vient de "Mor-Bihan", nom breton, signifiant "Petite Mer" (le golfe du Morbihan), par opposition à "mor braz", "grande mer", qui désigne localement la baie située entre Quiberon et Le Croisic. La transposition en français a pour conséquence le retrait du tiret, pour ne former finalement qu'un mot.

Le logo est constitué d’un rectangle bleu foncé qui évoque le cadre institutionnel. Une lettre « M » stylisée bleu clair (couleur symbolisant la mer et toute la vie qui lui est associée) brochant sur le tout est surchargée d'un disque orange qui représente le soleil. Cette lettre est accompagnée d'une « virgule » de couleur vert anis brochant sur la droite du rectangle et qui symbolise l’intérieur des terres (la campagne verdoyante) et de par son graphisme et sa couleur vive, le dynamisme (culturel, économique, social, etc.). Le nom du département apparaît en lettres blanches en bas du rectangle et l'institution départementale en lettres bleues en dessous.

L’ancien logo comportait les mêmes éléments disposés de façon légèrement différente et avec des couleurs plus vives : le soleil était jaune pur et la virgule rouge vif.

Le Morbihan fait partie de la région Bretagne. Il est limitrophe des départements du Finistère à l'ouest, des Côtes-d'Armor au nord, d'Ille-et-Vilaine à l'est et de la Loire-Atlantique au sud-est, et bordé par l'océan Atlantique formant la Côte des Mégalithes. Sa superficie est de pour de côtes.

L'espace morbihannais couvre un peu plus d'un dixième de la superficie du Massif armoricain. La superficie totale du département est de , dont boisés, soit plus de 16 % du territoire. Le Morbihan affecte la forme un quadrilatère dont l'axe principal, de direction nord/nord-est - sud/sud-ouest, mesure près de , tandis que, des crêtes des montagnes Noires jusqu'à la mer, la distance n'est, à vol d'oiseau, que de . Le Massif armoricain auquel appartient le Morbihan a subi depuis le passage Plio-Pléistocène (2,6 Ma) un basculement vers le sud qui a provoqué le soulèvement de sa marge nord et un effondrement relatif de sa marge sud dont le relief, étagé en gradins, descend progressivement vers l'Atlantique. Cette retombée méridionale cornouaillo-morbihannaise se traduit au niveau géomorphologique par une série de horsts et grabens, et surtout de blocs monoclinaux basculés vers le continent qui s'achèvent vers la mer par des escarpements de failles, selon un « maillage de dislocations assujetties à celles du tréfonds armoricain », cette tectonique en distension étant peut-être en relation avec l'ouverture du golfe de Gascogne.

Les plus grandes forêts se situent au nord du département (Paimpont, Lanouée, Quénécan…) et en son centre : les landes de Lanvaux (Bois de Saint-Bily, forêts domaniales de Camors, de Floranges, de Pontcallec…). Jusqu'au Moyen Âge, les forêts, comme partout en Bretagne, étaient bien plus étendues : la mythique forêt de Brocéliande s'est réduite comme une peau de chagrin pour ne laisser qu'une infime partie, Paimpont ; la forêt de Rhuys, où les ducs aimaient à chasser, a entièrement disparu. Le Morbihan est, de fait, le département breton le plus boisé (suivi d'assez loin par les Côtes-d'Armor avec près de 12 %).

Quant à la surface agricole utilisée, elle représente 57 % du territoire, c'est-à-dire que le Morbihan est le département de la Bretagne le moins exploité pour l'agriculture en termes de surface (l'Ille-et-Vilaine est le premier avec 76 % de la surface).

Comme ailleurs en Bretagne, les marées remplissent et vident les estuaires de fleuves – grands et petits – appelés localement en français rivières. C'est l'équivalent des abers du Léon et de ce que les géographes dénomment ria : ria d'Étel, ria de Pénerf.

Le littoral du Morbihan est particulièrement découpé : avec les îles, la longueur des côtes affiche (deuxième de Bretagne après le Finistère) alors que la distance à vol d'oiseau entre les estuaires de la Laïta et de la Vilaine (qui représentent les frontières naturelles est et ouest du département) n'est que de .

Le golfe qui a donné son nom au département contient 42 îles, dont deux forment communes : l'île d'Arz et l'île aux Moines.

Hors du golfe du Morbihan, dans le "Mor Braz" (« la grande mer », pour l'Océan) il y a quatre îles habitées :

Pour ce qui concerne le relief du département, le Morbihan est très plat sur le littoral en contradiction avec le reste de la Bretagne mais assez vallonné dans l'arrière-pays ouest (landes de Lanvaux, montagnes Noires proches de Gourin…). Son point culminant se situe au nord-est de Gourin, c'est le mont Saint-Joseph () dans les montagnes Noires.

Le climat est de type tempéré océanique voire océanique dégradé dans l'intérieur des terres, et sous l'influence du Gulf Stream et des perturbations atlantiques. Il se caractérise par sa douceur aussi bien en hiver qu'en été. En été, la chaleur reste modérée sauf à l'occasion de brefs et rares épisodes de canicule comme ce fut le cas en août 2003 (température de le à Lorient). En hiver, les gelées sont rares, surtout dans les îles et sur la côte. Les précipitations sont abondantes toute l'année, avec un maximum en hiver. Le littoral et la partie orientale du département sont les parties les moins arrosées. Les landes de Lanvaux et surtout le nord-ouest du département, au relief plus prononcé, reçoivent les précipitations les plus abondantes. Dans le secteur de Guiscriff, le cumul annuel dépasse les , alors qu'à Belle-Île il avoisine les . La côte morbihannaise bénéficie d'environ d'ensoleillement annuel. Il existe des microclimats surprenants tels que ceux de la presqu'île de Quiberon, de la presqu'île de Rhuys ou de Belle-Île.


Longtemps le Morbihan — comme la plupart des départements de l'Ouest — a eu une vocation essentiellement agricole. La présence d'un littoral étendu a toutefois apporté la diversité par la pêche depuis toujours, le commerce maritime depuis le Moyen Âge et les arsenaux à la fin du , en particulier à Lorient.

La Révolution industrielle a moins touché le département que le Nord-Est de la France ; on notera néanmoins le développement de la métallurgie (Forges d'Inzinzac-Lochrist, fonderies de Ploërmel…) et de la construction navale contemporaine (pays lorientais). La décentralisation industrielle après la Seconde Guerre mondiale a été bénéfique pour Vannes (implantation de Michelin). Au cours des décennies 50-80, l'agriculture se transforme profondément. Elle se modernise, se spécialise (élevage, en particulier avicole) et place sa production dans les premiers rangs des départements français. Elle est cependant confrontée actuellement à des problèmes de pollution des sols et des eaux. La pêche côtière, présente sur tout le littoral, a subi depuis vingt ans une crise qui a réduit d'un tiers ses effectifs ; elle reste un acteur fort de l'économie locale, tout comme la pêche industrielle à Lorient ( de France pour le tonnage). L'ostréiculture et la mytiliculture ont connu un essor dans les années 1960-90. Le Morbihan se classe juste derrière la Charente-Maritime dans ce domaine. Le secteur des services domine fortement aujourd'hui l'économie du département. Les villes moyennes de Lorient, Vannes et même Auray ou Pontivy constituent des pôles commerciaux notables. Le tourisme est le pilier de ce secteur, et entraîne avec lui le bâtiment, les travaux publics, les services à la personne.

Les habitants du Morbihan sont les "Morbihannais" et les "Morbihannaises".

La densité moyenne de la population s'élève à en 2008 et est légèrement inférieure à la densité moyenne de la France métropolitaine (). Cette moyenne recouvre d'importantes disparités puisque sur les 256 communes que compte le département, 28 comptent moins de et 10 plus de . La population se concentre majoritairement dans les zones proches du littoral où se trouvent notamment les aires urbaines de Lorient et de Vannes. Le taux d'urbanisation de la population est de 61 %. L'agglomération urbaine de Lorient, la plus peuplée du département, compte et se classe au troisième rang régional derrière Rennes et Brest tandis que celle de Vannes, la seconde la plus peuplée, compte et se classe au sixième rang régional juste derrière Saint-Brieuc et Quimper. La population de quatre autres agglomérations du département dépasse le seuil des . Il s'agit par ordre décroissant des agglomérations d'Auray, Hennebont, Pontivy et Locmiquélic.

Selon le recensement général de la population du janvier 2008, 19,2 % des logements disponibles dans le département étaient des résidences secondaires.

Ce tableau indique les principales communes du Morbihan dont les résidences secondaires et occasionnelles dépassent 10 % des logements totaux.

Sources :
Nombre et capacité des hôtels au janvier 2016

Nombre et capacité des campings au janvier 2016 d’après l’INSEE
Nombre d’autres hébergements collectifs au janvier 2016 d’après l’INSEE



Un Comité départemental du tourisme du Morbihan est en place depuis plus de 30 ans et veille au développement de l'économie touristique. Il anime la politique départementale du tourisme et accompagne les prestataires et les collectivités dans l'esprit du Schéma départemental de développement touristique.

Comme les Côtes-d'Armor, le Morbihan possède deux langues : le breton, dans sa partie occidentale, et le gallo, dans sa partie orientale, que sépare une frontière linguistique. Cette frontière a reculé au profit du français depuis le . Celle-ci courait depuis l'est de la ville de Rohan jusqu'à la ville de La Roche-Bernard au . Dans la première moitié du , les communes situées au sud de la Vilaine (Pénestin, Férel) ont été francisées, ainsi que celles d'Arzal, de Molac et d'Elven. En 1886, Paul Sébillot fixe la limite entre les deux idiomes. Une ligne nord-ouest — sud-est coupe le département : la zone bretonnante inclut dans sa limite orientale les communes d'Ambon, Berric, Sulniac, Treffléan, Plaudren, Saint-Jean-Brévelay, Bignan, Saint-Allouestre, Naizin, Kerfourn, Noyal et Croixanvec. Billiers, Muzillac, Noyal-Muzillac, La Vraie-Croix, Elven, Trédion, Plumelec, Billio, Guéhenno, Radenac, Réguiny, Crédin, Gueltas, Saint-Gonnery constituent la limite ouest du Gallo. Au cours du , avec la francisation de toute la zone bretonnante et la mobilité croissante de la population, cette frontière a progressivement perdu de son sens. Elle coïncide cependant avec la répartition des écoles où l'on enseigne le breton aujourd'hui.

Les effectifs pondérés que fournit l'enquête "Étude de l'histoire familiale" menée par l'INSEE en 1999 sont de plus de de plus de 18 ans pour ce seul département. S'y ajoutent notamment les effectifs des écoles bilingues qui se montent à à la rentrée 2005, ou encore les élèves suivant des cours de breton dans les établissements publics du secondaire (plus de 900 en 2002/2003). La signalisation routière bilingue (français/breton) est de plus en plus utilisée dans le département y compris sur sa partie gallèse.
Le breton vannetais diffère par de nombreux aspects de ses homologues Léonard, cornouaillais et trégorois.





La gastronomie dans le Morbihan occupe une place très importante. Elle dispose de nombreuses recettes des plus variées en passant du salé ou sucré. La gastronomie morbihannaise renferme beaucoup de spécialités de terroirs, artisanales L’alimentation bretonne présente donc des recettes très connues, des galettes et crêpes bretonnes au kouign-amann ainsi que les huîtres.

L’introduction de l’ingrédient principal de la galette bretonne est à aller chercher au moment des Croisades. C’est au , et en Asie, que les Croisés ont aperçu pour la première fois des fleurs roses s’étendant à perte de vue. Ces fleurs renferment le secret du blé noir. Ainsi, les Croisés s’empressent d’apporter leur trouvaille en Europe mais ils se rendent vite compte que sa culture n’est pas facile. Par conséquent, sa production est faible jusqu’à trouver un lieu d’implantation plus favorable, c’est-à-dire en Bretagne. La galette bretonne trouve un succès de par sa multitude de possibilités de garnitures.





</doc>
<doc id="1933" url="https://fr.wikipedia.org/wiki?curid=1933" title="Moselle (département)">
Moselle (département)

La Moselle ( ) est un département français faisant partie de la région Grand Est. Il doit son nom à la rivière de la Moselle, un affluent du Rhin, qui la traverse dans sa partie ouest et arrose Metz, son chef-lieu. D'autre part, c'est le département le plus peuplé de Lorraine, territoire dont il fait culturellement partie.

Ses habitants sont appelés les Mosellans. L'Insee et La Poste lui attribuent le code 57.

Le territoire de la Moselle est délimité à l'ouest et au sud par le département de Meurthe-et-Moselle, ainsi qu'à l'est par celui du Bas-Rhin. Au nord, le département est délimité par le Grand-duché de Luxembourg et par la République Fédérale d'Allemagne (Länder de Sarre et de Rhénanie-Palatinat).

Au niveau européen, la Moselle fait partie de la Grande Région et de l'Eurodistrict SaarMoselle (pour une partie du département). 

Cours d'eau principaux : la Moselle, la Sarre, la Seille, la Nied (dont l'allemande et la française), l'Orne, la Fensch, la Canner, le Conroy.

Le département est géographiquement organisé autour de la vallée de la Moselle. La région, couloir d'invasion depuis l'Antiquité, est longtemps restée une marche, entre Alsace et Nord, relativement pauvre jusqu'au , et donc peu urbanisée et peu peuplée.

L'environnement y a d'abord souffert de l'industrialisation lourde liée aux gisements de fer de Lorraine, qui a artificialisé les vallées et bords de cours d'eau. Les industriels ont créé dans les vallées de vastes emprises foncières en achetant des terres aux agriculteurs et en profitant d'un "droit d'eau" qui était en France avantageux pour les riverains.

Les questions de dégradation de l'environnement sont devenues politiques dès la fin du . Elles ont ensuite fait l'objet d'une sorte de consensus (la pollution étant une sorte de "rançon" acceptée de l'acier, gage de prospérité locale jusque dans les années 1960 avec la fragilisation de l'industrie métallurgique), selon R. Garcier.

Le climat en Moselle est océanique dégradé ou atténué à influence semi-continentale. Les saisons sont contrastées et bien marquées mais en fonction des vents dominants peuvent se succéder du jour au lendemain des périodes de précipitations (influence océanique) ou de forte amplitude thermique (influence continentale).

La Moselle est l'un des 83 départements conçus à la Révolution française, le en application de la loi du , à partir notamment de la partie nord de la Province de Lorraine et du temporel de l'Évêché de Metz. L'un de ses premiers préfets est le comte de Vaublanc, de 1805 à 1814. Le département connaît ensuite plusieurs rectifications de frontière jusqu'au traité de Paris de 1815, qui lui enlève la ville de Sarrelouis et ses environs, ainsi que les cantons de Sarrebruck et de Saint-Jean qui furent brièvement rattachés au département, de 1814 à 1815. Le département est alors divisé en quatre arrondissements : Metz (chef-lieu du département), Briey, Sarreguemines et Thionville.

La Moselle perd les communes de Bouquenom et de Sarrewerden en novembre 1793, ainsi que celle d'Obersteinbach en 1833. Ces trois localités furent rattachées au Bas-Rhin.

Par un traité de 1814, la Moselle perd au profit de la Prusse plusieurs communes et hameaux, dont le Canton de Tholey ainsi que sept communes du Canton de Sierck-les-Bains. En 1815 le département perd aussi les cantons de Relling et Sarrelouis qui deviennent partiellement allemands. Certaines des communes et hameaux concernés redeviennent français en 1829 (convention de délimitation du 23 octobre).

Le , ce département est rayé de la carte à la suite du traité de Francfort, celui-ci ayant pour origine une défaite militaire contre les Allemands. À la suite de cette défaite, la création de l'Empire allemand fut proclamé le 18 janvier précédent, dans la galerie des glaces du château de Versailles. La nouvelle Allemagne annexe la plus grande partie du département, ainsi qu'une part du département de la Meurthe et des Vosges. Seul l'extrême-ouest de la Moselle, correspondant à l'actuel arrondissement de Briey, reste français et forme avec les arrondissements du département de la Meurthe restés français, le nouveau département de Meurthe-et-Moselle. Les territoires devenus alors allemands comprennent non seulement la partie germanophone de la Lorraine, territoire dans lequel les habitants parlent le francique lorrain, ou "Platt", mais aussi des régions où l'on parle français, comme le Pays messin et la majeure partie du Saulnois. Les arrondissements existants depuis la Révolution sont redécoupés, et l'on crée le "Bezirk Lothringen", « district de lorraine » correspondant à l'actuel département de la Moselle. Il forme alors, avec l'Alsace, le "Reichsland d'Alsace-Lorraine", avec Strasbourg pour chef-lieu.

De là est né le mythe des « provinces perdues », correspondant en fait à cette nouvelle terre d’Empire, ou "", dont les traces subsistent dans le statut particulier de l'Alsace-Moselle. L'esprit de revanche, que nourrissait la perte de l'Alsace et de la Lorraine au sein de la population française et de sa classe politique, exalte en France un sentiment profondément germanophobe, propice aux velléités guerrières de la France. Lorsque la Première Guerre mondiale éclate, les Mosellans sont incorporés dans les troupes allemandes. Entre 1914 et 1918, si Alsaciens et Mosellans s'engagent dans l'Armée française, Alsaciens-Lorrains, soit plus de 95 % des conscrits, nés Allemands se battent pour l'Empire allemand jusqu’à la fin de la guerre. Pour éviter les désertions, la plupart sont envoyés sur le front Russe.. Leurs tombes sont aujourd'hui entretenues par le "Volksbund Deutsche Kriegsgräberfürsorge". Ceci explique la spécificité des monuments aux morts du département, qui ne portent souvent que l'inscription lapidaire « À nos morts », en lieu et place du traditionnel « Morts pour la France ».

Entre l'armistice du 11 novembre 1918 et la promulgation du traité de Versailles le 10 janvier 1920, la Moselle est, juridiquement, un territoire sous occupation de l'armée française. Quand en 1919, le traité de Versailles rend à la France les territoires lorrains perdus, on ne reconstitue pas les anciens départements, mais le "Bezirk Lothringen" devient le « nouveau » département de la Moselle, conservant les anciens arrondissements de Boulay-Moselle, Forbach, Metz, Sarreguemines et Thionville et ceux de Château-Salins et Sarrebourg, qui avant 1871, appartenaient à la Meurthe. Le département de Meurthe-et-Moselle reste de ce fait inchangé, conservant l'arrondissement « mosellan » de Briey.

Dans l'entre-deux-guerres, la Moselle reste traumatisée par les déchirures de la guerre et les dommages collatéraux des nationalismes.

Les intellectuels mosellans réagissent diversement au rattachement de la Moselle à la France. Si l'avocat Robert Schuman se montre pacifique et démocrate, certains s’engagent sur la voie d’un nationalisme pro-français, revanchard et cocardier. D’autres s’engagent sur la voie antagoniste d’un nationalisme pro-allemand, tout aussi vindicatif et belliqueux. D’autres encore, comme Adrienne Thomas, Polly Maria Höfler (1907-1952), Ernst Mungenast ou Alfred Pellon, hésitent entre un pacifisme sincère, mais naïf, et un régionalisme culturel identitaire. Ces mouvements, plus ou moins autonomistes, seront ensuite largement exploités par les nazis. Ce combat identitaire, souvent mené par des intellectuels idéalistes, qui s’inscrit parmi des courants de sensibilité à l’œuvre dans l’Europe entière, traduit aussi une crise d’identité propre à l’ensemble des Alsaciens-Lorrains.

La Moselle est touchée par la Seconde Guerre mondiale, dès la déclaration de guerre le 3 septembre 1939 : près de 30 % du territoire de la Moselle se trouve entre la Ligne Maginot et la frontière franco-allemande. personnes, soit 45 % de la population du département, sont évacuées pendant le mois de septembre 1939 vers des départements du Centre et de l'Ouest de la France, essentiellement la Charente, la Charente inférieure, la Vienne, la Haute-Vienne et enfin la Haute-Loire qui accueillent les mineurs. L'ordre d'évacuation pour les villages frontaliers comme Oberdorff a été donné dès le septembre. Parmi les quelque évacués, reviendront après la défaite.

Au cours de la Seconde Guerre mondiale, malgré l'armistice du , la Moselle est à nouveau annexée, en juillet de la même année, par l'Allemagne nazie. Elle n'est pas réunie à l'Alsace, qui subit le même sort, mais intégrée au "Gau Westmark", la "Marche de l'Ouest", comprenant aussi la Sarre et le Palatinat, Sarrebruck en était le chef-lieu. L'importance de la population francophone en Moselle, ou tout simplement francophile, amène le Gauleiter Bürckel à procéder à des expulsions massives vers la France. L'évêque de Metz, Joseph-Jean Heintz, expulsé dès le mois d'août, en est un bon exemple. Moins bien traités que les Alsaciens, les Lorrains expulsés se félicitèrent bientôt de leur destin quand, en 1942, les jeunes Mosellans restés ou retournés au pays furent soumis à l'incorporation de force dans les armées allemandes.

Comme dans le reste de la France, plusieurs types de résistance à l'annexion virent le jour, prenant parfois la forme de groupes organisés et structurés, comme le Groupe Mario, animé par Jean Burger, ou le Groupe Derhan. Au cours de ces années noires, plus de dix mille Mosellans furent déportés dans des camps, notamment dans les Sudètes, pour s'être opposés publiquement à l'annexion en janvier 1943. Si des villages lorrains furent libérés dès le début de septembre 1944, au début de la Bataille de Metz, la ville elle-même ne fut libérée que le 21 novembre et il fallut attendre le mois de mars 1945 pour voir les combats cesser dans le nord-est du département.

Le bilan matériel de la guerre est très lourd en Moselle. À partir du printemps 1944, les bombardiers américains se sont succédé par vagues au-dessus de la Moselle, faisant d’énormes dégâts collatéraux. Si les populations civiles furent durement touchées, les dégâts matériels furent plus grands encore. Les dévastations sont généralisées dans la vallée de la Seille, entre Dieuze et Metz, et au nord d'une ligne Forbach-Bitche. 23 % des communes de la Moselle furent détruites à plus de 50 %, et 8 % des communes le furent à plus de 75 %. Dans la seule journée du 9 novembre 1944, un total de bombardiers lourds B-17 et B-24 déversèrent tonnes de bombes, de à livres, sur les ouvrages fortifiés de la Moselstellung et les points stratégiques situés dans la zone de combat de la armée. Ce funeste ballet aérien ne prendra fin, au-dessus de la Moselle, qu’en mars 1945, lorsque le département sera entièrement libéré.

Le département de la Moselle est aujourd'hui sous régime concordataire et dispose d'un droit local spécifique.

Le conseil départemental de la Moselle a adopté, le 14 décembre 1948, un blason complexe, retraçant la formation du département :

Durant le Second Empire, le département de la Moselle portait : « écartelé, au 1 : parti d'argent et de sable (Metz) ; au 2 : d'or à trois pals alésés et fichés de gueules (Briey) ; au 3 : d'or à la bande de gueules chargée de trois alérions d'argent (Sarreguemines au ) et au 4 : d'azur au château donjonné de trois tourelles d'or, celle du milieu plus haute, le tout maçonné de sable (Thionville) ».

Pour développer l’économie locale, la Chambre de Commerce et d’Industrie de la Moselle  a mis en place le site « Achat-Moselle » dans les années 2000. Achat-Moselle est une réponse concrète de la CCI de la Moselle, adaptée aux enjeux du commerce électronique pour le commerce de proximité. Ce dispositif leur permet aux professionnels du commerce du département de créer un site internet pour être visible sur ce canal et développer leur activité. Un projet labellisé « Meilleure pratique européenne » par la Commission Européenne.

Les habitants de la Moselle sont les "Mosellans".

Son chef-lieu est sa commune la plus peuplée, Metz ( en 2012), sa commune la moins peuplée est Molring (8 habitants en 2012).

Autres communes de plus de : Thionville (sous-préfecture), Montigny-lès-Metz (unité urbaine de Metz), Sarreguemines (sous-préfecture), Forbach (sous-préfecture), Saint-Avold, Yutz (unité urbaine de Thionville), Hayange (UU de Thionville), Creutzwald, Freyming-Merlebach (unité urbaine de Forbach), Fameck (UU de Thionville), Woippy (UU de Metz), Stiring-Wendel (UU de Forbach), Sarrebourg (sous-préfecture), Florange (UU de Thionville), Maizières-lès-Metz (UU de Metz), Amnéville (UU de Metz), Rombas (UU de Metz), Marly (UU de Metz), Hagondange (UU de Metz).

La Moselle est un département densément peuplé, dont le développement industriel a fait apparaître de nombreuses villes moyennes. Hormis Metz, principale ville possédant une très longue histoire, et dont l'agglomération s'étend de plus en plus loin le long de la Moselle, les autres grandes agglomérations sont Thionville et Forbach, qui doivent leur importance à la sidérurgie et à la houille. C'est ce qui explique aussi le recul de ces villes à partir des années 1970, avec la désindustrialisation. Thionville semble aujourd'hui avoir réussi sa reconversion et retrouvé la croissance (la ville, ancienne possession luxembourgeoise, bénéficie de la proximité du Grand-Duché de Luxembourg, grand pourvoyeur d'emplois). L'ouest de son agglomération ainsi que l'agglomération de Forbach sont encore en déclin relatif.

Entre ces grandes agglomérations s'est développé un réseau de villes secondaires, surtout dans le nord (Sarreguemines, Saint-Avold). Le sud du département, notamment le Saulnois, (qui fit autrefois partie de la Meurthe), est resté plus rural. La seule ville importante y est Sarrebourg.

Environ quittèrent leur département entre 1825 et 1850. Cela principalement pour migrer vers les États-Unis et Paris.

Après avoir connu une très forte croissance de sa population dans les années 1950 et 1960, passant de habitants en 1946 à en 1968, la Moselle a connu un solde migratoire négatif, même si l'excédent naturel l'a compensé, de sorte que la population totale a continué à augmenter légèrement mais régulièrement, dépassant désormais le million d'habitants.

Plusieurs langues sont utilisées en Moselle.

Les principales sont :

Historique :

Le département de la Moselle ne constitue ainsi pas un ensemble culturel homogène, car à cheval sur ces deux régions linguistiques et culturelles qui composent la Lorraine administrative : la Lorraine thioise de langue francique ou Lorraine allemande, dite plus communément germanophone ou ""de dialecte germanique"", et la Lorraine francophone, dite ""latine ou romane"" et de ""patois roman"". Une frontière linguistique coupe le département en deux parties quasi égales : 

Metz et les "pays" de Moselle francophone se reconnaissent dans leurs confrères meurthe-et-mosellans et meusiens par la culture, l'architecture (excepté l'épisode marquant de l'annexion de Metz à l'Allemagne) et le patois (le patois de Nancy étant de la même famille que le patois messin, tous les deux étant des patois romans). La pseudo-frontière "culturelle" qui séparerait la Moselle romane de la Meurthe-et-Moselle n'est donc qu'un leurre issu de l'annexion de 1871. Les ethnologues et historiens tracent cette frontière linguistique à à l'est de Metz. La Moselle est ainsi un territoire administratif partagé entre deux cultures et traditions : l'une romane (avec le particularisme de Metz et celui de Nancy) et l'autre francique.

On notera cependant les ravages de l'histoire : Nancy doit sa prospérité et notamment son université à l'annexion de Metz et de Strasbourg à l'Allemagne en 1871. Quatre fois, en l'espace de 75 ans, Metz perdit son élite et ses habitants les plus dynamiques. Cela eut un effet très négatif sur son développement. Un antagonisme virulent oppose encore les deux villes lorraines (cf les discussions sur la gare de Lorraine TGV). Il se trouve encore des Nancéiens pour traiter les Messins de "Boches" à cause de l'annexion, et des Messins pour traiter les Nancéiens de "Polonais" à cause du roi Stanislas Leszczynski.

Les pays de Thionville, de Sarrebourg, de Boulay, de Saint-Avold, de Forbach, de Sarreguemines et de Bitche, quant à eux, ont une culture lorraine fortement influencée par les cultures, architectures et dialectes germaniques et partagent une proximité culturelle avec leurs voisins du Luxembourg, de la Sarre, du Palatinat et de l'Alsace. Les autochtones de cette partie de la Moselle ont eu, pendant longtemps, beaucoup plus de mal à s'identifier à leurs frères lorrains francophones de Metz et Nancy, provoquant un certain isolement de cette partie de la Moselle par rapport au reste de la région. L'appellation contemporaine de "Moselle-Est" utilisée pour désigner ce territoire culturel traduit bien cette distance.

La frontière linguistique séparant les "deux Moselles" ou plutôt, les "deux Lorraines", à l'Est, est très nette. Ainsi, à l'Est de Courcelles-Chaussy, la commune de Raville est considérée comme dernier village de Moselle romane avant la Moselle germanophone. Puis on passe à Fouligny (anciennement "Fullinga" et "Filling") , commune signalée comme étant toujours germanophone dans les années 1990, ainsi qu'en 2012 où il y persiste encore des habitants germanophones. Ensuite le prochain village est Marange-Zondrange, puis Zimming et enfin Bambiderstroff. Le changement de toponyme est radical. D'autre part, l'architecture du pays messin, marquée par des façades de pierre ocre (pierre de Jaumont) et de toit à pente relativement faible et propre au reste de la Lorraine romane, contraste très vite avec une architecture plus germanique. Les accents changent d'un village à l'autre. Ainsi les habitants de Servigny-lès-Raville ou de Herny, villages où l'on parlait le patois messin, n'ont pas l'accent germanique des habitants de Bambiderstroff et Mainvillers ("Maiwilla"), villages de dialecte francique (germanique) situé seulement quelques kilomètres plus loin.
L'expression des anciens de Courcelles-Chaussy (pays messin) "Après Fouligny, révise ton allemand !" traduit bien la ténacité de cette frontière linguistique. Mais en réalité les communes situées le long de cette frontière étaient plus ou moins bilingues, comme Fouligny. Car les habitants de chaque côté de la frontière étaient plus ou moins amenés à avoir certaines relations communes, chose qui se remarque dans le vocabulaire des dialectes locaux.

Au nord de Metz, la frontière linguistique est floue et a aujourd'hui, quasiment disparu pour se cantonner au niveau d'Algrange, qui est la commune la plus au sud-ouest de la zone germanophone de Thionville. il y a également les localités de Rédange, Russange et Nondkeil qui étaient germanophones a minima jusque dans les années 1980.

Aujourd'hui, certaines expressions péjoratives (désignant les patoisants ou ceux qui, héritiers de cette culture, en gardent un accent différencié) telles que les ""Ja ja"" ("Oui oui" en francique) ou bien l'expression de ""Moselle allemande"" perdurent dans l'aire traditionnelle des parlers d'oïl.
De même, les personnes d'expression francique traitent non sans humour ces derniers de "Français de l'intérieur".

Mais plus qu'une confrontation, cette diversité reste un atout majeur pour cette région située au cœur de l'Europe.

Dans cette partie de la Moselle appelée la Moselle thioise ou allemande les suffixes -"ingen" des villages lorrains furent, au fil des siècles et par l'influence de la proximité avec le Royaume de France, francisé en -"ange". Ainsi "Morchingen" devint "Morhange" et "Hagendingen" devint "Hagondange". Ce phénomène s'observe également en Meurthe-et-Moselle (Bezange-la-Grande, Godbrange et Herserange), dans les Vosges (Relanges), en Belgique, au sud du Luxembourg, ainsi que dans quelques localités de Moselle germanophone proche de la frontière linguistique (alentours de la commune de Boulay-Moselle par exemple).


Même chose pour les toponymes en -"viller" qui sont restés orthographiés -"willer" (parfois -"weiller"), jusqu'à la fin du dans les Bulletin des lois de la république et les dictionnaires.

Voir : 

La vie culturelle mosellane est bien représentée dans le département par des festivals, parfois aussi bien issu du folklore local que d'une culture d'immigration dans les pays miniers.La Moselle-Est conserve de nombreuses traditions locales comme les fêtes de la Kirb, célébrées en début octobre dans les milieux ruraux par des fêtes foraines et des repas festifs, ou la cavalcade de Sarreguemines le jour du Mardi gras. Du côté de Metz, le festival emblématique reste celui des fêtes de la Mirabelle fin août, mais se déroulent également des événements autour des arts et du spectacle, notamment durant l'été avec "HopHopHop" et la "journée Extra-Large", de plus en automne les scènes messines des Trinitaires et depuis 2014 de la BAM produisent deux séries de concerts à savoir "Musiques volantes" et "Metz en fête". Dans l'ancien bassin minier, a lieu chaque année le festival du film arabe de Fameck en raison de l'importante communauté immigrée au .

C'est en Moselle par ailleurs que se trouve le plus ancien théâtre de France, encore en activité. L'opéra-théâtre de Metz date en effet du et a depuis toujours gardé sa vocation d'origine. Outre l'opéra-théâtre, Metz est dotée d'une importante salle de spectacle, l'Arsenal où se représentent de nombreux artistes nationaux et internationaux de divers genres : aussi bien des humoristes que des orchestres symphoniques. La ville de Thionville quant à elle est dotée de l'organisme du NEST (Nord-Est Théâtre) qui regroupe le grand théâtre de la ville et un petit théâtre en bois et propose des productions théâtrales très diverses et souvent peu communes.

Depuis plus de vingt ans le Conseil départemental de la Moselle a engagé une véritable politique de développement touristique dans le département. La réalisation de zones de loisirs, de structures d’hébergement (hôtels, gîtes…), ainsi que divers équipements touristiques et l’ouverture de sentiers de randonnée et de pistes cyclables ont permis d’accroître sensiblement la fréquentation touristique en Moselle.

Aux côtés du Conseil départemental, l'Agence de développement et de réservation touristiques de la Moselle (Moselle Tourisme) est chargée de mettre en œuvre certaines actions de promotion, de commercialisation. Moselle Tourisme est membre du Réseau national des destinations départementales De nombreux autres partenariats sont activés, en particulier avec les collectivités locales et les professionnels du tourisme. Moselle Tourisme est copropriétaire du Système d'information touristique - Lorraine (SITLOR), dont les objectifs sont la collecte de l'offre touristique régionale et sa diffusion auprès du grand public.

Selon le recensement général de la population du janvier 2008, 1,8 % des logements disponibles dans le département étaient des résidences secondaires.

Ce tableau indique les principales communes de la Moselle dont les résidences secondaires et occasionnelles dépassent 10 % des logements totaux.

Sources :




Le CSA a lancé le 19 septembre 2007 une consultation auprès des acteurs publics et privés concernés afin de recueillir leurs remarques en vue de la diffusion hertzienne en mode numérique de télévisions locales (TNT).

Cette consultation avait également pour objet, dans le cadre de la préparation des futurs appels aux candidatures et de la planification en cours pour l'extension de la couverture de la TNT, de connaître les projets de télévisions locales existants ou en cours d'élaboration, en précisant la ou les zones concernées. Les contributions étaient attendues pour le 30 novembre 2007.
Ont répondu : TV8 Moselle, Communauté d'agglomération Forbach Porte de France, département de la Moselle, Canal local Mosaïk, TV2M, canal local TV Cristal à Bitche, canal local à Bischwiller, Communauté de communes Freyming-Merlebach.

Au deuxième trimestre 2008, le CSA lancera un appel à candidature auquel devront répondre les intéressés.

Une seule et même chaîne, Mirabelle TV, existe sur le canal local TNT réservé sur le R1.

Cette même chaine doit diffuser sur les émetteurs : Forbach, Longwy, Metz et Verdun (en Meuse) et couvre le département de la Moselle, le nord de la Meuse, le sud du Luxembourg, l'ouest de L'Allemagne.

Diffuser sur quatre émetteurs TNT plus les réémetteurs a un coût élevé, qui représente la totalité du budget d'une chaine locale existante.

Les chaînes locales existantes produisent, diffusent et rediffusent en moyenne 30 minutes de programmes quotidiens.

Cependant une syndicalisation des programmes permet l'échange entre chaînes de leurs émissions et de leurs reportages.
Ainsi, les chaînes accroissent leur programmation mais les sujets échangés ne correspondent plus au bassin de population visé.


Dans certains domaines comme la chasse, les associations, les religions, etc., le droit appliqué en Moselle, ainsi que dans les départements du Bas-Rhin et du Haut-Rhin, est un mélange de droit national et de droit local.

De nos jours en Moselle, les cultes catholique, israélite, protestant luthérien (ÉPCAAL) et protestant réformé (ÉPRAL) sont toujours officiellement reconnus et financés par l'état (application du droit local)


Le culte de l’Église de Jésus-Christ des saints des derniers jours, financièrement autonome, est représenté en Moselle avec deux paroisses : la paroisse de Metz et la paroisse de Forbach.

L'enquête de l’INSEE de 1962 constate que la Moselle comptait 4,1 % de protestants pour 85,5 % de catholiques (avec une forte proportion de "non déclarés").

"Article de fond : Administration de la Moselle"


Avec 650 instruments répartis sur tout son territoire, la Moselle est le second département en France qui possède le plus grand nombre d’orgues. Trois facteurs d'orgues œuvraient déjà en terre mosellane au mais c'est au courant du que la Moselle compta jusqu'à 17 facteurs d'orgues différents qui bâtirent de précieux instruments sur son territoire. De nos jours, cinq facteurs d'orgues encore en activité continuent d'enrichir le département en instruments de qualité. L'orgue le plus ancien du département est celui de la cathédrale Saint-Étienne de Metz qui date de 1537. Les grandes orgues les plus importantes du département (et qui figurent aussi parmi les grandes orgues rurales les plus importantes de France) sont celles d'Hayange. Elles comportent 53 jeux. On note aussi des instruments plus modestes et historiques comme l'orgue personnel d'Albert Schweitzer qui est conservé à L'Hôpital au sein de la paroisse protestante.

Afin de conserver ce patrimoine unique, le Conseil départemental de la Moselle a lancé un programme intitulé la « Route des Orgues » qui vise à restaurer, promouvoir et valoriser ces nombreux instruments souvent méconnus.

Au Moyen Âge il existe de nombreux châteaux, fermes et églises fortifiées en pays messin.

Les grandes demeures féodales disparaissent avec la politique d’expansion territoriale vers l’est de Louis XIII et Louis XIV qui appliquent une politique de démantèlement et de destruction des édifices. La guerre de Trente Ans ruine une partie de la noblesse dont les possessions, vendues ou confisquées, sont attribuées à de nouveaux venus ou de récents anoblis. Le château du Schossberg, le château de Turquestein ou celui de Faulquemont sont rasés en 1634 sur ordre de Richelieu, les deux châteaux d’Audun-le-Tiche en 1675, de même pour Lixheim, Sarralbe, Sarrebourg et Sarreguemines. Après la guerre de Trente Ans, disparaissent dans l’indifférence générale : le château du Falkenstein, ruiné par les troupes de Mansfeld en 1623, celui de Thicourt, incendié en 1635, le château des évêques d’Albestroff, le château de Créhange et celui de Fontoy, détruit en 1643. Le château de Raville est reconstruit fin puis détruit à la Révolution. Le château de La Grange est reconstruit en 1731. À Hombourg-Haut, le château des évêques de Metz puis des ducs de Lorraine est entièrement détruit vers 1735. Le château de Château-Voué est partiellement détruit à partir de 1795. Le château médiéval d’Ottange, en partie détruit en 1671, fut entièrement démoli en 1734. Disparaissent également les châteaux d’Hingsange et de Guermange. Certains sont vendus comme bien nationaux à la Révolution : le château d’Imling en 1795, il est détruit peu après et sert de carrière de pierres ; le château de Frescaty à Moulins-lès-Metz, construit pour l’évêque de Metz, détruit en 1944, il sert aujourd’hui de terrain d’aviation à la ville de Metz. Certains châteaux du pays messin sont transformés en fermes comme à Ancerville ou le château-ferme de Prayelles à Augny.

Les troubles des périodes de guerre retardent, à de rares exceptions près, l’apparition du classicisme en Moselle au , période de paix durant laquelle de nombreuses demeures sont remaniées ou reconstruites, en particulier par des officiers ou par des conseillers au parlement de Metz. À la fin de l’Ancien régime, dans les 250 maisons nobles — châteaux, maisons-fortes et manoirs —, existent en Moselle dont la moitié subsiste aujourd’hui.

Les guerres de l’époque contemporaine détruiront les châteaux de Colombey, incendié après la guerre de 1870, de Lorry-Mardigny (une partie subsiste), Sailly-Achâtel, Albestroff, Louvigny, Amanvillers, Lorry-lès-Metz, Arry, Coin-sur-Seille, Corny, Sillegny, Verny, Maizières victimes de la Seconde Guerre mondiale. Après les conflits, certains propriétaires préfèrent démolir plutôt que financer une réhabilitation ; les bâtiments abandonnés sont victimes du vandalisme. Le château d’Hayange, symbole de la famille de Wendel, est en partie démoli en 1935. Le château de Montois-la-Montagne est rasé vers 1950 au profit d’une cantine ouvrière. Le château de Reinange est rasé vers 1958-1960. Les châteaux de Florange, Francaltroff et Distroff sont aussi en ruine.

Certains chefs-d’œuvre du patrimoine architectural en péril sont restaurés à grand frais par les collectivités : le château de Malbrouck (originellement Schloss Meinsberg) ou le château de Courcelles. D’autres sont fidèlement entretenus par des familles respectueuses de la demeure ancestrale comme au château de Pange ou par une noblesse de cœur ayant envie de redonner une âme à ces monuments : Pouilly, Les Étangs, Mardigny ou le château de Landonvillers. Plusieurs sites sont en cours de sauvetage, par des associations ou autres initiatives, comme le château Saint-Sixte en restauration depuis 2007. Le parc du château de Mercy sert de terrain pour la construction du nouvel hôpital au sud-est de Metz prévu pour 2012. Plusieurs châteaux et ruines subsistent dans le Pays de Sarrebourg : une partie des fortifications médiévales de Sarrebourg, le château de Lutzelbourg et le château de Turquestein dans le massif des Vosges, le château de Fénétrange, le château de Geroldseck à Niederstinzel, le château du Sarreck à Oberstinzel ou encore le château de Réchicourt.

Inauguré le , ce bâtiment situé dans le centre-ville de Metz attire de nombreux visiteurs. Il accueille des expositions artistiques. Il est composé de superposées en forme de pavés sortant de son toit blanc aux formes rondes, d'où dépasse un mât.

Ouvert au public en 2007, le Haut Fourneau U4 de Uckange devient un espace dédié à la mémoire du passé sidérurgique de la France. Dès 2010, le Jardin des Traces s'étend sur 4 hectares au pied des infrastructures et par différents espaces à thèmes, il rend hommage aux installations et aux hommes et femmes qui les ont fait vivre pendant les 100 ans qu'a duré l'exploitation. Fondée en 1890, l'usine cesse sa production de fonte le 17 décembre 1991. Un lieu atypique proposant visites libres ou guidées ainsi que de nombreuses animations.

Dans l'antiquité, le poète latin Ausone célèbre souvent la table et surtout, le vin, le vin de Bordeaux dont le château Ausone prendra le nom, mais aussi les vins de Moselle. Jacques Brel chantera également bien plus tard le vin de Moselle dans la chanson "Jef". La Moselle fut très longtemps une terre de vignobles (Cf. Vignoble de Lorraine). 

L’irruption du phylloxéra à la fin du , puis la signature de l’Armistice de 1918 qui sonna le glas des débouchés sur le marché allemand, ont provoqué un déclin certain de la vigne en terre mosellane. Néanmoins les coteaux mosellans continuent de produire un vin de qualité. Depuis 2010, le moselle est un AOC.




</doc>
<doc id="1938" url="https://fr.wikipedia.org/wiki?curid=1938" title="Musique">
Musique

La musique est un art et une activité culturelle consistant à combiner sons et silences au cours du temps. Les ingrédients principaux sont le rythme (façon de combiner les sons dans le temps), la hauteur (combinaison dans les fréquences), les nuances et le timbre. Elle est aujourd'hui considérée comme une forme de poésie moderne.

La musique donne lieu à des créations (des œuvres d'art crées par des compositeurs), des représentations. Elle utilise certaines règles ou systèmes de composition, des plus simples aux plus complexes (souvent les notes de musique, les gammes et autres). Elle peut utiliser des objets divers, le corps, la voix, mais aussi des instruments de musique spécialement conçus, et de plus en plus tous les sons (concrets, de synthèses, abstraits).

La musique a existé dans toutes les sociétés humaines, depuis la Préhistoire. Elle est à la fois forme d'expression individuelle (notamment l'expression des sentiments), source de rassemblement collectif et de plaisir (fête, chant, danse) et symbole d'une communauté culturelle, nationale ou spirituelle (hymne national, musique traditionnelle, musique folklorique, musique religieuse, musique militaire, etc.).

L'histoire de la musique est une matière particulièrement riche et complexe, principalement du fait de ses caractéristiques : la difficulté tient d'abord à l'ancienneté de la musique, phénomène universel remontant à la Préhistoire, qui a donné lieu à la formation de traditions qui se sont développées séparément à travers le monde sur des millénaires. Il y a donc une multitude de très longues Histoires de la musique selon les cultures et civilisations. La musique occidentale (musique classique ou pop-rock au sens très large) ne prenant qu'au l'allure de référence internationale, et encore très partiellement.

La difficulté vient également de la diversité des formes de musique au sein d'une même civilisation : musique savante, musique de l'élite, musique officielle, musique religieuse, musique populaire. Cela va de formes très élaborées à des formes populaires comme les berceuses. Un patrimoine culturel d'une diversité particulièrement large, contrairement à d'autres arts pratiqués de manière plus restreinte ou élitiste (littérature, théâtre…). Enfin, avec la musique, art de l'instant, se pose la question particulière des sources : l'absence de système de notation d'une partie de la musique mondiale, empêche de réellement connaître l'étendue de la musique du temps passé, la tradition n'en ayant probablement sauvé qu'un nombre limité.

La réalisation d'une synthèse universelle apparaissant très difficile car beaucoup d'Histoire de la musique traitent essentiellement de l'Histoire de la musique occidentale. Il n'est en général possible que de se référer aux ouvrages et articles spécialisés par civilisation ou par genre de musique.

Il existe alors deux « méthodes » pour définir la musique : l’approche intrinsèque (immanente) et l’approche extrinsèque (fonctionnelle). Dans l'approche intrinsèque, la musique existe chez le compositeur avant même d’être entendue ; elle peut même avoir une existence par elle-même, dans la nature et par nature (la musique de la rivière, des oiseaux…, qui n'a aucun besoin d'intervention humaine). Dans l'approche extrinsèque, la musique est une fonction projetée, une perception, sociologique par nature. Elle a tous les sens et au-delà, mais n'est perçue que dans un seul : la musique des oiseaux n'est musique que par la qualification que l'on veut bien lui donner.

L'idée que l'être est musique est ancienne et semble dater des pythagoriciens selon Aristote. Dans la "Métaphysique" il dit : .

Il est à noter que la définition de la science des sons par les pythagoriciens est « une combinaison harmonique des contraires, l'unification des multiples et l'accord des opposés » la science des sons est une des quatre sciences de la mesure, supérieure aux mathématiques car elle s'appuie sur la justesse, si vous essayez de terminer l'opération de diviser 10 par 3 en mathématiques vous ne pouvez terminer cette opération alors que le temps musical le permet.

Les deux sciences sensibles de la mesure que sont la musique et l'astronomie ont été laissées de côté à l'époque de Platon pour ne conserver que les deux sciences techniques de la mesure que sont l'arithmétique et la géométrie. Il est bon de se rappeler qu'au départ la science des sons était éthique et médicale et servait à calmer les passions humaines et à remettre les facultés de l'âme à leur juste place, dixit Pythagore, et lorsque cette expérience était réalisée vous étiez capable d'être vous-même et de là d'acquérir les savoir-faire comme dans une sorte d'accordage de l'être humain qui vise à laisser s'exprimer la résonance universelle de la sagesse.

Cette définition intègre l'homme à chaque bout de la chaîne. La musique est conçue et reçue par une personne ou un groupe (anthropologique). La définition de la musique, comme de tout art, passe alors par la définition d'une certaine forme de communication entre les individus. La musique est souvent jugée proche du langage (bien qu'elle ne réponde pas à la définition ontologique du langage), communication universelle susceptible d'être entendue par tous et chacun, mais réellement comprise uniquement par quelques-uns. Boris de Schlœzer, dans "Problèmes de la musique moderne" (1959), dit ainsi : .

La musique est généralement considérée comme un pur artefact culturel. Certains "prodiges" semblent néanmoins disposer d'un don inné. Les neuropsychologues cherchent donc à caractériser les spécificités des capacités musicales. Le caractère plus ou moins inné des talents artistiques est scientifiquement discuté.

Pour beaucoup, la musique serait propre à l'humain et ne relèverait que peu de la biologie, si ce n'est par le fait qu'elle mobilise fortement l'ouïe. Un débat existe pourtant sur le caractère inné ou acquis d'une partie de la compétence musicale chez l'Homme, et sur le caractère adaptatif ou non de cette "compétence".

Plusieurs arguments évoquent une origine et des fonctions culturelles ou essentiellement socio-culturelles. De nombreux animaux chantent instinctivement, mais avec peu de créativité, et ils semblent peu réceptifs à la musique produite par les humains. Une rythmique du « langage » et du chant existe respectivement chez les primates et chez les oiseaux, mais avec peu de créativité.

Chez l'humain, le chant et le langage semblent relever de compétences cérébrales en partie différentes. L'alphabet morse est une sorte de code « musical » qui a un sens (caché pour celui qui ignore le code) ; C'est clairement un artefact culturel (personne ne naît en comprenant le morse, car ni sa production ni son interprétation ne sont inscrites dans nos gènes). Chez l'homme, la voix, le langage et la capacité à interpréter un chant évolue beaucoup avec l'âge , ce qui évoque un lien avec l'apprentissage.

Enfin, la musique n'est pratiquée « à haut niveau » que par quelques individus, et souvent après un long apprentissage ; ceci évoque une origine culturelle, ce que les ethnomusicologues et les compositeurs contemporains ont longtemps renforcé. Mais il existe des exceptions, et l'exploration du fonctionnement du cerveau questionne ce point de vue.

Les neuropsychologues ont dès le début du mis en évidence une composante génétique à certains troubles de l'élocution. Ils ont aussi démontré que certaines structures du cerveau (aires cérébrales frontales inférieures pour l'apprentissage de la tonalité, et l'hémisphère droit notamment) dont l'intégrité est indispensable à la perception musicale, révèlent l'existence d'un substrat biologique. Ce substrat neuronal peut d'ailleurs être surdéveloppé chez les aveugles (de naissance ou ayant précocement perdu la vue) ou être sous-développé chez les sourds. Certains auteurs estiment que tout humain a une compétence musicale. Ceci ne permet cependant pas d'affirmer que la compétence musicale est biologiquement acquise.

La musique, ou plus exactement la « capacité musicale », la « dysmélodie » et l"'amusie congénitale" (incapacité à distinguer les fausses notes, associée à une difficulté à faire de la musique, ou à « recevoir » la musique), qui toucherait 4 % des humains selon Kalmus et Fry (1980), ou les émotions suscitées par la musique évoquent une composante biologique importante, notamment étudiée par le Laboratoire international de Recherche sur le Cerveau, la Musique et le Son (BRAMS) de l'Université de Montréal. Des études pluridisciplinaires associant la musicologie à la génétique et aux recherches comportementales et comparatives permettraient de préciser les liens entre musique et fonctions cérébrales en neurosciences.

La pratique de la musique semble être un « fait culturel » très ancien, mais 96 % des humains présentent des capacités musicales jugées « spontanées » par les neuropsychologues. Au-delà des aspects neurologiques de l'émission et de l'audition de la voix et du chant, le cerveau animal (des mammifères et oiseaux notamment) montre des compétences innées en termes de rythme, notamment utilisées pour le langage. La musique et la danse ont des aspects fortement transculturels ; elles semblent universellement appréciées au sein de l'humanité, depuis ans au moins d'après les instruments découverts par l'archéologie, et la musique d'une culture peut être appréciée d'une autre culture dont le langage est très différent.

L'imagerie cérébrale montre que la musique active certaines zones de plaisir du cerveau et presque tous les humains peuvent presque spontanément chanter et danser sur de la musique ce qui peut évoquer des bases biologiques et encourager une "biomusicologie".

La mémorisation ou la production d'une mélodie semblent mobiliser des réseaux neuronaux particuliers.

La musique aurait-elle une fonction biologique particulière ? ... même si elle ne semble pas avoir une utilité claire en tant que réponse adaptative (tout comme la danse qui lui est souvent associée).

Quelques auteurs comme Wallin estiment que la danse et la musique pourrait avoir une valeur adaptative en cimentant socialement les groupes humains, via la « contagion émotionnelle » que permet la musique.

Les résultats de l'étude de la compétence musicale du bébé et du jeune enfant (ex : chantonnement spontané), et de l'émotion musicale et du « cerveau musical » dans le cerveau, apportent des données nouvelles. Hauser et McDermott en 2003 évoquent une « origine animale » à la musique, mais Peretz et Lidji en 2006 proposent un point de vue intermédiaire : il existe une composante biologique, mais "".

Si la musique produit des effets sur les groupes, c'est parce que dès qu’on entend une mélodie, on peut s’y associer. Les muscles s’activeraient pour que l’on puisse se mettre à chanter ou à danser comme les autres. Ainsi, le rythme d’une mélodie servirait de ciment social en tissant un lien physique.

D’ailleurs, la musique stimule des régions du cerveau dédiées à la perception du lien social. Il s’agit notamment du sillon temporal supérieur, une région du cortex cérébral localisée près des tempes, et qui s’active par exemple quand on observe les mouvements des yeux d’une personne, ou que l’on est sensible au ton de sa voix (et non à la signification des mots qu’elle prononce).

En 2008, Nikolaus Steinbeis, de l’Institut Max Planck pour la cognition humaine et les sciences du cerveau, et Stefan Koelsch, de l’Université de Sussex en Grande-Bretagne, ont montré que cette zone « sociale » s’active chez des personnes écoutant des accords musicaux. Tout se passe comme si, en entendant de la musique, notre cerveau se tournait vers l’autre. La musique contribuerait à tisser des liens sociaux ; les hymnes le font à l’échelle des nations, les groupes de rock à celle des communautés d’adolescents, les comptines entre parents et enfants.

La musique pourrait aussi avoir une base biologique forte, mais en quelque sorte résulter des hasards de l'évolution et n'avoir aucune fonction adaptative ; c'est une possibilité retenue en 1979 par Gould et Lewontin.

Selon Claude Debussy, . Mais pour Saint-Saëns, . Pour Stravinsky, .

Selon cette définition, la musique est l’« art des sons » et englobe toute construction artistique destinée à être perçue par l’ouïe.

La musique, comme art allographique, passe par l'œuvre musicale. Chacune est un objet intentionnel dont l'unité et l'identité est réalisée par ses temps, espace, mouvement et forme musicaux, comme l'écrit Roman Ingarden. Objet de perception esthétique, l'œuvre est certes d'essence idéale, mais son existence hétéronome se concrétise par son exécution devant un public, ou par son enregistrement y compris sa numérisation. Comme toute œuvre, l'œuvre musicale existe avant d'être reçue, et elle continue d'exister après. On peut donc s'interroger sur ce qui fait sa pérennité : combien d'œuvres survivent réellement à leurs compositeurs ? Et sont-elles vraiment toutes le reflet de son style, de son art ? On entend surtout par œuvre musicale le projet particulier d'une réalisation musicale. Mais cette réalisation peut être décidée par l'écoute qu'en fait chaque auditeur avec sa culture, sa mémoire, ses sentiments particuliers à cet instant précis autant que par la partition, transcription qui ne comporte pas toute la musique. À partir de la Renaissance et jusqu'au début du , l'unique support de la musique a été la partition de musique. Cette intrusion de l'écrit a été l'élément-clé de la construction de la polyphonie et de l'harmonie dans la musique savante. La partition reste unie au nom du ou des musiciens qui l'ont composée ou enregistrée. Certaines œuvres peuvent être collectives, d'autres restent anonymes. Depuis la généralisation des moyens techniques d'enregistrement du son, l'œuvre peut également s'identifier à son support : l'album de musique, la bande magnétique ou à une simple calligraphie de la représentation du geste musical propre à transcrire l'œuvre du compositeur.

L'informatique musicale a fait évoluer encore cette notion d'œuvre, puisqu'à présent un logiciel est susceptible d'engendrer « automatiquement » une œuvre musicale, ou de produire des sons auxquels l'interprète pourra réagir.

Dans son essai sur les « célibataires de l'art », Jean-Marie Schaeffer estime que, dans l’art moderne (et "a fortiori" dans l’art technologique du ), la question-clé : « Qu’est-ce que l’art ? » ou « Quand y a-t-il art ? » s’est progressivement transformée en : « Comment l’art fonctionne-t-il ? ». En musique, ce déplacement d’objet a posé le problème des éléments que l’on peut distinguer "a priori" dans l’écoute structurelle d’une œuvre. En 1945 apparurent les premières formes d'informatique, et en 1957 on a assisté, avec l’arrivée de l'électronique musicale, à un point de bifurcation. D'abord une nouvelle représentation du sonore qui, bien que difficile à maîtriser, a en fait ouvert des perspectives nouvelles. Ensuite, ces techniques ont remis en cause certaines réflexions théoriques sur la formalisation de la pensée créatrice, renvoyant le compositeur à la confrontation, essentielle dans sa démarche, entre un formalisme abstrait et l’élaboration d’un matériau fonctionnel. La transition vers l’atonalité a détruit les hiérarchies fonctionnelles et transformé le rôle tenu par les fonctions tonales, élaboré depuis Monteverdi.

De fait, la logique des formes musicales est donc devenue surtout une logique fonctionnelle, dans la mesure où elle permet de maintenir la cohésion de l'œuvre, même si les éléments de composition sont multiples (éléments rythmiques, contrapuntiques, harmoniques). La notion de processus compositionnel, a permis de passer de la vision statique de l’objet musical (celui que l’on peut répertorier, et qui cesserait de vivre en entrant dans le patrimoine) à une vision dynamique. Cette vision est évolutive, ce que ne prenaient pas en considération les théories fondées sur la GestaltPsychologie qui figent la pensée dans des images accumulées dans la mémoire. Le processus musical est plus que la structure : il est en effet une forme dynamique, un devenir. Ce devenir est marqué par l’empreinte du sonore, c’est-à-dire par un matériau musical, et pas uniquement par l’outil ou par la théorie.

À partir de la théorie de la communication de Shannon et Weaver, d'autres définitions insistent plus sur les moyens de réception que sur la chaîne de production de la musique.

L'utilisation de musique dans d'autres œuvres (qui sont donc des œuvres de collaboration tel qu'un film, un dessin animé ou un documentaire) pose la question des fonctionnalités de la musique, en particulier dans les contenus audiovisuels. La musique remplit des fonctions lorsqu’elle est utilisée (ou incorporée, synchronisée). La musicologue polonaise Zofia Lissa présente douze fonctions principales, la plupart n’étant pas mutuellement exclusives. Elle cherche à en comprendre la façon dont la musique est utilisée dans les films et l'effet qu'elle produit : par exemple la fonction de Leitmotiv qui contribue à tracer la structure formelle d'un film : description des personnages, des atmosphères, des environnements, ou encore la fonction d'anticipation d'une action subséquente. Plus largement, se pose la question des fonctionnalités de la musique dans un ensemble audiovisuel (qui peut être un flux radiophonique ou un flux télévisuel composé de contenus qui se succèdent sans interruption). Dans un tel contexte, la musique (sous la forme d'un indicatif d'émission, d'un jingle, etc.) remplit pour les diffuseurs diverses fonctions. Elle peut agir comme un élément d’accroche pertinente et capter une attention par sa capacité à séduire ou à émouvoir ou encore à annoncer. Mario d'Angelo, en s'appuyant doublement sur une compréhension des finalités recherchées du côté de l'offre (par les concepteurs des contenus audiovisuels et du flux télévisuel) et des finalités perçues du côté de la réception (par les téléspectateurs), retient six fonctions : mnémonique, identitaire, émotionnelle, esthétique, didactique et narrative ; elles ne sont pas mutuellement exclusives.

Le temps gouverne la musique comme il gouverne la perception du son : depuis le micro-temps, qui est l'échelle de la vibration sonore car le son est une mise en vibration de l'air, jusqu'à la forme musicale, construction dans un temps de l'écoute. Comme la forme musicale ne nous est révélée qu’au fur et à mesure, chaque instant est en puissance un moment d’avenir, une projection dans l’inconnu. C’est le sens du titre d’une œuvre d’Henri Dutilleux qui propose de nous plonger dans le « mystère de l’instant ». Le théologien suisse Hans Urs von Balthasar livre cette métaphore judicieusement musicale de la condition humaine : « Faites donc confiance au temps. Le temps c’est de la musique ; et le domaine d’où elle émane, c’est l’avenir. Mesure après mesure, la symphonie s’engendre elle-même, naissant miraculeusement d’une réserve de durée inépuisable ».

Dans cette composante temporelle, la musique peut se déployer selon trois dimensions fondamentales :

Selon les genres musicaux, l'une ou l'autre de ces trois dimensions pourra prédominer :

Grâce au développement des recherches de l'acoustique musicale et de la psychoacoustique, le son musical se définit à partir de ses composantes timbrales et des paramètres psychoacoustiques qui entrent en jeu dans sa perception. D'objet sonore, matériau brut que le musicien doit travailler, ce matériau devient objet musical ; la musique permet de passer à une dimension artistique qui métamorphose le « donné à entendre ». Le silence n'est plus « absence de son ». Même le fameux "4′33″" de John Cage, est un « donné à entendre ». Mais ce « donné à entendre » englobe désormais un matériau de plus en plus large. Depuis le début du , cet élargissement s’opère vers l’intégration des qualités intrinsèques de notre environnement sonore (concerts bruitistes, introduction des sirènes chez Varèse, catalogues d’oiseaux de Messiaen). Comment distinguer alors bruit et signal, comment distinguer ordre et désordre, création musicale et nuisance sonore ? Le bruit, c’est uniquement ce qu’on ne veut pas transmettre et qui s’insinue malgré nous dans le message ; en lui-même il n’a aucune différence de structure avec un signal utile. On ne peut plus distinguer comme auparavant le son purement musical et le bruit. Avec l’élaboration d’une formalisation par nature des fonctions du bruit, les sons inharmoniques (apériodiques) qui liés à la vie courante participent désormais, dans l’intégration du sonore, à la construction musicale. Tous les éléments de notre environnement sonore contiennent une certaine part de bruit, qui a vocation de devenir fonction structurante par destination.

L’ensemble de ces bouleversements conceptuels accompagne les découvertes scientifiques et techniques qui permirent de développer des factures instrumentales nouvelles (notamment avec l'électronique). L’instrument de musique primitif se voulait représentation des sons naturels (le vent dans les arbres se retrouvant dans le son de la flûte, le chant des oiseaux dans celui de l’homme…). À cette condition, il était le seul capable de traduire le musical (d’opérer une distinction entre sons harmoniques et bruits). L’extension des techniques aidant, la notion même d’instrument s’est trouvée redéfinie… . La machine et l’instrument se sont rejoints. Ce que les hommes acceptent de reconnaître comme musical correspond désormais à une appropriation d’un matériau sonore étendu, à une intégration de phénomènes jusqu’alors considérés comme bruits.

Avec la composition assistée par ordinateur, première expérimentation musicale à utiliser l’ordinateur, les théories musicales se sont tour à tour préoccupées d’infléchir ou de laisser l’initiative à la machine, et, parallèlement, de libérer totalement l’homme de certaines tâches de régulation ou de lui laisser une part importante de création. La problématique oscille ainsi, de façon quasi paradoxale mais finalement foncièrement dialectique, entre déterminisme et aléatoire, entre aléa et logique, entre hasard et nécessité. Le formalisme aléatoire (mathématisé) « calcule » sans qu’il n’empiète sur les atouts sensibles du compositeur. Les objets mathématiques qui se sont développés créent véritablement un intermédiaire vers des paradigmes esthétiques que l’expérimentation musicale essaie petit à petit de mettre à jour, intermédiaire qui se situerait entre une ordre régulier, périodique, et un chaos incontrôlé, aléatoire et singulier. Hiller, le père de la composition assistée par ordinateur, sans juger qui pourrait effectuer les compromis, considérait déjà que « la musique est un compromis, voire une médiation, entre la monotonie et le chaos ».

Artistiquement, à la théorie de l'information de Shannon répond la théorie de l’indétermination de John Cage (l’information est maximale donc nulle). En 1951, Cage et Feldman s’en remirent à l’aléatoire codifié du "I Ching" pour bâtir leur œuvre "Music of Changes". Cette œuvre, qui brise les carcans de la notion traditionnelle d’œuvre musicale, sert de manifeste artistique au concept de l’indétermination.

Cage introduit subrepticement le hasard dans la composition dans un sens plus combinatoire. "Music of Changes" laisse place à l’aléa contre la logique en réhabilitant le pouvoir créateur de l’expérience divinatoire, le pouvoir de la création par le hasard. John Cage, Morton Feldman et Earle Brown utilisaient aussi un hasard codifié, l’aléatoire du "I Ching", livre de divination chinois qui laisse entrevoir un certain nombre de combinaisons par pentagrammes. Le hasard est sublimé par le destin dans une prédication divinatoire (Concerto pour piano (1957)). Puis, chez Cage les théories devenant paroxystiques, il prône la raréfaction de la musique jusqu’au total aléa (4’33’’) : l’écoute est focalisée vers des objets sonores qui n’ont pas été directement prévus pour cela. Peu de critiques ont pu abonder dans son sens, déplorant que ces théories ne servent qu’à la justification d’un « coup » médiatique.

Pour tenter de réduire la proportion de hasard fatalement confiée à la technique, la machine fut utilisée par la suite pour ses fonctions de contrôle de l’automation qui assure un enchaînement continu d’opérations mathématiques et logiques. Pierre Barbaud débuta dans cet esprit ses travaux sur la composition « automatique » et mit au point avec Roger Blanchard en 1959 le programme ALGOM I-5 pour l’ordinateur "Gamma" 60 du Centre de calcul électronique de la compagnie Bull à Paris.

Cette mathématisation accrue des possibles, continua à être prise en compte, mais en essayant de reprendre à la machine la part de responsabilité qu’elle avait conquise. Dès 1954, Iannis Xenakis avait créé son opus un, "Metastasis" pour 65 instruments ; c’est la première musique entièrement déduite de règles et de procédures mathématiques. Pour son créateur, il s’agit de mettre en pratique une relation directe entre musique et architecture, combinaison certes peu commune, mais qui, pour l’assistant de Le Corbusier va de soi. Il la mettra à profit en utilisant les mêmes règles de construction dans l’élaboration des plans du pavillon Philips pour l’exposition universelle et internationale de Bruxelles en 1958 (pavillon où seront jouées dans un même concert les créations des œuvres de Varèse ("Poèmes électroniques") et de Xenakis ("Concret PH").

En 1956 est publiée une théorisation de la musique stochastique qui s’appuie entre autres sur la théorie des jeux de von Neumann. Le hasard n’y est déjà plus une simple chance ; contrairement à la troisième sonate de Pierre Boulez ou aux autres œuvres « ouvertes », contrairement à Cage, et à sa démission de compositeur, la probabilité est entièrement calculée, les règles explicitées ("Achorripsis" ou "ST/10-1" en 1961). Le processus global est prévisible, même si les évènements qui le composent sont aléatoires. Par cette philosophie de la création, Xenakis essaie de se rapprocher des phénomènes biologiques et des événements du monde vivant.

Un « système musical » est un ensemble de règles et d'usages attachés à un genre musical donné. On parle parfois de « théorie musicale ». La conception de la musique comme système peut aller très loin, et les anciens Grecs comptaient la musique comme une des composantes des mathématiques, à l'égal de l'arithmétique, de la géométrie et de l'astronomie. Voir l'article « Harmonie des sphères ». Plus près de nous, Rameau dans son "Dictionnaire de la Musique" arrive à considérer la musique comme étant à la base des mathématiques.

Certaines musiques possèdent en outre un "système de notation". La musique occidentale, avec son solfège, en est un exemple notoire. Dans ce cas, il est difficile de séparer le système musical du système de notation qui lui est associé. Certaines musiques traditionnelles sont uniquement de transmission orale, et développent des systèmes musicaux différents.

En occident, la musique s'écrit avec des signes : les notes de musique, les clés, les silences, les altérations Les notes de musique s'écrivent sur une portée, composée de 5 lignes parallèles. La portée comporte aussi des barres verticales. L'espace entre deux barres de mesure est une mesure. Il existe aussi des doubles barres. Les sept notes de musique sont : do (ou ut), ré, mi, fa, sol, la et si. Les notes s'écrivent sur la portée ou sur des lignes supplémentaires placées au-dessus ou en dessous de la portée. La portée va du plus grave (en bas) au plus aigu (en haut). Une même note peut être jouée de façon plus ou moins grave ou aigüe.

Une octave est la distance qui sépare deux notes identiques, plus ou moins graves. Il existe de très nombreux signes musicaux pour indiquer la durée d'une note. En particulier :

Les silences sont les moments du morceau de musique sans son. Il existe sept figures de silence : la pause, la demi-pause, le soupir, le demi-soupir, le quart de soupir, le huitième de soupir, le seizième de soupir. Les clés indiquent la base de départ de la lecture de la partition. Il existe trois clés : la clé de sol, la clé de fa et la clé d'ut.

Les altérations modifient le son d'une note pour le rendre plus grave ou plus aigu. Il existe trois altérations :
Il existe également des double-dièse et des double-bémol. Si l'altération concerne la ligne entière, elle est placée près de la clé et s'appelle armure. Si elle ne concerne qu'une note (ou toute note identique suivant la note modifiée dans la mesure), elle est placée juste avant la note.

Une succession de notes voisines (ou conjointes) forme une gamme. Celle-ci est donc une succession de sons ascendants (de plus grave au plus aigu) ou descendants (du plus aigu au plus grave). Lorsqu'elle respecte la loi de la tonalité, il s'agit d'une gamme diatonique. Une succession de notes qui ne sont pas voisines (ou disjointes) forme un arpège, un arpège est souvent le jeu égrainé d'un accord du plus aigu au plus grave ou l'inverse.

Les notes de la gamme (alors également appelées des degrés), sont séparées par des tons ou des demi-tons. On distingue les demi-tons diatoniques situés entre deux notes de nom différent et demi-tons chromatiques situés entre deux notes de même nom mais altérées par un dièse ou un bémol. La gamme chromatique comprend tons et demi-tons, tandis que la gamme diatonique ne contient que les tons.

Les notes sont séparés par des intervalles : lorsque les notes sont voisines, par exemple do et ré, l'intervalle est dit de deux degrés ou seconde. On parle ensuite, par éloignement croissant entre les notes, de tierce, quarte, quinte, sixte, septième et octave (intervalle de huit degrés). Comme il existe sept notes de musique, l'octave, qui relie 8 degrés, relie deux notes de même nom, mais de hauteur différente (on dit que la note est plus aigüe ou plus grave d'une octave). Au-delà de l'octave, on parle d'intervalle redoublé (on parle de neuvième, de dixième...). Par défaut, on considère que l'intervalle est ascendant (il va du grave à l'aigu) sauf si le contraire est indiqué. L'intervalle peut en effet être renversé. Les différents types d'intervalle sont également appelés mineur, majeur, juste. Ils sont alors dit diminués ou augmentés.

La mesure est la manière d'ordonner les notes et les silences. La mesure se lit sur la portée par la barre de mesure (verticale). Toutes les mesures doivent avoir une durée égale. La double barre de mesure indique la fin d'un morceau, la césure indique la fin d'une partie et la simple double barre indique un changement d'armure de la clé ou un changement de manière de compter les mesures lors d'un changement de mesure. Certains temps sont dits temps fort et d'autres temps faible.

La mesure se subdivise en deux, trois ou quatre temps. La mesure d'un morceau est annoncée par deux chiffres superposés après une double barre. Le chiffre supérieur indique le nombre de temps dans la mesure et le chiffre inférieur indique la durée allouée à chaque temps.

On distingue :

Le rythme est la manière dont sont ordonnées les durées (c'est-à-dire les notes et les silences). On distingue :

Le mouvement est la vitesse d'exécution du morceau de musique. Il est indiqué par un mot placé au début du morceau. Du plus lent au plus vif (rapide), les principaux mouvements sont : "Largo", "Lento", "Adagio", "Andante", "Andantino", "Moderato", "Allegretto", "Allegro", "Presto". Mais il en existe des dizaines d'autres.

La musique peut être réalisée avec des objets de la nature (bout de bois par exemple) ou de la vie quotidienne (verres à eau et couverts par exemple également), des parties du corps (battements de mains, de pieds) avec la voix humaine, ou avec des instruments spécialement conçus à cet effet.

Les instruments de musique peuvent être classés selon le mode de production du son :

Il existe plusieurs manières de classer la musique, notamment :

Un genre musical désigne des pratiques musicales de même nature et de même destination.
En France, les bibliothèques municipales suivent les catégories des principes de classement des documents musicaux suivants, qui sont ici limitées aux premières subdivisions de chaque classe suivies d'exemples de sous-catégories.

Cette définition considère la musique comme un fait de société, qui met en jeu des critères tant historiques que géographiques. La musique passe autant par les symboles de son écriture (les notes de musique) que par le sens qu’on accorde à sa valeur affective ou émotionnelle. En Occident, le fossé n’a cessé de se creuser entre ces musiques de l’oreille (proches de la terre, elles affirment une certaine spiritualité et jouent sur le parasympathique) et les musiques de l’œil (marquées par l’écriture, le discours, et un certain rejet du folklore). Les cultures occidentales ont privilégié l’authenticité et inscrit la musique dans une histoire qui la relie, par l’écriture, à la mémoire du passé. Les musiques d’Afrique font plus appel à l’imaginaire, au mythe, à la magie, et relient cette puissance spirituelle à une corporalité de la musique. L’auditeur participe directement à l’expression de ce qu’il ressent, alors qu’un auditeur occidental de concerts serait frustré par la théâtralité qui le délie de participation corporelle. Le baroque constitue en occident l’époque charnière où fut mise en place cette coupure. L’écriture, la notation, grâce au tempérament, devenait rationalisation des modes musicaux.

Chaque époque est tributaire des rapports entre l’art et la société, et plus particulièrement entre la musique et les formes de sa perception. Cette étude sociale aux travers des âges est menée dans un essai de Jacques Attali ("Bruits", Paris, PUF, 1978).

La libération esthétique du compositeur par rapport à certaines règles et interdits, fondés au cours de l’histoire de la musique, et celle, concomitante, des liens qu’il noue avec l’auditeur est un facteur d’évolution. Elle va rarement sans heurts. L’évolution historique des courants stylistiques est jalonnée de conflits exprimés notamment à travers la question classique : .

L’histoire évolue également par alternance de phases de préparations et de phases de révélation intimement liées entre elles. Ainsi, la place prépondérante qu’occupe Jean-Sébastien Bach dans le répertoire de la musique religieuse, conséquence du génie créatif de ce musicien d’exception, ne peut nous faire oublier tous les compositeurs qui l’ont précédé et qui ont tissé ces liens avec le public en le préparant à des évolutions stylistiques majeures. L’œuvre de Bach concentre de fait un faisceau d’influences allemandes (Schütz, Froberger, Kerll, Pachelbel), italiennes (Frescobaldi, Vivaldi), flamandes (Sweelinck, Reincken) et françaises (Grigny et Couperin), toutes embrassées par le Cantor.

Ce type d'évolution incite Nikolaus Harnoncourt à considérer que « Mozart n’était pas un novateur ». Pour lui, Mozart ne fut que le cristallisateur du style classique, et le génie qui sut porter à son apogée des éléments dans l’air du temps. Concentrer les influences d’une époque, consolider les éléments et en tirer une nouvelle sève, c’est là le propre de tout classicisme. Contrairement aux musiciens contemporains expérimentateurs qui cherchent à la fois « le système et l’idée » (selon un article fondateur de Pierre Boulez), Mozart n’aurait ainsi jamais rien inventé qui ne lui préexistait. Les mutations radicales qu’il a su imposer proviennent de conceptions déjà en germe. Le lien entre l’évolution des techniques et l’écriture, entre les données matérielles (instruments, lieux, espaces) et l’expression, contraint le compositeur dans la double ambiguïté du carcan systémique et de la libération expressive. Dans cette perspective, la musique se construit autour de structures, de catégories, qu’il faut savoir dépasser (« travailler aux limites »).

En termes de style, la musique semble avoir souvent oscillé au cours des siècles entre une rhétorique de la litote et du minimum d’éléments syntaxiques (c’est le cas de Bach ou de Lully, c’est aussi celui, à un degré extrême de John Cage) et une excessivité (Richard Wagner ou Brian Ferneyhough par exemple), dilution dans l’emphase (autre définition du baroque), révolte contre les alignements conceptuels. Avec le recul historique, les phases de cette élaboration paraissent suivre des paliers successifs. Le pouvoir expressif passe d’apports strictement personnels à une complexification qui dénature les premières richesses de la nouveauté en cherchant à épuiser les ressources du matériau initial.

La musique peut également être définie et approchée dans une perspective de recherche esthétique. Cette vision esthétique de la musique, peut être, du côté de l’auditeur, définie par la définition du philosophe français Jean-Jacques Rousseau : . De la Renaissance jusqu’au , la représentation des sentiments et des passions s’est effectuée par des figures musicales préétablies, ce que Monteverdi a appelé la "seconda prattica expressio Verborum". La simultanéité dans la dimension des hauteurs (polyphonie, accords), avancée de l’"Ars nova" au (Ph. De Vitry), a été codifiée aux ("Traité de l’harmonie universelle" du père Marin Mersenne, 1627, "Traité de l’harmonie réduite à ses principes naturels" de Jean-Philippe Rameau, 1722). Depuis, la représentation de la musique affiche des tendances plus personnalisées. Cette traduction de la personnalité aboutit tout naturellement au aux passions développées par la musique romantique.

Certains estiment que les grandes écoles de style ne sont souvent qu’un regroupement factice autour de théories "a priori". La musique passe autant par les symboles de son écriture que par les sens accordés à sa « valeur » (affective, émotionnelle…). Théorie et réception se rejoignent pour accorder à la musique un statut, artistique puisque communication, esthétique puisque traduction de représentation (cf. les théories de la réception et de la lecture selon l’école de Constance). Les trois pôles du phénomène musical sont le compositeur, l’interprète et l’auditeur. Cependant un fossé n’a cessé de se creuser dans la musique occidentale entre le compositeur et son public. Les recherches musicales actuelles tendent à faire de la musique un support de la représentation de la complexité de notre monde (de l’infiniment petit à l’infiniment grand). Elles se seraient alors éloignées de la recherche purement esthétique.
Chaque étape stylistique importante (Renaissance, baroque, classicisme, romantisme, et d’une certaine façon modernisme), porte ainsi en elle une ou plusieurs bifurcations esthétiques. Au milieu du , dans les années 1947-1950, après les assauts formalistes du sérialisme, le noyau fédérateur qui subsistait à l’arrivée du magnétophone et des techniques électroniques résidait dans la manifestation d’un sonore perceptible et construit. Les traités d’harmonie de la fin du (par exemple le "Traité d’harmonie" de Th. Dubois), reprenant la théorie de Rameau, s’étaient attachés à amarrer la tonalité à une nécessité développée par l’histoire depuis Monteverdi. Or, en rompant dès 1920 avec les schémas classiques de la tonalité, le aurait confiné le système tonal aux seuls et s, et même réduit à cette époque, dans la stricte délimitation géographique que nous lui connaissons, à savoir en Europe et aux États-Unis. La définition classique de la musique comme « art de combiner les sons » se serait effondrée peu après le milieu du .

Pendant longtemps la musique fut considérée comme une science au même titre que l’astronomie ou la géométrie. Elle est très liée aux mathématiques. De nombreux savants se sont penchés sur les problèmes musicaux tels que Pythagore, Galilée, Descartes, Euler.

Pythagore étudie la musique comme mettant en jeu des rapports arithmétiques au travers des sons. L'harmonie qui en procède se retrouve pour lui et gouverne l'ordonnancement de ses sphères célestes. Ainsi Platon dans "La République", VII, 530d, rappelle que la Musique et l'Astronomie sont des sciences sœurs. Au , Martianus Capella présente la musique comme un des sept arts libéraux. Avec Boèce, la théorie musicale est distinguée de la pratique musicale. La musique entendue comme activité ("praxis"), qui est la musique des musiciens, sera alors déconsidérée et considérée comme un art subalterne, un « art mécanique », de la musique entendue comme savoir ("théoria") qui seule sera reconnue comme vraie musique, et enseignée comme un des 7 arts libéraux, parmi les 4 disciplines scientifiques du second degré de cet enseignement, et que Boèce nomma le « "quadrivium" ». La musique (théorique) a alors le même statut que l'arithmétique, la géométrie et l'astronomie.

Jean-Philippe Rameau, considérait que « la Musique est une science qui doit avoir des règles certaines ; ces règles doivent être tirées d’un principe évident, et ce principe ne peut guère nous être connu sans le secours des Mathématiques ».

La musique est l'une des pratiques culturelles les plus anciennes et comporte le plus souvent une dimension artistique. La musique s'inspire toujours d'un « matériau sonore » pouvant regrouper l’ensemble des sons perceptibles, pour construire ce « matériau musical ». À ce titre elle a, dans les années récentes, été étudiée comme une science. La phénoménologie de Husserl, réfutait ces points de vue. L'ouïe, qui est le plus adapté de nos sens pour la connaissance des sentiments est, a contrario, le moins apte à la connaissance objective qui fonde la science. La musique est donc un concept dont la signification est multiple, elle peut tout bien être un concept hérité de fonctions de survies ancestrales capables, par le biais d'émotions, d'indiquer ce qui est propice à la survie de l'espèce ; mais elle est également dépendante des goûts de chacun. Il en résulte qu'il est difficile d'établir une définition unique regroupant tous les genres musicaux.

La musique est utilisée pour ses effets thérapeutiques pour des personnes atteintes de la maladie d'Alzheimer. Elle est également une aide pour réveiller des patients d'un coma à l'écoute d'une musique familière (plusieurs cas avérés). La musique a des bienfaits sur les personnes atteintes de la maladie de Parkinson ou victimes d'un accident vasculaire cérébral.

Des études ont montré que les enfants qui font de la musique voient leurs compétences scolaires s’améliorer. De manière plus générale, la pratique musicale permet de lutter contre les effets du vieillissement cognitif.

L'écoute de musique à très fort niveau sonore lors d'un concert, baladeur, par un musicien... sans protection auditive peut aboutir à un traumatisme sonore. Il se manifeste soit par des pertes auditives partielles ou totales, des acouphènes et de l'hyperacousie. Les conséquences sont parfois dramatiques dans la vie de l'individu car ces pathologies sont irréversibles.

La musique que les compositeurs créent peut être entendue par le biais de nombreux médias ; la manière la plus traditionnelle étant par la présence des musiciens eux-mêmes. La musique en direct peut être écoutée par radio, à la télévision ou sur Internet. Certains styles musicaux se focalisent plus sur la production d'un son pour une performance, tandis que d'autres se focalisent plus sur l'art de « mélanger » des sons lorsqu'ils sont joués en direct.

Le rapport entre la musique et les médias est une question complexe comportant plusieurs dimensions (esthétique, sémantique, cognitive, économique, sociale, et même organisationnelle). Pour Mario d'Angelo, il convient de développer une approche globale, systémique pour prendre en compte en même temps ces différentes dimensions. Ainsi pour la télévision par exemple, le flux d'image est accompagné d'un flux sonore qui comporte de la voix, des bruits et de la musique. La forte présence de musique trouve son origine bien au-delà des seules concerts ou spectacle avec musique retransmis (en direct ou en différé). La musique est également présente dans les contenus dits de stocks et dans les contenus dits de flux. Les premiers nécessitent une postproduction avec du montage et du mixage. La musique y est donc présente par incorporation (ou synchronisation) dans la bande son, selon des niveaux de mixages comme les films cinématographiques, feuilletons, séries télévisuelles, téléfilms, documentaires, docu-fictions, spots publicitaires etc.. Les seconds (les contenus dits de flux) nécessitent pas ou très peu de postproduction (journaux télévisés, magazines, reportages, émissions enregistrée depuis un plateau de télévision etc.).

Tandis que les sonorités cinématiques ont émergées au début du , un grand nombre de musiciens d'orchestre ont participé à ces enregistrements de son. Dans la plupart des cas, les performances en direct impliquent des compositions pré-enregistrées. Par exemple, un disc jockey utilise des platines pour le créer une sonorité appelée "scratching", et certaines compositions durant le font leur performance (le chant par exemple) en direct à l'aide d'un son pré-enregistré. Les ordinateurs et un bon nombre de claviers électroniques permettent une programmation à l'avance de notes qui peuvent être jouées (Musical Instrument Digital Interface, MIDI). Le public peut également faire une performance en participant lors d'un karaoké, une activité d'origine japonaise centrée sur des chansons composées par des professionnels et chantées par des particuliers. La plupart des machines de karaoké possèdent un écran adapté pour aider ces particuliers ; les particuliers peuvent ainsi chanter les paroles tout en gardant le rythme de la musique jouée en playback.

L'émergence d'Internet a considérablement changé la manière d'écouter la musique et contribue à un très large choix de styles musicaux. La musique sur Internet grandit également grâce aux communautés virtuelles telles que YouTube, Myspace et SoundCloud. De tels sites simplifient le contact et la communication avec des musiciens et facilitent grandement la distribution de la musique. Les musiciens professionnels tirent également profit de YouTube en tant que site gratuit promotionnel. YouTube n'est pas seulement un outil promotionnel gratuit pour les musiciens professionnels mais c'est aussi un moyen pour les amateurs de se faire connaître du grand public que ce soit de manière volontaire ou non, comme la chanteuse Irma qui a découvert par hasard qu'elle était en page d'accueil.

De nombreux philosophes ont développé des théories de la musique. C'est en particulier le cas d'Arthur Schopenhauer (joueur de flûte) pour qui la musique est l'art métaphysique par excellence. Sa philosophie eût une influence déterminante sur Richard Wagner. Friedrich Nietzsche, ami de Wagner et compositeur à ses heures, accorde également une place de choix à la musique dans sa pensée. Dans les religions, de nombreuses traditions de musiques sacrées existent, à l'instar de la musique chrétienne, de la musique bouddhique ou de la musique juive. Dans l'islam, les interprétations traditionnelles prévalant notamment dans le sunnisme tendent à considérer que la religion musulmane prohibait la musique, en exceptant certaines occasions comme les mariages. Aucun consensus entre autorités religieuses musulmanes n'existe cependant sur ce point, et la musique est présente dans les sociétés musulmanes. De riches traditions de musiques islamiques existent dans certaines branches de l'islam, comme le soufisme.





</doc>
<doc id="1939" url="https://fr.wikipedia.org/wiki?curid=1939" title="Mexique">
Mexique

Le Mexique, en forme longue les États-Unis mexicains, en espagnol ' et ', est un pays situé dans la partie méridionale de l'Amérique du Nord.

Le Mexique est membre de l'Accord de libre-échange nord-américain (ALENA, ou « TLCAN » en espagnol, « NAFTA » en anglais).

Délimité au sud par le Guatemala et le Belize, et au nord par les États-Unis d'Amérique, il est bordé à l'est par le golfe du Mexique et la mer des Caraïbes et à l'ouest par l'océan Pacifique. C'est le quatorzième pays en termes de superficie, avoisinant .

Il s'agit du onzième pays le plus peuplé du monde, l'Organisation des Nations Unies donnant le chiffre de d'habitants pour septembre 2017 dont la majorité ayant pour langue l'espagnol mexicain, reconnu par la loi ainsi que toutes les langues indigènes du pays comme langue nationale. Politiquement, le Mexique est une république constitutionnelle fédérale à régime présidentiel composée de 32 entités fédératives dont 31 ont le statut d'État, Mexico n'ayant pas ce statut car abritant la capitale politique du pays.

La présence humaine au Mexique remonte à avant le présent. Après des millénaires de développement culturel sont apparues les cultures mésoaméricaines, aridaméricaines et oasisaméricaines. Avant les premiers contacts avec les Européens, vivaient diverses civilisations, tels les Olmèques, les Toltèques, les Zapotèques, les Mayas, et les Aztèques. En 1521, l'Espagne conquit et colonisa le territoire depuis Mexico-Tenochtitlan, qui devint la capitale de la vice-royauté de Nouvelle-Espagne. Après près de 300 ans de colonisation espagnole, le territoire débuta une guerre d'indépendance contre l'Espagne en 1810, qui dans la foulée déclara sa séparation en 1813 pour établir le Mexique, avant de sortir victorieux en 1821. Le pays connaît ensuite un demi-siècle d'instabilité politique et financière, caractérisé par divers conflits dont une tentative de reconquête par l'Espagne en 1829, la guerre des Pâtisseries, une guerre contre les États-Unis, une guerre civile, une intervention française, trois républiques et deux Empires. 
Durant la présidence de Porfirio Diaz, le pays a connu une période de modernisation et de croissance économique importante. Díaz fut renversé à la suite d'une révolution en 1910, qui culmina avec la constitution de 1917 et la mise en place du système politique actuel.

Le Mexique fait partie des vingt premières puissances économiques mondiales (quinzième avec un PIB de de dollars. Mesuré en parité de pouvoir d'achat, son PIB arrive à la dixième place, devant l'Italie. Le Mexique est le neuvième plus grand producteur de pétrole au monde et le premier producteur d'argent. Puissance émergente, puissance moyenne à l'échelle mondiale et puissance régionale, le Mexique est le premier pays d'Amérique latine à avoir rejoint l'OCDE. Classé parmi les nouveaux pays industrialisés, il s'agit, selon la Banque mondiale, d'un pays à revenu intermédiaire supérieur. Son économie est fortement liée à celle des États-Unis, par son appartenance à l'ALENA. Selon l'organisation mondiale du tourisme, le Mexique est la principale destination d'Amérique latine et la treizième plus visitée au monde. En plus d’être l'un des dix-sept pays mégadivers de la planète (il abrite de 10 à 12 % de la biodiversité mondiale et comprend plus de ), il compte 33 sites culturels ou naturels inscrits par l'UNESCO au patrimoine de l'humanité. D'après le rapport de 2013 sur le développement humain "El ascenso del sur" de l'ONU, l'indice de développement humain du Mexique s'élève à 0,775 unités, et occupe la mondiale (au côté de l’Indonésie, la Turquie, la Thaïlande, l'Afrique du Sud) alors qu'en 1980 son indice ne s'élevait qu'à 0,598 unités. Le Mexique est également membre d'institutions internationales de grande envergure, telles que l'ONU, l'OMC et le G20.
Dès avant l'indépendance de la vice-royauté de la Nouvelle-Espagne, il fut décidé que le pays prendrait le nom de sa capitale, Mexico. L'utilisation de ce toponyme remonte à la fin de l'époque préhispanique (postclassique tardif), chez les Nahuas (et plus particulièrement les indigènes de la cité, les Mexica). Il était alors accolé au toponyme Tenochtitlan. 

L'origine et le sens de ce nom restent controversés. Le jésuite et historien Francisco Javier Clavijero soutient dans ses écrits qu'il dérive du mot nahuatl "Mexitl" ou "Mexitli", un nom secret de Huitzilopochtli, la divinité tutélaire des Mexica. Selon cette théorie, « Mexico » signifie « lieu [où vit] "Mēxitli" ou "Mēxtli" » ou « lieu où est construit le temple de Mexitli », en référence au "Templo Mayor". Cette thèse était aussi partagée par Fray Juan de Torquemada ; toutefois Torquemada ajoute que Mexitli viendrait des mots "metl" (« agave ») et "xictli" (« nombril »). Selon cette version, Mexico signifierait mot pour mot « lieu dans le nombril d'agave » ; cette interprétation est aussi soutenue par le franciscain Motolinia. D'autres historiens, comme Bernardino de Sahagún, José de Acosta et Diego Durán, avancent dans leurs travaux que « Mexico » vient de "Mecitl" ou "Mexi", qui était le nom d'un chef et prêtre qui guida les premiers Nahuas émigrés de la mythique Aztlan, qui étaient appelés "Mexicas", et par conséquent, ce mot signifierait « peuple de Mexi ».

De nombreux historiens, tels que Alfonso Caso, ont suggéré que Mexico viendrait des mots "metztli" (« Lune »), "xictli" (« nombril », « centre », « milieu », « fils »), et du suffixe locatif "-co". Par conséquent Mexico signifierait « lieu au milieu de la Lune » ou « lieu au centre du lac de la Lune », en faisant allusion au lac de Texcoco au milieu duquel a été construite la ville de Mexico. Cette version est fondée sur une légende aztèque qui raconte que lorsque les Mexicas arrivèrent pour la première fois au lac de Texcoco, ils y virent la Lune qui s'y reflétait.

Le nom de la ville fut translittéré en espagnol ("México") avec la valeur phonétique du x de l'espagnol médiéval, qui représentait la consonne fricative post-alvéolaire voisée //, représenté par un j, évolua vers la consonne fricative vélaire sourde /x/ durant le , qui conduisit à l'utilisation de la variante "Méjico" dans beaucoup de publications, en Espagne le plus souvent, tandis qu'au Mexique, "México" est restée la graphie préférée. Il y a quelques années, l’Académie royale espagnole, l'institution régulant la langue espagnole, statua que la graphie recommandée en espagnol serait "México", et la majorité des publications dans tous les pays hispanophones adhèrent aujourd'hui à la nouvelle norme, même si la variante désuète se rencontre parfois. En français, le x de Mexico et de Mexique ne représente ni le son originel ni le son actuel, mais la consonne //.

Le territoire a été découvert et habité par des groupes de chasseurs-cueilleurs nomades il y a environ 15000 ans.

Pendant plusieurs milliers d'années, les habitants de cette région d'Amérique pratiquèrent la chasse et la cueillette jusqu'à la découverte de l'agriculture. À Guilá Naquitz, ont été mis au jour les restes les plus anciens de la domestication de la courge et de la calebasse, qui datent du av. J.-C., mais l'agriculture s'est développée de façon précoce dans des sites comme la vallée de Tehuacán où la domestication du maïs a eu lieu aux alentours du av. J.-C.. Dès lors des groupes humains de cette région deviennent de plus en plus dépendants des produits agricoles, et ce jusqu'à l'apparition de hameaux agricoles et jusqu'à la dépendance totale qui a lieu durant la période classique. Tandis que l'agriculture prospère en Mésoamérique, les peuples au nord (Chichimèques) restent encore dépendants de la chasse et la cueillette.

L'histoire préhispanique de ce qui est actuellement le nord du Mexique est mal connue car les peuples qui occupèrent la région avaient une culture matérielle limitée. Ces peuples nomades qui habitaient les déserts, le littoral et les montagnes au nord de la Mésoamérique, ne partageaient pas la même culture. Le site de la grotte de la Perra (Tamaulipas) a connu l'invention de l'agriculture et connu la présence humaine à partir de . Il y a des traces de peuples nomades dans les sites tels que la grotte de la Candelaria (Coahuila, ) ou El Conchalito (Basse-Californie-du-Sud). On trouve également en Basse-Californie les peintures rupestres de la Sierra de San Francisco dont la fonction continue jusqu'au , lorsque les derniers indigènes disparaissent de la région.
Plusieurs auteurs prennent comme marqueur du début de la civilisation mésoaméricaine la controversée céramique Pox de Puerto Marqués, datée vers le Cette céramique mésoaméricaine pourrait avoir pour origine le contact entre la côte sud-américaine du Pacifique et l'occident de la Mésoamérique. Les nouvelles avancées techniques se diffusent dans toute la région si bien que, des siècles après, on produit une céramique dans d'autres hameaux du préclassique ancien (-) comme Chupícuaro et Tlatilco. Durant le préclassique moyen (ss. XIV-IV av. J.-C.), la culture olmèque se diffuse dans toute la Mésoamérique. Après le déclin olmèque, l'essor simultané de plusieurs peuples a lieu. Par exemple la culture des tombes à puits de probable influence sud-américaine, la culture épi-olmèque à Tres Zapotes, l'épanouissement d'Izapa et le développement du compte long.

À la fin de cette étape, Teotihuacán devient la ville la plus importante de la vallée de Mexico. Durant le Classique ancien (ss. II-VI/VIII), l'influence de Teotihuacán se fait sentir dans toute la Mésoamérique, appuyée par son pouvoir politique et commercial. Elle avait d'importants alliés, comme Monte Albán dans les vallées centrales d'Oaxaca. La civilisation mésoaméricaine s'étend plus au nord vers des sites comme La Quemada. En retour, des influences culturelles arrivent du nord, visibles dans la culture huastèque. La période classique est également l'époque de consolidation de la civilisation maya dans la péninsule du Yucatán et des hautes terres du Chiapas. D'un autre côté, dans les vallées et les montagnes du nord de la Sierra Madre occidentale, se développe la culture Paquimé, résultat de la consolidation de l'agriculture dans le nord-est et l'échange entre la Mésoamérique et l'Oasisamérique.

Entre les , le centre du Mexique est dominé par Tula, la capitale des Toltèques. La ville a établi des liens très forts avec plusieurs régions de Mésoamérique, mais particulièrement avec la péninsule du Yucatán, où se trouve la ville maya de Chichén Itzá. Au même moment, dans ce qui est actuellement l'état d'Oaxaca, les Mixtèques commencent un processus expansionniste qui les mène à occuper les vallées centrales où vivaient les Zapotèques. En 1325, les Mexicas fondent Mexico-Tenochtitlan, la capitale de l'État le plus vaste qu'a connu la Mésoamérique, qui rivalisait seul avec les Tarasques de Tzintzuntzan.

En 1519, les conquistadors, alliés à de nombreuses tribus ennemies des aztèques dont les Tlaxcaltèques et conduits par Hernán Cortés, se lancent à la conquête de l'Empire aztèque, aidés en cela par la supériorité et la qualité de leurs armes et de leurs tactiques de combat, mais aussi la supériorité numérique de leurs alliés indigènes. Le 13 août 1521, la fin du siège de Tenochtitlan signe la victoire des Espagnols et la fin de l'empire aztèque.

Cortés se lance alors dans la conquête d'un vaste empire colonial qui deviendra la Nouvelle-Espagne. Le territoire s'étendra jusqu'à une importante partie du sud des actuels États-Unis (notamment la Californie, l'Arizona, le Nouveau-Mexique et le Texas). Les principales villes mexicaines sont alors créées (Mexico (sur les ruines de Tenochtitlán), Guadalajara, Puebla et Monterrey.

En même temps que la colonisation espagnole, des missionnaires arrivent dans le pays pour évangéliser les populations indigènes qui avaient survécu à la conquête. Parmi ces évangélisateurs, Bartolomé de las Casas se distingue par son désir de protéger les populations indigènes.

Dès 1535, l’administration de la Nouvelle-Espagne est confiée à un vice-roi. Le premier sera Antonio de Mendoza, nommé par Charles Quint.

Pendant cette période, l’Espagne s'est enrichie grâce à la découverte et à l'exploitation des mines d'argent mexicaines, parmi les plus riches du monde, dont le produit transite via Anvers, première place financière mondiale, pour permettre d'importer des biens de l'Inde, où les marchands sont friands d'argent-métal. Les espagnols implantent aussi la culture de la canne à sucre et du café, alors que sur le plan humain, la population amérindienne chuta de 80 %, à cause principalement des épidémies et des travaux forcés. On estime qu'avant l'arrivée des Espagnols, le Mexique central comptait d'habitants. Il en restait un million vers 1650.

Les trois siècles de domination espagnole (1521 - 1821) coïncident avec la création du Mexique en tant que nation latine, hispanique, catholique et métisse telle que nous le connaissons aujourd’hui. L'architecture, la gastronomie, les fêtes mexicaines et la structure familiale sont encore aujourd'hui largement influencées par ces trois siècles de domination espagnole.

Après les très nombreuses destructions résultant de la colonisation du Mexique, une forme d'art colonial s'est développé à partir du ; et ce pour plusieurs raisons : contexte humaniste européen et développement des cabinets de curiosités, propagande religieuse, développement d'une élite métisse, explosion d'un commerce intercontinental, etc. Ce phénomène a en outre permis la conservation et la diffusion de nombreuses techniques précolombiennes uniques au monde, comme l'art de la laque mexicaine (technique de collage très différent de la laque asiatique), du papier d'amate ou celui de la mosaïque de plumes, d'une extraordinaire virtuosité au vu des moyens à la disposition des artisans précolombiens. De ces très nombreux ouvrages envoyés en Europe pour la délectation des princes et collectionneurs, très peu sont parvenus jusqu'à nous. Quatre tableaux de mosaïques de plumes sont aujourd'hui conservés en France, dont deux datant du : Le Triptyque de la crucifixion, conservé au Musée National de la Renaissance à Ecouen (Val d'Oise), et la Messe de Saint-Grégoire, conservée au Musée des Jacobins d'Auch (Gers).

Les populations indigènes ne furent pas entièrement soumises du fait de la chute de l'empire aztèque, d'autres ne firent que changer de maîtres, les talxcaltèques alliés des espagnols furent mieux traités et jouirent tout au long de la colonie de privilèges tels que pouvoir monter à cheval. Des nobles indigènes partirent pour l'Espagne où leurs descendants vivent toujours.
De très nombreuses révoltes locales eurent lieu durant les trois siècles de la période coloniale.

L'un de précurseurs de l'indépendance du Mexique est mort emprisonné dans la forteresse de San Juan de Ulúa en 1809. Il est l'auteur de textes où sont exposées les raisons qui devaient, selon lui, amener le pays à son émancipation de la couronne espagnole.

Dans la nuit du 15 au , depuis ce qui est aujourd'hui la ville de Dolores Hidalgo, dans l'État de Guanajuato, un Espagnol né au Mexique, le curé Miguel Hidalgo, aujourd’hui héros national, lève, au cri de (c'est-à-dire celui de Joseph Bonaparte, au pouvoir depuis l'invasion de l'Espagne par les Français), une armée hétéroclite et indisciplinée de villageois et d'indigènes pour le rétablissement de Ferdinand VII et contre les juntes espagnoles au service des Français. Il commence avec succès, mais échoue au Monte de las Cruces, dans sa tentative de prendre Mexico, et sera exécuté en 1811. 

Les (ne pas confondre avec l'acception française du terme créoles), descendants d'Européens, le plus souvent d'Espagnols, mais nés hors de la métropole espagnole au nombre d'un million en Nouvelle-Espagne devenue l'actuel Mexique, sont à la tête des métis et des mulâtres (qui ensemble sont ) et des indigènes () qui forment la majeure partie des six millions de la population d'alors, mais sont tenus à l'écart du pouvoir politique et économique, les fonctions les plus prestigieuses et lucratives étant réservées aux Espagnols dont le nombre n'était que de (peninsulares, nés dans la métropole, que les créoles nomment aussi ). 

Si le Grito de Dolores est à l'origine du processus d'indépendance du pays, il n'est cependant pas un appel à l'indépendance, mais bien une réaction à la destitution de Ferdinand VII par les Français.

Le premier acte d'indépendance est proclamé par le congrès de Chilpancingo, inspiré principalement par les écrits de José María Morelos y Pavón, et a été signé le . Il a été rédigé par Carlos María de Bustamante et Andrés Quintana Roo, et a été intitulé Acte solennel de la déclaration d'indépendance de l'Amérique septentrionale.

L’Acte de l’Indépendance de l'Empire mexicain sera finalement signé le .

L'Espagne ne reconnaîtra l'indépendance du Mexique que le 28 décembre 1836.

Parmi les éléments déclencheurs du mouvement indépendantiste, figurent la conquête et l’occupation française de l’Espagne, au début du , par les troupes de Napoléon et le rejet par les créoles de la Nouvelle-Espagne de la Constitution de Cadix jugée par eux anticléricale et trop libérale.

Avec l'indépendance, les Espagnols nés au Mexique purent devenir les maîtres du pays en accédant à toutes les fonctions auparavant réservées aux Espagnols nés en métropole qui furent expulsés en 1829, exception faite de ceux dont les capitaux étaient investis dans les mines et l'agriculture.

En 1821 l'empire est proclamé avec Agustín de Iturbide. Le , le Mexique se dote d’une Constitution républicaine ; la République est née.

Les troupes espagnoles débarquent près de Tampico en mars 1829, dans une ultime tentative de reconquête du pays, et sont repoussées par les troupes du général Antonio López de Santa Anna. Celui-ci acquiert un immense prestige par sa victoire, et devient le « Héros de Tampico ».

En raison des dégâts causés lors des troubles publics liés au chaos de la situation politique dans les années qui suivirent l'indépendance, des commerçants français déposèrent des réclamations au baron Deffaudis, ambassadeur français à Mexico ; parmi eux, un pâtissier du nom de Remontel réclama la somme exorbitante de en dédommagement du préjudice causé par des officiers à son établissement de Tacubaya (selon les sources, ayant profité d'émeutes pour partir sans payer leurs pâtisseries en 1832, d'où le surnom ironique donné ensuite au conflit par les Mexicains, ou ayant occasionné des dégâts à sa boutique en 1828). En 1837, le ministre mexicain des affaires extérieures, Luis G. Cuevas, répondit que le gouvernement n’était pas dans l'obligation d'indemniser ces pertes, étant donné qu'elles étaient la conséquence d'un mouvement révolutionnaire. Le 6 février 1838 (ou le 21 mars, selon d'autres sources), une flotte de 26 navires de guerre français arriva au large de Veracruz et le gouvernement de Louis-Philippe réclama une somme totale de , équivalant à l'époque à de francs or en réparation des pertes subies par ses sujets. Le 27 novembre, les Français bombardèrent la forteresse de San Juan de Ulua.

Les Français obtinrent des garanties quant au paiement de cette somme et se retirèrent après onze mois de blocus du port de Veracruz. Cela occasionna pour le trésor mexicain une perte, calculée par le "Journal des Débats", de soit de francs or.

En 1836, le Texas proclame son indépendance du Mexique. Il sera annexé plus tard par les États-Unis. En 1846, le Mexique revendique le territoire compris entre le rio Bravo et le rio Nueces. En effet, la limite de la province texane était le rio Nueces situé à au nord du rio Bravo. Dès lors la guerre éclate entre le Mexique et les États-Unis et durera de 1846 à 1848.

Les troupes américaines envahissent le pays et l’occupent de 1847 à 1848. Après la bataille de Chapultepec, le 14 septembre 1847, les troupes américaines hissent le drapeau américain sur le Palais National : la ville de Mexico est occupée. Sous le contrôle de Winfield Scott, ses troupes exécutent de nombreux soldats d'origine irlandaise du bataillon Saint Patrick, déserteurs de l’US Army, qui collaboraient avec la résistance mexicaine face à l’occupant.

La guerre se termine par la signature en 1848 du traité de Guadeloupe Hidalgo par lequel le Mexique reconnaît le rio Bravo comme sa frontière avec le Texas. De plus, le Mexique cède plus de 40 % de son territoire aux États-Unis, soit près de . Les États de Californie, Nouveau-Mexique, Arizona, Nevada, Utah, la majeure partie du Colorado et le sud-ouest du Wyoming représentent les territoires que les États-Unis ont annexés à la suite de la guerre américano-mexicaine. En 1857, est promulguée la constitution qui règle les institutions politiques mexicaines jusqu'en 1917.

En 1861, le gouvernement de Juárez décide la suspension du paiement de sa dette extérieure. La France, l’un des créanciers du Mexique, invoque le motif des dettes pour y intervenir militairement avec l’appui de l'ancienne puissance coloniale l’Espagne et de l’Angleterre. Profitant de la guerre civile qui déchire et absorbe les ressources du voisin du Nord, Napoléon III, avec la bénédiction du pape, pensait établir au Mexique un empire « latin » et catholique qui contrebalancerait le pouvoir grandissant des Américains. Des forces maritimes de ces trois pays débarquent à Veracruz, les Espagnols en décembre 1861, les Anglais et les Français en janvier 1862. Après des négociations, le gouvernement mexicain arrive à obtenir des Anglais et des Espagnols leur retrait (Convention de la Soledad). La France continue donc seule cette expédition visant à établir un empire catholique et ami au Mexique.

Hormis la première bataille de Puebla, gagnée par les forces libérales sous le commandement d’Ignacio Zaragoza, la campagne militaire française est un succès. La Légion étrangère s'y illustra lors du combat du 30 avril 1863 non loin du Cerro del Chiquihuite, à Camarón, rebaptisée plus tard Villa Tejeda (dite Camerone en français). Devant l’avancée des forces ennemies appuyées par les conservateurs, le gouvernement de Juárez est contraint de s'éloigner à San Luis Potosí le 31 mai 1863 puis finalement à Paso del Norte (devenue depuis Ciudad Juárez) près de la frontière avec les États-Unis. En juin 1863, Mexico tombe sous le contrôle des forces de Napoléon III et de celles des conservateurs mexicains. Le 10 juillet, une Assemblée des Notables à Mexico nomme Maximilien d’Autriche empereur. Il était un des frères de François-Joseph, empereur d'Autriche. Prince bien intentionné, il déçut souvent les conservateurs par ses idées modernes et libérales, allant jusqu'à demander à Juárez de gouverner avec lui, mais cet Habsbourg imbu d'étiquette commit des maladresses irréparables qui hâtèrent sa chute. Le pays resta peu sûr pour l'envahisseur, une guérilla féroce ne lui laissa aucun repos et épuisa ses forces et son moral, d'autre part les bandits pullulèrent, ce qui ne fit qu'aggraver la situation.

Dès la fin de la guerre de sécession en 1865, Juárez trouve auprès des États-Unis, en échange de promesses de concessions sur le territoire mexicain (isthme de Tehuantepec), un soutien en armes et en hommes, ainsi que diplomatique (doctrine de Monroe). Ce nouvel appui, les succès militaires des républicains, et surtout les menaces de guerre en Europe, forcèrent les troupes françaises à se retirer.
L'intervention au Mexique fut un grand échec pour Napoléon III. Le second empire mexicain durera jusqu’en 1867. L’empereur Maximilien est exécuté à Santiago de Querétaro. Durant toute cette période, Benito Juárez n'abandonna jamais le territoire national et continua d'exercer sa fonction de président de la République.

Héros de la guerre contre les Français, Porfirio Díaz devient président du Mexique en 1876. Sa présidence dure jusqu'en 1911. Il travaille pour la paix, le progrès et l'ouverture du pays aux investisseurs étrangers, par exemple pour dans le domaine minier, en accueillant la Compagnie du Boléo, dont l'usine de traitement du cuivre en plein désert emploie jusqu'à 5000 personnes. Malgré toute cette croissance, la politique économique de Díaz ne bénéficie pas a toute la population
Officiellement Díaz est réélu à chaque élection, mais les dysfonctionnements du vote et le mécontentement de la population en général —les paysans dépossédés de leurs terres, la classe moyenne instruite et désireuse d'accéder au pouvoir et la baisse des salaires réels — sont parmi les éléments déclencheurs de la Révolution Mexicaine.

Madero reprendra habilement le vieux slogan de Díaz, , pour sa campagne politique.

Díaz est l'auteur de la phrase (1878).

Porfirio Díaz, au pouvoir depuis une trentaine d'années, voulait se présenter à l’élection présidentielle de 1910 de même que Francisco Madero. Díaz fit emprisonner Madero puis le relâcha. Díaz sortit victorieux des élections. Madero ne recueillit que quelques centaines de voix à travers tout le pays.

En mai 1911, après la prise de Ciudad Juárez, par les troupes d'un ancien bandit Francisco Villa que Madero avait recruté en échange du pardon de ses crimes et d'un grade de colonel dans l'armée fédérale en cas de victoire, Díaz, qui voulait éviter une guerre civile préféra partir en exil en France.
La révolution dégénéra alors en une lutte pour le pouvoir entre révolutionnaires. Le président Madero (révolutionnaire) fut assassiné par Victoriano Huerta (réactionnaire) lui-même chassé par les troupes de Francisco Villa. Zapata fut assassiné en 1919, Venustiano Carranza, l'auteur intellectuel de l'assassinat de Zapata, en 1920, et Francisco Villa en 1923, sur ordre d'Alvaro Obregón.

La révolution se terminera officiellement en 1917, date de la nouvelle constitution mexicaine, mais la violence dura jusqu’aux années 1930 (assassinat d'Alvaro Obregón par un fanatique catholique en 1928). Une autre vague de violence suit l'application des mesures de laïcisation contenues dans la Constitution de 1917 et appliquées par le gouvernement dès 1926 : c'est la guerre des Cristeros.

À la mort d'Obregon, Plutarco Elías Calles devient le "Jefe maximo de la Revolución". En mars 1929, il fonde le Partido Nacional Revolucionario dans le but de contrôler et de surveiller les divers courants politiques et se nomme lui-même à la tête de ce parti. Dans le but d'éviter des conflits entre généraux, il fait nommer président de la République un civil Emilio Portes Gil pour la période de 1928 à 1930. Calles dut lutter contre une conjuration de militaires obregonistes menée par José Gonzalo Escobar écartés du pouvoir nommée « plan de Hermosillo ».

Les années 1930 furent marquées par la présidence de Cárdenas de 1934 à 1940 titulaire du prix Lénine pour la paix qui se proposait de faire du Mexique un pays socialiste et par des nationalisations, l'institution d'un plan sexennal imité de l'URSS, puis l'expropriation pétrolière en 1938, Cárdenas profitant de la baisse du prix du pétrole et de difficultés économiques des compagnies pétrolières étrangères en majorité anglo-néerlandaises et américaines alors au bord de la faillite. Staline et les communistes mexicains dirent alors que les principaux bénéficiaires de cette nationalisation seront les États-Unis car de compétiteur en matière de production le secteur pétrolier commença à dépendre de la technologie et des financements américains, il existe à la bibliothèque du Congrès des États-Unis des preuves de l'appui financier de Roosevelt à celui de Cárdenas.

À la suite du torpillage de navires mexicains par des sous-marins allemands, dont les pétroliers ' et ' en mai 1942, le gouvernement du général Manuel Avila Camacho déclara la guerre le 28 mai 1942 à l'Allemagne, à l'Italie et au Japon.

L'escadrille mexicaine , composée d'avions P-47, participa à la guerre contre le Japon et fut envoyée aux Philippines.

Des Mexicains participèrent aussi au débarquement du 6 juin 1944. L'un des plus connus d'entre eux est le pilote de chasse abattu le 19 juin 1944. Il repose au cimetière du village de Sassy.

D'autres participèrent sous l'uniforme américain à la bataille des Ardennes. Parmi eux, le sergent qui reçut les plus hautes distinctions militaires des États-Unis, la Medal of Honor et le Purple Heart, pour ses faits d'armes dont la neutralisation à lui seul lors d'un combat de plus de cent soldats ennemis.
Le Parti révolutionnaire institutionnel (PRI), membre de l'internationale socialiste prit son nom actuel en 1946 et dirigea le pays sans interruption jusqu’en 2000, date de la victoire au démocrate centriste Vicente Fox Quesada, candidat du PAN (voir la Liste des présidents du Mexique).
En 2006, Felipe Calderón (PAN) est le président du Mexique après avoir recueilli 35,88 % des suffrages à l’élection présidentielle du 2 juillet 2006 contre 35,31 % pour Andrés Manuel López Obrador (PRD) et 22,27 % pour Roberto Madrazo (PRI). Les résultats sont contestés mais le 6 septembre, le TEPJF a donné sa décision (sans appel) et a confirmé la victoire de Felipe Calderón qui a pris officiellement ses fonctions le décembre 2006.

En juillet 2012, le PRI revient au pouvoir avec la victoire d'Enrique Peña Nieto aux élections présidentielles. Avec près de 38 % des suffrages, il devance le candidat du PRD Andrés Manuel López Obrador (31 %), ainsi que la démocrate centriste Josefina Vazquez Mota du Parti d'action nationale (PAN ; près de 25 %).

Le Mexique est une république fédérale composée de 32 États. La séparation des trois pouvoirs (exécutif, législatif et judiciaire) est garantie par la Constitution de 1917.

Le chef de l’exécutif est le président de la République, élu pour une période de 6 ans, non renouvelable, au suffrage universel direct à un seul tour et à la majorité relative. Il n’y a pas de Premier ministre. Le président nomme et révoque les ministres, le procureur général, les ambassadeurs et les consuls généraux. En cas de démission ou de décès, le Congrès désigne un président intérimaire. Le président peut émettre des décrets dans le domaine économique et financier grâce aux pouvoirs que lui délègue le Congrès 

Depuis décembre 2012, le président de la république mexicaine en fonction est Enrique Peña Nieto.

Le Congrès est divisé en deux chambres :


En effet, le PAN était devenu la première force politique à la Chambre des députés avec 207 sièges, suivi par le PRD avec 160 sièges, et en par le PRI avec seulement 119 sièges. Néanmoins, le PAN sans majorité absolue à la Chambre des députés a dû obtenir l’appui de l’opposition pour faire passer ses lois.

Les élections de juillet 2009 ont vu le retour du PRI, qui en obtenant 237 sièges redevient la première force parlementaire du pays, le parti présidentiel PAN recule avec 143 députés, ainsi que le PRD qui ne conserve que 71 sièges.

Depuis 1997, le Congrès joue un plus grand rôle puisque l’opposition obtint plus de sièges grâce à la désignation de 200 sièges de députés élus à la proportionnelle.

Constitutionnellement le Mexique est composé de 32 entités fédératives ayant toutes leur propre constitution dont 31 sont des états, l'ex District fédéral devenu Ciudad de México est la entité fédérative du pays, sans avoir le statut d'État.

Le Mexique est un pays situé en Amérique du Nord. Il partage des frontières terrestres avec les États-Unis () au nord et avec le Belize () et le Guatemala () au sud.

Il possède de nombreuses façades maritimes () notamment avec l’océan Pacifique et le golfe de Californie () à l’Ouest et avec la mer des Caraïbes et le golfe du Mexique () à l’Est.

La superficie totale du pays est de en incluant d’îles ; les îles mexicaines se situent dans l’océan Pacifique (dont la plus grande est l'Île Cedros), le golfe de Californie (dont les plus grandes sont les îles Tiburón et Ángel de la Guarda), la mer des Caraïbes (dont la plus grande est Cozumel) et le golfe du Mexique.

La superficie maritime totale du Mexique est de ( dans l'océan Pacifique et dans le golfe du Mexique et la mer des Caraïbes). Elle se subdivise en une mer territoriale, qui s'étend sur 12 milles marins () autour des côtes, une zone contiguë, qui s'étend sur 24 milles marins autour des côtes () et une zone économique exclusive (ZEE) qui s’étend sur 200 milles marins () autour des côtes.

On trouve aussi de nombreux volcans. Le pic de Orizaba culmine à , tandis que le point le moins élevé est la Laguna Salada qui se trouve à en dessous du niveau de la mer. Le pays est sujet aux tremblements de terre, parfois très violents.

Parmi les ressources naturelles, on trouve l'argent, le cuivre, le gaz naturel, l’or, le pétrole, le plomb et le zinc.

Le Mexique est traversé par deux principales chaînes de montagne : La Sierra Madre occidentale et la Sierra Madre orientale. La Sierra Madre occidentale à l’ouest est le prolongement de la Sierra Nevada de Californie et la Sierra Madre orientale à l’est est le prolongement des Montagnes Rocheuses du Nouveau-Mexique et du Texas. Entre les deux principales chaînes de montagnes se trouve le plateau mexicain. La cordillère néovolcanique marque la limite sud des Sierra Madres occidentale et orientale. Le Mexique compte également d’autres chaînes de montagne moins importantes comme la chaîne californienne, la Sierra Madre del Sur, la Sierra Madre de Oaxaca, la Sierra Madre de Chiapas, et la Meseta Central de Chiapas.

Le point culminant du pays est le Pic d'Orizaba, qui se dresse à .

Principales chaînes de montagne et plateau principal :


Le Mexique possède peu de cours d’eau navigables.

Le Río Grande est appelé « Río Bravo del Norte » par les Mexicains.

Le Tropique du Cancer divise le pays en deux zones, l'une tempérée (climat subtropical humide) et l'autre au climat tropical. Le climat varie avec l’altitude. Les "tierras calientes" (terres chaudes), comprenant les plaines côtières, s’élevant jusqu’à environ . Au nord du , les températures sont plus froides pendant les mois d’hiver, tandis qu’au sud, elles restent constantes le long de l’année. Elles varient néanmoins en fonction de l’altitude.

Les zones au sud du :

Les pluies varient beaucoup selon la situation géographique et les saisons. Aride ou semi-aride en Basse Californie, le Nord-Ouest de l’État de Sonora, les plateaux du Nord et une partie des plateaux du Sud. Il pleut dans ces régions en moyenne entre 300 et par an. Dans les plateaux du Sud et notamment les régions les plus peuplées (comme Mexico et Guadalajara) il pleut en moyenne entre 600 et . Les basses terres le long du golfe du Mexique reçoivent plus de de pluies à l’année. La région au sud-est de Tabasco reçoit approximativement de pluies à l’année. Il neige occasionnellement sur certains des plateaux du nord et des hauts sommets de la Sierra Madre Occidentale et de la Sierra Madre Orientale.

Saison humide ou saison des pluies :

Le Mexique connaît une saison humide (ou saison des pluies) et une saison sèche marquées. La saison des pluies dure, dans la majeure partie du pays, de juin à mi-octobre. Il pleut nettement moins le reste de l’année. Février et juillet sont respectivement le mois le plus sec et le plus humide. Par exemple, la ville de Mexico reçoit environ de pluies en février et en juillet. Les régions côtières, et spécialement celle du golfe du Mexique reçoivent leurs précipitations maximales en septembre. Tabasco enregistre plus de de pluies pendant ce mois.

Une petite partie de la côte nord-ouest du Mexique autour de la ville de Tijuana possède un climat méditerranéen avec des brumes importantes et une saison des pluies en hiver.

Ouragans :

Le Mexique est situé dans la ceinture des ouragans et toutes les régions côtières sont susceptibles de subir une de ces tempêtes de juin à novembre. Les ouragans de la côte Pacifique sont moins fréquents et souvent moins violents que ceux qui affectent la côte est du pays. Plusieurs ouragans frappent chaque année les côtes du golfe du Mexique et de la mer des Caraïbes, avec des vents violents qui peuvent dépasser les , mettent en péril la vie des habitants et provoquent des dégâts importants aux hôtels et habitations de la région.

Le Mexique est un des 17 pays mégadivers identifiés en juillet 2000 par le programme des Nations unies pour l'environnement. Avec espèces différentes, le Mexique héberge 10 à 12 % de la biodiversité mondiale.

Le Mexique est le premier pays en nombre d'espèces de reptiles avec 707 espèces connues, second en nombre d'espèces de mammifères avec 438 espèces, le quatrième en nombre d'espèces d'amphibiens avec 290 espèces et quatrième en nombre d'espèces de plantes. Ce pays compte quelque espèces d'oiseaux, dont 101 endémiques. Le Mexique est également considéré comme le second pays en écosystèmes et le quatrième en nombre total d'espèces. Près de espèces sont protégées par la législation mexicaine. Le gouvernement mexicain a créé le Sistema Nacional de Información acerca de la Biodiversidad, qui se charge d'étudier et de promouvoir l'utilisation substantiel des écosystèmes.

Au Mexique, sont considérés comme des zones naturelles protégées. Trente-quatre réserves de biosphère (écosystèmes inaltérés), soixante-quatre parcs nationaux, quatre monuments naturels, vingt-six aires pour protéger la flore et la faune, quatre zones pour la protection naturelle et dix-sept sanctuaires (zone comportant une diversité riche en espèces).

La biodiversité est cependant menacée au Mexique à cause de la déforestation, en particulier dans les forêts tropicales humides.

"Source : OCDE" - Liste des pays par PIB



Même si le pétrole ne représente aujourd’hui qu’une partie des exportations mexicaines, les ressources financières dégagées par Pemex financent 30 % du budget de l’État. Cette situation a permis aux Mexicains de bénéficier d’une certaine clémence fiscale. En effet, le Mexique est le pays de l’OCDE et de toute l’Amérique latine dont le ratio recette fiscale / PIB est le plus faible (entre 15 et 17 % contre une moyenne de 30 % pour les pays de l’OCDE).

Le secteur primaire représente 4 % du PIB et emploie 18 % de la population active.




L’industrie représente 26,5 % du PIB (2004) et emploie 24 % de la population active.





Le secteur tertiaire représente 69,5 % du PIB et emploie 58 % de la population active.

En janvier 1994, le Mexique, le Canada et les États-Unis signent l’Accord de libre échange d’Amérique du Nord ALENA créant ainsi la plus vaste zone de libre-échange du monde. L’ALENA a fortement transformé le Mexique qui passa d’une politique économique marquée par son fort protectionnisme à une politique économique basée sur le libre-échange et l’insertion dans l’économie mondiale. L’année même de la mise en application de l’ALENA, le Mexique connut une grave crise économique marquée par une forte dévaluation du peso. Les raisons de cette crises sont multiples, adaptation imposée du tissu économique à ce nouvel environnement économique, politique monétaire.

En 2008 la dette extérieure ne représentait plus que 8 % du PIB contre 50 % en 1993. Le pourcentage de la dette publique en dollars a baissé de 95 % à 63 % en 2008.

Depuis 1994, l'économie mexicaine s’est remise de la crise économique. Les exportations ont connu une croissance très importante, notamment en direction des États-Unis et du Canada. Les "maquiladoras" ou zones franches sont un des éléments importants de ce succès. Aujourd’hui, le Mexique représente 50 % des importations et exportations d’Amérique latine et est devenu la commerciale du monde. Le PIB mexicain en valeur est le plus élevé d’Amérique latine, devant le Brésil et l’Argentine et la économique selon ce même critère. Les cinq principaux pays investisseurs au Mexique sont par ordre décroissant les États-Unis, l'Espagne, le Canada, les Pays-Bas et la Suisse.

En 2000, le Mexique connut sa première alternance politique depuis plus de avec l’arrivée au pouvoir de Vicente Fox. Ce dernier continua la politique économique de ses prédécesseurs avec une politique budgétaire et monétaire rigoureuse. L’inflation a fortement baissé et les finances publiques ont été fortement améliorées notamment grâce à la hausse du prix du pétrole dont le Mexique est le exportateur mondial. La dette publique ne représente plus aujourd’hui que 23,5 % du PIB et la dette extérieure mexicaine a été classée par Standard & Poor’s BBB soit le niveau le plus haut jamais atteint par le Mexique et la meilleure notation des grandes économies d’Amérique latine.

Afin de diversifier les débouchés des exportations mexicaines (dont plus de 80 % sont faites avec les États-Unis et le Canada), le Mexique a signé un grand nombre de traités de libre-échange, notamment avec l’Union européenne, le Japon, Israël. Il existe aussi un traité de libre-échange avec les pays de l'AELE entré en vigueur en 2001.

Entre 2001 et 2003, le Mexique connut une croissance économique médiocre (-0,3 % en 2001, +0,9 % en 2002 et +1,4 % en 2003). En effet, la Chine est devenue un concurrent important du Mexique, le salaire dans les ateliers chinois étant en moyenne quatre fois moins élevé qu’au Mexique. Le Mexique doit donc adapter son modèle économique à cette nouvelle situation internationale, notamment à travers des réformes structurelles qui se font très lentement.

Depuis 2004, la croissance économique s’est fortement accélérée : +4,3 % en 2004 et +3,8 % en 2005 (estimations), de nombreuses entreprises revenant au Mexique après être parties en Asie. Néanmoins, pour que cette reprise puisse être durable et que le Mexique puisse remplir les objectifs du millénaire dans la lutte contre la pauvreté qui touche encore 40 % de la population, d’importantes réformes structurelles doivent être entreprises.

Las remesas, ces remises ou transferts de fonds de la part des émigrés mexicains pour leurs familles qui sont restées au Mexique ont représenté en 2005 un record de plus de de dollars. Cela est l’équivalent de la moitié de la valeur des exportations pétrolières du pays, qui représentent à leur tour moins de 10 % des exportations totales de biens, au contraire des décennies précédentes où les exportations pétrolières prévalaient dans la balance courante. Cette formidable manne est supérieure aux investissements étrangers au Mexique et permet d’améliorer la situation économique de nombreuses familles rurales.

Il reste d’importants défis que le Mexique doit surmonter :

Pendant tout le , la population du Mexique a seulement doublé. Cette tendance continuera pendant les deux premières décennies du . En 1920, on assiste à une perte de deux millions d’habitants qui peut s’expliquer par la Révolution mexicaine entre 1910 et 1920.

Le taux de croissance de la population s’est fortement accéléré entre 1930 et 1980, avec des chiffres supérieurs à 3 %. La population mexicaine doublait tous les vingt ans et à ce rythme on estimait que le Mexique compterait 120 millions d'habitants en 2000. Le gouvernement fédéral créa alors le Conseil national de la population, CONAPO, avec pour mission d’établir des politiques de contrôle de la natalité et réaliser des études sur la population du pays. Ces mesures furent positives et le taux de croissance de la population baissa jusqu’à 1,6 % sur la période 1995 et 2000. Les projections de la CONAPO évaluent la population mexicaine, à la mi-2014, à habitants.

L'espérance de vie est passée de , en 1895, à en 2005. On estime donc que le Mexique est rentré dans la dernière phase de transition démographique. En effet, le taux de fécondité n’est plus que de par femme, et la mortalité infantile est de pour .

Taux d'excédent naturel total de la population (chiffres 2005) :

Le solde migratoire est traditionnellement négatif et s’élève à plus de par an. Les États-Unis restent la première destination.

Même si aujourd’hui le Mexique a une population jeune (seulement 5,6 % de la population a plus de 65 ans), le vieillissement de la population a commencé et s’accélèrera dans les prochaines années.

Au début du , près de 90 % de la population vivait dans les zones rurales. Lors du recensement de 1960 la population urbaine devient majoritaire pour la première fois avec 50,6 % de la population mexicaine vivant dans les villes et grandes agglomérations. Le nombre de personnes qui habitait dans leur État natal était en 1895 de 96,6 % alors qu’en 1950 plus de 80 % des Mexicains habitaient dans un autre État que celui où ils sont nés.
À travers ces chiffres on peut se rendre compte du phénomène de développement industriel des moyennes et grandes agglomérations mexicaines et l’exode rural qui y est lié. Aujourd’hui les Mexicains continuent à être très mobiles à l’intérieur du pays notamment entre les différentes agglomérations. Néanmoins, on peut considérer que l’exode rural massif des décennies précédentes fait partie du passé.

Les entités fédératives qui concentrent la plus grande partie de la population mexicaine sont Mexico, l'État de Mexico, Jalisco, Nuevo Leon, Puebla et Veracruz. 

L'aire urbaine de Mexico, avec plus de d'habitants, se classe deuxième au rang mondial fin 2012, après celle de Tokyo ( d'habitants) et devant Séoul ( d'habitants). Guadalajara et Monterrey sont respectivement les deuxième et troisième grandes villes du pays avec chacune plus de trois millions d’habitants.

Le Mexique, avec environ d'habitants en 2017, est le pays hispanophone le plus peuplé, largement devant l'Espagne, et le troisième pays le plus peuplé du continent américain après les États-Unis et le Brésil. Au niveau mondial c’est le onzième pays le plus peuplé après la Chine, l'Inde, les États-Unis, l'Indonésie, le Brésil, le Pakistan, le Bangladesh, la Russie, le Nigeria, et le Japon.

La population qui parle les langues indigènes (unique critère retenu par l’INEGI pour designer la population indigène) passa de 17 % en 1895 à seulement 7 % en 2000. Néanmoins en nombre absolu elle a cru en passant d'un million en 1895 à sept millions en 2000. Les spécialistes s'accordent pour dire qu’il y a plutôt d’indigènes qui parlent ou non une langue indigène au Mexique. Jusqu'en 1980, les populations indigènes émigraient en direction des métropoles régionales proches de leur lieu de naissance, mais, à partir des années 1990, l'émigration indigène se fit massivement en direction des États-Unis. Les salaires plus élevés aux États-Unis alimentaient inlassablement le flux de l'émigration. Les États-Unis ont entrepris de renforcer leur frontière avec le Mexique et des murs sur la frontière ont été installés en différents endroits à partir de 1996.

Les États-Unis sont le pays où vivent le plus de Mexicains après le Mexique. Il se dit que Los Angeles, la plus grande ville de Californie est aussi la deuxième ville mexicaine pour ce qui est de la population car le nombre d'immigrés et de descendants de Mexicains dépasse largement les quatre millions de personnes qui vivent à Guadalajara, seconde métropole mexicaine. La présence des Mexicains de l’autre côté du Río Grande commence lors de l’annexion par les États-Unis d’immenses territoires mexicains. Ainsi un certain nombre de Mexicains se trouvèrent "de facto" en territoire américain mais gardèrent leurs coutumes et leur langue. L’État du Nouveau-Mexique illustre bien cela. À ce nombre, il faut ajouter le nombre important de "braceros" qui partirent vivre aux États-Unis, parfois temporairement grâce à un accord laboral entre les gouvernements de Washington et de Mexico. Les dernières crises économiques du Mexique ont favorisé l’émigration vers le nord et on estime qu’au début du près de de Mexicains ou descendants de Mexicains vivent aux États-Unis. La grande partie de ceux-ci se situent en Californie, au Texas et au Nouveau-Mexique. On compte aussi de nombreux citoyens mexicains dans l'Union européenne, surtout en Espagne et en Allemagne. La Suisse compte de nombreux binationaux qui occupent souvent des postes de haute qualification professionnelle.

Aujourd’hui la grande majorité des indigènes sont bilingues (12 % des hommes et près de 21 % des femmes ne parlant pas espagnol en 2005).

En 1970, le Mexique fut le deuxième pays au monde (après l'Australie) à mettre en place un système d’enseignement à distance. Les écoles qui utilisent ce système sont appelées télécollèges. La diffusion de ce système s’étend aussi à certains pays d’Amérique centrale, à la Colombie et même à certains États du Sud des États-Unis.

Les trois universités publiques mexicaines les plus connues sont l’université nationale autonome du Mexique (UNAM) fondée en 1551, l'université autonome métropolitaine (UAM) et l’Institut polytechnique national (IPN) qui ont un grand prestige dans toute l’Amérique latine. Les quatre principales universités privées de reconnaissance internationale sont l’Institut technologique d’études supérieures de Monterrey (ITESM) qui est souvent désigné comme le "TEC de Monterrey", l’Institut technologique autonome de Mexico (ITAM), l’université Anáhuac (ANAHUAC) et son réseau d'universités affiliées (Espagne, Italie, et Chili) et l'université ibéro-américaine. Ces universités ont connu une croissance importante et ont su nouer des partenariats avec des universités étrangères les plus prestigieuses.

Les effectifs totaux des forces armées sont estimés en 2008 à hommes et femmes :

Les forces armées dépendent du Secrétaire de la défense nationale pour les armées de terre et de l'air, La marine dépendant elle du Secrétaire de la marine.

Le président de la République en est le chef suprême.

Le Mexique compte parmi les pays ayant un taux d'homicides volontaires les plus élevés du monde. En 2017, le pays est considéré comme le deuxième pays le plus meurtrier au monde par l'International Institute for Strategic Studies (IISS) avec commis pendant l'année 2016. Le kidnapping n'a cessé d'augmenter depuis les années 1980 ( enlèvements recensés par le gouvernement en 2013). Chaque jour, Mexicaines sont agressées et sept sont assassinées.

La lutte contre les activités des narcotrafiquants constitue une préoccupation majeure au Mexique. Le précédent président, Felipe Calderón, avait décidé d'engager les forces militaires dans le combat contre les cartels de la drogue et a défini le combat contre ces gangs comme l'une des principales priorités de son administration. Cependant, sur ce point, le bilan de Calderón a été mitigé. Au cours des cinq dernières années (2007-2011), les violences liées aux narco-trafiquants ont fait plus de au Mexique, notamment dans les villes du nord du pays. L'Institut national de statistiques et géographie avance des chiffres bien plus élevés en 2012 : homicides ont été enregistrés en 2011 et pour les années 2007-2011, le total s'élèverait à assassinats. La politique menée par l'ancien président Calderón - qui consistait à attaquer frontalement les bandes criminelles - n'ayant pas été concluante, l'arrivée au pouvoir de son successeur, Enrique Peña Nieto, a été marquée par une nouvelle orientation stratégique de la sécurité intérieure.

En 2013, sous Peña Nieto, le Mexique a enregistré une baisse de 17 % du nombre d'assassinats. Entre décembre 2012 et avril 2013, les homicides ont baissé de 18 %, ce qui représente morts en moins sur cette période. Les six premiers mois du mandat du président Peña Nieto ont ainsi été marqués par une baisse de près de 20 % des décès liés au crime organisé. Le 22 août 2014, le président met sur pied une nouvelle gendarmerie nationale dont les missions sont principalement axées sur la répression des bandes criminelles. Peña Nieto s'est aussi illustré par un fait divers d'envergure : l'arrestation en 2013 de Miguel Treviño, le chef des Zetas, le plus puissant gang du Mexique. La ville de Ciudad Juarez, après être devenue la capitale mondiale du crime, a enregistré en 2011 une baisse de près de 60 % de son nombre d'homicides. Malgré ces efforts, le Mexique a connu une augmentation de 11 % des homicides entre 2015 et 2016 d'après l'IISS.

Certains experts estiment que . Selon Ricardo Ravelo, les parrains mafieux et les grands barons contrôleraient plus de 70 % des du pays. En réalité, s'il est vrai que certains cartels disposent d'une influence sur le pouvoir politique, les chiffres révèlent que la situation n'est pas aussi simpliste. Les chiffres stipulent que l’activité mafieuse ne pèse pas très lourd sur le dynamisme économique du Mexique, activité qui coûterait à peine 2 % de son PIB au pays. Le plus grand pays hispanophone de la planète dispose en effet de bien d'autres atouts pour être dépendant des simples revenus du trafic. L'économie du Mexique est la quatorzième plus importante de la planète : le pays est premier producteur mondial d’argent, le septième producteur mondial de pétrole, le quatrième producteur de gaz, le dixième producteur d’or et est classé parmi les plus grands producteurs mondiaux de l'alimentaire (café, sucre, maïs, etc.).

La Loi des Droits Linguistiques de 2001 concède le statut de langue nationale à l’espagnol et à plus de soixante langues indigènes parlées par 7 % de la population. Néanmoins, même s'il n’existe pas de déclaration constitutionnelle qui fasse de l’espagnol la langue officielle, c’est celle-ci qui est utilisée pour tous les documents officiels et est parlée par la quasi-totalité des Mexicains. Les langues indigènes sont parlées par plus de 6 millions de personnes vivant du sud-est du Mexique jusqu’au Honduras. Leurs origines remontent à plus de cinq millénaires. De l’époque dite classique (ca. 300–800 ap. J.-C.) à la conquête espagnole, certaines de ces langues (en particulier le maya classique oriental et le nahuatl) furent écrites sur des bâtiments, de la poterie et des codex, grâce à un système d’écriture hiéroglyphique.

Les deux langues indigènes qui sont les plus parlées sont :

Les langues indigènes ont eu une grande importance tout au long de l’histoire et la culture mexicaine. Ainsi le nom du pays trouve-t-il son origine dans la langue nahuatl. De nombreux mots espagnols sont d’origine amérindienne, par exemple :
sans compter les nombreux produits de l’échange colombien.

De fortes communautés anglophones représentent 50 % de la population de villes telles que San Miguel de Allende, Chapala et Taxco. En y ajoutant celles de Basse-Californie on arrive au chiffre de .

Chipilo, une ville de l'État de Puebla, est peuplée de descendants de vénitiens, y parlent toujours le vénitien.

Les mennonites des États de Chihuahua, Zacatecas, et Durango, parlent encore le bas saxon ils sont si l'on y ajoute ceux des communautés de Tamaulipas et de Campeche.

Le Mexique est un pays laïque. Les Mexicains sont très majoritairement catholiques mais beaucoup de protestants sont comptés comme catholiques, car ils sont souvent persécutés.
Le syncrétisme entre les traditions religieuses européennes et préhispaniques indigènes (et, dans une moindre mesure, africaines) y est fréquent, surtout dans les populations rurales. Il se manifeste notamment dans le culte très populaire de Notre-Dame de Guadalupe (qui est le plus répandu au Mexique), celui de la Santa Muerte, les traditions du jour des morts, la santería (qui n'est pas traditionnelle au Mexique mais d'introduction récente par des émigrés cubains) et dans les rituels de nombreux groupes d'origine indigène.

La politique anticléricale du pays a pris fin en 1991 avec l’adoption d’amendements constitutionnels qui accordent un statut légal aux institutions religieuses et autorisent notamment l’organisation d’écoles paroissiales.

Les sports dans lesquels les Mexicains ont connu un relatif succès international sont la boxe et le football. Ce sont les sports les plus populaires du pays.

Le Mexique a accueilli les Jeux olympiques d'été de 1968, ainsi que deux coupes du monde de football, en 1970 et en 1986.

Le sport national est un sport équestre appelé « », juste après vient le baseball sur la côte atlantique. Les principales équipes de football sont Tigres UANL, Club América, Club de Fútbol Monterrey, Chivas de Guadalajara, Cruz Azul Fútbol Club, Club Universidad Nacional, Club de Fútbol Atlas et Deportivo Toluca Fútbol Club.

La cuisine mexicaine a été mise en 2010 sur la liste représentative du patrimoine culturel immatériel de l'humanité. Celle-ci est en fait constituée par une multitude de cuisines régionales qui sont très riches en subtilité et raffinement, très variées, et qui utilisent un grand nombre d’ingrédients.

Son origine date de la conquête espagnole, même si elle a de nombreuses influences indigènes. D’un côté, le maïs, les piments, les haricots noirs, les courges, l’avocat, la patate douce, les tomates, le cacao, la vanille, la dinde et de nombreux fruits et condiments originaires du nouveau monde. De l’autre côté, les Espagnols introduisirent les viandes des animaux domestiqués dans l’ancien monde tels que le porc, le bœuf et le poulet, mais aussi le poivre en grains, le sucre, le lait et ses dérivés, le blé, et le riz, les agrumes et une multitude d’ingrédients qui forment aujourd’hui une part importante de l’alimentation des Mexicains. De cette fusion naissent le pozole, le mole et les tamales dans leurs formes actuelles, le chocolat, un grand répertoire de grignotages mexicains ("antojitos").

La nixtamalisation du maïs et le broyage sur "molcajete" (mortier traditionnel) et "metate" ont fait place à des procédés industriels modernes. L'atole est à base de Maïzena et sa variante, le champurrado, qui lui est un atole à base de maïs ne sont pas des boissons mais se consomment au petit-déjeuner et le soir. On trouve des boissons alcoolisées régionales telles que le rompope. Il existe une confiture de lait de chèvre, la cajeta. Les flans à la vanille et au caramel y sont très populaires.

Le Mexique produit de nombreux spiritueux dont la tequila faite à partir de la distillation de l’agave bleue. 50 % de la production de tequila est exporté vers les États-Unis. La tequila possède une AOC et ne peut provenir que d’une région formée de 181 communes réparties sur cinq États (dont 125 dans l’État de Jalisco).

Les mexicains sont en 2012 les plus gros consommateurs d'œufs (consommés principalement au petit-déjeuner) par personne au monde.

Le Museo de Arte Popular (MAP) consacré à l'art populaire mexicain a ouvert ses portes en 2006. Il est installé dans un immeuble Art déco qui a été construit en 1928 pour héberger la caserne des pompiers. Très endommagé par le tremblement de terre de 1985, le bâtiment a été abandonné durant plus de dix ans. Restauré, il abrite aujourd'hui une collection d'artisanat. Plus de mille pièces, réparties sur trois étages : les animaux fantastiques fabriqués en papier mâché ou en bois (alebrijes), des ex-voto des masques, des costumes et des vêtements brodés et les mille représentations de la mort à la mexicaine.

Quelques personnalités :


Le terme de mariachi désigne tout à la fois un type de formation musicale originaire du Mexique, puis le style de musique associé, et une culture musicale. Un groupe de mariachis est constitué au moins de deux violons, deux trompettes, un joueur de guitare espagnole, un vihuela et d’un "guitarrón". Certaines formations comportent plusieurs dizaines de musiciens. Les mariachis sont originaires de l’État de Jalisco.

De nombreux groupes ou musiciens américains ont été influencés par la musique mexicaine : Flaco Jimenez, Los Lobos…

Des styles musicaux et danses populaires sont la "banda" (Nord) et la "salsa" (reste du pays).

Chaque région possède sa musique au même titre que sa cuisine et son artisanat.

Cinq d'entre elles se distinguent par la richesse et la variété de leur répertoire populaire :






Jours fériés officiels

Autres fêtes

Le tourisme au Mexique est une activité importante, aussi bien pour les Mexicains qui choisissent d'y passer leurs vacances, que pour les étrangers qui viennent y faire un séjour. Le Mexique est un pays de hauts plateaux enserrés entre deux chaînes montagneuses (Sierra Madre occidentale et orientale) qui s’abaissent vers d’étroites plaines côtières à l’est et à l’ouest. Ces deux chaînes de montagnes se rejoignent au sud-est du pays où elles forment la Sierra Madre du sud. Au nord-ouest, la Basse-Californie est une longue et étroite péninsule qui s’étend sur et prolonge la Sierra Nevada américaine.

Sites classés au patrimoine mondial de l’UNESCO

Le Mexique a pour codes :



</doc>
<doc id="1940" url="https://fr.wikipedia.org/wiki?curid=1940" title="Métal">
Métal

En chimie, les métaux sont des matériaux dont les atomes sont unis par des liaisons métalliques. Il s'agit de corps simples ou d'alliages le plus souvent durs, opaques, brillants, bons conducteurs de la chaleur et de l'électricité. Ils sont généralement malléables, c'est-à-dire qu'ils peuvent être martelés ou pressés pour leur faire changer de forme sans les fissurer ni les briser. De nombreuses substances qui ne sont pas classées comme métalliques à pression atmosphérique peuvent acquérir des propriétés métalliques lorsqu'elles sont soumises à des pressions élevées. Les métaux possèdent de nombreuses applications courantes, et leur consommation s'est très fortement accrue depuis les années 1980, au point que certains d'entre eux sont devenus des matières premières minérales critiques.

En astrophysique, et notamment en physique stellaire, on appelle métal tout élément chimique autre que l'hydrogène et l'hélium. Ces éléments sont produits par nucléosynthèse stellaire à partir d'hydrogène et d'hélium par fusion nucléaire, processus à l'origine de l'énergie libérée par les étoiles. De ce point de vue, la métallicité d'une étoile est la proportion d'éléments autres que l'hydrogène et l'hélium qui la constituent.

Les électrons des matériaux métalliques purs ou alliés se distribuent dans des niveaux d'énergie formant un continuum entre la bande de valence, occupée par les électrons de valence, et la bande de conduction, occupée par les électrons libres injectés thermiquement depuis la bande de valence par-delà le niveau de Fermi. Ces électrons libres forment une liaison métallique délocalisée dans tout le volume du matériau. On peut se représenter un métal comme un réseau tridimensionnel de cations métalliques baignant dans un fluide d'électrons très mobiles. Le modèle de l'électron libre permet de calculer la conductivité électrique ainsi que la contribution des électrons à la capacité calorifique et à la conductivité thermique des métaux, bien que ce modèle ne tienne pas compte de la structure du réseau cristallin du métal. Certains matériaux, comme les intermétalliques, présentent des liaisons partiellement métalliques et sont donc à la limite des céramiques.

La nature électronique particulière d'une liaison métallique est responsable de plusieurs propriétés macroscopiques des métaux : le fluide d'électrons libres assure à la fois une conductivité électrique et une conductivité thermique élevées en permettant la circulation d'un courant électrique et en favorisant la propagation des phonons dans le matériau ; elle rend compte de la ductilité, de la malléabilité et de la plasticité des métaux en maintenant leur cohésion en cas de déformation brisant les autres liaisons interatomiques ; elle confère aux métaux leur absorbance et leur éclat particulier par son interaction avec les ondes électromagnétiques, ainsi que leur point de fusion et leur point d'ébullition plus élevés que les non-métaux en renforçant les autres types de liaisons interatomiques. Ces dernières, notamment les liaisons covalentes de coordination, sont responsables des différentes structures cristallines formées par les métaux solides : la plus fréquente est la structure cubique centrée, suivie de la structure hexagonale compacte et de la structure cubique à faces centrées.

Dans une structure cubique centrée, chaque atome est situé au centre d'un cube formé par ses huit atomes voisins. Dans les structures cubique à faces centrées et hexagonale compacte, chaque atome est entouré par douze autres atomes, mais l'empilement de ces atomes diffère entre ces deux structures. Certains métaux peuvent adopter des structures cristallines différentes selon la température et la pression auxquels ils sont soumis.

Tous les métaux — notamment les alliages — ne sont cependant pas cristallins, et il peut se former des alliages métalliques amorphes par trempe rapide d'alliages métalliques fondus. On utilise pour ce faire des métaux fondus dont les atomes ont des tailles sensiblement différentes, ce qui limite la cristallisation lors d'un refroidissement rapide. Également appelés verres métalliques, les alliages métalliques amorphes présentent, par rapport aux métaux usuels, une meilleure ténacité, une moindre fragilité, ainsi qu'une plus grande résistance à la déformation et à la corrosion.

La force d'une liaison métallique dépend notamment du nombre d'électrons libres par atome métallique, et atteint un maximum au sein des métaux de transition vers le milieu du bloc d au niveau de la et au-delà, parmi les métaux réfractaires. Les liaisons métalliques subsistant à l'état liquide, contrairement aux autres liaisons interatomiques, le meilleur indicateur de la force de la liaison métallique d'un métal donné est sa température d'ébullition plutôt que sa température de fusion.

Dans le tableau périodique des éléments, les métaux occupent la gauche, le centre et une partie de la droite du tableau, séparés des non-métaux par les métalloïdes. Parmi les dont les propriétés chimiques ont été un tant soit peu caractérisées, on dénombre environ et . La ligne de démarcation entre métaux et non-métaux du tableau ci-contre est conventionnelle : elle est arbitraire et ne marque pas une rupture nette des propriétés macroscopiques entre éléments, dont la transition entre métaux et non-métaux est relativement continue, donnant lieu à la superposition de propriétés métalliques et non métalliques chez certains métalloïdes. De plus, un même élément peut exister selon plusieurs variétés allotropiques aux propriétés davantage métalliques pour les unes et davantage non métalliques pour les autres : un bon exemple est l'étain, qui existe d'une part sous une grise de structure cubique de type diamant, stable aux basses températures, aux propriétés métalloïdes proches d'un non-métal, et, d'autre part, sous une blanche de structure tétragonale, dont les propriétés sont celles d'un métal pauvre.

Les propriétés des métaux eux-mêmes ne sont pas uniformes, et l'on a coutume de les classer en familles plus ou moins informelles qui rendent compte des différences de propriétés entre ces éléments. Du point de vue chimique, le caractère métallique est d'autant plus marqué qu'on se déplace vers la gauche et vers le bas du tableau. Ainsi, les éléments les plus métalliques sont les métaux alcalins, tandis que les moins métalliques sont les non-métaux diatomiques, notamment les halogènes. Entre les deux, d'autres familles d'éléments sont traditionnellement définies, comme les métaux alcalino-terreux, les lanthanides, les actinides, les métaux de transition et les métaux dits « pauvres », ces derniers étant les métaux dont les propriétés métalliques sont les moins affirmées.

Du point de vue pratique, il existe une grande variété de termes désignant des familles d'éléments métalliques et d'alliages. On parle de métaux ferreux et non ferreux selon qu'on considère les alliages contenant ou dépourvus de ferrite, respectivement. On parle de métaux nobles pour désigner les éléments métalliques résistants à la corrosion et à l'oxydation dans l'air humide : ce sont le ruthénium, le rhodium, l'argent, l'osmium, l'iridium, le platine et l'or ; le mercure est parfois également considéré comme un métal noble, tandis que le titane, le niobium et le tantale, qui sont pourtant très résistants à la corrosion, ne sont pas considérés comme des métaux nobles. On parle de métaux précieux pour désigner les métaux les plus rares et dont la valeur marchande est la plus élevée, comme typiquement l'or, l'argent, le platine et le palladium, qui ont chacun un code monétaire ISO 4217 : XAU, XAG, XPT et XPD respectivement ; les platinoïdes sont également considérés comme des métaux précieux. On parle de métaux réfractaires pour désigner les métaux particulièrement résistants aux températures élevées et à l'usure : ce sont typiquement le niobium, le molybdène, le tantale, le tungstène et le rhénium ; le technétium est également réfractaire, mais n'est généralement pas mentionné comme tel car il est synthétique et radioactif.

Les métaux purs ont le plus souvent une conductivité électrique, une conductivité thermique et une masse volumique élevées. L'argent est ainsi le meilleur conducteur électrique (), suivi par le cuivre (), l'or () et l'aluminium (). La conductivité électrique du fer est de , tandis que celle de l'acier au carbone 1010 (fer à 0,10 % de carbone) est de seulement , ce qui illustre l'effet des impuretés sur la conductivité des métaux.

Bien que la plupart des métaux aient une masse volumique supérieure à celle de la plupart des non-métaux, celle-ci est très variable selon les matériaux considérés. Parmi les corps simples métalliques, le lithium est le moins dense ( à ) tandis que l'osmium est le plus dense (). Les métaux alcalins (dont fait partie le lithium) et alcalino-terreux sont les moins denses des métaux ; ils sont également les moins durs, et les métaux alcalins ont un point de fusion particulièrement bas : hormis le lithium, ils sont tous liquides à . La densité élevée de la plupart des métaux provient de leur structure cristalline compacte.

Les métaux sont en outre généralement caractérisés par une bonne malléabilité et une grande ductilité qui leur permettent de se déformer sans se briser. Ainsi, le cuivre pur peut être étiré pour former des fils électriques, des tuyaux (plomberie), être mis en plaque et martelé en forme de casseroles ; l'or pur peut également être mis sous forme de feuilles très fines. À l'inverse, certains éléments d'alliage permettent de durcir le métal : c'est par exemple le cas du carbone qui durcit le fer pour donner de l'acier, de l'étain qui durcit le cuivre pour donner le bronze, ou encore de l'argent et du cuivre qui durcissent l'or.

La force des liaisons métalliques est la plus élevée aux environs du centre de la famille des métaux de transition, au niveau des métaux réfractaires, car ces éléments ont un grand nombre d'électrons délocalisés dans leur structure. D'autres facteurs entrent cependant également en ligne de compte, comme le rayon atomique, le numéro atomique, le nombre d'orbitales liantes, la superposition des énergies des orbitales et le type de structure cristalline ; les structures cubiques centrées donnent ainsi des liaisons métalliques moins fortes que les structures cubiques à faces centrées et hexagonales compactes car ces dernières ont une coordinence plus élevée, c'est-à-dire qu'ils lient davantage d'atomes voisins que la première.

Les métaux ont une surface généralement brillante, et sont opaques dès que leur épaisseur dépasse quelques micromètres ; les feuilles d'or transmettent néanmoins une lumière verte.

La déformation élastique des métaux peut être modélisée par la loi de Hooke lorsque la déformation est une fonction linéaire de la contrainte. L'application de forces supérieures à la limite d'élasticité ou le chauffage peuvent conduire à une déformation permanente de l'objet, ce qui correspond à une déformation plastique. Cette modification irréversible de la disposition des atomes du matériau peut résulter de l'application :

L'écoulement visqueux autour des joints de grains, par exemple, peut donner lieu au fluage ou la fatigue du métal. Il peut également contribuer à d'importants changements dans la microstructure, comme la croissance des grains et l'accroissement localisé de la densité du matériau par élimination de la porosité intergranulaire. De plus, la nature non directionnelle des liaisons métalliques pourrait contribuer de manière significative à la ductilité des métaux solides.

Quelques métaux présentent des propriétés magnétiques remarquables comme le ferromagnétisme. Ce sont notamment, à température ambiante, le fer, le cobalt et le nickel. Certaines terres rares (lanthanides dans la classification périodique) sont également ferromagnétiques à basse température. Les propriétés magnétiques varient avec les alliages, ce qui peut être mis à profit pour créer des aimants puissants ou annuler le magnétisme d'un métal comme le fer.

Les métaux ont tendance à former des cations en perdant des électrons. Le sodium peut ainsi perdre un électron pour former le cation Na, le calcium deux électrons pour former le cation Ca, le fer deux électrons pour former le cation ferreux Fe ou trois électrons pour former le cation ferrique Fe. Ces ions métalliques se retrouvent en solution ou dans des sels, comme le chlorure de lithium LiCl ou le sulfure d'argent .

Les métaux réagissent avec l'oxygène de l'air pour former des oxydes de façon plus ou moins rapide : le fer forme de la rouille en plusieurs mois, voire années, tandis que le potassium brûle en quelques secondes. Les réactions suivantes sont des exemples d'oxydation de métaux :

Les métaux de transition tels que le fer, le cobalt et le nickel s'oxydent plus lentement car leur oxydation forme une couche de passivation qui protège l'intérieur du matériau. Certains forment une couche imperméable qui bloque complètement la progression de l'oxydation et permet de conserver pendant des décennies à la fois leur éclat métallique et leurs bonnes propriétés conductrices de l'électricité : ce sont par exemple l'aluminium, le magnésium, l'acier inoxydable et le titane. Les oxydes métalliques sont généralement basiques, par opposition aux oxydes des non-métaux, qui sont plutôt acides ; les oxydes métalliques acides se rencontrent avec les états d'oxydation très élevés, comme avec le trioxyde de chrome , l'heptoxyde de dimanganèse et le tétroxyde d'osmium , qui présentent des réactions strictement acides. D'autres métaux, tels que le palladium, le platine et l'or ne réagissent pas du tout à l'air libre : pour cette raison, ils sont appelés métaux nobles.

La corrosion des métaux peut être empêchée par leur peinture, leur anodisation ou encore l'apposition d'un revêtement. S'agissant d'une réaction électrochimique, il faut, pour que la protection soit efficace, utiliser un métal plus réducteur que le métal, sinon le revêtement peut favoriser la corrosion, surtout en cas de rayures.

Un alliage est un mélange de deux éléments chimiques ou davantage dont le principal constituant est un métal. La plupart des métaux purs sont trop mous, trop fragiles ou trop réactifs pour pouvoir être utilisés tels quels. Il est possible de moduler les propriétés des alliages en faisant varier les proportions relatives de leurs différents constituants. Il s'agit généralement de les rendre moins fragiles, plus durs, plus résistants à la corrosion, ou encore de leur donner une couleur et un éclat plus attrayants. De tous les alliages métalliques utilisés de nos jours, ceux du fer — acier, , acier à outils, acier au carbone, acier inoxydable, fonte par exemple — en représentent l'essentiel de la production, aussi bien en valeur qu'en volume. Le fer allié au carbone donne des aciers de moins en moins ductiles et résistants à mesure que le taux de carbone augmente. L'addition de silicium donne du ferrosilicium, souvent allié à la fonte, tandis que l'addition de chrome, de nickel et de molybdène à des aciers au carbone (à plus de 10 %) donne de l'acier inoxydable.

Outre les alliages de fer, ceux de cuivre, d'aluminium, de titane et de magnésium sont également importants d'un point de vue économique. Les alliages de cuivre sont connus sous forme de bronze depuis l'âge du bronze. Le billon était un alliage utilisé jusqu'au Moyen Âge pour faire des pièces de monnaie et constitué le plus souvent essentiellement de cuivre avec un peu d'argent et parfois de mercure. De nos jour, le bronze désigne spécifiquement un alliage de cuivre et d'étain, tandis que le laiton est un alliage de cuivre et de zinc, et que le maillechort est un alliage de cuivre, de zinc et de nickel. Ces alliages ont divers usages industriels, notamment dans les installations électriques. Les alliages d'aluminium, de titane et de magnésium ont été développés plus récemment, et sont intéressants en raison de leur grande résistance mécanique pour une masse volumique plutôt faible ; leur coût de revient est cependant élevé, ce qui restreint leur utilisation aux applications de haute technologie pour lesquelles les performances sont plus importantes que le coût. Parmi les différents alliages d'aluminium, on peut citer ceux pour corroyage et pour fonderie. Le zamak est formé de zinc allié à l'aluminium, le magnésium et le cuivre.

Outre des propriétés mécaniques remarquables, les alliages permettent également de faciliter la fusion des métaux, notamment les eutectiques. C'est par exemple le cas du système aluminium-silicium, avec un hypereutectique à environ 78 % d'aluminium, 17 % de silicium, 4 % de cuivre et 1 % de magnésium, utilisé dans l'industrie automobile, et l'alliage étain-plomb qui fond à — à comparer aux points de fusion respectifs de l'étain et du plomb, qui sont de et . L'un des alliages métalliques ayant le plus bas point de fusion est le galinstan, dont la composition massique est typiquement de 68 % de gallium, 22 % d'indium et 10 % d'étain, et qui est liquide à température ambiante. C'est également le cas de l'eutectique NaK, constitué de 77 % de potassium et 23 % de sodium, mais qui est corrosif et très inflammable à l'air libre, surtout en présence d'humidité, ce qui en limite l'usage à des applications très particulières.

Les alliages spéciaux destinés à des applications de pointe, dits superalliages, comme ceux des moteurs à réaction, peuvent contenir plus d'une dizaine d'éléments différents. Les alliages à mémoire de forme sont un autre type d'applications : les alliages Fe-Mn-Si, Cu-Zn-Al et Cu-Al-Ni, par exemple, sont assez bon marché, mais il en existe une très grande variété.

Les métaux présentent le plus souvent un état d'oxydation positif, c'est-à-dire qu'ils tendent naturellement à former des cations. Il existe cependant des anions métalliques, avec un état d'oxydation négatif, par exemple avec certains complexes carbonyles comme ou avec l'anion de sodium Na.

Étymologiquement, un métal est une substance extraite d'une mine — μέταλλον en grec ancien. En pratique, les métaux sont généralement extraits sous forme de minerais contenant les éléments recherchés. Ces minerais peuvent chimiquement être de nature très diverse. Ce sont souvent des oxydes, comme la bauxite (minerai d'aluminium), l'ilménite (minerai de titane), l'hématite et la magnétite (minerais de fer), ou encore la pechblende (minerai d'uranium). Il peut également s'agir de sulfates, comme la chalcopyrite (minerai de cuivre), la sphalérite (minerai de zinc), la molybdénite (minerai de molybdène) ou encore le cinabre (minerai de mercure). Il existe par ailleurs des silicates, comme le béryl (minerai de béryllium), des carbonates comme la dolomite (minerai de magnésium), et bien d'autres types de composés.

Une fois extraits des mines, les minerais sont traités pour isoler le métal recherché, le plus souvent par réduction chimique ou électrolytique. La pyrométallurgie utilise des températures élevées pour convertir les minerais en métaux bruts, tandis que l'hydrométallurgie passe par au moins une étape où le métal est solvaté dans l'eau. Les méthodes employées dépendent des métaux et de leurs impuretés.

Lorsque le minerai est constitué d'un composé ionique du métal avec un non-métal, le minerai doit généralement être fondu, c'est-à-dire chauffé en présence d'un réducteur pour en extraire le métal pur. De nombreux métaux communs comme le fer sont fondus en présence de carbone comme réducteur. D'autres métaux, en revanche, ne peuvent être réduits de cette façon, et sont purifiés par électrolyse : c'est le cas de l'aluminium et du sodium notamment. Les sulfures ne sont pas réduits directement, mais sont d'abord grillés à l'air libre pour être préalablement convertis en oxydes, qui sont ensuite traités de manière classique.

Certains minerais sont des éléments natifs, les plus connus étant le cuivre natif, l'argent natif, l'or natif, voire le fer météorique, mais il en existe bien d'autres, plus rares, comme le fer natif, le nickel natif (dans des roches d'origine météoritique (nickel-fer)), le cadmium natif, l'indium natif, l'étain natif, l'antimoine natif, le tellure natif, le mercure natif, le plomb natif, le bismuth, par exemple. Ces minerais sont solides, à l'exception du mercure, qui se présente à l'état liquide au-dessus de dans des poches généralement de petite taille ne dépassant quelques kilogrammes de métal et le plus souvent associées à des métaux nobles, avec lesquels il forme des amalgames. Les platinoïdes existent également sous forme minérale plus ou moins pure, comme le ruthénium natif, le rhodium natif, le palladium natif, l'osmium natif, l'iridium natif et le platine natif.

Certains métaux et alliages possèdent une résistance structurelle élevée par unité de masse, ce qui les rend utiles pour transporter des charges lourdes et résister à des chocs violents. Les alliages métalliques peuvent être conçus pour avoir une résistance élevée aux contraintes de cisaillement, de flexion et de déformation. Le même métal peut cependant être sujet à la fatigue à la suite de contraintes répétées ou d'un dépassement de la contrainte maximum. La résistance et la résilience des métaux a conduit à leur utilisation courante dans la construction des gratte-ciel et des ouvrages d'art ainsi que dans celle de tous types de véhicules, d'appareils et dispositifs, d'outils, de tuyaux, ou encore de voies ferrées.

Les deux métaux les plus utilisés, le fer et l'aluminium, sont également les plus abondants dans l'écorce terrestre. Le fer est le plus utilisé des deux : il est à la base de toutes les grandes constructions métalliques (poutre, rail, coque de navire). L'aluminium est presque toujours utilisé allié à d'autres métaux afin d'en améliorer les propriétés mécaniques, dans des applications tirant profit du fait qu'il est moins dense que le fer ( contre ) et meilleur conducteur électrique ( contre ) ; l'aluminium est par exemple utilisé préférentiellement au cuivre dans les câbles électriques à haute tension aériens.

Le cuivre reste utilisé essentiellement pour ses bonne propriétés de conducteur de l'électricité dans les câbles électriques, et de conducteur thermique dans les ustensiles de cuisine. Les propriétés de conducteur de la chaleur font de certains métaux des matériaux intéressants pour réaliser des dissipateurs thermiques destinés à éviter les surchauffes. Les métaux les moins abondants sont utilisés dans des alliages (chrome, manganèse, titane), et les plus rares interviennent souvent comme catalyseurs (platinoïdes, notamment) et parfois comme placements financiers ou en joaillerie (métaux précieux). La réflectivité élevée de certains métaux, comme l'argent, en font des matériaux de choix pour la construction de miroirs, notamment ceux des télescopes. Elle est également à l'origine de l'attrait esthétique de certains métaux utilisés en joaillerie. L'uranium est un métal qui, après séparation isotopique, permet d'alimenter des réacteurs nucléaires pour libérer leur énergie par fission. D'autres métaux, trop réactifs à l'air et/ou à l'eau sont rarement utilisés à l'état métallique (sodium, potassium, calcium).

Dans un certain nombre de cas, les métaux tendent à être remplacés par d'autres matériaux, en général pour des raisons de légèreté (polymères, matériaux composites, céramiques) ou de résistance à la corrosion ou à l'usure (céramiques). Ces matériaux ont toutefois eux aussi leurs limites par rapport aux métaux, en particulier les polymères et composites à matrice polymère ne sont pas utilisables à hautes températures et sont souvent plus souples, tandis que les céramiques résistent mal aux chocs.

Les métaux peuvent être dopés avec des molécules étrangères, qui peuvent être organiques, minérales, biologiques, ou encore des polymères. Ces molécules confèrent au métal des propriétés nouvelles qui peuvent être mises à profit pour des applications aussi variées que les catalyseurs, la médecine, l'électrochimie et la résistance à la corrosion<ref name="10.1021/ar4001982">
</ref>.

Les différents états d'oxydation, conformations, complexes ou formes transitoires représentent des espèces chimiques distinctes d'un élément et jouent un rôle majeur dans l'élaboration, la corrosion, ainsi que sur leur biodisponibilité et leur toxicité ou écotoxicité. Certaines espèces d'éléments traces métalliques (ÉTM) sont plus facilement assimilables par les organismes que d'autres, ce qui engendre des effets bénéfiques ou néfastes selon la nature et la concentration du métal (élément essentiel ou non).

Il ne faut pas confondre la spéciation chimique d'un élément avec son fractionnement ou sa partition. La littérature scientifique confond quelquefois ces concepts ce qui complexifie les recherches dans ces domaines.

Cette section décrit donc les principales catégories d'espèces chimiques relatives aux ÉTM et présente des exemples d'espèces chimiques de niveau toxique varié.

Comme indiqué précédemment, les métaux se trouvent en général naturellement dans des minerais ; ils sont à l'état oxydé. Par exemple, le fer se trouve à l'état Fe(III) dans l'hématite, à l'état Fe(II) et Fe(III) dans la magnétite, l'aluminium dans l'état Al(III) dans la bauxite… La métallurgie primaire consiste essentiellement en la réduction du minerai pour obtenir un état d'oxydation (0).

À l'inverse, en réagissant avec l'environnement, le métal va s'oxyder et se dissoudre dans l'eau ou bien se lier à d'autres atomes ou ions, en particulier l'oxygène et l'ion hydroxyle. C'est un des mécanismes principaux de la corrosion.

L'état d'oxydation des métaux dans un système influence leurs effets sur les organismes. Par exemple, le chrome(III) est un élément essentiel (c'est-à-dire nécessaire pour le bon fonctionnement de l'organisme) et pénètre difficilement les membranes lipidiques des cellules. En revanche, le Cr(VI), qui s'avère toxique pour certains gènes, est cancérigène et pénètre facilement dans les cellules grâce à des transporteurs spécifiques. Dans d'autres cas, ce sont les formes moins oxydées qui sont toxiques, par exemple avec l'arsenic dont la toxicité est plus importante pour As(III) que pour As(V).

La composition isotopique de quelques éléments influence leur abondance ou leur toxicité dans l'environnement. Par exemple, le plomb comporte une vingtaine d'isotopes dont quatre sont stables : Pb, Pb, Pb et Pb. Les Pb et Pb proviennent de la dégradation de l'uranium et le Pb résulte de la dégradation du thorium, deux éléments radioactifs ; ainsi, l'abondance de ces isotopes s’accroit dans le temps, et la composition isotopique du plomb évolue donc selon les sources d'émission stimulées. Un autre exemple intéressant de variation de la toxicité est lié à la composition isotopique de l'eau (HO) : remplacer 60 % de l'eau du corps de rongeurs par de l'HO est sans effet alors qu'une substitution de 30-40 % de cette eau par du DO engendre la mort de ces animaux.

On peut chercher à trier les isotopes, par exemple pour enrichir la matière en isotopes radioactifs, comme dans le cas de l'enrichissement de l'uranium pour produire du combustible nucléaire. On peut à l'inverse chercher à appauvrir le métal, comme dans le cas des munitions à uranium appauvri.

Les isotopes métalliques sont utilisés comme traceurs pour les phénomènes de diffusion : on élabore un métal contenant une quantité notable d'isotope radioactif, et le profil de radioactivité permet de suivre la progression de ces atomes.

Les métaux s'allient souvent à des ligands inorganiques pour former des composés ou complexes inorganiques possédant des propriétés physico-chimiques différentes. Par exemple, la charge, la solubilité, le coefficient de diffusion ou la force de liaison de ces composés influencent le transport et par conséquent la biodisponibilité et la toxicité des métaux dans les organismes. Par exemple, certains sels de nickel comme les chlorures (NiCl) et les sulfates (NiSO) sont solubles dans l'eau et de faible toxicité orale, alors que les sulfures de nickel (NiS) sont pratiquement insolubles dans l'eau mais sont cancérigènes.

Les composés organiques tel les sucres, acides organiques, lipides ou autres composés organiques de faible poids moléculaire ont des affinités plus ou moins importantes avec les métaux. Certains d'entre eux, des acides organiques comme l'acide citrique et l'acide malique, contiennent un groupement fonctionnel (l'hydroxylcarboxyle) qui se lie facilement aux métaux et qui diminuent leur biodisponibilité; ces composés sont très étudiés en écotoxicologie terrestre car ils sont excrétés par les racines des plantes et les micro-organismes du sol, créant une synergie qui diminue la toxicité des métaux dans le sol.

Certains composés organiques particuliers que l'on nomme chélateurs, comme l'EDTA, forment des complexes très stables avec les métaux. Les chélateurs sont des ligands solubles polydentés faiblement acides qui forment des complexes chélateur-métal thermodynamiquement forts ; ils sont quelquefois utilisés pour la restauration des eaux et des sols contaminés aux métaux ou dans les méthodes analytiques chimiques pour extraire les métaux d'une matrice.

Les composés organométalliques contiennent une liaison entre le carbone et le métal. Cette liaison peut être de nature covalente ou ionique; par exemple, les liaisons carbone-sodium et carbone-potassium sont fortement ioniques, les liaisons carbone-étain, carbone-plomb et carbone-mercure sont fortement covalentes et les liens carbone-lithium et carbone magnésium se situent entre la liaison ionique et la liaison covalente.

Par exemple, la bioalkylation, c'est-à-dire la formation d'un alkyle (CH) avec un métal par des micro-organismes spécifiques, est un processus fréquent dans les sols et les sédiments. Or, bien que la méthylation des métaux (lien CH-métal) forme des composés plutôt toxiques, certains alkyles métalliques d'arsenic et de sélénium détoxifient le métabolisme de l'humain et d'autres organismes vivants. Néanmoins, la plupart des produits organométalliques résultant d'une bioalkylation sont d'origine anthropogénique, comme certains fongicides ou produits de combustion d'essence, et sont très toxiques pour le système nerveux central de certains organismes (comme les dérivés d'alkyles d'étain, de plomb ou de mercure et d'or).

Les composés ou complexes macromoléculaires sont à la limite de représentation des espèces chimiques. Ils forment malgré tout une catégorie distincte car ils jouent un rôle particulièrement important dans la biodisponibilité des métaux pour les organismes vivants. En effet, les acides humiques et fulviques résultant de la biodégradation de la matière organique sont des anions mobilisant les ÉTM contenus dans les sols et dans les eaux. Les acides humiques et fulviques ont des structures et une composition très variables et complexes mais joueraient un rôle significatif sur la spéciation des métaux.

D'autres particules organiques et inorganiques tels la biomasse et les colloïdes adsorbent les métaux et diminuent ainsi leur toxicité en réduisant leur biodisponibilité. Par contre, d'autres macromolécules anioniques des organismes vivants, comme certains acides nucléiques ou les glycosaminoglycanes, se lient involontairement aux ÉTM et provoquent des mutagenèses dommageables pour l'organisme.

La spéciation des métaux dans les phases aqueuses et solides est influencée par plusieurs paramètres (Voir aussi section Environnement de cette page) :


Cette spéciation implique que l'équilibre chimique est atteint. Or, la complexation des métaux avec les ligands inorganiques est très rapide car ils sont nombreux dans la phase aqueuse, mais la complexation des métaux avec les ligands organiques nécessite plus de temps car les sites d'adsorption ou d'attachement sont moins accessibles. Par conséquent, il est préférable d'analyser la spéciation d'une contamination métallique sur une matrice contaminée stable depuis plusieurs années qu'une matrice fraîchement contaminée avec une dynamique chimique évolutive, sans quoi les analyses risquent d'être biaisées.

De plus, la constante d'équilibre relative à la notion d'équilibre chimique peut être illustrée par la réaction :

Métal + Ligand → Métal-Ligand

La constante d'équilibre K associée à cette équation varie selon le type de lien :

Ainsi, puisque K est relativement faible pour les paires ioniques et plus élevée pour les complexes, les métaux préfèrent s'associer à long terme aux complexes stables qu'aux paires ioniques de plus faible énergie de liaison.

Le développement de nombreuses industries telles que l'électronique, les technologies de l'information et de la communication, et l'aéronautique, et le pari du « tout technologique » dans la recherche du rendement et de l'efficacité, ont conduit à une augmentation sans précédent de la production et de la consommation de métaux. La période de croissance de 1990 à 2010 a conduit à un doublement de la production des principaux métaux. Alors que dans les années 1970, on utilisait moins de 20 métaux dans la table de Mendeleïev, on en consomme environ 60 depuis les années 2000.

Il y a une tendance à la baisse de concentration moyenne des minerais. Par exemple, la concentration moyenne des minerais de cuivre exploités est passée de 1,8 % dans les années 1930 à 0,8 % en 2010. Parallèlement, les réserves, exprimées au niveau de production 2008, se situent pour la plupart des métaux entre 20 et 100 ans de production annuelle.

Depuis très longtemps les mines de certains métaux (précieux ou communs mais indispensables à l'industrie), les installations de raffinage, voire certains secrets de fabrication étaient considérés comme d'intérêt stratégique par les États. Les raisons militaires et l'avènement des armes et munitions métalliques puis de l'énergie et de l'arme nucléaire ont accru l'importance de certains métaux. Même pour des métaux géologiquement non rares comme le cuivre, mais faisant l'objet d'un marché fluctuant, de fortes hausses de cuivre se traduisent aussi par l'accroissement de vols de métaux (à titre d'exemple, en France, en 2010, RFF et la SNCF ont subi de cuivre (quatre fois plus qu'en 2009) qui ont causé des dysfonctionnements et "plusieurs dizaines de millions d'euros de préjudice par an" pour la SNCF.

La consommation de certains métaux autrefois sans valeur s'est fortement accrue au , avec par exemple l'uranium (fortement demandé pour des usages militaires et civils), les métaux du groupe du platine (principalement utilisés pour les pots d'échappements catalytiques, comme catalyseur industriel ou pour les chimiothérapies anticancéreuses) la surexploitation des ressources minières les plus accessibles ou les plus « pures » et malgré les économies permises par un recyclage d'une partie des métaux constituant les produits en fin de vie ou les chutes de production, la notion de métaux stratégiques est encore prégnante. Ainsi, la France a créé en 2011 un « Comité pour les métaux stratégiques », chargé d'aider le ministère à élaborer et mettre en œuvre une politique rénovée de gestion de ces métaux, via notamment des approvisionnements mieux sécurisés. Le ministre chargé des matières premières en préside les trois collèges (administrations, organismes techniques et fédérations professionnelles et industriels). La FEDEREC (fédération des entreprises du recyclage) et la FEDEM (fédération des minerais, minéraux industriels et métaux non ferreux) y participent.

Contrairement aux composés organiques, les métaux ne sont pas biodégradables par les micro-organismes. Cette caractéristique engendre certains problèmes de gestion de la contamination métallique. En effet, le sort des métaux dans l’environnement pose de grands défis analytiques ; les métaux se retrouvent sous plusieurs formes dans le sol et dans l'eau (complexe avec la matière organique du sol, avec les minéraux, précipitation, ions libres) complexifiant les prédictions de toxicité et d'écotoxicité.

La toxicité et l'écotoxicité des métaux dans les sols sont étroitement liées à leur caractéristiques propres (radioactivité éventuelle et type de radioactivité, métal lourd, toxicité chimique, micro ou nanoparticules), spéciation chimique et biodisponibilité ; plus l'espèce métallique est libre et mobile, plus elle est biodisponible et plus il y a un risque de toxicité sur les organismes vivants. En général, les ions métalliques libres (en solution) constituent la forme chimique la plus disponible pour les organismes et donc la plus susceptible d'être toxique. Cependant, d'autres espèces ou fractions de métaux peuvent être instables et mobiles (fraction labile ou liée aux oxydes libres par exemple) et engendrer un risque pour les organismes.

Certains métaux (fer, cuivre et zinc notamment) sont des éléments essentiels. Ils sont toxiques au-delà d'une certaines dose, mais une carence entraîne des troubles métaboliques graves.

Ainsi, plusieurs paramètres influencent la toxicité des métaux dans les sols:


Pour aller de la mine à un objet façonné, il faut passer par de nombreuses étapes et utiliser beaucoup d'équipements qui consomment de l'énergie. Les métaux étant pratiquement tous sous forme d'oxydes ou de sulfures dans la nature, il faut, pour les obtenir sous forme métallique, fournir l'énergie nécessaire à casser les liaisons chimiques correspondantes.

L'empreinte énergétique d'un métal est la quantité d'énergie nécessaire pour obtenir du métal pur. Dans ce qui suit, la quantité d'énergie est mesurée en tep (tonne équivalent pétrole), pour une tonne de métal pur.

Pour obtenir l'énergie « contenue » dans un métal « neuf », issu de la première transformation du minerai, il faut prendre en compte :

(en tep — tonne d'équivalent pétrole — par tonne de métal brut)

(*) Source en MJ / kg et = 2,38.10 tep.
(**) Source en tec (tonne équivalent carbone) ; conversion utilisée : 1 tec = (valeur moyenne européenne).
(***) Énergie injectée dans les procédés uniquement : hors énergie d'extraction, des intrants (acides, solvants), de transport.

La consommation énergétique totale pour la production de métaux bruts est alors de 730 à , soit 7 à 10 % de l'énergie primaire mondiale. L'acier et l'aluminium en représentent la plus grande part, soit respectivement 544- et 147-.

Heureusement, les grands métaux étant globalement recyclables, l'énergie nécessaire au recyclage est bien moindre que l'énergie nécessaire à la fabrication du métal neuf. Par exemple, pour l'acier, l'énergie nécessaire au recyclage représente 25 à 40 % de l'énergie nécessaire à la production du métal primaire. Pour l'aluminium, dont la production à l'état primaire nécessite beaucoup d'énergie, ce pourcentage n'est que 4 à 5 %.

En planétologie, les métaux sont les matériaux les plus , comme le fer ou le nickel, qui composent le cœur des planètes rocheuses. C'est la catégorie des matériaux les plus lourds à côté des (hydrogène, hélium), des (composés contenant du carbone, de l'azote et/ou de l'oxygène, comme l'eau, le méthane et l'ammoniac) et des (silicates).

En cosmologie, on appelle métaux tous les éléments autres que l'hydrogène et l'hélium. La teneur en ces s'appelle en conséquence la métallicité, notée Z (X et Y représentant respectivement la proportion d'hydrogène et d'hélium).





</doc>
<doc id="1941" url="https://fr.wikipedia.org/wiki?curid=1941" title="Manga">
Manga

Un est une bande dessinée japonaise. 

Le mot « manga » est par ailleurs parfois utilisé pour désigner, par extension, une bande dessinée non japonaise respectant les codes des productions populaires japonaises ou pour nommer, par métonymie, d'autres produits visuels rappelant certaines de ces bandes dessinées (dessins animés, style graphique). 

Les mangas traduits en langue française se lisent généralement dans le sens d'origine (de droite à gauche). La plupart des mangas sont en noir et blanc.

La personne réalisant des mangas est appelée "mangaka".

Le mot japonais « "manga" » souvent traduit littéralement par « image dérisoire » ou « dessin non abouti », est composé de « "ga" » (), qui désigne la représentation graphique (« dessin », « peinture » ou toute image dessinée - comme l'estampe), et « "man" » (), « involontaire », « divertissant », « sans but », mais aussi « exagérer », « déborder » (qui peut être interprété comme caricature), ainsi qu'« au fil de l'idée ». Ainsi on pourrait aussi bien traduire ce mot par « dessin au trait libre », « esquisse au gré de la fantaisie », « image malhabile » ou tout simplement caricature ou grotesque dans le sens de Léonard de Vinci.

Le terme devient courant à partir de la fin du avec la publication d'ouvrages tels que "Mankaku zuihitsu" (1771) de Kankei Suzuki, "Shiji no yukikai" (1798) de Kyōden Santō ou "Manga hyakujo" (1814) de Minwa Aikawa. Également en 1814, Hokusai, futur peintre de "La Grande Vague de Kanagawa", donne à ses recueils d'estampes parfois grotesques le titre "Hokusai manga". C'est ce dernier ouvrage qui fait connaître le mot en Occident. Il aurait été ainsi choisi pour son analogie avec un terme similaire dans l'ancien temps mais dont l'écriture diffère et qui décrit la conservation de proies dans les becs des pélicans indiquant des scènes prises sur le vif - comme l'oiseau fondant sur sa proie.

Il ne prend le sens précis de « bande dessinée » qu'au cours du , avec l'introduction de celle-ci au Japon. Lorsqu'elle y devient très populaire, après 1945 et grâce à Osamu Tezuka, le terme s'impose pour finir par ne plus désigner qu'elle. C'est ce terme qui a été utilisé à l'étranger (France, États-Unis, Allemagne), pour caractériser la bande dessinée japonaise, dont il est devenu un synonyme, et parfois grossièrement ramené à un genre.

Le mot « manga » est pleinement intégré dans la langue française, comme l'atteste son intégration dans les dictionnaires usuels. Ceux-ci le donnent comme masculin (les mots japonais, eux, n'ont pas de genre grammatical), et c'est le genre qui prédomine largement. Toutefois, la première utilisation du mot en français revient à Edmond de Goncourt en 1895, dans une étude artistique dédiée à Hokusai, où il accorde « manga » au féminin pour désigner ce qu'il appela "La Mangwa" ("sic") de l'artiste. Le terme revêtait alors plutôt le sens de « miscellanées », c'est-à-dire un recueil de nature disparate. Depuis cette époque, "manga" a souvent été employé au féminin, et ce jusqu'à la popularisation de l'usage au masculin dans les années 1990 (notamment par les premiers journaux spécialisés et la télévision). Mais un argument en faveur de la féminisation du terme pourrait être que la locution équivalente en français, bande dessinée, est déjà de genre féminin. Plus récemment, l'auteur Frédéric Boilet parle de "manga" au féminin, notamment dans le cadre de son mouvement franco-japonais "La Nouvelle Manga".

"Manga" s'écrit "mangas" au pluriel, selon la règle du pluriel des mots étrangers intégrés dans la langue française (les dictionnaires actuels ne donnent d'ailleurs pas le mot comme invariable).

Les mangas se lisent originellement de droite à gauche, ce qui correspond au sens de lecture japonais. Cela amène une certaine confusion puisque la lecture des mots se fait alors dans le sens inverse de celui des cases (ce qui n'est pas le cas au Japon). Introduits en France en 1978 avec la revue "Le cri qui tue", les mangas ne sont publiés dans ce sens que depuis 1995 environ. Toutefois, les éditeurs français ne se plient pas systématiquement à cette spécificité. Certains choisissent alors de simplement retourner les images, ce qui occasionne des incohérences pouvant sembler douteuses (un droitier qui devient gaucher, un coup porté au cœur qui perd son sens ou encore un salut nazi effectué du bras gauche dans la série "L'Histoire des 3 Adolf"). D'autres adaptent entièrement les ouvrages en retournant seulement certaines images, en changeant la mise en page et en redessinant certains éléments graphiques, ce qui a pour mérite de faire correspondre la forme des phylactères avec l'horizontalité des systèmes d'écriture occidentaux (Casterman notamment, dans sa collection "Écritures"), mais génère toutefois un surcoût significatif.

La plupart des éditeurs français ont actuellement adopté le sens de lecture japonais, dans un but d'économie et de respect de l'œuvre. Cela les expose à se couper d'un lectorat plus large (notamment âgé) que les habitués du genre. Depuis son « invention » par Rodolphe Töpffer en 1827, la bande dessinée occidentale a été codifiée pour une lecture exécutée de gauche à droite et le lecteur risque donc de lire la fin d'une action ou d'un gag avant le début. Cependant, la vague de démocratisation qu'a connu le manga en France auprès des jeunes a fait qu'ils sont désormais plus habitués à un autre sens de lecture.

Le sens de lecture japonais est également devenu le standard de lecture des mangas aux États-Unis depuis le début des années 2000.

En 2002 le marché du manga représentait % des bénéfices de l'industrie éditoriale japonaise et % des livres et magazines publiés au Japon étaient des mangas. Le volume de vente de mangas au Japon représentait quant à lui, en 2006, environ 27 % du total des livres vendus au Japon. Le marché du manga génère ainsi une importante activité économique pour le pays avec un bénéfice de milliards de yens pour l'année 2007, et il est estimé que près d'un Japonais sur douze lit au moins une fois un manga par semaine.

La grande popularité des mangas rivalise avec les grands noms de la bande dessinée européenne ; ainsi, les 42 tomes de "Dragon Ball" se sont vendus à plus de 230 millions d'exemplaires dans le monde et les 86 tomes de "One Piece" se sont vendus à plus de 430 millions d'exemplaires dans le monde, un chiffre qui surpasse celui enregistré par "Les Aventures de Tintin et Milou" avec 24 albums édités à plus de 200 millions d'exemplaires. Rien qu'au Japon, le tirage de "One Piece" dépasse les d'exemplaires à la sortie du tome 86 le .

Les mangas sont vendus moins chers au Japon qu'en Europe, leur prix avoisinant les 500 yens ( euros en juillet 2012), alors qu'en France, le prix d'un manga se situe généralement entre 6 et selon le format et les éditions. Les mangas publiés dans les magazines de prépublication sont considérés au Japon comme des objets de grande consommation plutôt que comme des objets de valeur. Cependant, des éditions reliées et brochées à l'image de celles paraissant en Occident, sont destinées à être collectionnées et conservées.

Depuis son ouverture en , le musée international du manga de Kyōto offre une impressionnante collection de manga ( volumes, sachant que la collection est amenée à évoluer).

Le manga, bien que très ancré dans la culture japonaise moderne, trouve ses prémices dans la peinture narrative qui apparaît à l'époque de Nara, avec l'apparition des premiers rouleaux narratifs peints japonais : les "emakimono". Ces œuvres associaient en effet des peintures à des textes calligraphiés qui assuraient, ensemble, le récit d'une histoire que l'on découvrait au fur et à mesure que se déroulait le rouleau. Le premier des "emakimono", l’, illustration d'un sûtra, était la copie d'une œuvre chinoise et marquait une nette séparation entre le texte et la peinture. Pourtant, durant l'époque de Heian apparaissent les premiers "emakimono" de goût japonais (le style "yamato-e"), dont l"'emaki" du "Genji monogatari" datant du est l'un des plus anciens représentants conservés. Ces derniers faisaient souvent intervenir de courts textes explicatifs après de longues scènes peintes. Les "Chōjū-giga", soient « caricatures de la faune », une satire anthropomorphique, sont constitués uniquement de dessins à l'encre. Cette priorité accordée à l'image – qui peut assurer seule la narration – est aujourd'hui une des caractéristiques les plus importantes du manga.

De même, lors de la période Edo, les estampes étaient d'abord destinées à l'illustration de livres, mais, très vite, le rapport de force s'inversa et l'on vit l'apparition de « livres à lire » en opposition avec les « livres à regarder », les "" tels que le "kibyōshi". Puis vint la disparition relative des écrits complémentaires et la naissance de l'estampe « indépendante » en une seule illustration, qui est la forme la plus fréquente de l’"ukiyo-e". C'est d'ailleurs Katsushika Hokusai (1760-1849), le fondateur de l'estampe de paysage, qui donna son nom au manga (littéralement « dessins grotesques »), nommant ainsi ses célèbres caricatures, les "Hokusai Manga" qu'il publia de 1814 à 1834 à Nagoya.

Enfin, et notamment dans le manga de type "shōjo", l'Art nouveau occupe une place prépondérante parmi les influences des "mangaka", tout en sachant que ce mouvement a été provoqué en partie par le japonisme en Europe, à la suite de la découverte des estampes par les occidentaux.

Pendant la restauration Meiji, à partir de 1868, l’ouverture obligatoire du Japon au commerce extérieur s’accompagne d’une modernisation rapide du pays sous influence occidentale. De nombreux étrangers sont attirés au Japon pour enseigner les sciences et technologies occidentales et de riches Japonais voyagent en Europe. Edo, rebaptisée Tokyo, voit ses rues, éclairées par des réverbères, se peupler de pousse-pousses sans oublier les bicyclettes d'importation. C'est la création du yen et l'interdiction du et du port du . L'usage du "kimono "et du "hakama "(pantalon traditionnel) diminue au profit du costume occidental accompagné du chapeau et du parapluie, pour les hommes, et d'une coiffure européenne pour les femmes.

Les deux seuls quotidiens existants au début des années 1860 étaient à destination de la colonie étrangère, le "Nagasaki Shipping List and Advisor" (bihebdomadaire de langue anglaise) et le "Kampan Batavia Shinbun" ("Journal officiel de Batavia"). La presse japonaise naît avec le "Yokohama Mainichi Shinbun" en 1871 et le "Tokyo Nichinichi Shinbun" en 1872. C'est le "Shinbun Nishikie", créé en 1874, qui introduit le premier les estampes dans la presse japonaise.

La presse japonaise se transforme aussi sur le modèle de la presse anglo-saxonne avec l’apparition des dessins d’humour sur le modèle américain et des caricatures à la mode britannique à partir de 1874 avec le "E-Shinbun Nipponchi", créé par Kanagaki Robun et Kawanabe Kyōsai, et surtout avec le "Marumaru Shinbun" créé par qui a fait une partie de ses études en Grande-Bretagne. Imprimé entre 1877 et 1907, il publie des dessins de et de Kiyochika Kobayashi, créateur d'estampes "ukiyo-e", qui fut élève de Charles Wirgman.

Wirgman fait partie de ces trois Européens qui ont une influence certaine sur l'avenir de la bande dessinée et du manga. Ce caricaturiste anglais arrive à Yokohama en 1861 et l'année suivante il crée un journal satirique "The Japan Punch" dans lequel il publie, jusqu'en 1887, nombre de ses caricatures dans lesquelles il utilise des "balloons". Il enseigne en même temps les techniques occidentales de dessin et de peinture à un grand nombre d'artistes japonais comme Takahashi Yuichi.

Autre caricaturiste, le français Georges Ferdinand Bigot arrive à Yokohama en 1882, il enseigne les techniques occidentales du dessin et de l'aquarelle à l'École militaire de la ville. Parallèlement, il publie des caricatures dans des journaux locaux et édite des recueils de gravure. En 1887, il crée lui aussi une revue satirique, "Tôbaé", alors que Wirgman arrête la sienne, dans laquelle il démontre sa maîtrise de la technique narrative en introduisant la succession des dessins dans des cases au sein d'une même page. Il part en Chine en 1894 pour couvrir pour "The Graphic" de Londres le conflit sino-japonais. De retour en France en 1899, il collabore comme illustrateur pour l'imagerie d'Épinal.

C'est à cette période qu'un fils d'enseignant hollandais dans une mission de Nagasaki quitte le Japon pour suivre des cours d'art à Paris où il tente quelques bandes dessinées dans le Chat noir avant de s'exiler aux États-Unis. C'est là que Gustave Verbeck dessine un des "strips" les plus originaux de l'histoire de la bande dessinée "The Upside-Downs of Little Lady Lovekins and Old Man Muffaroo". Le "strip" de quatre cases se lit dans le sens normal de lecture de gauche à droite puis l’histoire se continue en retournant tête-bêche le journal et en relisant les cases dans le sens inverse, "lady Lovekins" se transforme alors en "old man Muffaroo", le chapeau de l’une devenant la barbe de l’autre.

C'est le caricaturiste australien Frank Arthur Nankivell qui travaille pour le , publié à Yokohama par E. B. Thorne, qui initie Yasuji Kitazawa, qui ne s'appelle pas encore Rakuten Kitazawa, à la caricature. En 1899, il quitte "Box of Curios" pour rejoindre le créée par l'intellectuel Yukichi Fukuzawa désireux de développer le mode satirique au Japon. C’est Kitazawa qui reprend le terme de "manga" pour désigner ses dessins, il se désigne lui-même comme "mangaka" (dessinateur de mangas). Le premier manga considéré comme tel date de 1902. Il s’agit d’une histoire dessinée par Kitazawa dans les pages illustrées du supplément du dimanche du "Jiji Shinpō". Kitazawa s’inspire beaucoup de la culture européenne, son premier manga reprend le thème de l’arroseur arrosé. Le supplément du "Jiji Shinpō" prend rapidement le nom de .

En 1905, Kitazawa crée son premier magazine le en s'inspirant de l'américain "Puck" et du "Rire" français. Ce magazine en couleurs paraît deux à trois fois par mois et contient des textes en japonais, chinois et anglais, des caricatures et un manga en six cases de Kitazawa. Plusieurs fois censuré pour ses caricatures féroces contre le pouvoir, il crée en 1912 deux nouveaux magazines et . Mais c'est en 1908 que Kitazawa innove dans la presse japonaise en publiant , un magazine en couleurs exclusivement réservé aux enfants. Devant le succès, il renouvelle l'expérience en 1914 en créant la revue dans laquelle il dessine "L'enfance de Toyotomi Hideyoshi". Ce succès allait marquer le marché des mangas pour longtemps. En 1914 paraît , en 1923 et en 1926 . En 1929, Kitazawa entreprend un long voyage en Europe, en Afrique et aux Amériques. De passage à Paris en 1929, il expose en présence de Léonard Foujita et y reçoit la Légion d'honneur.

À la fin de l'ère Meiji, à l'ère Taishō (1912-1926), dessine des mangas pour le quotidien "Asahi Shinbun". Il est l'un des inspirateurs du mouvement des « Nouveaux représentants progressistes du manga » qui introduit au Japon les "comics", entre autres "Bringing up Father" ("La famille Illico") de Geo McManus parait dans . Si à cette époque tous les mangas utilisent plus ou moins la bulle, il y a encore beaucoup de texte écrit dans les dessins. Le premier à généraliser l'emploi de la bulle est qui dessine accompagné de son écureuil dans le premier numéro de "Asahi Gurafu" en 1923.

C'est Okamoto qui invente le terme de et qui crée la première association de "mangaka" appelée en 1915, qui devient en 1923 le et en 1942 le avec pour premier président Kitazawa.

La satire et la caricature sont féroces envers le pouvoir en place et, en 1925, le gouvernement établit une censure en promulguant une « Loi de préservation de la paix ». La presse japonaise devient « politiquement correcte » mais la publication de mangas se développe. Des magazines féminins comme ou publient aussi des mangas à destination de leurs lectorats ou pour des mères de familles qui lisent ces mangas à leurs enfants. 

À partir de la guerre sino-japonaise, et comme plus tard aux États-Unis ou en Italie, la presse, y compris les mangas, se met au service de l'état pour soutenir l'effort de guerre. Ainsi le très militariste de nous montre un chien paresseux engagé dans l'armée impériale, première série longue. C'est comme cela que les Japonais lisent aussi les aventures de de , qui déjoue toutes sortes de conjurations étrangères, et celles de Dankichi dans de . Ce seront les séries les plus populaires au Japon jusqu'au milieu des années quarante pendant lesquelles toute la presse ainsi que toutes les activités culturelles et artistiques subissent la censure du gouvernement militaire, ce dernier n'hésitant pas à mobiliser ces milieux à des fins de propagande.

Sous l'occupation américaine, les "mangakas" d'après-guerre subissent l'énorme influence des comic strip qui sont alors traduits et diffusés en grand nombre dans la presse quotidienne japonaise. "Sazae-san" de Machiko Hasegawa sera le premier grand succès d'après-guerre. Cette génération a vu leurs villes rasées, leurs pères vaincus, leur empereur déchu de sa divinité, et ce que leurs idéologies véhiculaient jeté dans les poubelles de l'Histoire par les vainqueurs. Les bombardiers B29, les avions invulnérables, et les jeeps armées apparaissent dans la vision des futurs "mangaka" encore adolescents. Après sa défaite, le Japon s'est reconstruit au prix d'un lourd sacrifice ; d'ailleurs dans les mangas apparaît souvent la devise de Shōnen Jump : « Amitié, effort, victoire » (devise choisie par les lecteurs).

L'un d'entre eux, influencé par Walt Disney, révolutionnera le genre et donnera naissance au manga moderne : il s'agit du célèbre Osamu Tezuka. C'est en effet Tezuka qui introduira le mouvement dans la bande dessinée japonaise par des effets graphiques comme des traits ou des onomatopées soulignant toutes les actions comportant un déplacement, mais aussi et surtout par l'alternance des plans et des cadrages comme il est en usage au cinéma, rompant ainsi avec une tradition théâtrale, les personnages étant jusque-là toujours représentés en pied, à égale distance et au centre de l'image. On considère généralement , parue en 1947, comme marquant le début du manga moderne.

L'animation étant la véritable passion de Tezuka, il réalisa la première série d'animation japonaise pour la télévision en janvier 1963, d'après l'une de ses œuvres : , plus connue en France sous le nom d"'Astro, le petit robot". Finalement, le passage du papier au petit écran devint courant et l'aspect commercial du manga prit de l'ampleur. Tezuka bouleversa le mode d'expression du manga, en explora les différents genres – alors principalement infantiles – et en inventa de nouveaux. Il inspira de nombreux artistes tels que le duo Fujiko Fujio ("Obake no Q-tarō ", "Doraemon"), Fujio Akatsuka ("Tensai bakabon") et Shōtarō Ishinomori ("Cyborg 009", "Kamen Rider") qui se succédèrent au Tokiwasō, voire Leiji Matsumoto ("Galaxy Express 999").

Les années 1960 voient l'émergence de mangas plus dramatiques dans lesquels sont abordés des sujets plus « sérieux » et réalistes, appelés "gekiga". Initié par Yoshihiro Tatsumi et Takao Saitō ("Golgo 13"), le style influencera notamment Sampei Shirato ("Ninja bugeichō", "Kamui den"), Shigeru Mizuki ("Kitaro le repoussant") et le duo Tetsuya Chiba/Asao Takamori ("Ashita no Joe"), la plupart de ces auteurs participant au magazine d'avant-garde "Garo".

En 1964 naît l', qui décerne des prix annuels à partir de 1972.

Dans les années 1970, le manga pour filles, écrit par des femmes ("shōjo") se développe à l'initiative du groupe de l'an 24, notamment Moto Hagio ("Poe no ichizoku ") et Keiko Takemiya ("Kaze to ki no uta"), puis de Riyoko Ikeda ("La Rose de Versailles"), Suzue Miuchi ("Glass no Kamen"), et Yumiko Igarashi et Kyoko Mizuki ("Candy Candy"). Mettant en avant les relations psychologiques des personnages, il se détache des mangas pour garçons ("shōnen").

En 1985, Tezuka Osamu reçoit le prix culturel de Tokyo, et en 1990, un an après sa mort, le Musée d'art moderne de Tokyo lui consacre une exposition. Cet événement marque l'introduction du manga dans l'histoire culturelle japonaise. 

Ainsi, les mangas « grandissant » en même temps que leurs lecteurs et se diversifiant selon les goûts d'un public de plus en plus important, l'édition du manga représente plus d'un tiers par ses tirages, et près d'un quart par ses revenus, de l'ensemble de l'édition japonaise. En 2008, sur 3,2 milliards de publications vendues au Japon ( milliards de yens), on comptabilisait 669 millions de magazines de manga (21 % des publications) et 478 millions de recueils de manga (15 %), pour un chiffre d'affaires respectif de 211 et 237 milliards de yens (22 % des ventes totales), chiffre relativement stable depuis le début des années 1990. Les hommes de moins de 30 ans lisent environ six mangas par mois, contre trois pour les femmes. La vente de mangas numériques représentait déjà en 2008 3/4 des ventes de livres électroniques avec 35 milliards de yens.

Le manga va plus loin, il en existe des jeux de cartes, des jouets, des jeux vidéo, des films d'animation et des films; ces derniers pouvant même être à l'origine d'un manga (comme c'est le cas avec Pokémon qui était à l'origine un jeu vidéo). Il est devenu un véritable phénomène de société puisqu'il touche toutes les classes sociales et toutes les générations, traitant de tous les thèmes imaginables : la vie à l'école, celle du salarié, le sport y compris cérébral tel le jeu de go, l'amour, la guerre, l'épouvante, jusqu'à des séries plus didactiques comme la littérature classique, l'économie et la finance, l'histoire, la cuisine et même le code de la route, dévoilant ainsi ses vertus pédagogiques.

La génération des baby-boomers français a pu lire de la bande dessinée franco-belge pendant toute son enfance et son adolescence. , s'est jetée sur le manga qui, selon Jean-Marie Bouissou, a vocation à être un produit global en proposant beaucoup de séries propres à intéresser les clientèles les plus diverses par l'âge, le sexe et les goûts, à la différence de la bande dessinée française mais aussi des comics américains.
Il existe une volonté de la part du Japon de faire découvrir au reste du monde sa bande dessinée. À la fin de l'année 1970, une rétrospective sur les mangas est organisée au cœur même de Paris, au drugstore Publicis de St-Lazare, à la demande de l'ambassade du Japon si on en croit l'article sur les mangas paru dans le numéro 21 de la revue Phénix de 1972 et rédigé par Claude Moliterni et Kosei Ono.

La bande dessinée japonaise est très peu présente dans le monde francophone avant 1978 : quelques planches de "Bushidou Muzanden" d'Hiroshi Hirata dans "Budo Magazine Europe" publication consacrée au judo, en 1969, plusieurs mangas sur les samouraïs traduits et publiés au début des années 1970 dans la nouvelle formule de "Budo magazine Europe" et l'article « La Bande dessinée japonaise » de Claude Moliterni et Kosei Ono qui lui est consacré en 1972 dans "Phénix". En 1978, Atoss Takemoto publie le premier numéro du "Cri qui tue", fanzine d'assez mauvaise qualité (impression, choix des bandes). On y retrouve dans les six numéros qui paraissent jusqu'en 1981 "Golgo 13" de Takao Saito, "Le Système des Super Oiseaux" d'Osamu Tezuka, "Good bye" de Yoshihiro Tatsumi et des histoires de Shōtarō Ishinomori, Fujiko Fujio, Masashi Ueda. Toutes les planches sont adaptées au sens de lecture européen.

En 1979, Kesselring, associé à Takemoto, publie le premier album : "Le vent du nord est comme le hennissement d'un cheval noir" de Shōtarō Ishinomori. Le format choisi, supérieur à la norme européenne, met peu en valeur les particularités du format japonais, le lettrage est bâclé : comme le premier périodique, le premier album est un échec. En 1982, les éditions Télé-Guide, désireuses de profiter du succès de la série animée "Candy", publient avec succès la bande dessinée originelle de Yumiko Igarashi et Kyoko Mizuki dans les douze numéros de "Candy Poche". C'est pourtant dans les années 1980 le seul manga adapté en dessin animé à faire l'objet d'une traduction, les autres adaptations étant le fait de studios français, afin d'éviter de payer des droits d'auteurs. 

En 1983, le premier volume de "Gen d'Hiroshima" de Keiji Nakazawa est publié par Les Humanoïdes associés dans la collection « Autodafé », dans une édition correcte, mais qui ne rencontre aucun succès. De même, l’"Hiroshima" de Yoshihiro Tatsumi édité par Artefact en 1983 ne trouve pas son public. Les éditeurs sont alors refroidis par l'expérience et, dans un contexte de récession, plus aucune bande dessinée japonaise n'est éditée en album jusqu'à "Akira", hormis en 1989 chez Albin Michel le premier tome des "Secrets de l'économie japonaise en bandes dessinées" d'Ishinomori. Du côté des périodiques, le succès n'est pas plus au rendez-vous. Les premiers mangas pornographiques sont traduits, avec la publication chez Idéogram dans les onze numéros de la revue "Mutant", de janvier 1985 à janvier 1986, d"'Androïde", de Sesaku Kanō et Kazuo Koike et celle dans "Rebels" (juin 85) à 9 (janvier 86) de "Scorpia" de M. Yuu et K. Kazuya.

L'absence de traduction de ce que les spécialistes savent être le premier marché de la bande dessinée suscite cependant les interrogations de Thierry Groensteen en 1985 et la publication de divers articles dans "Les Cahiers de la bande dessinée". En septembre 90 naît "Mangazone", premier fanzine d'information sur la bande dessinée japonaise.

À partir de mars 1990, Glénat décide de traduire et publier "Akira", de Katsuhiro Otomo, en fascicules, d'après l'édition colorisée en Amérique. Bien que la série n'ait pas d'animé pour la porter, le renouvellement massif des codes du manga qu'introduit cette œuvre permet au succès d'être cette fois au rendez-vous, et l'édition cartonnée en couleur voit le jour dès la fin de l'année. "Mangazone" en profite pour devenir un magazine tenu par des professionnels, tiré à . La qualité de la revue fait débat et en mars 1990 naissent les fanzines "Yamato" et "Protoculture addict". En décembre 1990, le premier volume de "Gen d'Hiroshima" fait l'objet d'une nouvelle édition chez Albin Michel, sous le titre erroné de "Mourir pour le Japon". En 1991, alors que la suite d"'Akira" est publiée, "Rêves d'enfants", autre série d'Otomo, est éditée en 1991 par Les Humanoïdes associés. À la fin de l'année, un premier tome de "City Hunter" est publié. Cependant, seul "Akira" trouve alors son public. En mars naît "AnimeLand", fanzine luxueux qui remplace "Mangazone" comme référence francophone.

Alors que de plus en plus de voix s'élèvent pour protester contre les animes, toujours plus présents dans les programmes jeunesse, Glénat, une fois "Akira" achevé, publie d'autres mangas originaux d'animes à succès : "Dragon Ball" d'Akira Toriyama à partir de février 1993, "Ranma ½" de Rumiko Takahashi en février 1994. La réussite de l'entreprise permet à Glénat de traduire d'autres mangas, liés ou non à un anime : "Appleseed" de Masamune Shirow à partir de juin 1994, puis "Orion" du même auteur en septembre, "Crying Freeman" de Ryōichi Ikegami en janvier 1995, "Dr Slump" de Toriyama en février, "Gunnm" de Yukito Kishiro à partir de mars, "Sailor Moon" de Naoko Takeuchi dès le 30 février.

D'autres éditeurs traditionnels commencent à s'intéresser au manga. Casterman publie d'abord dans sa collection « Manga », créée en janvier 1995 deux bandes dessinées créées au Japon par des auteurs français ("Kiro" d'Alex Varenne puis en septembre "Au Nom de la famille" de Jerome Charyn et Joe Staton) avant de publier en septembre "Gon" de Masashi Tanaka, "L'Habitant de l'infini" d'Hiroaki Samura, et "L'Homme qui marche" de Jiro Taniguchi, premier manga d'auteur publié en France. « Casterman manga » accueille de nouveaux titres de qualité jusqu'en 1999, avant d'être remplacée par des collections plus spécialisées par la suite. Dark Horse France publie "" de de janvier 1995 à janvier 1996. Dargaud se lance également en créant la collection Kana avec "Angel Dick" puis "Armagedon" de la coréenne Hyun Se Lee.

Des éditeurs spécialisés naissent également : Samouraï Éditions, qui publie des mangas érotiques à partir de 1994 ("Ogenki Clinic" d'Inui Haruka) puis des mangas plus traditionnels l'année suivante ("Vampire Miyu" de et ), l'éphémère Star Comics en janvier 1995 avec "Takeru" de Buichi Terasawa, Kraken en avril (avant de disparaître l'année suivante) avec l'ambitieux "Shang Hai Kaijinzoku" de Takuhito Kusanagi puis "Les Élémentalistes" de Takeshi Okazaki ou encore "Vaelber Saga" de Nobuteru Yūki. Tonkam, qui se lance en juin 1995 avec "RG veda" de CLAMP devient le premier grand éditeur français spécialisé dans le manga. C'est également le premier à les publier dans le sens de lecture japonais, à la fois pour des raisons de coût et d'intégrité de l'œuvre, disposition qui devient rapidement la norme, sauf dans quelques cas particuliers (comme la collection « Écritures » de Casterman).

La vague est lancée : "Animeland" devient en 1996 avec son vingt-deuxième numéro le premier magazine consacré à l'animation et aux mangas diffusé en kiosque, de plus en plus d'éditeurs se joignent aux précurseurs, tandis que d'autres séries à grand succès sont traduites : de 2 en 1994, ce sont plus d'une quarantaine de séries différentes qui sont publiées ou lancées en 1996 (pour 105 albums, par Tonkam, Glénat et J'ai lu principalement), parmi lesquelles "Nicky Larson" de Tsukasa Hōjō, "" de Koji Inada, Riku Sanjo et Yuji Horii, "Ghost in the Shell" de Shirow, "Amer Béton" de Taiyō Matsumoto, "Bastard !!" de Kazushi Hagiwara, "Le Roi Léo", "Astroboy" et "Blackjack" d'Osamu Tezuka. En 1997 apparaissent "Détective Conan" de Gosho Aoyama, "3×3 Eyes" de Yūzō Takada, "Sanctuary" de Ryōichi Ikegami et Sho Fumimura, "Ah! My Goddess" de Kōsuke Fujishima, en 1998 "Neon Genesis Evangelion" de Yoshiyuki Sadamoto, "Cat's Eye" de Tsukasa Hojo, "Kenshin le vagabond" de Nobuhiro Watsuki, "Yu-Gi-Oh!" de Kazuki Takahashi ainsi que les premières réalisations de Naoki Urasawa, en 1999 "Ken le Survivant" de Tetsuo Hara et Buronson, "Captain Tsubasa" de Yōichi Takahashi, "Cardcaptor Sakura" de CLAMP, "Slam Dunk" de Takehiko Inoue.

Le marché continue à croître à un rythme soutenu : 151 albums en 1998, 200 en 1999, 227 en 2000, 269 en 2001. À partir de 1999, Kana s'affirme comme le quatrième grand acteur du secteur. Cependant, alors qu'à cette date les principales séries japonaises à succès des années 1980 et 1990 sont traduites, et qu'elles atteignent parfois d'enviables chiffres de vente (au début du millénaire exemplaires par volume de "Dragon Ball", environ pour les séries les plus populaires), que les magazines dédiés vont commencer à se multiplier, que les rencontres d'amateurs ont de plus en plus de succès, qu'Internet va favoriser le développement des mangas, le monde de la bande dessinée tel que le laisse percevoir le Festival d'Angoulême laisse peu de place à cette émergence, et les éditeurs alternatifs lui restent globalement indifférents, laissant inconnu du public le large patrimoine de bandes dessinées d'auteur japonaises, hormis Taniguchi. Des séries plus récentes sont alors traduites, et remportent également un grand succès : en 2000 "Hunter × Hunter" de Yoshihiro Togashi, "Shaman King" de Hiroyuki Takei, "One Piece" d'Eiichirō Oda, en 2001 "Great Teacher Onizuka" de Tōru Fujisawa, "I¨s" de Masakazu Katsura, "Samurai deeper Kyo" d'Akimine Kamijyō, "Angel Sanctuary" de Kaori Yuki, "Monster" de Naoki Urasawa, en 2002 "Love Hina" de Ken Akamatsu, "Gunnm Last Order" de Kishiro, "Fruits Basket" de Natsuki Takaya, "Naruto" de Masashi Kishimoto, "Bleach" de Tite Kubo.

Le premier festival de bande dessinée et d'animation japonaises, la Japan Expo, est créé en 1999. Il se tient au centre des nouvelles industries et technologies (CNIT) en 2003 et 2004, puis au parc des expositions de Paris-Nord Villepinte, où il attire en 2012 plus de personnes.

À partir de 2002, la hausse de la part des bandes dessinées asiatiques dans le marché des nouveautés s'accélère, à la fois en valeur absolue (377 en 2002, 521 en 2003, 754 dont 614 mangas en 2004) et relativement (25 % en 2002, 30 % en 2003, 36 % en 2004, 42 % en 2005, 44 % en 2006, environ 42 % en 2007). Les mangas restent les bandes dessinées asiatiques les plus vendues (), leur coût par tome plus faible et leur périodicité plus régulière que celle des bandes dessinées occidentales leur permet de toucher un public fidélisé, d'autant que les éditeurs peuvent sélectionner les bandes dessinées qui ont déjà passé l'épreuve du public au Japon. La plupart créent des collections dédiées, voire tentent de lancer des mangas « à la française ». En 2003, le tirage des quinze plus grandes séries oscille entre et ("Yu-gi-oh", et "Naruto" en 2004) exemplaires, en 2007 "Naruto" est imprimé à exemplaires, "Death Note" à , et le fonds reste attractif (avec "Dragon Ball" surtout). En valeur, le marché est détenu à 80 % par Pika, Kana et Glénat. En 2003, pour la première fois, un manga obtient un prix au festival d'Angoulême : "Quartier lointain", de Taniguchi, pour le prix du scénario. C'est un début de reconnaissance.

En 2005, la forte part des mangas édités dans le marché de la bande dessinée francophone a fait écrire à Gilles Ratier que 2005 avait été en France « l'année de la mangalisation », sans qu'il s'en offusque, . bandes dessinées asiatiques (soit 42 % des nouveautés) sont en effet éditées en 2005 dont 937 mangas, et en 2006 (soit 44 % des nouveautés), dont mangas. Les tirages à la nouveauté des bandes dessinées japonaises les plus populaires n'ont plus rien à envier à ceux des bandes dessinées traditionnelles populaires : exemplaires pour "Naruto", pour "Samurai deeper Kyo" ou "Fullmetal Alchemist" (de Hiromu Arakawa, traduit à partir de 2005), pour "Gunnm Last Order", "Hunter × Hunter", "Yu-Gi-Oh!", "Fruits Basket" et "Shaman King", pour "Neko Majin" de Toriyama, pour "Air Gear" (d'Ōgure Ito, traduit à partir de 2006) et "One Piece" d'Eiichirō Oda. Début 2006, la France est, avec plus de 13 millions d'exemplaires annuels, le plus gros « consommateur » de mangas au monde après le Japon et devant les États-Unis. Les mangas représentent 26 % du chiffre d'affaires de la bande dessinée et constituent la plus forte progression derrière la fiction jeunesse, se plaçant en deuxième position des secteurs de l'édition les plus dynamiques. De plus, sur le marché français, seulement dix séries mangas concentrent 50 % des ventes. 

Parallèlement à ce succès populaire, les maisons d'éditions commencent à développer l'édition patrimoniale : Vertige Graphic réédite "Gen d'Hiroshima" et publie Yoshihiro Tatsumi, un des pères du "gekiga" à partir de 2003, Ego comme X traduit "L'Homme sans talent" de Yoshiharu Tsuge en 2004, Cornélius publie Shigeru Mizuki depuis 2006, avec succès puisque "NonNonBâ" obtient le Prix du meilleur album à Angoulême en 2007, respectabilité qui avalise la forte pénétration de la bande dessinée japonaise sur le marché français. La bande dessinée d'auteur pour adultes, représentée d'abord par Jirō Taniguchi et Naoki Urasawa, se développe à partir de 2002, tandis que les jeunes auteurs les plus novateurs le sont, hormis Taiyō Matsumoto publié dès 1996, à partir de 2005. L'intérêt pour le manga pousse des éditeurs à s'intéresser également aux bandes dessinées coréenne et chinoise.

En 2010, Kana et Glénat sont toujours leaders dans le domaine du manga, fort du succès de "Naruto" et "One Piece" qui sont les bandes-dessinées les plus vendues de l'année toutes catégories confondues, mais ils perdent du terrain à l'avantage de maisons d'édition comme Pika Édition, Ki-oon ou Kazé, qui se fait une grande place dans le marché depuis son rachat en 2009 par Shōgakukan et Shūeisha. Certains éditeurs comme Tonkam, Panini ou encore Delcourt enregistrent des baisses très conséquentes, tandis que la petite maison d'édition Doki-Doki enregistre la plus grande progression de l'année. "Pluto", "Bakuman." et "Monster Hunter Orage" (par Hiro Mashima) sont les trois nouvelles licences les plus populaires en 2010.

En 2011, le trio de tête reste identique, mais Glénat passe largement en tête devant Kana, du fait de la montée des ventes de "One Piece" et du rythme de parution plus lent (3 tomes par an) de "Naruto". Glénat affiche donc une forte hausse (+13,3 %), alors que ses deux concurrents directs Kana et Pika Édition affiche des baisses (-17 % pour Kana et -2,9 % pour Pika). Kurokawa, Kazé et Ki-oon continuent leurs progressions et représentent à eux trois environ 20 % des ventes de manga en France, avec notamment la fin de "Fullmetal Alchemist" ou le novateur "Les Vacances de Jésus & Bouddha" pour Kurokawa, l'arrivée de titres comme "Blue Exorcist", "Beelzebub" ou "Toriko" pour Kazé et de "Judge", "Pandora Hearts" ou "Bride Stories" chez Ki-oon, mais également avec l'arrivée d'un catalogue pour les enfants plus important, avec notamment "Pokémon Noir et Blanc" ou "". Depuis le rachat de Tonkam et Soleil Manga par Delcourt, le groupe représente environ 10 % des ventes de manga en 2011, mais les trois maisons d'édition continuent leur chute. Seuls les petits éditeurs Taifu Comics et Doki-Doki sont à la hausse.

Pour autant, le secteur du manga a vu sa croissance s'arrêter et ses ventes diminuer depuis les années 2010. De fait, après avoir plus que quadruplé entre 2001 et 2008, les ventes des mangas en France ont marqué un recul de 15 % en volume entre 2008 et 2011. Après deux années propices à la stagnation, 2012 marquait cependant une hausse remarquable du nombre de séries asiatiques sur le sol français. Mais la tendance des sorties s'est inversée en 2013, avec parus (contre en 2012 et en 2011). Cependant, dans un marché général de la bande dessinée qui, pour la première fois depuis au moins 17 ans, est en baisse (-7,3 % de sorties), les sorties asiatiques se maintiennent et représentent une part des nouveautés légèrement plus importante (40,7 % du marché, contre 39,4 % l'année précédente). Cette légère baisse s'accompagne en revanche de ventes qui continuent de chuter de manière importante. En effet, alors que le marché général de la bande dessinée a bénéficié d'une hausse de 1,4 % en valeur sur la fin de l'année 2013 (porté par les best-sellers évènementiels que furent les derniers albums d"'Astérix", de "Blake et Mortimer" ou du "Chat"), le secteur du manga accuse une nouvelle chute de -8,5 % de son chiffre d'affaires, et ce alors qu'il avait déjà connu un recul de -3,8 % l'année précédente.

Comme pour les années précédentes, le marché du manga reste marqué par une très forte concentration, tant au niveau des séries à succès (une dizaine de titres représente à elle seule la moitié des tirages de l'ensemble du marché) que des éditeurs. Ainsi, les dix premières séries les plus vendues en 2013 (qui sont, dans l'ordre décroissant d'importance, "Naruto", "One Piece", "Fairy Tail", "Black Butler", "Bleach", "King's Game", "L'Attaque des Titans", "Judge", "Prophecy" et "Soul Eater") sont portées par seulement cinq éditeurs que l'on identifiera sans surprise comme faisant partie des premiers groupes éditoriaux du secteur : Glénat, Pika Édition, Kana, Ki-oon et Kurokawa. Bien mieux, en 2013, les trois plus importants leaders éditoriaux du marché que sont Glénat, Pika Édition et Kana ont cumulé à eux seuls près de 60 % des ventes.

Pour une grande partie des séries à succès des années 2000, les rythmes de parution en France rattrapent de plus en plus ceux du Japon et se font donc plus lents tandis que les nouveaux lecteurs se font de plus en plus rares, eu égard au nombre conséquent de tomes existants à rattraper ("Fairy Tail" et "Bleach" en comptent respectivement plus de 40 et 60 tandis que "Naruto" et "One Piece" ont déjà atteint les 70 tomes).

Or, au Japon, certaines grandes séries emblématiques des années 2000, arrivées à maturité, commencent à perdre plus de lecteurs qu'elles n'en gagnent. Ainsi, au sein du classement des quinze premières séries au Japon, "Naruto" est tombé à la cinquième place du fait son rythme de publication moins rapide, et surtout parce que la série n'attire plus autant de nouveaux lecteurs, voire lasse certains anciens lecteurs, au point de connaître une chute de ses ventes d'environ 15 %. Il en va de même pour "Hunter × Hunter" ( du fait de son rythme de publication irrégulier), "Fairy Tail" (), "Sawako" (), "Gintama" (), "Toriko" () ou encore "Bleach" (), qui baissent tous au profit de la nouvelle vague de titres emmenée par "L'Attaque des Titans", "Kuroko's Basket", "Magi", "Silver Spoon", disposant tous d'adaptations animées de qualité et mieux étudiées pour soutenir leurs ventes. De même, si "Fairy Tail" a toujours du succès en France, on constate que le premier volume n'est que au sein du classement par volume, et que la série a vu ses ventes baisser de 8 %, après avoir déjà connu une baisse de 12 % l'année précédente.

Les mangas japonais sont très rarement édités directement sous forme de volumes reliés ; ils paraissent tout d'abord de manière découpée dans des magazines de prépublication, des revues spécialisées qui leur sont consacrées.

Les rythmes de publication de ces magazines peuvent beaucoup varier, allant de l'hebdomadaire aux publications mensuelles voire trimestrielles. Les séries y sont souvent publiées par chapitres d'une dizaine à une vingtaine de pages. À l'intérieur d'un même magazine, le papier peut parfois changer de couleur, afin de distinguer rapidement les différentes séries les unes des autres.

Ces magazines, bon marché, s'écoulent en grand nombre, c'est-à-dire en millions d'exemplaires pour certains, et se lisent un peu partout. On en retrouve parfois abandonnés dans les trains, les rames de métro, les cafés Ils alimentent un système de lectures multiples : un même exemplaire serait lu par plusieurs personnes.

Principalement en noir et blanc, les premières pages des magazines sont souvent en couleurs, mettant tour à tour à l'honneur l'une de leurs séries vedettes à cet emplacement, souvent de manière à ce que le chapitre en cours soit un début de volume.

Ce n'est que dans un deuxième temps, lorsqu'un manga rencontre un certain succès, qu'il est édité en volumes reliés, similaire à ceux que l'on trouve en France, entamant ainsi une deuxième carrière. Ces volumes reliés sont appelés "tankōbon" (format poche), "bunkōbon" (format plus compact, utilisé pour des rééditions) ou "wide-ban" (format « luxe », plus grand que le format poche). En l'absence de succès auprès du public, une série pourra voir sa parution arrêtée, le "mangaka" étant prévenu peu avant pour trouver une fin rapide à son histoire et permettre une éventuelle parution en volumes. Certaines revues décident désormais de la fin d'une série dès la fin du second volume, conduisant à des histoires finales en quatre volumes. Dans certains cas, un "manga" à succès peut se voir adapté en "anime" (dessin animé).

Les magazines de prépublication hebdomadaires incluent notamment ces titres populaires : 

Certains titres atteignent couramment les 400 pages hebdomadaires. "Weekly Shōnen Jump" était vendu en 1994 à 6 millions d'exemplaires, mais son tirage pour 2008 s'établissait à 2,8 millions d'exemplaires.

Techniquement parlant, les mangas sont presque toujours en noir et blanc ce qui est directement lié au système de prépublication en magazine bon marché.

Les mangas comptent souvent un nombre important de pages (planches). À titre d'exemple, une bande dessinée européenne contiendra une quarantaine de planches quand le manga en comptera plus d'une centaine, voire plus de deux cents. Par ailleurs, le manga est le plus souvent une série en plusieurs volumes. Finalement, le nombre total de planches racontant une histoire dans un manga est beaucoup plus élevé que dans une bande dessinée européenne (même s'il s'agit d'une série). Ceci affecte par conséquent beaucoup la structure du récit et sa narration. D'où les techniques propres au manga.

Le dessin, en général, est moins « statique » que dans les bandes dessinées occidentales. Le manga utilise un découpage temporel proche de celui du cinéma, adoptant souvent ses cadrages et utilisant une décomposition similaire du temps et de l'action. On retrouve souvent une mise en scène comme la plongée ou la contre-plongée.

De nombreux codes graphiques sont utilisés pour symboliser l'état émotionnel ou physique d'un protagoniste. Les personnages ont souvent de grands yeux, ce qui permet de renforcer l'expressivité du visage. L'étonnement est souvent traduit par la chute du personnage ; l'évanouissement, par une croix remplaçant les yeux. Pour traduire l’excitation sexuelle chez un personnage masculin, un saignement de nez plus ou moins important est provoqué. Dans le manga "City Hunter" (connu à la télévision française sous le nom "Nicky Larson"), la colère de Kaori ("Laura") est souvent traduite par l'apparition inopinée d'une énorme massue qu'elle assène sur la tête de son partenaire (ce gag est si répandu dans les mangas qu'un univers parallèle où seraient stockés les marteaux a été inventé).

Il y a également une utilisation fréquente d'onomatopées relatives aux mouvements, actions ou pensées des personnages. Notons au passage que le japonais est beaucoup plus riche que le français en onomatopées et que leur champ d'application est plus large, incluant des concepts surprenants tels que l'onomatopée du sourire (), du silence () ou encore du scintillement (, d'où le nom de Pikachu).

Une particularité à noter est que la plupart des personnages ont souvent des traits occidentaux, au-delà du simple tracé des grands yeux des personnages. Un samouraï roux, un exorciste aux yeux bleus ou une écolière blonde n'ont rien d'étonnant pour le lecteur japonais, même s'ils sont censés être japonais ou de culture japonaise. La simple nécessité de distinguer physiquement deux personnages ne suffit pas toujours à expliquer cet aspect de la narration, puisque certains "mangakas" choisissent de donner à tous leurs personnages un aspect purement japonais, sans que cela pose de problème de compréhension de l'histoire. 

Les décors des scènes sont parfois moins fouillés que pour une bande dessinée occidentale. Cela peut aller jusqu'à faire évoluer les personnages dans un décor blanc. Ce parti pris a pour conséquence de focaliser l'attention du lecteur sur l'histoire en général et sur les dialogues en particulier. On note ainsi une certaine résurgence de l'aspect théâtral. Enfin, les personnages ont souvent des attitudes expressives à outrance : la colère, la jalousie ou la gêne se montrent facilement, alors que cette attitude est plutôt mal vue dans la culture japonaise, où le calme et la retenue sont de rigueur dans les rapports sociaux. Le passage de l'absurde et du comique au sérieux ou au drame, sans aucune transition, fait également partie de la narration, sans jamais susciter d'interrogation de la part du lecteur qui accepte par avance cette convention de lecture. 

Une autre particularité est le jeu de l'auteur avec le lecteur. Ainsi, dans "Rough", on peut voir les personnages faire de la publicité pour d'autres mangas de l'auteur, ou bien ramasser des phylactères tombés sur le sol. De manière générale, on peut noter une plus grande liberté quant à l'interaction entre les dessins et leur support (jeu avec les cadres, personnages sortant des cadres) Dans les mangas destinés à la jeunesse, les kanji, caractères chinois ou sinogrammes, sont souvent accompagnés de "furigana" pour faciliter la lecture.

Les mangas sont traditionnellement classifiés en fonction de l'âge et du sexe du lectorat visé. Il existe six classes démographiques :


Ces classes démographiques sont indicatives ; de nombreux lecteurs ne les suivent pas, et certains mangas tentent de toucher plusieurs publics à la fois.

Ces démographies sont généralement réutilisées telles quelles par les éditeurs occidentaux afin de créer leurs collections, toutefois les stéréotypes de genre et le rapport à la violence et au sexe n'étant pas les mêmes au Japon et en Occident, il arrive que les éditeurs occidentaux changent la démographie-cible d'un manga, typiquement les "shōnen" romantiques sont reclassés en "shōjo". Quelques rares éditeurs occidentaux préfèrent quant à eux totalement ignorer la classification japonaise à l'instar d'Akata.

Les mangas reprennent les genres et registres littéraires usuels, du roman d'amour à l'horreur en passant par la science-fiction, et n'hésitent pas à les mélanger. En plus de cela il existe quelques genres typiques des mangas et de ses dérivés, ou dont le nom japonais a pris le pas sur le nom français auprès des éditeurs et des fans :


Les mangas peuvent aussi être classifiés en fonction de leur format de publication.

Le "One shot" est une histoire qui tient en un seul volume voire un seul chapitre. Le est un manga en quatre cases, similaire au "comic strip". Quant au "Webcomic", c'est un manga publié directement sur internet.

Souvent, les séries à succès sont adaptées en "anime", sous forme de séries télévisées mais aussi de jeux vidéo. Mais parfois, ce sont les animes qui sont utilisés pour créer des bandes dessinées, soit simplement inspirées de la version animée (comme c'est le cas pour "Neon Genesis Evangelion"), soit directement copiées à partir des images animées. Pour cela, on met en page des images extraites de l'œuvre souhaitée, sur lesquelles on ajoute du dialogue. Ces bandes dessinées particulières sont alors appelées "animekomikkusu" (Anime comics).

De nombreux mangas ont aussi été adaptés en "drama" (série télévisée), dont certains sont très populaires comme "Hana yori dango".

Associés aux mangas, on trouve les "artbooks", recueils d'illustrations en couleur et d'images originales, incluant parfois des histoires courtes.
De même, du fait de la popularité grandissante des mangas, les produits dérivés sont de plus en plus nombreux : figurines, cahiers, calendriers, porte-clés, peluches, habits, costumes, accessoires La naissance de ces produits dérivés est généralement associée aux séries "Nonki na tōsan" (1924) et "Norakuro" (1931).

On trouve également des jeux de rôle développant un riche univers post-apocalyptique ou de fantasy tels que "Mekton Z", "Anima", "Final Fantasy" et "Manga BoyZ".

En France, de nombreux festivals appelés conventions ont fait leur apparition ces dernières années. Ces conventions sont des points de rassemblement pour les fans de mangas ou de culture japonaise moderne en général, proposant des projections, des jeux, des spectacles de "cosplay" et souvent complétées par un espace où se côtoient professionnels (magasins de livres et autres produits) et amateurs (clubs et associations exposant leurs propres œuvres). On compte parmi les conventions les plus connues : Cartoonist, Epitanime, Japan Expo, G.A.M.E. in Paris (France), Tokyo Zone (France), Polymanga (Suisse)




</doc>
<doc id="1942" url="https://fr.wikipedia.org/wiki?curid=1942" title="Microsoft">
Microsoft

Le siège social se situe à Redmond, près de Seattle (État de Washington) à l'ouest des États-Unis. Les meilleures ventes historiques sont portées par le système d’exploitation Windows et la suite bureautique Office qui alimentent à présent une politique de diversification. 

La vigoureuse stratégie commerciale menée par Microsoft (sigle boursier MSFT) à l'international s'appuie sur la « vente liée » : arsenal d'accords d'exclusivité passés avec les fabricants et distributeurs de matériels. Elle confère à la société une position dominante qui a imposé Windows sur la grande majorité des ordinateurs personnels, de bureau et portables. Désormais, l'objectif d'avoir « un ordinateur tournant sous Windows sur chaque bureau et dans chaque maison » s'avère presque atteint avec plus de 90 % de parts de marché dans le monde.

Dans le domaine de la téléphonie, la filiale Microsoft Mobile issue du rachat de Nokia Mobile OY est responsable de la production et du développement technologique des smartphones Lumia.

Microsoft est, cependant, présent dans d'autres secteurs activités : comme le moteur de recherche Bing, les périphériques (claviers, souris) et les consoles de jeu vidéo Xbox.

Le nom « Microsoft » est un mot-valise issu de la contraction de « », il apparaît pour la première fois dans un courrier adressé à Paul Allen par Bill Gates en juillet 1975, sous la forme « Micro-soft ». Le nom sous sa forme actuelle sera déposé le 26 novembre 1976 dans l’État du Nouveau-Mexique. Le siège social de cette société est situé à Redmond dans l’État de Washington ; ainsi Microsoft est parfois appelée « la firme de Redmond ».

Cette entreprise est surtout connue pour ses logiciels, comme les systèmes d’exploitation MS-DOS et Windows, la suite bureautique Microsoft Office, ses outils de développement, ses jeux vidéo, également pour divers produits matériels (périphériques pour PC, consoles de jeux Xbox, smartphones Lumia, et pour ses services Internet. "(voir la liste des activités pour plus de détails)" Microsoft domine depuis plusieurs années le marché des systèmes d’exploitation grand-public. Son système d’exploitation Windows, régulièrement réédité, s’est imposé comme un standard dans le domaine informatique.

Depuis le 27 mai 2010, Microsoft est la seconde capitalisation boursière du NASDAQ, derrière Apple. La société emploie environ personnes à travers le monde. Elle est dirigée, depuis le , par Satya Nadella qui succède à Steve Ballmer en qualité de directeur général. C'est le troisième à occuper ce poste.

En 2015, Microsoft est l'entreprise technologique qui a racheté le plus grand nombre de sociétés avec un total de 35 acquisitions, dépassant ainsi Google et Apple.

La société est née en sous le nom original de Micro-Soft, à Albuquerque, dans le Nouveau-Mexique, du besoin de deux étudiants américains, Bill Gates et Paul Allen, de formaliser la vente de l’interpréteur de langage informatique BASIC : Altair Basic adapté par eux deux et Monte Davidoff , pour ce qui est considéré comme le premier ordinateur personnel américain, l’Altair 8800, de la société MITS, avec le premier langage de programmation pour micro-ordinateur de l’histoire de l’informatique. La marque Microsoft (originellement , le trait d’union disparaissant quelques années plus tard) fut déposée le .

À l’origine, elle a été fondée pour développer et vendre des programmes informatiques BASIC pour l’Altair 8800, puis elle est devenue un des sous-traitants d'IBM. Microsoft a réussi à dominer le marché du système d’exploitation de l’ordinateur personnel avec MS-DOS au milieu des années 1980. Elle a pu ensuite s'affranchir d'IBM. L’introduction en bourse de la société, et l’envolée du prix des actions qui s’ensuivit, ont fait quatre milliardaires et environ parmi les employés de Microsoft.

Au cours de son histoire, la société a été critiquée pour abus de position dominante, parfois devant la justice américaine et européenne, ou pour des copies, ces dernières critiques s'étant atténuées après la de 1997.

Ce premier contrat de Microsoft représenta le véritable tour de force de Bill Gates, étant peut-être même plus important pour cette société que le rôle que jouera ensuite MS-DOS : contrairement à ce qui se faisait à l’époque, où les constructeurs achetaient aux éditeurs leurs logiciels avec tous les droits, Bill Gates et Paul Allen demandèrent de recevoir chacun trois mille dollars pour leur Altair Basic tout en restant propriétaires, et ne concédèrent qu’une licence à MITS, qui devait leur reverser trente-cinq dollars par exemplaire distribué. À titre indicatif le prix de vente de l’Altair 8800 étant de , la licence de Microsoft en représentait donc 8,8 %. C’est ainsi que le BASIC de Microsoft se retrouva dans deux micro-ordinateurs populaires introduits en 1977 : le PET de Commodore et le TRS-80 de Tandy.

Au cours de cette période, l'informatique évolue et dépend moins des « grands systèmes IBM », les constructeurs proposant une informatique plus décentralisée. En 1980, IBM s’apprêtant à lancer l’IBM PC, a demandé son BASIC (dont une version en mémoire ROM) à Microsoft. IBM a, par ailleurs, demandé à la société , dirigée par Gary Kildall, de lui fournir une version de son système d’exploitation CP/M.

L’histoire du contrat manqué par Gary Kildall est très controversée, cependant la version avancée par de nombreuses personnes, dont Tim Paterson, qui sera bientôt amené à travailler pour Microsoft, et la moins contestée, affirme que Kildall et son ancienne épouse, Dorothy McEwen, auraient refusé de signer un accord de confidentialité. De plus, ils ont refusé de modifier CP/M-86, ont demandé une redevance plus élevée, et surtout, le CP/M 86 étant totalement exempt de bugs, n’ont pas autorisé IBM à en modifier le codage.

IBM se tourna alors vers Microsoft, et voulut sous-traiter CP/M pour l’IBM PC. Le contrat avec Microsoft ne le permettant pas, celui-ci dépensa en pour une licence non exclusive pour un système d’exploitation, disponible à un stade expérimental, clone de CP/M, le QDOS (, système d’exploitation vite et mal fait). En , Microsoft engagea Tim Paterson pour porter QDOS sur l’IBM PC. En , Microsoft acheta tous les droits sur 86-DOS pour cinq mille dollars. IBM vit ainsi sauvé son projet d’IBM PC, mais au prix, qu’elle ignora, de la perte de sa position dominante : cet accord va permettre de réaliser des clones, et surtout, à IBM d’empocher des redevances sur le MS-DOS pour les correctifs qu’elle y a apportés (débogage). IBM avait détenu jusqu’à 66 % du marché des propriétaires ; sa part du marché des PC ne dépassa jamais un maximum de 21 %, atteint vers 1983, puis a décliné pour placer ce constructeur derrière Dell et Compaq (aujourd’hui intégrée par Hewlett-Packard), situation devenue marginale, inimaginable en 1981.

Microsoft vendit donc à IBM des licences pour ce système d’exploitation tiers, le 86-DOS, écrit par Tim Paterson de pour le microprocesseur Intel 8086 (le Intel 8088 qui équipa le Compatible PC est compatible avec le Intel 8086, et juste un peu moins rapide que ce dernier, et possède le même langage machine). Le MS-DOS devint ainsi l’un des trois systèmes d’exploitation disponibles pour l’IBM PC, avec CP/M 86 (Gary Kildall mis face à une concurrence a fini par se laisser convaincre) et PC/IX, une version d’UNIX ne possédant pas de mode de protection mémoire. Microsoft a acheté pour le logiciel qui va ériger son empire, même si elle a dû en compléter le développement pour répondre au cahier des charges d’IBM. Celui-ci fut édité sous le nom d’IBM PC-DOS 1.0 lors de l’introduction des IBM PC sur les marchés anglophones, le . Étant plus léger, moins cher et rendu plus disponible que ses deux concurrents, il devint rapidement le système d’exploitation installé d’office sur les IBM PC, puis plus tard des Compatible PC.

Comme pour le BASIC, Microsoft s’est réservé le droit de vendre des licences à d’autres constructeurs sous le nom de MS-DOS. Avec l’essor des Compatible PC dès le milieu des années 1980 (de Texas Instruments, Compaq, Seiko Epson, Thomson, Amstrad…), MS-DOS s’imposa rapidement et devient la plate-forme de référence professionnelle et, selon les points de vue, un monopole. En 1987, des milliers de constructeurs de compatible PC existaient dans le monde, et tous sans exception avaient un point de passage obligé qui était le système d’exploitation de Microsoft, le plus performant de tous, dans un souci, crucial pour le monde professionnel, d’unité, de standardisation, et de portabilité de tous les ordinateurs compatible PC.

D’abord simple environnement graphique pour MS-DOS, Windows est devenu entre 1993 et 2001 un système d’exploitation à part entière. Quelques coups de stratégie de marketing ne sont pas étrangers à ce succès, comme l’ajout de trois touches « Windows » sur les claviers afin de marquer celui-ci dans l’esprit du consommateur comme « étant fait pour Windows » et marginaliser ainsi le concurrent potentiel OS/2 développé par IBM, et codéveloppé initialement par Microsoft et IBM, jusqu’au divorce officiel entre les deux sociétés en . Selon Microsoft, un soin particulier a également été apporté aux questions d’ergonomie, et en particulier à la question des polices de caractères typographiques, dès les (Adobe Type Manager) et 3.1 (TrueType) de Windows.

Bien des années plus tard, Microsoft affirmera considérer son avance sur le plan de l’ergonomie comme l’atout qui permettra à Windows de survivre face à la concurrence libre de Linux et de KDE/GNOME. De fait, Microsoft consacre une part très importante de son budget aux questions d’ergonomie : un service observe toutes les hésitations d’utilisateurs novices, pour rendre les menus plus clairs, démarche fastidieuse et rarement réalisée sur des logiciels gratuits.

Windows est alors devenu le standard micro-informatique solidement soutenu par l’effet réseau indirect de milliers de logiciels et de périphériques matériels spécifiques à Windows qui ont nécessité des milliards de journées/hommes de développement.

Quelques-uns s’y risqueront : , avec un produit performant, rapide, et très riche en fonctionnalités, mais alors que beaucoup d’applications sont déjà portées ou en cours de portage sous Windows ( se reconvertira dans les interfaces de téléphones mobiles), ou Be Inc. avec le système d’exploitation BeOS, créé par Jean-Louis Gassée, ancien patron de la R&D d’. L’élaboration d’un produit capable de rivaliser avec Windows impliquerait de disposer, comme Microsoft, de revenus réguliers pendant les années nécessaires au développement d’un tel système. Or, le temps que celui-ci soit développé, Microsoft aurait déjà pris de l’avance, et éventuellement modifié les standards. La société Be propose tout de même BeOS, orienté d’emblée dans la gestion de la vidéo : ce système d’exploitation ne décollera jamais vraiment hors d’un cercle de passionnés. Et intentera d’ailleurs un procès antitrust contre Microsoft pour abus de position dominante, qui s’achèvera par un accord financier à l’amiable entre les deux sociétés.

En 1986 Microsoft lance l'environnement graphique Windows 1.0, peu après la sortie des produits concurrents GEM de Digital Research et Mac OS de Apple. La de Windows, rudimentaire - les fenêtres ne peuvent même pas se recouvrir -, n’inquiète pas sérieusement , qui ne réagit pas. La est une concurrence plus sérieuse, et intente un procès contre Microsoft pour plagiat, peu de temps après avoir intenté un procès similaire contre Digital Research. Cette dernière usa de tous les moyens légaux pour faire traîner le procès en longueur. perdit définitivement son procès contre Microsoft en appel en 1994. , en situation financière délicate, menaça ensuite d'attaquer à nouveau Microsoft, ce qui aboutit à un règlement à l’amiable en 1997, au moment du retour de Steve Jobs à la tête d’, appelé « de 1997, ce qui l'a sorti de grosses difficultés ».

Cet accord comprenait une prise de participation temporaire de Microsoft dans le capital d’ (à hauteur de de dollars soit 6 % du capital de "la pomme"), et l’obligation pour Microsoft de développer et pour Mac OS au moins jusqu’en 2002. En échange, abandonnait ses poursuites. Pendant douze ans et demi, Microsoft a conservé le record de la plus grande capitalisation de l'histoire boursière, avec le niveau de 620,58 milliards de dollars atteint le 30 décembre 1999. Le 20 août 2012, ce record est battu par Apple, à 622,10 milliards de dollars, grâce au succès populaire de ses appareils mobiles iPhone et iPad.

Un facteur important de l’adoption généralisée de Windows a été son rôle d’interface non seulement graphique, mais également de "pilotes". Sous MS-DOS, chaque éditeur de logiciel devait développer individuellement la gestion de tout le panel des milliers de périphériques compatible PC existants et à venir. Tâche colossale que les éditeurs de logiciels n’ont plus à gérer sous Windows dans la mesure où celui-ci se charge de gérer lui-même en standard tous les pilotes de périphériques de l’univers compatible PC. 

Le Campus Microsoft est le nom informel du siège social de Microsoft, situé au One Microsoft Way à Redmond, Washington. Microsoft s'est d'abord installée sur les lieux le 26 Février 1986, puis la société est devenue publique le 13 mars. Le siège a depuis connu de multiples expansions. On estime qu'il englobe plus de carrés () d'espace de bureau et . D'autres bureaux sont situés à Bellevue et Issaquah ( dans le monde). En janvier 2006, Microsoft a annoncé l'achat de campus de Redmond Safeco. (Anciennement un des principaux employeurs de Redmond, Safeco a commencé à consolider ses bureaux dans le quartier universitaire de Seattle à la Tour Safeco en 2005.)

En février 2006, Microsoft a annoncé qu'il avait l'intention d'étendre son campus de Redmond par carrés () à un coût de 1 milliard $ et a dit que ce serait pour créer un espace pour entre employés au cours des trois années suivantes.

L’histoire de Microsoft ne se résume cependant pas à celle de Windows. D’autres pans importants de l’activité de Microsoft ont permis sa croissance :

Le Seattle Times a rapporté, au début de Septembre 2015, que Microsoft avait embauché un cabinet d'architecture Skidmore, Owings & Merrill pour commencer une refonte de plusieurs milliards de dollars du campus de Redmond, en utilisant carré supplémentaire () autorisé par un accord avec la Ville de Redmond.

En août 2016, Microsoft effectue une émission obligataire d'un montant de 20 milliards d'euros permettant le financement de l'acquisition de LinkedIn. Cette émission obligataire est la cinquième plus grosse jamais lancée sur le marché du crédit américain.



Microsoft veut que Windows 10 harmonise l'expérience utilisateur et la fonctionnalité entre les différentes classes de périphériques, par exemple en corrigeant les lacunes de l'interface utilisateur qui ont été introduites dans Windows 8. Ou bien encore centraliser toutes les applications autour de Windows. Cette stratégie, Microsoft la nomme « One Windows » c'est-à-dire créer un seul écosystème regroupant les Windows pour smartphones, Xbox, les objets connectés ainsi que les autres projets de Microsoft comme le casque à réalité augmentée Hololens ou le tableau interactif de réunion Surface Hub.

Ces applications universelles sont faites pour fonctionner sur plusieurs plates-formes et les classes de périphériques, y compris les smartphones, tablettes, Xbox, et d'autres compatible avec Windows 10.Les applications universelles de Windows, partage du code entre les plates-formes pour s'adapter aux besoins de l'appareil et des taches demandée, pour synchroniser les données entre Windows 10 et les différents appareils qui sont distribués par le Windows Store qui est lui aussi unifié.

Les applications que Microsoft propose avec Windows 10 sont par exemple :



Windows est installé sur presque 90 % des ordinateurs personnels (postes "clients") vendus dans le monde, et dégage actuellement 87 % de marge bénéficiaire. Néanmoins, Windows perd petit à petit quelques parts de marché au profit de l'OS X d’. Linux a grignoté pour sa part une grande part du marché des serveurs, où l'ergonomie est un critère secondaire. Windows reste cependant en 2016 le produit le plus rentable de l’éditeur, suivi de près par la suite Microsoft Office. À ce jour la dernière version de Windows est Windows 10. C’est la première version de Windows qui est officiellement la même sur toutes les plateformes (ordinateur de bureau, ordinateur portable, tablette tactile, "smartphone", montre connectée, casque de réalité virtuelle) bien qu’il s’agisse d’une version modifiée du Windows 10 original. Microsoft a déclaré que c'est la dernière « grande » version disponible sur support physique et que les mises à jour suivantes seront disponibles gratuitement en OTA ("via" Internet) en tant que « "" » nommés "Windows 10.x.x". Une version spécifique, Windows 10 Mobile, sera disponible pour les appareils mobiles à processeurs ARM et dont l'écran fait moins de . Le système pour les tablettes de moins de aura donc une apparence proche de celle de Windows Phone alors que les tablettes de plus de auront obligatoirement une architecture 32 bits ou 64 bits et exécuteront Windows 10 pour PC. 
Bill Gates a appelé son service d'exploitation Windows (fenêtres), car l'innovation principale du système d'exploitation était l'emploi de fenêtres d'affichage.

La famille Windows Phone est un système d’exploitation mobile qui est le principal produit orienté mobiles de la firme. Il inclut des services de Microsoft comme Office, OneDrive, Xbox Live et Bing. Il intègre aussi des fonctionnalités axées vers les médias sociaux tels Facebook et Twitter. Comme Windows Phone est une nouvelle plate-forme, il n'existe aucune compatibilité avec les applications Windows Mobile. Les ventes des portables sous Windows Phone 7 sont limitées, avec seulement 3 % des ventes en 2011.

Les ventes de smartphones sous Windows Phone 8 partent dans une meilleure direction pour Microsoft. En effet en 2013, Windows Phone 8 dépasse les 10 % de parts de marché en Europe et se retrouve même devant iOS en Amérique latine. Windows 10 Mobile est la version mobile du système d'exploitation Windows 10 développé par Microsoft. Windows 10 Mobile succède à Windows Phone 8.1, et est conçu pour les smartphones et les tablettes tactiles d'une diagonale d'écran inférieure à , fonctionnant sur les architectures ARM, ainsi qu'IA-32. Sa sortie est prévue pour fin .

À l’origine, c’est une suite bureautique de l’éditeur, composée de nombreux logiciels dont le traitement de texte Word, le tableur Excel, le logiciel de présentation PowerPoint, le logiciel de publication Publisher, l’outil de communication et agenda Outlook et la base de données . Depuis 2003, la suite Office s’est largement étendue, avec de nombreux logiciels serveurs comme ou 
La suite Office est aussi éditée pour le système OS X d’Apple, mais avec un nombre de logiciels réduit. Office est un des logiciels les plus rentables de l’éditeur.

Dans les nouvelles versions d'Office, Microsoft axe sa stratégie sur des applications qui hébergent les données dans le Cloud, avec notamment la sortie de la licence Office 365. Cet outil en ligne permet d'avoir accès à de nombreuses ressources, telles que la suite Office 2016 (version en ligne ou locale), SharePoint, Office Online, OneNote, OneDrive, etc., en fonction du niveau de licence acquis.

Office Online, est une version en ligne gratuite de Microsoft Office accessible depuis un navigateur web, et dont certaines fonctionnalités sont toutefois réduites par rapport aux logiciels de la suite Office installés sur un ordinateur.
Office Online regroupe ainsi les services en ligne "Word Online, Excel Online, PowerPoint Online, OneNote Online mais aussi Outlook.com". La page d'accueil d'Office.com vous permet également d'accéder à vos espaces "Contacts", "Calendrier" et OneDrive. Il combine les fonctionnalités traditionnelles d'Office et des fonctions de co-création en temps réel, de sorte qu'il est possible de collaborer gratuitement avec vos proches et amis sur des documents texte, présentations, feuilles de calcul et bloc-notes partagés.

Microsoft propose aussi un drive qui se nomme Microsoft OneDrive qui est un ensemble de services en ligne : stockage et applications Word, Excel, PowerPoint et OneNote, dont les fonctionnalités sont toutefois réduites par rapport aux logiciels installés sur un ordinateur. Ce service a été créé en 2007 et a porté les noms Windows Live Folders, Windows Live SkyDrive, SkyDrive et enfin son nom actuel depuis janvier 2014.

Le service peut s'utiliser de deux manières : à travers un navigateur web, en téléchargeant des fichiers sur un serveur, en les récupérant sur son ordinateur au besoin et en les partageant avec des amis ou avec tous les internautes ; ou à travers le logiciel OneDrive qui permet une synchronisation rapide entre OneDrive et les supports informatiques compatibles. Ce service est une manifestation du concept de cloud computing.

Début 2012, Microsoft a annoncé l'intégration de SkyDrive dans Windows 8. En avril 2012, la capacité de stockage est abaissée à et devient payante au-delà, avec des offres allant de 8 à par an permettant d'atteindre jusqu'à . Toutefois, pour les utilisateurs déjà inscrits, il était possible de demander l'extension à gratuitement. Cette possibilité n'existe plus depuis le mai 2012.

À la fin de juillet 2013, à la suite d'un procès perdu contre le groupe audiovisuel britannique British Sky Broadcasting (BSkyB), Microsoft sera contraint de changer le nom de son service de stockage en ligne. Finalement, c'est le 27 janvier 2014 que Microsoft annonce le changement de nom en OneDrive. Le 18 février 2014 le nom de SkyDrive devient officiellement OneDrive.

Skype est un logiciel gratuit qui permet aux utilisateurs de passer des appels téléphoniques et vidéo via Internet, ainsi que le partage d'écran. Les appels d’utilisateur à utilisateur sont gratuits, tandis que ceux vers les lignes téléphoniques fixes et les téléphones mobiles sont payants. Il existe des fonctionnalités additionnelles comme la messagerie instantanée, le transfert de fichiers et la visioconférence.

En décembre 2014, Skype lance Skype Translator. Cette application permettra la traduction instantanée des communications vocales dans plus de 40 langues. Pour son lancement, les deux premières langues supportées par l'application sont l'anglais et l'espagnol.

Le , Microsoft annonce que les utilisateurs de Windows Live Messenger seront d'ici quelques mois intégré dans Skype. Windows Live Messenger sera désactivé le premier trimestre 2013 (exception faite de la Chine). À cela s'ajoute un changement technologique, puisque 80 % de la messagerie instantanée transiteraient par les serveurs de Messenger.

En décembre 2014, Skype lance Skype Translator. Cette application permettra la traduction instantanée des communications vocales dans plus de 40 langues. Pour son lancement, les deux premières langues supportées par l'application sont l'anglais et l'espagnol. Skype est désormais intégré à la suite bureautique en ligne qui se nomme Office Online.

Le 14 mars 2017, Microsoft lance Teams pour concurrencer Slack dans son offre Office 365 pour entreprise : un logiciel collaboratif basé sur Skype.

Microsoft voulait être influent dans le monde d'internet comme toute société à l'époque par exemple avec Internet Explorer qui est un navigateur web équipant un peu plus de 60 % des ordinateurs du monde en (Selon StatOwl.com). La version actuelle est la avec Windows 8. Une version d’Internet Explorer est disponible gratuitement pour les systèmes Mac OS d’ mais le développement de cette version a toutefois été arrêtée en 2003. Néanmoins, le navigateur perd des parts de marché depuis 2004, avec l’arrivée d’autres navigateurs comme Mozilla Firefox, Google Chrome et Safari. Cette baisse coïncide avec l'ajout du "ballot screen" en Europe, un écran obligeant le consommateur de choisir son navigateur web, une mesure imposée par la Commission européenne.

Notamment avec Bing, qui est le moteur de recherche développé par la société Microsoft. Il a été rendu public le . Au moment de sa sortie, en 2008, cela révélait un changement dans la stratégie commerciale de Microsoft, qui séparait son moteur de recherche de sa suite d’applications Windows Live. Dans sa version finale, Bing offre les options de recherches suivantes : sites web, images, vidéos, shopping, actualités, cartes, voyages...

Selon Microsoft, ce moteur de recherche innove en termes d’algorithmes, qui donne des résultats plus pertinents, mieux organisés et classés en rubriques thématiques. De plus, de nouvelles fonctionnalités ont été ajoutées, comme des comparateurs de prix.

L’objectif pour Microsoft avec ce quatrième moteur de recherche est de mieux concurrencer la suprématie du géant Google, le leader absolu dans ce secteur. Ce dernier compte, à la date d'avril 2009, 65,3 % de part de marché. Lors du lancement de "Bing", Microsoft a prévu un budget communication compris entre 80 et 100 millions de dollars.

Ce moteur de recherche est intégré aux pages de MSN et Windows Live. En juillet 2009, un partenariat a été conclu entre Microsoft et Yahoo!. Cet accord prévoit que "Bing" fournisse son algorithme au moteur de recherche Yahoo! Search sur les portails Yahoo!.

Concernant MSN, c'est un portail web offrant des sites et services Internet fourni par Microsoft. The Microsoft Network est originellement un service en ligne et fournisseur d'accès à internet commercialisé le , en même temps que la sortie de Windows 95.

MSN se présente comme une plateforme interactive, donnant à terme un accès facilité à l'ensemble des produits de Microsoft, comme Outlook.com, Skype, la suite Office en ligne et le service de stockage dématérialisé OneDrive. Facebook et Twitter sont quant à eux intégrés à l'ensemble du site afin de pouvoir communiquer et échanger des informations à tout moment par le biais des réseaux sociaux.

MSN se divise désormais en dix univers (actualités, sport, finance, lifestyle, cuisine et vins, divertissement, santé et forme, etc.). En vrac, les internautes peuvent y retrouver aussi bien les derniers résultats sportifs que des avis portant sur plus de 1,5 million de bouteilles de vin, de cuisine illustrées, mais aussi les horaires d'un vol, la météo des jours à venir ou encore les derniers cours de la bourse. Il est possible de configurer l'ensemble de ces services afin d'avoir toujours accès à tout moment ses rubriques favorites.

Microsoft propose aussi plusieurs outils pour les entreprises comme Microsoft SQL Server qui est le système de gestion de base de données (SGBD) phare de Microsoft, codéveloppé avec Sybase jusqu’en 1994. Microsoft Access est le SGBD personnel inclus dans la suite Office, et Extensible Storage Engine est le moteur de SGBD utilisé dans des produits de la marque tels que Exchange ou Active Directory. Plus d'une centaine d'outils périphériques sont disponibles, soit lors de l'installation (SQL Profiler, Database Tuning Advisor, Data Collector...), soit directement sur le site de Microsoft (SQLdiag, SQLioSim, SQL Server Best Practices Analyzer...), soit à travers le site communautaire opensource Codeplex (RML Utilities, PAL, Open DBDiff...).

Une instance de SQL Server est une installation de tout ou partie des services SQL Server sur une machine Windows et peut héberger de nombreuses bases de données. Un même OS supportant jusqu'à 50 instances différentes (ce qui n'est pas conseillé en production).

SQL Server existe en différentes éditions : CE (Compact Édition - solution embarquée pour les smartphones), Express (plusieurs déclinaisons - gratuite), Web (en mode SPLA : Service Provider License Agreement - auprès d'hébergeurs Web), Standard, BI et Enterprise - ceci pour la production. L'édition Developper (équivalente à l'édition Enterprise) est destinée au développement.

Il y a aussi Visual Studio qui est la suite de développement de la firme, incluant divers éditeurs et compilateurs, essentiellement une version améliorée de BASIC nommée Visual Basic, ainsi que des évolutions de C++ et de C#, qui constitue la réponse de Microsoft au langage Java. Cette suite permet aussi de tirer parti des fonctionnalités du .NET Framework.
Depuis le , une nouvelle version apporte :

Cette version marque une fusion entre les éditions "Premium" et "Ultimate" pour simplifier le choix : trois éditions sont disponibles : "Community", "Professionnel" et "Enterprise".

Le numéro de version interne de Visual Studio 2015 est 14.0 (le symbole codice_1 étant défini comme 1900).

Microsoft propose aussi la suite Microsoft Azure (Windows Azure jusqu’en 2014) qui est le nom de la plate-forme applicative en nuage pour entreprise de Microsoft. Son nom évoque le concept de « cloud computing » ou informatique en nuage (l'externalisation des ressources informatiques d'une entreprise vers des datacenters distants).
Il s'agit d'une offre d'hébergement (applications et données) et de services (workflow, stockage et synchronisation des données, bus de messages, contacts…).
Un ensemble d'API permet d'utiliser et d'accéder à cette plate-forme et aux services associés.
Un environnement d'exécution (le « Live Operating Environment ») permet une intégration étroite avec les principaux systèmes d'exploitation existant (Windows, Mac OS et Windows Phone).

Le 20 juillet 2016, Microsoft est accusé par la Cnil d'une collecte de données excessive avec son système d'exploitation pour ordinateurs et tablettes. La Cnil met en demeure Microsoft de "cesser la collecte excessive de données et le suivi de la navigation des utilisateurs sans leur consentement. Elle lui demande aussi d'assurer de façon satisfaisante la confidentialité des données des utilisateurs.

Microsoft est l’éditeur de nombreux jeux vidéo pour PC dont , , Fable ou . Un service communautaire a été lancé en 2006, « ». Cette gamme accueille plusieurs jeux comme ou "", mais est abandonnée par de grands éditeurs tels THQ, qui justifie son retrait par des . En 2013, le service Games for Windows est abandonné par Microsoft, face à la concurrence d'autres plateformes comme Steam, et pour mettre en valeur les nouveaux services de Microsoft comme le Windows Store qui propose lui aussi des jeux. Il propose également DirectX, une API multimédia (vidéo, son, réseau) pour le développement d’application Windows (principalement des jeux vidéo) et aussi qui permet de visionner des animations vectorielles (faisant ainsi concurrence à ), intégrant de l’audio et de la vidéo.

Microsoft Studios, créé en 2002 sous la dénomination "Microsoft Game Studios" (aussi appelé "Microsoft Game Division"), est une société détenue par Microsoft qui développe et édite des jeux vidéo pour les plates-formes Microsoft Windows ou les consoles Xbox, Xbox 360 et Xbox One. La société publie notamment les jeux des studios de développement internes comme 343 Industries ou Rare Ltd., mais aussi des studios de développement tiers comme BioWare ou Bizarre Creations.

Dans la semaine du 7 septembre 2014, des rumeurs circulent sur ce rachat. Aucune des deux sociétés n'a commenté cette rumeur. Finalement le 15 septembre Mojang est rachetée par Microsoft pour 2,5 milliards de dollars. Mojang explique que Minecraft, leur plus grand jeu, était devenu trop difficile à gérer et que depuis 2012 il travaillait en collaboration avec Microsoft, lequel était donc le meilleur choix. Markus Persson a aussi rajouté qu'il voulait se séparer depuis longtemps de Minecraft et s' occuper d'autres jeux comme Cobalt et Scrolls. Markus Persson subissait des pressions de sa communauté quant au manque de contenu dans son jeu principal, Minecraft. Il profita donc de cette occasion pour se séparer de son entreprise, qu'il ne souhaitait plus gérer. Par ailleurs, Markus Persson a affirmé sur son blog personnel, peu après la revente de Mojang à Microsoft, qu'il n'était pas question d'argent, mais que son départ était lié à une trop forte pression due à une communauté de plusieurs millions de personnes qui suivaient de près ce qu'il faisait.

Le 15 septembre 2014, Microsoft acquiert la société Mojang (à l'origine du jeu Minecraft) pour 2,5 milliards de dollars. Minecraft est un jeu vidéo indépendant de type « bac à sable » développé par le Suédois Markus Persson alias Notch, puis par le studio de développement Mojang. Ce jeu vidéo plonge le joueur dans un univers réaliste mais cubique : tout est composé de blocs en 3D pixelisés. Un des principes de base consiste à exploiter les ressources naturelles (minéralogiques, fossiles, animales et végétales) pour en faire de la nourriture, des produits manufacturés ou des matériaux de construction.

En août 2016, Microsoft annonce le rachat de la plateforme de streaming Beam afin de permettre aux spectateurs d'interagir avec joueurs sur des parties de jeux vidéo en direct .

Microsoft s’est lancé en 2001 dans ce secteur hautement concurrentiel, en sortant sa propre console de jeux vidéo la Xbox et en 2005 la . Elles proposent toutes les deux des centaines de jeux et un mode de jeu en ligne communautaire, le . La permet également de se connecter à Windows Live Messenger, Facebook et Twitter et d’utiliser d’autres services de Microsoft. Elle peut également lire des DVD, des vidéos, ou de la musique. Depuis une récente mise à jour s'inspirant de l'interface de Windows 8, différentes applications sont arrivées sur la console comme Internet Explorer Xbox 360, YouTube, Dailymotion et différentes télés de rattrapage (TF1, Pluzz, Arte, M6...)

La Xbox 360 possède environ 30 % des parts de marché sur le marché des consoles de jeux vidéo en . Elle est de plus rentable depuis 2008 pour l’éditeur. Les consoles Xbox au Japon ont peu de succès, contrairement à l'Amérique du Nord. Au Japon, la peine à atteindre d’unités vendues en , alors que la PS3 s’est vendue à presque en , et la Wii à en . Malgré quelques remontées en 2008 et 2009 grâce notamment à la sortie de plusieurs comme ' ou ', elle reste loin derrière. Néanmoins, cela ne l’empêche pas d’avoir un succès dans le reste du monde.

La Xbox One est une console de jeux vidéo de huitième génération développée par Microsoft. Dévoilée le , elle succède à la Xbox 360 et se place en concurrence frontale avec la PlayStation 4 de Sony, et plus indirectement avec la Wii U de Nintendo. Elle est disponible depuis le dans treize pays et depuis dans vingt-six autres pays. D'abord uniquement commercialisée avec Kinect, Microsoft propose la console seule à partir du .

La Xbox One S est sortie vers août 2016 et a été présentée lors de l'E3 2016. La Xbox One S est plus légère, 40 % plus petite que la Xbox One, possibilité de la mettre à la verticale, le bloc d'alimentation est intégré à l'intérieur, la technologie HDR est intégrée & un affichage UHD également. On note également une petite augmentation de puissance par rapport à la Xbox One. 

Lors de l'E3 2016, Microsoft annonce l'arrivé de " "la console la plus puissante au monde"" : la Xbox One X, et à l'E3 2017, Microsoft donne la date de mise sur le marché de la console : le 7 novembre 2017. Elle est capable de diffuser de la vraie "4K" & améliore les graphismes en "Full HD". Elle est également la premier console à utiliser un système de refroidissement à vapeur et refroidissement liquide. Elle est la console la plus puissante du marché.

Microsoft Lumia (appelé jusqu'en 2014 "Nokia Lumia") est une gamme d'appareils mobiles conçue et commercialisée par Microsoft Mobile et précédemment par Nokia. Lancée en novembre 2011, cette série de smartphones et d'une tablette tactile est le résultat de la coopération des sociétés Nokia, constructeur des téléphones, et Microsoft, concepteur du système d'exploitation Windows Phone. Le nom Lumia vient du mot finnois "lumi", qui signifie « neige ».

À la suite du rachat de la branche mobile de Nokia par Microsoft, celui-ci annonce en octobre 2014 que désormais la gamme Nokia Lumia s’appellera Microsoft Lumia.

Les appareils Lumia, outre les fonctions élémentaires (téléphonie, SMS et MMS), disposent, comme leurs concurrents, d'un écran tactile capacitif multipoint, d'un appareil photo équipé d'une caméra HD, d'un système de géolocalisation intégré, d'un logiciel de cartographie numérique inclus, d'un système d'écoute et de téléchargement de la musique, d'un client Internet, d'applications bureautiques Office, d'une plateforme permettant le téléchargement d'applications mobile (Windows Store).

Comme les appareils Lumia utilisent exclusivement Windows Phone, les mises à jour de l'OS sont souvent accompagnées de mises à jour firmware lors du déploiement. Nokia et Microsoft Mobile ont publié plusieurs mises à jour firmware exclusives aux dispositifs Lumia, celles-ci sont nommées grâce à une couleur qui leur sont associées, comme : « Lumia Black ». Les mises à jour peuvent contenir des améliorations logicielles de types photographies ou avancées technologiques ; mais aussi des corrections de bugs.

En Juillet 2015, Bloomberg a rapporté que Microsoft préparait une restructuration de Microsoft Mobile, qui comprend la gamme Lumia Microsoft. Microsoft a également signalé qu'il commercialisera moins de modèles.

Dans le cadre d'une restructuration plus importante du groupe d'ingénierie, Microsoft Devices & Studios a été fusionné avec le Groupe d'ingénierie des systèmes d'exploitation pour former le plus grand groupe d'ingénierie regroupant Windows et les périphériques, et en juillet 2015, il a été annoncé que le chef de la gamme Surface, Panos Panay serait à la tête de la nouvelle organisation Microsoft, qui comprend les Microsoft Lumia ainsi que divers autres produits matériels Microsoft tels que les Band, HoloLens, et Xbox.

Microsoft ne produit plus aucun Lumia depuis mai 2016 & n'en vend plus depuis décembre 2016. 

Microsoft Surface est une gamme de tablettes PC conçues et commercialisées par Microsoft. Cela comprend les tablettes PC (Surface et Surface Pro), les ordinateurs portables (Surface Book), et les tableaux interactifs (Surface Hub).

La gamme Surface est donc concentrée en trois grandes lignes majeures :

Le Surface Hub, le tableau interactif, est lui considéré comme un produit à part, qui par son prix élevé ciblera les entreprises

Actuellement il existe 5 modèles de cette tablette avec sa déclinaison Pro. Les Surface tourne sous Windows RT, Windows 8 et Windows 10. Au mois d'octobre 2015 Microsoft annonce qu'elle sortira son premier ordinateur portable : Le Surface Book, en concurrence direct avec le Macbook d'Apple

Microsoft Band est une montre connectée dévoilée le 29 octobre 2014. Disponible dès le lendemain aux États-Unis, elle est l'une des premières à fonctionner aussi bien avec Windows Phone, Android et iOS. L'utilisation de Microsoft Band avec un Windows Phone permet d'interagir avec Cortana, l'assistant personnel présent dans le système d'exploitation de la firme de Redmond, grâce au microphone intégré.

L'appareil se synchronise par le biais de l'application Microsoft Health disponible sur Windows Phone, Android, et iOS. Elle se connecte à un smartphone au moyen du système Bluetooth mais peut fonctionner de manière autonome puis être synchronisée avec un PC sous Windows ou un Mac.

Annoncé le , le Microsoft Band 2 est la seconde génération du bracelet connecté. Tout comme la première génération, il comprend un cardiofréquencemètre optique, un accéléromètre à 3 axes, un gyromètre, un GPS, un microphone, un photodétecteur, des capteurs de réponse corporelle, un capteur à ultraviolets, un thermomètre de peau et un capteur de déplacement capacitif ; il comprend également un nouveau capteur : le baromètre, permettant de mesurer l’altitude. Cortana est directement intégrée au bracelet.

Microsoft ne vend plus de Microsoft Band 2 depuis novembre 2016, a supprimé un projet pour un éventuel Band 3 & a supprimé les SDK de développement.

Présenté le 21 janvier 2015, HoloLens est un casque de réalité augmentée permettant de simuler des hologrammes qui s’intègrent dans le champ de vision de l’utilisateur. Selon Satya Nadella, dirigeant de la société, le dispositif va faire rentrer l'industrie dans l'ère de .
Le développement de Microsoft HoloLens est conduit par l’équipe d’Alex Kipman et s’est fait en partenariat avec la NASA. Le casque est dévoilé par Microsoft lors de la conférence "", le .
Le , Microsoft va encore plus loin et montre une démonstration en direct lors d'un événement consacré à l'avenir de Windows 10, baptisé "Build 2015". Le , Microsoft dévoile lors de l’E3 2015 une version de "Minecraft" compatible avec HoloLens. Mojang se livre même à une démonstration en direct sur scène.
Le , Microsoft annonce que les kits de développement pour Microsoft HoloLens seront distribués lors du premier trimestre de 2016, pour .

Le casque est un ordinateur complet équipé d’une version de Windows adaptée et compatible avec Windows 10. Trois processeurs sont utilisés : le premier est le "CPU" principal, le deuxième est un processeur graphique ("GPU") et le troisième gère les hologrammes (baptisé "HPU" pour « "" »). Des capteurs de mouvements permettent à l’utilisateur de se déplacer en l'utilisant, le son produit par le casque est spatialisé. La simulation des hologrammes fonctionne avec les gestes de l’utilisateur, une commande vocale est aussi disponible, le casque ne nécessite pas d’être connecté à Internet ou à un autre appareil pour fonctionner. Il pèse environ .

En 2017 Microsoft réalise sa première acquisition dans le domaine avec une entreprise canadienne, Maluuba. L'équipe dédiée à ce projet sera intégrée dans le pôle Intelligence artificielle et développement de Microsoft. La firme a déjà commencé à travailler dans le domaine de l'intelligence artificielle mais cette acquisition signe une volonté de se développer.


La société est dirigée par un conseil d'administration composé principalement de l'extérieur de l'entreprise, comme il est habituel pour les sociétés cotées en bourse. Les membres du conseil d'administration à partir du mois de septembre 2014 sont: John W. Thompson, Dina Dublon, Bill Gates, Maria Klawe, David Marquardt, Mason Morfit, Satya Nadella, Charles Noski, Helmut Panke et John W. Stanton.

Les membres du Conseil sont élus chaque année lors de la réunion annuelle des actionnaires en utilisant un système de vote à la majorité.

Il y a actuellement cinq comités au sein du conseil d'administration, qui supervisent les questions plus spécifiques de l'entreprise :

Ces comités comprennent le Comité économique, qui traite des questions comptables avec la société, y compris le Comité des finances et de la comptabilité ; le comité de rémunération, qui approuve la rémunération pour le chef de la direction et d'autres employés de la société ; le Comité des finances, qui gère les questions financières telles que proposer des fusions et acquisitions ; la gouvernance et des candidatures, qui gère diverses questions, y compris des entreprises nomination du conseil d'administration ; et le Comité de conformité antitrust, qui observe les pratiques de l'entreprise et juge si la société à violé les lois antitrust.

D’après les comptes annuels, le chiffre d’affaires de Microsoft s’élevait en 2005 à de dollars. C’est-à-dire une augmentation de 8 % depuis 2004. 
En 2009, le chiffre d’affaires était de de dollars pour de bénéfices.

En 2011, le chiffre d'affaires était de 69,9 milliards de dollars (+16,04 % par rapport à 2009).

Le chiffre d’affaires est réparti en de produits : client, serveur et outils, , , MSN, , .

La partie « Client » regroupe notamment Windows XP. Elle rapporte en 2005 de dollars (+6 %) de chiffre d’affaires. Ceci correspond à une augmentation de 12 % du nombre de licences "OEM" mais celle-ci est balancée par une diminution de 9 % du chiffre d’affaires issu des ventes au détail de ces mêmes licences. Ceci indique un changement de la structure du chiffre d’affaires : le système d’exploitation est de plus en plus une vente liée à l’achat du matériel.

La partie « Serveur et Outils » contient par exemple le serveur Exchange. Son chiffre d’affaires est de de dollars (+130 %) en 2005.

« » concerne essentiellement les outils bureautiques pour un chiffre d’affaires de de dollars (+3 %) en 2005. Les autres segments sont soit en perte soit apportent une contribution mineure en regard des .

Depuis le , Microsoft est coté en Bourse, au .

En , avec une valorisation boursière de 408,68 milliards de dollars, Microsoft est la grosse valorisation boursière au monde.

La société, qui auparavant n’avait jamais distribué de dividende, verse par action le . Ce versement de dividende ayant été aidé par une réduction de la taxation concernée de 35 à 15 %. La somme versée en "dividendes" aux actionnaires de Microsoft en 2004 approche donc les USD.

Avant même la polémique des années 2000 sur les stock-options, Microsoft fut l'un des premiers grands groupes à renoncer à ce type de rémunération, lui préférant le système d'actions gratuites, plus favorable à une croissance régulière, et étalée dans le temps, du chiffre d’affaires et des bénéfices. Lorsque Microsoft a lancé son public et offre publique initiale (IPO) en 1986, le prix des actions d'ouverture était de ; après la journée de négociation, le prix a clôturé à . En Juillet 2010, avec neuf divisions d'actions de la société, des actions IPO serait multiplié par 288 ; Si l'on devait acheter IPO aujourd'hui compte tenu des fractionnements et d'autres facteurs, il en coûterait environ 9 cents. Le prix de l'action a atteint un sommet en 1999 à environ ( tenu des fractionnements).

La société a commencé à offrir un dividende le , à partir de huit cents par action pour l'exercice suivi par un dividende de seize cents par action l'année suivante, le passage de chaque année à des dividendes trimestriels en 2005 avec huit cents par action par trimestre et un paiement ponctuel spécial de trois dollars par action pour le deuxième trimestre de l'exercice financier. Bien que la société ait eu des augmentations des distributions de dividendes, le prix de l'action de Microsoft est resté stable pour deux ans.

Standard and Poor et Moody's ont tous deux donné une cote de AAA à Microsoft, dont les actifs ont été évalués à 41 milliards $, comparativement à seulement 8,5 milliards $ de dettes non garanties. Par conséquent, en février 2011, Microsoft a publié une obligation d'entreprise d'un montant de 2,25 milliards de $ avec des taux d'emprunt relativement faible par rapport aux obligations d'État.

Microsoft possède des infrastructures et des bureaux partout dans le monde, même s'ils sont plus nombreux en Europe, aux États-Unis, en Inde et Chine littorale.


En , Microsoft Research a présenté sa prospective en réalité augmentée, notamment avec les projets Skinput et Light Space.

Le site de Microsoft France se situe à Issy les Moulineaux. Leur Campus d’Issy-les-Moulineaux incarne un engagement pour réduire leur empreinte écologique.
À partir de l’été 2012, Microsoft s’est engagé à présenter un bilan carbone neutre au niveau mondial. En France, pour optimiser leur empreinte écologique, ils ont choisi d’implanter leur Campus, certifié HQE Exploitation, au cœur d’un des plus importants programmes tertiaires Haute Qualité Environnementale d’Ile-de-France. Outre son siège d'Issy Les Moulineaux, Microsoft dispose également d'un accélérateur, Microsoft Ventures, localisé dans le quartier parisien du Sentier.

Dans ce complexe, 1700 salariés travaillent sous les ordres de Alain Crozier. En juillet 2014, Microsoft annonce la suppression de emplois, soit 14 % des effectifs de l'entreprise. La dernière vague de licenciements touche notamment le service informatique.

En 2011, le Greenpeace a sorti un rapport notant les dix premières grandes entreprises dans l'informatique dématérialisée sur leurs sources d'électricité pour leurs centres de données. À l'époque les centres de données consommés en hausse de 2 % de toute l'électricité globale (mondiale) et cette quantité(montant) ont été projetés pour augmenter. Phil Radford de Greenpeace a dit « nous sommes concernés dans cette nouvelle explosion dans l'utilisation d'électricité qui pourrait nous enfermer dans des sources d'énergie vieilles, polluantes au lieu de l'énergie propre disponible aujourd'hui Microsoft et d'autres leaders de l'industrie de technologie de l'information doivent embrasser l'énergie propre de faire fonctionner leurs centres de données ainsi que leurs cloud. » En 2013, Microsoft a consenti à acheter la totalité de la puissance produit par un projet éolien de Texas pour faire fonctionner un de ses centres de données.

Microsoft est classé à la place dans le Guide du "Greenpeace pour l'Électronique Plus verte", qui classe les 18 fabricants d'électronique selon leurs politiques sur les produits chimiques, toxiques, recyclant et sur le changement climatique.

Le campus américain principal de Microsoft a reçu une certification d'argent de la Direction(du Leadership) dans l'Énergie et le Design(la Conception) Environnemental (LEED) le programme en 2008 et il a installé plus de en plus de ses constructions de bâtiments dans son campus de la Silicon Valley, produisant approximativement 15 % de l'énergie totale nécessaire pour faire fonctionner le campus.

Microsoft se sert des formes alternatives de transition pour l'écologie. Il a créé un des plus grands systèmes de bus privés du monde, qui se nomme « le Connecteur », pour transporter les gens de l'extérieur de l'entreprise pour visiter le campus, « la Navette Connecteur » comporte une grande flotte de voitures hybrides pour économiser le carburant. L'entreprise subventionne aussi le transport public régional. En , Microsoft a pris position pour addition du transport public supplémentaire et le véhicule électrique en libre service ainsi que le pont flottant connectant Redmond à Seattle.

Microsoft était le classé numéro 1 dans la liste des Meilleurs Lieux de travail multinationaux du Monde par l'Endroit formidable pour travailler par l'Institut en 2011.

Le siège social, d'une façon informelle connu comme « Microsoft Redmond Campus » est localisé à dans Redmond, Washington. Microsoft s'est initialement déplacé sur les terres du campus le , deux semaines avant que l'entreprise ne soit entrée en bourse le 13 mars. Le siège social a depuis éprouvé des expansions multiples depuis son établissement.

Il est évalué pour englober plus de 80 kilomètres carrés () d'espace de bureau et travaillent sur le campus.

Microsoft propose également un programme de formation et certification (Microsoft certified systems engineer, Microsoft certified solution developer, Microsoft Certified Professional). Microsoft présente aussi un programme de distribution gratuite de logiciels pour les étudiants du second degré, appelé MSDN Academic Alliance.

En France, Microsoft revendique générer avec son écosystème environ emplois. En 2013, 96 % des revenus mondiaux de Microsoft étaient générés par son écosystème de partenaires équivalent à 14 millions d'employés.
Microsoft signe en 2007 son premier accord de partenariat avec une université française.

Dans son livre blanc sur les ressorts économiques du cloud computing, Microsoft revendique plus de partenaires dans plus de 200 pays. Alors qu'en France le nombre de partenaires Gold de l'éditeur a été volontairement réduit de moitié à 239 dont 12 grossistes, Microsoft annonce investir une enveloppe de Md$ de licences, de formations et de primes sur ses partenaires en 2011.

Microsoft a investi 13 millions d’euros afin de favoriser la réussite du Plan Numérique à l’École, réforme envisagé par la ministre Najat Vallaud-Belkacem. Cette démarche qui s’étendra sur les 18 prochains mois qui consiste notamment à assurer des formations aux enseignants et autres cadres de l’éducation, la mise en place de plateformes collaboratives, des mises à jour matérielles mais aussi une introduction expérimentale à l’apprentissage du code.

Ce partenariat survient quelque temps après la visite de Satya Nadella sur Paris. Début novembre, le PDG de Microsoft avait rencontré le président François Hollande afin de s’entretenir au sujet du Plan Numérique à l’École mais aussi discuter sur l’avenir des métiers du numérique. L’ambition première du projet est tout logiquement d’amener une modernisation globale du milieu éducatif. Cela passe par une refonte des méthodes d’enseignement mais aussi par un renouvellement des supports éducatifs.

Durant ces cinq prochaines années, le fondateur de Microsoft investira 2 milliards de dollars pour soutenir les technologies vertes. Bill Gates profitera du coup d'envoi de la Conférence de Paris sur le climat (COP21) pour annoncer un investissement massif dans les technologies vertes (« clean tech »).

Bill Gates a promis de débourser 2 milliards de dollars de sa poche durant les cinq prochaines années pour contribuer à combattre le réchauffement climatique. D'après le classement établi par Forbes, il demeure en 2015 l'homme le plus riche de la planète avec une fortune évaluée à plus de 79 milliards de dollars. On ne connaît pas encore comment ces fonds seront utilisés, mais ils soutiendront les nouvelles technologies.

En 2015, pour concurrencer Google avec son application Santa Tracker, le NORAD met en place un partenariat avec Microsoft pour l'utilisation de son logiciel cartographique Bing Maps, en contrepartie le NORAD devra faire la promotion du navigateur web Microsoft Edge ainsi que du système d'exploitation Windows 10.

Microsoft est inscrit depuis 2009 au registre de transparence des représentants d'intérêts auprès de la Commission européenne. L'entreprise déclare en 2015 pour cette activité des dépenses d'un montant compris entre et . Elle indique avoir perçu sur le même exercice de subventions de l'Union européenne, et remporté auprès de l'UE des marchés à hauteur de .

Selon le "Center for Responsive Politics", les dépenses de lobbying de Microsoft aux États-Unis s'élèvent en 2015 à .

Selon une étude BVA publiée par Capital le 23 juin 2011, Microsoft est la marque high-tech préférée des Français, juste devant Apple et Samsung. Les opérateurs Virgin Mobile et France Télécom ferment ce classement.
En 2004, Microsoft demande à des Institut de recherche de faire des études indépendantes comparant le coût total de possession de Windows Server 2003 pour Linux. Les études ont conclu que les entreprises trouvent plus facile à administrer Windows que Linux ; ainsi, celles sous Windows aurait administré rapidement une baisse des coûts. Cela a entraîné une vague d'études connexes, menée par le Yankee Group qui a conclu que la mise à niveau d'une version de Windows Server à un autre, coûte une fraction des coûts de commutation à partir de Windows Server pour Linux, bien que les entreprises interrogées aient noté la décroissement de la sécurité et la fragilité des serveurs Linux et le souci de se laisser enfermer dans l'aide de produits Microsoft.

Une autre étude, publiée par l'Open Source Development Labs, a affirmé que les études de Microsoft étaient « tout simplement pas à jour et unilatérale » et leur enquête a conclu que le coût total de possession de Linux a été plus faible en raison des administrateurs Linux qui gèrent plusieurs serveurs en moyenne.

. Après avoir subi les temps d'arrêt prolongé et de fiabilité, le LSE a annoncé en 2009 qu'il avait l'intention de déposer sa solution Microsoft et passer à un basé sur Linux en 2010.

En 2012, Microsoft a embauché un sondeur politique nommé Mark Penn. Il a créé une série de publicités négatives ciblant l'un des principaux concurrents de Microsoft, Google. Les annonces, appelés « Scroogled », pour tenter de faire remarquer au monde que Google est dangereux pour les consommateurs, avec les résultats de recherche truquées pour favoriser les annonceurs payés de Google, que Gmail viole la vie privée de ses utilisateurs, de placer les annonces liées au contenu de leurs courriels ainsi que les résultats commerciaux qui favorisent les produits Google.

Du 5 et 18 octobre 2015, Microsoft France met en place une opération de séduction à Paris avec son « Windows Cube », afin de promouvoir son nouveau système Windows 10. Situé sur le parvis de Beaubourg devant le Centre Pompidou, ce lieu permet de découvrir une gamme de nouveaux produits pour Windows 10, et de se familiariser avec ses fonctionnalités telles que Cortana, Windows Hello, le mode Continuum ou encore le navigateur Edge à travers différents pôles expérientiels.

Le , Microsoft a dévoilé le nouveau logo de l'entreprise à l'ouverture de sa boutique Microsoft Store à Boston indiquant le changement de cap de la société et de mise au point du style classique à l'interface moderne qui utilisera la plate-forme Windows, Windows Phone et la Xbox One. Le logo comprend quatre carrées avec les couleurs des produits de Microsoft, c'est-à-dire : Office pour le carré rouge, Xbox pour le carré vert, Windows pour le carré bleu, et Bing pour le carré jaune.




</doc>
<doc id="1943" url="https://fr.wikipedia.org/wiki?curid=1943" title="MD5">
MD5

L'algorithme MD5, pour Message Digest 5, est une fonction de hachage cryptographique qui permet d'obtenir l'empreinte numérique d'un fichier (on parle souvent de "message"). Il a été inventé par Ronald Rivest en 1991.

Si l'algorithme MD5 présente un intérêt historique important il est aujourd'hui considéré comme dépassé et absolument impropre à toute utilisation en cryptographie ou en sécurité.

MD5 ("") est une fonction de hachage cryptographique qui calcule, à partir d'un fichier numérique, son "empreinte numérique" (en l'occurrence une séquence de ou en notation hexadécimale) avec une probabilité très forte que deux fichiers différents donnent deux empreintes différentes.

En 1991, Ronald Rivest améliore l'architecture de MD4 pour contrer des attaques potentielles qui seront confirmées plus tard par les travaux de Hans Dobbertin.

Cinq ans plus tard, en 1996, une faille qualifiée de « grave » (possibilité de créer des collisions à la demande) est découverte et indique que MD5 devrait être mis de côté au profit de fonctions plus robustes comme SHA-1.

En 2004, une équipe chinoise découvre des collisions complètes. MD5 n'est donc plus considéré comme sûr au sens cryptographique. On suggère maintenant d'utiliser plutôt des algorithmes tels que SHA-256, RIPEMD-160 ou .

Cependant, la fonction MD5 reste encore largement utilisée comme outil de vérification lors des téléchargements et l'utilisateur peut valider l'intégrité de la version téléchargée grâce à l'empreinte. Ceci peut se faire avec un programme comme "md5sum" pour MD5 et "sha1sum" pour SHA-1.

Comme toute fonction de hachage cryptographique, MD5 peut aussi être utilisé pour calculer l'empreinte d'un mot de passe avec la présence d'un "sel" permettant de ralentir une attaque par force brute. Cela a été le système employé dans GNU/Linux. Ainsi, plutôt que de stocker les mots de passe dans un fichier, ce sont leurs empreintes MD5 qui étaient enregistrées (SHA-256, SHA-512 -par défaut- ou DES sont maintenant utilisés), de sorte que quelqu'un qui lirait ce fichier ne pourrait pas découvrir les mots de passe. La commande "" des commutateurs et routeurs Cisco, utilisait le hachage MD5 (5 pour indiquer MD5) pour stocker le mot de passe du mode privilégié dans le fichier de configuration de l'équipement. Les dernières versions d'IOS intègrent le hachage SHA256 (4 pour indiquer SHA256).

Le programme permet de casser (inverser la fonction pour) les MD5 triviaux par force brute. Il est incommode pour les clés longues, et ne fonctionne pas toujours si elles contiennent des caractères nationaux spécifiques (cela dépend en fait des dictionnaires utilisés).

Les "tables arc-en-ciel" (à accès direct, et qui font parfois plusieurs gigaoctets) permettent de les craquer souvent en moins d'une seconde. Ces tables utilisent des dictionnaires établis après plusieurs jours, mois ou années de calcul. Ceux-ci ne contiennent pas la totalité des clés MD5 possibles, ni ne sont destinés à un cassage par force brute (une empreinte comporte , ce qui représente environ (formula_1) de combinaisons), mais permettent par examen de l'empreinte d'éliminer de très grandes classes de combinaisons à "ne pas" tester, ce qui accélère la recherche plusieurs milliards de fois. L'efficacité des tables arc-en-ciel diminue si l'empreinte est calculée avec un « sel ».

Voici l'empreinte (appelée abusivement "signature") obtenue sur une phrase : 
En modifiant un caractère, cette empreinte change radicalement : 

Très concrètement, la vérification de l'empreinte ou somme de contrôle MD5 peut être réalisée de la façon suivante : lors du téléchargement d'un programme, on note la série de caractères nommée « Signature MD5 » indiquée sur la page de téléchargement. Quand ce téléchargement est terminé, on lance un utilitaire de calcul MD5 comme "HashCalc" ou "md5sums", qui indique entre autres la somme de contrôle correspondant au fichier. Si les deux valeurs correspondent, on peut alors raisonnablement considérer que le fichier n'a pas été corrompu (volontairement ou non d'ailleurs). On constate plusieurs fragilités dans ce processus : la page d'origine a pu être modifiée, et l'utilitaire de calcul peut être "adapté" pour fournir la signature attendue. C'est pourquoi il faut impérativement utiliser un utilitaire provenant d'une source de confiance. Il est aussi possible d'utiliser une extension pour le navigateur Mozilla Firefox comme "" afin d'automatiser ce contrôle.

À ses débuts, la fonction MD5 était considérée comme sûre, mais au cours du temps, des failles ont été découvertes dans son fonctionnement et durant l'été 2004, il a été cassé par des chercheurs chinois, Xiaoyun Wang, Dengguo Feng, Xuejia Lai (co-inventeur du célèbre algorithme de chiffrement IDEA) et Hongbo Yu. Leur attaque a permis de découvrir une collision complète (deux messages différents qui produisent la même empreinte) sans passer par une méthode de type recherche exhaustive. 

Sur un système parallélisé, les calculs n'ont pris que quelques heures. Le MD5 n'est donc plus considéré comme sûr, mais l'algorithme développé par ces trois chercheurs concerne des collisions quelconques et ne permet pas de réaliser une collision sur une empreinte spécifique, c'est-à-dire réaliser un deuxième message, à partir de l'empreinte d'un premier message, qui produirait la même empreinte. Un projet de calcul distribué lancé en , , visait à découvrir une collision complète mais a été subitement arrêté après la découverte de l'équipe chinoise. La sécurité du MD5 n'étant plus garantie selon sa définition cryptographique, les spécialistes recommandent d'utiliser des fonctions de hachage plus récentes comme le SHA-256.

On sait maintenant générer des collisions MD5 en moins d'une minute lorsque les deux blocs en collisions sont « libres ». On peut aussi générer une infinité de collisions avec un texte T à partir de deux messages M1 et M2 de même longueur qui sont en collision. Il suffit de concaténer M1 et M2 avec T, tel que et , afin d'obtenir une collision complète entre T1 et T2. On ne peut toutefois pas générer une signature particulière et la falsification de documents reste un exercice difficile.

Dès 2006, il est par exemple possible de créer des pages HTML aux contenus très différents et ayant pourtant le même MD5. La présence de métacodes de « bourrage » placés en commentaires, visibles seulement dans le source de la page web, trahit toutefois les pages modifiées pour usurper le MD5 d'une autre. La supercherie peut donc être levée si on examine les sources de la page en question.

En 2008, le logiciel BarWF utilise les ressources des instructions SSE2 et des processeurs massivement parallèles d'une carte graphique (CUDA) pour casser du MD5 en force brute à la vitesse annoncée de de clés par seconde.


MD5 travaille avec un message de taille variable et produit une empreinte de . Le message est divisé en blocs de , on applique un remplissage de manière à avoir un message dont la longueur est un multiple de 512. Le remplissage se présente comme suit : 


Ce remplissage est toujours appliqué, même si la longueur du message peut être divisée par 512. Cette méthode de "" est semblable à celle utilisée dans la plupart des algorithmes de "message digest" des familles MD (comme MD5 ou RIPEMD) ou SHA (SHA-1 ou SHA-512) mais différente de celle de l'algorithme qui utilise une convention dite d'ordonnancement des bits dans chaque octet.

La taille du message est codée en . Le message a maintenant une taille en bits multiple de 512, c'est-à-dire qu'il contient un multiple de de .

L'algorithme principal travaille avec un état sur . Il est lui-même divisé en de (en informatique, on utilise le terme "mot" pour désigner une valeur de 32 bits ou "word" en anglais) : "A", "B", "C" et "D". Ils sont initialisés au début avec des constantes. L'algorithme utilise ensuite les blocs provenant du message à hacher, ces blocs vont modifier l'état interne. Les opérations sur un bloc se décomposent en quatre rondes (étapes), elles-mêmes subdivisées en similaires basées sur une fonction non linéaire "F" qui varie selon la ronde, une addition et une rotation vers la gauche. Les quatre fonctions non linéaires disponibles sont : 


MD5 peut s'écrire sous cette forme en pseudo-code.




</doc>
<doc id="1947" url="https://fr.wikipedia.org/wiki?curid=1947" title="Monbazillac">
Monbazillac

Monbazillac est une commune française située dans le département de la Dordogne, en région Nouvelle-Aquitaine.

Elle a donné son nom au vin blanc liquoreux homonyme.

Située dans le Périgord pourpre et dans l'aire urbaine de Bergerac, à dix kilomètres au sud de Bergerac, la commune de Monbazillac est implantée sur les coteaux qui dominent la vallée de la Dordogne.

Connue pour son château du classé monument historique, elle est surtout renommée pour son vin blanc liquoreux.

En occitan, la commune porte le nom de "Mont Basalhac".

Dès 1790, la commune de Monbazillac a été rattachée au canton de Ribagnac qui dépendait du district de Bergerac jusqu'en 1795, date de suppression des districts. Lorsque ce canton est supprimé par la loi du 8 pluviôse an IX () portant sur la « réduction du nombre de justices de paix », la commune est rattachée au canton de Cunéges (devenu canton de Sigoulès en 1817) dépendant de l'arrondissement de Bergerac.

Fin 2001, Monbazillac intègre dès sa création la communauté de communes de Bergerac Pourpre. Celle-ci est dissoute au et remplacée au par la communauté d'agglomération bergeracoise. Celle-ci fusionne avec la communauté de communes des Coteaux de Sigoulès au pour former la nouvelle communauté d'agglomération bergeracoise.

Les habitants de Monbazillac sont appelés les Monbazillacois.

En 2012, parmi la population communale comprise entre 15 et 64 ans, les actifs représentent 421 personnes, soit 46,3 % de la population municipale. Le nombre de chômeurs (quarante) a augmenté par rapport à 2007 (trente-deux) et le taux de chômage de cette population active s'établit à 9,5 %.

Au 31 décembre 2013, la commune compte 117 établissements, dont cinquante-huit au niveau des commerces, transports ou services, quarante-quatre dans l'agriculture, la sylviculture ou la pêche, sept relatifs au secteur administratif, à l'enseignement, à la santé ou à l'action sociale, quatre dans la construction, et quatre dans l'industrie.

Dans le secteur agroalimentaire, parmi les entreprises ayant leur siège social en Dordogne, la Cave coopérative des grands vins de Monbazillac (vinification), implantée à Monbazillac, se classe avec , en termes de chiffre d'affaires hors taxes en 2012-2013.






</doc>
<doc id="1951" url="https://fr.wikipedia.org/wiki?curid=1951" title="Moteur de recherche">
Moteur de recherche

Un moteur de recherche est une application web permettant de trouver des ressources à partir d'une requête sous forme de mots. Les ressources peuvent être des pages web, des articles de forums Usenet, des images, des vidéos, des fichiers, etc. Certains sites web offrent un moteur de recherche comme principale fonctionnalité ; on appelle alors « moteur de recherche » le site lui-même.

Ce sont des instruments de recherche sur le web sans intervention humaine, ce qui les distingue des annuaires. Ils sont basés sur des « robots », encore appelés « "bots" », « "spiders" «, « "crawlers" » ou « agents », qui parcourent les sites à intervalles réguliers et de façon automatique pour découvrir de nouvelles adresses (URL). Ils suivent les liens hypertextes qui relient les pages les unes aux autres, les uns après les autres. Chaque page identifiée est alors indexée dans une base de données, accessible ensuite par les internautes à partir de mots-clés.

C'est par abus de langage qu'on appelle également « moteurs de recherche » des sites web proposant des annuaires de sites web : dans ce cas, ce sont des instruments de recherche élaborés par des personnes qui répertorient et classifient des sites web jugés dignes d'intérêt, et non des robots d'indexation.

Les moteurs de recherche ne s'appliquent pas qu'à Internet : certains moteurs sont des logiciels installés sur un ordinateur personnel. Ce sont des moteurs dits « de bureau » qui combinent la recherche parmi les fichiers stockés sur le PC et la recherche parmi les sites Web — on peut citer par exemple Exalead Desktop, Google Desktop et Copernic Desktop Search, Windex Server, etc.

On trouve également des métamoteurs, c'est-à-dire des sites web où une même recherche est lancée simultanément sur plusieurs moteurs de recherche, les résultats étant ensuite fusionnés pour être présentés à l'internaute. On peut citer dans cette catégorie Ixquick, Mamma, Kartoo, Framabee ou Lilo.

Les moteurs de recherche sont inspirés des outils de recherche documentaire (à base de fichiers inversés, alias fichiers d'index) utilisés sur les mainframes depuis les années 1970, comme le logiciel STAIRS sur IBM. Le mode de remplissage de leurs bases de données est cependant différent, car orienté réseau. Par ailleurs la distinction entre données formatées (« champs ») et texte libre n'y existe plus, bien que commençant depuis 2010 à se réintroduire par le biais du web sémantique.

Des moteurs historiques ont été Lycos (1994), Altavista (1995, premier moteur 64 bits) et Backrub (1997), ancêtre de Google.

Le fonctionnement d'un moteur de recherche comme tout instrument de recherche se décompose en trois processus principaux :


Des modules complémentaires sont souvent utilisés en association avec les trois briques de bases du moteur de recherche. Les plus connus sont les suivants :


Afin d'optimiser les moteurs de recherche, les webmestres insèrent des métaéléments (métatags) dans les pages web, dans l'en-tête HTML (head). Ces informations permettent d'optimiser les recherches d'information sur les sites web.

Les sites dont la recherche est le principal service se financent par la vente de technologie et de publicité.

Le financement par la publicité consiste à présenter des publicités correspondant aux mots recherchés par le visiteur. L'annonceur achète des mots-clés : par exemple une agence de voyage peut acheter des mots-clés comme « vacances », « hôtel » et « plage » ou « Cannes », « Antibes » et « Nice » si elle est spécialisée dans cette région. Cet achat permet d'obtenir un référencement dit « référencement payant » à distinguer du référencement dit « référencement naturel ».

Le moteur de recherche peut afficher la publicité de deux manières : en encart séparé ou en l'intégrant aux résultats de la recherche. Pour le visiteur, l'encart séparé se présente comme une publicité classique. L'intégration aux résultats se fait en revanche au détriment de la pertinence des résultats et peut avoir des retombées négatives sur la qualité perçue du moteur. De ce fait, tous les moteurs ne vendent pas de placement dans les résultats.

Les moteurs de recherche constituent un enjeu économique. La valeur boursière de Google, principal moteur de recherche, était de 394 milliards de dollars en février 2014.

L'importance des enjeux économiques a généré des techniques de détournement malhonnêtes des moteurs de recherche pour obtenir des référencements « naturels », le "spamdexing" (référencement abusif en français).

Les techniques les plus pratiquées de "spamdexing" sont :

Les techniques de référencement abusif sont pourchassées par les éditeurs de moteurs de recherches, qui constituent des listes noires, provisoires ou définitives.

On distingue le "spamdexing", détournement malhonnête, du « SEO », "Search Engine Optimization" (optimisation des moteurs de recherche en français). Les techniques de SEO sont commercialisées par des sociétés spécialisées.

Les grandes organisations (entreprises, administrations) disposent généralement de très nombreuses ressources informatiques dans un vaste intranet. Leurs ressources n'étant pas accessibles depuis Internet, elles ne sont pas couvertes par les moteurs de recherche du web. Elles doivent donc installer leur propre moteur si elles veulent mener des recherches dans leurs ressources. Elles constituent donc un marché pour les développeurs de moteurs de recherche.

Il arrive également que des sites web publics utilisent les services d'un moteur de recherche pour étoffer leur offre. On parle alors de « SiteSearch ». Ces logiciels permettent la recherche de contenus dans un ou plusieurs groupes de sites. Ces technologies sont particulièrement exploitées sur les sites de contenus et les sites de vente en ligne. La particularité de ces outils est souvent la complexité de mise en œuvre et les ressources techniques nécessaires disponibles.

Il arrive aussi que les grands portails exploitent la technologie des moteurs de recherche. Ainsi Yahoo!, spécialiste de l'annuaire web, a utilisé pendant quelques années la technologie de Google pour la recherche jusqu'à ce qu'elle lance son propre moteur de recherche Yahoo Search Technology en 2004 dont les fondations proviennent de Altavista, Inktomi et Overture, sociétés fondatrices des moteurs de recherche et rachetées par Yahoo!.

De plus en plus de producteurs de contenu, à la suite des recommandations du W3C sur le web sémantique, indexent leurs bases avec des métadonnées ou des taxinomies (ontologies), en vue de permettre aux moteurs de recherche de s'adapter aux analyses sémantiques.

Ces formes de recherches et d'analyses de corpus d'informations par voie informatique ne sont encore que des potentialités.

Par comparaison avec des recherches plein texte, des recherches réalisées sur le web sémantique doivent être plus conviviales pour l'utilisateur :

Il n'existe pas encore à proprement parler de moteur de recherche sémantique qui permette de comprendre une question en langue naturelle et d'adapter une réponse en fonction des résultats trouvés.

Quelques tentatives existent néanmoins pour chercher à répondre par des formes intermédiaires à cette problématique du sens dans la recherche d'information :

L'abandon peu à peu des annuaires papiers conduit les usagers à effectuer les mêmes recherches sur l'internet « profession+localité ». Google a donc acquis en 2010 un fichier d'entreprises (pour la France et un certain nombre de pays), pour effectuer un mixage des données web et annuaire lorsque les requêtes correspondent a une activité localisée. Cette nouvelle tendance se vérifie chez les principaux moteurs de recherche et de nouveaux « outils mixte » voient le jour. Yandex et Baidu n'ont pas encore adopté ce modèle de mixage.

Selon une étude réalisée par La Poste/Priceminister, seulement 52 % des PME Françaises disposaient d'une présence sur Internet en 2013. Selon une autre étude, cette proportion atteint 72 % pour les professions libérales (avocats, dentistes, médecins, notaires, huissiers, infirmières...).

Les moteurs de recherche qui par définition collectent uniquement des données issues de l'internet, ont donc été obligés d'acquérir et de proposer ces adresses d'annuaire en complément pour satisfaire la recherche d'adresses des internautes. Google a baptisé ces adresses « Google Adresses », puis d'office basculées vers « Google + » , actuellement « Google My Business ». Les moteurs de recherche Bing et Google ne communiquent pas l'origine de ces fichiers d'entreprises intégrés, hormis Yahoo! qui est en partenariat avec Pages Jaunes.


Les métamoteurs sont des outils de recherche qui interrogent plusieurs moteurs de recherche simultanément et affichent à l'internaute une synthèse pertinente.

Exemples : Ixquick, Kelseek, Searx, Seeks et Lilo.

On désigne par « multi-moteurs » (ou plus rarement, « super moteur ») une page web proposant un ou plusieurs formulaires permettant d'interroger plusieurs moteurs. Il peut également (mais plus rarement) s'agir d'un logiciel, d'une fonction ou d'un plugin de navigateur web, ou d'une barre d'outils…

Le choix d'un des moteurs peut se faire par bouton, bouton radio, onglet, liste déroulante ou autre.

Les premières pages de ce type recopiaient le code des formulaires de plusieurs moteurs. Avec l'apparition du JavaScript il est devenu possible de n'avoir plus qu'un seul formulaire.
On peut citer par exemple Creative Commons Search, Ecosia, Disconnect, Wiinkz, le moteur de recherche de Maxthon, HooSeek (fermé en 2012). Il y a également : kadaza.fr, vu.fr/search, sputtr.com, atunn.com, wiinkz.com, search-22.com, fefoo.com, turboscout.com, alltheinternet.com, …
Illustration :

Article détaillé (en anglais) : .

On désigne par « moteur de recherche solidaire », un moteur qui reverse une partie de ses revenus à des causes écologiques, sociales ou humanitaires. Ces moteurs sont nés du constat que les revenus annuels générés par la publicité sur les moteurs de recherche sont assez importants (environ par utilisateur pour Google). Les moteurs de recherches solidaires se distinguent notamment dans la façon de distribuer les revenus générés. Certains moteurs comme Ecosia reversent alors une partie des revenus à une seule et unique cause, alors que des moteurs comme Lilo permettent aux internautes de choisir les projets à financer.

Voir la liste des moteurs de recherche solidaires.

On désigne par « moteurs verticaux » une page web ou un service multimédia qui propose une recherche spécialisée dans un domaine professionnel ou qui est particulièrement ciblé. Cet outil de recherche est spécialisé dans un secteur particulier, tel que les télécommunications, le droit, la biotechnologie, la finance (assurance) ou encore l'immobilier.
Son fonctionnement général est basé sur une bases de données constituée à partir des bases de tous les sites spécialisés de l'activité ciblée.

Ce type de moteur est utilisé par les professionnels et ciblé sur le consommateur, avec le plus souvent une finalité économique qui dérive sur la géolocalisation.

On retrouve ainsi pour le grand public des annuaires, des comparateurs. Il en existe maintenant pour toutes les activités : immobilier, tourisme, recherche d'emploi, recrutement, automobile, loisirs, jeux.

L'explosion du nombre de contenus de formats divers (données, informations non structurées, images, vidéos…) disponibles dans les entreprises les poussent à s'équiper de moteur de recherche en interne.

Selon une étude menée par MARKESS International en février 2008, 49 % des organisations ont déjà recours à un moteur de recherche d'entreprise, et 18 % envisagent son utilisation d'ici à 2010. Ces moteurs de recherches sont en majeure partie intégrés aux postes de travail ou aux outils de gestion électronique des documents, mais ils sont dans un nombre grandissant d'entreprises capables de couvrir à la fois les contenus internes et externes de l'entreprise, ou encore intégrés aux outils de gestion de contenu ou aux solutions décisionnelles.

Parmi quelques acteurs proposant des moteurs de recherche d'entreprise figurent DatAnswers (Varonis), Google, Exalead, Multimédia SOLUTIONS, Sinequa, PolySpot, Synomia, OpenSearchServer, Verticrawl, Fast ESP, Endeca, Autonomy, Constellio, Aleph-networks, Pertimm, Datafari…

Les technologies d'analyse du langage, telles que la lemmatisation, l'extraction d'entités nommées, la classification et le clustering permettent d'améliorer grandement le fonctionnement des moteurs de recherche. Ces technologies permettent tout à la fois d'améliorer la pertinence des résultats et d'engager l'internaute dans un processus de recherche plus performante, comme c'est le cas avec la recherche à facettes .

Selon l'étude de l'ADEME « Internet, courriels, réduire les impacts » publiée en février 2014, aller directement à l’adresse d’un site, soit en tapant son adresse dans son navigateur, soit en l’ayant enregistré comme « favori » (plutôt que de rechercher ce site via un moteur de recherche) divise par 4 les émissions de gaz à effet de serre.







</doc>
<doc id="1952" url="https://fr.wikipedia.org/wiki?curid=1952" title="Mento">
Mento

Le mento est la première musique populaire jamaïcaine. Il apparait à la fin du dans les zones rurales de l'île. C'est un des styles musicaux à l'origine du reggae (À travers le ska, le rocksteady et le early reggae).

Le mento est tout simplement la musique que les paysans jamaïcains, après leur journée de labeur, aimaient jouer et écouter pour se divertir et oublier un instant la dureté de leur condition de vie. Ce terme décrit également la danse libre qui l'accompagne, et qui plonge ses racines dans les rituels ashantis, et d'autres ethnies ouest-africaines. En raison de la mode du calypso de l'île de la Trinité dans les années 1940, dès 1951 (année marquée par l'ouverture du premier studio d'enregistrement en Jamaïque par Stanley Beresford Brandon Motta) les premiers enregistrements de mento portent le plus souvent l'étiquette plus vendeuse de "calypso". Les deux plus grands succès de l'Américain d'origine jamaïcaine Harry Belafonte, "Day O" et "Jamaica Farewell" sont par exemple des mentos, et non des calypsos, mais ils ont été publiés dans l'album "Calypso", ce qui contribue à la confusion. Ce genre a précédé le ska et le reggae, apparus dans l'après-guerre sous l'influence des musiques populaires des États-Unis.

D'origine rurale, le mento utilise traditionnellement des instruments comme le banjo, la guitare, la flûte, le fifre, les maracas, des percussions, mais également un lamellophone basse appelé la rumba box (dérivée de la marimbula) ou thumb piano, le violon, et le saxophone de bambou. Il existe en version urbaine, interprété dans les cabarets et hôtels de Jamaïque, où il connaît une forte influence du jazz (saxophone, trompette, piano, etc.).

Les thèmes fréquemment abordés par le mento sont les critiques de la vie sociale et politique, des chants de travail, et des textes à double sens où la sexualité a une grande importance.




Les textes les plus complets parus en français sur l'histoire du mento se trouvent dans le livret du double CD "Jamaica - Mento 1951 - 1958" ( Frémeaux & Associés) et "Harry Belafonte - Calypso Mento Folk 1954 - 1957" (Frémeaux & Associés) et sont signés Bruno Blum.



</doc>
<doc id="1953" url="https://fr.wikipedia.org/wiki?curid=1953" title="Metatheria">
Metatheria

Les métathériens (Metatheria) constituent l'infra-classe de mammifères thériens regroupant les marsupiaux et toutes les espèces plus proches de ces derniers que des placentaires. C'est le taxon frère des euthériens, dont ils se seraient séparés il y au moins 147,4 millions d'années d'après l'horloge moléculaire. D'après les registres fossiles, ces deux taxons auraient divergé au Jurassique il y a au moins environ 160 millions d'années.

Les métathériens se distinguent des autres mammifères par divers aspects squelettiques, systémiques (développement, appareils reproducteurs, etc) et écologiques.

Parmi les caractéristiques des métathériens, ils conservent les os épipubis qui servent, chez beaucoup de femelles des lignées actuelles, au soutien du marsupium lorsqu'il y en a un. Au niveau du crâne, le palais osseux présente une à deux paires de fenêtres supplémentaires par rapport aux euthériens.

Le remplacement des dents tend aussi à différer des euthériens.

Les fossiles de métathériens primitifs sont caractérisables par leur formule dentaire, typiquement 4.1.3.4 pour le demi-maxillaire et 3.1.3.4 pour la demi-mandibule. Par ailleurs les sparassodontes font montre d'une étonnante convergence évolutive avec les euthériens relativement à la morphologie de leurs dents.

Chez les mâles, le pénis est généralement bifide et internisé dans le cloaque en dehors de la période de reproduction. Les femelles métathériennes ont deux ovaires, deux trompes de Fallope et deux utérus ayant une ouverture dans deux vagins séparés par une simple cloison dans leur partie antérieure qui débouche dans le sinus uro-génital. Cela est dû à une divergence évolutive d'avec les euthériens concernant le développement des canaux de Wolff et Müller.

En revanche, les protothériens actuels (monotrèmes) ont assez similairement des appareils reproducteurs dédoublés comme les métathériens.

L'organisation du néocortex chez les différentes lignées est grandement diversifée, à l'image des euthériens. Néanmoins il existe des recouvrements entre aires motrices et aires sensorielles, de façon plus importante que chez les euthériens.

La nécessité chez les métathériens d'avoir des membres antérieurs adaptés à la reptation de la larve vers les mamelle, ainsi que la très longue et très précoce période d'allaitement au cours du développement, seraient deux facteurs de survie et développement de l'organisme assez "contraignants", qui pourraient contribuer à limiter la diversification morphologique des épaules et des crânes au sein des métathériens, résultant notamment en une moindre occurrence d'espèces adaptées au vol ou à la vie aquatique au sein de ce clade.

Les métathériens se sont probablement séparés des euthériens (la branche des mammifères placentaires) au cours du Jurassique (il y a au moins 165 millions d'années), mais aucune trace fossile de métathériens n'est connue datant de cette époque. Cette datation est déduite de l'âge du plus ancien euthérien récemment découvert, "Juramaia".
Les métathériens fossiles se distinguent des euthériens par la forme de leurs dents, ils possèdent quatre paires de molaires à chaque mâchoire, tandis que les euthériens (y compris les placentaires) n'ont jamais plus de trois paires. Selon l'utilisation de ce critère, le plus ancien métathérien connu est "Sinodelphys", découvert en Chine et qui vivait il y a environ 125 millions d'années. Cela en fait un contemporain de certaines espèces d'euthériens qui ont été trouvés dans le même secteur.

Les ordres fossiles
Et les genres fossiles suivant.




</doc>
<doc id="1954" url="https://fr.wikipedia.org/wiki?curid=1954" title="Ménès (pharaon)">
Ménès (pharaon)

Mény (plus connu sous Ménès, son nom grec) est un roi considéré comme le fondateur de la thinite.

Il est souvent assimilé au roi Narmer (son prédécesseur) ou parfois au roi Hor-Aha (son successeur).

On situe son règne vers -3150.

Son règne se perd dans l'origine des mythes égyptiens qui font de lui le premier homme à avoir régné sur l'Égypte après le dieu Horus et les demi-dieux. Certains voient en lui celui qui a inspiré le mythe d'Osiris.

On considère généralement qu'il est le roi appelé Méni (ou Mény) par la liste royale d'Abydos et le canon royal de Turin, le roi Menas cité par Diodore de Sicile, et également Ménès par Manéthon qui lui compte soixante ans de règne.

On a retrouvé un nom Ménès sur une tablette du roi Hor-Aha mais c'est peut-être un roi défunt honoré par son successeur. Certaines hypothèses donnent à la racine "mn" (qui, inscrit dans un serekh, forme le nom de ce roi) le sens de « Celui qui établit », alors que d'autres lui donne le sens de « Quelqu'un ».

. Il semble qu'il ait apporté une grande prospérité au pays. Il crée des places fortes dans la région de Gaza et une seconde capitale au point de jonction des deux pays à Memphis, qui avec This, dont il est originaire, lui permet de mieux contrôler le pays. Il semble qu'il soit le premier à porter la double couronne, le pschent. Ce fait se renouvèlera à Memphis jusqu'à l'époque grecque. .

Il est enterré dans le cimetière (tombe B17-B18) d'Oumm El-Qaab à Abydos.



</doc>
<doc id="1956" url="https://fr.wikipedia.org/wiki?curid=1956" title="Militaire">
Militaire

Un militaire est un membre des forces armées « régulières », c'est-à-dire d'une institution de défense d'un État. On emploie également le terme soldat lorsqu'il s'agit d'un combattant, le terme mercenaires étant réservé aux combattants recrutés sans statut particulier le temps d'un conflit ou même d'une opération.

Les forces militaires sont constituées :

Pour acquérir une spécialité ou pour un entraînement général préalable, les militaires suivent des stages de préparation militaire et/ou de formation militaire.

Les militaires comprennent bien souvent des militaires d'active, en fonction effective dans les forces, et des militaires de réserve, entrainés en vue d'une mise à disposition éventuelle.

L'une des caractéristiques du militaire est son obéissance à la discipline militaire et aux ordres reçus, tout particulièrement en temps de guerre, circonstances dans lesquelles, d'une façon générale, ses droits personnels sont très limités. Le refus d'obéir ou la désertion sont rigoureusement punis, que ce soit dans une armée de métier ou de conscrits. Il est généralement proscrit, pour un militaire, d'être syndiqué, membre d'une association ni d'un parti politique.

De nombreux groupes armés en Afrique, en Asie et en Amérique du Sud intègrent des enfants (dits enfants soldats) à partir de l'âge de six ans. On estime le nombre de ceux-ci à au cours de l'année 2010.

Un militaire peut avoir une activité de terrain, de commandement. En effet, la mise en œuvre d'une armée nécessite une composante logistique et commandement militaire importante. Ces différentes fonctions sont souvent désignées comme des « échelons » : échelon de commandement, échelon logistique, échelon opérationnel.

Parmi les emplois opérationnels, on distingue :

Le commandement militaire épouse généralement la hiérarchie, reflétée par les grades. 

Le coût global d'un militaire pour l'État qui l'emploie (solde, formation, nourriture, logement, assurance, matériels, retraite, pensions d'invalidité, etc.) est extrêmement divers selon les situations. Alors que la grande majorité des soldats de l'Antiquité devaient se payer leur équipement, situation qui prévalut jusqu'au Moyen Âge avec les chevaliers ayant leur propre harnachement, celui-ci est désormais fourni dans l'immense majorité par les armées régulières.

Un simple conscrit employé comme fantassin durant les "guerres industrielles" du début du et dans des États du Tiers-Monde coûte évidemment beaucoup moins cher qu'un pilote de combat professionnel des années 2000 dont la formation s'étire sur des années.

Cas extrême, un militaire professionnel dans les forces armées des États-Unis, de la signature du contrat d'engagement à la mise en terre, coûte en moyenne cinq millions de dollars américains au département de la Défense en 2005 et ces frais ne cessent d'augmenter.

Le déploiement de troupes bien équipées sur les théâtres d'opérations lointains est également onéreux.

En 2010, un soldat des Forces canadiennes de la Force internationale d'assistance et de sécurité en Afghanistan, pays enclavé et ne disposant pas d'importantes infrastructures, revient, hors solde, hors acheminement initial, à environ dollars canadien ( euros) ; la dépense par soldat engagé outre-mer se situant en moyenne annuelle 5,16 millions d’euros dans l’Union européenne en 2011. Pour le soutien et le fonctionnement de base d'un militaire engagés en opérations extérieures, il faut en moyenne, toujours dans l'Union européenne, 16 militaires et 15 civils à domicile (35 militaires et 15 civils en Allemagne, 8 militaires et 2 civils en France, 9 militaires et 4 civils au Royaume-Uni).

Dans le cadre de son engagement dans les forces armées de son pays, le militaire s’astreint volontairement à servir la défense des intérêts vitaux du pays, tels qu’ils sont définis par son gouvernement, et sous les ordres de ses supérieurs hiérarchiques (officiers généraux). Les missions que le militaire effectue dans l’armée correspondent à 3 grands axes :

Ces 3 grands axes, qui déterminent l’action des forces armées d’un pays, se manifestent par une gamme étendue et variée de missions où le risque suprême de trouver la mort en exerçant son métier - qui est le propre de la profession de militaire - n’est pas conditionné par le caractère vital pour le pays des missions qui lui sont confiées. La défense de l’intégrité du sol et du territoire nationaux figure au premier plan de l’engagement particulier des membres des forces armées, qu’ils soient militaires du rang, officiers, volontaires recrutés dans la population civile ou conscrits.

C’est dans une telle éventualité que les militaires s’entraînent à combattre ou bien encore à soutenir les troupes engagées à l’avant du front. Une capacité de riposte permanente qui met aussi le professionnel de la guerre en mesure d’être engagé dans des opérations de moins haute intensité, sur le territoire ou à l’extérieur des frontières. On citera pêle-mêle la lutte contre le terrorisme, les menaces chimiques et bactériologiques, la surveillance du ciel, "etc".

Cette préparation à l’aptitude au combat offre également l’opportunité au gouvernement d’envoyer les militaires exercer leurs talents en dehors des frontières de leur pays, en les engageant dans des opérations de pacification sous mandat international, et/ou dans l’action civilo-militaire qui se situe souvent en corollaire de la première option.


</doc>
<doc id="1957" url="https://fr.wikipedia.org/wiki?curid=1957" title="Mario Vargas Llosa">
Mario Vargas Llosa

Mario Vargas Llosa (), marquis de Vargas Llosa, né le à Arequipa, région d'Arequipa, au Pérou, est un écrivain péruvien, auteur de romans et d'essais politiques. Il est notamment lauréat du prix Nobel de littérature 2010 .

Comme beaucoup d'auteurs hispano-américains, Mario Vargas Llosa s'est engagé en politique tout au long de sa vie. Ses opinions se sont progressivement déplacées du communisme au libéralisme. Il soutient initialement le gouvernement révolutionnaire de Fidel Castro, mais est rapidement déçu. En 1990, il est candidat à l'élection présidentielle péruvienne à la tête d'une coalition, le Front démocratique (Fredemo), qui perd face à Alberto Fujimori.

Mario Vargas Llosa est issu de la classe moyenne péruvienne. Il est le fils unique d'Ernesto Vargas Maldonado et de Dora Llosa Ureta. Ses parents se séparent quelques mois après sa naissance à la suite de la révélation, par son père, d'une liaison avec une femme allemande qui donnera deux demi-frères au jeune Mario : Ernesto et Enrique Vargas.
Élevé par sa famille maternelle, Mario Vargas Llosa passe du Pérou à la Bolivie où son grand-père tient une plantation de coton. Sous le gouvernement de José Luis Bustamante y Rivero, l'aïeul se voit offrir un poste diplomatique à Piura. Cet épisode marque le retour des Llosa au Pérou. En 1946, à l'âge de 10 ans, Mario part vivre à Lima où il rencontre son père pour la première fois alors qu'il l'avait longtemps cru mort. Ses parents se remettent ensemble et déménagent à Magdalena del Mar, une banlieue aisée de la capitale. Il est admis à l'école élémentaire catholique "Colegio La Salle"

À l'âge de 14 ans, il est envoyé en internat à l'Académie militaire de Lima par son père qui ne voit pas d'un bon œil sa vocation poétique naissante. Cet épisode lui laisse un sinistre souvenir et la matière de son livre "La Ville et les chiens".

Il étudie ensuite la littérature et le droit à l'Université San Marcos, une faculté publique, exerçant en parallèle différentes professions : correcteur littéraire puis collaborateur aux rubriques cinéma de la revue "Literatura" (1957-1958) et du journal "El Comercio". Durant ses études, il découvre l'œuvre de Jean-Paul Sartre et le marxisme qui le marquent durablement. Il combat également la dictature militaire du général Manuel Odría. Pendant une brève période, il s'implique dans une branche étudiante du Parti communiste péruvien qu'il abandonne en protestation de la ligne stalinienne du mouvement sur l'art et la littérature. La révolution cubaine fait un temps revivre ses espoirs d'une révolution progressiste.

Grâce à une bourse d'étude, il poursuit son cursus universitaire à Madrid où il soutient, en 1958, une thèse de doctorat sur Rubén Darío. Après avoir écrit un recueil de nouvelles remarqué, "Les Caïds" ("Los Jefes", 1959), œuvre qui obtient le Prix Leopoldo Alas, il épouse la belle-sœur de son oncle maternel : sa tante par alliance Julia Urquidi, de dix ans son aînée. Cette relation lui inspire des années plus tard le roman "La Tía Julia y el escribidor" ("La Tante Julia et le scribouillard"). En 1964, il se sépare de Julia Urquidi et se remarie avec sa cousine Patricia Llosa, avec qui il aura trois fils et dont il divorcera en 2015. Depuis 2015, il est en couple avec Isabel Preysler. Avec sa première épouse, il s'installe à Paris en 1959 dans l'espoir de recevoir une bourse pour reprendre des études, mais sa demande est rejetée. Le couple reste malgré tout dans la capitale française et Vargas Llosa y travaille en tant que professeur d'espagnol à l'école Berlitz, puis journaliste pour l'Agence France-Presse et la télévision. Il se passionne pour la littérature du pays, suit avec intérêt la querelle opposant Sartre à Albert Camus et écrit de manière prolifique. Il part ensuite pour Londres et Barcelone où il côtoie les grandes figures de la Gauche divine. Pendant son séjour en Europe, il se lie d'amitié avec d'autres jeunes auteurs, futurs piliers du boom latino-américain : l'Argentin Julio Cortázar, le Mexicain Carlos Fuentes et le Colombien Gabriel García Márquez. Il retourne à Lima en 1974 et est élu à l'Académie péruvienne un an plus tard.

Avec Julio Cortázar, Carlos Fuentes, Juan Rulfo, Gabriel García Márquez, Juan Carlos Onetti et José Donoso, Mario Vargas Llosa est considéré comme l'un des grands noms du boom de la littérature latino-américaine des années 1960. À des degrés divers, tous ces auteurs prennent leurs distances avec la narration traditionnelle et revendiquent l'influence des courants littéraires moderniste et postmoderne européens ou nord-américains auxquels ils empruntent des procédés novateurs (détournement des codes fictionnels, multiplicité des points de vue, polyphonie, morcellement de la chronologie, monologue intérieur ou encore flux de conscience sur l'exemple de James Joyce et William Faulkner). Leur style visionnaire, foisonnant et luxuriant a révélé au monde entier la complexité artistique, idéologique et politique du continent sud-américain qu'ils peignent comme une entité pittoresque, morcelée et paradoxale.

Dès la parution de son premier roman, Vargas Llosa devient un écrivain reconnu, régulièrement invité dans les universités du monde entier pour y donner des cours et des conférences. À la fois chroniqueur et pourfendeur de l'Amérique latine, il est considéré par une partie de la critique comme le maître du « bouillonnement romanesque ». Contrairement à ses collègues du boom, Vargas Llosa s'écarte totalement du réalisme magique en vigueur. Mais ses récits gardent la spécificité latino-américaine de changer régulièrement de voix pour passer du général au particulier en opposition aux littératures européenne et anglo-saxonne qui ont tendance à partir d'un caractère particulier pour dériver vers le général. Le romancier cherche également à rompre avec la veine indigéniste, dominante dans les lettres sud-américaines visant avant tout à atteindre l'universel dans l'écriture.

Les ouvrages de Vargas Llosa trahissent l'influence de Faulkner pour les recherches stylistiques et Balzac pour la densité de l'observation psychologique et sociale. Ils se démarquent par un style polyphonique, une ironie mordante et une tonalité dramatico-bouffonne dans l'évocation des mythes et des aspirations des peuples latino-américains écrasés par les dictatures. Ses récits sont identifiables par une fragmentation de la chronologie et la pluralité de narrateurs. Par ailleurs, ses personnages sont inséparables du climat et du cadre culturel, historique et géographique dont ils sont issus. L'action de ses romans débute sur une acmé qui installe une atmosphère oppressante, enfermant les protagonistes dans un engrenage implacable. Par le biais d'une écriture épique, apparemment sans effets, Vargas Llosa retranscrit les mutations brutales d'une civilisation marquée par la violence et le sexe. Dans ses fictions, les pouvoirs politiques (notamment le caudillisme) apparaissent comme le symbole du pourrissement moral de la société. Au fil de son travail romanesque, Vargas Llosa dessine une cartographie métissée et cosmopolite issue de ses voyages et de ses expériences personnelles, le Pérou étant néanmoins un invariant thématique dans ses romans.

Vargas Llosa rédige "La Ville et les Chiens" à Paris en 1963, ouvrage qui fait de lui un auteur de renom (prix Biblioteca Breve du roman et ). Son roman est traduit presque aussitôt dans une vingtaine de langues et se voit salué par la presse étrangère pour son originalité. Vargas Llosa y juxtapose une tradition romanesque classique à des recherches d'écriture novatrices sur le plan narratif et formel. Dans cette œuvre, un réalisme folklorique lié au costumbrismo se mêle à des envolées poétiques proches du symbolisme. Le romancier décrit alors la vie menée par les cadets (les "chiens") et met en contraste l'oppression de la discipline, la violence et les brimades subies par les jeunes gens avec le vent de liberté qui souffle sur la ville. L'auteur est vivement critiqué dans son pays pour s'être attaqué à l'institution militaire. On l'accuse d'être stipendié par l'Équateur pour déstabiliser l'armée péruvienne et cent exemplaires du roman sont brûlés lors d'une cérémonie expiatoire dans la cour du collège militaire de Lima. Cependant, le livre n'est pas interdit à la vente et connaît un grand succès public au Pérou.

Dans "La Maison verte" (1966), l'auteur évoque, avec un grand souci du détail et un impressionnant souffle narratif, la vie dans la lointaine forêt péruvienne et la zone urbaine de Piura. Il y met en scène une maison close dans laquelle se croisent divers personnages. Ce roman lui vaut à nouveau le prix de la Critique, puis le prix international de littérature Rómulo Gallegos en 1967. Vargas Llosa y approfondit sa technique expérimentale de « narrations télescopiques » et de « vases communicants », selon ses propres termes, qu'il tire de Faulkner. Ce procédé consiste à entrecroiser simultanément plusieurs histoires se déroulant en divers lieux et époques.

Parmi les autres romans de Vargas Llosa, on retient "Conversation à la cathédrale" (1969), variation kaléidoscopique sur la figure du père et portrait corrosif des dirigeants péruviens dans un récit qui emprunte sa structure au roman policier. Comme conteur expérimenté, l'auteur continue d'entrelacer histoires, situations, temporalités, personnages et décors de manière vertigineuse. Il s'agit de l'ouvrage qui lui demande le plus de travail et qu'il sauverait s'il fallait n'en garder qu'un. "Pantaléon et les visiteuses" (1973) se conçoit comme une satire paillarde, burlesque et subversive du fanatisme militaire et religieux au Pérou. "La Guerre de la fin du monde" (1982), qui traite de la politique brésilienne au et de la guerre de Canudos, rencontre un immense succès critique et public, marquant le sommet de sa carrière de romancier. "Qui a tué Palomino Molero ?" (1986) est un roman consacré aux violences politiques péruviennes. Dans cette œuvre, Vargas Llosa donne un court .

En dehors des grandes fresques, Vargas Llosa s'essaie à un registre intimiste et semi-autobiographique avec "La Tante Julia et le Scribouillard" (1977) et "Éloge de la marâtre" (1990). "La Fête au bouc" (2000), qui évoque les derniers jours du dictateur dominicain Rafael Leonidas Trujillo, revient à la polyphonie, au genre épico-politique et à la peinture romanesque du pouvoir dans le pur esprit ibéro-américain. En effet, l'ouvrage est caractéristique du roman du dictateur, représenté entre autres par Miguel Ángel Asturias ("El señor Presidente"), Augusto Roa Bastos ("Moi, le Suprême") et Gabriel García Márquez ("L'Automne du patriarche"). "Le héros discret" (2013) fonde la chronique du Pérou actuel, de sa grande bourgeoisie à ses classes les plus défavorisées, et brosse un portrait au vitriol d'une société gangrénée par la corruption, la pauvreté, les inégalités sociales et la culture de masse.

Vargas Llosa a également écrit des pièces de théâtre et des essais littéraires comme "L'Orgie perpétuelle" (1975) et "La Tentation de l'impossible" (2008), consacrés respectivement à Gustave Flaubert et Victor Hugo. Il a, de plus, publié des mémoires ("Contre vents et marées", "Le Poisson dans l'eau") et des réflexions politiques sur l'Amérique latine ("La Voie de la liberté"). En 2012, il signe un essai intitulé "La civilización del espectáculo" dans lequel il fustige la société de divertissement contemporaine et le dépérissement des arts.

Vargas Llosa est d'abord tenté par le communisme et déclare son soutien à la guérilla péruvienne, considérant la lutte armée « seul recours » pour changer les choses au Pérou. Mais la révolution cubaine, qu'il soutient ardemment au départ, le déçoit à tel point qu'il se tourne directement vers le libéralisme. Le Printemps de Prague en 1968 et ses lectures d'Alexandre Soljenitsine, Raymond Aron et Jean-François Revel le confortent dans son changement brutal d'opinion, l'éloignant encore un peu plus de l'idéal révolutionnaire. Dès lors, il ne manifeste aucune retenue dans sa virulente critique du castrisme ou encore de la Révolution sandiniste au Nicaragua. Son positionnement est qualifié d'« ultra libéral » par l'universitaire Serge Audier (Paris IV). Son parcours intellectuel est influencé par quatre auteurs : Adam Smith, Karl Popper, Friedrich Hayek et Isaiah Berlin. Il lit également avec avidité les ouvrages d'économie de Milton Friedman et apporte son soutien aux politiques austéritaires de Ronald Reagan et Margaret Thatcher. Au Pérou, il fonde le mouvement de droite libérale "Libertad".

Candidat libéral à l'élection présidentielle péruvienne de 1990, il est sévèrement battu au second tour, contre toute attente car il a l'appui des médias et des élites (sa campagne électorale est la plus chère de l'histoire du Pérou), par un inconnu d'origine japonaise, Alberto Fujimori contre lequel ses partisans tentent, malgré lui, de monter la population péruvienne en stigmatisant la communauté asiatique. À la suite de cette défaite, il quitte le Pérou pour s'établir en Espagne, à Madrid. Vargas Llosa, qui a demandé et obtenu la nationalité espagnole en 1993 du gouvernement de Felipe González, reconnaît qu'il se sent espagnol, autant que péruvien. Ce changement de nationalité, trois ans seulement après avoir été candidat à l'élection présidentielle de son pays, provoque des réactions très critiques au Pérou. Ainsi, dans la conférence du 7 décembre 2010 en tant que lauréat du prix Nobel, il déclare : .
Devant l'Académie de Stockholm, il déclare également, à propos de ses positions : .

Partageant sa vie entre l'Europe et l'Amérique du Sud, il continue de soutenir la politique de rigueur des gouvernements conservateurs occidentaux, notamment de José María Aznar en Espagne et Silvio Berlusconi en Italie. Il se tourne vers des positions néo-conservatrices sur les questions internationales, justifiant l'invasion de l'Irak en 2003 et le coup d’État militaire en 2009 contre le gouvernement de gauche de Manuel Zelaya au Honduras.

En 2007, Vargas Llosa est membre fondateur du parti espagnol UPyD (Union, progrès et démocratie) qui s'auto-définit comme progressiste.

En avril 2011, lors des élections présidentielles péruviennes, il appuie le vote du candidat nationaliste Ollanta Humala, contre la candidate Keiko Fujimori, fille de l'ancien président Alberto Fujimori (condamné pour corruption), son adversaire durant les présidentielles de 1990. 

En avril 2016, son nom, ainsi que celui de son ex-épouse Patricia Llosa Urquidi, figurent dans les documents du cabinet panaméen Mossack Fonseca dans l'affaire des "Panama Papers".

Dans le cadre des troubles politiques qui suivent le référendum de 2017 sur l'indépendance de la Catalogne, il se positionne contre l'indépendance en prenant la parole à la fin d'une manifestation.

En 2017, il s'oppose à la grâce d' Alberto Fujimori accordée par le président Pedro Pablo Kuczynski.

Mario Vargas Llosa est membre de l'Académie royale espagnole. Il a reçu les récompenses les plus prestigieuses de la littérature hispanophone et mondiale, notamment le prix Rómulo Gallegos en 1967, le prix Cervantes en 1994, le prix Jérusalem en 1995 et, en 2005, le "Irving Kristol Award" de l'American Enterprise Institute. Il prononce alors un discours remarqué, "Confessions d'un libéral" ("Confessions of a Liberal").

Vargas Llosa est titulaire de quarante doctorats "honoris causa" à travers le monde, parmi lesquels celui de l'université nationale majeure de San Marcos (son "alma mater"), de l'université Rennes 2 Haute Bretagne, de l'université de Reims Champagne-Ardenne depuis le ou encore de l'université de Bordeaux 3 depuis le .

Le , il reçoit le prix Nobel de littérature pour , selon l'explication de l'Académie suédoise.

La même année, il est titré marquis de Vargas Llosa par le roi d'Espagne, Juan Carlos. 

En 2016, il devient le premier auteur de langue étrangère à entrer de son vivant dans la Bibliothèque de la Pléiade.

Grand aficionado, Mario Vargas Llosa a pris la tête d'un mouvement de défense de la corrida qu'il considère comme une culture de masse, et une culture à protéger. Pour cela, il a publié un manifeste dans lequel il déclare : L’écrivain Bryce Echenique, le poète Antonio Cisneros se sont associés à lui. Il a aussi recueilli l'appui d'un nombre de personnalités du monde de la culture qui ont signé ce manifeste, d'un groupe d'intellectuels, d'artistes. Il a reçu également l'appui du juriste Diego García Sayán, vice-président de la Cour interaméricaine des droits de l'homme lors de ses déclarations au Pérou. Enfin il ne perd jamais une occasion de rédiger des articles de soutien à la corrida dans plusieurs journaux.

Après avoir fait l'éloge de "Cent ans de solitude", qualifié de d'Amérique latine, Vargas Llosa se lie d'amitié avec Gabriel García Márquez lorsqu'il le rencontre à l'aéroport de Caracas le . Les deux auteurs participent alors au international de littérature ibéro-américaine et le Péruvien reçoit le Prix Rómulo Gallegos pour "La Maison verte", récompense que le Colombien obtient cinq ans plus tard pour "Cent ans de solitude". Toutefois, Vargas Llosa refuse de reverser l'argent de la distinction au régime castriste comme il y est incité alors que García Márquez financera un mouvement révolutionnaire vénézuélien grâce au prix.

En 1971, Vargas Llosa publie "García Márquez : Histoire d’un déicide", livre critique dans lequel il fait part de son admiration pour son aîné. Les deux complices sont par ailleurs un temps voisins à Barcelone. Cette relation amicale très forte s'achève brutalement le lorsqu'à la première des "Survivants des Andes", García Márquez reçoit un coup de poing en plein visage de la part de Vargas Llosa dans le hall d'un cinéma de Mexico. Les motifs de cette querelle restent flous mais seraient d'ordre privé : soit il s'agirait de la relation difficile, en raison d'infidélités répétées, entre l'écrivain péruvien et sa seconde épouse Patricia Llosa dont García Márquez aurait pris la défense, soit d'une liaison qu'aurait eue l'auteur colombien avec elle. D'autres raisons moins triviales, notamment la divergence de points de vue politiques, sont évoquées.

Les deux anciens amis, qui ne se reverront plus, refusent de révéler la moindre information sur le sujet. Durant 35 ans, Vargas Llosa fait interdire toute nouvelle publication de son livre sur García Márquez. Après la mort de ce dernier en 2014, le Péruvien affirme avoir noué un pacte avec lui pour garder à jamais le silence sur la cause de cette amitié brisée. Reconnaissant à son ex-complice d'avoir tenu sa promesse jusqu'à la fin, il affirme vouloir en faire autant et laisser les historiens et biographes faire la vérité sur cette affaire.









</doc>
<doc id="1958" url="https://fr.wikipedia.org/wiki?curid=1958" title="Mitsubishi">
Mitsubishi

Créée en 1869 par Iwasaki Yatarō, elle est d'abord une entreprise de transport maritime exploitant des bateaux à vapeur. Le nom Mitsubishi, vient du pavillon des bateaux de la société, qui représente trois diamants ("Mitsu-" Trois et "-bishi" mâcre nageante, macle ou losange).

C'est en 1873 que la compagnie prend le nom de "Mitsubishi Shokai" et commence à investir dans l'exploitation minière avec l'achat de la mine de cuivre de Yoshioka, de l'importante mine de charbon de Takashima en 1881, qui représentera 91,6 % des bénéfices totaux du groupe en 1885, et de la mine d'argent d'Ikuno en 1896. Sous l'impulsion de son fondateur, Mitsubishi deviendra un des zaibatsu les plus puissants du Japon.

À partir de la fin du , la compagnie (qui gère à elle seule la moitié du trafic maritime japonais) entre dans une phase de diversification qui aboutira à la création de diverses entités dont :


Pendant les guerres menées par le Japon en Asie l'entreprise, en symbiose avec l'armée japonaise, s'occupait du commerce entre le Japon, la Chine et le Manchoukouo, et en particulier de l'importation de l'opium persan, participant ainsi à l'intoxication massive du continent chinois.

À la fin de la Seconde Guerre mondiale, le Japon est occupé par les Américains. Le général MacArthur gouverne le pays. Les États-Unis veulent éliminer les "zaibatsu", les principaux groupes du complexe militaro-industriel japonais, tels Mitsubishi (qui avait produit, entre autres, les fameux chasseurs Zero).
L'administration prononce donc la dissolution du groupe, avec interdiction de se reformer, le . D'autres groupes, tel Sumitomo, subiront le même sort. Les différentes sociétés du groupe Mitsubishi ont interdiction de conserver des liens financiers, et leur emblème est même interdit. Ce dernier reviendra progressivement.

Officiellement, depuis cette époque, les sociétés n'ont plus qu'un seul lien : une ancienne maison, où se retrouvent régulièrement les présidents pour distribuer des subventions (autorisées) aux partis politiques et décider des filiales autorisées ou non à porter le nom et l'emblème Mitsubishi.
Mais de fait, très lentement, on assiste, de ci, de là, à des regroupements, ou rapprochements.

Les sociétés Mitsubishi sont considérées comme traditionnelles, plutôt productrices de biens et services haut de gamme. Elles sont en général très attentives à leur image de marque. Beaucoup travaillent en grande partie pour les administrations.

"Selon le site officiel, en août 2005:"




</doc>
<doc id="1959" url="https://fr.wikipedia.org/wiki?curid=1959" title="Méga">
Méga

Méga (symbole M) est le préfixe du Système international d'unités, qui multiplie par 10 (un million) l'unité qui le suit (exemple : 1 mégapixel = 1 million de pixels). Il est parfois abrévié, comme dans « mégohm » ( = ).


</doc>
<doc id="1961" url="https://fr.wikipedia.org/wiki?curid=1961" title="Mythologie romaine">
Mythologie romaine

La mythologie romaine ou latine est l'ensemble des légendes et des mythes de la Rome antique.

D'origine indo-européenne, la mythologie romaine a emprunté au fil des siècles des conceptions religieuses et culturelles aux pays qui ont été peu à peu intégrés dans la sphère de Rome : la Grèce, l'Égypte, la Syrie, etc.

Les Romains se sont approprié puis ont adapté ces mythologies pour créer un ensemble syncrétique qui se manifeste dans la religion romaine.

La majorité des divinités du panthéon romain a très tôt subi l'influence de la Grèce antique et les divinités locales (ou « indigètes »), à quelques rares exceptions, ont souvent été assimilées à leurs homologues grecs. Pour cette raison, les articles consacrés aux dieux romains sont traités avec leurs équivalents grecs. Rome ayant largement assimilé la culture hellénistique, il est difficile de cerner les croyances des premiers Romains. Pourtant, les dieux de Rome ont des noms originaux qui les différencient de leurs homologues grecs : certes, à l'époque impériale, de nombreux dieux romains ont absorbé les attributs de dieux grecs ; néanmoins, l'étude minutieuse des noms romains de ces dieux ainsi que des cultes qui leur sont liés permet parfois de retrouver la nature première de ces anciennes divinités, qui étaient à l'origine proprement italiques.

Si on considère à tort la mythologie romaine comme négligeable par rapport à la mythologie grecque, c'est parce que les mythes romains portent principalement sur l'histoire de Rome, tandis que les mythes grecs sont axés sur les dieux et les héros. Mais malgré l'absence de cosmogonie ou de théogonie d'origine romaine, la mythologie romaine n'en existe pas moins, notamment à travers un nombre conséquent de récits de fondations de cités. Des chercheurs tels que Georges Dumézil et T. P. Wiseman insistent également sur le fait que les notions de mythe et d'histoire ne sont nullement exclusives l'une de l'autre dans la Rome antique, et qu'il est donc possible de parler de mythologie romaine, même si elle s'est construite en bonne partie sur des récits conçus comme historiques.

Les divinités des premiers Romains ("numina") ont rapidement disparu en raison de leur caractère abstrait qui s'oppose à l'anthropomorphisme grec. Malgré l'influence hellénistique, plusieurs divinités locales ont subsisté, notamment dans le culte de Janus, de Saturne, de Quirinus et le culte privé de Vesta ou des dieux Lares.

Vulcain, identifié au dieu grec Héphaïstos est le dieu forgeron. Ses attributs sont un marteau, une enclume et une tenaille. Ses fonctions sont de protéger contre le feu destructeur et de veiller au feu civilisateur, et son épouse est la déesse Vénus. Vulcain est le seul enfant de Junon et Jupiter. Son corps est difforme, mais, il réussit à épouser Vénus, la déesse de la beauté.

Janus est une des seules divinités des premiers Romains ayant subsisté. Profondément lié au mythe de l'âge d'or, Janus serait le roi latin ayant accueilli Saturne lors du règne de celui-ci sur terre. Après la modification du panthéon romain, Janus gardera une place moindre, celle de dieu des ouvertures et des portes et de protecteur de Rome en temps de guerre. Saturne, plus tard assimilé à Cronos (à ne pas confondre avec Chronos dieu du temps), est également honoré durant les Saturnales.

Dieu archaïque, Quirinus est originellement le protecteur des citoyens romains (les Quirites) et, associé à Jupiter et Mars, fait partie de la triade primitive de la mythologie romaine. Il sera plus tard assimilé à Romulus divinisé.

Protecteur des troupeaux contre les loups (d'où son second nom Lupercus), il sera honoré durant les Lupercales jusqu'en 496. On parlera ensuite de faunes, pluralité qui les associera aux satyres grecs et qui assimilera Faunus à Pan.

La plupart des divinités des premiers Romains liées au foyer demeurent dans le culte romain : les Romains vénèrent les Lares et les Pénates, ainsi que les ancêtres morts (mânes). Selon la légende, les Pénates originels proviendraient de Troie. C'est Énée qui, en s'enfuyant avec son père Anchise sur le dos et son fils Iule à la main, les aurait emportés. À Troie, ils avaient, semble-t-il, le même rôle que celui qui leur fut dévolu à Rome. Le culte public de Vesta, plus tard assimilé à Hestia, est également hérité des croyances anciennes (la mère de Romulus et Rémus est une vestale).

Rome possède ses propres mythes, souvent liés à sa fondation et à son histoire. Elle assimilera ensuite les mythes grecs mais gardera ses mythes fondateurs au centre de sa culture.

La période de l'âge d'or, également appelée « règne de Saturne » est la période durant laquelle Saturne, détrôné par son fils, fut accueilli en Italie par le roi Janus avec qui il partagea le pouvoir. Cette période fut marquée par une prospérité et une équité absolues : les hommes vivaient de cueillette sans avoir à travailler, ne connaissaient pas la guerre et vivaient en harmonie avec les dieux et la nature. Les cultes de Saturne et de Janus viennent de cette légende. Le mythe des races est quant à lui hérité de la culture grecque.

Le mythe d'Énée fait partie des légendes de la fondation de Rome. Il décrit le voyage d'Énée depuis sa fuite de Troie jusqu'à son arrivée dans le Latium. Commandée par Auguste à Virgile, l"'Énéide" a surtout pour but de montrer le caractère divin de Rome et l'ascendance divine de la "gens" Julia (à laquelle appartient Auguste). La légende donne également à Rome une revanche sur la Grèce en montrant que Troie n'a pas été détruite mais qu'au contraire, les survivants ont fondé une cité puissante capable de l'anéantir. Cette perspective de propagande laisse penser que Virgile a remodelé la légende afin de satisfaire les demandes d'Auguste, mais l'épopée s'appuie d'abord sur la tradition qui donnait pour ancêtres au peuple romain Énée et les derniers Troyens.

Cette légende, probablement la plus célèbre de la mythologie romaine et narrée de nombreuses fois par les auteurs latins, est à l'origine des institutions romaines : le meurtre de Rémus par Romulus montre la prédominance de la patrie sur les liens du sang, l'enceinte ("pomœrium") de Rome tracée par Romulus, demeurera sacrée (sauf pour les triomphes). La légende donne également une origine divine à Rome, Mars étant le père des jumeaux.

Les nombreuses légendes qui entourent l'histoire de Rome consolident de même les institutions romaines. Elles sont racontées aux jeunes romains et constituent la seule littérature enfantine de l'époque. Certaines vantent la "uirtus" latine (vertu et courage), ce sont les "exempla" ; d'autres expliquent la fondation de Rome, ce sont les mythes fondateurs. On peut citer parmi les plus célèbres celles de l'enlèvement des Sabines, de Tarpéia (peine de mort pour les traîtres), de Clélie, d'Horatius Coclès et de Mucius Scaevola, de Lucrèce (fin de la royauté à Rome) et celle du combat des Horaces et des Curiaces.

Il est important de préciser certains points avant d'aborder cette liste d'articles consacrés aux divinités romaines. Tout d'abord, les dieux et déesses présentés comme « équivalents » d'un dieu grec ne le sont que par syncrétisme, et possédaient avant cela des caractères propres et souvent très différents de leurs homologues grecs. Cependant, l'influence de la culture grecque a fait que de nombreux dieux romains, dont la figure originelle nous est aujourd'hui difficile à entrevoir, ont récupéré les attributs de dieux grecs et en sont devenus les stricts homologues. Ainsi dans les articles sur les dieux romains, ce sont souvent les attributs de dieux grecs qui leur sont associés, tant la confusion a profondément influencé la culture populaire. Certes, les dieux romains de l'époque impériale avaient acquis une ressemblance indéniable avec les dieux grecs, mais il faut garder à l'esprit que les premiers avaient eu leur signification propre avant cette association, et sont donc largement plus que de pâles copies. La mythologie romaine, particulièrement dans le cadre du culte impérial, possédait de nombreuses « vertus », personnifications divines de vertus associées aux empereurs déifiés :

d'Aphrodite chez les Grecs) ;





</doc>
<doc id="1962" url="https://fr.wikipedia.org/wiki?curid=1962" title="Mars">
Mars

Mars est originellement le nom du dieu de la guerre dans la mythologie romaine
De cela proviennent plusieurs noms :

"Mars" peut aussi faire référence à :


















Des places, voies, sites ou édifices contiennent ce mois dans leur nom, avec ou sans quantième.





</doc>
<doc id="1963" url="https://fr.wikipedia.org/wiki?curid=1963" title="Écran d'ordinateur">
Écran d'ordinateur

Un écran d'ordinateur est un périphérique de sortie vidéo d'ordinateur. Il affiche les images générées par la carte graphique de l'ordinateur. Grâce au taux de rafraîchissement d'écran élevé, il permet de donner l’impression de mouvement. Il permet donc de travailler agréablement, de visionner de la vidéo, des films, de jouer à des jeux vidéo, etc.

Un écran à cristaux liquides (LCD) se compose d'une dalle (qui est le support des images), des circuits vidéo dont un multiplexeur électronique et une alimentation stabilisée.

Type d'écran le plus ancien : les écrans à tube cathodique (ou écran CRT, qui est une abréviation de l'anglais "Cathode Ray Tube") sont analogiques. Ils ont un angle de vision large et un rendu des couleurs fidèle mais ils sont lourds, volumineux et grands consommateurs d'énergie.

Ils consomment deux à trois fois plus de courant qu’un écran LCD. Leur durée de vie moyenne est d'environ soit si l’écran reste allumé par jour.

Les écrans cathodiques sont toujours utilisés en mode paysage, mis à part quelques exceptions, comme les écrans des Xerox Alto, et de certains écrans Apple comme celui des Macintosh IIsi de secrétariat.

Comme pour les écrans de télévisions, l'année 2009 a signé l'arrêt de production des écrans à tube cathodique.


Les écrans plats sont de plus en plus utilisés. Ils affichent environ couleurs, étendues par tramage à environ 17 millions.

En général, les deux premiers chiffres du numéro de modèle d'un écran plat indiquent la diagonale en pouces.

Ces écrans ont pour avantage un encombrement réduit. Le temps de latence de plus en plus faible permet (pour certains modèles, en dessous de ) d’utiliser des jeux d’action, tels que les FPS, sans avoir à subir des traînées d’affichage lors de mouvements rapides. Ces traînées étaient un frein à leur utilisation dans le grand public.

De par leur poids réduit, ils sont plus faciles à pivoter, ce qui permet plus facilement d'utiliser soit l'écran en mode portrait, soit l'écran en mode paysage. Cette fonctionnalité est prise en charge par Windows et X.Org, ainsi que la plupart des cartes graphiques
. Elle nécessite cependant un écran adapté en terme d'angle de vue comme de pieds. Sous Windows, la combinaison de touches ctrl + alt + flèches de directions offre la possibilité d'orienter l'écran.

Deux gammes d'écrans plats sont disponibles:

Tous les écrans cathodiques sont brillants.

La fréquence de rafraîchissement est définie par le nombre d'images s'affichant sur l'écran par seconde. Cette valeur varie généralement entre 50 et 165 Hz sur certains écrans haut de gamme. Plus cette fréquence est élevée, meilleur est le confort visuel.

Depuis 2007, les ventes de moniteurs de diagonale supérieure à 20 pouces ont fortement augmenté, ces écrans sont principalement au format 16/10, ou 16/9, désormais très rarement au format 4/3 ou 5/4. Avant 2007, le format d'image 4/3 était prédominant pour une diagonale inférieure à . Ce changement de format permet d'optimiser la découpe industrielle des dalles, afin de réduire les coûts de production. Pour des raisons économiques et de rentabilité, le format des écrans d'ordinateur suit donc le format des télévisions.

Le pouce est généralement utilisé pour exprimer la taille de la diagonale l'écran. Un pouce correspond à . Un "écran de " est en réalité un "écran de ", taille déjà classique pour les tubes des téléviseurs dans les années 1950.
Ces tailles sont approximatives et on trouve sous le nom de « » des écrans allant de 41 à . Le tableau de droite donne quelques correspondances.
Le code pénal français interdit l'utilisation d'unités de mesure différentes de celles établies par les lois et réglementations en vigueur (article R643-2), ceci afin de garantir une information juste du client ; en l'occurrence, la France est liée par la Convention du Mètre. Ici, la loi est contournée habilement, le nombre de pouces désignant alors une « classe d'appareils », d'où la valeur élastique constatée du « pouce ».

L'utilisation des écrans a suscité des controverses dans le domaine de la santé.





</doc>
<doc id="1964" url="https://fr.wikipedia.org/wiki?curid=1964" title="Martin Scorsese">
Martin Scorsese

Martin Scorsese, né le à New York (États-Unis), est un réalisateur américain.

De parents d'origine sicilienne, il passe son enfance dans le quartier de qui lui inspire de nombreux films.

Après six nominations, Scorsese remporte finalement, en 2007, l'Oscar du meilleur réalisateur pour "Les Infiltrés" ("") qui est également gratifié de ceux du meilleur film et de la meilleure adaptation. Auparavant, il obtient la Palme d'or au Festival de Cannes 1976 pour "Taxi Driver" ainsi que le Prix de la mise en scène cannois, en 1986 pour "After Hours" et le Lion d'argent à la Mostra de Venise 1990 pour "Les Affranchis". Il est président du jury au Festival de Cannes 1998.

Il est le fondateur de la , récipiendaire de l' pour sa contribution au cinéma et le président de la , une organisation à but non lucratif dédiée à la préservation du patrimoine cinématographique et à la prévention contre la décomposition des pellicules de films en stock.

L’œuvre de Scorsese, reconnue pour sa force et son audace, aborde des thèmes divers tels que l'identité italo-américaine, les notions catholiques de bien et de mal ou encore la culpabilité, la rédemption, le machisme, la grandeur et la décadence, la perdition et la violence. Scorsese est largement considéré par la critique internationale comme l'un des cinéastes américains les plus importants et influents de sa génération grâce à des classiques tels que ', ', Mean Streets", "Les Affranchis" et "Casino," tous interprétés par Robert De Niro.

Il a obtenu une maîtrise en réalisation cinématographique à la à New York et a remporté, en plus de son Oscar et de nombreux prix dans les festivals internationaux, des récompenses aux , aux BAFTA et aux .

Martin Scorsese est né de l'union de et de , tous les deux acteurs. Martin Scorsese est né à , dans l'arrondissement de Queens à New York, au sein d'une famille sicilienne catholique traditionaliste. Enfant asthmatique et frêle, le jeune Martin Scorsese ne peut pratiquer de sport et ses parents l'emmènent fréquemment au cinéma. Épris d'une foi profonde, il se destine d'abord à une vie religieuse et entre au séminaire en 1956 afin d'être ordonné prêtre. Jugé trop jeune (il n'est alors âgé que de 14 ans) et indiscipliné pour s'engager si tôt dans le ministère sacerdotal, il est renvoyé au bout d'un an. Il termine ses études à la "Cardinal Hays School" (dans le Bronx) puis intègre l'université de New York (NYU) en 1960 où il fréquente les cours de cinéma de la et obtient une maîtrise en 1966. Il sera d'ailleurs professeur dans cette université de 1968 à 1970.

Il réalise plusieurs courts métrages, dont le très remarqué ', qui remportent de nombreux prix. Puis il signe son premier long métrage, ', sorti le 15 novembre 1967, soit trois ans après le premier tour de manivelle. Ce film marque la rencontre avec l'un de ses acteurs fétiches, Harvey Keitel.

Il participe en tant que monteur et assistant réalisateur au film "" de Michael Wadleigh sur le Festival de en 1969. Au début des années 1970, Martin Scorsese déménage à et obtient un emploi de monteur à la . Il rencontre alors le producteur Roger Corman qui lui offre la possibilité de tourner son premier film hollywoodien : "Bertha Boxcar" (") avec Barbara Hershey et David Carradine.

Encouragé par John Cassavetes à poursuivre un style de réalisation plus personnel, Scorsese commence à travailler sur le film ", qui relate le parcours de deux jeunes ambitieux de . Première œuvre du réalisateur acclamée par la critique, ce film est aussi le théâtre de la rencontre la plus importante de sa carrière : celle avec l'acteur Robert De Niro qui devient désormais son alter ego à l'écran. Le cinéaste s'apprête à devenir l'une des têtes de proue du Nouvel.

L'année suivante, Francis Ford Coppola lui ouvre les portes des studios . Il rencontre son premier succès public avec le drame intimiste "Alice n'est plus ici" qui dénote l'influence de Cassavetes et permet à Ellen Burstyn d'obtenir l'Oscar de la meilleure actrice, en 1975, pour son interprétation de femme au foyer malheureuse.

Dès son film suivant, il obtient la Palme d'or au Festival de Cannes, en 1976. ', drame psychologique sur fond de difficile réinsertion des anciens combattants de la guerre du Viêt Nam, est interprété par Robert De Niro, Jodie Foster et Harvey Keitel. Ce film, écrit par Paul Schrader, assoit définitivement l'univers scorsesien : faune new-yorkaise, personnages à la dérive, confusion du bien et du mal, violence cathartique et questionnement métaphysique. ' reçoit également quatre nominations aux Oscars en 1977.

Fort de ce nouveau succès, l’année suivante, Scorsese et De Niro se retrouvent une nouvelle fois pour ' avec Liza Minnelli, qui raconte une histoire d'amour mouvementée entre une chanteuse et un saxophoniste en quête de gloire. Le film est un cuisant échec commercial. En 1977, Minnelli propose malgré tout à Scorsese de mettre en scène un spectacle à , ', mais il abandonne au bout de quelques semaines car cette expérience lui déplaît. Le cinéaste cohabite alors avec Robbie Robertson, ex-guitariste et leader du groupe , avec lequel il passe des nuits blanches à regarder des films, fréquenter des cocktails et discuter musique et cinéma. Époque à laquelle Scorsese est sérieusement dépendant de la cocaïne.

En 1978 sort le documentaire "La Dernière Valse" ("") consacré au dernier concert du groupe de Robbie Robertson.

Scorsese a filmé ce concert le jour de 1976 au de . Parmi les invités du Band, figurent Neil Young, Joni Mitchell, Ringo Starr, Neil Diamond, Van Morrison, Eric Clapton et Bob Dylan. Fan du groupe, Scorsese toutes les chansons avant le concert.

Deux années sont nécessaires pour la sortie de "La Dernière Valse" en salles. Le réalisateur tourne par ailleurs des interviews et des morceaux supplémentaires tout au long des années 1977 et 1978. Il en sort fatigué sur le plan intellectuel, physique et psychologique en raison de sa forte consommation de cocaïne.

C'est dans un état physique et psychologique épouvantable qu'il se remet à l'ouvrage, bien épaulé par Robert De Niro, pour réaliser l'un de ses chefs-d'œuvre : '. Le film, porté par une grande intensité dramatique, manifeste un usage très personnel du noir et blanc, des mouvements de caméra et des ralentis. Pour sa performance mémorable dans le rôle du boxeur Jake LaMotta, Robert De Niro reçoit l'Oscar du meilleur acteur. Désormais considéré comme l'un des cinéastes américains les plus inventifs et les plus audacieux, Scorsese enchaîne les films remarqués : "La Valse des pantins" en 1983, satire du milieu télévisé et de la célébrité, ' en 1985 qui narre l'errance nocturne d'un informaticien dans la jungle new-yorkaise puis "La Couleur de l'argent", en 1986, qui prend l'univers du billard en toile de fond. Ce dernier film, interprété notamment par Tom Cruise, vaut à Paul Newman l'unique Oscar du meilleur acteur de sa carrière.

Martin Scorsese réalise ensuite son rêve d'enfant en 1988 en signant un film sur le Christ : "La Dernière Tentation du Christ" ("") adapté du roman éponyme de Níkos Kazantzákis. Le film fait scandale car il met en scène Jésus rêvant sur la croix qu'il échappe à la crucifixion. Se déroule alors en rêve une vie heureuse dans laquelle Jésus devient un patriarche entouré d'enfants. Des manifestations ont lieu un peu partout où le film sort et le cinéma Espace Saint-Michel à Paris est incendié ; un des attentats perpétrés à la sortie du film a fait un mort. Pour autant, le film concourt aux Oscars et Scorsese reçoit sa seconde nomination comme « Meilleur réalisateur ».

Parallèlement à sa carrière et en grand amoureux de l'histoire du cinéma, il crée en 1990 avec sept de ses amis. Cette fondation a pour but d'encourager la restauration et la préservation du patrimoine cinématographique mondial.

S'ensuivent le film de gangsters "Les Affranchis" en 1990 (qui vaut l'Oscar du meilleur second rôle à Joe Pesci) et le thriller "Les Nerfs à vif" en 1991, deux succès, avec de nouveau Robert De Niro. La même année, il est récompensé par la "Cinémathèque américaine" pour l'ensemble de son œuvre.

En 1992, il crée , une fondation qui restaure et exploite les grands classiques du cinéma, puis réalise son premier film à costume avec Daniel Day-Lewis, Michelle Pfeiffer et Winona Ryder, "Le Temps de l'innocence", d'après le roman éponyme d'Edith Wharton, publié en 1920. L'œuvre est un nouveau succès critique et public qui croule sous une pluie de nominations aux Oscars. Mais le cinéaste rate à nouveau la statuette.

Avec "Casino" en 1995, Scorsese retrouve le monde des gangsters dans une grandiose épopée sur l’ascension et la chute du patron d’un grand hôtel-casino de Las Vegas, inspiré de Frank Rosenthal, dans les années 1970. Il retrouve pour la huitième fois, et dernière jusqu'à "The Irishman", Robert De Niro, mais aussi Joe Pesci et Sharon Stone qui remporte le Golden Globe de la meilleure actrice dans un film dramatique et reçoit une nomination à l'Oscar de la meilleure actrice. Après "Casino", il termine son fameux documentaire de quatre heures sur le cinéma américain avec Michael Henry Wilson, "Un voyage avec Martin Scorsese à travers le cinéma américain", commandé par le pour célébrer le centenaire de la naissance du cinéma. Puis, Martin Scorsese est honoré de la prestigieuse récompense du "" par l' en 1997, pour l'ensemble de sa carrière.

Entre les films "Kundun" en 1997, qui évoque la jeunesse du dalaï-lama et "À tombeau ouvert" avec Nicolas Cage, en 1999, il préside le jury du Festival de Cannes 1998 qui décerne à l'unanimité la Palme d'or à "L'Éternité et Un Jour" de Theo Angelopoulos. Mais l'image forte de cette édition reste la remise du Grand prix à Roberto Benigni pour "La vie est belle" au cours de laquelle l'acteur-réalisateur italien se jette aux pieds de Scorsese avant de le prendre dans ses bras et de le soulever de joie.

Lors du festival de Cannes 2002, il est président du jury des courts métrages.
2002 marque une nouvelle date essentielle dans la carrière du réalisateur, puisqu'à l'occasion du film ', fresque épique et flamboyante sur les premières guerres de clans dans le New York du , Scorsese rencontre Leonardo DiCaprio avec lequel il tourne deux autres films consécutivement : ', en 2004, qui s'inspire de la vie de Howard Hughes et vaut à Cate Blanchett l'Oscar du meilleur second rôle pour son interprétation de Katharine Hepburn, puis "Les Infiltrés", en 2006, porté par une distribution de premier ordre : Jack Nicholson, Matt Damon, Mark Wahlberg, Alec Baldwin ou encore Vera Farmiga. Grâce à ce dernier film, remake du film hongkongais ' réalisé par Andrew Lau et Alan Mak, il obtient le plus grand succès public de sa carrière et remporte le Golden Globe du meilleur réalisateur avant de triompher aux Oscars du cinéma. "Les Infiltrés" gagne en effet quatre statuettes en 2007 : Meilleur film, Meilleur réalisateur, Meilleur scénario adapté (William Monahan, d'après ' de Siu Fai Mak et Felix Chong) et Meilleur montage (Thelma Schoonmaker dont c'est le troisième trophée remporté grâce à un film de Scorsese, après ' et '). L'année suivante, sort son documentaire très personnel sur les , axé sur l'enregistrement d'un spectacle du mythique groupe de rock britannique : "".

En 2010, il retrouve pour la quatrième fois l'acteur Leonardo DiCaprio dans "", adaptation du thriller du même nom de Dennis Lehane.

Le 13 décembre 2010, Martin Scorsese participe à un gala de bienfaisance en faveur de la Fondation David Lynch au à New York, il parle par vidéo de son expérience de la méditation transcendantale qu'il pratique depuis plusieurs années : "".

Scorsese dit qu'il ne lui est pas possible d'imaginer le genre de stress qui affecte les anciens combattants qui sont victimes de trouble de stress post-traumatique et il demande au public de soutenir la Fondation David Lynch.

" est un documentaire sur la vie de George Harrison. Scorsese et Harrison sont liés par la musique et la spiritualité, pratiquant tous deux la Méditation transcendantale.
Le nom du film " ("Vivre dans le monde matériel") est emprunté au titre de l'album studio de l'ex- George Harrison. Olivia Harrison participe activement à l'élaboration à ce film en fournissant notamment de nombreux documents personnels.

Martin Scorsese et Olivia Harrison choisissent symboliquement pour la première du film, le théâtre de la petite ville de , dans l'État américain de l'Iowa, qui est le siège d'une université fondée par Maharishi Mahesh Yogi (que George Harrison avait rencontré en 1967 et suivi en Inde en 1968), le public était composé de cinq cent méditants.

Cette projection exclusive est offerte en soutien à la Fondation David Lynch dans son entreprise à enseigner la Méditation transcendantale aux écoliers des quartiers difficiles, aux détenus des prisons, aux anciens combattants, aux sans-abris, aux Amérindiens et aux autres populations à risque.

Scorsese tourne à Paris "Hugo Cabret" qui sort en salles en décembre 2011. Le film est une adaptation libre du roman "L'Invention de Hugo Cabret" de Brian Selznick qui évoque la vie de Georges Méliès, et lui rend hommage. Georges Méliès y est interprété par Ben Kingsley. A l'heure actuelle, son plus gros budget (plus de 170 000 000 millions de $), et c'est la première fois qu'il tourne un film pour enfants. Scorsese utilise la technologie 3D. Succès critique, le film aura par contre peu de succès auprès du public". Hugo Cabret" vaut au cinéaste un nouveau Golden Globe et gagne ensuite cinq Oscars lors de la , en 2012.

En août 2012, il entame le tournage de "Le Loup de" (""), d'après les mémoires du courtier en bourse Jordan Belfort, incarné par Leonardo DiCaprio, pour sa avec le réalisateur. Le film est sorti fin 2013, avec un scénario racontant l'ascension et la chute de ce courtier véreux au début des années 1990.

En 2013, il est également l'un des producteurs délégués du film franco-américain "Malavita" de Luc Besson.

Fin 2013 il préside le jury du Festival international du film de Marrakech.

Du 14 octobre 2015 au 14 février 2016, la Cinémathèque française accueille une exposition entièrement dédiée à Martin Scorsese, conçue à Berlin par la Deutsche Kinemathek. Des œuvres, objets et photographies issus de sa collection et des archives personnelles, ainsi que de Robert De Niro ou Paul Schrader, sont exposés. Il s'agit de la première exposition mondiale qui lui est consacrée. La Cinémathèque organise également, à cette occasion, une rétrospective intégrale de son œuvre.

Fin janvier 2015, il commence le tournage d'un projet de longue date, "Silence". Dans cette adaptation du roman homonyme de Shūsaku Endō, il dirige à nouveau Liam Neeson, aux côtés de plus jeunes acteurs comme Andrew Garfield et Adam Driver. Le film sort en première vision au Vatican en novembre 2016.

En mars 2015, il est annoncé comme le réalisateur d'un film sur la vie de Mike Tyson, avec le rôle-titre tenu par Jamie Foxx.

En octobre 2015, le Prix Lumière lui est décerné à Lyon par l'Institut Lumière « pour l’ensemble de son œuvre, pour sa cinéphilie généreuse, pour son inlassable combat en faveur de la sauvegarde du cinéma du passé, pour ses fictions, pour ses documentaires, pour son amour de la musique, pour sa bienveillance à l’égard des jeunes cinéastes du monde entier ».

Au fil du temps, Martin Scorsese s'est entouré de nombreux acteurs avec lesquels il a pu travailler à plusieurs reprises.
Robert De Niro a tourné dans 8 de ses films et dans ' produit par Scorsese. Grâce à leur collaboration, il a gagné l'Oscar du meilleur acteur pour son interprétation du boxeur Jake LaMotta dans '.

Harvey Keitel est considéré comme le premier acteur fétiche du réalisateur, puisqu'il a tourné 5 fois sous sa direction dont leur premier film respectif "" en 1969.

Leonardo DiCaprio tient également le rôle principal dans 5 de ses films : ', ', "Les Infiltrés", "" et "Le Loup de". 
Cette longue collaboration avec le réalisateur lui a permis de gagner le Golden Globe du meilleur acteur dans un film dramatique pour son interprétation d'Howard Hughes dans "" et d'être nommé à l'Oscar du meilleur acteur pour ce même film. Il a ensuite gagné le Golden Globe du meilleur acteur dans un film musical ou une comédie pour son interprétation du trader Jordan Belfort dans "Le Loup de" et d'être nommé à l'Oscar du meilleur acteur.

Joe Pesci et Frank Vincent figurent tous deux dans la distribution de "", "Les Affranchis" et "Casino".

Jodie Foster et Daniel Day-Lewis ont joué deux fois sous sa direction.

Pour ses scénarios, il collabore avec Paul Schrader qui a écrit ', ', "La Dernière Tentation du Christ" et "À tombeau ouvert".

Thelma Schoonmaker est sa monteuse attitrée puisqu'elle s'est occupée du montage de la plupart de ses films et documentaires.

Barbara De Fina, son ex-femme, a produit tous ses films à partir de "La Couleur de l'argent" jusqu'à "À tombeau ouvert".

Pour la musique de ses films, Scorsese utilise fréquemment des musiques pré-existantes, avec l'aide de son ami et superviseur musical Robbie Robertson. En revanche, pour les musiques originales, il a souvent fait appel à Elmer Bernstein (3 films) et Howard Shore (6 films à ce jour).

Enfin, les directeurs de la photographie Robert Richardson et Michael Ballhaus ont éclairé la majorité de ses films.











</doc>
<doc id="1966" url="https://fr.wikipedia.org/wiki?curid=1966" title="Mythologie égyptienne">
Mythologie égyptienne

Les Égyptiens de l'Antiquité ont cherché à interpréter tous les phénomènes qu'ils pouvaient observer à travers le prisme de croyances séculaires. La notion de cycle y est essentielle :

La grande diversité du culte de l'Égypte antique se retrouve également dans les mythes de la création qui varient en fonction des régions (ou même des villes) et de leurs dieux tutélaires : Rê, Isis, Seth, Horus, Anubis. Ainsi, ce n'est pas une, mais plusieurs cosmogonies (mythes de la création du Monde) qui coexistaient dans les différentes parties du royaume. Les plus connues sont celles d'Héliopolis, d'Hermopolis, de Thèbes et de Syène (Éléphantine-Assouan).

Les cosmogonies admettent toutes l'existence d'un principe créateur, mais chaque nome voit dans son dieu tutélaire le démiurge à l'origine de cette création. La cosmogonie la plus répandue est celle d'Héliopolis qui a pour créateur un démiurge solaire (Rê sous l'une de ses formes) et donne une généalogie divine descendant jusqu'au dieu pharaonique Horus.

Pour les anciens Égyptiens, l'univers n'était au commencement qu'un grand océan primordial nommé le Noun. C'est de Noun que naquit Atoum, le soleil. Atoum engendra Chou (le dieu du souffle) et Tefnout (la déesse de l'humidité). Chou sépara le ciel de la terre. Ainsi naquirent Nout (la déesse du ciel) et Geb (le dieu de la terre). De l'union de Nout et Geb naquirent deux fils Osiris et Seth, et deux filles, Isis et Nephthys.

Geb offrit le pouvoir sur terre à Osiris qui fut le premier des pharaons. Il régna au côté de sa sœur et épouse Isis. Son règne empreint de bonté, de justice et de sagesse rendit Seth fou de jalousie. Il complota contre son frère. Il invita son frère à un grand banquet. Seth proposa alors que chacun des convives se couche dans un magnifique coffre. Celui qui serait aux mesures du coffre le gagnerait. Osiris se couche alors dans le coffre : il est à ses dimensions (piège de Seth). Les convives se jettent tous sur le coffre et y enferment Osiris. Seth le jette dans le Nil. Grâce à l'aide de Nephthys, Isis la magicienne réussit à retrouver le corps de son mari et à le cacher dans un marais. Seth l'apprend et, furieux, arrive à retrouver le corps, et le déchire en quatorze ou seize morceaux selon les versions.

Avec l'aide de sa sœur Nephthys et d'Anubis, Isis retrouve les morceaux éparpillés dans toute l'Égypte, sauf son sexe, mangé par un poisson. Ils reconstituent alors Osiris le temps d'une union d'où naquit Horus (le dieu des pharaons). Horus vainquit Seth après maints duels et régna sur l'Égypte. Osiris, lui, devint le roi du royaume des morts.

Chez les Égyptiens de l'Antiquité, les cérémonies et croyances liées à la mort représentaient une part importante de leur vie. Les préoccupations liées à la mort au cours de l'Égypte antique étaient d'ordre religieux et constituaient une étape importante de la vie du pharaon,incarnation des dieux sur terre, qui devait après son décès vivre auprès des dieux dans un repos éternel. Les égyptiens considéraient qu'après le décès, l'âme du défunt pouvait renaître et accéder au « "royaume des morts" » et au "repos éternel".

Le mythe de la mort peut être décomposé en deux parties :

Dans la mythologie égyptienne, le corps est divisé en plusieurs parties dont le djet, qui correspond au corps, et le ka, qui correspond au double spirituel accompagnant le corps depuis la naissance de l'individu jusqu'à son décès. Pour que le défunt puisse accéder au royaume de l'au-delà par l'intermédiaire de son ka, l'embaumement du djet est nécessaire. En effet, si le corps n'est pas embaumé, le djet devient le khat après la mort et ne peut accéder au repos éternel. Le rite de l'embaumement fut créé par Isis, aidée par Anubis, lorsqu'elle reconstitua le corps de son époux Osiris afin de lui redonner vie. Ce rite symbolise donc la "renaissance" du défunt et l'accès au « royaume des morts » et au repos éternel. Les statues et offrandes présentes aux côtés du défunt dans son sarcophage permettent de l'accompagner dans son chemin vers le jugement de l'âme.

Ce chemin vers l'au-delà est pris en compte dans l'architecture des pyramides. En effet, au sein des pyramides, les couloirs s'élevant vers les sommets de la pyramide et le ciel depuis la chambre funéraire du défunt, semblent être des passages permettant à l'âme de s'élever et d'atteindre le royaume des morts Le livre des morts des Anciens Égyptiens, placé aux côtés du défunt, avait pour but de le guider vers le royaume des morts et de le préparer au jugement de l'âme à l'aide de prières.

La pesée de l'âme (psychostasie) consiste à mettre le cœur du défunt sur une balance et de l'autre côté une plume (représentant la déesse Maât) ; si le cœur est plus léger (ce qui signifie que le cœur n'est pas entaché de péchés), le défunt peut rejoindre le royaume des morts. Sinon, il se fera dévorer par un monstre (la plupart du temps symbolisé par la déesse Taouret ou par Ammout qui a une tête de crocodile, un corps de lion et un arrière-train d'hippopotame.) et son âme sera perdue à tout jamais. Osiris ne devint dieu du royaume des morts qu'après avoir passé avec succès l'épreuve de la pesée de l'âme. Les défunts voulaient donc s'identifier à Osiris pour atteindre le royaume des morts et reposer en paix.

Le mythe décrit le combat que mène Rê chaque nuit contre les « forces du chaos » représenté par le serpent Apophis afin de permettre la réapparition du soleil chaque matin sur le « monde d'en haut ».

Rê étant considéré comme le dieu du soleil, entre autres, lorsque le soleil disparaissait chaque soir à l'horizon, le dieu Rê changeait de moyen de transport pour adopter une barque sacrée et traverser le Nil souterrain. Au cours de ce périple, Rê traversait les douze portes correspondant aux douze heures de la nuit (de 5 h du soir à 5 h du matin) dans le monde souterrain, la douat, et devait déjouer les pièges des forces du chaos tentant de renverser à tous moments la barque du dieu du soleil. Il est aidé en cela par le dieu Seth qui, se tenant à la proue de la barque solaire, lance ses traits sur Apophis. Ce périple avait pour but la renaissance du dieu Rê chaque matin ramenant ainsi la lumière aux habitants du « monde d'en haut ». Cette "renaissance" de Rê, représentée par le lever du soleil, était considérée également comme la renaissance du monde et le signe que le dieu Rê avait triomphé des forces du chaos durant son périple.

On retrouve également la notion des douze portes au sein des pyramides d'Égypte dont le couloir menant au sarcophage est constitué de douze encadrements de porte, correspondant à chacune des heures de la nuit.

Ce combat entre le dieu Rê et Apophis, chaque nuit, dès le coucher du soleil, et conduisant à un nouveau lever de soleil, chaque matin, constitue donc le mythe du cycle du jour dans la mythologie égyptienne.

Après la mort de Rê, c'est la déesse Bastet qui combattit le serpent Apophis dans le douat.





</doc>
<doc id="1967" url="https://fr.wikipedia.org/wiki?curid=1967" title="Master boot record">
Master boot record

Le ou MBR (parfois aussi appelé « zone amorce ») est le nom donné au premier secteur adressable d'un disque dur (, et , ou en adressage logique) dans le cadre d'un partitionnement Intel. Sa taille est de . Le MBR contient la table des partitions (les primaires) du disque dur. Il contient également une routine d'amorçage dont le but est de charger le système d'exploitation, ou le chargeur d'amorçage () s'il existe, présent sur la partition active.

À l' du MBR, le mot codice_1, appelé "nombre magique" ou , doit impérativement être présent pour que le BIOS charge et exécute la routine de démarrage présente dans le MBR. En effet, après la phase de test du BIOS (appelée POST), le BIOS lit le premier secteur des périphériques amorçables qui ont été définis par l'utilisateur à l'aide du programme SETUP (section ). Lorsqu'il trouve un périphérique contenant le , il charge le code d'amorçage à l'adresse mémoire 0x7C00 et l'exécute. La main est alors donnée au chargeur d'amorçage () par ce code d'amorçage.

Sous MS-DOS et les versions grand public de Windows jusqu'à Windows Millenium, il est possible de recréer la routine de du MBR sous DOS à l'aide de la commande Fdisk /MBR, pour les versions plus récentes et Windows 10 la commande est bootrec /fixmbr.

Le est ainsi réécrit.

Cela permet d'éliminer certains virus de (si la commande est exécutée depuis une disquette car les virus de détournent souvent l'), de restaurer un MBR endommagé (le PC ne démarre plus), ou de supprimer un chargeur de démarrage installé dans le MBR (exemples : LILO, GRUB, si une distribution Linux a été installée parallèlement à Windows).

Sous Windows XP, la commande à utiliser pour restaurer le MBR est codice_2. Elle est accessible depuis la console de récupération.

Sous Windows Vista et , la commande à utiliser pour restaurer le MBR est codice_3. Elle est accessible depuis la console de récupération.

Sous Linux, l'utilitaire Boot-Repair permet de restaurer le MBR.

Sous UNIX et Linux, la commande "codice_4" permet de copier n'importe quelle portion d'un fichier. On peut donc l'utiliser pour sauvegarder le MBR d'un disque, ou pour le restaurer. Celui-ci se trouve sur les octets du disque.

Cette opération est risquée, si l'utilisateur se trompe de disque à copier ou a restaurer. Par exemple, restaurer le MBR d'un disque dur sur un autre disque, remplacera la table des partitions du second disque par celle du premier. Il y a de fortes chances que votre second disque soit alors illisible. La seule exception à cette règle concerne le cas où les deux disques durs sont les mêmes ainsi que leur partitionnement (cas fréquent dans un parc de machines en entreprise).

Dans l'exemple qui suit, on sauve le MBR du disque codice_5 dans un fichier nommé codice_6 à l'aide de la commande codice_4 :

codice_8

On le restaure de cette manière (codice_6 est le fichier qui a été sauvegardé ci-dessus) :

codice_10

Pour ne sauver que le programme et le désassembler :
Si la table de partition n'a pas changé, on peut très bien ne recharger que les octets du MBR (en indiquant codice_11).

En général, l'installation d'un système GNU/Linux modifie le MBR initial pour qu'il pointe sur le chargeur d'amorçage de Linux (GRUB, LILO). Sur certains systèmes (dits tatoués), il est impossible de démarrer Windows lorsque le MBR est modifié. Il faut donc utiliser une méthode d'installation différente pour faire cohabiter les deux systèmes, ou bien restaurer le MBR d'origine lorsqu'on veut réinstaller un système Windows. .

Du fait de ses limitations — il ne gère pas les disques de plus de —, le système de partitions MBR est remplacé la plupart du temps depuis 2013 par le système GPT.




</doc>
<doc id="1968" url="https://fr.wikipedia.org/wiki?curid=1968" title="Mandrake">
Mandrake

Mandrake peut désigner :





</doc>
<doc id="1969" url="https://fr.wikipedia.org/wiki?curid=1969" title="Musique française">
Musique française

La musique française est née au Moyen Âge, avec le genre proche du plain-chant grégorien, nommé organum. Elle s'est ensuite développée sous l'égide de l'école de Notre-Dame ou de l'Ars antiqua avec le conduit, un chant de procession. À cette époque, les troubadours et les trouvères affirmaient davantage l'indépendance de l'art lyrique face au clergé.

Puis dans les classes aisées s'est développée l'école de l'Ars nova autour de la création de motets et de messes, mais aussi de chansons profanes, dédiées à la distraction.

La Renaissance a vu se développer la polyphonie et le chant. À partir du tournant du au , début de la période de la musique baroque, on vit apparaître de nouveaux genres comme l'air de cour, le ballet de cour, et la suite de danses. La musique instrumentale prit davantage d'importance. L'opéra fut introduit en France par Jean-Baptiste Lully dans les années 1670. Enfin, les chants régionaux, militaires, marins, religieux, ouvriers et paysans ont fait leur réapparition dans le patrimoine musical français ces dernières années, notamment grâce au Chœur Montjoie Saint Denis.

Les troubadours et les trouvères étaient parfois de simples artistes invités dans les cours des nobles, qui firent de ceux-ci des mélomanes en ramenant la musique aux affaires (de cœur) humaines par le moyen de chansonnettes. Cet engouement musical suscita la vocation des jongleurs, ces musiciens itinérants qui n'étaient que les interprètes de compositions créées par les précédents, et qui n'hésitaient pas à parler la langue vulgaire.

Dans les siècles qui suivirent l'Ars nova, la création musicale va s'affiner et se complexifier, mais elle se fera toujours loin du peuple, qui lui préférera les chants à boire, les chants de table, les chants de travail ou les chants à danser, pratiqués sur des instruments moins « nobles », et donc plus proches de la nature.



La période baroque voit se développer différents genres autour du ballet à la cour de France :







Si on ne peut exactement dater l'apparition de la musique traditionnelle ou folklorique car elle est essentiellement de transmission orale, des instruments tels que la cornemuse semblent avoir une longue histoire au sein des divers peuples formant la France. De toute évidence, une musique régionale a pu se développer en même temps que les patois, loin des villes et des classes aisées. Il est probable que le métier de musicien ambulant fut très tôt apprécié par les campagnes où ses chansonnettes venaient compléter celles existant déjà dans les divers corps de métiers. Plus récemment, le Chœur Montjoie Saint Denis, fondé en 1979, a largement contribué à la redécouverte des chants régionaux, militaires, marins, religieux, ouvriers et paysans.

Il existe aux quatre coins de la France des musiques régionales, propres à des langues régionales, des traditions. Ces musiques se rattachent parfois à des genres plus vastes comme la musique germanique pour l'alsacienne ou la musique celtique pour la bretonne par exemple. Malgré leurs parentés internationales, elles se déclinent néanmoins de façon particulière en France. Les musiques à danser trouvent une richesse et une origine souvent localisable dans les régions de france. Mais ces danses ont tellement voyagé, parfois dans toute l'Europe et même au-delà, qu'elles constituent un vaste patrimoine qui ne demande qu'à être partagé.

Musiques régionales


Musiques d'outremer


Vents :

Cordes :


Percussions :

Depuis l'Assassinat du duc de Guise (1908) dont la musique est composée par Camille Saint-Saëns jusqu'aux dernières productions des années 2010, plusieurs générations de compositeurs se sont succédé.

Bien que l'origine des musiques populaires remonte au , c'est au qu'elles prennent leur essor, avec d'une part des genres propres à la France, et d'autre part, des genres importés. Toutefois « l'exception » française, avec sa politique de soutien de l'identité culturelle nationale, se retrouve même dans ces genres importés qui sont interprétés avec un style français. Ce n'est pas simplement le fait de chanter en français qui distingue ici les artistes, mais bien un style musical ou une instrumentation spécifique.

La chanson française est un style typique de la musique française, et est très populaire en France. Les artistes classiques les plus importants sont : Charles Trenet, considéré comme le père de la chanson moderne après le renouveau de rythmes et de mots qu'il apporta dans les années 1930, Édith Piaf, Monique Serf (Barbara), Georges Brassens, Jacques Brel, Léo Ferré, Jean Ferrat, Annie Cordy, Charles Aznavour, Mireille Mathieu, Dalida, Frida Boccara, Gilbert Bécaud, Serge Gainsbourg, Salvatore Adamo, et Brigitte Fontaine.

Pendant les années 1970, dans des styles variés, des artistes ont renouvelé la chanson française : Johnny Hallyday, Michel Sardou, Claude François, Joe Dassin, Serge Lama, Sheila, Dalida, Frida Boccara, Eddy Mitchell, Jacques Dutronc, Françoise Hardy, Véronique Sanson, Renaud, Francis Cabrel, Alain Souchon, Jacques Higelin, Bernard Lavilliers, Alain Chamfort, et aussi dans les années 1980 (Mylène Farmer, Alain Bashung, Étienne Daho, Têtes Raides), jusque maintenant (Vanessa Paradis, Matthieu Chedid, Jean-Louis Murat, Miossec, Mathieu Boogaerts, Daniel Darc, Vincent Delerm).

Le côté plus commercial et populaire de la « chanson » est appelé « variété », et comprend des artistes comme Francis Cabrel, Alain Souchon, Laurent Voulzy, et Jean-Jacques Goldman. Plus récemment, le succès de l'émission Star Academy a engendré une nouvelle génération d'artistes de musique populaire comme Jenifer et Nolwenn Leroy. Mylène Farmer a inspiré des artistes pop rock comme Zazie, Alizée et Lorie, et des chanteuses comme Nadiya et Ophélie Winter.

À partir des années 1990, de nombreux artistes Français se distinguent à l'international par leur créativité en musique électronique, qui signe la naissance de la musique house Française que l'on surnomme la French Touch.





</doc>
<doc id="1972" url="https://fr.wikipedia.org/wiki?curid=1972" title="Miséricorde (homonymie)">
Miséricorde (homonymie)

Miséricorde peut désigner :




</doc>
<doc id="1973" url="https://fr.wikipedia.org/wiki?curid=1973" title="Mâchicoulis">
Mâchicoulis

Un mâchicoulis est une galerie formant un encorbellement, soit en position mitoyenne ou en couronnement d'une enceinte militaire (tour, courtine, rempart, etc.) et dont le plancher ajouré permettait, si besoin, de flanquer divers projectiles au pied de mur, zone souvent vulnérable.

Ce système de défense active en maçonnerie, surtout sous la forme de « mâchicoulis sur consoles » se répand à la fin du Moyen Âge (deuxième moitié du ) en remplacement de celui des hourds en bois.

Le terme est mentionné pour la première fois en 1402 - 1404, c'est-à-dire postérieurement à la technique elle-même, sous la forme "machecoleis" « galerie extérieure de pierre, en encorbellement et percée d'ouvertures destinées au tir plongeant »

Le mot est issu du moyen français *"machecolis" dérivé à l'aide du suffixe "-is" [?]. Il est attesté en latin médiéval sous la forme "machecollum". Il s'agit d'un probable composé du vieux français "macher" « battre, frapper, meurtrir » et de col (cou en vieux français). Les projectiles lancés des mâchicoulis étaient effectivement destinés à « briser le cou » des assaillants. 

Les mâchicoulis sont des structures en pierre taillée pourvus d'ouvertures carrées ou de larges rainures pratiquées dans le sol, qui garnissent un chemin de ronde d'une tour ou d'une courtine, et permettent d'en défendre le pied, notamment pour éviter le travail de sape. Les mâchicoulis sont une transposition en pierre des hourds et des bretèches de bois que l'on élevait sur les murailles ou les tours dans les premiers temps du Moyen Âge. Ces dispositifs architecturaux durables en encorbellement ne semblent pas avoir existé dans les fortifications grecques et romaines. Ils sont apparus à la fin du premier millénaire dans des fortifications byzantines puis arabes (présence de bretèches à mâchicoulis discontinues sur accès) et ont perduré en tant qu'organes défensifs fonctionnels jusqu'à la première moitié du . 

Le système a également été utilisé dans les constructions des croisés en Terre Sainte et en Europe, comme aux Pays-Bas, où il subsiste : le Muiderslot, au château de Wijk bij Duurstede et dans la porte des Écluses "(Sassenpoort)" à Zwolle.

Dans les châteaux-palais de la Renaissance française, le mâchicoulis est un élément décoratif rappelant la fonction défensive du château fort des siècles précédents.

On en distingue trois types :

Contrairement à ce que montrent les films de guerre se déroulant au Moyen Âge et les topos hérités de l'historiographie du , lors du siège d'un château les défenseurs ne jetaient pas d'huile bouillante sur les assaillants du haut des remparts. En effet, c'était un aliment rare et cher à l'époque. Les défenseurs jetaient toutes sortes de projectiles, pierres, poutres, parfois de la poix, du soufre, du sable rougi ou de l'eau bouillante. Mais l'eau et le bois de chauffe étaient également une ressource rare lors d'un siège, si bien que leur emploi a dû être limité.

Les mâchicoulis étaient plutôt utilisés pour des tirs à l'arc plongeants. Ou pour jeter des pierres, des charognes pour propager des épidémies ou des tonneaux remplis d'excréments.

Le pied des remparts était d'ailleurs parfois oblique et non vertical. Cela provoquait un ricochet des projectiles jetés du haut des remparts pour un effet encore plus dévastateur.



</doc>

<doc id="6431" url="https://fr.wikipedia.org/wiki?curid=6431" title="Territoires du Nord-Ouest">
Territoires du Nord-Ouest

Les Territoires du Nord-Ouest sont un territoire fédéral du Nord canadien, bordés à l'ouest par le Yukon, à l'est par le Nunavut et au sud par la Colombie-Britannique, l'Alberta et la Saskatchewan. S'étendant sur une superficie de , il s'agit de la troisième plus grande délimitation du pays, après le Nunavut et le Québec. Avec une population de (données de 2016), les Territoires du Nord-Ouest occupent le onzième et antépénultième rang des provinces et territoires canadiens les plus peuplés, devant le Nunavut. et leur capitale est Yellowknife.

C'est un territoire réputé pour la beauté de ses paysages. Il faut citer le Grand lac de l'Ours , le Grand lac des Esclaves, le fleuve Mackenzie et les canyons de la réserve de parc national de Nahanni. Le territoire s'étend également sur la partie occidentale de l'archipel arctique et comprend les îles Banks et du Prince-Patrick, et des portions des îles Victoria et Melville. Son point culminant est le mont Nirvana, près de la frontière avec le Yukon, culminant à d'altitude. Le territoire est en grande partie recouvert de taïga.

En anglais, les Territoires du Nord-Ouest portent le nom de "Northwest Territories". Il est abrégé par TNO en français et NWT en anglais. Le nom fait référence à la fois au statut de l'entité (un territoire, non pas une province) et à sa situation géographique au sein du territoire canadien (même si autrefois cette appellation a couvert un territoire beaucoup plus vaste : voir le paragraphe "Évolution territoriale").

En inuktitut, le territoire est appelé "nunatsiaq" (écrit ᓄᓇᑦᓯᐊᖅ dans les syllabaires autochtones canadiens), c'est-à-dire . Après la séparation du Nunavut et des TNO en 1999, il a été discuté de la possibilité de changer leur nom, éventuellement vers une langue autochtone. L'une des propositions était "Denendeh" ( en na-dené), une idée soutenue entre autres par Stephen Kakfwi, ancien premier ministre territorial. Une autre idée, qui avait débuté sous la forme d'une blague, était de nommer le territoire . Cette proposition atteignit la tête des sondages. Au bout du compte, le consensus demeura pour la conservation du nom actuel.

Les Territoires du Nord-Ouest s'étendent, comme leur nom l'indique, sur le nord-ouest du Canada. Ils sont bordés à l'ouest par le territoire du Yukon et à l'est par celui du Nunavut; au sud, le marque leur frontière avec les provinces de la Colombie-Britannique, de l'Alberta et de la Saskatchewan; au nord s'étend l'océan Arctique. Le territoire comprend également certaines îles occidentales de l'archipel Arctique, comme les îles Banks et du Prince-Patrick, et des portions des îles Victoria et Melville. Situés dans le nord du continent américain, les Territoires du Nord-Ouest s'étendent largement au-delà du cercle arctique. Leur partie continentale, sur le bouclier canadien est parsemée de lacs, dont le Grand lac de l'Ours et le Grand lac des Esclaves; ils sont marqués par le bassin du Mackenzie.

La superficie des Territoires du Nord-Ouest atteint , dont de terres et d'eaux. Formant 13,5 % de la superficie totale du Canada, le territoire est cependant plus petit que le Nunavut (21,0 %) et le Québec (15,4 %).

Les Territoires du Nord-Ouest contiennent l'essaim de dykes de Mackenzie, le plus grand essaim de dykes connu sur terre. Il y a entre 1,269 et 1,267 milliard d'années, le craton des Esclaves a été partiellement soulevé et pénétré par l'essaim, rayonnant d'un panache provenant du centre-ouest de l'île Victoria. Il s'agit du dernier événement majeur à avoir affecté le cœur du craton, même si des événements magmatiques plus récents en ont affecté les bords.

Les Territoires du Nord-Ouest couvrant plus de , le climat y varie fortement entre le nord et le sud. La partie méridionale du territoire connaît un climat subarctique tandis que le nord subit un climat polaire.

Les étés dans le sud sont courts, mais relativement chauds, les températures atteignant en moyenne le jour et la nuit. Les hivers sont longs et froids, les températures journalières montant à le jour et descendant à la nuit. Les températures extrêmes sont courantes, les températures maximales en été atteignant et les minimales en hiver descendant bien au-delà de . Dans le nord, il n'est pas anormal que les températures descendent à en hiver, mais elles peuvent également monter au-dessus de le jour.

Les orages ne sont pas rares au sud; au nord, ils sont très peu fréquents. Le territoire possède un climat assez sec du fait des montagnes s'étendant à l'ouest.

La moitié des Territoires du Nord-Ouest est située au sud de la limite des arbres, laquelle s'étend du nord-ouest au sud-est, depuis le delta du Mackenzie dans l'océan Arctique jusqu'au coin sud-est du territoire. L'est du territoire et les îles arctiques ne sont pas recouverts de forêts, mais de toundra.

La majorité des habitants est concentrée au sud de la limite des arbres, où la terre est recouverte de forêts et le sol riche en minerais.

Depuis 1967, la capitale (et la plus grande ville) des Territoires du Nord-Ouest est Yellowknife, sur la rive nord du Grand lac des Esclaves. Elle regroupe près de la moitié des habitants du territoire. Cinq autres communautés comptent plus de : Hay River, Inuvik, Fort Smith, Behchokǫ̀ et Fort Simpson.

28 autres communautés officiellement reconnues comptent moins de : Aklavik, Colville Lake, Délįne, Dettah, Enterprise, Fort Good Hope, Fort Liard, Fort McPherson, Fort Providence, Fort Resolution, Gamèti, Hay River Reserve, Jean Marie River, Kakisa, Lutselk'e, Nahanni Butte, N'Dilo, Norman Wells, Paulatuk, Sachs Harbour, Sambaa K'e (Trout Lake), Tsiigehtchic, Tuktoyaktuk, Tulita, Ulukhaktok, Wekweeti, Whatì et Wrigley.

La densité de population est extrêmement faible, avec 0,036 habitants/km², 100 fois moins que la densité moyenne du Canada. Les Territoires du Nord-Ouest sont deux fois moins densément peuplés que le Yukon, mais deux fois plus que le Nunavut ; leur densité est comparable à celle du Groenland.

Les Territoires du Nord-Ouest comptent trois parcs nationaux :

Par ailleurs, la réserve de parc national du Bras-Est du Grand lac des Esclaves, créée en 2007, est inscrite sur la carte de parachèvement des parcs nationaux du Canada.

En tant que territoire, les Territoires du Nord-Ouest possèdent moins de droits qu'une province. La dévolution du pouvoir aux Territoires est une préoccupation constante de leur horizon politique depuis la première assemblée territoriale, en 1881.

Le commissaire des Territoires du Nord-Ouest est, en théorie, le chef de l'exécutif. Il est nommé par le gouverneur en conseil du Canada sur recommandation du ministre des Affaires autochtones et du Nord canadien. Le poste comporte à l'origine plus de responsabilités qu'actuellement, mais il est devenu symbolique du fait de la dévolution progressive de pouvoirs à l'assemblée depuis 1967. Depuis 1985, le commissaire ne préside plus les réunions du Conseil exécutif, et le gouvernement fédéral demande aux commissaires de se comporter comme des lieutenants-gouverneurs de province. À la différence de ceux-ci, le commissaire des Territoires du Nord-Ouest n'est pas un représentant officiel du souverain du Canada. En 2016, suite à la fin du mandat de George Tuccaro, la poste de commissaire est resté vacant pendant un an. En juin 2017, il a été annoncé que Margaret Thom serait nommée commissaire des Territoires du Nord-Ouest.

Contrairement aux gouvernements provinciaux et du Yukon, celui des Territoires du Nord-Ouest n'a pas de partis politiques. C'est un gouvernement de consensus appelé l'Assemblée législative. Cette assemblée possède un représentant de chacune des 19 circonscriptions du territoire. Après chaque élection générale, la nouvelle assemblée élit le premier ministre et son président, à bulletins secrets. Sept membres sont choisis comme ministres, les autres formant l'opposition. Depuis 2011, le Premier ministre des Territoires du Nord-Ouest est Bob McLeod.

Au niveau fédéral, le territoire est représenté à la Chambre des communes par un député et au Sénat par un sénateur ; la circonscription électorale des Territoires du Nord-Ouest est éponyme, mais elle s'appelait, avant 2014, "Western Arctic". Le député à la Chambre des communes est le libéral Michael McLeod (élu en 2015) ; le sénateur est Nick Sibbeston (depuis 1999).

Les Territoires du Nord-Ouest sont créés en 1870, à la suite du transfert des territoires possédés par la Compagnie de la Baie d'Hudson au gouvernement du Canada. Avant ce transfert, la Compagnie possède une immense région recouvrant tout le Canada moderne à l'exception de la colonie de la Colombie-Britannique sur la côte pacifique, de la confédération canadienne (côtes des Grands Lacs, vallée du Saint-Laurent, tiers sud de l'actuel Québec et Provinces maritimes, de l'île de Terre-Neuve et de la côte du Labrador sur l'Atlantique et des îles de l'archipel arctique (mis à part la moitié sud de l'île de Baffin). La région est alors divisée en deux territoires, la Terre de Rupert (bassin de la baie d'Hudson) et le territoire du Nord-Ouest (bassin des océans Arctique et Pacifique).

Après la création de la Confédération canadienne en 1867, son gouvernement achète la Terre de Rupert et le Territoire du Nord-Ouest à la Compagnie de la Baie d'Hudson en 1869. L'entrée de ces terres dans la confédération est retardée du fait de la rébellion de la rivière Rouge. Les Territoires du Nord-Ouest sont créés en 1870, en même temps que la province du Manitoba (alors une petite région carrée autour de Winnipeg).

Le conseil temporaire du Nord-Ouest est créé en 1870. Le premier gouvernement du territoire s'installe en 1872.

Jusqu'en 1912, des parties des Territoires du Nord-Ouest ont été progressivement rattachées à d'autres provinces, ou détachées pour former de nouvelles entités. Le district de Keewatin, au centre du territoire, a été séparé en 1876. Le Yukon a été formé sur sa partie occidentale en 1898 afin de mieux gérer les intérêts locaux lors de la ruée vers l'or du Klondike. Le sud-ouest du territoire a servi à créer les provinces de l'Alberta et de la Saskatchewan en 1905.

Le Manitoba a été élargi en 1881, l'Ontario en 1874 et en 1889, et le Québec en 1898. Les frontières de ces trois provinces ont été largement repoussées vers le nord en 1912.

En 1880, le Royaume-Uni a cédé au Canada la souveraineté sur les îles de l'archipel arctique. Elles ont été intégrées aux Territoires du Nord-Ouest. En 1905, le district de Keewatin a finalement été réintégré aux Territoires du Nord-Ouest.

Au total, les districts suivants ont été détachés des Territoires du Nord-Ouest :

En 1912, les Territoires du Nord-Ouest ne conservent que trois districts : Keewatin, Franklin et Mackenzie. Ils mesurent toutefois , une superficie supérieure à celle de l'Inde.

Lorsqu'en 1905, les provinces de l'Alberta et de la Saskatchewan sont détachées des Territoires du Nord-Ouest, la population de ces derniers passe de à . Parmi ces derniers, sont des Amérindiens qui ne disposent pas du droit de vote. Le territoire revient alors à son statut de 1870 et passe sous contrôle fédéral, gouverné depuis Ottawa.

Entre 1907 et 1947, les Territoires du Nord-Ouest ne sont pas représentés à la Chambre des communes. En 1947, le district électoral du Yukon-Mackenzie est créé; il n'inclut que le district de Mackenzie. Le reste du territoire n'est pas représenté avant 1962, lors de la création du district électoral des Territoires du Nord-Ouest, en reconnaissance de l'accord du droit de vote aux Inuits en 1953.

Les élections locales font leur réapparition en 1951, mais sous une forme partielle, le Conseil et l'Assemblée étant alors un mélange de membres élus et nommés. En 1975, le gouvernement territorial redevient un organe entièrement élu.

En avril 1982 débute une longue période qui viendra amputer les Territoires du Nord-Ouest d'une partie de leur population et d'un vaste territoire géographique.

En effet en cette année 1982, une majorité des habitants des Territoires votent en faveur d'une partition de la région, et le gouvernement fédéral donne son accord sous conditions sept mois plus tard. Après de longues négociations territoriales entre l'Inuit Tapiriit Kanatami et le gouvernement fédéral (débutées dès 1976), un accord est trouvé en septembre 1992. 

En juin 1993, la Loi concernant l’Accord sur les revendications territoriales du Nunavut et la Loi sur le Nunavut sont votées par le parlement canadien. Après une période de transition, la partie orientale du territoire (incluant la totalité du district de Keewatin et une grande portion des deux autres) devient un territoire distinct sous le nom de Nunavut le .

La dévolution du pouvoir est en constante évolution aux Territoire du Nord-Ouest, que ce soit par le transfert du siège du gouvernement territorial d’Ottawa à Yellowknife dans les années 1960 ou encore par la création du Nunavut dans les années 1990.

De plus, la gouvernance par consensus a aussi été remise en cause par l'ancien Premier ministre Roland qui, en 2009, a établi 10 nouveaux principes du gouvernement par consensus sur les Territoires du Nord-Ouest. 

Les cartes suivantes montrent l'évolution territoriale des Territoires du Nord-Ouest entre 1867 et la période contemporaine :

En 2016, la population des Territoires du Nord-Ouest est estimée à .

Le tableau suivant résume l'évolution de la population des Territoires depuis leur formation en 1871. Le Yukon a été séparé des Territoires en 1898 et les recensements antérieurs incluent sa population. De même, l'Alberta et la Saskatchewan ont été créés à partir du territoire en 1905. Le Nunavut a été créé en 1999 ; les chiffres de population l'incluent avant 1996.

Le christianisme est la principale religion pratiquée aux TNO. Le catholicisme y arrive au premier rang, suivi des nombreuses dénominations protestantes. Selon le recensement fédéral de 2001, les principales affiliations religieuses du territoire sont :
Depuis 1988, les Territoires du Nord-Ouest reconnaissent onze langues officielles :

Le français a été constitué langue officielle en 1877 par le gouvernement, mais l'assemblée des Territoires a annulé cette décision en 1892, ne conservant que l'anglais comme langue officielle. Dans les années 1980, le gouvernement fédéral a fait pression sur celui du territoire afin de réintroduire le français comme langue officielle. À la suite des protestations des membres autochtones de l'assemblée, plusieurs autres langues autochtones ont également été officialisées.

Les résidents des Territoires du Nord-Ouest peuvent utiliser n'importe laquelle des langues officielles dans une cour de justice territoriale, ainsi que dans les débats de l'assemblée. Cependant, seules les versions anglaises et françaises des lois ont valeur légale, et le gouvernement ne publie de documents officiels dans l'une des autres langues que lorsque l'assemblée le demande expressément. Qui plus est, l'accès aux services dans une langue est limité aux institutions et aux circonstances où il existe une véritable demande et où cette attente est raisonnable. De façon pratique, l'anglais est universellement employé dans ces services et, à part dans le domaine judiciaire, il n'y a aucune garantie qu'une autre langue (y compris le français) soit utilisée.

L'industrie minière est le secteur le plus important des Territoires du Nord-Ouest. Les ressources du territoire incluent l'or, les diamants, le gaz naturel et le pétrole. Ce dernier est pompé et raffiné à Tulita et Norman Wells sur le Mackenzie. Le cuivre est extrait de la rivière Coppermine. La région possède également du tungstène, de l'argent, du cadmium et du nickel.

Selon l'acte de dévolution des pouvoirs, le territoire a pris le contrôle de ses ressources naturelles en avril 2014.

Le piégeage est la plus ancienne industrie de la région. La pêche, basée sur la truite grise et le corégone, est centrée sur le village de Hay River, sur le Grand lac des Esclaves. L'agriculture est presque impossible, mis à part au sud du Mackenzie, de façon limitée.

Du fait de la taille et de la situation géographique des Territoires du Nord-Ouest, les transports et les communications peuvent y être difficiles. De façon générale, le commerce, l'approvisionnement et les déplacements se font beaucoup par voie aérienne et le territoire possède de nombreux aérodromes. Un programme de construction de routes, initié en 1966, ouvre progressivement l'accès à la région. La route de la Liard, ouverte en 1984, relie Fort Simpson à la route de l'Alaska. D'autres routes relient Inuvik au Yukon et Hay River aux routes de l'Alberta. En 2012, le Pont de Deh Cho, qui enjambe le fleuve Mackenzie, a été inauguré, permettant de relier Yellowknife au réseau routier national toute l'année. Une ligne ferroviaire, la "Great Slave Railway", relie l'Alberta à la région du Grand lac des Esclaves.

Le territoire occupant une partie de l'archipel arctique canadien est traversé par le Passage du Nord-Ouest. Celui-ci est emprunté par les navires pétroliers et de marchandises de nombreuses nations qui considèrent ces eaux libres. Cette situation est l'objet d'un enjeu économique important pour le Canada qui revendique sa souveraineté sur ses eaux intérieures. 

L'hiver, la navigation fluviale est interrompue pendant deux mois. Certaines rivières et lacs gelés sont alors utilisés pour le transport terrestre.




</doc>
<doc id="6433" url="https://fr.wikipedia.org/wiki?curid=6433" title="Zodiaque">
Zodiaque

Le zodiaque () est une zone circulaire (de 360 degrés) de la sphère céleste, dont l'écliptique occupe le milieu. Il y a une différence d'acception de sens du terme zodiaque : zodiaque astronomique (zodiaque de treize constellations que le Soleil semble traverser en une année) et zodiaque astrologique (zodiaque des douze signes astrologiques de 30 degrés chacun que le Soleil semble aussi traverser en une année). Le plus souvent, le grand public a entendu parler de ce second zodiaque (astrologique) et non du premier.
Le substantif masculin "zodiaque" a été emprunté au grec ancien / "", signifiant proprement « [cercle de] petits animaux », de ζῴδιον / "zốdion", diminutif de ζῷον / "zỗon" : « animal ». Ce nom vient du fait que toutes les constellations du zodiaque (sauf la Balance, anciennement partie du Scorpion et le Verseau) figurent des créatures vivantes.

La trajectoire du Soleil sur la voûte céleste est l'écliptique. Les planètes et la Lune s'en écartent plus ou moins, et l'on retient comme limite conventionnelle du zodiaque une bande de huit degrés d'arc de part et d'autre de l'écliptique. L'écliptique traverse treize constellations dans le ciel, mais l'une d'entre elles, Ophiuchus (ou le Serpentaire), ne fait pas partie du zodiaque "traditionnel" de l'astrologie. Celui-ci a été divisé au en douze parties égales (une pour chaque mois de l'année) auxquelles on a donné le nom de la constellation la plus proche.

Mais d'après Sénèque, Bérose enseignait à prophétiser tous les événements et cataclysmes futurs au moyen du zodiaque, et on constate que les époques fixées par lui, pour la conflagration du Monde – Pralaya – et pour un déluge, correspondent à celles qui sont indiquées dans un ancien papyrus égyptien. Une semblable catastrophe se produit à chaque renouvellement du cycle de la Grande Année (ou Année platonique) de 25 868 ou 25 920 ans. Les noms des mois, chez les Akkadiens, étaient dérivés des noms des signes du zodiaque, et les Akkadiens sont fort antérieurs aux Chaldéens. M. Proctor démontre dans ses que les Astronomes antiques étaient arrivés à une Astronomie très exacte 2 400 ans avant Jésus-Christ ; les Hindous font remonter le début de leur Kali Yoûga à une grande conjonction périodique des planètes trente et un siècles avant Jésus-Christ. Donc le zodiaque est beaucoup plus ancien que le 

Les constellations "présentes" dans le zodiaque sont : le Bélier, le Taureau, les Gémeaux, le Cancer (ou le Scarabée, ou le Crabe), le Lion, la Vierge, la Balance, le Scorpion, l'Ophiuchus (ou le "Serpentaire"), le Sagittaire, le Capricorne (ou la "Chèvre"), le Verseau et les Poissons.

Le zodiaque a été divisé au en douze parties égales de 30° de large chacune (une pour chaque mois de l'année) portant le même nom que la constellation se trouvant "derrière" (certains affirment que les constellations ont donné leur nom aux signes; d'autres soutiennent l'inverse):

Ces signes sont des secteurs réguliers de 30°, conventionnellement décomptés à partir du point vernal. Ils n'ont dès l'origine qu'un rapport lointain avec les constellations du même nom, dont les limites et positions sont irrégulières. De plus, ce rapport s'est constamment distendu au fil du temps, du fait de la précession des équinoxes. Les signes du zodiaque dit "tropique"/"tropical" (du grec "tropikos", qui tourne) ne doivent donc pas être confondus avec les constellations du même nom, qui appartiennent au zodiaque dit "sidéral".

Pendant plus de deux millénaires, les astronomes (et les astrologues) ont repéré le mouvement des corps célestes non pas en degrés depuis le point vernal (longitude écliptique) comme de nos jours, mais en degrés depuis le signe courant. Ces deux méthodes sont équivalentes : une position planétaire à 17° du Lion (le cinquième signe) est à 4 × 30° + 17° = 137° du point vernal. Cette notation a été abandonnée par les astronomes à la seconde moitié du .

Voici un tableau des signes et des constellations du zodiaque avec les dates auxquelles le Soleil s'y trouve. Les deux colonnes montrent la différence entre un signe et une constellation du Zodiaque.

Une personne de notre époque née quand le Soleil se trouve dans la constellation de la Balance ne sera pas du signe de la Balance. La précession des équinoxes a déplacé le point de référence du zodiaque tropique, la position du Soleil par rapport aux étoiles lors de l'équinoxe du printemps, qui en est le point de départ (début du signe du Bélier). Le point vernal a ainsi reculé de la constellation du Taureau (il y a environ ans) à celle du Bélier (il y a environ 4 000 ans), et de la constellation du Bélier à celle des Poissons (il y a environ 2 000 ans). 
Les constellations étant de tailles inégales, une astrologie entièrement sidérale verrait des irrégularités comme le fait de présenter une proportion importante de gens nés sous le signe de la Vierge. 
Une autre question pour l'astrologie sidérale est de situer précisément le début de son zodiaque. Les constellations n'ayant que des limites conventionnelles (dessinées en 1930), où situer la limite entre le Bélier et les Poissons autrement que par convention ?

En Unicode, les symboles sont encodés dans le bloc des symboles divers:

Les signes du zodiaque sont utilisés dans l'astrologie comme repères spatio-temporels permettant d'établir les correspondances sur lesquelles repose cette pratique. Elle utilise pour cela la position de divers objets dans le zodiaque. Entre autres : les planètes, le Soleil, la Lune, et sur le plan local : l'horizon (l'ascendant étant le point de l'écliptique coupé par l'horizon est) et le méridien (le milieu du ciel correspondant au point où se trouve le Soleil à midi).

Le zodiaque dit tropical (mot venant du grec "tropikos" signifiant "qui tourne") est le "zodiaque des saisons". Le schéma ci-joint représente la trajectoire apparente annuelle du Soleil lorsqu'en vision géocentrique, il semble se déplacer autour de la Terre. On distingue bien les 4 "temps forts" correspondant aux 4 boules : les deux solstices (le Soleil arrête de ou de et inverse sa tendance) et les deux équinoxes (où le Soleil passe la "même durée de temps" et ). Cela délimite les 4 saisons. Chaque saison est subdivisée en 3 (selon la distinction ci-dessus : "Cardinal, Fixe et Mutable") ce qui permet d'obtenir les 12 signes de l'astrologie tropicale (celle des journaux).

L'astrologie sidérale, pratiquée essentiellement hors d'Occident (astrologie chinoise et astrologie védique ou jyotish), divise également l'écliptique en douze zones de grandeur égale, mais elle aligne la frontière de la constellation astrologique du Bélier avec une étoile particulière plutôt qu'avec l'équinoxe de printemps, ce qui fait que les signes astrologiques sidéraux sont assujettis à la même précession que les constellations.

Le décalage entre les signes tropicaux et les signes sidéraux (les constellations) est de nos jours (en 2004) de l'ordre de 25° environ selon la mesure de l"'Ayanamsa" par les astrologues hindous.

Les astrologies chinoises et indiennes ont une tradition propre pour désigner les signes, dont la liste n'a pas de lien avec les signes du zodiaque traditionnels.

À noter l'astrologie hellénistique, qui semble utiliser une astrologie sidérale fondée sur le zodiaque ptolémaïque.

Cette symbolique a été fréquemment et largement utilisée depuis l'époque gréco-romaine jusqu'à nos jours. Selon Jacques Halbronn, le zodiaque a subi diverses corruptions et les attributions des dieux-planètes aux signes ne correspondent pas. Ainsi, les gémeaux évoquaient au départ un couple (dans les almanachs et les livres d'heures, le mois de mai représente un couple, comme dans "Les Très Riches Heures du duc de Berry") ce qui correspond à Vénus et non à Mercure comme on peut le lire dans le Tetrabiblos de Claude Ptolémée ( de notre ère). 

Dans certaines représentations de la France romane, on voit le Christ éclairant de son auréole, tel un soleil, entouré de douze animaux représentant ses apôtres.

Il y a aussi l'association traditionnelle des quatre évangélistes aux quatre signes fixes : Luc et le Taureau, Marc et le Lion, Jean et le Scorpion (représenté sous la forme transfigurée de l'aigle) et Matthieu et le Verseau (l'Homme déversant le flot de la connaissance), composant ainsi le Tétramorphe. Cette symbolique est sans doute issue d'une tradition plus ancienne symbolisant les quatre saisons, d'après la concordance entre ces différentes saisons et la position du soleil dans ces différentes constellations :
Notons que les quatre étoiles fixes dites royales correspondent à une telle distribution : Aldébaran dans la constellation du Taureau, Régulus dans celle du Lion, Antarès dans celle du Scorpion et enfin Fomalhaut dans celle du Poisson Austral, à proximité de la constellation du Verseau.

Enfin, certains auteurs ont établi un parallèle entre les douze tribus d'Israël et les signes du zodiaque. Jésus de Nazareth est originaire de la tribu de Juda, dont il est dit dans la Genèse qu'elle est .




</doc>
<doc id="6441" url="https://fr.wikipedia.org/wiki?curid=6441" title="Résonance paramagnétique électronique">
Résonance paramagnétique électronique

La résonance paramagnétique électronique (RPE), résonance de spin électronique (RSE), ou en anglais electron spin resonance (ESR) désigne la propriété de certains électrons à absorber, puis réémettre l'énergie d'un rayonnement électromagnétique lorsqu'ils sont placés dans un champ magnétique. Cette propriété est analogue à celle des noyaux atomiques en résonance magnétique nucléaire (RMN). Seuls les électrons non appariés (ou électrons célibataires), présents dans des espèces chimiques radicalaires ainsi que dans les sels et complexes des métaux de transition, présentent cette propriété.

Ce phénomène est utilisé dans la spectroscopie par résonance paramagnétique électronique. Celle-ci permet de mettre en évidence la présence d'électrons non appariés dans des solides, des liquides ou des gaz et de déterminer l'environnement de ces derniers. Il sera possible, par exemple, de connaître le type de noyaux atomiques à proximité de ces électrons non appariés et d'en déduire éventuellement la structure d'une molécule.

La majorité des espèces chimiques stables possédant des paires d'électrons appariés, cette technique permettra de mettre en évidence les espèces chimiques possédant des électrons non appariés de manière très spécifique. Par ailleurs, du fait de l'importance du moment magnétique de spin de l'électron, cette technique est également très sensible. 

L'expression « spectroscopie par résonance paramagnétique électronique » laisse imaginer que seules des interactions paramagnétiques peuvent être étudiées. En réalité, des interactions de type ferromagnétiques, ferrimagnétiques ou antiferromagnétiques interviennent, surtout dans le cas des matériaux magnétiquement concentrés. Il serait donc préférable de désigner ce type de spectroscopie par l'expression plus générale de « spectroscopie par résonance de spin électronique » (RSE). Néanmoins, les applications utilisant cette spectroscopie concernent essentiellement des espèces paramagnétiques, d'où l'usage de « RPE » plus fréquent que celui de « RSE ».

Par ailleurs, dans le reste de cet article, l'expression « résonance de spin électronique » pourra désigner indifféremment la propriété électronique ou la spectroscopie associée.

La résonance de spin électronique a été découverte en 1944 par le physicien soviétique Yevgeny Zavoisky de l'université d'État de Kazan ainsi que de manière concomitante et indépendante par Brebis Bleaney à l'Université d'Oxford. La technique de la RSE en onde continue a longtemps été la plus utilisée. Dans celle-ci, le système est irradié en continu par un rayonnement micro-onde à une fréquence donnée. La RSE impulsionnelle est de plus en plus utilisée, notamment en biologie structurale où elle permet de connaître les distances entre radicaux, en science des matériaux pour connaître la structure locale autour de défauts paramagnétiques.

Le principe de la RSE repose sur l'effet Zeeman : soumis à l'action d'un champ magnétique extérieur H, les niveaux d'énergie d'un spin S se séparent en (2S + 1) états, chacun affecté d'un nombre quantique de spin formula_1 (formula_1 = -S, -S+1, -S+2, ..., S). Cette séparation des niveaux est d'autant plus grande que H est intense (voir figure). Cette levée de dégénérescence d'états de spin électronique est identique à la levée de dégénérescence d'états de spin nucléaire qui permet la RMN.

Ainsi, pour le cas d'un ion paramagnétique ne présentant qu'un seul électron célibataire (donc pour lequel S = 1/2), la présence du champ magnétique extérieur donne lieu à (2S + 1) = 2 états, correspondant à formula_3 et formula_4. L'énergie magnétique associée à chacun de ces états est formula_5, où


Les électrons présents dans le système vont pouvoir occuper les différents niveaux électroniques. Au niveau macroscopique, la présence dans l'un ou l'autre niveau est déterminé par la statistique de Maxwell-Boltzmann :

formula_11

avec :


Le niveau formula_3 est de ce fait légèrement plus peuplé que le niveau supérieur. Le rapport entre les populations de chacun des deux niveaux vaut 0,998, pour une énergie de 40 µeV (différence d'énergie observée pour un électron dans un champ de 3350 Gauss) et une température de 298 K. De ce fait, la perturbation induite par le champ magnétique est relativement faible.

En présence d'une onde électromagnétique de fréquence ν (champ hyperfréquence ou micro-onde) perpendiculaire au premier et d'amplitude beaucoup plus faible, un photon peut être absorbé ou émis si son énergie hν est égale à la différence entre les niveaux formula_1 -1/2 et 1/2 (soit formula_17). 

Ainsi, la condition de résonance se résume par : 

formula_18

avec les mêmes notations que précédemment ainsi que 


D'une façon plus générale, lorsqu'il y a plus que 2 états, les transitions permises sont celles pour lesquelles les états (initial et final) satisfont aux conditions (appelées règle de sélection) ΔS = 0 et Δ"m" = ± 1. Les autres transitions sont généralement interdites, sauf en cas de mélange entre les états quantiques.

La condition de résonance exprimée ci-dessus (Éq. 1) peut également s'écrire :

formula_22

où γ désigne le rapport gyromagnétique.

La recherche de la résonance peut être effectuée en fixant le champ magnétique H et en faisant varier la fréquence de l'onde électromagnétique ν, comme cela est couramment réalisé en RMN. Une autre possibilité, plus fréquemment employée, est de fixer la fréquence de l'onde électromagnétique (elle sera, par exemple, à 9388,2 MHz pour un spectromètre utilisant la bande X) et de faire varier le champ magnétique.

De ce fait, les diagrammes présentent de manière classique l'absorbance ou sa dérivée en ordonnée et le champ magnétique statique H en abscisse, exprimé en Gauss, comme cela est illustré par la figure ci-contre.

La plupart des spectromètres de RSE utilisent une détection synchrone, c'est-à-dire que le champ magnétique est modulé autour d'une valeur centrale telle que

formula_23

Le signal d'absorption peut s'écrire à l'ordre 1 sous la forme : formula_24

En filtrant la fréquence correspondant à ω, la courbe dérivée formula_25 est obtenue. 

Chaque signal ainsi enregistré est caractéristique de l'élément magnétique en présence et des interactions que cet élément ressent. Ces informations s'obtiennent à partir de :

Le nombre de signaux indique parfois le nombre de site ou bien des interactions hyperfines avec un (ou plusieurs) spins nucléaires (pour des détails, voir "" ). 

En effet, l'hamiltonien d'un électron de spin formula_26 couplé à un champ magnétique formula_27, entouré de formula_28 noyaux à spin non nuls formula_29, est donné par la somme de trois termes :

formula_30



Afin de simplifier certains spectres, il peut être opportun d'ajouter un rayonnement radiofréquence du type de ceux utilisés en RMN. Il s'agit alors de résonance double électronique et nucléaire, ou ENDOR ( "").

De la même manière que la spectroscopie RMN, cette technique utilise des champs magnétiques de l'ordre de grandeur du tesla. Pour cela, des bobines constituées de matériaux supra-conducteurs sont employées, dans lesquelles est introduite une intensité électrique importante. La circulation de ce courant électrique dans une bobine est à l'origine de ce champ magnétique. Afin de permettre la supraconductivité de ces matériaux, les bobines sont refroidies par de l'hélium liquide.

Le champ radiofréquence est, lui, généré par des bobines conventionnelles.

La RSE est utile pour l'étude de la structure locale de l'ensemble des matériaux pouvant présenter en leur sein un élément paramagnétique, utile pour pouvoir décrire l'environnement local. Elle peut servir à l'étude, par exemple, de défauts créés par le passage d'un rayonnement (α, β, γ, particules chargées...), donnant ainsi une mesure absolue de leur concentration dans la structure (à condition d'avoir un étalon). On peut étudier des ions paramagnétiques dans les structures de complexes dans les composés organiques ou de minéraux. De nombreux objets peuvent ainsi être étudiés grâce à cette technique, allant des matériaux désordonnés (verres) à des structures cristallines (minéraux) en passant par des matériaux organiques.

L'exploitation des spectres RSE passe parfois par une étape de simulation afin de pouvoir en déduire les différentes informations concernant l'environnement des éléments paramagnétiques présents et de suivre l'évolution du système en fonction des conditions expérimentales et de mesure.

La RSE est également utilisée dans le cadre de datations en archéologie préhistorique. Alors que la thermoluminescence s'applique aux cristaux de quartz et de feldspath, la RSE s'applique aux cristaux de quartz, calcite, apatite (os et dents), sulfates et phosphates qui ont subi des bombardements radio-actifs naturels, venant des sols (sédiments et roches ambiants), endommageant les mailles cristallines de minéraux cristallisés en déplaçant des électrons qui sont ensuite piégés dans d'autres défauts de la maille. Ces cristaux, chauffés (par action naturelle ou humaine), exposés à la lumière (cas des sédiments avant enfouissement) ou nés d'une nouvelle cristallisation, ont leurs pièges vidés, ce qui remet l'horloge à zéro. La datation consiste à mesurer l'accumulation, qui est fonction du temps, des électrons à nouveau piégés. La RSE s'applique en particulier à l'émail dentaire de grands mammifères fossiles, à des grains de quartz extraits de sédiments archéologiques ou à des carbonates (stalagmites, coraux, etc.). Son champ d'application est très étendu, d'environ à un million d'années.

La RPE permet de sonder les intermédiaires réactionnels lors de réactions chimiques par voie radicalaire. Cela permet de progresser dans la connaissance des mécanismes réactionnels.

La RSE s'applique aussi aux modèles biologiques.

La RPE s'est avérée une technique de choix pour déterminer les paramètres associés à la fluidité membranaire.

La RPE a été utilisée pour mesurer la quantité d'énergie appliquée à l'échelle moléculaire lors de réactions mécanochimiques.





</doc>
<doc id="6442" url="https://fr.wikipedia.org/wiki?curid=6442" title="RPE">
RPE

RPE est un sigle qui peut vouloir dire : 

</doc>
<doc id="6444" url="https://fr.wikipedia.org/wiki?curid=6444" title="Île de Montréal">
Île de Montréal

L’île de Montréal est située au confluent du fleuve Saint-Laurent (Canada) et de la rivière des Outaouais dans le sud-ouest du Québec. C’est la plus grande île de l’archipel d’Hochelaga et la deuxième île fluviale la plus peuplée au monde, derrière Zhongshan Dao et devant Manhattan. Elle coïncide avec une grande partie du territoire de la ville de Montréal et avec celui d’autres municipalités. À elle seule, elle regroupe environ le quart de la population du Québec.

L’île de Montréal est séparée de l’île Jésus (Laval) par la rivière des Prairies. L’île a la forme d’un boomerang, et son extrémité ouest est baignée par les lacs des Deux Montagnes et Saint-Louis. Elle est délimitée au sud par les rapides de Lachine et le fleuve Saint-Laurent.

L’île mesure à peu près de long et à son point le plus large. Elle possède de berges et couvre une superficie de . Elle est dominée par le mont Royal qui culmine à , lui-même constitué de trois collines : le Mont-Royal qui donne son nom à l’ensemble, la colline d’Outremont qui culmine à et celle de Westmount qui dont le sommet s’élève à . Au cœur du Mont-Royal se trouve le lac aux Castors.

L’île de Montréal est le composant principal du territoire de Montréal (région administrative) et l’agglomération de Montréal, avec l’île Bizard, l’île Sainte-Hélène, l’île Notre-Dame, l’île des Sœurs, l’île Dorval, et à peu près plus petites. Ce territoire regroupe et les 19 arrondissements de la ville de Montréal.

La région administrativede Montréal, comprenant aussi l'île Bizard, comptait en 2016 une population de , dont environ les trois quarts () dans la ville de Montréal. L’île de Montréal regroupe presque le quart de la population totale du Québec. Elle forme une des 17 régions administratives du Québec. C’est la région administrative la plus peuplée du Québec, tout en étant l’une des deux régions les plus petites en superficie (la plus petite en superficie étant celle de Laval).

. Il s’agit du plus faible taux de francophones parmi les administratives québécoises. Néanmoins, tous groupes linguistiques confondus, 96 % des résidents de l’île ont une certaine connaissance de la langue française. L’île de Montréal a toujours été au cœur des enjeux linguistiques au Québec.

En 1535, l’explorateur Jacques Cartier nomma Mont Royal la montagne qui se trouve sur l’île (qui s'écrit Monte Real sur la carte italienne de l'époque et Mont Réal en moyen français). En 1603, Samuel de Champlain visita l’île et y séjourna.

Les Kanienkehaka (Mohawks) désignent l'île "Tiohtià:ke", « le lieu où les nations et les rivières s'unissent et se divisent » .

Au moment de la fondation de Montréal, on la nommait .


</doc>
<doc id="6445" url="https://fr.wikipedia.org/wiki?curid=6445" title="Rivière des Outaouais">
Rivière des Outaouais

La rivière des Outaouais (en anglais "Ottawa River") est le principal affluent du fleuve Saint-Laurent (provinces de l‘Ontario] et du Québec, Canada). La rivière constitue un important segment de la frontière entre les provinces canadiennes du Québec et de l'Ontario. L’Outaouais a joué un rôle déterminant de voie de pénétration dans le continent nord-américain, au nord vers la baie d’Hudson et à l’ouest vers les Grands Lacs. Elle est la plus longue rivière du Québec. Elle est nommée en l'honneur de la nation des Outaouais.

Le bassin versant de l’Outaouais s'étend sur environ . D'une longueur de , l’Outaouais prend sa source principale dans le lac des Outaouais dans la région de l’Outaouais (Québec) (municipalité régionale de comté de La Vallée-de-la-Gatineau), à au nord-ouest de Montréal. De là, la rivière alimente le lac Capimitchigama dans la Réserve faunique La Vérendrye et ensuite les réservoirs Cabonga et Dozois, puis le Grand Lac Victoria et le lac Granet. Elle quitte la réserve faunique avant de rejoindre le réservoir Decelles, le lac Simard et le lac des Quinze. Jusque-là, elle s’est écoulée en zigzaguant vers l'ouest à travers les régions de l’Outaouais et de l'Abitibi-Témiscamingue. À partir du lac Témiscamingue, la rivière bifurque vers le sud et devient la frontière naturelle entre l'Ontario et le Québec. Elle poursuit alors son cours dans un axe nord-ouest sud-est, jusqu'à Hawkesbury Est où son rôle de frontière naturelle s’arrête. Elle continue de descendre, au Québec exclusivement, jusqu’à se jeter dans le Saint-Laurent, après son élargissement au lac des Deux Montagnes. La confluence des deux cours d'eau crée l'archipel d'Hochelaga.

L’Outaouais compte plusieurs dizaines d’affluents dont la Dumoine, la Noire, la Coulonge et la Quyon. Dans la partie inférieure de la rivière des Outaouais, les principaux affluents sont, en aval de:

Plus loin, à la hauteur de Grenville sur le flanc québécois et de Hawkesbury sur le flanc ontarien, la rivière s'élargit pour former le lac Dollard-des-Ormeaux qui est borné en aval par le barrage de Carillon. Au pied du barrage se trouve un élargissement naturel de la rivière, le lac des Deux Montagnes, lui-même alimenté par quelques affluents.

L’Outaouais termine sa longue course dans le fleuve Saint-Laurent, principalement par la rivière des Prairies qui reçoit de son eau et, dans une moindre mesure, par la rivière des Mille Îles (qui rejoint la rivière des Prairies avant que celle-ci se jette dans le fleuve). Le reste s'écoule vers le fleuve par le lac Saint-Louis.

L’Outaouais est une importante voie navigable parsemée d’obstacles naturels notamment les rapides, les chutes d’eau et plusieurs zones de haut-fond. Dès la première moitié du siècle on a creusé et aménagé quatre canaux sur son parcours, auxquels s’ajoutèrent, formant un ensemble complexe, les canaux des autres cours d’eau auxquels il est rattaché (notamment, le canal Lachine et le canal Rideau). C’est cet ensemble qui permit la libre circulation des bateaux dans un vaste réseau navigable jusqu’au cœur de l’Amérique du Nord.

En remontant le fleuve, on rencontrait un premier obstacle de taille, à Montréal: les rapides de Lachine. Il était nécessaire de les contourner pour aller plus loin. Pour ce faire, on creusa dans un premier temps le canal Lachine () puis, au , on aménagea la voie maritime du Saint-Laurent. Ces deux ouvrages permettaient de passer du fleuve Saint-Laurent au lac Saint-Louis situé quelque plus haut. La route de l’Outaouais passait par le lac des Deux Montagnes. Pour rejoindre ce lac (situé plus haut) à partir du lac Saint-Louis, il fallait un canal, le premier sur l’Outaouais, celui de Sainte-Anne-de-Bellevue. Plus loin on arrivait aux rapides du Long-Sault. Pour les contourner il fallut creuser et aménager trois canaux successifs sur une distance d’environ , celui de Carillon, celui de la Chute-à-Blondeau et celui de Grenville. À la hauteur des villes de Gatineau et Ottawa le passage était bloqué par les chutes des Chaudières et il l’est toujours aujourd’hui. L’amont de la rivière n’est plus directement accessible par eau à partir de ce point. Il n’existe aucun canal qui permet de contourner ces chutes ni tous les rapides qui se trouvent en amont sur le reste du parcours. Pourtant, la navigation était possible plus en amont et jusque dans les lacs d’Abitibi grâce aux trains de portage qui permettaient de transporter les marchandises et les passagers au-delà de chaque nouvel obstacle.

Du pied des rapides de Lachine dans le Vieux port de Montréal à l’entrée du canal Rideau à Ottawa, l’escalier d’eau comptait, au siècle, dix-sept marches, qui étaient autant d’écluses sur le parcours. Depuis les années 1960 et l’avènement de la voie maritime du Saint-Laurent et de la centrale hydro-électrique de Carillon, les navigateurs ont, selon la taille de leur bateau, le choix entre deux voies, soit le passage du canal Lachine, soit celui de la voie maritime. Pour remonter vers Ottawa et Gatineau l’escalier compte désormais quatre marches (par la voie maritime) ou sept marches (par le canal Lachine).

Le cours de la rivière des Outaouais est ponctué de dix barrages qui captent et transforment son énergie en hydroélectricité et contrôlent par le fait même son débit. En amont du lac Témiscamingue, six structures sont en place. Toutes ces installations à l’exception d’une seule, la centrale de Rapide-7, sont dites centrales au fil de l’eau. Le premier barrage a produit de l’électricité dès 1910, à Hull, aujourd’hui Gatineau mais on a produit de l’électricité sans barrage dès 1881 à l’usine E.B. Eddy, puis en 1882 à Ottawa.

Dans la partie où elle est la frontière naturelle entre les deux provinces, la production hydroélectrique est parfois le fait de l’Ontario Power Generation, parfois celui d’Hydro-Québec, les deux sociétés d'État provinciales de production d'électricité, et elle est parfois le fait des deux sociétés, conjointement. Certaines centrales sont privées, ou l’ont été. Au barrage de Carillon, en amont de Montréal, la moyenne du débit s'établit à /s ; ce débit connaît des variations entre 700 et /s selon les saisons. En 1974, la crue amène des inondations importantes à Hudson dans Vaudreuil-Soulanges.

Tous ces barrages ont haussé le niveau de la rivière et modifié ses rives. Ils ont noyé des rapides et altéré les écosystèmes. Les eaux vives se sont transformées en eaux calmes. Certains de ces obstacles naturels, connus des communautés autochtones et notés dans les rapports d’expédition des explorateurs européens, sont disparus sous les eaux. C’est notamment le cas des rapides du Long-Sault en amont du barrage et de la centrale de Carillon ainsi que des rapides des Sept-Chutes en amont du barrage de Bryson.

La rivière des Outaouais comprend plusieurs lac fluviaux et plusieurs lacs de barrages, des élargissements qui sont tantôt l’œuvre de la nature et tantôt la conséquence de la construction de barrages permettant d’exploiter l’énergie de la rivière.
La rivière des Outaouais est parsemée de centaines d’îles. Les plus importantes portent un nom. Certaines sont habitées. D’autres abritent des écosystèmes variés. Les deux plus grandes îles de l’Outaouais sont situées en territoire québécois. La première est l’île aux Allumettes qui est habitée par une population de 1346 personnes (2014), en face de Pembroke (Ontario). La seconde en importance est l’île du Grand-Calumet, tout près de Fort-Coulonge. L’île de Hull compte une partie de la population de la ville de Gatineau.

Pas moins de dix-huit ponts routiers et six traversiers permettent de passer d’une rive à l’autre de l’Outaouais, sans parler des ponts ferroviaires.

Les villes d'Ottawa en Ontario (capitale fédérale canadienne, ) et Gatineau au Québec () forment le noyau principal de population sur la rivière. Plus en aval, on retrouve les municipalités québécoises de Montebello, Papineauville, Grenville, Rigaud et Saint-André-d'Argenteuil ainsi que la ville ontarienne de Hawkesbury.

Il y a , la mer de Champlain (carte), créée à la suite du retrait des glaciers, commence à se retirer vers l'est, formant ainsi les vallées de l'Outaouais et du St-Laurent. À son plus haut niveau, elle occupait les terres situées sous d'altitude (par rapport au niveau actuel des mers).

La rivière a été pendant longtemps le chemin privilégié par les amérindiens et par les premiers explorateurs pour atteindre les Grands Lacs et l'ouest du Canada (Pays d'en Haut) par la rivière Mattawa puis le lac Nipissing et vers le nord par le lac Témiscamingue, le cours supérieur de l'Outaouais et les cours d'eau qui y communiquent. Son nom actuel vient d'une tribu originaire de l'Île Manitoulin (située dans le lac Huron) qui s'en servait pour venir faire la traite des fourrures avec les premiers explorateurs français. Son nom algonquin, Kitchesippi, signifie "Grande rivière". La rivière des Outaouais a aussi porté les noms suivants: "La Grande rivière" (Galinée 1670), "Rivière des Outaouai", "Rivière des Hurons", "Rivière des Outaouacs" ou "des Prairies" (Bernou 1680), "Outaouais" (Alexander Henry), "Utawas River" (Alexander Mackenzie).

Le la portion québécoise de la rivière des Outaouais, en excluant les îles, a été désigné comme lieu historique par le ministère de la Culture et des Communications.



</doc>
<doc id="6450" url="https://fr.wikipedia.org/wiki?curid=6450" title="Vitruve">
Vitruve

Marcus Vitruvius Pollio, connu sous le nom de Vitruve, est un architecte romain qui vécut au (on situe sa naissance aux alentours de 90 et sa mort vers 20). Son prénom Marcus et son surnom (cognomen) Pollio sont eux-mêmes incertains.

C'est de son traité, "De Architectura", que nous vient l’essentiel des connaissances sur les techniques de construction de l'Antiquité classique.

Cetius Faventinus parle de dans son épitaphe. Il est possible que le cognomen dérive de cette mention par Cetius et qu’il s’agisse d’une erreur d’interprétation, celle-ci signifiant Vitruve, Polio et d’autres. La plupart des faits connus sur sa vie sont extraits de son seul ouvrage, "De architectura", qui nous est parvenu. Il semble cependant être connu de Pline l’Ancien qui l'évoque dans sa description de la construction de mosaïques dans "Naturalis Historia" sans toutefois le nommer explicitement. Frontin se réfère à dans son traité de la fin du , "Sur les aqueducs".
Après avoir été soldat en Gaule, en Espagne et en Grèce, constructeur de machines de guerre, Vitruve devient architecte à Rome. Il nous dit de lui-même qu’il n’est pas grand, et se plaint des affres de l’âge. Sa prose, à la fois technique et imagée, comporte essentiellement des phrases brèves, et son vocabulaire paraît avoir été celui des artisans.

Principalement connu pour ses écrits, Vitruve était lui-même architecte. Dans l’Antiquité romaine, l’architecture était entendue comme un vaste domaine qui comprenait la gestion de la construction, le génie civil, le génie chimique, la construction, le génie des matériaux, le génie mécanique, le génie militaire et la planification urbaine. Frontin mentionne Vitruve dans le cadre de la standardisation de la taille des tuyaux.

Le seul bâtiment, cependant, que nous savons être attribué à Vitruve est une basilique achevée en 19. Elle a été construite à Fanum Fortunae, aujourd’hui la ville moderne de Fano. La basilique de Fano a disparu totalement, si bien que son site même est encore incertain malgré plusieurs tentatives de localisation. La transformation chrétienne de la basilique civile romaine en église à plan basilical, suggère que la basilique a pu être intégrée à l’actuelle cathédrale de Fano. Léonard de Vinci en a fait une œuvre de cet architecte, car pour lui ceci méritait d'être exposé et non ignoré.

Vitruve est l’auteur d’un célèbre traité nommé "De architectura" (en français, « au sujet de l’architecture »), écrit à la fin de sa vie (), et qu’il dédie à l’empereur Auguste. Dans la préface du livre , Vitruve donne comme but à ses écrits d’exposer sa connaissance personnelle de la qualité des bâtiments à l'empereur. Vitruve fait allusion à la campagne de réparations et d’améliorations des bâtiments publics menée sous Marcus Agrippa. "De architectura" est le seul livre majeur qui nous reste sur l’architecture de l’Antiquité classique.

Ce texte selon Petri Liukkonen (2008). Excepté le "De architectura", le livre majeur le plus ancien sur l’architecture dont nous disposons est la reformulation par Alberti des "Dix Livres" en 1452.

Vitruve est resté célèbre pour avoir fait valoir dans son "De architectura" qu’une structure devait présenter les trois qualités de "firmitas", "utilitas", et "venustas" — autrement dit forte (ou pérenne), utile et belle. Selon Vitruve, l’architecture est une imitation de la nature. C’est ce que l’on appellera par la suite la conception "classique" de l’architecture.

En perfectionnant cet art de la construction, la Grèce antique a inventé les ordres architecturaux : dorique, ionique et corinthien. Elle leur a donné un sens des proportions, culminant dans la compréhension des proportions du corps humain. Ceci conduit Vitruve à sa définition de l’homme vitruvien, qui sera ultérieurement réactualisé avec Léonard de Vinci et son célèbre dessin : le corps humain inscrit dans le cercle et le carré (tracé géométrique des caractéristiques fondamentales de l’ordre cosmique).

Vitruve est parfois considéré hâtivement comme le premier architecte. Il est plus exact de le décrire comme le premier architecte romain dont les écrits nous soient parvenus. Il cite lui-même une multitude de travaux qui lui sont antérieurs, moins complets que les siens. Il est moins un penseur original qu’un codificateur de la pratique architecturale de son époque. Les architectes romains pratiquaient une grande variété de disciplines ; en termes modernes, ils pourraient être décrits comme étant des ingénieurs, architectes, architectes-paysagistes, artistes et artisans. Étymologiquement, le mot architecte dérive du mot grec qui signifie « maître » et « constructeur ». Le premier des dix livres traite de nombreux sujets qui se situent dans le champ que l’on définit de nos jours par le paysage.

Les livres VIII, IX et X du "De architectura" forment la base d'une grande partie de ce que nous savons sur la technologie romaine. Cette connaissance est aujourd'hui complétée par l'étude archéologique des vestiges qui subsistent, tels que les moulins à eau de Barbegal en France.

Le travail de Vitruve tire une grande partie de son importance de la description des différentes machines utilisées pour des ouvrages d'art (palans, grues et poulies en particulier) ainsi que des machines de guerre (catapultes, balistes, machines de siège). En tant qu’ingénieur en exercice, Vitruve parle de son expérience personnelle et ne fait pas que rapporter ou commenter le travail de ses prédécesseurs. Il décrit également la construction de cadrans solaires et d’horloges à eau, ainsi que l'utilisation d'un Éolipyle (la première machine à vapeur) dans une expérience visant à démontrer la nature des mouvements de l'air atmosphérique (vent).

Sa description de la construction d'aqueduc comprend la façon dont ils sont suivis et entretenus, ainsi que le choix attentif des matériaux nécessaires. Frontin, un siècle plus tard, donne beaucoup plus de détails sur les problèmes pratiques liés à leur construction et leur entretien.
Le travail de Vitruve date du , soit la période au cours de laquelle un grand nombre des plus grands aqueducs romains ont été construits (et survivent jusqu'à ce jour) tels que l’aqueduc de Ségovie et le pont du Gard.
L'utilisation du siphon inversé est décrite en détail, ainsi que les problèmes posés par les hautes pressions développées dans le tuyau à la base du siphon, problème pratique que Vitruve semble bien connaître.
De architectura était semble-t-il déjà considéré comme un ouvrage de référence par Frontin, un général nommé à la fin du pour administrer les aqueducs de Rome.
Il est à l’origine de la découverte de la différence entre l'apport et la fourniture d'eau causée par des conduites illégales insérées dans les canaux pour détourner l'eau.

Vitruve a décrit de nombreux matériaux de construction utilisés pour une grande variété de structures différentes. Il a également fourni une description détaillée de la peinture sur stuc. Il s’est particulièrement intéressé au béton et à la chaux auxquels il consacre de larges passages de son œuvre. Il explique en particulier l'intérêt de la pouzzolane pour le béton hydraulique qui durcit sous l'eau. La longévité de beaucoup de bâtiments de l’époque romaine est encore aujourd’hui le témoin de la maîtrise avancée par des Romains des matériaux de construction et leur utilisation.

Vitruve est bien connu et souvent cité comme l'une des premières sources à avoir indiqué que le plomb ne devrait pas être utilisé pour transporter l'eau potable. Il s’est fait l’avocat des tuyaux en terre et des canaux en maçonnerie. Il en arrive à cette conclusion dans le livre du "De Architectura" après observation empirique d’ouvriers malades dans les fonderies de plomb. Vitruve nous rapporte l'histoire célèbre d'Archimède détectant de l'or frelaté par un alliage dans une couronne royale. Archimède se rendit compte que le volume de la couronne pouvait être mesuré exactement par le déplacement créé dans un bain d'eau. Cette découverte lui permit de comparer la densité de la couronne avec celle de l’or pur, et ainsi de montrer que la couronne était composée d’un alliage d’or et d’argent.

Vitruve a décrit la construction d'une vis d’Archimède au chapitre X du "De architectura". Il n’y mentionne cependant pas le nom d’Archimède.

Il s’agissait à l’époque d’un dispositif déjà largement utilisé pour élever l’eau afin d'irriguer les champs et pour drainer les mines. Parmi les autres machines de levage qu’il décrit, on trouve notamment une chaîne sans fin de seaux et une roue à aube.

Des vestiges de roues à aubes ont été découverts dans les mines antiques, comme celle du Rio Tinto en Espagne et Dolaucothi dans l’ouest du Pays de Galles. Celles-ci sont exposées au British Museum, et au Musée national du Pays de Galles. Les restes ont été découverts à l’occasion de la réouverture de ces mines dans le cadre de tentatives d’exploitation minière moderne.

Vitruve démontre sa maîtrise de l’arpentage dans ses descriptions des instruments d’arpentage, en particulier le niveau à eau ou chorobate, qu’il préfère à la groma, un dispositif utilisant un fil à plomb. Ces instruments sont essentiels dans toutes les opérations de construction, et tout particulièrement dans la construction d’aqueducs, où s’assurer de l’uniformité de la pente était crucial afin de préserver un approvisionnement régulier en eau sans endommager les parois du canal. Il a également développé l'un des tout premiers odomètres, constitué d'une roue de circonférence connue qui laisse tomber un caillou dans un récipient à chaque rotation.

Vitruve a décrit les nombreuses innovations intervenues dans la conception des bâtiments pour améliorer les conditions de vie des habitants. La plus importante de ces innovations est le développement de l’hypocauste, un type de chauffage central où l’air chaud généré par un feu de bois est canalisé sous le plancher et à l’intérieur des murs des bains publics et des villas.
Il donne des instructions explicites sur la façon de concevoir de tels bâtiments afin d’en optimiser l’efficacité énergétique (par exemple, il conseille de placer le caldarium à côté du tepidarium suivi du frigidarium afin de limiter les déperditions énergétiques). Il conseille également d’utiliser une sorte de régulateur pour contrôler la chaleur dans les pièces chaudes. Il s’agit d'un disque en bronze, installé dans une ouverture circulaire pratiquée dans le toit, et qui pourrait être relevé ou abaissé par une poulie pour ajuster la ventilation. Bien qu’il ne les propose pas lui-même, il est probable que ses dispositifs de roues à aubes aient été utilisés dans les bains les plus vastes pour soulever l'eau dans la partie supérieure des thermes, comme dans les thermes de Dioclétien et les thermes de Caracalla.

Vitruve décrit trois méthodes de construction des brise-lames, des jetées et quais :
Son texte a fait l’objet de diverses analyses par des ingénieurs maritimes modernes, dont le Professeur italien de génie maritime Leopoldo Franco.






</doc>
<doc id="6451" url="https://fr.wikipedia.org/wiki?curid=6451" title="Ilocano">
Ilocano

L’ilocano (parfois ilokano, nom natif "ti pagsasao nga Iloco" ; parfois Iluko, Iloco, Iloco, Ylocano ou Yloco) est une langue appartenant au groupe malayo-polynésien occidental de la famille des langues austronésiennes. Il est parlé aux Philippines par environ de locuteurs représentant 11,5 % de la population totale du pays, principalement dans la partie nord de Luçon. Comprise par près de de Philippins, c'est la troisième langue du pays.



</doc>
<doc id="6453" url="https://fr.wikipedia.org/wiki?curid=6453" title="Nom d'Horus">
Nom d'Horus

Le nom d'Horus est l'un des cinq noms qui composent la titulature des pharaons.

Il assimile pharaon au dieu Horus, fils et successeur d'Osiris, et maître d'Hiérakonpolis d'où est issu Narmer, le premier roi. C'est le nom le plus ancien. Il est toujours précédé de l'image du faucon représentant Horus ("Her"), d'où sa désignation.

Exemples :
Certains pharaons, surtout les plus anciens, ne sont connus que par leur nom d'Horus.

Dans les premiers temps, ce nom était inscrit à l'intérieur d'un serekh :
G5-
Les cinq noms de la titulature utilisés à partir du roi Khéphren sont :


</doc>
<doc id="6454" url="https://fr.wikipedia.org/wiki?curid=6454" title="Nom de Nebty">
Nom de Nebty

Le nom de Nebty ("Les deux Maîtresses" ou "Les deux Déesses") est l'un des cinq noms qui, à partir de Khéphren, forment la titulature des pharaons. Celle-ci comprend :


Le titre "Nebty", qui introduit le nom proprement dit, place le roi sous la protection des deux déesses tutélaires d'Égypte, Nekhbet, la déesse-vautour de Haute-Égypte, et Ouadjet, la déesse-cobra de Basse-Égypte, chacune étant posée sur le caractère signifiant « maître/maîtresse », d'où l'appellation « les deux Maîtresses » :

G16

Amenhotep portait le nom de Nebty « Les deux Maîtresses : "Semene-hepou segereh-taouy" » (Qui établit les lois et apaise le Double Pays), Toutânkhamon celui de « Les deux Maîtresses : "Néfer-hépou segereh-taouy sehetep-netcherou-nebou" » (Celui dont les lois sont parfaites, qui pacifie les Deux Terres et satisfait tous les dieux).

"Nebty" s'écrit avec un y à la fin et non un i ; cette écriture définit un troisième nombre grammatical, le "duel", qui se réfère à deux personnes, tout comme "Taouy" (le Double Pays ou les Deux Terres) ci-dessus qui se réfère à deux choses.


</doc>
<doc id="6455" url="https://fr.wikipedia.org/wiki?curid=6455" title="Géographie de l'Argentine">
Géographie de l'Argentine

LArgentine est un pays d'Amérique latine situé au sud du continent. Il se trouve à l'est du Chili ( de frontières), au sud de la Bolivie (), du Paraguay () et du Brésil () ainsi qu'à l'ouest de l'Uruguay (). L'Argentine possède de côtes.

Le pays occupe une surface de dont de mer.
L'Argentine revendique aussi de terres situés dans l'Atlantique sud et dans l'Antarctique.

Même sans les territoires contestés, l'Argentine est le plus grand pays d'Amérique après le Canada, les États-Unis et le Brésil et le plus grand dans le monde.

Son point culminant est le mont Aconcagua, , qui se situe dans la Cordillère des Andes.


L'Argentine héberge sur son territoire la plupart des plus hauts sommets du continent. Ce sont surtout des volcans, quoique l'Aconcagua et le Mercedario soient d'origine orogénique par plissement. Voici la liste des dix plus hauts sommets, ceux qui ont plus de mètres :


L'Argentine compte ainsi dix des douze plus hauts sommets d'Amérique (y compris ceux situés à la frontière chilienne). Les deux seuls manquant à la liste sont le Huascarán péruvien avec mètres et le Yerupajá de mètres, lui aussi péruvien.

L'Argentine, comme le Chili voisin, possède dans la région occidentale andine ou préandine une grande quantités de volcans. Le département d'Antofagasta de la Sierra, petite partie de la province de Catamarca, on en compte près de 200.

Le fleuve principal de l'Argentine est le Paraná qui, associé à l'Uruguay, constitue le grand estuaire du Río de la Plata.

Les autres grands cours d'eau sont le Paraguay, le Bermejo, le Río Negro, le Río Colorado, le Río Salado del Norte, le Río Santa Cruz, le Río Desaguadero, le Río Neuquén et le Río Limay.

L'Argentine possède de nombreux lacs, dont beaucoup sont situés en Patagonie. Les principaux sont le lac Argentino, le lac Nahuel Huapi, le lac Futalaufquen, le lac Buenos Aires, le lac Viedma, le lac San Martín, le lac Lácar, le lac Huechulafquen, la laguna del Diamante et le lac Menéndez.

Parmi les lacs et lagunes salées, citons la Mar Chiquita, lac salé tellement imposant qu'il a reçu le titre de « mer », la laguna Urre Lauquén, la lagunilla Llancanelo, le lac Colhue Huapi et le lac Cardiel.

Le climat de l'Argentine est différent selon les régions, son climat principal est le climat subtropical humide, mais d'autres sont aussi bien présents, comme le climat désertique en Patagonie, le climat semi-aride, le climat océanique ou celui de la toundra dans la Terre de Feu.
D'abord subtropical au nord, le climat devient tempéré dans le Rio de la Plata puis froid en Patagonie et en Terre de Feu.
L'altitude (de à presque ) et la longueur du pays (du 22e parallèle sud jusqu'au 55e parallèle sud) font des climats divers dans l'Argentine.
C'est un pays globalement désertique, en dehors de la partie méridionale, très humide. Les régions du Nord-Ouest au Sud-Est sont désertiques, c'est ce que l'on appelle la "diagonale aride".


Hormis la capitale qui relève d'un statut spécial, le pays est divisé en régions, elles-mêmes divisées en provinces. Le pays comporte en tout vingt-trois provinces.

 donnait les chiffres suivants.

Population : habitants - estim. juillet 2014 - ( en 2001). 0-14 ans: 25,2%; 15-64 ans: 64,1%; + 65 ans: 10,6 %
Superficie : 
Densité : 14 hab./km
Frontières terrestres : km (Chili km; Paraguay km; Brésil km; Bolivie ; Uruguay )
Littoral : km
Extrémités d'altitude : - > + 
Espérance de vie des hommes : 72,4 ans (en 2008)
Espérance de vie des femmes : 80,05 ans (en 2008)
Taux de croissance de la pop. : 0,95 % (en 2008)
Taux de natalité : (en 2014)
Taux de mortalité : (en 2006)
Taux de mortalité infantile : (en 2008)
Taux de fécondité : 2,25 enfants/femme (en 2014)
Taux de migration : (en 2006)
Indépendance : (ancienne colonie espagnole)
Lignes de téléphone : 8,8 millions (en 2004)
Téléphones portables : 22,1 millions (en 2004)
Postes de radio : 24,3 millions (en 1997)
Postes de télévision : 7,95 millions (en 1997)
Utilisateurs d'Internet : 10 millions (en 2005)
Nombre de fournisseurs d'accès Internet : 33 (en 2000)
Routes : km (dont km goudronnés dont d'autoroutes) (en 2004)
Voies ferrées : km (en 2005)
Voies navigables : km
Nombre d'aéroports : (dont 154 avec des pistes goudronnées) (en 2006)



</doc>
<doc id="6456" url="https://fr.wikipedia.org/wiki?curid=6456" title="Nom d'Horus d'or">
Nom d'Horus d'or

Le nom d'Horus d'or est l'un des cinq noms qui constituent la titulature des pharaons à partir de Khéops.

Il associe le roi à l'Horus solaire et céleste. Il est introduit par le titre :

À partir du roi Khéphren, la titulature royale est formée des noms suivants :


</doc>
<doc id="6457" url="https://fr.wikipedia.org/wiki?curid=6457" title="Nom de Nesout-bity">
Nom de Nesout-bity

Le nom de Nesout-bity ("n(y)-swt-bity" en translittération) est le « nom de couronnement » parmi les cinq noms de la titulature des pharaons dès l'Ancien Empire ; il est dans un cartouche.

"N(y)-swt-bity", littéralement « Celui qui appartient au jonc ("swt") et à l'abeille ("bity") », que les égyptologues traduisent par « Roi de Haute et de Basse-Égypte », associe le roi à la flore et à la faune symboliques des deux parties du royaume.

Le nom de Nesout-bity est également appelé "prænomen" (c'est-à-dire « prénom », bien qu'il n'ait rien à voir avec le prénom moderne) ou encore « nom de couronnement ».

Exemples :

Ce titre est suivi du « nom de naissance » de Pharaon, le nom de Sa-Rê (fils de Rê), également dans un cartouche.

Les cinq noms suivants, dans cet ordre, furent utilisés à partir du roi Khéphren :


</doc>
<doc id="6458" url="https://fr.wikipedia.org/wiki?curid=6458" title="Nom de Sa-Rê">
Nom de Sa-Rê

Le nom de Sa-Rê, qui signifie « fils de Rê », est l'un des cinq noms faisant partie de la titulature des pharaons à partir de Khéphren.

Il est formé des hiéroglyphes du canard ("sa", fils) et du dieu-soleil ("Rê"), suivis du nom de naissance inscrit dans un cartouche, et rattache ainsi charnellement pharaon à la puissance cosmique de l'univers. 

On l'appelle également "nomen" ou encore « nom de naissance ».


Les cinq noms suivants furent utilisés à partir du roi Khéphren :

! Pharaon
! Cartouche
! Translittération (Unicode), transcription et traduction


</doc>
<doc id="6460" url="https://fr.wikipedia.org/wiki?curid=6460" title="George Herbert (égyptologue)">
George Herbert (égyptologue)

George Edward Stanhope Molyneux Herbert, de Carnarvon, plus connu en tant que Lord Carnarvon (, château de Highclere, Newbury – , Le Caire), est un égyptologue britannique.

Il suit ses études au Eton College et au Trinity College. Vicomte de Porchester, il voyage beaucoup, comme tout jeune noble, sportif accompli : en Afrique du Sud, en Australie et au Japon, passant d'un continent à l'autre.

Il devient à 23 ans, à la suite du décès de son père, le Carnarvon et se trouve à la tête d'une fortune colossale et du château avec (). Il se marie en 1895 avec la très riche Almina Wombwell, fille naturelle du banquier Alfred de Rothschild. Il est grièvement blessé à l'âge de 34 ans dans un accident de voiture. Il en souffrira toute sa vie. Ses nombreuses lectures durant sa longue convalescence et notamment celle du livre de Gaston Maspero le décident à partir pour l'Égypte pour acheter une concession et pouvoir faire des fouilles.

Lord Carnarvon se marie avec Almina Victoria Maria Alexandra Wombwell le 26 juin 1895 à l'église St Margaret de Westminster. De leur union naissent deux enfants :


Après le grave accident qui faillit lui coûter la vie, Lord Carnavon suit une longue rééducation et se consacre à la photographie. Puis il se découvre une vocation d'archéologue en prenant connaissance des travaux de Schliemann sur la ville de Troie. La lecture d'un ouvrage de Gaston Maspero éveille en lui la vocation d'égyptologue. Le climat chaud et sec de l'Égypte pourrait lui être favorable. Il s'installe au Caire à l'hôtel Bristol en 1903. Il s'intéresse à l'achat d'une concession pour se livrer à des fouilles sur l'Égypte pharaonique. Il sollicite cette concession auprès de Gaston Maspero en fin d'année 1904. En 1906, il obtient le permis de fouilles sur la colline de Cheikh Abd el-Gournah. Gaston Maspero lui recommande alors Howard Carter.

Gaston Maspero, alors directeur du Service des antiquités, introduit Howard Carter auprès de Lord Carnarvon.

La rencontre des deux hommes eut lieu au Louxor Hôtel. Dès leur première rencontre le pharaon Toutânkhamon est au centre de leurs discussions animées et d'abord conflictuelles. Carter avait une certaine prévention contre la noblesse anglaise. Il accepte de relever le défi.

À partir de l'automne 1907, commence une campagne de fouilles peu productives. Lord Carnarvon obtient un permis de fouilles pour la vallée des rois en . Commence alors une longue campagne de fouilles ponctuée de ci, de là, par des découvertes intéressantes. L'aiguillon d'un concurrent, Theodore Monroe Davis pousse l'émulation. Davis finit par abandonner, persuadé que la tombe de Toutânkhamon est introuvable.

Howard Carter s'acharne. Il va s'intéresser en 1918 au secteur des tombes ramessides et notamment à une zone occupée par des huttes d'ouvriers situées à douze pieds en dessous de l'entrée de la tombe de Ramsès . La guerre qui fait rage en Europe contrarie le déroulement des fouilles ainsi que l'agitation indépendantiste en Égypte (Saad Zaghloul du parti Wafd).

Le , Carter découvre l'entrée d'une sépulture qui était cachée par les déblais. C'est l'entrée de ce qui sera reconnu quelques jours plus tard comme la tombe inviolée du pharaon Toutânkhamon. Lord Carnarvon assiste à la découverte de cette tombe malgré sa santé chancelante, mais n'a pas la chance d'assister à la fouille finale. Une piqûre de moustique tailladée par un rasoir, au Caire, cause sa mort le à 1 h 45. Il est inhumé sur Beacon Hill, dans le parc du château de Highclere.

Une longue opération de déblaiement des accès commence avec l'évacuation lente mais scrupuleusement menée des nombreux objets (mobilier, chars, objets divers, ...). Une chambre se révèle être celle d'un sarcophage inviolé. Les fouilles sont poursuivies avec l'aide financière et le soutien actif de l'épouse (Lady Almina) et de la fille (Evelyn) de feu Lord Carnarvon.

Le refus obstiné de Howard Carter d'admettre la visite de la tombe avant la fin du déblaiement ainsi que les menées des indépendantistes provoquent le retard des opérations, l'expulsion de Carter. La fouille se termine, enfin, par l'ouverture du sarcophage et des trois cercueils abritant la momie.

Lord Carnavon aurait été la première victime de la malédiction du pharaon. Au moment de sa mort le , la rumeur affirme que son chien, Susie, poussa un hurlement avant de mourir lui aussi, et toutes les lumières du château de Highclere ainsi que celles de la ville du Caire en Égypte s'éteignirent, et qu'on ne trouva aucune explication à ces pannes.

Vingt-six autres personnes ayant un lien avec la découverte de la tombe de Toutânkhamon seraient mortes prématurément sans qu'on comprenne de quelle maladie elles étaient atteintes. croient que ce serait causé par l'air de la tombe restée fermée pendant . Toutefois, Howard Carter, le premier à y entrer, n'est mort qu'en 1939 de cause naturelle, ainsi que des centaines d'ouvriers, de curieux, et parmi eux des souverains étrangers, qui visitèrent la tombe.





</doc>
<doc id="6462" url="https://fr.wikipedia.org/wiki?curid=6462" title="Naine brune">
Naine brune

Une naine brune est, d'après la définition provisoire adoptée, en 2003, par l'Union astronomique internationale, un objet substellaire dont la vraie masse est inférieure à la masse minimale nécessaire à la fusion thermonucléaire de l'hydrogène mais supérieure à celle nécessaire à la fusion thermonucléaire du deutérium, correspondant à une masse située entre 13 "M" et 75 "M". En d'autres termes, il s'agit d'un objet insuffisamment massif pour être considéré comme une étoile mais plus massif qu'une planète géante. Il y a un accord sur la limite supérieure en deçà de laquelle une naine brune ne peut entretenir la réaction de fusion nucléaire de l'hydrogène : moins de 0,07 masse solaire pour une composition chimique solaire. La limite inférieure quant à elle ne fait pas unanimité ; un critère couramment retenu est la capacité à fusionner le deutérium, soit environ 13 masses "M".

La classification spectrale des naines brunes a motivé une extension de celle des étoiles : elles ont pour type spectral M, L, T voire Y pour les plus froides.

L'énergie lumineuse d'une naine brune est quasi exclusivement tirée de l'énergie potentielle gravitationnelle, transformée en énergie interne par contraction, contrairement à une étoile de la séquence principale qui tire son énergie des réactions nucléaires. La contraction s'achève lorsque se produit la dégénérescence de la matière, la naine brune a alors un diamètre de l'ordre de celui de la planète Jupiter. En l'absence d'autre source d'énergie, une naine brune se refroidit au cours de son existence, et parcourt les types spectraux M, L et T ; ceci diffère d'une étoile de la séquence principale dont la température effective et le type spectral restent sensiblement constants.

Bien que leur existence fût postulée dès les années 1960, c'est seulement depuis le milieu des années 1990 qu'on a pu établir leur existence.

"Naine brune" est le calque de l'anglais "" qui a été introduit, en 1975, par l'astronome américaine Jill Tarter.

Ce nom, ellipse d'« étoile naine brune », provient de la logique des noms donnés aux étoiles de la séquence principale (« naines ») en fonction de leur couleur (laquelle dépend de leur masse) : naines jaunes (comme le Soleil), naines orange puis naines rouges pour les moins massives, et enfin donc naines brunes pour les objets de masse encore plus faible.

Antérieurement, plusieurs termes avaient été utilisés pour désigner ces objets, tels que "planetar" ou "substar", diminutif du terme général objet substellaire, ou encore « naine noire ». Néanmoins il convient de distinguer les naines brunes de ce que l'on appelle aujourd'hui naine noire, objets très différents : une naine noire est, en quelque sorte, le dernier stade d'une naine blanche, alors qu'une naine brune est un genre d'étoile « ratée », ayant une masse insuffisante pour démarrer ou maintenir les réactions de fusion nucléaire qui ont lieu dans les « vraies » étoiles.

Dès les années 1960, on postule l'existence de corps de masse trop faible pour entretenir la combustion stable de l'hydrogène (Kumar 1963). 

Puisque les naines brunes n'émettent qu'un faible rayonnement, principalement dans l'infrarouge (un domaine de longueur d'onde pour lequel les détecteurs sont restés longtemps très peu sensibles), elles n'ont pas été détectées avant de nombreuses années.

C'est en 1995 qu'on a observé pour la première fois des naines brunes. D'abord, Teide 1, un objet de 40-60 fois la masse de Jupiter a été découvert dans les Pléiades. Quelques mois plus tard, la découverte de Gliese 229B, une naine brune de 20-50 fois la masse de Jupiter, est annoncée. Cette dernière se trouve en orbite autour d'une étoile de faible masse, Gliese 229.

Les naines brunes ont une masse qui se situe entre les planètes les plus massives et les étoiles les moins massives. En raison de cette masse trop faible, la température et la pression du cœur ne sont pas suffisantes pour maintenir les réactions de fusion nucléaire de l'hydrogène. Une naine brune peut, à une certaine époque, avoir réussi à démarrer des réactions de fusion, mais n'avoir jamais atteint un état stable et avoir fini par « s'éteindre ». C'est en quelque sorte une étoile avortée. Une fois la courte phase de réactions nucléaires terminée, la chaleur émise par une naine brune provient uniquement de sa contraction gravitationnelle "via" le mécanisme de Kelvin-Helmholtz.

En général, on considère qu'une naine brune doit avoir une masse supérieure à 13 fois celle de Jupiter, ce qui est la masse inférieure à laquelle un astre peut fusionner du deutérium, et inférieure à 0,07 masse solaire, masse au-dessus de laquelle les réactions de fusion (de l'hydrogène) peuvent s'enclencher durablement. 

Alternativement, il a été proposé qu'une naine brune se distingue d'une planète géante gazeuse par son mode de formation. En effet, la plupart des naines brunes flottent seules dans l'espace. Cela confirme qu'elles se forment comme des étoiles, c'est-à-dire de la fragmentation d'un nuage moléculaire, et non comme des planètes, qui naissent plutôt dans l'effondrement local d'un disque présent autour d'une étoile. 

La découverte d'une naine brune entourée d'un disque protoplanétaire (voir Cha 110913-773444) laisse à supposer que la formation des planètes, sous-produits naturels de la formation stellaire, est possible aussi autour des naines brunes.

On qualifie une naine brune de "froide" à °C, et de "chaude" à partir de °C. La chaleur émise par une naine brune étant le résidu de sa formation, une jeune naine brune sera plutôt chaude, puis se refroidira lentement au cours de son existence. D'ailleurs, les jeunes naines brunes ont des températures de surface semblables à celles des étoiles peu massives et plus âgées et en sont presque indifférenciables. Ce n'est qu'après quelques dizaines à quelques centaines de millions d'années (dépendant de la masse de la naine brune) que celles-ci atteignent les températures des étoiles les plus froides (environ K). Quand les naines brunes atteignent des âges de plusieurs milliards d'années, elles ont des températures de surface allant de 400 K à K, les rendant peu différentes à ce niveau de certaines géantes gazeuses supermassives.

En 2004, il est découvert la première exoplanète orbitant autour d'une naine brune à environ 170 années-lumière. Il s'agit de la planète 2M1207 b orbitant autour de 2M1207. La masse de ce corps est estimée entre 3 et 10 masses de Jupiter.

Les naines brunes se subdivisent en plusieurs types spectraux :









</doc>
<doc id="6469" url="https://fr.wikipedia.org/wiki?curid=6469" title="Java (langage)">
Java (langage)

Java est un langage de programmation orienté objet créé par James Gosling et Patrick Naughton, employés de Sun Microsystems, avec le soutien de Bill Joy (cofondateur de Sun Microsystems en 1982), présenté officiellement le au "".

La société Sun a été ensuite rachetée en 2009 par la société Oracle qui détient et maintient désormais Java.

La particularité et l'objectif central de Java est que les logiciels écrits dans ce langage doivent être très facilement portables sur plusieurs systèmes d’exploitation tels que Unix, Windows, Mac OS ou GNU/Linux, avec peu ou pas de modifications. Pour cela, divers plateformes et frameworks associés visent à guider, sinon garantir, cette portabilité des applications développées en Java.

Le langage Java reprend en grande partie la syntaxe du langage C++, très utilisé par les informaticiens. Néanmoins, Java a été épuré des concepts les plus subtils du C++ et à la fois les plus déroutants, tels que les pointeurs et références, ou l’héritage multiple contourné par l’implémentation des interfaces. Les concepteurs ont privilégié l’approche orientée objet de sorte qu’en Java, tout est objet à l’exception des types primitifs (nombres entiers, nombres à virgule flottante, etc.).

Java permet de développer des applications client-serveur. Côté client, les applets sont à l’origine de la notoriété du langage. C’est surtout côté serveur que Java s’est imposé dans le milieu de l’entreprise grâce aux servlets, le pendant serveur des applets, et plus récemment les JSP (JavaServer Pages) qui peuvent se substituer à PHP, ASP et ASP.NET.

Java a donné naissance à un système d'exploitation (JavaOS), à des environnements de développement (eclipse/JDK), des machines virtuelles (, JRE) applicatives multiplate-forme (JVM), une déclinaison pour les périphériques mobiles/embarqués (J2ME), une bibliothèque de conception d'interface graphique (AWT/Swing), des applications lourdes (Jude, Oracle SQL Worksheet, etc.), des technologies web (servlets, applets) et une déclinaison pour l'entreprise (J2EE). La portabilité du bytecode Java est assurée par la machine virtuelle Java, et éventuellement par des bibliothèques standard incluses dans un JRE.
Cette machine virtuelle peut interpréter le bytecode ou le compiler à la volée en langage machine.
La portabilité est dépendante de la qualité de portage des JVM sur chaque OS.

Le langage Java est issu d’un projet de Sun Microsystems datant de 1990 : l’ingénieur Patrick Naughton n’était pas satisfait par le langage C++ utilisé chez Sun, ses interfaces de programmation en langage C, ainsi que les outils associés. Alors qu’il envisageait une migration vers NeXT, on lui proposa de travailler sur une nouvelle technologie et c’est ainsi que le Projet (furtif) vit le jour.

Le Projet fut rapidement rebaptisé avec l’arrivée de James Gosling et de Mike Sheridan. Ensemble, aidés d’autres ingénieurs, ils commencèrent à travailler dans un bureau de la rue Sand Hill à Menlo Park en Californie. Ils essayèrent d’élaborer une technologie pour le développement d’applications d’une nouvelle génération, offrant à Sun la perspective d’opportunités uniques.

L’équipe envisageait initialement d’utiliser le langage C++, mais l’abandonna pour différentes raisons. Tout d’abord, ils développaient sur un système embarqué avec des ressources limitées et estimaient que l’utilisation du C++ demandait un investissement trop important et que cette complexité était une source d’erreur pour les développeurs. L'absence de ramasse-miettes impliquait que la gestion de la mémoire devait être programmée manuellement, un défi mais aussi une source d’erreurs.

L’équipe était également insatisfaite des lacunes du langage C++ au niveau de la sécurité, de la programmation distribuée, du . De plus, ils voulaient une plate-forme qui puisse être portée sur tout type d’appareils ou de plates-formes.

Bill Joy avait envisagé un nouveau langage combinant le meilleur du langage de programmation et du langage C. Dans un article appelé "Plus loin ()", il proposa à Sun que ses ingénieurs développent un environnement orienté objet basé sur le langage C++. À l’origine, Gosling envisageait de modifier et d’améliorer le langage C++, qu’il appelait C++ ++ --, mais l’idée fut bientôt abandonnée au profit du développement d’un nouveau langage de programmation qu’ils appelèrent (chêne) en référence, selon la légende, à un arbre planté devant la fenêtre de leur bureau.

L’équipe travailla avec acharnement et, à l’été 1992, ils furent capables de faire une démonstration constituée d'une plate-forme incluant le système d’exploitation Green, le langage Oak (1992), les bibliothèques et le matériel. Leur première réalisation, présentée le , fut la construction d’un PDA appelé Star7 ayant une interface graphique et un agent intelligent appelé Duke pour prêter assistance à l’utilisateur.

En novembre de la même année, le fut abandonné pour devenir FirstPerson, Inc, appartenant en totalité à Sun Microsystems et l’équipe fut relocalisée à Palo Alto. L’équipe FirstPerson était intéressée par la construction d’outils hautement interactifs et quand Time Warner publia un appel d’offres en faveur d’un décodeur multifonctions, FirstPerson changea d’objectif pour proposer une telle plate-forme.

Cependant, l’industrie de la télévision par câble trouva qu’elle offrait trop de possibilités à l’utilisateur et FirstPerson perdit le marché au profit de Silicon Graphics. Incapable d’intéresser l’industrie audiovisuelle, la société fut réintégrée au sein de Sun.

De juin à , après trois jours de remue-méninges avec John Gage, James Gosling, Joy, Naughton, Wayne Rosing et Eric Schmidt, l’équipe recentra la plate-forme sur le web. Ils pensaient qu’avec l’avènement du navigateur Mosaic, Internet était le lieu où allait se développer le même genre d’outil interactif que celui qu’ils avaient envisagé pour l’industrie du câble. Naughton développa comme prototype un petit navigateur web, WebRunner qui deviendra par la suite HotJava.

La même année le langage fut renommé Java après qu’on eut découvert que le nom était déjà utilisé par un fabricant de carte vidéo.
Le nom n'est pas un acronyme, il a été choisi lors d'un brainstorming en remplacement du nom d'origine , à cause d'un conflit avec une marque existante, parce que le café ( en argot américain) est la boisson favorite de nombreux programmeurs. Le logo choisi par Sun est d'ailleurs une tasse de café fumant.

En , HotJava et la plate-forme Java furent présentés pour . Java 1.0a fut disponible en téléchargement en 1994 mais la première version publique du navigateur HotJava arriva le à la conférence .

L’annonce fut effectuée par John Gage, le directeur scientifique de Sun Microsystems. Son annonce fut accompagnée de l’annonce surprise de Marc Andressen, vice-président de l’exécutif de Netscape que Netscape allait inclure le support de Java dans ses navigateurs. Le , le groupe Javasoft fut constitué par Sun Microsystems pour développer cette technologie. Deux semaines plus tard la première version de Java était disponible.

L'apparition de la version 1.2 du langage marque un tournant significatif : c'est en 2000 qu'apparait simultanément la déclinaison en deux plateformes Java :

Sun les qualifie alors de plateforme Java 2 par opposition aux premières générations 1.0 et 1.1. Toutes les versions ultérieures, de J2EE 1.2 à Java SE ou Java EE 7 restent désignées sous le qualificatif de plateformes Java 2, bien que le '2' ait été depuis officiellement abandonné.

Applets

Historiquement, la possibilité des navigateurs Web de lancer des applets Java était la seule solution pour afficher des applications clientes riches (RIA pour rich Internet application). Puis des technologies concurrentes ont émergé parmi lesquelles Macromedia Flash, le DHTML JavaScript, Silverlight basé sur XAML ou Xul.

Les applets sur le poste Client peuvent communiquer avec des servlets sur le Serveur, tout comme Javascript peut communiquer avec le Serveur au moyen d’AJAX. Flex utilise quant à lui la technologie Flash par le biais du Adobe Flash Player.

À une époque où JavaScript souffrait de problèmes de compatibilité inter-navigateurs, les applets Java avaient l'avantage de la portabilité car le portage d'interfaces complexes était difficile à assurer pour tous les navigateurs du marché.

Outre la retombée de la « mode » Java, les progrès faits dans les technologies concurrentes à Java ont amené la plupart des développeurs à se détourner des applets Java et des problèmes inhérents à cette technologie (incompatibilités entre les JVM, mauvaises performances, pauvreté des bibliothèques graphiques, complexité). Enfin, les navigateurs modernes n'incluent plus systématiquement l'environnement Java à cause de sa taille importante, et le taux de machines capables d'afficher des applets n'était plus que de 70 % en 2010, bien plus faible que pour Flash par exemple. En 2010, la quasi-totalité des applications clients riches utilisent des technologies alternatives ; Flash pour l'essentiel mais aussi GWT.

Enfin, la perspective de l'arrivée prochaine de HTML5, destiné à embarquer de nombreuses fonctionnalités RIA et multimédia, rend également les applets caduques.

JavaFX

Avec l'apparition de Java 8 en mars 2014, JavaFX devient l'outil de création d'interface graphique ('GUI toolkit') officiel de Java, pour toutes les sortes d'application (applications mobiles, applications sur poste de travail, applications Web…), le développement de son prédécesseur Swing étant abandonné (sauf pour les corrections de bogues). JavaFX est une pure API Java (le langage de script spécifique qui lui a été un temps associé est maintenant abandonné). JavaFX contient des outils très divers, notamment pour les médias audio et vidéo, le graphisme 2D et 3D, la programmation Web, la programmation parallèle, etc.

Avec les serveurs d’applications, on utilise des EJB pour encapsuler les classes définies précédemment. Ces éléments sont utilisés dans des architectures J2EE pour des applications multicouches.
L'avantage qu'on tire de ce travail est de pouvoir cacher au client l'implémentation du code côté serveur.

L’utilisation native du langage Java pour des applications sur un poste de travail restait jusqu'à présent relativement rare à cause de leur manque de rapidité. Cependant, avec l’accroissement rapide de la puissance des ordinateurs, les améliorations au cours des années 2000, de la machine virtuelle Java et de la qualité des compilateurs, plusieurs technologies ont gagné du terrain comme NetBeans et l’environnement Eclipse, les technologies de fichiers partagés LimeWire, Vuze (ex Azureus), et I2P. Java est aussi utilisé dans le programme de mathématiques MATLAB, au niveau de l’interface homme machine et pour le calcul formel. Les applications Swing apparaissent également comme une alternative à la technologie .NET.

Oracle annonce début octobre 2012 à la conférence JavaOne sa volonté de proposer des solutions Java pour le domaine des logiciels embarqués, pour processeurs moins puissants que ceux habituellement disponibles sur les PC. Oracle fédère autour d'elle tout un éco-système d'entreprises spécialistes de ces segments de marchés, comme l'éditeur MicroEJ ou encore STMicroelectronics qui propose du Java sur ses STM32 dont le cœur est un CortexM3/M4.

Java, notamment via Eclipse et NetBeans, offre déjà des environnements de développement intégrés pour mobile. Java est le principal langage utilisé pour développer des applications pour le système d'exploitation libre pour Mobile de Google : Android.

JavaFX peut aussi permettre l'utilisation de Java sur mobiles, bien que ce ne soit pas son objectif principal.

Microsoft a fourni en 2001 un environnement de travail de type Java, dénommé J++, avec ses systèmes d’exploitation avant la sortie de Windows XP. À la suite d'une décision de justice, et au vu du non-respect des spécifications de ce langage, Microsoft a dû abandonner celui-ci et créer un nouveau langage, de nom C# (cf. chapitre « Indépendance vis-à-vis de la plate-forme » plus bas)

Beaucoup de fabricants d’ordinateurs continuent d’inclure un environnement JRE sur leurs systèmes Windows.

Java apparaît également comme un standard au niveau du Mac OS X d’Apple aussi bien que pour les distributions Linux. Ainsi, de nos jours, la plupart des utilisateurs peuvent lancer des applications Java sans aucun problème.
Toutefois, sur ordinateur Apple, la distribution de Java 5 à Java 6 fut assurée directement par Apple, et non par Oracle. Cette politique entraîna des retards et des restrictions de version :

Le , le code source du compilateur javac et de la machine virtuelle HotSpot ont été publiés en Open Source sous la Licence publique générale GNU.

Le , Sun Microsystems annonce le passage de Java, c’est-à-dire le JDK (JRE et outils de développement) et les environnements Java EE (déjà sous licence CDDL) et Java ME sous licence GPL d’ici mars 2007, sous le nom de projet OpenJDK.

En , Sun publie effectivement OpenJDK sous licence libre. Cependant OpenJDK dépend encore de fragments de code non libre que Sun ne détient pas. C'est pourquoi la société Redhat lance en le projet qui vise à remplacer les fragments de code non libre et ainsi rendre OpenJDK utilisable sans aucun logiciel propriétaire. En , le projet IcedTea a passé les tests rigoureux de compatibilité Java (TCK).
IcedTea est donc une implémentation open-source des spécifications de Java. Sun, puis Oracle, garde toutefois le contrôle de la technologie par le biais d'un catalogue de brevets s'appliquant à Java, ainsi que par le maintien du TCK sous une licence propriétaire.

La société Oracle a acquis en 2009 l'entreprise Sun Microsystems. On peut désormais voir apparaître le logo Oracle dans les documentations de l'api Java.

Le 12 avril 2010, James Gosling, le créateur du langage de programmation Java, démissionne d’Oracle pour des motifs qu’il ne souhaite pas divulguer. Il était devenu le directeur technologique de la division logicielle client pour Oracle.

Le langage Java a connu plusieurs évolutions depuis le JDK 1.0 () avec l’ajout de nombreuses classes et packages à la bibliothèque standard. Depuis le J2SE1.4, l’évolution de Java est dirigée par le JCP () qui utilise les JSR () pour proposer des ajouts et des changements sur la plate-forme Java. Le langage lui-même est spécifié par le JLS (), les modifications du JLS étant gérées sous le code JSR 901.

Il faut noter que les évolutions successives du langage ne portent guère sur sa syntaxe -relativement stable depuis le début- mais principalement sur l'enrichissement de ses fonctions, avec l'embarquement et l'optimisation de bibliothèques logicielles (API) dans des domaines très variés de l'informatique : bases de données, gestion XML, informatique distribuée et web, multimédia, sécurité…
Il faut distinguer la version du langage Java de celles des plateformes et du JRE :

Deux versions peuvent parfois être proposées simultanément, telles que 8u65 & 8u66 : la différence consiste généralement en des corrections de bugs mineurs (sans incidence de sécurité notamment), pour lesquelles la mise à jour à la toute dernière version n'est pas critique et est de ce fait laissée au choix des administrateurs (JRE) ou développeurs (JDK).

Les versions publiques de Java peuvent être suivies de versions non publiques, dites Advanced, réservées à des usages commerciaux ; ainsi Java 1.6u45 est la dernière version publique de Java6, mais 6u113 l'ultime version disponible fin mars 2016.


Sorti le - classes et interfaces), son nom de code est . Initialement numérotée 1.5, qui est toujours utilisé comme numéro de version interne). Développé par JSR 176, ajoute un nombre significatif de nouveautés au langage :
Cet exemple parcourt le contenu de l’objet widgets de la classe Iterable et contenant uniquement des références vers des objets de la classe Widget, assignant chacun de ces éléments à la variable w et ensuite appelle la méthode display() sur l’élément w (spécifié dans JSR 201). Une syntaxe similaire sera introduite en 2011 dans C++11.

En plus des changements au niveau du langage, des changements plus importants ont eu lieu au fil des années qui ont conduit des quelques centaines de classes dans le JDK 1.0 à plus de dans J2SE 5.0. Des API entières, comme Swing ou Java2D, ont été ajoutées et beaucoup de méthodes de l’original JDK 1.0 ont été déclarées (c’est-à-dire déconseillées, elles pourraient être supprimées dans une version ultérieure de Java).

Sorti le , contient classes et interfaces dans plus de 20 paquetages). Son nom de code "Mustang". Une version bêta est sortie le 15 février 2006, une autre bêta en juin 2006, une version « » en novembre 2006, et la version finale le 12 décembre 2006. Avec cette version, Sun remplace définitivement le nom J2SE par Java SE et supprime le .0 au numéro de version.

Cette version a été l'objet de nombreuses failles de sécurité et leurs mises à jour correctives, conduisant à la version 1.6.0_45 par Oracle et même 1.6.0_51 pour sa version Mac OS. C'est d'ailleurs là la dernière version de Java fonctionnant pour Mac OS X 10.6 et antérieurs.

Sorti le , contient classes et interfaces). Son nom de code . Il s’agit de la première version sous la licence GNU GPL.

Dès l'update 6 (7u6), l'édition standard Oracle de Java supportant de nouveau pleinement Mac OS X, les mises à jour pour cet OS ne sont plus prises en charge par Apple mais par Oracle. Toutefois cette version de Java n'est pas supportée par Mac OS X v10.6 : En effet certaines API requises par Java 7 ont bien été incluses par Apple dans Mac OS X 10.7.3, mais il n'est pas prévu qu'elles soient implémentées sur les précédentes versions de Mac OS.
La version 7u90 d'avril 2015 est la dernière mise à jour de Java 7 disponible publiquement.

Java 7 propose entre autres les nouveautés suivantes :

Nom de code Kenaï. Diverses releases en cours de développement du JDK sont disponibles au téléchargement dès l'automne 2013, et Java 8 sort mi-mars 2014 conformément à une roadmap présentée par Oracle dès mai 2013.

Une des nouveautés majeures de cette version est l’ajout des , entraînant une refonte d'une partie de l'API, notamment les collections et la notion de "stream". Les autres ajouts notables incluent les optionnels, les implémentations par défaut au sein d'une interface, une refonte de l'API date, etc.
En revanche la version Enterprise Edition (Java 8 EE) n'est pas attendue avant 2017.

La modularisation de la JVM avec le projet Jigsaw, initialement prévue pour cette version, est quant à elle reportée à la version 9, du fait notamment des failles de sécurité rencontrées par Java 6 dont Oracle a privilégié la correction en 2013 par rapport aux évolutions de Java.

Initialement prévu pour 2015 mais reportée en partie à cause du projet Jigsaw, cette version est finalement sortie le 21 septembre 2017.

Java 9 intègre :

Cette version est sortie le 20 mars 2018.

Cette nouvelle version intègre notamment :

Lors de la création du langage Java, il avait été décidé que ce langage devait répondre à cinq objectifs :

La première caractéristique, le caractère orienté objet (« OO ») et familier, fait référence à une méthode de programmation et de conception du langage et le fait qu'un programme écrit en Java ressemble assez fort à un programme écrit en C++.

Bien qu’il existe plusieurs interprétations de l’expression orienté objet, une idée phare dans ce type de développement est que les différents types de données doivent être directement associés avec les différentes opérations qu’on peut effectuer sur ces données. En conséquence, les données (appelées "Propriétés") et le code les manipulant (appelé "Méthodes") sont combinés dans une même entité appelée "Classe" d'objet. Le code devient logiquement découpé en petites entités cohérentes et devient ainsi plus simple à maintenir et plus facilement réutilisable, étant intrinsèquement modulaire.

D’autres mécanismes tels que l’"héritage" permettent d’exploiter toutes les caractéristiques d’une "Classe" précédemment écrite dans ses propres programmes sans même avoir à en connaître le fonctionnement interne — on n’en voit que l’"interface" (l'interface décrit les propriétés et les méthodes sans fournir le code associé). Java interdit la notion d'héritage depuis plusieurs classes parent sauf si elles sont des interfaces.

Dans la version 1.5 du langage ont été rajoutés les "génériques", un mécanisme de polymorphisme semblable (mais différent) aux du langage C++ ou aux foncteurs d’OCaml. Les génériques permettent d’exprimer d’une façon plus simple et plus sûre les propriétés d’objets comme des conteneurs (listes, arbres…) : le type liste est alors considéré génériquement par rapport au type d’objet contenu dans la liste.

Cet élément contribue à la robustesse et à la performance des programmes, le ramasse-miettes () est appelé régulièrement et automatiquement pendant l'exécution du programme. Sur les systèmes multi-processeurs et/ou multi-cœurs, celui-ci emploie même des threads multiples à faible priorité afin de perturber le moins possible l'exécution du programme. En outre, le programmeur peut au besoin suggérer de lancer le ramasse-miettes à l’aide de la méthode "System.gc()".

Un grief récurrent à l’encontre de langages comme C++ est la lourde tâche d’avoir à programmer manuellement la gestion de la mémoire. En C++, la mémoire allouée par le programme pour créer un objet est désallouée lors de la destruction de celui-ci (par exemple par un appel explicite à l'opérateur ). Si le programmeur oublie de coder la désallocation, ceci aboutit à une « fuite mémoire », et le programme en consomme de plus en plus. Pire encore, si par erreur un programme demande plusieurs fois une désallocation, ou emploie une zone de mémoire après avoir demandé sa désallocation, celui-ci deviendra très probablement instable et se plantera.

En Java, une grande partie de ces problèmes est évitée grâce au ramasse-miettes. L'espace mémoire nécessaire à chaque objet créé est alloué dans un tas de mémoire () réservé à cet usage. Le programme peut ensuite accéder à chaque objet grâce à sa référence dans le tas. Quand il n'existe plus aucune référence permettant d'atteindre un objet, le ramasse-miettes le détruit automatiquement — puisqu'il est devenu inaccessible — libérant la mémoire et prévenant ainsi toute fuite de mémoire.
Le ramasse-miettes emploie un algorithme de marquage puis libération () qui permet de gérer les cas complexes d'objets se référençant mutuellement ou de boucles de références (cas d'une liste à chaînage double par exemple). En pratique, il subsiste des cas d'erreur de programmation où le ramasse-miettes considèrera qu'un objet est encore utile alors que le programme n'y accèdera plus, mais dans l’ensemble, le ramasse-miettes rend plus simple et plus sûre la destruction d’objets en Java (en supprimant la nécessité de placer au bon endroit du code l'appel à l'opérateur ).

L’indépendance vis-à-vis de la plate-forme signifie que les programmes écrits en Java fonctionnent de manière parfaitement similaire sur différentes architectures matérielles. La licence de Sun pour Java insiste ainsi sur le fait que toutes les implémentations doivent être compatibles. On peut ainsi théoriquement effectuer le développement sur une architecture donnée et faire tourner l’application finale sur toutes les autres.

Ce résultat est obtenu par :

Noter que même s’il y a explicitement une première phase de compilation, le bytecode Java est soit interprété, soit converti à la volée en code natif par un compilateur à la volée (, JIT).

Les premières implémentations du langage utilisaient une machine virtuelle interprétée pour obtenir la portabilité. Ces implémentations produisaient des programmes qui s’exécutaient plus lentement que ceux écrits en langage compilé (C, C++, etc.) si bien que le langage souffrit d’une réputation de faibles performances.

Des implémentations plus récentes de la machine virtuelle Java (JVM) produisent des programmes beaucoup plus rapides qu’auparavant, en utilisant différentes techniques :

Après que Sun eut constaté que l’implémentation de Microsoft ne supportait pas les interfaces RMI et JNI, et comportait des éléments spécifiques à certaines plates-formes par rapport à sa plate-forme initiale, Sun déposa plainte en justice contre Microsoft, et obtint des dommages et intérêts (20 millions de dollars). Cet acte de justice renforça encore les termes de la licence de Sun. En réponse, Microsoft arrêta le support de Java sur ses plates-formes et, sur les versions récentes de Windows, Internet Explorer ne supporte pas les applets Java sans ajouter de plug-in. Cependant, Sun met à disposition gratuitement des environnements d’exécution de Java pour les différentes plates-formes Microsoft.

La portabilité est techniquement un objectif difficile à atteindre et le succès de Java en ce domaine est mitigé. Quoiqu’il soit effectivement possible d’écrire des programmes pour la plate-forme Java qui fonctionnent correctement sur beaucoup de machines cibles, le nombre important de plates-formes avec de petites erreurs et des incohérences a abouti à un détournement du slogan de Sun « » () en () !

L’indépendance de Java vis-à-vis de la plate-forme est cependant un succès avec les applications côté serveur comme les services web, les servlets et le Java Beans aussi bien que les systèmes embarqués sur OSGi, utilisant l’environnement .

La plate-forme Java fut l’un des premiers systèmes à offrir le support de l’exécution du code à partir de sources distantes. Une applet peut fonctionner dans le navigateur web d’un utilisateur, exécutant du code téléchargé d’un serveur HTTP. Le code d’une applet fonctionne dans un espace très restrictif, ce qui protège l’utilisateur des codes erronés ou mal intentionnés. Cet espace est délimité par un objet appelé "gestionnaire de sécurité". Un tel objet existe aussi pour du code local, mais il est alors par défaut inactif.

Le gestionnaire de sécurité (la classe SecurityManager) permet de définir un certain nombre d’autorisations d’utilisation des ressources du système local (système de fichiers, réseau, propriétés système…). Une autorisation définit :

Les éditeurs d’applet peuvent demander un certificat pour leur permettre de signer numériquement une applet comme sûre, leur donnant ainsi potentiellement (moyennant l’autorisation adéquate) la permission de sortir de l’espace restrictif et d’accéder aux ressources du système local.

Voici un exemple d’un programme typique écrit en Java :
Le fichier source porte presque toujours le nom de la classe avec l'extension ".java" (ici "HelloWorld.java", ce serait même obligatoire si la classe avait l'attribut public dans sa déclaration — la rendant alors accessible à tout autre programme). On peut compiler puis exécuter cet exemple sur le terminal avec les commandes suivantes (sous Linux) :

La ligne « export CLASSPATH=. » sert à indiquer à Java qu’il doit également chercher les programmes class dans le répertoire courant.
Ce chemin peut également être spécifié au lancement du programme par l’option -classpath (ou -cp en abrégé) :

Notes :

Une classe est la description de données appelées attributs, et d’opérations appelées méthodes. Il s'agit d'un modèle de définition pour des objets ayant le même ensemble d’attributs, et le même ensemble d’opérations. À partir d’une classe on peut créer un ou plusieurs objets par instanciation ; chaque objet est une instance d’une seule classe.

Un attribut se définit en donnant son type, puis son nom, et éventuellement une partie initialisation.

Une méthode est définie par :

Le tableau ci-dessus recense les types de base, cependant il existe en Java d'autres types qui sont des objets et sont à utiliser en tant que tel. Par exemple pour définir un entier on peut utiliser le type 'Integer' dont la valeur d'initialisation par défaut vaudra codice_20

Pour instancier une variable, la syntaxe (ici la même qu'en C) est la suivante :

"maVariable" est alors allouée sur la pile.

Il est souvent nécessaire de stocker de nombreuses données dans des collections : liste d’achats, notes des élèves, etc.
Les collections peuvent être consultées, modifiées, on peut les trier, les recopier, les supprimer, etc. Elles peuvent avoir une taille fixe ou variable.

Les collections à taille fixe sont moins lourdes que les collections à taille variable.


La classe abstraite codice_33 est fournie pour implémenter les collections à taille variable.

Pour initialiser une ArrayList il faut importer la classe codice_34 et écrire codice_35 ou codice_36 depuis le JDK 7.0.

Depuis le JDK 1.5, on a la possibilité d’indiquer le type des éléments contenus dans une ArrayList : Entiers, chaînes de caractères ou autres objets.

Pour ajouter un élément on écrit codice_37

Pour accéder à un élément de l’ArrayList : codice_38

Pour connaître le nombre d’éléments que contient une liste : codice_39

Pour supprimer un élément : codice_40 les éléments qui suivent l’élément supprimé seront décalés à gauche.

Bien qu’elles aient toutes un rôle similaire, chaque boucle est pourtant adaptée à une situation :





Le choix d'exécution des instructions est déterminé par l'expression suivant switch dont le type peut être entier (int, char, byte ou short ou classes enveloppes correspondantes), énuméré (enum) ou String (chaîne de caractères, depuis Java 7 seulement).

Remarque : L' "opérateur conditionnel" ou "opérateur ternaire" ? : peut permettre d'éviter l'utilisation d'une instruction conditionnelle. Une expression conditionnelle aura l'une ou l'autre valeur après test de la condition booléenne :


Le bloc de code finally sera exécuté quel que soit le résultat lorsque le programme sortira du bloc try-catch.

Voici un exemple de capture d’une exception :
Cet exemple permet d’illustrer le mécanisme des exceptions en Java. Dans le cas d’une erreur d’entrée/sortie dans le bloc try, l’exécution reprend dans le bloc catch correspondant à cette situation (exception de type IOException).

Dans ce bloc catch, la variable e référence l’exception qui s’est produite. Ici, nous invoquons la méthode printStackTrace() qui affiche dans la console des informations sur l’exception qui s’est produite : nom, motif, état de la pile d’appels au moment de la levée de l’exception et, éventuellement, numéro de ligne auquel l’erreur s’est produite.

Le bloc finally est ensuite exécuté (ici pour refermer les ressources utilisées). Il ne s’agit ici que d’un exemple, l’action à mettre en œuvre lorsqu’une exception survient dépend du fonctionnement général de l’application et de la nature de l’exception.

Un type générique est autrement appelé un Template, il prend un ou plusieurs autres types en arguments.
Le type passé en paramètre est déterminé lors de l'instanciation.

Cela permet notamment dans le cadre des ArrayList d'éviter les transtypages.

Ces types génériques ne sont utilisés qu'à la compilation, et non directement dans le bytecode.

Différence avec le C++ : les templates en C++ dupliquent une classe pour chaque type. Java, au contraire, agit au moment de la compilation comme si on avait dupliqué les classes de ces types intrinsèques mais ne traite en réalité qu'avec une seule classe.

Les spécifications du langage Java précisent qu’il est formé de caractères au format UTF-16, ce qui permet l’utilisation dans le code source de tous les caractères existant dans le monde :

Pour assurer la portabilité entre plates-formes, les noms de classes devraient néanmoins être formés uniquement de caractères ASCII.

Ne pas confondre l'opérateur de test == et l'opérateur d'affectation =.
Pour combiner des conditions il est possible d'utiliser des opérateurs supplémentaires : le « ET » logique && et le « OU » logique || ; il existe également le « NON » logique ! qui permet d'inverser le résultat d'une condition.

Les JavaStyle sont des conventions de programmation en langage Java définies par Sun. Le respect de conventions strictes assure une homogénéité dans le code source d’une application développée par toute une équipe et favorise la diffusion du code source auprès d’une communauté partageant les mêmes conventions de codage.

Le "lower camel case" est utilisé pour les noms de méthodes et de variables.

Sun fournit un grand nombre de et d’API afin de permettre l’utilisation de Java pour des usages très diversifiés.

On distingue essentiellement quatre grands :

La persistance est fondée sur les standards :

On trouve toutefois de nombreuses autres technologies, API et extensions optionnelles pour Java :

La programmation peut se faire depuis une invite de commande en lançant un compilateur Java (souvent nommé javac), mais pour avoir plus de confort, il est préférable d’utiliser un environnement de développement intégré ou IDE, certains sont gratuits.
Par exemple :
Un programme Java peut être produit avec des outils qui automatisent le processus de construction (c'est-à-dire l'automatisation de certaines tâches faisant appel à un nombre potentiellement grand de dépendances comme l'utilisation de bibliothèques, la compilation, la génération d'archives, de documentation, le déploiement, etc.).
Les plus utilisés sont :

Résultats :




</doc>
<doc id="6501" url="https://fr.wikipedia.org/wiki?curid=6501" title="Gaston Maspero">
Gaston Maspero

Gaston Camille Charles Maspero (né le à Paris et mort le ) est un égyptologue français, professeur au Collège de France (1874), membre de l’Académie des inscriptions et belles-lettres (1883), commandeur de la Légion d'honneur (1896).

Gaston Maspero est né à Paris de parents originaires de Lombardie, au nord de l'Italie. Il épouse en 1880 Louise Balluet d'Estournelles de Constant de Rebecque (1856-1953), petite-nièce de Benjamin Constant et sœur de Paul d'Estournelles de Constant, sénateur et Prix Nobel de la paix en 1909. Il est le père de Georges Maspero et d'Henri Maspero, tous deux sinologues et de Jean Maspero, helléniste égyptologue.

Il fait ses études secondaires au lycée Louis-le-Grand puis à l'École normale supérieure (1865). Il s'intéresse très tôt aux langues orientales et traduit le texte de la « stèle de Napata », rapportée par Auguste Mariette. Il passe une année en Amérique latine, notamment en Uruguay (1867-1868), puis il obtient un poste de répétiteur de langue et d'archéologie égyptiennes, à l'École pratique des hautes études, qui venait d’être créée, et où enseigne Emmanuel de Rougé. Durant la guerre franco-prussienne de 1870, il s’engage comme garde mobile et prend la nationalité française. Le 11 novembre 1871, il épouse la journaliste Harriett Yapp, dite Ettie, proche de Mallarmé. Il soutient en 1873 sa thèse de doctorat de lettres, intitulée "Le genre épistolaire chez les anciens Égyptiens", première thèse d'égyptologie universitaire réalisée en France.

En 1872, après la mort d’Emmanuel de Rougé, à 26 ans, Gaston Maspero est proposé à la chaire de philologie et antiquités égyptiennes du Collège de France, mais le ministère l'estime trop jeune et le nomme chargé de cours, il ne sera titularisé qu'en . En 1880, l'état de santé d'Auguste Mariette s'est altéré, et le cabinet du ministre de l’Instruction publique Jules Ferry nomme Gaston Maspero à la tête d'une mission archéologique permanente, sous le nom d’École française du Caire. Le décret est signé par Jules Ferry le 28 décembre 1880. La mission de Maspero consiste à . Après la disparition de Mariette en janvier 1881, Maspero prend sa succession à la direction du Service des antiquités égyptiennes et du musée d’Archéologie égyptienne de Boulaq, au Caire.

Il découvre en 1881 à Saqqarah les "Textes des Pyramides", textes religieux et rituels, qui avaient pour fonction d'aider le mort à accomplir le passage funéraire. Ces textes concernent plusieurs pharaons, notamment Ounas, Pépi et Pépi. D'autres textes funéraires sont trouvés dans la pyramide de Mérenrê. L’année suivante, Maspero, qui a demandé qu'une enquête officielle soit menée à propos de la cachette royale de Deir el-Bahari, dont des antiquités étaient proposées sur le marché depuis quelques années, peut y accéder. Son collaborateur, Emil Brusch, archéologue allemand, se rend sur le site, revendiquant les momies pour le Service de conservation des antiquités de l'Égypte, et obtient le dégagement et la fouille de la cachette des momies royales pour la mission française.

Au début de 1886, Maspero conduit les travaux de désensablement du Sphinx de Gizeh, tandis que quatre habitants de Gournah, fouillant à Deir el-Médineh, trouvent un puits d’accès à une tombe ; Maspero pénètre dans le tombeau de Sennedjem, un fonctionnaire ramesside.
Les découvertes sont acheminées vers le musée de Boulaq, devenu trop exigu et que Maspero projette de transférer au Caire. En 1886, Maspero rentre en France, et reprend ses cours au Collège de France et à l'École des hautes études. Il est rappelé en Égypte en 1899, et y reste alors jusqu'en sa retraite en 1914. Il dirige le déménagement du musée d'égyptologie - entre-temps transféré au palais de Giza de 1889 à 1902 — c'est la création du musée égyptien du Caire. L’inauguration officielle a lieu en .

À Louxor, dans les temples de Karnak, il fait dégager le site qui est fouillé méthodiquement : Voici vingt mois que nous pêchons la statue dans le temple de Karnak. [...] Sept cents monuments en pierre sont déjà sortis de l’eau, mais [...] c’est un peuple complet qui remonte à la lumière et qui vient réclamer un abri aux galeries de notre musée.

En 1904, alors que les Britanniques décident de rehausser de sept mètres le barrage d'Assouan, il parvient à lever les fonds nécessaires pour isoler, consolider, mais aussi étudier un grand nombre d'édifices religieux de Basse-Nubie, menacés d'engloutissement.

Gaston Maspero quitte définitivement l’Égypte en 1914, laissant la direction générale des Antiquités à Pierre Lacau. Il devient secrétaire perpétuel de l’Académie des inscriptions et belles-lettres, et, le , alors qu'il assiste à une séance, il est victime d’un malaise et meurt sur son banc. Sur sa tombe du cimetière du Montparnasse est gravé "Ma spero" (Mais j’espère).





</doc>
<doc id="6507" url="https://fr.wikipedia.org/wiki?curid=6507" title="Man">
Man

Man peut désigner :



MAN est un acronyme qui peut signifier :



</doc>
<doc id="6509" url="https://fr.wikipedia.org/wiki?curid=6509" title="Banque du Mexique">
Banque du Mexique

La Banque du Mexique (en espagnol "Banco de México") est la banque centrale du Mexique.

Elle fournit de nombreux statistiques économiques sur son site, notamment :






La Banque du Mexique est parfois appélé Banxico. En espagnol, on rencontre fréquemment l'abréviation BdeM (pour Banco de México)




</doc>
<doc id="6512" url="https://fr.wikipedia.org/wiki?curid=6512" title="Alcool (chimie)">
Alcool (chimie)

En chimie organique, un alcool est un composé organique dont l'un des carbones (celui-ci étant tétraédrique ) est lié à un groupe hydroxyle (-OH). L'éthanol entrant dans la composition des boissons alcoolisées est un cas particulier d'alcool. Le méthanol et l'éthanol sont toxiques et mortels à haute dose.

Lorsque l'alcool est la fonction principale, il suffit de remplacer la voyelle terminale « e » de l'alcane correspondant par le suffixe -"ol" et d'indiquer le numéro de l'atome de carbone où le groupe hydroxyle est fixé, bien que, parfois, lorsqu'elle n'est pas nécessaire à la description, cette dernière information soit omise.

Si elle n'est pas la fonction principale, il faut ajouter le préfixe "hydroxy-" précédé du numéro de l'atome de carbone où le groupe est fixé.

Pour la base conjuguée de l'alcool, l'ion alcoolate (voir paragraphe acidité), il suffit de remplacer la voyelle terminale « e » par le suffixe "-olate" (ne pas confondre avec le suffixe "-oate" caractéristique du carboxylate, base conjuguée de l'acide carboxylique).

Exemples : 


De manière générique, un alcool contient donc la séquence 
où R est un radical organique variable, souvent un alkyle.

Selon la nature du carbone portant le groupement alcool, on distingue :



Les alcools peuvent être produits par fermentation alcoolique, notamment le méthanol à partir du bois et l'éthanol à partir des fruits et des céréales. L'industrie n'y a recours que dans le cas de l'éthanol pour produire du combustible et des boissons. Dans les autres cas, les alcools sont synthétisés à partir des composés organiques tirés du gaz naturel ou du pétrole notamment par hydratation des alcènes.

Les alcools sont utilisés dans l'industrie chimique comme :

Les alcools de faible masse moléculaire se présentent à température ambiante comme des liquides incolores ; les alcools plus lourds comme des solides blanchâtres.

Le groupe hydroxyle rend généralement la molécule d'alcool polaire. Cela est dû à sa géométrie (coudée, de type AXE), et aux électronégativités respectives du carbone, de l'oxygène et de l'hydrogène (χ(O) > χ(C) > χ(H)). Ces groupes peuvent former des liaisons hydrogène entre eux ou avec d'autres composés (ce qui explique leur solubilité dans l'eau et dans les autres alcools).

Le point d'ébullition est élevé chez les alcools :

Aussi, le point d'ébullition des alcools est-il d'autant plus élevé que :

La solubilité dans l'eau des alcools dépend des deux mêmes facteurs que précédemment, mais qui sont ici antagonistes :

Ainsi, les alcools sont d'autant plus solubles dans l'eau que :

Les alcools de faible masse moléculaire sont généralement solubles dans les solvants organiques comme l'acétone ou l'éther.

La polarisation forte de la liaison O-H donne la possibilité d'une rupture ionique : les alcools constituent donc des acides faibles, et même très faibles (pK compris en général entre 16 et 18, 10 pour les phénols, dans l'eau) par libération d'un proton H du groupe hydroxyle. Ils sont donc bien plus faibles que l'eau (à l'exception du méthanol) et ne manifestent leur caractère acide que dans des solutions non aqueuses, en réagissant par exemple avec la base dans une solution d'ammoniaque. On appelle la base conjuguée d'un alcool un ion alcoolate (ou alkoxyde).

L'un des doublets libres de l'oxygène est capable de capturer un proton : l'alcool est donc une base de Brönsted, indifférente (pK(ROH/ROH) d'environ -2), son acide conjugué, l'ion alkyloxonium, étant un acide fort, ne pouvant être présent qu'en très petite quantité (sauf en présence d'une concentration importante en acide fort).

Grâce à la réactivité de ces doublets, l'alcool est aussi une base de Lewis.

Les alcools sont de très bons nucléophiles, propriété toujours due à la réactivité des doublets libres de l'oxygène, réaction de surcroît rapide.

Cette propriété lui permet notamment, comme nous allons le voir par la suite, de participer à des réactions de substitutions nucléophiles et des réactions d'éliminations.

Les alcools peuvent subir une substitution nucléophile dans laquelle le groupe hydroxyle est remplacé par un autre radical nucléophile.

Synthèse de Williamson

L'alcool réagit avec un hydracide (chlorure, bromure, fluorure ou iodure d'hydrogène) pour former un halogénoalcane :

Il s'agit de la réaction inverse de la réaction d'hydrolyse des dérivés halogénés.


Selon la classe de l'alcool, des mécanismes limites sont envisageables :


Différents composés peuvent être employés pour permettre une halogénation des alcools.

Les alcools peuvent subir une réaction d'élimination d'eau (réaction de déshydratation) à haute température en milieu acide et produire des alcènes :

Cette réaction peut être inversée pour synthétiser des alcools à partir d'alcènes et d'eau (réaction d'hydratation des alcènes), mais reste peu fiable car elle produit des mélanges d'alcools.

En réagissant avec un acide carboxylique, l'alcool forme un ester.

Les alcools peuvent être oxydés en aldéhydes, cétones ou acides carboxyliques.

Les alcools peuvent être réduits en alcanes à l'aide de tributylétain par une substitution radicalaire appelée réaction de Barton-McCombie.


L'éthanol est une substance psychotrope toxique voire mortelle en grande quantité, même en quantité modérée en cas de consommation régulière (voir alcoolisme).

Les autres alcools sont généralement beaucoup plus toxiques car :
Leur ingestion est considérée comme une urgence médicale.



</doc>
<doc id="6514" url="https://fr.wikipedia.org/wiki?curid=6514" title="Cétone">
Cétone

Une cétone est un composé organique, faisant partie de la famille des composés carbonylés, dont l'un des carbones porte un groupement carbonyle.

Contrairement aux aldéhydes (qui n’ont autour de ce groupement qu’un carbone primaire), c'est un carbone secondaire (lié à exactement 2 atomes de carbones voisins) qui porte le groupement carbonyle pour les cétones.

Une cétone contient donc la séquence R-CO-R (image ci-contre), où R et R sont des chaînes carbonées, et pas de simples atomes d'hydrogène liés au carbone central porteur de la fonction carbonyle.


On peut par exemple citer l'acétone (dont le nom normalisé est le propan-2-one) CH-CO-CH.

Elles sont générales aux aldéhydes et aux cétones, en particulier concernant leur équilibre céto/énol. Voir l'article général composé carbonylé.

On obtient les cétones avec une oxydation ménagée d'un alcool secondaire. Cela nécessite un fort agent oxydant. Cependant, ici, il n'y a pas de "risque" : une cétone après être formée ne peut plus être oxydée, contrairement à l'aldéhyde, qui peut encore l'être en acide carboxylique.


On utilise souvent le dichromate de potassium, ou le Cr(VI) (via du PCC par exemple) , mais le réactif le plus couramment utilisé est le réactif de Jones : CrO en présence d'acide sulfurique dans l'acétone. Parfois, si la présence d'un acide est gênante, on utilise le réactif de Sarett : CrO dans la pyridine.

En ajoutant de l'eau à un alcyne en milieu acide, un groupement carbonyle se formera sur le carbone porteur de la triple liaison ayant le moins d'hydrogènes (règle de Markovnikov), à condition d'avoir du mercure oxydé +2 comme catalyseur (Hg). L'autre carbone porteur de la triple liaison sera réduit en gagnant deux atomes d'hydrogène.

L'ozonolyse consiste à traiter un réactif à l'ozone et à l'eau. Ici, l'alcène subit une ozonolyse pour former un groupement carbonyle, à condition d'avoir comme catalyseur de l'acide éthanoïque (ou acide acétique) et du zinc oxydé +2 (Zn). Ceci forme un intermédiaire cyclique très instable : l'ozonide, qui se transforme ensuite en cétone ou en aldéhyde. Afin d'obtenir une cétone à partir d'un alcène, au moins un carbone porteur de la double liaison doit être exclusivement lié à d'autres carbones, faute de quoi, un aldéhyde sera formé. La réaction dégage également du peroxyde d'hydrogène.

Par exemple, en traitant le 2-méthylbut-2-ène à l'ozone et à l'eau (avec les catalyseurs nécessaires), on obtient la propanone (une cétone), l'éthanal ou l'acétaldéhyde (un aldéhyde) et le peroxyde d'hydrogène.

Le principe consiste à mettre en présence un benzène avec un chlorure d'acyle et du AlCl. Le résultat est une cétone fixée sur le benzène et du HCl. Le mécanisme de réaction suit le principe d'une addition électrophile sur un aromatique, d'où la présence d'AlCl (acide de Lewis) qui va servir à créer un électrophile "(voir Réaction de Friedel-Crafts)".

Les cétones sont utilisés dans la fabrication de matières plastiques, comme solvants, mais aussi comme colorants en parfumerie et pour les médicaments comme les aldéhydes.

L'acétalisation est une réaction réversible permettant de transformer un composé carbonylé et deux alcools (ou un diol) en acétal. Cette réaction permet en fait de protéger le groupe carbonyle, ou l'alcool.


Quand on utilise un aldéhyde, on nomme le produit obtenu « acétal ». Quand on utilise une cétone, on nomme le produit obtenu « cétal ». Cependant, de plus en plus on utilise le terme acétal comme terme générique pour désigner les 2 produits. On appelle par contre toujours la réaction (aussi bien pour les aldéhydes et pour les cétones) « acétalisation ».

Cette réaction est en général en défaveur du cétal (contrairement aux acétals). Si on veut la favoriser dans le sens de la formation de celui-ci, on utilise un excès d'alcool (qui sert par la même occasion de solvant). Il faut aussi distiller l'eau (distillation hétéroazéotropique) à l'aide de l'appareil de Dean et Stark. Pour favoriser la réaction inverse, c'est le contraire : on met un excès d'eau pour hydrolyser le cétal.

Quand on veut protéger la fonction carbonyle, on utilise le plus souvent un diol, comme l'éthane-1,2-diol:
Cette réaction peut aussi très bien servir à protéger une fonction alcool, en particulier les diols vicinaux.

Un organomagnésien mixte réagit sur une cétone pour former un alcoolate tertiaire. Le carbone porteur de l'organomagnésien est ajouté sur le carbone porteur du groupement carbonyle. En milieu aqueux, l'alcoolate, base forte, gagne un proton pour former un alcool.

La réaction de Wittig transforme une cétone en dérivé éthylènique. Il transforme en fait la liaison C=O en liaison C=C.

On utilise le dihydrogène H dans un solvant inerte, en présence d'un catalyseur (catalyse hétérogène). Il s'agit bien souvent de métaux, comme le platine (Pt), le palladium (Pd), le nickel (Ni) ou le rhodium (Rh).

Si jamais la cétone possède aussi une liaison C=C, celle-ci aussi est hydrogénée (la réaction est plus facile sur C=C que sur C=O). Pour éviter cela, il faut effectuer une addition nucléophile d'hydrures.
La réaction est exothermique.

Voir addition nucléophiles d'hydrures (plus bas).


L'addition d'hydrures permet la réduction de l'oxygène du groupement carbonyle sans toutefois altérer une double liaison (la partie alcène) d'une molécule, comparativement à la simple addition d'hydrogène.

Une cétone portant également un alcène, lorsque traitée à un hydrure, produit un alcoolate qui porte un alcène. Cet alcoolate, traité avec un acide faible (comme l'eau) est alors transformé en alcool qui porte un alcène.

organomagnésiens mixtes, ou organolithiens.

Transformation en ester à l'aide d'un peroxyde comme l'eau oxygénée ou un peracide comme l'acide méta-chloroperbenzoïque (mCPBA).


On peut également obtenir un dérivé carbonylé (cétone ou aldéhyde) par hydrolyse d'une fonction imine, de formule générale RR'C=N, en catalyse acide ou basique.
Cette hydrolyse permet par exemple d'obtenir une cétone comme produit final de l'addition d'un réactif de Grignard (organomagnésien mixte) sur un nitrile.


Mais ces méthodes utilisant des réactions chimiques avec la cétone, la détruisent.
On utilise maintenant plus volontiers les méthodes de spectrométrie (RMN et infrarouge) permettent de détecter cette fonction sans destruction de l'échantillon.



</doc>
<doc id="6519" url="https://fr.wikipedia.org/wiki?curid=6519" title="Ester">
Ester

En chimie, la fonction ester désigne un groupement caractéristique formé d'un atome lié simultanément à un atome d'oxygène par une double liaison et à un groupement alkoxy du type R-COO-R'. Quand cet atome est un atome de carbone, on parle d'esters carboxyliques. Cependant, ce peut être aussi un atome de soufre (par exemple dans les esters sulfuriques, sulféniques, etc.), d'azote (esters nitriques, etc.), de phosphore (esters phosphoriques, phosphoniques, phosphéniques, etc.), etc. En fait, tout oxoacide organique, inorganique ou minéral a la capacité de former des esters.

Les esters carboxyliques sont des dérivés des acides carboxyliques, ils résultent très généralement de l'action d'un alcool sur ces acides avec élimination d'eau. Les fonctions esters se retrouvent dans de nombreuses molécules biologiques, notamment les triglycérides. Les esters carboxyliques ont souvent une odeur agréable et sont souvent à l'origine de l'arôme naturel des fruits. Ils sont aussi beaucoup utilisés pour les arômes synthétiques et dans la parfumerie.

Le nom d'un ester comporte deux termes : 


Pour les dérivés de l'acide formique, de l'acide acétique, les noms traditionnels sont utilisés alors que pour les autres acides, le nom systématique est recommandé, ce qui donne :

Les noms en gras du tableau ci-dessus ceux de la nomenclature IUPAC.
Les noms de la nomenclature systématique sont donnés quand ils sont différents de ceux de l'IUPAC.
En italique : d'autres noms qui ne relèvent ni de la nomenclature systématique ni de l'IUPAC mais qui sont néanmoins, pour l'acide propionique et l'acide butyrique, acceptés par l'UIPAC, leurs esters reprenant obligatoirement les noms systématiques.

Si un autre groupe caractéristique a la priorité, il y a 2 possibilités pour désigner un ester:



La méthode de synthèse la plus simple et la plus courante est appelée estérification. Il s'agit de la condensation d'un acide carboxylique ou de l'un de ses dérivés (chlorure d'acyle, anhydride d'acide) avec un alcool, donnant l'ester et un autre composé (eau, acide chlorhydrique ou acide carboxylique). 

Dans le cas de la réaction d'un acide carboxylique avec un alcool, on parle de réaction ou estérification de Fischer : 

Cette réaction est lente, presque athermique (légèrement exothermique en fait) et réversible (la réaction inverse appelée rétro-estérification est une hydrolyse acide de l'ester), ce qui la rend limitée. Son rendement dépend particulièrement de la classe de l'alcool utilisé (rendement moyen à bon pour des alcools primaires et secondaires, mauvais rendement pour des alcools tertiaires).
Il est possible d'améliorer la cinétique de cette réaction (qui sinon met plusieurs mois à atteindre son équilibre) en chauffant (ce qui n'a pas d'influence sur le rendement), ou en la catalysant par un acide. Le rendement quant à lui peut être amélioré par exemple en mettant un réactif en excès ou en enlevant l'eau produite lors de la réaction avec un appareil de Dean Stark.

Une autre possibilité est d'utiliser des dérivés d'acides pour synthétiser des esters :



Ces réactions, contrairement à l'estérification de Fischer présentent l'avantage d'être rapides et totales. 

Il est aussi possible de synthétiser un ester à partir d'un autre ester et d'un alcool. On parle alors de transestérification.

Cette réaction est utilisée dans l'industrie pour la fabrication du polyester et du biodiesel. Elle est aussi présente en biologie ; c'est entre autres le mécanisme qui permet l'épissage des introns lors de la maturation des ARNm.

Outre le fait d'obtenir un ester, utile dans l'industrie agroalimentaire, en parfumerie ou d'autres secteurs industriels, l'estérification est utile, de par son caractère réversible (pour les acides carboxyliques et les alcools, tout du moins), dans le cadre de la protection de fonctions.

Puisque la transformation est réversible, elle permet de protéger soit la fonction alcool, soit la fonction acide carboxylique, soit les deux. En effet, si on veut protéger un alcool, on le fait réagir avec un acide carboxylique pour former un ester ; on fait la réaction que l'on voulait effectuer ; une fois celle-ci finie, on renverse la réaction d'estérification pour retrouver l'alcool.

Il existe deux méthodes pour renverser l'estérification :

Les esters sont aussi un constituant de base dans l'industrie des plastiques. Ils sont à la base d'un des plastiques les plus utilisés, le polyester.

Il s'agit d'un moyen de former les lactones : estérification intramoléculaire à partir d'un hydroxyacide carboxylique.

Les esters peuvent être réduits :





</doc>
<doc id="6520" url="https://fr.wikipedia.org/wiki?curid=6520" title="Fonction acide en chimie organique">
Fonction acide en chimie organique

En chimie organique, un grand nombre de groupes fonctionnels présentent une acidité selon Brönstedt.

R-CO-OH.
Du fait du niveau d'oxydation du carbone, la fonction acide est toujours située en bout de chaîne. Un tel composé est nommé acide carboxylique.

Pour le nommer en suivant la nomenclature internationale, on utilise le nom de la chaîne carbonée sur laquelle est fixée la fonction, que l'on précède du mot « acide » et auquel on ajoute le suffixe « -oïque ». Par exemple : acide éthanoïque (ou acide acétique) pour formula_1.

On peut obtenir un acide carboxylique par exemple en oxydant un aldéhyde (voir oxydation), lui-même pouvant être issu de l'oxydation d'un alcool primaire).


</doc>
<doc id="6524" url="https://fr.wikipedia.org/wiki?curid=6524" title="Gaz à effet de serre">
Gaz à effet de serre

Les gaz à effet de serre (GES) sont des composants gazeux qui absorbent le rayonnement infrarouge émis par la surface terrestre et contribuent à l'effet de serre. L'augmentation de leur concentration dans l'atmosphère terrestre est l'un des facteurs à l'origine du réchauffement climatique.

Un gaz ne peut absorber les rayonnements infrarouges qu'à partir de trois atomes par molécule, ou à partir de deux si ce sont deux atomes différents.

Les principaux gaz à effet de serre qui existent naturellement dans l'atmosphère sont :

Les gaz à effet de serre industriels comprennent aussi des hydrocarbures halogénés comme :
Notes : 

Sous l'effet des gaz à effet de serre, l'atmosphère terrestre se comporte en partie comme la vitre d'une serre, laissant entrer une grosse partie du rayonnement solaire, mais retenant le rayonnement infrarouge réémis. Mais dans une serre il y a, en plus, l'absence de convection qui accentue l'échauffement de l'air.

La transparence de l'atmosphère (dans l'ordre du spectre visible) permet au rayonnement solaire d'atteindre le sol. L'énergie ainsi apportée s'y transforme en chaleur. Comme tout corps chaud, la surface de la Terre rayonne sa chaleur. Mais les GES et les nuages sont opaques aux rayons infrarouges émis par la Terre. En absorbant ces rayonnements, ils emprisonnent l'énergie thermique près de la surface du globe, où elle réchauffe l'atmosphère basse. Les nuages qui sont des particules de glace (ou d'eau liquide) réfléchissent le rayonnement solaire vers l'espace et le rayonnement terrestre vers elle. Les nuages ont un effet sur le climat encore mal connu au début du car ils atténuent le rayonnement infrarouge reçu à la surface de la Terre ; mais ils participent à la réflexion vers la Terre du rayonnement infrarouge.

L'effet de serre, principalement dû à la vapeur d'eau (0,3 % en volume, 55 % de l'effet de serre) et aux nuages (17 % de l'effet de serre) soit environ 72 % pour , les 28 % restant étant pour l'essentiel le fait du , porte la température moyenne à la surface de la Terre de (ce qu'elle serait en son absence) à +.

Selon Sandrine Anquetin, directrice de recherche CNRS au Laboratoire d’étude des transferts en hydrologie et environnement (LTHE) de Grenoble, les scientifiques observent et anticipent une intensification du cycle hydrologique. L’augmentation de la température globale moyenne conduit à une augmentation de l’évaporation de l’eau, et donc à davantage d’humidité dans l’atmosphère. Plus l’atmosphère se réchauffe, plus elle stocke et transporte l’humidité. L’intensification du cycle de l’eau est scientifiquement avérée à l’échelle globale. Il convient désormais de comprendre et d’anticiper sa déclinaison à l’échelle régionale.

Les concentrations en gaz à effet de serre dans l'atmosphère terrestre augmentent depuis le pour des raisons essentiellement anthropiques avec un nouveau record en 2012 selon l'Organisation météorologique mondiale (OMM). L'accroissement des principaux gaz à effet de serre est essentiellement dû à certaines activités humaines dont :

Le protocole de Kyoto, qui s'était donné comme objectif de stabiliser puis de réduire les émissions de GES afin de limiter le réchauffement climatique, n'a pas tenu ses objectifs.


Chaque GES a un effet différent sur le réchauffement global. Par exemple, sur une période de , un kilogramme de méthane a un impact sur l'effet de serre plus fort qu'un kilogramme de . Alors pour comparer les émissions de chaque gaz, en fonction de leur impact sur les changements climatiques on préfère utiliser des unités communes : l'équivalent ou bien l'équivalent carbone, plutôt que de mesurer les émissions de chaque gaz.

L'équivalent est aussi appelé potentiel de réchauffement global (PRG). Il vaut 1 pour le dioxyde de carbone qui sert de référence. Le potentiel de réchauffement global d'un gaz est la masse de qui produirait un impact équivalent sur l'effet de serre. Par exemple, le méthane a un PRG de 25, ce qui signifie qu'il a un pouvoir de réchauffement supérieur au dioxyde de carbone.

Il n'y a pas de PRG pour la vapeur d'eau : la vapeur d'eau en excès réside moins de dans l'atmosphère, dont elle est éliminée par précipitation. 

Pour l'équivalent carbone, on part du fait qu' de contient de carbone. L'émission d' de vaut donc d'équivalent carbone. Pour les autres gaz, l'équivalent carbone vaut :
On peut noter que la combustion d'une tonne de carbone correspond bien à l'émission d'une tonne équivalent carbone de , car le rapport est de 1:1 (il y a un atome de carbone C dans une molécule de ).

Cette unité de mesure, utile pour comparer les émissions produites, est utilisée dans la suite de cet article.

Hormis la vapeur d'eau, qui est évacuée en quelques jours, les gaz à effet de serre mettent très longtemps à s'éliminer de l'atmosphère. Étant donné la complexité du système atmosphérique, il est difficile de préciser la durée exacte de leur séjour. Ils peuvent être évacués de plusieurs manières :

Voici quelques estimations de la durée de séjour des gaz, c'est-à-dire le temps nécessaire pour que leur concentration diminue de moitié.

L’Organisation météorologique mondiale (OMM) a annoncé le que les concentrations mondiales de gaz à effet de serre avaient atteint de nouveaux records en 2016 : 

L’Organisation météorologique mondiale (OMM) a annoncé le 26 mai 2014 qu'en avril, pour la première fois, les concentrations mensuelles de dans l'atmosphère ont dépassé le seuil symbolique de par million (ppm) dans tout l'hémisphère nord ; dans l'hémisphère sud, les concentrations sont de , du fait de la moindre densité de population et d'activité économique ; la moyenne mondiale à l'époque préindustrielle était de .

La Convention-cadre des Nations unies sur les changements climatiques fournit sur son site internet de nombreuses données sur les émissions des pays parties à ladite convention :

Après trois ans de relatif répit, les émissions mondiales de gaz à effet de serre devraient croître d'environ 2 % en 2017 par rapport à 2016 et atteindre le niveau record de 36,8 milliards de tonnes, selon les estimations établies par le Global Carbon Project, une plate-forme animée par des scientifiques issus du monde entier.

Eurostat publie des statistiques destinées au suivi des engagements du Protocole de Kyoto :

Remarques : 

La progression et les fluctuations de la teneur en sont retracées quasiment en temps réel sur le site ESRL (Earth System Research Laboratory).

Les émissions de dans le monde ont augmenté de 33,4 % entre 1990 et 2006. Entre 2005 et 2006, elles ont augmenté de 3,2 %.

La situation est très contrastée selon les zones géographiques. En 2006, les deux pays plus gros émetteurs de étaient les États-Unis (20,3 % des émissions mondiales), suivis de très près par la Chine (20,2 %). Toutefois, étant donné le fort taux d'augmentation annuel de la Chine, celle-ci est devenue depuis 2006 le plus gros émetteur mondial de . Dans l'Union européenne, la France est l'un des plus faibles émetteurs, par rapport à sa population, ce qui est dû à une très forte proportion de production d'électricité d'origine nucléaire et hydraulique.

Il y a une forte disparité dans les taux d'augmentation des émissions entre 1990 et 2006 selon les zones géographiques dans le monde. La plus forte augmentation est au Moyen-Orient, avec un taux de + 119,6 %. Puis c'est l'Extrême-Orient avec + 108,6 %, mais il faut distinguer dans cet ensemble la Chine qui a une augmentation de + 151,7 %, et l'Inde de + 112,1 %. L'Amérique latine a vu ses émissions progresser de + 61,2 %, et l'Afrique de + 55,5 %, mais leurs émissions sont encore relativement faibles en valeur absolue (3,5 % du total mondial pour l'Amérique latine, et 3,1 % pour l'Afrique). L'Océanie a vu ses émissions progresser de + 53,4 %. L'Amérique du Nord a vu ses émissions progresser de + 19 %. La seule zone géographique qui a vu ses émissions baisser est l'Europe et l'ex-URSS avec - 14,8 %, chiffre dû surtout à la Russie et à l'Europe de l'Est, L'union européenne à 15 ayant augmenté de + 5,4 %.

Les émissions de la France étaient de de par habitant en 2002, ce qui la plaçait en dans le monde, comme l'un des pays développés avec les plus faibles émissions par habitant, alors que les États-Unis avaient des émissions de de par habitant, ce qui les plaçait en dans le monde.

L'étude du "Global carbon project", publiée le 21 septembre 2014, avant le sommet de l'ONU sur le climat, annonce que les émissions de devraient atteindre 37 milliards de tonnes en 2014 et 43,2 Mds de tonnes en 2019 ; en 2013, elles avaient progressé de 2,3 % pour atteindre 36,1 Mds de tonnes. En 2013, un Chinois émet désormais davantage qu'un Européen, avec 7,2 tonnes de par tête contre 6,8 tonnes par tête dans l'Union européenne, mais un Américain émet 16,4 tonnes de ; la progression de ces émissions est très rapide en Chine (+4,2 % en 2013) et en Inde (5,1 %) alors qu'en Europe elles reculent (-1,8 %). Le "Global carbon project" souligne que la trajectoire actuelle des émissions de gaz carbonique concorde avec le pire des scénarios évoqués par le GIEC, qui table sur une hausse de la température mondiale de 3,2 à 5,4°C d'ici 2100.

Les émissions de liées à l'énergie ont enregistré un coup d'arrêt en 2014 ; c'est la première fois, depuis 40 ans que l'Agence internationale de l'énergie (AIE) établit ses statistiques d'émissions de , que ces émissions cessent de croître dans un contexte de croissance économique (+3 %) ; elles avaient connu trois baisses : au début des années 1980, en 1992 et en 2009, toutes causées par un recul de l'activité économique. Le secteur de l'énergie a émis de comme en 2013. L'AIE attribue les mérites de cette stabilisation pour l'essentiel à la Chine et aux pays de l'OCDE. En Chine, « l'année 2014 a été marquée par la croissance de la production électrique issue des énergies renouvelables, hydraulique, solaire, éolienne. L'électricité fournie par les centrales au charbon a moins compté », et la consommation s'est fortement ralentie. Les pays développés de l'OCDE sont parvenus à découpler la croissance de leurs émissions de gaz à effet de serre de celle de leur économie, grâce à leurs progrès dans l'efficacité énergétique et l'utilisation des énergies renouvelables.

Les émissions de liées à l'énergie sont reparties à la hausse en 2017, après trois années de stagnation, selon l'Agence internationale de l'énergie, à , soit +1,4 %. Cette augmentation résulte d'une robuste croissance économique mondiale (+3,7%), de prix bas pour les combustibles fossiles et de moindre efforts réalisés en matière d'efficacité énergétique. Les émissions de de la plupart des grandes économies ont augmenté en 2017, mais elles ont reculé au Royaume-Uni, au Mexique, au Japon et aux États-Unis ; leur recul de 0,5 % aux États-Unis s'explique par le déploiement plus important d'énergies renouvelables, combiné à un déclin de la demande d'électricité. L'Asie est responsable des deux tiers de l'augmentation des émissions ; les émissions n'ont progressé que de 1,7 % en Chine malgré une croissance de près de 7%, en raison du déploiement d'énergies renouvelables et du remplacement de charbon par du gaz. Dans l'Union européenne, les émissions ont progressé de 1,5 %, inversant les progrès réalisé ces dernières années, en raison d'un recours accru au pétrole et au gaz.

La question de la répartition des responsabilités des émissions anthropiques a été un des points les plus épineux des négociations internationales sur le réchauffement climatique. Les pays émergents font valoir que les objectifs d'efforts de réduction des émissions devraient être répartis en fonction des émissions cumulées depuis le début de l'ère industrielle de chaque pays.

Le point de vue adopté le plus fréquemment consiste à attribuer à chaque pays les émissions produites sur son territoire. Or deux autres points de vue peuvent être soutenus :

Depuis 2006, la Chine a dépassé les États-Unis pour les émissions de gaz à effet de serre. Les émissions de dioxyde de carbone de la Chine étaient en 2012 de 7955 millions de Tonnes Équivalent Carbone, contre 5287 Mt pour les États-Unis, 1653 Mt pour la Russie et 1745 Mt pour l’Inde ; elles sont passées de 5,8 % du total mondial en 1973 à 25,5 % en 2011 ; mais en tonnes par habitant, les États-Unis restent largement en tête avec /hab contre /hab pour la Russie, /hab pour la Chine, /hab pour l'Inde et /hab pour la moyenne mondiale.

L'association Oxfam a publié le décembre 2015, lors de la COP21, le rapport « Inégalités extrêmes et émissions de » qui fournit de nouvelles estimations des émissions selon le mode de vie et la consommation de différentes catégories de la population : la moitié la plus pauvre de la population mondiale, les 3,5 milliards de personnes les plus menacées par l’intensification catastrophique des tempêtes, des sécheresses et autres phénomènes extrêmes liée au changement climatique, n’est responsable que de 10 % des émissions de , alors que les 10 % les plus riches de la planète sont responsables d’environ la moitié des émissions de mondiales. 

Une personne faisant partie des 1 % les plus riches au monde génère en moyenne 175 fois plus de qu’une personne se situant dans les 10 % les plus pauvres ; une personne parmi les 10 % les plus riches en Inde n’émet en moyenne qu’un quart du émis par une personne de la moitié la plus pauvre de la population des États-Unis ; un Américain parmi la moitié la plus pauvre de la population de son pays génère en moyenne vingt fois plus d’émissions que son pendant indien.

Les habitants les plus pauvres de la planète, qui sont les moins responsables du changement climatique, sont de surcroit en général les plus vulnérables face à ses conséquences et les moins préparés pour l’affronter. Ainsi, en Californie, plus de 80 % des terres arables sont irriguées, tandis que moins de 1 % d'entre elles le sont au Niger, au Burkina Faso et au Tchad. De même, alors que 91 % des agriculteurs aux États-Unis ont une assurance-récolte qui couvre leurs pertes en cas de phénomène météorologique extrême, ils ne sont que 15 % en Inde, 10 % en Chine et tout au plus 1 % au Malawi.

Jean-Marc Jancovici propose dans l'outil de bilan carbone proposé par l'ADEME trois démarches pour agréger les résultats de mesure :







</doc>
<doc id="6526" url="https://fr.wikipedia.org/wiki?curid=6526" title="Biochimie">
Biochimie

La biochimie est l'étude des réactions chimiques qui se déroulent au sein des êtres vivants, et notamment dans les cellules. La complexité des processus chimiques biologiques est contrôlée à travers la signalisation cellulaire et les transferts d'énergie au cours du métabolisme. Depuis un demi-siècle, la biochimie est parvenue à rendre compte d'un nombre considérable de processus biologiques, au point que pratiquement tous les domaines de la biologie, depuis la botanique jusqu'à la médecine, sont aujourd'hui engagés dans la recherche biochimique, voire biotechnologique. L'objectif principal de la biochimie de nos jours est de comprendre, en intégrant les données obtenues au niveau moléculaire, comment les biomolécules et leurs interactions génèrent les structures et les processus biologiques observés dans les cellules, ouvrant la voie à la compréhension des organismes dans leur ensemble. Dans ce cadre, la chimie supramoléculaire s'intéresse aux complexes moléculaires tels que les organites, qui constituent un niveau d'organisation de la matière vivante intermédiaire entre les molécules et les cellules.

La biochimie s'intéresse en particulier aux structures, aux fonctions et aux interactions des macromolécules biologiques telles que les glucides, les lipides, les protéines et les acides nucléiques, qui constituent les structures cellulaires et réalisent de nombreuses fonctions biologiques. La chimie cellulaire dépend également de molécules plus petites et d'ions. Ces derniers peuvent être inorganiques, par exemple l'ion hydronium , l'hydroxyle OH ou des cations métalliques, ou bien organiques, comme les acides aminés qui constituent les protéines. Ces espèces chimiques sont essentiellement constituées d'hydrogène, de carbone, d'oxygène et d'azote ; les lipides et les acides nucléiques contiennent en plus du phosphore, tandis que les protéines contiennent du soufre et que les ions et certains cofacteurs sont constitués ou comprennent des oligoéléments tels que le fer, le cobalt, le cuivre, le zinc, le molybdène, l'iode, le brome et le sélénium.

Les résultats de la biochimie trouvent des applications dans de nombreux domaines tels que la médecine, la diététique ou encore l'agriculture ; en médecine, les biochimistes étudient les causes des maladies et les traitements susceptibles de les soigner ; les nutritionnistes utilisent les résultats de la biochimie pour concevoir des régimes alimentaires sains tandis que la compréhension des mécanismes biochimiques permet de comprendre les effets des carences alimentaires ; appliquée à l'agronomie, la biochimie permet de concevoir des engrais adaptés aux différents types de cultures et de sols ainsi que d'optimiser le rendement des cultures, le stockage des récoltes et l'élimination des parasites.

On prête à Carl Neuberg l'introduction de ce terme en 1903 à partir de racines grecques, mais ce terme circulait déjà en Europe depuis la fin du . Avec la biologie moléculaire et la biologie cellulaire, la biochimie est l'une des disciplines qui étudient le fonctionnement du vivant. Elle recouvre elle-même plusieurs branches, telles que la bioénergétique, qui étudie les transferts d'énergie chimique au sein des êtres vivants, l'enzymologie, qui étudie les enzymes et les réactions qu'elles catalysent, ou encore la biologie structurale, qui s'intéresse aux relations entre les fonctions biochimiques des molécules et leur structure tridimensionnelle.

Environ 25 éléments chimiques sur les naturels de la classification périodique sont nécessaires à différentes formes de vie. Les éléments présents à l'état de traces dans le milieu naturel ne sont généralement pas utilisés par les êtres vivants, à l'exception notable de l'iode et du sélénium, tandis que certains éléments abondants tels que l'aluminium ou le titane ne sont pas nécessaires à la vie. La plupart des organismes utilisent les mêmes éléments chimiques, mais il existe quelques différences chez les plantes et les animaux. Par exemple, certaines algues océaniques utilisent le brome tandis que les plantes terrestres et les animaux ne semblent pas en avoir besoin. Tous les animaux ont besoin de sodium, mais certaines plantes s'en dispensent. En revanche, les plantes ont besoin de bore et de silicium pour se développer, tandis que les animaux ne semblent pas en faire usage.

La masse du corps humain est constituée approximativement à 65 % d'oxygène et à 98,5 % de seulement six éléments chimiques : outre l'oxygène, ce sont environ 18 % de carbone, 10 % d'hydrogène, 3 % d'azote, 1,4 % de calcium et 1,1 % de phosphore. On compte également des quantités plus faibles de potassium, soufre, sodium, chlore, magnésium, fer, fluor, zinc, silicium et d'une douzaine d'autres éléments, qui ne sont pas tous nécessaires à la vie.

Les quatre classes principales de molécules biochimiques, également appelées "biomolécules", sont les glucides, les lipides, les protéines et les acides nucléiques. De nombreuses macromolécules biochimiques sont des polymères, constitués de l'assemblage d'unités plus petites appelées "monomères" ; ces monomères sont de petites molécules qu'il est possible de libérer du biopolymère par hydrolyse. Plusieurs de ces biomolécules sont susceptibles de former des complexes moléculaires de grande taille qui assurent souvent des fonctions biochimiques indispensables à la vie de la cellule.

Les glucides sont constitués de monomères appelés oses. Le glucose, le fructose et le galactose sont des oses. Ces derniers sont classés en fonction du nombre de leurs atomes de carbone : trioses en C, tétroses en C, pentoses en C, hexoses en C, heptoses en C.

D'un point de vue chimique, on distingue d'une part les aldoses, qui sont composés d'une chaîne d'alcools secondaires ayant à une extrémité un groupe aldéhyde, et d'autre part les cétoses, qui possèdent une fonction cétone dans leur chaîne carbonée, les autres atomes de carbone étant porteurs d'une fonction alcool primaire ou secondaire selon la position.

Les oses jouent un rôle majeur dans le métabolisme énergétique de la cellule, mais aussi dans la biosynthèse des acides nucléiques, des cérébrosides et des glycoprotéines. Ils peuvent également intervenir dans certains mécanismes de détoxication, par exemple à travers la glycuroconjugaison.

Deux oses peuvent s'unir à travers une liaison osidique pour former un diholoside : le saccharose est un diholoside constitué d'un résidu de glucose d'un résidu de fructose unis par une liaison osidique (1→2) ; le lactose en est un autre constitué d'un résidu de lactose et d'un résidu de glucose unis par une liaison osidique β(1→4). Au-delà de deux résidus, on parle d'oligosaccharides jusqu'à dix résidus et de polysaccharides au-delà : sont des biopolymères constitués plusieurs résidus osidiques d'oses qui interviennent dans le stockage de l'énergie (amidon, glycogène) et dans la rigidité de certains organismes (cellulose, chitine).

Chez les bactéries, les glucides constituent selon les cas l'essentiel du peptidoglycane ou du lipopolysaccharide de la paroi bactérienne. Ils sont responsables des réactions immunitaires de l'organisme exposé à ces bactéries. Ce sont également des déterminants antigéniques, ou "épitopes", importants à la surface des cellules d'eucaryotes. Ils déterminent les groupes sanguins et sont une part importante du complexe majeur d'histocompatibilité, ou CMH.

Quelques exemples de glucides :







Formules cycliques du glucose, fructose et saccharose

Les lipides, du grec « "lipos" » (« graisse »), constituent une classe assez hétérogène de molécules. Sont regroupées sous cette dénomination les molécules ayant un caractère hydrophobe marqué, c'est-à-dire très peu solubles dans l'eau mais solubles dans la plupart des solvants organiques, comme le chloroforme, par exemple. Nous trouvons aussi des lipides dans la cire de bougie, les graisses animales, l'huile d'olive et pratiquement tous les corps gras. La biochimie a complété cette définition en montrant que les lipides possédaient des voies de synthèse communes. Cependant, il n'existe pas encore de définition unique d'un lipide reconnue par l'ensemble de la communauté scientifique. Ceci tient probablement au fait que les lipides forment un ensemble de molécules aux structures et aux fonctions extrêmement variées dans le monde du vivant.
D'un point de vue métabolique, les lipides constituent des réserves énergétiques. Les sucres sont par exemple transformés en lipides et stockés dans les cellules adipeuses en cas de consommation supérieure à l'utilisation.

Les lipides, en particulier les phospholipides, constituent l'élément majeur des membranes cellulaires. Ils définissent une séparation entre le milieu intracellulaire et le milieu extracellulaire. Leur caractère hydrophobe rend impossible le passage de molécules polaires ou chargées, comme l'eau et les ions, car ils forment des groupes très compacts issus de liaisons covalentes faibles appelées interaction hydrophobe. Seules voies de passage possible : les protéines membranaires où, par exemple, les ions entrent et sortent de la cellule par le biais de canaux ioniques.

Plusieurs hormones sont des lipides, en général dérivées du cholestérol (progestérone, testostérone, etc.), ce qui permet d'agir comme filtre aux entrées des cellules. Les vitamines liposolubles peuvent aussi être classées parmi les lipides.

Contrairement aux acides nucléiques ou aux protéines, les lipides ne sont pas des macromolécules constituées d'une succession d'unités de base.

Les lipides peuvent être classés selon la structure de leur squelette carboné (atomes de carbone chaînés, cycliques, présence d'insaturations, etc.) : 
Les phospholipides : lipides qui constituent la membrane cellulaire permettant le passage de certains minéraux ;






Pour des raisons pratiques et historiques, acylglycérols et phosphoglycérides sont souvent considérés comme deux catégories différentes, de même que phosphoglycérides et phosphosphingolipides peuvent être regroupés sous l'appellation de phospholipides.

Quelques exemples de lipides

Les protéines (du grec "prôtos", premier) sont des polymères composés d'une combinaison de quelque 20 acides aminés. La plupart des protéines sont formées de l'union de plus de aminés (résidus) reliés entre eux par des liaisons peptidiques. Pour un nombre moins important de résidus on parle de peptides (< ) et de polypeptides (≥ ).

Les acides aminés (« amin » du grec "ammôniakos", ammoniac) sont des composés organiques azotés qui possèdent une formule générale du type :

L'atome de carbone central "C"α (carbone alpha) est relié à un groupe amine (), à un groupe carboxyle acide (-COOH) et à une chaîne latérale R variable d'un acide aminé à un autre. Les chaînes latérales (R) peuvent avoir des propriétés différentes, certaines sont hydrophiles, d'autres hydrophobes. Certaines, en solution aqueuse, s'ionisent positivement (basiques) et d'autres négativement (acides) ou restent neutres. Les mammifères possèdent les enzymes nécessaires pour la synthèse de l'alanine, l'asparagine, l'aspartate, la cystéine, le glutamate, la glutamine, la glycine, la proline, la sérine, et la tyrosine. Quant à l'arginine et l'histidine, ils sont produits mais en quantité insuffisante surtout pour les jeunes individus. En revanche, l'isoleucine, la leucine, la lysine, la méthionine, la phénylalanine, la thréonine, le tryptophane, et la valine ne peuvent pas être produits par notre organisme. Pour éviter tout carence, ils doivent être apportés régulièrement par l'alimentation dans les bonnes proportions : ce sont les acides aminés essentiels.

Les acides aminés peuvent se lier les uns aux autres par une liaison peptidique au cours de la biosynthèse des protéines dans les ribosomes. La liaison peptidique s'établit entre le carboxyle (COOH) d'un acide aminé et le groupe amine () de l'autre :

la réaction produit un dipeptide :

Dans la cellule, cette réaction est catalysée par la peptidyltransférase, elle nécessite l'hydrolyse d'ATP (source d'énergie) et la présence d'ions magnésium. Pour chaque liaison formée, une molécule d'eau est formée.

La séquence des acides aminés d'une protéine (l'arrangement et l'ordre des résidus) constitue la structure primaire. Par exemple, pour construire un peptide de à l'aide de la collection de aminés, on dispose de . En solution aqueuse, les radicaux possèdent des propriétés chimiques différentes. Certains radicaux peuvent former des liaisons chimiques plus ou moins fortes avec d'autres radicaux de la même chaîne peptidique. Certains se repoussent et d'autres se rapprochent et forment des liens chimiques. La chaîne d'acides aminés aura donc tendance à se replier sur elle-même pour adopter une structure tridimensionnelle précise. Et cette dernière dépend avant tout de la séquence des acides aminés formant la chaîne. 
En effet, quatre grands types d'interactions interviennent dans le repliement de la chaîne peptidique :


Ces quatre premiers types d'interactions sont considérés comme étant faibles (forts lorsque nombreux cependant).


Ainsi certaines parties de la chaîne peptidique adoptent une structure régulière appelée structure secondaire. On en reconnaît, selon les angles de torsion des liaisons, trois grands types :




La forme finale de la chaîne peptidique, c’est-à-dire la structure tridimensionnelle qu'adopte la chaîne d'acides aminés, constitue la structure tertiaire de la protéine (voir la figure de la myoglobine en 3D).

Certaines protéines, plus complexes, résultent de l'assemblage des différentes chaînes (monomères) ce qui constitue la structure quaternaire de la protéine. Par exemple, l'hémoglobine est formée de l'association de quatre chaînes peptidiques.

La structure de la protéine peut être dénaturée par plusieurs facteurs, notamment la température, les pH extrêmes et l'augmentation de la force ionique dans le milieu ou par des agents chimiques dénaturants (2-mercaptoéthanol). La dénaturation de la d'une protéine a généralement pour conséquence la perte de sa fonction. On parle de « relation structure-fonction ».

Les protéines assurent plusieurs fonctions au sein des cellules et de l'organisme, qui sont à l'essence même de la vie. En voici une liste non exhaustive avec quelques exemples :


Exemples de quelques protéines

Pour un total d’environ à gènes (génome), on estime à un million le nombre de protéines différentes qui peuvent être produites dans les cellules humaines (protéome). Le nombre de protéines produites par le cerveau humain, dont le rôle est essentiel pour son fonctionnement, est estimé à environ .

Les acides nucléiques ont été isolés initialement des noyaux des cellules eucaryotes (du latin "nucleus", noyau). Ce sont des macromolécules comportant des sous-unités appelées nucléotides. On peut en distinguer deux grands types : les acides désoxyribonucléiques (ADN) et les acides ribonucléiques (ARN). L'ADN est le support universel de l'information génétique (sauf pour certains virus). Grâce à deux fonctions catalytiques, cette molécule assure la "transmission" et l"'expression" de l'information qu'elle contient :



Le nucléotide, unité de base des acides nucléiques, comporte trois composants : de l'acide phosphorique, un pentose et une base nucléique :




Dans l'ADN bicaténaire, les bases nucléiques des deux brins s'apparient suivant la règle de complémentarité : A apparié avec T, C apparié avec G. Cet appariement est maintenu grâce à des liaisons hydrogène et peut donc être affecté par la chaleur (dénaturation thermique). Par convention, la séquence d'un acide nucléique est orientée dans le sens de l’extrémité 5' (comportant un groupe phosphate) vers l’extrémité 3' qui possède un OH libre. Ainsi, dans l'ADN bicaténaire (double brin), les deux brins sont disposés dans deux sens opposés. Les extrémités 5' et 3' de l'un des brins correspondent aux extrémités 3' et 5' du brin parallèle complémentaire (anti-parallèles). Dans l’espace, les deux chaînes présentent une configuration hélicoïdale. Elles s’enroulent autour d’un axe imaginaire pour constituer une double hélice à rotation droite (dans les formes "A" et "B" de l’ADN) ou plus exceptionnellement à rotation gauche (dans la forme "Z" de l’ADN).

Classiquement, on considère que le gène est une région d'un brin d'ADN dont la séquence code l'information nécessaire à la synthèse d'une protéine. Trois types d'ADN différents constituent le génome (l'ensemble des gènes d'un individu ou d'une espèce) :




Le génome humain comprend environ 3 milliards de paires de bases représentant près de (en fait, dans les estimations récentes, c'est entre et ). Toutefois, il ne semble pas y avoir de relation systématique entre le nombre de paires de bases par génome et le degré de complexité d'un organisme. Ainsi, certaines plantes et organismes amphibiens possèdent un génome comptant plus de 100 milliards de paires de nucléotides, soit 30 fois plus qu'un génome humain. En effet, le génome des cellules eucaryotes semble contenir un large excès d'ADN. Chez les mammifères, moins de 10 % du génome serait utile à l'expression en protéines ou à la régulation de cette expression.

La taille des gènes peut varier de quelques centaines à plusieurs dizaines de milliers de nucléotides. Cependant même les gènes les plus longs n'utilisent qu'une faible portion de leur séquence pour coder l'information nécessaire à l'expression en protéines. Ces régions codantes sont appelées exons et les séquences non codantes introns. D'une manière générale, plus l'organisme est complexe, plus la quantité et la taille des introns est importante. Ainsi la présence d'introns sur l'ADN d'organismes procaryotes est extrêmement rare. Certaines régions de l'ADN sont impliquées dans la régulation de l'expression des gènes. Ces séquences de régulation sont généralement localisées en amont (du côté 5') ou en aval (côté 3') d'un gène et plus rarement à l'intérieur d'introns ou d'exons.

Les vitamines (du latin "vita", vie) sont des composés organiques essentiels à la vie, agissant à faibles quantités, pour le développement, l'entretien et le fonctionnement de l'organisme. Nos cellules sont incapables de les synthétiser et elles doivent être apportées par l'alimentation sous peine d'avitaminose ; l'excès de vitamines est la survitaminose. La vitamine B (thiamine) est la première vitamine à avoir été découverte par le japonais Umetaro Suzuki cherchant à soigner le béribéri (une maladie due au déficit en vitamine B, caractérisée par des atteintes musculaires et neurologiques). Elle fut isolée par Kazimierz Funk (biochimiste américain d'origine polonaise) en 1912. Aujourd'hui, on connaît 13 vitamines différentes pour l'homme. C'est un ensemble hétérogène du point de vue chimique et physiologique (mode d'action).

Les vitamines se divisent en deux grandes catégories : les vitamines hydrosolubles (groupes B et C) et les vitamines liposolubles (les groupes A, D, E, et K). Les vitamines hydrosolubles ne peuvent pas franchir la membrane cellulaire et elle doivent se fixer à un récepteur pour pénétrer la cellule. Elles sont facilement éliminées par les reins et la sueur, l'alimentation doit les fournir quotidiennement. Les vitamines liposolubles peuvent facilement traverser la membrane cellulaire. Leurs récepteurs se trouvent dans la cellule, soit dans le cytosol, soit dans le noyau. Elles sont stockées dans le tissu adipeux et le foie (d'où le risque de surdosage, surtout pour les vitamines A et D). Certaines vitamines sont des cofacteurs nécessaires à l'activité d'enzymes (vitamines du groupe B), d'autres constituent une réserve de pouvoir réducteur (vitamine C, E). Les fonctions des autres vitamines restent à élucider.


Pour mener à bien leurs études, les biochimistes font appel à des techniques et des connaissances issues de nombreuses disciplines scientifiques autres que la biologie, par exemple :

L'idée que l'activité de la « matière vivante » provienne de réactions chimiques est relativement ancienne (Réaumur, Spallanzani, etc.). La synthèse de l'urée, réalisée en 1828 par le chimiste allemand Friedrich Wöhler, en sera une des confirmations les plus décisives réalisées au . Avant cette date, on considérait que la substance présente dans les organismes présentait des particularités propres au vivant (théorie du vitalisme ou des humeurs héritée des Grecs anciens Aristote, Gallien ou Hippocrate).

Un autre Allemand, Justus von Liebig sera le promoteur d'une nouvelle science, la biochimie, qui sera un domaine d'illustration pour plusieurs de ses compatriotes jusqu'à la seconde guerre mondiale. Parmi les plus célèbres on retiendra Hermann Emil Fischer (la célèbre projection de Fischer des glucides), Eduard Buchner (biochimie de la fermentation) et Richard Willstätter (mécanisme des réactions enzymatiques).

Dès lors l'exploration de la cellule connaît un nouvel essor mais on s'intéressera plus particulièrement à ses constituants chimiques et à la façon dont ils réagissent entre eux afin de réaliser un métabolisme au niveau cellulaire. Après les travaux de Louis Pasteur, la recherche va se porter dans les substances intervenant dans les fermentations et les digestions (les ferments solubles). Antoine Béchamp les nommera en 1864 « zymases » mais on préfèrera utiliser le nom d'enzymes introduit dès 1878 par Wilhelm Kühne.

Les autres composants attirant l'attention sont des molécules « albuminoïdes » nommées protéines depuis 1838. Celles-ci sont considérées comme des agrégats de petites molécules à l'origine de l'état colloïdal du hyaloplasme de la cellule. Selon Friedrich Engels, elles sont la manifestation même de la vie ("Dialectique de la nature", 1835) ; cela suscite dès lors une attitude vitaliste qui en France sera défendue par Émile Duclaux. Marcellin Berthelot permet une avancée majeure en décrivant le fonctionnement de l'invertase : dès 1860, il décrit la façon dont l'hydrolyse de liaisons glucidiques est catalysée par ce glucose hydrolase. Dès 1920, une autre interprétation s'impose avec la mise en évidence de la nature moléculaire des protéines par Hermann Staudinger. Ce nouveau statut est accompagné de caractéristiques structurales qui conduisent à de nouvelles interprétations fonctionnelles, certaines protéines pouvant être des enzymes, comme Victor Henri l'avait pressenti dès 1903.

Otto Warburg met en place la chimie cellulaire et met le microrespiromètre à la disposition des chercheurs. Cet appareil va aider le Hongrois Albert Szent-Györgyi puis l'Allemand Hans Adolf Krebs à élucider le mécanisme de la respiration cellulaire. Il est démontré alors que le gaz carbonique produit à cette occasion est le résultat d'une série de réactions biochimiques effectuées à l'aide d'enzymes spécifiques, le cycle de Krebs. On établit aussi que toutes les cellules tirent leur énergie d'une même molécule, l'adénosine triphosphate ou ATP, découverte en 1929 par Karl Lohmann.

Au début des années 1940, Albert Claude montre que la synthèse de l'ATP se déroule au niveau de la membrane interne des mitochondries. Dans le même temps, le Britannique Peter Mitchell explique le mécanisme de cette réaction, qui s'accompagne de formation d'eau.

L'étude des thylakoïdes dans les chloroplastes des végétaux chlorophylliens permet de comprendre progressivement le mécanisme de la photosynthèse. En 1932, Robert Emerson reconnaît une phase lumineuse et une phase obscure et en 1937 Archibald Vivian Hill démontre que la production d'oxygène caractéristique de la photosynthèse résulte de la photolyse (décomposition chimique par la lumière) de l'eau. Enfin à partir de 1947, Melvin Calvin décrit la fabrication des substances carbonées à partir du dioxyde de carbone absorbé, c'est le cycle de Calvin.

En 1951, Erwin Chargaff montre que la molécule d'ADN, connue depuis 1869, est essentiellement présente au niveau des chromosomes. On remarque aussi qu'il y a autant d'adénine que de thymine, de guanine que de cytosine. Le jeune James Dewey Watson et Francis Harry Compton Crick vont publier la structure en double hélice de l'ADN dans la revue "Nature" le 25 avril 1953. Ils se basent sur les images en diffraction des rayons X obtenues par Maurice Wilkins et Rosalind Elsie Franklin.

Toutes ces découvertes sont le prélude à une meilleure compréhension moléculaire de la vie et à de nombreuses autres avancées médicales et biologiques.

C'est en 1929 que Theodor Svedberg a l'idée de soumettre le matériel cellulaire à une centrifugation poussée (ultracentrifugation) afin d'isoler les différents constituants des cellules. En 1906, le botaniste Mikhaïl Tswett met au point la chromatographie, technique permettant de séparer les biomolécules. La technique d'électrophorèse a été développée en 1930 par Arne Wilhelm Tiselius, elle permet la séparation des biomolécules chargées sous l'effet d'un champ électrique. Le biochimiste britannique Frederick Sanger développa en 1955 une nouvelle méthode pour analyser la structure moléculaire des protéines (séquence d'acides aminés) et montra qu'une molécule d'insuline contenait deux chaînes peptidiques, reliées ensemble par deux ponts disulfure.




</doc>
<doc id="6528" url="https://fr.wikipedia.org/wiki?curid=6528" title="Cétène (groupe)">
Cétène (groupe)

Un cétène est un composé organique contenant la séquence :
. Ils ont été découverts vers 1905, par Hermann Staudinger alors âgé de 24 ans pendant un post-doctorat. Le cétène, CH=C=O, est le composé parent de ce groupe.

À partir d'un chlorure d'acyle portant un hydrogène en α.

Une base, généralement la triéthylamine, va arracher le proton acide en α du groupement carbonyle induisant la formation de la double liaison carbone-carbone et le départ de l'anion chlorure.

La cycloaddition [2+2] d'un cétène sur une cétone ou un aldéhyde va conduire à la formation de bêta-lactones.
La cycloaddition [2+2] d'un cétène sur un alcène conduit à la formation d'oxétanes alpha-méthyléniques. C'est la réaction de Staudinger. En général, la LUMO du cétène réagit avec l'HOMO de l'alcène.

Le même type de réaction, entre une cétène et une imine va conduite à la formation de bêta-lactames.



</doc>
<doc id="6529" url="https://fr.wikipedia.org/wiki?curid=6529" title="Vol à vue">
Vol à vue

Le vol à vue est la façon la plus simple de voler, la plus libre aussi, où il s'agit simplement de "voir et d'éviter". Ce mode de pilotage n'est autorisé que sous certaines conditions de visibilité, de base des nuages et de couverture nuageuse et il est réglementé par le VFR ("visual flight rules"). C'est la technique qui nécessite le moins d'instruments et la première technique à avoir été utilisée au début de l'aéronautique. 

Il s'oppose aux règles de vol aux instruments, ou IFR ("instrument flight rules"), qui ne nécessite pas de visibilité extérieure. Les règles du VFR peuvent varier si certaines conditions sont réunies :
VFR au-dessus des nuages, VFR spécial et VFR de nuit.

Pour le pilotage en vol à vue, le pilote doit maintenir l'assiette de l'avion en se basant sur la ligne d'horizon. Pour les manœuvres d'approche, il détermine la trajectoire à suivre en voyant la position de la piste par rapport à son avion. Pour la navigation, le navigateur (qui, en aviation générale, est souvent le pilote) détermine la position géographique de son appareil en se basant sur des repères au sol : ville, route, fleuve, voie de chemin de fer. 

Les "règles du vol à vue" (soit visual flight rules ou VFR en anglais) correspondent donc à une définition précise : c'est un vol qui respecte certaines conditions de visibilité et de distance horizontale et verticale par rapport aux nuages (on parle de conditions VMC — "visual meteorological conditions"). De plus les règles dépendent de la classification des espaces aériens. Certains espaces aériens sont libres au VFR, d'autres sont interdits au VFR (survol de Paris par exemple), d'autres autorisés avec accord préalable du contrôle aérien. 

Le pilote désirant aller d’un point A à un point B en suivant ces règles choisit sur sa route des points de repère qu’il survole successivement à quelques minutes d’intervalle. Une fois en vol, il prend le cap devant l’amener à son premier point de repère, cap qu’il corrige en fonction du vent. Une fois à proximité du point de repère, le pilote constate la dérive subie et ajuste l’évaluation du vent pour corriger son cap ainsi que l’heure d’arrivée estimée sur son point de repère suivant.

Le vol à vue au-dessus des nuages est possible de jour ("VFR on top"), à condition que l'appareil soit doté de moyens de navigation électromagnétiques VOR ou électroniques GPS.

Pour comprendre ces règles, voici quelques concepts de mesure propres à l'aviation :


Ces règles permettent de garantir la sécurité des vols en permettant à tous les acteurs, pilotes comme contrôleurs, de communiquer sans ambiguïté sur les caractéristiques de chaque trajectoire de vol.

Dans les espaces aériens non contrôlés (de classe G), le vol à vue doit respecter les conditions suivantes :

Dans les espaces aériens contrôlés (de classe C, D ou E), le vol à vue doit s'effectuer à des nuages horizontalement et verticalement, la visibilité doit être supérieure à jusqu'au FL 100 et supérieure à pour un vol au FL100 ou au-dessus.

De nuit, pour un vol local, il faut :

De nuit, pour un voyage, il faut

Des dérogations peuvent être accordées par les organismes de la circulation aérienne. On parle alors de "VFR spécial".

Il n'existe que quatre classes d'espaces aériens autorisant le VFR : G, E, D et C (jusqu'au FL 195 sans autorisation PPR).

L'espace G, part du sol et monte jusqu'à 600 m (2000 ft) AGL (Above Ground Level - par rapport au sol). Les règles de vol à vue y sont les suivantes :

L'espace E va de 600 m AGL jusqu'au FL 100 en plaine/Jura, FL 150 dans les Alpes (FL 130 si activité militaire).

Au delà c'est l'espace C.

L'espace D se situe principalement dans les CTR et quelques TMA.

Règles VFR pour les espace C, D, E 
En espace E, il peut y avoir des appareils en vol IFR. Transpondeur obligatoire en espace E à partir de 7000 ft.



</doc>
<doc id="6532" url="https://fr.wikipedia.org/wiki?curid=6532" title="Hello world">
Hello world

Certains des programmes imprimant ces mots sont étonnamment complexes, particulièrement dans un contexte d’interface graphique. D’autres sont très simples, particulièrement ceux qui utilisent un interpréteur de ligne de commande pour afficher le résultat. Dans plusieurs systèmes embarqués, le texte peut être envoyé sur une ou deux lignes d’un afficheur LCD (ou dans d’autres systèmes, une simple DEL peut se substituer à un ").

Alors que les petits programmes de test existaient depuis le début de la programmation, la tradition d’utiliser ' comme message de test a été initiée par le livre de Brian Kernighan et Dennis Ritchie. Le premier exemple de ce livre affiche ' (sans majuscule ni point final, mais avec une virgule et un retour à la ligne final). Le premier " dont Ritchie et Kernighan se souviennent provient d’un manuel d’apprentissage du langage B écrit par Kernighan. Un ordinateur, ou plutôt un « Programmable Data Processor », Le PDP-11 16 bits, l’un des ordinateurs les plus populaires de l’époque, contribue à la diffusion de l’expression, les usagers se procurant l'ouvrage "The C Programming Language" pour apprendre à utiliser la machine. Dans une interview à l'édition indienne du magazine Forbes, Brian Kernighan explique que cette phrase provient d'un dessin animé qu'il avait vu où un poussin sortait de son œuf en disant .

Au , les programmes affichent plus souvent "" comme une phrase, avec majuscule et point d’exclamation final.

La mise en œuvre GNU est sensiblement plus complexe que l’algorithme de base décrit par Kernighan et Ritchie. Elle fait ici figure d’exemple canonique pour l’empaquetage d’un programme GNU, et va jusqu’à servir de modèle aux normes de codage GNU ainsi qu’aux pratiques en vigueur au sein du projet.

Il s’agit du fameux programme qui affiche "" : cinq lignes de C, empaquetées comme s’il s’agissait d’une distribution GNU. Par conséquent, il contient toute l’information relative à Texinfo et à Configure. Et tout le reste des rouages d’ingénierie logicielle que le projet GNU a inventés pour permettre le portage aisé vers les autres environnements. C’est un travail extrêmement important, et ça n’affecte pas seulement les logiciels de Stallman, mais aussi tous les autres logiciels du projet GNU.

Ce programme écrit en C est maintenu depuis 1992. La dernière version a été réalisée en novembre 2014.


</doc>
<doc id="6533" url="https://fr.wikipedia.org/wiki?curid=6533" title="Vol aux instruments">
Vol aux instruments

Un pilote effectue un vol selon les règles de vol aux instruments (soit en anglais, ' ou IFR) lorsqu'il respecte des principes et des méthodes lui permettant, à l'aide d'indications données par des instruments embarqués et des directives reçues du contrôle aérien, de :

Notamment, lorsqu'il n'est pas possible de maintenir les conditions VMC ("visual meteorological conditions") permettant de voler à vue — dites aussi « conditions IMC » ("instrumental meteorological conditions") — le vol doit s'effectuer selon les règles de vol aux instruments.

Très tôt après les premiers vols, les pilotes ont cherché à repousser leurs limites par mauvaises conditions météorologiques notamment pour accélérer le développement des vols commerciaux, d'abord de courrier, puis de fret et de passagers. Déjà en 1910, on embarque une radio à des fins militaires, pour faire de l'avion un véritable outil d'observation du champ de bataille. L'année suivante, on s'essaye aux vols de nuit.

Plus tard, en 1920, la marine américaine déroule un câble sous la mer qui trace une route électromagnétique de près de 100 milles nautiques (185 km). Un hydravion rejoint ainsi un navire au large des côtes et retourne à sa base en recevant un signal à son bord.

En 1923, les Américains installent des balises lumineuses sur plusieurs centaines de kilomètres pour permettre de guider les pilotes la nuit. De la même manière, en France, des « phares » sont installés dans quelques lieux stratégiques pour guider les avions des premières lignes aériennes dont celles de Latécoère. C'est pourtant Maurice Noguès qui effectue le premier vol de nuit entre Strasbourg et Le Bourget le 2 septembre 1923 pour le compte de la ligne Franco-Roumaine.

La même année, en France, des avions sont équipés des premiers cadres goniométriques qui permettent de détecter la direction de provenance d'un signal radio émis par une station au sol. Le radiocompas est né. L'année suivante, au Bourget, la première tour de contrôle est équipée elle aussi d'un goniomètre et peut guider les avions jusqu'au terrain en leur indiquant la route à suivre. La première radiobalise NDB à usage aéronautique est installée en 1925 à Orly.

En août 1928, l'Armée de l'air débute les premières formations de pilotes au vol sans visibilité. Le futur général de corps aérien Gaston Venot y contribue.

En 1929, on démontre aux États-Unis que le "vol aux instruments" est possible grâce à un horizon artificiel, un altimètre de précision et une aide au sol. Sous l'impulsion des frères Farman et du directeur de l'aéroport de Toussus-le-Noble, Lucien Rougerie, une première école de pilotage sans visibilité est créée, avec les pilotes Lucien Coupet, Marcel Lalouette et Joanny Burtin comme premiers instructeurs, mais il faut attendre 1930 pour que Gaston Génin effectue le premier atterrissage en conditions réelles aux instruments. Alors qu'il passe le terrain de Dortmund dans la brume, l'opérateur au sol lui transmet le signal ZZ. Il tourne alors à gauche de 12°, s'éloigne pendant 3 minutes guidé par l'opérateur, fait demi-tour par la droite en une minute, revient vers la piste, sort de la couche à quelques mètres du sol et se pose.

Le premier ILS rudimentaire est installé à l'aéroport de New York en 1932 et au Bourget en 1933. À partir de 1935 en France, il faut être capable d'effectuer un atterrissage "sans visibilité" pour devenir pilote de ligne.

Pendant la Seconde Guerre mondiale, la technique du radar est largement améliorée. On met au point les bases du système Loran où un navigateur à bord de l'avion mesure la différence de propagation de 2 signaux émis par des stations au sol pour en déduire sa position au-dessus de l'Europe.

En 1946, Orly est équipé pour l'atterrissage sans visibilité. C'est dans les années 1950 que l'on installe les premiers VOR aux États-Unis, en France il faudra attendre 1953 pour voir le premier d'entre eux à Orgeval puis Coulommiers, Bray et Orly en 1957.

En 1955, le premier radar météo fait son apparition à bord d'un Douglas DC-3.

En 1961, l'IFR devient obligatoire pour tous les vols de transport aérien public en Europe.

Intégrant les progrès dans les pilotes automatiques, la Caravelle est le premier avion à se poser sans l'aide du pilote (qui doit quand même freiner puis rouler jusqu'au parking) en 1962. À Londres en 1965, le Trident est le premier appareil à atterrir en conditions réelles, avec des passagers, par des conditions dites de "catégorie II" soit 200 pieds (60 m) de plafond et 400 m de visibilité. En 1969, encore la Caravelle, un avion de ligne est certifié pour l'atterrissage avec des passagers en "catégorie IIIA" soit 100 pieds (30 m) de plafond et 200 m de visibilité (à 140 nœuds, vitesse d'atterrissage typique d'un avion de ligne, c'est la distance franchie en 3 s). C'est un Airbus A300 qui réalise le premier atterrissage en "catégorie IIIB" en 1976 avec 125 m de visibilité.

Les systèmes d'évitement de collision avec d'autres aéronefs (TCAS) ou le relief (GPWS) font leur apparition dans les années 1990, décennie qui popularise aussi le GPS inventé 15 ans plus tôt pour des applications militaires. Avec l'ADS-B, les avions envoient maintenant leur position au contrôle aérien sans que celui-ci n'ait besoin d'un radar.

Un nouveau système, l'EGPWS permet maintenant aux pilotes de visualiser une représentation du sol en trois dimensions sur un écran, d'après une base de données du relief. On retrouve ce type d'écran sur des avions de ligne comme sur des avions d'affaires ou de tourisme.



</doc>
<doc id="6534" url="https://fr.wikipedia.org/wiki?curid=6534" title="Atmosphère">
Atmosphère

L’atmosphère est l'enveloppe gazeuse entourant certains astres, comme l'atmosphère d'une planète ou d'une planète naine ou encore l'atmosphère stellaire d'une étoile. Dans le cas d'une planète « gazeuse », l'atmosphère désigne la partie de la planète dont la matière est en phase gazeuse. On en rencontre des atmosphères sur :







</doc>
<doc id="6536" url="https://fr.wikipedia.org/wiki?curid=6536" title="Liste des municipalités du Chiapas">
Liste des municipalités du Chiapas

L'état mexicain de Chiapas comprend 118 municipalités et sa capitale est Tuxtla Gutiérrez.
Le code INEGI complet de la municipalité comprend le code de l'État - 07 - suivi du code de la municipalité. Exemple : Amatán = 07005. Chaque localité de la municipalié a aussi son code INEGI. Ainsi pour le chef-lieu de la municipalité d'Amatán, Amatán = 070050001.




</doc>
<doc id="6538" url="https://fr.wikipedia.org/wiki?curid=6538" title="Vim">
Vim

Vim est un éditeur de texte, c’est-à-dire un logiciel permettant la manipulation de fichiers texte. Il est directement inspiré de vi (un éditeur très répandu sur les systèmes d’exploitation de type UNIX), dont il est le clone le plus populaire. Son nom signifie d’ailleurs , que l’on peut traduire par « VI aMélioré ».

Vim est un éditeur de texte extrêmement personnalisable, que ce soit par l'ajout d'extensions, ou par la modification de son fichier de configuration, écrits dans son propre langage d'extension, le Vim script.

Malgré de nombreuses fonctionnalités, il conserve un temps de démarrage court (même agrémenté d'extensions) et reste ainsi adapté pour des modifications simples et ponctuelles (de fichiers de configuration par exemple).

Vim se différencie de la plupart des autres éditeurs par son fonctionnement modal, hérité de vi. En effet, il possède trois modes: le mode normal (dans lequel vous êtes lorsque Vim démarre), le mode commande, et le mode édition.

Vim est un logiciel libre. Son code source a été publié pour la première fois en 1991 par Bram Moolenaar, son principal développeur. Depuis, ce dernier a continué de l’améliorer, avec l’aide de nombreux contributeurs.

Dans un terminal informatique, Vim fonctionne en mode texte plein écran, comme l’éditeur de texte vi.
Il propose énormément de fonctions qui sont, pour la grande majorité, accessibles au clavier. Ceci s’explique par des raisons historiques : les premières versions de vi datent du milieu des années 1970, époque à laquelle les terminaux les plus répandus ne disposaient que d’un clavier et d’une liaison série très lente avec l’ordinateur central.

Vim peut, si l'utilisateur le souhaite , être compatible à 100 % avec vi. De très nombreuses améliorations et fonctionnalités ont été ajoutées à Vim, et ne sont pas présentes dans Vi, comme la gestion de la souris, le repliement, la coloration syntaxique...
De plus, depuis la version 4.0, Vim dispose d’une interface utilisateur graphique moderne, "GVim".

Il existe deux autres versions de Vim. Une simplifiée pour les utilisateurs débutants, "eVim" (le "e" pour "easy"), et une autre compacte, "Vim-Tiny". Cette dernière est une version minimale de vim compilée sans interface graphique et avec seulement un petit sous-ensemble de fonctions n’est que très légèrement plus grosse que nvi. Certaines fonctionnalités, telles que la coloration syntaxique, ne sont pas incluses dans Vim-Tiny.

Vim est utilisable sur de nombreux systèmes d’exploitation et est disponible sur pratiquement toutes les distributions GNU/Linux. En général, l’exécutable "vi" sur Linux correspond à un lien ou à un alias appelant Vim. Quand Vim est déjà lancé, il est possible de connaître sa version en utilisant la commande :version du mode Normal.

Au début des années 1970, l’éditeur standard d’Unix était ed de Ken Thompson. ed ne permettait qu’une édition ligne par ligne, et n’affichait pas l’intégralité du texte comme cela est aujourd’hui habituel. Il fallait pour cela demander explicitement l’affichage du fichier. Pour pouvoir travailler malgré les limites imposées par l’édition ligne par ligne, il était nécessaire d’utiliser un mode dédié à l’insertion et un mode dédié au passage de commandes.

Les commandes de ed sont principalement composées d’une lettre, précédée d’une indication donnant les lignes sur lesquelles agir. En 1976, quand Bill Joy développa vi, le progrès
fut considérable, puisqu’il considérait l’écran dans son ensemble et qu’il reportait les modifications apportées au texte édité dans la représentation qu’il en donnait. vi a hérité de ed le concept des modes et les instructions obscures. On retrouve aujourd’hui encore ces particularités dans les éditeurs compatibles avec vi, en particulier Vim.

À la fin des années 1980, quand Bram Moolenaar s’est acheté un ordinateur de marque Commodore Amiga, il souhaitait avoir la possibilité d’utiliser l’éditeur qu’il connaissait et auquel il était habitué sur UNIX. Mais aucun vi n’existait à cette époque sur Amiga, il a développé en 1988 la version 1.0 de Vim, sur la base de « Stevie » (STvi), un clone de vi écrit pour Atari. À cette époque, Vim signifiait , car son objectif principal consistait d’abord à reproduire les fonctionnalités de vi. En 1991, Vim 1.14 fut distribué pour la première fois sur la disquette , une collection de logiciels libres pour Amiga. En 1992, la version 1.22 de Vim est portée sur UNIX et MS-DOS. C’est à ce moment que l’acronyme Vim a changé de signification pour devenir « VI iMproved (aMélioré en français) ».

Au cours des années suivantes, Vim a connu de nombreuses améliorations. Une étape importante fut l’introduction des fenêtres multiples dans la version 3.0 (en 1994). Avant cela, il était déjà possible d’éditer plusieurs fichiers dans Vim, mais un seul fichier était visible à la fois, il n’était pas possible d’afficher plusieurs fichiers simultanément. Avec la version 4.0 (datant de 1996) fut ajoutée pour la première fois une interface utilisateur graphique, en grande partie écrite par Robert Webb. Depuis 1998, Vim (dans sa version 5.0) propose la coloration syntaxique.

La version 6.0 a vu le jour en 2001 : repli du texte, greffons, support de l’internationalisation, partage vertical des fenêtres. Depuis la sortie de la version 5.4 en , des patchs sont publiées pour corriger les erreurs et éliminer les bugs.

La dernière étape notable de Vim est survenue en 2006 avec la version 7.0. Son développement a débuté en 2004 et elle intègre en particulier la vérification et la correction orthographique, le support des onglets et un complètement personnalisable selon les langages. Le principe fondamental du développeur principal de Vim reste toujours le même : Vim doit rester un éditeur de texte et être le plus possible au service de l’utilisateur.

Note : Dans son aide interne, Vim comporte le détail des modifications entre chaque version. Par exemple, la commande du mode Normal :help version-5.4 permet de connaître les différences entre les versions 5.3 et 5.4. De cette manière, il est possible de remonter jusqu’aux différences entre les versions 3.0 et 4.0.


L’efficacité de Vim est notamment due à plusieurs améliorations marquantes par rapport à vi.

Vim a l’avantage de fonctionner sur de nombreux systèmes d’exploitation. Cet aspect multi-plateforme est très important pour les utilisateurs travaillant sur plusieurs environnements différents, comme les administrateurs systèmes. Avec Vim, ils trouvent un éditeur identique (aux capacités de chaque système d’exploitation près) pour tous ces systèmes :

AmigaOS, Atari MiNT, BeOS, MS-DOS, Mac OS, NeXTSTEP, OS/2, OSF, RiscOS, SGI, UNIX, VMS, Win16 + Win32 (Windows 95/98/2k/NT/XP/Vista/2k3), et tous les systèmes BSD et GNU/Linux.

Vim possède une documentation établie selon le précepte : « Une fonctionnalité non documentée est une fonctionnalité inutile ». La documentation en mode texte occupe pratiquement 4 mégaoctets. L’utilisateur a la possibilité d’utiliser différentes fonctions de recherche.

Grâce à la coloration syntaxique et à une manière spécifique de rédiger les fichiers d’aide, les notions principales sont mises en valeur. Les sujets documentés (désignés par des mots-clés) ont une
couleur particulière et apparaissent entourés par des barres verticales. Le déplacement dans l’aide s’effectue grâce à des liens hypertextes. Cette navigation se pratique à l’aide de commandes claviers, mais est également possible par l’intermédiaire de la souris, dans l’interface graphique.

Il existe d’autres commandes encore pour faciliter les recherches de l’utilisateur, en particulier la commande :helpgrep ("Image 3"). Avec cette commande, l’utilisateur peut chercher un mot-clé au travers de la totalité de l’aide, afficher les résultats dans une fenêtre et atteindre l’emplacement
correspondant dans les fichiers de l’aide. Il est possible de faire des recherches sur les résultats de la recherche afin d’y trouver d’autres mots-clés.

L’aide interne est complétée par une version HTML disponible en ligne sur Internet, une Foire Aux Questions, une littérature nombreuse et variée en français ou en anglais, et bien d’autres sources d’informations encore. Le site VimDoc donne accès à de nombreuses informations sur la documentation existante.

Vim donne la possibilité d’afficher facilement deux (ou trois) versions d’un fichier côte à côte et de mettre dans une couleur différente leurs différences. Les lignes ajoutées ou supprimées ont leur propre couleur, tandis que les lignes communes sont repliées afin de les masquer.

Le défilement vertical des fenêtres des fichiers comparés est synchronisé, afin de conserver les lignes identiques constamment face à face. Des commandes permettent de placer le curseur sur les différences suivantes ou précédentes ([c et ]c) puis de reporter ces différences depuis ou vers l’autre fichier (do et dp). La coloration syntaxique est automatiquement remise à jour pour en tenir compte.

Vim est un éditeur de texte écrit pour les programmeurs. Pour leur faciliter la tâche, il existe un mode de fonctionnement appelé "Quickfix", simplifiant le cycle « édition-compilation-correction ». Comme dans un environnement de développement intégré, le code source édité est directement compilé depuis Vim, qui appelle le compilateur approprié. Si des erreurs surviennent pendant la compilation, elles sont affichées dans une nouvelle fenêtre. Dans la fenêtre contenant le code source, il est alors possible de sauter directement à la ligne concernée par le message d’erreur pour la corriger. Ainsi un nouveau cycle de compilation peut recommencer et les éventuelles nouvelles erreurs être corrigées. Vim offre aussi la coloration syntaxique et la possibilité de replier certaines parties de leur code (voir :help quickfix).

Vim dispose d’un langage de script programmable et extensible. Ainsi les traitements trop complexes pour être effectués à l’aide d’une macro peuvent être automatisés. Les scripts Vim peuvent être appelés explicitement au démarrage de Vim avec l’option "-s" de la ligne de commande, ou bien automatiquement s’ils se trouvent dans le répertoire approprié. Quand Vim est déjà démarré, la commande :source permet de les charger. Un exemple de script Vim est le fichier de démarrage, ".vimrc" sur Unix et "_vimrc" sur MS-Windows, qui contient l’ensemble des réglages à effectuer. Ce fichier est automatiquement chargé au lancement de Vim.

Dans le langage de script Vim, toutes les commandes du mode ligne-de-commande sont disponibles en tant qu’instructions, ainsi que les commandes du mode Normal grâce à la commande :normal. Il existe deux types de données : les Nombres et les Chaînes de caractères (plus, dans la version 7 uniquement, listes, dictionnaires et pointeurs de fonction). Les booléens sont évalués comme des Nombres, avec la convention suivante : nul = FAUX et non nul = VRAI. Les principaux opérateurs de comparaisons, les opérateurs logiques et
les opérations arithmétiques de base sont intégrés. Les structures de contrôle "if - then - elseif - else" et les boucles "while" sont disponibles. L’utilisateur peut définir ses propres fonctions et dispose de plus de cent fonctions prédéfinies—essentiellement des appels systèmes. Les scripts
peuvent être testés en mode de débogage.

Avant d’écrire un script, il vaut mieux vérifier la page des scripts Vim, pour s’assurer qu’un script fonctionnellement équivalent n’existe pas déjà. Bien que de nombreux scripts soient destinés à faciliter la programmation, les utilisateurs non-programmeurs auront tout à gagner à y jeter un œil. En effet, tout utilisateur ayant résolu un problème général a pu mettre ses scripts à disposition. Par exemple, le script Vim « calendar » permet d’organiser ses échéances de manière très simple. Une fois copié dans le répertoire adapté, il suffit de l’appeler avec la commande :Calendar dans Vim.

Vim est un éditeur modal. Cela signifie que l’on effectue différentes tâches dans différents modes, ce qui pose bien souvent des problèmes aux débutants. Pour voir dans quel mode on se trouve, il est possible d’activer l’option 'showmode'. Seuls les six modes de base sont indiqués et brièvement décrits dans cet article. Les cinq modes supplémentaires sont des variantes des modes de base. Ils ne seront pas exposés ici, afin de faciliter la compréhension.
Pour plus d’informations, consultez l’aide intégrée de Vim :help vim-modes.

Vim démarre en mode Normal, aussi appelé mode Commande. Dans ce mode, il est par exemple possible de copier des lignes ou de les déplacer grâce à des raccourcis, de mettre du texte en forme, ou de se déplacer dans le fichier. Il s’agit du mode central, en ce sens qu’il permet d’accéder à tous les autres modes.

Le mode Insertion est le plus naturel, car il se comporte comme la plupart des autres éditeurs. On peut passer du mode Normal au mode Insertion avec la commande i (comme « Insertion »). Il existe de nombreuses autres commandes pour passer dans ce mode. En mode Insertion, l’édition du texte se fait de manière habituelle. "eVim" démarre directement dans ce mode. Mais dans ce cas, toutes les commandes du mode Normal ne sont pas disponibles ou facilement accessibles, ce qui est contraire au critère d’efficacité prôné pour Vim. La touche "Echap" permet de retourner dans le mode Normal.

On atteint ce mode en tapant d’abord le caractère deux-points « : ». Ensuite, un certain nombre de commandes peuvent être saisies. Parmi les plus courantes on trouve :

Après un appui final sur la touche "Entrée", la commande est exécutée et Vim retourne dans le mode Normal (exception faite de la commande « q »).

Ce mode constitue une amélioration par rapport à vi et ressemble au mode Normal. À l’aide d’un raccourci, une zone de texte, par exemple une suite de caractères (raccourci v), un ensemble de lignes (raccourci MAJ-V) ou bloc rectangulaire (raccourci CTRL-V), peut être délimitée à l’aide des touches fléchées. La zone ainsi sélectionnée est signalée par une coloration particulière. Ensuite, des commandes du mode Normal peuvent être appliquées à cette zone. Dans ce mode, le travail est facilité par le fait que l’on voit avec précision la zone sur laquelle on agit.

Ce mode débute par le mode Visuel, dans lequel on choisit une zone de texte. Ensuite, on accède au mode Sélection avec le raccourci CTRL-G. Dans ce mode, dès qu’un caractère imprimable est saisi, la zone sélectionnée est supprimée et Vim passe en mode Insertion, c’est-à-dire que le texte sélectionné est remplacé par le texte tapé. La touche "Echap" permet de sortir de ce mode. Ce mode constitue une amélioration par rapport à vi.

Ce mode ressemble au mode Ligne-de-commande, à la différence que Vim ne
retourne pas dans le mode Normal après l’exécution de chaque commande. Le raccourci Q permet de passer dans ce mode tandis que :vi permet d’en sortir.

Vim est réputé pour être l’un des éditeurs de texte les plus puissants. Son principal concurrent est Emacs ; la lutte entre Vimistes et Emacistes est une source intarissable de trolls. "Cf." guerre d’éditeurs.

Les habitués d'éditeurs simples comme Gedit ou même, en mode texte, nano, ne seront pas tout de suite pleinement efficaces avec des éditeurs tels que vi ou Vim, dont le principe de fonctionnement diffère de beaucoup d'autres éditeurs par ses deux modes, « commande » et « insertion ». Souvent, l’utilisateur ne sait pas dans quel mode il se trouve.

Pour le néophyte, le fonctionnement d'Emacs peut sembler plus naturel : chaque pression d'une touche du clavier affiche le caractère correspondant à l'écran. Emacs, pour sa part, ne nécessite pas de va-et-vient entre les modes « commande » et « insertion ». Vim tout comme Emacs, en raison de leur puissance, sont riches de commandes et chacun doit de ce fait être étudié quelque temps pour pouvoir être utilisé. Emacs étant bien plus qu'un éditeur de texte, la comparaison de Vim et Emacs n'a pas beaucoup plus de sens que celle entre Notepad (ou Notepad++) et la suite LibreOffice.

De nombreuses commandes de Vim (comme de Vi) possèdent des raccourcis obscurs que très peu d'utilisateurs connaissent intégralement, ce qui peut entraîner des frustrations. En contrepartie, Vim et Vi accélèrent considérablement les tâches répétitives comme on en rencontre dans une migration ou un portage de code source.

Les défenseurs de Vim répondent que la contrepartie du temps important de formation est une productivité plus importante comparée aux autres éditeurs. Les raccourcis des commandes sont bien souvent des initiales des phrases indiquant la tâche à effectuer. Par exemple la commande daw efface un mot (, traduction littérale en anglais). Ces raccourcis permettent d’effectuer de nombreux traitements sans quitter le clavier et avec peu de saisie, comme copier ou coller du texte, le mettre en forme, effectuer un tri, ce qui n’est pas toujours immédiat avec d’autres éditeurs de texte. Bien qu’entièrement contrôlable avec le clavier, Vim permet aussi de travailler avec des menus et la souris en mode graphique.

Les défenseurs de Vim comparent souvent celui-ci à Emacs car, pour l'édition de texte, le paramétrage des raccourcis par défaut y est plus rapide et ergonomique.

Enfin, Vim est léger et rapide. Disponible sur de nombreuses plates-formes, il convient parfaitement au traitement des fichiers de configuration.

Pour pouvoir travailler efficacement avec Vim, il est d’abord nécessaire de comprendre le principe des modes. Avec une connaissance de base de quelques commandes du mode Normal, Vim est utilisable. Toutefois, pour travailler efficacement, et devenir potentiellement plus productif qu’avec tout autre éditeur, il est nécessaire de connaître un grand nombre de raccourcis et d’atteindre une certaine maîtrise dans leur utilisation.

L’une des autres qualités de Vim est qu’il est hautement paramétrable (raccourcis, touches, menus, définition de nouvelles fonctions, etc.). Cette caractéristique a permis notamment d’introduire le mode "Vim Easy" (depuis la série 6.x), permettant à Vim de se comporter comme un éditeur de texte classique (« amodal »), ce qui le met à la portée des non-initiés.

D'autres sites tels que Facebook ou encore Github utilisent les raccourcis Vim pour la navigation.

En 1999, Vim a reçu le puis, en 2000, le en tant que 

De 2001 à 2005, Vim a gagné le prix du "Linux Journal" dans la catégorie .

Vim est un logiciel libre dont la licence est compatible avec la GPL, ce qui signifie que vous pouvez le distribuer librement. Vim est aussi un caritaticiel — c’est-à-dire que si vous l’utilisez, vous êtes encouragé à faire un don en faveur d’"ICCF Holland", pour les orphelins en Ouganda. Cette association, basée aux Pays-Bas, a été fondée par Bram Moolenaar et il en est le trésorier.

Vim est principalement développé par Bram Moolenaar, aidé par de nombreux contributeurs. La page correspondante de l’aide intégrée (:help credits) fait figurer plus de 50 contributeurs. À ceux-ci s’ajoutent de nombreux anonymes, qui offrent leur aide, non seulement pour les futures fonctionnalités de Vim, mais aussi pour le portage sur d’autres systèmes d’exploitation, les tests fonctionnels, la recherche de bugs, la précision de la documentation ou la traduction. De plus, une attention particulière est donnée aux utilisateurs, sous la forme de sondage sur les fonctionnalités les plus demandées,
l’intégration de Vim dans d’autres projets, etc.

Tous les utilisateurs peuvent publier leurs scripts Vim ou donner leurs astuces et conseils d’utilisation sur le site Internet de Vim. Une liste de diffusion très active permet à chacun, débutant ou utilisateur expérimenté, d’obtenir rapidement des réponses à ses questions.





</doc>
<doc id="6541" url="https://fr.wikipedia.org/wiki?curid=6541" title="Polymère">
Polymère

Les polymères (étymologie : du grec "polus", plusieurs, et "meros", partie) constituent une classe de matériaux. D'un point de vue chimique, un polymère est une macromolécule (molécule constituée de la répétition de nombreuses sous-unités).

Les polymères les plus connus sont :
Ils sont très utilisés pour les matrices des matériaux composites.

Ils ont les propriétés générales suivantes :
Les propriétés sont déterminées par :

Les polymères de synthèse sont devenus l'élément essentiel d'un nombre très important d'objets de la vie courante, dans lesquels ils ont souvent remplacé les substances naturelles. Ils sont présents dans de nombreux domaines industriels.

Il existe une corrélation étroite entre le produit national brut (PNB) d'un pays et sa consommation de polymères.

Les polymères naturels ont été parmi les premiers matériaux utilisés par l'Homme : bois et fibres végétales, cuir, tendons d'animaux, laine

La notion de macromolécule n'est apparue que tardivement dans l'histoire de la chimie. Bien que présagée par Wilhelm Eduard Weber ou encore Henri Braconnot au début du , de nombreux chercheurs ne voient là que des agrégats ou micelles. Le terme « polymère » est utilisé pour la première fois en 1866 par Marcellin Berthelot. La vulcanisation du caoutchouc en 1844 et la production de la Bakélite en 1910 figurent parmi les premières applications industrielles. Mais il faut attendre les années 1920-1930 pour que l'idée de macromolécule soit acceptée, notamment grâce aux travaux d'Hermann Staudinger.
Le développement industriel consécutif de la science macromoléculaire a été accéléré ensuite par la Seconde Guerre mondiale. Les États-Unis ont été privés lors de leur entrée en guerre de leur approvisionnement en caoutchouc naturel en provenance d'Asie du Sud-Est. Ils ont alors lancé un immense programme de recherche visant à trouver des substituts de synthèse.

Pour répondre à toutes les exigences en termes de formes et de cadences, plusieurs méthodes de mise en forme des matériaux polymères ont été développées, dont :
Concernant les plastiques renforcés, une quinzaine de procédés de mise en œuvre est de nos jours disponible (pultrusion…).




Les mélanges polymère-polymère thermoplastiques (en anglais, "polyblend" ; « alliage » est un terme impropre) sont des mélanges mécaniques intimes de deux (ou plusieurs) polymères différents et compatibles. À la différence des copolymères, il ne se forme pas de liaison chimique. Exemples :

Un polymère tridimensionnel est constitué d'une seule macromolécule qui se développe dans les trois directions de l'espace ; cette macromolécule tridimensionnelle atteint des dimensions macroscopiques ( un phénoplaste).

Une macromolécule est une molécule de masse moléculaire élevée, généralement constituée par la "répétition" d'atomes ou de groupes d'atomes, appelés "unités constitutives" et dérivant, de fait ou conceptuellement, de molécules de faible masse moléculaire.

Dans de nombreux cas, une molécule peut être considérée comme ayant une masse moléculaire élevée lorsque l'addition ou la suppression d'une ou de quelques unités n'a qu'un effet négligeable sur les propriétés moléculaires.

En fait, il n'existe que très peu d'exemples de macromolécules qui ne soient obtenues par la "répétition" d'une unité structurale. On les trouve plutôt dans le domaine des macromolécules naturelles, certaines protéines notamment.

Le terme "polymère" a encore actuellement plusieurs définitions selon le point de vue qu'on adopte. On peut encore trouver les oligomères (qui ne sont pas constitués de macromolécules) inclus dans la famille des polymères. Auparavant, et encore récemment, les "polymères proprement dits" (au sens actuel du terme) étaient aussi appelés « hauts polymères ».
On peut noter que l'expression « degré de polymérisation » est toujours utilisée pour désigner le nombre d'unités monomères aussi bien d'une macromolécule que d'une molécule oligomère.

Un polymère est organique (le plus souvent) ou inorganique.

Il est issu de l'enchaînement covalent d'un grand nombre de motifs monomères identiques ou différents.

Un polymère peut être "naturel" ( polysaccharides, ADN) ; "artificiel", obtenu par modification chimique d'un polymère naturel ( acétate de cellulose, méthylcellulose, galalithe) ; ou "synthétique", préparé par polymérisation de molécules monomères ( polystyrène, polyisoprène synthétique).

Les polyoléfines, représentées principalement par les polymères thermoplastiques de grande consommation polyéthylène et polypropylène, constituent la plus importante famille de polymères.

Un polymère peut se présenter sous forme liquide (plus ou moins visqueux) ou solide à température ambiante. À l'état solide, il peut être utilisé comme matériau moyennant des propriétés mécaniques suffisantes. Un polymère liquide à température ambiante peut être transformé en matériau s'il est réticulable ; les élastomères sont des matériaux obtenus par réticulation de polymères linéaires liquides à température ambiante.

Les polymères ont un comportement viscoélastique. En effet, ils démontrent simultanément des propriétés élastiques et un caractère visqueux.

L'enchaînement des motifs monomères peut se faire de façon linéaire (polymères linéaires), présenter des ramifications aléatoires (polymères branchés et hyperbranchés) ou systématiques et régulières (dendrimères).

Du fait des degrés de liberté de la conformation (disposition dans l'espace) de chaque motif monomère, la conformation du polymère résulte de cet enchaînement mais également des interactions entre motifs.

Les polymères fabriqués à partir d'un seul type de monomère sont désignés par "homopolymère" ( polyéthylène, polystyrène). Dès qu'au moins deux types de monomère participent à la formation des macromolécules, on parlera de "copolymères" (cas du styrène-butadiène). La variété des copolymères est très importante. Ces matériaux possèdent des propriétés physico-chimiques et mécaniques intermédiaires avec celles obtenues sur les homopolymères correspondants.

On distingue deux grandes catégories de réactions chimiques permettant la préparation des polymères : la polymérisation en chaîne ou polyaddition (pour produire par exemple le polyéthylène, le polypropylène, le polystyrène) et la polymérisation par étapes ou polycondensation (pour synthétiser par exemple le poly(téréphtalate d'éthylène), de sigle PET).

Le terme « polymère » désigne des matières abondantes et variées : des protéines les plus ténues aux fibres de Kevlar haute résistance. Certains polymères sont utilisés en solution par exemple dans les shampooings ; d'autres forment des matériaux solides.

Pour ces applications, les polymères sont généralement mélangés à d'autres substances – charges telles la craie (matière très bon marché), plastifiants, additifs tels les antioxydants – dans des opérations de formulation. La fabrication des objets finis résulte la plupart du temps d'une opération de mise en œuvre qui relève souvent du domaine de la plasturgie.

Les polymères sont souvent classés d'après leurs propriétés thermomécaniques. On distingue :

La description des polymères en tant qu'objet physique permettant de comprendre leurs propriétés relève de la physique statistique.

Les polymères sont des substances composées de macromolécules résultant de l'enchaînement covalent (voir Liaison covalente) de "motifs de répétition" identiques ou différents les uns des autres. La masse molaire de ces molécules dépasse souvent . Les liaisons covalentes constituant le squelette macromoléculaire sont le plus souvent des liaisons carbone-carbone (cas du polyéthylène, du polypropylène), mais peuvent également résulter de la liaison d'atomes de carbone avec d'autres atomes, notamment l'oxygène (cas des polyéthers et des polyesters) ou l'azote (cas des polyamides). Il existe aussi des polymères pour lesquels l'enchaînement résulte de liaisons ne comportant pas d'atomes de carbone (polysilanes, polysiloxanes).

Cet enchaînement de motifs répétés présente chez les polymères les plus simples une structure linéaire, un peu comme un collier de perles. On peut également rencontrer des chaînes latérales (elles-mêmes plus ou moins branchées), résultant soit d'une réaction chimique parasite au cours de la synthèse du polymère (par exemple dans le cas du polyéthylène basse densité ou PEBD), soit d'une réaction de greffage pratiquée volontairement sur le polymère pour en modifier les propriétés physico-chimiques.

Dans le cas où la macromolécule est composée de la répétition d'un seul motif – ce qui résulte le plus souvent de la polymérisation d'un seul type de monomère –, on parle d"'homopolymères". Si au moins deux motifs différents sont répétés, on parle de "copolymères". Voir aussi Terpolymère.

On distingue ensuite plusieurs types de copolymères suivant la manière dont les motifs monomères sont répartis dans les chaînes moléculaires :

Il existe parfois des liaisons covalentes vers d'autres parties de chaînes polymères. On parle alors de "molécules « branchées » ou ramifiées". On sait synthétiser par exemple des molécules en "peigne" ou en "étoile". Lorsque de nombreuses chaînes ou chaînons ont été réunis par un certain nombre de liaisons covalentes, ils ne forment plus qu'une macromolécule gigantesque ; on parle alors de "réseau macromoléculaire" ou de "gel".

Les forces qui assurent la cohésion de ces systèmes sont de plusieurs types : interactions de van der Waals ou liaisons hydrogène. Leur intensité est respectivement de 2 à , et . Il existe également des interactions liées aux charges.

La densité d'énergie cohésive (valeurs tabulées) permet d'avoir une idée de la cohésion des polymères.

Lors de la réaction de polymérisation, lorsque chaque unité monomère est susceptible de se lier à deux autres, la réaction produit une chaîne linéaire. Typiquement, ce cas est celui des polymères "thermoplastiques".

Du fait des degrés de liberté de la conformation de chaque unité monomère, la façon dont la chaîne occupe l'espace n'est cependant pas rectiligne.

Chaque unité monomère présente une certaine rigidité. Souvent, cette rigidité influence l'orientation de l'unité monomère voisine. Toutefois, cette influence s'estompe au fur et à mesure que l'on s'éloigne de l'unité monomère initiale et finit par disparaître au-delà d'une distance formula_1, dite « longueur d'un maillon statistique de la chaîne ». Techniquement, cette longueur est la longueur de corrélation de l'orientation d'un maillon. Elle se nomme longueur de persistance du polymère.

Ayant introduit cette notion, il est alors possible de renormaliser la chaîne en considérant maintenant le maillon statistique comme son motif élémentaire. Pour décrire la conformation de cette chaîne, les particularités propres à la structure chimique du motif monomère n'interviennent plus.

Le cas le plus simple est celui de l'enchaînement linéaire de maillons n'exerçant pas d'interaction entre eux. À l'état liquide, la chaîne adopte dans l'espace une conformation qui pour une molécule donnée change sans cesse du fait de l'agitation thermique. À l'état de solide amorphe ou à un instant donné dans le cas d'un liquide, la conformation des chaînes est différente d'une molécule à l'autre. Cette conformation obéit néanmoins à des lois statistiques.

Soit dans la séquence primaire de la chaîne un maillon donné pris pour origine. Lorsque les maillons n'interagissent pas, la probabilité que le formula_2 maillon de la chaîne soit à une distance formula_3 de l'origine obéit à une loi normale ou loi Gaussienne de moyenne nulle et de variance formula_4. Une longueur caractéristique de la chaîne est la distance formula_5 entre ses deux extrémités (dite « distance bout-à-bout »). La moyenne arithmétique de formula_5 est nulle. Ainsi, pour caractériser la taille de la pelote que forme la chaîne, il faut considérer la moyenne quadratique, notée ici formula_7. En raison de la loi normale, cette moyenne varie comme la racine carrée du nombre formula_8 de maillons.

formula_9

La conformation statistique d'une telle chaîne est l'analogue de la trace laissée par un marcheur aléatoire où formula_8 représente le nombre de pas de la marche, formula_1 leur longueur et formula_12 le déplacement carré moyen du marcheur.

Dans les années 1970, Pierre-Gilles de Gennes a montré l'analogie entre la description d'une chaîne polymère et les phénomènes critiques. Aussi, l'utilisation de la lettre formula_13 pour désigner l'exposant, obéit à la nomenclature des exposants critiques. Les objets rencontrés dans les phénomènes critiques ont des propriétés d'autosimilarité et peuvent être décrits en termes de géométrie des fractales, dans ce cas l'exposant formula_13 représente l'inverse de la dimension fractale formula_15

formula_16.

Une conformation gaussienne des chaînes se rencontrent dans deux cas :


Lorsque les interactions répulsives entre maillons d'une même chaîne dominent (typiquement à température plus haute que la température -formula_17), la conformation de la chaîne s'en trouve « gonflée » par rapport à sa conformation idéale. Dans ce cas, la valeur de l'exposant formula_13 et la taille caractéristique de la pelote que forme la chaîne sont supérieures à celle d'une chaîne idéale

formula_20.

La valeur approchée de cet exposant fut établie par Paul Flory dans les années 1940. Bien qu'il ait été démontré depuis que le raisonnement utilisé à l'époque était erroné, la valeur formula_21 est étonnamment proche de la valeur exacte formula_22 trouvée depuis par les méthodes beaucoup plus sophistiquées du groupe de renormalisation.

En termes de marche aléatoire, une chaîne gonflée correspond à la trace laissée par un marcheur effectuant une .

Lorsque les interactions attractives entre maillons d'une même chaîne dominent (typiquement à température plus basse que la température -formula_17), la chaîne s'effondre sur elle-même et adopte une conformation compacte dite « globulaire » (à opposer au terme « pelote » utilisé pour les conformations idéale ou gonflée). Dans ce cas

formula_24.

Le terme « conformation compacte » se comprend mieux en écrivant la relation « taille caractéristique-nombre de maillons » sous la forme formula_25, qui exprime que le volume du globule est proportionnel au nombre de maillons. Ce comportement est celui d'un objet homogène dont la masse volumique est une constante indépendante de sa taille.

Implicitement nous avons considéré jusqu'ici une chaîne seule dont les maillons seraient comme les molécules d'un gaz. Dans la pratique, les chaînes sont soit en présence de leurs semblables, très proches les unes des autres et entremêlées (cas évoqué au paragraphe « chaîne gaussienne »), soit en présence d'un solvant. Ce dernier cas est celui d'une solution de polymère.

En solution, la conformation du polymère résulte du bilan des interactions « monomère-monomère », « monomère-solvant » et « solvant-solvant ». Il est possible de rendre compte de ce bilan au moyen d'un paramètre effectif d'interaction, appelé paramètre de . Trois cas sont envisageables :


En solution suffisamment diluée, les chaînes sont bien séparées les unes des autres. La conformation d'une chaîne ne dépend alors que du bilan des interactions effectives entre ses propres maillons. En solvant -formula_17, la conformation est idéale (formula_31), en bon solvant elle est gonflée (formula_32) et en mauvais solvant elle est globulaire .

Certaines molécules ont la propriété de pouvoir se lier aléatoirement à au moins trois autres durant leur réaction de polymérisation. Les polymères qui en résultent ne sont plus linéaires mais branchés et réticulés et leur taille très largement distribuée. La moyenne de cette distribution augmente avec l'avancement de la réaction. L'ensemble de la population des molécules est soluble (on la désigne par le terme "sol") jusqu'à ce que la molécule la plus grande soit de taille macroscopique et connecte les deux bords du récipient contenant le bain de réaction. Cette molécule est appelée le "gel". Typiquement, ce type de réaction est à la base des résines thermodurcissables.

L'apparition du gel confère au bain de réaction, initialement liquide, une élasticité qui est la caractéristique d'un solide. Cette transition de phase est bien décrite par un modèle de percolation (conjecture émise en 1976 de façon indépendante par Pierre-Gilles de Gennes et et bien vérifiée expérimentalement depuis) qui prévoit la forme de la fonction de distribution, formula_33, du et la façon dont elles occupent l'espace. Jusqu'à la molécule la plus grande, formula_33 est une loi de puissance du type

formula_35.

Une taille caractéristique, formula_36, de chaque molécule peut être définie par la moyenne quadratique des distances entre monomères, on parle de rayon de giration. La relation entre cette longueur et le nombre de monomères est également une loi de puissance

formula_37

où formula_15 est la dimension fractale des molécules. Les valeurs particulières de ces exposants font qu'ils obéissent à la relation dite d'hyperéchelle reliant les exposants critiques à la dimension de l'espace formula_39.

formula_40

L'implication majeure de cette relation est que les polymères branchés occupent l'espace à la façon des poupées russes, les petits à l'intérieur du volume occupé par les plus grands.

Les caractéristiques structurales des polymères sont accessibles expérimentalement par des expériences de diffusion élastique de rayonnement : diffusion de la lumière, diffusion aux petits angles des rayons X et des .

Ces expériences consistent à éclairer un échantillon par une onde plane, monochromatique, de vecteur d'onde formula_41. Une partie de cette onde est déviée par les atomes constituants l'échantillon. Les ondes ainsi diffusées produisent des interférences dont l'analyse peut fournir des renseignements sur certaines caractéristiques de l'échantillon.

L'intensité totale, formula_42, de l'onde diffusée dans une certaine direction est recueillie à une distance formula_43 de l'échantillon. En toute généralité, on peut écrire

formula_44
où formula_45 est l'intensité de l'onde incidente (exprimée en nombre de particules, photons ou neutrons selon le rayonnement, par unité de temps et de surface). La grandeur formula_46, appelée section efficace différentielle de diffusion de l'échantillon, est homogène à une surface et contient l'information qui nous intéresse.

La diffusion élastique de rayonnement suppose que les ondes diffusées, de vecteur d'onde formula_47, sont de même longueur d'onde que l'onde incidente : formula_47 et formula_41 ont la même norme. La section efficace différentielle de diffusion est mesurée en fonction du vecteur de diffusion formula_50.

Si l'échantillon est isotrope, la mesure ne dépend pas de l'orientation de formula_51 mais uniquement de sa norme qui s'écrit

formula_52

où formula_53 est la longueur d'onde et formula_17 l'angle de diffusion.

Pour une solution suffisamment diluée de polymères, on peut montrer que la section efficace différentielle de diffusion s'écrit (cf. Diffusion élastique de rayonnement)

formula_55

où formula_4 est le nombre de chaînes en solution, formula_57 le volume de chacune d'elles (c'est-à-dire la somme des volumes de tous ses monomères et non pas le volume de la sphère contenant la pelote), et formula_58 un facteur représentant le contraste entre le polymère et le solvant et qui dépend du rayonnement utilisé. Ce facteur de contraste peut se mesurer où se calculer à partir de données tabulées : c'est une grandeur connue dans la plupart des cas.

La grandeur formula_59 est appelée facteur de forme des polymères en solution. C'est une grandeur normalisée telle que formula_60. Pour une chaîne polymère de formula_8 maillons de masse molaire formula_62 et de masse volumique formula_63, l'équation précédente devient

formula_64
où formula_65 est la concentration de la solution exprimée en masse par unité de volume (g/cmformula_66 par exemple).

On peut montrer (voir Diffusion élastique de rayonnement), qu'à petit vecteur de diffusion comparé à la taille moyenne des pelotes, la section efficace par unité de volume s'écrit

formula_67

formula_68 est une grandeur caractéristique de la taille moyenne des pelotes que forment les chaînes. Cette grandeur appelée « rayon de giration » est la moyenne quadratique des distances des monomères au centre de gravité de chaque pelote.

Pour une solution très diluée, de concentration formula_69 connue et pour un rapport formula_70 également connu : la grandeur mesurée, formula_71, varie de façon affine avec le carré formula_72 du vecteur de diffusion. L'ordonnée à l'origine permet de déterminer le nombre formula_8 de maillons des chaînes en solution. La pente de la droite permet de déterminer le rayon de giration formula_68. Ce type d'expériences a permis de vérifier les relations entre ces deux grandeurs et de déterminer en particulier l'exposant formula_75 qui les unit.

Les objets ordinaires ont une masse, formula_76, qui varie comme la puissance 1, 2 ou 3 de leur taille formula_5. Pour les objets fractals, cette puissance n'est pas nécessairement entière

formula_78

formula_15 est la dimension fractale de cette famille d'objets. Elle caractérise la façon dont ils remplissent l'espace.

Les fractals sont le plus souvent autosimilaires, c'est-à-dire invariants par changement d'échelle.

Une fois grossie, une petite partie est statistiquement semblable à l'objet entier.

Observons sous différents grossissements la pelote que forme une chaîne polymère de rayon de giration formula_68. Si formula_81 est la taille de la zone observable, pour formula_82 (petit grossissement), la masse visible, formula_83, est égale à la masse totale formula_76. Par contre, pour formula_85 (fort grossissement), la masse visible décroît lorsque le grossissement augmente. Supposons une loi du type

formula_86.

Pour déterminer l'exposant formula_87, on utilise un argument d'échelle qui postule :


La relation précédente donne formula_91.

C'est ce qui se passe lors d'une expérience de diffusion de rayonnement pour laquelle l'échelle d'observation peut être assimilée à l'inverse du vecteur de diffusion : formula_92. En solution très diluée, la grandeur physique donnant accès à la masse d'un objet est la section efficace de diffusion cohérente par unité de volume, de concentration et de contraste

formula_93.

Dans le régime intermédiaire du vecteur de diffusion tel que formula_94, la mesure sonde l'intérieur de l'objet et est sensible à son autosimilarité. L'argument d'échelle postule qu'une seule longueur est pertinente (or formula_68 est déjà nécessaire à formula_96) et que l'expérience est insensible à formula_76.

formula_98

Une expérience de diffusion de rayonnement réalisée à grand vecteur de diffusion par rapport au rayon de giration des chaînes donne directement accès à la dimension fractale des chaînes en solution. Cette façon de déterminer formula_15 utilise l'autosimilarité d'une chaîne seule, tandis que la précédente basée sur des utilise l'autosimilarité des chaînes entre elles.

La nomenclature IUPAC recommande de partir du motif de base de la répétition.
Cependant de très nombreux polymères ont des noms usuels ne respectant pas cette nomenclature, mais sont basés sur le nom des molécules servant à synthétiser le polymère.
Exemple : le polymère de formule formula_100 est couramment appelé polyéthylène (sigle PE). Pour respecter la nomenclature IUPAC, il devrait être nommé « polyméthylène » car le motif constitutif n'est pas le groupe éthylène formula_101 mais le groupe méthylène formula_102.

Il existe à présent deux nomenclatures recommandées par l'IUPAC :

Références : les recommandations de l'IUPAC et le glossaire du JORF du mars 2002 pour les adaptations en langue française.

(*) Macromolécule régulière : macromolécule qui résulte de la répétition d'unités constitutives toutes identiques et reliées entre elles de la même manière.


Exemples : propane ("ane" désigne un alcane), glycérol ("ol" désigne le groupe hydroxyle formula_103 des alcools et des phénols).

Exemples : pentane ("penta-" = cinq (atomes de carbone), et "ane"), 1,3-thiazole.

1 - Nomenclature systématique

Elle est basée sur la "structure" des macromolécules constituant les polymères ; l'unité structurale est l"'unité constitutive de répétition" (CRU), ou "motif constitutif" (MC).


2 - Nomenclature dérivée des précurseurs

Les précurseurs servant à synthétiser les polymères sont le plus souvent des monomères.


3 - Exemples

3-1 "Monomère" : l"'éthène" formula_104 (formule semi-développée)


"Remarque" : la formule formula_100 est plus souvent utilisée ; elle est acceptable en raison de son utilisation passée et de la tentative de conserver une certaine similitude avec les formules d’homopolymères issus d’autres monomères dérivés de l’éthène, comme celui de l’exemple 3-2. Le nom du polymère associé à cette formule est "polyéthylène" ; le nom "éthylène" devrait être utilisé pour désigner le groupe divalent formula_101 seulement, et non pour nommer le monomère formula_104 ("éthène"). 

3-2 "Monomère" : le "chlorure de vinyle" formula_111 (nom systématique : "chloroéthène")


3-3 "Récapitulation et autres exemples"

Source : nomenclature des polymères organiques monocaténaires réguliers (recommandations IUPAC 2002 en anglais).

(*) Le motif formula_124 est constitué de deux "sous-unités" : formula_125 (préfixe "oxy-") et
formula_101 (groupe "éthylène") ; l'hétéroatome O a priorité sur la sous-unité carbonée ; il est placé en première position.

(**) La priorité revient à la sous-unité la plus insaturée : formula_127.

(***) Certains polymères sont obtenus par modification chimique d'autres polymères de telle façon que l'on puisse penser que la structure des macromolécules qui constitue le polymère a été formée par homopolymérisation d'un "monomère hypothétique". Ces polymères peuvent être considérés comme étant des homopolymères. C'est le cas du poly(alcool vinylique).

(****) De nombreux polymères sont obtenus par réaction entre monomères mutuellement réactifs. Ces monomères peuvent facilement être visualisés comme ayant réagi pour donner un "monomère implicite" dont l'homopolymérisation conduirait à un produit qui peut être vu comme un homopolymère.
Le poly(téréphtalate d'éthylène) est obtenu par réaction entre l'acide téréphtalique (acide dicarboxylique) et l'éthylène glycol (diol) ; ces monomères ont réagi pour donner un "monomère implicite".





</doc>
<doc id="6542" url="https://fr.wikipedia.org/wiki?curid=6542" title="Isocyanate">
Isocyanate

Un isocyanate est un ion de formule N=C=O, base conjuguée de l'acide isocyanique, qui peut former des sels tel l'isocyanate de sodium, et des composés chimiques portant le groupe fonctionnel isocyanate -N=C=O.

Les composés isocyanates sont une ressource importante de divers secteurs industriels : papier, textile, adhésif, isolation (mousse polyuréthane).

Selon leur structure, les isocyanates organiques sont classés en séries aliphatiques, alicycliques et aromatiques. Ces derniers sont plus rigides et plus réactifs que les premiers. 

Une molécule peut contenir une ou plusieurs fonctions isocyanates.


Les diisocyanates sont surtout utilisés pour la fabrication des polyuréthanes. Les polyuréthanes obtenus à partir de diisocyanates aromatiques sont relativement plus stables que ceux issus de diisocyanates aliphatiques.

On distingue les diisocyanates symétriques et dissymétriques. La symétrie des diisocyanates affecte la morphologie des domaines rigides des polyuréthanes qui les contiennent.


Comme pour les diisocyanates, les polyisocyanates sont surtout utilisés pour la fabrication des polyuréthanes, par exemple le poly-diisocyanate de diphénylméthylène (PMDI).

La fonction isocyanate peut réagir surtout avec les fonctions chimiques à hydrogène labile. Voici quelques exemples de réaction avec les isocyanates :

L’isocyanate peut aussi réagir avec ses semblables : une dimérisation donne des urétidinediones et une trimérisation donne des isocyanurates.

En général, la vitesse de réaction pour l’addition d’un composé avec un groupement isocyanate est déterminée par sa basicité : plus le composé est basique plus sa vitesse de réaction avec un groupement isocyanate sera importante. La vitesse de réaction diminue dans l’ordre suivant :

amines aliphatiques primaires > amines aliphatiques secondaires > NH > amines aromatiques > urées aliphatiques > alcools primaires > eau > acides carboxyliques > alcools secondaires > phénols > urées aromatiques > alcools tertiaires > uréthanes > amides.

D’autre part, la réactivité des groupements isocyanates dépend de l’effet inductif donneur – attracteur du groupement R de l’isocyanate R–NCO. La réactivité forme alors la suite croissante suivante :

- ter-butyl- < cyclohexyl- < n-alkyl- < benzyl- < phényl- < para-nitrophényl- < chlorosulfonyl-.

Cependant la stabilité thermique de l’uréthane formé augmente aussi le vitesse de la réaction.

Afin de maîtriser la réactivité des fonctions isocyanates, le recours à leur blocage est effectué notamment pour permettre de stocker et conserver au cours du temps ces composés. Le blocage (appelé parfois protection, le terme protection ici ne correspond pas exactement au terme utilisé en chimie organique) est effectué en utilisant des molécules dites agent de blocage telles que les phénols, les lactames, les oximes, etc. D'une manière générale les agents de blocage sont des composés à hydrogène labile. Pour obtenir des isocyanates bloqués soluble dans l'eau, l'agent de blocage classiquement utilisé est le bisulfite (HSO). Les isocyanates ainsi protégés restent réactifs avec de bons nucléophiles tels que les amines.

Les diisocyanates et les polyisocyanates sont utilisés pour la synthèse de polymères dont la chaîne squelettique contient les groupes fonctionnels composés de carbone, oxygène et azote comme surtout les polyuréthanes et les polyurées et beaucoup moins les polyimides et les polyamides.

Les isocyanates sont généralement des composés toxiques et réactifs, qu'il convient de manipuler avec précautions. Toute exposition directe aux isocyanates ou à leurs vapeurs doit être strictement évitée. Par exemple, l'isocyanate de méthyle est à l'origine de la catastrophe de Bhopal, qui tua plusieurs milliers de personnes, en 1984.



</doc>
<doc id="6543" url="https://fr.wikipedia.org/wiki?curid=6543" title="Éthanol">
Éthanol

L’éthanol, ou alcool éthylique (ou plus simplement alcool), est un alcool de formule semi-développée . C'est un liquide incolore, volatil, inflammable et miscible à l'eau en toutes proportions. C'est un psychotrope, et l'une des plus anciennes drogues récréatives, sous la forme de boisson alcoolisée. L'éthanol est utilisé par l'industrie agroalimentaire (pour la production de spiritueux notamment), la parfumerie et la pharmacie galénique (comme solvant) ainsi qu'en biocarburant (bioéthanol). Il est en outre utilisé dans les thermomètres à alcool.

L'éthanol est un alcool primaire à deux carbones de formule brute et de formule semi-développée ou , indiquant que le carbone du groupe méthyle () est attaché au groupement méthylène () lui-même attaché au groupement hydroxyle (). C'est un isomère de constitution de l'éther méthylique. L’éthanol est usuellement désigné par l’abréviation « EtOH », le groupement éthyle () étant communément abrégé « Et » en chimie organique.

"Éthanol" est le nom systématique défini par la nomenclature des composés organiques pour une molécule avec deux atomes de carbone (préfixe "éth-") ayant une liaison simple entre eux (suffixe "-ane") et attaché à un groupe hydroxyle (suffixe "-ol").

La fermentation des sucres en éthanol est l'une des plus anciennes biotechnologies employée par l'homme et a été utilisée depuis la Préhistoire pour obtenir des boissons alcoolisées. Des analyses chimiques de composés organiques absorbés dans des jarres datant du Néolithique trouvées dans un village de la province du Henan en Chine, ont révélé que des mélanges de boissons fermentées composés de riz, de miel et de fruits étaient produits dès le  millénaire .

Bien que la distillation soit une technique connue des alchimistes gréco-égyptiens (comme Zosime de Panopolis), les premières traces écrites de production d'alcool à partir du vin ne remontent qu'au avec les travaux des alchimistes de l'école de médecine de Salerne. La première mention de la distinction entre alcool absolu et mélanges eau-alcool est rapportée elle au par Raymond Lulle. On notera cependant que le savant persan Rhazès aurait isolé l'éthanol dans le courant du .

En 1796, Johann Tobias Lowitz obtient de l'éthanol pur en filtrant sur du charbon actif de l'éthanol distillé. Antoine Lavoisier détermine que l'éthanol est composé de carbone, d'oxygène et d'hydrogène, et en 1808, Nicolas Théodore de Saussure détermine sa formule brute. En 1858, Archibald Scott Couper publie la structure chimique de l'éthanol, qui est l'une des premières structures déterminées.

L'éthanol est pour la première fois préparé de façon synthétique en 1826 par les travaux indépendants de Georges Serullas en France et de Henry Hennel au Royaume-Uni. En 1828, Michael Faraday synthétise de l'éthanol par l'hydratation en catalyse acide de l'éthylène, une synthèse similaire au procédé industriel utilisé de nos jours.

L'éthanol a aussi été utilisé comme combustible dans les lampes, et comme carburant pour les automobiles jusque dans les années 1930. Par exemple, la Ford T pouvait fonctionner jusqu'en 1908 avec de l'éthanol pur.

L'éthanol est un sous-produit du métabolisme des levures, et est donc présent dans l'habitat de ces organismes. On peut aussi retrouver fréquemment de l'éthanol dans les fruits murs, et dans de nombreuses plantes du fait de l'anaérobie naturelle durant la germination. De l'éthanol a aussi été détecté dans l'espace, recouvrant sous forme solide des grains de poussière dans les nuages interstellaires.

L'éthanol est un liquide volatil, incolore et qui a une odeur. Sa combustion est sans fumée et donne une flamme bleutée. Les propriétés physico-chimiques de l'éthanol proviennent principalement de la présence du groupement hydroxyle et de la courte chaîne carbonée. Le groupement hydroxyle peut former des liaisons hydrogène, rendant l'éthanol plus visqueux et moins volatil que des solvants organiques de masses moléculaires équivalentes. L'indice de réfraction de l'éthanol est légèrement plus élevé que l'eau ( à ). Le point triple de l'éthanol est observé à pour une pression de .

L'éthanol est un solvant polaire protique. Il est miscible avec de nombreux solvants organiques, comme les solvants chlorés (tétrachlorure de carbone, chloroforme, trichloroéthane, tétrachloroéthylène), les hydrocarbures aliphatiques (pentane, hexane), les solvants aromatiques (benzène, toluène, pyridine), l'acide acétique, l'acétone, l'éther diéthylique, l'éthylène glycol ou encore le nitrométhane. Néanmoins la miscibilité de l'éthanol avec les hydrocarbures aliphatiques tend à diminuer avec l'augmentation de la longueur de la chaîne carbonée de l'alcane et la diminution de la température, la limite de miscibilité étant par exemple de pour le dodécane.

Du fait de la nature polaire du groupement hydroxyle, l'éthanol peut aussi dissoudre des composés ioniques, comme les hydroxydes de sodium et de potassium, les chlorures de magnésium, de calcium et d'ammonium ou encore les bromures d'ammonium et de sodium. Les chlorures de sodium et de potassium ne sont eux que légèrement solubles dans l'éthanol.

La partie apolaire de l'éthanol lui permet de dissoudre des substances hydrophobes, et notamment des huiles essentielles et de nombreux composés odorants, colorants et médicinaux.

L'éthanol peut être utilisé comme solvant dans de nombreuses réactions chimiques lors de synthèses, comme dans les substitutions nucléophiles S1, lors des hydrogénations catalytiques, lors des réactions d'aldolisation, lors des réactions de Wittig, lors des réactions de Diels-Alder ou lors de réactions de diazotation.

L'éthanol est inerte vis-à-vis de la quasi-totalité des surfaces plastifiées de la vie courante, les vernis (hormis les vernis cellulosiques, et ceux à la gomme laque), les peintures acryliques et glycérophtaliques tout en étant un très bon solvant. Ceci en fait un solvant de nettoyage très utilisé seul ou en mélange avec d'autres composés.

Les mélanges eau-éthanol occupent un volume inférieur à la somme des volumes des deux composants pris individuellement. Le mélange d'un volume d'eau et d'un volume d'éthanol donne par exemple un volume équivalent de . La réaction de mélange de l'eau et de l'éthanol est exothermique, et à jusqu'à peuvent être libérées. Le caractère polaire de l'éthanol le rend hygroscopique, à tel point que, pur, il absorbe l'humidité de l'air.

Un azéotrope se forme avec l'eau à %mol d'éthanol et %mol d'eau à pression atmosphérique. Le point d'ébullition de l'éthanol est de et de pour l'eau, mais l'azéotrope bout lui à , ce qui est inférieur aux points d’ébullition de chacun des constituants. Les proportions du mélange azéotropique varient en fonction de la pression. 

L'ajout de quelques pour cent d'éthanol dans l'eau diminue de façon drastique la tension superficielle de l'eau. Cette propriété permet d'expliquer le phénomène des larmes de vin. Lorsque l'on fait tournoyer le vin dans un verre, l'éthanol s'évapore plus rapidement dans le film mince le long des parois du verre. La proportion d'éthanol diminue, donc la tension de surface augmente et le film se transforme en gouttelettes. Ce phénomène est appelé effet Marangoni, et a été décrit et expliqué en 1855 par James Thomson.

Le titre alcoométrique volumique, aussi appelé degré alcoolique, est le rapport entre le volume d'alcool contenu dans le mélange et le volume total de ce mélange à . On l'utilise pour déterminer la proportion d'alcool, c'est-à-dire d'éthanol, dans une boisson alcoolisée. L'unité utilisée pour exprimer le titre est la fraction volumique (%vol) ou degré (noté « ° »).

Les mélanges eau-éthanol contenant plus de d'éthanol sont inflammables à température ambiante, mais en chauffant un mélange contenant moins de d'éthanol peut s'enflammer. La technique de flambage en cuisine fait appel à cette propriété. L'alcool ajouté dans une poêle chaude se consume en flammes et donne une réaction complexe. La température de l'alcool qui brûle peut alors dépasser les et conduit à la caramélisation des sucres présents.

L'éthanol peut être produit industriellement à partir de la pétrochimie par hydratation de l'éthylène, et par fermentation alcoolique de levures ou de cellulose. Le procédé le plus économique dépend principalement du marché pétrolier.

Ce procédé développé dans les années 1970 a complètement supplanté les autres méthodes de production. L'éthanol ainsi produit est utilisé par l'industrie comme matière première ou comme solvant. Il est produit grâce à la pétrochimie, en utilisant l'hydratation par catalyse acide de l'éthylène, suivant la réaction :

Le catalyseur le plus communément utilisé est l'acide phosphorique, adsorbé sur un support poreux comme un gel de silice ou de la célite. Une augmentation de la température aide à déplacer l'équilibre vers la production d'éthanol, suivant le principe de Le Chatelier, et il est donc nécessaire d'utiliser le catalyseur à des hautes températures () sous haute pression de vapeur d'eau pour approcher l'équilibre rapidement. Le produit final est un mélange eau-éthanol contenant entre 10 %m et 25 %m d'éthanol.

Un procédé plus ancien, mais aujourd'hui obsolète, est développé dans les années 1930 par Union Carbide, et sera utilisé durant une grande partie du . L'éthylène est estérifié par l'acide sulfurique pour produire du sulfate d'éthyle, qui est ensuite hydrolysé pour donner de l'éthanol et de l'acide sulfurique, qui sert donc de catalyseur lors de cette réaction :

Ce procédé consomme de grandes quantités d'acide sulfurique et nécessite en entrée un mélange gazeux de bonne qualité.

L'éthanol utilisé dans les boissons alcoolisées est produit par la fermentation alcoolique. De même, la majeure partie du bioéthanol provient du traitement par fermentation de plantes sucrières, comme la betterave et la canne à sucre, ou de céréales, comme le maïs et le blé. Certain types de levures, comme "Saccharomyces cerevisiae", métabolisent les sucres en éthanol et dioxyde de carbone, suivant ces réactions :

La fermentation est le processus de culture de levures dans des conditions favorables pour produire de l'alcool, à une température d'environ . Les souches de levures les plus résistantes peuvent survivre à une concentration d'environ 15 %vol d'éthanol. La toxicité de l'éthanol pour la levure limite la concentration d'alcool qui peut être obtenue par brassage, et des concentrations plus élevées peuvent être obtenues par mutage ou distillation. Lors de la fermentation, des produits secondaires sont formés, comme du glycérol, de l'acide succinique, de l'acide acétique et de l'alcool amylique.

Pour produire de l'éthanol à partir d'amidon, provenant par exemple de graines de céréales, celui-ci doit tout d'abord être transformé en sucres. Lors du brassage de la bière, on laisse une graine germer (le maltage), ce qui permet la production de certaines enzymes, comme les cytases, les amylases, les phosphatase ou les peptidases, nécessaires à la saccharification de l'amidon. Pour le bioéthanol, ce processus peut être accéléré en utilisant de l'acide sulfurique ou en utilisant une amylase produite à partir de champignons.

La fermentation alcoolique peut aussi être obtenue à partir de la cellulose, mais jusqu'à récemment le coût de la cellulase, une enzyme capable de décomposer la cellulose, n'a pas permis à la filière de se développer industriellement. En 2004, la compagnie canadienne Iogen Corporation a construit la première usine basée sur la production d'éthanol à partir de cellulose. Le développement de cette technologie pourrait permettre d'utiliser et de recycler de nombreux déchets végétaux provenant de l'agriculture contenant de la cellulose, comme la sciure de bois ou la paille. D'autres entreprises de biotechnologie développent actuellement des champignons capables de produire de larges quantités de cellulase et de xylanase afin de permettre de convertir d'autres résidus agricoles en cellulose, comme les déchets de maïs ou la bagasse de la canne à sucre.

Les différents procédés de production de l'éthanol produisent des mélanges eau-éthanol. Pour une utilisation dans l'industrie ou comme carburant, l'éthanol doit être purifié. La distillation fractionnée permet de concentrer l'éthanol jusqu'à %vol ( %mol) et forme un mélange azéotropique avec l'eau dont le point d'ébullition est de . Les méthodes usuelles pour obtenir de l'éthanol absolu incluent l'utilisation de desséchants, tels que l'amidon, le glycérol ou les zéolithes, la distillation azéotropique et la distillation extractive.

Différentes qualités d'éthanol sont disponibles pour différents emplois :


L'éthanol est un alcool primaire et un acide faible (pK = ) dont la base conjuguée est l'éthanolate. C'est aussi un bon nucléophile, du fait de la réactivité des doublets non liants de l'oxygène. Sa réactivité, principalement due à la présence du groupement hydroxyle, peut impliquer des réactions de déshydratation, de déshydrogénation, d'oxydation et d'estérification.

L'éthanol est une molécule neutre, et le pH d'une solution d'éthanol dans l'eau est de . L'éthanol peut être converti quantitativement en sa base conjuguée, l'ion éthanolate, en le faisant réagir avec un métal alcalin, comme le sodium :

2 + 2 → 2 + 

ou avec une base forte, comme l'hydrure de sodium :

L'éthanol réagit avec les acides carboxyliques en présence d'un acide fort (catalyse) pour donner des esters éthyliques et de l'eau selon la réaction suivante :

Cette réaction est réversible et l'équilibre est atteint lentement, nécessitant l'utilisation de catalyseurs acides, comme l'acide sulfurique, l'acide chlorhydrique, le trifluorure de bore ou l'acide paratoluènesulfonique. Pour atteindre des rendements corrects, cette réaction, qui est utilisée dans l'industrie à grande échelle, nécessite l'élimination de l'eau du mélange réactionnel au fur et à mesure qu'elle se forme. L'éthanol peut aussi former des esters avec les acides inorganiques, et permet de préparer des composés très utilisés en synthèse organique, comme le sulfate de diéthyle ou le triéthylphosphate . Le nitrite d'éthyle peut de même être préparé à partir d'éthanol et de nitrite de sodium, et a été couramment utilisé comme diurétique ou de remède contre la grippe ou le rhume, avant d'être retiré du marché au début des années 1980.

La réaction de saponification, qui permet de reformer de l'éthanol à partir d'esters éthyliques en présence d'un acide ou d'une base pour donner un carboxylate, est utilisée pour la préparation des savons.

L'utilisation d'acides forts peut permettre de déshydrater l'éthanol pour former de l'éther diéthylique, et plusieurs tonnes d'éther diéthylique sont produites chaque année en utilisant de l'acide sulfurique comme catalyseur. La réaction peut être décrite comme suit :

2 → + (à )

L'éthylène est le produit majoritaire lorsque la température excède :

La combustion de l'éthanol est sans fumée et donne une flamme bleutée. La réaction de combustion forme du dioxyde de carbone et de la vapeur d'eau :

Le pouvoir calorifique (ΔH°) de la combustion de l'éthanol est de à , et la capacité thermique massique (C) est de .

L'oxydation (ou déshydrogénation) de l'éthanol en présence de catalyseurs permet d'obtenir de l'éthanal (ou acétaldéhyde), suivant la réaction suivante :

Dans les hépatocytes du foie, l'enzyme alcool déshydrogénase convertit de même l'éthanol en éthanal. L'éthanal est plus toxique que l'éthanol, et pourrait être responsable de nombreux symptômes de la gueule de bois, même s'il n'est pas présent dans le sang pendant les symptômes. Dans le cerveau, l'alcool déshydrogénase a un rôle mineur lors de la conversion de l'éthanol en éthanal, et c'est l'enzyme catalase qui catalyse principalement cette réaction.

Les dernières étapes de la fermentation alcoolique impliquent la conversion du pyruvate en éthanal par l'enzyme pyruvate décarboxylase, suivie de la réduction (ou hydrogénation) de l'éthanal en éthanol par l'enzyme alcool déshydrogénase, catalysant dans ce cas la réaction opposée. En tant que métabolite endogène l'éthanal est toxique et c'est un cancérigène suspecté. Il endommage les cellules souches hématopoïétiques (chargées de constamment renouveler le sang) ; d'une part il est source de cassures de l'ADN double-brin de ces cellules (ce qui favorise leur déclin et crée des réarrangements chromosomiques), et d'autre part il empêche la bonne réparation ([p53]]) de ces dommages, ce qui provoque des malignités.

L'éthanol réagit avec les acides halogénohydriques pour produire des halogénoalcanes, comme le chloroéthane et le bromoéthane, via une réaction S2 :

Cette réaction nécessite l'emploi d'un catalyseur, comme le chlorure de zinc. Les halogénoalcanes peuvent aussi être synthétisés en utilisant des agents halogénants, comme le chlorure de thionyle ou le tribromure de phosphore :

La réaction haloforme est une réaction chimique consistant à synthétiser un trihalogénométhane, comme le chloroforme (), le bromoforme () ou l'iodoforme (), et est l'une des plus vieilles réaction organique décrite en tant que telle. En 1822, Georges Serullas rapporte la réaction de l'éthanol avec du diiode et de l'hydroxyde de sodium dans l'eau, formant du formiate de soude et de l'iodoforme, appelé alors « hydroïodide de carbone ».

L'éthanol réagit avec l'hypochlorite de sodium pour donner le chloroforme, suivant la synthèse suivante :

De même, le bromoforme, et l'iodoforme sont obtenus respectivement à partir d'hypobromite de sodium et d'hypoiodite de sodium. L'éthanol est le seul alcool primaire qui permet cette réaction.

L'éthanol produit dans le monde est principalement utilisé comme carburant. La quantité d'éthanol peut varier de quelques pour cent dans l'essence en Europe de l'Ouest à 95 %vol dans l'essence au Brésil, où des nouveaux véhicules utilisent la technique "Flex fuel" et peuvent rouler avec ce mélange. Plus de de l'éthanol utilisé comme carburant provient de la fermentation alcoolique. Le carburant E85 distribué en France contient pour sa part d'éthanol et d'essence. Dans l'histoire, associé à l'eau, il était le carburant de bon nombre d'avions (moteurs à combustion interne et fusée) pendant la deuxième guerre mondiale, pour les pays disposant de peu de ressources pétrolifères.

La contenance en éthanol (ou degré d'alcool) des boissons alcoolisées varie avec le mode de production et la matière première. La plupart des boissons alcoolisées peuvent être classées entre boissons fermentées et boissons spiritueuses. Les boissons fermentées principales sont les bières, produites à partir de céréales ou d'autres plantes produisant de l'amidon, les vins et les cidres, produits à partir de jus de fruits, et l'hydromel, produit à partir de miel. Les boissons spiritueuses sont produites par distillation de boissons fermentées. On peut distinguer différents types de boissons spiritueuses, comme le whisky, distillés à partir de graines de céréale fermentée, les eaux-de-vie, distillées à partir de jus de fruits fermentés, ou le rhum, distillé à partir de mélasse ou de jus de canne à sucre. De nombreuses eaux-de-vie et liqueurs sont aussi préparées par infusion de fruits, d'herbes et d'épices dans des boissons spiritueuses, comme le gin qui est créé en infusant des baies de genévrier dans de l'alcool rectifié.

D'autres méthodes peuvent être employées afin d'augmenter le degré d'alcool, comme la solidification fractionnée, qui est utilisée pour la préparation de l'applejack à partir de jus de pomme. Le vin muté est lui préparé en ajoutant de l'eau-de-vie ou d'autres boissons spiritueuses à du vin partiellement fermenté, ce procédé tuant les levures tout en conservant une partie des sucres.

Les boissons alcoolisées sont utilisées en cuisine pour leurs parfums et du fait que l'alcool dissous les composés odorants hydrophobes. On utilise aussi l'éthanol de ces boissons pour produire du vinaigre, de la même manière que l'éthanol industriel est utilisé pour la production d'acide acétique.

L'éthanol est un intermédiaire de synthèse important dans l'industrie chimique. Il est notamment utilisé lors de la préparation des halogénures d'éthyle, des esters éthyliques, des amines éthyliques, du diéthyléther, de l'acide acétique, et dans une moindre mesure du butadiène.

L'éthanol est utilisé dans le domaine médical dans les compresses comme antiseptique. Il est aussi employé dans les solutions hydro-alcooliques à une concentration d'environ 60 %vol. L'éthanol tue les organismes en dénaturant leurs protéines et en dissolvant leurs lipides. Il est efficace contre la plupart des bactéries et champignons, de nombreux virus, mais est inefficace contre les spores.

L'éthanol est miscible avec l'eau, et est un bon solvant. On peut le trouver dans les peintures, les teintes industrielles, les vernis traditionnels, les alcoolature, les marqueurs, et dans les produits cosmétiques comme les parfums et les déodorants.

L'éthanol absolu est parfois injecté dans des tumeurs afin de provoquer leur nécrose. Il n'a toutefois pas une activité ciblée puisqu'il provoque indifféremment la nécrose des tissus sains et cancéreux.

L'éthanol est parfois utilisé pour traiter les intoxications au méthanol ou à l'éthylène glycol. Dans ces cas, l'éthanol entre en compétition avec les autres alcools pour être métabolisé par l'enzyme alcool déshydrogénase, diminue ainsi les dérivés toxiques résultant de leur métabolisation en aldéhydes et acides carboxyliques, et réduit les effets toxiques dus à la cristallisation de l'oxalate de calcium (issu de éthylène glycol) dans les reins.

Avant le développement des médecines modernes, l'éthanol était utilisé pour différents usages médicaux, et il était notamment employé pour le traitement de la dépression et comme anesthésique. Il est aussi connu comme pouvant servir de sérum de vérité.

Comme carburant, il a été utilisé dans les moteurs de fusée combiné à de l'oxygène liquide. Durant la Seconde Guerre mondiale, la roquette « V2 » allemande utilisait de l'éthanol mélangé à 25 %vol d'eau pour diminuer la température de la chambre de combustion. Le missile balistique « PGM-11 Redstone » américain, inspiré par la conception du V2 allemand, utilisait lui aussi de l'éthanol mélangé à de l'oxygène liquide. Les carburants à base d'alcool pour les fusées et les missiles sont par la suite tombés en désuétude alors que de nouveaux carburants étaient développés.

Les deux voies principales du métabolisme de l'éthanol dans le foie sont l'alcool déshydrogénase et le . À de faibles concentrations d'éthanol, l'alcool déshydrogénase oxyde l'éthanol en acétaldéhyde, mais à de plus fortes concentrations, ou lors de prises répétées d'éthanol, c'est le système d'oxydation de l'éthanol microsomal qui prédomine.

Dans la pharmacopée européenne, « éthanol » désigne l’éthanol anhydre, c'est-à-dire pur à , en opposition à l'éthanol à 95 %vol ou 70 %vol que l'on peut trouver en pharmacie. À cela, s'ajoute la qualité « alcool Ph. Eur. » (pour "Pharmacopée européenne"), qui désigne une qualité d'éthanol dont on a quantifié de nombreuses traces et impuretés.

La prise de boissons alcoolisées entraine différents effets, l'alcoolisme étant la consommation excessive de boissons contenant de l'éthanol qui entraîne une dépendance, ce qui classe l'éthanol parmi les drogues. Celle-ci serait la plus nocive des drogues pour les sociétés.

L'alcoolémie est la quantité d'éthanol dans le sang ; elle est généralement exprimée en grammes par litre de sang. Un taux d'alcool dans le sang dépassant peut entraîner la mort, et le taux devient létal au-dessus de . Des faibles doses d'éthanol, en dessous de , provoquent un sentiment d'euphorie, les personnes devenant plus loquaces, moins inhibées, et montrant une capacité d'analyse diminuée. À plus hautes doses, au-dessus de , l'éthanol agit comme dépresseur sur le système nerveux central, les symptômes impliquant un ralentissement de la cognition, une diminution des capacités sensorielles et des fonctions de motricité, perte de la conscience, jusqu'à la mort.

L'éthanol est un dépresseur du système nerveux central, et il est considéré comme une drogue psychotrope. Il agit sur le système nerveux central en interagissant principalement avec les récepteurs GABA, augmentant l'effet d'inhibiteur du neurotransmetteur acide γ-aminobutyrique (ou GABA). Le GABA est un ralentisseur de l'activité des neurones (à l'opposé du glutamate qui est lui un accélérateur de l'information) ; ainsi, en se fixant sur les récepteurs du GABA, l'éthanol exagère le ralentissement du cerveau. Il agit donc comme un modulateur allostérique positif. L'éthanol agit aussi sur les récepteurs de la sérotonine, du glutamate, de l'acétylcholine et de la dopamine.

Une consommation prolongée d'éthanol peut ainsi provoquer des lésions permanentes au cerveau et aux autres organes. Le sevrage alcoolique peut provoquer divers symptômes, comme le trouble du déficit de l'attention, une augmentation de la transpiration, de la tachycardie, des trémulations (tremblement des extrémités), parfois des nausées ou des vomissements, une déshydratation, des malaises, de l'hypertension artérielle. Parfois il s'accompagne d'une crise d'épilepsie, d'hallucinations visuelles, tactiles ou auditives, c'est le "delirium tremens" dans sa forme la plus sévère. Éventuellement, et dans de rares cas, des douleurs du niveau de la mâchoire jusqu'au crâne peuvent apparaître. Il a aussi été mis en évidence que l'éthanol entrainait des modifications visibles à l’œil nu de la taille de la matière grise.

En tant que tel, l'éthanol est un nutriment. Dans le corps humain, il est métabolisé en acétaldéhyde par l'enzyme alcool déshydrogénase. L'acétaldéhyde est par la suite converti par l'acétaldéhyde déshydrogénase en acétyl-coenzyme A, qui est le produit final des métabolismes des glucides et des lipides. Cependant, l'acétaldéhyde est en lui-même bien plus toxique que l'éthanol, et est en partie responsable de la plupart des effets cliniques dus à l'alcool, comme la gueule de bois. Il a été notamment démontré qu'il augmente le risque de cirrhose du foie et est lié à de nombreuses formes de cancer.

L'éthanol est classé comme agent tératogène, et peut induire le syndrome d’alcoolisation fœtale, qui est une intoxication alcoolique de l'embryon ou du fœtus due à la consommation d'alcool par la mère pendant la grossesse, et qui perturbe le développement des organes.

La consommation régulière d'alcool est aussi un facteur contribuant à l'augmentation dans le sang des triglycérides qui favorise l'apparition de maladies cardio-vasculaires.

L'éthanol dans les boissons alcoolisées est considéré comme cancérigène certain et fait donc partie des cancérogènes du groupe 1 du CIRC.

L'éthanol peut augmenter le phénomène de sédation causé par les médicaments dépresseurs agissant sur le système nerveux central, comme les barbituriques, les benzodiazépines, les opioïdes, les phenothiazines et les antidépresseurs.




</doc>
<doc id="6544" url="https://fr.wikipedia.org/wiki?curid=6544" title="Éthylène">
Éthylène

L'éthylène, ou, selon la nomenclature de l'IUPAC, éthène, est un hydrocarbure insaturé. On peut aussi le trouver sous l'appellation R1150.

L'éthylène est le plus simple des alcènes.

C'est un gaz incolore, volatil, de densité proche de l'air avec lequel il forme des mélanges explosifs. 
À partir de , il s'enflamme et brûle avec une flamme claire ; chaleur de combustion : kJ/kg.

C'est un gaz très réactif.

L'éthylène peut être hydraté en éthanol par addition d'une molécule d'eau en milieu acide.

Par halogénation, l'éthylène conduit au dibromoéthane.

L'éthylène est hydroxylé en 1,2-éthanediol (glycol) en présence de catalyseurs.

La polymérisation de l'éthylène en polyéthylène basse densité s'effectue par une polymérisation radicalaire à haute pression. Quant à la production de polyéthylène haute densité, ou basse densité linéaire, elle est possible grâce à la polymérisation coordinative en utilisant des catalyseurs type Ziegler-Natta, métallocène ou post-métallocène.

La production globale d´éthylène a été de 138 millions de tonnes en 2010 et 141 millions de tonnes en 2011. L'Europe de l´ouest en a produit 25 millions de tonnes en 2011 ; dont 2,46 millions de tonnes en Belgique ; 3,37 millions de tonnes en France ; 5,74 millions de tonnes en Allemagne ; 2,17 millions de tonnes en Italie ; 3,96 millions de tonnes aux Pays-Bas ; 1,43 million de tonnes en Espagne et 2,85 millions de tonnes au Royaume-Uni.

Dans l'industrie pétrochimique, l'éthylène est obtenu :

Les principales sociétés fabriquant l'éthylène sont la Sabic, Dow Chemical, ExxonMobil Chemical, Royal Dutch Shell, Sinopec et Total. Technip est le leader mondial de la conception d’unités de production d’éthylène, de la conception à la construction et la mise en service.

Les sous-produits gazeux obtenus sont : le dihydrogène, le méthane, l'acétylène, l'éthane, le propadiène, le propène. Ces gaz sont ensuite séparés.

L'éthylène est à la base d'un grand nombre de molécules dans l'industrie chimique. Avec ses dérivés immédiats, il est à la source d'un grand nombre de polymères et de matières plastiques.

Les produits issus de l'éthylène sont entre autres :
le chlorure de vinyle, 
l'éthylbenzène, 
l'oxyde d'éthylène, 
l'éthanol (ou alcool éthylique), 

Le monomère peut être utilisé directement pour produire du polyéthylène.

Le chlorure de vinyle est polymérisé en polychlorure de vinyle (PVC), matière plastique bien connue, et l'une des plus anciennes.

L'éthylbenzène donne deux produits : du styrène et du caoutchouc SBR (Styrene-Butadiene-Rubber).

Le styrène est polymérisé en polystyrène, avec lequel on produit du polystyrène expansé ou des matières plastiques.

À partir du caoutchouc SBR, on obtient d'autres copolymères, comme le SBS (Styrene-Butadiene-Styrene).

L'oxyde d'éthylène ((CH)O) donne de l'éthylène-glycol, qui lui-même, combiné à l'acide téréphtalique, fournira des fibres polyesters. 

L'éthanol peut être utilisé comme simple solvant.

L'oxyde d'éthylène ((CH)O) est un produit très instable à cause de «l'insaturation» de sa structure chimique. Il explose immédiatement en présence d'oxygène, c’est-à-dire qu'il cherche à combler le vide atomique de sa structure en captant les atomes d'oxygène de l'air. En 1957, un réacteur pilote d'oxyde d'éthylène a explosé à Anvers (Belgique), faisant plusieurs morts. Le souffle de l'explosion a déplacé un spectromètre de masse (de plus de cent tonnes) de plusieurs dizaines de mètres. L'éthylène-glycol sert également dans la fabrication des antigels.

Le PTFE (Poly-Tétra-Fluor-Éthylène), plus connu sous son nom commercial, le Téflon, est un polymère technique utilisé industriellement dans une grande variété d'applications - il est notamment l'un des meilleurs isolants électriques connus à ce jour, est chimiquement inerte, et a des propriétés tribologiques hors du commun. Il est également connu du grand public comme joint de plomberie ou comme revêtement anti-adhérant pour les poêles de cuisson.

C'est aussi une phytohormone (hormone végétale) aux effets multiples.

Le rôle d'hormone végétale de l'éthylène a été découvert en 1901 : on remarqua que les feuilles des plantes situées à proximité des lampadaires (à bec de gaz) tombaient prématurément.

En 1910, on s'aperçoit qu'un fruit confiné mûrit plus vite qu'un fruit à l'air libre. On fait alors un premier rapprochement avec l'éthylène.
En 1934 on découvre les voies métaboliques de l'éthylène.

En 1960, par chromatographie en phase gazeuse, on arrive à doser l'éthylène émis par les plantes.

En 2015, Voesenek et al. supposent que la capacité de production d'éthylène a été sélectivement perdue au cours de l'évolution chez certaines plantes terrestres qui sont devenues entièrement aquatiques, peut-être car l'éthylène pourrait interférer avec la croissance dans les environnements perpétuellement subaquatiques.

L'éthylène a pour origine la méthionine.

Dans le cycle de Yang la méthionine est transformée en S-adénosylméthionine (SAM) par la "SAM Synthétase" :

Méthionine + ATP → SAM + PPi + P ("SAM Synthétase")

La SAM est ensuite dégradée en 5'méthylthioadénosine (qui est réutilisé par le cycle de Yang) et en acide 1-aminocyclopropane-1-carboxylique (ACC) par l' "ACC Synthase". Une partie de l'ACC est ensuite convertie en éthylène grâce à l"'ACC Oxydase", le reste va se conjuguer avec du N-Malonyl pour donner du N-Malonyl ACC stocké en une réserve métabolique qui pourra être hydrolysée en fonction des besoins de la plante.

Le facteur limitant est la production d'ACC (Acide 1-AminoCyclopropane-1-Carboxilique) par l"'ACC Synthase". Cette hormone est présente en quantité très faible dans le cytosol, dans les fruits en maturation (au moment où l'éthylène est le plus abondant), elle représente environ 0,0001 %. Sa production est régulée par des facteurs environnementaux comme une blessure, le froid, un stress hydrique, une diminution de l'O (immersion dans l'eau); ainsi que par des facteurs endogènes : l'auxine ou les cytokinines, mais aussi l'éthylène. Pour cela, la production d'éthylène est un phénomène autocatalytique.

L'AOA (acide aminooxyacétique) et l'AVG (Aminoéthoxyvinyglycine) bloquent le fonctionnement de l"'ACC Synthase".

Une absence d'oxygène (anaérobie), des fortes températures (sup. à ), des ions cobalt Co, inhibent le fonctionnement de l"'ACC Oxydase".

Le nitrate d'argent AgNO, le thiosulfate d'argent Ag(SO) ou un milieu enrichi en CO, inhibent en aval l'action de l'éthylène.

Le cyclopropène de méthyle (1-MCP) se fixe de façon presque irréversible sur les récepteurs éthylène, qui transmettent alors un signal conduisant à l'inactivité du système de perception, malgré la présence de molécules d'éthylène sur des récepteurs proches.

L'éthylène module de nombreux métabolismes (réponses des plantes aux stress biotiques et abiotiques), est impliqué dans les étapes de floraison et stimule la maturation de nombreux fruits. Cette molécule a des effets variés parce qu'elle est très simple et donc peu spécifique. 

L'éthylène est un catalyseur essentiel de la maturation des fruits. Par exemple, un avocat ne mûrit pas sur l'arbre mais six à huit jours après la cueillette. On observe alors un pic de production d'ACC, puis d'éthylène qui déclenche la maturation du fruit.
Un fruit dont la maturation est dépendante de l'éthylène est classé comme fruit climactérique.

La banane produit de l'éthylène pour mûrir. Pour empêcher le murissement, le froid ne suffit pas. Il faut aussi ventiler pour éviter l'accumulation d'éthylène. Quand on veut redémarrer le murissement, il suffit de diffuser de l'éthylène.

On peut ajouter du permanganate de potassium dans les sachets contenant des bananes ou des tomates afin d'oxyder l’éthylène en éthylène glycol ce qui arrête le mûrissement et prolonge la durée de vie des fruits jusqu'à 4 semaines sans nécessiter de réfrigération.

La sénescence des organes est un processus génétiquement programmé influençant l'âge physiologique des entités vivantes. Un apport exogène d'ACC ou d'éthylène entraîne une sénescence prématurée, alors qu'un apport exogène de cytokinine retarde le processus.

Une augmentation de la production d'éthylène est associée à une perte de chlorophylle des feuilles, une dégradation des protéines et des ARN, une perte de pigmentation des fleurs, et autres symptômes de vieillissement.

"/!\ Ne pas confondre avec l'acide abscissique."

Les cellules des zones nécessitant une abscission répondent spécifiquement à l'éthylène. Une multitude d'enzymes hydrolytiques telles que des "pectinases" ou des "polygalacturonases" (qui dégradent l'acide galacturonique) sont alors stimulées, lysent les parois cellulaires et fragilisent la structure du végétal. Le plus souvent un agent extérieur tel que le vent, donne le coup de grâce et fait tomber l'organe.

Les jeunes feuilles produisent de l'auxine qui les insensibilise à l'éthylène. Après le développement de la feuille, la production d'auxine diminue puis s'arrête : les cellules du pétiole sont alors exposées à des concentrations de plus en plus fortes d'éthylène. Au bout d'un certain temps les zones d'abscission répondent par la synthèse d'enzymes hydrolytiques.

De très fortes concentrations d'auxine inhibent la production d'éthylène et donc la chute des feuilles, alors qu'avec de faibles concentrations d'auxine, l'inhibition de la synthèse d'éthylène n'a plus lieu, ce qui permet l'augmentation de la concentration d'éthylène et donc la chute des feuilles.

Les racines perçoivent l'inondation par une forte diminution de la concentration en dioxygène dans le milieu. L'anoxie stimule la production de SAM ("SAM Synthétase") et entraîne une augmentation de la teneur en ACC car l"'ACC Oxydase" ne fonctionne pas : elle ne peut pas oxyder sans oxygène ! L'ACC excédentaire des racines finit par se retrouver dans les feuilles pour être transformé en éthylène. C'est cet éthylène qui est responsable des mouvements d'épinastie.

Cette hormone inhibe la floraison sauf chez certaines espèces comme la mangue ou l'ananas, chez lesquels on synchronise la floraison des fruits en apportant de l'éthylène sur l'arbre.

L'éthylène peut changer la nature des organes floraux. Chez les espèces monoïques, c'est une hormone féminisante.




</doc>
<doc id="6545" url="https://fr.wikipedia.org/wiki?curid=6545" title="Depeche Mode">
Depeche Mode

Depeche Mode est un groupe de new wave britannique, originaire de Basildon (comté d'Essex). Formé en 1979, le groupe apparait au sein du courant de la synthpop et devient rapidement influent et populaire sur la scène internationale. Son nom provient d'un magazine français, "Depeche Mode". Le groupe est repéré par l'agent de Soft Cell, puis dirigé par Daniel Miller qui le fait signer sur son label Mute Records en 1981. Le groupe connaît son premier succès en Europe à partir du mois de septembre de cette même année avec ' et aux États-Unis en 1984-1985 avec le single '.

Le succès de leur synthpop au style très empreint de musique industrielle est constant jusqu'en 1990 avec l'album "Violator", incluant les titres ', ' et surtout '. Les années 1990 sont marquées par la dépendance à la drogue, la surdose et la tentative de suicide du chanteur principal, Dave Gahan, qui n'est désintoxiqué qu'en 1996, et par le départ d'Alan Wilder en 1995. Le groupe, qui ne comporte plus que trois membres, continue à sortir des albums (' en 1997, ' en 2001, ' en 2005, ' en 2009, ' en 2013 et "Spirit" en 2017).

La quasi-totalité des chansons du groupe est composée par Martin L. Gore, sauf celles du premier album ("Speak and Spell") qui sont majoritairement l'œuvre de Vince Clarke, rapidement parti fonder Yazoo puis Erasure. Depuis 2005, le chanteur Dave Gahan participe à l'écriture de certains morceaux. Martin L. Gore œuvre beaucoup pour que Depeche Mode ne soit pas uniquement considéré comme un groupe « de synthés », en utilisant notamment la guitare.

Le groupe se classe cinquante fois dans le "UK Singles Chart", et plusieurs albums ont été classés numéro un au Royaume-Uni, aux États-Unis et dans d'autres pays européens. Selon Mute Records, en 2008 le groupe compte 75 millions d'albums vendus dans le monde et plus de 100 millions de disques en incluant les singles.

C’est à Basildon en 1977 (dans l'Essex), que Vince Clarke et Andrew Fletcher, alors adolescents, décident de créer un groupe et centrer leur créativité sur un nouvel instrument, le synthétiseur, peu utilisé à l’heure où le Royaume-Uni résonne au son du punk rock. Ils sont rejoints dès 1978 par un ami de lycée, Martin L. Gore et fondent Composition of Sound. Le groupe trouve en Dave Gahan, repéré lors d'une audition alors que ni Gore ni Clarke ne se voient comme chanteur principal, la voix que le trio recherchait. Ce dernier est d’ailleurs à l’origine du nouveau nom du groupe inspiré par un magazine français : "Dépêche Mode". Les Anglais, désireux d'en comprendre la signification s'amusent à traduire ce nom par ou bien encore , commettant ainsi une erreur de sens car ils confondent alors le terme « dépêche » ("dispatch" en anglais, mot d'ailleurs issu de l'ancien français "despeche") avec le verbe « se dépêcher » ( en anglais).

Leurs premières compositions sont refusées par des maisons de disques qui n’apprécient guère la surcharge de synthétiseurs utilisés (instruments de prédilection du groupe à ses débuts).

Stevo Pearce, manager du groupe Soft Cell et fondateur du label Some Bizzare Records, les repère et fait paraître leur titre ' sur une compilation intitulée ' (1981) où ils côtoient alors d'autres futures formations des années 1980 telles The The et Soft Cell. Puis c’est lors d’un concert dans un club londonien que Daniel Miller repère le groupe et décide de le signer sous son nouveau label : Mute Records. En 1981, Depeche Mode sort son premier single ', qui se classe dans le Top 75 anglais, suivi peu après du titre ' qui devient le premier vrai succès du groupe en Angleterre ; en fin d'année paraît le troisième single qui devient un tube au Royaume-Uni comme au niveau international : '. Ces premiers titres sont extraits de l’album ' dont les compositions sont principalement de Vince Clarke (Martin L. Gore n’y est l’auteur que de deux chansons). Les synthétiseurs y tiennent une place prépondérante sur fond de boîtes à rythme. Mais aussitôt le succès acquis, Clarke quitte le groupe et part fonder le duo à succès Yazoo avec Alison Moyet (une copine du lycée de Basildon), puis l'éphémère The Assembly. Vince Clarke se stabilisera finalement en 1985 en créant son groupe Erasure qui, depuis lors, publie régulièrement des albums dont certains obtiendront un grand succès international, mais ne susciteront que peu d'intérêt en France.

Depeche Mode devient alors un trio sans parolier, et son avenir paraît compromis. Les trois acolytes décident néanmoins de poursuivre l'aventure : Martin L. Gore est désormais l'auteur/compositeur du groupe qui recrute finalement Alan Wilder en 1982 sur petite annonce parue dans le '. Celle-ci demande un homme de moins de 21 ans et un véritable musicien. Wilder, qui triche sur son âge, est un pianiste expérimenté. Il est avant tout engagé pour assurer les lives, c'est la raison pour laquelle il ne participe pas à l'élaboration de ', deuxième album du groupe qui paraît fin 1982 et est marqué par le single '. Ce titre, sorti plus tôt dans l'année, devient alors le plus grand succès de DM dans son pays d'origine, atteignant la dans le Top 40. Début 1983, paraît le single ' (absent de tout album studio), celui-ci propose un son plus mûr que les précédentes compositions de Gore, augurant un changement de direction musicale pour Depeche Mode.

L'apport artistique d'Alan Wilder est perceptible dans l'album ' (1983) où il signe intégralement deux titres (' et '). Dans ce troisième opus, apparaissent les premiers samples nourris de la musique industrielle allemande et les sons deviennent plus travaillés, confirmant donc l'évolution musicale de la formation anglaise. Quant aux paroles, elles prennent des tournures plus politiques : le premier extrait de cet album et tube ' traite par exemple, sur un mode ironique, des dérives du capitalisme notamment via les grandes majors du disque. " connaît un vrai succès au Royaume-Uni (y étant certifié disque d'or fin 1983) et se classe dans plusieurs pays européens, confirmant l'impact grandissant de Depeche Mode au niveau international ; notamment en Allemagne, où leurs albums étaient produits au Studio Hansa.

L'amour, la religion, la sexualité, l’ennui deviennent les thèmes de prédilection des compositions de Martin L. Gore, dans un contexte musical qui confirme ses goûts pour la musique industrielle allemande (notamment Kraftwerk) qui nourrit dès lors les samples du groupe : bruits lourds, métalliques et percussions froides martèlent les titres de la formation anglaise. Et c'est dans ce cadre que paraît au printemps 1984 le single qui donne définitivement à Depeche Mode son envergure internationale : '. Ce titre permet au groupe de connaître un grand succès un peu partout sur la planète (notamment aux États-Unis où il atteint le Top 20 quelques mois plus tard) et annonce le prochain album, ', qui paraît finalement à l'automne de la même année. Ce single constitue aussi pour la formation de Basildon son tout premier titre à se classer (en Allemagne). Par ailleurs, fort du succès rencontré par ' en Amérique du Nord, un album baptisé du même nom, incluant ce morceau et d'autres singles est publié exclusivement aux États-Unis et au Canada durant l'été 1984, afin de mieux faire connaître la genèse musicale du groupe au public nord-américain, qui le découvre alors. Quant à l'album ', il rencontre un grand succès grâce à des singles à l'impact international certain ; ainsi après ', c'est le titre ', paru à la fin de l'été 84, qui connaît à son tour les honneurs des hit-parades. Depeche Mode s'engage alors dans une tournée de plusieurs mois, de l'automne 84 à l'été 85, qui mène le groupe jusqu'au Japon.

Ce succès est renforcé par la parution mondiale, fin 1985, d'une première compilation officielle regroupant les singles édités depuis les débuts de Depeche Mode. Elle contient notamment le tube ', sorti plus tôt dans l'année et classé au Top 20 dans de nombreux pays. En Europe, de la Scandinavie à l'Italie, le groupe acquiert une grande popularité - comme notamment en France et en Allemagne - devenant l'une des formations new-wave anglophones les plus plébiscitées du moment avec, entre autres, Tears for Fears, Duran Duran, Eurythmics, Simple Minds, Talk Talk ou encore The Cure. Durant cette période, le groupe, en raison de son succès, est contacté afin de participer au ', en juillet 1985. Cependant, il refuse de s'y produire. Martin Gore estime notamment que les artistes participant à ce type de concert cherche davantage à se faire valoir qu'à s'investir réellement au niveau caritatif, et que Depeche Mode n'a donc finalement pas sa place lors d'un tel événement.

D'ailleurs, durant ce même été 1985, le groupe participe à ", un festival exceptionnel organisé notamment à l'initiative des Ministères grec et français de la culture et composé d'une série de concerts regroupant principalement des formations tendance new wave (en dépit du nom du festival) célèbres à ce moment-là. On y retrouve donc Depeche Mode mais aussi, entre autres, Talk Talk, The Cure, The Stranglers, Culture Club et également le groupe français Téléphone. C'est aussi durant cette période que la formation de Basildon, qui rencontre désormais un réel engouement international, voit son succès paradoxalement s'affaiblir dans son pays d'origine, où plus aucun de ses singles n'atteint le Top 10, et ce, durant plusieurs années.

Au printemps 1986, paraît le nouvel album, ', dont le premier extrait, ', confirme l'orientation du groupe vers une musique plus sombre composée sur la base d'échantillonnages divers. Cet album asseoit la réputation de la formation anglaise au Royaume-Uni (même si aucun des singles extraits n'y atteint les premières places) et lui donne le statut de groupe culte en Amérique du Nord, où DM est considéré comme une formation underground ; alors qu'elle est parfois perçue comme plus commerciale en Europe (ses singles sont édités en une multitude de remixes, pouvant accroître leurs ventes). Cela dit, l'approche électronique et expérimentale du groupe permet de développer de nombreuses variations de ses titres. Depeche Mode collabore avec de nombreux arrangeurs/producteurs et disc jockeys et offre à ses fans une multitude de reflets sonore.

Le groupe sort un album par an et sa popularité s’accroît de manière importante. Cependant, le désir de s’affranchir de l'image de garçons coiffeurs à synthés, que certains critiques attribuent à ses membres, se fait sentir. C’est notamment pour cette raison qu'ils sollicitent le photographe hollandais Anton Corbijn pour refaçonner leur image. À partir de 1986, l'intervention de Corbijn sur les clips et les photographies devient déterminante pour l'imaginaire visuel de la formation anglaise. .

La première vidéo réalisée par le photographe hollandais illustre le troisième titre extrait de ', ' qui paraît à l'été 1986 et devient rapidement un tube.

1987 marque une étape de plus dans la carrière de Depeche Mode. Le groupe publie au printemps de cette année-là le single ', qui rencontre un réel succès international et annonce le nouvel album à venir. Enregistré en France au studio Guillaume Tell de Suresnes sous la houlette de David Bascombe – Daniel Miller et Gareth Jones préférant s'éclipser - ce sixième album paraît finalement à l'automne 87. Intitulé ', il reflète la nouvelle approche musicale de la formation anglaise et s'ouvre sur ', un rock synthétique virant à l'onirisme noir, qui devient avec le temps l'hymne des concerts de Depeche Mode à travers le monde. Il donne le ton d'un album abouti qui entre pour la première fois de l'histoire du groupe dans le Top 40 américain, obtenant en seulement quelques mois la certification disque d'or (pour plus de exemplaires vendus) puis finalement disque de platine quelques années après. . Cet album et ses singles, toujours mis en image par Corbijn (', ' et ') connaissent ainsi au niveau mondial des scores de ventes importants, faisant de DM l'un des groupes anglophones les plus en vue du moment (avec U2, The Cure ou encore INXS). Cependant, l'accueil au Royaume-Uni y est toujours plus mitigé.

Ce succès international autorise une grande tournée qui passe notamment par les États-Unis et que le cinéaste américain D.A. Pennebaker immortalise dans le documentaire "101", filmé le 18 juin 1988 au stade Rose Bowl de Pasadena devant spectateurs. Depeche Mode est devenu . Le titre "101" (suggéré par Alan Wilder) fait allusion aux 101 concerts donnés pendant cette tournée. Le documentaire de Pennebaker suit en parallèle huit fans transportés pendant dix jours à travers les États-Unis dans un autobus fourni par la production, ainsi que le groupe lors de ces mêmes déplacements, des interviews, et durant ses concerts.

Un album live (le premier pour DM), reflet de cette tournée et également intitulé "101", paraît en mars 1989 et devient .

Depeche Mode connaît désormais un grand succès commercial, tout en finissant de convaincre une presse jusqu'alors plutôt réservée. Sitôt leur tournée 101 achevée, Martin L. Gore soumet ses nouvelles compositions de ce qui est leur album le plus célèbre à ce jour.
En 1989, les quatre musiciens rentrent en studio à Milan pour travailler sur les maquettes très épurées de Martin L. Gore. David Bascombe n'étant pas disponible, le groupe fait alors appel au producteur Flood afin d'enregistrer ce nouvel album, où l’on retrouve les titres ', ', ', ', ' et surtout le single qui reste l'un des plus célèbres du répertoire de DM, et son plus gros hit à ce jour : '. Initialement composé comme une ballade (la démo minimaliste de Martin L. Gore était composée sur un harmonium), Flood et Alan Wilder y décèlent tout de suite un fort potentiel : ils demandent alors à Gore de composer une ritournelle mélodique supplémentaire à la guitare (qui sera déclinée à différentes octaves et jouée également aux claviers), le rythme est accéléré et un soin tout particulier est apporté à la production de la chanson (pour preuve, le titre est le seul qui fut mixé par Daniel Miller et Flood et non par le disc jockey français François Kevorkian qui mixe le reste de l'album). En tant qu'auteur, Gore déclare que ses dix thèmes favoris durant cette période sont : .

Le premier extrait de l'album ' paraît sur les ondes dès août 1989, et détone. Avec une chanson rock construite autour d'un riff de guitare blues (façon John Lee Hooker), Depeche Mode est là où on ne l'attendait pas. Cet inattendu mélange au rythme lourd, à la mélodie simple et marqué d'un slogan – davantage qu'un refrain – donne le ton. Ne perdant pas de vue le champ d'expérimentation électronique, ' se conclut par une phrase instrumentale où les programmations semblent livrées à elles-mêmes, percutées par un rythme lourd. Ce titre est une idée de Gore qui, en lisant une biographie d'Elvis, a appris que Priscilla Presley appelait son mari « mon Jésus personnel ». Par ailleurs, les couplets, évoquant une conversation téléphonique , font en fait écho à l'existence aux États-Unis d'une ligne téléphonique où l'on pouvait joindre un prêtre pour se confesser. Mute n'espérait pas un tel engouement pour ', qui connaît un grand succès international (notamment en Europe et aux États-Unis), misant davantage sur la sortie d', prévue stratégiquement au moment de la parution de l'album, début 1990 ; baptisé ', celui-ci paraît finalement au mois de mars de cette année-là et, propulsé par les ventes records de ses singles (notamment ', premier titre de DM à se classer au Top 10 américain et single du retour du groupe dans le Top 10 anglais), devient l'album le plus connu de Depeche Mode, écoulé à plus de 10 millions d'exemplaires, dont près de 4 millions pour les seuls États-Unis.

Un événement particulier illustre cette immense popularité acquise, notamment sur le sol américain. Le 20 mars 1990, lors d'une journée de promotion à Beverly Hills, fans se pressent pour obtenir un autographe, mais cela tourne vite à l'incident public. Rapidement, les vitres du disquaire où se trouve le groupe - qui sera promptement évacué - cèdent sous la pression de la cohue en proie à une hystérie collective, et les autorités, craignant une émeute (évitée de peu), « envoient sur place plusieurs hélicoptères [et] quatre divisions de police » afin de disperser la foule ; plusieurs personnes sont blessées lors de cet incident inédit qui est largement relayé par les médias, locaux comme nationaux.

Afin de promouvoir l'album, Depeche Mode s'embarque pour plusieurs mois dans le , qui démontre, une fois de plus, la grande renommée acquise par la formation anglaise. Celle-ci joue par exemple dans plusieurs stades aux États-Unis : ainsi tickets sont vendus en l'espace de quelques heures pour un concert au Giants Stadium (dans le New Jersey) et tickets sont écoulés en seulement une demi-heure pour un show au Dodger Stadium (à Los Angeles). En 1991, Depeche Mode contribue, avec le titre "", à la bande originale du film "Jusqu'au bout du monde", de Wim Wenders.

À ce moment-là de sa carrière, le groupe est au faîte de sa gloire, autant plébiscité par le public que par la critique ; les années qui suivent vont cependant se révéler plus chaotiques.

Après l’immense succès de ', DM revient en 1993 avec l'album '. Les titres ', ', ' et ' sont des morceaux encore plus rock, plus bruts, le tout produit de nouveau par Flood et Wilder. Choristes de gospels et sections de cordes interviennent sur un album que le groupe sait très attendu. Et si le succès est à nouveau au rendez-vous (le disque se classe dès sa sortie 1 des ventes au Royaume-Uni, dans de nombreux pays d'Europe ainsi qu'aux États-Unis), l'accueil reste mitigé. Le premier extrait ' ne se vend pas autant aux États-Unis que sur le continent européen où il devient d'ailleurs l'un des singles les mieux classés de l'histoire du groupe ; Depeche Mode décide alors de promouvoir la face B du maxi-single, ', sans grand résultat. Sans atteindre les chiffres de "Violator", l'album est tout de même, au final, une réussite commerciale, se vendant à plusieurs millions d'exemplaires à travers le monde (dont plus d'un million et demi aux USA). La tournée qui suit, puis , qui dure quatorze mois se révèle très éprouvante : énièmes tensions au sein du groupe, prises excessives de drogues et d'alcool, et Fletcher, victime d’une dépression, est remplacé pour quelques dates.

Le bilan au milieu des années 1990 n'est guère réjouissant malgré leur immense popularité : Gahan est devenu un véritable junkie vivant presque avec ses dealers, Gore s'isole, et Fletcher essaie de maintenir la cohésion d'un groupe qui voit l'un de ses membres quitter l'aventure. En effet, estimant que son travail n’est pas reconnu à sa juste valeur et éprouvé par les tensions qu’il ressent au sein du groupe, Alan Wilder décide de le quitter en 1995. Il se consacre alors à plein temps à son projet solo Recoil. DM perd son pilier technique et créatif. La même année, Dave Gahan est hospitalisé à la suite d'une tentative de suicide. Rétabli, il retrouve les deux membres restants début 1996 pour enregistrer un nouvel album mais, victime d’une overdose peu de temps après et condamné par la justice californienne à un an de mise à l'épreuve, il entreprend une nouvelle cure de désintoxication.

En 1997, Depeche Mode réapparaît avec le single ', qui annonce la sortie d', un album du « retour sur certains acquis, un pont solide entre les recettes du passé et les opportunités du présent ». Le producteur Tim Simenon est aux commandes et assure la continuité. Pour mener à bien la production, il fait appel à toute son équipe studio (ses acolytes de Bomb The Bass) pour pallier l'absence d'Alan Wilder qui occupait une place essentielle lors des précédents enregistrements studio. Les ventes d'Ultra sont bonnes, il se classe 1 dans plusieurs pays dont l'Allemagne et l'Angleterre, et dans le Top 5 aux États-Unis. En 1998, le groupe sort une compilation ', assortie d'un single inédit ('), ainsi qu'une réédition de ". « Le disque réaffirme (si besoin est) l'importance acquise par le groupe au cours de la dernière décennie tout en lui offrant la légitimité d'un retour sur scène ». Une tournée de soixante-cinq dates, baptisée , est organisée, et le public est au rendez-vous.

En 2001, Depeche Mode revient avec l'album ", qui se classe très rapidement en tête des ventes dans différents pays et hormis « quelques commentaires sévères, la majorité des critiques rock saluent la sortie de cet album avec le respect traditionnellement alloué aux intouchables de la pop ». Une tournée de quatre-vingt dates, baptisée « Exciter Tour », est organisée.

En 2003, Martin L. Gore et Dave Gahan sortent respectivement leurs albums solos ("Counterfeit²" et "Paper Monsters", ce dernier faisant son petit effet dans les charts internationaux) avant de se retrouver en janvier 2005 pour enregistrer le nouvel album. Intitulé "Playing the Angel", il paraît finalement à l'automne et constitue pour le groupe une nouvelle étape dans sa carrière ; sur cet album, salué par la critique, Dave Gahan, enhardi par son récent succès en solo, signe ses premières chansons (dont les musiques sont co-composées avec Christian Eigner et Andrew Philpott). En sollicitant Ben Hillier à la production (qui avait collaboré avec Blur), DM persiste à se forger un son drainant des guitares saturées et des synthétiseurs hors d'âge. Quelques mois après la sortie de l'album, la formation anglaise se lance dans une nouvelle tournée mondiale, « Touring the Angel », qui se révèle être la plus grande de sa carrière, elle comporte en effet pas moins de 123 concerts et un total de 33 pays visités. Pendant ce temps, "Playing the Angel" et ses divers singles - dont notamment le premier édité "Precious" - rencontrent un vrai succès dans les classements.

À l'automne 2004, Depeche Mode avait publié un album de remixes intitulé "Remixes 81-04" contenant notamment une reprise d’"Enjoy the Silence", par Mike Shinoda, du groupe Linkin Park. Cette version initialement nommée "Enjoy the Silence (Reinterpreted)" est éditée en single à cette époque sous le nom d"'Enjoy the Silence 04" (avec un mix légèrement différent) et connaît un certain écho dans les charts, atteignant par exemple le Top 10 au Royaume-Uni et en Allemagne, étant aussi remarquée pour sa vidéo réalisée principalement à base d'animations.

Fin 2006, paraît une compilation (audio et vidéo) intitulée "Best Of : volume 1" (sur laquelle figure l'inédit ', issu des chutes de "Playing the Angel") puis, l'année suivante, Gahan édite un nouvel album solo, ', qui n'est suivi d'aucune tournée. En mai 2008, Depeche Mode entre en studio pour enregistrer un douzième album ; celui-ci paraît finalement le et s'intitule '. Derrière ce titre pompeux, il dissimule des sonorités audacieuses mettant en valeur les mélodies de Gore, mais aussi celles de Dave Gahan (toujours aidé par Eigner et Philpott pour la musique), obtenant ainsi définitivement sa légitimité d'auteur (qu'il avait entamé sur le précédent album ainsi que sur son album solo '). Ben Hillier (qui était déjà aux manettes du précédent album) en a assuré la production. Le premier extrait, "", est un single martial aux sonorités synthétiques massives, sans réel refrain (il se rapproche en cela de ') où Gahan scande le portrait d'un anti-héros à qui rien n'a jamais souri dans son existence. Le deuxième single, ', est une ballade électronique sirupeuse qui divise les amateurs du groupe, qui semble se chercher un nouvel hymne pour ses concerts. Une tournée, baptisée débute par un warm-up le 6 mai 2009 à la Rockhal d'Esch-sur-Alzette (Luxembourg) et inclut un concert au Stade de France le 27 juin 2009, un autre au Zénith de Nancy le 28 juin 2009, à Carcassonne le 6 juillet, à Lyon et à Liévin. Pour la Belgique, Depeche Mode est la tête d'affiche de l'édition 2009 du festival TW Classic Werchter se déroulant le 20 juin. Pour la Suisse, trois concerts sont prévus, deux à Zurich et l'autre à Genève le 10 novembre 2009. La conférence annonçant cette tournée a lieu le lundi 6 octobre 2008 au Stade olympique de Berlin où le groupe joue le 10 juin 2009 devant personnes.

Le début de cette tournée est marqué par l'annulation de plusieurs dates en raison de l'hospitalisation du chanteur Dave Gahan. En effet, quelques minutes avant de monter sur scène à Athènes (le 12/05/09, véritable concert de la tournée), il est pris d'une violente gastro-entérite. Plus tard, les médecins décèlent une tumeur bénigne à la vessie, et le groupe ne reprend la route que début juin, annonçant des reprogrammations de certaines dates annulées et d'autres dates additionnelles pour l'hiver 2009 et début 2010, notamment au POPB (palais omnisports de Paris-Bercy) les 19 et 20 janvier 2010. Les concerts de Porto et Séville (11 et 12 juillet) font également l'objet d'annulation en raison d'une blessure à la jambe de Dave Gahan. Mais le véritable événement de cette tournée survient le 17 février 2010 lors d'un concert de charité organisé au Royal Albert Hall à Londres quand Alan Wilder rejoint le groupe sur scène le temps d'un titre. Wilder n'était plus apparu sur scène avec le groupe depuis 1994, l'année précédant son départ de DM. Selon Alan . Le DVD de la tournée nommé "" sort le 8 novembre.

Des rumeurs indiquent ensuite la sortie d'un album remix pour le début de l'année 2011. Ces rumeurs sont avérées puisque le , le groupe confirme sur son site officiel la sortie d'un album de remixes pour le 6 juin de la même année. Cinq jours après cette annonce, une version de "Personal Jesus" remixée par Alex Metric est alors dévoilée. En mars 2012, Martin Gore confirme lors d'une interview que le groupe doit entrer en studio et espère terminer un nouvel album avant la fin de l'année. Gore ajoute que les retrouvailles avec Vince Clarke pour l'album de VCMG, "Ssss", ont renforcé sa créativité : « c'était un break sympa pour moi... je suis retourné à l'écriture pour le groupe avec beaucoup plus d'entrain après cette expérience ».

Un nouvel album ainsi qu'une nouvelle tournée mondiale sont officiellement annoncés le à Paris, lors d'une conférence de presse du groupe. L'album, intitulé "Delta Machine", sort finalement le . Celui-ci reçoit des critiques en majorité positives. Peu après, débute la tournée mondiale de promotion de l'album, nommée "", qui commence en France, à Nice, le 4 mai et s'achève en mars 2014 à Moscou, avec la possibilité d'ajouts de dates supplémentaires en Amérique du Sud ainsi qu'en Asie, durant l'été de cette même année.

Le dans l'épisode 68 de "The Robcast", Martin L. Gore annonce que le groupe doit se réunir pour enregistrer son prochain album à partir du mois d'avril de cette même année. Le groupe dévoile le le titre de cet album à paraître le : "Spirit".<br>
Sa sortie est précédé du single "Where's the Revolution" publié le ; une tournée internationale suit. Cet album est favorablement accueilli par la critique.

Le groupe débute sa carrière en tant que quatuor. Leur point de départ est une fascination commune pour le groupe de musique électronique allemand Kraftwerk. Martin Gore déclare : . Martin Gore a aussi cité Elvis Presley, Johnny Cash, the Beatles et les Rolling Stones parmi ses autres artistes préférés. Dave Gahan a nommé les Sparks, Siouxsie and the Banshees et Roxy Music parmi ses groupes préférés.

Johnny Cash reprend ' dans l'album ' en 2002, accompagné par le guitariste des Red Hot Chili Peppers, John Frusciante.

Une seconde version de ' met en scène le chanteur de Coldplay en tenue royale, à l'image de Dave Gahan dans ' ; cette vidéo est tournée par Anton Corbijn en hommage à Depeche Mode. Kruder & Dorfmeister ont remixé la chanson ' dans l'album ' en 1998.

Marilyn Manson fait une reprise de '. Tori Amos réinterprète à sa manière ' sur son album de reprises '. Lacuna Coil, un groupe de métal gothique, a fait une reprise du célèbre titre ', dans leur album ', en 2006. Rammstein reprend en 1998 "Stripped" sur l'album ' (avec d'autres artistes comme The Cure, Smashing Pumpkins, Gus Gus) et réalisent un clip du single en utilisant des extraits du film de propagande de la cinéaste Leni Riefenstahl "Les Dieux du Stade" (") pour les jeux Olympiques de Berlin en 1936.

A-ha reprend ' au cours d'une émission radio de la BBC2 avec Dermot O'Leary diffusée le 25 juillet 2009 dans le cadre de la promotion de leur album ' en Grande-Bretagne. In Flames, groupe de death metal mélodique suédois reprend ', dans leur album ' sorti en 1997. Sylvain Chauveau, compositeur et pianiste français rend hommage à Depeche Mode avec l'album " (2005).

Placebo fait une reprise de ' en 2004. "Nouvelle Vague" reprend en version bossa nova ' et ' respectivement sur les albums "Nouvelle Vague" et "3". Real Life reprend ' dans l'album '. Michael Gregorio reprend ' dans son spectacle "Michael Gregorio pirate les chanteurs", et "" dans son spectacle "Michael Gregorio en concert(s)". Le groupe Indochine reprend "Personal Jesus" dans l'émission de variétés "Taratata". Nina Hagen reprend "Personal Jesus" en 2010.

Le groupe Nada Surf reprend "Enjoy the silence" en 2010 dans son album "If I Had a Hi-fi". Kim Wilde en fait de même (en concert) dans le mini-album "Baby Obey Me" sorti en 2007 en Allemagne. Susan Boyle reprend également "Enjoy the Silence" en 2011 dans son album "". John Lord Fonda reprend "Personal Jesus" en 2005 dans son album "DeBaSer". A Perfect Circle reprend "People Are People" en 2004 dans son album "eMOTIVe".

Pour le jeu "Les Sims 2", Electronic Arts et Depeche Mode se sont entendus afin d'inclure ', l'un des morceaux de l'album '. La version proposée est cependant un peu différente de celle du disque : les paroles ont été traduites en simlish (le langage incompréhensible parlé par Les Sims). Dans le jeu vidéo ', la chanson ' peut être entendue sur une des stations de radio virtuelles du jeu, Radio X, alors que dans ', c'est la chanson ' que l'on peut entendre sur une autre radio fictive du jeu, Wave 103. Hideo Kojima reprend "" en 2012 pour la bande d'annonce du dernier Metal Gear (série) qui s'intitule ' sorti en février 2013. Dans le jeu vidéo ' développé par le studio Valve, on peut voir un des survivants (Rochelle) porter un tee-shirt à l'effigie de Depeche Mode. Durant le générique de fin du jeu vidéo "", la chanson "Angel", tirée de l'album "Delta Machine", peut être entendue.




Depuis 1993, on constate que Depeche Mode sort un nouvel album tous les quatre ans.




</doc>
<doc id="6546" url="https://fr.wikipedia.org/wiki?curid=6546" title="Vi">
Vi

vi est un éditeur de texte en mode texte plein écran écrit par Bill Joy en 1976 sur une des premières versions de la distribution Unix BSD.

Il est présent d'office sur la majorité des systèmes Unix actuels, souvent sous la forme d'un clone du logiciel "vi" originel.

Le nom vi provient de l'abréviation la plus courte possible (c'est-à-dire sans ambiguïté) de la commande visual de l'éditeur de texte ex, car cette commande fait passer l'éditeur ex du mode "ligne par ligne" au mode "plein écran" : ce n'est donc au départ qu’une interface visuelle (en anglais, "Visual Interface") de l'éditeur ex (lui-même extension de l'éditeur en ligne ed). ex est toujours disponible dans vi en appuyant sur : en mode commande. En pratique, lors de l'invocation de "vi" ou "ex", un unique programme est démarré : son comportement est décidé par le nom avec lequel on l'a appelé.

Le nom vi correspondant à la fois à des initiales et au nom de son fichier d'installation, il est usuellement prononcé en énonçant les deux lettres en anglais, c'est-à-dire "« vi-aille »", , plutôt que comme un mot à part entière . L'analogie de ce nom avec le chiffre romain "VI" (six) est fortuite.

vi est un éditeur modal, c'est-à-dire que la signification des boutons et des touches change selon le mode dans lequel il se trouve.

En mode "insertion", le texte tapé est inséré dans le document. Appuyer sur la touche "Echap" depuis le mode insertion permet de passer dans le mode "commande", dans lequel les touches correspondent à des déplacements du curseur ou à des fonctions d'édition. Par exemple, j descend le curseur d'une ligne, x efface le caractère sous le curseur (la position « sous le curseur » peut désigner la droite du curseur si ce dernier se place entre les caractères, au-dessus du caractère de soulignement ou sous le bloc rectangulaire, selon la manière dont le terminal représente le curseur).

Les touches tapées en mode commande ne sont pas insérées dans le texte, ce qui est une cause fréquente de confusion pour les utilisateurs débutants avec vi.

En mode "commande", de nombreuses opérations peuvent être effectuées en série avec des séquences de touches simples, sans qu'il soit nécessaire de maintenir les touches "Alt" ou "Ctrl" enfoncées. Les opérations les plus élaborées sont composées d'opérations plus primaires. Par exemple, d3w efface trois mots (d pour "delete" (effacer) et w pour "word" (mot), c2fa change ("change") le texte jusqu'à ce qu'il trouve ("find") le second ("2") "a". Pour les utilisateurs expérimentés, cela permet de travailler très efficacement. Cela permet également à l'utilisateur de conserver en permanence ses mains sur le clavier.

Les premières versions de vi ne donnaient aucune indication sur le mode dans lequel elles se trouvaient. Il était fréquent que les utilisateurs tapent machinalement sur la touche "Echap" pour s'assurer que l'éditeur était bien dans le mode commande (vi émet un signal sonore s'il est déjà dans ce mode). Les versions plus modernes de vi indiquent le mode dans une barre d'état ou graphiquement (par exemple la forme ou la couleur du curseur). Des implémentations graphiques de vi (par exemple GVim) supportent aussi l'utilisation de la souris et des menus pour accéder aux fonctions d'édition.

vi est devenu de facto l'éditeur standard d'unix et il a été l'éditeur favori de nombreux hackers jusqu'à l'arrivée d'Emacs en 1984. Il est à noter qu'Emacs est bien plus qu'un simple éditeur de texte et est pourtant souvent mis en concurrence avec vi. À ce jour (2011), vi ou l'un de ses clones peut être trouvé dans presque toutes les installations de Unix. La Single UNIX Specification (plus particulièrement l'« IEEE standard 1003.2, Part 2: Shell and utilities ») inclut vi. Ainsi, tout système se conformant à cette spécification intègre vi.

vi est encore largement utilisé par les utilisateurs des différentes variantes d'Unix. Il démarre plus vite que les versions lourdes de l'« éditeur » Emacs et utilise moins de mémoire. Conséquemment, mêmes des fans d'Emacs l'utilisent comme éditeur pour le courrier électronique ou pour de petites éditions.

Lors de la création d'une disquette de récupération ("rescue disk", pour les systèmes dont le disque dur ne fonctionne plus correctement), vi est bien souvent choisi comme éditeur, en raison de sa compacité (la place est très limitée sur les disquettes) et du fait que la majorité des gens effectuant des opérations de récupération sont capables de l'utiliser.

vi et Emacs sont les éternels belligérants de la guerre des éditeurs.

De nombreux éditeurs de texte basés sur vi existent. Les clones sont des mises en œuvre, libres ou non, de vi, c'est-à-dire qu'ils respectent théoriquement le jeu de commandes standard du vi de POSIX. Toutefois, la majorité de ces clones supportent également des fonctionnalités supplémentaires. Les dérivés sont des programmes, éditeurs de texte ou non, qui ont conservé le principe de fonctionnement de vi (en partie ou complètement) pour leur propre fonctionnement.

Une liste plus complète peut être trouvée dans les liens externes ci-après.

Les utilisateurs, débutant avec vi, sont souvent confrontés à des difficultés, d'une part à cause des raccourcis utilisés pour chacune des commandes, ensuite parce que l'effet de ces raccourcis change selon le mode dans lequel se trouve vi.

Un moyen efficace de débuter avec vi est de suivre d'un bout à l'autre un tutoriel expliquant les commandes les plus simples. La plupart des systèmes fournissant une implémentation de vi contiennent également un tutoriel pour vi.

En comprenant son principe de fonctionnement avec quelques explications et des exemples simples, la majorité des utilisateurs peuvent parvenir à leurs fins avec vi. Toutefois, l'aisance avec vi est généralement considérée comme plus longue à venir qu'avec les autres éditeurs avec lequel il est souvent comparé. Les défenseurs de vi affirment que cette difficulté initiale est largement compensée par l'efficacité de l'éditeur une fois que l'on est à l'aise.

Les débutants sont souvent déroutés par les commandes de vi. Ces dernières sont bien souvent réduites à une seule et unique lettre, et bien difficiles à mémoriser au premier abord, d'autant que l'éditeur présente différentes perspectives (lecture, insertion, open mode, etc.) qui ne sont pas clairement distinguées et la transition entre ces modes passe souvent inaperçue.

La plupart des commandes de vi sont choisies de façon à :

Lorsque l'on débute avec vi, il est primordial de comprendre que vi possède plusieurs modes de fonctionnement. Les deux principaux modes sont le mode "commande" (dans lequel vi démarre) et le mode "insertion". Les commandes i (insertion) ou a (ajout) du mode commande permettent de passer en mode insertion, tandis que la touche "Echap" permet de quitter le mode insertion et retourner au mode commande.

vi possède également un autre mode de fonctionnement, qui offre la possibilité de définir des macro-commandes en intégrant un véritable langage de programmation pour automatiser des tâches d'édition de texte.

Il est aussi possible d'appeler vi avec des arguments afin qu'il exécute automatiquement des commandes lors de l'ouverture d'un fichier. Par exemple, la ligne de commande :
lance vi, qui ouvre le fichier "FICHIER", puis remplace tous les "Deux" par des "Trois" dans le fichier, et enfin enregistre le fichier et quitte.

vi permet de définir des macro-commandes et des abréviations.

Les macros permettent de remplacer une séquence de touches par une autre, ce qui permet de définir de nouvelles commandes. Par exemple, 
crée la commande v du mode commande, qui supprime des blocs de 10 lignes.

Les abréviations permettent d'éviter de taper une séquence de lettres plutôt longue. Par exemple :
crée l'abréviation "GNU", qui en mode insertion est remplacée par "gnu is not unix" dès que suffisamment de caractères sont tapés pour lever toute ambiguïté (les abréviations sont généralement étendues quand une espace est insérée pour indiquer la fin du mot).

Le comportement de vi et de ses commandes peut être ajusté grâce à ce que vi appelle des « options ». La commande :set permet de les consulter et de les modifier. Certaines options sont booléennes, c'est-à-dire qu'elles peuvent prendre deux valeurs seulement : "vrai" ou "faux", ou plus exactement "activée" et "désactivée". Par exemple :
que l'on peut abréger par
ou encore
affiche le numéro de chaque ligne.
sont équivalentes et permettent de désactiver cette option.

Parmi les options booléennes, on peut trouver (liste non exhaustive) :
Les autres options prennent une valeur. Par exemple :
indique à vi où chercher le fichier de marqueurs.

L'ensemble de toutes les options disponibles peut être affiché avec

L'ensemble des options de vi (ainsi que les macros et les abréviations) peuvent être fixées pour chaque session d'édition en les ajoutant dans le fichier "~/.exrc". Ce fichier est lu automatiquement au démarrage de vi, et les commandes qu'il contient sont exécutées comme si elles étaient tapées par l'utilisateur (le : initial est inutile). Exemple :

Le tableau qui suit donne quelques commandes basiques de l'éditeur vi, nécessaires aux tâches d'édition les plus simples. Pour entrer ses commandes, il faut préalablement quitter le mode éditeur en appuyant sur la touche "Echap".

Les commandes suivantes montrent ce que permet vi quand un utilisateur cherche des commandes d'édition plus pointues.





</doc>
<doc id="6547" url="https://fr.wikipedia.org/wiki?curid=6547" title="Knoppix">
Knoppix

Knoppix est une distribution GNU/Linux basée sur le système de paquets du système Debian GNU/Linux.

Klaus Knopper, son principal développeur dont elle tire son nom, l'a conçue pour être utilisable sans installation. Cette idée, d'abord introduite par l'un des modes d'utilisation de la distribution Slackware, est reprise ici pour la première fois sur une base Debian au cours de l'année 2000 et restera une pionnière notable de ce type de distribution.

Ce type d'utilisation se démocratise ensuite petit à petit avec de nombreuses distributions spécialisées, dont une partie non négligeable elle-même issue de Knoppix. Elle garde de ce statut de pivot quelques singularités, notamment :

Knoppix est développé à l'origine pour une utilisation depuis un Live CD, mais permet tout autant une installation « classique ».

Depuis sa création, les supports Live se sont diversifiés et la distribution est aussi proposée en Live DVD, et elle permet également depuis la session Live de créer une installation sur d'autres supports amovibles (clé USB ou autres supports externes), supports qui seront à leur tour utilisables sans installation sur n'importe quel ordinateur compatible.
Kaella (Knoppix Linux Azur) est une francisation et adaptation de la Knoppix. Elle a été créée en 2004 par Yann Cochard, qui l'a maintenue jusqu'en 2007.
Elle a été distribuée par des magazines à plusieurs occasions :




</doc>
<doc id="6548" url="https://fr.wikipedia.org/wiki?curid=6548" title="Liste des gouverneurs du Chiapas">
Liste des gouverneurs du Chiapas

Listes des gouverneurs du Chiapas depuis 1924




</doc>
<doc id="6550" url="https://fr.wikipedia.org/wiki?curid=6550" title="Coïncidence (informatique)">
Coïncidence (informatique)

En informatique, l'opérateur logique coïncidence, également appelé ET inclusif, NON-OU exclusif (XNOR) et équivalence logique, peut se définir par la phrase suivante :
On peut noter qu'il s'agit de la négation du OU exclusif, souvent noté XNOR. On le nomme parfois (bien qu'abusivement) « identité ».

Son symbole est traditionnellement un point ("DOT" en anglais) dans un cercle : « ⊙ ».

Appelons A et B les deux opérandes considérés. Convenons de représenter leur valeur ainsi :

L'opérateur XNOR est défini par sa table de vérité, qui indique pour toutes les valeurs
possibles de A et B la valeur du résultat S :


Exemple d'utilisation : Le Circuit intégré 747266 TTL ou le circuit intégré CMOS 747266 intègre quatre portes logiques du type NON-OU exclusif.
Illustration : Exemple : La lampe s'allume si l'on appuie sur rien, ou si l'on appuie sur « a » et « b » simultanément.



</doc>
<doc id="6551" url="https://fr.wikipedia.org/wiki?curid=6551" title="Wiktionnaire">
Wiktionnaire

Le Wiktionnaire est un dictionnaire de la Wikimedia Foundation, dont l'objectif est de définir tous les mots dans toutes les langues. Le projet existe en plus de 150 langues. Le terme « Wiktionnaire » désigne la version en français de ce projet, Wiktionary étant le nom officiel en anglais. Il est fondé sur un système de wiki et son contenu est librement réutilisable (sous CC-BY-SA).

Daniel Alston (connu sous le nom de contributeur "Fonzy") est l'un des principaux initiateurs et promoteurs de ce projet. Le projet a été ouvert sur http://wiktionary.wikipedia.org le , puis déplacé à http://www.wiktionary.org le . Le , la version francophone a été créée à http://fr.wiktionary.org.

Les projets Wiktionnaire ont pour vocation de devenir ensemble un dictionnaire descriptif doublement multilingue (les entrées et les définitions sont multilingues). À terme, il devrait par exemple contenir la définition en wolof de mots inuktitut.

"Wiktionnaire", terme francisé, désigne la partie francophone du projet, c'est-à-dire celle contenant des définitions écrites en français. Mais les mots de toutes les langues peuvent y être définis. Si certains ont une graphie identique, ils sont séparés en paragraphes classés par ordre alphabétique sur les noms de leurs langues respectives.

Chacun de ces paragraphes doit donc contenir des définitions de mots, complétées par les éléments suivants dans l'ordre :

Ces pages sont répertoriées dans des catégories thématiques formant des champs lexicaux.

Par ailleurs, un outil JavaScript et Perl pouvant servir à la recherche de rimes, d'anagrammes, ou de mots (utilisable pour les mots croisés) fonctionne à partir de dumps du Wiktionnaire.





Selon le service de statistiques de Web Alexa, en avril 2017, environ de pages sont vues chaque jour sur l'ensemble des sites "wiktionary.org", ce qui en fait approximativement le 600 site le plus consulté du Web, et l’un des principaux sites en matière de lexicographie.

Grâce à l'insertion automatique de plusieurs dictionnaires libres, la version francophone grandit très rapidement fin 2005 et en 2006. Le 6 avril 2005, le Wiktionnaire compte environ d'entrées. En novembre de la même année, il dépasse les d'entrées, toutes langues confondues. Aux premiers jours de 2006, il dépasse les d'entrées, et devint temporairement le Wiktionnaire le plus important, toutes langues confondues. En novembre 2007, la version francophone contient plus de d'entrées, concernant plus de mots en 658 langues, dont plus de mots en français. Le 17 avril 2008, le cap des d'entrées est franchi, suivi le 19 octobre 2008 de celui d'un million d'entrées. Le 18 avril 2011, il compte plus de d'entrées. Le 10 octobre 2016, il compte plus de d'entrées.

En 2017, la version en français comprend plus de trois millions d'entrées et représente 18 % du trafic généré par "wiktionary.org", contre 40 % pour la version anglophone, 14 % pour la version russophone et 9,4 % pour la version germanophone. Comme 12,6 % de ce trafic vient de France, une bonne part de l'utilisation du Wiktionnaire provient du reste de la francophonie et des pays non francophones.




</doc>
<doc id="6552" url="https://fr.wikipedia.org/wiki?curid=6552" title="Jeu de stratégie en temps réel">
Jeu de stratégie en temps réel

Le jeu de stratégie en temps réel (STR ou RTS pour la dénomination du genre en anglais : ) est un type de jeu de stratégie particulier qui notamment et par opposition au jeu de stratégie au tour par tour n’utilise pas un découpage arbitraire du temps en tours de jeu, ce n’est toutefois pas le seul élément qui fait d’un jeu de stratégie un STR.

Cet article parle précisément du jeu de stratégie en temps réel tel qu’on l’entend aujourd’hui. D’autres genres de jeux stratégiques se jouent en temps réel comme les jeux de guerre ou les et, généralement d’une moindre mesure stratégique, les jeux de simulation et les jeux de gestion, ce n’est pas le cadre de cet article.

Il n'est pas non plus question ici des jeux autres que vidéo car il n'est bien sûr pas possible de restituer le "temps réel" sans l'assistance de la puissance de calcul d'une machine.

Le terme est utilisé pour la première fois par Brett Sperry en 1992 pour désigner le genre du jeu "Dune II" alors développé par . Une de ses premières préoccupation est alors d’éviter de désigner celui-ci comme un ou un . À l’époque, ces derniers correspondent en effet à un marché de niche et il craint de décourager certains joueurs d’essayer le système de jeu inédit de "Dune II". De nombreuses itérations sont ainsi nécessaire pour choisir une nouvelle désignation, les développeurs hésitant entre des termes comme ou , mais c’est finalement le terme qui est choisi, les développeurs considérant qu’il définit parfaitement le système de jeu de leur nouveau titre.

Bien que la définition du genre fasse l’objet de débats, les jeux de stratégie en temps réel sont traditionnellement définis par les termes , c’est-à-dire comme des jeux de stratégies militaires incluant des éléments de gestion et de construction de bases et dont l'action se déroule en temps réel. Ainsi, si l’action de jeux comme "Populous" ou ' se déroule en temps réel, ils ne sont pas considérés comme des jeux de stratégie en temps réel à proprement parler. Ils sont généralement désignés comme des et, bien qu’ils aient des points communs avec les jeux de stratégie en temps réel, les deux genres sont considérés comme distincts et comme attirant des publics différents. Cette définition exclut également du genre les jeux comme ' dont les combats se déroulent en temps réel mais pour lequel l’expérience de jeu est dominé par ses phases de stratégie au tour par tour. En revanche, des jeux comme ', ' ou "" peuvent être inclus dans le genre, le tour par tour n’y étant qu’un composant additionnel. L’absence de l’aspect gestion exclut enfin les jeux comme "" ou "Blitzkrieg" dont l’action se focalise uniquement sur les combats et qui sont généralement désignés sous le terme .

Si l’aspect technique des jeux de stratégie en temps réel s’est peu à peu amélioré, leur système de jeu n’a que peu évolué depuis la naissance du genre malgré l’introduction de concepts inédits dans certains jeux. Ainsi, le principe de base des jeux de stratégie en temps réel reste défini par le concept du et par les règles introduites par "Dune II" en 1992 et reprises depuis dans une large majorité des jeux de ce genre incluant ', ', ', ' et "". Comme dans ces derniers, le joueur doit généralement gérer des ressources, développer des bases et créer des unités pour combattre l’adversaire. Plusieurs factions sont en général disponibles, chacune disposant de spécificités et d’un arbre technologique. L’action se déroulant en temps réel, le joueur ne dispose que d’un temps limité pour gérer ses ressources et ses bases et contrôler ses unités, ce qui introduit les notions de rapidité et de micromanagement en plus de la dimension stratégique et explique que le contrôle à la souris et au clavier soit généralement privilégié.

L’équilibrage entre les factions est généralement considéré comme l’un des aspects les plus importants d’un jeu de stratégie en temps réel. D'après certains concepteurs, cet aspect ne doit cependant pas se faire aux dépens des spécificités des factions, assurer une réelle diversité entre les forces en présence étant également important pour s’assurer que les joueurs ont de nombreuses stratégies à leur disposition. Ainsi, si les deux factions d’"Herzog Zwei" sont strictement identiques, des différences entre les factions apparaissent dès "Dune II". On retrouve ensuite cette notion de spécificités dans la plupart des jeux de stratégie en temps réel de l’époque mais les différentes factions restent quasiment identiques. Dans ce domaine, certains jeux comme "" ou "StarCraft" font mieux que la plupart de leurs concurrents, ce dernier étant notamment considéré comme la référence du genre dans ce domaine pour avoir introduit trois factions distinctes ayant chacune leurs propres unités et caractéristiques, rendant les mécanismes de jeu de chacune d’elles fondamentalement différents. Cela complique néanmoins le travail des développeurs, de tels jeux étant beaucoup plus difficiles à équilibrer comme en témoigne le nombre de patch d’équilibrage ayant été nécessaire pour arriver à un équilibre dans "StarCraft".

L’action se déroulant en temps réel, le joueur ne dispose que d’un temps limité pour gérer ses ressources et ses bases et contrôler ses unités. Cela ajoute une nouvelle dimension au jeu en plus de la dimension stratégique déjà présente dans un jeu de stratégie au tour par tour . Ainsi, en plus de devoir planifier sa stratégie, le joueur doit assurer le micromanagement de ses bases et de ses unités, c’est-à-dire qu’il doit simultanément assurer la récolte des ressources, la construction de bâtiments, la production d’unités et contrôler les différentes unités de son armée, parfois en plusieurs endroits. Cette notion de micromanagement est parfois désignée comme faisant uniquement appel aux du joueur mais est considéré comme un des aspects les plus importants des jeux de stratégie en temps réel car permettant de les différencier des jeux de stratégie au tour par tour.

Les premiers jeux vidéo de stratégie font leur apparition à la fin des années 1970, d’abord sur ordinateur central, puis sur les premiers micro-ordinateurs. La plupart de ces jeux s’inspirent des jeux de plateau, et notamment des jeux de guerre, et ont donc en commun de se dérouler au tour par tour. La distinction entre jeu de stratégie au tour par tour et jeu de stratégie en temps réel est ainsi un concept relativement récent et, jusque dans les années 1990, la grande majorité des jeux de stratégie se déroulent au tour par tour, à l’opposé des jeux d’action et de leur "" en temps réel. Du fait de leurs origines respectives, les deux genres sont en effet considérés comme étant incompatibles, les premiers ayant hérité de la lenteur et de la complexité des jeux de guerre sur table et les seconds de la rapidité et de la simplicité des jeux d’arcades, et l’idée d’introduire des éléments de temps réel dans un jeu de stratégie est rarement exploitée. Les jeux de stratégies en temps réel n’émergent ainsi que graduellement, à partir de tentatives ponctuelles visant à combiner l’excitation et la rapidité des jeux d’action avec la réflexion et la profondeur des jeux de stratégie. Le premier à exploiter cette idée est sans doute Don Daglow avec le jeu de stratégie "Utopia" (1981) qui met les joueurs aux commandes de deux nations insulaires dans une course au développement. En se démarquant clairement des jeux de plateau, le jeu introduit en effet de nombreuses évolutions qui lui valent notamment d’être considéré comme un des précurseurs des jeux 4X mais aussi des ' et des '. Bien qu’il se déroule au tour par tour, il permet de plus aux joueurs de manœuvrer leurs navires de pêches ou de guerre en temps réel au cours d’un tour de jeu, ce qui lui vaut d’être régulièrement cité comme un des précurseurs des jeux de stratégie en temps réel, même si son ' s’apparente plus à celui de ' qu’à celui d’un jeu comme "Dune II". L’idée de combiner action et stratégie est également à l’origine de "" (1982) de Dan Bunten dans lequel deux joueurs s’affrontent dans une arène par l’intermédiaire d’une armée de robots avec pour objectif de détruire le centre de commandement adverse. Pour cela, les joueurs disposent de plusieurs types de robots et peuvent prendre le contrôle de générateurs d’énergie, qui leur permettent de fabriquer de nouveaux robots. Contrairement à "Utopia", le jeu se déroule entièrement en temps réel et contient certains des éléments caractéristiques du genre, mais pas la totalité, ce qui lui vaut d’être parmi les titres les plus souvent cités comme des précurseurs du genre. Il ne rencontre cependant qu’un succès limité et ne semble pas avoir eu d’influence sur le genre.

Après ", d’autres ' comme " (1982) de Chris Crawford ou ' (1983) font évoluer le genre vers un style plus centré sur l’action et la tactique. ' propose en effet une adaptation en temps réel des mécanismes de jeu des ' traditionnels et rencontre un certain succès dans la presse spécialisée, mais quelques bugs critiques entravent sérieusement ses ventes. ', qui propose de son côté une simulation relativement simple et réaliste d’affrontement entre les Romains et les barbares à l’Antiquité, ne rencontre lui aussi qu’un succès mitigé. Son système de jeu préfigure cependant un nouveau genre de jeux de stratégie, les jeux de tactique en temps réel, qui rencontre un premier succès avec le ' ' (1984) mais qui devra cependant attendre de nombreuses années avant d’être reconnu comme un genre à part entière. Entre-temps, ' (1987) approfondi les concepts de base de ce que deviendront les jeux de stratégie en temps réel. Dans celui-ci, les joueurs fabriquent des robots avec lesquels ils peuvent détruire ou capturer des usines, qui leur fournissent les composants nécessaires à la production d’autres robots, l’objectif final étant de détruire la base de l’adversaire. Les joueurs peuvent contrôler chaque robot individuellement mais ils peuvent également leur donner des ordres comme détruire, capturer, tuer ou défendre. Un autre jeu de Dan Bunten, "" (1988), se focalise de son côté sur la tactique mais inclut de nombreux éléments caractéristiques du genre, dont un brouillard de guerre et la prise en compte de l’influence du terrain sur les déplacements et les combats. 

La même année, le studio japonais Technosoft publie le jeu de tactique "Herzog" qui pose les bases d’"Herzog Zwei" (1989) qui est généralement considéré comme le premier véritable jeu de stratégie en temps réel. Celui-ci oppose deux joueurs qui contrôlent chacun un robot de combat transformable qui leur permet notamment d’acheter des unités, de les transporter ou de leur donner des ordres. Chaque joueur dispose au départ une base principale et de bases secondaires et tente de capturer de nouvelles bases, qui permettent d’accélérer la production d’unité, l’objectif final étant de détruire la base principale de l’adversaire. Malgré une intelligence artificielle décevante, "Herzog Zwei" se révèle difficile du fait notamment de son rythme soutenu. Le joueur doit en effet jongler entre la production des unités et la micro-gestion de ces dernières, ce qui préfigure en quelques sortes le "" des jeux de stratégie en temps réel multijoueur. Bien que le ' d’"Herzog Zwei" se rapproche de celui de "Dune II", les jeux de stratégie en temps réel ne trouvent pas tout de suite ses marques, leurs mécanismes changeant d’un jeu à l’autre. Avec ' (1990), Bullfrog propose ainsi un jeu combinant des éléments de "god game" repris de son prédécesseur, "Populous", avec des éléments de stratégie en temps réel, le joueur pouvant donner des ordres directement à ses troupes sur une carte en trois dimensions. Un nouveau jeu de Dan Bunten, "" (1991), propose de son côté une sorte d’adaptation en temps réel du jeu de plateau "Risk" dans laquelle le joueur doit notamment gérer les aspects économiques et militaires d’une nation au cours de différentes guerres mondiales. Dans un style différent, "Mega Lo Mania" (1991) de Sensible Software, retrace la totalité de l’histoire humaine et permet au joueur d’allouer ses hommes à différent tâches, comme la recherche, la collecte des ressources, l’industrie ou l’armée. Comme dans "Powermonger", la dimension tactique des combats y est en revanche absente.

C’est finalement en 1992 que le terme jeu de stratégie en temps réel est utilisé pour la première fois, par Brett Sperry, le producteur de "Dune II", pour tenter de décrire le jeu alors développé par Westwood Studios. En termes de concept, ce dernier ne constitue qu’une avancée mineure par rapport à ses prédécesseurs, mais son succès et son influence est considérable. Il devient ainsi le premier jeu de stratégie en temps réel à marquer les esprits, celui qui définit le genre et les jeux qui lui succèdent. Pour le concevoir, les développeurs de Westwood – jusque-là connu pour le jeu vidéo de rôle ' – s’inspirent principalement de ', du ' "Populous" et de l’interface graphique à la souris du Macintosh pour créer une adaptation très libre de la saga "Dune", qui sort seulement quelques mois après une autre adaptation, le jeu d’aventure "Dune" de Cryo Interactive. Ils s’appuient également sur le principe des jeux 4X, qu’ils transposent cependant à une échelle plus réduite. Le scénario du jeu relate un affrontement entre trois factions – les Atréides, les Harkonnen et les Ordos – pour le contrôle et l’exploitation d’une ressource, l’Épice, sur la planète Arrakis. De son côté, le ' du jeu nécessite d’envoyer des moissonneuses collecter de l’épice, qui est converti en crédit qui peuvent être utilisé pour construire de nouveau bâtiment et créer de nouvelles unités pour combattre l’adversaire, suivant le principe du qui définit le genre.

Malgré le succès du "Dune II", aucun jeu de stratégie ne reprend son système de jeu dans les deux années qui suivent sa sortie. C’est finalement Blizzard Entertainment qui solidifie les bases posées par "Dune II" pour en faire un véritable genre en transposant son système de jeu à un univers médiéval-fantastique avec ' (1994). Dans celui-ci, la récolte de l’épice est remplacer par la collecte d’or et de bois et le joueur peut suivre les étapes de la construction d’un bâtiment à l’écran, mais c’est d’abord par l’introduction de deux factions doté d’une véritable personnalité qu’il se distingue de son prédécesseur. Il introduit également un mode deux joueurs (en réseau local ou par Internet) qui marque le début d’un changement progressif de la manière de concevoir les jeux de stratégie en temps réel, comme la popularité des matchs à mort de "Doom" à transformer le jeu de tir à la première personne. Le mode multijoueur de ' est en effet particulièrement populaire et continue d’intéresser les joueurs longtemps après qu’ils aient terminé les deux campagnes du jeu, ce qui ne manque pas de remarquer.

Westwood Studios développe ensuite ' (1995), la suite spirituelle de "Dune II", qui retrace une guerre uchronique pour le contrôle d’une substance extraterrestre, le tibérium, entre deux factions, la et la confrérie du Nod. Celui-ci propose notamment un ' plus rapide que son prédécesseur et introduit plusieurs amélioration par rapport à ce dernier, dont la possibilité de sélectionner plusieurs unités à la fois avec un cliquer-glisser, un mode multijoueur, une interface simplifiée et des cinématiques, filmées avec de vrais acteurs sur un fond vert. Il se distingue également de son prédécesseur en dotant ses deux factions d’une personnalité propre, retranscrite par l’intermédiaire des différentes cinématiques et missions du jeu, avec notamment la méchanceté gratuite dont fait preuve la confrérie du Nod qui contribue à populariser la série. Avec le ' ' (1996), la série consolide ensuite sa place dans la culture populaire en transposant le conflit dans un alternatif qui tient lieu de préquelle au premier jeu et qui convient finalement encore mieux à son style. "Alerte Rouge" introduit également une plus grande différenciation entre les deux factions du jeu, qui se voient doter de compétences et de capacités différentes. Le résultat n’est cependant pas une réussite en termes d’équilibre et le jeu multijoueur ne tarde pas à être dominé par la stratégie du " de tank du camp soviétique.

En parallèle, Blizzard Entertainment surf sur le succès critique et commercial de ' pour en développer une suite, ' (1995). Sur le plan technique, cette dernière améliore ses graphismes et son interface, qui permet ainsi de sélectionner plusieurs unités à la fois comme dans ', de visualiser facilement les statistiques des troupes sélectionnés et d’ordonner aux unités de rester en place, de poursuivre un ennemi ou d’effectuer une patrouille. ' introduit également une nouvelle ressource, des unités navales et aériennes et une plus grande différenciation entre les deux factions. Il intègre enfin un nouveau système de brouillard de guerre qui, en plus de cacher au joueur les zones de la carte non explorées, assombrit les zones déjà explorées mais qui ne sont plus dans le champ de visions des unités du joueur. Le terrain reste ainsi visible mais les unités ennemies sont cachées à la vue du joueur. Ce système inédit change profondément le genre, auquel il apporte une nouvelle dimension stratégique, et est ensuite repris la quasi-totalité des jeux de stratégie en temps réel.

Porté par le succès des premiers jeux de stratégie en temps réel de et de et par la compétition entre leurs titres respectifs, le genre devient particulièrement populaire et donne lieu à de nombreux clones, plus ou moins réussis. Parmi les STR de science-fiction qui inonde alors le marché émerge notamment ' (1997) dans lequel des extraterrestres disposent de la capacité de se déguiser en élément du décor ou en véhicule et peuvent combiner certaines unités pour en former une plus puissante. La même année, ' introduit une sorte de héros, sous la forme d’une unité de commandement, et des artéfacts que les joueurs peuvent capturer et utiliser pour changer le court d’un affrontement, par exemple en transformant les arbres en armes ou en aspirant les ennemis dans un trou noir. Toujours la même année, " met également à disposition du joueur des généraux disposant de capacités spéciales pour commander et organiser les groupes d’unités, mais ces derniers se révèlent souvent être des fardeaux du fait de leur intelligence limité. Avec le jeu "Z" (1996), le studio britannique se démarque de ses concurrents. Plutôt que des affrontements entre races extraterrestres ou entre des chefs de cultes mégalomanes, son scénario retrace des affrontements entre des armées de robots pour le contrôle de cinq planètes avec un peu d’ironie et d’humour anglais. Il est également un des premiers STR à proposer une approche différente du genre. Il remplace en effet le système de récolte de ressources de ses prédécesseurs par un système de zone de contrôle, les cartes étant divisées en zones qui rapportent des ressources au joueur qui les contrôlent. Alors que dans les autres jeux du genre, le début d’une partie est plutôt lent et la fin tourne à la guerre d’attrition, il propose une série constante d’affrontement pour la domination de la carte. Il introduit également plusieurs innovations, comme la possibilité de tuer le conducteur d’un véhicule ennemi pour en prendre possession. Il apporte ainsi la preuve qu’un autre genre de RTS est possible, mais ce n’est pas suffisant pour changer la voie tracé par ses prédécesseurs.

Le genre rompt finalement avec la science-fiction et la fantasy en 1997 avec deux nouveaux jeux de stratégie en temps réel très différents, mais s’appuyant sur la même idée. Sous la direction d’un des créateur de "Civilization", Ensemble Studios développe ainsi ' (1997) qui retrace des affrontements entre douze civilisations de l’Antiquité à travers quatre âge. Tout comme "Civilization", celui-ci nécessite notamment de faire progresser une civilisation (de l’âge de pierre à l’âge du fer) afin de débloquer de nouvelles technologie et permet au joueur de construire des bâtiments spécifiques, les merveilles, qui peuvent lui donner la victoire. Le jeu reste cependant très proche d’' et ' malgré le passage à quatre ressources et à un arbre technologique plus complexe. Un mois après la sortie d’', le studio publie ' (1997) qui passe relativement inaperçu malgré une profondeur stratégique bien plus importante que la plupart des STR de l’époque. Celui-ci s’éloigne en effet du rythme effréné de la plupart des jeux du genre pour se concentrer sur le développement économique. Pour chaque unité et bâtiment, le joueur doit ainsi payer les frais d’entretien, en plus de leur coût initial, qui peuvent être couvert grâce aux taxes ou au commerce. Chaque résident des cités sous le contrôle du joueur paye en effet des taxes. Celles-ci peuvent être augmentées, au détriment de la loyauté des habitants qui deviennent alors susceptibles de quitter leur cité ou d’être recruter comme espion. Les autres ressources sont extraites de mines et peuvent ensuite être revendues sur le marché ou transformé en biens, qui peuvent également être vendus, dans des usines. Le système de commerce est aussi plus complexe que dans ', le joueur devant chercher les routes les plus rentables pour ses caravanes et en assurer la sécurité. En contrepartie de son système économique complexe, le jeu met en revanche de côté l’aspect tactique des combats sur lesquels le joueur n’a pas vraiment de contrôle.

Après s’être contenté de graphismes en deux dimensions pendant plusieurs années, les jeux de stratégie en temps réel commence à utiliser des terrains en 3D en septembre 1997 avec la sortie de ". Bien que graphiquement similaire à ses prédécesseurs, celui-ci utilise en effet une carte avec des terrains de différents niveaux de terrains qui affectent la distance de vue des unités. Les unités peuvent ainsi voir plus loin en se plaçant en hauteur, mais aussi se mettre à couvert derrière un arbre ou un rocher pour préparer une embuscade ou se cacher. Outre cette évolution, le jeu introduit également quelques innovations, comme la possibilité de désactiver des bâtiments pour économiser de l’énergie et d’assigner des missions a des unités sans que celles-ci ne nécessite ensuite d’intervention du joueur. Publié un mois plus tard, " s’appuie sur des idées similaires. Il propose en effets des cartes en 3D pré-rendues, associé à un angle de vue fixe, qui affectent les animations et la distance de vue des unités ainsi qu’un moteur physique, qui gère notamment les explosions, les tirs d’artillerie et les débris qui s’envolent. Ce n’est cependant pas ses graphismes qui font du jeu un triomphe, mais plutôt l’équilibre de son système de jeu. Dans celui-ci, toutes les actions nécessitent de l’énergie, que ce soit la construction, la production des unités ou les combats, les armes des unités nécessitant de l’énergie. Le joueur doit ainsi en permanence équilibrer sa production et sa consommation d’énergie afin de s’assurer que ses installations ou ses unités ne tombent pas en panne, sans tomber dans l’excès inverse. Cette notion d’équilibre se retrouve également dans ses différentes unités, qui ont généralement un point faible qui peut être exploité. Toujours en 1997, le jeu de tactique en temps réel "" montre la manière dont un véritable moteur 3D peut améliorer l’aspect tactique d’un STR en donnant au joueur un total contrôle sur la caméra pour lui permettre d’avoir un meilleur point de vue sur le champ de bataille. Presque deux ans sont ensuite nécessaire pour voir un tel moteur 3D dans un STR. "" (1999) est ainsi le premier jeu de stratégie en temps réel entièrement en 3D, et aussi le premier à se dérouler dans l’espace. Son principe de base est très similaire à celui des autres STR, avec la nécessité de récolter des ressources, de commander une flotte, de construire des vaisseaux et de rechercher de nouvelles technologies. Il diffère cependant des autres jeux du genre par son environnement – l’espace – qui ajoute une troisième dimension par rapport aux jeux se déroulant sur la terre ferme.

En parallèle de l’émergence de STR en 3D, les STR traditionnels en vue isométrique assure leur survie au long terme grâce à la sortie du STR le plus populaire de tous les temps. Après le succès croissant de Blizzard Entertainment avec les deux premiers "Warcraft" puis "Diablo", le studio se lance dans la science-fiction avec Starcraft (1998) qui retrace l’affrontement entre les Terrans (les humains) et deux races extraterrestres, les Protoss et les Zergs dans un secteur lointain de la galaxie. Initialement imaginé comme un "Warcraft" dans l’espace, le jeu diffère finalement grandement de ses prédécesseurs grâce notamment à ses trois factions complètements distinctes, que ce soit en termes d’unités, de bâtiments ou de méthode pour collecter les ressources, mais néanmoins parfaitement équilibrées. Cet équilibre n’est cependant pas immédiatement atteint et à la sortie du jeu, avec par exemple des problématiques liées au rush des Zergs, et le studio doit donc retravailler cet équilibre avec la publication fréquente de patch. Ses trois races uniques et son équilibre vont cependant lui permettre de devenir le plus grand succès commercial du genre grâce notamment à son immense popularité en Corée du Sud où il devient un phénomène culturel et un véritable sport national. Plus de dix ans après sa sortie, des matchs professionnels de "Starcraft" continuent ainsi d’être diffusés sur des chaines de télévisions dédiées et les meilleurs joueurs deviennent de véritables stars capables de remplir des stades.

Ensemble Studios propose ensuite une alternative à "Starcraft" et à son thème de science-fiction avec la suite de son STR historique, ' (1999). Bien qu’il soit considéré comme une version améliorée de son prédécesseur plus que comme une véritable suite, il connait un important succès commercial grâce à une large campagne marketing, des perfectionnements de son système de jeu et quelques innovations importantes. Il devient ainsi possible d’appeler les paysans à venir défendre leur quartier général pour repousser une attaque et d’assigner des formations à un groupe d’unités, qu’elles conservent même en mouvement. Si ces améliorations le propulsent au rang de bestseller, c’est son générateur de cartes aléatoires et son équilibre entre les différentes conditions de victoire qui en font un succès durable en multijoueur. Plus de dix ans après sa sortie, il va ainsi bénéficier d’une réédition en haute-définition et de nouvelles extensions, développées par des moddeurs de sa communauté. ' (2001) tente ensuite de ravir la place de meilleur STR historique à ' avec ses graphismes en 3D, son système de héros et en couvrant l’ensemble de l’histoire de l’humanité plutôt que de se concentrer sur une seule période. Son ampleur, sa complexité et l’aide de Rick Goodman, le concepteur d’', ne suffisent cependant pas à dépasser le succès de son prédécesseur, que ce soit en termes de critiques ou de ventes. 

En parallèle, continue sur sa lancée. Le studio publie ainsi un ' de "Dune II", "Dune 2000" (1998), juste avant son rachat par Electronic Arts mais celui-ci ne rencontre qu’un succès mitigé, du fait notamment de ses graphismes d’un autre âge et de ses problèmes d’équilibrage. Il est suivi de ' (1999) qui va encore plus loin dans le côté loufoque de la série et apporte de nombreuses améliorations dont la gestion dynamique de l’éclairage, notamment utilisée pour les projecteurs des tours de garde, et un très bon système de point de passage pouvant être utilisé pour mettre en place des patrouilles ou définir le chemin d’une unité. Avant sa fermeture en 2003, le studio publie enfin ' (2000) puis ' (2001), la suite de "Dune II", qui bénéficie d’un nouveau moteur 3D. 

Si les bestsellers du genre continuent d’exploiter le concept de "Dune II", de nouveaux types de jeux de stratégie en temps réel commencent à apparaitre à la périphérie du genre. Avec ' (1998), Blue Byte Software s’éloigne par exemple du concept de ' de ' et ' pour se rapprocher des STR en supprimant la nécessité de construire des routes et en appondissent le système de combat du jeu. Ce troisième volet reste néanmoins focaliser sur la gestion en conservant les nombreuses ressources et les douzaines de métiers différents des premiers jeux de la série. Dans le même esprit, le jeu ' (2000) se focalise sur l’exploration, la construction et la gestion économique d’une petite tribu avec pour objectif de la développer plutôt que de conquérir le monde. Cette même idée est au centre de "Anno 1602" (1998), qui donne naissance à une longue série de jeu combinant stratégie en temps réel et gestion. Dans celui-ci, comme dans ses suites, le combat devient un problème inévitable dans un jeu qui se focalise sur l’exploration, le commerce et le développement d’une colonie. Il est ainsi plus lent que la plupart des jeux du genre et les combats n’y jouent pas un rôle aussi important, mais son modèle économique complexe en fait néanmoins un jeu passionnant. ' (2001) démontre de son côté l’importance que peut avoir un modèle économique complexe dans un RTS plus traditionnel dont l’objectif est de détruire l’ennemi. 

Au début des années 2000, la popularité du genre commence a décliné et les éditeurs abandonnent les clones de "". Le publique des STR se divise entre un groupe loyal de fans se concentrant sur l’aspect multijoueur et la maitrise dans ses moindre détail d’un jeu compétitif comme "StarCraft", et un groupe plus périphérique préférant les campagnes solo ou les jeux hybrides comme "Anno", "Settlers" ou de jeu de grande stratégie comme "Europa Universalis". Les développeurs et les éditeurs qui tentent de percer dans le domaine du STR changent donc de stratégie et experimentent de nouveaux concepts afin de toucher une audience plus large, souvent au croisement des STR et d’autres types de jeu vidéo. Avec ' (2000), Cyberlore Studios propose ainsi une version plus moderne de ' (1993) avec des unités complètement autonome pour combiner le concept de city-builder de son modèle avec le gameplay des STR. De son côté, ' (2001) s’inspire de la rigueur des jeux de simulations pour proposer un STR dans la lignée de Age of Empires se focalisant sur la période médiévale. La même année, ' transpose le gameplay des STR, qui se focalise généralement sur la tactique, à une plus grande échelle en permettant au joueur de contrôler des armées entières dont les unités sont regroupés en formations pouvant compter jusqu’à 196 soldats. L’année suivante, ' (2002) se distingue des autres jeux du genre par sa campagne innovante et variée et par ses gigantesques batailles en 3D. Toujours en 2002, ' combine STR et jeu vidéo de rôle avec notamment un mode aventure dans lequel le joueur ne contrôle qu’une poignée de héros.

Ces expérimentations n’épargnent pas les ténors du genre. Dans ' (2002), intègre ainsi des éléments de jeu de rôle, avec notamment l’ajout de héros et de groupes de monstres gardant certains emplacements stratégiques de la carte, qui permettent de récupérer des objets et de l’expérience. Ces ajouts modifient profondément certain aspect de son gameplay. Ils poussent en effet le joueur à être actif sur la carte pour faire progresser son héros, ce qui rend les stratégies passives, consistant à être passif et à amasser le plus de troupes possibe avant d’attaquer, beaucoup moins efficaces. Ces innovations vont également permettre l’émergence d’un nouveau genre de jeux de stratégie en temps réel, les MOBA, qui deviennent populaire en 2003 grâce à une carte personnalisé créé par des fans du jeu, ', en s’inspirant d’une carte personnalisé de ' baptisé '. De la même manière, s’éloigne du thème historique et du gameplay des deux premiers ' avec le développement d’un spin-off, ' (2002), qui s’inspire des mythologies grecques, égyptienne et scandinaves. Le nombre de faction jouable est notamment réduit à trois, contre 13 dans ", mais les différences entre celles-ci deviennent plus nombreuses. Outre des unités et des bâtiments spécifiques, elles se voient en effet doté de divinité, que le joueur peut choisir pour obtenir des bonus spécifiques, et de moyens différents d’obtenir la ferveur nécessaire pour bénéficier des pouvoirs que leurs confèrent ces divinités.

Enfin, avec ' (2003), Brian Reynolds (le concepteur de ' et de ") signe la première tentative de combiner le gameplay des STR traditionnels avec celui des jeux 4X. Malgré son côté révolutionnaire et un bon accueil critique et commercial, le jeu n’a cependant que peu d’impact sur le genre.

Le succès de ' (2005), qui se classe dans les dix jeux les plus vendus en Amérique du Nord en 2005, suggère à l’époque que le genre à encore une longue vie devant lui, même s’il n’a pas vraiment évolué en dix ans. L’industrie du jeu sur ordinateur est cependant dans une période difficile, dont les revenus sont en baisse depuis 2001. Les éditeurs deviennent ainsi de plus en plus réticents à investir dans ce domaine, au contraire du secteur des jeux sur console, qui connait une importante croissance. La situation se révèle particulièrement problématique pour les STR qui ne sont pas vraiment adaptés à la console et à sa manette de jeu. C’est dans ce contexte, alors que Blizzard se contente de publier des patchs pour "StarCraft" et qu’Ensemble Studios publie deux extensions pour ', que Chris Taylor (le concepteur de ') tente de réinventer le genre en le recentrant sur la stratégie plutôt que sur la tactique, à l’opposé des classiques du genre. Avec ' (2007), il propose ainsi un jeu de stratégie en temps réel à grande échelle se déroulant sur des cartes pouvant couvrir une surface allant jusqu’à 6500 km2 et dont le zoom permet au joueur d’observer le conflit depuis l’espace. Il offre ainsi de nouvelles options stratégiques au joueur mais souffre cependant du même défaut que la plupart des STR de l’époque, la difficulté de sa prise en main. Plus encore que sa stagnation, c’est peut-être en effet la complexité croissante des STR qui, en dépassant le seuil acceptable pour les joueurs occasionnels, explique le déclin du genre. En effet, si le principe de base des STR n’a pas vraiment changé, il devient de plus en plus difficile de connaitre leurs nombreux raccourcis clavier, règles, ordres de constructions et stratégies et de savoir les exécuter rapidement, et donc d’être capable de les maitriser en mode multijoueur. Ainsi, à moins de n’être intéressé que par la campagne solo, les STR deviennent réservés aux joueurs susceptibles de consacrer beaucoup de temps pour apprendre à les maitriser. 

Comme les city-builder ou les simulateurs de vol, qui commencent à décliner avec l’avènement de l’accélération 3D du fait de leur obsession pour plus réalisme, les STR sombrent dans leur obsession de plus de rapidité, jusqu’à ne représenter qu’un secteur de niche, réservé à des passionnés du genre. En mettant l’accent sur la rapidité des actions — un jeu comme ' peut nécessiter jusqu’à 150 actions par minutes pour être joué de manière compétitive — ils se focalisent ainsi trop sur l’action et plus assez sur la stratégie. Alors que leurs dérivés, comme les tower defense et les MOBA, explosent, les STR traditionnels disparaissent peu à peu jusqu’à n’être plus représentés que par des suites, comme ' (2008), ' (2009), ' (2010), ' (2010), ' (2013) et d'autres jeux Anno.

Le jeu de tactique en temps réel est un sous-genre des jeux de stratégie en temps réel qui met de côté les éléments de construction de base, de gestion des ressources et de recherche technologique de ces derniers pour se concentrer uniquement sur les combats. Ainsi, si les jeux de stratégie en temps réel incluent des éléments de stratégie et de tactique, les jeux de tactique en temps réel se focalisent uniquement sur la tactique et nécessite généralement de remplir un objectif avec un nombre prédéfini d’unités. Ils se caractérisent généralement par des mécanismes de combats plus riches qui mettent notamment l’accent sur la formation et le positionnement des unités et sur la prise en compte des avantages liés au terrain. 

Les jeux de tactique en temps réel trouvent leurs origines dans les années 1980 avec l’apparition des premiers ' tactiques dont l’action se déroule en temps réel. C’est notamment le cas du ' ' (1982) de Chris Crawford qui propose une simulation relativement simple et réaliste d’affrontements entre les Romains et les barbares à l’Antiquité. Si celui-ci ne rencontre à l’époque qu’un succès mitigé, son système de jeu préfigure un nouveau genre de jeu de stratégie, le jeu de tactique en temps réel, qui rencontre un premier succès avec le ' ' (1984) mais qui doit cependant attendre de plusieurs années avant d’être reconnu comme un genre à part entière. Si quelques jeux proposant un gameplay similaires sont publiés dans les années 1980, il faut attendre le début des années 1990 pour voir se développer les caractéristiques clés du genre avec des jeux comme ' (1989) de Westwood Studios, ' (1993) de Sensible Software, ' (1993) d’Electronic Arts et ' (1993) de Bullfrog Productions. Lorsque Strategic Simulations publie ' en 1995, le genre est ainsi déjà en grande partie codifié comme une variante plus rapide et plus fluide du jeu de tactique au tour par tour. C’est cependant l’influence des jeux de stratégie en temps réel et le succès de ' (1996) puis de ' (1997) qui permet au jeu de tactique en temps réel de s’imposer comme un genre à part entière, dérivé des STR et destiné à un public plus interessé par les combats que par la gestion des ressources. Ces succès ouvrent la voie à de nombreux jeux de tactique en temps réel. La plupart, comme "Blitzkrieg" (2003), "" (2004), "" (2004) et "Company of Heroes" (2006), ont pour cadre la Seconde Guerre mondiale mais le genre s’ouvre également à la science-fiction avec des jeux comme "MechCommander" (1998), "" (1999) ou "Ground Control" (2000).

Une arène de bataille en ligne multijoueur est un sous-genre des jeux de stratégie en temps réel qui oppose deux équipes dont chaque membre contrôle un unique personnage. Chaque camp est aidé par des unités contrôlées par l’ordinateur et tente de détruire la base adverse. Comme les jeux de tactique en temps réel, le genre met de côté les éléments de construction de base et de gestion des ressources. Il intègre en revanche des éléments de jeu de rôle. Les personnages contrôlés par les joueurs peuvent en effet accumuler de l’expérience qui leur permet d’améliorer leurs caractéristiques et de débloquer de nouvelles capacités. Le genre trouve son origine dans le mod ' (1999) pour "StarCraft" avant d’être popularisé par le mod ' (2003), son adaptation pour ' puis pour son extension '. Dans les années 2000, le succès de ' donne lieu a de nombreuses adaptation commerciales dont ' (2009), ' (2010), "Dota 2" (2013) ou encore ' (2015).



</doc>
<doc id="6555" url="https://fr.wikipedia.org/wiki?curid=6555" title="Hydrocarbure">
Hydrocarbure

Un hydrocarbure (HC) est un composé organique constitué exclusivement d'atomes de carbone (C) et d'hydrogène (H). Leur formule brute est donc de la forme : CH, sachant que n et m sont deux entiers naturels.

Sous forme de carbone fossile, ils constituent une ressource énergétique essentielle pour l'économie depuis la révolution industrielle, mais sont aussi source de gaz à effet de serre issus de leur utilisation massive.
Il s'agit de fait de ressources non renouvelables (à l'échelle chronologique humaine) dont les gisements commencent localement à s'épuiser ou à être très coûteux et difficiles à exploiter (gisements marins ou très profonds, et souvent de moindre qualité), qu'il s'agisse du charbon, du pétrole ou du gaz naturel.

On peut trouver des lacs d'hydrocarbures sur Titan, un satellite de Saturne et on trouve des taches d'hydrocarbures sur Pluton.

Remarque :
On utilise aussi le mot hydrocarbure pour faire référence, en particulier, au pétrole et au gaz naturel.

On distingue selon leur nature :

De plus, il existe plusieurs enchaînements possibles :

On distingue selon leur provenance :
Les 3 derniers de ces hydrocarbures forment en réalité un continuum (de qualité de plus en plus mauvaise du point de vue industriel et environnemental)

Les hydrocarbures saturés linéaires ou ramifiés possèdent la formule brute suivante: CH, où n est un nombre entier naturel non nul.
"Exemple : molécule de méthane, un atome de carbone : C1 d'où le nombre d'atomes d'hydrogène H(1*2+2) : CH"

Les hydrocarbures saturés cycliques possèdent une formule brute différente. Celle-ci varie en fonction du nombre de cycles que contient la molécule. S'il n'y a qu'un cycle : CH. S'il y en a deux : CH. Chaque cycle requiert une paire d'atome d'hydrogène en moins.
La formule brute générale est
CH c étant le nombre entier naturel de cycles.

Les hydrocarbures insaturés linéaires ou ramifiés possèdent la formule brute : CH, où n est un entier naturel non nul et i est le nombre entier d'insaturation.

Les hydrocarbures insaturés cycliques possèdent la formule brute suivante: CH, où n est un nombre entier naturel non nul et i est le nombre entier naturel d'insaturation, c étant le nombre entier naturel de cycles.

Radical en fonction du nombre de carbones




 0,63 < d < 0,77
 0,77 < d < 0,90




</doc>
<doc id="6556" url="https://fr.wikipedia.org/wiki?curid=6556" title="Château-Thierry">
Château-Thierry

Château-Thierry ( ) est une commune française située dans le département de l'Aisne, en région Hauts-de-France. 

Le gentilé historiquement juste est Castrothéodoricien qui provient de la forme latine "castrum" "theodorici", cependant l'usage a popularisé l'utilisation de Castelthéodoricien.

L'origine du nom de la ville est inconnue. La tradition locale l'attribue à Thierry IV, avant-dernier roi mérovingien, qui y aurait été enfermé par Charles Martel, sans source fiable. Château-Thierry est le lieu de naissance de Jean de La Fontaine et est située dans la région des .

L'arrondissement de Château-Thierry est appelé le pays de l'Omois.

Château-Thierry est l'une des soixante-quatre villes françaises à avoir reçu la Légion d'honneur.

Château-Thierry se situe dans un vallon de la vallée de la Marne. Dans ce secteur, l'urbanisation s'étale sur toute la vallée depuis le lit de la rivière, jusqu'en haut des coteaux.

La ville est donc en dénivelé, son altitude étant de au-dessus du niveau de la mer pour son point le plus bas, et de pour son point le plus haut.

La ville se situe aux confins de trois régions administratives :


Située administrativement dans la région Hauts-de-France, Château-Thierry appartenait à la Champagne jusqu'à la Révolution française. Il est vrai que le paysage de la ville fait vraiment penser à une ville champenoise, avec ses coteaux et ses vignes. Sa situation dans la vallée de la Marne comme son réseau de transports (autoroute et voie ferrée de Paris à Strasbourg) font de Château-Thierry une ville de l'Est de la France, dans la grande banlieue parisienne.

Voici quelques distances avec des villes voisines :


La Marne traverse la partie sud du territoire communal.

Celui-ci accueille la confluence avec le ru de Brasles, le ruisseau de Chierry et le ru de Bascon.

La commune de Château-Thierry regroupe en 2014. Elle est au centre d'une agglomération (ou unité urbaine) de et d'une aire urbaine de en 2014. L'aire urbaine est composée des communes suivantes : 
L'ensemble de ces communes forme le périmètre de l'ex-communauté de communes de la Région de Château-Thierry (CCRCT), quatrième agglomération du département après celles de Saint-Quentin, Soissons, et Laon.


La ville de Château-Thierry possède de bonnes infrastructures routières :

La circulation routière est proche de la saturation à Château-Thierry. Voici les problèmes de transport dans la ville :

La gare de Château-Thierry est le terminus d'une ligne de Transilien - le Transilien Paris-Est (Transilien P), mais est aussi une des gares de la ligne inter-régionale (Lorraine, Picardie, Champagne) de la ligne TER Vallée de la Marne.

La gare est l'une des plus fréquentées du département. Ceci est dû à la proximité de Paris, qui engendre d'importants flux journaliers entre le sud de l'Aisne et la capitale.

Un petit aérodrome, situé à au nord-ouest du centre-ville, dessert la ville. 

Le trafic de cet aérodrome n'est plus restreint aux appareils de tourisme en provenance des aérodromes et aéroports voisins. Il est possible de se rendre en avion-taxi de Reims à Château-Thierry.

La ville est divisée en différents quartiers, dont la création est plus ou moins récente.
La liste présentée correspond au découpage moderne de la ville.

C'est un des quartiers les plus récents de la ville, qui s'est véritablement développé à l'aube des années 1960. Il se situe à l'extrême nord de la ville, en haut de coteaux de la Marne. Ce quartier est une réelle "porte" d'entrée de la ville car il est installé près de l'échangeur de l'autoroute A4 et est traversé par l'une des principales voies de communication du département, la qui relie le nord du département au sud. Les Blanchards est constitué d'immeubles et de lotissements construits dans les années 1960 et 1970. C'est dans ce quartier que se trouve l'une des principales salles de spectacle de la ville, le Palais des Rencontres. Mais le quartier est assez isolé du reste de la ville, en raison de sa situation (en haut des coteaux). Deux petits bois y sont enclavés, ainsi qu'un coteau planté de vignes. Il abrite le nouveau cimetière de la ville.


Ce quartier se situe au nord-est de la ville. Il est constitué des Chesnaux, de la Charité (où se situe le pôle santé) et des Chopinettes (enclavé entre Château-Thierry et Brasles). Il est en partie installé sur les coteaux de la vallée. C'est l'un des quartiers les plus calmes de la ville, excentré, constitué de vieilles demeures en pierres meulières, de résidences HLM et de pavillons récents. Le vieux cimetière de la ville, ainsi que la nécropole nationale « Chesnaux », y est installé.

C'est un quartier constitué d'anciens hameaux qui ont été rejoints par la ville avec l'urbanisation croissante. 

Il conserve de cette période un certain charme et une architecture urbaine rappelant celle des villages de la région. De plus, comme il est situé sur les pentes de la vallée de la Marne, le panorama sur la ville augmente son attrait. Une grande partie du quartier est encore non-urbanisée ; on y trouve en particulier une grande surface boisée et la majeure partie des vignes implantées sur le territoire de la commune de Château-Thierry. Les flancs de la vallée servaient au Moyen Âge de carrière, dont une partie a été transformée et utilisé en tant que caves. La maison de Champagne Pannier, principal négociant de Champagne de la ville, a racheté ces caves en 1937 et installé son siège social à l'entrée de ces caves.

Les bâtiments modernes du lycée Jean-de-La Fontaine ont été construits en 1974 à la limite sud du quartier du Buisson.

C'est le quartier le plus animé de la ville, constitué de sous-quartiers : 
C'est le cœur historique et touristique de la ville, abritant presque tous les musées et monuments de la ville. C'est aussi un vaste espace commerçant. Ce quartier fut partiellement détruit lors de la Première Guerre mondiale.

Au nord du quartier de la Madeleine, se trouve le centre pénitentiaire de Château-Thierry.

Malgré son apparence, le quartier des Vaucrises est le quartier le plus ancien de la ville, c'est le berceau de Château-Thierry. Ce quartier était une ville à l'époque gallo-romaine. Ce site est appelé le site d'Otmus, on y a par exemple retrouvé les restes d'un théâtre. Mais ce quartier s'est vraiment développé en 1967, date de la construction de grands ensembles immobiliers et de lotissements. C'est l'un des quartiers les plus denses de la ville.

C'est l'un des plus pittoresque de la ville ; c'est un vrai village dans la ville. Sa place et ses petites rues lui donnent vraiment un caractère campagnard. Aujourd'hui on y trouve un nouveau lotissement, constitué de pavillons et d'immeubles.

Ce quartier est très apprécié pour son calme et ses conditions de vie. Il est exclusivement constitué de maisons et de lotissements. Courteau a été divisé en deux (Courteau et Bas-Courteau) lors de la construction de la Voie Express et la Mare-Aubry s'est modernisée en accueillant l'hypermarché Carrefour. Ce quartier est aussi composé de deux grandes avenues : l'avenue d'Essômes et l'avenue de Paris.

Aussi appelé faubourg de Marne, c'est un point de passage obligé pour traverser la Marne et rejoindre la gare et le quartier de l'Europe. Il se subdivise en trois parties :
L'île a été créée au , avec le creusement de la Fausse-Marne. Architecturalement, ce quartier est un mélange de constructions anciennes (collège Jean-Racine), d'immeubles datant de l'après-guerre (la rue Carnot), de pavillons et d'immeubles récents. C'est un haut-lieu du commerce qui jouit d'une jeunesse due à de nombreuses infrastructures scolaires.

Datant du , ce quartier s'est construit autour de deux axes : l'avenue de la République, une large avenue qui était auparavant la route principale de Paris à Épernay (ex-nationale 3), et l'avenue de la Gare, aujourd'hui avenue Wilson, qui relie la gare à la ville. On y rencontre à la fois des grandes demeures bourgeoises en pierres-meulières, dont on peut voir un exemple dans le film "Tatie Danielle", et des habitations plus récentes (barres d'immeubles et immeubles récents).

Il est aussi appelé quartier de la Prairie.

C'est le quartier le plus récent de la ville, il s'agit presque exclusivement d'une zone industrielle et commerciale.

Au , à partir de mentions du "pagus otmensis" dans des textes de la période carolingienne, les érudits locaux ont nommé Otmus l'ancienne ville d'origine antique. Cette hypothèse s'appuie aussi sur l'existence de monnaies mérovingiennes portant la mention . Étymologiquement, ce nom proviendrait d’"", qui indiquerait le rôle de marché de la ville. 

La première mention du nom Château-Thierry (' ou ') date de 923 dans les "Annales" écrites par Flodoard.

Pendant la période de la Terreur (29 octobre 1793), son nom fut changé et devint "Château-Égalité" puis "Égalité-sur-Marne".

Après cette période, Château-Thierry reprit son nom.

Les premières traces de civilisation datent de l'âge du fer. Des artefacts et une sépulture de cette période ont été découverts lors de fouilles archéologiques. Ces indices d'occupation se situent sous l'actuel quartier des Vaucrises et sur le plateau au nord de la ville, au nord du quartier des Blanchards. À ces premières occupations, succède un habitat groupé d'époque romaine ; une véritable agglomération secondaire prend place à partir du et perdure jusqu'au . Ce vicus est implanté au croisement de la Marne et de la route Soissons-Troyes, profitant probablement de la présence d'un passage à gué du cours d'eau. Il se situe sur la colline dites des Vaucrises. 

De nombreux vestiges de cette agglomération ont été mis au jour par des érudits locaux depuis le , puis par les fouilles archéologiques menées sur le territoire de la ville depuis 1986. Déjà observés en 1889, une opération d'archéologie préventive menée en 2000 a identifié et caractérisé les vestiges d'un théâtre. Toutes ces découvertes permettent d'estimer la surface de cette ville antique à environ .

Cette agglomération se situe sur le territoire des Suessionnes, et il s'agissait probablement d'un chef-lieu de pagus. Ce pagus a été identifié au comme le "pagus otmensis". De là vient le nom d"'Otmus", qui serait la contraction du nom "Odomagus". Cette hypothèse provient de monnaies mérovingiennes portant la mention "ODOMO FIT".

D'après les fouilles, la ville antique est abandonnée au ou au . L'hypothèse communément admise est une rétractation de la ville sur le versant sud de la colline des Vaucrises, autour de l'ancienne église Saint-Martin. Lors de la construction de l'école Jules-Maciet, en 1934, à l'emplacement de cette ancienne église, des sarcophages en plâtre ont été découverts. Ces sarcophages témoignent d'une occupation pouvant remonter à l'époque mérovingienne.

Des monnaies portant la mention "ODOMO FIT", dont quatre ont été découvertes à Château-Thierry témoignent peut-être de la pérennité d'une importante occupation à l'époque mérovingienne.

Au nord-est de l'ancienne église Saint-Martin, au lieu-dit les Chesneaux, une nécropole mérovingienne a été découverte et détruite en 1862, lors de la réfection de la route Château-Thierry-Soissons et en 1883 lors de la construction de la voie ferrée reliant Amiens à Dijon. Le mobilier découvert situe cette nécropole entre le et le . La question de la localisation de l'habitat correspondant à cette nécropole reste ouverte. Plusieurs hypothèses ont été avancées par les érudits locaux. Cependant, il faut écarter l'hypothèse d'un lien avec la ville antique puisque les datations ne correspondent pas. Certains ont voulu rapprocher cette nécropole de la présence supposée d'une résidence aristocratique située au toponyme "le Montmartel". Cependant aucune preuve historique ou archéologique n'est venue étayer cette légende locale. La seule occupation connue à proximité et daté de l'époque de l'utilisation de cette nécropole est la butte sur laquelle le château médiéval fut construit.

À l'époque carolingienne, le territoire de l'actuelle ville de Château-Thierry est étroitement lié à la puissante famille des Herbertiens. Herbert est un descendant de Charlemagne par Pépin d'Italie et par sa mère, il est l'héritier de la puissante famille des Thierry. La famille des Herbertiens est à l'origine de la formation du comté de Champagne. Herbert a reçu le "pagus otmensis", dont Château-Thierry est le chef-lieu.

En 910, Herbert II de Vermandois hérite de son grand-père. En 918, il reçoit par son mariage avec Adèle le comté de Meaux. C'est de l'époque d'Herbert II que date la première attestation d'un site fortifié et du nom de Château-Thierry dans les Annales de Flodoard. En effet, en 923, le comte y enferme Charles III, dit le Simple, pendant quatre ans. Mais après un incendie de la tour, où il est enfermé, le roi est transféré à Péronne. Entre 933 et 936, lors d'un conflit avec Raoul, roi de Bourgogne, la forteresse est assiégée par deux fois. Elle revient à Herbert II, en 936, à la mort de Raoul.

À la mort d'Herbert II, son fils Herbert III dit le Vieux, comte de Troyes et de Meaux, hérite de la forteresse de Château-Thierry avec le comté d'Omois et l'abbaye Saint-Médard de Soissons.

Herbert III meurt en 980/984 sans héritiers. Le roi Lothaire partage le domaine entre les neveux d'Herbert III : Eudes, Herbert le Jeune et son frère Albert. Eudes reçoit le comté de l'Omois et l'abbaye Saint-Médard.

À la mort d'Eudes , en 996, c'est son fils Eudes II qui lui succède. En 1004, il hérite du comté de Blois avec Tours et Chartres en plus de Provins, Château-Thierry, Reims et du Tardenois. En 1021, il est fait comte de Troyes et de Meaux par Robert le Pieux.

Étienne II, son fils cadet, lui succède à sa mort en 1037.

Étienne meurt entre 1044 et 1048, léguant à son fils mineur Eudes III ses possessions. Thibaud III, son frère et comte de Blois assure la régence. Eudes III épouse, en 1060, la sœur de Guillaume le Conquérant, Adélaïde. Pendant qu'il participe à la conquête de l'Angleterre en 1066, son frère Thibaud III s'empare de ses comtés champenois. Guillaume le Conquérant pour compenser ses pertes lui donne le comté d'Aumales en Normandie et celui d'Holderness en Angleterre.

Hugues hérite du comté de Champagne de son père Thibaut III, à la mort de ce dernier en 1089. Hugues est le premier à porter le titre de comte de Champagne. En 1125, il se fait templier et c'est son neveu Thibaud II qui reçoit la Champagne. Entre 1064 et 1124, Château-Thierry est tenu en fief par une dynastie de chevalier nommés Hugues. Mais à partir de l'arrivée de Thibaud II de Champagne, le château et la ville qui l'entoure dépendent directement du comte.

Au Moyen Âge, existe dans la ville une communauté juive comptant plusieurs intellectuels influents. Ainsi, une célèbre école rabbinique y est construite et d'où émergeront de nombreux tossafistes au tels que Samuel d'Évreux.

Château-Thierry demeure une ville et une forteresse du comté de Champagne jusqu'en 1285, date à laquelle ce dernier passe au domaine royal par le mariage de Jeanne de Navarre avec Philippe IV le Bel.

Pendant la guerre de Cent Ans, la ville est tenue par le duc de Châtillon pour le parti anglais avant de se rendre à Jeanne d'Arc en 1429 sans combattre, la ville avait été ruinée par cette occupation. En 1544, la ville est prise et pillée par Charles Quint. Jusqu'à la Révolution française, elle appartient à la Champagne.

Elle devient chef-lieu de district en 1790. En 1800, elle devient chef-lieu d'arrondissement qui est supprimé en 1926 et restauré en 1942.

Le chemin de fer (ligne Paris-Strasbourg dessert la ville dès 1849 et favorise son développement.

Château-Thierry a été le site d'une importante bataille en 1814. Le 12 février, les armées russes et prussiennes qui ont été repoussées par Napoléon à Montmirail, effectuent leur retraite vers le nord, tentent d'empêcher les troupes françaises de franchir le pont sur la Marne. Les combats se déroulent dans les rues de la ville, les Russes et les Prussiens sont repoussés vers Soissons. Au matin du 13 février, Napoléon établit son logement à l'auberge de la poste qu'il quitte dans la nuit.

Château-Thierry a été l'un des points clés durant les batailles de la Première Guerre mondiale en 1918, entre les troupes américaines et les troupes allemandes. Le juin 1918, durant la troisième bataille de l'Aisne, la division d'infanterie coloniale et la division d'infanterie américaine arrêtent l'offensive allemande.

La ville était située sur le front de 1918, matérialisé par les bornes Vauthier. Le peintre officiel des armées françaises François Flameng y a réalisé de nombreux croquis et dessins sur ces douloureux événements qui seront publiés dans la revue "L'Illustration".

La ville a aussi été le théâtre de combats lors de la bataille de France (1940) durant la Seconde Guerre mondiale. Le pont principal de Château-Thierry a été défendu par les hommes de l'Aspirant de Rougé. Le nouveau pont construit à la place porte son nom.
La commune fait partie depuis 1942 de l'arrondissement de Château-Thierry du département de l'Aisne. Pour l'élection des députés, elle dépend depuis 1986 de la cinquième circonscription de l'Aisne.

Elle était depuis 1793 le chef-lieu du canton de Château-Thierry. Dans le cadre du redécoupage cantonal de 2014 en France, la commune est désormais le bureau centralisateur de ce canton, dont la composition a été modifiée.

La commune était le siège de la communauté de communes de la Région de Château-Thierry (CCRT), créée le , et qui, en 2016 regroupait 25 communes.

Dans le cadre des dispositions de la loi portant nouvelle organisation territoriale de la République (Loi NOTRe) du 7 août 2015, qui prévoit que les établissements publics de coopération intercommunale (EPCI) à fiscalité propre doivent avoir un minimum de habitants, la CCRT fusionne avec plusieurs petites intercommunalités.

C'est ainsi qu'est créée le la communauté d'agglomération de la Région de Château-Thierry (CARCT), qui compte à cette date 87 communes, et dont Château-Thierry est désormais le siège.

La liste de Jacques Krabal (PRG, divers-gauche) est arrivée première aux municipales de 2008 à Château-Thierry, avec près de 52 % des voix, devançant la liste du PS, menée par le maire sortant Dominique Jourdain (près de 34 % des voix) et celle de l'UMP, dirigée par Robert Djellal (sous-préfet de Château-Thierry en 2006 qui entre ensuite au cabinet ministériel d'Azouz Begag puis d'Éric Besson), qui n'obtenait que 11,94 % des voix.

La ville a engagé une politique de développement durable en lançant une démarche d'Agenda 21 en 2007.

La ville de Château-Thierry est jumelée avec :

Depuis 2009, un rapprochement important s'est également effectué avec la ville d'Indianapolis aux États-Unis.


Le maximum de population a été atteint en 1990. Depuis, le nombre d'habitants a légèrement régressé.

La population de la commune est relativement âgée. Le taux de personnes d'un âge supérieur à 60 ans (22,6 %) est en effet supérieur au taux national (21,6 %) et au taux départemental (21,2 %).
À l'instar des répartitions nationale et départementale, la population féminine de la commune est supérieure à la population masculine. Le taux (53,2 %) est supérieur au taux national (51,6 %).

La répartition de la population de la commune par tranches d'âge est, en 2007, la suivante :

Château-Thierry dépend de l'académie d'Amiens.


Au premier degré d'éducation, la ville dispose de neuf écoles maternelles (dont une privée) et également de neuf écoles élémentaires (même remarque). Pour le secteur secondaire, la ville possède deux collèges publics (collège Jean-Rostand au nord de la ville et Jean-Racine sur l'Ile) et un collège privé (également sur l'Ile). La ville dispose aussi de deux lycées publics (lycée polyvalent Jean-de-La Fontaine et la cité technique Jules-Verne) ainsi qu'un lycée privé (lycée général et hôtelier Saint-Joseph) et un lycée agricole et viticole (à Crézancy).

Six Brevets de Technicien Supérieur (BTS) sont présents à Château-Thierry :


La ville dispose aussi d'un institut de formation de soins infirmiers, d'un Centre de formation d'apprentis (CFA) et d'un Centre de formation professionnelle et de promotion agricole.

La ville dispose d'une inspection primaire de l'Éducation nationale, d'un Centre de Formation et d'Information et d'un relais information de l'INSEE.

L’hôpital de Château-Thierry a été fondé en 1304 par Jeanne de Navarre, qui est actuellement l'hôtel-Dieu (pour voir l'histoire de l'hôtel-Dieu, cf. "supra").

Aujourd'hui, la ville est équipée d'un centre hospitalier général qui se situe dans un "pôle santé". Ledit pôle est constitué du centre hospitalier, des urgences, d'une maternité, d'une maison de retraite ainsi que d'une halte-garderie actuellement en construction. Un centre de secours ainsi que d'autres infrastructures devrait compléter ce "pôle santé".
L'hôpital de Château-Thierry, selon l'hebdomadaire "L'Express", est l'un des meilleurs établissements de soins français inférieurs à 300 lits. Sur le classement, il se situe en effet à la , sur 326. De plus, l'hôpital de Château-Thierry est de très loin le mieux placé dans l'Aisne pour les établissements inférieurs à 300 lits.

Un autre hôpital se situe à proximité de Château-Thierry, il s'agit de l'hôpital de Villiers-Saint-Denis, la Renaissance Sanitaire. Cet hôpital fut créé en 1930 par Almire Breteau et avait à l'époque une capacité de 742 lits, réservés aux hommes. Aujourd'hui sa capacité est de 405 lits. Cet hôpital est spécialisé dans la réadaptation cardiaque, la réhabilitation pneumologique (étude du sommeil), les soins palliatifs, accidents vasculaire et problèmes liés au diabète, la médecine physique et réadaptation, les consultations anti-douleur et anti-tabac et les hospitalisations de jour.

Aujourd'hui, il existe une vraie coopération entre le centre hospitalier général et la Renaissance Sanitaire.

La ville dispose également d'une clinique.


La ville de Château-Thierry possède un service et un office municipal des sports ainsi que plusieurs stades municipaux, quatre grands gymnases, un palais des sports situé près du mémorial Jean Moulin, un centre d'activités, une piscine (datant de 1970, une autre est à prévoir), un skatepark, un gymnase nautique et un centre d'activités.

De nombreux clubs de sports sont présents à Château-Thierry (une cinquantaine) qui regroupent environ cinq mille licenciés.

Voici quelques clubs castels :



La police nationale compétente sur les zones urbaines où les problématiques de sécurité sont particulièrement prégnantes, dispose d'une implantation sur la commune de Château-Thierry. Le commissariat de police, autrefois implanté en centre-ville, est désormais situé dans le quartier des Vaucrises. La police nationale assure une présence H24 et 365 jours par an. Rattaché à la direction départementale de la sécurité publique (DDSP) de l'Aisne, il est placé sous l'autorité du commandant de police Louis Vitone depuis le mois de janvier 2011.

Il assure la sécurité publique sur la seule commune de Château-Thierry. Les communes situées dans la continuité urbaine relèvent de la gendarmerie nationale chargée de la sécurité dans les secteurs ruraux.

Nous retrouvons à Château-Thierry la présence des trois secteurs d'activités :

Son bassin d'emplois regroupe 135 entreprises de plus de 10 salariés dont 25 de plus de 100 salariés. De plus, la ville compte 220 commerçants et artisans. La ville possède quatre zones d'activités, ce qui représente aménageables, 60 entreprises et plus de emplois.
Aujourd'hui la ville se développe dans la logistique (dans la ZID de l'Omois, à la sortie de la A4) avec l'arrivée du groupe FM logistics.

La ville a créé deux grandes associations :

Château-Thierry compte parmi ses entreprises célèbres :


Elle comprend un centre consulaire de la Chambre de commerce et d'industrie de l'Aisne au 1, avenue de l'Europe.


Point d'origine de la ville moderne, le château domine la vallée de la Marne. Construit à partir du , il a été profondément remanié au fil du temps. Aujourd'hui, le vieux château est un lieu de promenade et héberge un spectacle de fauconnerie.

L'église Saint-Crépin est la seule église de Château-Thierry, qui en comptait pourtant trois sous l'Ancien Régime (détruites après la vente des biens nationaux, en 1793).
Elle était initialement (au ) implantée hors des murs de l'enceinte, mais est maintenant située un peu à l'est du centre-ville. La tour du clocher, haute de , est visible depuis les rives de la Marne.


La tour Balhan est un vestige d'un hôtel urbain : "l'hôtel du mouton d'or" (construit par Jean Balhan en 1480). Elle est classée monument historique depuis 1926. La tour renferme une cage d'escalier à vis, une ancienne chapelle et une salle de guet.

Ce qui reste, ou ce qui a été restauré du Fort Saint-Jacques, résidence des comtes de Champagne qui le préféraient au château lorsqu'ils venaient, de Provins, passer quelques jours chez nous.

Une tour carrée de de haut, surmontée d'un toit en forme de flèche octogonale couvert d'ardoises. À l'est, deux tourelles rondes, pourvues de toits hexagonaux. À l'ouest, deux petites pyramides triangulaires fixées sur la tour.

Un escalier intérieur bien conservé conduit à une chapelle gothique puis à l'ancienne chambre du guet et enfin, au pied de l'échelle qui permet d'accéder à la plate-forme du campanile.

On célébra la messe dans la chapelle jusqu'à la Révolution. La tour Balhan, qui tient son nom de Balchan ou Balhan, maître du grenier à sel de Château-Thierry à la fin du et sans doute, donateur d'une cloche, a joué dans l'histoire de la ville bien plus le rôle d'un beffroi que celui d'un poste de défense.

Depuis 1874, deux cadrans extérieurs complètent bien l'ensemble « horloge communale » dont la tour s'est acquise le privilège au cours des siècles.


Ancien hôpital de la ville, aujourd'hui transformé en musée. Il a été fondé en 1304 par Jeanne de Navarre.


Construit en 1933 sur la Cote 204, à à l'ouest de Château-Thierry, ce monument domine la ville et la vallée, et offre une vue étendue de celle-ci.

Le musée Jean-de-La-Fontaine est consacré au fabuliste et situé dans sa maison natale. L'auteur de "La Cigale et la Fourmi" y naquit en 1621.


Construit par l'architecte Jean Bréasson, il fut achevé en 1893 et inauguré le 15 mars 1893 par Raymond Poincaré, qui était alors ministre de l'Instruction publique. Architecture d'inspiration Renaissance, il remplace une bâtisse beaucoup plus petite, qui était partagée entre l'autorité publique et judiciaire. Il se situe sur la place de l'Hôtel de Ville, là où se trouvent le cinéma-théâtre, le marché couvert, le temple protestant, les imposants escaliers montant jusqu'au vieux château.

C'est aussi sur cette place que se rejoignent la Grand Rue, allant vers la maison natale de Jean de la Fontaine, et la rue du Château, se dirigeant vers la porte Saint-Pierre.
La place de l'Hôtel-de-Ville compte aussi parmi ses monuments, la façade du cinéma-théâtre classé "Art déco". Proposant des films en 2D et 3D, le cinéma-théâtre est l'unique lieu de projection cinématographique de la ville.


Pendant la Première Guerre mondiale, une Église méthodiste américaine a créé un fonds pour le soutien moral des soldats engagés sous la bannière étoilée. Le solde des sommes récoltées a servi à construire ce temple inauguré en 1924.

L'un des vitraux du temple représente La Fayette, Foch, Joffre, Pétain et Nivelle.

Cette porte construite dans la première moitié du est la seule rescapée d'une série de quatre qui était incluses dans les fortifications de la ville. Les autres furent détruites pendant la Révolution.


Cette porte est l'entrée principale du château médiéval.


La médiathèque est située dans l'ancien couvent des Cordeliers. Construit en 1496 le monastère est consacré par l'évêque de Soissons. À la Révolution, les moines sont chassés et le couvent est ensuite réquisitionné en 1792 par les autorités révolutionnaires : il devient le tribunal du district de Château-Thierry.

En 1804, la municipalité y ouvre une école d'enseignement secondaire. Dans les années 1980, l'école est fermée, et devient le centre culturel Jean-Macé.

Inaugurée le 24 janvier 2004, la médiathèque Jean-Macé a pris le relais de la bibliothèque municipale.



"vallée de la Marne" reliant Château-Thierry à Épernay. Ce circuit à une distance de .

3 fleurs attribuée en 2007 par le Conseil des Villes et Villages Fleuris de France au Concours des villes et villages fleuris.

De plus, le rond-point construit en 2007 à l'entrée nord de la ville a gagné en 2008 le titre de plus beau rond-point de France. 
Ce rond-point est composé au milieu d'une reproduction miniature du vieux-château avec autour des pieds de vignes représentant les coteaux alentour.

La ville a remporté pour la seconde fois le prix du plus beau rond-point de France en 2012 pour l'aménagement des anciens carrefours de l'avenue Paul-Lefebvre. Le prix récompense les nouveaux carrefours réalisés sur le thème de "La Cigale et la Fourmi", "Le Lièvre et la Tortue", "La Grenouille qui se veut faire aussi grosse que le bœuf"…

Château-Thierry est le lieu de naissance de :

Autres personnalités :





</doc>
<doc id="6558" url="https://fr.wikipedia.org/wiki?curid=6558" title="Alcool allylique">
Alcool allylique

Un alcool allylique est un composé organique portant un groupement hydroxyle en position allylique. L'alcool allylique (2-propèn-1-ol) CHO ou HC=CH-CH-OH est un liquide incolore à odeur piquante (point d'ébullition ). On le trouve dans les fruits en décomposition.

On le prépare industriellement par hydrolyse à la soude du chlorure d'allyle, obtenu par chloration radicalaire du propène. Il est utilisé comme matière première de résines, de plastifiants.



</doc>
<doc id="6562" url="https://fr.wikipedia.org/wiki?curid=6562" title="Ontario">
Ontario

L'Ontario (en anglais ) est la province la plus peuplée du Canada, comptant près de 13,5 millions d'habitants en 2016. Elle se trouve dans la partie centre-est du pays et a pour capitale Toronto (également la plus grande ville canadienne), située dans le sud de la province. Vers le sud et le sud-ouest, le territoire de la province partage une longue frontière avec les États-Unis (États du Minnesota, du Michigan, de l'Ohio, de la Pennsylvanie et de New York), dont une grande partie est constituée par les lacs Supérieur, Huron, Érié et Ontario. À l'ouest, la province est bordée par le Manitoba tandis qu'à l'est, elle partage une frontière avec le Québec.

Ottawa, capitale fédérale du Canada, se situe sur son territoire, à la confluence de la rivière des Outaouais. Si le Sud est très urbanisé avec la Golden Horseshoe, le Nord et l'Ouest de la province sont en revanche assez ruraux.
C'est la deuxième province la plus riche du pays après l'Alberta ; si elle était indépendante, elle serait la économique du monde, devant la Suisse.

Sa superficie est du même ordre de grandeur que celle de l'Égypte.

La province fut nommée d'après le lac Ontario, un terme dérivant de "Ontarí:io", mot venant de la langue huronne et signifiant « grand lac » ou bien « belle eau scintillante ».

Avant l'arrivée des Européens, la région, sans villages, était habitée par les peuples algonquiens (les Saulteux, les Cris et les Algonquins) et iroquoïens (les Iroquois, les Hurons et les Neutres).

L'explorateur français Étienne Brûlé explora une partie de la région de la baie Georgienne de 1610 à 1612. L'explorateur anglais Henry Hudson navigua sur la baie d'Hudson et la baie James en 1611, ce qui permit à l'Angleterre de revendiquer les alentours, tandis que l'explorateur français Samuel de Champlain atteignit le lac Huron en 1615 et les missionnaires français commencèrent à établir des missions aux abords des Grands Lacs. L'exploration française fut entravée par les hostilités avec les Iroquois, qui s'allièrent plus tard aux Anglais.

La Grande-Bretagne établit des comptoirs à la baie d'Hudson vers la fin du , commençant une lutte pour la domination de l'Ontario. Le traité de Paris, en 1763, mit fin à la guerre de Sept Ans en cédant presque tout l'Empire français en Amérique (la Nouvelle-France) aux Britanniques et aux Espagnols. La région aujourd'hui appelée Ontario fut annexée au Québec en 1774. À la suite de la guerre d'indépendance américaine, plusieurs colons américains demeurés loyaux, loyalistes à la couronne britannique vont immigrer dans la province de Québec. Ces derniers, désirant vivre selon les coutumes britanniques vont revendiquer des amendements à l’acte de Québec de 1774. La loi constitutionnelle de 1791 scinda le Québec en deux parties, les Canadas :

Les troupes américaines de la guerre de 1812 incendièrent Toronto en 1813. Après la guerre, beaucoup d'immigrants britanniques vinrent s'installer en Haut-Canada, et commencèrent à s'irriter contre l'aristocratique "Family Compact" qui gouvernait la région, de même que la "Clique du Château" gouvernait au Bas-Canada. Alors, la rébellion en faveur du gouvernement responsable se leva aux deux régions, sous Louis-Joseph Papineau par les Patriotes canadiens-français au Bas-Canada, et sous William Lyon Mackenzie au Haut-Canada par les « "Patriots" » écossais.

Bien que les deux rébellions fussent écrasées, le gouvernement britannique envoya lord Durham pour enquêter sur les causes des émeutes. Il recommanda l'octroi d'autonomie politique et la refusion des colonies afin d'assimiler les Canadiens (le terme canadien-français va apparaître plus tard car les anglophones se considéraient encore comme des Anglais), en leur rendant impossible une majorité en chambre par la fusion des deux assemblées législatives. Les deux colonies furent alors fusionnées, par l'acte d'Union, dans la province du Canada-uni en 1840, avec l'Ontario sous le nom de Canada-Ouest. Le gouvernement parlementaire autonome fut octroyé en 1848.

Craignant une possible agression américaine causée par la fin de la guerre de Sécession et les idées expansionniste de certains dirigeants américains mais aussi à cause de l'instabilité politique dans la colonie et le besoin de créer un marché intérieur à la suite du non-renouvellement du traité de réciprocité avec les États-Unis (1854-1864), le Canada-Uni, le Nouveau-Brunswick et la Nouvelle-Écosse décidèrent de fusionner en 1867 pour former un pays, à savoir le Canada. Le conflit soutenu entre les deux parties de la province du Canada causa leur séparation : elles entrèrent elles aussi dans la fédération (nommée à tort "confédération)" comme deux provinces distinctes, l'Ontario et le Québec.

À la fin du , la partie du Nord-Ouest fut attribuée à l'Ontario, mais il y reste un fort courant séparatiste qui voudrait qu'elle soit rattachée au Manitoba.

Commençant avec la construction du chemin de fer transcontinental à travers les Grandes plaines jusqu'à la Colombie-Britannique, l'industrie ontarienne connut un grand essor. L'exploitation minière commença au début du . Le mouvement nationaliste au Québec poussa plusieurs sociétés commerciales à migrer vers l'Ontario. Toronto remplaça alors Montréal comme métropole et centre économique du Canada.

Les partis politiques provinciaux principaux sont les progressistes-conservateurs, les libéraux, et les néo-démocrates. Les "droitistes" progressistes-conservateurs de Mike Harris détrônèrent les "gauchistes" néo-démocrates en 1995 ; le gouvernement Harris mit en œuvre un programme libéral de coupures dans les dépenses sociales et d'abaissement des taxes (la « Révolution du bon sens »). Cette politique équilibra le budget mais fut dénoncée pour avoir entraîné une hausse de la souffrance et de la pauvreté, surtout à Toronto. En particulier, les critiques de ce gouvernement blâment les coupures au ministère de l'Environnement pour son manque de surveillance, responsable de la « tragédie de Walkerton », une épidémie d"'E. coli" causée par l'eau contaminée à Walkerton, qui causa plusieurs morts et maladies en mai 2000. Harris quitta son poste en 2002 et fut remplacé par Ernie Eves. Les conservateurs furent défaits l'année suivante par le Parti libéral aux élections de 2003 avec comme Premier ministre actuel Dalton McGuinty. Réélu en 2006, il démissionna en janvier 2013 et fut remplacé par Kathleen Wynne.

Aujourd'hui, c'est Justin Trudeau qui est le premier ministre Canadien il a été élu en 2015

Dès l'époque de la Nouvelle-France, le territoire actuel de l'Ontario est parsemé d'implantations françaises qui sont à l'origine de nombreuses agglomérations ontariennes aujourd'hui.

On retrouve plusieurs types d'implantation en Ontario : des religieuses, des militaires et des civiles.

Militairement, on retrouve entre autres le Fort Niagara, le Fort Douville, le Fort Frontenac, le Fort Portneuf, le Fort Rouillé, le Fort Saint-Louis, le Fort Caministigoyan, le Fort Sainte-Anne, le Fort Michipicoton, le Fort Saint-Pierre et le Fort Tourette.

Du point de vue religieux, le meilleur exemple d'implantation est la mission jésuite française de Sainte-Marie-du-Sault qui s'établit en 1668.

À cela il faut ajouter des implantations telles le Fort Pontchartrain du Détroit fondé en 1701 par le Sieur Cadillac sur les rives de la rivière Détroit. Pour le côté ontarien, on verra en 1748 la rive canadienne étendre ainsi cette implantation française.

Aujourd'hui, ces implantations françaises en Ontario sont devenues sous le régime anglais et donc aujourd'hui au Canada des villes telles Windsor, Sault-Sainte-Marie, Niagara Falls et plusieurs autres.

L'Ontario est bordé au nord par la baie d'Hudson, à l'est par le Québec, à l'ouest par le Manitoba et au sud par les États américains du Minnesota, Michigan, Ohio, Pennsylvanie et New York. La plus grande partie de la frontière américaine se trouve dans les quatre Grands Lacs limitrophes : le lac Supérieur, le lac Huron (incluant la baie Georgienne), le lac Érié et le lac Ontario, qui donna à la province son nom ; ainsi que dans le fleuve Saint-Laurent.

La capitale de l'Ontario et sa métropole est Toronto, le composant principal de la conurbation dite le « Golden Horseshoe » (le Croissant d'or) autour de l'extrémité ouest du lac Ontario. La capitale du pays, Ottawa, se trouve à l'extrême est de la province, sur la rivière des Outaouais, qui constitue la plus grande partie de la frontière québécoise.

La ville de Niagara Falls et les chutes du Niagara se trouvent sur la frontière new-yorkaise, près de Buffalo (New York, États-Unis).

La province est constituée de trois régions géographiques principales : le Bouclier canadien aux portions occidentale et centrale, une région majoritairement infertile, riche en minéraux et parsemée de lacs et de rivières ; la basse-terre de la baie d'Hudson au nord-est, principalement marécageuse et boisée ; et la région la plus populeuse (90 %) et tempérée, la vallée des Grands-Lacs et du Saint-Laurent, au sud-est. L'industrie et l'agriculture se concentrent dans cette région, avec son accès à l'océan Atlantique assuré par la voie maritime du Saint-Laurent. Le point culminant de la province est la crête Ishpatina () dans le district de Sudbury.

Le premier Premier ministre de l'Ontario a été le libéral Georges William Ross (libéral) de 1899 jusqu'en 1905.

La Première ministre actuelle de l'Ontario est Kathleen Wynne.

La politique de l'Ontario est gouvernée par une législature monocamérale, l'Assemblée législative de l'Ontario, qui opère dans un style de gouvernement de type Westminster. Normalement, le parti politique qui gagne le plus grand nombre de sièges dans la législature forme le gouvernement, et le chef de ce parti devient le Premier ministre de la province (c'est-à-dire le chef du gouvernement).

Les fonctions de la reine, la Reine Élisabeth II, sont exercées par le lieutenant-gouverneur de l'Ontario. Le lieutenant-gouverneur est nommé par le gouverneur général du Canada sous la recommandation du Premier ministre du Canada.

Le Parlement ontarien compte 107 sièges et les élections y sont tenues à date fixe. Lors de la dernière élection le le Parti libéral, conduit par Kathleen Wynne, a obtenu un quatrième mandat consécutif, avec une majorité gouvernementale formée de 58 députés.

Le pouvoir judiciaire en Ontario s'oriente autour de trois tribunaux officiels :

L'Ontario fait partie de l'Assemblée parlementaire de la francophonie. Le Ministère des affaires francophone de l'Ontario s'assure de son côté de l'accessibilité des ressources gouvernementales en français sur le territoire ontarien dans le respect du patrimoine linguistique et culturel des francophones de l’Ontario.

L'Ontario est représentée au Parlement du Canada par 120 députés et par 21 Sénateurs.

L'industrie principale de la province est la fabrication, se localisant particulièrement au Golden Horseshoe, la région la plus industrialisée du pays. Des produits d'importance particulière incluent les automobiles, le fer, l'acier, la nourriture, les appareils électriques, la machinerie, les produits chimiques et le papier. Le secteur haute-technologie est aussi important, surtout dans les régions de Waterloo et d'Ottawa. L'agriculture est aussi signifiante dans le Sud-Ouest et la vallée du Saint-Laurent, et l'industrie minière, surtout autour de Sudbury, est importante dans le Bouclier canadien. Les rivières de l'Ontario le rendent riche en énergie hydroélectrique. La filière Candu des centrales nucléaires, en difficulté, ne permet pas de fermer les énormes centrales au charbon dont celle de Nanticoke.

De son côté la Chambre de Commerce de l'Ontario est présente depuis plus d'un siècle pour incarner la voix, de façon "indépendante et non-partisane", de la communauté économique de la province. Sa mission est de supporter la croissance économique de l'Ontario en défendant les priorités du marché.

Le budget de 2017-2018 de l’Ontario est de 141,2 milliards de dollars. Ce budget prévoit entre autre une augmentation de 145,0 millions de dollars accordés dans le cadre de l’accord bilatéral sur l’apprentissage et la garde des jeunes enfants.

En 2016, la population de l'Ontario est de . La croissance démographique annuelle est de 1,1 %. L'espérance de vie est de 83,6 ans pour les femmes et 79,2 ans pour les hommes.

Près de 6 millions de personnes vivent dans la région métropolitaine de Toronto.

L'immigration depuis tous azimuts, surtout vers Toronto et ses banlieues, est en train de diversifier rapidement la composition ethnique de la province.

La religion chrétienne est la religion la plus importante dans la province de l'Ontario. Par contre il faut diviser environ en deux le nombre de chrétiens si on veut les répartir selon le mode catholique () ou protestant ().

La présence de protestants en Ontario, comme partout au Canada, impose aussi une dualité historique entre l'Église anglicane et catholique en Ontario.

Ainsi les catholiques sont représentés par l'Assemblée des évêques catholiques de l’Ontario et les anglicans par le Diocèse anglican de l'Ontario.

Au niveau des aliments, on retrouve beaucoup de légumes et de fruits (tomates, pêches, fraises, cerises et raisins ) en Ontario. On retrouve aussi de la viande, du poisson, du fromage et des produits laitiers. Enfin le vin est pour sa part une boisson propre à la province.

Parmi les plats les plus représentatifs de l'Ontario on peut noter le rostbeff et les tartes au beurre.

La province de l'Ontario a une forte prédominance anglaise, selon les études de Statistique Canada effectuées sur la période 1971-2006, 69,1 % des Ontariens déclarent avoir pour langue maternelle l'anglais, 4,2 % le français et 26,6 % des tierces langues (italien, allemand, russe, arabe, pendjabi). Selon Statistique Canada en 2011, 11 % de la population ontarienne parle anglais et français.

Plus de francophones ("Franco-Ontariens") habitent la province, la plus grande communauté francophone canadienne hors du Québec en nombre absolu mais pas en pourcentage (les francophones ne représentent que 3 à 4 % de la population ontarienne, à comparer aux 33 % de francophones du Nouveau-Brunswick, province officiellement bilingue). Seulement 62 % (recensement 2001) des Franco-Ontariens utilisent encore le français comme langue d'usage, soit environ personnes « effectivement » francophones (les autres, soit 38 %, ont adopté l'anglais comme langue d'usage).

La population de langue française se trouve surtout concentrée dans les régions situées à proximité de la frontière avec le Québec, c'est-à-dire dans l'Est ontarien (le long de la rivière des Outaouais) et le Nord-Est ontarien. Dans un seul comté de l'Ontario, celui de Prescott-Russell (jouxtant le Québec), les francophones sont numériquement majoritaires (à environ 66 %), ce qui constitue un cas unique pour toutes les provinces du Canada se trouvant à l'ouest de la rivière des Outaouais.

La communauté francophone a le droit aux écoles et aux conseils scolaires en français, institutions gérées par les Franco-Ontariens eux-mêmes. Au niveau de l'enseignement supérieur, il y a cinq universités bilingues où le français est une langue officielle : l'université d'Ottawa, l'université Saint-Paul, toutes deux situées à Ottawa, le collège universitaire Glendon (de l'université York) à Toronto, l'université Laurentienne à Sudbury et le collège militaire royal du Canada à Kingston. De plus, l'université de Hearst, affiliée à l'université Laurentienne, offre des cours en français dans une variété de programmes à ses campus de Hearst, Kapuskasing et Timmins.

Depuis 1986, la "Loi sur les services en français" garantit au public le droit de recevoir en français les services du gouvernement provincial. En cela, le Gouvernement de l'Ontario a fait des efforts considérables et réels pour améliorer et renforcer le statut du français en Ontario.

De plus, il a tenu compte de l'influence et de la ténacité des organismes franco-ontariens pour veiller à respecter les droits linguistiques effectifs des francophones : par exemple, dans l'affaire de l'hôpital Montfort, unique hôpital francophone de la région d'Ottawa et institution très importante pour les Franco-Ontariens, que le Gouvernement ontarien avait prévu de fermer à la fin des années 1990, les autorités provinciales ont dû renoncer en raison de la mobilisation de l'ensemble de la communauté et des poursuites judiciaires, ce qui a permis à l'hôpital Montfort de demeurer ouvert aujourd'hui.

De nombreux étudiants anglophones qui habitent dans la province ont une connaissance scolaire du français (voire des connaissances approfondies), et la langue est apprise comme langue seconde (après leur langue maternelle, qui est l'anglais) par plusieurs milliers d'Ontariens anglophones, surtout à l'est de la province (proche du Québec). Comme partout au Canada anglophone, le français est la langue seconde privilégiée dans le système éducatif : si une minorité d'anglophones ontariens sont parfaitement bilingues (bien que ce nombre et le pourcentage augmentent sans cesse), en revanche la très grande majorité des Ontariens apprennent le français à l'école et sont donc généralement capables de lire, de comprendre et de parler la langue dans le cadre de conversations simples.

Cependant, une nouvelle définition de la francophonie ontarienne, entrée depuis peu en vigueur, augmente le nombre de francophones de : cela tient à l'élargissement de la définition provinciale de ce qu'est un francophone. Un francophone ontarien sera donc, désormais, une personne qui a le français pour langue maternelle, pour langue d'usage, ou qui le connaît sans l'avoir pour langue maternelle. Ceci afin de ne plus exclure des statistiques les immigrants haïtiens, congolais, algériens... qui s'installaient en Ontario. En vertu de cette nouvelle définition, l'Ontario compte francophones, ce qui correspond à 4,8 % de la population de la province.

Au niveau des médias de l’Ontario, on peut faire la répartition suivante:


Parmi les principales chaînes de télévision on trouve :


Parmi les chaînes ontariennes de radio on trouve notamment :


La devise de l'Ontario est "Ut incepit fidelis sic permanet" (Fidèle elle commença, fidèle elle restera).

L'emblème de l'Ontario est la fleur provinciale : le trille blanc, "Trillium grandiflorum". L'oiseau provincial est le plongeon huard ("Gavia immer"), comme le Canada ; l'arbre provincial est le pin blanc ("Pinus strobus"), et le minéral provincial est l'améthyste.

Enfin le tartan officiel de la province qui est un tissu de laine à carreau typique des peuples celtes et symbole au Canada, du Régime anglais, a été dessiné en 1965 par Rotex Ltd et adopté officiellement en l'an 2000.





</doc>
<doc id="6566" url="https://fr.wikipedia.org/wiki?curid=6566" title="Alcool homo-allylique">
Alcool homo-allylique

Un alcool homo-allylique est un composé organique portant un groupement hydroxyle en position homo-allylique.



</doc>
<doc id="6572" url="https://fr.wikipedia.org/wiki?curid=6572" title="Cyclohexane">
Cyclohexane

Le cyclohexane est un hydrocarbure alicyclique non éthylénique de la famille des (mono)cycloalcanes de formule brute CH. Le cyclohexane est utilisé comme solvant apolaire dans l'industrie chimique, mais aussi comme réactif pour la production industrielle de l'acide adipique et du caprolactame, intermédiaires utilisés dans la production du nylon. La formule topologique du cyclohexane est présentée ci-contre.




La rotation autour des liaisons C-C permet au cyclohexane d'adopter une infinité de conformations.
Quand on représente la molécule de cyclohexane en 3D, deux conformations peuvent être utilisées : « chaise » et « bateau ».

Le passage d'une conformation chaise à l'autre, dite « inversion de conformation chaise », se fait par rotation autour des liaisons carbone-carbone et par torsion. 
Du point de vue énergétique, les conformations chaises sont les plus stables, en effet à température ambiante plus de 99 % des molécules de cyclohexane sont dans cette conformation.
Comme le montre la représentation, les liaisons entre l'atome de carbone et l'atome d'hydrogène peuvent être de deux types différents :
L'inversion de conformation chaise transforme une liaison équatoriale en une liaison axiale et réciproquement.
Cependant si le cyclohexane est substitué, par un groupe méthyle par exemple, ce dernier se trouvera en position équatoriale. Cette position est la plus favorable énergétiquement (position d'énergie potentielle la plus basse) car elle minimise les interactions avec les atomes voisins qui peuvent êtres des hydrogènes ou tout autre type d'atome: on appelle ces interactions "interactions 1-3 diaxiales".
Il existe également d'autres conformations, moins stables : la conformation "twist" très peu stable, où aucune des liaisons n'est parallèle. La conformation dite "twist-boat" correspond à une conformation bateau déformée comme dans le twistane, par exemple. La conformation "enveloppe", avec 5 atomes de carbone dans le même plan et un dans un plan différent.

Le cyclohexane est un liquide incolore, d'odeur relativement agréable, insoluble dans l'eau, soluble dans les solvants saturés cycliques ; le cyclohexane est chimiquement assez inerte, à l'image des paraffines. Il se dissocie en butadiène au-dessus de .

Pour ses applications industrielles, il est oxydé en mélange de cyclohexanol et de cyclohexanone pour aboutir aux matières premières :

Il est possible d'obtenir le cyclohexane par une triple hydrogénation du benzène en présence d'un catalyseur à base de nickel.




</doc>
<doc id="6573" url="https://fr.wikipedia.org/wiki?curid=6573" title="Jorge Luis Borges">
Jorge Luis Borges

Jorge Luis Borges , de son nom complet Jorge Francisco Isidoro Luis Borges Acevedo, est un écrivain argentin de prose et de poésie, né le à Buenos Aires et mort à Genève le . Ses travaux dans les champs de l’essai et de la nouvelle sont considérés comme des classiques de la littérature du .

Jorge Luis Borges est le fils de Jorge Guillermo Borges, avocat et professeur de psychologie féru de littérature et de Leonor Acevedo Suárez, à qui son époux a appris l’anglais et qui travaille comme traductrice. La famille de son père était pour partie espagnole, portugaise et anglaise ; celle de sa mère espagnole et vraisemblablement portugaise aussi. Chez lui, on parle aussi bien l’espagnol que l’anglais, et cela depuis son enfance. Borges est donc bilingue, même s’il dira toute sa vie qu’il ne maîtrise pas parfaitement l’anglais.

Pendant la Première Guerre mondiale, la famille Borges habite durant trois années à Lugano puis à Genève où le jeune Jorge étudie au collège Calvin. Après la guerre, la famille déménage de nouveau à Barcelone, Majorque puis Séville et enfin Madrid. En Espagne, Borges devient membre d’un mouvement littéraire d’avant-garde ultraïste. Son premier poème, "Hymne à la mer", écrit dans le style de Walt Whitman, est publié dans le magazine "Grecia".

Il retourne à Buenos Aires en 1921 et s’engage dans de multiples activités culturelles : il fonde des revues, traduit notamment Kafka et Faulkner, publie des poèmes et des essais.

À la fin des années 1930, il commence à écrire des contes et des nouvelles et publie l’"Histoire universelle de l’infamie", qui le fait connaître en tant que prosateur.

Principalement connu pour ses nouvelles, il écrit aussi des poèmes et publie une quantité considérable de critiques littéraires dans les revues "El Hogar" et "Sur" dont il est un temps le secrétaire. Il est également l’un des auteurs des récits policiers parodiques signés Bustos Domecq, écrits en collaboration avec son ami Adolfo Bioy Casares, et de chansons sur des musiques d’Astor Piazzolla. 
En 1938, il obtient un emploi dans une bibliothèque municipale de Buenos Aires. C’est à cette époque qu’il écrit "Pierre Ménard, auteur du Quichotte", son premier conte fantastique. Il perd cet emploi en 1946 en raison de ses positions contre la politique péroniste, et devient inspecteur des lapins et volailles sur les marchés publics.

En 1955, le gouvernement « révolutionnaire » militaire, qui chasse Juan Perón du pouvoir, nomme Borges directeur de la bibliothèque nationale. Il devient également professeur à la faculté de lettres de Buenos Aires. Comme son père avant lui, il souffre d’une grave maladie qui entraîne une cécité progressive, laquelle deviendra définitive en 1955. Devenant peu à peu un personnage public, il préside la Sociedad Argentina de Escritores.

C’est seulement dans les années 1950 que Borges est découvert par la critique internationale. L’écrivain Roger Caillois, qui avait proposé des nouvelles de lui en octobre 1944 à Buenos Aires, dans la revue "Lettres françaises" (numéro 14), offre "Fictions", en 1951, dans la collection « La Croix du Sud », chez Gallimard. C’est une découverte pour le public français et européen. Après Drieu La Rochelle et l’importante action de Roger Caillois — reconnue par J.L. Borges lui-même qui fait de lui son « inventeur » — c’est la revue "Planète" qui le fait connaître du grand public.

La reconnaissance internationale de Borges commence au début des années 1960. En 1961, il reçoit le prix international des éditeurs, qu’il partage avec Samuel Beckett. Alors que Beckett est bien connu et respecté dans le monde anglophone, Borges est inconnu et non traduit, ce qui ne manque pas de susciter la curiosité des locuteurs anglophones. Le gouvernement italien le nomme "Commendatore" et l’université du Texas à Austin le recrute pour un an. La première traduction de son œuvre en anglais date de 1962, avec des lectures en Europe et dans la région des Andes les années suivantes. Borges reçoit de nombreuses distinctions, telles que le prix Cervantes en 1979, le prix Balzan en 1980 (pour la philologie, la linguistique et la critique littéraire), le prix mondial Cino-Del-Duca en 1980 et la Légion d’honneur en 1983. Il est même nommé plusieurs fois pour le prix Nobel de littérature mais ne l’obtiendra jamais, pour des raisons inconnues qui ont donné lieu à de nombreuses spéculations.
Après la mort de sa mère (en 1975), Borges se met à voyager partout à travers le monde et ce, jusqu’à la fin de sa vie.

Borges se marie deux fois. En 1967, il épouse une vieille amie, Elsa Astete Millán, veuve depuis peu. Le mariage dure trois ans. Après le divorce, il retourne chez sa mère. Pendant ses dernières années, Borges vit avec son assistante, María Kodama, avec qui il étudie l’anglo-saxon pendant plusieurs années. En 1984, ils publient des extraits de leur journal, sous le nom d’"Atlas", avec des textes de Borges et des photographies de Kodama. Ils se marient en 1986, quelques mois avant sa mort.

Borges meurt d’un cancer du foie à Genève en 1986 ; il a choisi, à la fin de sa vie, de retourner dans la ville où il a fait ses études. Il est incinéré et ses cendres reposent au cimetière des Rois.

Politiquement, Borges se définit volontiers comme un conservateur et, vers la fin de sa vie, a exprimé ouvertement son scepticisme face à la démocratie. Ce scepticisme transparaît dans certains de ses textes. Quand Juan Perón revient d’exil et est réélu président en 1973, Borges renonce à son poste de directeur de la bibliothèque nationale. Opposé à « l’abominable dictature du général Perón », il reste silencieux face aux crimes de la junte militaire au pouvoir en Argentine dans les années 1970, pendant la période qualifiée de « guerre sale ». Le 22 septembre 1976, il serre la main du général Pinochet auquel il exprime publiquement son admiration, ce qui lui coûta le prix Nobel selon sa veuve. Trois ans plus tard, il scandalise encore en disant de Lincoln qu’il était un criminel de guerre.

Pourtant, plusieurs nouvelles de "Fictions" peuvent être lues comme des dénonciations du totalitarisme. Par exemple "La Loterie à Babylone" ou encore "Tlön, Uqbar, Orbis Tertius", dont la spécialiste Annick Louis affirme dans "Le Magazine littéraire" qu’elle peut être lue .

Borges privilégie l’aspect fantastique du texte poétique, rejetant une écriture rationnelle, qu’il juge insuffisante et limitée. Une des influences majeures du réalisme magique latino-américain, Borges est aussi un écrivain universel dans lequel chacun peut se reconnaître. Son travail érudit, et à l’occasion délibérément trompeur ("Tlön, Uqbar, Orbis Tertius"), traite souvent de la nature de l’infini ("La Bibliothèque de Babel", "Le Livre de sable"), de miroirs, de labyrinthes et de dérive ("Le Jardin aux sentiers qui bifurquent"), de la réalité, de l’identité ou encore de l’ubiquité des choses ("La Loterie à Babylone").

Des ouvrages comme "Fictions" ou "L’Aleph" contiennent des textes souvent courts et particulièrement révélateurs du talent de Borges pour l’évocation d’univers ou de situations étranges qui lui sont propres. Dans "Le Miracle secret", un écrivain, face au peloton d’exécution, dans la seconde qui précède sa fin, se voit accorder la grâce de terminer l’œuvre de sa vie. Le temps se ralentit infiniment. Il peaufine mentalement son texte. Il retouche inlassablement certains détails… Il fait évoluer le caractère d’un personnage à la suite de l’observation d’un des soldats qui lui font face… Dans un autre récit, "Histoire d’Emma Zunz" ("Fuera de Emma Zunz"), une jeune fille trouve un moyen inattendu, cruel et infaillible, de venger son honneur et celui de sa famille…

Homère surgit peu à peu d’un autre texte, "L’Immortel", après un extraordinaire voyage dans l’espace et le temps. Dans "Pierre Ménard, auteur du Quichotte", Borges nous dévoile son goût pour l’imposture, et un certain humour littéraire souvent rare, mais qui dans l’ouvrage "Chroniques de Bustos Domecq", écrit en collaboration avec Adolfo Bioy Casares, s’épanouira dans l’évocation d’une étonnante galerie de personnages artistes dérisoires et imposteurs.

La concision, les paradoxes, les associations fulgurantes de mots comme « perplexes couloirs » sont typiques de son style unique.

Borges est devenu aveugle assez jeune mais de façon progressive, ce qui eut une forte influence sur ses écrits. Dans une de ses nouvelles, "L’Autre", il se rencontre lui-même plus jeune, sur un banc, et se livre à quelques prédictions : 
À ce sujet, il raconte dans l’"Essai autobiographique" que cette cécité était probablement d’origine héréditaire et que certains de ses ascendants avaient connu la même infirmité. N’ayant jamais appris le braille, il dut compter sur sa mère pour l’aider, puis sur son assistante Maria Kodama. Il se faisait lire journaux et livres et dictait ses textes.

Outre les fictions, son œuvre comprend poèmes, essais, critiques de films et de livres. On y trouve une sorte de réhabilitation du roman policier, plus digne héritier de la littérature classique à ses yeux, que le nouveau roman. Ce genre littéraire demeure seul, selon lui, à préserver le plan de la construction littéraire classique, avec une introduction, une intrigue et une conclusion.

On trouve également parmi ses écrits de courtes biographies et de plus longues réflexions philosophiques sur des sujets tels que la nature du dialogue, du langage, de la pensée, ainsi que de leurs relations. Il explore aussi empiriquement ou rationnellement nombre des thèmes que l’on trouve dans ses fictions, par exemple l’identité du peuple argentin. Dans des articles tels que "L’histoire du Tango" et "Les traducteurs des Mille et Une Nuits", il écrit avec lucidité sur des éléments qui eurent sûrement une place importante dans sa vie.

Il existe de même un livre qui réunit sept conférences dans diverses universités, qu’on peut considérer comme sept essais, clairs, ordonnés, d’une simplicité dérivant de leur caractère oratoire. Dans ce petit recueil de savoir, "Les Sept Nuits (Siete Noches)", on trouve un texte sur les cauchemars, sur les "Mille et une nuits", sur la "Divine Comédie" de Dante, sur le bouddhisme et d’autres thèmes que Borges exploite et nous fait partager avec l’autorité didactique et la simplicité pédagogique d’un véritable professeur, érudit de la littérature.

Écrits entre 1923 et 1977, ses poèmes retrouvent les thèmes philosophiques sur lesquels repose la pluralité de l’œuvre de Borges. Des poèmes comme "El Reloj de Arena" ("Le Sablier") ou "El Ajedrez" ("Les Échecs") reconstruisent les concepts borgesiens par excellence, comme le temps, instable et inéluctablement destructeur du monde, ou le labyrinthe comme principe de l’existence humaine, mais d’un point de vue poétique, condensé dans des images surprenantes. Ces poèmes sont réunis dans "Antologia Poética 1923-1977 (Recueil Poétique)."

Sous le pseudonyme de H. Bustos Domecq, il écrit en collaboration avec Adolfo Bioy Casares "Six problèmes pour Don Isidro Parodi", série d’énigmes mi-mondaines mi-policières. Le héros, Don Isidro Parodi, joue les détectives depuis la prison où il est enfermé et dans laquelle il est sollicité par une étrange galerie de personnages. L’isolement forcé semble stimuler sa clairvoyance car, sans quitter sa cellule, il résout chaque énigme aussi facilement que les autres détectives de la littérature, tels Auguste Dupin, Sherlock Holmes ou Hercule Poirot.


Par ailleurs, Borges a publié un grand nombre de chroniques, notamment dans "Proa" (1924-1926), "La Prensa" (1926-1929), "Sur" et "El Hogar" (1936-1939).

Dans une entrevue, à l’automne 2010, María Kodama suggère, à qui veut s’initier à l’œuvre de Borges, de commencer par "Le Livre de sable" (1975), "Les Conjurés" (1985) et "Le Rapport de Brodie" (1970), avant d’aborder "Fictions" (1944) et "L’Aleph" (1949).








</doc>
<doc id="6576" url="https://fr.wikipedia.org/wiki?curid=6576" title="Réduction de Wolff-Kishner">
Réduction de Wolff-Kishner

En chimie organique, la réduction de Wolff-Kishner permet la réduction d'un groupement carbonyle en hydrocarbure saturé en deux étapes. La condensation du groupement carbonyle avec de l'hydrazine permet la formation d'une hydrazone intermédiaire, qui est ensuite déprotonée sous l'action d'une base forte (NaOH, KOH, NaOEt…) pour conduire à l'hydrocarbure saturé attendu.

Cette réaction a été découverte indépendamment par le russe Nikolai Kischner et l'allemand Ludwig Wolff, respectivement en 1911 et 1912.

La procédure standard a longtemps préconisé l'emploi d'hydrazine pure dans un solvant à haut point d'ébullition, tel que l'éthylène glycol, en présence d'un large excès de base (NaOH, NaOEt, etc.), le milieu réactionnel étant chauffé à reflux pendant plusieurs jours. Cette méthode est donc intéressante pour des molécules stables thermiquement et insensibles aux conditions très basiques, et présente donc un intérêt limité en synthèse organique.

Afin de faciliter la mise en œuvre de cette réaction, de nouvelles conditions ont été développées par plusieurs groupes de recherche. Ainsi, l'élimination après la première étape de l'eau et de l'excès d'hydrazine par distillation a permis d'augmenter la température de la seconde étape à presque , réduisant ainsi considérablement le temps de réaction (3-6h). Cette approche conduit à de meilleurs rendements, et autorise l'emploi d'hydrate d'hydrazine et de bases hygroscopiques (NaOH et KOH). Une méthode plus douce repose sur l'addition de l'hydrazone préformée dans une solution de "tert"-butylate de potassium dans le DMSO. Bien que la réduction soit alors possible à température ambiante, il a été montré que cette réaction était difficile à mettre en pratique sur petite échelle et qu'elle était très dépendante du substrat.

Enfin, une nouvelle modification des conditions de Wolff-Kishner a été proposée, permettant la réduction de groupements carbonyles avec une très bonne chimiosélectivité. La cétone est tout d'abord convertie en N-TBS-hydrazone en présence d'un acide de Lewis à température ambiante, puis une solution de "tert"-butylate de potassium/"tert"-butanol dans le DMSO est ajoutée, ce qui provoque la réduction de l'hydrazone en alcane.

Les hydrazones intermédiaires ont pu être synthétisées de manière chimiosélective, puis purifiées par colonne chromatographique sur silice (désactivée avec de la triéthylamine). Les auteurs ont également démontré l'efficacité de cette méthodologie pour la réduction de molécules aliphatiques, aromatiques ou polycycliques complexes avec de très bons rendements.

En présence d'ions H, la cétone est convertie en hydrazone, il se forme une imine. Le traitement à l'aide d'une base forte (ici la potasse) va alors permettre la déprotonation de l'hydrazone, suivie de la protonation du carbone en présence d'une molécule d'eau. La répétition de cette séquence va conduire à la génération de l'hydrocarbure attendu (CH) et à la libération d'une molécule de diazote, rendant cette réaction irréversible.



</doc>
<doc id="6578" url="https://fr.wikipedia.org/wiki?curid=6578" title="Diazote">
Diazote

Le diazote, est une molécule diatomique composée de deux atomes d'azote. Elle est notée N.

Dans les conditions normales de température et de pression, les molécules de diazote forment un gaz incolore, composant 78 % de l'air.

Au , le diazote est généralement obtenu par liquéfaction de l'air, dont il est le principal constituant avec une concentration de 78,06 % en volume et de 75,5 % en masse, suivie d'une distillation fractionnée.

Le diazote atmosphérique peut être converti en ammoniac grâce au procédé "Haber-Bosch". L'ammoniac ainsi produit sert surtout à la fabrication d'engrais.

L'extraction du diazote hors de l'air peut entre autres être réalisée au moyen de membranes semi-perméables alimentées en air comprimé. Ces membranes sont composées de faisceaux de fibres d'oxyde polyphénilique creuses à enveloppe perméable enduites d'une couche de .

La pureté du diazote produit par une membrane dépend du débit demandé : par exemple, l'obtention d'une pureté de 95 % permet des débits allant jusqu'à  Nm/h, alors qu'une production d'azote à 99,5 % ne permet que 0,5 Nm/h.

Une autre méthode pour produire de diazote à partir d'air comprimé est par adsorption : ce type de générateur d'azote est composé d'un système symétrique de réservoirs remplis d'un tamis moléculaire à base de carbone (CMS). L'air comprimé passe au travers de la colonne « en ligne » et durant ce passage l'O et les autres gaz atmosphériques sont absorbés. Le gaz restant est du diazote prêt à être utilisé. Après un temps préréglé, le cycle s'inverse, la colonne « en ligne » passe en mode régénération pour relibérer les gaz capturés et les relâcher dans l'atmosphère (pureté jusqu'à d'O).

Production mondiale en millions tonnes en 2014 :
Plusieurs bactéries sont capables de fixer le diazote moléculaire de l'air, première étape avant de pouvoir l'incorporer dans des molécules organiques comme les protéines ou les bases nucléiques constitutives des acides nucléiques support de l'hérédité comme l'ADN et l'ARN. On rencontre ces bactéries en symbiose dans les racines des plantes de la famille des fabacées.

Le diazote, caractérisé par la présence d'une liaison covalente triple (une liaison σ et deux liaisons π), est une molécule très stable qui est pour cette raison utilisée comme gaz inerte pour remplacer l'atmosphère en synthèse chimique. Le diazote ne réagit directement qu'avec le lithium et le magnésium pour former les nitrures correspondants LiN et MgN.

La stabilité de la molécule de diazote est la force motrice, l'origine de l'instabilité, voire de l'explosivité des composés pouvant libérer une molécule de diazote : azotures, sels de diazonium, azodicarbonamide





Il a été utilisé pour la conservation de la viande.

Le diazote, contrairement aux gaz inhibiteurs chimiques halogénés et aux CFC ne présente "a priori" aucun effet nocif pour l'environnement (pas d'impact sur l'effet de serre, ni sur la couche d'ozone). Mais il requiert des réservoirs volumineux, des canalisations adaptées et des mesures constructives pour faire face à la détente brutale d'un équivalent de 40 à 50 % du volume protégé.

Risque d'anoxie : le cas le plus fréquemment rencontré est celui de personnes pénétrant dans des réservoirs remplis d'azote sans s'en apercevoir, du fait que ce gaz est inodore et ne provoque pas de sensation de suffocation (causée par l'excès de dioxyde de carbone, et non par l'absence d'oxygène). Ces personnes sont alors prises de malaises, perdent connaissance, et, si on ne les retire pas très rapidement de cette situation, succombent. Il est nécessaire de vérifier la présence d'une proportion suffisante d'oxygène dans de tels espaces confinés avant d'y pénétrer, ou de s'équiper d'un appareil respiratoire autonome.



Masse molaire du diazote 



</doc>
<doc id="6580" url="https://fr.wikipedia.org/wiki?curid=6580" title="Potasse (minerai)">
Potasse (minerai)

La potasse désigne communément un minerai salin dit de roches évaporitiques, à base de chlorure de potassium, utilisé comme point de départ des principaux dérivés potassiques ainsi que dans l'industrie des engrais. On peut aussi l'extraire en grande quantité des eaux de la mer.

Les gisements de potasse sont peu nombreux, mais souvent activement exploités, dans le monde.

Les plus importants producteur mondiaux en 2014 :
Les plus grands producteurs mondiaux de potasse en 2000 sont le Canada, la Fédération de Russie et ses anciens états voisins dont la Biélorussie. Le Brésil, la Chine, l'Allemagne, les États-Unis et Israël ont également des gisements importants. La France l'était encore avant 1990.

Actuellement la Saskatchewan, au Canada, est le plus important exportateur mondial de potasse, sous forme principale de minerai concentré en sylvine. Jusqu'en 2013, deux cartels contrôlaient presque 70 % de la production mondiale de potasse, le cartel canadien Canpotex et le cartel russe-biélorusse appelé la « Compagnie des Potasses de Biélorussie » (CPB), résultat des accords entre Belaruskali et Uralkali. Le cartel canadien assurait l'approvisionnement de l'Amérique du Nord, et CPB l'Europe, la Chine et plus généralement l'Asie. En juillet 2013, face à la chute persistante des cours, Uralkali a dénoncé ces accords et a mis fin au cartel PCB, ce qui a provoqué la chute des cours de bourse des entreprises du secteur.

La potasse, en particulier la sylvinite, a été exploitée intensivement en Alsace, dans les environs de Mulhouse, de 1904 à la fin du . Au début du , suivant les couches, la teneur en chlorure de potassium KCl du minerai des Mines de Potasse d'Alsace variait de 20 % à 80 %.

L'Allemagne a aussi exploité les mines de potasse de Stassfurt, principalement à base de carnallite, avec aussi de la kaïnite et de polyhalite.

Les engrais K ont été supplantés par les engrais NPK, mélanges complexes et dosés. Les premiers, autrefois les plus communs, étaient le produit de deux filières :

Les engrais à base de kaïnite sont intermédiaires entre ces deux filières.




</doc>
<doc id="6581" url="https://fr.wikipedia.org/wiki?curid=6581" title="Hydroxyde de sodium">
Hydroxyde de sodium

L'hydroxyde de sodium pur est appelé soude caustique, dans les conditions normales, c'est la forme solide cristaline. C'est un corps chimique minéral composé de formule chimique NaOH, qui est à température ambiante un solide ionique. Il est fusible vers , il se présente généralement sous forme de pastilles, de paillettes ou de billes blanches ou d'aspect translucide, corrosives. Il est très hygroscopique, il est d'ailleurs aussi souvent commercialisé sous la forme dissoute dans l'eau. Il y est en effet très soluble et seulement légèrement soluble dans l'éthanol. Sa dose journalière admissible est non spécifiée depuis 1965. Le code du travail français ne permet pas à des mineurs de fabriquer ou de manipuler ce produit chimique.

Sa présentation la plus connue est celle mélangée à l'eau, sa solution aqueuse. C'est l'hydroxyde de sodium qui est souvent appelée en raccourci soude ou aussi lessive de soude. C'est une solution transparente visqueuse. Elle est encore plus corrosive que pur. Son agressivité est amplifiée par son aspect mouillant qui augmente l'action et le contact avec la peau.

Ce produit, matière importante de l'industrie chimique, capitale pour le contrôle d'un milieu alcalin ou la régulation d'acidité dans un procédé, est aussi courant dans le commerce, sous forme de paillette ou de solution ; il est vendu par exemple comme déboucheur de canalisations, produit de nettoyage ou agent de neutralisation (d'acides).

Les propriétés chimiques de l'hydroxyde de sodium sont surtout liées à l'ion hydroxyde HO qui est une base forte. En outre, l'hydroxyde de sodium réagit avec le dioxyde de carbone () de l'air et se carbonate.

La solubilité de la soude caustique dans l'eau augmente avec la température, à pression constante ou ambiante. Elle atteint à (solution à 50 %) et atteint à . 

Cette solubilité élevée, très supérieure par exemple à celle de la chaux ou des autres autres hydroxydes alcalino-terreux, l'abondance de sa production industrielle et son prix de revient inférieure à la potasse caustique, en font la base minérale la plus utilisée dans le monde.

Évolution de la masse volumique de la solution aqueuse en fonction de la concentration :

Dans l'Antiquité, on utilisait la soude soit d'origine minérale soit d'origine végétale. La soude, prototype de l'alcali minéral, désigne alors un carbonate de sodium plus ou moins pur, qui provenait dans le premier cas de dépôts lacustres à base de natron, éventuellement purifiés et séchés, et dans le second, des sels de lixiviation des cendres obtenues par la combustion de plantes halophytes comme la salicorne ou les "Soudes". La soude caustique était ensuite obtenue par caustification (voir ci-dessous). 

Entre 1771 et 1791, le chimiste Nicolas Leblanc invente un procédé permettant d'obtenir du carbonate de sodium à partir d'eau de mer, procédé coûteux en combustible qui sera supplanté par le procédé Solvay plus économique entre 1861 et 1864 (mis au point par l'entrepreneur et chimiste belge Ernest Solvay). Ces procédés, surtout le second, permettent de réduire les coûts de revient de la soude et font disparaître les anciennes techniques.
À la fin du , l'avènement de l'électricité permet la production directe de soude par électrolyse d'une solution aqueuse de chlorure de sodium, dont les deux compartiments d'électrode sont séparés par une membrane évitant la migration des gaz dissous.

On a ainsi les deux demi-réactions suivantes :

soit la réaction globale caractéristique de l'électrolyse de l'eau salée : 2 NaCl + 2 → 2 NaOH+ Cl + H 

Aujourd'hui, 99 % de la soude produite est d'origine électrochimique.

La soude est obtenue par électrolyse du chlorure de sodium (NaCl).

La soude s'obtient pour le moment majoritairement par une "électrolyse avec cathode de mercure" (anode : titane ; cathode : mercure). Cette opération produit en même temps du chlore, de la soude en solution et de l'hydrogène. Mais le mercure est un métal lourd nocif par bioaccumulation et à très faible dose, plus encore lorsqu'il est transformé en mono- ou di-méthylmercure par les bactéries. Il est volatil et non dégradable, et passe facilement la barrière des poumons, ce qui en fait un des polluants majeurs de l'environnement, en augmentation dans toutes les mers. C'est l'une des raisons pour lesquelles les sociétés européennes impliquées se sont engagées à faire disparaître ce procédé à l'horizon 2020, procédé en cours de remplacement par des "électrolyses à membranes".

Il existe un autre procédé : "électrolyse à diaphragme", qui comportait de l'amiante, substituée en France par un matériau composite depuis la fin des années 1990.

Cette technique était celle utilisée autrefois en Égypte ou en Turquie. Elle est encore utilisée en Amérique du Nord où se trouvent des gisements naturels de carbonate de sodium. C'est un ajout de chaux au carbonate de sodium. On parle de "caustification" ou de "caustication". La réaction s'écrit :

La production annuelle mondiale entre 1991 et 1998 est estimée à 45 millions de tonnes.

L'hydroxyde de sodium est utilisé en grande quantité par plusieurs industries. La moitié de la production reste dans l'industrie chimique, où elle participe à l'élaboration de plus de 400 produits de base, par des procédés de chimie minérale ou de synthèse organique.

L'autre emploi, principalement en tant que base, la rend indispensable, par ordre décroissant des besoins :

La soude est utilisée dans certains produits défrisants pour cheveux, mais a tendance à être abandonnée dans les cosmétiques modernes.

L'hydroxyde de sodium sert à :

La soude peut être utilisée pour stocker de l'énergie solaire sous forme chimique. En effet, la réaction entre la soude et l'eau est fortement exothermique. Une fois la soude diluée, il suffit d'utiliser directement l'énergie solaire pour faire s'évaporer l'eau et revenir à l'état initial.

La réaction de l'hydroxyde de sodium avec l'eau et l'aluminium produit un dégagement d'hydrogène qui peut faire fonctionner un moteur à explosion sans émission de dioxyde de carbone.

L'hydroxyde de sodium est la base la plus communément utilisée en laboratoire. Elle sert à de nombreux dosages, ainsi qu'à la précipitation d'hydroxydes. Elle intervient dans les réactions d'hydrolyse. 

La soude est également utilisée comme un réactif pour des tests de chimie. En effet, en présence de certains cations métalliques, la soude forme un précipité d'une certaine couleur.

L'hydroxyde de sodium est un produit dangereux, Non par sa toxicité métabolique (Il est utilisé dans certaines saumures alimentaires et la réaction chimique avec l'acide chlorhydrique du suc gastrique produit simplement du sel qui n'est pas toxique); mais plutôt parce qu'il est extrêmement corrosif dès 0,5 % de concentration et son contact direct détruit les tissus organiques. 

Au même titre que l'acide sulfurique et contrairement à l'acide chlorhydrique, le danger ne réside pas tant dans sa caractéristique de base forte, que dans son avidité pour l'eau qui «brûle» les tissus en les déshydratant, tout en rendant la réaction fortement exothermique. De plus, le caractère visqueux des solutions concentrées (ex: à 50 %) aggrave les contacts accidentels. 

La dissolution de NaOH dans l'eau étant fortement exothermique, la préparation ou dilution présente des risques d'éclaboussures par ébullition : il faut donc verser le produit dans l'eau et jamais l'inverse. On verse lentement, en agitant avec précaution pour homogénéiser la température du récipient qui peut être élevée et produire des vapeurs.

La soude réagit également très violemment avec les acides et certains métaux et il ne faut donc pas non plus la diluer dans un récipient métallique tel qu'un seau en aluminium ou en zinc. 

La soude caustique est irritante et corrosive pour la peau, les yeux, les voies respiratoires et digestives. Elle doit être manipulée avec des gants, des lunettes de protection et une protection intégrale du visage et des voies respiratoires si présence de poussière ou d'aérosol. En cas de contact très important, la soude peut interférer avec les transmissions nerveuses, atténuant la douleur de la brûlure et retardant la prise de mesures.

En cas de contact avec la peau, il faut rincer le plus rapidement possible avec de l'eau (ou de la diphotérine), retirer les vêtements imprégnés en évitant de propager le contact de la base avec la peau, et consulter un médecin. 

En cas d'ingestion accidentelle, il ne faut pas faire vomir à cause du risque de double brûlure (aller et retour). Il faut appeler les secours ou un centre antipoison le plus vite possible.

En cas de contact avec les yeux, l'idéal est d'utiliser très rapidement une solution de diphotérine pour faire un lavage prolongé, sinon se précipiter sous un robinet d'eau courante et faire couler l'eau quinze-vingt minutes, et ensuite consulter un médecin ophtalmologiste. La soude caustique traverse toute la cornée de l'œil en moins de quarante secondes.

La soude caustique augmente le pH des cours d'eau, représentant ainsi une menace pour la faune et la flore aquatiques.

La soude caustique s'infiltre dans la terre et peut nuire à l'agriculture comme à l'environnement des végétaux, des minéraux et des animaux proches ou lointains (rivière, fleuve, nappe phréatique).




</doc>
<doc id="6582" url="https://fr.wikipedia.org/wiki?curid=6582" title="Hydroxyde de potassium">
Hydroxyde de potassium

L'hydroxyde de potassium, dénommé de façon usuelle la potasse caustique au laboratoire, est un corps composé minéral de formule brute KOH. Ce composé chimique caustique, à la fois corrosif et fortement basique est, à température et pression ambiante, un solide blanc dur et solide, mais très hygroscopique et déliquescent à l'air humide. 

Il fond sans se décomposer avant 400°C. Du point toxicologique, cet alcali caustique, très soluble dans l'eau et dans l'alcool, connu de toute antiquité, est un poison énergique.

L'hydroxyde de potassium est obtenu par électrolyse des solutions aqueuse de chlorure de potassium KCl. Cette opération produit également du chlore et de l'hydrogène.

Autrefois, la potasse caustique était obtenue, comme son nom l'indique, par une réaction de caustication, c'est-à-dire une décomposition par la chaux ou un lait de chaux du carbonate de potassium ou potasse, grâce à un chauffage vigoureux en pot de céramique ou métallique, récipient à l'origine du terme néerlandais "potasch" ou cendre de pot. 

KCO + CaO + HO → 2 KOH + CaCO

Cette "potasse à la chaux" servait à la fabrication de pierre à cautères. Par dissolution dans l'alcool puis évaporation dans une capsule d'argent, on obtenait une potasse caustique purifiée, dite "potasse à l'alcool". 

Déjà à la Belle Époque, la potasse caustique était obtenue par électrolyse de lessive de KCl. La production mondiale, très inférieure à celle de la soude caustique NaOH, n'atteint pas le million de tonnes dans les années 1990, oscillant autour du demi-million.

L'hydroxyde de potassium est très soluble dans l'alcool éthylique, mais insoluble dans l'éther.
Le cation K est moins soluble dans l'eau que le cation Na. De plus, il se complexe plus facilement, et peut être retenu plus facilement par des composés, comme les argiles.

L'hydroxyde de potassium se dissocie totalement dans l'eau pour former une solution aqueuse d'hydroxyde de potassium K + OH. La solubilité reste inférieure à celle de l'hydroxyde de sodium. À 15°C, elle ne correspond qu'à 107 g/100g d'eau. Elle croît assez peu avec la température : 178 g/100 g d'eau à 100°C. 
La solution KOH est bonne conductrice du courant. La conductivité de l'ion K+ est en effet meilleure que Na+. L'électrolyse alcaline de l'eau utilise donc une solution de KOH, avec des électrode en acier ou nickel. L'hydrogène est produit à la cathode, l'oxygène à l'anode. Les gaz dégagés sont purs, mais le coût électrique peut être dissuasif.. 
En solution à 25-40 %, il est utilisé comme électrolyte dans la plupart des piles alcalines.

En solution à 5%, il est utilisé comme traitement des Molluscum Contagiosum par application cutanée.

Il existe deux composés à température ordinaire avec l'eau : le dihydrate KOH 2 HO et le sesquihydrate 3 KOH 2 HO d'hydroxyde de potassium. Le second n'est pas au-delà de 33°C et se transforme en monohydrate KOH HO à 50°C. Le premier de masse molaire 92,14 g/mol est soluble dans l'eau dans les proportions suivantes pour 100 g d'eau : 103 g à 0°C et 10°C, 112 g à 20°C, 138 g à 40°C, 178 à 100°C.

L'hydroxyde de potassium est employé dans l'industrie des engrais, dans l'industrie chimique (par exemple pour l'électrolyse alcaline de l'eau), dans la fabrication du savon liquide ou autrefois en savonneries traditionnelles, dans l'industrie de l'eau comme adoucisseur...

Il est présent dans les produits lessives, utilisé dans le blanchiment, dans le nettoyage des peintures, dans l'industrie du caoutchouc de synthèse, dans les installations pétrolières pour l'enlèvement du soufre mais aussi en pharmacie et médecine...

KOH est un agent séchant pour gaz, à la fois basique et rapide, mais de faible capacité. Il est aussi employé, parfois en association avec NaOH, comme agent séchant pour solvant et solution : le séchage est énergique et rapide, la capacité est bonne. L'hydroxyde de potassium est surtout utilisé pour sécher les amines.

Au laboratoire, la potasse caustique peut être utilisée pour absorber le gaz carbonique et attaquer les acides, en particulier les silicates et les verres.

Il peut être aussi utilisé en microbiologie pour déterminer le type Gram d'une bactérie. 

La potasse caustique était un caustique de la peau.

L'hydroxyde de potassium est corrosif et sa préparation dégage des vapeurs toxiques de potassium. La dissolution d'hydroxyde de potassium dans l'eau est très exothermique, ce qui rend les éclaboussures dangereuses. La potasse caustique est irritante et corrosive pour la peau, les yeux, les voies respiratoires et digestives. Elle doit être manipulée avec des gants, des lunettes de protection et une protection des voies respiratoires. En cas de contact avec la peau ou les yeux, rincez abondamment (15 à 20 minutes) avec de l'eau et consultez un médecin.




</doc>
<doc id="6583" url="https://fr.wikipedia.org/wiki?curid=6583" title="Hydroxyde de lithium">
Hydroxyde de lithium

L’hydroxyde de lithium (LiOH, CAS : 1310-65-2), aussi appelé lithine, est une base corrosive, produite par exemple lors du mélange réactif entre le lithium métal solide et l'eau :

Cet alcali sous forme de cristaux incolores, très caustique, est analogue à la soude (NaOH) et la potasse (KOH), bien que certaines de ses propriétés soient uniques. Son principal avantage par rapport à celles-ci concerne sa faible masse et sa plus grande densité, ce qui rend ses utilisations en milieu confiné plus pratiques. On peut ajouter, à cette compacité et légèreté, la grande puissance du couple redox Li/Li, en particulier son potentiel d'oxydoréduction et sa durabilité. 

L'hydroxyde de lithium se présente sous la forme d'un cristal blanc hygroscopique. Il est soluble dans l'eau ( à ), et faiblement dans l'éthanol. Il fond à .

La lithine caustique a été une base forte employée en savonnerie. Elle permet d'obtenir des savons gras de lithium, par exemple des stéarate de lithium, par réaction de saponification. Ces savons servent à fabriquer des lubrifiants et anti-adhésifs de fonderie. 

L'hydroxyde de lithium sert à fabriquer des sels ou polymères techniques principalement à propriétés anti-statiques.

L'hydroxyde de lithium est utilisé à partir des années cinquante lors des missions spatiales et dans les sous-marins pour purifier l'air. En effet, l'hydroxyde de lithium réagit avec le dioxyde de carbone suivant la réaction acido-basique suivante:

D'autres utilisations le font intervenir dans les synthèses de polymères ou comme électrolyte dans les piles, puissantes et à usage fiable de longue durée, mais aussi dans les accumulateurs au lithium. 

Il est également utilisé par l'industrie nucléaire pour le conditionnement des circuits : il vient y contrer l'acidité de l'acide borique (lui-même injecté comme neutrophage). Le choix s'est porté sur lui car une faible partie du bore 10 se transforme en Lithium par réaction B(n, alpha)Li (environ 2 %).

Le nom lithine est mentionné en 1827 dans le dictionnaire de l'académie. Il désigne le plus souvent l'alcali caustique LiOH, mais parfois encore l'(hémi)oxyde de lithium. Autrefois, il désignait plus souvent le carbonate de lithium.

C'est un alcali très caustique, pour les muqueuses et la peau.


</doc>
<doc id="6587" url="https://fr.wikipedia.org/wiki?curid=6587" title="JSP">
JSP

JSP est l'abréviation de :

</doc>
<doc id="6589" url="https://fr.wikipedia.org/wiki?curid=6589" title="Wilhelm Ostwald">
Wilhelm Ostwald

Friedrich Wilhelm Ostwald, né le à Riga en Livonie dans l'Empire russe (actuelle Lettonie) et mort le à Grossbothen, Allemagne, est un chimiste germano-balte. Il a notamment reçu le prix Nobel de chimie de 1909 .

Ce physico-chimiste germano-livonien commença sa carrière comme assistant en physique à l'université de Dorpat (gouvernement de Livonie, actuelle Estonie), avant de devenir professeur à Rīga en 1881, puis professeur à l'université de Leipzig en 1887 où il enseigna la chimie et la philosophie. Il devint ensuite directeur de l'institut de physico-chimie.

Il a mis au point en 1900, avec son gendre Eberhard Brauer, un procédé de synthèse de l'acide nitrique à partir d'ammoniac, le procédé Ostwald.

Il a été récompensé par le prix Nobel de chimie en 1909 pour ses travaux sur la catalyse chimique et ses recherches sur les principes fondamentaux qui gouvernent l'équilibre chimique et les vitesses de réaction. Il a également été lauréat du "Faraday Lectureship" de la "Royal Society of Chemistry" en 1904.

Il est aussi connu pour ses travaux sur la théorie de la dilution qui débouchèrent notamment sur la loi de la dilution qui porte son nom.

À partir de 1901, Ostwald s'intéressa à la théorie des couleurs, et il eut pour objectif de fonder scientifiquement le système chromatique tout en rendant possible une théorie de l'harmonie des couleurs qui puisse servir de base à l'esthétique et à l'art de la peinture.

Outre ses contributions proprement scientifiques, Ostwald développa un certain nombre de conceptions philosophiques quant à la nature de la réalité. Il fut un fervent partisan de l'énergétisme, avant de se convertir tardivement à l'atomisme. Il chercha à développer scientifiquement cette théorie générale qui soutenait que l'énergie était la véritable forme de la matière. Il fut à partir de 1910, avec Ernst Haeckel, une figure dominante du monisme allemand, approche qui se présentait comme une conception scientifique du monde.

Ostwald défendit également le projet d'établir des normes universelles pour la monnaie ou encore pour le langage. Il s'intéressa dans ce cadre à la création de langues universelles, comme l'espéranto et surtout l'ido, dont il fut l'un des principaux artisans. Il fut en effet président du Comité de travail élu en 1907 par la « Délégation pour l'adoption d'une langue auxiliaire internationale » qui résolut de modifier l'espéranto selon les idées du projet Ido. Il fut ensuite membre de la Commission permanente de ce même comité chargé de développer l'ido.

À la suite , il se retira de la présidence de la Commission permanente. En 1916, il proposa une langue allemande mondiale, le "Weltdeutsch", qui ne vit cependant jamais le jour. Dix ans plus tard, le 29 septembre 1926 dans le quotidien "Vossische Zeitung", il s'expliqua sur le besoin de créer une nouvelle langue internationale.

En 1931, revenu à l'ido et devenu président d'honneur de l'académie de l'ido, Ostwald écrivit un article en ido dans la revue "Progreso" intitulé "La Mondlinguo - Un Necesajo" ("La langue mondiale - une nécessité") dans lequel il compare l'humanité tout entière à un organisme vivant et le langage à son système nerveux. Il conclut par ses mots aux consonances européennes :

Il est enterré dans la sépulture familiale du grand cimetière de Riga.

En 1970, l'union astronomique internationale a donné le nom de Ostwald à un cratère lunaire.





</doc>
<doc id="6590" url="https://fr.wikipedia.org/wiki?curid=6590" title="Isomérie">
Isomérie

En chimie organique, on parle d'isomérie lorsque deux molécules possèdent la même formule brute mais ont des formules développées ou stéréochimiques différentes. Ces molécules, appelées isomères, peuvent avoir des propriétés physiques, chimiques et biologiques différentes.

Le terme isomérie vient du grec ίσος ("isos" = identique) et μερος ("meros" = partie).

On distingue différents types d'isomérie, notamment l'isomérie de constitution (isomérie plane) et l'isomérie de configuration (stéréoisomérie).

L'isomérie a été remarquée la première fois en 1827, quand Friedrich Woehler a préparé l'acide isocyanique (H-N=C=O) et a noté que, bien que sa composition élémentaire soit la même que celle de l'acide fulminique (H-CNO, préparé par Justus von Liebig l'année précédente), les propriétés chimiques de ces substances sont radicalement différentes. Cette découverte était en contraste avec les théories de l'époque dans le cadre desquelles l'on pensait que les propriétés d'une substance étaient entièrement déterminées par sa formule brute.

Le terme (ou plus exactement ) a été proposé par le chimiste suédois Jöns Jacob Berzelius en 1830

On parle d'isomérie de constitution (ou encore d'isomérie plane ou d'isomérie de structure) lorsque les molécules ont la même formule brute mais des formules développées différentes. Elles diffèrent donc dans l'enchaînement des atomes.

L'isomérie de chaîne (ou de squelette) désigne les isomères qui diffèrent par leur chaîne carbonée.

Exemple : CH

L'isomérie de position de fonction qualifie les isomères dont un groupe fonctionnel est placé sur des carbones différents de la chaîne carbonée, ce qui veut dire que c'est la fonction qui se déplace à l'intérieur du squelette.
Exemple : CHO

Certaines réactions chimiques permettent de passer d'un isomère de position à l'autre, elles sont appelées réaction de réarrangement. 
Par exemple, les dérivés de l'anthraquinone sont connus pour leur transformation réversible photoinduite en para-ana-quinoïde. Ce phénomène implique la migration d'un atome ou groupe d'atome lors du passage de la forme "trans" à la forme "ana".

L'isomérie de nature de fonction caractérise les isomères dont les groupes fonctionnels sont différents, donc de propriétés physiques et chimiques différentes. On appelle ces isomères, des isomères de « fonction ».

Exemple : CHO

L'isomérie d'insaturation caractérise les isomères dont les insaturations sont différentes. 

Exemple : CH

La stéréoisomérie désigne les isomères de disposition dans l'espace, c'est-à-dire les molécules de constitution identique (elles ont la même formule semi-développée) mais dont l'organisation spatiale des atomes est différente. On parle parfois d'isomérie structurale. La représentation de Cram, notamment, permet de différencier des stéréoisomères (ou stéréomères).

On distingue les stéréoisomères de conformation, qui ne diffèrent que par des rotations autour des liaisons simples et les stéréoisomères de configuration, séparés en deux grands groupes : les énantiomères et les diastéréoisomères.

Ces isomères ont une même formule développée. Ils ne se différencient que par rotation autour d'une liaison simple (liaison sigma), sans la rompre. Par exemple le butane (CH) a trois conformères.

Ces isomères ont une même formule développée. On parle d'atropoisomérie lorsque la barrière d'activation autour d'une liaison simple (liaison sigma) est suffisante pour qu'on puisse caractériser chacun des deux atropoisomères qui sont énantiomères l'un de l'autre. Couramment utilisée en catalyse asymétrique, cette forme d'isomérie est classiquement observée sur des biphényles substitués en "ortho".

Aussi appelés isomères optiques, les énantiomères sont deux molécules qui sont l'image l'une de l'autre par un miroir et ne sont pas superposables : elles présentent en effet une chiralité (dextrogyre ou lévogyre). Elles sont donc symétriques l'une par rapport à l'autre, le plan de symétrie étant le miroir. L'exemple le plus concret de chiralité est celui d'une main droite non superposable sur une main gauche.

Les différents énantiomères sont nommés avec les règles Cahn-Ingold-Prelog (UIPAC) qui précisent la configuration absolue des centres stéréogènes, en utilisant les descripteurs R et S.

Exemples d'énantiomères ayant une efficacité clinique en médecine :

Les diastéréoisomères (que l'on écrit aussi diastéréo-isomères) sont les stéréoisomères de configuration qui ne sont pas énantiomères.

Cas intéressant, un isomère méso est un stéréoisomère possédant un nombre pair d'atomes de carbone asymétriques et un plan de symétrie interne (son image dans un miroir lui est superposable), il est achiral.

Lorsque, sur chacun des deux côtés d'une double liaison, on trouve deux groupes différents, on distingue deux configurations : "Z" et "E". En effet, la libre rotation autour de la double liaison n'est pas possible, du fait de la présence d'une liaison π : il en résulte une molécule plane ayant deux configurations possibles.

En utilisant l'ordre de priorité d'après la convention CIP (Cahn, Ingold, Prelog) on définit les deux isoméries ainsi :

Historiquement, ce type d'isomérie ne correspond pas exactement à l'isomérie cis-trans où la priorité des substituants est définie par leur encombrement stérique. Un exemple typique de cette correspondance erronée est celui du cis-2-chlorobut-2-ène qui est aussi le (E)-2-chlorobut-2-ène.

Exemple : l'acide 3-aminobut-2-ènoïque, ci-contre.
Les priorités sont COOH > H et NH > CH. On a donc dans le premier cas les groupes prioritaires du même côté du plan : c'est donc la représentation de l'acide (Z)-3-aminobut-2-ènoïque. À l'inverse, dans la seconde représentation, les groupes prioritaires sont opposés : la molécule représentée est donc l'acide (E)-3-aminobut-2-ènoïque.

De manière générale, les configurations "Z" sont plus rares car les groupements prioritaires (souvent les plus volumineux) sont déstabilisés par leur encombrement stérique. Mais certaines configurations peuvent être stabilisées, notamment par chélation. Il est également possible de transformer un en (ou vice versa) via une photoisomérisation.

"N.B. : Dans le cas d'une diastéréoisomérie due à la présence d'une double liaison, on utilise moins les termes "cis" et "trans" qui qualifient plutôt des positions relatives de groupements sans tenir compte de leur priorité. Par exemple, sur l'illustration de la configuration "E", on dira que le "H" est en "trans" du "CH", et que le "H" est en "cis" du "NH"."

Autre diastéréoisomérie géométrique, beaucoup plus rare, qui apparaît quand l'inversion d'atome comme l'azote est suffisamment bloqué pour caractériser les invertomères.

Très présente dans les systèmes polycycliques possédant des substituants, ce type de diastéréoisomérie apparaît quand un substituant peut adopter deux positions relatives au branchement polycyclique, au plus près ou au plus loin - voir, par exemple, le tropanol.

Deux épimères ne diffèrent entre eux que par la configuration absolue d'un seul carbone asymétrique, comme entre le D-mannose et le D-glucose ou encore entre le D-glucose et le D-galactose.

C’est un cas particulier d’épimèrie pour le carbone 1 des oses. Il permet de décrire notamment la convention α et β. Si la fonction hydroxyle du carbone 1 est en dessous du plan (représentation de Haworth), l’ose est dit α (alpha) alors que si l’hydroxyle du est au-dessus du plan, l’ose est dit β (bêta).

Exemple : α-glucose : sur l'image, le carbone 1 est à droite et la fonction hydroxyle n'est ni en haut ni en bas (il faudrait choisir pour avoir du α ou β).

Cette nomenclature est très importante pour décrire les liaisons chimiques contractées dans les disaccharides et les polysaccharides.

Exemple : le saccharose (α-D-glucopyrannosyl(1→2)β-D-fructofurannoside) est un dissacharide formé d'α-glucose et d'β-fructose liés en α1-2.

Il y a diastéréoisométrie entre deux molécules isomères possédant un même enchaînement de liaison, qui comportent deux ou plusieurs centres de chiralité (des atomes portant quatre substituants différents) et qui ne sont pas des énantiomères. Exemple : les formes "(R)"-"(S)" et "(R)"-"(R)" de l'acide tartrique sont des diastéréoisomères et possèdent des propriétés physiques différentes.


</doc>
<doc id="6591" url="https://fr.wikipedia.org/wiki?curid=6591" title="Chloroforme">
Chloroforme

Le chloroforme ou trichlorométhane est un composé chimique organochloré de formule brute CHCl.

Fréquemment utilisé comme solvant, le chloroforme tend à être remplacé actuellement par le dichlorométhane, aux propriétés similaires mais moins toxique. Le chloroforme fut autrefois utilisé comme anesthésique dans les blocs opératoires et comme conservateur pour la viande.

Eugène Soubeiran (en France), Justus von Liebig (en Allemagne) et Samuel Guthrie (en Amérique) ont découvert en même temps le chloroforme, préparé pour la première fois en 1831. Il est progressivement utilisé au cours du pour soulager la douleur et pour anesthésier les blessés.

Le chloroforme est un liquide hautement volatil. Toutefois les vapeurs de chloroforme ne forment pas de mélanges explosifs avec l'air.

Le chloroforme est un excellent solvant pour de nombreux matériaux organiques tels que graisses, huiles, résines, cires, etc. Il est complètement miscible avec de nombreux solvants organiques et dissout l'iode et le soufre.

Le chloroforme forme de nombreux mélanges azéotropiques avec d'autres liquides tels que l'acétone, l'éthanol, l'eau et le méthanol.

Le chloroforme a une constante molale cryoscopique de ∙kg/mol et une constante molale ébullioscopique de ∙kg/mol.

La production de chloroforme est liée à celle d'autres chlorocarbones. En effet, le chloroforme est produit par chlorations successives du méthane ou de l'éthanol.

Un autre procédé utilise le méthanol comme réactif au lieu du méthane. Il s'agit d'un mélange de chloration/hydrochloration qui permet d'éviter la séparation de l'acide chlorhydrique du méthane lors du recyclage du réactif non utilisé.

Il peut être obtenu industriellement par réaction du fer et d'acide sur le tétrachlorométhane.

On peut aussi obtenir du chloroforme plus simplement en traitant l'acétone par le dichlore en présence d'une base. Il se forme par l'intermédiaire de la trichloracétone, substance que la base scinde facilement en chloroforme et en sel acétique :

Néanmoins, cette dernière synthèse peut être assez dangereuse. En effet, il y a un risque de former du phosgène, un gaz très toxique voire mortel qui a été utilisé comme gaz de combat lors de la première guerre mondiale.

Au cours d'un stockage prolongé, en présence d'oxygène et sous l'action de la lumière, le chloroforme a tendance à se décomposer en donnant du chlorure d'hydrogène, du chlore et de l'oxychlorure de carbone (phosgène) qui est un produit extrêmement toxique. Absorbé ou inhalé à forte concentration, il peut conduire à un coma, voire entraîner des troubles respiratoires et cardiaques qui peuvent s'avérer mortels. Son utilisation en anesthésie a été abandonnée.





</doc>
<doc id="6592" url="https://fr.wikipedia.org/wiki?curid=6592" title="Tétrachlorométhane">
Tétrachlorométhane

Le tétrachlorométhane ou tétrachlorure de carbone est un composé chimique chloré de formule brute : CCl.

À pression et température ambiante, c'est un liquide incolore, très volatil dont les vapeurs sont nocives pour les êtres vivants et dangereux pour la couche d'ozone.

En chimie organique, le tétrachlorométhane est souvent employé en tant que solvant ou réfrigérant, sous le code R-10, même si son usage tend à diminuer en raison de sa forte toxicité. Il est ainsi souvent remplacé par le chloroforme ou le dichlorométhane.

En raison de son atteinte à la couche d'ozone, cette substance est interdite au niveau mondial en un usage industriel massif par le protocole de Montréal depuis 1985 mais sa présence dans l’atmosphère a diminué moins vite que prévu et en 2014 une augmentation inexpliquée a même été constatée.

Les effets de tétrachlorure de carbone sur la santé humaine et l'environnement ont été évalués au titre de REACH en 2012 dans le cadre de l'évaluation de substance par la France. Par la suite, des informations complémentaires ont été demandées aux producteurs, mais plus tard cette décision a été renversée.




</doc>
<doc id="6593" url="https://fr.wikipedia.org/wiki?curid=6593" title="Oxime">
Oxime

Une oxime est une imine (composé organique azoté) particulière dont l'atome d'azote possède un groupement hydroxyle.

Deux voies d'accès sont possibles :





</doc>
<doc id="6595" url="https://fr.wikipedia.org/wiki?curid=6595" title="Amide">
Amide

Un amide est un composé organique dérivé d'un acide carboxylique. Un amide possède un atome d'azote lié à son groupe carbonyle. Les amides sont un groupe important en biochimie, parce qu'ils sont responsables de la liaison peptidique entre les différents acides aminés qui forment les protéines.

Comme certaines autres familles de composés organiques (alcools, amines, etc.), les amides peuvent être classés en trois familles — amides primaires, secondaires ou tertiaires — selon le "nombre de groupes R-C=O liés à l'atome d'azote" :


Attention, contrairement à ce qui est parfois rencontré dans la littérature, ce classement tient en compte uniquement le nombre de groupes R-C=O liés à l'atome d'azote, et pas des éventuels groupes alkyle qui pourraient y être liés.

Ainsi, les amides primaires peuvent être :

Lorsque la fonction amide est prioritaire, le nom des amides primaires non substitués est celui de l'acide carboxylique correspondant en substituant la terminaison "oïque" par la terminaison amide (et en enlevant le terme acide) : 


Les amides primaires substitués sur l'atome d'azote sont nommés en faisant précéder le nom de l'amide de la lettre N suivie du nom du groupe substituant. S'il y en a plusieurs, chacun est précédé de N et ils sont énoncés dans l'ordre alphabétique :


Lorsque la fonction amide n'est pas prioritaire (par exemple par rapport à une fonction ester), elle est nommée par le préfixe "amido" :


Lorsque l'amide comporte un groupe phényle lié à l'atome d'azote, on parle alors d'anilide (dérivé acylé de l'aniline).

Lorsque le groupe phényle est lié au carbone du groupe amide, la règle générale s'applique, on a un dérivé de l'acide benzoïque appelé benzamide.

Les amides cycliques sont appelés lactames. Leur nom est celui de l'alcane cyclique correspondant, précédé du préfixe "aza" et terminé par le suffixe "one". Ils sont cependant souvent nommés par une autre nomenclature qui consiste à utiliser le nom de l'amide non cyclique correspondant, dans lequel le suffixe "amide" est remplacé par "lactame". Comme leurs équivalents oxygénés, les lactones, ils sont précédés d'une lettre de l'alphabet grec indiquant le nombre d'atomes composant le cycle : 

Les peptides et les protéines sont des chaines constituées de plusieurs acides aminés (au moins 100 pour les protéines) liés entre eux par des fonctions amides, liaisons appelées peptidiques (ou pseudo-peptidique dans le cas du glutathion par exemple, où la liaison se fait entre l'azote α et le carbone secondaire appartenant à un acide aminé acide).

Certains antibiotiques possèdent un cycle de bêta-lactame. On les appelle les bêta-lactamines, parmi lesquels on trouve notamment la pénicilline et ses dérivés, les céphalosporines, les monobactames ou les carbapénèmes. Dans ces antibiotiques, ce sont justement les cycles de bêta-lactame qui sont actifs, bloquant la synthèse de la paroi des bactéries.

Les amides sont généralement formés à partir d'un acide carboxylique et d'une amine :

Cette réaction étant un équilibre défavorable à la formation de l'amide, il existe de nombreuses façons de déplacer cet équilibre vers la droite, la plupart du temps en « activant » l'acide, par exemple en le transformant en chlorure d'acyle (R-CO-Cl) avec libération de chlorure d'hydrogène HCl, piégé par l'amine.
Le chlorure d'hydrogène (HCl) produit peut être neutralisé par l'amine mais également par l'ajout d'un capteur de proton comme la pyridine par exemple.
La meilleure méthode connue utilisant cette technique est la réaction de Schotten-Baumann.

Une autre possibilité est de partir d'un anhydride d'acide (R-CO-O-CO-R) avec libération d'acide carboxylique, qui réagit avec un deuxième équivalent de l'amine.

L'acide produit n'est pas assez réactif pour former un amide avec l'amine.
Il est également possible d'utiliser l'acide carboxylique activé par un réactif de couplage peptidique, de faire réagir un acide carboxylique et une amine pour former un sel qui peut être converti en amide par déshydratation, ou de partir d'oximes comme dans le réarrangement de Beckmann.

La réaction simple et directe entre un alcool et une amine n'a jamais été essayée avant 2007, date à laquelle a été rapporté un catalyseur à base de ruthénium capable d'effectuer une « acylation déshydrogénative » :

La génération de dihydrogène gazeux compense la thermodynamique défavorable. On pense que cette réaction procède par une déshydrogénation de l'alcool en aldéhyde suivie par la formation d'un hémiaminal, et ensuite une seconde déshydrogénation pour former l'amide. L'élimination d'eau de l'hémiaminal pour former une imine n'a pas été observée.

Les amides sont des composés peu réactifs. Ils peuvent être hydrolysés par chauffage en milieu acide, pour générer un acide carboxylique et une amine. Il est aussi possible de les réduire à l'aide de tétrahydruroaluminate de lithium LiAlH pour générer une amine substituée.

Les polyamides sont des polymères comportant la fonction amide. Le polyamide 6,6 est un nylon ; le polyamide 11, quant à lui, a pour nom commercial Rilsan. Les chiffres indiqués correspondent aux nombres d'atomes de carbone sur la chaîne polyamide entre deux fonctions amides.




</doc>
<doc id="6596" url="https://fr.wikipedia.org/wiki?curid=6596" title="Réarrangement de Beckmann">
Réarrangement de Beckmann

En chimie organique, le Réarrangement de Beckmann est une réaction permettant de convertir une oxime en amide et implique la migration d'un groupement alkyle sur un azote rendu électrophile (pauvre en électron).

Cette réaction repose sur la conversion du groupement hydroxyle de l'oxime en un groupement partant à l'aide d'un acide (de Lewis ou non), d'un anhydride ou d'un halogénure d'acyle.

Le groupement hydroxyle de l'oxime est tout d'abord converti en groupement partant à l'aide de X. L'ionisation et la migration du groupement -R ont alors lieu au cours d'un processus concerté : c'est donc le groupement -R en "anti" par rapport au groupe hydroxyle de la fonction oxime (et non -R') qui migre. On obtient alors un ion nitrillium qui va capturer un nucléophile (HOX), et l’intermédiaire obtenu sera alors converti en amide lors de l'hydrolyse.



</doc>
<doc id="6600" url="https://fr.wikipedia.org/wiki?curid=6600" title="Organisation armée secrète">
Organisation armée secrète

L'Organisation armée secrète, ou Organisation de l'armée secrète, surtout connue à travers le sigle OAS, est une organisation politico-militaire clandestine française, créée le pour la défense de la présence française en Algérie par tous les moyens, y compris le terrorisme à grande échelle.

Un an après l'échec de la semaine des barricades, alors que le gouvernement français souhaite manifestement se désengager en Algérie, elle est créée à Madrid, lors d'une rencontre entre deux activistes importants, Jean-Jacques Susini et Pierre Lagaillarde, ralliant par la suite des militaires de haut rang, notamment le général Raoul Salan.

Le sigle « OAS » fait volontairement référence à l’Armée secrète (AS) de la Résistance. Il apparaît sur les murs d'Alger le , et se répand ensuite en Algérie et en métropole, lié à divers slogans : « L'Algérie est française et le restera », « OAS vaincra », « l'OAS frappe où elle veut et quand elle veut », etc.

Sur le plan pratique, il ne s'agit pas d'une organisation centralisée unifiée ; d'une façon très générale, elle est divisée en trois branches plus ou moins indépendantes, parfois rivales : l'« OAS Madrid », l'« OAS Alger » et l'« OAS Métro ».

L'histoire de l'OAS se présente comme la manifestation la plus radicale d'une partie de l'armée et de civils de conserver l'Algérie française, où vivaient un million d'habitants ayant le statut de citoyens français, et huit millions d'autochtones sous statut de l'indigénat. L'Algérie ayant alors le statut de département français, l'OAS voulait s'opposer par tous les moyens à la politique d'autodétermination mise en place par Charles de Gaulle à partir de la fin de l'année 1959.

Le général de Gaulle est arrivé au pouvoir en 1958, à la suite du coup d'État du 13 mai 1958, et ses premières déclarations (« Je vous ai compris » le 4 juin 1958 à Alger et « Vive l'Algérie française » le 6 juin à Mostaganem) semblent porter les valeurs de l'Algérie française. Néanmoins, il ne manque pas d'annoncer à plusieurs occasions (à Alger, Oran, Constantine, Bône) « qu'il n'y a en Algérie que des Français à part entière avec les mêmes droits et les mêmes devoirs », signifiant de fait que le système colonial tel qu'il existait en Algérie ne pouvait plus être maintenu en l'état, et, après Mostaganem, il ne prononcera plus les mots d'« Algérie française ». La mise en place du plan de Constantine (économique) en octobre 1958 et du plan Challe (militaire) en février 1959 indique la volonté du gouvernement de conserver une Algérie où la France joue un rôle actif et rassure provisoirement les Français d'Algérie. À plusieurs occasions au cours de l'année 1959, de Gaulle souligne le caractère propre de l'Algérie et que son avenir dépendra du vote de ses habitants, mais la plupart de ces déclarations ne suscitent guère de réactions. C'est le discours de De Gaulle du 16 septembre 1959 proposant l'autodétermination sur l'avenir de l'Algérie qui suscite la surprise dans tous les milieux, et la stupéfaction dans la population européenne d'Algérie. Ce discours ouvre trois possibilités : la francisation (un seul pays réunissant la France et l'Algérie et dont tous les citoyens ont les mêmes droits), l'autonomie (une fédération entre la France et l'Algérie), la sécession (conduisant à l'indépendance). Pour la première fois, il ouvre la possibilité de l'indépendance de l'Algérie. Le 15 octobre 1959, l'Assemblée nationale valide la politique d'autodétermination par 441 pour et 23 contre. Mais pour ceux qui refusent cette politique, regroupant certains membres de la classe politique (Jacques Soustelle, Georges Bidault), ainsi qu'une partie de l'armée et des Français d'Algérie, c'est une trahison. Le premier fait marquant de la révolte de la population française d'Algérie contre ce discours est la semaine des barricades du 24 janvier au février 1960, mais l'armée ne bascule pas du côté des insurgés.

Le 8 janvier 1961, le référendum sur l'autodétermination en Algérie est approuvé par 75 % des votants. Pour les partisans de l'Algérie française, ce référendum annonce l'abandon de celle-ci. En février 1961, un groupe, exilé à Madrid à la fin de l'année 1960 pour échapper au procès de la semaine des barricades, se forme autour du général Salan, de Pierre Lagaillarde et de Jean-Jacques Susini et crée l'OAS. Le 22 avril 1961, se déroule le putsch des généraux à Alger, suivi par environ deux cents officiers. Néanmoins, la plupart des officiers supérieurs adopte une attitude attentiste et la majorité de l'armée reste loyale au pouvoir métropolitain, entraînant l'échec du putsch en quelques jours. À la suite de cet échec, une bonne partie des insurgés ainsi que de nombreux civils désertent et rejoignent la lutte clandestine dans les rangs de l'OAS, La cassure est totale avec de Gaulle et il s'ensuit une véritable guerre entre les membres de l'OAS et l'État. De Gaulle utilisera contre l'OAS aussi bien la police que des groupes illégaux (les barbouzes), mais laissera l'armée en retrait car elle compte dans ses rangs de nombreux sympathisants à la cause « Algérie française » qui sont tentés de rejoindre l'Organisation. Elle ne sera utilisée contre l'OAS qu'après la signature des accords d'Évian, au moment du siège de Bab El Oued, de l'épisode de la fusillade de la rue d'Isly et du maquis de l'Ouarsenis.

Les attentats de l'OAS viseront des personnalités politiques et administratives du gouvernement légal français, des intellectuels ou des organes de presse favorables à une négociation avec le FLN, en Algérie comme en métropole, ainsi que la population musulmane, soupçonnée de soutenir le FLN. Ses commandos prendront également pour cible les policiers, les enseignants, les fonctionnaires de l'administration fiscale, les commerçants musulmans. Les membres de l'OAS sont eux-mêmes pourchassés sans répit par les forces gaullistes. L'OAS sera largement soutenue par la population française d'Algérie, mais ses nombreux attentats aveugles la feront rejeter par l'opinion publique métropolitaine.

Dès le printemps 1961, le commissaire Grassien, sous-directeur de la Police judiciaire (PJ), arrive en Algérie à la tête de quinze officiers. Mais les résultats sont limités, et le groupe regagne la métropole le 9 novembre 1961. Quelques semaines plus tard, le directeur de la PJ, Michel Hacq, les relaie, avec pas moins de deux cents inspecteurs, qui forment la Mission « C ». Ces policiers sont renforcés par un peloton de quinze gendarmes, dirigé par le capitaine Lacoste, et qui avait déjà combattu le Front de libération nationale (FLN). Ce sont ces gendarmes qui arrêtent le général Raoul Salan, le 20 avril 1962, grâce aux renseignements fournis par la Police judiciaire parisienne.

De son côté, le général Charles Feuvrier, chef de la Sécurité militaire (SM), crée une structure spécifiquement chargée de la lutte anti-OAS en Algérie, la Division des missions et recherches. La Sécurité militaire était jugée peu sûre voire favorable aux idées OAS.

Ces forces officielles sont aidées par des agents de police parallèle, les célèbres « barbouzes » (dont le rôle secret et méconnu pourrait avoir été d'attirer l'attention de l'OAS afin que la Mission « C » puisse travailler sereinement et en profondeur), ainsi appelés en raison des postiches qu’ils étaient censés porter (le terme s’est ensuite appliqué à tous les agents secrets, réguliers ou non). Sans mandat officiel, les barbouzes sont recrutés dans divers milieux : des champions d’arts martiaux (Jim Alcheik, Raymond Buy Tré...etc), des Vietnamiens ayant choisi la France pendant la guerre d’Indochine, des marginaux, et des truands, comme Jean Augé et le proxénète Georges Boucheseiche, ancien de la Gestapo française et par intermittence, des musulmans (sous la houlette du cheikh Zeknini et de ses fils). Ce recrutement, ainsi que l'acheminement vers l'Algérie, sont assurés par deux ardents partisans du général de Gaulle, Lucien Bitterlin, chef du Mouvement pour la communauté, et par Pierre Lemarchand. Les barbouzes sont chargés de faire du contre-terrorisme, c’est-à-dire des plasticages (à la place de la Sécurité militaire, qui ne pouvait elle-même commettre des attentats), de réaliser des interrogatoires, en collaboration étroite avec les forces de gendarmerie du colonel Debrosse (au cours desquels la torture est utilisée, selon les membres de l'OAS qui les ont subis, Geneviève Salasc, capitaine Noëlle Lucchetti, Bonadé, Jean Hourdeaux, Charles Daudet, Albert Garcin, Rodenas, Ziano, etc.).

La branche de la mission « C » chargée du renseignement, le CRC (Centre de recherches et de coordination), parvient très rapidement à établir des listes de personnes susceptibles d'appartenir à l'OAS, et manipulant adroitement les barbouzes, remet dès janvier 1962 à Lucien Bitterlin, qui la fait transmettre au FLN par l'intermédiaire de Smaïl Madani, une première liste de membres de l'OAS (noms et pseudonymes, âges et adresses). Avec les accords d'Évian, le rapprochement s'opère directement entre mission « C » et FLN à Alger et à Oran, et quelques jours plus tard, une seconde liste de membres de l'OAS est remise à Si Azzedine, chef de la Zone autonome d'Alger. Après avoir démantelé en quelques mois l'OAS par l'arrestation de ses chefs, la mission « C » pense ainsi transmettre la basse besogne au FLN. Mais d'après l'historien Jean-Jacques Jordi .

Les barbouzes ont été décimés par l'OAS, par des attaques récurrentes, notamment celle du nouvel an 1962 où deux des villas PC furent attaquées par plusieurs deltas équipés de lance-roquettes et surtout par l'explosion d'une machine à ronéotype (qui décimera la première équipe de barbouzes fin janvier 1962), livrée pourtant sous le sceau du secret mais piégée lors de son transit en douane. Puis lorsqu'une seconde équipe de barbouzes moins expérimentée est arrivée, par les attaques contre l'hôtel Rajah où elle se trouve et sa destruction. Le ministre de l'Intérieur Roger Frey jette alors l'éponge pour l'Algérie et fait rapatrier les quelques survivants le 8 mai 1962. Cependant, l'activité de barbouzes et les déplacements fréquents de Pierre Lemarchand entre l'Algérie et la métropole, sous une fausse identité, sont encore relevés fin mai. Le Service d'action civique (SAC) a participé ensuite à la répression de l'OAS, mais de manière assez marginale. En revanche, nombre de ces agents de police parallèle ont rejoint le SAC après 1962, notamment Augé (qui devient chef du SAC pour Lyon et sa région), ou Georges Boucheseiche.

En métropole, la lutte contre l’organisation armée devient efficace en décembre 1961, avec la formation du Bureau de Liaison. Ce Bureau de Liaison regroupe tous les agents des forces de l’ordre chargés d’enquêter sur l’OAS et d’arrêter ses membres : PJ, DST, RG, Gendarmerie nationale, Sécurité militaire de métropole. Les chefs du Bureau de Liaison se réunissent tous les soirs et travaillent en contact direct avec le ministre de l’Intérieur Roger Frey, le conseiller de celui-ci chargé la lutte anti-OAS, Alexandre Sanguinetti, et le conseiller spécial de Michel Debré chargé de coordonner l’action des services secrets, Constantin Melnik. En avril 1962, le célèbre éditorialiste Jean Grandmougin est licencié de Radio Luxembourg.

En 1962, un groupe de membres de l'OAS s'était réfugié à Profondeville (Belgique). Tous étaient partis moins d'une heure avant l'intervention de la Gendarmerie belge. De nombreuses armes avaient été retrouvées dans l'immeuble de la rue Antoine Gémenne.


C'est entre la mi-mai et la fin août 1961 que l'OAS d'Alger se structure, principalement sous la directive du colonel Godard pour les militaires, et Jean-Jacques Susini pour les civils. À Oran, l'OAS est chapeauté par le général Jouhaud. Le général Salan accepte de prendre la tête de l'organisation début septembre, à laquelle se rallie l'OAS de Madrid fin novembre. En métropole, se crée en juin 1961 un réseau créé par le capitaine Sergent, lié à l'OAS d'Alger, mais également d'autres groupes indépendants, l'un créé par Jeune Nation, l'autre par le Maquis Résurrection Patrie de Marcel Bouyer. André Canal arrive également en métropole à la mi-décembre 1961 et mène ses propres actions indépendamment de celles de Pierre Sergent. De septembre à décembre 1961, l'OAS est en phase de montée en puissance. Mais l'État français est soutenu par l'opinion publique métropolitaine et la majorité des musulmans algériens, l'armée reste légitimiste et l'OAS ne parvient pas à obtenir de nouveaux ralliements. À partir de janvier 1962, elle se radicalise et se lance dans une insurrection armée.



Dans les mois précédents le cessez-le-feu les attentats du FLN, comme ceux de l'OAS, se monteront à plusieurs centaines par mois. Après le cessez-le-feu, les attentats de l'OAS augmenteront encore en intensité. Le FLN se livrera davantage à l'enlèvement d'européens, suivi de leur disparition.

La signature des accords d'Évian marque pour les Français d'Algérie une période de désillusion, d'abandon et de désespoir. La rupture avec l'armée se produit lors de la Fusillade de la rue d'Isly. L'OAS va tenter d'empêcher l'application des accords et, ne parvenant pas à enrayer le départ de la population européenne d'Algérie, se lance dans une entreprise de destruction.


Seule une partie des membres de l'OAS est connue, car arrêtés ou identifiés, mais leur nombre est supérieur à ces seules listes. On estime que l'OAS a compté environ à membres actifs, dont 500 dans l'Ouest algérien, 200 en métropole et une vingtaine en Espagne. Les civils représentaient environ 2/3 des effectifs, l'autre tiers étant constitué de militaires, pour la plupart engagés, sous-officiers et officiers.

Parmi les militaires, on trouve surtout des soldats d'élite comme des légionnaires ou des parachutistes, fort peu de marins. L'état d'esprit des militaires ayant rejoint le mouvement est résumé dans la déclaration du commandant Hélie Denoix de Saint Marc à son procès : maintien de la souveraineté de la France, lutte contre le communisme, volonté que tous les morts ne l'aient pas été pour rien, respect de la promesse donnée aux populations indigènes ralliées à la France.

Parmi les civils, on dénombre surtout des employés, cadres moyens, commerçants, artisans, peu de cadres supérieurs ou professions libérales. L'organisation compte une minorité de femmes, surtout affectées au transport de courrier et de fonds.

En ce qui concerne la sensibilité politique de ses membres, Guy Pervillé y distingue trois courants principaux : un courant néo-fasciste inspiré par Jeune Nation, un courant traditionaliste proche du mouvement poujadiste ou de l'hebdomadaire "Rivarol", mais parfois aussi du traditionalisme catholique, et enfin un courant nationaliste. Il serait cependant réducteur de considérer l'OAS simplement comme une organisation d'extrême droite. Parmi ses membres d'un certain âge, on comptait de nombreux anciens résistants (parmi les plus connus, on peut citer Georges Bidault, Jacques Soustelle, Raoul Salan, Pierre Chateau-Jobert, Yves Godard, Pierre Sergent, Jacques Achard). On y trouve également des communistes du quartier populaire de Bab-El-Oued. Mais la provenance diverse de ses membres limitait l'action de l'OAS au seul maintien de l'Algérie française et au rejet de la politique du général de Gaulle, sans qu'un programme politique puisse définir l'avenir de l'Algérie.

Selon Vitalis Cros, dernier préfet de police d'Alger, les attentats de l'OAS ont été quatre fois plus nombreux de décembre 1961 à juin 1962 que ceux du FLN pendant six ans. Quant au nombre de ses victimes, selon les chiffres d'un ouvrage collectif d'historiens français et algériens, s'il surpasse celui du FLN à Oran et à Alger en 1962, il est resté nettement inférieur jusqu'aux premiers mois de 1962, et son total d'ensemble est également moins élevé.

. L'historien Olivier Dard estime ces bilans très exagérés.

Parmi les travaux récents, l'historien français Rémi Kauffer estime que l'OAS a assassiné entre et personnes. Le journaliste américain Paul Hénissart cite lui une source officieuse selon laquelle le nombre de victimes assassinées en Algérie s'élève à 2 200. L'historien français Guy Pervillé, s'appuyant sur deux rapports des forces de l'ordre (l'un de la Sûreté nationale, l'autre du général Fourquet, commandant supérieur des troupes françaises), et considérant l'« escalade de la violence » entre le printemps et l'été 1962, estime que ce chiffre est peut-être inférieur à la réalité. Jean-Louis Planche donne le nombre de morts dont 239 Européens.

En mars 1993, un trio d'anciens de l'OAS assassine Jacques Roseau, lui-même ancien membre de l'OAS et président de l'association de rapatriés « Recours », faisant de lui la dernière victime de l'organisation.

L'OAS a elle aussi subi des pertes et officiellement 119 membres ont été tués. En 1962, 635 membres de l'OAS sont arrêtés. 224 sont ensuite jugés, dont 117 acquittés, cinquante-trois condamnés à une peine de prison avec sursis, trente-huit à une peine de prison ferme, trois sont condamnés à mort et fusillés (Roger Degueldre, Claude Piegts et Albert Dovecar) ; le lieutenant-colonel Bastien-Thiry est également passé par les armes. Son appartenance à l'OAS est sujette à discussion, mais, selon les membres du commando, il appartenait bien, comme eux, à la dernière formation de l'OAS-Métropole, également appelée OAS-CNR. Jusqu'en 1965, les arrestations s'élèveront à dix mille personnes, et le nombre de condamnés à (décompte de Rémi Kauffer, qui estime par ailleurs que les policiers, gendarmes, militaires et barbouzes ont tué plusieurs centaines de Français d'Algérie durant la guerre). Plusieurs membres de l'OAS se sont réfugiés à l'étranger, notamment en Espagne, au Portugal et en Amérique du Sud. Plusieurs sont condamnés à mort par contumace (Joseph Ortiz, le colonel Château-Jobert, André Rossfelder, le colonel Arnaud de Seze, le colonel Yves Godard, les capitaines Pierre Sergent et Jean Biraud). La peine de mort pour motif politique, abolie depuis 1848, ayant été rétablie par ordonnance du 4 juin 1960.

En ce qui concerne les seuls officiers de l'armée française membres ou sympathisants OAS, environ 490 ont été condamnés à de la prison, 530 rayés des cadres, 1300 démissionnent.

La première amnistie date du 17 décembre 1964 et concerne les « événements » d'Algérie. Le 21 décembre 1964, les prisonniers condamnés à des peines inférieures à quinze ans de détention (soit 173 anciens membres de l'OAS) sont libérés par grâce présidentielle, car il s'agit de prisonniers politiques. D'autres mesures de grâce sont prises à Noël 1965. En mars 1966, une centaine de condamnés sont graciés et, le 17 juin 1966, une seconde loi d'amnistie efface les condamnations des condamnés libérés. Le général Jouhaud, condamné à mort le 13 avril 1962, passe 235 jours dans une cellule de condamné à mort, sa peine est commuée en perpétuité, il est libéré en décembre 1967. En 1968, des anciens de l'OAS rencontrent Jacques Foccart pour lui proposer leur ralliement au régime gaulliste contre la « "chienlit" », et demander l'amnistie intégrale des membres de l'organisation encore incarcérés, ce qu'ils obtiendront le 7 juin 68 après les événements de mai 68 : le gouvernement, craignant un coup de force des communistes, efface toutes les sanctions disciplinaires et professionnelles et réintègre les amnistiés dans leurs droits, pensions et décorations. Cette amnistie est promise par de Gaulle à Massu, lors de sa visite à Baden Baden. En juillet 1974, une amnistie complémentaire efface d'autres condamnations pénales, autorisant ainsi la réintégration de cadres de l'OAS dans la vie politique française notamment par l'intermédiaire des Républicains indépendants de Valéry Giscard d'Estaing, ce qui permet à certains comme Pierre Sergent de devenir députés. Dans le cadre de l'élection présidentielle française de 1981, des négociations menées par des proches du candidat François Mitterrand aboutissent à l'appel du général Salan à voter Mitterrand et sanctionner Giscard d'Estaing. Le 3 décembre 1982, les officiers survivants sont réintégrés dans les cadres de l'armée par une nouvelle amnistie (loi du 24 novembre 1982), à l'exclusion des officiers généraux. En 1987, une loi sur les rapatriés amnistie les dernières condamnations encore effectives.


Général Raoul Salan

Général Paul Gardy (Adjoint Colonel Yves Godard)







Les attentats du FLN touchent également les synagogues et les rabbins : en janvier 1962, le FLN commet des attentats dans le quartier juif de Mostaganem ; une grenade est lancée sur un marché du quartier juif de Constantine en mai ; etc.

Durant cette période, la communauté juive s'oriente d'une manière générale vers une position de neutralité. Les organisations communautaires font preuve d'une extrême modération et refusent de prendre politiquement position ; pourtant certains de leurs membres s'engagent au sein de l'OAS, de manière individuelle comme Jean Ghenassia qui deviendra le lieutenant de Joseph Ortiz (selon Emmanuel Ratier), ou collectivement comme à Alger et à Oran où ils seront particulièrement actifs. Regroupés au sein des « Commandos Colline », ces groupes sont liés aux réseaux France Résurrection conduits par "Elie Azoulai" et "Ben Attar". Ils assassineront certains élus musulmans, essaieront de mettre le feu à la prison où sont détenus des hommes du FLN, et abattront des officiers français, comme le lieutenant-colonel Pierre Rançon, chef du Bureau d'Oran, chargé de la lutte anti-OAS.

L'OAS avait lancé un appel aux français-musulmans pour combattre auprès d'eux, contre le FLN. En 1962, on dénombre au moins deux musulmans parmi les jeunes dirigeants de l'OAS.

En 1963, Bachaga Boualam explique ainsi l'engagement de nombreux musulmans au sein de l'OAS : .









</doc>
<doc id="6602" url="https://fr.wikipedia.org/wiki?curid=6602" title="Front de libération nationale (Algérie)">
Front de libération nationale (Algérie)

Le Front de libération nationale (FLN), (en ) est un parti politique algérien, aujourd'hui présidé par l'actuel président algérien Abdelaziz Bouteflika.

Il a été créé en novembre 1954 pour obtenir de la France l'indépendance de l'Algérie, alors divisée en départements français d'Algérie. Le FLN et sa branche armée, l'Armée de libération nationale (ALN), commencent alors une lutte contre l'empire colonial français. Par la suite, le mouvement s'organise et, en 1958, le FLN forme un gouvernement provisoire, le GPRA. C'est avec le GPRA que la France négocie en 1962 les accords d'Évian.

À l'indépendance, le FLN prend ainsi le pouvoir légitimement, et s'en assure l'exclusivité en instaurant le système de parti unique. Après d'importantes luttes internes, Ahmed Ben Bella prend la tête du parti, et donc de l'État. Il sera renversé trois ans plus tard par Houari Boumédiène (1965 – 1978) qui prend les pleins pouvoirs, réduisant largement la place du parti.

Le FLN reprend une importance centrale avec Chadli Bendjedid (1979 – 1992), qui, dans les années 1980, est poussé, par de nombreuses protestations, à approuver une nouvelle Constitution et à introduire le multipartisme.

Avec les premières élections libres, en 1991, le FLN subit une lourde défaite mais l'ascension du FIS (Front islamique du salut) est empêchée par un coup d'État militaire. Le pouvoir militaire dirige et codirige alors l'État, légitimé par l'urgence de la « guerre civile », mais sans le soutien du FLN mis à l'écart. Le parti sort de cette « décennie noire » affaibli, alors que son existence même avait été mise en cause. Ayant définitivement perdu son statut de « parti du pouvoir », il conserve cependant une place importante dans la politique de l'Algérie.

Fondé le en Algérie, le FLN apparut publiquement le novembre 1954 pour engager une lutte de libération nationale contre la « France coloniale », présente depuis 1830, et pour la création d'un « État algérien démocratique et populaire ».

Il a été créé à l'initiative du Comité révolutionnaire d'unité et d'action (CRUA), en appelant à l'union de toutes les forces politiques nationalistes pour la lutte de libération du pays. Il est composé par ceux qui deviendront les six « chefs historiques » du FLN : Krim Belkacem, Mostefa Ben Boulaïd, Larbi Ben M'Hidi, Mohamed Boudiaf, Rabah Bitat, Didouche Mourad.

Sa première apparition se traduisit dans les faits par des attaques plus ou moins désordonnées contre l'armée française, installations militaires, commissariats, entrepôts, équipements de communications, et des bâtiments publics, principalement dans les régions des Aurès et de la Kabylie. Ces attaques, qui prirent par la suite le nom de "Toussaint rouge", s'accompagnèrent de la déclaration dite du « novembre 1954 », dans laquelle le FLN invitait le peuple d'Algérie à s'associer à la « lutte nationale ». Le front nationaliste algérien est le premier mouvement nationaliste à user de la violence et l'adoptant comme mode principal d'action. Il marquait ainsi une rupture avec les autres mouvements tels que l'Union démocratique du manifeste algérien (UDMA) de Ferhat Abbas, le Mouvement national algérien (MNA) de Messali Hadj ou encore l'Association des oulémas musulmans algériens. Cependant, en mai 1955, Ferhat Abbas rejoint le FLN dans sa lutte pour l'indépendance. Des attaques furent organisés par le FLN (Front de libération nationale) contre les colons européens d'Algérie. Une trentaine d'attaques plus ou moins désordonnées ont lieu le jour de la Toussaint dans tout le territoire algérien. Le bilan est de sept morts.

Le FLN était doté d'une branche armée, l'Armée de libération nationale (ALN), dotée d'un état-major ainsi que d'une organisation militaire par laquelle les zones de combat furent divisées en 5 régions militaires. L'armement provenait essentiellement d'Égypte et était acheminé via le Maroc ou la Tunisie. À partir de 1958, les combattants FLN étaient suffisamment bien équipés et entraînés pour mener une guerre de guérilla. Devant la forte répression des forces françaises, les dirigeants de la rébellion durent constituer des dépôts d'armes en territoires tunisien et marocain. Lorsque la France renforce le contrôle des frontières et installe les lignes Challe et Morice, une grande partie de l'armement n'est pas acheminé. Il en résulte, à la fin du conflit, une armée des frontières du FLN surarmée, qui échappait le plus souvent aux forces françaises, et des « moudjahidine » du maquis en Algérie presque sans équipement.

Le mouvement se structura ensuite grâce notamment à la plate-forme politique du Congrès de la Soummam d'août 1956, organisé principalement par Abane Ramdane lui-même représentant le FLN durant le congrès, qui donnait un statut pour l’Armée de libération nationale devant se soumettre aux « lois de la guerre » et des instances politiques dirigeant la « Révolution », et affirmait la « primauté du politique sur le militaire et de l’intérieur sur l’extérieur ».

En continuité avec cette construction institutionnelle fut créé le Gouvernement provisoire de la République algérienne (GPRA) pour parachever la mise en place des institutions de la Révolution et la reconstruction d'un État algérien moderne. Ce gouvernement provisoire, qui donnait une reconnaissance internationale au mouvement, avait mis les autorités françaises devant le fait accompli, autorités qui finirent par négocier, après que le Général de Gaulle l'eut décidé.

Analysant l'importance du FLN au sein de la population durant la guerre d'Algérie, l'historienne Sylvie Thénault rappelle que le FLN ne fut pas un « parti de masse », et qu'il « ne bénéficiait pas d'une assise sociale. Il ne profita qu'à de rares moments d'une mobilisation populaire. » L'adhésion de masse au projet indépendantiste est d'autant plus difficile à mesurer du fait « de la violente pression qu'a exercé le FLN sur les populations ».

Le combat armé sur le territoire algérien et métropolitain pour l’indépendance, avec le slogan « La Révolution algérienne, un peuple au combat contre la barbarie colonialiste », dura jusqu'au 18 mars 1962, date à laquelle le gouvernement français signa les accords d'Évian qui aboutirent à un accord de cessez-le-feu avec le FLN. En juillet de la même année, le peuple algérien vota par référendum pour l’indépendance et ratifia les accords d'Évian qui prévoyaient une coopération économique et sociale entre les deux pays.

Dès la proclamation de l’indépendance, le FLN prit le pouvoir, apparaissant comme le mouvement ayant permis à l’Algérie d'accéder à l'indépendance. Toutefois, d’importantes luttes intestines déchiraient le parti, et déclenchaient de nombreuses « purges ». Ainsi Ahmed Ben Bella, partisan d'un pouvoir militaire, soutenu par l'armée des frontières suréquipée qui s'imposa largement face aux militants des maquis presque désarmés, dissolut le GPRA œuvrant pour une Algérie démocratique, et prit la tête de l'État.

Aussi, le parti d'« après l'indépendance » n'est plus le même que le « FLN historique », front rassemblant les différentes forces politiques des anciens partis nationalistes d'avant 1954. Et c'est en s'appropriant cette légitimité historique que les directions successives du FLN ont justifié le monopole du parti unique, interdisant le Parti communiste en 1963, ainsi que le Parti de la révolution socialiste (PRS) de Mohamed Boudiaf et, plus tard, le Front des forces socialistes (FFS) de Hocine Aït Ahmed.

De fait, le renversement de Ahmed Ben Bella était prévisible dès la clôture du congrès du parti du FLN (avril 1964) dont les résolutions finales accordaient au Président des prérogatives telles qu'il affirmait n'être responsable, ni devant les instances dirigeantes du FLN, ni devant l'Assemblée nationale. Ben Bella avait, depuis longtemps entamé le démantèlement de ce qu'il convenait d'appeler le « groupe d'Oujda » (entourage proche du colonel Boumédiène, alors premier vice-président de la République et ministre de la Défense nationale).

Après les limogeages de Kaïd Ahmed, Chérif Belkacem, Ahmed Medeghri et la prise en main personnelle des ministères vacants, la décision de mettre fin aux fonctions d'Abdelaziz Bouteflika, ministre des Affaires étrangères, allait précipiter les événements. Houari Boumédiène, pressé par son entourage, restait convaincu de pouvoir ramener Ben Bella à de meilleurs sentiments et au respect des institutions de l'État. Deux rencontres au moins se déroulèrent entre les deux hommes sans que Ben Bella ne changeât d'attitude.

Lorsque Houari Boumédiène comprit que le prochain sur la liste des éliminés serait lui-même, il se décida à mettre fin au régime de Ben Bella. Il confia au commandant Slimane Hoffman, responsable des blindés, la surveillance de l'immeuble de la radio, du palais du Peuple (siège du gouvernement) et de la Villa Joly (siège du bureau politique du Parti du FLN et résidence de Ben Bella). Les colonels Tahar Zbiri et Abbès furent chargés de l'arrestation du président, cueilli au saut du lit. Le lendemain, Boumédiène apparaissait à la télévision pour annoncer la fin du régime de pouvoir personnel ; une proclamation, dite « du 19 juin » qui contenait le catalogue des critiques de la gestion de Ben Bella, fut lue. Coup d'État, pour les uns, et renversement du président Ben Bella ; redressement révolutionnaire pour d'autres.

La vision de Houari Boumédiène se situait en complète opposition avec le système politique précédent et consacrait la primauté de la construction de l'État sur l'action politique. Dans un nationalisme étatiste-socialiste et anticolonialiste, Boumédiène régnait par décret et selon une « légitimité révolutionnaire », marginalisant le FLN en faveur de son propre pouvoir et celui de l'armée, tout en maintenant le système de parti unique. À partir de 1976, la première révision constitutionnelle a fait du pays un « État socialiste » sous contrôle du FLN, qui servait ainsi encore d'appui pour les décisions de Boumédiène.

Durant ces années, les cadres dirigeants des années de guerre forment progressivement, à l'instar d'autres pays du tiers monde, « une couche bureaucratique privilégiée ».

À la mort de Houari Boumédiène en 1978, son successeur, le colonel Chadli Bendjedid, réorganise le parti, qui prend alors une place centrale après une longue période de mise à l'écart. Les militaires gardaient encore une large représentation dans le Comité central du FLN, et possédaient en grande partie le contrôle de l'État.

Durant les années 1980, le FLN modère la teneur « socialiste » de son programme, opérant quelques réformes de libéralisation du marché et écartant les lieutenants restants de Boumédiène. Cependant, les principales avancées démocratiques ne purent avoir lieu qu'avec les émeutes d'octobre 1988 qui secouèrent le pays vers des réformes politiques essentielles. Le 28 février 1989, une révision constitutionnelle instaura le multipartisme et la liberté d’expression ; le régime du parti unique fut donc dissous. Le FLN coupa ainsi ses liens particuliers avec l'armée nationale et privilégiés avec l'appareillage de l'État.

Ces premières tentatives d'ouverture permirent au Front islamique du salut (FIS) d'arriver en tête au premier tour des élections législatives de 1991. Le FIS obtint 188 sièges sur 231, le FFS 25 sièges et le FLN seulement 15 sièges, alors que les candidats indépendants en remportèrent 3. Prenant acte de cette montée du parti islamiste, l'armée décide le 11 janvier 1992 d'un véritable coup d'État militaire en poussant le président Chadli Bendjedid à la démission et en interrompant les élections.

En janvier 1992, Mohamed Boudiaf accepte le poste de président du Haut Comité d'État qui lui est proposé, mais est assassiné quelques mois plus tard. L'Algérie tombe alors encore sous le contrôle direct des militaires.

Le parti demeure alors dans l'opposition au gouvernement durant la « guerre civile ». Après une restauration partielle de la démocratie, en 1995, une élection présidentielle est organisée. Mais celle-ci est boudée par les grandes formations de l'opposition, et le FLN, comme le FIS et le FFS, en appellent à l'abstention.

Le 16 novembre 1995, le général Liamine Zéroual est élu président de la République, le premier à la suite d'un scrutin pluraliste. Le FLN reste extérieur au pouvoir ; les clans militaires appuient en effet leur légitimité politique sur d'autres partis. Le FLN signe avec six autres formations politiques en 1995, la plate-forme de Rome, qui critique directement le pouvoir militaire et sa gestion de la crise ; cependant, après d'importants débats internes, la position officielle du parti changea pour un soutien de la présidence.

En 1998, Liamine Zéroual annonce officiellement la tenue d'une élection présidentielle anticipée pour février 1999, à laquelle il ne se présentera pas. Cette sortie précipitée du président est alors attribuée aux généraux de l'armée craignant une nouvelle montée des intégristes religieux au pouvoir avec lesquels Liamine Zéroual entretenait des relations de plus en plus étroites. Pour l'élection, aux côtés des principaux candidats comme Hocine Aït Ahmed, Mouloud Hamrouche et Taleb Ibrahimi, les généraux font appel, pour lui attribuer leur soutien, à Abdelaziz Bouteflika, l'ancien fidèle ministre de Boumédiène qui avait quitté la scène politique depuis 1981, et qui est alors soutenu par le FLN. Le 15 avril, Abdelaziz Bouteflika remporte l'élection présidentielle avec 73,8 % des suffrages, mais l'opposition dénonce des fraudes massives.
Le parti reçoit, lors des élections législatives de 2002, 34,3 % des voix parlementaires et obtient 199 membres au parlement. En 2004, Ali Benflis se présente à l'élection présidentielle, comme candidat du FLN, et n'obtient que 6,4 % des voix, contre 85,0 % pour Abdelaziz Bouteflika, mais dans un scrutin fort contesté. La victoire d'Abdelaziz Bouteflika entraîne ainsi une reprise en main du parti. En 2005, le huitième congrès national du FLN nomme Abdelaziz Bouteflika président du parti et Abdelaziz Belkhadem secrétaire général du FLN. Ce dernier est ensuite nommé Premier ministre, le 24 mai 2006, par le président Abdelaziz Bouteflika, succédant à Ahmed Ouyahia du RND. Le FLN fait aujourd'hui partie de la coalition parlementaire appelée « alliance présidentielle » avec le Rassemblement national démocratique (RND), et le Mouvement de la société pour la paix (MSP). Il en forme l'aile nationaliste.

Lors des élections législatives de 2012, le FLN remporte 220 des 462 sièges et gagne le scrutin.

L'idéologie du FLN est principalement nationaliste, comprise comme un mouvement à l'intérieur d'un nationalisme arabe plus large. Le parti tire sa politique d'auto-légitimation de trois sources : le nationalisme et la guerre révolutionnaire contre la France (idéologie anticoloniale) ; le socialisme, vaguement interprété comme un credo anti-exploitation populaire ; l'islam, défini comme le fondement principal de la conscience nationale et facteur déterminant dans la consolidation de l'identité algérienne comme distincte de celle des Algériens français.

Sa vision nationaliste est également étroitement liée à la solidarité pan-arabe. Cet aspect a conduit à la négation ou au refus de traiter avec l'identité distincte berbère détenue par les berbérophones algériens qui représentaient environ 50 % de l'Algérie, ce qui provoqua une vive opposition et a conduit à l'éclatement du mouvement immédiatement après l'indépendance, alors que Hocine Aït Ahmed mettait en place le berbériste Front des forces socialistes (FFS) réclamant davantage de pluralisme politique.

L'organisation se dit engagée vis-à-vis du socialisme, mais le comprend dans les lignes du socialisme arabe et est opposée au marxisme doctrinaire. L'existence de différentes classes dans la société algérienne a été généralement rejetée, même si plusieurs des meilleurs idéologues du parti ont été influencés à des degrés divers par l'analyse marxiste. La terminologie marxiste a généralement été réinterprétée par les radicaux du parti en termes de conflit avec la France, par exemple en plaçant le colonisateur dans le rôle de l'exploiteur-oppresseur économique aussi bien que dans celui de l'ennemi national, alors que l'étiquette de « bourgeoisie » était appliquée aux élites non coopératives ou pro-françaises. Le FLN a, pour des raisons pragmatiques, absorbé les militants communistes dans ses rangs au cours de la guerre d'indépendance, mais a refusé de leur permettre de s'organiser séparément après guerre et a rapidement dissous le Parti communiste algérien (PCA). Alors que le FLN mettait rapidement en place un système de parti unique dans les premiers temps de l'Algérie indépendante, nombre d'intellectuels communistes étaient cooptées dans le régime à divers niveaux, notamment au cours des premières années Ben Bella et plus tard Boumédiène.

L'islam algérien, en particulier la variété réformiste nationaliste défendue par Abdelhamid Ben Badis et son groupe des oulémas musulmans algériens, a été une influence idéologique fortement présente pour le FLN. Le mouvement rejetait absolument l'athéisme et n'était pas ouvertement laïque, contrairement à la perception largement répandue à l'ouest. Ainsi, pendant la guerre d'indépendance, l'islam fut peut-être l'idéologie mobilisatrice la plus importante du parti. Pourtant, après l'indépendance, le parti, dans la pratique, admettra une interprétation moderniste de l'islam, soutenant la transformation sociale de la société algérienne et fonctionnera par le biais d'institutions laïques. La religion a donc été reléguée au rôle de facteur de légitimation pour le parti-régime. Cela a été particulièrement le cas sous la présidence du colonel Houari Boumédiène (1965 – 78), mais même alors, l'islam était considérée comme la religion d'État et comme une partie cruciale de l'identité algérienne, et Boumédiène était lui-même fier de sa formation coranique. Son prédécesseur Ahmed Ben Bella (1962 – 65) était plus attaché à la composante islamique du régime, bien que toujours considérée davantage comme un nationalisme arabe qu'un activisme islamique. Le successeur de Boumédiène, le colonel Chadli Bendjedid (1979 – 1992) voulut atténuer l'aspect socialiste du mouvement, et au cours de la seconde moitié des années 1980, il réintroduit une législation religieuse conservatrice dans une tentative d'apaiser l'opposition croissante des islamistes. Pendant et après la guerre civile algérienne, la position du parti est restée identique : proclamant l'islam algérien comme une influence principale, tout en faisant valoir simultanément que cette influence doit être exprimée comme une foi progressiste et moderne, même si le parti reste globalement en ligne avec les mœurs sociales conservatrices de la population algérienne. Il a fermement condamné les enseignements religieux radicaux fondamentalistes du Front islamique du salut (FIS) et d'autres groupes islamistes, tout en soutenant l'inclusion de partis islamistes non violents dans le système politique et en collaborant avec eux.

Au cours de toutes les périodes de l'histoire post-coloniale algérienne, sauf pour les années 1990 – 1996, le FLN a été un pilier du système politique, et a d'abord été considéré comme un parti « pro-système ». Son rôle en tant que libérateur de l'Algérie est resté la pierre angulaire absolue de l'auto-perception du parti et la caractéristique déterminante de son idéologie pour le reste quelque peu fluide. Gérard Chaliand parle même de « vide politique » sitôt la guerre d'indépendance achevée. Aujourd'hui, le FLN est proche du président Abdelaziz Bouteflika, qui a été nommé président d'honneur. Il mêle ses interprétations populistes traditionnelles du patrimoine nationaliste-révolutionnaire et islamique de l'Algérie avec un conservatisme pro-système, et le soutien aux réformes pro-marché progressives. Depuis l'effondrement du système du parti unique et son détachement de la structure de l'État dans les années 1988 – 1990, le FLN a été en faveur de la démocratie multipartite, alors qu'auparavant, il se présenta comme la seule organisation représentant le peuple algérien.

Mohammed Harbi est l'un des premiers historiens à décrire le fonctionnement du FLN de l'intérieur dans son livre "Aux origines du FLN. Le populisme révolutionnaire en Algérie" (1975). Il y dévoile notamment le fossé entre les idéaux de certains de ses membres et les méthodes adoptées par le parti nationaliste : .

Pour Gilbert Meynier, la révolution dont se targue le parti demeure celle de l'idéologie anti-colonialiste qui forme sa justification. Pour le reste, les dirigeants 









</doc>
<doc id="6603" url="https://fr.wikipedia.org/wiki?curid=6603" title="Raoul Salan">
Raoul Salan

Raoul Salan, né le à Roquecourbe (Tarn) et mort le à Paris, est un général français, et le militaire le plus décoré de France. Son état de service porte de 1917 à 1959 où il prend sa retraite. Il participe au putsch des généraux en 1961. Il est également le chef de l'Organisation armée secrète (OAS) qui lutte pour le maintien du "statu quo" de l'Algérie française. Il est condamné à la prison à perpétuité, puis amnistié en 1968 et réintégré dans le corps des officiers.

Il passe à Roquecourbe une enfance tranquille et heureuse, une enfance en pleine nature. Son père ayant accepté un poste de fonctionnaire à Nîmes comme chef de poste des contributions indirectes. Il est élève du lycée de Nîmes et obtiendra une bourse nationale. Il va entrer à Saint-Cyr avec une dispense d'âge en 1917. 

Son frère, , ancien résistant et militant communiste, médecin de profession, vit son cabinet nîmois plastiqué par un commando de l'OAS, le .

Il a épousé Lucienne Bouguin en mars 1939. Le couple a eu deux enfants, Hugues (né le 30 juin 1943 à Dakar (Sénégal), décédé le juin 1944 à Alger) et Dominique (née le à Hanoï). Cette dernière a été rapatriée en France en septembre 1946, sur le navire « "Maréchal Joffre" ». Raoul Salan est également le père de Victor, né le 23 mars 1932 à Muong-Sing (Laos).

Selon sa fille, les modèles du général Salan sont le général Charles Mangin et le maréchal Joseph Gallieni.

Il s’engage pour la durée de la guerre le 2 août 1917, est admis à École spéciale militaire de Saint-Cyr le 21 août 1917 dans la promotion La Fayette. Il en sort aspirant le 25 juillet 1918, est affecté au régiment d’infanterie coloniale (RIC) à Lyon le 14 août 1918.

Chef de section à la , il participe aux combats dans la région de Verdun (Saint-Mihiel, Les Éparges, Fort de Bois-Bourru, Côte de l’Oie, Cumières-le-Mort-Homme). Il est cité à l’ordre de la brigade par l’ordre en date du 29 décembre 1918.

Il est affecté à l’armée d’occupation en Allemagne jusqu’en mai 1919, puis il retourne à l’École spéciale militaire de Saint-Cyr le 7 mai 1919. Il est nommé sous-lieutenant à titre définitif le 21 septembre 1919 et affecté au régiment d’infanterie coloniale du Maroc (actuel régiment d'infanterie-chars de marine), à Landau, en Allemagne, le 3 décembre 1919.

Sur sa demande, il est envoyé au Levant au régiment de tirailleurs sénégalais (RTS), en tant que chef de poste à Radjou, en Syrie sur la frontière avec la Turquie. Il est promu lieutenant le 11 septembre 1921, grièvement blessé au combat d’Accham le 24 octobre 1921.

Il est à nouveau cité, à l’ordre de l’armée et nommé chevalier de la Légion d'honneur, il est décoré sur son lit d’hôpital, à Alep, par le général Gouraud, haut-commissaire au Levant.

Il fait l'objet d'un rapatriement sanitaire le 25 janvier 1922, est soigné à l’hôpital Sainte-Anne à Toulon, puis au Val-de-Grâce à Paris, est affecté pendant sa convalescence au RIC à Paris et désigné sur sa demande pour l’Indochine le 2 janvier 1924.

Il est alors affecté au régiment de tirailleurs tonkinois comme adjoint au chef de poste de Nguyen-Binh (Tonkin) qu’il rejoint le 15 avril 1924. Détaché hors-cadre le 14 décembre 1924, il est délégué administratif du Commissaire du gouvernement chef de la province du Haut-Mékong, à Muong Sing, aux confins de la Chine, de la Birmanie et du Siam, du 15 avril 1925 au 26 mai 1928. Après un retour en métropole du 6 juillet 1928 au 2 août 1929, il assure, en position hors-cadre, l’intérim du Commissaire du Gouvernement, Lapeyronie, pour la province du Haut-Mékong, à Houei Sai. Il est promu capitaine le 25 mars 1930 et retourne à Muong-Sing en mars 1931, rédige un « Manuel de lecture de la langue « Lu » et « Youne » avec traduction correspondante en langue laotienne ». Il quitte l’Indochine pour la métropole le 28 avril 1933.

Il prend le commandement de la Compagnie d'essais techniques le et participe avec cette unité à des manœuvres au Larzac au printemps 1934, puis est renvoyé en Indochine le , où il prend le commandement comme capitaine de la du régiment mixte d’infanterie coloniale tout en assumant les fonctions de délégué administratif de Dinh-Lap au Tonkin.

Avec son fils Victor, âgé de cinq ans, il revient le 8 avril 1937 en métropole, à bord du "Chenonceaux" ; il y fait connaissance de sa future épouse, Lucienne Bouguin. Il est détaché au ministère des Colonies le 1937, comme adjoint au chef du (renseignement), est promu au grade de commandant le 22 mars 1938, devient chef du Service de renseignement intercolonial et est en relation quotidienne avec Georges Mandel, ministre des Colonies à partir d’avril 1938.

Il mène à l’automne 1939, après la déclaration de guerre de la France à l’Allemagne, une mission secrète au Caire et à Khartoum d’aide à la résistance abyssine contre l’occupation de l’Éthiopie par les troupes italiennes.

Il revient à Paris le 19 novembre 1939. En il prend la tête d’un bataillon du régiment d’infanterie coloniale mixte sénégalais.

Le 5 juin 1940, Salan est avec son bataillon en première ligne sur la Somme lorsque les Allemands déclenchent leur offensive après l’encerclement et la défaite des forces françaises et britanniques dans la poche de Dunkerque. Il se replie sur ordre avec les restes de son bataillon en menant des actions retardatrices sur la Seine puis sur la Loire.

Il est cité deux fois à l’ordre du régiment par ordres des 12 et 13 juillet 1940, puis à l’ordre de l’armée et promu officier de la Légion d'honneur le 21 août 1940.

Il est ensuite détaché à l’état-major général des Colonies, au Secrétariat d’État aux Colonies à Vichy, le 16 juillet 1940. Il est promu lieutenant-colonel le 25 juin 1941.

Il est désigné pour servir en Afrique-Occidentale française (AOF) le 24 septembre 1941. Il rejoint Dakar le 8 mars 1942 après avoir fait connaissance de la ville d’Alger ; il y est affecté comme chef du (renseignements) à l’état-major du général Barrau, commandant supérieur en AOF. En sa compagnie, il effectue une tournée du Sénégal, du Soudan français et de la Guinée. Il rédige avec son équipe et celle du une « Instruction sur la conduite de la guerre sur les arrières de l’ennemi » diffusée jusqu’à l’échelon de la compagnie. Il est promu colonel le 25 juin 1943.

Il est désigné pour continuer ses services en Afrique du Nord et arrive à Alger le 31 août 1943 où il est affecté au de l’état-major de l’armée de terre, chargé de l’action psychologique et de la direction du journal "Combattant 43" dont l’un des collaborateurs est le peintre André Hambourg. Évincé de son poste par André Le Troquer, commissaire à la Guerre et à l’Air, pour avoir refusé de publier le compte rendu d’une conférence de celui-ci mettant en cause l’honneur des cadres de l’armée de 1939-1940, il est mis à la disposition de la division d’infanterie coloniale (DIC) sous les ordres du général Magnan le 4 mai 1944.

Il prend le commandement du régiment de tirailleurs sénégalais (RTS), en Corse, le 30 mai 1944. Il rencontre pour la première fois à Bastia, le 16 juin 1944, le général de Lattre de Tassigny qui a demandé à voir le RTS et son colonel.

Il participe au débarquement de Provence à la tête de son régiment avec lequel il débarque le 19 août 1944 au matin sur la plage de La Nartelle dans le Var. Il atteint Toulon le 26 août 1944, après six jours de combats intenses sur l’axe Solliès-Pont, La Farlède, La Valette-du-Var et Toulon. Le RTS déplore 587 tués, blessés et disparus. Une citation à l’ordre de l’armée rend hommage à ces actions.

Il quitte Toulon le 9 septembre avec son régiment reconstitué par incorporation d'éléments des Forces françaises de l'intérieur (FFI) qui « blanchissent » progressivement le régiment. Par note du 13 octobre 1944, le RTS devient le régiment d’infanterie coloniale (RIC).

Le 14 novembre 1944, le RIC démantèle la résistance allemande dans la poche du Doubs. Le 23 novembre, le régiment est à Blotzheim, dans le sud du Haut-Rhin, alors que les Allemands tiennent de solides têtes de pont sur la rive française du Rhin ; par la suite, il libère Village-Neuf, Huningue, Loechle et l’usine hydro-électrique de Kembs.

Il est appelé au commandement de l’infanterie de la DIC. Raoul Salan est promu général de brigade le 25 décembre 1944. Il a 45 ans. Il participe à la réduction de la poche de Colmar à la fin de et au début de . Il est cité à l’ordre de l’Armée et promu commandeur de la Légion d'honneur.

Le 20 février 1945, il prend le commandement de la division d’infanterie, l’ancienne division du général de Lattre reconstituée à partir d’unités issues des FFI et de FTP (Francs tireurs et partisans), dont la brigade Alsace-Lorraine aux ordres d’André Malraux. Il termine la guerre sur le front européen près de Donaueschingen dans la Forêt-Noire.

Il est cité deux fois à l’ordre de l’Armée, les 29 avril et 2 décembre 1945 pour son action à la tête du RIC et à la tête de l’infanterie de la DIC.

Octobre 1945 : retour en Indochine en tant que Commandant des forces françaises de Chine et d’Indochine du Nord.

Janvier 1946 : participe aux négociations concernant le départ des troupes chinoises, du Tonkin.

Février 1946 : fait la connaissance d'Hô Chi Minh.

Avril-mai 1946 : participe aux négociations avec Hô Chi Minh à Đà Lạt.

Juillet-septembre 1946 : accompagne Hô Chi Minh aux négociations de Fontainebleau.

Mai 1947 : commande les troupes françaises dans le Nord du Viêt-Nam.

Février-avril 1948 : assure l'intérim du général Valluy remplacé par le général Blaizot comme commandant en chef en Indochine.

6 décembre 1950 - 5 janvier 1952 : adjoint militaire du général de Lattre de Tassigny, haut commissaire en Indochine.

11 janvier 1952 : mort du général de Lattre.

6 janvier 1952 - 8 mai 1953 : commandant en chef en Indochine.

Juin - octobre 1954 : adjoint militaire du général Ely, haut commissaire en Indochine.

20 septembre : en désaccord avec le général Ely il demande et obtient son rappel en France.

9 octobre : il quitte l'Indochine. Il est remplacé par le général Pierre-Élie Jacquot.

Après un intermède parisien de 1954 à 1955, le général Salan est nommé, le , commandant supérieur Interarmées de l'Algérie (militaire) en remplacement du général Henri Lorillot. Il prend ses fonctions à Alger le .

Le , un attentat au bazooka est commis contre Raoul Salan par l'ORAF, il coûte la vie au commandant Rodier. Les auteurs de l'attentat étaient les contre-terroristes Philippe Castille et Michel Fechoz. Le commanditaire, René Kovacs, un médecin algérois militant pour l'Algérie française, voulait remplacer Salan par le général René Cogny, le premier étant perçu comme « le bradeur de l'Indochine » — et donc de l'Algérie — au même titre que Pierre Mendès France.

Castille mit en cause des personnalités de premier plan, les sénateurs députés gaullistes Michel Debré et Jacques Soustelle ainsi que le député Pascal Arrighi (RRRS), mais sans apporter de preuves. L'enquête n'aboutit pas.

Le 13 mai 1958, après la mise à sac du bâtiment de la Délégation générale en Algérie par des manifestants, il donne son accord au général Massu pour que celui-ci entre dans le Comité de salut public alors formé à Alger. Dans la soirée, le président du conseil démissionnaire, Félix Gaillard, lui délègue les pouvoirs civils et militaires en Algérie. Dans la nuit, Pierre Pflimlin, qui vient d'être investi président du Conseil par l'Assemblée nationale, confirme cette délégation.

Le 15 mai, Raoul Salan termine, devant une foule rassemblée à Alger, un discours par « Vive la France ! Vive l’Algérie française ! » puis, poussé par le gaulliste Léon Delbecque, il ajoute « Vive de Gaulle ! ». Cette intervention contribue au retour du général de Gaulle, qui est nommé président du conseil le 29 mai et investi par l'Assemblée nationale le juin.

Le 6 juin, Raoul Salan reçoit du général de Gaulle la charge et les attributions de délégué général du gouvernement en Algérie cumulées avec celles de commandant en chef des forces en Algérie.

Le , Paul Delouvrier est nommé délégué général, et le lendemain, le général Maurice Challe succède au général Salan comme commandant en chef ayant reçu délégation de pouvoirs du gouvernement. De Gaulle, voulant réduire le pouvoir de l'armée coloniale qui a été pétainiste et a presque tous les pouvoirs, nomme Salan comme inspecteur général de la Défense nationale, poste honorifique puis gouverneur militaire de Paris le 5 février 1959.

Le général Salan quitte le service actif le 10 juin 1960.

Il s’installe à Alger avec sa famille le 30 juillet 1960. 

Le 11 septembre 1960, de passage à Paris, il se voit interdire de retourner à Alger. 

Le 26 octobre 1960, il donne une conférence de presse à l'hôtel de la gare d’Orsay pour réaffirmer son attachement à l’Algérie française.

À la fin du mois d’octobre 1960, menacé d’arrestation, il s'exile volontairement en Espagne.

Il est intéressant de noter la progression de la relation entre les 2 hommes entre le 24 octobre 1958 et le 12 décembre 1958, date à laquelle Salan se voit attribuer les fonctions d’inspecteur général de la Défense et la fin de la mission de délégué général et commandant en chef des Forces en Algérie.
Passant de rapports de « "sincère amitié" » à des « "sentiments cordiaux" » avec sa hiérarchie, en moins de 3 mois le général Salan passe d'un poste hautement opérationnel à un poste purement honorifique.

Partisan de l'Algérie française, Salan dirige l'OAS après l'échec du putsch des généraux en 1961. Il est arrêté à Alger le vendredi après un an de clandestinité et le , après avoir revendiqué ses responsabilités à la tête de l’OAS, est condamné par le Haut Tribunal militaire à la peine de détention criminelle à vie, verdict que le chef de l’État — souhaitant que Salan soit fusillé — considérait trop clément, ce qui entraînera la dissolution du tribunal par le général de Gaulle le , alors que le général Jouhaud avait été condamné à mort par le même tribunal le 13 avril précédent. Le , il est transféré en même temps que le général Jouhaud à la prison de Tulle où sont incarcérés les officiers généraux et supérieurs impliqués dans les combats pour l’Algérie française.

Le , dernier occupant de la prison de Tulle, il est libéré par grâce présidentielle à la suite des événements de mai 1968.

Le , durant le putsch d'Alger, le général Salan adresse un communiqué radio visant à mobiliser huit classes d’Algériens et de reconstituer les Unités territoriales (UT) dissoutes après la « Semaine des barricades » de janvier 1960.

Le , le général Salan, qui est entre temps devenu chef de l’O.A.S., envoie une lettre aux parlementaires reformulant sa demande d'avril.

Conscient que la partie sur le terrain était jouée, refusant de fuir au Portugal comme on le lui conseillait, Salan dira que son départ d’Algérie aurait porté aux Européens d'Algérie un coup dont ils ne se seraient plus relevés. Il lui restait une dernière carte à jouer, afin de renverser l'équilibre des forces, une alliance avec le rival et ennemi du FLN, c'est-à-dire le Mouvement national algérien (MNA) dirigé par Messali Hadj.

L'OAS veut maintenir l'autorité des Français. Les messalistes réclament l'indépendance sous certaines conditions dictées par eux, mais admettent la possibilité pour les Européens de rester sur le territoire et de participer au développement de l'économie algérienne. Ce qui est important, c’est que les deux fronts craignent le FLN pour son intransigeance.

Messali Hadj refuse tout contact avec le parti qu'il appelle « Organisation fasciste ». Alors, Salan découragé adresse une lettre à un groupe de messalistes dissidents, le FAAD (Front algérien d’action démocratique).

Le vendredi 20 avril Salan descend de son appartement situé au cinquième étage et se rend à son bureau qui se trouve au rez-de-chaussée du même immeuble, c’est-à-dire au 25 rue Desfontaines où il avait rendez-vous avec Jacques Achard, "alias" Alpha, chef de l’OAS du secteur Orléans-Marine, lui-même chargé de rencontrer le FAAD.

Une Peugeot noire remonte le boulevard Saint-Saëns, tourne dans la rue Desfontaines et s’arrête. Les gardes du corps de Salan attendent dans une 403 grise dans cette même rue, voient un véhicule dans le rétroviseur, mais pensent qu’il s’agit du commando Delta. Le quartier est encerclé, Jean-Marie Lavanceau (agent infiltré) frappe à la porte du bureau. Salan, Jean Ferrandi et une troisième personne sont à l’intérieur. Lavanceau demande où se trouvent les toilettes, et au même moment quelqu’un sonne. Ferrandi observe par le judas, et crie « Nous sommes faits ». Salan était pris au piège, et avant que ses gardes du corps postés devant l’immeuble ne puissent réagir, les policiers prennent rapidement position en sortant des véhicules blindés. Le chef de l’OAS est bel et bien tombé dans un piège.

Une heure plus tard, Alger apprend par un communiqué de la délégation générale, que Salan a été arrêté lors d'une banale et routinière recherche d’émetteur clandestin.

Nous savons maintenant que les services secrets ont, pendant plus d'un an, préparé prudemment des travaux d'approche et infiltré à l'échelon le plus haut de l'OAS, des agents comme Lavanceau (treize tentatives d'arrestation avaient été infructueuses auparavant).

Les Algérois refusèrent de croire à cette nouvelle, peu à peu les magasins de la ville se ferment, Radio-pirate OAS confirmera peu de temps après la nouvelle en ces termes : 

Le procès de Salan s'ouvre le 15 mai 1962. Le 23 mai, il est condamné à la détention à perpétuité.

Entre 1970 et 1974, il publie ses Mémoires couvrant la période 1918-1960, sous le titre "Fin d’un Empire". En 1975, il publie "Indochine Rouge", le message d’Hô Chi Minh. 

À la suite de l’amnistie votée par le Parlement en 1982, il est réintégré dans ses prérogatives de général d’armée et de grand-croix de la Légion d'honneur.

Malade à partir de mai 1984, il meurt le 3 juillet 1984 à l’hôpital d'instruction des armées du Val-de-Grâce. Il repose au cimetière de Vichy. L'inscription sur sa tombe porte seulement, en plus de son prénom, de son nom et des années de naissance et de mort, la mention : « Soldat de la Grande Guerre ».

- Chevalier le 5 avril 1922
- Officier le 21 août 1940
- Commandeur le 10 février 1945
- Grand officier le 27 octobre 1948
- Grand croix le 28 août 1952

Le , la ville de Toulon, dirigée par Jean-Marie Le Chevallier, maire issu du Front national, baptise un carrefour Général Raoul Salan - Libérateur de Toulon - le 26 août 1944. En 2005, la municipalité dirigée par Hubert Falco, issu de l'Union pour un mouvement populaire, renomme le carrefour en Colonel Salan - Libération de Toulon - août 1944.

Il existe une avenue du Général-Salan à Marignane (Bouches-du-Rhône), une rue du Général-Salan à Blotzheim (Haut-Rhin), une place Général-Salan à Saint-Seurin-sur-l'Isle (Gironde) et une rue Colonel-Salan à Solliès-Ville (Var).

Raoul Salan est nommé caporal-chef d'honneur de la Légion étrangère.






</doc>
<doc id="6619" url="https://fr.wikipedia.org/wiki?curid=6619" title="Alcyne">
Alcyne

Les alcynes sont des hydrocarbures possédant une insaturation caractérisée par la présence d’une triple liaison carbone-carbone. Les deux carbones sont hybridés sp. La triple liaison est la combinaison de deux liaisons π et d’une liaison σ. La densité électronique de la molécule est répartie de façon cylindrique le long de la liaison C-C.

On dit que l’on a affaire à un alcyne vrai ou à un alcyne terminal si R ou R' est un hydrogène.

Les alcynes, ou hydrocarbures acétyléniques, ont pour formule générale CH, et sont caractérisés par la présence dans leur molécule d’une triple liaison et d’une coexistence possible de doubles liaisons.

Les alcynes se nomment comme les alcènes correspondants, en remplaçant la terminaison « -ène » par la terminaison « -yne ».
Hors nomenclature IUPAC, le premier terme, CH, est également appelé acétylène.
Les alcynes linéaires ont pour formule générale CH.

Exemples :

Les alcynes sont caractérisées par des spectres infrarouges (bande d’absorption ).
L’acétylène, le propyne et le butyne-1 sont des gaz à la température ordinaire ; les autres termes sont liquides puis solides, à mesure que leur poids moléculaire augmente, pour être solides à partir de C.

Ils sont présents, en faible quantité, dans les pétroles et les gaz naturels.

Il convient de distinguer celles de la triple liaison, qui sont communes à tous les alcynes, et les propriétés de l’hydrogène contigu à la triple liaison, qui sont particulières aux alcynes « vrais ».
La triple liaison présente le même type de réactivité que la double liaison, mais avec une intensité parfois différente. Comme elle, elle se prête à des réactions d’addition et de polymérisation et elle constitue un point de moindre résistance de la chaîne.
Les alcynes sont insolubles dans l’eau mais solubles dans l’alcool, l’éther, les solvants organiques apolaires.

Outre des réactions d’hydratation (acétaldéhyde), les alcynes sont facilement hydrogénés en alcanes ou en alcènes. Le choix du catalyseur permet de différencier les isomères cis ou trans à produire. C’est ainsi que l’on peut obtenir du propane à partir du propyne par une réaction d’hydrogénation avec comme catalyseur le platine ou le nickel.

L’acétylène est peu soluble dans l’eau, mais beaucoup plus dans l’acétone.
Sa formation à partir de ses éléments est endothermique, elle absorbe .

L’acétylène est instable, pour cette raison, et tend à se décomposer de façon parfois violente, surtout s’il est comprimé ou liquéfié. Cette particularité a longtemps gêné l’utilisation de l’acétylène dans diverses applications qu’il peut recevoir. On peut cependant le transporter sans danger sous forme de solution dans l’acétone, et l’utiliser sous une légère pression dans les synthèses industrielles à condition de le diluer avec un gaz inerte (azote) et de chasser totalement l’air des installations.



</doc>
<doc id="6621" url="https://fr.wikipedia.org/wiki?curid=6621" title="Prix Nobel de physique">
Prix Nobel de physique

Le prix Nobel de physique est une récompense attribuée par la Fondation Nobel, selon les dernières volontés du testament du chimiste Alfred Nobel. Il distingue des figures scientifiques éminentes ayant rendu de grands services à l'humanité par une œuvre et des travaux considérés comme une exceptionnelle contribution en physique. Le prix est décerné chaque année en octobre par les membres de l'Académie royale des sciences de Suède. Après la révélation du nom du lauréat en début octobre, la médaille et le diplôme de la fondation sont officiellement remis par le roi de Suède, le 10 décembre, jour anniversaire de la mort du fondateur du prix.

Pendant une dizaine d'années, il a été doté d'un montant de dix millions de couronnes suédoises, un peu plus d'un million d'euros. Cependant en juin 2012, la Fondation Nobel a décidé que les lauréats des prix recevraient une gratification inférieure de 20 % à celle versée à leurs prédécesseurs. La raison de cette révision à la baisse compte tenu de la crise financière. Le montant de la dotation s'élève à huit millions de couronnes suédoises (soit environ en 2012).

Selon le testament d'Alfred Nobel, le prix doit récompenser . Nobel voulait qu'il soit attribué par l'Académie royale des sciences de Suède, comme pour le prix en chimie. L'Académie délègue l'étude des candidatures au Comité Nobel, qui dépend de la Fondation Nobel, déterminant la spécificité de chacune des branches à récompenser. Les membres du comité sont au nombre de cinq. Ils sont désignés par cooptation parmi les membres de l'Académie royale pour une période de trois ans. Ceux-ci s'appuient sur différentes figures d'autorité pour établir leurs nominations : physiciens reconnus, cercles d'éminents professeurs en université, associations de chercheurs, anciens lauréats du prix, dirigeants de grands centres nationaux ou internationaux de recherches scientifiques… Des courriers du comité sont envoyés à l'automne afin d'être retournés pour le choix du lauréat de l'année suivante. Il est interdit à chacune des personnes sollicitées de voter pour elle-même. Plusieurs centaines de propositions annuelles, obligatoirement argumentées et détaillées, sont ainsi soumises au comité qui en étudie la fiabilité, la légitimité et la crédibilité. Le comité ne conserve qu'une cinquantaine de candidatures soumises dès le printemps aux autres académiciens qui doivent souscrire à quelques recommandations. La liste finale, statuée par le comité Nobel, comprend cinq noms ou groupes de noms associés à une recherche précise. L'élection du ou des lauréats se fait en octobre au scrutin majoritaire. Tous les membres de l'Académie royale participent au vote. L'identité du ou des récipiendaires est révélée lors d'une conférence de presse officielle. Les nominations et le cadre des délibérations sont normalement tenus au secret pour cinquante années avant que les archives de la Fondation Nobel soient ouvertes. Les jurés sont obligés de respecter l'instruction du testament d'Alfred Nobel : , pour attribuer le prix. Entre la découverte du scientifique et sa désignation en tant que récipiendaire peut s'écouler une période très longue, jusqu'à plusieurs dizaines d'années. Ainsi, Subrahmanyan Chandrasekhar fut honoré en 1983 pour ses recherches sur la structure et l’évolution des étoiles qui dataient des années 1930. Des savants dont l'œuvre a été primordiale dans l'amélioration des connaissances en physique n'ont pas été récompensés, le prix ne pouvant être attribué de manière posthume.

Ce tableau présente la liste des 198 lauréats du prix Nobel de physique depuis sa création en 1901 jusqu'en 2017. Les citations présentées sont directement traduites du site de la Fondation Nobel, avec l'autorisation de l'Académie des Sciences de Suède, mais sous la seule responsabilité du traducteur. Les fractions de prix attribuées aux bénéficiaires sont décidées au coup par coup, en même temps que l'attribution des prix. C'est pourquoi elles sont indiquées sur le tableau.

Le prix n'a pas été décerné à six reprises (1916, 1931, 1934 et 1940, 1941, 1942). 

Abrikosov (2003) - Akasaki (2014) - Alferov (2000) - Alfvén (1970) - Alvarez (1968) - Amano (2014) - Anderson (C.D.) (1936) - Anderson (P.W.) (1977) - Appleton (1947)

Bardeen (1956 et 1972) - Barish (2017) - Barkla (1917) - Basov (1964) - Becquerel (1903) - Bednorz (1987) - Bethe (1967) - Binnig (1986) - Blackett (1948) - Bloch (1952) - Bloembergen (1981) - Bohr (A.) (1975) - Bohr (N.) (1922) - Born (1954) - Bothe (1954) - Bragg (W.H.) (1915) - Bragg (W.L.) (1915) - Brattain (1956) - Braun (1909) - Bridgman (1946) - Brockhouse (1994) - de Broglie (1929)

Chadwick (1935) - Chamberlain (1959) - Chandrasekhar (1983) - Charpak (1992) - Chu (1997) - Cockcroft (1951) - Cohen-Tannoudji (1997) - Compton (1927) - Cooper (1972) - Cornell (2001) - Cronin (1980) - Curie (M.) (1903) - Curie (P.) (1903)

Dalén (1912) - Davis (2002) - Davisson (1937) - Dehmelt (1989) - Dirac (1933)

Einstein (1921) - Englert (2013) - Esaki (1973)

Fermi (1938) - Fert (2007) - Feynman (1965) - Fitch (1980) - Fowler (1983) - Franck (1925) - Frank (1958) - Friedman (1990)

Gabor (1971) - Gell-Mann (1969) - de Gennes (1991) - Giacconi (2002) - Giaever (1973) - Ginzburg (2003) - Glaser (1960) - Glashow (1979) - Glauber (2005) - Goeppert-Mayer (1963) - Gross (2004) - Grünberg (2007) - Guillaume (1920)

Haldane (2016) - Hall (2005) - Hänsch (2005) - Haroche (2012) – Heisenberg (1932) - Hertz (1925) - Hess (1936) - Hewish (1974) - Higgs (2013) - Hofstadter (1961) - 't Hooft (1999) - Hulse (1993)

Jensen (1963) - Josephson (1973)

Kajita (2015) - Kamerlingh Onnes (1913) - Kapitsa (1978) - Kastler (1966) - Kendall (1990) - Ketterle (2001) - Kilby (2000) - von Klitzing (1985) - Kobayashi (2008) - Koshiba (2002) - Kosterlitz (2016) - Kroemer (2000) - Kusch (1955)

Lamb (1955) - Landau (1962) - von Laue (1914) - Laughlin (1998) - Lawrence (1939) - Lederman (1988) - Lee (D.M.) (1996) - Lee (T.D.) (1957) - Leggett (2003) - Lenard (1905) - Lippmann (1908) - Lorentz (1902)

Marconi (1909) - Maskawa (2008) - Mather (2006) - McDonald (2015) - van der Meer (1984) - Michelson (1907) - Millikan (1923) - Mössbauer (1961) - Mott (1977) - Mottelson (1975) - Müller (1987)

Nakamura (2014) - Nambu (2008) - Néel (1970)

Osheroff (1996)

Paul (1989) - Pauli (1945) - Penzias (1978) - Perl (1995) - Perrin (1926) - Phillips (1997) - Planck (1918) - Politzer (2004) - Powell (1950) - Prokhorov (1964) - Purcell (1952)

Rabi (1944) - Rainwater (1975) - Raman (1930) - Ramsey (1989) - Rayleigh (1904) - Reines (1995) - Richardson (O.W.) (1928) - Richardson (R.C.) (1996) - Richter (1976) - Rohrer (1986) - Röntgen (1901) - Rubbia (1984) - Ruska (1986) - Ryle (1974)

Salam (1979) - Schawlow (1981) - Schrieffer (1972) - Schrödinger (1933) - Schwartz (1988) - Schwinger (1965) - Segrè (1959) - Shockley (1956) - Shull (1994) - Siegbahn (K.) (1981) - Siegbahn (K.M.B.) (1924) - Smoot (2006) - Stark (1919) - Steinberger (1988) - Stern (1943) - Störmer (1998)

Tamm (1958) - Taylor (J.H.) (1993) - Taylor (R.E.) (1990) - Tcherenkov (1958) - Thomson (G.P.) (1937) - Thomson (J.J.) (1906) - Thorne (2017) - Thouless (2016) - Ting (1976) - Tomonaga (1965) - Townes (1964) - Tsui (1998)

Veltman (1999) - van Vleck (1977)

van der Waals (1910) - Walton (1951) - Weinberg (1979) - Weiss (2017) - Wieman (2001) - Wien (1911) - Wigner (1963) - Wilczek (2004) - Wilson (C.T.R.) (1927) - Wilson (K.G.) (1982) - Wilson (R.W.) (1978) – Wineland (2012)

Yang (1957) - Yukawa (1949)

Zeeman (1902) - Zernike (1953)





</doc>
<doc id="6622" url="https://fr.wikipedia.org/wiki?curid=6622" title="Mémétique">
Mémétique

La mémétique utilise le concept, dû à Richard Dawkins, de mème (élément de comportement transmis par imitation) pour étudier les évolutions de la culture dans une approche darwinienne étendue. Un des exemples les plus connus est le parallèle fait entre l'évolution du vivant et celle des langues. Ainsi, si la génétique se base sur le concept de gène pour étudier le vivant, la mémétique se base sur le concept de mème pour étudier la culture. On y étudie le comportement des codes et schémas informationnels appelés « mèmes », dans leurs milieux physico-chimiques et socio-culturels : l’homme, l’animal, la machine (tout support mémoriel).

Le mème peut se définir comme "un élément d'une culture pouvant être considéré comme transmis par des moyens non génétiques, en particulier par l'imitation".

Elle associe les sciences de : 

Elle étudie l'évolution des phénomènes culturels dans le temps et l'espace physico-social, dans une approche culturo-centrée. En s'intéressant autant à l'aspect neuronal que mental, symbolique que pratique, elle se positionne naturellement au sein des sciences humaines et sociales, même si elle n'est pas centrée sur l'humain.

Elle s'inscrit dans le mouvement du darwinisme étendu, qui est une application de la théorie de l'évolution à des domaines aussi variés que ceux de l'intelligence artificielle, des neurosciences, de la systémique, la psychologie, ou la sociologie. Par exemple, les chercheurs en intelligence artificielle utilisent l'algorithme génétique, pour résoudre des problèmes d'ingénierie, en modélisant l'évolution de créatures mathématiques virtuelles. En neuroscience, on considère peu à peu que l'apprentissage se base sur des mécanismes darwiniens au niveau des synapses.

Le mot mémétique a été formé dans les années 1980 à partir du terme mème. Les deux accents aigus rappellent l'analogie avec le mot génétique.

Le mot mème vient de l'anglais "meme", introduit en 1976 par Richard Dawkins dans son livre "Le gène égoïste". Il a été créé à partir de : 

La mémétique applique à la culture humaine les concepts et modèles issus de la théorie de l'évolution (et spécialement de la génétique des populations). Elle essaye ainsi d'expliquer de nombreux phénomènes culturels, tels que : 

L'approche d'Aaron Lynch est un exemple. Mais aussi celles d'Howard Bloom, de Susan Blackmore, ou encore de Pascal Jouxtel.

De nombreuses têtes pensantes se demandent si l'analogie entre patrimoine génétique et patrimoine culturel peut soutenir un examen approfondi, et comment elle pourrait être vérifiée. Depuis sa création en 2003, la Société francophone de mémétique favorise en langue française leur travail et leur collaboration. Une série de séminaires pluridisciplinaires permettent de consolider peu à peu cette science en devenir, à commencer par son positionnement.

La mémétique doit être clairement distinguée de la sociobiologie. En sociobiologie, les entités considérées sont des gènes et des humains ; on s'y intéresse exclusivement à la part génétique des comportements. La mémétique, elle, s'intéresse à la part acquise (non génétique), et manipule les concepts de mèmes et de solutions culturelles. La sociobiologie s'intéresse aux origines biologiques des comportements humains, alors que la mémétique considère l'Homme non seulement comme un produit de l'évolution biologique, mais aussi comme un environnement pour l'évolution des éléments culturels.

Récemment, la découverte des neurones miroirs est venue renforcer l'idée de la reproduction mémétique. En effet la réplication d'actions, d'émotions et d'intentions, d'une personne à une autre, pourrait être favorisée par ce type de neurones.

La dérive mémétique se définit comme le processus par lequel une idée ou mème se transforme lors de sa transmission d'une personne à l'autre. Très rares sont les mèmes montrant une inertie mémétique forte, qui est la capacité d'un mème à être transmis de la même façon et à conserver son impact, quel que soit l'émetteur ou le récepteur. La dérive mémétique augmente quand le mème est transmis en exprimant l'idée de façon gauche, tandis que l'inertie mémétique est renforcée quand elle est exprimée en rimes ou par d'autres moyens mnémotechniques utilisés par le transmetteur pour en conserver le souvenir. L'article sur la loi de Murphy présente un exemple de dérive mémétique. Les légendes urbaines et les histoires drôles constituent des types de mème à forte inertie. Le marketing viral est une tentative d'exploitation de l'inertie mémétique à des fins de promotion commerciale. 

Une grande partie du vocabulaire de la mémétique est formé en ajoutant le préfixe 'mèm(e)-' à un terme existant, provenant en général de la biologie, ou en substituant 'mèm(e)' à 'gèn(e)'. Par exemple : pool mémétique, mémotype, ingénierie mémétique, complexe mémétique. En tant qu'adjectif, mémétique est parfois remplacé par mémique, avec la même relation qu'entre génétique et génique.

Les méméplexes sont des ensembles de mèmes qui, une fois associés, optimisent leurs chances de se répliquer (tout comme les gènes, lorsqu'ils sont associés dans un organisme, optimisent leurs chances d'être conservés par la sélection naturelle). Par exemple, les religions sont des méméplexes particulièrement adaptés à leur environnement, tout comme la notion de responsabilité, ou l'univers automobile.

Dawkins observa que les cultures peuvent suivre une évolution similaire à celle de populations d'organismes vivants. Différents concepts peuvent passer d'une génération de créatures culturelles à l'autre et influencer, en bien ou en mal, les chances de survie de ces créatures culturelles hébergées par les humains. La mort de l'humain porteur est la plus forte contrainte, pour une créature mémétique, mais pas la seule. Par exemple, différentes cultures peuvent concevoir indépendamment des outils uniques et spécifiques, et leur efficacité peut influencer directement les chances de prospérité de leur concepteur, et procurer ainsi des avantages sur les cultures concurrentes. Les méthodes prouvées ainsi meilleures peuvent alors être adoptées par une plus grande proportion de la population (ainsi la numération romaine a-t-elle été abandonnée au profit de la numération arabe). Chaque mème agit de façon similaire à un gène biologique qui peut être présent ou absent dans des populations de solutions culturelles données, la présence et la transmission génération après génération d'un mème sont directement liées à sa fonction. Le mème 'présence d'une chute' dans un contenu audiovisuel, augmentera sa capacité à distraire, et donc à se re-produire (le contenu sera conseillé, ou revu, plus que d'autres).

Une caractéristique clé d'un mème vient de son mode de propagation par imitation, comme l'a montré le sociologue français Gabriel Tarde. Le phénomène d'imitation appliqué à l'homme peut être considéré comme un comportement augmentant la capacité d'un individu à se reproduire et propager son génome. Les imitateurs auraient ainsi réussi à génétiquement accroître la capacité du cerveau à bien imiter. Dans ce sens, imiter signifie la capacité d'absorber des informations en utilisant ses sens pour ensuite les réutiliser. Cette information peut être inanimée, comme un livre, ou plus typiquement animée comme un autre humain dont le comportement va fournir des informations qui pourront être réutilisées. Les sources inanimées d'information ont été appelées systèmes de rétention. Sans le support de cerveaux suffisamment évolués pour évaluer leurs points clés de comportement et l'intérêt de les copier, les mèmes ne pourraient pas se propager. Les mèmes (ou des comportements acquis et propagés par imitation) ont été observés chez d'autres espèces que l'homme telles que les dauphins, les primates ou certains oiseaux. D'autres exemples de transmission de mèmes simples ont été aussi mis en évidence artificiellement chez les céphalopodes ou les rats. Une fois cette couche biologique installée, elle favorise une certaine indépendance du culturel sur le biologique.

Les mèmes et les gènes peuvent survivre plus longtemps que les organismes qui les transportent. Par exemple, un gène fournissant un avantage évolutif tel qu'accroître la force des dents d'une population de lions peut rester inchangé pendant des générations. De même, un mème particulièrement efficace peut se propager par copie d'un individu à l'autre longtemps après sa première apparition.

Une solution culturelle est une réponse, souvent partielle, aux contraintes actuelles d'une situation. C'est l'expression du code mémétique dans les systèmes mentaux, sociaux et physiques. Les solutions culturelles sont les véhicules des mèmes, les "individus" de l'algorithme évolutionnaire.
Une fois définie la solution, le méméticien va :

Quelques mèmes très répandus sont décrits ci-dessous :

Certains auteurs critiquent la mémétique sur son absence de prédictions concernant les évolutions de la culture. Si elle décrivait adéquatement les mécanismes mis en œuvre, elle devrait pouvoir le faire. Certains répondent qu'elle se place dans la catégorie des phénomènes complexes, comme l'économie ou la météo, et que la recherche de la prédiction parfaite est impossible, par manque d'informations sur les conditions initiales (il faudrait scanner et interpréter le "contenu" de tous les cerveaux).

D'autres auteurs, comme le darwinien et chrétien Connor Cuningham, pensent que la théorie des mèmes comporte une erreur intrinsèque, dès sa formation. Si l'on applique la théorie des mèmes à la culture, aux connaissances, alors cette théorie est un mème au même titre que la théorie créationniste, ou religieuse. Et si je suis victime de l'expansion d'un mème, que ce soit en défendant l'une ou l'autre de ces théories, qu'est-ce qui distingue les mèmes "corrects" des "incorrects" ? Cette théorie ne le permet pas. Elle permet par contre, de placer au centre du débat nos filtres personnels ou collectifs, nous permettant d'évaluer et de comparer.
Le fait qu'une théorie puisse s'appliquer à elle-même n'est d'ailleurs pas considéré ordinairement comme une faiblesse, ainsi les sociologues du programme fort en sociologie de la connaissance scientifique font de la réflexivité de leur théorie un des 4 piliers essentiels de la discipline.





</doc>
<doc id="6641" url="https://fr.wikipedia.org/wiki?curid=6641" title="Fusillade de la rue d'Isly">
Fusillade de la rue d'Isly

La fusillade de la rue d’Isly, appelée aussi le massacre de la rue d'Isly, a eu lieu le devant la Grande Poste de la rue d’Isly (dont le nom commémore la bataille homonyme) à Alger, département d'Alger.

Ce jour-là une manifestation de citoyens français, civils pieds noirs non armés, partisans du maintien du "statu quo" de l’Algérie française, décidée à forcer les barrages des forces de l'ordre qui fouillaient le quartier de Bab El-Oued en état de siège à la suite du meurtre de six appelés du contingent par l'OAS, se heurta à un barrage tenu par l'armée française qui mitrailla la foule. Le bilan est de 80 morts et 200 blessés. C'est un des derniers exemples de guerre civile entre Français. C'est selon l'historien Benjamin Stora un des exemples les plus marquants de la censure pratiquée pendant la guerre d'Algérie. Le gouvernement français n'a jamais reconnu sa responsabilité ce qui renforce l'amertume de beaucoup de pieds noirs
se sentant discriminés mémoriellement.

En réponse à la déclaration télévisée du président Charles de Gaulle proclamant le cessez-le-feu — donc la fin des opérations militaires françaises en Algérie — le , l'OAS met Alger en état de siège.

En 1962, le général Salan (chef de l'OAS) avait pour objectif de contrer la mise en œuvre des accords d'Évian signés le 18 mars, en provoquant le soulèvement commun des Français d'Algérie (Européens et musulmans) et des unités de l'armée française rejetant la sécession de l'Algérie par rapport à la métropole ; le l'ONU rejetait le droit de l'Algérie à l'indépendance.

Le 22 mars à Bab El Oued, des éléments de l'OAS abattent six appelés du contingent en Algérie du 360e CIT de Beni Messous et en blessent une dizaine ; appelés en intervention ils ont été pris en embuscade dans un camion militaire, place Desaix. Le 23 mars, des commandos de l'OAS prennent le contrôle du quartier européen de Bab El Oued, qui se trouve isolé du reste d'Alger par les forces de l'ordre et par l'armée qui fait intervenir l'aviation. Pour tenter de rompre l'encerclement de Bab El Oued, l'OAS lança un appel à la grève générale et organisa une manifestation devant se rendre à Bab El Oued en passant devant la Grande Poste, à l'entrée de la rue d'Isly d'Alger.

Le service d'ordre était assuré par l’armée qui avait reçu de Paris la consigne de ne pas céder à l'émeute. Le barrage à l'entrée de la rue d'Isly était tenu par 45 tirailleurs du RT du colonel Goubard. Les tirailleurs sont des soldats et, équipés comme tels, ne sont pas formés et adaptés aux missions de maintien de l'ordre. Leur précédente affectation était à Berrouaghia près de Médéa. Cette consigne est traduite par le commandement de la région militaire aux soldats dirigeant le barrage de la rue d'Isly par : « Si les manifestants insistent, ouvrez le feu » mais nul n’a voulu confirmer cet ordre par écrit. Selon d'autres sources, ces ordres n'auraient pas été transmis.

Malgré une interdiction, les manifestants se rassemblent. Puis ils « forcent » le premier barrage tenu par les tirailleurs en petit nombre. Peu après des coups de feu éclatent et les soldats ripostent.

Selon les militaires, des coups de feu d'origine inconnue - peut-être une provocation - seraient à l'origine du déclenchement du tir des militaires, qui mitraillent alors la foule à bout portant. . Yves Courrière montre les positions de tir supposées de membres de l'OAS. Il montre que si les circonstances restent peu claires, il est évident que les soldats n'ont pas tiré à bout portant dans la foule car vu le nombre de munitions tirées, il y aurait eu alors plusieurs centaines de morts.

. On s'époumone « Halte au feu » mais les rafales durent plus d'un quart d'heure.

, ces gendarmes mobiles auraient mitraillé la foule à partir des terrasses, notamment à l'angle des rues Charras et Charles Péguy. , le servant du fusil mitrailleur du R.T.A. aurait longuement « arrosé » les manifestants.

Le dernier bilan officiel est de 46 morts et 150 blessés, bien que de nombreux blessés meurent à l'hôpital Mustapha, où la morgue est débordée. Il y aurait en tout 80 morts. Aucune liste définitive des victimes n'a jamais été établie. Toutes les victimes étaient des civils, européens, quelques juifs séfarades. Toutefois en 2003, dans sa contre-enquête "Bastien-Thiry : Jusqu'au bout de l'Algérie française", Jean-Pax Méfret avance le nombre de 80 morts et 200 blessés au cours de ce qu'il nomme « le massacre du 26 mars ». L'association des victimes du 26 mars publie une liste de 62 morts, tous des civils ; 7 militaires (dont 2 gendarmes) sont tués.

Il faut attendre le pour que la télévision française (France 3) consacre une émission à cet événement méconnu, "Le massacre de la rue d'Isly", documentaire de 52 minutes, réalisé par Christophe Weber conseillé par l'historien Jean-Jacques Jordi.

Des Européens, revenus de leur stupeur, rendent les musulmans responsables de la tuerie. Pour eux, ce sont des agents provocateurs FLN qui l'ont organisée.

La fusillade achevée, ils se « font justice » au quartier de Belcourt où 10 musulmans sont lynchés sur le champ.

Les séquences filmées le 26 mars notamment par la RTBF sont censurées en France et ne seront diffusées que le 6 septembre 1963, dans l'émission "Cinq colonnes à la une" dédiée à la "Rétrospective Algérie".

Au soir du 26 mars 1962, le président Charles de Gaulle s'adresse au peuple français par l'intermédiaire d'une allocution télévisée. Il demande au peuple de voter « oui » à l'imminent référendum portant sur l'autodétermination de l'Algérie et déclare que « "En faisant sien ce vaste et généreux dessein, le peuple français va contribuer, une fois de plus dans son Histoire, à éclairer l'univers" », mais ne fait aucune référence au massacre qui a eu lieu dans la journée ; bien qu'un reportage ait été filmé par un correspondant de la RTF à Alger. 45 minutes avant l'allocution du Président démarrant à 20h, Inter Actualités rapporte les événements de la rue d'Isly par un reportage radiodiffusé de Claude Joubert, envoyé spécial à Alger. L'attitude du Général de Gaulle ne fera qu'exacerber le ressentiment des pieds noirs à son égard, d'autant plus qu'un décret du 20 mars 1962 empêche les Français des départements d’Algérie de participer au référendum pour ratifier ou non les accords d'Evian.

Deux jours après la fusillade du lundi 26 mars, le Haut-Commissaire de France Christian Fouchet, s'adresse aux « Français et Françaises d'Alger » par le biais d'une allocution télévisée.

La fusillade de la rue d'Isly marque la fin des espoirs placés par certains « Pieds-noirs » dans la capacité de l'OAS d’empêcher l'indépendance et contribue à généraliser l'idée qu'un retour en métropole (« la valise ou le cercueil ») est désormais inévitable pour les non-musulmans.

Pas plus que pour le massacre du 17 octobre 1961 ou pour l'affaire du métro Charonne, en métropole, il n'y a eu à ce jour de commission d'enquête officielle créée pour éclaircir les faits et les responsabilités dans ce massacre de civils.

Deux historiennes se sont livrées à une enquête complète qu'elles relatent dans leur livre "Un crime sans assassin". Elles démontent les témoignages d'un coup de feu venant des immeubles et soulignent que la plupart des journalistes présents désignent les tirailleurs et en particulier le servant du FM comme à l'origine de la fusillade. Elles posent la question capitale : « Pourquoi des troupes fatiguées et n'ayant jamais été confrontées au maintien de l'ordre en ville ont-elles été placées avec des ordres stricts à cet endroit ? »

Leurs soupçons sont confortés par la déclaration du préfet de police Vitalis Cros dans son livre "Le temps de la violence" : « "la nouvelle que nous redoutions et espérions à la fois arriva, les tirailleurs avaient ouvert le feu" ».

Cette position favorable à la défense des intérêts de l'État semble confirmée par l'essai de Jean Mauriac : "L'Après De Gaulle ; Notes Confidentielles, 1969-1989", dans lequel il rapporte (page 41) les rancœurs de Christian Fouchet, haut-commissaire de l'Algérie française, le 28 octobre 1969 : « "J'en ai voulu au général de m'avoir limogé au lendemain de mai 1968. C'était une faute politique. Il m'a reproché de ne pas avoir maintenu l'ordre : "Vous n'avez pas osé faire tirer. J'aurais osé s'il l'avait fallu", lui ai-je répondu. "Souvenez-vous de l'Algérie, de la rue d'Isly. Là, j'ai osé et je ne le regrette pas, parce qu'il fallait montrer que l'armée n'était pas complice de la population algéroise."" ».

Cette hypothèse, et ces propos indirectement rapportés, trente ans après la mort des principaux témoins, sont formellement démentis, et en désaccord total avec le livre "Au service du général de Gaulle" (Plon, 1971) où Christian Fouchet écrit (page 155) : . D'après la chronologie des évènements, il n’aurait donc pas pu avoir été le donneur d'ordre.

Il est à noter que les familles n'ont jamais eu le droit de récupérer les corps des victimes. Beaucoup ayant été clandestinement enterrés au cimetière Saint Eugène.






</doc>
<doc id="6642" url="https://fr.wikipedia.org/wiki?curid=6642" title="Violon">
Violon

Le violon est un instrument de musique à cordes frottées. Constitué de de bois (érable, buis, ébène, etc.) collés ou assemblés les uns aux autres, il possède quatre cordes accordées généralement à la quinte, que l'instrumentiste, appelé violoniste, frotte avec un archet ou pince avec l'index ou le pouce (en pizzicato).

Dans les formations de musique classique telles que le quatuor à cordes ou l'orchestre symphonique, le violon est l'instrument le plus petit et de tessiture la plus aiguë parmi sa famille ; celle-ci inclut l'alto, le violoncelle et la contrebasse. Sa création remonte au . Très vite popularisé, il occupe une place importante de la musique classique occidentale : de grands compositeurs ont écrit pour cet instrument (concertos, musique de chambre, pièces symphoniques, etc.) voire en jouaient eux-mêmes (Vivaldi, Bach, Mozart, etc.), et certains violonistes du ont acquis une grande renommée, notamment Paganini.

Un violon se compose de principales : la caisse de résonance, le manche et les cordes.

La longueur du violon est variable. Un violon de taille maximale est appelé un "entier", et est destiné aux violonistes ayant atteint leur taille adulte ; il mesure généralement de long, du bouton à l'extrémité de la tête, et la longueur du coffre est comprise entre . Il existe une échelle non proportionnelle de longueur des violons, les violons "non entiers" (quart, demi, trois-quarts...) étant généralement destinés aux enfants :

L'existence de violons de petite taille est avérée au moins depuis le milieu du .

Les luthiers sont les artisans qui créent et entretiennent les instruments à cordes. Les plus connus sont Antonio Stradivari, la famille Amati, la famille Guarneri, Jean-Baptiste Vuillaume et Nicolas Lupot qui tous deux ont reçu le surnom de "Stradivarius français". Certaines de leurs créations sont restées célèbres (voir la catégorie à ce sujet).

L'influence du bois utilisé a été étudiée. Étant un composant de l'instrument largement majoritaire face au vernis, son rôle semblait devoir être également prépondérant. De nombreuses hypothèses ont été échafaudées, certains allant jusqu'à supposer que le bois des violons provenait de la charpente de cathédrales ou de châteaux, ce qui lui aurait donné un âge exceptionnel. Lloyd Burckle et Henri Grissino-Mayer ont quant à eux fait l'hypothèse que le bois utilisé par les grands maîtres italiens provenait des vingt années précédant le minimum de Maunder additionnés au minimum lui-même. Ce minimum de Maunder est une période de froid intense qui a eu lieu en Europe ; avec les vingt années précédentes, cela correspond à la période 1625-1715. Selon les deux auteurs, ce froid aurait provoqué une pousse ralentie des arbres, leur conférant une densité de cernes de croissance par unité de longueur élevée. Mais cette hypothèse a été également rejetée à cause des temps de séchage probablement adoptés par les luthiers italiens, et les auteurs reconnaissent eux-mêmes qu'aucun traitement particulier (séchage, stockage ou vernis) n'a jamais été identifié comme cause certaine de supériorité.

Joseph Nagyvary et son équipe ont analysé le bois de cinq instruments (dont un Stradivarius et un Guarnerius) datant d'entre 1717 et 1840. « Dans deux des instruments censés être des merveilles d'acoustique, le bois a été traité par des produits chimiques », d'après Nagyvary : des molécules d'hémicellulose ont été brisées à la suite de l'oxydation causée par un pesticide. Selon le chercheur, le bois aurait été bouilli dans de l'eau chimiquement traitée, dans le but de protéger l'instrument contre les vers et les moisissures. Modifiant la structure du bois, cette opération aurait donc eu des répercussions inattendues. Mais l'agent oxydant employé reste inconnu.

Les bois utilisés sont :

L'érable a été choisi parce qu'il n'est pas trop lourd, et il est dur et élastique en même temps. Certains auteurs de lutherie classique prétendent que le peuplier ou le frêne, utilisés par les anciens luthiers italiens, ont été écartés car trop mous et donnant des sons creux et en dedans, de même Tolbecque critique lui aussi les vieux fonds en peuplier.
Or on sait que ces considérations sont très subjectives et sujettes à interprétation personnelle, le son n'étant pas uniquement le résultat d'une ou deux données mécaniques du bois, d'autant plus que certaines informations étaient erronées autrefois : le frêne européen par exemple, "fraxinus excelsior" , n'est pas un bois mou, il est plus dur que l'érable sycomore . Le bois du peuplier lui, est bien mou, mais est utilisé dans d'autres domaines de la lutherie classique.

Les bois doivent être vieillis avant d'être utilisés, dans un endroit plutôt froid et à l'abri de l'humidité, du vent, de la poussière et des insectes xylophages.

Pour pouvoir reproduire un modèle de violon, le luthier fabrique des moules et des modèles : pour le contour de la table, pour les ouïes, pour l'épaisseur de la voûte, pour la tête.

La table et le fond sont formés ; le plus souvent la table est constituée de deux pièces afin d'assurer une symétrie des largeurs des fibres de part et d'autre de l'axe central pour des raisons de sonorité, tandis que le fond peut être d'une ou de deux pièces selon le choix arbitraire du luthier. Les tables et fonds en deux parties sont obtenus à partir d'un morceau fendu en deux. Le luthier trace une ébauche de la forme de la voûte et détermine les contours exacts de la table et du fond.

Une fois les contours de la table et du fond découpés (à la scie à chantourner par exemple) puis affinés (canif, lime), le luthier procède à l'élaboration des voutes. À l'aide de larges gouges d'abord, puis de rabots « noisettes » ensuite, les voutes sont ébauchées. On procède aux finitions en utilisant des racloirs de diverses formes, jusqu'à l'obtention des voutes désirées dont le galbe revêt beaucoup d'importance dans la production de la sonorité.

L'étape du filetage consiste à poser les filets à du bord. Ce sont trois fines lignes qui ont, en plus d'un rôle décoratif, un rôle de protection contre les chocs, susceptibles d'amorcer des fissures dans le sens du fil du bois : les filets préviennent la propagation de telles fissures. Les deux lignes noires peuvent être en alisier teint, en ébène, en poirier, voire en baleine ou en cellulose issue du carton et compressée. La partie blanche peut être en houx, en buis ou en charme. Ils sont dans certains cas inexistants, et ne figurent alors que les emplacements creusés des deux filets noirs ; enfin, dans l'état de finition le moins avancé, ils ne sont que peints à l'encre de Chine. Certains luthiers, comme Maggini ont utilisé des doubles filets : il semble que cette technique soit purement décorative.

Puis le luthier creuse le fond au rabot et à la gouge et place sept taquets (petites pièces de bois) sur celui-ci au niveau du joint (s'il y en a un), afin de le consolider. Il donne au fond son galbe définitif.

La même étape de creusage a lieu sur la table et les ouïes sont percées. La barre d'harmonie est alors placée ; il s'agit d'une longue pièce de bois, placée sous la table au niveau du pied gauche du chevalet. Elle sert à aider le violon à vibrer, et à résister à l'importante pression exercée par les cordes.

En vue de l'assemblage du fond et de la table, pour former une caisse de résonance, on en construit les bords verticaux que sont les éclisses. Contrairement à la table, l'orientation des fibres du bois n'a ici qu'un rôle décoratif. Les éclisses sont cintrées au fer chaud. Puis on les assemble sur une forme au moyen de petites pièces de soutien qui contribueront à la rigidité de la caisse de résonance : tasseaux, coins, contre-éclisses.

Le manche et la touche sont les dernières grandes pièces à réaliser. L'étape délicate de la conception du manche est la taille de la volute, car le modèle utilisé est difficile à appliquer à la pièce à cause du relief (la volute « monte » en même temps qu'elle « tourne »). On creuse le chevillier, partie où passent les cordes, entre le sillet et les chevilles ; dans certains cas, il a même été creusé entièrement, sans que cela ait d'autre incidence qu'esthétique. On taille la touche, large de au niveau du sillet, de 49 côté chevalet, et arrondie comme le chevalet.

Puis vient l'assemblage global : on colle le fond sur les éclisses puis, après avoir retiré la forme, on fixe la table et enfin on enclave le manche dans le corps du violon en forçant un peu et on le colle. On fixe alors la touche au manche avec quelques gouttes de colle, afin qu'elle tienne le temps de concevoir le sillet (que l'on ne fixe lui aussi que très légèrement) et de tailler proprement le manche. Puis on fait sauter touche et sillet : l'instrument est terminé "en blanc". On procède à l'encollage, c'est-à-dire que l'on enduit le violon d'une sous-couche empêchant le vernis de pénétrer dans les pores du bois. Cette sous-couche peut être à base de gélatine, de blanc d'œuf, d'huile… Le violon peut à présent être verni.

Le luthier recolle alors la touche, taille et place les chevilles, puis le bouton. Vient ensuite la taille et la pose de l'âme, située en largeur au niveau du pied droit du chevalet, et en longueur à en arrière de celui-ci. Le luthier vernit le manche avec un vernis peu coloré et plus résistant aux frottements de la main et à la sueur. Enfin, il retaille le chevalet brut fourni par le fabricant et le place en même temps qu'il installe les cordes. Le violon est à présent terminé, et ne restent plus à faire que des réglages de la sonorité.

Le vernis a un rôle esthétique et un rôle de protection contre l'humidité due à la sueur et à l'air ambiant, dont l'hygrométrie est variable. Plusieurs recettes de vernis existent : à l'alcool, à l'huile de lin, à l'essence grasse (procédé Mailand), propolis… La technique consiste en un mélange variable de solvant et de laques, essence de térébenthine, résines, gommes et colorants, que l'on applique en couches successives sur le violon, et que l'on polit (d'où l'intérêt d'ôter la touche, afin de pouvoir polir le vernis situé sous son emplacement).

Le vernis peut avoir différentes teintes, extrêmement variables d'un cas à l'autre suivant les colorants utilisés, l'usure et la patine. Ainsi, il peut aller du jaune doré pour les Amati au rouge brun des Bergonzi en passant par l'orange foncé pour les Stradivarius ou le brun terne des instruments bas de gamme de l'école allemande du .

L'influence du vernis sur la sonorité a été âprement discutée. La recette utilisée par l'école de Crémone ayant été perdue, et les luthiers cherchant la ou les causes de la qualité des violons issus de celle-ci, on a supposé que le vernis jouait un rôle fondamental quant à la sonorité du violon.

Le temps nécessaire à la fabrication d'un violon est délicat à estimer, car il dépend de l'expérience de l'artisan. On considère qu'un artisan confirmé fabrique un violon en , le bois étant déjà sec. L'étape la plus longue à réaliser est le vernissage, car chacune des nombreuses couches n'est appliquée qu'après le séchage de la précédente, or il peut y avoir jusqu'à trente applications successives.

Le prix d'un violon est très variable. Ainsi, un violon d'usine fabriqué en Chine dans les années 1980, vendu avec mentonnière et étui, coutait moins de . À l'exact opposé, les anciens violons des grands luthiers italiens atteignent des sommes considérables lors de ventes aux enchères. Le dernier record enregistré revient à un Guarnerius del Gesù de 1742 qui a été joué par Yehudi Menuhin et qui a été vendu le à Zurich à plus de de dollars. Il existe un juste milieu ; Menuhin propose par exemple comme bons violons les productions de la lutherie espagnole du ou celles de la lutherie tchèque. Un violon à l'état brut, sans vernis, se nommera "violon sylvicole" ou tout simplement un "violon en blanc".
La fonction de la caisse de résonance est d'amplifier le son provoqué par la vibration des cordes.

La face supérieure d'un violon est appelée table d'harmonie. Faite (le plus souvent) de deux morceaux d'épicéa collés dans le sens de la longueur, elle est bombée et percée de deux orifices en formes de "ƒ", les ouïes, qui ont pour vocation de libérer les vibrations provenant de la caisse de résonance. La face inférieure, communément appelée le « dos » ou le « fond », est formée d'une pièce en érable, ou de deux pièces collées ensemble dans le sens de la longueur. Elle est également bombée mais souvent dans une moindre mesure. Sur les bords des deux faces, on distingue une double ligne noire enserrant une ligne de même couleur que la table (blanche avant le vernissage) : les filets.

Les flancs en érable, appelées éclisses, réunissent la table d'harmonie et le fond afin de former une boîte qui forme la caisse de résonance. Au niveau du chevalet, les côtés du violon sont en forme de "C" (en creux vers l'intérieur) : ce sont les échancrures, dont le but est de permettre le passage de l'archet. Les petites pointes à leurs extrémités se nomment les onglets.

À l'intérieur du violon, on trouve l'âme et la barre d'harmonie, qui jouent un rôle essentiel dans la transmission des vibrations des cordes et dans la résistance face à la pression qu'exercent les cordes.

C'est dans la caisse de résonance que l'on trouve l'étiquette mentionnant le nom du luthier fabricant et l'année de fabrication.
Il permet d'obtenir la bonne longueur de cordes, d'ajuster la tension de celles-ci et autorise le jeu du violoniste.

Il s'agit d'une pièce d'érable terminée par la tête, décorée d'un ornement en forme de spirale, la volute. Dans la construction baroque et classique, jusque vers 1800, le manche du violon était ajusté contre le tasseau et cloué à lui. Maintenant il est enclavé et collé dans le tasseau supérieur. Sur la tête, des chevilles sont fixées latéralement afin de contrôler la tension des cordes. Facilement reconnaissable par sa couleur noire, une longue plaque d'ébène, la touche, non frettée, est collée sur le manche. La touche est terminée au niveau de la tête du violon par le sillet, petite pièce en ébène qui fait office de guide pour les cordes.

La volute peut également servir à poser un violon sur un support de partitions.

Les quatre cordes sont la partie du violon qui, mise en vibration par l'archet ou par les doigts, produit le son. Les cordes, de la plus grave à la plus aiguë, sont accordées à la quinte de la manière suivante : "sol, ré, la" et "mi". On accorde le violon soit avec les chevilles, qui sont situées sur la volute (tête du violon), ou avec les vis (les tendeurs), qui elles, sont situées sur le cordier. Il faut savoir que l'accordage par les tendeurs est plus subtil, et plus facile pour les débutants. On peut cependant accorder le violon autrement pour obtenir un effet, la scordatura.

Pendant une très longue période, les cordes étaient en boyaux et seule la corde de "sol" était entourée d'un fil d'argent ou de cuivre (elle était dite « filée »). Le boyau employé n'était pas de chat comme le veut une idée très populaire. Cette erreur est par exemple reprise dans la définition humoristique du violon donnée par Ambrose Bierce dans son "Dictionnaire du Diable" de 1911 : « Violon : instrument destiné à chatouiller les oreilles de l'homme par le frottement de la queue d'un cheval sur les boyaux d'un chat ».

L'erreur pourrait provenir d'une compréhension trop littérale de "catgut", corde de boyau utilisée en chirurgie. En réalité, on emploie pour fabriquer les cordes en boyau la tunique médiane de l'intestin grêle du mouton, dont les fibres sont résistantes. Plusieurs fils obtenus par découpage dans le sens de la longueur sont tordus ensemble, et la tunique médiane est si fine que les intestins grêles de quatre à cinq moutons sont nécessaires pour faire environ vingt-cinq cordes de "la".

Une corde de boyau doit être dans toute sa longueur cylindrique, de même diamètre, élastique, d'une souplesse régulière et de couleur transparente. Un épaississement ou une densité irrégulière de la structure du matériau empêchent un accord tout à fait juste. La justesse d'une corde ne s'évalue sur la régularité du diamètre que si elle est de densité régulière, cette dernière condition n'étant remplie que pour les cordes de bonne qualité.

Au début du , la corde de "mi" fut remplacée par un fil d'acier, car elle se brisait trop souvent. Plus tard, on a également muni d'un enroulement d'aluminium les cordes de "la" et de "ré", dont le noyau est en boyau ou en matière synthétique, tel que l' (alliage d'aluminium et de magnésium résistant à la corrosion due à la sueur). Cette dernière solution est maintenant privilégiée (sauf pour la musique ancienne) : elle est moins sensible au désaccord ; elle n'est pas aussi exigeante lors de la fabrication, et peut donc être réalisée de manière industrielle. On utilise également beaucoup des cordes basses filées à noyau d'acier, qui produisent une sonorité claire mais sèche. Pour accorder plus facilement les cordes d'acier, beaucoup plus sensibles à la tension, ont été mis au point des mécanismes à vis spéciaux, fixés au cordier, les tendeurs, petites molettes fines.

Si toutes les cordes avaient le même diamètre, la tension devrait diminuer pour les cordes basses. La répartition de la pression sur la table serait alors irrégulière et la sonorité des cordes basses insatisfaisante à cause de la faiblesse de résonance. C'est pourquoi les cordes ont des diamètres différents, mais une tension presque égale. On tend plus fortement la corde de mi, qui repose sur le pied droit du chevalet afin de lui conférer un volume sonore et un éclat accrus. Un violon de type Stradivarius pèse entre tout compris ; la table, , et le fond, , doivent résister, par l'intermédiaire des éclisses, à la tension des cordes égale à . Comme dans le cas du violon la pression transmise à la table vaut par kg de tension, on trouve une pression exercée sur la table valant .

Autrefois on désignait sous le nom de "bourdon" la corde de "sol". On en trouve la trace dans la traduction de la méthode de Leopold Mozart par exemple. Actuellement, on note les cordes de I à IV, la première corde étant celle de "mi", également nommée chanterelle.

Yehudi Menuhin dit des cordes : « La corde de "sol", la plus grave, suscite une sonorité riche, profonde, et inspire un sentiment de noblesse. La corde de "ré" se distingue par son caractère plus passionné, plus vif. La corde de "la" s'ouvre et s'épanouit dans l'espace. La plus brillante et la plus extravertie des quatre est la corde de "mi". »
Le chevalet est une planchette sculptée en érable sycomore placée perpendiculairement à la table d'harmonie entre les ouïes et qui assure deux fonctions. Il maintient les cordes dans une configuration arquée (les cordes ne sont pas dans un même plan), permettant que chacune puisse être frottée séparément. Il a également une influence sur la sonorité du violon, car il communique les vibrations des cordes à la table d'harmonie. Tout comme l'âme, cette pièce n'est pas collée à l'instrument, mais maintenue en place par la seule pression des cordes.

Les quatre cordes peuvent être accordées au niveau de la tête grâce aux chevilles ; à la base du violon, les tendeurs permettent un accord plus fin. Ces tendeurs sont attachés au cordier, pièce noire en ébène fixée à la caisse par un bouton.

De très nombreuses techniques existent sur le violon pour obtenir une large palette sonore et tirer toutes les possibilités de l'instrument. Le plus souvent, on joue du violon en posant le "bas" de l'instrument (le côté chevalet - cordier, et non le côté manche - volute) sur la clavicule gauche, tandis que les doigts de la main gauche (excepté le pouce) appuient sur les cordes au niveau de la touche et que la main droite tient l'archet et frotte avec celui-ci les cordes. Cette façon de jouer est indépendante de la latéralisation (droitier ou gaucher). Quelques violonistes jouent en posant l'instrument sur leur clavicule droite, et donc en inversant tous les gestes, mais la première manière est très largement majoritaire. Les explications qui suivent considèrent donc le cas le plus courant.


La même note jouée sur deux cordes différentes, sonne avec une couleur différente, plus ou moins « chaude ». Ces différences sont exploitées par le violoniste en fonction de l'effet recherché. Remarque : comme le pouce ne sert pas à autre chose que tenir le manche, on appelle "premier doigt" l'index, et ainsi de suite jusqu'à quatre seulement.

La mentonnière et le coussin sont deux pièces pouvant s'ajouter ou s'enlever librement du violon, et dont la fonction est de faciliter l'adaptation du corps à la forme des éclisses.

Le coussin se place sous le violon, et évite ainsi à la clavicule de subir trop durement le contact des bords du violon.

La mentonnière se place sur le violon, à gauche du cordier, ou l'enjambant, et comme son nom l'indique, on y place le menton. Elle permet d'éviter de mettre massivement la sueur du violoniste en contact avec la table, abîmant alors le vernis. Louis Spohr semble avoir été un des premiers à l'utiliser, en 1819. L'accessoire était à ses débuts assez grossier, s'implantant dans le tasseau avec une vis à bois. Il fut largement critiqué, comme étant « un champignon qui aurait poussé sur le bord du violon », ou étant jugé gênant, ridicule, augmentant sans raison la hauteur des éclisses, empêchant de faire corps avec l'instrument, modifiant le son du violon… Tolbecque considérait dans les années 1900 qu'il avait fallu 70 ans pour que l'usage se répande, et qu'il était à présent tout à fait adopté. C'est cependant inexact, les instrumentistes de musique traditionnelle (par exemple en musique cadienne) jouent souvent le violon posé contre le haut de la poitrine, et donc n'ont pas besoin de mentonnière. Quant aux Tsiganes, ils jouent souvent les contre-temps le violon en l'air, la table basculant de l'horizontale à la verticale (l'axe du violon conserve son orientation habituelle).

La sourdine se place sur le chevalet. Son action consiste à ajouter de la masse au chevalet ce qui restreint la transmission des vibrations des cordes au chevalet et donc à la caisse de résonance via l'âme. Le but premier de la sourdine est de réduire l'intensité sonore du violon, mais ce n'est pas son seul effet. La sourdine permet aussi de modifier le timbre de l'instrument.
Il existe deux catégories de sourdines : les sourdines d'orchestre et les sourdines d'appartement. Les sourdines d'orchestre (petites sourdines en caoutchouc ou en ébène) utilisées à l'orchestre donnent un timbre plus doux et feutré. Les sourdines d'appartement (sourdines peigne en caoutchouc ou en plomb qui sont beaucoup plus lourdes) ont pour but de réduire l'intensité sonore au maximum pour ne pas déranger les voisins.

Par exemple : une simple pince à linge ; un petit morceau de caoutchouc posé sur le chevalet entre les cordes de "ré" et de "la" ; une plus large bande (à quatre branches), recouvrant entièrement le dessus du chevalet, du "sol" au "mi" ; un modèle métallique (le plus puissant : en plomb).

Bien qu'il existe une représentation d'un violon (forme en huit, volute, ouïe en forme de C) sur une statue d'un temple en Inde datée du , on estime habituellement que le violon naît dans les années 1520, dans un rayon de autour de Milan en Italie. Il y a indétermination entre les villes de Brescia et de Crémone. Le premier luthier ayant fabriqué un violon pourrait être ou Zanetto Montichiaro ; rien ne permet d'affirmer que ce soit Andrea Amati (v. 1505/1510-1577), et (contrairement à ce qu'affirme la littérature du ) il ne s'agit sûrement pas de Gasparo da Salò, né en 1540. Il semble que le violon ait emprunté des caractéristiques à trois instruments existants : le rebec, en usage depuis le (lui-même dérivé du rebab de la musique arabe), la vièle et la "lira da braccio".

La première mention du violon dont on ait trace est une note de décembre 1523, dans un registre de la Trésorerie générale de Savoie (la langue y est alors le français), pour le paiement des prestations des « trompettes et vyollons de Verceil ». La première apparition du violon dans l'art est due à Gaudenzio Ferrari (circa 1480 - 1546), auteur de la "Madonna degli aranci" (La Madone à l'oranger), datant de 1529-30, dans l'église Saint-Christophe de Verceil. Enfin, une des premières descriptions explicites de l'instrument et de son accord en quintes figure dans l"'Epitomé musical des tons, sons et accordz, es voix humaines, fleustes d'Alleman, fleustes à neuf trous, violes, & violons." de Philibert Jambe de fer, publié à Lyon en 1556. Philibert Jambe de fer écrit : « Le violon est fort contraire à la viole… Nous appelons viole c'elles desquelles les gentils hommes, marchantz et autres gents de vertuz passent leur temps… L'autre s'appelle violon et c'est celuy duquel ont use en danceries. »

Le violon se répand rapidement à travers l'Europe, à la fois comme instrument de rue, populaire, et comme instrument apprécié de la noblesse : ainsi, le roi de France Charles IX aurait commandé à Amati 24 violons en 1560. Le plus ancien violon qui nous soit parvenu serait un de ceux-là et porte le nom de leur commanditaire.

Vers 1630, Pierre Trichet écrit, dans son "Traité des instruments de musique" que « les violons sont principalement destinés aux danses, bals, ballets, mascarades, sérénades, aubades, fêtes et tous passe-temps joyeux, ayant été jugés plus appropriés à ces genres de passe-temps que tout autre instrument ». L'avis de Trichet n'est pas isolé au début du : à ses débuts, le violon est considéré comme criard et juste bon à faire danser. De fait, la France du recherche plus les sonorités intimistes propres à l'expression individuelle que les effets spectaculaires des virtuoses et le son brillant du violon. Cependant, il a déjà commencé sa conquête du monde musical en Italie dès les années 1600.

Sous l'influence de premiers virtuoses tels que Balthazar de Beaujoyeulx, à la tête du groupe de violons italiens emmenés du Piémont en 1555 par Charles de Cossé, la famille des violons connaît un succès croissant qui va l'amener à supplanter progressivement la viole de gambe. Dans "Circé ou le ballet comique de la reine" (à l'origine, "balet comique de la Royne"), dont la production globale avait été confiée à Beaujoyeux, se trouvent deux séries de danses instrumentales qui sont spécifiquement destinées à être jouées par des "violons". Le texte et la musique en sont publiés en 1582, formant ainsi la première partition jamais imprimée pour le violon. L'établissement du violon en France se poursuit avec la création en 1626 des Vingt-quatre Violons du Roi, et surtout grâce à l'influence du compositeur et violoniste italien Jean-Baptiste Lully (1632 - 1687), qui, prenant la tête de La Petite Bande en 1653, la fait progresser jusqu'à la mettre en concurrence avec les Vingt-quatre Violons.

C'est néanmoins en Italie que le violon connaît son essor le plus rapide et le plus spectaculaire. La virtuosité des violonistes italiens est exploitée en la deuxième partie du à Brescia avec les virtuosos Giovan Battista Giacomelli et Giovan Battista Fontana et dès le début de la période baroque par Claudio Monteverdi, qui use de "trémolos" et de "pizzicatos" dans ses opéras, dont l'un des plus connus pour son usage du violon est "L'Orfeo" (1607). Il faut attendre plusieurs décennies avant que des virtuoses tels que Heinrich von Biber (1644 - 1704) atteignent hors d'Italie un degré de maîtrise virtuose tel que celui développé par les maîtres italiens.

La seconde partie du voit la domination de l'école de Bologne qui produit des musiciens tels que Arcangelo Corelli, son élève Francesco Geminiani, ou encore Giovanni Battista Vitali, et voit naître des formes telles que la Sonate et le Concerto grosso. C'est à Crémone, près de Bologne que Niccolò Amati, Andrea Guarneri et surtout Antonio Stradivarius amènent le violon à sa forme actuelle et produisent des exemplaires d'une très grande qualité, à tel point que les Stradivarius et, dans une moindre mesure les Guarnerius, sont toujours aujourd'hui les violons les plus onéreux et les plus recherchés. Parmi les virtuoses ayant possédé un Stradivarius, citons Niccolò Paganini, Joseph Joachim, David Oïstrakh ou encore Jascha Heifetz (qui jouait aussi un Guarnerius).

Plus tard, au cours du , c'est à Venise, avec Antonio Vivaldi, à Rome avec Pietro Locatelli ou Padoue avec Giuseppe Tartini que se développent le plus sensiblement la technique et le répertoire du violon. Les "Quatre Saisons" pour violon et orchestre de Vivaldi, ou la "Sonate des trilles du Diable" de Tartini, tiennent toujours une place de choix dans le répertoire du violon.

La période classique voit l'émergence d'une école de violon germanique influencée par les Italiens qui ont désormais acquis une notoriété suffisante pour faire des tournées dans toute l'Europe. Johann Georg Pisendel (1687 - 1755) voyage entre la cour de Dresde et ses maîtres italiens Giuseppe Torelli et Vivaldi. Ce sont les œuvres pour violon solo de Pisendel qui auraient influencé Bach pour écrire ses Sonates et partitas pour violon seul (BWV 1001 à 1006), qui exaltent les capacités polyphoniques du violon : chaque sonate comprend une fugue à quatre voix pour violon seul, et la Partita pour violon seul n° 2 inclut la célèbre Chaconne. Les compositeurs virtuoses de l'école de Mannheim, Johann Stamitz (1717 - 1757), Carl Stamitz (1745 - 1801) et Christian Cannabich (1731 - 1798) ainsi que leur contemporain Leopold Mozart (1719 - 1787), sont tous des violonistes de renom, exerçant bien au-delà des frontières germaniques.

Un peu plus tard, Wolfgang Amadeus Mozart (1756 - 1791), compositeur et violoniste virtuose, écrit de nombreuses sonates pour violon et clavier, cinq concertos pour violon (KV 207, 211, 216, 218, 219) et la symphonie concertante (KV 364).

En 1740, Louis Le Blanc publie un traité pour défendre la viole contre "les entreprises du violon et les prétentions du violoncelle", signe qu'encore au milieu du , la querelle entre les partisans des deux familles d'instruments ne s'est pas épuisée. Simon McVeigh note que la résistance des Français concerne plus la musique italienne, en adéquation avec l'esthétique française d'alors, qu'un simple problème d'instrument. Toutefois, la présence de violonistes virtuoses tels que Jean-Marie Leclair (1697 - 1764) dans le paysage musical français d'alors laisse percevoir la perméabilité qu'a acquise en un demi-siècle la musique française aux influences italiennes. Les réticences de Le Blanc finissent par être balayées, avec l'abandon à la fin du de la viole.

Un autre abandon datant de cette époque renforce le rôle du violon, cette fois-ci en orchestre : la basse continue disparaissant peu à peu à partir de 1770, le premier violon, membre du plus important groupe de l'orchestre, la supplante à la direction. C'est ainsi que jusqu'à la fin du , en incluant même Pierre Monteux et Charles Munch, presque tous les chefs d'orchestre français sont violonistes, et que jusqu'à Jules Pasdeloup et Édouard Colonne (à ses débuts), ils dirigent avec l'archet.

Dans les dernières décennies du siècle, Paris est devenu un centre cosmopolite pour les violonistes, accueillant non seulement Mozart mais aussi plusieurs virtuoses renommés, notamment Joseph Bologne de Saint-George ou Giovanni Battista Viotti, qui se produit avec le "Concert Spirituel" dès 1782. Au travers de ses 32 concertos pour violon et grâce à sa maîtrise technique, notamment de l'archet, Viotti influence durablement l'art du violon pour les décennies à venir.

Alors que l'école française de violon devait conquérir une place de plus en plus prééminente durant l'ensemble du , grâce notamment à la fondation du Conservatoire de Paris, en 1795, c'est encore l'école italienne qui fournit au monde du violon d'alors, en la personne du virtuose Niccolò Paganini (1782-1840), l'un de ses plus remarquables talents. La publication de ses 24 Caprices pour violon solo, opus 1, et de ses concertos pour violon, marque une avancée décisive dans les possibilités virtuoses de l'instrument, préparant celui-ci au répertoire flamboyant du , en introduisant notamment des pizzicati de la main gauche, des coups d'archets en ricochets, des doubles cordes harmoniques… C'est pourquoi Paganini représente dans l'imaginaire romantique la « virtuosité transcendante quasi diabolique ». Ses seuls élèves connus, Camillo Sivori (1815-1894) et Antonio Bazzini (1818-1897) devaient poursuivre l'œuvre du maître, mais l'on peut affirmer que la carrière brillante de Paganini marque la fin de la grande école de violon italienne.

Le début du voit l'émergence en France d'une génération de violonistes brillants largement influencés par Viotti. Parmi ses élèves et disciples, Pierre Rode (1774 - 1830), Rodolphe Kreutzer (1766 - 1831) et le belge Charles-Auguste de Bériot (1802 - 1870) connaissent la gloire de carrières internationales ; ils enseignent au Conservatoire de Paris et laissent un important matériel pédagogique, très utilisé par la suite : 24 caprices de Rode, 42 études de Kreutzer, concertos de Bériot… La société bourgeoise du veut se divertir grâce à la musique, mais aussi l'apprendre ; elle est ainsi l'instigatrice de ces méthodes et études pour violon de l'école franco-belge.

Parmi les élèves de Bériot, Henri Vieuxtemps (1820 - 1881) écrit une abondante littérature violonistique (ses concertos et pièces de virtuosité sont encore largement présentes au répertoire aujourd'hui). Vieuxtemps a lui-même pour élève Eugène Ysaÿe (1858 - 1931), compositeur de six sonates pour violon, opus 27. Tous ces violonistes font significativement évoluer la technique du violon et l'interprétation du répertoire. Parallèlement, à part Camille Saint-Saëns et Édouard Lalo, les compositeurs romantiques français n'apportent que peu de grandes œuvres de bravoure au violon, laissant aux compositeurs germaniques le soin d'écrire les grands concertos du répertoire.

Le , en Allemagne, est l'époque de fondation du grand répertoire du violon. Les compositeurs allemands écrivent quatre des plus célèbres concertos pour l'instrument, tous toujours très joués à l'heure actuelle :
On peut également souligner la composition des 10 sonates pour violon de Beethoven, dont "Le Printemps" et la "Sonate à Kreutzer".

Hors d'Allemagne, Piotr Ilitch Tchaïkovski compose en 1878 son concerto pour violon en ré majeur, Antonín Dvořák, en 1879, son concerto en la mineur op. 53 (B108). Pablo de Sarasate, virtuose renommé, écrit plusieurs morceaux de bravoure, mettant en valeur sa brillante technique ; parmi eux, la Fantaisie de concert sur des thèmes de Carmen, Zigeunerweisen ou encore la Habanera.

Le continue à consolider la place du violon dans le répertoire classique. Bien que de nouveaux styles apparaissent, et que l'avant-garde futuriste rejette les « vieux instruments », de nombreux compositeurs ajoutent leur contribution au répertoire violonistique. Le siècle s'ouvre avec le concerto en ré mineur (op. 47) de Jean Sibelius, datant de 1903 et qui restera le concerto du le plus joué et probablement le plus admiré. Il se poursuit avec Sergeï Prokofiev et ses Concerto en ré majeur (1916) et en sol mineur (1935), Georges Enesco et sa Sonate "dans le caractère populaire roumain" (1926) ou Maurice Ravel et sa Sonate pour violon et piano (1922-27) ainsi que Tzigane (1924). Le grand violoniste Fritz Kreisler écrit de nombreuses pièces pour son instrument, notamment son "Praeludium et Allegro", ses "Liebesleid" et "Liebesfreud", le "Tambourin chinois", le "Caprice viennois"… .

Nombreux sont les compositeurs qui s'essayent, avec plus ou moins de bonheur à l'écriture d'un concerto. La production russe est sans doute une des plus importantes : les concertos de Prokofiev sont suivis par celui d'Aram Khatchaturian (1940) ; Chostakovitch en écrit un premier en 1947 et un second vingt ans plus tard. Igor Stravinsky a tenté de renouveler le genre et achevé son concerto, à Nice, en 1931.

Le est aussi le moment où l'on fixe la hauteur du "la3", ou diapason. Celui-ci a considérablement varié au cours du temps : par exemple, entre les différents opéras d'Europe, et entre 1810 et 1860, le diapason a pris des valeurs entre 423 et 452 hertz. L'organisation internationale de normalisation fixe en 1955 le "la" à . Les fluctuations du "la" ont d'importantes conséquences sur les instruments à cordes. En effet, pour obtenir un diapason plus élevé, ce n'est pas l'épaisseur de la corde qui est modifiée mais la tension. La pression exercée sur la table varie ainsi fortement au cours du temps. La montée du "la3" par rapport à celui de l'époque Stradivari entraîne un renforcement du barrage des instruments anciens pour mieux résister à la pression accrue exercée par les cordes.

Mais les années 1950 signent aussi le retour du jeu sur violon baroque (et donc souvent l'emploi d'un "la3" plus bas), avec la formation du Concentus Musicus Wien par Nikolaus Harnoncourt, le pionnier du genre. S'ensuivent les mises en place de la Cappella Coloniensis par le WDR en 1954, du Collegium Aureum (1962) par le label Harmonia Mundi, du Alarius Ensemble Bruxelles (1956) et après 1972 de La Petite Bande (fondations de Sigiswald Kuijken et de son entourage), de l'Academy of Ancient Music (1973)… Les instrumentistes baroques réutilisent des violons de montage baroque qui n'ont pas été modifiés, ou des copies d'après modèles ; suivant les cas, ils y ajoutent les cordes en boyau, l'archet convexe… .

Le violon a rencontré un grand succès partout dans le monde étant donné sa versatilité, sa petite taille et son poids minime. Il a non seulement réussi à intégrer des musiques savantes, mais il a aussi réussi à supplanter des vièles traditionnelles. Il a également suscité une nouvelle attitude des musiciens folkloriques face à la musique écrite. Il a été un trait d'union entre les diverses classes sociales en Europe notamment, où les Tziganes par exemple faisaient le lien entre bourgeoisie et paysannerie. Il a été modifié, adapté, transformé, tant dans sa structure que dans son jeu ou sa tenue, mais il est resté identique et authentique à lui-même finalement.

Dans beaucoup de pays européens, le violon est entré par la petite porte dans la vie musicale, accompagnant la musique à danser populaire, laissant la musique savante à la viole. Grâce à sa large diffusion, il a permis aussi la mise en valeur du patrimoine traditionnel qui accéda à des scènes jusques là réservées à la musique savante.

Finlande : Le violon y a été introduit au dans la musique à danser d'origine centrale européenne (polska, polka, mazurka, scottish, quadrille, valse et menuet) appelée "pelimanni" ou "purppuri" (de « pot-pourri », suite de danse) qui devint le genre principal de la musique finlandaise.

Norvège : C'est aussi au que le violon y apparaît avec la musique à danser continentale appelée "slåtter" ; il y côtoie la vièle "flatfelan". Il a subi quelques transformations pour devenir le "hardanger" (ou "hardingfele") : adjonction de cordes sympathiques, de marqueterie, etc. Il put ainsi continuer à accompagner les musiques rurales lors des réunions festives où les danses de couples avaient cours. Au , la technique de jeu évolua par le retour d'émigrants américains et par l'arrivée de nouvelles danses plus enjouées.

Suède : Le violon y arriva au dans la musiques de danse "gammaldans" importée du continent au fil des siècles ; son développement fut stoppé à l'orée du par les fondamentalistes religieux qui assimilèrent le violon au démon, d'où l'apparition de l'accordéon à sa place dans la "spelmansmusik". Il subit de plus la concurrence de la vièle nyckelharpa.
Lituanie : Depuis le , on trouve dans ce pays un violon fait maison en diverses tailles, avec trois à cinq cordes, le smuikas. Il est accordé en quinte mais parfois en quarte selon les formations folkloriques qu'il intègre. Les musiciens placent parfois une petite pièce de bois sur la table d'harmonie afin de modifier le son.

Lettonie : On le joue depuis le sous le vocable "vijole" dans la "spēlmanis", musique à danser d'origine germanique accompagnée du cymbalum ou de la cornemuse.

Estonie : On le retrouve dans le folklore estonien sous le vocable "viiul".

Royaume-Uni : En Écosse il a vite supplanté le rebec et la vièle médiévale qui s'y trouvait. Aux Îles Shetland, on le tient contre la poitrine où on le tourne pour pouvoir atteindre les diverses cordes avec l'archet. Au Pays de Galles, le "ffidil" a supplanté le "crwth" au mais est resté un instrument populaire sans lettre de noblesse. Il a failli y disparaître sans le soutien de familles tziganes. En Angleterre il fut concurrencé au par la viole de gambe mais trouva dans la musique à danser un répertoire bienveillant.

Irlande : En Irlande, l'instrument se nomme fiddle et est joué par un fiddler ; là aussi c'est dans la musique à danser qu'il trouve son accomplissement. Kevin Burke est un musicien irlandais de renom ayant participé notamment au groupe The Bothy Band.

Les Tziganes et les musiciens Juifs (jouant la musique klezmer) ont su eux développer un style de jeu sophistiqué ; musiciens itinérants, la portabilité de l'instrument fut très vite appréciée. Les danses et les cérémonies de mariages étant très en vogue dans ces pays, le violon s'y est développé dans les campagnes.

Hongrie : Les Tziganes et les Roms se sont très vite intéressés à cet instrument portable et adaptable à tous type de musique. Dès le on trouve des ensembles composés de cymbalum et de violons ("prima" et "kontra"), considérés comme des instruments populaires alors. Des virtuoses tels Elek Bacsik ou Roby Lakatos s'y sont fait une réputation.

Moldavie : On y trouve un violon à sept cordes sympathiques, sans doute influencé par la gadulka, une vièle bulgare.

Pologne : L'instrument a rencontré un accueil très favorable dans ce pays où les danses étaient plébiscitées. Le violon a subi des modifications pour ressembler aux vièles existantes avant son arrivée : "mazanki" (à une frette), "gesliki" et "suka". Le "skrzypce" est taillé dans un bloc de bois monoxyle et a de trois à quatre cordes ; une allumette placée sous les cordes au niveau de la touche fait office de capodastre.

Roumanie : La "vioarâ" est très prisée dans ce pays où les musiciens usent à volonté de "scordatura". Il existe des variations locales : le "contra" n'a que trois cordes ; la "vioarâ cu goarnâ" est un « Stroh violon » ou « violon à pavillon » appelé aussi "lauta (ou hidede) cu tolcer".

Serbie : Le "violina" se joue en trio dans le folklore.

Slovaquie : L"'oktavka" (« violon octave ») et le "shlopcoky" (« violon boîte ») sont des variantes rustiques côtoyant la version originale.

Ukraine : Le "skripka" a la particularité d'être joué essentiellement sur la deuxième ou la troisième corde.

Albanie : Il s'y jouait en duo pendant l'entre-deux-guerres, accompagné d'un tambour sur cadre.

Espagne : La présence du "violin" est attestée depuis le mais de manière discrète, sauf dans les célébrations anciennes de verdiales où une formation appelée "panda" intègre un violon soliste au jeu antiphonique et criard qu'on retrouve uniquement au Mexique.

France : Il y apparaît au dans la musique populaire. On le retrouve notamment en petit ensemble de cordes dans la danse alpine rigaudon mais aussi dans d'autres régions du centre. Jean-François Vrod en est un interprète reconnu. Françoise Étay a publié des études ethnomusicologiques sur la tradition du violon en Auvergne et Limousin.

Grèce : Les Tziganes se sont ici aussi emparés du "violi" avec un accord "alla turca" (Sol - Ré - La - Ré), joué sur la poitrine, ou "alla franca" (Sol - Ré - La - Mi), joué sous le menton. On trouve en Crète des instrument à cordes sympathiques.

Italie : Parallèlement à la musique classique, le violon devait y connaître un destin folklorique, étant joué dans bien des régions pour accompagner la danse. Au , il subit la concurrence de l"'organetto", un accordéon diatonique. L'ensemble "suonatori" rassemble trois violons. Dans le sud du pays, il intègre la formation accompagnant la danse thérapeutique tarentelle.

Portugal : Ce pays a joué un grand rôle dans la dissémination du violon dans ses colonies ou comptoirs. On l'y nomme "viola" pour le différencier des autres vièles rustiques appelées "rebecs".

Le violon a été adopté par les pays du Maghreb (Maroc, Algérie, Tunisie, Égypte) avec l'accord Sol - Ré - Sol - Ré et il a été intégré aux ensembles "takhts" jouant la musique savante arabo-andalouse dès le . Le violon y est joué verticalement et s'y nomme "kamân", "kamanja", "kemala" ou encore "jrâna", remplaçant à volonté la vièle rabâb. Il s'est non seulement parfaitement adapté à l'art de la nouba, mais il a en plus intégré bien des genres semi-classiques voire populaires sous forme d'orchestres de cordes "firqa". Au cours du , en Tunisie et en Égypte, on est revenu à la tenue occidentale, sous le menton.

Les principaux violonistes sont Ridha Kalai, Abdou Dagher et Jasser Haj Youssef. Ce dernier, reconnu également dans le jazz, est le premier violoniste à adapter le jeu du violon arabe sur une viole d'amour.

Le violon a intégré sans changement organologique la musique orientale depuis le , mais sa technique et sa position de jeu ont évolué : on le tient en effet à la verticale sur le genou et les mélismes y sont fréquents. Il s'adapte parfaitement aux contraintes microtonales.

Irak : Le violon appelé "keman" remplace à volonté la vièle djoza au sein des ensembles exécutant les maqâms de la musique arabe savante.

Iran : Le violon est très apprécié ici et il remplace à volonté la vièle kamânche dans les ensembles jouant la musique iranienne savante. Il est sans doute à l'origine de l'ajout d'une quatrième corde au kamânche d'ailleurs. Il existe une grande école de violon, et les compositeurs n'hésitent pas à écrire des concertos selon tel ou tel dastgâh pour lui.

Israël : à la suite de l'immigration massive des juifs ashkénazes, nombre de musiciens talentueux des pays de l'Est se sont retrouvés dans ce pays où le violon accompagne désormais les danses folkloriques.

Turquie : Le violon a été intégré à la musique savante turque et celle des derviches tourneurs (avec la viole d'amour) sous le nom de "keman" ; il remplace à souhait la vièle kemençe dans l'interprétation des makams. Les Tziganes l'utilisent aussi dans la musique populaire.

Le violon est largement joué dans la musique indienne depuis le , surtout dans la musique carnatique mais aussi dans la musique hindoustanie où il subit toutefois la concurrence du sarangi, une vièle traditionnelle au jeu très difficile et qui est souvent réservée aux Musulmans. Certainement importé par des colons portugais ou britanniques (d'où son nom "violon"), à moins qu'il ne soit une invention autochtone (cf. "supra"), il est devenu un véritable instrument classique là-bas aussi ; il est utilisé tant en solo, accompagné d'une percussion (tablâ ou mridangam), qu'en accompagnement des chanteurs ou danseurs. C'est aussi un instrument qui se retrouve souvent dans les maisons indiennes. On en trouve aussi une version folklorique dans l'ancienne province portugaise de Goa où on l'appelle "rebec".

On en joue d'une manière particulière. L'accord est en Sol2 - Ré3 - Sol3 - Ré4 pour le solo et Sol3 - Do3 - Sol3 - Do4 pour le chant masculin (sa - pa - sa - pa). L'instrument est tenu à l'envers, la tête reposant sur la cheville du musicien assis en tailleur, et le tasseau arrière reposant sur la poitrine, laissant ainsi maintenu, la main gauche libre pour exécuter les glissandos ("jâru") si fréquents dans cette musique. On y joue tous les râgas possibles.

Les principaux violonistes sont le L. Subramaniam, M. S. Gopalakrishnan, V. G. Jog, et le N. Rajan. Le frère du premier, L. Shankar (à ne pas confondre avec Ravi Shankar), est aussi un violoniste reconnu dans la world music.

Le violon tend à remplacer peu à peu les vièles esraj et dilruba dans la musique semi-classique ainsi que dans les musiques de film. On le retrouve aussi bien au Sri Lanka où on l'appelle "ravikinna", qu'au Bangladesh.

On trouve le violon de manière très épisodique dans ces anciennes contrées coloniales.

Indonésie : Les Portugais y ont introduit le violon dès le sous le nom de "biola". Il était joué par des esclaves dans les maisons coloniales qui entretenaient des orchestres de chambre. On le voit même dans le gamelan gandrung de Java.

Malaisie : Cette même influence s'est répandue ici ou le violon a intégré les orchestres de Cour sous le nom de "biola" également. Cet instrument accordé à l'européenne intègre aussi des ensembles accompagnant les danses ou théâtres locaux.

Philippines : Les Espagnols ont aussi apporté avec eux le "biyolin" au sein de leur lointaine colonie asiatique, où les musiciens locaux jouent des sérénades de types européens. De là l'instrument s'est aussi répandu dans les ethnies plus reculées, où on l'appelle "gologod" ou "gitgit".

Avec l'immigration massive d'Irlandais, d'Écossais, de Scandinaves, de Slaves, et autres creusets violonistiques, on y retrouve la plus forte concentration de styles de jeu et de danses en tout genre.

Il est joué par un violoniste, en musique classique ou moderne, et par un violoneux en musique traditionnelle du Québec et du Nouveau-Brunswick. Jean Carignan est considéré comme l'un des grands violoneux traditionnels.

Il existe aussi une grande tradition de "fiddler" dans ce pays où on a tendance à jouer sur le bras, la poitrine ou la joue en usant de scordatura.

Les colons espagnols et portugais apportèrent ici aussi leur précieux chargement qui fut bien reçu par les communautés locales.

Guatémala : Depuis le , les Amérindiens Mayas jouent aussi ici du "rabel", un violon rustique à la caisse de résonance taillée dans un bloc de bois monoxyle.

Jamaïque : Le violon connaît un franc succès ici dans la musique à danser, où il est joué avec le style de Floride, tenu contre la poitrine. Il existe un « violon de bambou » tiré d'un segment encore vert de bambou d'où quatre languettes sont détachées à titre de cordes ; l'archet en bambou lui aussi doit être plongé dans l'eau avant de jouer.

Mexique : Le violon a été adopté par certains groupes d'Indiens, mais on le rencontre surtout au sein des orchestres traditionnels mariachis ou huapangos. Le rabel est aussi utilisé par certains Amérindiens.

Panama : Le violon y a finalement remplacé la vièle à trois cordes "rabel".

Argentine : Certains Indiens y ont adopté le violon. Il est aussi joué dans la province de Santiago del Estero pour jouer la danse locale, la zamba, non pas sous le menton, mais contre la poitrine. On les appelle alors "violineros".

Bolivie : On trouve au cœur de la forêt amazonienne des orchestres baroques (formés par les missionnaires) qui ont conservé de façon orale un patrimoine musical unique.

Brésil : On trouve dans le pays la variante "rabeca" issue du Portugal.

Chili : Le "rabel" a trois cordes se rencontre ici aussi.

Colombie : Le "rabel" était joué dans les églises du pays au .

Équateur : Les Amérindiens y jouent également la version rustique appelée "rabel".

Paraguay : Les missionnaires l'ont apporté auprès des Amérindiens qui connaissaient des vièles également. Ils y jouent des pièces créoles et baroques.

Pérou : Les Indiens y fabriquent une vièle en balsa avec deux cordes : le "kitaj".

Le violon déjà présent au début du siècle dans les "strings bands" noirs, fait des débuts timides dans l'histoire du jazz, puisqu'on le trouve parfois dans des orchestres de Jazz Nouvelle-Orléans. Deux courants se développent, un violon rural noir issu du blues, et un violon rural blanc "country" héritier des traditions populaires d'Europe occidentale. Ainsi on voit naître le "western swing" de Bob Wills et le "blue grass". C'est toutefois Joe Venuti, considéré comme « le père du violon jazz » qui fit émerger cet instrument en tant que soliste, notamment par ses duos avec le guitariste Eddie Lang dans les années 1920-1930. Eddie South, Ray Nance et Stuff Smith enrichiront chacun avec leur style propre la palette d'expression du violon jazz américain dans les années 1930.

En Europe se développe avant-guerre une tradition solide du violon jazz avec le danois Svend Asmussen, les Français Michel Warlop et surtout Stéphane Grappelli qui a véritablement montré que le violon pouvait swinger, et est devenu une référence incontestable. « Le violon […] a fait avec lui une entrée fracassante dans l'univers du jazz. L'apport de Grappelli est absolument unique dans l'histoire du jazz comme dans l'histoire du violon. » Malgré le succès et l'influence qu'a exercée Grappelli, l'importance du violon dans le jazz est toutefois restée assez mineure. Quelques musiciens d'origine Tsigane l'utilisent naturellement, par exemple Elek Bacsik, ou plus récemment Florin Niculescu, qui remporte un beau succès "revivaliste", et se pose en héritier de Stéphane Grappelli.

Certains musiciens du free jazz en font une utilisation déstructurante, par exemple Ornette Coleman, qui n'ayant pas de réelle technique sur l'instrument, l'utilise comme moyen d'instabilité. Noël Akchoté l'utilise de façon bruitiste ou pour ses possibilités de longs glissandos.

Dans les années 1970 et avec l'apparition du jazz-rock, Jean-Luc Ponty a un très grand succès commercial en utilisant un violon amplifié, et divers effets sonores, puis un violon électrique. Des compositeurs comme John McLaughlin, Frank Zappa, contribuent à élargir l'utilisation de cet instrument.

Bien que le violon reste marginal par rapport aux instruments "traditionnels" du jazz comme le saxophone ou la trompette, il existe aujourd'hui de nombreux instrumentistes de talent, comme Didier Lockwood, Regina Carter, Adam Taubitz… .

Certains violonistes explorent de nouveaux horizons, comme en témoigne la musique de Jasser Haj Youssef qui réunit le jazz et la musique orientale avec subtilité.

Plus récemment, le jazz contemporain s'empare de l'instrument, et l'utilise d'une manière beaucoup plus proche du classique et de la musique contemporaine, en utilisant ses capacités d'expressivité mélodique, et ses possibilités de techniques de jeu étendues, en particulier les harmoniques. On peut citer Dominique Pifarély, Mark Feldman, Régis Huby… .

Si les tentatives de réintroduire le violon dans le rock restent relativement rares, ses potentialités restent intactes, comme en témoigne le double disque d'or obtenu par le groupe Louise Attaque avec un jeu qui tient largement la place à la fois rythmique et mélodique de la guitare et avec des harmonies et des effets qui ne tombent pas dans le bluegrass ou le free jazz.

On peut retrouver également le violon dans certains groupes de folk metal : Ithilien, Korpiklaani, Mago de Oz, Eluveitie, Turisas, Cruachan, Niflheim… Le violon sert alors à évoquer des univers féériques, mythologiques et/ou médiévaux, et il est souvent accompagnés d'autres instruments traditionnels (flûtes, mandoline, violoncelle, cornemuse…) en plus des instruments plus "classiques" utilisés dans la musique metal (voix, guitare, basse, batterie).

Le violon a eu plusieurs descendants, que ce soient des instruments conçus dans une optique d'amélioration du son, ou que ce soient des innovations destinées à utiliser les nouveaux matériaux et techniques.

Félix Savart mit au point au un violon à caisse de résonance trapézoïdale, pourvu d'ouïes rectilignes. La forme de la caisse se justifiait par l'obstacle à la propagation du son que représentait la forme en voûte de la table, tandis que celle des ouïes avait pour but de restreindre la perte de surface causée par la forme de ces ouvertures en ƒ au niveau de celles-ci, évaluée à un tiers. Tolbecque juge que ce violon ne ressemblait qu'à un « vulgaire soufflet de cuisine », et que « malheureusement, au point de vue du son, [il] ne devait pas être mieux réussi ». Un exemplaire en est conservé à l'École polytechnique de Palaiseau.

Le violon proposé par François Chanot en 1819, s'il conservait plus la forme globale de l'instrument, était aussi fort différent du modèle. Les table et fond n'avaient aucun angle au niveau des échancrures, les ouïes étaient des fentes de largeur constante qui suivaient les bords de la table, la tête était retournée afin de faciliter la mise en place de la deuxième corde, et les cordes ne s'attachaient plus au cordier mais directement dans la table. Ce dernier point avait pour conséquence l'arrachement de la table, et après des critiques fort élogieuses quant à la sonorité, comparée à celle d'un Stradivarius, et quant au prix (cent écus), l'instrument ne fut pas adopté massivement.

Suleau partit de l'observation que pour augmenter le volume sonore du violon, il fallait agrandir la surface vibrante. Ne pouvant ni trop élargir la caisse, ni l'approfondir démesurément, ni modifier sa longueur à cause des habitudes des violonistes, il décida de creuser des sillons, orientés perpendiculairement au sens des fibres, tout en maintenant une épaisseur de table constante, ce qui donnait à la table vue de profil l'aspect d'une succession de vagues régulières. Les résultats sonores n'étant pas à la hauteur de ses attentes, il essaya de mettre les sillons dans le même sens que les fibres du bois, mais sans succès.

Contraction de "Lata" (boîte de conserves) et "violín" (violon, en espagnol), ce dérivé a été fabriqué pour l'ensemble d'instruments informels argentin Les Luthiers.

Ce n'est pas un nouvel instrument, mais la parodie d'un violon, dont le premier prototype date de 1968. Il a de vrais composants de violon: le chevalet, la touche, des chevilles et les cordes. C'est le corps, fait avec une boîte de conserves (jambon ou biscuits), qui donne l'originalité de la parodie. À cause de son faible son, il a besoin d'être amplifié.

Il est habituellement exécuté par le chef d'orchestre et compositeur Carlos López Puccio, que l'on peut voir jouer de l'instrument ici.

Augustus Stroh conçut et breveta en 1899 un violon sans table, ainsi décrit :
Le chevalet est placé de manière à transmettre les plus légères vibrations à un levier ; ce levier est lui-même en communication avec un diaphragme d'aluminium, non uni. Ce diaphragme est la partie principale du violon ; c'est lui qui donne au son la force nécessaire ; il est fixé par deux coussinets de caoutchouc au bâti du violon. Près du diaphragme s'ouvre un pavillon métallique qui sert à renforcer les sons.
Qualifié à ses débuts de « futur roi de l'orchestre », le violon à pavillon fut utilisé quelque temps pour les enregistrements phonographiques, sa puissance résolvant le problème des microphones peu sensibles. Son usage n'a ensuite probablement pas cessé de se restreindre, puisque les témoignages à son sujet, au-delà des premières années, sont rares, et l'on ne dispose pas de données permettant d'évaluer combien d'exemplaires sont actuellement joués. Il a connu un succès en Roumanie.

Auguste Tolbecque explique dans son ouvrage qu'il a fabriqué un violon dont les ouïes sont situées sur les éclisses, au niveau des échancrures, ceci toujours dans l'optique d'éviter de perdre un tiers de la surface de la table au niveau du chevalet. Cependant, on ne dispose pas de plus de données quant à son usage.

Dans la seconde moitié du a été mis au point le violon à table pleine et à amplification électrique, selon le même principe que la guitare électrique à corps plein inventée en 1942. Il a notamment été utilisé en jazz par Jean-Luc Ponty et Laurie Anderson, en variété par la chanteuse Catherine Lara et par la jeune interprète Vanessa-Mae et dans la world music par L. Shankar qui dispose d'un violon stéréophonique à dix cordes et double manche.

Vers les années 1990, des violons en fibre de carbone ont été mis au point avec un avantage considérable : leur prix peu élevé. Ils ne servent le plus souvent que de violons d'étude car bien que des musiciens les eussent choisis pour les concerts pour leurs qualités de puissance, de clarté et d'intelligibilité, ils les trouvent à la longue ennuyeux, à cause d'un son « plat », toujours le même quelle que soit la nuance de jeu, sans expressivité. Ces violons sont aisément reconnaissables grâce à leur table noire qui comporte un fin quadrillage sombre.

En 2002, la firme Yamaha présente sa gamme Silent, où l'on trouve entre autres des violons dits silencieux (moins sonores est plus exact) car privés de caisse de résonance. L'amplification est assurée par un système électronique, auquel on a ajouté un préamplificateur. L'encombrement est légèrement réduit en épaisseur et en largeur grâce à des arceaux démontables, ce dernier point étant beaucoup plus flagrant sur les contrebasses de la gamme, qui peuvent faire de large une fois partiellement démontées. Les violons de cette gamme coûtent environ .

Gildas Bellego a mis au point un violon formé d'une table en épicéa et d'un fond et d'éclisses en fibre de carbone et polyéthylène, la caisse étant sans angles au niveau des table et fond comme dans le violon Chanot, mais également au niveau des jointures fond-éclisses et éclisses-tables. Le moulage de ce "fond étendu" diminuant le nombre de pièces à monter à 15, le prix diminue également, à .

Enfin, la firme américaine QRS a construit « Virtuoso Violin », un violon qui joue seul les partitions au format MIDI, grâce à un système mécanique pour l'archet et un système électromagnétique pour la détermination de la hauteur des notes.






Avant de jouer, on met de la colophane sur l'archet. Or, en jouant, celle-ci se détache de l'archet sous forme d'une fine poussière blanche, qui se dépose sur la table d'harmonie, entre le chevalet et la touche. Après avoir joué, il faut donc nettoyer la zone avec un chiffon sec de soie ou de coton. L'emplacement de la poussière de colophane est un bon indicateur du placement global de l'archet. Le jeu est incorrect lorsque l'on joue trop sur la touche, ce qui est difficile à voir sans miroir quand on exécute un morceau, mais très simple à constater grâce à l'emplacement de la poussière de colophane.

À cause des ouïes, l'intérieur de la caisse de résonance du violon communique avec l'air extérieur : la poussière entre donc librement dans l'instrument. Il faut donc régulièrement nettoyer l'intérieur de la caisse en y introduisant quelques grains de riz (non cuit pour éviter d'empâter l'intérieur de la caisse). Quand on agite le violon, les grains font s'agglomérer la poussière en moutons qui ressortent ensuite aisément par les ouïes.

Le bois du violon craint les changements de température et de taux d'humidité. Il est impossible de "sécher" le violon si le taux d'humidité augmente (les sachets de poudre séchante sont inappropriés), mais la manœuvre contraire est réalisable grâce à des humidificateurs à placer dans la boîte de l'instrument (petit flacon d'eau percé de trous) ou directement dans la caisse de résonance (tube de plastique troué contenant de l'éponge que l'on a imbibée d'eau). Il est conseillé de maintenir le violon à une température comprise entre 16 et , et à un taux d'humidité entre 40 et 65 %.

Les cordes métalliques sont sujettes à l'usure à la fois mécanique (frottement des doigts, particulièrement à cause des démanchés) et chimique (sorte de rouille, à cause de la sueur) due au jeu. Il faut les nettoyer, elles et la touche, de la graisse laissée par la sueur des doigts, en utilisant de l'alcool ou de l'Eau de Cologne. Il faut aussi les changer régulièrement, la corde de "mi" étant la plus touchée du fait de son faible diamètre, la corde de "sol" étant au contraire assez résistante au problème. Une corde de "mi" est ainsi changée tous les mois quand on joue quotidiennement quelques heures, tandis qu'un "sol" peut tenir trois mois avant que l'altération soit vraiment sensible à l'oreille. En effet, une corde usée devient difficile à accorder aux autres, semblant sonner toujours faux quand on en joue à vide ; c'est un signe tardif, postérieur au « seuil » d'usure réellement convenable, et nécessitant le changement immédiat de la corde. Une corde largement trop usée peut « claquer », c'est-à-dire se briser brusquement (par exemple sous l'effet de la chaleur, d'un trop brusque coup d'archet…).
On conserve un violon dans une boîte dont la forme et le matériau peuvent varier. Cette boîte contient nécessairement le violon, l'archet, l'épaulière, la colophane, un chiffon doux pour l'entretien et des cordes de rechange. Elle peut contenir également, selon les cas, les partitions, d'autres archets, un métronome, un hygromètre, un humidificateur, de la craie pour l'entretien des chevilles, une sourdine… .

Le luthier peut réparer des fractures de la table ou du fond.

Les déformations de la voûte sont corrigées grâce à une mise sous presse de la table avec un moule ayant exactement la forme à donner à la table pendant vingt-quatre heures.

Le doublage consiste à coller une pièce de bois supplémentaire à une partie de l'instrument devenue trop mince et trop fragile. Plusieurs doublages sont possibles. Dans tous les cas, l'opération n'a lieu que sur une table saine, c'est-à-dire dont les fractures ont été réparées et la forme de la voûte corrigée.

La sueur abîme le vernis et peut donc rendre nécessaire le changement d'une partie d'éclisse à droite du manche. On construit donc la nouvelle partie d'éclisse, que l'on courbe ; puis on amincit aux abords du collage les deux pièces, de manière à les faire se chevaucher en épaisseur, ce qui donnera de la solidité à la réparation. Les éclisses peuvent aussi être rehaussées si leur trop faible hauteur nuit à la puissance sonore de l'instrument.

Les fractures qui ont été réparées sont souvent soutenues par des taquets, petites pièces de bois identiques à celles posées sur le joint du fond lors de la fabrication du violon. Leur nombre ne doit cependant pas être trop important, car de toute évidence ils gênent la propagation du son.

Les chevilles, sous la traction des cordes, peuvent déchirer leurs emplacements. Si l'on tient à conserver la tête pour sa beauté, la difficulté est de conserver le haut des emplacements (appelés joues du chevillier) en y adjoignant une nouvelle pièce pour le bas.

Si le manche est défectueux, mais que l'on conserve la tête, on pratique une enture du manche : la tête est encastrée dans le manche, passant sous lui.

Enfin, toutes les pièces neuves sont de couleur différente des pièces originales car elles n'ont pas été vernies. Le luthier effectue donc des raccords de vernis, avec une base peu colorée (pour qu'elle ne s'impose pas à la couleur originale), à laquelle il ajoute petit à petit les colorants. Puis il le polit et essuie l'instrument avec un lainage imprégné d'huile de lin, afin de redonner à l'ensemble un aspect net et brillant.

La plasticité cérébrale est la capacité du cerveau à modifier sa structure ou son fonctionnement après sa mise en place au cours de l'embryogenèse. Plusieurs travaux scientifiques se sont servis de l'exemple du violon pour illustrer cette propriété.

La zone du cerveau qui commande les mouvements fins de la main est l'opercule pariétal. Celle-ci est particulièrement impliquée dans le jeu du violon. Cependant, des analyses de 1999 du cerveau d'Albert Einstein, conservé par Thomas Stoltz Harvey, analyses effectuées par une équipe de l'Université McMaster, ont montré qu'Einstein n'avait pas d'opercule pariétal, et qu'un mécanisme de compensation s'était mis en place, lui accordant un lobe pariétal inférieur d'une taille plus grande que la moyenne de 15 %.

La représentation corticale des doigts de la main gauche chez un violoniste, obtenue par imagerie par résonance magnétique fonctionnelle, montre une augmentation de la zone corticale activée par des stimuli de ce doigt. La taille de la zone du cortex consacrée à l'auriculaire devient similaire à celle du pouce, ce qui n'est pas le cas du non-violoniste. Cependant, ces modifications, liées à l'apprentissage, varient en fonction de l'âge d'acquisition : l'agrandissement de cette zone est plus importante chez les individus ayant commencé la pratique du violon avant l'âge de 13 ans ; cette surface atteint son maximum chez les artistes qui ont débuté le violon avant l'âge de cinq ans ; selon certains, elle reste cependant importante chez ceux ayant commencé plus tard alors que d'autres considèrent que ceux qui ont commencé l'apprentissage après 7 ans ne présentent pas de différences significatives avec des non-musiciens.

L'effet de l'augmentation de taille de la zone de représentation sensorielle de la main gauche est une dextérité accrue : le violoniste est capable de placer ses doigts dans des positions différentes tous les dixièmes ou vingtièmes de seconde, avec une précision de quelques dixièmes de millimètre, quand le non-violoniste les place tous les quarts ou demies secondes et avec une précision d'un millimètre. Le violoniste confirmé est capable de corriger la justesse d'une note en un dixième de seconde, au quart de ton ; il peut, dans un mouvement rapide, jouer 12 notes à la seconde, il les anticipe alors d'au moins 700 millisecondes.

Le syndrome de la gouttière cubitale se produit lorsqu'il y a compression du nerf cubital, soit lors de son passage dans la gouttière entre l'olécrane et l'épitrochlée, soit lorsqu'il passe dans l'avant-bras proximal enchâssé dans le canal cubital entre des structures musculaires et ligamentaires. Le coude le plus susceptible d'en être atteint est le coude du bras qui tient le manche du violon. Cependant, le coude du bras tenant l'archet peut également être atteint de ce syndrome à cause des mouvements répétitifs de flexion et d'extension. Dans les deux cas, les symptômes sont les mêmes : douleurs dans l'avant-bras, dans les quatrième et cinquième doigts, sensation d'engourdissement de ces zones et de faiblesse lors des mouvements.

Le musicien atteint d'une dystonie de fonction n'arrive plus à contrôler le mouvement d'un ou de plusieurs doigts : d'après la définition de Raoul Tubiana, il subit des « contractions musculaires passagères, involontaires, non douloureuses, entraînant une incoordination de ses mouvements, uniquement lors d'un passage musical bien déterminé, troubles qui persistent malgré l'effort qu'il fait pour les corriger. » En général, la récupération fonctionnelle à un haut niveau technique n'est qu'exceptionnelle, et la guérison totale n'est pas possible.

Les tendinites particulières au violoniste touchent les extenseurs ou les fléchisseurs des doigts, la partie externe du coude, ou l'épaule. Celles-ci sont caractérisées, essentiellement, par une douleur le long du trajet du tendon concerné.

Le syndrome de compression vasculo-nerveux (ou syndrome du canal carpien) provoque des fourmillements au niveau des doigts et un manque de sensibilité digitale. Le canal carpien, gaine située dans la face intérieure de la main, renferme les tendons fléchisseurs des doigts et le nerf qui leur permet d'être sensibles.

On peut aussi voir des névrites douloureuses des nerfs digitaux dues à une irritation mécanique. Des troubles globaux de la main sont à craindre, et leurs causes courantes sont les mauvaises positions, une pratique intensive, un changement de technique, une hygiène de vie insuffisante et l'anxiété.





</doc>
<doc id="6644" url="https://fr.wikipedia.org/wiki?curid=6644" title="Mohamed Boudiaf">
Mohamed Boudiaf

Mohamed Boudiaf (en ), né le à M'Sila et assassiné le à Annaba, est un homme d'État algérien. Il est président de l'Algérie du au .

Fonctionnaire de profession, membre fondateur du Front de libération nationale (FLN), un des chefs de la guerre d'indépendance algérienne et membre du Gouvernement provisoire de la République algérienne (GPRA), au poste de ministre d'État de 1958 à 1961 puis vice-président jusqu'en 1962, il entre en opposition contre les premiers régimes mis en place à l'indépendance de son pays, et s'exile durant près de 28 ans au Maroc. Rappelé en Algérie, en 1992 en pleine crise politique, la dissolution de l'APN par le président Chadli Bendjedid, la proclamation de l'état d'urgence et la démission celui-ci le 11 janvier 1992. L'établissement d'un Haut Comité d'État de cinq membres. Le conseil le nomme président de la République algérienne démocratique et populaire, du jusqu'à son assassinat lors d'une conférence des cadres à Annaba le .

Mohamed Boudiaf est né le 23 juin 1919 à Ouled Madi dans l'actuelle wilaya de M'Sila en Algérie. Après avoir effectué ses études à M'sila, il devient fonctionnaire dans l'administration. Adjudant dans l'armée française en 1942 pendant la Seconde Guerre mondiale, il est commis au service des contributions à Djileli, puis est envoyé sur le front en Italie où il participa à la bataille de Monte Cassino, ainsi que Krim Belkacem qui était caporal, Larbi Ben M'Hidi qui était sergent et Rabah Bitat. Cependant ces hommes, qui servaient dans des divisions différentes, ne se connaissaient pas à cette époque. Après les massacres de Sétif de 1945, il s'engage dans les mouvements nationalistes algériens, et adhère au Parti du peuple algérien (PPA) de Messali Hadj, puis participe à la création de l’Organisation spéciale (OS), branche armée secrète du Mouvement pour le triomphe des libertés démocratiques (MTLD). Vers la fin de 1947, il en constitue une cellule pour le département de Constantine. L'OS est démantelée par la police française en 1950, et avec les autres membres dirigeants de l'organisation, il est jugé et condamné par contumace pour ses activités militantes. En 1952, il est muté en France par le MTLD où il milite au sein de la communauté immigrée algérienne.

Il rentre en Algérie en mars 1954 et crée, avec huit autres militants, qui devinrent les « chefs historiques du FLN », avec pour objectif l'indépendance de l'Algérie par la lutte armée, le Comité révolutionnaire d'unité et d'action (CRUA) dont il est élu président lors de la réunion .

Après l'échec du CRUA, il fait partie, une nouvelle fois comme coordonnateur général, du « groupe des 22 », qui organise la préparation de la lutte armée désormais certaine. Titulaire de la carte 1 du Front de libération nationale (FLN), créé pour rassembler dans la lutte les différentes forces nationalistes, il est décidé comme date du déclenchement des « hostilités » le novembre 1954 — date qui marque le début de la guerre d'Algérie.

À l'issue du Congrès de la Soummam, en août 1956 il devient membre du CNRA (Conseil national de la révolution algérienne). Le 22 octobre 1956, il est arrêté, avec d'autres chefs du FLN, par l'armée française à la suite du détournement de l’avion civil marocain qui le menait vers la Tunisie. Il dirige alors depuis sa prison la fédération de France du FLN et est nommé en 1958 ministre d’État du Gouvernement provisoire de la République algérienne (GPRA), à sa création, puis vice-Président en 1961. Il est libéré le 18 mars 1962 après les accords d'Évian.

À l'indépendance en juillet 1962, il entre en désaccord avec Ben Bella, soutenu par le commandement de l'Armée de libération nationale (ALN) de l'extérieur, qui crée un bureau politique du FLN pour remplacer le GPRA. Le 20 septembre 1962, alors que le bureau politique constitue la première assemblée nationale algérienne, Mohamed Boudiaf fonde en opposition son propre parti, le Parti de la révolution socialiste (PRS). Le 23 juin 1963, il est arrêté sur le pont d'Hydra, puis séquestré à Tsabit dans le Sud algérien où il entame une grève de la faim avec ses compagnons de cellule. Il sera détenu avec 3 autres prisonniers dont Mohand Akli Benyounes durant plusieurs semaines avant d'être transféré vers Saïda, où il retrouvera Salah Boubnider en prison. Il réussit à faire passer une lettre à sa famille où il dénonce sa séquestration ; l'affaire est médiatisée. Il est transféré une dernière fois près de Sidi Bel Abbes. L'exil vers la Suisse lui est proposé mais il refuse. Il prend position contre la nouvelle constitution et la politique du régime. Condamné à mort en 1964 par le régime Ben Bella, il quitte l'Algérie et rejoint la France puis le Maroc. Il œuvre au sein de son parti, et anime à partir de 1972 entre la France et le Maroc plusieurs conférences où il expose son projet politique pour l'Algérie, et anime la revue "El Jarida". Son livre "Où va l'Algérie", qui livre un témoignage lucide sur l'après-indépendance et la prise du pouvoir par les militaires, résume ses propositions politiques. En 1979, après la mort de Houari Boumedienne, il dissout le PRS et va se consacrer à ses activités professionnelles en dirigeant à Kénitra au Maroc une briqueterie.

Le 16 janvier 1992, après la démission du président Chadli Bendjedid (au soir du 11 janvier), il revient en Algérie. Alors que le FIS, parti islamiste, emporte une large majorité au des élections législatives, Chadli Bendjedid, après avoir dissout l'Assemblée nationale et laissé un vide constitutionnel, démissionne et le Haut conseil de sécurité (HCS) annule les élections. Mohamed Boudiaf est rappelé en Algérie pour devenir le président du Haut Comité d’État, en charge provisoire des pouvoirs de chef de l'État. Par son long exil, il apparaissait en effet paradoxalement comme un homme neuf, non impliqué dans les tribulations du régime algérien et donc susceptible de sortir le pays de l’impasse. Souhaitant une Algérie démocratique tournée vers la modernité, il disait vouloir mettre fin à la corruption qui gangrenait l'État.

Le 29 juin 1992, Mohamed Boudiaf est assassiné au cours d'une conférence des cadres qu'il tenait dans la ville d'Annaba (ancienne Bône). Un sous-lieutenant du Groupement d'intervention spécial (GIS), Lambarek Boumaarafi, jeta une grenade pour faire diversion et tira à bout portant sur le président le tuant sur le coup. La motivation de son assassinat est sujette à controverse, entre la piste d’une action isolée commise par un militaire ayant des sympathies islamistes et celle d’un complot plus vaste impliquant des généraux de l'armée. Sans faire la lumière sur l'assassinat de Boudiaf, la commission d’enquête instituée par le gouvernement algérien écarte la thèse de l’« action isolée » d’un officier de l’armée ayant agi pour des motifs strictement religieux.







</doc>
<doc id="6659" url="https://fr.wikipedia.org/wiki?curid=6659" title="Guerre d'Algérie">
Guerre d'Algérie

La guerre d’Algérie ou révolution algérienne (en arabe : الثورة الجزائرية‎‎ "Al-thawra Al-Jazaa'iriyya", en berbère : Tagrawla Tadzayrit) aussi connue comme la guerre d'indépendance algérienne, ou guerre de libération nationale, est un conflit armé qui s'est déroulé de 1954 à 1962 en Algérie, colonie française depuis 1830, divisée en départements depuis 1848. L'aboutissement est la reconnaissance de l'indépendance du territoire le 5 juillet 1962.

En tant que guerre d'indépendance et de décolonisation, elle oppose des nationalistes algériens, principalement réunis sous la bannière du Front de libération nationale (FLN), à la France. Elle est à la fois un double conflit militaire et diplomatique et aussi une double guerre civile, entre les communautés d'une part et à l'intérieur des communautés d'autre part. Elle a lieu principalement sur le territoire de l'Algérie française, avec également des répercussions en France métropolitaine.

Elle entraîne de graves crises politiques en France, avec pour conséquences le retour au pouvoir de Charles de Gaulle et la chute de la Quatrième République, remplacée par la Cinquième République. Après avoir donné du temps à l'armée française pour lutter contre l'ALN en utilisant tous les moyens à sa disposition, De Gaulle penche finalement pour l'autodétermination en tant que seule issue possible au conflit, ce qui conduit une fraction de l'armée française à se rebeller et entrer en opposition ouverte avec le pouvoir, rapidement matée. 

La guerre d'Algérie présente un bilan lourd et les méthodes employées durant la guerre par l'armée française (torture, répression de la population civile algérienne) furent controversées. Plus de 300 000 d'Algériens sont tués dans cette guerre, et jusqu'à 3 000 000 envoyés dans des camps de regroupements (sur une population de 10 000 000 de personnes). Le conflit débouche, après les accords d'Évian du , sur l'indépendance de l'Algérie le 3 juillet suivant, et précipite l'exode des habitants d'origine européenne, dits Pieds-Noirs et des Juifs, ainsi que le massacre de plusieurs dizaines de milliers de harkis.

Le terme officiellement employé à l'époque par la France était « événements d'Algérie », bien que l'expression « guerre d'Algérie » ait eu cours dans le langage courant. L'expression « guerre d'Algérie » a été officiellement adoptée en France le .

La guerre d'Algérie prend place dans le mouvement de décolonisation qui affecta les empires occidentaux après la Seconde Guerre mondiale. Elle s'inscrit dans le cadre du combat anti-impérialiste et conduira au terme d'une Histoire sociale de l'Algérie française parfois antagoniste.

Elle oppose principalement le Front de libération nationale (FLN), à l'origine de l'insurrection, et sa branche armée l'Armée de libération nationale (ALN, constituée de "moudjahidines", "djoundis", "moussebilines", etc.) à l'armée française (comptant troupes d'élite (parachutistes, légionnaires), goums marocains jusqu'en 1956, gardes mobiles, CRS, appelés du contingent ou supplétifs musulmans).

Entre 1952 et 1962, ou rappelés et d'active (soit ) ont été envoyés en Algérie. Près de algériens (réguliers et supplétifs) ont également combattu du côté français pendant la guerre d'Algérie (d'autres chiffres, « gonflés », ont été lancés à des fins de propagande).

Le conflit se double d'une guerre civile et idéologique à l'intérieur des deux communautés, donnant lieu à des vagues successives d'attentats, assassinats et massacres sur les deux rives de la Méditerranée. Côté algérien, elle se traduit par une lutte de pouvoir qui voit la victoire du FLN sur les partis algériens rivaux, notamment le Mouvement national algérien (MNA), et par une campagne de répression contre les Algériens pro-français soutenant le rattachement de l'Algérie à la République française. Par ailleurs, elle suscite côté français l'affrontement entre une minorité active hostile à sa poursuite (Libéraux d'Algérie, mouvement pacifiste), une seconde, favorable à l'indépendance (les « porteurs de valises » du Réseau Jeanson, le Parti communiste algérien), et une troisième, voulant le maintien de l'« Algérie française » (Front Algérie française, Jeune Nation, Organisation armée secrète (OAS)).

Selon Guy Pervillé, le nombre d'Algériens engagés dans l'un et l'autre camp (partisans de la présence française et FLN) serait du même ordre de grandeur.

Cette guerre s'achève à la fois sur la reconnaissance de l'indépendance de l'Algérie le , lors d'une allocution télévisée du général de Gaulle faisant suite au référendum d'autodétermination du juillet prévu par les accords d'Évian du , sur la naissance de la République algérienne démocratique et populaire, le 25 septembre, et sur l'exode d'une grande partie des Pieds-Noirs (au nombre d'un million).

Contrairement à des colonies de peuplement telles les États-Unis (Amérindiens) ou l'Australie (Aborigènes d'Australie), la population indigène diminue sensiblement entre 1830 et 1868 puis croît fortement durant la colonisation française de l'Algérie entre 1880 (environ 3 millions de musulmans, pour environ non-musulmans) et 1960. À cette date, l'Algérie compte environ 9,5 millions de musulmans et environ 1 million d'Européens non-musulmans dont juifs séfarades.

, mais la population musulmane urbaine progresse pendant toute la première moitié du . En 1954, certaines villes sont à majorité musulmane comme Sétif (85 %), Constantine (72 %) ou Mostaganem (67 %).

Pour la période 1950-1954, l'espérance de vie à la naissance de la population algérienne musulmane est la moitié de celle de la population européenne (respectivement 34 et 60 ans pour les hommes et 33 et 67 ans pour les femmes). Selon la Banque Mondiale, elle s’établit à 46 ans en 1960 pour la moyenne de l'ensemble des populations. La mortalité infantile est très élevée en Algérie. Elle diminue fortement pour les populations européennes entre 1946 et 1954 (environ 50 pour 1000), mais reste très forte pour les musulmans (environ 85 pour 1000 en 1954).

En 1954, la population algérienne est divisée en deux catégories distinctes, soumises à des statuts juridiques inégaux nés du sénatus-consulte du 14 juillet 1865 : d'une part, un million d'Européens, citoyens français de statut civil de droit commun (surnommés plus tard les « Pieds-Noirs ») qui étaient installés en Algérie souvent depuis plusieurs générations et auxquels étaient associés les juifs autochtones (excepté pour la période du statut des Juifs de 1940 à 1943 avec l'abrogation du décret Crémieux), et d'autre part, près de neuf millions d'Algériens, sujets français de statut personnel de droit local (appelés « Musulmans » ou « indigènes »).

Cependant, si les citoyens français jouissaient exactement des mêmes droits et devoirs que leurs compatriotes métropolitains, les sujets algériens qui étaient soumis aux mêmes devoirs (ils étaient notamment mobilisables par le contingent), étaient privés d'une partie de leurs droits civiques (ils votaient au Second collège électoral où il fallait neuf de leurs voix pour égaler la voix d'un seul votant du Premier collège).

L'arrivée au pouvoir de Charles de Gaulle en 1958 et la promulgation des ordonnances du 15 novembre 1958 uniformise le statut des populations d'Algérie par l'adoption du collège unique.

En Algérie, depuis les années 1930, près d'un million de Pieds-Noirs y vivent dont quelques milliers possèdent les meilleures terres agricoles.

De nombreux agriculteurs européens sont des viticulteurs ( consacrés à la vigne en Algérie) dont les productions sont exportées surtout vers la France métropolitaine qui l'achète assez cher (selon l'historien Daniel Lefeuvre) pour maintenir le niveau de vie des colons. L'agriculture n'occupe que 9 % de la population active française (contre 26 % en métropole) mais les paysans d'origine française occupent l'essentiel des meilleures terres cultivables. Cependant certaines sources certifient que ne sont attribuées aux colons que des terres alors en friche. Le colon en réclame autant qu'il pense pouvoir en cultiver avec sa famille, mais doit dès la deuxième année acquitter l'impôt foncier proportionnel à la surface, ce qui dissuade les abus.

L'essentiel de la population musulmane est pauvre. Ce sont essentiellement de petits propriétaires terriens vivant sur les terres les moins fertiles, ou des journaliers. Dans les années 1950, les surfaces cultivables stagneraient autour de d'hectares. La production agricole augmente peu entre 1871 et 1948, contrairement au nombre d'habitants. Selon Daniel Lefeuvre, la production annuelle de céréales passe de à L'Algérie doit donc importer des produits alimentaires.

Le chômage est important, 1,5 million de personnes sont sans emploi en 1955. La commune d'Alger aurait compté avec en 1953.

Si la population musulmane est majoritairement pauvre, Daniel Lefeuvre rapporte qu'environ musulmans « appartiennent aux groupes sociaux les plus favorisés » (grands propriétaires fonciers, professions libérales, membres de l'armée et de la fonction publique).

D'une manière générale, l'Algérie, loin de présenter une source économique avantageuse, est un lourd fardeau pour la métropole et ses contribuables.

Après la fin de la Seconde Guerre mondiale, le plan Marshall prévoit une aide économique à la France et l'Algérie.

Le 18 avril 1951, la France signe le traité instituant la Communauté européenne du charbon et de l'acier (CECA). Le 27 mai 1952, le traité instituant la Communauté européenne de défense (CED) est adopté par le gouvernement français (mais ne sera pas ratifié par le Parlement). Le juin 1955 se tient la conférence de Messine préparant le traité de Rome du 25 mars 1957 qui institue la Communauté économique européenne, prélude à l’Union européenne d’aujourd’hui, née le 7 février 1992.

À la suite de la Seconde Guerre mondiale, la France s'engage résolument dans une politique européenne qui dessine l'avenir de la nation. Au début de la guerre d'Algérie, des forces politiques encore puissantes essayent de maintenir ce qui reste de l'Empire colonial français.

Le conflit s'inscrit dans le cadre du processus de décolonisation qui se déroule après la fin de la Seconde Guerre mondiale. Pour la France, cela concerne entre autres les colonies françaises d'Indochine (guerre d'Indochine de 1946-1954), la Guinée, Madagascar (insurrection malgache de 1947), l'Afrique-Équatoriale française et l'Afrique-Occidentale française, ainsi que les protectorats du Maroc qui obtient son indépendance le 2 mars 1956 et de la Tunisie le 20 mars 1956 .
La principale cause du déclenchement de cette guerre réside dans le blocage de toutes les réformes, dû au fragile équilibre du pouvoir sous la République, et à l'opposition obstinée de la masse des Pieds-Noirs et de leurs représentants hostiles à toute réforme en faveur des musulmans, tout comme celle des Algériens : ainsi, la loi sur le nouveau statut de l'Algérie, proposée en 1947, n'est votée ni par les députés du colonat, ni par les quinze représentants des « Français musulmans » d'Algérie.

Alors que des dizaines de milliers d’habitants de l'Algérie française, estimés à combattants, ont participé à la libération de la France et que plusieurs intellectuels revendiquent l’égalité des droits, les habitants musulmans de l'Algérie française sont à l'époque considérés comme des citoyens de second ordre, alors même que le régime de l'indigénat est abrogé en théorie en 1945.

En 1947, l'application du nouveau statut de l'Algérie fut presque ouvertement faussée par l'administration, qui fit arrêter les « mauvais » candidats et truqua les résultats en faveur des intransigeants, au point que certains furent élus çà et là par plus de 100 % des inscrits.

Pendant les douze mois qui précédèrent le déclenchement du novembre, ce ne sont pas moins de 53 attentats (« anti français ») qui furent commis.

En 1960, (appelés, engagés, militaires d'active, appelés FSNA ou Français de souche nord-africaine) servaient dans l'Armée régulière plus environ (, , et ) soit au total près de combattant aux côtés des soldats français.

Au total, un peu plus de FSNA furent incorporés dans l'armée régulière de 1956 à 1961.

Le 19 mars 1962, jour du cessez-le-feu, selon le rapport à l'ONU du contrôleur général aux armées Christian de Saint-Salvy, on dénombrait en Algérie, engagés du côté français ( (FSNA), dont et francophiles) représentant, familles comprises, plus de de personnes menacées sur de musulmans algériens.

L'Armée française recruta également environ éléments du FLN et de l'ALN dont certains formèrent le célèbre Commando Georges du lieutenant Georges Grillot. La plupart d'entre eux furent victimes de représailles à partir de 1962.

Selon Maurice Faivre, on comptait ainsi quatre fois plus de combattants musulmans dans le camp français que dans celui du FLN.

Au début du , plusieurs dirigeants algériens exigent de la France le droit à l'égalité ou à l'indépendance.

Plusieurs partis vont être créés et plusieurs pamphlets seront écrits pour défendre le droit pour les Algériens. Plusieurs penseurs algériens vont villipender les plus importantes personnalités du régime colonial français.

La plupart des figures du mouvement algérien vont être surveillées de près par les services policiers français, d'autres seront exilées vers d'autres pays comme l'a été l'émir Khaled El-Hassani Ben El-Hachemi en Égypte puis en Syrie.

Malek Bennabi, Mohamed Hamouda Bensai, Saleh Bensai, Messali Hadj, Ben Badis, Mohamed Bachir El Ibrahimi, Fodil El Ouartilani, Larbi Tébessi, Ferhat Abbas, Omar Ouzeggane, etc., tous vont diverger entre eux sur la question algérienne, cela provoquera l'émergence de plusieurs associations et partis algériens: Parti de la réforme ou mouvement pour l'égalité, Association des oulémas musulmans algériens, association de l'Étoile nord-africaine, le parti Parti du peuple algérien, Amis du Manifeste des Libertés, Parti communiste algérien, etc.

Le 8 mai 1945 ont lieu des manifestations d’Algériens dans plusieurs villes de l’Est du pays (Sétif, et le Constantinois), qui devaient permettre de rappeler leurs revendications nationalistes, de manière concomitante avec la liesse de la victoire. À Sétif, après qu'un policier eut tué un jeune scout ayant brandit le drapeau algérien, la manifestation tourne à l’émeute et la colère des manifestants se retourne contre les « pieds noirs » : 27 Européens et sont assassinés (103 trouveront la mort dans les jours suivants), ainsi que 700 musulmans. La répression de l’Armée française est brutale, quelques images de ces évènements ont été archivées et diffusées par la télévision algérienne en 2005.

Officiellement, elle fait morts parmi les musulmans, chiffre sous-estimé et probablement plus proche des à selon Charles-Robert Ageron, ou des à calculés par le Service Historique de la Défense et Roger Vétillard, tout en précisant qu'il s'agit d'une estimation haute. Selon l’historien Benjamin Stora, il s'élève entre et . Le Parti du peuple algérien (PPA) estime qu'il y a eu . Du fait de la radicalisation qu'ils ont engendrée dans les milieux nationalistes algériens, certains historiens considèrent ces massacres comme le véritable début de la guerre d'Algérie, opinion qui, pour Charles-Robert Ageron, « ne peut pas être acceptée comme un constat scientifique ».

Dans son rapport, le général Duval, maître d'œuvre de la répression, se montra prophétique : .

À la suite de la mort de Ben Badis en 1940, de l'emprisonnement de Messali Hadj et de l'interdiction du Parti du peuple algérien, le parti Mouvement pour le triomphe des libertés démocratiques (MTLD) revendique après le statut de l'égalité l'indépendance de l'Algérie en 1948. L 'Association des oulémas musulmans algériens est alors interdite. L'Organisation spéciale apparait et a pour but de rassembler des armes pour le combat. Mohamed Belouizdad est le premier chef de l'organisation clandestine. Hocine Aït Ahmed prend ensuite la tête de l'Organisation et continue à œuvrer pour l'achat des armes. La poste d'Oran est attaquée par les membres de l'OS.

Ahmed Ben Bella prend la place de Hocine Aït Ahmed en 1949. Le plan de l'organisation est dévoilé et des arrestations en chaîne sont opérées par les autorités françaises en 1950. Le Mouvement pour le triomphe des libertés démocratiques nie tout relation avec l'Organisation spéciale afin d'éviter des arrestations.

Le CRUA, fondé en mars 1954, organise la lutte armée. Le parti du Mouvement national algérien (MNA) est fondé en juillet 1954 par les messalistes. Par la suite, le Front de libération nationale (Algérie) (FLN) est fondé en octobre 1954 par la branche du CRUA (Comité révolutionnaire d'unité et d'action).

Le Front de libération nationale (Algérie) (FLN) et le Mouvement national algérien (MNA) rivalisent non seulement pour prendre le contrôle de la révolution mais surtout pour la représentation du futur État. Messali Hadj sera libéré de prison en 1958 et sera assigné à résidence surveillée en France.

Un vaste mouvement de révoltes naît au fil des ans. L'Algérien sujet, sans droit politique, de la France devient citoyen français par la loi du 20 septembre 1947 et peut désormais circuler librement entre l'Algérie et la métropole. Selon le journaliste et écrivain Yves Courrière : . La majorité des Algériens vivaient dans les campagnes. Avec l'aide américaine du plan Marshall, élèves sont scolarisés dans l'enseignement primaire en 1951 - 1952 à travers tout le territoire de l'Algérie. Cependant le programme pour agrandir les villes et diminuer la proportion de gens des campagnes n'a été réalisé que partiellement par le gouvernement français. En 1954, l'élimination des nationalistes algériens lors des élections de l'Assemblée algérienne marque le point de rupture politique et l'échec des nationalistes. Lors de la réunion des 22, le vote se prononce en faveur de la lutte armée. L'action armée va venir du CRUA. Le déclenchement de la révolution algérienne a été décidé à Alger lors de la réunion des 6 chefs du Comité révolutionnaire d'unité et d'action (CRUA). Le CRUA se transformera en Front de libération nationale (FLN). Les six chefs du FLN qui ont fait le déclenchement des hostilités le novembre 1954 sont Rabah Bitat, Mostefa Ben Boulaïd, Didouche Mourad, Mohamed Boudiaf, Krim Belkacem et Larbi Ben M'Hidi. La Déclaration du 1er novembre 1954 est émise par radio depuis Tunis. Dans la nuit du , la caserne de la ville de Batna est attaquée par les moudjahidines. Cette nuit sera appelée par les historiens français « Toussaint rouge ». Un caïd et deux enseignants français vont être abattus sur la route de Biskra et Arris. Il y aura deux versions différentes des faits. Des attentats sont enregistrés dans les trois districts de Batna, Biskra et Khenchela et le reste du pays.

Au cours d'un voyage en Algérie, François Mitterrand, alors ministre de l'Intérieur dans le gouvernement Pierre Mendès France, déclare . Les opérations sont déclenchées dans les Aurès. L'Armée de libération nationale (ALN) ne dispose alors que de qui seront, après quelques mois, plus de à défier l'autorité française. français sont affectés dans les Aurès et plus tard ils seront plus de en Algérie. Le général Cherrière donne l'ordre de faire le ratissage des Aurès. Il croit gagner, mais va subir une grosse défaite.

Les massacres du Constantinois des 20 et 21 août 1955, notamment à Philippeville (Skikda) par leur cruauté du côté des insurgés comme par la terrible répression du côté français sont une étape supplémentaire dans la guerre. La même année, l'affaire algérienne est inscrite à l'ordre du jour à l'Assemblée générale de l'ONU. À noter aussi la mort de Mostefa Ben Boulaïd, de Zighoud Youcef, etc. Plusieurs chefs sont emprisonnés.

Des intellectuels français vont aider le FLN. Maurice Audin fut torturé et tué par les services français. Frantz Fanon s'engage auprès de la résistance algérienne et a des contacts avec certains officiers de l'ALN (Armée de libération nationale) et avec la direction politique du FLN, Ramdane Abane et Benyoucef Benkhedda en particulier. Il donne sa démission de médecin-chef de l'hôpital psychiatrique de Blida-Joinville en novembre 1956 au gouverneur Robert Lacoste, puis est expulsé d'Algérie en janvier 1957. Albert Camus, natif d'Algérie, fut un défenseur des droits algériens, dans les années 1940, avant de refuser de prendre position pour l'indépendance par ces phrases prononcées à Stockholm en 1957 : . Dès 1956, Jean-Paul Sartre et la revue "Les Temps modernes" prennent parti contre l'idée d'une Algérie française et soutiennent le désir d'indépendance du peuple algérien. Sartre s'élève contre la torture, revendique la liberté pour les peuples de décider de leur sort, analyse la violence comme une gangrène, produit du colonialisme. En 1960, lors du procès des réseaux de soutien au FLN, il se déclare « porteur de valise » du FLN. Cette prise de position n'est pas sans danger, son appartement sera plastiqué deux fois par l'OAS et "Les Temps modernes" saisis cinq fois.

Après la condamnation de Larbi Ben M'Hidi et après le déroulement du Congrès de La Soummam, le FLN intègre les dirigeants du Mouvement national algérien (MNA). Plusieurs partis algériens adhèrent à la cause du FLN. Le Front de libération nationale et l'armée française tiennent le même langage : .

La guerre éclate entre les chefs kabyles (Krim Belkacem, Ouamrane, etc) et les chefs chaouis et aussi entre les chefs chaouis des Aurès et les chefs chaouis de Nemencha. Abdelhai et Abbès Leghrour seront condamnés à mort par les partisans du Congrès de la Soummam et le Comité de coordination et d'exécution (CCE). Il y a aura aussi un conflit entre les hommes du Sud algérien et les dirigeants kabyles. La Tunisie va être le théâtre d'affrontement entre les différents chefs. Le président Bourguiba devait intervenir pour pacifier les choses. Les Aurès, le Constantinois, l'Ouest de l'Algérie, la Kabylie, seront les zones les plus stratégiques de la révolution. Le Maroc aussi va jouer un rôle important, notamment pour faire transiter les armes, organiser des réunions du FLN et héberger des troupes militaires algériennes. Le Maroc et la Tunisie, sous protectorat français jusqu'en 1956, hébergeront néanmoins les deux armées de l'ALN aux frontières ainsi que plusieurs chefs du FLN comme Ferhat Abbas

L'armée française fait construire le barrage de la mort, de long, , un poste de contrôle chaque , des milliers de mine terrestre, etc., pour empêcher le passage des armes dans les Aurès et dans tout l'est de l'Algérie. Mais les éléments de l'ALN (Armée de libération nationale) vont déjouer toute la stratégie militaire française. Les villes (population algérienne) seront sous le contrôle de l'Armée de libération algérienne. La bataille d'Alger fera la une de la presse internationale et interne. Le conflit est porté jusqu'à L'ONU. Aussi, il y aura plusieurs grèves et manifestations dans les villes. Les protestations ont été organisées par le FLN.

Le colonel Amirouche Aït Hamouda fera un massacre dans les Aurès en voulant intervenir pour unifier des zones des Aurès et faire passer les armes en Kabylie. L'Aurès fut le lieu de passage des armes vers l'intérieur du pays. Le colonel Amirouche Aït Hamouda réussira à faire passer les armes qui provenaient d'Égypte en passant par la frontière de Tunisie et de l'Algérie. Il franchira les Aurès pour rejoindre la Kabylie. Une vingtaine de chaouis vont être du voyage, mais à la fin, ils abandonneront les troupes du colonel Amirouche pour revenir aux Aurès. Krim Belkacem voulait contrôler la région des Aurès pour établir l'union des forces. Les hommes de Ben Bella et de Abdelhafid Boussouf désiraient aussi avoir un pied dans les Aurès. Au même moment, la France connaîtra sa crise interne jusqu'à l'arrivée au pouvoir du général Charles de Gaulle à cause de la situation en Algérie. Les ultras européens veulent garder l'Algérie française. L'Armée française décide de créer les zones interdites sous contrôle des SAS (sections administratives spécialisées) et entame une lutte contre les Djounoudes (maquisards) et la population locale, dans les villes, dans les villages, dans les douars et sur tous les territoires sensibles au FLN. Les bombardements massifs, les tueries, les massacres, la torture, les viols, etc., tous les actes de crime ont été employés dans cette guerre. Plusieurs attentats seront organisés par l'ALN dans les villes et les villages, dans les zones interdites et dans les zones montagneuses des Aurès. Le CCE (Comité de coordination et d'exécution) s'est agrandi et décide de garder le cap sur les objectifs militaires et ainsi que la primauté de l'intérieur par rapport à l'extérieur. Une grave crise apparaît entre les membres du Comité de coordination et d'exécution.

Selon Yves Courrière, Ramdane Abane s'opposera sévèrement aux militaires. Il choisira de prendre le maquis et désignera Hadj Ali, un homme de l'Aurès, pour renverser le CCE à Tunis mais sera condamné à la prison au Maroc par le CCE. Plus tard, il sera tué au Maroc, mais les sources de FLN diront qu'il aurait été tué lors d'un accrochage avec l'Armée française. Le général Charles de Gaulle chef de l'État français engage une lutte contre les éléments de l'armée de libération nationale algérienne et il apporte les réformes tant attendues pour donner tous les droits aux Algériens. L'Armée française élimine presque tous les réseaux de l'Armée de libération nationale en Kabylie et dans quelques régions sensibles dans l'Opération jumelles. Les colonels Amirouche Aït Hamouda et Si el haouès sont tués lors d'un accrochage avec les éléments de l'Armée française. Le FLN appelle les éléments de son armée à tenir jusqu'au bout.

La Délégation des principaux dirigeants du FLN (Mohamed Khider, Mostefa Lacheraf, Hocine Aït Ahmed, Mohamed Boudiaf et Ahmed Ben Bella) est arrêtée, à la suite du détournement, le 22 octobre 1956 par l'armée française, de leur avion civil marocain, entre Rabat et Tunis, en direction du Caire (Égypte).

En 1959, Messali Hadj sort de prison, et est assigné à résidence surveillée en France. Les Algériens en France organisent des attentats et des manifestations en France en faveur du FLN.

1960, la semaine des barricades à Alger fait 22 morts algériens et des centaines de prisonniers. Le général de Gaulle annonce la tenue du référendum pour l'indépendance de l'Algérie. Les Algériens sont tenus de se prononcer. Certains généraux français se rebellent contre l'autorité du général de Gaulle (le putsch d'Alger (1958) et putsch des généraux). 

Le général de Gaulle reprend en main le destin de la France. Il annonce la tenue de référendum et invite le FLN à faire la paix des braves. Au même moment, le Gouvernement provisoire de la République algérienne est proclamé. Ferhat Abbas décline l'invitation française. Le colonel Houari Boumédiène est alors le chef de Armée de libération nationale.

En 1960, l'ONU annonce le droit à l'autodétermination du peuple algérien. Le côté français organise des pourparlers avec le Gouvernement provisoire de la République algérienne. Plusieurs réunions à l'extérieur du pays vont aboutir aux accords d'Évian. Le colonel Houari Boumédiène refuse que les pieds-noirs restent en Algérie.

Le 17 octobre 1961, des Algériens sont tués lors d'une manifestation du FLN à Paris (Massacre du 17 octobre 1961). Il y aura aussi des milliers d'arrestations. Ce fait survient à la suite de l'instauration d'un couvre-feu à Paris et sa banlieue pour les seuls Algériens à la suite de l'assassinat de 21 policiers français par le FLN. À Alger. Le peuple algérien sort dans les rues pour manifester sa joie à l'indépendance. Il y aura plusieurs morts et blessés par la police française.

L'Organisation armée secrète (OAS) organise des attentats contre les Algériens malgré l'accord de cesser le feu et les résultats du référendum pour l'indépendance pour sanctionner les gens qui étaient pour. L'indépendance de l'Algérie est proclamée après les résultats. La plus grande bibliothèque d'Alger a été complètement détruite par l'OAS (Organisation armée secrète).

Des éléments de l'armée française restent en Algérie pour évacuer un million de Français (pieds-noirs, les Harkis, les Juifs, etc.). Un million de réfugiés algériens reviennent en Algérie.

Le 25 juillet 1954, dans une modeste villa du Clos Salambier, un misérable quartier musulman d'Alger, vingt-deux Algériens (les « Cinq » du début, Mostefa Ben Boulaïd, Mohamed Boudiaf, Larbi Ben M'Hidi, Didouche Mourad et Rabah Bitat, ont beaucoup recruté) se prononcent « pour la révolution illimitée jusqu'à l'indépendance totale ». C'est de ce jour-là que date véritablement la guerre d'Algérie. Les chefs de régions sont désignés: Aurès-Némentchas : Ben Boulaïd, Département Nord-Constantinois : Rabah Bitat, Kabylie : Krim Belkacem, Algérois-Orléansvillois : Didouche Mourad, Oranie : Larbi Ben M'Hidi. Fin octobre, ces cinq responsables décident de créer l’« ALN » (Armée de libération nationale).

Le 15 octobre 1954, le C.R.U.A. se transforme et devient le F.L.N. : « Front de libération nationale ». Les revendications de l'organisation comportent : reconnaissance de la nationalité algérienne, ouverture de négociations, libération des détenus politiques. Les Français demeurant en Algérie pourront choisir leur nationalité, Français et Algériens obtenant des droits égaux. Sont définis les cibles qui devaient être attaquées dans la nuit du 31 octobre au novembre. Ils avaient prévu, en accord avec la Délégation extérieure, d’en faire l’annonce à la radio du Caire le jour du déclenchement de la Révolution.

Plus de trente attentats ont lieu, dans la nuit du 31 octobre au novembre 1954, en différents points du territoire algérien. Bilan : huit tués, dont pour moitié des civils, et des dégâts matériels. L'opinion s'émeut surtout de l'attaque du car Biskra-Arris, dans les Aurès, principal foyer de l'insurrection : deux passagers, le caïd Hadj Sadok, ancien lieutenant de l'armée française, et l'instituteur Guy Monnerot sont abattus. Une proclamation diffusée dans la presse revendique ces actions au nom d'un mystérieux groupe : le FLN, Front de libération nationale. Son but: l'indépendance d'un « État algérien souverain démocratique et social dans le cadre des principes islamiques ». Et ce, « par tous les moyens ». Personne, en France ou en Algérie, ne pense qu'une guerre vient de commencer.

Président du Conseil depuis le 18 juin 1954, Pierre Mendès France est surpris par la révolte algérienne. Il affirme aussitôt avec force que « l'on ne transige pas quand il s'agit de défendre la paix intérieure de la nation, l'unité et l'intégrité de la République » Son ministre de l'Intérieur, François Mitterrand, en visite en Algérie le 12 novembre 1954 réagit brutalement : « l’Algérie, c’est la France !, la négociation avec les rebelles c'est la guerre. » Des renforts sont acheminés, des milliers de nationalistes arrêtés. Mais 99 % d'entre eux n'ont aucun rapport avec le FLN. Car Mitterrand, à l'image de presque tous les responsables, se trompe : il croit que les attentats sont liés au MTLD (Mouvement pour le triomphe des libertés démocratiques), le parti du vieux nationaliste Messali Hadj, et que leur tête pensante est au Caire, autour de Ben Bella.

Pierre Mendès France propose aussi un plan de réformes en faveur des musulmans, ce qui occasionne sa chute, le 6 février 1955. Son tombeur est René Mayer, député de Constantine, représentant de la ligne dure des pieds noirs : foin des réformes, d'abord la répression. Alors que la chambre vote la défiance, un Mendès France écarté du pouvoir a ces mots : « en Afrique du Nord... soit il y aura une politique de réconciliation... soit une politique de force et de répression - avec toutes ses horribles conséquences ».

Dans les premiers temps, les hommes du FLN visent principalement les musulmans proches des Européens. De novembre 1954 à avril 1955, font et par les musulmans. Le but de ces opérations est de terroriser la population indigène francophile et de séparer les deux populations. Les Européens ne sont pas visés en tant que tels, mais en tant que représentants de l'ordre colonial comme les policiers, les élus et les fonctionnaires. Néanmoins, rapidement, des civils sont également victimes des embuscades.

Après quelques mois, les rebelles sont en difficulté cernés par des forces de police beaucoup plus nombreuses. En dehors du Nord-Constantinois, le calme règne.

Du 20 au 26 août 1955, la guerre change radicalement de visage avec les évènements sanglants qui secouent le Nord du département de Constantine et plus particulièrement la ville de Philippeville où surviennent de terribles massacres de civils.

Les massacres ont éclaté à l'initiative de Youcef Zighoud, responsable du Nord-Constantinois du FLN dans le but de relancer un mouvement qui s'essouffle et de contrecarrer les avances faites par Jacques Soustelle, délégué général du gouvernement français en Algérie, en creusant un infranchissable fossé de sang entre les Algériens et les Français par des massacres aveugles.

Dans la zone Collo-Philippeville-Constantine-Guelma, moins de de l’ALN s’attaquent sans grand succès à des gendarmeries et des postes de police. Ils encadrent plusieurs milliers de paysans mal armés mobilisés de gré ou de force qui se lancent à l’assaut d’une trentaine de villes et villages et assassinent à coup de haches et de pioches Européens ou indigènes favorables à la France. sont tués ainsi qu'une centaine de musulmans francophiles et des forces de police. La presse est indignée par les exactions d'El Halia où ont été égorgés dont dix enfants et trois bébés de moins de deux ans.

La réponse des autorités françaises est démesurée et touche des innocents. L'aviation bombarde les douars des environs, plus particulièrement le hameau du Béni Malek. Des civils armés font eux-mêmes acte de vengeance dans une répression aveugle. Le nombre de victimes atteint plusieurs milliers : entre trois et sept mille cinq cents morts. 

L'indignation suscitée par ces massacres de civils a attiré l'attention de l'opinion internationale sur le combat algérien pour l'indépendance réalisant l'un des buts poursuivis par le FLN, qui voulait par ailleurs semer la peur dans les rangs de l'ennemi, des colons et de leurs auxiliaires musulmans et réduit à néant tout espoir de paix. 

Pour beaucoup d'historiens, ce sont les massacres d'août 1955 et non pas de ceux de Sétif (mai 1945) qui marquent le vrai passage de l'insurrection vers la guerre à outrance comme unique moyen de parvenir à se faire écouter des autorités coloniales françaises.

Le , à la suite des élections législatives, l'Assemblée nationale investit le gouvernement Guy Mollet qui entre en fonction le . Le , le général Georges Catroux est nommé Résident Général en Algérie en remplacement de Jacques Soustelle, le départ de ce dernier provoque une forte mobilisation de soutien à Alger : la foule le suit jusqu'au quai d'embarquement et déborde le service d'ordre ; monsieur Soustelle est obligé d'emprunter une échelle volante pour se rendre à bord du bateau le ramenant en métropole.

Le , le voyage à Alger de Guy Mollet (Front Républicain), selon ses termes « pour étudier sur place la situation », résulte sur un incident passé à la postérité sous le nom de « journée des tomates ». Lorsque le cortège officiel se rend au Monument aux Morts d'Alger, il est conspué et accueilli par une foule hostile menée par le Comité d'Entente des Anciens Combattants qui lui lance, entre autres, tomates et quolibets en signe de mécontentement face à la nomination du général Catroux. La voix des maires d'Alger est relayée dans la presse locale, c'est la célèbre formule de "L'Écho d'Alger" : « "le maintien du général Catroux signifierait l'effondrement de l'Algérie" », ce à quoi le journal socialiste "Le Populaire" répond que « "les pressions des ultras, les démonstrations de force et de violence seront sans efficacité" ». À la suite de la journée des tomates, Georges Catroux présente sa démission à René Coty pour éviter d'« "entrer en conflit de conception et d'action sur un problème national capital avec ses anciens compagnons d'arme" »; le général Catroux est remplacé par Robert Lacoste.

Pierre Mendès France nomme Jacques Soustelle gouverneur de l'Algérie. Gaulliste réputé de gauche, cet universitaire connu définit sa politique par le terme d'intégration : égalité des droits entre musulmans et Européens. Cette volonté de réformes n'empêche pas l'intensification des activités militaires. Constatant la faiblesse et la mauvaise volonté de l'administration, Soustelle invente les Sections administratives spécialisées (SAS), dirigées par des jeunes officiers, et qui ont pour but d'améliorer le sort matériel des musulmans. C'est le premier pas dans l'implication socio-politique.

Il lui faudra mettre en œuvre le plus rapidement possible des réformes, en particulier rendre effectif le statut de 1947, resté lettre morte. Il devra donc appliquer une politique d'intégration de la population musulmane, qui doit bénéficier des mêmes droits que la communauté européenne d'Algérie ou de la métropole. Cette politique va susciter l'opposition de nombreux pieds-noirs, d'autant que Soustelle se propose de réorganiser l'administration algéroise, d'où une autre levée de boucliers contre lui. Le nouveau gouverneur doit enfin surmonter un autre handicap : il a été nommé par Pierre Mendès-France, homme fort peu apprécié de la communauté européenne d'Algérie qui le soupçonne de pratiquer une politique d'abandon de l'Algérie, comme il lui est reproché de l'avoir fait pour l'Indochine, la Tunisie et le Maroc. Soustelle quittera en fin de compte l'Algérie caché dans le coffre arrière d'une voiture.

À la suite des accords de La Celle-Saint-Cloud, le gouvernement français reconnaît l'indépendance du Maroc le 12 mars 1956, puis, le 20 mars de la même année, celle de la Tunisie (avec ses «dépassements»). Inévitables, ces deux événements n'en fournissent pas moins au FLN deux bases arrière « sanctuarisées »

Deux années après l'insurrection de la Toussaint 1954, le commandement français s'inquiète de l'activité du FLN dans la région de la Kabylie et décide de monter des commandos qui débusqueraient les maquisards de Krim Belkacem. Dans le courant de 1955, Henry Paul Eydoux, conseiller technique au cabinet du gouverneur général Jacques Soustelle a l'idée de monter un « contre-maquis » en Kabylie maritime. L'opération K, dite habituellement par la suite Oiseau Bleu a pour but de recruter des hommes en Kabylie, de les équiper en armes (environ 300 seront livrées) et de monter une contre insurrection contre le FLN. Elle est confiée à la DST puis au Service de renseignement opérationnel (SRO). L'opération échoue complètement, l'argent et les armes sont détournées au profit du FLN.

L'opération Djenad, montée par la DIA du 9 au 12 octobre 1956 dans la forêt d'Adrar, permet au RPC du général Bigeard de mettre hors de combat 130 rebelles. Globalement, l'opération K est un grave échec notamment pour les services de renseignement. Ses conséquences sur le plan militaire le sont moins.

Après dix mois de calme, la Grande Kabylie s'embrase grâce en partie aux armes, aux équipements et à l'argent fournis par la France.

Le massacre est commis le , par une unité de l'armée française, le bataillon de chasseurs à pied ( BCP) au cours de la guerre d'Algérie, cette unité a massacré 79 villageois algériens du hameau du Beni Oudjehane qui comptait 300 habitants, situé dans la presqu’île de Collo non loin d'El Milia dans la wilaya de Jijel, (ex Département de Constantine).

Ce drame resurgit en 2013 avec une enquête, menée conjointement en France et en Algérie par une historienne Claire Mauss-Copeaux et deux blogueurs, André, un ancien militaire français appartenant au BCP et Nour, un enseignant algérien de la région d'El Milia, ils se sont donné pour but de reconstituer ce qui s’est passé ce jour du 11 mai 1956.
Près de Palestro, à à l'Est d'Alger, le 18 mai 1956, 19 soldats du contingent sont tués dans une embuscade. La presse se fait l'écho de cet accrochage sanglant. Les cadavres mutilés frappent l'opinion. . Dans l'après-midi qui suit la découverte des cadavres français, alors que .

Au même moment Guy Mollet envoie de nombreux appelés en Algérie. L'émotion est intense en métropole. Le conflit apparaît sous un jour nouveau. L'Algérie n'est plus comme l'Indochine, un conflit lointain mené par des professionnels mais une affaire intérieure française à laquelle chacun participera, via un fils, un frère, un mari. Du coup, l'opinion métropolitaine devient potentiellement l'acteur principal du drame.

Dès le mois de , le dirigeant du FLN Ramdane Abane conduit des rencontres avec ceux qui désiraient participer à la guerre pour l'indépendance. Un accord entre le Parti communiste algérien (PCA) et le FLN est négocié par Bachir Hadj Ali et Sadek Hadjerès. Il n'admet l'adhésion de communistes au FLN qu'à titre individuel et non en tant que groupe. La collaboration entre le PCA et le FLN est néanmoins loin d'être sans heurts. Différents combats ont opposé les rebelles communistes et ceux du FLN sur le terrain ». Le PCA sera progressivement marginalisé par le FLN durant la guerre.

Les leaders FLN d'Alger et surtout parmi eux, Abane Ramdane, ont pensé, très tôt, à réunir une vaste assemblée de cadres qui permettrait au FLN d'affirmer sa cohésion, de préciser sa doctrine et de définir concrètement ses structures organisationnelles. À la fin du mois de mars 1956, Saad Dahlab a rencontré en grand secret, dans le Constantinois, le chef de la zone 2, Zighout Youssef, et son adjoint Lakhdar Bentobal et il leur a soumis cette idée, qui a été favorablement accueillie. Larbi Ben M'hidi, en mission au Caire à la même époque, a fait part du projet à la « délégation extérieure du FLN », qui a accepté le principe d'un grand rassemblement clandestin de responsables FLN sur le sol algérien, et qui a même remis à l'envoyé spécial d'Alger en prévision de cette réunion, un texte politique dit « rapport Khider ». Ce n'est cependant qu'à la fin du printemps de 1956 que la préparation du congrès de la Soummam entre dans une phase active. Abane Ramdane et Krim Belkacem envoient des messages à tous les chefs de zone pour leur demander d'envoyer des délégués à une « rencontre préliminaire » dans une forêt de la région montagneuse des Bibans, aux confins de la Kabylie. La discussion sur le « projet de plate-forme politique » permet à Abane d'insister fortement sur les principes fondamentaux qui inspirent son programme.

Principal organisateur du congrès de la Soummam, Ramdane trace ainsi les grandes lignes du mouvement révolutionnaire consistant à créer un État dans lequel l'élément politique l'emporte sur l'élément militaire

En 1956, la France qui soupçonne le colonel Nasser de soutenir le FLN en moyens et en armes s'engage dans l'expédition du canal de Suez, ce qui gèle ses relations avec les pays arabes et l'URSS.

Avec l'aide d'Israël et du Royaume-Uni, les parachutistes français battent les Égyptiens et reprennent le contrôle du canal de Suez mais le président de l'URSS Nikita Khrouchtchev menace de faire usage de l'arme nucléaire contre Londres et Paris si le corps expéditionnaire anglo-français ne se retire pas d'Égypte. Les États-Unis font alors pression sur le premier ministre britannique Anthony Eden en le menaçant de dévaluer la monnaie de son pays si ses troupes ne se retirent pas d'Égypte, ce qu'elles feront ainsi que leurs alliés français (la flotte du corps expéditionnaire est placée sous haut commandement britannique).
Le à Rabat, cinq dirigeants du Front de libération nationale (FLN) prennent place à bord d'un DC-3 de la compagnie Air Atlas-Air Maroc. Ahmed Ben Bella, Hocine Aït Ahmed, Mostefa Lacheraf, Mohamed Khider et Mohamed Boudiaf, devaient initialement prendre place à bord de l'avion du sultan du Maroc, Mohammed V. Les cinq hommes doivent se rendre à Tunis pour un sommet organisé par Habib Bourguiba.

Les services secrets français du SDECE ont eu connaissance de la date exacte du voyage et organisent son détournement qui peut avoir des conséquences diplomatiques, l'avion appartenant au sultan du Maroc. L'opération est réalisée sans prévenir Guy Mollet le président du conseil, ni apparemment Robert Lacoste, ministre résident en Algérie. 

Avec les chefs du FLN, les autorités saisissent des documents apportant la preuve formelle de l'aide égyptienne au FLN. Mais la révélation de ce soutien ne suffit pas à calmer le jeu, bien au contraire. Au Maroc, de violentes émeutes anti-françaises font une soixantaine de morts, les victimes, toutes européennes, ayant été sauvagement massacrées. De son côté, le sultan durcit sa position en rappelant son ambassadeur en poste à Paris. Habib Bourguiba adopte une position analogue et, dans le monde arabe, la France est sévèrement jugée. La presse française de gauche est d'une extrême virulence, Alain Savary secrétaire d'État aux Affaires marocaines et tunisiennes démissionne. Bruno de Leusse, ambassadeur de France à Tunis, quitte lui aussi ses fonctions.

Pour le FLN, la capture de Ben Bella et de ses compagnons n'est pas une perte irrémédiable, car les cinq hommes sont des politiques qui peuvent être assez facilement remplacés. La rupture des relations avec la France est en revanche un coup dur pour les deux parties. Si en effet le FLN est désormais assuré de recevoir une aide puissante de l'Égypte, de la Tunisie et du Maroc, il n'a plus de dialogue direct avec la France. De son côté, Guy Mollet est furieux. L'opération s'est faite sans qu'il ait été informé et les négociations engagées secrètement à Rome avec le FLN sont rompues. .

L'année 1957 voit le déroulement de la bataille d'Alger. Sous les ordres du général Massu, la division parachutiste fait du maintien de l'ordre dans la capitale. Les parachutistes () parviennent à anéantir les poseurs de bombes. Le FLN perd la bataille et sa structure dans la capitale est détruite.

Dans le même temps, le général Salan organise la contre-guérilla grâce à des techniques de quadrillage. Moins entrainés, les hommes du contingent ainsi que nombre de réservistes plus âgés sont le plus souvent cantonnés dans des casernes ou à établir des missions de surveillance tandis que les troupes mobiles organisent, sur le terrain, l'éradication des maquis. Des ratissages et des opérations de recherche-destruction sont menés en permanence à l'aide d'hélicoptères. Des centaines de hameaux sont investis par les forces spéciales à la recherche de caches d'armes de la guérilla indépendantistes donnant lieu à un nombre élevé de dérapages.

Depuis 1954, accrochages et embuscades se succèdent dans le bled. L'attention se focalise sur les campagnes, notamment dans les Aurès et la Kabylie. Mais à partir de 1956, la direction du FLN s'oriente vers l'offensive urbaine et décide de faire de la capitale le théâtre d'une épreuve de force. Le but est de frapper au cœur de l'appareil colonial, de manière beaucoup plus spectaculaire. Il s'agit de démontrer la force du FLN aussi bien aux yeux de l'opinion publique française que de celle des pays étrangers. Les chefs nationalistes : Ramdane Abane, Krim Belkacem, Larbi Ben M'Hidi, Saad Dahlab et Benyoucef Benkhedda, s'installent donc clandestinement dans la Casbah d'Alger. Les cinq hommes ont créé la Zone autonome d'Alger (ZAA) et commencèrent par se répartir les tâches de la façon suivante : Ben Khedda se réserva les contacts avec les Européens et la direction de la nouvelle zone autonome d'Alger, détachée désormais de la wilaya IV, Dahlab, la propagande et la direction du journal "El Moudjahid", Ben M'Hidi choisit d'être responsable de l'action armée à Alger (il était donc le supérieur direct de Yacef Saadi), Krim Belkacem s'attribua les liaisons avec toutes les wilayas, ce qui faisait de lui le chef d'état-major et le stratège de la lutte armée; Abane Ramdane enfin, devint le responsable politique et financier, c'est-à-dire, en fait, le en dépit de la collégialité voulue par les « cinq ».

Alger, capitale de l'Algérie, vaste agglomération de près de un million d'habitants, est en effet le symbole de la réussite française en Algérie. Centre nerveux de l'administration, elle est la principale place des affaires, le premier port, le plus grand aéroport. Surtout, elle abrite une partie importante des Français d'Algérie. Et c'est là que la presse française et internationale vient chercher ses informations. La ville symbolise aussi la situation du pays. Quoique française dans sa majorité Alger a toujours conservé un quartier « arabe », la célèbre Casbah. De plus, l'explosion démographique qui touche la population musulmane a entraîné l'installation, à la périphérie, de masses croissantes de prolétaires qui peuplent les bidonvilles.

Le dispositif du FLN repose sur un petit nombre de militants plus de 2000 à peu près qui ont su tisser, par la conviction ou la peur, un vaste réseau de soutiens et de complicités. Un groupe qu'on appelle « réseau bombes » chargé de la fabrication de bombes préréglées (dites « à retardement ») est mis sur pied. Pour les poser, on choisit des jeunes femmes, moins susceptibles d'éveiller les soupçons et ils dépendent tous d'un autre chef algérois nommé Yacef Saadi un fils de la Casbah. Les services de police enregistrent attribués au FLN durant l'année 1956.

Les attentats créent depuis mai 1956 une véritable psychose. Les engins meurtriers font blessés et morts dans toute la grande agglomération. Le FLN présente son action comme une riposte. C'est sa réponse aux premières exécutions de ses militants FLN condamnés à mort et guillotinés dans la fameuse prison Barberousse, ainsi à l'attentat meurtrier de la rue de Thebes dans la Casbah du 10 août qui a coûté la vie à entre 15 et 70 personnes et au moins 40 blessés, l'attentat perpétré par des « ultras - activistes » pieds noirs de l'Organisation de la résistance de l'Algérie française (ORAF) de la La Main rouge. Le but du FLN de faire régner une atmosphère d'insécurité générale, en multipliant attentats individuels et poses de bombes destinées à tuer des civils européens.

Au total, dans le grand Alger, le bilan officiel des attaques du FLN en quatorze mois est de 751 attentats, 314 morts et 917 blessés.

Le gouvernement français décide de réagir et donne pour mission au général Massu de rétablir l'ordre. Il est fait appel à la division parachutiste. Ses quatre régiments s'ajoutent aux éléments déjà sur place, notamment la police, la gendarmerie et les fantassins du régiment de zouaves qui surveillent la Casbah. En tout, ils sont près de . Bien entraînés et très bien encadrés, les sont spécialisés dans la lutte contre la guérilla. Leurs officiers se sentent profondément impliqués dans le conflit, très sensibles à sa dimension tant politique que militaire. Et beaucoup d'entre eux ont réfléchi aux techniques de la guerre subversive, notamment à partir de l'expérience indochinoise.

Le 7 janvier 1957, les paras entrent dans Alger, c'est le début de la bataille d'Alger. Chaque régiment s'attribue le contrôle d'un quartier, sous l'autorité du général parachutiste Jacques Massu qui a reçu tous les pouvoirs de police sur l'ensemble de l'agglomération algéroise. Grâce au fichier des renseignements généraux, les hommes de Massu établissent des listes de « suspects » en relation avec l'organisation clandestine. Ils sont interrogés, sommés de donner le nom du collecteur de fonds du FLN auquel ils versent leur cotisation. Grâce à ces informations, les militaires remonteront ensuite vers des chefs plus importants. Par la suite, les militaires vont interpeller de plus en plus d'Algériens, du militant qui peuvent détenir des informations très importantes au simple sympathisant. Les énormes opérations de contrôle effectuées quartier par quartier vont se révéler très efficace.

En riposte, les responsables du FLN préparent une grève générale fixée au 28 janvier 1957. La date coïncide avec l'ouverture, à l'assemblée générale de l'Onu, d'un débat sur la question algérienne. C'est le moment idéal pour attirer l'attention de l'opinion publique internationale. Cette grève pourrait constituer le début, ou au moins la répétition générale, d'un vaste mouvement insurrectionnel fatal à la cause française. 

Pour obtenir les renseignements, l'armée française utilise interrogatoires musclés, pressions morales, menaces sur les familles. Mais, la menace des bombes pousse à exiger des réponses rapides pour prévenir de prochains attentats. Elle incite à recourir à des méthodes brutales, d'autant plus facilement que certaines officines de la police et des services de renseignements de l'armée les utilisent déjà. Simples bousculades, violences aussi actes de torture devant la famille de la personne impliquée font partie du quotidien. Et les erreurs sur les personnes, parfois dues à de simples homonymie, ne sont pas rares. Le recours à la torture est très rapidement dénoncé en métropole par les plus grands organes de presse et par les activistes du Parti communiste français comme Henri Alleg.

Dans le même temps, les officiers s'efforcent de prendre en main la population musulmane pour l'arracher au contrôle du FLN. La Casbah est divisée en groupes d'immeubles ou « îlots » (d'où « l'ilotage » donné au système). À chacun d'eux est affecté un habitant responsable, désigné par l'autorité, et chargé de servir à la fois de relais et d'informateur. Le quadrillage de la ville a permis également de stopper le contre-terrorisme européen.

Les succès obtenus sont indéniables. Le 28 janvier, la tentative de grève générale dite : « Grève de huit jours » est brisée par des méthodes expéditives : les ouvriers et les employés sont conduits au travail sous la contrainte. . 

Nombre de responsables FLN sont arrêtés : Larbi Ben M'Hidi le 23 février. Le 27 février 1957, la direction du FLN (Zone autonome d'Alger), menacée elle aussi d'arrestation, doit quitter Alger pour l'étranger, avec son leader, Ramdane Abane et les autres trois nationalistes Krim Belkacem, Saad Dahlab et Benyoucef Benkhedda. 

Le « réseau bombes » est aussi démantelé. De 112 en janvier, le nombre d'attentats passe à 29 en mars : le commandement français pense avoir remporté la victoire. Ce n'est pourtant qu'un répit. Le 3 juin, une bombe explose près d'un arrêt de bus. Le 9 juin, un attentat vise un dancing au Casino de la Corniche. Mais des opérations de « retournement » d'anciens militants du FLN sont mises en place .

Le 24 septembre, Yacef Saadi le chef du « réseau bombes » et de la guérilla urbaine est arrêté et ses derniers compagnons ont péri dans leur cache de la Casbah dynamitée par la division parachutiste. Ce fait d'armes marqua la fin de la bataille d'Alger.

Au total, la « guerre urbaine » du FLN se solde par un cuisant échec. Une partie des réseaux de la Zone autonome d'Alger est démantelée. L'autre partie est forcée à rentrer dans l'ombre et pour longtemps. En octobre 1957, l'armée française a éliminé du FLN dont plus de 200 ont été tués, 253 arrêtés, ainsi que de fonds, , des cellules. ont été saisies, ainsi que et d'explosif.

La rivalité entre le Front de libération nationale (FLN) et le Mouvement national algérien (MNA) donne lieu au massacre de Melouza.

C'est en 1956 que le douar de Melouza, un bourg situé sur les hauts plateaux au nord de la ville de M'Sila, à la charnière du Constantinois et de la Kabylie, passa au FLN (Front de libération nationale). Néanmoins, l'importante population des Beni-Illemane suit le MNA (Mouvement national algérien) du « général » Mohammed Bellounis partisan de Messali Hadj, rival du FLN. Ces troupes du MNA bénéficient de la neutralité, voire d'un soutien discret de l'armée française qui trouve là un moyen de contrer le FLN. Celui-ci, pour lequel la région de Melouza revêt une grande importance stratégique, s'en voit peu à peu éliminé. Certains émissaires sont abattus. Les clivages culturels enveniment le conflit, une grande partie de la population étant arabophone et supportant mal les exigences des maquisards kabyles.

À l'aube du 28 mai 1957, 400 hommes de l'ALN encerclent le village. À midi, la résistance bellouniste cesse, faute de munitions. les djounouds de l'ALN font sortir les hommes du village et, à coups de crosse, au milieu des gémissements des femmes et des enfants, les font avancer, vers Mechta-Kasbah, petit hameau situé au-dessus du village. Tous les prisonniers sont abattus au fusil, au couteau, à coups de pioche.

Dans les maisons et les ruelles transformées en abattoir, l'armée française, à son arrivée sur les lieux deux jours plus tard, dénombrera .

Pour les combattants de l'Armée de libération nationale (ALN), l'approvisionnement en armes et en munitions est une question vitale. En décembre 1954, ils ne disposent que de 400 fusils de chasse.

En 1955, la situation n'évolue guère, car les troupes françaises présentes au Maroc et en Tunisie assurent la surveillance des frontières. Tout change en mars 1956 avec la proclamation de l'indépendance de ces deux pays. L'ALN en profite pour y installer des bases où arrivent les armes achetées à l'étranger.

Le plus difficile est de leur faire franchir la frontière, car la Marine nationale surveille étroitement la côte algérienne et le Sud saharien, très inhospitalier, est régulièrement survolé par l'aviation. Restent les frontières terrestres de l'est et de l'ouest. Le relief montagneux y est plutôt favorable à l'ALN et des bandes s'installent à cheval sur la frontière tunisienne.

En juillet 1956, elles compteront jusqu'à dont la majorité en Tunisie. En octobre 1956, le bureau d'Alger estime que quatre bases sont constituées à l'est, deux à l'ouest et trois au sud du Maroc. Ce sont à la fois des centres de transit et des camps d'entraînement, des unités de guérilla et même des troupes régulières. À l'époque, les frontières sont assez perméables puisque, entre 1956 et 1957, de guerre rejoignent l'Algérie à partir du Maroc et de la Tunisie.

Le commandement français comprend que pour vaincre l'Armée de libération nationale (ALN), il faut stopper les renforts venus de l'extérieur.

Le problème est que ces frontières sont très difficiles à surveiller : d'une part, elles sont très étendues et, d'autre part, elles traversent des régions montagneuses et des plateaux désertiques. De plus, il faut éviter de consacrer trop d'hommes à cette mission car, à l'intérieur de l'Algérie, l'armée doit consacrer de nombreux effectifs au quadrillage du terrain et à la pacification.

À l'origine, il n'est pas question d'établir un barrage continu, mais simplement d'affecter des détachements très mobiles à la surveillance des points de passage habituels de l'ALN. Mais cette tactique trouve rapidement ses limites et, en 1956, un réseau de barbelés de de large est établi sur la frontière marocaine. On s'aperçoit alors qu'il est impossible d'empêcher les combattants de l'ALN de passer sans tirer sur eux alors qu'ils n'ont pas encore franchi la frontière, eux-mêmes ne se gênant pas pour ouvrir le feu contre les troupes françaises depuis le territoire marocain.

Pour éviter la multiplication des incidents, le barrage est reporté de quelques kilomètres à l'intérieur. En même temps que l'on isole la frontière, on assure une meilleure protection de la voie ferrée Oran-Méchria, Aïn Sefra-Colomb Béchar, qui est l'objet de nombreux sabotages.

Le barrage lui-même est renforcé par de nombreux postes de surveillance fortifiés. Des mines ancrées au sol par des plaques de béton sont mises en place. Il est impossible à l'adversaire de les relever pour les réutiliser comme l'avait fait le Việt Minh pendant la guerre d'Indochine.

Dans la région de Maghnia à la frontière marocaine, un officier du Génie, le colonel Durr, expérimente un barrage électrifié sur une dizaine de kilomètres. Le résultat si concluant que ce type d'obstacle va devenir la norme. On aura donc un réseau trapézoïdal de barbelés à l'intérieur duquel passe un courant électrique de . En arrière de ce premier obstacle, une seconde ligne électrifiée à précède un fouillis de barbelés, lui-même suivi d'un champ de mines et de piquets métalliques « tapis de fakir ». C'est du moins ce qu'on montre aux journalistes car, en 1956, le barrage est loin d'être terminé, Il faudra attendre le 15 septembre 1957 pour que les de la frontière ouest soient efficacement protégés.

À l'est, la défense a longtemps reposé sur les groupes d'intervention de l'armée de terre, mais le développement de l'activité de l'ALN en Tunisie va bientôt imposer la construction d'un barrage similaire. La Tunisie est en effet dans une situation géographique encore plus favorable que le Maroc, puisque les armes que l'ALN achète à l'étranger transitent librement par la Libye. Comme à l'ouest, le barrage permettra de protéger la voie ferrée Bône-Tébessa-Negrine.

Par une directive du 26 juin 1957, André Morice, ministre de la Défense, accorde la priorité à ce barrage en y affectant crédits et effectifs, d'importants moyens de génies venus de métropole . Le barrage électrifié jusqu'à Tébessa doit être impérativement terminé en octobre 1957 puis, le 14, la décision est prise de le prolonger jusqu'à Negrine, d'abord par un réseau non électrifié mais couplé avec une surveillance par canons assistés de radars, ce que le terrain plat et dégagé au sud rend possible.

Commencée en août 1957, la « Ligne Morice » à la frontière algéro-tunisienne, le réseau de la ligne est formé de 2 haies centrales de de hauteur en haute tension de , sera déclarée opérationnelle le 15 septembre 1957, en même temps que la Ligne Pedron, nom qui a été donné au barrage ouest à la frontière marocaine. Il ne s'agit pas d'un obstacle infranchissable, mais les militaires l'apprécient car il représente pour eux une alarme signalant et localisant un franchissement. Les troupes interviennent alors avec éventuellement l'appui des blindés et de l'aviation. En arrière de la piste technique permettant aux électromécaniciens d'entretenir et de réparer la haie électrifiée, court une piste tactique destinée à la circulation rapide des blindés de surveillance. Les hommes surnommeront rapidement ce dispositif la « herse ». Et comme l'importance de la Ligne Morice est vitale, un second barrage est établi à la frontière marocaine à partir de la fin 1958. Il renforce la « Ligne Morice » en avant de laquelle il est installé.
De 1958 à 1962,pour une longueur totale de plus de sont posées sur les barrages orientaux (« Ligne Morice » à la frontière algéro-tunisienne) soit plus du double que pour le barrage occidental. Le total cumulé pour la guerre d'Algérie, selon le colonel Jacques Vernet, est de (antipersonnel, « mines bondissantes » et « mines éclairantes »). Résultat: l'ALN perd sur le barrage Est et 600 sur le barrage Ouest, les troupes françaises déplorent 146 et 109 tués, Le barrage, ce sont donc aussi ces débris humains et animaux, projetés par l'explosion de mines sur les barbelés.

Sur les hauteurs d'Alger, le monument mémorial des Martyrs propose depuis le novembre 1984, dans une salle dédiée aux « Lignes Challe et Morice », un échantillonnage de tous ces engins sournois. Les mines qui se confondent avec le sol lessivées par les pluies, entraînées par les glissements de terrain, continuent de frapper hommes et bêtes sur les frontières occidentale et orientale de l'Algérie après l'indépendance.

En novembre 2007, lors de sa visite d'État en Algérie, le président Nicolas Sarkozy offre à son homologue algérien les plans des zones minées sur les barrages Est-Ouest.

Fin février 1957, laminé lors de la bataille d'Alger, le Front de libération nationale (FLN) connait une passe difficile. Pour Krim Belkacem, le dernier de ses fondateurs encore vivant, il en va même de la survie de l'organisation. L'« historique » sonne donc le ralliement des chefs de l'Armée de libération nationale (ALN) contre les « politiques » , rangés derrière Ramdane Abane, étoile montante de la révolution. Une coalition hétéroclite se forme autour de Krim Belkacem dont Lakhdar Bentobal et Abdelhafid Boussouf, habitué des pratiques policières qui sèment la terreur dans la population immigrée comme parmi les combattants Fort du principe de la prééminence du politique sur le militaire, Abane dénonce brutalement Boussouf une fois de plus lors de la réunion du Comité de coordination et d'exécution (CCE) en juillet 1957. C'est une fois de trop aux yeux de certains de ses opposants. La session du Conseil national de la révolution algérienne (CNRA) d'août 1957 voit le triomphe de Krim Belkacem et la première résolution adoptée stipule: « "Il n'y a pas de primauté du politique sur le militaire ni de différence entre l'intérieur et l'extérieur." » Désormais, la militarisation du FLN est totale : il n'aura aucune existence propre en dehors de l'ALN. Ainsi, note l'historien Mohammed Harbi, commence l'ère des seigneurs de la guerre. « À la direction, écrit-il, il n y a plus de tendances politiques, mais des clans. les liens d'intérêts personnels prennent la place des affinités politiques. Personne n'a de stratégie cohérente pour le présent et pour l'avenir. le problème est de durer. Chacun se méfie de chacun et se préoccupe surtout de réagir à toute initiative pour pouvoir éventuellement la neutraliser. ».

Vaincu, Ramdane Abane continue de gêner, car il s'obstine à dénoncer les dangers que font courir les « féodaux» à la révolution, menaçant de retourner bientôt dans le maquis pour reprendre en main la résistance intérieure. Le 27 décembre 1957, ses adversaires à leur tête Abdelhafid Boussouf l'attirent dans un guet-apens au Maroc et l'étranglent avec un fil de fer dans une ferme près de Oujda. Maquillé par ses responsables en glorieuse mort au combat, cet assassinat inaugure une florissante tradition de meurtres entre dirigeants après l'indépendance.

La « Bleuite », appelée parfois le « complot bleu », est une opération d'infiltration et d'intoxication à grande échelle, montée par le SDECE (services secrets français) à partir de 1957. Cette opération consista à dresser des listes de prétendus collaborateurs algériens de l'armée française et à les faire parvenir jusqu’aux chefs de l’Armée de libération nationale (ALN), le bras armé du FLN, pour y susciter des purges internes.

Fin 1957, après la bataille d'Alger, le FLN de la capitale est exsangue, et ses chefs morts ou en prison. Le colonel Amirouche le chef ALN de la wilaya III en Kabylie, entre en contact avec le dernier survivant des militants FLN de la Zone autonome d'Alger, Ghandriche, dit Safy « le Pur ». Il le charge de reconstituer son réseau aux côtés de deux autres hommes, Rani Mohamed à Alger et Kamal dans le maquis. Mais Safy et Hani sont des « retournés », de l'équipe des « bleus de chauffe » manipulés par un officier parachutiste Paul-Alain Léger des services du Groupe de renseignements et d'exploitation une branche du SDECE auprès de l'état-major Alger-Sahel qui a joué un grand rôle dans le démantèlement de la Zone autonome d'Alger durant la bataille d'Alger.

Cette opération d'intoxication entraînera d'importantes pertes humaines parmi les katibas des wilaya III et wilaya IV en particulier, chef de la zone de la wilaya III. Ahcène Mahiouz, chef de la zone 1 de la wilaya III et adjoint du colonel Amirouche, s'étant fait ainsi intoxiquer fait torturer et liquider des centaines d'hommes et de femmes voyant des traîtres et des espions partout, notamment les jeunes intellectuels, étudiants, médecins qui avaient rejoint le maquis. Il réussit à convaincre le colonel Amirouche que la trahison règne partout et qu'il faut épurer massivement, ce dernier écrit aux chefs des autres wilayas, le 3 août 1958 pour les avertir.

La « bleuite » fera plusieurs milliers de victimes. Des katibas de l’Armée de libération nationale (ALN) en sortent très affaiblies et hors d'état d'entreprendre des opérations pendant de longs mois. Quelques voix, telle celle de Mohand Oulhadj, futur chef de la wilaya III, essayèrent de faire entendre raison au colonel Amirouche. Les estimations des pertes sont de dans la wilaya III (Kabylie), en wilaya I (Aurès), en wilaya IV (Algérois) et 500 en wilaya V (Oranais). Une conséquence plus lointaine des purges menées dans les différentes wilayas sera la perte de ces jeunes intellectuels pour l'Algérie indépendante.

Décimés et découragés, les maquis de l'ALN ne purent qu’attendre le coup de grâce. Il leur fut donné, lorsque le commandement français décida de déclencher les grandes opérations prévues par le plan Challe.

Depuis janvier 1958 l'Armée de libération nationale (ALN) trop éprouvé par le choc frontal avec les barrages électrifiés de la Ligne Morice et les unités parachutistes de l'armée française cherche par tous les moyens à faire rentrer en Algérie le maximum possible d'unités de combat et des armes destinés aux chefs de l'ALN qui commandent les combats contre l’armée française à l'intérieur du pays. Confronté à une situation toujours plus délicate, l'armée française cherche des parades efficaces aux infiltrations des frontiere algéro-tunisienne plus nombreuses depuis l'indépendance de la Tunisie en 1956. En automne 1957, plus de par mois passent la frontière et sont distribuées dans les willayas I, II et III. Le gouvernement français exerce de fortes pressions sur la Tunisie, la menaçant même de représailles si les franchissements continuent. En vain. La solution ne peut être que militaire. La mission principale des forces françaises devient l'interception et la destruction des bandes armées qui traversent le barrage de la ligne Morice, s'étendant sur de la Méditerranée aux confins sahariens.

Conscients du danger d'asphyxie que représentent pour eux le barrage électrifié et miné de la « Ligne Morice » aux frontières, particulièrement celui qui les isole de la Tunisie, les chefs de l'ALN s'efforcent de trouver la parade. Dès la fin de l'année 1957, ils ont multiplié les sabotages de la haie électrifiée, creusé des tunnels pour passer sous l'obstacle et tenté de déborder le barrage par le sud.

La bataille des Frontières qui débute en janvier 1958 et dure jusqu'en mai va porter un coup fatal aux katibas de l'ALN. Cette défaite va déboucher sur une crise politique sans précédent au sein du FLN. Elle fut la plus grande bataille de toute la guerre d'Algérie qui a marqué néanmoins un tournant en faveur de l'armée française.

Les pertes françaises sont élevées : 273 tués et 800 blessés. Celles de l'ALN sont encore plus lourdes : près de , . Une énorme quantité d'armes individuelles et collectives a été saisie. Surtout, l'Algérie est hermétiquement « encagée ». Ayant perdu la bataille des frontières, l'Armée de libération nationale (ALN) ne peut plus être ravitaillée de l'extérieur. Militairement, la France a pratiquement gagné la bataille des frontières.

À la suite du départ de Félix Gaillard qui laisse vacant le poste de chef du gouvernement, une grave crise ministérielle s'installe le 15 avril. L'armée prend alors le pouvoir le 13 mai 1958, à Alger.

À 18 heures, Pierre Lagaillarde, leader étudiant de la rébellion contre la république française et commandant de réserve, lance ses miliciens du Groupe des 7 à l'assaut de l'immeuble du Gouvernement Général d'Alger, symbole de l'autorité nationale et de la République française. À 18 h 30 le « GG » présidé par le gouverneur Lacoste (SFIO) tombe aux mains des rebelles. À Paris, en réaction au « putsch d'Alger », le Gouvernement Pierre Pflimlin (MRP) est créé, il durera jusqu'au 28 mai 1958. L'image de la France dans le monde, et plus particulièrement en Europe occidentale en est fortement dégradée.

Pendant ce temps à Alger, le général Massu, commandant la division parachutiste de la bataille d'Alger, prend la tête du comité de Salut Public et fait savoir au président René Coty de l'Union républicaine (UR) qu'il attend la formation d'un « gouvernement de Salut Public ».

Le 16 mai, des manifestations de « fraternisation » entre Européens et musulmans ont lieu sur la place du Forum à Alger. À propos de ces événements, le Président du Conseil de Gaulle déclare lors de son premier voyage en Algérie, le 6 juin 1958 à Mostaganem, département d'Oran :

Le juin, à la suite de l'Opération Résurrection en Corse qui annonce l'imminence d'un putsch à Paris, le président annonce qu'il délègue ses pouvoirs au « plus illustre des Français », le général de Gaulle. Celui-ci forme un gouvernement de salut public et dans la foulée annonce la création d'une nouvelle constitution. C'est la fin de la Quatrième République.

Proposé sous la présidence de la République de René Coty et du gouvernement dirigé par Charles de Gaulle, le Référendum du 28 septembre 1958 demandait aux Français de ratifier le texte de la nouvelle Constitution qui posait les fondements de la Cinquième République. Confortée par plus des quatre cinquièmes des voix, la Constitution fut promulguée le 4 octobre 1958 et la République proclamée le jour suivant. Dans les colonies françaises, le référendum vise également à la création de la Communauté française.

Concernant la signification du référendum en l'Algérie, le général de Gaulle déclare le 30 août 1958 :

Par leur vote, les habitants de l’Algérie vont fournir une réponse à la question de leur propre destin. Les bulletins qu’ils mettront dans l’urne auront, sur un point capital, une claire signification. Pour chacun, répondre « oui » dans les circonstances présentes, cela voudra dire, tout au moins, que l’on veut se comporter comme un Français à part entière et que l’on croit que l’évolution nécessaire de l’Algérie doit s’accomplir dans le cadre français 96 % des Algériens, Européens et musulmans, soit 75 % des inscrits, disent « oui » à la nouvelle Constitution malgré les appels en faveur du boycottage lancé par le FLN. Il s'agit du premier scrutin auquel les femmes algériennes participent. Après les résultats du référendum en Algérie, de Gaulle déclare le 3 octobre à Constantine :

Trois millions et demi d'hommes et de femmes d'Algérie, sans distinction de communauté et dans l'égalité totale, sont venus des villages de toutes les régions et des quartiers de toutes les villes apporter à la France et à moi-même le bulletin de leur confiance. Ils l'ont fait tout simplement sans que quiconque les y contraigne et en dépit des menaces que des fanatiques font peser sur eux, sur leurs familles et sur leurs biens. Il y a là un fait aussi clair que l'éclatante lumière du ciel. Et ce fait est capital […] pour cette raison qu'il engage l'une envers l'autre et pour toujours l'Algérie et la France.

Il annonce également un vaste plan d'investissement en Algérie, le Plan de Constantine, laissant entendre un engagement durable de la France en Algérie. Cependant la toute nouvelle constitution prévoit dans son article 53 qu'une partie du territoire français puisse être cédée avec l'accord des populations concernées en vertu d'une simple loi.

Le 16 septembre 1959, De Gaulle ouvre dans un discours la voie à l'autodétermination. Il annonce que l'ensemble des Algériens auront à se prononcer sur leur avenir. Trois options se dessinent :

De Gaulle ne cache pas son hostilité aux deux premières solutions. Selon lui, la première risque de conduire à la misère et à une dictature communiste. En ce qui concerne la seconde, il avait expliqué à Alain Peyrefitte, en mars 1959 : 

La possibilité d'une sécession, ouverte par ce discours du 16 septembre et l'utilisation du suffrage universel, inquiète les partisans de l'Algérie Française.

En août 1958, les Français de la métropole découvrent que la guerre a franchi la Méditerranée. Dans la nuit du 26, une quinzaine d'attentats ont été commis dans plusieurs régions ayant pour cibles militaires, postes de police, voies ferrées, dépôts d'essence et raffineries. Les attentats du mois d'août font 17 morts parmi les policiers, 6 parmi les militaires.

Ces attentats ont pour but de démontrer à l'opinion publique française que le FLN est toujours actif. Néanmoins, l'essentiel de l'effort militaire de l'organisation algérienne se porte contre le Mouvement national algérien (MNA) beaucoup mieux implanté en France. Cette guerre civile entre les deux organisations indépendantistes sera extrêmement sanglante. Elle est à l'origine de dont algériens pour seulement civiles françaises, , et musulmans.

La Fédération de France du FLN est ainsi parvenue à prendre le contrôle de la communauté algérienne établie en France et des importantes collectes de fonds venant de la métropole en éliminant les partisans de Messali Hadj.

Le colonel Amirouche qui voulait se présenter à Tunis pour rencontrer le Gouvernement provisoire de la République algérienne (GPRA), le 6 mars 1959, se met en route accompagnés par 40 combattants. Son itinéraire fut vraisemblablement communiqué au commandement français par un opérateur radio du MLAG aux ordres d'Abdelhafid Boussouf, qui désirait se débarrasser de deux « contestataires » trop encombrants.

Pris dans une embuscade, le groupe se fait encercler par des éléments importants de l'armée française. Après un combat, violent et inégal, on dénombre cinq prisonniers et trente-cinq tués algériens. Parmi les cadavres, le colonel Amirouche et Si El Haouès.

Jusqu'à la fin de 1958, l'initiative appartient aux maquisards de Armée de libération nationale (ALN). Le général Challe nommé en décembre commandant en chef des forces armées en Algérie, est chargé d'inverser la tendance. Avec ses collaborateurs il présente le plan qui portera son nom. Il doit permettre aux Français de profiter à plein de l'avantage que leur donne la puissance de leur armée, supérieurement équipée et ravitaillée, et composée de contre dans l'ALN. Même s'il apparaît essentiellement comme un homme d'état-major, le général Challe va très vite donner confiance aux cadres de l'armée qui ont longtemps eu l'impression de manquer de perspectives d'ensemble.

Jusque-là, l'armée française privilégiait le « quadrillage » : des unités fixes étaient chargées de tenir les points sensibles du pays. Tandis que les unités mobiles de « réserve générale » comprenant en particulier les régiments parachutistes se trouvaient trop souvent réduites à des missions ponctuelles. Le « plan Challe » consiste à dégager le maximum de ces troupes de réserve et à les engager de manière systématique, en concentrant successivement leurs efforts sur une série de zones données. Le but est : d'affaiblir et de désorganiser les unes après les autres les unités de l'ALN. Elles seront en effet incapables de se reconstituer puisque, depuis 1958, des barrages électrifiés solidement gardés verrouillent l'accès aux frontières marocaine et tunisienne, et la Marine assure une surveillance à peu près totale des côtes. Cette phase de démantèlement achevée, les troupes de quadrillage seront ensuite assez fortes pour affronter seules ce qui subsistera des groupes armés, au moyen de petites unités légères et mobiles, les « commandos de chasse ».

La mise en œuvre du plan repose sur deux éléments essentiels : le renseignement et la mobilité des troupes. Le renseignement est placé sous la responsabilité du CCI (centre de coordination interarmées), représenté, aux échelons régionaux, par les DOP (Dispositif opérationnel de protection, qui travaillent avec les officiers de renseignements (OR) des unités. Les informations obtenues lors des interrogatoires de prisonniers permettent d'étudier minutieusement les zones de déplacement et de refuge des unités de l'ALN. Les troupes d'intervention seront alors envoyées dans les délais les plus rapides, notamment par hélicoptère. Tandis que les « commandos de chasse » constitués par les harkis - Algériens musulmans engagés aux côtés de l'armée française régulière - pourront traquer l'adversaire dans les terrains les plus difficiles.

Le général Challe compte beaucoup sur ces supplétifs, volontaires pour un service court de six ou douze mois renouvelables. « Nous ne pacifierons pas l'Algérie sans les Algériens », écrit-il en 1959. En plus des (chiffre de 1960), il veut parler des et des des Groupes mobiles de sécurité (GMS), nouveau nom des GMPR (groupes mobiles de protection rurale). Ou encore des petites milices dites « groupes d'autodéfense » (GAD), organisées, plus ou moins spontanément, dans des villages hostiles aux combattants de l'ALN. Leur effectif se monterait à une soixantaine de milliers d'hommes, dont une trentaine de milliers armés par la France.

Les soldats français vont chercher les maquisards de l'ALN sur leur terrain. Entre février 1959 et septembre 1960, les opérations militaires prévues par le « plan Challe » balaient le nord de l'Algérie d'Ouest en Est. Du plus facile, l'Oranais, au plus difficile, le Nord Constantinois, largement dominé par l'ALN. Après le départ de Challe en avril 1960, le général Crépin prend le relais et complète le dispositif avec les opérations « Cigale », « Prométhée », « Flammèches » et « Trident » qui s'étalent jusqu'en avril 1961. Les chefs de l'armée française créent des zones interdites qu'ils vident de leur population. Ils veulent ainsi isoler les combattants de l'ALN des civils qui les nourrissent, les soignent et les cachent. les habitants sont regroupés dans des villages près des postes militaires. En 1960, plus de deux millions de personnes sont concernées. Pauvreté accrue, perte des valeurs, les conséquences humaines, économiques et sociales sont dramatiques pour ces civils coupés de leurs terres.
Que ce soit par la route, par les airs ou encore par voie maritime vingt-cinq mille hommes venus renforcer les quinze mille militaires du « plan Challe », il commence par la wilaya V, la plus avancée dans la voie de la pacification, du 6 février au 6 avril 1959, puis il continue en wilaya IV par l'opération « Courroie », couronne montagneuse de l'Algérois et Ouarsenis, du 18 avril au 19 juin, et, avec une moindre intensité, dans le Sud Département d'Oranais, du 15 mai au 15 octobre. Pour éviter un repli vers l'est des unités kabyles, l'opération « Étincelle » traite le monts du Hodna, reliant la wilaya III à la wilaya I, du 8 au 20 juillet, puis l'opération « Jumelles » s'appesantit sur la wilaya III, du 22 juillet 1959 à la fin de mars 1960. Peu après, les opérations « Pierres précieuses» (« Rubis », « Saphir», « Turquoise », « Émeraude » et « Topaze ») s'abattent sur la wilaya II, entre le 6 septembre et le 9 novembre 1959, jusqu'en avril 1960; puis une deuxième série d'opérations « Pierres précieuses » revient sur les mêmes régions pendant plusieurs mois, jusqu'en septembre 1960.

Après le départ du général Challe en avril, son successeur, le général Crépin, revient encore sur l'Ouarsenis (« Cigale », du 24 juillet au 24 septembre 1960) et sur l'Atlas saharien (opérations « Prométhée », d'avril à novembre 1960), mais il porte son principal effort sur la wilaya I : opération « Flammèches » dans les monts du Hodna, du 21 au 31 mai, puis opérations «Trident » d'octobre 1960 jusqu'en avril 1961. Dans toutes ces régions, les commandos de chasse prennent la relève des réserves générales. En même temps, l'armée continue à démanteler par tous les moyens l'OPA qui encadre la population. C'est la tâche des officiers de renseignement et d'organismes spécialisés en marge de la hiérarchie militaire ordinaire : les DOP créés en 1957 dans le cadre du Centre de coordination interarmées (CCI), et les centres de renseignement d'action (CRA), créés en 1959.

Le « plan Challe » a permis à l'armée française de reprendre assez largement l'initiative des opérations. Il a infligé à l'Armée de libération nationale (ALN) de grosses pertes, sans doute la moitié de son potentiel estimé, soit . Les survivants ont été contraints de se disperser et de se cacher. Leur moral s'est trouvé d'autant plus atteint qu'ils ont eu le sentiment de ne pas avoir été soutenus par la direction de leur mouvement, installée en Tunisie et au Maroc. Un nombre non négligeable de combattants sont passés dans le camp français. Certains responsables ont même accepté d'entrer en contact avec les autorités françaises pour mettre fin aux combats. Le chef de la wilaya IV, Si Salah, a été ainsi reçu secrètement à l'Élysée le 10 juin 1960. Mais certains militaires iront jusqu'à accuser ouvertement l'entourage du général de Gaulle d'avoir refusé d'exploiter ces ouvertures et d' avoir contribué à faire disparaître Si Salah (tué en juillet 1961 dans une embuscade) pour supprimer un témoin gênant.

Cette victoire militaire est-elle totale? La manière dont les services de propagande de l'armée ont présenté le « bilan » en termes de « hors-la-loi abattus », « armes saisies », ou « populations ralliées » comme s'il s' agissait d'autant de coups décisifs portés à l'ennemi, est sans doute exagérément optimiste. Les réalités sont moins satisfaisantes. Ainsi, le colonel Bigeard, recevant le général de Gaulle à Saïda en août 1959, déclare, après avoir présenté un ensemble de très brillants résultats : la « pacification semble se dérober comme un mirage, en dépit de progrès indiscutables, à mesure que le temps passe. [...] Le mal est profond, le cancer bien accroché.» La dissolution des katibas de l'ALN, éclatés en petits groupes de quelques hommes, moins vulnérables, pose notamment problème.

Par ailleurs, si l'efficacité militaire des « bandes » est devenue à peu près nulle, les capacités d'actions dites « terroristes » demeurent. Le général De Gaulle avait déclaré, le 16 septembre 1959, qu'on pourrait considérer comme acquis le retour à la paix lorsque le nombre « d'embuscades et attentats mortels » serait inférieur à 200 en un an. Or, à la fin de 1960, le nombre d'attentats contre les civils se monte à environ 300 par mois. Le nombre de morts du seul côté français s'élève à . La moitié sont des civils. Surtout, la guerre est loin d'être gagnée sur le plan politique. En Algérie, le réseau des militants FLN, capable de continuer l'action de propagande et d'encadrement, a réussi à survivre. Les manifestations musulmanes d'Alger, en décembre 1960, soulignent la popularité de l'idée d'indépendance. En France, la guerre divise de plus en plus l'opinion, et la participation des appelés du contingent aux opérations est de plus en plus mal acceptée. À l'étranger, le GPRA (Gouvernement provisoire de la République algérienne) bénéficie d'une audience croissante non seulement dans le monde arabe et les pays de l'Est (l'URSS et les États satellites, la Chine), mais aussi dans le Tiers-Monde, et même chez les alliés de la France, (comme les États-Unis et jusqu'à la République fédérale allemande). En octobre 1959, le général Challe déclare devant son état-major : « Leur propagande est meilleure que la nôtre. »

L'année 1960 débute dans une atmosphère tendue. De nombreux pieds-noirs n'acceptent pas que le général de Gaulle ait évoqué le droit à l'autodétermination du peuple algérien. Le rappel à Paris du général Massu va servir de détonateur à des journées insurrectionnelles appelées "semaine des barricades". Les ultra de l'Algérie française, avec à leur tête Pierre Lagaillarde, Jean-Jacques Susini et Joseph Ortiz, organisent une grande manifestation de protestation au cours de laquelle des incidents éclatent. Lagaillarde occupe avec ses partisans le quartier des facultés tandis que Joseph Ortiz investit le bâtiment de la Compagnie algérienne. Sur le plateau des Glières, là où se tient la manifestation, la foule n'a pas l'ampleur celle du 13 mai mais des barricades sont dressées. Alors que les gendarmes interviennent pour dégager les rues, des coups de feu éclatent: sont tués et une centaine sont blessés alors que les manifestants comptent et .
Lagaillarde reste retranché dans le quartier des facultés, appuyé par plusieurs unités de territoriaux en armes. Michel Debré ordonne à Delouvrier d'employer la force si nécessaire pour mettre fin aux émeutes d'Alger. De leur côté, les musulmans ne suivent pas et, sans que l'armée soit obligée d'ouvrir le feu, les pieds-noirs rentrent progressivement chez eux. Restent Lagaillarde et son dernier carré de fidèles. Le 30 janvier 1960, le colonel de parachutistes Dufour négocie avec les « rebelles » une sortie honorable. Lagaillarde et ses hommes défileront en silence avant de se rendre aux mains des forces de l'ordre. 
Les meneurs sont arrêtés et jugés par un tribunal militaire en métropole. Le procès dit « des Barricades » se tient à Paris au mois de novembre 1960. Les accusés Pierre Lagaillarde et Joseph Ortiz, mis en liberté provisoire pour la durée du procès, s'enfuiront à Madrid où ils fonderont l'OAS.

Dans un contexte où les maquis de l'intérieur ont été durement éprouvés, Si Salah responsable par intérim de la wilaya IV, depuis mai 1959, décide, en tant que responsable d'ouvrir des négociations directes avec les autorités françaises. Les dernières opérations ont fait perdre à la wilaya IV plus de 50 % de son armement et 45 % de ses effectifs. Elle compte encore environ. Si Salah, a fait un voyage en Tunisie d'où il est revenu « écœuré des intrigues, qui occupent les dirigeants du GPRA (Gouvernement provisoire de la République algérienne) à Tunis, beaucoup plus que le sort des maquisards ». 

En juillet 1959, il souligne la « désaffection des populations pour la cause ». Dans ce contexte, l'offre de la « Paix des Braves » présentée le 21 octobre 1958 par le chef de l'État, le général De Gaulle, puis le discours du 16 septembre 1959, qui ouvre la voie à l'autodétermination de l'Algérie, recueillent un écho favorable parmi les maquisards. 

Le 10 juin 1960, il se rend secrètement à l'Élysée et négocie directement avec le général de Gaulle un possible cessez-le-feu. Selon Bernard Tricot, témoin direct de la scène 

Un an plus tard, Si Salah, convoqué par le GPRA, se rendait en Tunisie, avec une faible escorte. Il sera tué le 20 juillet 1961 sur le chemin dans une embuscade tendue par un commando de chasse de l'armée française à Maillot, dans la région de Bouira (Kabylie). Ses derniers mots seront : « De Gaulle nous a trahis. C'est lui le responsable de mon sort.

Certains officiers français en vaudront beaucoup au général de Gaulle de n'avoir pas su utiliser cette occasion avec Si Salah pour engager des négociations avec les combattants de l'ALN de l'intérieur (et qui s'opposaient au GPRA basé en Tunisie). Cette « affaire Si Salah » sera l'une des causes du putsch contre de Gaulle en avril 1961.

Avec le putsch des généraux, l'armée française va connaitre sa plus forte crise de conscience. Après l'opération avortée du 10 décembre 1960 qui visait à faire basculer l'armée dans un anti-gaullisme intransigeant, l'insurrection d'avril 1961 de quatre généraux (Maurice Challe, Edmond Jouhaud, Raoul Salan et André Zeller) a tenu le monde en haleine pendant quatorze jours.

Le , par un référendum sur l'autodétermination en Algérie organisé en métropole et en Algérie, les électeurs s'étaient prononcés à près de 75 % en faveur de l'autodétermination. C'est alors que des négociations secrètes avaient été ouvertes entre le gouvernement français de Michel Debré et le Gouvernement provisoire de la République algérienne (GPRA) lié au FLN (Front de libération nationale). Une partie des cadres de l'armée, qui avaient mené sept années de durs combats sous la direction de plusieurs gouvernements depuis le début de la guerre d'Algérie, se sentit trahie par le général de Gaulle, et voulut s'opposer par un coup de force aux projets d'indépendance de l'Algérie. Le pouvoir gaulliste était bien informé depuis plusieurs mois par la police judiciaire d'Alger et les services de renseignements des intentions de certains militaires. L'année précédente, le 25 janvier 1960, pendant la semaine des barricades, le colonel Antoine Argoud s'était même entretenu avec Michel Debré pour demander un changement de politique, faute de quoi « une junte de colonels » renverserait le gouvernement pour maintenir l'Algérie comme territoire français. La tension étant montée tout au long de l'année 1960, une possibilité de coup d'État est alors dans tous les esprits, en particulier au printemps 1961.

Dans la nuit du 21 au 22 avril 1961, des unités de parachutistes, dirigés par les généraux Challe, Jouhaud, Salan et Zeller s'emparent des centres stratégiques d'Alger. À leurs côtés se trouve une équipe de colonels baroudeurs, pour la plupart anciens d'Indochine. Par ce coup de force, ils entendent tenir le serment fait par l'armée, de garder l'Algérie à la France.

Grâce à l'effet de surprise, le putsch semble, avoir parfaitement réussi à Alger. La radio, rebaptisée Radio France, diffuse des communiqués de victoire et des appels au ralliement. Cependant, les directeurs de service de la délégation générale, représentants de l'autorité officielle en Algérie, ne sont pas ralliés et les soutiens attendus des forces armées du Constantinois, de la Kabylie et de Oranais ne sont pas au rendez-vous.

En métropole, c'est le désarroi. Le renom des chefs des insurgés, la qualité des unités putschistes, la réussite apparente de l'opération témoignent de la gravité des événements. Le régime issu du coup d'État du 13 mai 1958 va-t-il être chassé par un coup d'État militaire? Le gouvernement est surpris par le déclenchement de l'opération. Certes, dès le 22 avril au matin, la branche parisienne du putsch est écrasée par une dizaine d'arrestations. Lors d'un Conseil des ministres extraordinaire, de Gaulle fait preuve de détermination. L'état d'urgence est proclamé.

Mais, au cours de la journée du dimanche , 23 avril, l'inquiétude s'accroît. L'attentisme est de rigueur. Le refus d'affrontement risque de tourner à l'avantage des putschistes.

De Gaulle décide d'intervenir à la télévision le soir même, à 20 heures. Il est en uniforme, il stigmatise un « quarteron de généraux en retraite », qu'il rend par avance responsables d'un éventuel drame national. Il délie les soldats de leur serment d'obéissance et annonce le recours à l'article 16. Solennellement, le Général demande leur aide aux Français. « Au nom de la France, j'ordonne que tous les moyens, je dis tous les moyens, soient employés pour barrer partout la route à ces hommes-là, en attendant de les réduire. J'interdis à tout Français et, d'abord, à tous les soldats, d'exécuter aucun de leurs.»

À 23 heures 45, c'est le Premier ministre, Michel Debré, qui appelle les Français à résister dans l'éventualité d'une opération aéroportée en métropole.

Le mouvement s'effrite. Le trouble est profond chez les officiers. Les putschistes sont accusés de diviser l'armée. 

Surtout, le contingent affirme son opposition. La résistance, passive et spontanée, constitue un sérieux revers. Malgré une manifestation de masse dans le centre d'Alger où le général Salan répète son serment de garder « l'Algérie à la France », le vent a tourné. Challe, convaincu de l'échec, est soucieux de mettre fin à l'affaire, d'autres sont tentés par la radicalisation et poussent le général Salan à prendre les commandes.

Mardi 25 avril au matin. La situation s'est encore aggravée pour les insurgés. Les défections se sont multipliées et le pouvoir central a repris l'offensive. Challe veut se rendre. Jouhaud et Salan veulent continuer. À 20 heures, les quatre généraux lancent un appel à la population d'Algérie.

Dans la soirée, la dramatisation est à son comble. Plusieurs milliers d'Algérois convergent au Forum aux cris d'« Algérie française ». À minuit et demi, Challe apparait pour la dernière fois au balcon du gouvernement général et veut parler : le micro ne fonctionne pas. Vers 1 h 50, les quatre généraux s'égaillent : Zeller disparaît dans la foule. Salan, Jouhaud et Challe rejoignent le PC du REP. Jouhaud et Salan décident de fuir, Challe est conduit à Paris le mercredi 26 avril. L'insurrection a pris fin en quatre jours.

La révolte d'avril 1961 est bien un putsch. : à la fois coup de force et entreprise militaire. Cette insurrection d'une partie de l'armée a constitué l'une des rares tentatives d'intervention de l'armée dans la politique que la France ait connues. La rupture entre une armée de plus en plus éloignée et ignorante des réalités de la France, en raison de son engagement dans les guerres de décolonisation, et la nation en pleine évolution, se manifeste à cette occasion.

Le 28 juin 1961, le Premier ministre Michel Debré fait officiellement part d'un ultime recours, la partition de l'Algérie, en s'appuyant sur l'expérience d'une même sécession dans plusieurs autres pays à cette époque (Afrique du Sud, Allemagne, Corée, Viêt Nam). L'idée est étudiée par le député Alain Peyrefitte à la demande de de Gaulle, le député gaulliste propose de regrouper entre Alger et Oran tous les Français de souche et les musulmans pro-français, de transférer dans le reste de l'Algérie tous les musulmans préférant vivre dans une Algérie dirigée par le FLN et de mettre en place une ligne de démarcation dans Alger, à l'instar de Berlin et Jérusalem, qui séparerait le quartier européen du quartier musulman. La proposition est rejetée par de Gaulle en novembre 1961. Pour Maurice Allais, si la solution de la partition, dont on a souvent dressé des , a rencontré peu de faveur c'est pour la seule raison qu'elle a été farouchement rejetée par les extrémistes des deux camps. Selon lui, cette partition était .

En juillet, à la suite de nouvelles tensions entre la Tunisie indépendante et Paris à propos de la base navale stratégique française de Bizerte, une guerre brève mais meurtrière ( à morts) éclate entre la France et la Tunisie, alliée du FLN, dont le territoire sert de sanctuaire à l'ALN.
L'importante communauté immigrée venue d'Algérie qui penchait majoritairement pour le MNA a été prise en main par le FLN en éliminant la plupart des cadres et des soutiens du Parti du chef nationaliste Messali Hadj. Pendant l'été 1961, la guerre d'Algérie entre dans une phase critique. Les négociations entre le gouvernement français et le gouvernement provisoire de la République algérienne (GPRA), émanation du FLN, en vue de la prochaine indépendance algérienne, provoquent des dissensions dans chaque camp. Au sein du FLN se joue une lutte entre les différents courants internes pour l'accès au pouvoir du futur État algérien. Fin août, le FLN reprend plus intensément ses attaques contre les policiers, amplifiant la frustration de ces derniers qui désapprouvent la « lenteur » et l'« indulgence » de la justice à l'égard des commandos appréhendés précédemment.

Pour lutter contre ce regain de violence, décision est prise en conseil interministériel le 5 octobre, d'instituer un couvre-feu envers les seuls Algériens. Ce couvre-feu entrave considérablement le FLN dans ses activités vespérales et nocturnes de réunions, de prélèvement des « cotisations », de préparation d'opérations, d'application de « sanctions » et d'exécutions sommaires qui se réduisent fortement. La Fédération de France menace d'étouffer.

En riposte, elle décide d'organiser une manifestation pacifique contre le couvre-feu imposé par le préfet de police de Paris Maurice Papon au soir du 17 octobre 1961 tout en sachant que celle-ci est « vouée, d'emblée à être durement réprimée ». La manifestation qui rassemble retentit des cris et slogans de « Vive l'Algérie algérienne ».

Les policiers, répondant aux ordres, pénètrent et disloquent le cortège. La manifestation est violemment matée par les forces de police : des blessés gisent sur la chaussée. Plus de sont arrêtées dans la nuit, sont parqués au Palais des Sports, au Stade Pierre-de-Coubertin et 860 au Centre d'identification de Vincennes. Enfin , classés comme meneurs ou dangereux, sont refoulés, le jeudi 19, par avion vers l'Algérie.

Le bilan officiel de la préfecture de police est de 3 morts et 64 blessés. Le bilan réél est très discuté : de 30 à 50 morts pour le 17 octobre et les journées suivantes selon Jean-Paul Brunet, 98 pour Benjamin Stora et Linda Amiri, 120 pour House et MacMaster qui incluent un « cycle de deux mois connaissant son pic le plus visible dans la nuit du 17 octobre ».

Le 28 octobre, lorsque les émissaires français et algériens se rencontrent à nouveau à Bâle, les dirigeants français comme ceux du FLN reconnaissent implicitement qu'il est dans leur intérêt mutuel d'oublier les événements sanglants du 17 pour pouvoir passer à autre chose. Le GPRA est parvenu le 17 octobre à faire accroître la pression sur le gouvernement français au moment même où il s'apprête à négocier avec lui.

L'OAS est une organisation politico-militaire clandestine française, créée le pour la défense de la présence française en Algérie par tous les moyens, y compris le terrorisme à grande échelle. Un an après l'échec de la semaine des barricades, alors que le gouvernement français souhaite manifestement se désengager en Algérie, elle est créée à Madrid, lors d'une rencontre entre deux activistes importants, Jean-Jacques Susini et Pierre Lagaillarde, ralliant par la suite des militaires de haut rang, notamment le général Raoul Salan. Sur le plan pratique, l'organisation ne sera ni centralisée ni unifiée. Elle est divisée en trois branches plus ou moins indépendantes, parfois rivales : l'« OAS Madrid », l'« OAS Alger » et l'« OAS Métro ».

On estime que l'OAS a compté environ à membres actifs, dont 500 dans l'Ouest algérien, 200 en métropole et une vingtaine en Espagne. Les civils représentaient environ 2/3 des effectifs, l'autre tiers étant constitué de militaires, pour la plupart engagés, sous-officiers et officiers.

Les attentats de l'OAS viseront des personnalités politiques et administratives du gouvernement légal français, des intellectuels ou des organes de presse favorables à une négociation avec le FLN, en Algérie comme en métropole, ainsi que la population musulmane, soupçonnée de soutenir le FLN. Ses commandos prendront également pour cible les policiers, les enseignants, les fonctionnaires de l'administration fiscale, les commerçants musulmans. Les membres de l'OAS sont eux-mêmes pourchassés sans répit par les forces gaullistes. L'OAS sera largement soutenue par la population française d'Algérie, mais ses nombreux attentats aveugles la feront rejeter par l'opinion publique métropolitaine.

Rejetant le cessez-le feu proclamé le 13 mars 1962 par de Gaulle, les activistes de l'OAS se retranchent dans leur bastion de Bab El Oued, dit « quartier européen » d'Alger. La bataille qui s'ensuit donne lieu à une lutte entre les extrémistes du commando Delta et les gardes mobiles français. Bientôt l'aviation de l'aéronavale pilonne les bâtiments occupés par l'OAS, tandis que les chars de l'armée française prennent position dans le quartier en état de siège.

La vague des attentats commis par l'OAS culmine le 2 mai 1962, avec l'explosion d'un camion piégé au port d'Alger qui fait 110 morts et 150 blessés, en majorité des dockers et des demandeurs d'emploi. Au vaste élan de solidarité déclenché à partir des différents quartiers de la capitale par toute la population, européens et musulmans confondus, répondent les tirs des ultras de l’OAS à partir d'immeubles avoisinants, lesquels ont pris pour cibles les blessés, les ambulances et les personnes venues nombreuses participer aux opérations de secours, provoquant ainsi un véritable carnage.

Les différents attentats et attaques de l'OAS feront entre et ..

Officiellement 119 membres de l'OAS ont été tués. En 1962, 635 membres de l'OAS sont arrêtés. 224 sont ensuite jugés, dont 117 acquittés, cinquante-trois condamnés à une peine de prison avec sursis, trente-huit à une peine de prison ferme, trois sont condamnés à mort et fusillés (Roger Degueldre, Claude Piegts et Albert Dovecar)

Au 18 mars 1962, à la suite des accords d'Évian, Charles de Gaulle annonce à l'RTF (alors l'autorité en matière de radio et télévision) le cessez-le-feu et la tenue prochaine d'un référendum en métropole concernant l'autodétermination de l'Algérie. Ce référendum eut lieu le 8 avril 1962 et recueillit 90 % de oui. Il sera suivi d'un second référendum portant sur le collège unique en Algérie.

Le a lieu à Alger une manifestation de citoyens français, civils non armés, partisans du maintien du "statu quo" de l’Algérie française, décidée à forcer les barrages des forces de l'ordre qui fouillaient le quartier de Bab El Oued en état de siège à la suite du meurtre de six appelés du contingent par l'OAS, se heurta à un barrage tenu par l'armée française qui mitrailla la foule. Le dernier bilan officiel de la fusillade sera de et .

À la suite des accords d'Évian, le cessez-le-feu entre en vigueur le 19 mars 1962.

Après la répression d'Isly et l'anarchie créée par les attentats conjugués et croisés de l'OAS et du FLN, une fraction de la population européenne d'Algérie qui a adhéré aux visions de l'OAS refuse de vivre dans un territoire non français et commence un exode massif. Ce n'est que plus tard, en juin 1962, et dans la hâte, que se décideront les autres Français d'Algérie (Européens et israélites) - d'abord indécis car attachés à ce pays - à se faire « rapatrier » en France, sous la pression, tandis qu'une infime partie d'entre eux (les plus extrémistes), se sentant trahis par le gouvernement français, émigre à l'étranger, en particulier à Alicante sur la côte espagnole (foyer historique de nombreuses familles du département d'Oran) ou en Argentine.

Les pieds-noirs du département de Constantine rentreront presque tous en France où, malgré les mauvaises conditions d'accueil, ils continueront à vivre.

En juin 1962, Jacques Chevallier sert d'intermédiaire à des contacts secrets entre Jean-Jacques Susini, théoricien de l'OAS, et Abderrahmane Farès, président de l'Exécutif provisoire, en vue d'un accord pour l'arrêt des violences commises par l'OAS en contrepartie d'une amnestie de ses membres. Cependant, les accords sont dénoncés par des dirigeants du FLN, tandis que parallèlement des chefs de l'OAS refusent les principes d'un tel accord, accusant Susini de haute trahison. Il est menacé de mort par ses pairs. 

Finalement, l'accord tourne court et l'OAS poursuivra sa politique de la terre brûlée (sabotage du port d'Oran, incendie de la bibliothèque d'Alger, plastiquages, assassinats, etc.).

Le 3 juillet 1962, trois mois après les accords d'Évian et deux jours après le référendum d'autodétermination du juillet en Algérie, le président de Gaulle annonce officiellement la reconnaissance par la France de l'indépendance de l'Algérie, et un échange de lettres entre lui et le président de l'Exécutif Provisoire constate le transfert de souveraineté. L'Exécutif Provisoire était un organisme mis en place par les accords d'Évian et chargé d'assurer la direction du pays pendant la période de transition entre le cessez-le-feu et le transfert de souveraineté puis jusqu'à l'élection d'une assemblée constituante en Algérie.

L'arrivée au pouvoir du général de Gaulle après le 13 mai 1958 avait renforcé la croyance en un avenir possible pour l'Algérie française. Mais les annonces successives rapides de l'évolution de la politique algérienne du général de Gaulle instillent le doute, puis la révolte et enfin une forme de désespoir chez les partisans de l'Algérie française. Les temps forts de la période sont le Référendum sur l'autodétermination en Algérie (janvier 1961), l'échec du putsch d'Alger (avril 1961), le cessez-le-feu (mars 1962), étapes conduisant l'Algérie dans une spirale de violences réciproques. L'OAS (Organisation armée secrète), organisation clandestine anti-indépendantiste composée de militaires et de civils (d'Algérie et de métropole), est refondée début mai 1961 à Alger, et se lance dans l'action (Raoul Salan), c'est-à-dire des hold-up, vols d'armes, attaques de policiers, de barbouzes, de gendarmes mobiles. Puis, après le 19 mars 1962, l'OAS utilise en Algérie des méthodes terroristes en organisant aussi des attentats contre les Algériens et Européens qui étaient pour l'indépendance. Parallèlement, le FLN intensifie les attentats aveugles (pendant la trêve unilatérale de mai à août 1961) et décide de cibler davantage l'OAS à partir de novembre 1961. Le début de 1962 voit une escalade sans précédent du terrorisme, le nombre des attentats de l'OAS dépassant à la mi-janvier ceux du FLN dont les attentats s'arrêtent quelque temps autour du 19 mars 1962, pour reprendre sélectivement contre des membres de l'OAS, puis rapidement contre tout Européen, quel qu'il soit, notamment sous la forme d'enlèvements, le FLN n'appliquant pas les accords d'Evian, la France laissant faire.

Mais la violence prend aussi un aspect de guerre civile franco-française. Le quartier européen de Bab el Oued entre en insurrection le 23 mars 1962 et il s'ensuit une bataille entre pieds noirs européens anti-indépendantistes et métropolitains appelés du contingent. Afin de briser le blocus de Bab el Oued, des tracts de l'OAS appellent les civils à venir manifester sans armes et en arborant le drapeau français. Un barrage est forcé et le régiment de tirailleurs mitraille le cortège et fait 80 morts et 200 blessés civils. En effet, de nombreux blessés décéderont les jours suivants à l'hôpital Mustapha.

À l'approche du référendum d'autodétermination, des commandos de l'OAS lancent l'« opération 1830 », avant de quitter l'Algérie, en juin ; cela consiste à redonner à l'Algérie son état pré-colonial en pratiquant la politique de la terre brûlée pour supprimer toutes traces de la présence française, à cet effet le port d'Oran est incendié et la bibliothèque de l'Université d'Alger est détruite par le feu.

C'est ce contexte qui conduit un million de Français (Pieds-Noirs, les Harkis, les Juifs, etc.) à quitter l'Algérie en quelques mois, principalement d'avril à juin 1962. Un million de réfugiés algériens reviennent en Algérie. L'historien JJ Jordi n'hésitant pas à parler d'épuration ethnique.

L'indépendance de l'Algérie est proclamée après les résultats du référendum d'auto-détermination, mené cette fois dans les départements d'Algérie.

Quant aux colons stricto-sensu (c'est-à-dire dans le sens d'usage courant en Algérie de l'époque, de propriétaires-exploitants agricoles), leur départ est plus échelonné que celui de la masse des Pieds-Noirs. Il y aurait eu en septembre 1962 encore exploitant leurs terres en Algérie, sur un total estimé de 22 000. Toutes les terres, propriétés des Européens, étant nationalisées en octobre 1963, le départ définitif des colons et de leurs familles sera terminé en 1964.

Le texte des accords d'Evian précise : « Aucun Algérien ne pourra être contraint de quitter le territoire algérien ni empêché d'en sortir ». De nombreux Harkis ne sont pas cependant autorisés à être rapatriés au même titre que les Européens ou les Juifs , ou en sont . Des réfugiés ayant clandestinement rejoint la métropole sont rembarqués à destination de l'Algérie, tandis que les officiers français les ayant aidés (en désobéissant aux directives officielles) sont sanctionnés. Les clauses des accords d'Evian portant sur l'amnistie générale des crimes commis pendant la guerre et les garanties concédées aux Européens ne sont pas respectées par les indépendantistes. De nombreux Harkis et sont torturés et massacrés par des pro FLN dès la proclamation du cessez-le-feu, le 19 mars 1962, sans que le FLN y fasse obstacle, et en dépit des accords signés.

Lors de la proclamation de l'indépendance le 5 juillet 1962, des exécutions sommaires, des lynchages (place d'Armes, boulevard de Sébastopol, place Karguentah, boulevard de l'Industrie, rue d'Arzew et ailleurs), des actes de torture (pendaison, pendaison à des crocs de boucher) sont commis contre la minorité européenne lors du massacre d'Oran. L'armée française présente sur place assiste à la scène sans intervenir, De Gaulle ne voulant pas recommencer une guerre. Le bilan est de 700 morts pour les Pieds-Noirs...

Durant la période de la guerre d'Algérie, trois communautés principales cohabitent dans les départements français d'Algérie. La communauté majoritaire est celle dite « musulmane » formée d'Arabes (dont les descendants des Maures d'Espagne), de Kabyles, d'autres Berbères et des descendants des Ottomans. Elle cohabite avec deux minorités. La plus nombreuse des deux est la communauté dite de l'ensemble des populations d'origine « européenne » (principalement Alsace-Lorraine, Languedoc, Suisse) et « méditerranéenne » (Corse, Malte, les Pouilles, la Sardaigne, les Baléares, l'Andalousie) dénommés Pieds-Noirs (désignation à l'origine incertaine), tandis que la plus ancienne est celle dite des Juifs, autochtones au pays (la présence juive en Algérie est très ancienne et remonte à ) et qui n'a cessé de recevoir des apports au fil des siècles, notamment d'Espagne après la reconquista et de Livourne et dont les origines précèdent le débarquement français de 1830.

La diversité individuelle de chaque communauté se retrouve aussi dans la diversité et la contradiction des engagements politiques au sein de chacune d'elles ; en ce sens cette guerre, du moins telle qu'elle est vécue en Algérie, peut être perçue comme une guerre civile. Chaque communauté devient l'enjeu des différents belligérants qui tentent de susciter l'intérêt pour sa lutte et le ralliement de la population à sa cause.

Pour empêcher les populations d'aider le FLN, l'armée concentre, selon le rapport Rocard de 1959, un million de civils (dont la moitié d'enfants) des zones rurales dans des « camps de regroupement ». En février 1959, Michel Rocard, élève à l'ENA et en stage en Algérie, adresse un rapport sur les camps de regroupement à un proche de Paul Delouvrier, délégué général en Algérie. Le 31 mars, ce dernier donne l'ordre aux autorités militaires de suspendre les regroupements, et de concentrer les moyens sur l'amélioration des camps existants. Cet ordre sera assez mollement suivi. L'existence des camps de regroupement et leur état est en général ignoré de la population métropolitaine, jusqu'au 22 juillet 1959, où "Le Figaro" fait la une avec un reportage de Pierre Macaigne qui scandalise les lecteurs. Une campagne d'opinion se lance. La comparaison avec les camps de concentration est évoquée.

L'armée bombarde massivement au napalm et aux bombes incendiaires des villages et des hameaux dans les Aurès afin de mater la rébellion Chaouis.

De son côté, le FLN a recours aux attentats ciblés, aux assassinats et aux massacres de rivaux notamment du MNA.

Après l'indépendance, l'Armée française refuse d'intervenir pour assurer la sécurité de ses supplétifs musulmans, tout comme elle n'intervient pas le 5 juillet 1962 à Oran pour protéger les Européens.

Entre et auraient été massacrés par le FLN et se réfugient en France où ils sont parqués dans des camps d'internement sur ordre du gouvernement. En suivant les travaux d'autres historiens, le chiffre varie entre et sans toutefois parvenir à une estimation réelle, vu que le problème a pris une tournure idéologique partisane marquée par les passions, notamment en France.

Pendant la guerre d'Algérie, la communauté juive, profondément algérienne s'est orientée d'une manière générale vers une attitude neutraliste. Les organisations communautaires font preuve d'une extrême modération et refusent de prendre politiquement position, car elles considèrent que ce n'est pas de leur ressort, pourtant - malgré les nombreux attentats - certains embrassent la cause du FLN et d'autres s'engagent dans l'OAS.

Certains intellectuels juifs, comme Henri Alleg, ont pris fait et cause pour les nationalistes algériens du FLN (Front de libération nationale).

À l'inverse, des Juifs sympathisent avec l'OAS, à Alger et à Oran essentiellement (ils sont particulièrement actifs à Oran). Regroupés au sein des « Commandos Colline » Ces groupes sont liés aux réseaux « France Insurrection » et conduits par Elie Azoulai et Ben Attar, tuent certains élus musulmans, essaient de mettre le feu à une prison où sont détenus des hommes du FLN et abattent des officiers français (dont le lieutenant-colonel Rançon).

Des Juifs d'Algérie furent également victimes de l'OAS (en novembre 1961 à Alger : William Lévy, en décembre 1961 : Moïse Choukroun…).

La Radiodiffusion-télévision française (RTF), unique organisme audiovisuel français, est composée de cinq chaînes de radio et deux chaînes de télévision. Elle est placée sous le contrôle direct de l'État, conformément à l'ordonnance de 1945 sur le monopole d'État sur les ondes nationales. En conséquence, l'Élysée contrôle entièrement l'information et s'arroge le droit de censure, la métropole ne perçoit les événements d'Algérie que par le prisme de l'État.

En Algérie, la presse écrite, aux mains du capital des grands propriétaires terriens, publie chaque jour tout le long du conflit des unes dans lesquelles l'information s'apparente plus aux communiqués de la victoire en usage dans les dictatures totalitaires et où les victimes civiles musulmanes étaient systématiquement assimilées, d'abord, à des hors-la-loi, puis à des combattants rebelles. Les bilans des tués parmi ces derniers, fournis par les services du renseignement étaient toujours importants.

La guerre d'Algérie est le principal épisode de censure et de contrôle de l'information de l'après guerre. Les gouvernements des et Républiques l'emploient non seulement pour des raisons de sécurité militaire, mais aussi pour préserver le moral et empêcher la constitution d'une opposition organisée à la guerre. Répression et intimidation sont également employées, avec arrestations des journalistes et accusation des rédactions d'être des traitres à la patrie. Les saisies de journaux sont courantes, certains titres comme "France Observateur" ou "L'Humanité" sont régulièrement retirés des ventes en France, en Algérie c'est "L'Express", "Libération" et l"'Humanité" qui sont complètement interdits. Les saisies ont un impact économique sur la presse vu qu'elles ont lieu après l'impression, ce qui multiplie les coûts de production. Le plus grand scandale dû à la censure au cours de cette période est lié au livre "La Question" qui raconte l'interdiction du journal Alger républicain en Algérie, l'arrestation de son directeur et sa torture par des militaires. Les articles qui traitent du livre sont saisis de même que le livre lui-même, ce qui constitue en France la première saisie d'un livre pour motifs politiques depuis le selon son éditeur. C'est lors de la guerre d'Algérie que "Le Canard enchaîné" devient le journal d'investigation que l'on connaît aujourd'hui, dévoilant dans une chronique intitulée "Carnets de route de l'ami Bidasse" le quotidien des soldats appelés du contingents, mais aussi des informations militaires confidentielles. L'armée française cherche activement les informateurs du "Palmipède", classant les suspects en tant que « BE » (« bidasse éventuel »), ou « BP » (« bidasse probable »). Cette nouvelle orientation accompagnée d'une liberté de ton originale permet au "Canard" d'atteindre les diffusés en 1962.

Durant la guerre d'Algérie, le ministère de l'Intérieur français obtint en 1957 la possibilité de recourir à nouveau à l'internement administratif collectif. Plusieurs centres d'assignation à résidence surveillée furent créés en métropole sur des sites militaires : Larzac (Aveyron), Rivesaltes (Pyrénées-Orientales), Saint-Maurice-l'Ardoise (Gard), Thol (Ain), Vadenay (Marne). Près de suspectés d'être membres du Front de libération nationale (FLN) y furent internés.

En réaction à un article de "L'Express" condamnant la pratique de la torture par l'armée française en Algérie, une manifestation accusant la torture pratiquée par le FLN est organisée. Le 22 juillet 1956, le capitaine Moureau, chargé des affaires indigènes, est enlevé à Bouizakarne, Maroc et remis au FLN qui le torture pendant un an. Un commando français le retrouve, mutilé, et abrège ses souffrances à sa demande.

La manifestation en hommage au capitaine Moureau, captif du FLN, a lieu à la Place de l'Étoile le 3 avril 1957 et dégénère en affrontement avec la police parisienne sur les Champs-Élysées.

Le 13 mai 1958 a lieu aux Champs-Élysées une manifestation en mémoire des trois militaires français faits prisonniers par le FLN qui les a exécutés le 9 mai.

Les Trente ou Action civique non violente (ACNV) est un groupe hétéroclite, composé de déserteurs refusant de remplir leur obligation de servir en Algérie, d'hommes d'église et de musulmans, qui s'est livré à des manifestations pacifistes en métropole.

À la suite du procès des « porteurs de valises » du réseau Jeanson, des intellectuels et artistes lancent le manifeste des 121 le . En réaction, le maréchal Alphonse Juin lance le « contre-manifeste des 340 ».

Le maire d'Évian, est assassiné par l'OAS le 31 mars 1961 pour avoir officiellement accueilli dans sa ville des délégués du FLN venus négocier avec une délégation française.

Au début de l'année 1962, sous l'impulsion d'André Canal, la Mission III (branche métropolitaine de l'OAS) multiplie les attentats en région parisienne. Le 4 janvier, un commando en voiture mitraille l'immeuble du Parti communiste, place Kossuth, blessant grièvement un militant au balcon du étage.

Dans la nuit du 6 au 7 janvier, c'est le domicile de Jean-Paul Sartre qui est l'objet d'un plasticage. Le 24 janvier, on compte 21 explosions dans le département de la Seine, visant des personnalités ou des organisations supposées hostiles à l'idéologie de l'OAS.

Un quadrillage policier est mis en place dans Paris, ce qui n'empêche que, dans l'après-midi du 7 février, dix charges plastiques explosent au domicile de diverses personnalités : deux professeurs de droit, Roger Pinto et Georges Vedel, deux journalistes, Pierre Bromberger, du "Figaro", et Vladimir Pozner, blessé grièvement, deux officiers, le sénateur communiste Raymond Guyot dont la femme est blessée. Un dernier attentat qui vise André Malraux blesse au visage une fillette de 4 ans, Delphine Renard.

Le bilan humain a longtemps été difficile à établir étant données les divergences de l'histoire officielle reconnue par les deux pays.
Les sources divergent beaucoup sur la question des pertes algériennes, qui sont difficiles à évaluer avec précision, faute d'une enquête réalisée dans toutes les localités d'Algérie. Benjamin Stora évoque la difficulté de décompter le nombre des victimes civiles: tués dans les zones éloignées, victimes des opérations armées ou de bombardements de l'armée française, exécutées par l'ALN et ceux pris entre deux feux.

Le général de Gaulle parlait de victimes en octobre 1958, et de en novembre 1959.

Selon Djamila Amrane (archives algériennes), sur un total cumulé de "moudjahidines" ( du FLN et du FLN) ont été tués, valeur qui correspond sensiblement à l’évaluation du Bureau.

Les civils forment la majeure partie des pertes humaines des populations musulmanes. Les chiffres de un million (journal "El Moudjahid", dès 1959), puis de un million et demi de morts, ont été avancés en Algérie, sans base historique sérieuse.

Des historiens et des démographes se sont penchés sur la question : 



La "World Peace Foundation", estime à les décès algériens globaux, incluant des combattants. Martin Evans donne un aperçu des sources, et des débats sur le nombre de personnes décédées à la suite du conflit. Citant le travail de l'historien Charles-Robert Ageron, Evans note une tendance à la hausse de la violence entre le FLN et l'armée française qui commence en novembre 1954, et culmine en avril 1958. La violence commise par l'OAS atteint son apogée juste après la période de cessez-le-feu. Les massacres anti-harkis ont pris de l'ampleur en juillet 1962.

La guerre fratricide entre le FLN et le MNA, mouvement de Messali Hadj fait et en France et environ et blessés en Algérie.

Le FLN a été responsable, entre 1954 et le 19 mars 1962, de la mort de algériens et de , selon Guy Pervillé.

Quant au nombre de harkis massacrés après le cessez-le-feu, en violation des accords d'Evian selon lesquels il n'y aurait de représailles ou d'épuration ni du côté algérien, ni du côté français, les estimations varient entre et . Les chiffres des morts sont encore contestés car basés sur des témoignages locaux extrapolés à l’ensemble du pays, ce qui est peu probable. Toutefois selon Jean-Charles Jauffret : . Les massacres de supplétifs ont commencé dès mars 1962 et ont culminé à l’automne. Ils sont dus à des règlements de compte entre clans rivaux, des vengeances mais aussi au zèle des « marsiens », ralliés FLN de la , voulant montrer leur patriotisme de façade.

Par ailleurs, la torture pendant la guerre d'Algérie a été pratiquée aussi bien par l'armée française que par les insurgés algériens. Le nombre de torturés n'est pas connu avec certitude mais il devrait concerner des centaines de milliers d'indigènes et des centaines de prisonniers français.

Le nombre de disparus n'a jamais été connu. Certains ayant été exécutés et ensevelis dans des fosses communes clandestines ou dans des stades municipaux.

Les chiffres concernant les Français de métropole et d’Algérie, les « Français musulmans » et les légionnaires sont les mieux connus (JO des 4 et 7 août 1986) : militaires décédés dont 371 marins, de l'Armée de l'air et 487 gendarmes. Après le cessez-le-feu, 360 autres militaires sont morts, sans compter les disparus. Sur ces près de militaires décédés, on dénombre environ « Français musulmans » tués dont les deux tiers étaient des appelés et 1500 Légionnaires. En outre, on compte blessés militaires.

Pour les civils français, le total est de tués. Il faut y ajouter, après le cessez-le-feu, dont retrouvés (déclaration de Broglie du 24 novembre 1964, confirmée par lettre Santini du 9 novembre 1994), chiffres proches de ceux de Pervillé qui évoque sur 

Après le 2 juillet 1962, il est à déplorer plus de 500 militaires « morts pour la France » en Algérie, dont plus de la moitié ont été tués ou enlevés par l'ALN.

D'après le Service historique du ministère de la Défense, il est constaté :

La pratique de la torture par l'armée française est une réalité avérée notamment par les nombreux témoignages et des rapports furent adressés aux responsables politiques comme Pierre Mendès France ou Charles de Gaulle. Cependant on ne sait pas dans quelle mesure les responsables politiques étaient au courant de ces pratiques.
Selon Raphaëlle Branche, l'origine de la pratique de la torture pendant la guerre d'Algérie a au moins deux origines. D'une part, durant la colonisation, elle constituait un outil policier pour le maintien de l'ordre et, d'autre part, elle est issue des crispations de l'armée française qui ne voulait pas connaitre une deuxième décolonisation après la guerre d'Indochine.

Mais la torture ne peut être séparée du contexte de la guerre violente qui a lieu tant du côté français que du côté algérien. En effet, surpris par les attentats du FLN et sa progression dans l'opinion algérienne, l'État Français mettra en place des mesures spéciales, renforçant notamment l'usage à la justice militaire et la limitation de certaines libertés, comme la liberté de réunion. Dès le mois de mai 1955, commence à se répandre le concept de « responsabilité collective ». Elle est appliquée dans les Aurès en premier lieu, où le général Pallange décrète que le ' le plus proche d'un lieu de sabotage ou d'un attentat est collectivement responsable de l’événement. Les mesures prises peuvent aller d'une corvée collective pour réparer les dégâts le plus souvent, à une « prise d'otage » ou un bombardement du ' en question. Le principe de punition collective devient donc rapidement le corollaire de la responsabilité collective.

En 1955, Maurice Bourgès-Maunoury et le général Koenig, respectivement ministre de l'intérieur et de la défense, rédigent une « instruction concernant l'attitude à adopter vis-à-vis des rebelles en Algérie », prônant une réaction militaire « plus brutale, plus rapide, plus complète ». Ce texte confirme l'extension de la guerre non seulement aux rebelles armés mais aussi à la population algérienne susceptible de soutenir les rebelles. Max Lejeune, devenu secrétaire d'État aux forces armées, dira à propos du soulèvement populaire dans le Constantinois du 20 août 1955 réprimé qu'il n'est pas faux de parler de « répression aveugle ».
Le FLN, en grand partie dans la clandestinité, exerce son contrôle sur la population algérienne, y compris par des assassinats. Le plus connu est le massacre de Melouza, village qui aurait été acquis au messalistes (rivaux du FLN). Cependant, le général Salan constate que la population des douars environnants accuse l'armée française d'être responsable du massacre. Le FLN mène un guérilla intense contre les troupes françaises et des attentats qui touchent les infrastructures, mais aussi la population civile européenne. Le bilan des pertes occasionnées par le FLN est cependant plus lourd pour les militaires que les civils.

Concernant les exécutions capitales, le 17 mars 1956 sont publiées au J.O les lois 56-268 et 56-269 qui permettent aux tribunaux militaires français d’appliquer - sans instruction préalable - la peine de mort aux membres du FLN pris les armes à la main. Pour les bourreaux d'Alger, commencent alors les cadences infernales, avec les exécutions multiples qui se poursuivent jusqu'en 1958. Dans ses mémoires, le bourreau Fernand Meyssonnier rapporte « Dans l'histoire, c'est assez rare [...] En Algérie, entre 1956 et 1958, il y a eu seize exécutions doubles, quinze triples, huit quadruples et une quintuple. Oui, pendant le FLN c'était à la chaîne [...] Pour arriver à de telles hécatombes, il faut des époques politiques troubles comme la Terreur pendant la Révolution, l'Occupation où il y a eu neuf exécutés d'un coup le mai 1944, et... les "événements" d'Algérie  ». Au total, entre 1956 et 1962 pour environ condamnations prononcées, 222 Algériens ont été officiellement exécutés pendant la guerre d’Algérie. 142 l’ont été sous la République : 45 pendant que François Mitterrand était garde des Sceaux, soit une exécution tous les 10 jours en moyenne. La plus forte fréquence revient au Gouvernement Maurice Bourgès-Maunoury, qui a commis 29 exécutions en trois mois (soit une tous les trois jours). 80 exécutions ont eu lieu sous de Gaulle (soit une tous les 20 jours), bien qu’il ait amnistié 209 condamnés à mort en janvier 1959, commuant leur condamnation en peine de prison à vie.

Des lois d'amnistie ont été promulguées durant et après la guerre.

La première loi d'amnistie a concerné les membres du FLN et a été instaurée en Conseil des Ministres, le , par le premier président de la République prenant fonction, Charles de Gaulle, dans le cadre de la paix des braves ; il s'agit du premier conseil des ministres du président de Gaulle.

Les Accords d'Évian ont stipulé la garantie de non poursuite pour tous les actes commis par les parties en conflit en Algérie avant le 19 mars 1962. Cette politique a perduré après la guerre (lois ou décrets de 1962, 1966, de 1968, de 1974, de 1982 et de 1987). Une loi spécifique est votée le 17 juin 1966 pour amnistier les responsables de l'affaire Audin. Après les doubles lois de 1962, les seuls actes pouvant être poursuivis sont ceux de torture commis par les forces françaises contre des membres de l'OAS.

Dans son arrêt du 17 juin 2003, la Cour de cassation considère qu'il n'y a pas eu de crime contre l'humanité pendant la guerre d'Algérie. Elle écarte ainsi la possibilité de poursuites contre le général Paul Aussaresses. Sans nier les faits de torture, ni leur qualification de crime contre l'humanité au sens du code pénal actuel (entré en vigueur le mars 1994), la jurisprudence actuelle écarte la qualification de crime contre l'humanité au sens du code pénal de l'époque : dès lors que les événements sont antérieurs au mars 1994, seuls les faits commis par les puissances de l'Axe sont susceptibles de revêtir la qualification de crime contre l'humanité.

Des associations de défense des droits de l'homme comme la FIDH demandent un revirement.

En 1982, sous le gouvernement Mauroy, dans la continuité des amnisties antérieures, intervient l’« ultime normalisation administrative », la « révision de carrière » et la réhabilitation des généraux putschistes.

La loi du 23 février 2005 (dont seul l'article 4 a été retiré) accorde une « indemnité forfaitaire » et non imposable aux « personnes […] ayant fait l’objet, en relation directe avec les événements d’Algérie […], de condamnations ou de sanctions amnistiées » (art.13), et ne figurant pas parmi les bénéficiaires mentionnés dans la loi () du 3 décembre 1982. Athanase Georgopoulos, ancien de l'OAS réfugié en Espagne avant de revenir en France, a été nommé à la Commission chargée d'implémenter ces indemnisations (arrêté du 29 décembre 2005).

Le général de Bollardière, sanctionné de soixante jours d'arrêts de forteresse pour avoir dénoncé la torture, n'a par contre pas été réhabilité. Il fut à l'époque le seul officier supérieur français à condamner la torture.

Les accords signés à Évian le contenaient une clause dans laquelle le FLN, cosignataire des accords, (et non le Gouvernement provisoire de la République algérienne (GPRA), garant de l'autorité dans l'Algérie indépendante, mais qui n'en est pas officiellement signataire.), s'engageait à observer une amnistie générale garantissant formellement la non poursuite des militaires français ayant commis des crimes de sang .

La visée de cet accord était, pour le président de Gaulle, d'obtenir des garanties concernant l'Armée française et certains segments de la population européenne qui, en théorie, devait choisir de rester ou non en Algérie ainsi que la fraction de la partie musulmane ayant combattu avec l'armée française pour le maintien de l'Algérie au sein du territorial national français. Ainsi, les dispositions communes du "Chapitre II De l'indépendance et de la coopération / A) DE L'INDÉPENDANCE DE L'ALGÉRIE / II - Des droits et libertés des personnes et de leurs garanties" stipulent que :

Des violations des accords d'Évian ont débouché, d'une part, sur des incidents isolés tel le massacre d'Oran (communauté européenne) consécutif à la proclamation d'indépendance le et, d'autre part, au début des représailles et règlements de comptes contre des Harkis ainsi que ces derniers entre eux. (communauté musulmane). Globalement, aucun criminel de guerre, appartenant à l'un ou l'autre des belligérants n'a été poursuivi après la guerre.

Le FLN bénéficie de plusieurs soutiens étrangers, d'abord de la part des « pays frères », le royaume du Maroc et la république de Tunisie, qui échappant au statut de protectorats français en 1956 et dont les territoires de leurs confins frontaliers respectifs avec l'Algérie jouent le rôle de sanctuaires et de bases arrière. Cette complicité conduira à la bataille des frontières, qui amènera les Français à édifier sur la frontière algéro-tunisienne la fameuse Ligne Morice ainsi que la Ligne Challe. Autres conséquences, l'arraisonnement de l'avion Royal Air Maroc de Ben Bella et l'affaire de Sakiet qui, elle, débouche sur le coup d'État du 13 mai 1958 à Alger et la chute de la Quatrième République.

La Libye joue un rôle clé dans l'organisation des convois d'armes à destination des maquis algériens.

L'Égypte est également active dans le soutien du FLN, où le lieutenant-colonel Nasser qui prône une politique de panarabisme (nassérisme) et qui bénéficie de l'appui soviétique, fournit des armes au FLN. Les services secrets français (SDECE) qui surveillent le raïs, parviennent à démontrer l'aide militaire fournie au FLN par l'Égypte. Ainsi est arraisonné le cargo Athos transportant plus de d'armes tchèques le . Faisant valoir le soutien apporté par l'Égypte au FLN, la France planifie avec ses alliés israéliens et britanniques la campagne de Suez.

Globalement, la plupart des pays membres de la Ligue arabe, ceux du bloc afro-asiatique et les pays communistes d'Europe de l'Est ont soutenu l'indépendance de l'Algérie. Des pays d'Europe de l'Est comme la Yougoslavie (le maréchal Tito soutient dès le début l'insurrection algérienne), la Tchécoslovaquie et la Hongrie soutiendront activement le FLN de 1957 à 1962 en fournissant la plupart des armes à destination des maquis internes de l'ALN.

Les principaux soutiens diplomatiques du FLN puis du GPRA (gouvernement provisoire de la république algérienne) sont l'Union soviétique, la République populaire de Chine, l'Inde, l'Indonésie et le Viêt Nam.

Le 13 juin 1956, les 13 pays afro-asiatiques ont demandé la tenue d'une session extraordinaire consacrée à la situation en Algérie. Cependant, l'Alliance Atlantique dont les pays membres soutenaient la position de la France avait poussé le Conseil de sécurité à rejeter la demande.

Le octobre 1956, les délégations des pays africains, arabes et asiatiques introduisent une nouvelle demande pour inscrire la question algérienne à l'ordre du jour des travaux de la session de l'ONU.

En décembre 1957, le FLN parvient à faire inscrire « la question algérienne » à l'ordre du jour de la commission politique des Nations unies.

Le 16 juillet 1957, lors de la tenue de la session, le groupe de pays afro-asiatiques présenta de nouveau une demande d'inscription de la question algérienne à l'ordre du jour.

Au cours de la session tenue en 1958, le principe du droit du peuple algérien à l'autodétermination fut évoqué. Cet évènement a coïncidé avec la constitution, le 19 septembre 1959, du gouvernement provisoire algérien.

Le 16 juin 1959, le comité d'orientation des Nations unies a recommandé l'inscription officielle de la question algérienne à l'ordre du jour des travaux de l'assemblée générale et ce en dépit de l'opposition du délégué de la France, .

Le 22 août 1960, le Gouvernement provisoire algérien demande à l'ONU l'organisation d'un référendum populaire sous son contrôle.

Le 20 décembre 1961, l'Assemblée générale des Nations unies rend public un communiqué demandant aux deux parties de poursuivre les négociations. La résolution fut adoptée par 62 voix contre 32.

Dans un rapport de mission en Afrique remis au président Eisenhower au début de l'année 1957, Richard Nixon émettra de très sérieuses critiques à l'égard de la politique de la France en Algérie et se montrera favorable à l'indépendance de l'Algérie.

Le 2 juillet 1957, le sénateur John Fitzgerald Kennedy prononce un discours à la chambre haute (Sénat) des États-Unis sur le thème de la guerre d'Algérie dans lequel il souligne que cette guerre atroce a cessé de représenter un problème interne purement français et que les Américains sont directement concernés par ce conflit lequel a « dépouillé jusqu'à l'os les forces continentales de l'OTAN ».

À l'issue de ce discours, le sénateur Kennedy exhorte son pays à s'engager en faveur de l'indépendance de l'Algérie. Un projet de résolution est adressé à cet effet à l'administration US du président Dwight Eisenhower mais n'aboutira pas pour cause de procédures.

Sur le plan stratégique, le Pentagone s'inquiétait de voir les moyens de l'OTAN de plus en plus détournés au profit du soutien logistique des armées françaises en Algérie au détriment de l'équilibre des forces en Europe face au bloc de l'est.

Depuis mai 1958, le Comité International de la Croix Rouge sollicite le GPRA d'adhérer à l'article 3 des quatre conventions de Genève, possibilité offerte aux acteurs non internationaux d'un conflit armé de s'engager à respecter un minimum d'obligations humanitaires. Le GPRA dépose finalement le 20 juin 1960 ses instruments d'adhésion auprès du gouvernement suisse qui est dépositaire de ces accords, en exploitant habilement l'enregistrement de sa candidature par la Suisse comme reconnaissant internationalement un État algérien. La Suisse doit rappeler par une note du 20 juillet, que cette adhésion est sans portée juridique pour les États qui n'ont pas reconnu le GPRA, qu'elle comporte tout au plus un engagement pris par le GPRA de respecter les Conventions de Genève. Toutefois, succédant au GPRA, l'État algérien est réputé aujourd'hui avoir adhéré aux Conventions de Genève le 20 juin 1960.

La guerre d'Algérie offre une occasion inespérée à l'Italie de reprendre son rôle en Méditerranée sur fond d'anciennes rivalités avec la France. Tandis que la France s'oppose totalement à toute tentative d'internationalisation du conflit, l'Italie oscille, en fonction d'intérêts géopolitiques et de politique interne, entre une bienveillance manifeste à l'égard des indépendantistes algériens et une solidarité envers la France.

Cependant, c'est la gauche italienne qui se montre la plus intransigeante contre ce qu'elle appelle l’« impérialisme français » en Afrique du Nord. De nombreux activistes italiens soutiendront les réseaux du FLN en Europe.

Dès 1957, alors qu'il était à la tête de la compagnie pétrolière ENI, l'industriel italien Enrico Mattei tente de transformer l'Italie en base arrière du FLN en lui fournissant secrètement des financements et des facilités avant de disparaître dans un accident aérien suspect en octobre 1962.



114 unités de la force locale de l'ordre algérienne ont été créés dans toute l'Algérie. Elles étaient constitués de 10 % de militaires français de métropole et 90 % de militaires algériens, qui pendant la période transitoire étaient être au service de l'Exécutif provisoire algérien, jusqu'à l'indépendance du pays.

Les forces de l'ordre opérant en Algérie qualifièrent les groupes armés de l'ALN de « fellagas », en référence aux « coupeurs de têtes » ou « bandits » de grand chemin et aux « hors-la-loi », à partir de 1956, cette sémantique ne reflétait plus la réalité du terrain. En effet dans chacune des six zones géographiques (wilayas) créées deux ans après l'insurrection, les maquis s'étaient peu à peu structurés en véritables unités militaires avec leur règlement, leur état major et leur réseau logistique et leur service de renseignement, le MALG. Les combattants de la Wilaya III sous le commandement du colonel Amirouche en Kabylie par exemple portaient des insignes de grades comme une armée conventionnelle. Le congrès de la Soummam décida de réorganiser et surtout d'harmoniser l'existant et d'orienter l'ALN vers un fonctionnement centralisé, sinon bureaucratique désigné par les militaires français sous le nom de : OPA (Organisation politico-administrative du FLN).

Les historiens ont toujours buté sur la difficulté d'avancer un chiffre qui corresponde à la réalité. Pour des raisons de propagande mais aussi à cause du double mouvement du reflux à la suite des « pertes nombreuses » et de flux du fait du « recrutement incessant », il serait aléatoire de donner un chiffre précis.

Les estimations approximatives établies par des sources algériennes, notamment Mohamed Téguia, indiquent que l'ALN s'est engagée dans la lutte armée le novembre 1954 avec un millier d'hommes et qu'elle avait atteint son apogée en 1958 avec un maximum de hommes (de 60 à à l'intérieur et de 15 à sur l'ensemble des frontières au-delà des barrages) avant de subir de sérieux revers lors des opérations meurtrières du Plan Challe durant les années 1959-1961 et de connaître une régression pouvant aller jusqu'à 50 % des effectifs, soit quelque 30 à hommes à la fin de la guerre.

D'après des sources françaises et A. Clayton, l'ALN compte entre et combattants réguliers, tous dotés d'armes de guerre, et auxiliaires pourvus d'armes de complément. Pierre Clostermann devant l'Assemblée nationale : « En décembre 1955, l'effectif total des bandes armées ne dépassait pas hommes et les civils apportant leur soutien ne dépassaient pas un dixième de la population. En mai 1958, le FLN aligne combattants de première ligne qui disposent de l'aide de presque toute la population. »

Selon le général Maurice Faivre, il y avait trois à quatre fois plus de musulmans en armes du côté français que du côté de l'armée de libération. Face aux musulmans de l'Armée française (réguliers et supplétifs), l'ALN n'a jamais dépassé armés, dont en Tunisie et au Maroc ; à l'intérieur, il reste en janvier 1962 réguliers et , disposant de de guerre et de de chasse. Selon le ministère algérien des Anciens combattants, ont servi dans l'ALN, dont ont été tués.

L'ALN connaît un reflux dans les dernières années de la guerre, réduisant les effectifs de 50 %, soit entre et .

D'après les statistiques du Bureau (renseignements militaires), le nombre de combattants de l'ALN-FLN en 1960 était de l'ordre de répartis en petits groupes de 10 ou pouvant parfois se regrouper en des unités de .

Les armes utilisées durant les premiers mois de l'insurrection n'étaient que des armes de chasse et des poignards et quelques-unes de ces armes dataient de la Deuxième Guerre mondiale voire de la Première. L'armement a commencé à se développer et à se diversifier au fur et à mesure de l'évolution du combat. Ainsi les responsables de l'ALN ont pu obtenir des armes plus performantes en les récupérant d'abord auprès de l'ennemi en organisant des embuscades ainsi qu'en les faisant acheminer clandestinement à travers les frontières : mitrailleuses, mortiers, bazooka, pistolets mitrailleurs, fusils semi-automatiques, fusil militaire simple à recul et une variété de grenades, d'explosifs et de mines.

D'un autre cote, le FLN a pu avoir des armes des autorités françaises comme lors de l'Opération Oiseau bleu où les services secrets français SDECE voulaient créer des contre maquis en Kabylie à Azzazga où les hommes de Krim Belkacem Mehlal Said et Zaidet Ahmed jouant double jeu ont pu tromper l'ennemi et récupérer 300 armes modernes que les moudjahidines n'avaient pas..

Les armes utilisées autant par les militaires français que par les guérilleros algériens étaient principalement de fabrication française, allemande et américaine. Cette guerre fut l'occasion pour l'armée française d'utiliser à grande échelle des hélicoptères dans le cadre de la lutte anti-insurrectionnelle et des armes dites de troisième dimension dans l'un des engagements militaires les plus intensifs de l'histoire militaire de la France.

Une grande partie des combattants européens d'Algérie sont des militants du Parti communiste qui donne alors pour mot d'ordre « de concourir à la défaite de l'armée française partout où elle se trouve ». 

Daniel Timsit, est un étudiant en médecine et militant du Parti communiste algérien, qui, en désaccord avec ce dernier rejoint clandestinement le FLN afin d'y constituer « une « branche européenne » regroupant des militants pieds-noirs, chrétiens et juifs ». Les membres du réseau Timsit prennent part à la mise en place de laboratoires d'explosifs (élaboration de bombes à retardement) et à la lutte armée. Timsit est incarcéré en 1956.

L'aspirant du Train Henri Maillot, militant du Parti communiste algérien (PCA) et secrétaire général de l"'Union de la Jeunesse Démocratique algérienne", déserte son unité le 4 avril 1956 en emportant avec lui un camion d'armes et de munitions pour rejoindre un groupe de maquisards communistes qui s'était constitué dans la région d'Orléansville sous la responsabilité d'un membre du bureau clandestin du PCA, Abdelkader Babou. Le , le groupe de huit maquisards du « maquis rouge » que commande Henri Maillot est surpris par les troupes françaises près de Lamartine dans la région d'Orléansville. Trois membres du groupe sont tués au combat : Belkacem, Hammi et un Européen, Maurice Laban, membre du Parti communiste algérien. Henri Maillot, pris vivant, sera sommairement exécuté.

Raymonde Peschard (1927-1957), membre du Parti communiste algérien (PCA) et militante de la cause algérienne. Expulsée de Constantine en 1955. Interdite de séjour dans le Constantinois et l'Oranie. Recherchée par l'armée française, elle passe dans la clandestinité et elle s'engage dans l'(ALN), elle trouve la mort le 26 novembre 1957 dans l'Est-Constantinois au cours d'un accrochage entre l'A.L.N. et l'armée française.

Danièle Minne, membre du Parti communiste algérien (PCA), poseuse de bombe à l'Otomatic (26 janvier 1957) pour le compte du FLN, et complice de l'attentat à la bombe contre le "Milk Bar", militante active de la cause algérienne, amnistiée en 1962.

L'engagement militaire de la France durant la guerre d'Algérie fut massif et total. Tout ce qui restait de l'armée coloniale fut ramené en Algérie. Jusqu'à l'été 1955, les opérations militaires sont relativement réduites et mobilisent des effectifs peu nombreux, pour une part composés des forces stationnées au Maghreb et pour une autre part de renforts acheminés de métropole, il en va autrement à partir du mois d'août 1955 qui marque un tournant dans la guerre. Le cabinet Edgar Faure croit pouvoir maîtriser la situation grâce au dispositif en place. À la veille de l'insurrection de novembre 1954, la région militaire qui couvre tout le territoire de l'Algérie, compte moins de hommes. Les généraux français Cherrière et Larillot, commandants successifs en Algérie, réclament avec insistance des renforts. Une année après, c'est le rappel des disponibles.

Guy Mollet décide de faire appel au contingent pour ce que l'on appelle la pacification en Algérie. Entre mai et août 1955, supplémentaires débarquent en Algérie. Le général Salan prend le commandement de la région militaire que constitue l'Algérie, ce sont plus de qui quadrillent le territoire algérien . Les attentats se multiplient dans tout le territoire et la guérilla commence à se signaler dans les montagnes, les légionnaires et les parachutistes doivent intervenir régulièrement dans les Aurès, en Kabylie et ailleurs. L'armée française est sur la défensive. Chaque mois, des milliers d'attentats ont lieu, augmentant la violence de la répression, ainsi, rien que durant le mois de janvier 1957, le FLN a conduit plus de 100 attentats à Alger et près de dans le reste du pays. La guerre a mobilisé plus de 1,5 million de jeunes appelés entre 1954 et 1962 avec l'allongement de la durée du service militaire.

À ces effectifs, il faudrait ajouter le personnel de l'armée de l'Air, celui de la marine et celui de la gendarmerie, soit à la fin de 1959. Ce qui donne le chiffre, hors supplétifs, de en 1959 et qui estime les forces militaires françaises engagées en Algérie entre et hommes, il s'agit de la « plus formidable armada jamais vue en opération sur un territoire colonial ». Ces troupes sont encadrées par un nombre considérable d'officiers : , 600 à et lieutenants-colonels, à . Face à cette armée d'officiers, dans le meilleur des cas, il n'y avait que six colonels de Armée de libération nationale (ALN) dirigeant les six wilayas et à peine , tous formés sur le tas.
« Harki », terme désignant les auxiliaires algériens de l'armée française servant d'éclaireurs, d'interprètes, se déplaçant sans cesse dans le pays ou pour combattre tout simplement aux côtés des soldats français. Leur destin fut un des aspect douloureux de la guerre d'Algérie. Dès novembre 1954, le préfet Vaujour, directeur de la Sûreté nationale à Alger, obtient du gouverneur Roger Léonard et du ministre de l'Intérieur François Mitterrand l'autorisation de créer un corps spécial principalement composé de musulmans. En 1955, ce corps comprend 35 Groupes mobiles de protection rurale (GMPR). En 1957, ils seront 70 et deviendront l'année suivante les Groupes mobiles de sécurité (GMS). Les Sections administratives spécialisées (SAS) créées par Jacques Soustelle le 5 septembre 1955 sont dirigées par de jeunes officiers d'active ou de réserve. Elles sont protégées par un maghzen, groupe de 20 à 50 guerriers. Les harkas « troupes mobiles » sont officiellement reconnues le 8 février 1956 par le général Lorillot et leurs effectifs ne cessent d'augmenter. Dans son rapport sur le moral de l'armée, le général Salan indique que les harkis sont passés de à au cours de l'année 1957. Initialement pourvus d'armes de chasse.

Le 13 mars 1962, un rapport transmis à l'ONU évalue le nombre de musulmans pro-français à : de carrière, du contingent, , unités supplétives formées à partir de groupes civils d'autodéfense, parfois promus « commandos de chasse » ces unités, prévues à raison d'une par secteur militaire, sont constituées en Kabylie, dans les Aurès et l'Ouarsenis, , éléments de police constitués à l'échelon des localités, et placés sous les ordres des chefs des sections administratives spéciales (SAS), des GMPR (groupes mobiles de protection rurale), dénommés plus tard groupes mobiles de sécurité, assimilés aux CRS, de groupes civils d'autodéfense, , anciens combattants, fonctionnaires.

Si l'armée française disposait d'un armement désuet en Indochine, en Algérie, elle sera dotée des armes modernes et puisera dans les stocks de l'OTAN de la Seconde Guerre mondiale dans le cadre de la guerre froide. Le matériel fourni par l'OTAN est largement utilisé. Les équipements et les armements sont surtout américains durant les premières années de la guerre.







Napalm

Pour les fantassins français opérant au sol dans un pays immense et par des conditions climatiques extrêmes, l'appui de l'aviation s'est vite révélé indispensable. La flotte aérienne est impressionnante : plus de ( et ) seront affectés à la région militaire, soit plus de la moitié de la flottille aérienne française totale, ainsi que de . Durant les huit ans de guerre, les pilotes et les équipages se sont efforcés d'épauler leurs frères d'armes arpentant les djebels, leur conférant ainsi une supériorité qui compensait l'avantage du terrain dont bénéficiaient parfois les combattants de Front de libération Nationale (ALN).
Avions à réaction ou de vétérans de la Seconde Guerre mondiale comme le Republic P-47 Thunderbolt ou North American T-6 Texan de l'Armée de l'Air, le Corsair bleu de l'Aéronavale ou les Piper d'observation de l'Armée de terre qui n'hésitaient pas à piquer pour baliser les objectifs, les aviateurs français ont été engagés dans tous les combats, les zones d'opérations : la Kabylie, le Constantinois, l'Ouarsenis ou le sud Oranie au cours desquels les mitrailleuses des combattants algériens ne les ont pas ménagés.

À bord d'hélicoptères dont l'opération aéroportée est née durant la Guerre d'Algérie, entre toutes les armes, l'hélicoptère sera l'arme anti-guérilla par excellence, le plus connu est le Sikorsky H-34, armé d'un canon automatique MG 151/20 et de deux mitrailleuses M 2 de , qui participera à toute la guerre d'Algérie dans la lutte contre-guérilla. Cette guérilla va se développer dans les maquis dès 1955, surtout durant les grandes opérations du plan Challe, avant d'être finalement battue sur son propre terrain. La carrière de cet hélicoptère, au cours de ce conflit, est d'autant plus chargée de difficultés que ce terrain est en majorité montagneux et que les combattants algériens de l'ALN sont de redoutables coureurs de djebels. Ils ont parfaitement assimilé les principes mêmes de leur combat : harceler, disparaître, refuser le combat inégal. Pour les dénicher, il fallut des hélicoptères, et le Sikorsky H-34 était devenu l'outil indispensable par excellence.
Parmi les bases les plus importantes, on peut citer Oran-La Senia, qui couvre la frontière algéro-marocaine et qui est équipée d'avions de chasse, de bombardiers et d'hélicoptères, Bône-Les Salines chargée de la surveillance de la frontière algéro-tunisienne et spécialisée dans l'interception avec avions de chasse et bombardiers, Alger-Maison Blanche, Blida et Boufarik, Hassi Messaoud et Ouargla, pour le transport des troupes.

La Marine française joua également dans les opérations d'arraisonnement des cargaisons suspectées de trafic d'armes à destination du FLN et dans les opérations terrestres, sur les côtes ou en bombardant à l'artillerie navale gros calibres les zones contrôlées par les combattants de l'Armée de libération nationale (ALN). Les ports d'Alger, d'Oran-Mers el Kébir , de Bône, de Bejaïa et de Skikda ont joué un rôle important dans une guerre qui se déroule sur tous les fronts.

Appelée « Surmar Algérie » dès sa création tout au début des hostilités, puis rapidement simplement « Surmar », la surveillance maritime du littoral algérien est une organisation calquée sur celle que la Marine nationale a mise en œuvre pendant la guerre d'Indochine. Les choses sont toutefois un peu différentes puisque le Việt Minh recevait ses armes par la frontière de Chine, la voie maritime n'étant utilisée que pour des trafics complémentaires. De son côté, l'ALN est entièrement dépendante de l'étranger pour son approvisionnement en armes et en munitions. Avant l'indépendance du Maroc et de la Tunisie, la présence de troupes françaises dans ces deux pays rendait difficile l'établissement de bases relais où seraient stockées les armes avant de passer la frontière. L'ALN voit la situation s'améliorer en 1956, grâce à l'indépendance de la Tunisie et du Maroc, mais l'établissement des barrages aux frontière le ramène a une situation encore plus difficile que précédemment.

L'Armée de libération nationale (ALN) ne peut se contenter d'aussi modeste livraisons. Il fait venir des cargaisons importantes par des bâtiments de fort tonnage naviguant sous divers pavillons. Ces navires ne peuvent utiliser les ports d'Algérie. trop étroitement surveillés par la marine, aussi leurs capitaines préfèrent-ils rejoindre les ports du Maroc et de la Tunisie en évitant de pénétrer dans les eaux territoriales algériennes. Il est alors nécessaire de les intercepter en haute mer, ce qui aurait d'ailleurs pu poser un problème lors de la capture de l'Athos.

Dès le 31 mai 1955, une « instruction provisoire sur la surveillance des frontières maritimes » est publiée. Elle est suivie d'un arrêté interdisant la navigation et la pêche dans certaines zones côtières propices à la contrebande. Le 17 mars 1956 enfin, un décret étend la zone de visite douanière à de la côte. Sont également définies les mesures pouvant être prises par les bâtiments ou les aéronefs de la « Surmar » à l'encontre d'un navire suspect. Elles sont au nombre de cinq :

Selon les règles du droit international, la poursuite ne doit pas être interrompue mais elle peut être menée par plusieurs bâtiments et aéronefs se relayant.

Devant la guerre d'Algérie, les intellectuels français sont partagés.

Albert Camus, prix Nobel, lance à Alger, le 22 janvier 1956, "L'Appel pour une Trêve Civile", tandis que dehors sont proférées à son encontre des menaces de mort. Son plaidoyer pacifique pour une solution équitable du conflit est alors très mal compris, ce qui lui vaudra de rester méconnu de son vivant par ses compatriotes pieds-noirs en Algérie puis, après l'indépendance, par les Algériens qui lui ont reproché de ne pas avoir milité pour cette indépendance. Haï par les défenseurs du colonialisme français, il sera forcé de partir d'Alger sous protection. Il s'efforça toujours de rester entre deux extrêmes dénonçant l'injustice faite aux musulmans d'un côté, tout en déniant la caricature du « pied noir exploiteur » de l'autre.

L'histoire retient cette déclaration faite au lendemain de son obtention du prix Nobel de Littérature : « J'ai toujours condamné la terreur, je dois aussi condamner un terrorisme qui s'exerce aveuglement dans les rues d'Alger et qui peut un jour frapper ma mère ou ma famille. Je crois à la justice, mais je défendrai ma mère avant la justice » Camus proclame sa fidélité à l'« Algérie française » et doute de l’Algérie algérienne.

En septembre 1960 le Manifeste des 121, titré « Déclaration sur le droit à l’insoumission dans la guerre d’Algérie », est signé par des intellectuels, universitaires et artistes et publié dans le magazine "Vérité-Liberté". Il est né dans le sillage du groupe de la rue Saint-Benoît. Il a été pensé puis rédigé par Dionys Mascolo et Maurice Blanchot. Ce traité a permis de regrouper des personnalités de divers horizons dans un esprit libertaire et orienté à gauche. 

Jean-Paul Sartre, signataires du Manifeste des 121, est également un soutien de poids des membres du Réseau Jeanson lors de leur procès en septembre 1960. Il rédigea la préface de l'essai "Les Damnés de la Terre" » de Frantz Fanon et se prononça clairement pour l'indépendance de l'Algérie. Dans cette célèbre préface, il va jusqu'à écrire : « il faut tuer : abattre un Européen c'est faire d'une pierre deux coups, supprimer en même temps un oppresseur et un opprimé : restent un homme mort et un homme libre ; le survivant... ». Son appartement sera plastiqué deux fois par l'OAS

Ce texte provoqua rapidement un contre-manifeste, le "Manifeste des intellectuels français pour la résistance à l'abandon", paru en octobre 1960 dans l'hebdomadaire "Carrefour", dénonçant l'appui apporté au FLN par les signataires du Manifeste des 121 - ces « professeurs de trahison » - et défendant l'Algérie française. Il soutient l'action de la France et de l'armée en Algérie (« L'action de la France consiste, en fait comme en principe, à sauvegarder en Algérie les libertés (...) contre l'installation par la terreur d'un régime de dictature »), taxe le FLN de « minorité de rebelles fanatiques, terroristes et racistes » et dénie « aux apologistes de la désertion le droit de se poser en représentants de l'intelligence française ». Ce contre-manifeste bénéficie de soutiens plus nombreux.

Certains intellectuels, tels Francis Jeanson, mettent en pratique leurs idéaux anticolonialistes en transportant des fonds à destination du FLN. Reconnu coupable de haute trahison, celui-ci sera condamné en octobre 1960 à dix ans de réclusion.

Pour Edgar Morin, qui anima un comité contre la guerre d'Algérie et défendit Messali Hadj, une partie de la gauche française pensait avec "Les Temps modernes" « que le FLN était l'avant-garde de la révolution mondiale. Il y avait bel et bien une mythologie du FLN et celle-ci écartait tous les éléments gênants qui pouvaient la contredire »

Frantz Fanon, un psychiatre né à la Martinique, s'engage auprès des indépendantistes algériens dès le début de la guerre d'Algérie, en 1954, et noue des contacts avec certains officiers de l'ALN (Armée de libération nationale) ainsi qu'avec la direction politique du FLN, Ramdane Abane et Benyoucef Benkhedda en particulier. Il théorisa la terreur comme tactique révolutionnaire à travers son livre, "Les damnés de la terre" publié en 1961 et qui deviendra plus tard la bible de tous les mouvements révolutionnaires. En hommage à son soutien à la cause algérienne, deux hôpitaux en Algérie, l'hôpital psychiatrique de Blida, où il a travaillé, et l'hôpital de Béjaïa, portent son nom.

Les archives officielles de la guerre d'Algérie ne sont encore que partiellement disponibles et accessibles aux chercheurs en France, et inaccessibles en Algérie. La loi française du 15 juillet 2008 relative aux archives a raccourci les délais de communication des archives publiques, y compris pour certaines archives classifiées « secret défense » communicables après un délai de 50 ans. En 2008, au cours de la discussion de ce texte au Parlement, un amendement adopté par le Sénat français visait à prescrire un délai de 75 ans concernant les pièces « susceptibles de porter atteinte à la vie privée ». Cette disposition, vivement critiquée par des historiens car elle aurait accru les délais de communicabilité des archives relatives à la guerre d'Algérie, a finalement été retirée du texte au cours de son examen à l'Assemblée nationale.

En Algérie, cette guerre est appelée « Révolution algérienne » . Elle se nommera très peu de temps après son déclenchement « guerre de libération nationale » ou « guerre d'indépendance ». Le terme « révolution » est massivement adopté par le FLN à partir de 1956, année de son congrès de La Soummam, en Kabylie. Le terme prendra des connotations résolument socialistes dans les premières années d'indépendance. Pour les populations algériennes dans leur ensemble, ce fut « La Guerre ».

, la France a reconnu qu'il s'agissait d'une guerre en 1999, sous la présidence de Jacques Chirac. Toutefois, dans les textes législatifs notamment, l'expression officielle consacrée continue d'être « les événements d'Algérie ». Ce fait demeure unique dans l'époque contemporaine et constitue une exception française.

Le l'ONU ne reconnaît pas le droit de l'Algérie à l'indépendance à la suite du seul vote d'opposition de la France qui considère que le problème algérien est un problème interne.

Avec la reconnaissance officielle de la guerre par la représentation nationale française, la première position est en passe de disparaître, d'autant plus que l'expression « guerre d'Algérie » était déjà utilisée par les historiens et les journalistes français et étrangers depuis le déclenchement du conflit et que le grand public reconnaît également cette expression.

À l'époque même des faits, le gouvernement français, et un bonne partie de l'opinion publique (qui évolua d'ailleurs au cours de la guerre) considéraient qu'il ne s'agissait pas d'une guerre mais de troubles à l'ordre public et plus substantiellement des troubles contre l'ordre établi. Ces mots avaient des conséquences pratiques importantes : les insurgés ne pouvaient bénéficier du statut de prisonnier de guerre, et ce n'est que bien après la guerre que les pensions versées aux soldats français ou leurs veuves furent alignées sur celles versées dans le cas de guerres officielles.

Concernant les prisonniers français du FLN, des recherches récentes ont montré que la prise de prisonniers par le FLN était liée à sa stratégie visant à se poser en belligérant légitime, du fait de la non-reconnaissance de la guerre par le pouvoir français.

Si la France a reconnu la guerre, seuls ont été également reconnus officiellement des actes individuels commis par les militaires sans pour autant les condamner. Le caractère organisé, n'a, lui, jamais été reconnu, tout comme celui de l'abandon d'un certain nombre de harkis par la France, lors des derniers jours de la guerre.

Côté algérien, à l'occasion du cinquantième anniversaire du congrès de la Soummam et des massacres de Philippeville (actuellement Skikda), le président Abdelaziz Bouteflika a reconnu dans un message du 20 août 2006 que « Notre guerre de libération nationale a été menée par des hommes et des femmes, que l'élan libérateur portait le plus souvent à un haut niveau d'élévation morale, mais elle comporte des zones d'ombre à l'instar de tous les processus de transformation violente et rapide des sociétés humaines ». L’État algérien reconnaissait alors l'existence de faits jusque-là tabous et occultés comme la vérité sur la mort et le testament du colonel Amirouche ou même sur la personne qui a vendu Ali la Pointe… En 2009, il affirme que la Révolution a respecté les « Conventions internationales, dont celles de Genève ». À noter que la convention de Genève, dans sa version de 1949, interdit les attaques sans discrimination contre les populations civiles ce qui inclut et les bombardements d'objectifs civils et les attentats terroristes.

La guerre d'Algérie est devenu l'enjeu d'une bataille mémorielle entre la France et l'Algérie, chaque nation essayant d'imposer à l'autre sa version nationale/nationaliste des faits. De cette « guerre des mémoires » naissent, côté français, en 2005 des polémiques concernant ce que les médias locaux ont appelé, le projet de loi sur « le rôle positif de la colonisation » ainsi que sur le choix de la date du 19 mars pour commémorer la fin de la guerre.

Pour l'historien Guy Pervillé, en dépit de tous les apports de l'histoire récente, « la perception de la guerre d'Algérie par les militants et par les journalistes engagés [...] est restée très proche de ce qu’elle était en 1962 ». Loin de se pacifier et de voir les passions s'estomper, en France comme en Algérie, « la mémoire de la guerre d’Algérie a pris le dessus sur l’histoire ».

La guerre mémorielle a été un facteur constant des relations franco-algériennes depuis l'indépendance, relancée à chaque fois par le pouvoir algérien quand celui-ci souhaitait faire pression sur la France. Elle ressuscite lors de la crise déclenchée par la nationalisation des hydrocarbures sahariens en 1971, et particulièrement lors de la période de froid diplomatique entre les deux pays voulue par Houari Boumédiène en réponse au soutien accordé par Giscard d'Estaing au Maroc dans la répression du Front Polisario, soutenu par Alger dans le cadre du conflit du Sahara ex-espagnol (1975-1978).

La querelle franco-française liée à la date 19 mars se prolonge, le , avec la proposition de loi de Thierry Mariani (UMP) « visant à établir la reconnaissance par la France des souffrances subies par les citoyens français d'Algérie, victimes de crimes contre l'humanité du fait de leur appartenance ethnique, religieuse ou politique ».

La question de la repentance est une des constantes des relations franco-algériennes. Dès 1964, des voix se sont élevées pour condamner l'amnistie totale et générale accordée à tous les criminels de guerre et auteurs de crimes contre l'humanité durant le conflit. Cette amnistie est prévue par le texte même des accords d'Évian (deux fois : titre II, chapitre 1 §K pour les personnes déjà jugées et emprisonnées à l'époque et chapitre 2 §A, « Dispositions communes » pour tous les faits jusqu'à la date des accords), et aurait été confirmée en France par la loi du 31 juillet 1968 et en Algérie par son intégration dans le Code pénal bien que cela soit contesté par quelques avocats du côté algérien et FLN et du côté de certains Français d'Algérie.

Au niveau des manuels scolaires en usage en Algérie, les crimes colonialistes français en Algérie sont qualifiés de génocide et de crimes contre l'humanité depuis 1979. Un manuel d'histoire datant de 1985, retiré depuis une dizaine d'années, va encore plus loin en qualifiant la colonisation de vaste processus de destruction de la vie et de la culture de l'homme sur terre.

En 1999, 2004 et en 2007, le président algérien Abdelaziz Bouteflika a, en différentes occasions, qualifié la colonisation française de génocide culturel et a appelé la France à assumer son histoire en présentant des excuses formelles.

Le , le député FLN Moussa Abdi, l'un des deux cents députés de la chambre basse du parlement algérien, dans une proposition de loi au parlement algérien, déclare : « nous envisageons de créer des tribunaux spéciaux pour juger les responsables de crimes coloniaux ou de les poursuivre devant les tribunaux internationaux ».

En février 2010, cent vingt-cinq députés de différents partis politiques algériens, dont le FLN au pouvoir, signent une proposition de loi criminalisant le colonialisme français en Algérie.

Le conflit armé qui débute le novembre 1954 est qualifié de « Glorieuse Révolution » dans les discours officiels de la Présidence algérienne. Les agents du FLN se définissent comme des « résistants », alors que les autorités politiques françaises les qualifient de « rebelles ».

Parallèlement, en France, les manifestations d'hommage aux anti-indépendantistes, désignés dans la presse de l'époque comme des « activistes » mais qualifiés par l'ADIMAD (association proche de l'OAS) de « partisans patriotes de l’Algérie française », crée des controverses notamment en 2005 lors de l'affaire de la stèle de Marignane et 2006 lors de l'affaire de la flamme du soldat inconnu.

Algérie a eu depuis l'indépendance une politique mémorielle très active, politique qui s'inscrit dans continuité de « la propagande du PPA-MTLD et du FLN, et orchestrée par l’État ». Dès 1966; l'usage de la torture par l'Armée française contre le FLN et les communistes est dénoncé dans le film produit par Yacef Saâdi, membre du FLN ("La Bataille d'Alger").

Selon Guy Pervillé, « l'histoire officielle algérienne, qui est en réalité une mémoire officielle, n'a pas cessé de conditionner l’opinion publique ». Il estime qu'au-delà du crédit que les Algériens ont pu accorder à cette présentation officielle de l'histoire, « il faut néanmoins constater que la répétition d’une même propagande durant plusieurs générations finit par laisser des traces en effaçant la frontière entre le vrai et le faux ». Mohammed Harbi souligne également le rapport « très problématique » du pays avec son histoire. Selon lui, dans peu de pays autant qu'en Algérie, « l'Histoire est instrumentalisée pour traiter des problèmes actuels. ».

La modification du régime en 1989, l'instauration du multipartisme et de liberté de la presse, ne met pas un terme à l’existence d'une mémoire officielle, « qui rappelle ses principes fondamentaux dans son préambule et dans plusieurs de ses articles ». Durant longtemps, les historiens algériens bénéficient d'une liberté très limitée chez eux et publient en dehors de l'Algérie ce qu'il ne leur est pas permis de publier dans leur propre pays. La guerre civile algérienne ravive encore cette politique mémorielle avec la réutilisation par les deux camps d'un vocabulaire hérité de la guerre d'Algérie.

Cette difficulté à sortir de l'histoire officielle se manifeste notamment lors de l’interdiction du film de Jean-Pierre Lledo, "Algérie, histoires à ne pas dire" (2007).

En France, la mémoire de la guerre se caractérise longtemps par ce que Guy Pervillé décrit comme « une politique de l'oubli », l'incapacité devant un conflit qui avait profondément déchiré les Français entre eux « de reconstituer une mémoire nationale consensuelle ». Pour cette raison, la guerre d’Algérie est demeurée durant des années une guerre sans nom et sans commémoration officielle.

En France, une bataille mémorielle et politique divise en deux camps les anciens combattants d'Algérie, mais aussi en partie l'échiquier politique. L'enjeu en est la commémoration - ou pas - de la date du qui est celle du cessez-le-feu bilatéral en Algérie et donc de la fin formelle de la guerre d'Algérie. Ceux qui dénoncent cette date arguent du fait qu'il s'agit en réalité d'un arrêt formel des hostilités mais pas des violences, puisque d'une part l'OAS a poursuivi sa lutte anti-indépendantiste (bataille de Bab El Oued) en ignorant le cessez-le-feu entre la France et le FLN (termes des accords d'Évian du ) et que, de l'autre, les massacres ont continué telle la fusillade de la rue d'Isly le ou le massacre d'Oran le ). Le sénateur Paul Girod (UMP), en quête d'un « consensus », estime à le nombre des morts de l'après cessez-le-feu dans la Question écrite publiée dans le journal officiel du Sénat le .

Les partisans de la commémoration du 19 mars soutiennent la proposition pendant une décennie, comme en atteste le dossier de 2005 portant sur la « journée nationale du souvenir de la guerre d'Algérie ». Initialement, le Bernard Charles (Radical-citoyen-vert) dépose une proposition de loi « tendant à instituer une journée nationale du souvenir des victimes civiles et militaires de la guerre d'Algérie et des combats du Maroc et de Tunisie. » fixée par l'Article 2 au 19 mars, entre 2000 et 2001. Cette proposition est redéposée par Jean-Pierre Soisson (UMP), Alain Bocquet (Parti communiste français), Jean-Pierre Michel (Radical-citoyen-vert), Alain Néri (Parti Socialiste) et Didier Julia (UMP). Le texte dit « petite loi » est adopté par l'Assemblée nationale en première lecture le .

En 2003, le maire de Paris, Bertrand Delanoë (Parti socialiste), commémore le cessez-le-feu en baptisant une voie « place du Dix-Neuf-Mars-1962 ». De même, il existe des « rue du 19-mars-1962 » et « avenue du 19-mars-1962 » en France.

Depuis une loi française du 6 décembre 2012, le 19 mars, « jour anniversaire du cessez-le-feu en Algérie », est déclaré « journée nationale du souvenir et de recueillement à la mémoire des victimes civiles et militaires de la guerre d'Algérie et des combats en Tunisie et au Maroc ».















</doc>
<doc id="6660" url="https://fr.wikipedia.org/wiki?curid=6660" title="Évaluation économique de la biodiversité">
Évaluation économique de la biodiversité

Plusieurs tentatives d’évaluation économique de la biodiversité sont en cours depuis le début du , à la suite notamment de l'évaluation des écosystèmes pour le millénaire. 

Ces approches consistent généralement d'une part à définir les valeurs attachées à la biodiversité, et d'autre part porte sur les techniques disponibles pour mesurer les valeurs des services écologiques, dont leur valeur économique.

Certaines ont un volet prospectif nécessitant d'utiliser des scénarios d'évolution de la biodiversité (par exemple en France, le Commissariat général au développement durable (la veille de la mission prospective) a publié un document "Horizons 2030-2050" intitulé . 

Elles s'inscrivent dans une perspective de monétarisation de la nature et d'éléments environnementaux (temps, bruit, pollution, aménités, etc.).
Selon le Rapport dit "« rapport Chevassus-au-Louis »" (du nom de son auteur Bernard Chevassus-au-Louis), un des enjeux de cette monétarisation est de changer la perception que nous avons de la biodiversité ; souvent limitée " ; "
Elle s'inscrivent aussi dans l'initiative (2009) « l’Économie des écosystèmes et de la biodiversité » (TEEB) elle-même appuyant la révision du système
des Nations unies de comptabilité nationale, ou encore sur « l’Initiative pour une économie verte », avec un objectif sous-tendu ou annoncé de verdir la fiscalité, associer des critères d'écoéligibilité aux subventions publiques pour limiter leurs effets négatifs sur la biodiversité. 
Certains (vision plutôt libérale) imaginent aussi qu'un mécanisme de paiement pour les services écosystémiques ou encore l’intégration de la biodiversité dans la comptabilité des entreprises permettrait, en donnant une valeur monétaire à la biodiversité de la mieux faire respecter. D'autres estiment qu'un effet pervers pourrait alors être une dérive vers la marchandisation du vivant et des services écosystémiques (sur le modèle du marché du carbone) ainsi qu'à la financiarisation de la nature.

Toute mesure précise du prix de la nature ou de la valeur de la totalité des services rendus par le Vivant et sa diversité au niveau global est impossible. 

Et dans le domaine de la biodiversité, souligne le "rapport Chevassus" , ce qui rend la monétarisation d'un service écosystémique particulièrement délicate. Un des objectifs du rapport Chevassus-au-Louis était d'ailleurs "" 

En théorie, à une échelle beaucoup plus réduite, sur une zone humide ou un bassin versant par exemple, on peut arriver à quantifier certains services rendus par ces écosystèmes, à condition de les avoir préalablement bien définis. Il faut dans ce cas aussi tenir compte du contexte, car un même service quantifié (par exemple épuration d'un litre d'eau par une zone humide) aura une valeur économique très différente selon le lieu où il est produit (par exemple selon qu'il est rendu au cœur de l'Amazonie ou dans une oasis saharienne pour reprendre l'exemple du litre d'eau).

L'évaluation peut porter :

Ces évaluations partent de l'hypothèse que donner un prix à la biodiversité, sur la base de méthodes partagées, devrait permettre d'encourager sa meilleure prise en compte comme « "capital naturel" », et d'aussi donner un coût négatif aux phénomènes de destruction ou de surexploitation de milieux, de ressources et d'espèces vivantes. Du point de vue de l'économiste, ces évaluations pourraient aussi faciliter une meilleure hiérarchisation des enjeux et certains choix stratégiques. 

La biodiversité et ses produits (par exemple notre nourriture, tout l'oxygène que nous respirons, sa contribution au cycle de l'eau et à l'entretien du climat…) sont "a priori" et objectivement "inestimables". Et, comme elle n'a pas de "prix" (au sens économique du terme), certains économiste ont fait remarquer que des individus et groupes importants sont amenés à agir comme si elle n'avait pas de valeur. Les agents économiques tendent alors à ne pas prendre en compte la biodiversité dans leurs calculs, ou à la prendre en compte de manière biaisée ou incomplète ; 

Certaines décisions peuvent alors entrainer une mauvaise allocations des ressources (Exemple : destruction ou conservation peu justifiée), avec un impact négatif sur le bien-être collectif ou le bien commun, à court, moyen ou très long terme.

Les évaluations géographique mettent aussi en exergue certaines responsabilités particulières pour certaines régions et pays.

À titre d'exemple et de ce point de vue, la France apparait comme ayant une responsabilité de tout premier plan en raison du patrimoine naturel exceptionnel qu'abritent ses territoires d'outre-mer (Forêt tropicale de Guyane, écosystèmes et espèces endémiques de Nouvelle-Calédonie, Biodiversité marine de son espace maritime…), ce qui l'a fait classer parmi les 18 "« pays de mégadiversité biologique »"(pays abritant au moins 1 % ()
des quelque espèces de plantes vasculaires endémiques du monde .

Les évaluations économiques de ce type visent "in fine" à apporter des éléments d'information, d'aide et conseil, les plus objectifs possibles, sur lesquels pourront s'appuyer les décisions et comportements publics et privés, collectives et individuelles :

Évaluer économiquement la biodiversité nécessite que plusieurs conditions soient réunies :

Actuellement, les études sectorielles concernent ;

De façon générale et pour des raisons pratiques, ce qui est évalué dans ces études sont plutôt les ressources et non les fonctions écologiques plus ou moins vitales et auto-adaptatives (via les processus de l'évolution et de la sélection naturelle) que la biodiversité assure et entretient.

La dégradation de la biodiversité et donc des écosystèmes induit des pertes de services écosystémiques (qui étaient gratuitement rendus par les écosystèmes), ce qui se traduit par des coûts économiques largement ignorés jusque dans les années 1990. 

Pour la décennie 2000-2010, La perte directe induite par la perte de services écosystémiques était estimée à environ 50 milliards d’euros par an. 
Mais des estimations portent à 7 % du PIB mondial les pertes cumulées en termes de bien-être d’ici à 2050. 

Un sondage de 2002, effectué auprès de a conclu que les Français étaient alors, selon eux, prêts à payer par foyer et par an pour maintenir la biodiversité en forêt (soit, par extrapolation ou .

En 2008-2009, les premiers résultats d'une étude internationale sur "l'économie des écosystèmes et de la biodiversité" laissent penser que l'humanité est entrée dans un processus de perte de capital environnemental qu'elle n'a jamais connu.



</doc>
<doc id="6661" url="https://fr.wikipedia.org/wiki?curid=6661" title="Cataphilie">
Cataphilie

La cataphilie est l'activité qui consiste en la visite clandestine des anciennes carrières souterraines de Paris. Il s'agit d'un abus de langage relatif aux catacombes de Paris qui se situent dans les carrières souterraines, parfois abrégées en « catas ». Le pratiquant est un cataphile.

Un livre, "La cité des cataphiles — Mission anthropologique dans les souterrains de Paris", popularisa le terme de « cataphile » à partir de 1983, mais l'expression existait sans doute avant parmi les initiés. Il s'agit d'un néologisme, étymologiquement issu du grec κατκιροσ, « le royaume des morts », qui donna le mot catacombes en latin ecclésiastique "catacumbæ", par métathèse et par attraction de "–cumbere", « reposer », de l’expression "cata tumbas", « parmi les tombes », du grec κατα, « en bas », et τυμβος, « tombe » ; auquel est adjoint φἱλειν, « aimer » (suffixé en « –phile », « celui qui aime »). Le cataphile est également souvent un amateur de toutes les carrières et espaces souterrains, naturels ou non. Le terme de cataphilie désigne quant à lui, parfois la communauté des cataphiles, plus souvent l'ensemble des activités et légendes urbaines associées à l'univers cataphile. Il convient de distinguer le cataphile, qui aime et donc respecte ces lieux, du cataclaste, qui les dégrade.

Il n'existe pas en réalité de mouvement unitaire qui rassemblerait l'ensemble des cataphiles. Toutefois, il a existé plusieurs groupes et associations de cataphiles. Depuis les années 1980, de nombreuses incursions et fêtes clandestines ont lieu dans les carrières souterraines de Paris, sous l'égide de quelques habitués détenant souvent un plan des galeries souterraines et connaissant des points d'entrée et de sortie. L'essentiel de la fréquentation a néanmoins pour objet la promenade, par des groupes indépendants d'explorateurs qui se retrouvent le plus souvent au hasard dans les galeries.

Depuis les années 1960, il existe un engouement pour la visite des catacombes non officielles. Cette activité consiste à emprunter d'autres chemins que ceux balisés de l'ossuaire municipal, en exploitant les puits d'accès de la voie publique, voire les accès souterrains à partir de caves, galeries techniques, réseaux de transports en sous-sol, etc. Ces accès et déambulations sont illégaux.

Une culture propre au milieu cataphile s'est développée, avec sa communauté plurielle, ses règles et ses conflits. Quelques exemples :
Il existe aussi des conflits concernant le fait de parler de ces lieux ou non, certains prônent un grand secret de ces lieux, alors que d'autres prônent le partage de la connaissance et des documents iconographiques ou topographiques (sous différentes formes : livres, sites Internet, expositions) afin de sensibiliser la population à ce riche patrimoine. 

Cependant, la brigade sportive de la police nationale, mise en place en 1980, patrouille dans les carrières pour faire respecter l'arrêté du , qui interdit à toute personne étrangère au service officiel ou en dehors des circuits balisés de circuler dans les carrières souterraines de la ville de Paris. Des amendes de ou plus peuvent être délivrées. Depuis 2007, une comparution devant le tribunal de police peut être engagée.
Accéder aux catacombes via une voie ferrée appartenant à la SNCF est punissable d'une peine maximum étant de d'amende et d'emprisonnement.

Est qualifié de cataphile tout individu qui pénètre dans les anciennes carrières souterraines de Paris et en parcourt les galeries. Les personnes qui descendent pour la première fois dans ces carrières sont qualifiés avec humour de « touristes » par les cataphiles. Souvent accompagnés de guides et peu équipés, les touristes sont sujets à railleries. Les motivations cataphiles sont très diverses : sont souvent avancés l'intérêt historique, le besoin de solitude, le goût de l'exploration, l'attrait du monde minéral, etc. Des profils manifestement variés partagent cependant un goût pour une activité très commune consistant à parcourir un réseau sans objectif précis, en creusant des chatières, des connexions entre les lieux et en s'arrêtant régulièrement dans les nombreuses « salles » aménagées dans les vides des réseaux. 

D'autres activités courantes sont l'aménagement du réseau, la réalisation de fresques, de sculptures, de bancs, ou même une restauration du patrimoine : restauration des hagues (murs), des piliers, noircissage et rempaillage des plaques de rues et des plaques commémoratives.

Néanmoins le graffiti, activité polémique et persistante, s'est développé de plus en plus dans ce lieu, aboutissant parfois à des actes de vandalisme sur des éléments du patrimoine. Certains cataphiles opposés à cette pratique nettoient les parois peintes.

De nombreux chantiers d'anonymes se sont ouverts afin de les effacer des sites les plus sensibles (galeries anciennes voire classées) comportant de nombreuses plaques et inscriptions historiques. Toutefois celles effectuées a la mine de crayon parfois vielles de plusieurs siècles appelées « épures » (schéma techniques effectués par les carriers ou agents de l'Inspection générale des carrières) étant définitivement perdues sous la peinture des tags, en dépit de leur dimension historique et patrimoniale. Les cataphiles se distinguent essentiellement des spéléologues en ce que la passion des cataphiles vise plutôt à l'exploration des lieux souterrains construits par la main de l'homme et présentant donc un attrait historique.
Certaines techniques de la spéléologie peuvent parfois être mises à profit pour l'exploration de parties difficiles d'accès (puits sans échelons, effondrements, galeries ennoyées…)

La cataphilie se diffère aussi de la subterranologie, ("stricto sensu" : la science des souterrains) qui est l'étude des cavités artificielles et des infrastructures souterraines ; la première est une démarche ludique, la seconde est scientifique.

Les carrières souterraines de Paris (à savoir les trois grands réseaux de carrières souterraines parisiennes dits réseaux « 13 » , « GRS » et « 16 ») constituent une dimension parallèle à la ville et au monde réel par extension : l'absence de lumière du jour permet d'oublier la notion de temps, et là où seuls quelques endroits permettent du fait du bruit du métro de savoir si on est le jour ou en pleine nuit, le cataphile peut faire l’expérience d'un ermitage total, le rendant seul maître de ce qu'il voit, et de ce qu'il entend.

De nombreux cataphiles, habitués de cette expérience, la rééditent souvent, chaque semaine pour certains, et passent parfois plusieurs jours sous terre avec nourriture, hamac et duvets. La plupart des aménagements de consolidation, de creusages et de confort effectués illégalement dans les galeries sont le fait de ces semi-habitants du sous-sol.





</doc>
<doc id="6666" url="https://fr.wikipedia.org/wiki?curid=6666" title="Pompier (homonymie)">
Pompier (homonymie)

Le terme pompier peut désigner :

</doc>
<doc id="6667" url="https://fr.wikipedia.org/wiki?curid=6667" title="Vatican">
Vatican

Le Vatican, en forme longue lÉtat de la Cité du Vatican (en italien , ; en latin ) est un pays d'Europe. Il s'agit du support territorial du Saint-Siège enclavé dans la ville italienne de Rome. En 2014, il compte sur une superficie totale de , ce qui en fait le plus petit État au monde ainsi que le moins peuplé. Le Vatican se compose de deux entités juridiques distinctes, le Saint-Siège, entité spirituelle et l'État de la Cité du Vatican, entité temporelle. Le lien entre ces deux entités est le pape, chef du spirituel et du temporel, disposant du pouvoir absolu (exécutif, législatif et judiciaire).

La colline du Vatican est déjà mentionnée sous la République romaine. De nos jours, le Vatican est la représentation temporelle du Saint-Siège et de l'ensemble des institutions de l'Église catholique romaine : l'État de la Cité du Vatican est, lui, créé le aux termes des accords du Latran, signés par l'Italie représentée par Mussolini et par le Saint-Siège représenté par le cardinal Gasparri.

Le Vatican, important site archéologique du monde romain, situé sur la colline du même nom, est le siège de la papauté et du monde catholique. Selon la tradition catholique, il remonte à saint Pierre lui-même, comme premier évêque de Rome et est le centre officiel de tout le christianisme depuis l'empereur Constantin (), mais ce point de vue n'est pas forcément partagé par tous les historiens ni par toutes les confessions chrétiennes.

L'État du Vatican est une monarchie absolue, de droit divin et élective dirigée par l'évêque de Rome, c'est-à-dire actuellement le pape François, élu le , à la suite de la renonciation de Benoît XVI, le de la même année. Le pape y exerce souverainement le triple pouvoir exécutif, législatif et judiciaire.

Selon les étymologistes anciens comme Festus Grammaticus (cité par Paul Diacre), ce nom de "Vaticanus" tirerait son origine du mot "Vaticinium", ou plus exactement "Vātēs" ou "Vātis" signifiant « devin » ou « voyant », parce que beaucoup de devins auraient résidé de ce côté du Tibre, car on sait notamment que sous Tibère, l’art de la divination était interdit à Rome même (c’était un délit passible de la confiscation des biens et de la relégation).

Cette étymologie étant incertaine, d'autres parlent d'une ville étrusque nommée "Vaticum", qui aurait jadis existé à cet endroit ou du dieu Vaticanus qui présidait aux premières paroles des enfants et dont le temple était construit sur l'ancien site de "Vaticanum", la colline du Vatican. En effet, cette colline était "la maison des Vates" longtemps avant l'époque préchrétienne de Rome.

La Cité du Vatican actuelle peut être considérée comme le reliquat des anciens États pontificaux. L'origine ancienne de ce territoire des États pontificaux est une accumulation de donations foncières reçues par les papes successifs, depuis l'époque constantinienne jusqu'à celle du Royaume lombard (avec par exemple la donation de Sutri). Le pape s'est ainsi trouvé placé à la tête d'un important domaine foncier connu sous le nom de patrimoine de Saint-Pierre, sous suzeraineté byzantine.

Une justification longtemps avancée pour le pouvoir temporel du pape réside dans la donation de Constantin, un faux par lequel l'empereur Constantin aurait donné au pape Sylvestre la primauté sur les Églises d'Orient et l"'imperium" (pouvoir impérial) sur l'Occident (le caractère apocryphe de ce document a été établi en 1442 par l'humaniste Lorenzo Valla). La justification réelle réside essentiellement dans la donation de Pépin de 754 confirmée par Charlemagne en 774, donation cette fois bien réelle.

La cité se situe sur ce que l'on appelait dans l'Antiquité l"'ager Vaticanus" qui se compose d'une petite plaine (la plaine vaticane) aux bords du Tibre, se relevant à quelque distance en une colline d'une faible élévation, les "Montes Vaticani" (colline Vaticane).

Quelques villas, bâties autour de « jardins impériaux » y furent propriété d'Agrippine. Le fils de cette dernière, l’empereur Caligula (37-41 ap. J.-C.), y fit réaliser un cirque privé, le "Circus Vaticanus", dont l'actuel obélisque du Vatican constitue un des seuls vestiges. C’est là, ainsi que dans les jardins adjacents, qu’eut lieu le martyre de nombreux chrétiens de Rome à l’époque de Néron (54-68). On dit que saint Pierre fut enterré au nord de ce cirque, dans une nécropole qui longeait une route secondaire, la "via Cornelia". Sur le lieu de sa sépulture, l’empereur Constantin fit édifier entre 326 et 333 une basilique grandiose à l'emplacement du site de l'ancien cirque romain qui fut alors démoli. L'édifice a été remplacé par la basilique actuelle au cours des .

Au , le pape Symmaque y fit construire une résidence dans laquelle certains personnages illustres vinrent séjourner, tel Charlemagne lors de son couronnement (800). Au , Célestin II, puis Innocent III la firent rénover. La construction du palais du Vatican débuta sous le pontificat de Nicolas V durant la première moitié du .

Le 20 septembre 1870, après l'évacuation des troupes françaises, Rome est conquise par les troupes piémontaises et rattachée au royaume d'Italie. Le pape Pie IX qui résidait au palais du Quirinal (devenu depuis, la résidence officielle des rois d'Italie, puis du président de la République italienne), se réfugie alors au palais du Vatican. Son refus de l'annexion entraîne une dimension politique et diplomatique au conflit causé par l'État italien (c'est le début de la « question romaine »). Cette controverse dure jusqu'aux accords du Latran en 1929, qui assurent que le gouvernement italien respecte les frontières de l’État qu'il reconnaît alors "de facto".

Le pape dispose du pouvoir absolu (exécutif, législatif et judiciaire). Le pouvoir exécutif est délégué à un gouverneur nommé qui est également chargé de la représentation diplomatique. Une commission composée de cinq à sept cardinaux exerce par délégation le pouvoir législatif. Les institutions du Vatican sont réglées par une constitution, dont la première mouture a été rédigée par Pie XI au moment des accords du Latran. Actuellement, le Vatican est régi par la loi fondamentale du (entrée en vigueur le ). Ses lois sont consignées dans les "Acta Apostolicæ Sedis".

Le Vatican est une monarchie absolue et élective : le pape est élu à la majorité qualifiée (2/3 des voix) lors du conclave, et règne à vie en principe, mais il peut aussi renoncer, cette possibilité a été exploitée par Benoît XVI en 2013. Il peut également se définir comme une théocratie dans la mesure où son existence, son fonctionnement et son action sont dominés par un impératif religieux.

La citoyenneté vaticane n'est pas l'expression d'une appartenance nationale. Elle est liée à l'exercice de fonctions au sein du Vatican ou du Saint-Siège. Par conséquent, cette citoyenneté vient toujours s'ajouter à une nationalité d'origine. Dès que ces fonctions cessent, la citoyenneté cesse. Ainsi, un prélat de la Curie prenant des fonctions pastorales perd sa citoyenneté. Celle-ci est attribuée également au conjoint et à la famille (ascendants, descendants et collatéraux directs) des fonctionnaires du Vatican, à l'âge de 25 ans pour les garçons et au moment de leur mariage pour les filles. 

C'est le Saint-Siège, organe de gouvernement de l'Église catholique romaine, et non l'État de la Cité du Vatican, qui fait l'objet d'une représentation internationale. Il dispose d'un siège d'État non membre observateur à l'ONU.

Le Vatican a exprimé le désir de rejoindre l'espace Schengen.

La plus vieille armée encore en exercice, si l'on peut dire, est celle du Vatican. Elle comptait encore en 1977, quatre-vingt-neuf officiers et hommes de troupes, recrutés depuis 1506, exclusivement dans les cantons suisses. Les troupes pontificales ne sont plus montées au feu des combats depuis leur défaite par les troupes italiennes, survenue en 1870.

La diplomatie du Saint-Siège est l'activité de négociation internationale de l'Église catholique. Avant la Réforme et le siècle des Lumières, la papauté a exercé à plusieurs reprises des fonctions d’arbitre entre les souverains chrétiens européens. La diplomatie du Saint-Siège trouva sa première expression formelle véritable vers la fin du quand le pape commença à envoyer des légats vers les différents royaumes de la chrétienté. Il s’agissait de permettre au clergé résident d’avoir une plus grande marge de manœuvre à l’égard des autorités civiles locales.

À partir du , les premières nonciatures apparaissent, avec à leur tête un archevêque venant de Rome. Fragilisée par la Réforme et le développement de la philosophie des Lumières, l’autorité du Saint-Siège est contestée, mais celui-ci reste toujours présent sur la scène internationale. La légitimité de la diplomatie pontificale dans la sphère internationale est ensuite entérinée à plusieurs reprises par des traités de référence (le congrès de Vienne en 1815 et la conférence de Vienne de 1961 codifiant le droit diplomatique).

Du fait de sa très faible superficie, le Vatican est le plus petit pays du monde. Toutefois « l’État du Vatican » n’est pas un État souverain au sens strict, et se fait représenter par le Saint-Siège, dont les compétences s’étendent au-delà du seul État du Vatican aux ambassades, sous l’autorité du pape qui est à la fois le souverain du Saint-Siège et le dirigeant du Vatican. De plus, il n'a pas de nationaux en propre et sa puissance souveraine sur son territoire est, dans certaines circonstances et sur certaines parcelles définies par l'accord du Latran, partagée avec l’État italien (notamment la place Saint-Pierre). De ce fait, selon la convention de Montevideo, le statut juridique international du Vatican n'est, d'après certains juristes, pas celui d'un État, mais plutôt celui d'un sujet international analogue à une organisation internationale telle que l'ONU.

À ce titre, les ambassades (nonciatures) et propriétés du Saint-Siège hors-les-murs ne relèvent pas de l’État du Vatican, mais de la seule autorité du Saint-Siège, manifestée à travers ses institutions (regroupées dans la Curie romaine siégeant au Vatican) et son souverain.

La superficie du Vatican représente un cinquième de celle de la principauté de Monaco : le Vatican peut être qualifié de micro-État. Il est enserré dans des murailles imposées par l'article 5 des accords du Latran, entièrement enclavé dans la ville de Rome, dans le territoire italien. Cette enclave comprend notamment la place Saint-Pierre, la basilique Saint-Pierre, le Palais apostolique, les musées du Vatican et des jardins.

Le Saint-Siège a également la pleine propriété sur plusieurs bâtiments situés hors de la Cité vaticane, qui bénéficient d'un statut d'immunité diplomatique, à l'instar d'une ambassade. Il s'agit notamment de :

En outre, l'Université grégorienne, la station d'émission de Radio Vatican située dans la banlieue de Rome et divers autres bâtiments sont exempts d'impôts et préservés de toute expropriation. Ces bâtiments et propriétés ne font pas partie "stricto sensu" de l'État de la Cité du Vatican mais leur superficie cumulée représente environ le double de celle du Vatican (voir Propriétés du Saint-Siège en Italie).

En 2002, le déficit consolidé du Vatican s'élevait à 13,5 millions d'euros pour millions d'euros de recettes. Les dépenses sont principalement les salaires des (dont environ ). En 2010, l'économie vaticane a réalisé un excédent budgétaire de 10 millions d'euros, malgré la baisse des dons des fidèles.

Outre les revenus touristiques tels les revenus des musées du Vatican (91,3 millions d'euros de recettes en 2011), l'organisation de voyages et pèlerinages, l'émission de timbres postaux et de monnaies recherchés par les collectionneurs et la vente de publications, les revenus viennent de placements mobiliers (32 millions d'euros de plus-value en 2002) et immobiliers (12,9 millions d'euros).

Un autre poste financier non négligeable est le denier de Saint-Pierre qui a avoisiné les 50 millions d'euros en 2002, même si une partie de cette somme seulement est affectée au budget du Vatican . Son origine remonte au , quand les Anglo-Saxons commencèrent à envoyer une contribution annuelle au pape. Cet usage s'étendit ensuite aux autres pays d'Europe et a été reconnu officiellement par le pape Pie IX le dans l'encyclique "Sæpe venerabilis".

Depuis le janvier 2013, la "Deutsche Bank", qui gère les paiements monétiques au sein de la Cité vaticane, s'est vue dans l'obligation de désactiver l'utilisation de tous ses terminaux électroniques sur ordre de la Banque d'Italie, car le Saint-Siège n'a pas encore atteint les standards requis au niveau international contre le blanchiment d'argent. Les membres du comité Moneyval (un comité d'experts dépendant du Conseil de l'Europe qui repère notamment les blanchiments des capitaux et les sources occultes de financement du terrorisme) estiment en effet que le Vatican remplit à peine 9 des 16 recommandations clés et lui attribuent 7 mentions négatives. Le Vatican a lancé depuis 2010 une série de réformes à la suite d'importants scandales financiers ayant impliqué sa banque, l'Institut pour les œuvres de religion (IOR) et qui gère en 2011 plus de 6,3 milliards d'euros répartis en , dont de la famille du pape, , et , couvents ou abbayes. L’IOR s’est trouvé au cours des années au cœur de nombreux scandales notamment sous le mandat de Paul Casimir Marcinkus, ex-directeur de la banque du Vatican. L’établissement était le principal actionnaire du "Banco Ambrosiano", banque accusée dans les années 1980 de blanchiment d’argent de la drogue pour la mafia. En mai 2012, l’IOR refait parler d’elle avec le limogeage de son président Ettore Gotti Tedeschi. Les États-Unis ont ajouté en 2012 le Vatican à une liste de 68 États dont la situation est jugée préoccupante, selon le rapport annuel du Département d'État américain sur la lutte contre le trafic de drogue dans le monde.

Le pape François tend à sortir l'économie du Vatican des réseaux mafieux, et a d’ailleurs fait plusieurs déclarations à ce sujet.

La quasi-totalité des habitants vivent à l'intérieur des murs de la cité. Ce sont principalement des membres du clergé, incluant les hauts dignitaires, les prêtres, les religieuses. La fameuse garde suisse pontificale, chargée de la protection du pape, réside également au Vatican. Près de étrangers composent la majorité de la main-d'œuvre du pays, tout en résidant en dehors du Vatican. Sauf exception, les personnes possédant un passeport de la cité du Vatican conservent leur nationalité d'origine. Faute de maternité, il n'y a aucune naissance au Vatican.

Le Vatican comptait 921 habitants en 2014, ce qui en fait le pays le moins peuplé du monde. En revanche, il en est l'un des plus densément peuplés avec plus de 2 000 habitants par kilomètre carré (le troisième derrière Monaco et Singapour). En effet, cette population est concentrée sur une superficie de seulement.

Les langues officielles de la Cité du Vatican sont :

Sont également utilisés :

En tant que siège du catholicisme, le Vatican a une influence culturelle très importante. Il a aussi une activité culturelle propre, comme sa radio, Radio Vatican, qui émet en plusieurs langues.

Les onze musées du Vatican possèdent de riches collections d'art sacré et profane ainsi que des antiquités étrusques et égyptiennes et des œuvres de peintres, dont Michel-Ange.
Ils ont été fondés par Clément XIV au .

Football : voir Équipe du Vatican de football




Le Vatican a pour codes :






</doc>
<doc id="6669" url="https://fr.wikipedia.org/wiki?curid=6669" title="ATP">
ATP

ATP peut faire référence à :


</doc>
<doc id="6672" url="https://fr.wikipedia.org/wiki?curid=6672" title="Commerce extérieur du Mexique">
Commerce extérieur du Mexique

Commerce extérieur du Mexique

Le Mexique est un grand exportateur de pétrole et de minerai. 

Le principal partenaire commercial du Mexique sont les États-Unis tant pour les exportations que pour les importations.

Il ne faut pas oublier la contribution significative des Mexicains vivant aux États-Unis et qui envoient de l'argent à leur famille.

La balance commerciale a été négative depuis 1993, sauf en 1997 où elle était légèrement positive. Le déficit s'élevait à 8 milliards de dollars en 2000 et 9,954 milliards en 2001. 

La balance commerciale est positive avec les États-Unis (26,529 milliards de dollars en 2001), mais négative avec d'autres pays, surtout avec l'Union européenne (10,832 milliards de dollars en 2001), le Japon (7,5 milliards de dollars en 2001), la Corée (3,3 milliards) et Taïwan (2,9 milliards)

Ces statistiques sont issues du site du ministère de l'économie.

Exportations en millions de dollars


</doc>
<doc id="6676" url="https://fr.wikipedia.org/wiki?curid=6676" title="Ouadji">
Ouadji

Ouadji ("le serpent") est le quatrième souverain connu de la (Période thinite).

Il est le fils de Djer. Il épouse sa demi-sœur "Merneith" (ou "Meret-Neith") dont il a un enfant, Den qui lui succède.

Son nom d'Horus, « Ouadji » ou « Djet » signifie « le serpent ». La table d'Abydos l'appelle « "ita" » et le canon royal de Turin « "Itioui" ». Manéthon le nomme Ouénèphès et lui compte quarante-deux ans de règne, mais il aurait plutôt régné autour de vingt ans.

On situe son règne vers -3040 à -3020.

On lui attribue une expédition dans la mer Rouge. Sa stèle provient d’Abydos (Haute-Égypte).

Un mastaba dans lequel plusieurs objets portent son nom a été fouillé sur le site de Saqqarah. Cependant, il est enterré dans le cimetière d'Umm el-Qa'ab, la nécropole royale d'Abydos.

Un sceau qui a été découvert dans la tombe de son fils Den confirme que "L’aimée de Neit" est l’épouse et la demi-sœur de Djet-Ouadji. De plus, la présence d’une tombe (Y 41) pour elle toute seule à Umm el-Qa'ab à Abydos, située non loin de celle de Djet et entourée par de nombreux tombeaux de ses domestiques, peut faire penser à un règne personnel de cette reine pendant la minorité de son fils Den. Son nom apparaît sur des objets façonnés sous la forme masculine mr-nit et féminine mrt-nit. Cependant, son nom, dans un serekh, n’est pas surmonté d’un faucon (Horus), symbole de la royauté.



</doc>
<doc id="6679" url="https://fr.wikipedia.org/wiki?curid=6679" title="Drapeau de l'Égypte">
Drapeau de l'Égypte

Le drapeau de l'Égypte est le pavillon civil et d'État ainsi que le pavillon marchand et d'État de l'Égypte. Il a été adopté sous sa forme actuelle le . Il se compose de trois bandes horizontales, rouge, blanche et noire, avec l'emblème national de l'Égypte, l'aigle de Saladin, au centre de la bande blanche.

La couleur rouge renvoie à l'époque précédant le coup d'État militaire qui renversa le roi Farouk, en 1952. Ce coup d'État, perpétré sans verser de sang, est symbolisé par la couleur blanche. Enfin, le noir représente la fin de l'oppression des colons britanniques sur le peuple égyptien.

Ces trois couleurs sont, avec le vert, les couleurs panarabes, qu'on retrouve sur les drapeaux du Yémen, de la Syrie et de l'Irak.

Pour signaler son autonomie vis-à-vis de la Sublime Porte, ainsi que son ambition de concurrencer le sultan pour la domination de la totalité de l'Empire ottoman, Méhémet Ali introduisit un drapeau rappelant fortement celui des Ottomans, avec trois croissants et étoiles blancs sur fond rouge, peut-être pour symboliser les victoires de ses armées sur trois continents (Afrique, Asie et Europe), ou ses possessions d'Égypte, de Nubie et du Soudan. 

Les forces britanniques occupèrent le pays en 1882, avivant la flamme du nationalisme égyptien. Celle-ci atteignit un sommet lors de la révolte de 1919, durant laquelle on put voir dans les rues le drapeau de Mehemet Ali ainsi qu'un drapeau vert portant un croissant et une croix (symbolisant l'union des musulmans et des coptes contre les Britanniques).

Le premier véritable drapeau national de l'Égypte moderne fut adopté par décret royal en 1923, les Britanniques ayant officiellement reconnu l'indépendance du royaume d'Égypte l'année précédente. Il comprenait un croissant blanc et trois étoiles de la même couleur, sur fond vert. Les étoiles représentaient soit les trois parties du royaume (Égypte, Nubie et Soudan), soit les trois communautés religieuses du pays (musulmans, chrétiens et juifs).

Après le coup d'État de 1952, la république d'Égypte conserva le drapeau du royaume d'Égypte jusqu'en 1958. Le drapeau tricolore rouge, blanc et noir avec l'aigle est le drapeau révolutionnaire, donc non officiel. En 1958, sur le drapeau de la République arabe unie, formée de l'Égypte et de la Syrie, il fut remplacé par décret présidentiel par deux étoiles vertes ; si la Syrie reprit son indépendance dès 1961, l'Égypte continua d'utiliser ce drapeau jusqu'en 1972. Entre 1972 et 1984, ce fut un faucon d'or de Quraych ; ce drapeau fut aussi celui de la Syrie (1971-1980) et de la Libye (1972-1977), ces trois pays formant l'Union des Républiques arabes. Enfin, depuis 1984, sur la bande centrale blanche du drapeau figure l'Aigle de Saladin, portant un écu et un bandeau avec le nom du pays en arabe.

En mai 2014, le président intérimaire Adli Mansour promulgue une loi pénalisant la profanation du drapeau national. Ceci est puni d'une peine d'emprisonnement d'un an et d'une amende de , soit d'environ .



</doc>
<doc id="6682" url="https://fr.wikipedia.org/wiki?curid=6682" title="Politique en Égypte">
Politique en Égypte

La République arabe d'Égypte est une république démocratique.

Le pouvoir exécutif est détenu par le président de la République. Entre 1981 et 2011, Hosni Moubarak occupe le poste de président de la République, réélu lors de référendums tous les six ans. En 2005, l'élection pour la présidence était pour la première fois ouverte à d’autres candidats. Le pouvoir législatif appartient à la Chambre des députés (membres élus pour une durée de cinq ans par suffrage universel). Enfin une assemblée consultative, appelée la Choura, est consultée par le président de la République et l'Assemblée du Peuple sur les décisions politiques. Cette assemblée est composée de 264 membres dont deux tiers sont élus et un tiers nommé par le président de la République.

Une vague de protestations populaires entraîne le départ du président Hosni Moubarak en février 2011.

En juin 2012, Mohamed Morsi remporte l'élection présidentielle et devient ainsi le premier président du pays élu au suffrage universel dans une élection libre.

Un an après son arrivée au pouvoir, le président Morsi est massivement contesté par l'opposition qui regroupe diverses factions entre laïcs de gauche, anciens partisans du régime de Moubarak et différents groupes révolutionnaires, dont (Rebellion). Une grande partie de la population reproche au nouveau président une dérive dictatoriale et une politique menée dans le seul intérêt de son organisation, les Frères musulmans. Après des rassemblements massifs dans tout le pays, l'armée, dirigée par le général Abdelfatah Khalil al-Sisi, lance un dernier ultimatum le juillet 2013. Celui-ci est rejeté le lendemain par Mohamed Morsi qui défend sa légitimité en soulignant qu'il a été élu démocratiquement, avec 52 % des voix. Cependant, selon des observateurs, l'ultimatum a été lancé dès le mois d'avril 2013, par la coalition des opposants, alors que la situation économique était au plus mal.

Il est remplacé par le président de la Haute Cour constitutionnelle, Adli Mansour, qui prête serment comme président par intérim.
Le 4 juillet 2013, on apprend que Mohamed Morsi est détenu par l'armée et que des mandats d'arrêt sont émis à l'encontre des dirigeants des Frères musulmans. Le 5 juillet 2013, le Parlement est dissous. Le 26 juillet 2013, l'armée déclare que Mohamed Morsi est en prison dans l'attente de son procès pour collusion avec le mouvement palestinien du Hamas.

Un référendum constitutionnel a lieu en janvier 2014.

L'organisation d'élections législatives se déroule entre février et mars 2014, de même que l'élection présidentielle au début de l'été de la même année, le pouvoir intérimaire exerçant ses prérogatives jusque-là. À la fin de 2013, le nouveau pouvoir militaire est à son tour la cible de contestations, notamment à cause de la répression de manifestations et de l'arrestation d'activistes démocrates.


Les principaux partis politiques en 2013 sont :





</doc>
<doc id="6700" url="https://fr.wikipedia.org/wiki?curid=6700" title="Dioxyde de carbone">
Dioxyde de carbone

Le dioxyde de carbone, aussi appelé gaz carbonique ou anhydride carbonique, est un composé inorganique dont la formule chimique est , la molécule ayant une structure linéaire de la forme O=C=O. Il se présente, sous les conditions normales de température et de pression, comme un gaz incolore, inodore, à la saveur piquante.

Le est utilisé par l'anabolisme des végétaux pour produire de la biomasse à travers la photosynthèse, processus qui consiste à réduire le dioxyde de carbone par l'eau, grâce à l'énergie lumineuse reçue du soleil et captée par la chlorophylle, en libérant de l'oxygène pour produire des oses, et en premier lieu du glucose par le cycle de Calvin. Le est libéré, à travers le cycle de Krebs, par le catabolisme des plantes, des animaux, des "fungi" (mycètes, ou champignons) et des micro-organismes. Ce catabolisme consiste notamment à oxyder les lipides et les glucides en eau et en dioxyde de carbone grâce à l'oxygène de l'air pour produire de l'énergie et du pouvoir réducteur, sous forme respectivement d'ATP et de . Le est par conséquent un élément fondamental du cycle du carbone sur notre planète. Il est également produit par la combustion des énergies fossiles telles que le charbon, le gaz naturel et le pétrole, ainsi que par celle de toutes les matières organiques en général. C'est un sous-produit indésirable dans les processus industriels à grande échelle. Un exemple est la production d'acide acrylique qui est produite dans une quantité de plus de 5 millions de tonnes/an. Le défi dans le développement de ces procédés est de trouver un catalyseur et des conditions de procédé appropriés qui maximisent la formation du produit et minimisent la production de CO2. Des quantités significatives de sont par ailleurs rejetées par les volcans et autres phénomènes géothermiques tels que les geysers.

En septembre 2016 le dioxyde de carbone était présent dans l'atmosphère terrestre à une concentration de (parties par million en volume), soit 0,0401 %. En 2009, cette concentration atteignait précisément , contre seulement en 1839 d'après les carottes de glace prélevées dans la région du cap Poinsett dans l'Antarctique, soit une augmentation globale d'environ 42 % en .

Le est un gaz à effet de serre bien connu, transparent en lumière visible mais absorbant dans le domaine infrarouge, de sorte qu'il tend à bloquer la réémission vers l'espace de l'énergie thermique reçue au sol sous l'effet du rayonnement solaire ; il serait responsable de 26 % de l'effet de serre à l'œuvre dans notre atmosphère (la vapeur d'eau en assurant 60 %)<ref name="10.1175/2008BAMS2634.1">

</ref>, où l'augmentation de sa concentration serait en partie responsable du réchauffement climatique constaté à l'échelle de notre planète depuis les dernières décennies du . Par ailleurs, l'acidification des océans résultant de la dissolution du dioxyde de carbone atmosphérique pourrait compromettre la survie de nombreux organismes marins<ref name="10.1126/science.1208277">
</ref> et les coquillages<ref name="10.1073/pnas.0913804107">
</ref>, mais aussi de certains poissons.

À pression atmosphérique, il se sublime à (passage de l'état solide à l'état gazeux), mais ne fond pas (passage de l'état solide à l'état liquide).

La phase liquide ne peut exister qu'à une pression minimale de (soit ), et dans un intervalle de température allant de (point triple) à au maximum à (soit ) (point critique).

Il existerait "au moins" cinq phases solides moléculaires (existant à « basse » pression, moins de 30 à 60 GPa) et trois phases solides polymériques (aux pressions plus élevées) du :

Le se dissout dans l’eau et y forme de l’acide carbonique :
Il est également liposoluble (soluble dans les corps gras).

L’acide carbonique n’est que modérément stable et il se décompose facilement en et . En revanche, lorsque le dioxyde de carbone se dissout dans une solution aqueuse basique (soude, potasse…), la base déprotone l’acide carbonique pour former un ion hydrogénocarbonate HCO, aussi appelé ion bicarbonate, puis un ion carbonate CO. De cette façon, la solubilité du est considérablement augmentée. 
Par exemple, une solution aqueuse saturée de carbonate de potassium a une concentration de 112 % (en masse) en carbonate à .

C'est ainsi que le calcaire se dissout dans l'eau, dans la plage de pH dans laquelle l'hydrogénocarbonate acide est stable, en produisant une solution d'hydrogénocarbonate(s) (de calcium et de magnésium…). Il est donc susceptible de précipiter lorsque le dissous est dégazé, comme dans la formation des stalagmites et des stalactites. Le calcaire a ainsi, en présence de , une solubilité qui diminue quand la température augmente, à l'instar des gaz et au contraire de la plupart des solides (dont la solubilité augmente généralement avec la température).

Dans certaines conditions (haute pression + basse température) le peut être piégé dans des cages d'eau dites clathrates. C'est un des moyens possibles de séparation industrielle du contenu dans un gaz en pré- ou post-combustion. C'est aussi un des moyens envisagés de séquestration de industrielle ou de stockage géologique étudié, éventuellement corrélativement à la dessalinisation d'eau de mer (il peut théoriquement même être substitué au Méthane d'hydrate de méthane).

Le dioxyde de carbone est l'un des premiers gaz (avec la vapeur d'eau) à avoir été décrit comme étant une substance distincte de l'air. Au , le chimiste et médecin flamand Jean-Baptiste Van Helmont observa qu'en brûlant du charbon de bois en vase clos, la masse des cendres résultantes est inférieure à celle du charbon. Son interprétation était que la masse manquante s'était transmutée en une substance invisible qu'il nomme « "gas" » ou "spiritus sylvestre" (« esprit sauvage »).

Les propriétés du dioxyde de carbone furent étudiées plus en détail dans les années 1750 par le chimiste et physicien écossais Joseph Black. Il découvrit qu'en chauffant ou en versant un acide sur du calcaire (roche composée de carbonate de calcium), il en résultait l'émission d'un gaz, qu'il nomma « air fixe », mettant à mal la théorie du phlogiston encore enseignée à cette époque. Il observa que celui-ci est plus dense que l'air et qu'il ne peut ni entretenir une flamme, ni la vie d'un animal. Black découvrit également que lorsque le dioxyde de carbone est introduit dans une solution calcaire (hydroxyde de calcium), il en résulte un précipité de carbonate de calcium. Il utilisa ce phénomène pour illustrer le fait que le dioxyde de carbone est produit par la respiration animale et la fermentation microbienne.

En 1772, le chimiste anglais Joseph Priestley publia un ouvrage intitulé "Impregnating Water with Fixed Air" dans lequel il décrivit un processus consistant à verser de l'acide sulfurique (ou « huile de vitriol » comme on la nommait à cette époque) sur de la craie afin de produire du dioxyde de carbone, puis forçant le gaz à se dissoudre dans un bol d'eau. Il venait d'« inventer » l'eau gazeuse. Le procédé est ensuite repris par Johann Jacob Schweppe qui fonda, en 1790, à Londres une usine de production de soda connue sous le nom de Schweppes.

En 1781, le chimiste français Antoine Lavoisier mit en évidence le fait que ce gaz est le produit de la combustion du carbone avec le dioxygène.

Le dioxyde de carbone fut liquéfié pour la première fois en 1823 par Humphry Davy et Michael Faraday. La première description du dioxyde de carbone en phase solide fut écrite par , qui en 1834 ouvrit un container pressurisé de gaz carbonique liquéfié et découvrit que le refroidissement produit par la rapide évaporation du liquide générait de la « neige » de .

Le a de nombreuses utilisations, dont : 

Sous forme liquide, il est utilisé comme :

Quand il est utilisé comme fluide frigorigène, le porte la dénomination R744. Son utilisation comme fluide frigorigène tend à se démocratiser ces dernières années : il est considéré comme "frigorigène naturel", et son Potentiel de Réchauffement Global est très bas comparé aux fluides frigorigènes "traditionnels".

À pression atmosphérique, le dioxyde de carbone n’est jamais sous forme liquide. Il passe directement de la forme solide à la forme gazeuse (sublimation).

Le dioxyde de carbone sous forme solide a de nombreuses appellations : « glace carbonique », « neige carbonique », « Carboglace™ », « glace sèche ». Il est issu de la solidification du liquide. On obtient de la neige carbonique qui est ensuite comprimée pour obtenir de la glace carbonique.

Dans sa phase solide, cette glace carbonique se sublime en ne laissant aucun résidu, avec une enthalpie de sublimation de (soit ), à et à . On lui a donc rapidement trouvé de multiples utilisations en tant que réfrigérant.

Il est commercialisé sous différentes présentations selon son usage :

Le dioxyde de carbone solide est également présent sous forme de neige aux pôles de la planète Mars, où il couvre pendant l'hiver local les calottes glaciaires (composées d'eau très majoritairement) et leurs périphéries, ainsi que sous forme de givre carbonique à plus basse latitude, en fin de nuit au début des printemps locaux (photographies prises par les atterrisseurs "Viking", le rover "Sojourner", l’atterrisseur "Phoenix", et de nombreuses images HRSC).

Au-delà de son point critique, le dioxyde de carbone entre dans une phase appelée supercritique. La courbe d'équilibre liquide-gaz est interrompue au niveau du point critique, assurant à la phase supercritique un continuum des propriétés physico-chimiques sans changement de phase. C'est une phase aussi dense qu'un liquide mais assurant des propriétés de transport (viscosité, diffusion) proches de celles d'un gaz. Le dioxyde de carbone supercritique est utilisé comme solvant vert, les extraits étant exempts de trace de solvant. 
Sous cette forme, il sert comme : 

Le dioxyde de carbone est une molécule très stable, avec une enthalpie standard de formation de . Le carbone présente une charge partielle positive, ce qui rend la molécule faiblement électrophile. Par exemple, un carbanion va pouvoir réaliser une addition nucléophile sur le et former un acide carboxylique après hydrolyse. Par ailleurs, le peut être utilisé pour former des carbonates organiques, par addition sur des époxydes.

Enfin, le peut être réduit, par exemple en monoxyde de carbone par électrochimie avec un potentiel redox de par rapport à l'électrode standard à hydrogène ou par hydrogénation.

L'air extérieur contient aujourd'hui environ 0,04 % de . 

À partir d'une certaine concentration dans l'air, ce gaz s'avère dangereux voire mortel à cause du risque d'asphyxie ou d'acidose, bien que le ne soit pas chimiquement toxique. La valeur limite d'exposition est de 3 % sur une durée de . Cette valeur ne doit jamais être dépassée. Au-delà, les effets sur la santé sont d'autant plus graves que la teneur en augmente. Ainsi, à 2 % de dans l’air, l'amplitude respiratoire augmente. À 4 %, la fréquence respiratoire s'accélère. À 10 %, peuvent apparaître des troubles visuels, des tremblements et des sueurs. À 15 %, c'est la perte de connaissance brutale. À 25 %, un arrêt respiratoire entraîne le décès.

L'inhalation de dioxyde de carbone concentré entraîne un blocage de la ventilation, parfois décrit comme une violente sensation d'étranglement, un souffle coupé, une détresse respiratoire ou encore une oppression thoracique, pouvant rapidement mener au décès si l'exposition est prolongée.

Des études signalent selon l'ANSES . Une acidose respiratoire peut survenir dès 1 % () de dans l'air, s'il est respiré durant 30 minutes ou plus par un adulte en bonne santé avec une charge physique modérée, et probablement plus tôt chez des individus vulnérables ou sensibles. Ces taux "sont supérieures aux valeurs limites réglementaire et/ou normative de qualité du renouvellement d'air en France et au niveau international, qui varient usuellement entre de ". Une petite étude expérimentale (ayant concerné 22 adultes a conclu à un effet du sur la psychomotricité et la fonction intellectuelle (prise de décision, résolution de problèmes) dès de (étude Satish et al., 2012), mais cette étude doit être confirmé par des travaux ayant une puissance statistique plus élevée. L'Anses note qu'il y a finalement peu d'études épidémiologiques sur ce polluant commun, dont sur d'éventuels effets CMR (cancérigènes, mutagènes et reprotoxiques).

Le dioxyde de carbone étant un gaz incolore et lourd s'accumulant en nappes, il est difficilement détectable par une personne non expérimentée. 

Selon l'ANSES, en France, le taux de dans l'air intérieur des bâtiments (lié à l'occupation humaine ou animale et à la présence d'installations de combustion), pondéré par le renouvellement de l'air, est "". D'après les résultats de campagnes de mesures nationales réalisées dans les logements, les écoles, les crèches et les bureaux par l'Observatoire de la qualité de l'air intérieur (Oqai), le ministère chargé de l'Écologie et le CSTB, il n'y a pas de relations systématiques entre les taux de et d'autres polluants.
Il n'est pas réglementé dans l'air du domicile ; mais il doit être mesuré en tant qu"indicateur de confinement et de la qualité du renouvellement de l'air" dans certains lieux confinés, sur la base de normes dont l'ANSES estime qu'elles n'ont pas de bases sanitaires.

En France, le règlement sanitaire départemental (RSD) recommande de ne pas passer le seuil de (partie par million) "dans des conditions normales d'occupation", avec une tolérance à dans les lieux où il est interdit de fumer ( selon l'Anses. 
Un décret du 5 janvier 2012 impose une surveillance de la qualité de l'air intérieur à certains établissements recevant du public sensible tel que les enfants ; il propose le calcul d'un "indice de confinement" dit "indice Icone" (proposé par le Centre scientifique et technique du bâtiment (CSTB) sur la base de la fréquence de dépassement des niveaux de par rapport à deux seuils de dans les salles de classe.

En milieu professionnel, la question de la sécurité et de la prévention liée aux risques d'intoxication au dioxyde de carbone est une préoccupation majeure pour limiter les risques d'accident du travail. Faute de données épidémiologiques, il n'a cependant pas été considéré comme pertinent en France comme indicateur de la qualité sanitaire de l'air intérieur par l'Anses qui ne prévoit pas de valeur guide de qualité d'air intérieur (VGAI) pour ce polluant. Dans de fortes concentrations approchant les 50 à 100 %, telles que celles retrouvées dans les nappes de dioxyde de carbone d'origine artificielle en milieu professionnel, il peut se produire un effet de sidération nerveuse et une perte de conscience immédiate, suivie d'une mort rapide en l'absence d'aide extérieure. Ces accidents présentent un risque élevé de suraccident, des témoins pouvant se précipiter au secours de la victime sans penser à leur propre sécurité et devenir eux aussi victimes de l'intoxication.

Le dioxyde de carbone n'est normalement présent dans l'atmosphère terrestre qu'à l'état de traces. Il est mesuré via un indice, nommé "Annual Greenhouse Gas Index" (AGGI) depuis 1979 par un réseau d'une centaine de stations sur terre et en mer, disposées de l'Arctique au pôle Sud.

Depuis la révolution industrielle, en raison de la combustion constante de très grandes quantités de carbone fossile, alors que la régression des incendies, des forêts et des superficies végétalisées se poursuit, le taux de dans l'air augmente régulièrement (actuellement : 405 parties par million (ppm) en volume. Ceci correspond à une masse totale de atmosphérique d'environ 3,16×10 kg (environ gigatonnes). L'année 1990 (qui correspond à un surplus d'environ 2,1 watts/m² par rapport à 1980) est l'année de référence retenue pour le protocole de Kyoto (elle a donc un « indice AGGI » de 1). Un groupe de recherche spécifique sur le cycle du carbone et les gaz a effet de serre a été mis en place 

À un instant "« t »", la teneur en diffère dans chaque hémisphère, avec dans chaque hémisphère des variations saisonnière régulières (cf motif « en dents de scie » sur le graphique de droite, montrant une baisse du en saison de végétation et une augmentation en hiver). Il existe aussi des variations régionales en particulier au niveau de la couche limite atmosphérique, c'est-à-dire dans les couches proches du sol.

Les taux de sont généralement plus élevés dans les zones urbaines et dans les habitations (jusqu'à 10 fois le niveau de fond).

Peu après la formation de la terre (bien avant l'apparition de la vie), alors que le soleil était presque deux fois moins « chaud », la pression initiale de était environ plus élevée qu'aujourd’hui (30 à 60 atmosphères de (soit ), soit la quantité actuelle de il y a environ 4,5 milliards d'années).

Puis la vie et la photosynthèse sont apparues, prélevant le de l'atmosphère et de l'eau pour le transformer en roches carbonatées et en charbon, pétrole et gaz naturel, en grande partie enfouis dans les profondeurs de la terre. Le taux de a néanmoins encore connu quelques pics de bien moindre importance (20 fois plus élevée qu'aujourd'hui il y a environ un demi milliard d'années, mais le soleil était alors moins chaud qu'aujourd'hui (le rayonnement solaire croît avec le temps ; il a augmenté d'environ 40 % dans les 4 derniers milliards d'années). Le taux de a encore chuté de 4-5 fois durant le Jurassique, puis a diminué lentement, sauf, de manière accélérée durant un épisode géologiquement bref, dit « "évènement Azolla" » (il y a environ 49 millions d'années).

Le volcanisme émet aussi du (jusqu'à 40 % des gaz émis par certains volcans lors des éruptions subaériennes sont du dioxyde de carbone) et certaines sources chaudes en émettent aussi (par exemple sur le site italien de Bossoleto près de Rapolano Terme où dans une dépression en forme de cuvette d'environ de diamètre, par nuit calme, le taux de peut grimper de 75 % en quelques heures, assez pour tuer les insectes et petits animaux. Mais la masse de gaz se réchauffe rapidement quand le site est ensoleillé et est alors dispersée par les courants de convection de l'air durant la journée . Localement des concentrations élevées de , produites par la perturbation de l'eau d'un lac profond saturé en peuvent aussi tuer (exemple : 37 morts lors d'une éruption de à partir du lac Monoun au Cameroun en 1984 et 1700 victimes autour du lac Nyos (Cameroun également) en 1986 .
Les émissions de par les activités humaines sont actuellement plus de 130 fois supérieures à la quantité émise par les volcans, d'un montant de près de 27 milliards de tonnes par an en 2007. En 2012, la Chine est le premier émetteur mondial de dioxyde de carbone avec 27 % du total, et les États-Unis, en deuxième position, produit 14 % du total mondial. En 2016, l’agence météorologique de l’ONU indique que la concentration de dioxyde de carbone a atteint un nouveau record historique, soit .

Le principal élément nutritif intervenant dans la nutrition végétale est le carbone, tiré du dioxyde de carbone de l'air par les plantes autotrophes grâce au processus de la photosynthèse. Des apports de sont effectués dans les cultures sous serres afin d'accélérer la croissance des végétaux. On recommande en général un taux moyen de .

Le serait le deuxième gaz à effet de serre le plus important dans l'atmosphère après la vapeur d'eau, contribuant respectivement à hauteur de 26 % et 60 % à ce phénomène. La réalité du réchauffement climatique observé à l'échelle planétaire depuis le siècle dernier n'est aujourd'hui plus guère contestée d'un point de vue scientifique, mais la part exacte de responsabilité du dioxyde de carbone dans ce processus (par rapport au méthane notamment) doit encore être précisée, grâce aux enregistrements fossiles des paléoclimats notamment. 
Une réduction des émissions anthropiques est visée par le protocole de Kyōto ainsi que par la directive 2003/87/CE ; sa séquestration géologique à long terme fait l'objet de recherches mais est une solution controversée quand il s'agit simplement d'injecter du dans les couches géologiques.

Le a un certain effet eutrophisant (c'est un nutriment de base, essentiel pour les plantes), mais il est aussi un facteur d'acidification des océans et de certaines masses d'eau douce, qui peut négativement interférer avec de nombreuses espèces (dont certaines microalgues et autres microorganismes aquatiques protégées par des structures calcaires que l'acide carbonique peut dissoudre). L'acidification favorise aussi la libération et la circulation et donc la biodisponibiltié de la plupart des métaux lourds, métalloïdes ou radionucléides (naturellement présent dans les sédiments ou d'origine anthropique depuis la révolution industrielle surtout).

L'augmentation de la teneur de l'atmosphère en peut aussi avoir des effets différentiés voire antagonistes selon son taux, le contexte environnemental et biogéographique et selon des données plus récentes selon la saison et les variations saisonnières de la pluviométrie (au-dessus des forêts notamment) ;

Il existe chez les écologues associés à l'étude des effets du changement climatique un consensus sur le fait qu'au-delà d'une augmentation de en un siècle, les écosystèmes terrestres et marins seront sérieusement négativement affectés.

En 2013, la réponse réelle des écosystèmes au et ses modulations biogéographiques sont encore considérées comme complexes et à mieux comprendre, en raison de nombreux . Elle doit être néanmoins élucidée si l'on veut correctement évaluer voire prédire les capacités planétaires ou locales des écosystèmes en termes de stockage naturel du carbone et d'amortissement des effets du dérèglement climatique induit par l'Homme.

Les rétroactions médiés par le cycle hydrologique sont particulièrement importantes et la pluviométrie y joue un rôle majeur. La physiologie des plantes a au moins un rôle bien connu ; jusqu'à un certain stade (au-delà duquel la plante dépérit), l'augmentation du taux de de l'air réduit la conductance stomatique et augmente les besoins en eau de la plante (la quantité d'eau nécessaire pour produire une unité de matière sèche). Il a été estimé (en 2008) que les effets de l'augmentation du dans l'air sur l'écosystème devrait donc être exacerbés quand l'eau est un facteur limitant (mais les apports d'azote sont aussi à prendre en compte) ; ceci a été démontré par quelques expériences, mais est un facteur qui a été "oublié" par de nombreuses études.

Cette relation semble si forte qu'elle permet - en zone tempérée de prédire avec précision les variations annuelles de la stimulation de la biomasse aérienne à la suite de l'élévation du taux de dans une prairie mixte contenant des végétaux de type C3 et C4, sur la base du total des précipitations saisonnières ; les pluies d'été ayant un effet positif, alors que celles d'automne et du printemps ont des effets négatifs sur la réponse au . L'effet du taux croissant de dépendra donc principalement des nouveaux équilibres ou déséquilibres qui s'établiront entre les précipitations estivales et d'automne / printemps.

Le lien à l'azote (autre élément perturbé par les activités humaines dont par l'agriculture industrielle, l'industrie et les émissions de la circulation automobile) est ici retrouvé : c'est en partie car les fortes précipitations en saisons froides et humides conduisent à limiter l'accès des plantes terrestres à l'azote qu'elles réduisent ou interdisent la stimulation de la biomasse par un taux de élevé. Il a aussi été noté que cette prédiction valait aussi pour des parcelles « réchauffées » de ou non-réchauffées, et était similaire pour les plantes en C3 et de la biomasse totale, ce qui semble permettre aux prospectivistes de faire des prévisions robustes sur les réponses aux concentrations élevées de de l'écosystème. Ceci est un atout précieux car les projections climatiques des modèles à haute résolution confirment la très forte probabilité de changements importants dans la répartition annuelle des pluies, même là où la quantité annuelle totale de pluie tombée au sol ne changera pas. Ces données scientifiquement confirmées (en 2013) devraient aider à expliquer certaines différences apparues dans les résultats des expériences basées sur l'exposition de plantes à un taux accru de , et améliorer l'efficacité prospective des modèles qui ne tenaient pas assez compte des effets saisonniers des précipitations sur les réponses de la biodiversité au 14, dont en milieux forestiers.

Enfin, les effets de l'augmentation du sur les plantes se montrent plus préoccupants que ce qui était prédit par les premiers modèles des années 1990 et du début des années 2000. Morgan et al. sur la base d'expériences de laboratoire et "in situ", ont confirmé dès 2004 que dans les écosystèmes émergés, le , même quand il améliore la productivité en termes de biomasse, peut néanmoins avoir des effets négatifs en modifiant la composition des espèces et en réduisant la digestibilité des graminées courtes par exemple dans la végétation steppique)

Des études récentes laisseraient entendre que l'accroissement des taux de pourrait aussi avoir une incidence défavorable sur les plantes utilisées pour la consommation humaine en accroissant la quantité d'hydrates de carbone qu'elles contiennent au détriment de nutriments essentiels comme les protéines, le fer ou le zinc. Les recherches dans ce domaine n'en sont qu'à leurs débuts mais les chercheurs se posent des questions sur l'incidence que pourrait avoir cette évolution sur la santé humaine si elle se confirme.

Plusieurs voies sont explorées ou déjà mises en œuvre pour limiter l'accumulation du dans l'atmosphère. Elles peuvent faire appel à des processus naturels comme la photosynthèse ou à des procédés industriels. Il faut également distinguer la capture à la source de la capture dans l'atmosphère.

La start-up indienne "Carbon Clean Solutions" (CCSL) a lancé avec succès sa première installation, qui capte et réutilise à 100 % les émissions de ( par an) d'une petite centrale à charbon en Inde, à Chennai (Madras) ; ce est purifié, puis revendu à un industriel local, qui l'utilise pour fabriquer de la soude. La technologie de CCSL ramène le coût de revient du vendu à 30 dollars la tonne en Inde et à 40 dollars en Europe ou aux États-Unis, très en dessous du prix du marché : 70 à 150 dollars la tonne. Veolia a signé avec CCSL un contrat pour commercialiser ce procédé à l'international.



</doc>
<doc id="6705" url="https://fr.wikipedia.org/wiki?curid=6705" title="Voile (navire)">
Voile (navire)

Une voile est une pièce de tissu, dont la taille peut varier de quelques mètres carrés à plusieurs centaines de mètres carrés, qui, grâce à l'action du vent, sert à faire avancer un véhicule. Les voiles sont utilisées sur des voiliers, planches à voile, mais aussi sur des véhicules terrestres (chars à voiles, voile sur glace). 

Le type de voile utilisé et leur agencement sur les mâts, vergues et/ou étais s'appellent le gréement.

L'effet des courses au large a motivé la recherche dans ce domaine, tout comme dans la conception des coques par adjonction de foils, de ballasts.

Une voile est caractérisée principalement par sa forme, son grammage et le(s) matériau(x) dont elle est composée. 

Les trois angles de la voile (sur une voile à forme triangulaire) ont une appellation spécifique :


Chacune des extrémités de la voile reçoit un renfort () constitué de plusieurs épaisseurs de tissus cousues ensembles parfois renforcées par une structure rigide. La têtière () est la partie renforcée de l'extrémité supérieure de la voile. Un œillet situé à chacun des angles permet de fixer la voile au gréement. 

Les côtés d'une voile triangulaire sont :

Sur la grand-voile la tension de la bordure (passée dans la bôme) et du guindant est modulée selon la force du vent. Plus la voile est « étarquée », plus elle est plate et inversement. Ceci permet d'adapter le creux à la pression du vent qui s'exerce sur elle.

Sur la grand-voile (sur les voiles d'avant c'est beaucoup plus rare depuis l'avènement des focs et trinquettes à enrouleurs) on trouve également 2 à 3 bandes de ris () - zones horizontales en partie renforcées et comportant des œillets aux extrémités qui sont utilisées pour réduire la surface de la grand-voile lorsque le vent forcit (prise de ris)

Une voile est généralement composée de laizes () bandes de tissus cousues, découpées de manière à répartir l'effort en faisant éventuellement varier le grammage et positionner le creux de la voile (une voile n'est pas plate sauf s'il s'agit d'une voile de tempête comme le tourmentin).

Les voiles les plus performantes ne sont plus en textile tissés et cousues, mais elles sont en fibre de carbone ou d'aramide, qui est un composite de fibres polymérisées qui leur donne plus de résistance et de légèreté, la voile ne se déformant pas tout en étant plus légère. Il y a trois fabriquants de voiles en fibre de carbone dans le monde : l'américain North Sails, le Français Incidence et le Néo-Zélandais Doyle Stratis

La chute des grand-voiles modernes est arrondie : c'est le rond de chute qui est autorisé par 3 à 4 lattes (). Sur les voiliers très rapides, la voile peut aussi être complètement lattée, la tension des dites lattes permettant d'obtenir très exactement le profil souhaité. 

Le guindant de la grand-voile est rendu solidaire du mât soit grâce à des coulisseaux () fixés à la voile et passés dans la gorge du mât soit grâce à une ralingue (c’est-à-dire un cordage cousu le long de la voile). La bordure de la grand-voile est également tenue par une ralingue ou un/des coulisseau(x) passés dans la gorge de la bôme.

Sur un spinnaker (spi), symétrique par définition, le point de drisse est clair tandis que les deux autres points sont ceux d’amure et d'écoute. Ils peuvent être assujettis par un tangon. Quand le point est côté tangon il est appelé point d'amure, l'autre point est appelé point d'écoute. On les échange au moment de l'empannage.

Le principe de fonctionnement d'une voile dépend de l'allure du navire, c'est-à-dire de la direction du navire par rapport au vent. Une voile travaille de deux façons :

Lorsque le navire remonte par rapport au vent, l'écoulement du vent le long de la voile crée une différence de pression entre le côté au vent (intrados) et le côté sous le vent (extrados). 
Une dépression se forme sur l'extrados, ce qui « tire » le navire c'est la portance, et lui permet de remonter au vent. C'est ce même phénomène, appliqué à une aile d'avion, qui lui permet de voler.

On peut considérer une voile en écoulement attaché comme un système chargé de dévier une masse d'air. Tout comme un lanceur de poids reçoit une force centrifuge en faisant tourner son marteau, la voile reçoit une force proportionnelle à la masse de l'air dévié et à l'angle de déviation. 
La quantité d'air dévié est le produit de :
L'angle de déviation maximum que l'on puisse obtenir est égal à l'angle d'incidence du vent par rapport à l'axe du bateau. Il est en effet contre-productif de border sa voile au-delà de l'axe du bateau.
L'angle effectif de la déviation dépend de la capacité de l'air à suivre le profil de la voile. Si le rayon de la courbe à suivre est trop court, la dépression sur l'extrados devient trop forte, et l'air décroche (a tendance à reprendre la direction du vent). 
Idéalement, une voile a donc un profil courbe, l'avant du profil étant dans l'axe du vent, et l'arrière du profil dans l'axe du bateau. La distance entre le bord d'attaque et le bord de fuite doit être la plus courte possible, afin de limiter les frottements (la traînée), tout en étant suffisamment longue pour répartir la dépression. 
La dépression est d'autant plus forte que la direction forcée du flux d'air s'écarte de l'axe du vent. On peut donc se permettre un rayon de courbure plus élevé à l'avant du profil que vers l'arrière. Il en résulte un profil optimal asymptotique, où le creux se situe au premier tiers du profil.
La quantité de creux (la cambrure) et la position du creux déterminent le coefficient de portance d'une voile, d'où les nombreux dispositifs qui tentent à influencer ces paramètres. Par exemple pour une grand-voile :

De même qu'en aéronautique, lorsque les écoulements autour de l'aile décrochent, la voilure perd de son efficacité; les marins soucieux de performances savent qu'une voile développe sa plus grande force lorsqu'elle est proche du décrochement. C'est pourquoi les régatiers modifient sans cesse leurs réglages pour garder leur voile le plus proche possible du décollement, sans pour autant la faire décrocher. Ceci demande une attention constante, car le réglage doit être adapté aux variations de vitesse, de cap, et aux changements du vent. Des brins de laine ou des rubans("faveurs, penons") sont souvent fixés en plusieurs endroits du creux des voiles, afin de matérialiser l'écoulement des filets de vent, et signaler le décrochage.

Afin d'améliorer les performances des voiliers, les architectes de bateaux de course jouent aussi sur la forme du mât (mâts aile), afin d'améliorer encore ces écoulements.

Aux allures du près, la voile exerce une force propulsive tant que son angle par rapport au vent apparent reste suffisamment grand (de l'ordre de vingt degrés). Ceci a pour conséquence qu'il est possible, avec un véhicule offrant une faible résistance à l'avancement, d'aller plus vite que le vent réel. C'est le cas par exemple des planches à voiles, des multicoques, de certains monocoques conçus pour déjauger, et des chars à voiles. Les engins à voile les plus rapides étant les chars à glace capables d'atteindre quatre à cinq fois la vitesse du vent.

La force exercée par le vent sur la voile est à peu près perpendiculaire à la corde du plan de voilure. La composante de cette force qui est parallèle à l'axe du navire est la force propulsive. L'autre composante, perpendiculaire à l'axe du navire, a tendance à le faire dériver, mais peut aussi provoquer une gîte (bande) (le navire penche sur le côté), et peut compromettre dangereusement son équilibre, voire le faire chavirer.

Pour compenser cet effet néfaste, plusieurs stratégies sont utilisées :

Lorsque le navire s'éloigne du vent, l'écoulement le long de la voile peut « décrocher ». Le vent pousse littéralement la voile. Pour obtenir une propulsion maximale, il faut alors orienter différemment la voile de manière à ce qu'elle soit perpendiculaire à l'axe du vent. Il faut aussi régler la voile de façon à ce qu'elle soit la plus creuse possible. Des voiles extrêmement creuses ont été conçues à cet effet, telles les spinnakers (ou « spis ») ou les gennakers.

Si le bateau est au vent arrière, la vitesse a tendance à réduire le vent apparent. Ainsi, contrairement à l'intuition, cette allure n'est pas la plus rapide, car il n'est pas possible d'aller plus vite que le vent réel. La façon la plus rapide pour aller à un point sous le vent consiste alors parfois à tirer des bords dans une direction légèrement éloignée de l'axe du vent (grand largue), ce qui augmente le vent apparent.

Aux allures portantes, la force du vent sur la voile a tendance à enfoncer l'avant du bateau. Cela peut être dangereux, particulièrement sur les multicoques, et provoquer un enfournement. La coque sous le vent plonge alors brutalement sous l'eau. Le ralentissement violent qui en résulte peut faire chavirer le bateau sur l'avant (sancir). À grande vitesse, cette allure nécessite une attention soutenue de l'équipage. Pour éviter ces enfournements et aussi lorsque le vent forcit, on déplace le centre de gravité vers l'arrière : déplacement de l'équipage, utilisation de ballasts, par exemple, selon la taille du bateau. On recule et oriente également le centre de poussée vélique (inclinaison ou quête du mât).

En fonction de l'époque et du lieu, le type de gréement utilisé varie significativement : celui-ci est caractérisé non seulement par la forme des voiles mais par leur nombre et la nature des parties fixes et mobiles qui permettent de les maintenir en place et de les manœuvrer. La forme d'une voile ayant d'un point de vue théorique le meilleur rendement en écoulement est une demi-ellipse verticale dont la base touche la surface de l'eau. Les contraintes de mise en œuvre n'ont permis que très récemment de s'en rapprocher sans avoir à utiliser de nombreux espars et manœuvres courantes dont le poids et le fardage compensent souvent le gain apporté.

C'est le type de gréement le plus ancien en Europe. La documentation disponible indique qu'il était utilisé dès l'Antiquité, de la Baltique à la Méditerranée sur les navires marchands et militaires, qu'ils soient de mer ou de rivière.

Au l'introduction de la voile latine amorce le déclin de cette voile en Méditerranée où le régime des vents est trop irrégulier pour pouvoir l'utiliser. En Atlantique elle perdure au-delà même du Moyen Âge, des drakkars des Vikings aux cogues hanséatiques, en passant par les nefs françaises et anglaises. Les siècles suivants confirment son maintien comme en témoignent les vaisseaux produits tant pour le commerce que pour le combat.

Pendant l'essor de la marine à voile (-) l'augmentation de la dimension des navires a considérablement augmenté la hauteur des mâts et, l'on a multiplié le nombre de voiles carrées sur chaque mât (on a eu jusqu'à 7 étages) afin qu'elles restent cargables (repliables) par un nombre acceptable de marins.

Sur les longs bords de portant, les capitaines de clippers faisaient quelquefois ajouter à l'extérieur, des rallonges de vergues pour porter des voiles appelées bonnettes qui permettaient de gagner un petit peu de vitesse. Cette opération délicate et risquée était redoutée des gabiers car la chute était la promesse d'une mort certaine par noyade, le navire étant incapable de faire demi-tour pour venir le rechercher.

La compilation et la publication au milieu du par le capitaine américain Matthew Fontaine Maury des "wind charts" (somme des statistiques des vents dominants par secteurs) sur des cartes marines a permis de déterminer des routes où les vents portants ("trade winds", les vents commerciaux) étaient les plus réguliers et où ces gréements puissants étaient efficaces. Il a ainsi contribué à l'essor des grands voiliers dits à « phares carrés » ().

C'est au cours de la première moitié du que disparaît peu à peu cette voile, en particulier avec la fin des grands voiliers à prime, une des générations les plus abouties pour la taille et la vitesse, dont le "Belem" est un survivant en France, à la différence du "Duchesse Anne" qui témoigne d'une génération de grands navires école de cette période révolue.

La machine à vapeur et le moteur à combustion interne ont eu raison de cette voilure plus que millénaire.

Une voile austronésienne est une voile triangulaire traditionnelle caractéristique des populations parlant des langues austronésiennes : Océanie et Asie du Sud-Est (dunord des Philippines à la Nouvelle-Zélande), Madagascar, Ile de Pâque). Il s'agit d'une voile triangulaire de forme inversé par rapport à une voile latine (pointe de la voile vers le bas). Libérant ainsi une prise au vent important en sommet de voile.

Apparue au , d'inspiration arabe, la voile latine est surtout répandue en Méditerranée.

Sa grande vergue se nomme antenne. Pour qu'elle soit efficace sur les deux amures, il est nécessaire de la changer de côté à chaque virement. Cette manœuvre consiste à gambeyer.

Elle remplaça vite les voiles carrées utilisées depuis le temps des Romains, tant sur les navires marchands (tartanes) que militaires (galères, chébecs) car plus adaptée aux régimes de vent de cette région où elle perdure toujours sur des embarcations comme les pointus méditerranéens.

Elle est devenue aux environs du la voile auxiliaire des navires « ronds » de l'Atlantique, comme les caraques, les caravelles, puis les galions du et enfin les grands vaisseaux du et s, avant d'être détrônée sur l'océan par les différentes voiles auriques, plus aisées à manœuvrer. La voile latine continue à être utilisée en Méditerranée car elle bien adaptée aux vents de cette mer.
La voile arabe est identique à la voile latine, à cette différence près que la pointe avant de la voile est tronquée. Elle est utilisée principalement sur le Nil, en Mer Rouge et dans l’Océan Indien. Elle équipe en particulier les felouques du Nil ou les boutres.

C'est l'ancêtre du mât à haubanage moderne, les mâts étant le plus souvent inclinés vers l'arrière et voile au départ sans rond de chute maintenue au mât par un transfillage, précédant le système de haubanage dit « Marconi » (mât vertical et voile maintenue par coulisseaux sur un rail) en référence au gréement à barres de flèches nécessaire pour la supporter qui ressemblaient aux premières antennes de radio. C'est actuellement la voile la plus répandue en plaisance du fait de sa polyvalence et de ses performances notamment aux allures du près et de la facilité et simplicité de manœuvre. C'est une évolution des versions antérieures en deux pièces : la grand-voile (à corne) et une voile appelée le « flèche », frappée sur la corne et hissée au mât, système dur et complexe à manœuvrer et moins performant. Le système Bermudien/Marconi a lui-même succédé au Houari aux performances assez proches. L'on trouve maintenant des voiles entièrement lattées et dont le rond de chute est beaucoup plus important, reconstituant ainsi la forme théoriquement plus efficace des gréements traditionnels sans le fardage, le poids et les difficultés de manœuvre.

Cette voile fait partie de la famille des voiles auriques. Évolution de la voile au tiers, elle augmente encore les performances en ramenant toute la surface en arrière du mât, libérant la partie avant de celui-ci pour l'installation d'une trinquette et de focs. La forme de la voile qui déverse beaucoup au niveau de la corne, la rend peu efficace au plus près du vent mais permet cependant de porter une grande surface de toile pour un mât court.

Dans la partie supérieure peut être gréé le « flèche », ce qui permet d'augmenter la voilure notamment par petit temps…

Elle équipe nombre de gréements traditionnels de travail comme les cotres, les dundees thoniers, les coquilliers…

Des reconstitutions de navires militaires de petit tonnage comme "le Renard" (cotre corsaire) ou la goélette "Recouvrance" mettent en évidence son utilisation sur ces unités vouées à la rapidité.

À ses débuts cette voile était peu différente de la voile carrée, sa vergue étant horizontale, mais avec des performances nettement améliorées au près, notamment par « apiquage » de la vergue, c'est-à-dire que la vergue devient plus verticale en se rapprochant de l'axe du mât.

Elle fait partie de la famille des voiles auriques. Elle connut son heure de gloire avec les bateaux de pêche côtière du et au début du , surtout en Bretagne, avec par exemple le sinagot du Morbihan ou la chaloupe sardinière, qui régnait de Concarneau à la rade de Brest. Jusqu'en 1940, on la retrouve encore sur certaines unités motorisées comme les pinasses (inspirées du Sud-Ouest, mais adaptées aux conditions de travail et de mer de la région), soit en appoint, soit en gréement complet.

Le rendement d'une voile au tiers est meilleur sur une amure que sur l'autre. On parle d'amure lorsque la vergue est sous le vent venant de bâbord (gauche) ou de tribord (droite), cela a amené les chaloupes sardinières, puis les pinasses qui ont conservé le gréement au tiers, en particulier celles de Douarnenez, à adopter un gréement inversé : la voile de misaine (à l'avant du navire) était hissée sur bâbord, tandis que le taillevent (au centre du bateau) était hissé sur tribord. Cela permettait de conserver une voile avec une amure positive s'il n'était pas possible de gambeyer.

La livarde est également connue sous le nom de balestron, un espar qui permet de tendre la voile aurique en partant du mât vers le haut, ou de tendre une voile triangulaire du mât vers le point d'écoute, comme sur les sharpies.

Cette voile connut ses heures de gloire dans la marine fluviale : simple à mettre en œuvre, elle était adaptée aux mâts rabattables ou amovibles de diverses embarcations, comme les chalands, les barges et certaines péniches. Parmi les embarcations ayant porté ce gréement à la perfection, nous comptons les barges de la Tamise dont certains exemplaires naviguent encore aujourd'hui à la plaisance, tandis que d'autres sommeillent dans un musée. 

Assez peu répandue aujourd'hui, ce type de voile équipe, dès l'origine, tous les Optimist depuis 1947.

Ce gréement équipe les pirogues des pécheurs Vezo, au sud ouest de Madagascar
C'est, "stricto sensu" une voile triangulaire enverguée sur un espar parallèle au mât, d'une taille voisine de celui-ci, et hissée au moyen d'une drisse unique. Par extension et abus de langage ce terme désigne aussi une voile à corne dont la corne est trop apiquée pour permettre l'usage de la voile de flèche (Flech). Ce mode de gréement précède le type « Marconi ». 

Ce type de gréement permet d'obtenir une bonne partie des avantages de la « voile bermudienne » (plan de voilure dans l'axe, centre de voilure plus haut, écoulement laminaire amélioré...) sans avoir besoin d'un mât très long. 

On le retrouve au début du en France, dans le nord Finistère, en particulier sur nombre de cotres de pêche de la baie de Morlaix ; ex. : "Jeanne d'Arc" lancé en 1909. L'avantage de ce gréement simple à mettre en œuvre, est une certaine légèreté, d'excellentes performances aux allures du près tout en maintenant une surface de toile importante propice à la vitesse (Les premiers arrivés au port obtenaient le prix le plus élevé pour leur pêche, question de fraîcheur aussi).

Assez en faveur au début du sur les voiliers de plaisance, les progrès des matériaux composant les mâts (aluminiums, puis composites) et leurs haubanages les ont rendus rapidement obsolètes. Le Star est représentatif de cette évolution.
Ce type de gréement et voile est issu des bateaux traditionnels asiatiques : la jonque dont les voiles sont munies de lattes sur toute la surface, leur conférant une importante rigidité.

Les mâts des jonques ne portent qu'une voile de nattes tendue au moyen de bambous placés de distance en distance parallèlement à la vergue supérieure. Ces voiles sont d'un poids énorme et lorsqu'il s agit de les hisser cette opération dure souvent une demi-journée. Quelquefois dans les beaux temps on place une espèce de hunier de toile de coton au-dessus de celle du centre. Lorsqu'une jonque est en mer et que le vent augmente on ouvre un sabord pratiqué dans la voile qui en donnant une issue au vent diminue son action. Si le vent devient encore plus fort on amène une portion de la voile. Milieu , les nattes destinées à former les voiles des jonques sont tressées à plat et à la main avec les feuilles du coix lacryma. En plusieurs endroits elles sont renforcées par des liens de rotin. Cette fabrication occupe dans le Kouangtong et le Fo kiènn des milliers de vanniers.

La voile située à l'arrière d'un mât unique est nommée grand-voile. Elle peut être à corne ou bermudienne. Dans un gréement classique, les voiles situées à l'avant du mât sont nommées focs. Elles sont nommées selon leur taille et leur coupe (de la plus petite à la plus grande) : tourmentin, solent, foc, foc ou génois inter (entre le génois et le foc), yankee, génois, et reacher (génois très creux). On parlera plutôt de génois quand la bordure descend au plus près du pont, limitant l'échappement de l'air de l'intrados vers l'extrados, le point d'écoute pouvant être amené au-delà du mat vers l'arrière.

Quand plusieurs voiles sont associées en avant du mât, on parle de trinquette pour la première, de focs pour les suivantes. La voile ronde légère et généralement colorée utilisée par vent venant de l'arrière est nommée spinnaker ou spi. Il en existe deux types : les symétriques et les asymétriques. Elle dérive du « foc ballon » qui était utilisé dans les mêmes conditions de navigation, mais avec une manœuvrabilité et une efficacité moindres.

Sur un bateau à voile au tiers, la grand-voile est appelée « "taillevent" ».

Cette voile d'avant (et ses déclinaisons en voiles d'étai) est retenue (endraillée) par un câble (souvent un étai) sur son envergure. Elle est amurée à l'avant sur le pont, le bout-dehors ou le beaupré. Elle est intéressante à deux titres : d'une part, ne subissant pas les perturbations aérodynamique dues à un espar rigide elle est efficace malgré sa forme inadaptée et, d'autre part, l'interaction entre son bord de fuite et la dépression provoquée par l'extrados de la voile située en arrière accélère le flux d'air et augmente son rendement, ce qui permet aux allures de près, de bien remonter au vent.

Sur les voiliers anciens, on pouvait en avoir cinq ou six qui portaient les noms de : trinquette, petit foc, grand foc, foc volant, faux foc, clinfoc. De même, des voiles similaires étaient gréées entre les mâts, les voiles d'étai, qui favorisaient la remontée au vent des voiliers à gréement carré.

Sur les voiliers de plaisance modernes, le foc est souvent devenu la plus grande voile du système propulsif. Un voilier de plaisance traditionnel en avait trois ou plus : génois, inter (entre le génois et le foc), foc 1, foc 2 et le tourmentin qui servait pendant les forts coups de vent à assurer le maintien du bateau dans le vent et le garder manœuvrant. Pour un même voilier, plus la surface de la voile diminue, plus le grammage de la toile augmente, et moins il y a de creux (coupe de voile).

Maintenant les génois sont souvent pris sur un enrouleur qui sécurise les réductions de voilure. Les spinnakers ou « focs ballons » (apparus vers 1880) sont des très grandes voiles légères utilisées aux allures portantes. Les « tri-radials » ou gennakers sont des intermédiaires, se portant comme un génois mais d'une très grande surface.

Les voiles à l'ancienne sont fabriquées en forte toile de coton et sont formées de plusieurs largeurs ou laizes cousues côte à côte. Pour la consolider, on la munit d'un ourlet (ou gaine) renforcé par un cordage appelé ralingue. Les cosses aux points hauts, bas, intérieurs et extérieurs, servent à recevoir les cordages ou manœuvres courantes destinées à établir la voile elle-même.

La toile noyale est une toile très forte, servant à fabriquer les grandes voiles de navires à voiles. Ce fut l'un des produits dominant l'essor économique de la période faste de la Bretagne - du au .

Pour les rendre résistantes aux moisissures, intempéries et UV, elles sont régulièrement passées dans un bain chaud appelé tannée ou cachoutage : le traitement est obtenu par décoction de poudres riches en tanin, le meilleur produit étant le cachou, issu d'un arbre exotique. Elles ressortent colorées de tons allant du brun-rouge au marron-noir ; frottées afin de bien les imprégner, elles sont ensuite trempées dans l'eau de mer, le sel agissant comme fixateur. Elles sont ensuite gréées sur le navire où elles sèchent au vent. C'est parti pour environ un an.

Les voiles modernes sont constituées de fibres synthétiques. Les voiles grand public sont en majorité fabriquées en polyester (ou Dacron). Les voiles constituées de carbone, Mylar ou de Kevlar sont utilisées pour les compétitions. Ces fibres permettent de diminuer le poids des voiles tout en augmentant leur rigidité, mais elles sont peu résistantes aux UV qui affaiblissent leur souplesse et leur solidité.Selon une technique récente (3DL), les voiles haut de gamme ne sont plus constituées de panneaux de tissus assemblés par couture, mais d'un seul panneau d'un sandwich film-fibres-film, où les fibres sont positionnées par un robot sur un moule en forme, en privilégiant le sens des efforts, ce qui aboutit à une voile plus légère et un meilleur contrôle des déformations induites par le vieillissement. 

Les voiles sont fabriquées dans des ateliers spécialisés, les voileries, généralement dirigées par des maîtres voiliers qui participent à leur conception et à leur façonnage.

Sur les voiliers modernes les voiles d'avant sont fréquemment installées sur un enrouleur (ou à emmagasineur) c'est-à-dire qu'elles s'enroulent autour d'un tube pour diminuer leur surface exposée. Ce système permet de réduire considérablement le nombre de voiles nécessaires, et de manœuvres pour les gréer.

Grâce à ce dispositif, on n'utilise plus qu'une seule voile d'avant au lieu des quatre ou cinq précédentes : génois, inter (entre le génois et le foc), foc, trinquette, tourmentin. Une voile ainsi réduite est moins efficace qu'une voile entière de surface équivalente. En effet, le centre de voilure se retrouve plus haut et, de plus, la coupe obtenue n'est pas optimale : plus la voile est réduite plus elle devient creuse, alors qu'on souhaiterait avoir une voile plus plate pour naviguer par vent plus fort.

Un système approchant existe aussi pour la grand-voile qui s'enroule cette fois à l'intérieur de la bôme ou du mât, permettant de réduire la toile.

Ces matériels permettent de remplacer mécaniquement l'opération de diminution de toile sur une voile classique qui se nomme prendre un ris. Le problème est la propension de ces systèmes à se coincer, du fait d'un enroulement défectueux ou de l'action oxydante des sels marins.

La voile utilisée de cette manière est soumise à une usure plus rapide que la voile manœuvrée de manière classique : plus de frottements, de mauvaises tensions, accumulation d'une humidité résiduelle, macération due aux sels marins…

Sur les voiliers traditionnels, compte tenu des qualités très différentes des tissus employés avant l'arrivée des matériaux synthétiques, les voiles sont généralement « enverguées » c'est-à-dire fixées en partie haute et/ou basse sur un espar (en bois ou en métal) appelé vergue et qui sert à la déployer.

Dans les voiliers à gréement carré, la vergue étant retenue par le milieu, ses deux extrémités sont dirigées par des bras (bras au vent et bras sous le vent) et les deux angles inférieurs de la voile par des écoutes (sous le vent) et des amures (au vent) qui servent à brasser (régler l'incidence par rapport au vent) la voilure.

Jusqu'au , les voiles étaient réduites ou ferlées (repliées) par un grand nombre de gabiers (matelots) qui devaient remonter à la main des surfaces importantes de tissu lourd, souvent mouillé, voire gelé ce qui était très difficile et dangereux. Au cours du temps, avec l'augmentation des tonnages des navires et la nécessité d'améliorer la vie à bord des marins, on a progressivement de plus en plus divisé les surfaces de voile en augmentant le nombre de vergues, notamment sur les basses voiles, les plus grandes, et on a pu simplifier les opérations de réduction en installant des « cargues », cordages qui servent à retrousser les voiles depuis le pont. On les appelle « cargue-point », « cargue-fond » ou « cargue-bouline » selon le point d'attache sur la voile.

À bord des navires, voiles et cordages étaient également tannés, afin de lutter contre la pourriture des textiles, qu'il s'agisse de lin, de chanvre ou de coton. En France, cette opération fut longtemps effectuée à base de tan, écorce de chêne moulue et longuement bouillie. Dans le Golfe du Morbihan, connu par ses voiles rouges, on utilisait de l'écorce de pin pilée. À partir de la fin du , le tan à base d'écorces locales est remplacé par le cachou, qui provient du "Areca catechu", un bois exotique, ce qui donna aux voiles leur couleur rouge brun longtemps si caractéristique. À Douarnenez par exemple, les marins en utilisaient une dose plus forte que dans les autres ports, ce qui expliquait la couleur plus sombre des voiles des bateaux de ce port. Des tannages mixtes, à base par exemple de tan et d'ocre mêlés, ou utilisant divers produits locaux, selon les endroits, étaient aussi pratiqués.

Les petites voiles étaient tannées directement dans de grandes cuves qui appartenaient généralement au bistrot où l'équipage faisait escale. Les plus grandes voiles étaient enduites au balai sur le quai ou sur les dunes. Progressivement, après la Première Guerre mondiale, le cachou fut remplacé par de la poudre d'ocre additionnée parfois d'huile de lin, ce qui donna aux voiles une couleur brique intense et un aspect velouté.







</doc>
<doc id="6706" url="https://fr.wikipedia.org/wiki?curid=6706" title="Voilier (homonymie)">
Voilier (homonymie)



</doc>
<doc id="6707" url="https://fr.wikipedia.org/wiki?curid=6707" title="Vent">
Vent

Le vent est le mouvement au sein d’une atmosphère, masse de gaz située à la surface d'une planète, d'une partie de ce gaz. Les vents sont globalement provoqués par un réchauffement inégalement réparti à la surface de la planète provenant du rayonnement stellaire (énergie solaire), et par la rotation de la planète. Sur Terre, ce déplacement est essentiel à l'explication de tous les phénomènes météorologiques. Le vent est mécaniquement décrit par les lois de la dynamique des fluides, comme les courants marins. Il existe une interdépendance entre ces deux circulations de fluides.

Les vents sont généralement classifiés selon leur ampleur spatiale, leur vitesse (ex : échelle de Beaufort), leur localisation géographique, le type de force qui les produit et leurs effets. La vitesse du vent est mesurée avec un anémomètre mais peut être estimée par une manche à air, un drapeau, etc. Les vents les plus violents actuellement connus ont lieu sur Neptune et sur Saturne.

Le vent est l'acteur principal de l'oxygénation des océans ainsi que des lacs de haute montagne, par agitation et mise en mouvement de leurs surfaces. Il permet le déplacement de nombreux agents organiques et minéraux et d'expliquer la formation de certaines roches sédimentaires (ex: Lœss). Il influence le déplacement des populations d’insectes volants, la migration des oiseaux, il façonne la forme des plantes et participe à la reproduction de certains végétaux. L'érosion éolienne participe parfois à la morphologie du relief local (ex: congère de neige, dunes). Le vent a inspiré dans les civilisations humaines de nombreuses mythologies. Il a influé sur les transports, voire les guerres, mais également fourni des sources d’énergie pour le travail purement mécanique (ex. : moulins à vent, éoliennes) et pour l’électricité. Il participe même aux loisirs.

Le vent fait le plus souvent référence aux mouvements de l’air dans l'atmosphère terrestre. Par extension, le mouvement de gaz ou de particules polarisées allant du Soleil vers l’espace extérieur est appelé vent solaire et l’échappement gazeux de particules légères d’une atmosphère planétaire vers l’espace est nommé le "vent planétaire".

Les vents sont souvent classifiés selon leur force et la direction d’où ils soufflent. Il existe plusieurs échelles de classification des vents dont les plus connues sont l'échelle de Beaufort et l'échelle de Fujita. La première classe la force des vents selon treize niveaux qui vont du calme à celui des vents de force d'ouragan, en passant par la brise, le coup de vent et la tempête. La seconde classifie la force des vents dans une tornade.

Les pointes de vents au-dessus du vent moyen sont appelées rafales. Lorsque le vent moyen augmente durant une courte période, il s'agit de "bourrasques" de vents. Des vents violents associés à un orage sont appelés rafales descendantes, connues en mer comme des grains. Des vents violents sont associés avec plusieurs autres phénomènes météorologiques tels les cyclones tropicaux, les tempêtes et les tornades.

Le premier instrument de mesure du vent est la girouette, invention de la Grèce antique destinée à indiquer la direction du vent. Nous devons la première description scientifique des phénomènes éoliens à Evangelista Torricelli qui mit en évidence la pression atmosphérique de l'air avec son baromètre et à Blaise Pascal qui fut le premier à décrire le vent comme un mouvement de l'air, un courant d'air plus ou moins puissant ainsi que la diminution de pression avec l'altitude puis Robert Hooke construira le premier anémomètre. Benjamin Franklin se lancera lui dans les premières descriptions et analyses de vents dominants et de systèmes météorologiques.

Lorsqu’un véhicule ou une personne se déplace, le vent ressenti au cours du déplacement peut être très différent du vent généré par les conditions météorologiques avec des conséquences parfois importantes. On distingue :



Plusieurs échelle de classification des vents existent, la plus commune est celle de Beaufort utilisée par les marins. Celle-ci est une échelle de mesure empirique, comportant 13 degrés (de 0 à 12), de la vitesse moyenne du vent sur une durée de dix minutes utilisée dans les milieux maritimes. Initialement, le degré Beaufort correspond à un état de la mer associé à une « fourchette » de la vitesse moyenne du vent. Même si, de nos jours, cette vitesse peut être mesurée avec une bonne précision à l'aide d'un anémomètre, il reste commode, en mer, d'estimer cette vitesse par la seule observation des effets du vent sur la surface de la mer.

L'échelle de Fujita est une échelle de classement de la force des tornades selon les dommages causés. Elle est utilisée aux États-Unis pour remplacer l'échelle originale de Fujita depuis la saison estivale 2007. Elle a été développée pour pallier les faiblesses notées dans l'échelle originale qui montraient des incertitudes quant à la force des vents nécessaires pour causer certains dommages et à l'évaluation de situations similaires mais ayant affecté des constructions de différentes solidités. 

Finalement, l'échelle de Saffir-Simpson pour les cyclones tropicaux, nommés « ouragans », se formant dans l'hémisphère ouest, qui inclut les bassins cycloniques de l'océan Atlantique et l'océan Pacifique nord à l'est de la ligne de changement de date. Elle est graduée en cinq niveaux d'intensité, correspondant à des intervalles de vitesses de vents normalisés. Pour classer un cyclone sur cette échelle, la vitesse des vents soutenus est enregistrée pendant une minute à une hauteur de 10 mètres (33 pieds), la moyenne ainsi obtenue est comparée aux intervalles (Voir les catégories d'intensité).

On distingue trois zones de circulation des vents entre l'équateur et les Pôles. La première zone est celle de Hadley qui se situe entre l'équateur et 30 degrés Nord et Sud où l'on retrouve des vents réguliers soufflant du nord-est dans l'hémisphère nord et du sud-est dans celui du sud : les alizés. Les navigateurs à voile ont depuis longtemps utilisé cette zone de vents réguliers pour traverser les océans. La seconde se situe aux latitudes moyennes et est caractérisée par des systèmes dépressionnaires transitoires ou les vents sont surtout d'ouest, c'est la cellule de Ferrel. Finalement, la cellule polaire se retrouve au nord et au sud du avec une circulation de surface généralement d'est. Entre ces trois zones, on retrouve les courant-jets, des corridors de vents circulant autour de la planète à une altitude variant entre 10 et et qui sont le lieu de frontogenèses.

Ces traits généraux de la circulation atmosphérique se subdivisent en sous-secteurs selon le relief, la proportion mer/terre et d'autres effets locaux. Certains donnent des vents ou des effets sur de grandes étendues alors que d'autres sont très locaux.
La cellule du Pacifique, entièrement océanique, est particulièrement importante. On lui a donné le nom de cellule de Walker en l'honneur de Sir Gilbert Walker, dont le travail a conduit à la découverte d'une variation périodique de pression entre les océans Indien et Pacifique, qu'il dénomma l’oscillation australe. Le courant de Humboldt, venant de l'Antarctique, refroidit la côte occidentale de l'Amérique du Sud, créant une grande différence de température entre l'ouest et l'est du continent, laquelle donne lieu à une circulation directe semblable à celle de Hadley mais limitée à la zone Pacifique. "El Niño" est un courant d'eau chaude de surface qui envahit la partie orientale du Pacifique Sud à la suite d'un affaiblissement des alizés, vents équatoriaux, déplaçant la cellule de Walker et permettant à l'eau plus chaude du Pacifique Sud-Ouest de se déplacer vers l'est. Les remontées d'eau froide qui se retrouve habituellement le long de la côte de l'Amérique du Sud sont coupées ce qui modifie grandement le climat, non seulement dans le Pacifique Sud mais également la circulation atmosphérique générale à des degrés divers. Par exemple, El Niño empêche la formation de tempêtes tropicales et d'ouragans sur l'océan Atlantique, mais augmente le nombre de tempêtes tropicales qui touchent l'est et le centre de l'océan Pacifique.

"La Niña" est l'inverse du phénomène El Niño alors que l'eau chaude de surface se déplace plus vers l'Asie. Il ne s'agit pas d'un retour vers la situation normale mais un extrême de l'autre côté. Il n'y a pas de symétrie entre les deux phénomènes, on a relevé par le passé davantage d'épisodes El Niño que d'épisodes La Niña.

La mousson est le nom d'un système de vents périodiques des régions tropicales, actif particulièrement dans l'océan Indien et l'Asie du Sud. Il est appliqué aux inversions saisonnières de direction du vent le long des rivages de l'océan Indien, particulièrement dans la mer d'Arabie et le golfe du Bengale, qui souffle du sud-ouest pendant six mois et du nord-est pendant l'autre semestre. La mousson est un exemple extrême des brises de terre et brises de mer car elle ne s'inverse pas sur un mode nocturne/diurne

Il existe également des systèmes météorologiques si anciens et si stables que ces vents ont reçu un nom, voire étaient parfois considérés comme des divinités comme au Japon pour les kami kaze. De très nombreux vents célèbres existent autour du monde tels le couple Mistral/Tramontane, le sirocco, le Chinook, le Khamsin ou encore le Simoun.

Les causes principales des grands flux de circulation atmosphérique sont : la différence de température entre l’équateur et les pôles, qui provoque une différence de pression, et la rotation de la Terre qui dévie le flot d'air qui s'établit entre ces régions. Des différences locales de pression et de températures vont quant à elle donner des circulations particulières comme les brises de mer ou les tornades sous les orages.

La pression atmosphérique en un point est la résultante surfacique du poids de la colonne d’air au-dessus de ce point. Les différences de pression qu’on note sur le globe terrestre sont dues à un réchauffement différentiel entre ces points. En effet, l’angle d’incidence du rayonnement solaire varie de l’équateur aux pôles. Dans le premier cas, il est normal à la surface de la Terre alors que dans le second, il est rasant. Cette variation conditionne le pourcentage d’énergie solaire reçue en chaque point de la surface terrestre. De plus, les nuages reflètent une partie de cette énergie vers l’espace et elle est absorbée différemment selon le type de surface (mer, forêt, neige, etc.).

La différence de pression ainsi créée induit un déplacement d’air des zones de haute pression vers les zones de basse pression. Si la Terre ne tournait pas sur son axe, la circulation serait rectiligne entre les régions de haute et les régions de basse pression. Cependant, la rotation de la Terre entraîne une déviation de la circulation sous l'effet de la force de Coriolis, cette déviation étant vers la droite dans l'hémisphère Nord et vers la gauche dans l'hémisphère Sud. L'air subit ainsi une somme vectorielle des deux forces (force de Coriolis et résultante des forces de pression).

À mesure que les parcelles d'air changent de direction, la force de Coriolis change également de direction. Lorsque les deux sont presque égales et de directions opposées, la direction du déplacement de l’air se stabilise pour être presque perpendiculaire au gradient de pression (voir figure ci-contre). La petite différence qui subsiste laisse une accélération vers la plus basse pression, la direction du vent reste donc orientée un peu plus vers les basses pressions ce qui fait que le vent tourne autour des systèmes météorologiques. Aux forces de pression et de Coriolis, il faut ajouter le frottement près du sol, la force centrifuge de courbure du flux et la tendance isallobarique, pour correctement évaluer le vent dans le cas général.

À grande échelle dans l'hémisphère nord, les vents tournent donc dans le sens horaire autour d'un anticyclone, et anti-horaire autour des dépressions. L'inverse est vrai pour l'hémisphère sud où la force de Coriolis est inverse. On peut déterminer sa position entre ces deux types de systèmes selon la loi de Buys-Ballot : un observateur situé dans l'hémisphère nord qui se place dos au vent a la dépression à sa gauche et l'anticyclone à sa droite. La position des zones de pressions est inversée dans l'hémisphère sud.

La force de Coriolis s’exerce sur de longues distances ; elle est nulle à l’équateur et maximale aux pôles. Dans certaines situations, le déplacement d’air ne s’exerce pas sur une distance suffisante pour que cette force ait une influence notable. Le vent est alors causé seulement par le différentiel de pression, le frottement et la force centrifuge. Voici quelques cas qui se produisent lorsque la circulation générale des vents est nulle, très faible ou quand on doit tenir compte d'effets locaux:

Les montagnes ont différents effets sur les vents. Le premier est l’onde orographique lorsque le vent soufflant perpendiculairement à une barrière montagneuse doit remonter la pente. Si l'environnement est stable, la masse d'air redescendra du côté aval de l'obstacle et entrera en oscillation autour d'une hauteur qui peut être largement supérieure au sommet de celui-ci. Par contre, si l'air est instable, l'air continuera de s'élever, avec ou sans oscillation. Dans ces conditions, le frottement et la poussée d'Archimède doivent être pris en compte lors de la modélisation du vent, comme c'est le cas pour le foehn. Les pluies en sont modifiées.

L’air froid plus dense en haut d’une montagne y crée une pression plus forte que dans la vallée et provoque un autre effet. Le gradient de pression fait alors dévaler la pente à l’air sur une distance insuffisante pour que la force de Coriolis le dévie. Cela engendre donc un vent catabatique. On rencontre ce genre d’effet le plus souvent la nuit. Ils sont également très communs au front d’un glacier, par exemple, sur la côte du Groenland et de l’Antarctique à toute heure.

Le vent anabatique est un vent ascensionnel d'une masse d'air le long d'un relief géographique dû au réchauffement de celui-ci et donc l'opposé du vent précédent. Diverses conditions météorologiques peuvent créer un vent anabatique, mais il s'agit toujours de la formation d'une différence de température entre les masses d’air au-dessus des vallées et celles réchauffées sur leurs pentes qui cause une circulation d’air. Il est donc aussi appelé vent de pente et se produit le plus souvent le jour.

La "rugosité" du paysage et en particulier la rugosité "molle" (celle des forêts, bocages, savanes, par rapport aux roches et immeubles qui ne bougent pas) des arbres a un impact sur les vents et les turbulences, et indirectement sur les envols ou dépôts de poussières, la température, l'évaporation, le mélange de la partie basse de la colonne d'air (de la hauteur des pots d'échappement à la hauteur où sont émis les panaches de cheminées d'usines ou de chaudières urbaines par exemple), la régularité du vent (important pour les installations d'éoliennes ou de fermes éoliennes), etc. À cet effet, Kalnay et Cai dans la revue Nature, avaient en 2003 posé l'hypothèse que les arbres freinaient significativement le vent. En forêt tropicale dense, hormis lors des tempêtes, au sol on ne sent presque plus les effets du vent. La plupart des arbres n'y produisent leurs puissants contreforts que quand ils émergent au niveau de la canopée où ils sont alors exposés à un éventuel déracinement par le vent.

On a récemment réanalysé les données météorologiques de mesure des vents de surface (vent à de hauteur) qui confirment dans l’hémisphère nord une tendance au ralentissement. Il semble que les forêts puissent, dans une certaine mesure, freiner le vent et la désertification l'exacerber. Là où la forêt a regagné du terrain, la force du vent a diminué (de 5 à 15 %), de manière d'autant plus visible que le vent est fort. Les vents géostrophiques (induits par les variations de pression atmosphérique) n'ont pas diminué, et les radiosondes ne montrent pas de tendance au ralentissement en altitude. Le bocage est une structure écopaysagère qui modifie également les effets du vent en créant des microclimats atténuant le vent, mais aussi les chocs thermo-hygrométriques et l'érosion des sols.

Durant le jour, près des côtes d’un lac ou de la mer, le soleil réchauffe plus rapidement le sol que l’eau. L’air prend donc plus d’expansion sur terre et s’élève créant une pression plus basse que sur le plan d’eau. Une fois encore cette différence de pression se crée sur une distance très faible et ne peut être contrebalancée par les forces de Coriolis. Une brise de mer (lac) s’établit donc. La même chose se produit la nuit mais en direction inverse, la brise de terre.

On observe des différences de pressions jusqu'à deux millibars et proportionnelles aux masses de terre et d'eau en présence. Cette brise peut résister à un autre vent jusque de l'ordre de (8 nœuds) ; au-delà, elle est en général annulée ce qui ne signifiera pas un calme plat mais plutôt un système météo instable. Ceci explique également pourquoi il y a très rarement un calme plat en bord de mer mais aussi des vents plus tourmentés qu'à l'intérieur des terres ou en mer.

Dans certaines conditions de contraintes, par exemple dans des vallées très encaissées, l’air ne peut que suivre un chemin. Si le gradient de pression devient perpendiculaire à la vallée, le vent sera généré exclusivement par la différence de pression. C'est le vent antitriptique. On trouve aussi des accélérations dans les resserrements par effet Venturi qui donne un « vent de goulet » et un « courant-jet de sortie de vallée » alors que l'air descendant la vallée envahit la plaine.

Dans d’autres cas, la pression et la force centrifuge sont en équilibre. C’est le cas des tornades et des tourbillons de poussières où le taux de rotation est trop grand et la surface de la trombe est trop petite pour que la force de Coriolis ait le temps d’agir.

Finalement, dans le cas de nuages convectifs comme les orages, ce n’est pas la différence de pression mais l’instabilité de l’air qui donne les vents. Les précipitations ainsi que l’ d’air froid et sec dans les niveaux moyens amènent une poussée d'Archimède négative (vers le bas) dans le nuage. Cela donne des vents descendants qui forment des fronts de rafales localisés.

Le vent dépend donc de plusieurs facteurs. Il est la résultante des forces qui s’exercent sur la parcelle d’air : la pression, la force de Coriolis, le frottement et la force centrifuge. Le calcul complet se fait avec les équations du mouvement horizontal des équations primitives atmosphériques. En général, la force centrifuge est négligée car la vitesse de rotation autour de la dépression est trop lente et sa valeur est donc très petite par rapport aux autres forces. Cependant, dans une circulation rapide comme celle d’une tornade, il faut en tenir compte.

Avec ces équations, les cartes météorologiques permettent d’estimer le vent en connaissant la pression, la latitude, le type de terrain et les effets locaux même si on n’a pas de mesure directe. Pour l’aviation au-dessus de la couche limite atmosphérique, où le frottement est nul, on utilise une approximation du vent réel que l'on peut obtenir par les équations du vent géostrophique. Il est le résultat de l'équilibre entre les forces de Coriolis et de la variation horizontale de pression seulement. Ce vent se déplace parallèlement aux isobares et sa vitesse est définie approximativement par le gradient de pression.

Le vent du gradient est similaire au vent géostrophique mais en reprenant en plus la force centrifuge (ou accélération centripète) quand la courbure du flux est significative. Il est, par exemple, une meilleure approximation du vent autour d'une dépression ou d'un anticyclone.

Près du sol, dans la couche limite, le frottement cause une diminution des vents par rapport à l’estimation précédente selon ce qu’on appelle la spirale d'Ekman. En général, le vent est de 50 à 70 % du vent géostrophique sur l’eau et entre 30 et 50 % de ce vent sur la terre ferme. Plus le vent est diminué par le frottement, plus il tourne vers la plus basse pression ce qui donne un changement vers la gauche dans l’hémisphère nord et vers la droite dans celui du sud. Cette différence entre les vents réels et géostrophiques se nomme le vent agéostrophique. Il est donc particulièrement important dans la couche limite mais existe également au-dessus de celle-ci car le vent géostrophique n'est qu'une approximation. Le vent agéostrophique est important dans l'alimentation en air humide des dépressions ce qui leur fournit de l'énergie.

Dans les endroits accidentés où le flux d’air est canalisé ou dans les situations où le vent n’est pas dû à un équilibre entre pression, force de Coriolis et frottement comme mentionné précédemment, le calcul est beaucoup plus difficile. Parmi ces cas figurent :

Pour calculer la variation du vent avec l'altitude, le concept de vent thermique a été développé. Il s'agit de la différence du vent géostrophique entre deux niveaux de l'atmosphère. Il porte le nom de thermique parce que la variation du vent avec l'altitude dépend de la variation horizontale de température comme vu antérieurement. Ainsi dans une masse d'air isotherme, dite barotrope, le vent ne varie pas avec l'altitude alors qu'il variera dans une atmosphère barocline. C'est dans cette dernière situation, près des fronts météorologiques, que l'on retrouve des vents qui augmentent rapidement avec l'altitude pour donner un corridor de vent maximal juste sous la tropopause que l'on appelle un courant-jet.

Pour une altitude inférieure à mètres environ, là où se trouvent les ouvrages bâtis, les forces de frottement dues à la rugosité du sol et les phénomènes thermiques régissent en grande partie les écoulements d’air. Ces phénomènes engendrent des fluctuations de la vitesse du vent, dans le temps et dans l’espace, susceptibles d’exciter les structures les plus souples. Cette zone est appelée "couche limite de turbulence atmosphérique".

L’analyse spectrale de la vitesse du vent dans la couche limite turbulente permet de mettre en évidence plusieurs échelles temporelles de fluctuation. La figure ci-contre montre l’allure d’un spectre de densité de puissance représentatif de la vitesse horizontale du vent à au-dessus du sol d’après Van der Hoven. Il s'agit d'une représentation statistique de la répétitivité des fluctuations de puissance du vent en ce point : 

La partie gauche du graphe concerne les systèmes à l'échelle planétaire qui ont une périodicité entre 1 jour et un an, ce qui correspond à une période de retour de différents types de systèmes météorologiques synoptiques. Ainsi, un an représente les vents annuels comme les alizés, quatre jours les vents associés avec la période moyenne entre deux dépressions météorologiques et 12 heures les vents diurnes et nocturnes en alternance. La partie droite du graphe concerne les conditions locales reliées à des conditions de relief ou autres effets de méso-échelle comme la distribution des nuages, le gradient thermique vertical, la vitesse moyenne du vent, la rugosité des sols, etc. Le « trou » entre une heure et dix minutes au milieu correspond à des périodes de grand calme quand les turbulences s'annulent elles-mêmes.

Les sollicitations répétées et aléatoires des turbulences peuvent solliciter les modes propres de certains ouvrages et conduire à leur ruine si cela n’a pas été pris en compte lors du dimensionnement (comme le pont du détroit de Tacoma en 1940).

Le vent est un élément majeur des systèmes météorologiques puisqu'il est leur médium de transport. Cependant, la Terre est très irrégulière dans la forme de ses continents et l'ensoleillement dépend non seulement des saisons mais également de la couverture nuageuse. Cette dernière est soumise au vent qui tire son énergie des différences de températures qui sont une des résultantes de l'ensoleillement. Le vent obéit ainsi aux lois de l'effet domino, la difficulté résidera dans le nombre de facteurs à prendre en compte quant à la prévisibilité du résultat puisque le vent se nourrit de multiples sources : d'autres vents, les différences de températures entre deux zones géographiques ou entre deux couches d'atmosphère, la rotation de la Terre, l'attraction terrestre, les effets sur le relief, etc.

Par exemple, un ouragan né dans l'Atlantique peut très bien rentrer par le golfe du Mexique et venir mourir aux Grands Lacs, perturbant tous les vents locaux sur et autour de sa trajectoire. L'origine de la création de ce cyclone tropical peut très bien être un déséquilibre engendré par un creux barométrique en altitude venant du Sahara qui a été déporté jusque dans l'Atlantique par l'anticyclone des Açores. La prévision des vents jusqu'à plusieurs jours est possible de façon déterministe grâce à la résolution des équations primitives atmosphériques des forces en présence si on tient compte de tous ces facteurs.

Cependant, les valeurs de chaque variable de ces équations ne sont connues qu'en des points distincts de l'atmosphère selon les observations météorologiques. Une légère erreur de ces valeurs peut causer de grande variation et c'est pourquoi l'on peut dire que la théorie du chaos, les systèmes complexes et plus particulièrement l'effet papillon s'appliquent très bien à la prévision des vents. Edward Lorenz a démontré que les prévisions n'étaient possibles à long terme (un an) que de façon probabiliste car le nombre de facteurs d'environnement est immense mais aussi qu'ils interagissent entre eux ce qui donne une instabilité temporelle à la résolution des équations.


Les roses des vents sont aussi utiles aux architectes et urbanistes, notamment pour la construction bioclimatique.

Par exemple, dans l'image de droite, la rose des vents montre les vents dominants et leur variation de force moyenne selon leur orientation et direction. Les vents les plus forts se superposent globalement aux courants et à la direction (résultante) du déplacement de la masse d'eau de la Manche vers la Mer du Nord. Ces vents quand ils vont dans le même sens que la marée peuvent causer des "surcotes" de marée haute, c'est-à-dire une mer plus haute qu'annoncée par le calcul du simple coefficient de marée, dont la hauteur est estimée par la partie du bas.

Au sol, en mer et en altitude, le vent est mesuré en kilomètres par heure, en mètres par seconde ou en nœuds. La mesure directe du vent se fait dans des stations météorologiques sur la terre ferme ou en mer grâce à un anémomètre, qui en donne la vitesse, et une girouette, qui en donne la direction. Les anémomètres mécaniques sont formés de coupelles qui tournent autour d'un axe quand le vent souffle. Il existe d'autres versions dont celles dites à "fil chaud" où le changement de température d'un thermistor causé par le flux d'air correspond à la vitesse de ce dernier.

On obtient par radiosondage la variation des vents avec l'altitude en suivant le mouvement d’un ballon-sonde depuis le sol. La mesure du déplacement d'un ballon ascensionnel dépourvu de sonde à l'aide d'un théodolite constitue une alternative économique au radiosondage. Depuis l’espace, grâce aux instruments d’un satellite météorologique, on peut estimer les vents partout sur Terre. Ces données sont particulièrement utiles aux endroits inhabités comme les déserts et les océans. C'est également de cette façon que les vents sur les autres planètes sont estimés. En aviation, la vitesse du vent est estimée en utilisant deux tubes de Pitot, le premier dans la direction opposée au déplacement et le second perpendiculairement à celui-ci.

Les radars météorologiques Doppler, les profileurs de vent, les lidars Doppler et les sodars sont des instruments de télédétection au sol capables de mesurer la vitesse du vent en altitude.

Le vent peut également être estimé par une manche à air au sol et les marins l'estime en utilisant l’échelle de Beaufort, échelle fermée à 13 niveaux de force 0 à force 12, s’ils n’ont pas d’instruments pour la mesurer. Cette échelle relie l’effet du vent sur la mer (hauteur des vagues, production d’embruns, etc.) à sa vitesse. L'échelle de Fujita et l'échelle de Fujita améliorée utilisent les dommages causés par une tornade pour estimer la force qu'avaient ses vents.

L’Organisation météorologique mondiale (OMM) a homologué début 2010 le record du vent le plus violent jamais observé scientifiquement sur Terre, hors ceux des tornades. Il s'agit de rafales de enregistrées le 10 avril 1996 à l’île de Barrow (Australie-Occidentale) lors du passage du cyclone Olivia. Le précédent record de datait d'avril 1934 au sommet du mont Washington aux États-Unis. Cependant, le cyclone Olivia n'est pas considéré lui-même comme le plus violent à avoir affecté la région australienne car ce record ne représente pas l'intensité générale du système.

La mesure record dans une tornade a été effectuée à Moore en Oklahoma lors de la série de tornades de l'Oklahoma du 3 mai 1999. À , un radar météorologique Doppler mobile a détecté des vents de ± dans le tourbillon près de "Bridge Creek" à une hauteur de au-dessus du sol. Le record précédent était de 414 à mesuré dans une tornade près de "Red Rock" (Oklahoma). Cependant, les vents au sol ont pu être plus faibles à cause du frottement.

Le record du monde de vent enregistré par une station au niveau de la mer dans des conditions non reliées aux tornades ou aux cyclones tropicaux est celui de la station météorologique de la base antarctique Dumont d'Urville en Terre Adélie. Celle-ci est en opération depuis 1948 et les vents catabatiques y soufflent presque constamment. Leur moyenne annuelle est d'environ et le nombre de jours avec des vents de plus de est d'environ 300. Le record à cette station date du 16 juin 1972 à locale, lors d'un phénomène de "Loewe" de changement brusque de la force des vents catabatiques, alors que le vent atteignit pendant 5 minutes, avec une pointe de .

Enfin, lors de la tempête Martin, le 27 décembre 1999 à minuit, un radiosondage effectué par Météo-France a enregistré une vitesse du vent exceptionnelle de dans le courant-jet à d'altitude au-dessus de Brest.

Des vents de plus de soufflent sur Vénus et font que ses nuages font le tour de la planète en 4 à 5 jours terrestres.

Lorsque les pôles de la planète Mars sont exposés aux rayons du soleil à la fin de l'hiver, le CO congelé est sublimé, créant ainsi des vents quittant les pôles à plus de ce qui va alors transporter des quantités importantes de poussière et de vapeur d'eau à travers tous les paysages martiens. Il existe également des vents subits et liés à l'activité solaire qui ont été surnommés "" par la NASA parce qu'ils apparaissaient subitement et dépoussiéraient tout, y compris les panneaux solaires.

Sur Jupiter, les vents soufflent jusqu'à () dans les zones de courant-jet. Saturne fait partie des records du système solaire avec des pointes à plus de (). Sur Uranus, dans l'hémisphère nord jusqu'à 50° de latitude, la vitesse peut monter à () « seulement ». Finalement, par-dessus les nuages de Neptune, les vents dominants peuvent atteindre () le long de l'équateur et jusqu'à () à ses pôles. Il existe en outre un courant-jet extrêmement puissant à 70° de latitude Sud qui peut atteindre ().

Les vents sont une source d’énergie renouvelable, et ont été utilisés par l'Homme à travers les siècles à divers usages, comme les moulins à vent, la navigation à voile ou plus simplement le séchage. Différents sports utilisent le vent dont le char à voile, le cerf-volant, le vol à voile, la planche à voile et le kitesurf. Il sert également à aérer, assainir, rafraîchir les milieux urbains et les bâtiments. Le vent est une de nos plus anciennes sources d'énergie et une grande partie de toutes nos productions tire parti du vent ou lui est adapté. Aujourd'hui encore, il est un intense sujet de recherche car son potentiel d'utilisation demeure encore largement inutilisé tant via des éoliennes que des systèmes de pompe à chaleur ou pour assainir l'air urbain par une urbanisation raisonnée des villes en tenant compte du vent.

La première utilisation du vent par l'Homme fut simplement l'aération et le séchage. En effet, un lieu où l'air stagne va assez rapidement se charger en odeur mais aussi permettre le développement de différentes maladies et développement de moisissures (s'il y a un minimum d'humidité).

Très vite, l'Homme découvrit que des objets laissés au vent séchaient plus vite, cela est dû à deux phénomènes distincts : d'une part, l'air en mouvement vient frapper l'objet désiré et va donc communiquer une énergie qui permet d'arracher l'humidité à l'objet, poreux ou non, si l'objet est poreux et se laisse traverser par le vent, l'efficacité sera renforcée. D'autre part, l'air et les objets en contact avec celui-ci ont tendance à vouloir équilibrer leur taux d'humidité. Cependant, l'eau, même sous forme de vapeur, a une forte valeur de tension superficielle (comme une bulle d'air dans l'eau) et si elle va se dissiper dans les environs immédiats de l'objet qui sèche, les forces de tension vont globalement créer une bulle d'humidité, et ce d'autant que l'air chargé d'humidité est plus lourd et voit sa montée contrariée par l'air plus froid au-dessus de lui, ce qui crée une colonne de pression locale prenant la forme d'une demi-bulle en l'absence de vent. Le soleil ne va aider ici qu'à augmenter la quantité de vapeur soluble dans l'air. Sans vent, le séchage va s'arrêter même en plein air car la diffusion de l'humidité dans l'air se fera de manière très lente et même freinée par les forces intermoléculaires mais aussi par le fait que l'air ne se sature pas plus en humidité que son point de rosée ne le permet. Ce point de rosée dépend de la température de l'air. La température engendre un mouvement brownien permettant le transfert léger au sein de la masse d'air. Cet effet a été mis en évidence, étudié et très bien calculé dans le séchage du bois. Toute masse d'air est donc hydrophile jusqu'au maximum de son point de rosée. Dans une atmosphère non renouvelé, le séchage ne pourrait s'achever que si la quantité d'eau à extraire était inférieure au point d'équilibre du milieu.

De même, dans le cas des marais salants, le soleil va fournir l'énergie de réchauffement qui optimisera la présence de vapeur d'eau libre en surface de l'eau et augmentera la quantité d'eau captable dans l'air. C'est le vent qui va alors emporter cette eau via l'air déplacé et donc contribuer au séchage en renouvelant l'atmosphère ce qui empêche le milieu d'atteindre son point de saturation.

L'aération est donc également une méthode pour éviter la prolifération d'humidité due aux activités diverses dans un bâtiment, or l'aération dépend de la présence de vent.

Exemples de relation sécheresse d'un bois/paramètres de séchage.

Selon ce tableau, on voit bien que pour sécher un bois jusqu'au bout, il faut renouveler l'atmosphère, sans quoi il ne descendra jamais en dessous d'un certain seuil d'hygrométrie.

Les montgolfières utilisent le vent pour des petits voyages. Le vent de face augmente la portance lors du décollage d'un avion et augmente la vitesse de ce dernier s'il est dans la même direction que le vol, ce qui aide à l'économie de carburant. Cependant, en règle générale le vent gêne le mouvement des aéronefs lors de voyages aller-retour. En effet soit v la vitesse du vent et soit a la vitesse relative de l'aéronef par rapport à la masse d'air. En vent arrière, la vitesse de l'aéronef est v + a et en vent de face, la vitesse de l'aéronef est v - a. On note que cette quantité peut être négative si v > a. Dans ce cas, l'aéronef ne peut pas revenir à son point de départ.

La vitesse moyenne au cours de l'aller retour est formula_1. La perte de performance est du second ordre, ce qui signifie que pour des vents faibles, cette perte de performance est négligeable. Toutefois, en cas de vitesses et/ou directions de vent variables en fonction de l'altitude, les avions à moteur peuvent effectuer des économies de carburant en exploitant ces différentiels. En outre, les planeurs peuvent aussi exploiter ces différentiels de vitesse de vent en effectuant un piqué dos au vent et une ressource face au vent à la manière de certains oiseaux à la surface de la mer. Comme la vitesse du vent augmente avec l'altitude, le planeur peut gagner de l'énergie de cette manière. Il a été prouvé qu'un gradient de par mètre est suffisant.

Le système le plus efficace actuellement est celui du cerf-volant (ou du parachute ascensionnel). La force du vent tend à faire monter l'engin si celui-ci est face au vent. Les planeurs peuvent aussi directement utiliser l'énergie éolienne en effectuant un vol de pente. Lorsque le vent rencontre une chaîne de montagnes continue, la masse d'air doit s'élever. Ceci est aussi vrai pour les parapentes et les deltaplanes. En règle générale, le planeur ayant le taux de chute le plus faible sera le plus efficace pour exploiter le vol de pente et des pilotes ont ainsi pu parcourir des distances de plus de . Dans certains cas, le parapente peut être plus efficace car il pourra exploiter des ascendances de petite dimension grâce à sa vitesse réduite. Cependant, le fait que seuls certains lieux géographiques et saisons soient propices à leur utilisation les cantonnent essentiellement à un loisir et pas à un mode de transport.

Les zones de cisaillement des vents causées par des conditions météorologiques diverses peuvent devenir extrêmement dangereuses pour les avions et leurs passagers.

La marine à voile existe depuis les temps les plus anciens, au Néolithique, avant même la naissance de l'écriture, et s'est perfectionnée jusqu'à nos jours où malgré les simulations par ordinateur, les calculs de profils, les nouveaux matériaux et les essais en soufflerie, les découvertes continuent. Aujourd'hui, dans les pays développés, les bateaux à voile sont essentiellement devenus des bateaux de loisirs, mais il reste encore l'un des modes de locomotion le plus utilisé à travers le monde car simple, propre, nécessitant peu d'entretien et surtout qui se passe de carburant. La marine à voile est intimement liée à toute notre histoire que ce soit pour migrer, peupler, commercer, échanger, communiquer, se battre ou conquérir. L'Homme fit le tour de la Terre dans ces bateaux bien avant l'invention du bateau à vapeur ou autres engins modernes.

C'est l'utilisation la plus marginale du vent car assez peu adaptée. Il existe, pour le loisir, des chars à voile essentiellement utilisés dans des grandes plaines mais surtout en bord de mer. Des traîneaux à voile ont parfois été utilisés en zones enneigées et praticables comme les pôles. Les zones terrestres sont souvent très encombrées, pas très planes et avec des vents déformés, la liberté de mouvement réduite et les trajets tortueux rendent donc cet usage compliqué et dangereux. Le traîneau à voile apparaît dans "Le Tour du monde en quatre-vingts jours".

Depuis l'Antiquité, des moulins à vent convertissent le vent en énergie mécanique pour moudre du grain, presser des produits oléifères, battre le métal ou les fibres et pomper de l'eau. Ils seront introduits en Europe par l'Espagne, grâce aux Maures.
Il faudra attendre Zénobe Gramme et sa dynamo en 1869 pour que le moulin puisse donner naissance à l'éolienne. En 1888, Charles F. Brush est le premier à avoir construit une petite éolienne pour alimenter sa maison en électricité, avec un stockage par batterie d'accumulateurs. La première éolienne « industrielle » génératrice d'électricité est développée par le Danois Poul La Cour en 1890, pour fabriquer de l'hydrogène par électrolyse. Les recherches les plus intenses actuellement sur l'utilisation du vent portent sur les éoliennes afin d'augmenter leur rendement en prise sur le vent, résistance aux fluctuations, rendement en production électrique et la meilleure détermination des corridors de vent.

Eole Water est une entreprise française dans le domaine des systèmes de production d’eau par condensation de l’air. Elle a développé des capacités de production d'eau potable à partir de l'énergie éolienne ou solaire.
Le vent interagit avec toute chose, y compris les constructions humaines. Nos villes ont d'ailleurs parfois généré un urbanisme si particulier que certaines grandes places publiques deviennent infréquentables à pied si le vent se lève un peu. En effet, bloquer le vent par des structures urbaines ne fait que le canaliser tout en l'intensifiant. Par contre, un bon arrangement des lieux aère, nettoie, contrôle la température et purifie les lieux.

Les différents types d'effets des vents urbains :


Un bâtiment, suivant son affectation et sa localisation, est conçu pour profiter ou éviter des propriétés particulières du vent. Le vent, par convection, dissipe ou accélère la dissipation de la chaleur par les parois. L'effet produit est un refroidissement des murs et de l'atmosphère, ce qui peut être bénéfique dans les climat chauds, mais préjudiciable dans les climats septentrionaux.
Le vent contribue d'autre part à la ventilation du bâtiment et à l'évacuation de l'humidité ambiante, ou stockée dans les murs. Le tirage thermique des cheminées peut être affecté par le vent.

Dans les régions chaudes, pour refroidir les habitations, on ajoure les murs d'un bâtiment par des fenêtres ornées ou non de grilles ou de "Moucharabieh" (fermeture d'une ouverture conçue pour laisser passer l'air et la lumière mais ne permettre de voir que depuis l'intérieur) mais également grâce à des conditionnements d'air mécaniques comme les tours à vents ou "Badgir" qui permettent de puiser un air d'altitude plus frais mais également moins chargé en sable. Ce système est à ce point efficace qu'il permet même de fournir en permanence un refroidissement des réservoirs d'eau. Un projet actuellement réalisé reprend ce même principe en Égypte, il s'agit du marché de New Baris. Il permet aussi de faire l'inverse, de réchauffer les habitations en hiver en capturant la chaleur de l'air pour le quartier de Bedzed à Beddington au Royaume-Uni.

Les moulins à vents comme les éoliennes quant à eux cherchent les points les plus exposés au vent pour profiter de l'énergie cinétique éolienne.

Le vent est parfois utilisé pour les distractions comme dans les cas des cerf-volants, pour les sports nautiques ou le vol à voile voire dans les vols de montgolfières. Les bulles de savon demandent également un léger vent pour pouvoir être utilisées, tout comme les moulins à vent de plage ou les maquettes de voilier. Le vent sert aussi indirectement en créant des vagues qui seront utilisées pour le surf.

Il existe certains équipements destinés à produire un son par le vent, tels les mobiles-carillons ou la tuile à loups auvergnate qui était orientée de manière à provoquer un ronflement caractéristique lorsque les vents venaient du nord. Les vents du nord provoquent un refroidissement de la région et diminuent le gibier disponible rendant les loups affamés et donc dangereux pour le bétail et même les hommes, c'était donc un signal d'alerte.

Le vent a inspiré dans les civilisations humaines de nombreuses mythologies ayant influencé le sens de l’Histoire. Beaucoup de traditions religieuses personnifient le vent :


La tradition orale canadienne française raconte que « lorsqu'on aperçoit un pied-de-vent, c'est que le bon Dieu descend sur Terre ».

Le vent étant omniprésent, il a suscité beaucoup d'expressions populaires dont quelques-unes sont détaillées ici car ne décrivant pas de vents mais s'inspirant de son comportement. Ces expressions se réfèrent au vent pour sa vitesse, sa force, son homogénéité, son symbolisme ou au fait que c'est juste un mouvement d'air donc sans substance réelle ou à l'inverse soulignent la tendance aléatoire et anarchique du vent.

En voici quelques-unes des principales :

Le vent est présent dans le dessin, la peinture, les infographies mais aussi les sculptures. Il existe des arts spécifiques sur le vent : les mobiles. Il existe essentiellement deux catégories de mobiles : les solides en équilibre et les mobiles suspendus. Dans les suspendus, certains sont faits d'agencements de solides mis en mouvement par le vent comme dans les cultures asiatiques ou bien d'autres sont des suspensions plus éthérées comme les attrapeurs de rêves de la culture amérindienne. Tous ont cependant la même philosophie : accueillir le vent et avoir des effets de mouvement sur les différentes parties de l'assemblage. Certains ont des fonctions symboliques comme les pièges à rêves censés protéger des mauvais esprits, d'autres produisent des sons comme les mobiles suspendus traditionnels chinois que l'on nomme d'ailleurs parfois carillons ou carillons-mobiles qui sont parfois aussi des porte-bonheur.
Le vent est aussi d'une importance primordiale dans certains romans, notamment dans La Horde du Contrevent de Alain Damasio, où le vent, son étude, son utilisation, et la résistance contre lui deviennent l'objet principal de l'intrigue et les personnages nombreux évoluent tous par rapport au vent.

En français un aérophone est aussi dénommé Instrument à vent. Ce qui est également vrai pour l'anglais ("wind instrument") ou l'espagnol ("instrumento de viento") ne l'est pas pour d'autres langues comme l'italien ("strumento a fiato") ou l'allemand ("Blasinstrument") qui basent le nom de l'instrument sur le souffle plutôt que sur le vent. Ce n'est que par une convention de langage que ces instruments sont, en français, associés au vent : le son de ces instruments n'est pas produit librement par le vent mais de manière volontaire par le souffle de l'instrumentiste ou par une soufflerie mécanique. L'émission de ce souffle crée une colonne d'air sous pression produisant des vibrations modulées par le jeu de l'instrument indiqué par la partition du compositeur ou l'invention du musicien improvisateur. Par métonymie, le pupitre qui regroupe les instrumentistes à vent à l'orchestre est appelé le pupitre des « vents », qui réunit les bois et les cuivres. La voix est le plus ancien des instruments à vent. L'éoliphone ou « machine à vent » porte plus exactement son nom puisque l'instrument est employé à l'opéra pour reproduire le son du vent.

Le vent est souvent une source d'inspiration pour les artistes. Anne Sylvestre l'utilise dans ses chansons "La Femme du vent", "Monsieur le vent", son album "Par les chemins du vent" ou sa comédie musicale pour enfants "Lala et le cirque du vent". Bob Dylan fut également inspiré par le vent avec la chanson Blowin' in the Wind ("La réponse est soufflée dans le vent").

Le vent est essentiel à tous les phénomènes météorologiques et donc au cycle de l'eau sans lequel nulle vie à base d'eau comme nous la connaissons sur Terre ne serait possible hors des océans. Le vent est également l'acteur principal de l'oxygénation des océans par agitation de sa surface. La circulation engendrée par les vents permet de disperser de nombreux agents minéraux et organiques. Ainsi, le vent a un rôle important pour aider les plantes et autres organismes immobiles à disperser leurs graines (anémochorie), spores, pollen, etc. Même si le vent n'est pas le vecteur principal de dispersion des graines chez les plantes, il fournit néanmoins ce service pour un très large pourcentage de la biomasse des plantes terrestres existantes. Il façonne également la forme des plantes par thigmomorphogenèse (ou anémomorphose). Le vent influence le déplacement des populations d’insectes volants et la migration des oiseaux.

Les vents sculptent également les terrains via une variété de phénomènes d’érosion éolienne qui permettent par exemple de créer des sols fertiles comme les lœss. Dans les climats arides, la principale source d'érosion est éolienne. Le vent entraîne de petites particules telles la poussière ou le sable fin parfois par-dessus des océans entiers, sur des milliers de kilomètres de leur point d'origine, qui est désigné comme le site de déflation. Par exemple, des vents du Sahara qui provoquent régulièrement des pluies sablonneuses en Europe centrale.

Le vent a aussi des effets sur l’ampleur des feux de forêt, tant par l’alimentation plus ou moins abondante en oxygène des flammes que par le transport d’éléments enflammés ou incandescents permettant ainsi à l’incendie de « sauter » les obstacles.

Quand le vent se combine avec des basses ou des hautes températures, il a une influence sur le bétail et les humains. Le refroidissement éolien peut radicalement modifier les rendements du cheptel ou même tuer par perte de chaleur corporelle. Le vent influe également sur les ressources alimentaires de la faune sauvage mais aussi sur les stratégies de chasse et de défense des animaux voire des chasseurs. Finalement, le vent est également un facteur important de la régulation thermique, hygrométrique ou de niveau de pollution des régions.

L'érosion peut être le résultat du mouvement de déplacement par le vent. Il y a deux effets principaux. D'abord, les petites particules sont soulevées à cause du vent et se retrouvent donc déplacées dans une autre région. Ceci s'appelle la déflation. En second lieu, ces particules suspendues peuvent se frotter sur des objets pleins causant l'érosion par l'abrasion (succession écologique). L'érosion par le vent se produit généralement dans les secteurs avec peu ou pas de végétation, souvent dans les secteurs où il y a des précipitations insuffisantes pour soutenir la végétation. Un exemple est la formation des dunes, sur une plage ou dans un désert.

Le lœss est une roche homogène, en général non-stratifiée, poreuse, friable, sédimentaire (éolien) souvent calcaire, à grain fin, vaseuse, jaune pâle ou de couleur chamois, ébouriffée par le vent. Il se produit généralement comme un dépôt qui recouvre des superficies de centaines de kilomètres carrés et des dizaines de mètres en profondeur. Le lœss se retrouve souvent dans les paysages raides ou verticaux et tend à se développer en sols fertiles. Dans des conditions climatiques appropriées, les secteurs avec le lœss sont parmi les plus productifs au monde sur le plan agricole. Les gisements de lœss sont géologiquement instables et s'éroderont donc très aisément. Par conséquent, des coupe-vent (tels que de grands arbres et buissons) sont souvent plantés par des fermiers pour réduire l'érosion par le vent du lœss.

Les océans sont des zones à surfaces relativement plates mais également majoritairement des zones d'eaux trop profondes pour permettre le développement d'algues à photosynthèse. Les mécanismes qui fonctionnent en eau douce (agitation, chute, algues, etc.) ne suffisent donc pas pour les océans. L'action du vent en créant des vagues mais aussi grâce au ressac sur les côtes crée donc l'oxygénation principale des océans.

La hausse du taux de CO dans l'atmosphère modifie le phénomène en accentuant plus l'acidification que l'oxygénation. Ceci n'est pas irréversible car les milieux océaniques ont toujours joué leur rôle de tampon et transformé le CO en acide carbonique qui acidifie l'eau avant de précipiter avec le temps en carbonate de calcium ou d'être absorbé par les organismes marins. Mais c'est un phénomène lent et, en attendant, le taux d'acide carbonique augmente l'acidité des océans mais diminue également la solubilité de l'oxygène dans cette même eau.

Le vent joue donc globalement le rôle d'un agent mécanique de solubilisation grâce une agitation qui augmente la surface de contact entre l'air et l'eau, par les vagues, peu importe le gaz. C'est moins évident avec l'azote de l'air parce qu'il est beaucoup moins soluble : à , contre à pour l'oxygène et à pour le dioxyde de carbone. La majorité de l'azote injecté dans les océans l'est via la pollution par les fleuves lorsqu'ils se jettent dans la mer et non par le vent.

La dispersion de graines par le vent ou anémochorie ainsi que la dispersion de pollen ou fécondation anémophile est un des moyens les plus primitifs de dispersion du vivant. Cette dispersion peut prendre deux formes principales : un entraînement direct des graines, sporanges, pollens dans un vent (comme le pissenlit) ou bien le transport d'une structure contenant les graines ou les pollens et qui va les disperser au fur et à mesure de leur déplacement par le vent (exemple des virevoltants). Le transport de pollen requiert à la fois des masses très importantes mais aussi des zones à vents complexes. En effet, la circulation d'air doit être très fluctuante afin que ces pollens rencontrent un arbre de la même espèce, surtout si ce ne sont pas des plantes auto-fertilisantes qui comportent des plants mâles et femelles distincts. De plus, il faut une synchronisation entre la production de pollen (mâle) sur des étamines mûres et la disponibilité de pistils (femelles) mûrs au même moment.
Certaines plantes ont développé un système aérien complémentaire permettant une autonomie de transport par le vent plus grande. Ce sont les aigrettes, comme le pissenlit ou le salsifis, et les ailettes greffées à l'akène. Ces dernières semblent une adaptation évolutive de ces plantes au vent afin de maximaliser leur aire de dispersion. Les ailettes se divisent en deux groupes : les samares (par exemple l'orme) et les disamares (par exemple l'érable).

La productivité par dispersion aérienne est une technique très aléatoire qui requiert un nombre énorme de graines car chacune ne peut germer que dans un endroit favorable et si les conditions de milieu le permettent. Par contre, sur certaines îles, des plantes semblent s'adapter et réduire leur aire de dispersion, en effet, les graines qui tombent à l'eau sont perdues.

Le vent a également une influence sur le type de végétation, comme dans les régions à fort vent, où les sols sont soumis à une forte érosion éolienne qui les amincit voire les dénude. Les végétaux développent alors des formes résistantes aux vents. Celles-ci sont mieux enracinées et plus trapues car elles combinent des efforts sur la structure aérienne de la plante et des sols minces donc moins riches. Le vent est également un important agent sélecteur des arbres en éliminant les plus affaiblis ou ceux malades en les brisant ou en les déracinant. On observe de plus que certaines plantes côtières sont comme taillées en arrière, vers les terres, à cause du flux de sel apporté par le vent depuis la mer. Les effets d'un vent salé, en zones montagneuses ou en zones d'érosion forte sur la flore locale est également un facteur. Tous ces effets du vent sur la forme et la croissance des plantes se nomment anémomorphose et sont en grande partie dus à la thigmomorphogenèse.

Le vent est autant utilisé que subi par les espèces animales mais on observe une adaptation au vent chez beaucoup d'espèces.
Les protections de poils ou de laine des bovidés sont par exemple inefficaces si une combinaison de basses températures et d'un vent de plus de survient.

Les manchots, qui sont pourtant bien équipés contre le froid par leurs plumes et leur graisse, sont plus sensibles au niveau de leurs ailes et de leurs pieds. Dans ces deux cas de figure, ils adoptent un comportement de rassemblement en un groupe compact qui alterne sans cesse les positions de ses membres entre une position intérieure ou extérieure permettant ainsi de réduire la perte de chaleur jusqu'à 50 %.

Les insectes volants, un sous-ensemble d'arthropodes, sont balayés par les vents dominants ; cela influe énormément sur leur dispersion et leur migration.

Les oiseaux migrateurs tirent beaucoup plus parti du vent au lieu de le subir. Ils s'en servent afin de planer au maximum après avoir utilisé des courants thermiques ascendants pour prendre le plus d'altitude possible. La sterne arctique est un des plus grands champions de la discipline en réussissant des vols transatlantiques, voire plus, de cette manière. Le champion de l'océan Pacifique est le puffin fuligineux et l'un des vols les plus impressionnants sur des vents d'altitude est le grand albatros. Les records d'altitudes sont tenus par les oies à et les vautours jusqu'à . On remarque également que les axes de migration utilisent les vents dominants saisonniers.

Certains animaux se sont adaptés au vent tel le pika qui crée un mur de cailloux pour stocker des plantes et herbes sèches à l'abri. Les cancrelats savent tirer parti des vents légers pour échapper à leurs prédateurs. Les animaux herbivores se positionnent en fonction du vent et de la topographie afin de bénéficier du transport des odeurs, comme des bruits, par le vent et ainsi percevoir l'approche d'un prédateur qui s'est lui-même adapté en approchant autant que possible sous le vent donc avec un vent soufflant de sa proie vers lui.

Des rapaces et autres oiseaux prédateurs utilisent les vents pour planer sans effort jusqu'à repérer une proie tels les goélands bourgmestre qui attendent que les vents dépassent les pour accentuer leurs attaques sur les colonies de guillemots.

Le bruit du vent est appelé sifflement. Le sifflement du vent est réputé aigu, lugubre, oppressant. Le vent est un mouvement de l'air et ne produit pas de sons au sein d'un système homogène à même vitesse mais par frottement sur des systèmes d'air de vitesses différentes ou à la suite du frottement sur des solides ou des liquides

Parfois aussi le son du vent est modulé par la forme des solides qu'il traverse et selon sa direction comme dans les gorges ou les grottes. Même au sein des habitations, le vent peut générer des bruits. Les instruments à vent sont exactement basés sur ce même principe naturel mais en modulant la pression, l'ampleur et la vitesse, le tout combiné parfois à des volumes de résonance. Cet effet sonore du vent est d'ailleurs une grande source de nuisance lorsque l'on fait des enregistrements en extérieur et les micros doivent être enveloppés d'une couche protectrice poreuse afin que le vent ne rende pas tous les sons alentour inaudibles en traversant la structure interne du récepteur du microphone.

Lorsque le vent est sauvage, on parle souvent des hurlements ou des rugissements du vent pendant les tempêtes, tornades, à travers des arbres dénudés de leur feuillage ou avec des violentes rafales. Les sons sont plus apaisants à l'oreille humaine lorsque des brises roulent du sable sur une grève, font bruisser les feuilles des arbres ou frisent la surface de l'eau de vaguelettes. Lorsque le vent est très aigu, on dit qu'il fait des miaulements.

Le vent porte également les bruits en déformant l'onde circulaire naturelle de tout bruit. En plus de son bruit propre, il change également la répartition de tous les bruits environnementaux. On étudie désormais sérieusement les effets des vents dominants sur le transport du bruit des avions, des autoroutes ou des industries car le vent peut autant augmenter la distance de perception de bruits qu'aider à les étouffer plus vite, selon sa direction.

Le vent n'est pas que pacifique, il est essentiel à l'écosystème mais parfois le système s'emballe et le vent devient alors une force destructrice que l'on ne peut maîtriser.

Le vent peut se déchaîner dans une tempête, comme un cyclone tropical, et détruire des régions entières. Les vents de force d'ouragan peuvent endommager ou détruire des véhicules, des bâtiments, des ponts, etc. Les vents forts peuvent aussi transformer des débris en projectiles, ce qui rend l'environnement extérieur encore plus dangereux. Les vents peuvent également venir s'ajouter à d'autres phénomènes comme des vagues, se combiner aux éruptions volcaniques, aux feux de forêts… comme détaillé ci-dessous.

Le vent peut accentuer des grandes marées comme lors de la tempête Xynthia en France en 2010 où sa direction est venue s'additionner au sens de montée de la mer. En se déplaçant, l'air agit par friction sur la surface de la mer. Cet effet crée une accumulation d'eau dans les régions sous le vent, similaire à celui qui crée un effet de seiche, qui est inversement proportionnel à la profondeur et proportionnel à la distance sur laquelle le vent s'exerce. Ceci s'ajoute à l'augmentation du niveau de la mer créé par la pression plus faible au centre du système météorologique et à d'autres facteurs. On appelle ce phénomène une "onde de tempête".

Le "coup de mer" est une perturbation de la mer, souvent brève, localisée, due aux vents et pouvant être très violente alors qu'il n'y a pas de tempête au lieu où l'effet est noté. Il s'agit de la combinaison d'une dépression et de vents violents directionnels près d'une dépression qui se situe loin au large et provoque un effet de succion sur la surface de la mer. Cette colline liquide va donc augmenter jusqu'à l'équilibre puis s'effondrer lors du déplacement de la dépression. Si le mouvement du système est rapide, la chute est brutale ; elle va créer des fronts de vagues plus ou moins importants qui seront entretenus en partie par des vents de surfaces s'ils existent. Si ceux-ci sont violents, ils peuvent même l'alimenter. Si ces vagues ont une ampleur telle qu'elles commettent des dégâts sur les côtes ou causent des naufrages, on les appellera « vagues-submersion ». Comme ce phénomène a lieu au large, si la dépression ne se dissipe pas d'elle-même l'ampleur des vagues explosera en se rapprochant de la côte parce que le volume d'eau déplacé par la dépression restera le même alors que la profondeur diminue jusqu'à devenir nulle.

Les différences entre un "coup de mer" et un tsunami sont l'origine éolienne au lieu de géologique, l'aspect limité de son action géographique et temporelle, mais aussi que les vagues sont formées dès le large et non par la collision des fronts d'onde sur le plateau continental qui ici ne fera qu'amplifier des vagues déjà existantes. Ce phénomène est par exemple observable deux à quatre fois par an sur la Côte d'Azur ou en Corse comme à Cannes en 2010 où ce phénomène habituellement limité à des vagues de 4 à culmina avec des lames de 6 à emportant tout sur leur passage.

Sur les structures des ponts suspendus, il a déjà provoqué des phénomènes de mise en résonance allant jusqu'à la destruction de l'ouvrage comme pour le pont du détroit de Tacoma en 1940, le pont de la Basse-Chaîne (Angers) en 1850 ou le pont de La Roche-Bernard en 1852. Dans ces cas, il a un échange d’énergie mécanique qui se produit entre le vent et le pont qui oscille. En condition normale, l’énergie mécanique engendrée par une petite oscillation initiale extérieure est transférée du pont vers le vent qui la dissipe. Mais si la vitesse moyenne du vent est suffisamment élevée, au-dessus de ce que l’on appelle la « vitesse critique », le pont est instable et l’oscillation initiale s’amplifie. L'énergie se transfère alors du vent vers le pont et les oscillations s’amplifient à cause du couplage aéroélastique jusqu'à entraîner parfois la rupture des structures du pont.

Lorsqu'il érode des sols, il peut aller jusqu'à la roche et/ou désertifier complètement une région comme pour la "mer de sable" du Hoge Veluwe aux Pays-Bas, phénomène se nomme également "déflation". Le vent peut également provoquer des tempêtes de sables comme par le chammal ou de poudrerie ("chasse-neige") comme le blizzard. En outre, si l'érosion éolienne, pluviale, maritime et fluviale n'était pas contrebalancée par les mouvements magmatiques divers, la Terre serait recouverte d'eau depuis longtemps car cette érosion aurait effrité tous les solides dépassant une couche de boue sous-marine. Le vent érode et transporte les roches qui finiront par s'accumuler dans la mer jusqu'à une modification de relief terrestre à la suite de mouvements tectoniques qui pousseront ces sédiments comprimés par la pression de l'eau vers le haut. C'est donc un des mécanismes de création des roches sédimentaires qui seront alors à nouveau érodées par le vent dès qu'elles seront découvertes à l'air libre.

Les orages sont souvent accompagnés de rafales violentes ou de tornades qui produisent des dégâts importants le long d'un corridor au sol. Ils sont également accompagnés de turbulence, par cisaillement des vents dans le nuage, qui peut endommager des avions ou même les faire écraser si elle se produit relativement près du sol.

La production de foudre est causée par la différence de charges électriques entre la base et le sommet du nuage orageux, entre le nuage et le sol ou entre deux nuages. Ces charges sont produites par collisions des gouttelettes et cristaux de glaces dans le courant ascendant, ou vent vertical, dans le nuage.

Dans des cas de pollution, il permet d'épurer les régions touchées mais va répandre celle-ci sur d'autres régions jusqu'à dilution des polluants ou précipitation par la pluie comme dans le cas du nuage de Tchernobyl ou dans les cas de pluie acide. Plus récemment, l'éruption de l'Eyjafjöll a paralysé les trois quarts du trafic aérien européen.

Bien des maladies sont transportées par les vents, peu importe qu'elles soient virales, bactériennes ou fongiques. Souvent, le vent ne va permettre que des petits sauts de quelques centimètres à plusieurs mètres. Mais, les grands vents ou des cyclones peuvent transporter des infections sur des centaines de kilomètres. Quelques infections courantes utilisant le vent : la rouille noire, la rouille du maïs, le mildiou, les fusarium… Il importe d'ailleurs peu que le vent charrie directement l'infection (certains organismes peuvent aussi s'encapsuler durant le transport pour mieux résister) ou transporte des matériaux contaminés.

Les insectes volants bénéficient souvent ou se sont adaptés à des régimes de vent particuliers. Ce qui permet à ces bêtes assez petites de franchir des très longues distances que leurs seules forces ne leur permettraient pas.
Les ravageurs les plus courants sont actuellement les cicadelles, sauterelles, fourmis, les abeilles tueuses ou le criquet pèlerin.

Le vent agit également dans les cadres des incendies de forêt auxquels il fournit une force de déplacement d'une part mais également une alimentation en oxygène qui entretient voire attise les flammes comme l'Homme s'en est inspiré pour créer les soufflets. Le vent permet également ce que l'on appelle les sauts de feu, que ce soit sous forme de touffes enflammées ou simplement de braises qui permettent de franchir des obstacles tels les rivières, failles ou les coupe-feu.

Le vent solaire est assez différent du vent terrestre car il se compose de particules polarisées qui sont éjectées de l'atmosphère du Soleil. Par contre, le vent planétaire est lui semblable au vent solaire et est composé de gaz légers qui s'échappent de l'atmosphère de leur planète. Sur de longues périodes de temps, ce vent planétaire peut radicalement changer la composition de l'atmosphère d'une planète.

Des vents hydrodynamiques dans les couches supérieures de l'atmosphère permettent à des éléments chimiques légers comme l'hydrogène de se déplacer vers l’"exobase", partie inférieure de l'exosphère où ces gaz peuvent acquérir la vitesse de libération et donc s'échapper dans l'espace interplanétaire sans que d'autres particules ne contrarient leur mouvement ; c'est un peu une forme d'érosion gazeuse.
Ce type de processus sur des temps extrêmement longs, de l'ordre de milliards d'années, peut faire que des planètes riches comme la Terre évoluent en des planètes comme Vénus.
Des planètes avec une atmosphère basse très chaude peuvent générer une atmosphère haute très humide et donc accélérer le processus de perte de l'hydrogène.
L'énergie nécessaire à l'arrachage de ces éléments légers étant fournie par le vent solaire.

À la différence de l'air, le vent solaire est à l'origine un flux de particules polarisées comparable à un courant électrique ou à un plasma éjecté par la couronne solaire dont la chaleur permet des vitesses de fuite de plus de (). Il est majoritairement constitué d'électrons et de protons avec une énergie de l'ordre de 1 keV. Ce flux de particules varie en température et en vitesse au fur et à mesure du temps. Il existerait également des mécanismes internes au Soleil permettant de transmettre à ces particules une haute énergie cinétique mais leur fonctionnement reste encore actuellement un mystère. Le vent solaire crée l'héliosphère, vaste bulle qui contient tout le système solaire et s'étend jusque dans l'espace interstellaire.

C'est aussi ce qui explique que seules des planètes disposant d'un très puissant champ magnétique peuvent supporter sans dommage ce vent solaire continuel, réduisant ainsi l'ionisation de la haute atmosphère. Divers phénomènes observables sont dérivés du vent solaire telles les tempêtes électromagnétiques qui peuvent affecter les équipements électriques, les aurores boréales ou encore le fait que les comètes qui traversent le système solaire ont toujours leur queue dirigée à l'opposé du Soleil.

Cependant, au fur et à mesure que ce vent solaire croise des planètes, il est alimenté par le vent planétaire et prend alors des caractéristiques plus proche des vents terrestres dans certains de ses effets, des systèmes solaires très denses pourraient ainsi en arriver à avoir une atmosphère ténue.

Certains tests sont actuellement effectués sur les voiles solaires et il avait même été imaginé une course de voiles solaires. Le principe est semblable à celui des voiliers, à ceci près qu'il s'appuie sur la lumière (les photons) émis par le Soleil. Compte tenu de la faible propulsion générée, le procédé ne permet pas de quitter la surface d'une planète (même dénuée d'atmosphère, et donc de friction). Il est en revanche utilisable sur un appareil ayant déjà atteint la vitesse de satellisation minimale, voire la vitesse de libération. La difficulté de mise en œuvre réside dans la faiblesse de la poussée : une voile de est nécessaire pour obtenir une poussée de .

Cependant, cet effet est déjà utilisé sur les sondes spatiales afin de rectifier une trajectoire ou de fournir une poussée supplémentaire comme pour la sonde Mariner 10.




</doc>
<doc id="6708" url="https://fr.wikipedia.org/wiki?curid=6708" title="Kiwi (homonymie)">
Kiwi (homonymie)

Kiwi est un terme maori qui peut désigner un fruit ou un oiseau, symbole de la Nouvelle-Zélande






</doc>
<doc id="6709" url="https://fr.wikipedia.org/wiki?curid=6709" title="Kiwi austral">
Kiwi austral

Le Kiwi austral ("Apteryx australis") est une espèce d'oiseau qui habite la Nouvelle-Zélande. Son nom est un emprunt au terme māori "kivi-kivi" qui désigne cet oiseau.

Le kiwi, de la taille d'une poule (environ pour 2 à ), est doté d'ailes, réduites à des moignons (il est donc incapable de voler), d'un long bec, d'un plumage brunâtre et est dépourvu de queue. Les kiwis ont une vue médiocre, mais un odorat développé. Leurs narines sont situées à l'extrémité de leur long bec. Le kiwi reste caché le jour et sort la nuit chercher sa nourriture, des larves d'insectes qu'il trouve dans le sol grâce à son odorat. C'est un animal qui vit généralement en milieu forestier.

Les kiwis vivent en couple, et ce pendant une trentaine d'années. La femelle est plus grosse que le mâle, et pond des œufs qui représentent environ 20 % de son poids. Proportionnellement, c'est le plus gros œuf du monde. C'est le mâle qui couve les œufs. L'incubation dure de 70 à 80 jours.

Selon et d'après Alan P. Peterson, cette espèce est constituée des deux sous-espèces suivantes :

Le Kiwi brun de l'île du nord ("Apteryx mantelli") est maintenant considéré comme une espèce séparée du Kiwi austral.




</doc>
<doc id="6714" url="https://fr.wikipedia.org/wiki?curid=6714" title="Géographie de la France">
Géographie de la France

D'une superficie de ( avec l'outre-mer), la géographie de la France s'étend sur du nord au sud et d'est en ouest (plus grande distance nord-sud : Bray-Dunes - Cerbère). La France est le troisième plus grand pays d'Europe, après la Russie et l'Ukraine ( en comptant l'outre-mer). La France métropolitaine a quatre façades maritimes sur (du nord au sud) : la mer du Nord, la Manche, l'océan Atlantique et la mer Méditerranée. La longueur totale de ses côtes atteint km.

À l'exception de sa frontière nord-est, le pays est délimité principalement par des mers, l’océan et des frontières naturelles : le Rhin, le Jura, les Alpes, et les Pyrénées.

 au total, dont 2 913 km en France métropolitaine et 1 623 km en outremer, répartis ainsi :
À ces frontières peut s'ajouter environ avec l'Australie, si la Terre-Adélie (en Antarctique) est prise en compte.


Le relief de la France métropolitaine est caractérisé par le « S français » qui part du sud des Vosges, descend la vallée du Rhône et s'infléchit vers l'ouest pour longer le sud du Massif central et le nord des Pyrénées.

Au nord-ouest de cette ligne se trouve la zone hercynienne datant de l'ère primaire et secondaire, au sud-est se trouve la zone alpine datant de l'ère tertiaire et quaternaire. Cette ligne est également une frontière altimétrique : la zone hercynienne a des pentes arrondies tandis que la zone alpine est plus escarpée ; et une ligne de partage des eaux : à l'ouest, les cours se jettent dans l'Atlantique, à l'est, dans la Méditerranée. L'ouest subit une influence océanique tandis que l'est subit une influence méditerranéenne s'atténuant en arrivant au Jura.

Au nombre de 27 jusqu'en 2015, les régions françaises sont 18 depuis le : de France métropolitaine, auxquelles s'ajoutent la Corse, qui n'a pas la dénomination de région mais en exerce les compétences, et cinq départements et régions d’outre-mer (dont le département de Mayotte qui exerce également les compétences d'une région).
"Voir aussi l'article suivant :

Île de La Réunion, Guadeloupe, Martinique, Mayotte, Îles Kerguelen, Corse, Île de Bréhat, Île de Batz, Île d'Ouessant, Île de Molène, Île de Sein, Île de Groix, Belle-Île, Île de Houat, Île de Hoëdic, Île d'Arz, Île aux Moines, Île de Noirmoutier, Île d'Yeu, Île de Ré, Île d'Aix, Île d'Oléron, Nouvelle-Calédonie

De nombreux cours d'eau coulent en France :

Les canaux les plus remarquables sont :

Du point de vue hydrologique, la France occupe une position assez forte en Europe. Les précipitations y sont en effet assez élevées, et alimentent de puissants cours d'eau coulant soit vers les mers, soit vers les pays voisins (nord et nord-est). L'eau venue de France procure une part des disponibilités en eau de la Belgique, du Luxembourg, de l'Allemagne et indirectement des Pays-Bas.
D'après Aquastat, la hauteur d'eau annuelle moyenne des précipitations est de , soit pour une superficie de kilomètres carrés, un volume de précipitations annuelles de cubes, arrondis à (478 milliards de mètres cubes).

Pour les années les plus sèches (période de retour de 10 ans), le volume annuel des précipitations est de cubes.

De ce volume précipité, l'évapo-transpiration consomme . Restent cubes d'eau de surface (cours d'eau) produits sur le territoire du pays (on parle d"eau de surface produite en interne"). La lame d'eau moyenne écoulée sur l'ensemble du territoire tous bassins confondus est donc de annuellement.

Il faut ajouter à cela d'eau souterraine produits en interne, ce qui fait un total de cubes d'eau produits en interne.

En outre, une quantité non négligeable d'eau provient de certains pays voisins. Ce sont cubes supplémentaires qui viennent ainsi de l'étranger, dont de Suisse (cours supérieur du Rhône, Doubs) et d'Espagne (cours supérieur de la Garonne). Les apports de la Belgique et de l'Allemagne sont considérés comme négligeables, car ne faisant qu'une brève incursion en France (Blies en Moselle, Semoy en Champagne-Ardenne). Enfin l'apport du Rhin de constitue un cas particulier. Ce fleuve est frontalier sur une longue distance, mais ne pénètre jamais le territoire français. On considère dès lors que la moitié de son débit à l'entrée (Bâle) fait partie des ressources en eau de la France produites à l'étranger, soit cubes.

Les ressources renouvelables totales en eau du pays se montent donc à cubes ( = 1 milliard de m) dont :

Le taux de dépendance vis-à-vis de l'étranger est de 12,37 %.

En moyenne chaque année d'eau quittent annuellement le territoire, à destination des pays voisins :

La quantité d'eau disponible (qui comprend l'ensemble des ressources créées en interne, plus les apports extérieurs) est de par an, soit pour une population évaluée à 61,9 millions d'habitants (fin 2007), par habitant et par an.

Il y a évidemment une grande incertitude concernant le centre géographique.
Sa détermination dépend en grande partie de la méthode utilisée pour le calcul, et notamment selon que l'on tient compte ou non de la Corse et des DOM-TOM.

Quelques candidats :

La France reste aussi présente dans d'autres continents par des dépendances aux statuts administratifs divers :



</doc>

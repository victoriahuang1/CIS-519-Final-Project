<doc id="14077" url="https://fr.wikipedia.org/wiki?curid=14077" title="Nombre d'or">
Nombre d'or

Le nombre d'or (ou section dorée, proportion dorée, ou encore divine proportion) est une proportion, définie initialement en géométrie comme l'unique rapport entre deux longueurs et telles que le rapport de la somme des deux longueurs sur la plus grande () soit égal à celui de la plus grande () sur la plus petite () c'est-à-dire lorsque :
formula_1
Le découpage d'un segment en deux longueurs vérifiant cette propriété est appelé par Euclide découpage en « extrême et moyenne raison ». Le nombre d'or est maintenant souvent désigné par la lettre φ (phi).

Ce nombre irrationnel est l'unique solution positive de l'équation "x" = "x" + 1. Il vaut :

formula_2.

Il intervient dans la construction du pentagone régulier. Ses propriétés algébriques le lient à la suite de Fibonacci et au corps quadratique ℚ(). Le nombre d'or s'observe aussi dans la nature (quelques phyllotaxies, par exemple chez les capitules du tournesol, pavage de Penrose de quasi-cristaux) ou dans quelques œuvres et monuments (architecture de Le Corbusier, musique de Xenakis, peinture de Dalí).

L'histoire de cette proportion commence à une période de l'Antiquité qui n'est pas connue avec certitude ; la première mention connue de la division en extrême et moyenne raison apparaît dans les "Éléments" d'Euclide. À la Renaissance, Luca Pacioli, un moine franciscain italien, la met à l'honneur dans un manuel de mathématiques et la surnomme « divine proportion » en l'associant à un idéal envoyé du ciel. Cette vision se développe et s'enrichit d'une dimension esthétique, principalement au cours des où naissent les termes de « section dorée » et de « nombre d'or ».

Il est érigé en théorie esthétique et justifié par des arguments d'ordre mystique, comme une clé importante, voire explicative, dans la compréhension des structures du monde physique, particulièrement pour les critères de beauté et surtout d'harmonie ; sa présence est alors revendiquée dans les sciences de la nature et de la vie, proportions du corps humain ou dans les arts comme la peinture, l'architecture ou la musique. Certains artistes, tels le compositeur Xenakis ou le poète Paul Valéry ont adhéré à une partie de cette vision, soutenue par des livres populaires. À travers la médecine, l'archéologie ou les sciences de la nature et de la vie, la science infirme les théories de cette nature car elles sont fondées sur des et des hypothèses inexactes.

Le nombre d'or possède une première définition d'origine géométrique, fondée sur la notion de proportion :
Il existe une interprétation graphique de cette définition, conséquence des propriétés des triangles semblables illustrée par la figure 1. Les segments bleus sont de longueur "a" et le rouge de longueur "b". Dire que la proportion définie par "a" et "b" est d'or revient à dire que les triangles "OAB" et "OCA" sont semblables. Euclide exprime la « proportion d'or », qu'il appelle « extrême et moyenne raison », de la manière suivante : .

Le rapport "a"/"b" ne dépend pas des deux valeurs "a" et "b", dès lors que ces deux nombres sont en proportion d'extrême et de moyenne raison. Cela donne une nouvelle définition du nombre d'or :

Sa valeur approximative est donc .

La proportion (1), définissant la proportion d'or, peut être écrite de la manière suivante, obtenue en multipliant l'égalité par "a"/"b" :
formula_3
ce qui revient à dire que φ est solution d'une équation du second degré. Cette propriété donne lieu à une troisième définition :

Cette équation est équivalente à celle indiquant que l'inverse de l'inconnue "x" est égal à "x" – 1 (ce qui implique que 1/φ est égal à la partie fractionnaire de φ). Plus généralement, toutes les puissances de φ, d'exposant "n" entier positif ou négatif, peuvent s'écrire sous la forme φ = "a" + "b"φ, où "a" et "b" sont des entiers relatifs.

Il existe deux modes de définition du nombre d'or, celle géométrique qui s'exprime en termes de proportion et celle algébrique qui définit le nombre comme l'unique racine positive d'une équation. Cette double approche permet de résoudre un problème d'algèbre, en l'occurrence une équation du second degré, à l'aide de méthode géométrique : on parle d'algèbre géométrique.

L’objectif est de construire la figure 1. Dans un premier temps, on considère deux points "O" et "A" du plan euclidien situés à une distance "a" l'un de l'autre. Soit "I" un point tel que les droites "AI" et "OA" soient perpendiculaires et tel que la distance "AI" soit égale à "a"/2. Soit γ le cercle de centre "I" et passant par "A". Enfin, les deux points "B" et "C" sont les intersections de la droite "OI" et du cercle γ, dans l'ordre indiqué sur la figure. On définit "b" comme la distance séparant "O" de "B". Par construction, la distance séparant "B" de "C" est égale à "a".

Une fois la figure construite, il reste à montrer que les triangles "OAB" et "OCA" sont semblables. Pour cela, il suffit de montrer qu'ils possèdent deux angles en commun. L'angle "AOB" est partagé par les deux triangles, il suffit donc de montrer que l'angle "BAO" est égal à "OCA". Comme la droite "OA" est tangente au cercle, ce résultat est une conséquence du théorème de l'angle inscrit. Les triangles sont bien semblables.

Deux triangles semblables sont proportionnels, ce qui montre que la base du grand triangle "OC" est à "OA" la base du petit triangle, ce que "OA" un côté du grand triangle est à "OB" le côté équivalent du petit triangle. On obtient la formule (1).

Soit "a" une longueur strictement positive, et "c" un nombre réel plus petit que "a" tel que la proportion "a"/"c" soit d'extrême et de moyenne raison. Soit "OBC" trois points alignés tel que la distance "OB" soit égale à "c" et "BC" à "a". Soit γ le cercle de diamètre "BC" et "A" le point de γ tel que la droite "OA" soit tangente au cercle.

Les arguments de la démonstration précédente montrent que les triangles "OAB" et "OCA" sont semblables et que la figure obtenue est celle du paragraphe précédent. En conclusion la valeur "c" est égale à "b", calculé au paragraphe précédent. Ceci montre l'unicité de "b".


Pour calculer la valeur de φ, on peut utiliser le fait que si "a" et "b" sont en extrême et moyenne proportion, alors ("a" + "b") / "a" est égal à φ. La longueur "a" peut être choisie quelconque, une méthode simple consiste à la choisir égale à 1. La valeur φ est alors égale à "a" + "b" ou encore à 1 + "b". La longueur de "OC" est égale à la somme de la longueur de "OB" et de celle de "BC", et donc à "b" + 1, le nombre d'or. Ici le nombre 1 représente le diamètre du cercle "C" de rayon 1/2, par construction.

La longueur de "OC" est égale à φ et aussi à la somme de la longueur de "OI" et de "IC". Le théorème de Pythagore montre que la distance entre "O" et "I" est égale à /2, la longueur de la diagonale d'un rectangle de côté de longueurs 1 et 1/2. Celle de "I" à "C" est égale au rayon du cercle 1/2. La longueur "OC" est à la fois égale au nombre d'or φ et à (1+)/2, ce qui montre le résultat recherché.

Une autre solution pour le calcul de φ consiste à faire usage de la troisième définition. La valeur φ est donnée par la solution positive de l'équation du second degré :
formula_4
équation dont on montre facilement qu'elle est équivalente à formula_5

Le discriminant de l'équation du second degré est égal à 1 + 4 = 5, il existe deux solutions, une seule est positive, on en déduit : 
formula_6
Un calcul ne faisant pas appel au discriminant est proposé en introduction dans l'article équation du second degré.

Les calculs précédents permettent, à l'aide d'une règle et d'un compas de dessiner une proportion d'extrême et de moyenne raison. La méthode est illustrée sur la figure de gauche. On dessine un cercle de centre "C" et de rayon "1" (en orange). Puis, de l'extrémité du rayon, on élève un segment (en vert) perpendiculaire au rayon, de longueur 1/2, et on trace le cercle de centre "C′" et de rayon 1/2. Le segment bleu qui a pour extrémités "C" et le point du cercle "C' "dans le prolongement de "C" "C′" est de longueur φ. Cette méthode permet aussi de construire un « rectangle d'or », c'est-à-dire un rectangle de longueur "a" et de largeur "b" tel que "a" et "b" soient en proportion d'extrême et de moyenne raison. En d'autres termes, un rectangle est dit d'or si le rapport entre la longueur et la largeur est égal au nombre d'or.

Pour tracer un rectangle d'or de longueur "a" et de largeur "b", le plus simple est de dessiner un carré de côté "b". En prenant le milieu de la base comme centre, on trace un cercle passant par les deux sommets opposés. L'intersection de la droite prolongeant la base du carré et du cercle détermine l'extrémité de la base du rectangle d'or. Il apparaît comme construit par l'adjonction à un carré de côté de longueur "b", d'un rectangle de côtés de longueur "b" et "a" − "b", comme le montre la figure de droite. Un rapide calcul montre que ce rectangle est encore d'or :

formula_7

En disposant côte à côte deux rectangles identiques, l'un en format paysage et l'autre en format portrait, on dessine les contours d'un nouveau rectangle. Le rectangle de départ est d'or si et seulement si sa diagonale est confondue avec la diagonale du grand rectangle.

Pour se faire une idée de ce qu'est un rectangle d'or, on peut regarder une carte de paiement de format ISO 7810 (à condition de réduire son petit côté d'au moins un millimètre, le rapport entre longueur et largeur est inférieur d'environ 2 % au nombre d'or), ou bien, parmi les nombreux formats de livre de poche, un livre de format 11 × 18 cm (à condition de réduire son grand côté d'au moins deux millimètres, le rapport est cette fois supérieur d'un peu plus de 1 %). Une feuille de papier au format A4 est trop large pour représenter un rectangle d'or, il faudrait enlever à son petit côté plus de deux centimètres et demi pour l'en rapprocher.

En intégrant un carré de côté "b" dans un rectangle d'or de côtés "a" × "b", il reste un rectangle qui est encore d'or. Il est possible de réitérer le processus et d'intégrer un carré de côté "a" − "b" dans le rectangle d'or de côtés "b" × ("a" − "b"), comme indiqué sur la figure de gauche. Cette méthode peut être prolongée indéfiniment. Si, dans chaque carré est dessiné un quart de cercle d'extrémités deux côtés du carré, comme sur la figure, on obtient une spirale. Ce graphique est une bonne approximation d'une spirale d'or, d'équation polaire :
formula_8

Cette spirale est un cas particulier de spirale logarithmique. Comme toute spirale de cette famille, elle possède une propriété caractéristique, si "A" est un point de la spirale, alors la droite passant par le centre de la spirale et "A" fait un angle constant avec la tangente à la spirale en "A". Une telle spirale est dite équiangle.

D'autres figures se dessinent à l'aide du nombre d'or à l'instar de l'« œuf d'or ».

Un pentagone régulier se construit à l'aide de la proportion d'extrême et moyenne raison. Soit un cercle de diamètre "OP" et de rayon "a", illustré sur la figure de gauche. Si "b" est le nombre réel plus petit que "a" tel que "a" et "b" soient en proportion d'or, et "P", "P", "P" et "P" les intersections du cercle de diamètre "OP" avec les deux cercles de centre "O" et de rayon "a" + "b" et "b", alors les cinq points "P" définissent un pentagone.
Le pentagramme associé, c'est-à-dire la figure composée des cinq diagonales du pentagone (Cf. figure de droite), contient aussi de multiples proportions d'extrêmes et moyennes raisons. Elles s'expriment simplement à l'aide de triangles isocèles dont les longueurs des côtés sont en proportion d'or. De tels triangles sont appelés triangles d'or. Il en existe de deux types différents, les jaunes ayant une base proportionnelle à "a" et deux côtés à "b" et les orange ayant une base proportionnelle à "b" et deux côtés à "a". Les triangles foncés sont semblables aux plus clairs de même couleur, la proportion entre clair et foncé est encore d'or.

Les triangles jaunes possèdent deux angles de 36°, soit le cinquième d'un angle plat et un de 108°, soit les trois cinquièmes d'un angle plat. Un tel triangle est parfois appelé « triangle d'argent ». Les triangles orange possèdent deux angles de 72°, soit les deux cinquièmes d'un angle plat et un angle de 36°. Avec des triangles d'or et d'argent dont les côtés sont toujours "a" et "b", il est possible de paver intégralement un plan euclidien de manière non périodique. Un tel pavage est dit de Penrose.
La trigonométrie permet de montrer les différentes propriétés exposées dans ce paragraphe, il est aussi possible d'établir ces résultats à l'aide de la géométrie.

Le premier lemme est la clé des différentes preuves. Soient "a" et "b" avec "a" > "b", deux longueurs en proportion d’extrême et de moyenne raison. Soit " ABD" un triangle d'or tels que "A" et "B" soient situés à une distance "a" l’un de l’autre et "B" et "D" à une distance "b".
Cette proposition correspond à la figure de droite. Par construction, les distances "AB" et "AD" sont égales à "a". Considérons le point "E" du segment "AB" situé à "b" de "A" et montrons que le triangle "AEC" (en vert) est égal à "BCD" (en jaune). Il suffit de montrer qu’ils disposent d’un angle et de deux côtés égaux. Les deux triangles "AEC" et "ABD" sont semblables (car tous deux isocèles de même sommet) et dans un rapport de "a"/"b". Comme la distance entre "B" et "D" est égale à "b", celle entre "C" et "E" est égale à "a" - "b" (car "b"/("a" - "b") = "a"/"b"). Or cette distance est la même que celle qui sépare "C" et "D". Le caractère semblable des triangles "ACE" et "ADB" montre que l’angle "ACE" est égal à "ADB". Enfin, la distance "DB" est égale à celle de "AC". Les deux triangles disposent bien de deux côtés et d’un angle égaux, ils sont identiques. Le triangle "ACE" étant semblable au triangle d'or "ADB", c'est un triangle d'or ainsi que le triangle "BDC". Il est en proportion "a"/"b" avec le triangle initial.

Il reste à prouver que le triangle "ACB" est bien d'argent. Il suffit de prouver que la distance de "B" à "C" est égale à "b". Or le triangle "BDC" étant un triangle d'or, on sait que la distance "BC" est égale à celle de "BD" et donc à "b", ce qui termine la démonstration.

Le lemme précédent nous affirme que le triangle "ABC" est isocèle de sommet C. Donc l'angle "DCB" est égal au double de l'angle "CAB" soit avec les notations de droite : μ = 2θ.
D'autre part, le triangle "BCD" étant aussi un triangle d'or, il est isocèle de sommet "B". Ses angles sont θ, 2θ, 2θ. La somme des angles valant 180°, on a 5θ = 180°, soit θ = 36°
Il vient alors immédiatement que μ = 2θ = 72° puis que η = 180° – μ = 108°.

On remarque que θ est égal à un cinquième de demi-tour, μ à deux cinquièmes et η à trois cinquièmes.


La méthode utilisée ici consiste à montrer que si deux sommets sont consécutifs, alors leur angle avec le centre du cercle est de 72°.

Cette première étape est la conséquence du fait que les points "P" et "P" sont définis comme l'intersection du cercle de centre "O" et de rayon "b" avec le cercle de centre "A" et de rayon "a". les triangles "P""AO" et "OAP" sont d'or, les angles "P""AO" et "OAP" font chacun 36°, ce qui permet de conclure.

La distance entre "O" et "P" est égale à "a" + "b", celle entre "O" et "A" ainsi que celle entre "A" et "P" est égal à "a". On en déduit que le triangle "OAP" est un triangle d'argent. L'angle "OAP" fait donc 108°. Comme l'angle "P""AO" fait 36°, par différence, on obtient que l'angle "P""AP" est de 108° – 36°, soit 72°.

L'angle "OAP" fait 108° et l'angle "OAP" est plat donc l'angle "P""AP" est égal à 180° – 108°, soit 72°.

Il reste encore à mesurer les angles "P""AP" et "P""AP". Pour cela, il suffit de remarquer que la droite "OA" est un axe de symétrie du pentagone, en conséquence l'angle "P""AP" est égal "P""AP" et "P""AP" est égal à "P""AP", ce qui termine la démonstration.
L'analyse des mesures des triangles d'argent et d'or permettent de déterminer les valeurs trigonométriques associées au pentagone. Considérons un triangle d'argent de base φ et donc de côtés adjacents de longueur 1. Ce triangle, coupé en son milieu, comme sur la figure de droite, est un triangle rectangle d'hypoténuse de longueur 1. Sa base est de longueur φ/2 car elle correspond à la demi-base du rectangle d'argent. On en déduit :
formula_9
Un raisonnement analogue s'applique au triangle d'or. Les côtés ont toujours une longueur 1, la base est en proportion d'or donc de longueur φ –1. On en déduit que le cosinus de 72° est égal à (φ – 1)/2. À partir de ces valeurs et de différentes formules, il est possible de calculer les images par les fonctions trigonométriques des multiples ainsi que les moitiés de l'angle 36°.

Une autre manière de déterminer les différentes valeurs caractéristiques d'un pentagone consiste à utiliser le plan complexe. Les affixes des sommets sont les racines cinquièmes de l'unité. Comme 5 est un nombre de Fermat, le théorème de Gauss-Wantzel a pour conséquence que le pentagone régulier est constructible à la règle et au compas : les racines s'obtiennent par résolutions successives d'équations du second degré. Dans le plan complexe, les affixes des sommets du pentagone sont 1 et les racines du cinquième polynôme cyclotomique "X" + "X" + "X" + "X" + 1.

,\quad \cos\,(18^\circ)= \frac 12 \sqrt{ 2+\varphi }</math>

On peut aussi déterminer le cosinus des angles de la forme formula_12 en appliquant la formule du cosinus de l'angle moitié :

De manière générale :

Un autre chemin que celui de la géométrie permet de mieux comprendre les propriétés du nombre d'or, l'arithmétique. Elle met en évidence ses propriétés algébriques ainsi que les profondes relations entre des sujets de prime abord différents comme la suite de Fibonacci, les fractions continues ou certaines équations diophantiennes. Une équation diophantienne est une équation dont les coefficients sont entiers et dont les solutions recherchées sont entières. 

Carl Friedrich Gauss, un mathématicien du , écrivait que le charme particulier de la théorie des nombres vient de la simplicité des énoncés jointe à la difficulté des preuves.

Le nombre d'or est solution de l'équation formula_16. Cette propriété possède des conséquences remarquables si φ est utilisé comme base d'un système de nombre (voir base d'or). Elle permet également d'écrire :formula_17

Le nombre d'or est également lié à un certain anneau d'entiers algébriques. Les repères sont modifiés par rapport à ceux des entiers relatifs, mais le mot « entier » est encore utilisé, par analogie : le nombre d'or est un entier algébrique et même un entier quadratique. Le mot accolé à « entier » marque la différence. Par exemple 11, qui est un nombre premier dans les entiers usuels, n'est pas un élément premier dans ce nouvel univers de nombres.

La fraction continue est une manière d'approcher un nombre réel, dans le cas du nombre d'or, elle est simple. On peut l'approcher par les valeurs 1 ou 1 + 1/1. La fraction suivante est plus précise :
formula_18
Le prolongement à l'infini de cette méthode donne exactement le nombre d'or :
formula_19

En effet, le membre de droite représente un irrationnel positif formula_20 qui vérifie, par construction, formula_21 c'est-à-dire formula_22. Ce nombre formula_20 est donc égal à φ.

La fraction continue approximant le nombre d'or possède systématiquement la plus petite valeur possible pour chacun de ses coefficients, à savoir 1. Ce nombre irrationnel et tous ceux qui lui sont équivalents sont ceux qui s'approximent le plus mal par des rationnels. On dit de lui qu'il est « le plus irrationnel » des nombres réels (cf. Théorème de Hurwitz sur les approximations diophantiennes).

Une démonstration plus classique et rigoureuse est proposée dans l'article détaillé.

Une manière d'illustrer la fraction continue est la suivante. Dans un premier temps, on dessine un rectangle formé de deux carrés côte à côte et de côté 1. Ce sont les deux carrés numérotés "1" sur la figure de droite. Le rapport entre la longueur et la largeur de la figure est égal à 2, la meilleure approximation en nombre entier du nombre d'or. On ajoute un carré de côté égal à la longueur de la figure précédente. Un tel carré est de côté 2 qu'il est judicieux d'écrire ici 1+1. On obtient un rectangle, composé de trois carrés (les deux numérotés 1 et celui numéroté 2) dont le rapport de la longueur sur la largeur est égal 3/2 qui s'écrit 1 + 1/2 ou encore 1 + 1/(1+1). En réitérant avec un carré de côté égal à la longueur du rectangle précédent, soit celui numéroté 3 sur la figure, on trouve :
formula_24

L'approximation commence à être précise : elle vaut 1,66… ; celle du nombre d'or est 1,62… On recommence le processus avec un carré de côté la longueur du précédent ; on obtient comme rapport 8/5, qui s'écrit 1 + 3/5 et avec le calcul précédent :
formula_25

La dernière itération de la figure donne un rectangle dont le rapport de la longueur sur la largeur vaut 13/8 approximation précise à plus de un centième. Si le processus est réitéré à l'infini, on obtient une expression du nombre d'or en "fraction continue" :
formula_26

Ce résultat possède une conséquence géométrique déjà citée. Si le processus de génération de rectangle est itéré un nombre suffisant de fois. Le retrait d'un carré de dimension maximale laisse une surface rectangulaire de même proportion que le rectangle initial, aux erreurs de mesure près. On obtient un rectangle d'or.
Le calcul des couples de numérateurs et dénominateurs obtenus par la fraction continue donne les valeurs suivantes (1,1), (2,1), (3,2), (5,3), … le dénominateur correspond au numérateur de la fraction précédente. Il est aussi égal au "n"-ième terme de la suite de Fibonacci ("F"). Elle est définie par récurrence :
formula_27
La suite de Fibonacci fournit donc des approximations du nombre d'or :
formula_28
La vitesse de convergence est linéaire ; la différence entre "F"/"F" et φ est, en valeur absolue, inférieure au carré de l'inverse de "F". Par exemple, la fraction "F"/"F" = 987/610 = offre une précision proche du millionième.

Réciproquement, la formule de Binet exprime la suite de Fibonacci en fonction du nombre d'or :
formula_29
On en déduit l'équivalent :
formula_30

En effet, –1/φ est strictement compris entre –1 et 0 donc ses puissances s'approchent de plus en plus de 0, tandis que celles de φ tendent vers l'infini. Si l'on prend l'entier le plus proche de l'expression précédente en négligeant le terme en (–1/φ), on obtient :
formula_31

La fraction continue offre des approximations rationnelles "F"/"F" qui sont « presque » des solutions à l'équation (1) ci-dessus. Plus précisément, ("F"/"F") – ("F"/"F") – 1 n'est bien sûr pas égal à 0 (puisque le nombre d'or est irrationnel) mais à (–1)/"F", ou encore :
formula_32
Ceci est lié à l'équation diophantienne :
formula_33

Le cas "n" = 5 de l'identité de Brahmagupta prend, par changement de variables, la forme suivante :

formula_34

Si ("a", "b") et ("c", "d") forment deux couples solutions de l'équation (2), cette identité fournit donc une nouvelle solution ("e", "f"), donnée par "e" = "ac + bd" et "f" = "ad + bc + bd". La découverte de la « multiplication » particulière suivante permet ainsi de construire autant de solutions que désiré, à partir d'une solution non triviale :
formula_35
En effet, en combinant une solution ("x", "y") avec elle-même, on en obtient une nouvelle : ("x" + "y", 2"xy" + "y"), et l'on peut réitérer cette opération.

Remarquons aussi qu'en combinant ("F", "F") avec ("F", "F"), on obtient ("F", "F").

L'ensemble, noté ℤ[φ], des nombres réels de la forme "a" + φ"b" (avec "a" et "b" entiers relatifs) est stable par addition, mais aussi par multiplication puisque φ = 1 + φ (de proche en proche, toutes les puissances de φ sont donc dans ℤ[φ] ; plus précisément, φ = "F" + "F"φ, où ("F") désigne la suite de Fibonacci).

On obtient ainsi une structure équipée d'une addition et d'une multiplication, qui est un anneau commutatif intègre. On montre que ℤ[φ] est l'anneau des éléments « entiers » du corps quadratique ℚ(), c'est-à-dire ceux qui sont racines d'un polynôme de la forme "X" + "cX" + "d", avec "c" et "d" entiers relatifs.

L'anneau ℤ[φ] est euclidien, c'est-à-dire qu'il dispose d'une division euclidienne semblable à celle de l'anneau ℤ des entiers relatifs. Les outils de l'arithmétique usuelle sur ℤ, comme le théorème de Bachet-Bézout, le lemme d'Euclide ou le théorème fondamental de l'arithmétique, sont tous des conséquences de la division euclidienne.

La compréhension de l'arithmétique de ℤ passe souvent par celle des nombres premiers. L'anneau ℤ[φ] a aussi ses propres éléments premiers. Un nombre premier de ℤ n'est pas toujours premier dans ℤ[φ], comme le montre le contre-exemple 11 = (3 + 2φ)(5 – 2φ). Cette différence engendre des modifications dans l'application des théorèmes classiques. Par exemple, un analogue du petit théorème de Fermat indique qu'un nombre premier "p" ne divise φ – 1 que s'il est congru à ±1 modulo 5.

Certains historiens considèrent que l'histoire du nombre d'or commence lorsque cette valeur fit l'objet d'une étude spécifique. Pour d'autres, la détermination d'une figure géométrique contenant au moins une proportion se calculant à l'aide du nombre d'or suffit. La pyramide de Khéops (vers 2600 av. J.-C.) devient, selon cette dernière convention, un bon candidat pour l'origine. D'autres encore s'appuient sur les restes d'un monument dont les dimensions permettent d'approximer le nombre d'or. Selon ce critère, un amas de pierres sous la mer des Bahamas serait une origine plus ancienne. Ces vestiges, dont l'origine humaine et la datation sont incertaines, sont dénommés « temple d'Andros ».

Les historiens s'accordent tous sur l'existence d'une , mais l'absence de document d'époque définitif interdit une connaissance indiscutable de l'origine. Dans ce cadre, l'hypothèse est parfois émise que le nombre d'or a son origine chez les pythagoriciens : ils auraient connu et construit empiriquement le dodécaèdre régulier.

Les pythagoriciens connaissaient déjà une construction du pentagone à l'aide de triangles isocèles. À cette époque, l'étude du nombre d'or est essentiellement géométrique, Hypsiclès, un mathématicien grec du , en fait usage pour la mesure de polyèdres réguliers. Elle revient chaque fois qu'un pentagone est présent.

L'approche arithmétique est initialement bloquée par le préjugé pythagoricien qui voudrait que tout nombre soit rationnel (rappelons que le nombre d'or ne l'est pas). Platon évoque cette difficulté. Les premières preuves du caractère irrationnel de certaines diagonales de polygones réguliers remontent probablement au . Platon cite les travaux de son précepteur, Théodore de Cyrène, qui montre l'irrationalité de et, . .

Le premier texte mathématique indiscutable est celui des "Éléments" d'Euclide (vers 300 av. J.-C.). Dans la du Livre, le nombre d'or est défini comme une proportion géométrique :

Sa relation avec le pentagone, l'icosaèdre et le dodécaèdre régulier est mise en évidence. Il est donc lié aux problèmes géométriques déjà résolus par les pythagoriciens, mais selon l'historien des sciences Thomas Heath (s'appuyant sur Proclus), c'est probablement Platon qui en avait fait ensuite un objet d'étude en soi :

Les mathématiques arabes apportent un nouveau regard sur ce nombre, plus tard qualifié d'or. Ce n'est pas tant ses propriétés géométriques qui représentent pour eux son intérêt, mais le fait qu'il soit solution d'équations du second degré. Al-Khawarizmi, un mathématicien perse du , propose plusieurs problèmes consistant à diviser une longueur de dix unités en deux parties. L'un d'eux possède comme solution la taille initiale divisée par le nombre d'or. Abu Kamil propose d'autres questions de même nature dont deux sont associées au nombre d'or. En revanche, ni pour Al-Khawarizmi ni pour Abu Kamil, la relation avec la proportion d'extrême et moyenne raison n'est mise en évidence. Il devient ainsi difficile de savoir si la relation avec le nombre d'or était claire pour eux.

Leonardo Pisano, plus connu sous le nom de Fibonacci, introduit en Europe les équations d'Abu Kamil. Dans son livre "Liber Abaci", on trouve non seulement la longueur des deux segments d'une ligne de 10 unités mais aussi, clairement indiquée la relation entre ces nombres et la proportion d'Euclide. Son livre introduit la suite qui porte maintenant son nom, connue « aux Indes » depuis le . En revanche la relation avec le nombre d'or n'est pas perçue par l'auteur. Un élément de cette suite est la somme des deux précédents.

En 1260, Campanus démontre l'irrationalité de φ par une descente infinie que l'on peut visualiser dans la spirale d'or.

À la fin du , Luca Pacioli rédige un livre intitulé "La divine proportion", illustré par Léonard de Vinci. Si l'aspect mathématique n'est pas nouveau, le traitement de la question du nombre d'or est inédit. L'intérêt du nombre ne réside pas tant dans ses propriétés mathématiques que mystiques, elles . Pacioli cite les dix raisons qui l'ont convaincu. L'incommensurabilité prend, sous la plume de l'auteur, la forme suivante .

Pacioli rédige ainsi l'envoi de son livre : , il est en revanche discret sur la manière dont s'applique cette proportion. Dans son traité d'architecture, l'auteur se limite aux proportions de Vitruve, un architecte de la Rome antique. Elles correspondent à des fractions d'entiers, choisies à l'image du corps humain. S'il cite comme exemple une statue du grec Phidias, ce n'est que pour y voir le nombre d'or dans un dodécaèdre régulier, une figure associée au pentagone symbole de la quintessence, une représentation du divin. Les architectes de la Renaissance n'utilisent pas le nombre d'or.

Les mathématiciens de l'époque ne sont pas en reste. Les spécialistes des équations polynomiales que sont Gerolamo Cardano et Raphaël Bombelli indiquent comment calculer le nombre d'or à l'aide d'équations de second degré. Un résultat plus surprenant est anonyme. Une note manuscrite, datant du début du et écrite dans la traduction de Pacioli des éléments d'Euclide de 1509, montre la connaissance de la relation entre la suite de Fibonacci et le nombre d'or. Si l'on divise un terme de la suite par son précédent, on trouve une approximation du nombre d'or. Plus le terme est élevé, plus l'approximation est bonne et elle peut devenir aussi précise que souhaitée. Ce résultat est, plus tard, retrouvé par Johannes Kepler puis par Albert Girard. Kepler est fasciné par le nombre d'or, il dit de lui .

Sur le front des mathématiques, l'intérêt diminue. Au , le nombre d'or ainsi que les polyèdres réguliers sont considérés . Concernant le nombre d'or, on lui prête encore un peu d'attention au siècle suivant : Jacques Binet retrouve en 1843 un résultat oublié, démontré initialement par Leonhard Euler en 1765. Si la lettre φ désigne le "nombre d'or", le "n"-ième terme de la suite de Fibonacci est donné par la formule . Ce résultat est maintenant connu sous le nom de "Formule de Binet". L'essentiel des travaux se reporte sur la suite de Fibonacci. Édouard Lucas trouve des propriétés subtiles associées à cette suite, à laquelle il donne pour la première fois le nom de « suite de Fibonacci ». Son résultat le plus important porte le nom de "Loi d'apparition des nombres premiers au sein de la suite Fibonacci".
C'est durant ce siècle que les termes de « section dorée », puis « nombre d'or » apparaissent. On les trouve dans une réédition d'un livre de mathématiques élémentaires écrit par Martin Ohm. L'expression est citée dans une note de bas de page : . Cette réédition fait surface dans une période située entre 1826 et 1835, en revanche son origine est un mystère.

L'intérêt resurgit au milieu du siècle, avec les travaux du philosophe allemand Adolf Zeising. Avec lui, le nombre d'or devient un véritable système, une clé pour la compréhension de nombreux domaines, tant artistiques — comme l'architecture, la peinture, la musique —, que scientifiques — avec la biologie et l'anatomie. Une dizaine d'années plus tard, il publie un article sur le pentagramme, . Une relecture de la métaphysique pythagoricienne lui permet de conclure à l'existence d'une loi universelle fondée sur le pentagramme, et donc, sur le nombre d'or. Malgré une approche scientifique douteuse, la théorie de Zeising obtient un franc succès.

En France, pouvoir codifier de manière scientifique la beauté est une idée qui séduit. Les dimensions du Louvre, de l'Arc de triomphe sont mesurées avec attention. Des délégations sont chargées de mesurer précisément la taille des pyramides d'Égypte ainsi que du Parthénon. Les cathédrales ne sont pas en reste. La France trouve son champion en Charles Henry, un érudit qui s'inscrit dans l'esprit positiviste de son temps. Dans un texte fondateur, à l'origine du mouvement pointilliste, il associe au nombre d'or, une théorie de la couleur et des lignes. Son influence auprès de peintres comme Seurat ou Pissarro n'est pas négligeable, mais son attachement au nombre d'or n'est pas aussi profond que chez son collègue allemand : en 1895, il finit par abandonner définitivement l'idée de quantifier le beau.

Loin de s'éteindre avec le déclin du positivisme, la popularité du nombre d'or ne fait que croître durant la première partie du siècle. Le prince roumain Matila Ghyka en devient l'incontestable chantre. Il reprend les thèses du siècle précédent et les généralise. Tout comme Zeising, il s'appuie tout d'abord sur les exemples issus de la nature, comme les coquillages ou les plantes. Il applique cette universalité à l'architecture avec des règles plus souples que son prédécesseur. Cette théorie avait déjà influé sur les notations, le nombre d'or étant noté φ en référence au sculpteur Phidias, concepteur du Parthénon.

La dimension mystique n'est pas absente chez Ghyka et trouve ses origines dans la philosophie pythagoricienne. L'absence de trace écrite sur le nombre d'or chez les pythagoriciens s'expliquerait par le culte du secret. Cette idée est largement reprise et généralisée par les mouvements de pensées ésotériques au . Le nombre d'or serait une trace d'un savoir perdu, nommé Tradition Primordiale ou Connaissance Occulte chez les Rose-Croix ou des mouvements connexes. Il se retrouve chez les passionnés de l'Atlantide, qui voient dans la pyramide de Khéops ou le temple d'Andros la preuve d'un savoir mathématique oublié. Ce mouvement de pensée reprend des idées développées en Allemagne au par Franz Liharzik (1813 - 1866), pour qui la présence du nombre d'or, de π et de carrés magiques est la preuve « incontestable » d'un groupe restreint d'initiés possédant la science mathématique absolue.

En 1929, une époque troublée par des idées d'un autre âge, Ghyka n'hésite pas à tirer comme conclusion de son étude sur le nombre d'or, la suprématie de ce qu'il considère comme sa race : . Si le prince n'insiste que très médiocrement sur cet aspect du nombre d'or, d'autres n'ont pas ses scrupules. Ils usent de l'adéquation de la morphologie d'une population avec les différentes "proportions divines" pour en déduire une supériorité qualifiée de "raciale". Ce critère permet de fustiger certaines populations, sans d'ailleurs la moindre analyse. Le nombre d'or est, encore maintenant, sujet à de prétendues preuves de supériorité culturelle, sociale ou ethnique.

Sans cautionner ces idées extrêmes, certains intellectuels ou artistes éprouvent une authentique fascination pour le nombre d'or ou son mythe. Le compositeur Iannis Xenakis utilise ses propriétés mathématiques pour certaines compositions. L'architecte Le Corbusier reprend l'idée consistant à établir les dimensions d'un bâtiment en fonction de la morphologie humaine et utilise pour cela le nombre d'or. Le poète et intellectuel Paul Valéry célèbre le nombre d'or dans son "Cantique des colonnes" (1922) :

Le peintre Salvador Dalí fait référence au nombre d'or et à sa mythologie dans sa peinture, par exemple dans un tableau dénommé Le Sacrement de la dernière Cène.

Sur le plan mathématique, le nombre d'or suit une trajectoire inverse, son aura ne fait que diminuer et il quitte le domaine de la recherche pure. Il existe néanmoins une exception, la revue "Fibonacci Quarterly" sur la suite de Fibonacci. En revanche, le nombre d'or apparaît comme la clé de quelques sujets scientifiques. La question de phyllotaxie, se rapportant à la spirale que l'on trouve dans certains végétaux comme les écailles de la pomme de pin est-elle vraiment liée à la proportion d'Euclide ? Cette question fait couler beaucoup d'encre dès le siècle précédent. Wilhelm Hofmeister suppose que cette spirale est la conséquence d'une règle simple. Pour le botaniste allemand Julius von Sachs, ce n'est qu'un orgueilleux jeu mathématique, purement subjectif. En 1952, un scientifique, père fondateur de l'informatique, Alan Turing propose un mécanisme qui donnerait raison à Hofmeister. Deux physiciens français, Stéphane Douady et Yves Couder, finissent par trouver l'expérience confirmant Hofmeister et Turing. La présence du nombre d'or dans le monde végétal ne semble ni fortuite ni subjective.

La thèse de l'omniprésence du nombre d'or est souvent reprise. Si un avis définitif sur ce phénomène est difficile à propos de l'œuvre des hommes, il est plus aisé de comprendre la différence d'opinion que soulève cette question pour les sciences de la nature. Elle provient de l'usage des critères utilisés pour lier ou non le nombre d'or avec un phénomène.

Dans le monde végétal, les écailles des pommes de pin engendrent des spirales particulières, dites logarithmiques. Ces spirales se construisent à l'aide d'un nombre réel non nul quelconque. Si ce nombre est égal au nombre d'or, les proportions correspondent à la moyenne et extrême proportion d'Euclide et la suite de Fibonacci apparaît. Ce phénomène se produit sur les étamines d'une fleur de tournesol. La présence du nombre d'or n'est pas controversée dans ce cas.
En revanche, si ce nombre n'est pas égal au nombre d'or, alors ni proportion d'or, ni suite de Fibonacci ne sont pertinentes dans l'étude de la spirale logarithmique correspondante, comme celles que forment la coquille du mollusque le nautilus, les yeux sur les plumes d'un paon ou encore certaines galaxies.

En minéralogie, il existe des cristaux dont les atomes s'organisent selon un schéma pentagonal. Les proportions entre les côtés et les diagonales du pentagone font intervenir le nombre d'or. Il est aussi présent dans des structures dites quasi cristallines. Les atomes dessinent des triangles d'or qui remplissent l'espace sans pour autant présenter de périodicité, on obtient un pavage de Penrose. Pour la même raison que précédemment, le nombre d'or est présent et l'on retrouve la suite de Fibonacci. Le pentagone n'est pas présent dans tous les cristaux. La structure cubique à faces centrées d'un diamant ne fait pas intervenir le nombre d'or.

Ainsi, selon l'axe d'analyse, la réponse sur l'omniprésence du nombre d'or est différente. Pour un scientifique spécialiste dans un domaine, l'usage du nombre d'or est finalement plutôt rare, limité à quelques sujets comme la phyllotaxie du tournesol ou la cristallographie du quartz. S'il recherche des concepts explicatifs pour mieux comprendre son domaine, la proportion d'Euclide est rarement de ceux-là. D'autres utilisent l'analogie ainsi que l'esthétique comme critère. La divine proportion est pour eux présente dans les cieux, la vie animale et végétale, les minéraux et finalement dans toute la nature.

En biologie, l'ordonnancement des écailles d'une pomme de pin ou de l'écorce d'un ananas induit des spirales ordonnées par des nombres entiers, souvent associés au nombre d'or. Sur la figure de gauche, on observe 8 spirales, chacune formée de 13 écailles dans un sens et 13 spirales formées de 8 écailles dans l'autre sens. Les proportions de ces spirales ne sont pas très éloignées de celles d'une spirale d'or. Les nombres 8 et 13 sont deux nombres consécutifs de la suite de Fibonacci et leur rapport est proche du nombre d'or. Un phénomène analogue se produit avec les étamines des tournesols, cette fois avec les couples d'entiers (21,34), (34,55) et (55,89). Chacun de ces couples correspond à deux entiers consécutifs de la suite de Fibonacci.

La phyllotaxie ne suit pas toujours les lois du nombre d'or. À droite, on voit un mécanisme analogue sur des feuilles, les deux spirales sont toujours logarithmiques mais ne suivent plus la proportion d'or. Les nombres de spirales dans un sens et dans l'autre sont égaux.

Ce mécanisme est régi par la règle de Hofmeister : . Un primordium correspond à un embryon de partie de plante : écaille, feuille, d'étamine, etc. Ce mécanisme est contrôlé par la production d'une substance inhibitrice, appelée morphogène, émise par les primordia. Ainsi une nouvelle pousse ne peut naître que le plus loin possible des précédentes.

Dans le cas de l"'Achimenes erecta", la tige pousse rapidement par rapport à la feuille, la deuxième feuille naît dans la direction opposée, le rapport entre la croissance de la tige et le temps d'apparition d'un nouveau primordium fait que la troisième position la meilleure est à un angle d'un tiers de tour par rapport à la première feuille et deux tiers par rapport à la deuxième. Finalement on obtient l'apparition de trois feuilles, décalées d'un tiers de tour l'une par rapport à l'autre, puis d'un nouveau jeu de trois feuilles, décalé d'un sixième de tour par rapport au jeu précédent.

La pomme de pin suit la même règle pour le primordium de l'écaille. La croissance de la tige entre deux primordia est beaucoup plus modérée. Le troisième primordium naît en conséquence entre les deux premiers, avec un angle légèrement plus faible du côté du premier primordium, la tige ayant un peu grandi. Douady et Couder ont montré qu'un tel mécanisme produit deux jeux de spirales d'or de directions opposées dont les nombres de spirales par jeu correspondent à deux éléments consécutifs de la suite de Fibonacci. Plus la croissance entre l'apparition de deux primordia est petite, plus élevés sont les deux éléments consécutifs de la suite.

Le corps humain est un enjeu souvent corrélé à celui du nombre d'or. Il comporte différentes facettes. Tout d'abord scientifique : la question maintes fois posée est de savoir si le corps, à l'image de la fleur de tournesol, possède une relation plus ou moins directe avec le nombre d'or. En termes artistiques, la « divine proportion » est-elle utilisable pour représenter le corps ? Il existe enfin un enjeu esthétique. Si le nombre d'or, comme le pense le compositeur Xenakis, est relié à notre corps, son usage peut être une technique pour obtenir de l'harmonie.
La première corrélation recherchée est dans les dimensions du corps humain. Elle débouche sur la tentative d'un système de mesure construit à l'aide du seul nombre d'or. Zeising fonde toute une anatomie sur cette arithmétique. Après un vif effet de mode, cette approche est finalement abandonnée. Ses proportions sont trop imprécises, et elles correspondent trop mal à l'anatomie du corps humain. Les proportions du crâne, par exemple, ne sont pas réalistes. D'autres raisons, plus profondes encore, sont la cause de l'abandon d'une démarche de cette nature. L'anatomie médicale n'est pas à la recherche d'une proportion particulière, mais des limites qui, si elles sont dépassées, deviennent pathologiques. Elle utilise des fractions simples ainsi que des plages de longueur, mais jamais le nombre d'or. Là où certains voient une "divine proportion", comme dans le rapport de la longueur de l'avant-bras sur celui de la main, l'anatomiste scientifique qui calcule le rapport entre la longueur de la main et celle de l'avant-bras voit 2/3. La différence entre les deux approches, inférieure à 8%, ne lui paraît pas justifier une telle complexité, au vu des variations observées entre les individus. Stephen Jay Gould, un paléontologue, a montré à quel point les mesures anthropométriques visant à étayer les doctrines de cette époque étaient biaisées par leurs auteurs.

Une autre raison est que les dimensions d'un être humain sont en constante évolution. En un siècle, la stature du Français moyen a augmenté de 9 centimètres, et cette croissance n'est pas uniforme. Le jeu des proportions d'un corps humain étant essentiellement dynamique, on imagine mal une proportion unique, clé universelle de l'anatomie humaine. Une approche de cette nature, trop normative et intemporelle, n'a pas beaucoup de sens scientifique en anatomie. Si cet axe de recherche n'est plus d'actualité, cela ne signifie pas l'abandon de la quête du nombre d'or dans le corps humain. Le cerveau est maintenant source d'attention. Cette théorie reste minoritaire et controversée.

Les contraintes artistiques sont de nature différente. Les artistes, attentifs au travail des médecins, ont imaginé des modules, ou systèmes de proportions, propres au corps humain. C'est le désir de le représenter qui impose cette démarche. Un très ancien module est celui des Égyptiens; la classique proportion qu'est le rapport de la taille complète à la hauteur du nombril est estimée à 19/11, relativement loin du nombre d'or. Les modules sont, en général, purement fractionnaires. Tel est le cas de celui inventé par les Égyptiens, par Polyclète, qui nous est rapporté par Vitruve, de celui de Cousin, de Vinci ou de Dürer. Il est néanmoins difficile d'en déduire que Dürer croyait en un canon universel. Il initie une conception fondée sur la pluralité des types de beauté, ayant chacune ses proportions propres.

L'idée que le nombre d'or possède une qualité visuelle intrinsèque est largement citée. Un argument est la présence de la divine proportion dans de nombreux chefs-d'œuvre. Cependant les commentaires précis sont rares, ce qui amène à rechercher le rapport d'Euclide, sans information directe de la part de l'auteur. L'existence d'une forme géométrique ayant des concordances avec le tableau est, pour certains, un élément de preuve. Pour d'autres, une démarche de cette nature est peu convaincante.

Un exemple est celui de "La Naissance de Vénus" de Sandro Botticelli. Ses dimensions, 172,5 × , respectent précisément la proportion. Le carré, associé au rectangle d'or, correspond à un rythme du tableau ; enfin, la diagonale du rectangle restant, ainsi que celle symétrique, sont des lignes de force. Ce raisonnement n'a pas convaincu certains spécialistes. Le tableau semble faire partie d'un diptyque avec "Le Printemps", un autre tableau du maître. L'aile d'Aura, un des dieux, est étrangement coupée. Pour en avoir le cœur net, une analyse finit par être faite. Le verdict est sans appel : Botticelli avait choisi une taille analogue à celle du "Printemps" ; le haut de "La Naissance "est amputé de et avait, à sa conception, la taille du "Printemps". Dans ce cas, la "divine proportion" n'a pas été choisie par le créateur.
Pour certains, il existe un fondement scientifique à la beauté : . Cette idée n'est pas une invention de Pacioli, le traité de peinture de Leon Battista Alberti, établissant les premières règles de la perspective, était déjà l'illustration d'une philosophie analogue. La découverte de lois "scientifiques", modifie la peinture et permet d'incarner un nouvel idéal. Si l'approche mathématique d'Alberti obtient un large consensus, peu d'éléments laissent penser à un succès analogue pour la loi de la divine proportion.

Un exemple est le cas Vinci. Pacioli est un de ses amis proches, Vinci connaît suffisamment ses théories pour illustrer son livre. À travers ses codex, son "Traité de la peinture" et les multiples analyses de ses sources, la pensée de Vinci sur la proportion en peinture nous est connue. Si, pour le maître, la peinture s'apparente à une science, ses thèses sont forts éloignées de celle de son ami. Sa première source est l'observation et l'expérience, et non les mathématiques : . Cette attitude se traduit, par exemple pour le choix des proportions humaines. À travers de multiples dissections, il mesure systématiquement les rapports entre les dimensions des différents os et muscles. Ses planches médicales l'amènent à une conception de l'anatomie dont les rapports sont de même nature que celle de la médecine moderne : ils sont fort nombreux et s'expriment à l'aide de fractions composées de petits facteurs entiers. La science de Vinci s'applique aussi sur des sujets déjà traités comme la perspective. Une fois encore, sa logique est plus proche de l'observation que de la rigidité mathématique. Les lois qu'il ajoute à celles d'Alberti traitent de la couleur : une chose éloignée voit sa couleur tirer vers le bleu, ainsi que de la netteté . Les règles régissant la proportion chez Vinci sont subtiles et en opposition avec des , comme l'application directe d'une proportion sans lien avec ses observations.

À l'instar du "Saint Jérôme" à droite, beaucoup d'exemples de "rectangle d'or" trouvés chez un peintre supposent une approche de la proportion sans justification de la part du peintre ou, comme ici, contraire aux règles établies par son auteur. Ni Arasse dans son volumineux ouvrage sur Vinci, ni Marani dans le sien ne font référence à une explication de cette nature.

Le nombre d'or a aussi influencé les peintres du groupe de Puteaux, appelé aussi « Section d'or », groupe qui se crée autour de Jacques Villon en 1911. Leur emploi du nombre d'or en peinture est cependant davantage intuitif que purement mathématique.

L'usage du nombre d’or dans les constructions anciennes est un sujet de controverse. Pour le prince Ghyka, l’archéologie offre la preuve de l'universalité du canon de beauté qu'est le nombre d'or. L'argument principal est le vaste nombre d'exemples. Le prince reprend les travaux de son prédécesseur Zeising et l'enrichit considérablement. Le théâtre d'Épidaure possède deux séries de gradins l'une de 21 et l'autre de 34 marches, deux éléments consécutifs de la suite de Fibonacci.

Les plus convaincus citent le temple d'Andros et celui de Salomon comme exemple d'utilisation du nombre d'or. Pour le temple d'Andros, sa forme actuelle est un losange dont deux côtés ont un rapport approximativement égal à 5/3, une valeur proche du nombre d'or. L'origine de ces vestiges, qui daterait de ans, n'est pas avérée. Ce site, non reconnu par les archéologues officiels est pour ses partisans une preuve de l'existence de l'Atlantide. Le temple de Salomon aurait une dimension d'un rapport 2/1, certains remarquent que ce sont deux termes consécutifs de la suite de Fibonacci, un élément suffisant à leurs yeux pour voir la trace du nombre d'or.

La grande pyramide de Gizeh convainc un public plus vaste. Cet exemple est cité depuis le milieu du , une époque où la méconnaissance presque totale de l'égyptologie donne naissance à d'innombrables mythes. La coïncidence entre les dimensions de la pyramide et le nombre d'or est ici excellente. Le rapport entre la longueur de la plus grande pente d'une des faces et la demi-longueur d'un côté correspond au nombre d'or avec une précision de moins de 1%. Le scepticisme des professionnels est la conséquence de la connaissance actuelle de la civilisation égyptienne. En effet, les systèmes de longueur utilisés dans les documents connus pour mesurer les pentes et les longueurs horizontales ne coïncident pas, interpréter leur rapport n’a donc pas beaucoup de sens. On ne trouve pas non plus la moindre trace religieuse ou esthétique qui justifie un choix de cette nature. Cette faiblesse pousse Taylor, à l'origine de cette hypothèse, à créer de toutes pièces une citation de Hérodote.

Le cas grec est encore plus populaire et très largement étayé. Mais l'écart entre la culture grecque et le nombre d'or laisse perplexe les spécialistes. Ces proportions incommensurables, que sont la diagonale d'un carré ou celle d'Euclide, sont vécues comme un scandale, une trahison des dieux à l'époque de Pythagore. Un grec n'imagine pas qu'un nombre puisse être autre chose qu'une fraction d'entiers. L'existence de proportions, comme celle d'Euclide, qui ne sont pas des nombres est une source de chaos intellectuel, à l'opposé des valeurs philosophiques et mystiques des pythagoriciens. On raconte que Hippase de Métaponte aurait été exclu de la confrérie des pythagoriciens pour avoir dévoilé le scandale de l'incommensurabilité d'une diagonale d'un dodécaèdre régulier, une autre indique qu'il aurait péri noyé, conséquence de son impiété. Qu'une proportion aussi négative soit utilisée pour les monuments apparaît étonnant. Les textes d'architecture grecs confirment l'usage des nombres rationnels pour définir les proportions des bâtiments. Les "proportions harmonieuses" sont longuement relatées par Vitruve un architecte, auteur du célèbre traité "De architectura" en dix volumes. Pour ce faire, il utilise largement, au volume , les mathématiques de Platon, Pythagore ou d'autres mathématiciens. Les proportions proviennent du module de Polyclète, un sculpteur grec contemporain de Phidias. Le traité de Vitruve ne contient aucune trace de proportion irrationnelle à l'exception de la diagonale du carré.

Enfin, les exemples choisis par le prince sont controversés. Retrouver la "divine proportion" dans la façade du Parthénon demande des conventions spécifiques, comme d'inclure trois des quatre marches du fronton ou de tronquer le toit. L'usage de mesures non spécifiques donne une proportion différente. Pour faire apparaître le nombre d'or dans les proportions des monuments grecs, Ghyka n'hésite pas à utiliser des fractions comme 1/φ. Patrice Foutakis a examiné les dimensions de 15 temples, 18 tombeaux monumentaux, 8 sarcophages et 58 stèles funéraires pour la période du avant notre ère au de notre ère. Les temples étaient l'endroit par excellence pour la communication entre les humains et les dieux, tandis que les tombeaux, sarcophages et stèles funéraires étaient directement liés au passage des mortels de la vie matérielle à celle immortelle. Si le nombre d'or impliquait des propriétés divines, mystiques ou esthétiques, dans ce cas la plupart de ce type des constructions obéiraient à la règle de la proportion d'or. Le résultat de cette recherche originale est sans appel : le nombre d'or était complètement absent de l'architecture grecque du avant notre ère, et quasiment absent pendant les six siècles suivants. Quatre exemples très rares, et pour cela précieux, d'application du nombre d'or ont été identifiés dans une tour antique à Modon, le Grand autel de Pergame, une stèle funéraire d'Édessa et un tombeau monumental à Pella. C'est la première fois qu'une preuve est apportée pour une utilisation du nombre d'or dans des constructions de la Grèce antique, toutefois, selon cet auteur, utilisation marginale qui témoigne de l'indifférence des Grecs anciens pour le nombre d'or en architecture.

Le Corbusier est l'architecte qui théorise l'usage du nombre d'or dans son métier. S'il reprend l'idée de Vitruve, consistant à proportionner un bâtiment aux dimensions d'un corps humain, il y associe d'autres éléments justifiant l'usage de la proportion d'Euclide.

Le nombre d'or permet de créer un curieux système de numération. Les mathématiques nous apprennent qu'il est possible de construire une numération positionnelle, non seulement avec dix, comme celle des humains, ou avec deux, pour les ordinateurs, mais avec n'importe quel nombre réel strictement positif et différent de "un". Celui construit avec le nombre d'or, appelé base d'or, lui semble le plus adapté à l'architecture. Au premier contact, il est un peu étrange. Par exemple dans ce système 100 est égal à 10 + 1, ce qu'un mathématicien lit φ = φ + 1.

Cette "échelle harmonique", pour reprendre son expression, permet de réconcilier les atouts du système métrique décimal, pratique et abstrait, avec ceux du système anglais des pouces et des pieds, naturel mais peu pratique. En calant les différentes "dizaines", c'est-à-dire ici les puissances du nombre d'or, sur les dimensions humaines, Le Corbusier cherche à obtenir un système alliant les deux avantages. La deuxième unité correspond à la taille d'un avant-bras, la troisième à la distance entre le nombril et le sommet de la tête, la quatrième à celle entre le sol et le nombril d'un homme debout et la cinquième à la taille d'un adulte.

En termes d'architecture, cette démarche offre un moyen naturel pour incarner l'idéal de Vitruve. Chaque dizaine correspond à une proportion humaine et les différentes proportions se répondent entre elles. En termes d'urbanisme, Le Corbusier cherche à trouver un moyen de normalisation. En 1950, date de parution du premier tome sur le Modulor, nom qu'il donne à ce système, les besoins de reconstruction sont vastes et la rationalisation de la production, un impératif. L'auteur parle de "machine à habiter". Cette démarche, vise aussi un objectif esthétique. La normalisation dispose d'un avantage, elle permet "plus d'harmonie". Le tracé régulateur, c'est-à-dire l'échelle construite sur la suite de Fibonacci y joue un rôle : .

À partir des années 1950, Le Corbusier utilise systématiquement le modulor pour concevoir son œuvre architecturale. La Cité radieuse de Marseille ou la Chapelle Notre-Dame-du-Haut de Ronchamp sont deux exemples célèbres.

En musique, le nombre d'or est recherché à la fois dans l'harmonie et dans le rythme.

Le terme d'harmonie désigne ici une technique permettant de choisir les différentes notes jouées simultanément. Durant une période qui s'étend du au début du , elle est essentiellement tonale, à l'image de la musique de Bach ou Mozart. Aucune série de deux notes ne définit une "proportion d'or". L'approximation la plus proche étant la sixte mineure obtenue par deux sons dont les fréquences définissent un rapport de 8/5 = 1,6 (la sixte majeure correspondant à un rapport de fréquence de 5/3 = 1,66 est une approximation moins bonne). Pour cette raison, le nombre d'or est souvent recherché dans la musique du . De nouvelles gammes sont explorées, comme la gamme décatonique ou 10-TET (ten-ton equal temperament). Dans celle-ci, l'octave est partagée en 10 parties égales. Chaque degré représente alors un écart de 2. Pour cette gamme, le nombre d'or est proche du rapport défini par deux notes séparées de 7 degrés. La présence du nombre d'or ici est néanmoins un peu fortuite. Un écart entre 7 degrés donne une proportion de 2 approximativement égal à 1,624.

Le rythme est plus largement associé au nombre d'or et sur une période musicale plus vaste. Son traitement par Bach est l'objet d'une thèse de doctorat, sur l'analogie entre les rythmes de la "Suite en do mineur pour luth" (BWV 997) et la "Passion selon saint Matthieu" (BWV 244). Roy Howat montre que Debussy était associé à des revues symbolistes auxquelles il participait et qui analysaient les proportions et le nombre d'or. Il montre aussi comment on retrouve cette approche à travers des œuvres comme "La Mer" ou "Reflets dans l'eau". Des études montrent des résultats analogues pour Erik Satie, Béla Bartók ou encore Karlheinz Stockhausen. Certains compositeurs de musique électroacoustique ont fabriqué des sons synthétiques dont les fréquences des partiels sont basées sur le nombre d'or.

À l'exception de compositeurs comme Xenakis où l'usage du nombre d'or est explicité par l'auteur, l'absence de preuve définitive empêche le consensus. La polémique est néanmoins de nature différente de celle qui sévit, par exemple en archéologie. Ici la position favorable à l'existence d'un usage large du nombre d'or est défendue par des institutions professionnelles comme l'Ircam ou une thèse d'université comme celle de Montréal.

Une question récurrente est celle de l'existence ou non d'une réalité scientifique de l'idée de beauté associée au nombre d'or. Elle s'inscrit dans le cadre général d'une théorie scientifique de l'esthétique. Certains artistes, comme Xenakis en sont persuadés : . Charles Henry, dans le domaine des arts picturaux, inscrit le nombre d'or dans une vaste théorie de cette nature, traitant non seulement des proportions, mais aussi de la couleur et des contrastes.

Préfigurant une démarche de nature sociologique comme celle d'Émile Durkheim, le philosophe allemand Gustav Fechner tente des expériences statistiques pour "valider scientifiquement" une association humaine entre le beau et le rectangle d'or. Des formes sont présentées à un public qui évalue les proportions les plus esthétiques. Si les résultats vont dans le sens de l'existence d'un canon de beauté construit à l'aide de la divine proportion, le protocole choisi ne correspond pas aux critères actuels de rigueur. Une deuxième expérience, plus objective met en évidence une préférence pour un format proche du "16/9" de la télévision. Une fois encore, et malgré son caractère plus rigoureux, le caractère universel d'un tel format n'est pas établi.

Si l'intuition d'artistes comme Xenakis, Valéry ou Le Corbusier laisse présager l'existence d'une transcendance esthétique du nombre d'or, aucune approche scientifique ne permet aujourd'hui de confirmer cette hypothèse.




</doc>
<doc id="14079" url="https://fr.wikipedia.org/wiki?curid=14079" title="Metazoa">
Metazoa

Métazoaire (nom de taxon : Metazoa, du grec "meta" 'après' et "zōon" 'animal', par opposition aux protozoaires) est un nom de clade désignant les animaux. Les organismes ainsi qualifiés sont des eucaryotes (ont des cellules avec un noyau), multicellulaires, hétérotrophes. Ces hétérotrophes ne se nourrissent pas par absorption, contrairement aux champignons. 

Le concept de métazoaire est apparu par opposition aux protozoaires, qui sont généralement unicellulaires, à une époque où ces derniers étaient parfois inclus dans le règne animal. La compréhension actuelle de la phylogénie a conduit à limiter la notion d'animal (nom de taxon : Animalia) aux seuls métazoaires.

Ayant divergé des Parazoaires il y a 940 millions d'années, les Métazoaires regroupent plus d'un million d'espèces décrites mondialement réparties dans tous les milieux, y compris les plus extrêmes. Ils possèdent de vrais tissus avec la présence de plusieurs cellules qui en se différenciant permettent la spécialisation cellulaire, ce qui entraîne une plus grande efficacité énergétique.

La phylogénie des métazoaires se précise à l'aide de nouveaux outils d'analyse.

Le taxon Metazoa (identique à l'actuelle version du taxon Animalia) se révèle phylogénétiquement plus proche d'un groupe renfermant la majorité des champignons (Fungi) que de la plupart des formes unicellulaires qui lui avaient été rattachées dans une ancienne version du règne Animalia (plus ou moins équivalent au regroupement des Metazoa et Protozoa actuelles).

Le caractère holophylétique du groupe des métazoaires semble bien établi. En termes plus simples, tous les animaux multicellulaires ont un ancêtre commun dont les descendants sont tous des animaux multicellulaires (à condition d'inclure avec ce qualificatif les myxozoaires). Plusieurs théories sont proposées pour expliquer l'origine des métazoaires. La théorie symbiotique présume que des cellules indépendantes ont développé une relation symbiotique si étroite qu'elles ont perdu leur autonomie et ont dû s'associer. La théorie coloniale suggère que les métazoaires dérivent de colonies de Choanoflagellés. La théorie syncytiale ou plasmodiale fait dériver les métazoaires d'un protozoaire multinucléé qui devient pluricellulaire, en compartimentant sa masse par des cloisons formant autant de cellules qu'il y a de noyaux.

Les principales caractéristiques propres aux métazoaires (synapomorphies) sont :





</doc>
<doc id="14080" url="https://fr.wikipedia.org/wiki?curid=14080" title="Kylix (informatique)">
Kylix (informatique)

Kylix est un environnement de développement intégré (IDE) sous Linux supportant le langage Pascal Objet et C++. Il s'agit d'une tentative de portage par Borland de son RAD phare Delphi et de son « petit frère » C++ Builder réalisé à l'aide de Wine.

La première version est sortie début 2001, un peu jeune ; une deuxième a suivi très vite ; la troisième et dernière version (Kylix 3) est sortie en juillet 2002. Kylix a par la suité été totalement abandonné par Borland, du fait officiellement des faibles retombées économiques du produit.

C'était un produit très proche de Delphi : même principe, même interface, fonctionnant sous Linux et permettant de créer des programmes pour ce système. Le même code source pouvait en théorie être compilé sous Linux et Windows (respectivement avec Kylix et Delphi) grâce à l'utilisation de la bibliothèque objet CLX qui s'appuie sur la bibliothèque graphique Qt.

Il existait une version en téléchargement gratuit, permettant d'écrire des programmes sous licence GPL. 

Les projets initialement développés sous Kylix ont ainsi dû se tourner vers la solution Lazarus, concurrent libre et suivi de Kylix.




</doc>
<doc id="14082" url="https://fr.wikipedia.org/wiki?curid=14082" title="Qt">
Qt

Qt (prononcé officiellement en anglais ' () mais couramment prononcé ' ()) :

Qt permet la portabilité des applications qui n'utilisent que ses composants par simple recompilation du code source. Les environnements supportés sont les Unix (dont GNU/Linux) qui utilisent le système graphique X Window System ou Wayland, Windows, Mac OS X et également Tizen. Le fait d'être une bibliothèque logicielle multiplateforme attire un grand nombre de personnes qui ont donc l'occasion de diffuser leurs programmes sur les principaux OS existants.

Qt supporte des bindings avec plus d'une dizaine de langages autres que le C++, comme Ada, C#, Java, Python, Ruby, Visual Basic, etc.

Qt est notamment connu pour être le framework sur lequel repose l'environnement graphique KDE, l'un des environnements de bureau par défaut de plusieurs distributions GNU/Linux.

C'est au "Norwegian Institute of Technology" à Trondheim que Haavard Nord (CEO de Trolltech) et Eirik Chambe-Eng (président de Trolltech) se rencontrent. En 1988, Haavard Nord est chargé par une entreprise suédoise de développer une bibliothèque logicielle en C++ pour gérer une interface graphique, c'est la première fois qu'il commence à aborder le sujet. Deux ans plus tard, avec Chambe-Eng, ils développent une application multiplateforme (Unix, Macintosh et Windows) et commencent sérieusement à réfléchir à la conception d'une bibliothèque graphique multiplateforme généraliste. 

En 1991, ils entament le développement de cette bibliothèque. L'année suivante, Chambe-Eng propose le principe des « signaux et slots », qui devient la pierre angulaire de Qt. Et en 1993, le noyau de Qt est prêt et permet aux informaticiens de développer leurs propres composants graphiques. C'est à la fin de cette année que Haavard Nord propose de créer une entreprise pour commercialiser leur bibliothèque.

"Quasar Technologies" est créé le et renommé six mois plus tard en Troll Tech, puis Trolltech, puis Qt Software et enfin Qt Development Frameworks. Les débuts sont particulièrement difficiles financièrement. Mais ils ont la chance d'être mariés : leurs femmes subviennent à leurs besoins.

Le projet a été nommé "Qt" parce que le caractère "Q" était joli dans l'écriture Emacs de Haavard, et le "t" provient de . Le tout se prononçant en anglais , ce qui signifie « mignon ». Le "t" étant minuscule, ne pas prononcer "" () : ce n'est pas un sigle (QT).

C'est en que Trolltech a son premier client, l'entreprise norvégienne Metis. Et durant presque un an, elle n'en a pas d'autre, rendant l'entreprise très fragile financièrement. Son second client, l'Agence spatiale européenne (ESA), lui achète dix licences en mars 1996.

Le est annoncée la première version publique de Qt sur le newsgroup comp.os.linux.announce. Et un an plus tard la version 0.97, puis le la version 1.0 est publiée et annoncée quelques jours plus tard.

C'est en 1997 que le projet KDE est lancé par Matthias Ettrich (qui est embauché par Trolltech l'année suivante). Ce dernier prend la décision d'utiliser Qt comme bibliothèque de base. Le fait qu'un projet de cette envergure utilise Qt sera une très bonne publicité pour Trolltech et sa bibliothèque. Depuis, les liens entre Trolltech et KDE n'ont fait que se renforcer.

La seconde version majeure de Qt est publiée en et une version pour les systèmes embarqués, "Qt/Embedded", connue depuis sous le nom de Qtopia, est publiée en 2000. Cette dernière version est conçue pour Linux et utilise directement son "framebuffer", sans passer par le système de fenêtrage X11 (qui est inadapté pour les systèmes embarqués).

Les deux premières versions majeures de Qt sont disponibles uniquement pour X11 et Windows, le support de Mac OS X arrive avec la version 3.0, publiée en 2001. Par rapport à la version 2.0, cette nouvelle version apporte un meilleur support de l'internationalisation, de l'Unicode ou encore des expressions rationnelles comme en Perl.

Le , la version 4 est publiée et améliore notamment le moteur de rendu, désormais appelé "Arthur", la séparation entre données et présentation et sépare la bibliothèque en modules :
À cela s'ajoute pour la version commerciale sous Windows deux autres modules liés à l'utilisation d'ActiveX : "QAxContainer" et "QAxServer".

Avec l'évolution de Qt 4, d'autres modules sont conçus :

Le , Nokia lance une OPA amicale pour racheter Qt et Trolltech. Trolltech, renommé en Qt Software, devient une division de Nokia. Dès lors, Nokia prend la décision en d'abaisser le maximum de barrières pour faciliter l'adoption de Qt, qui depuis est utilisé par leurs développements en interne :

Nokia se recentrant sur Windows, elle cède en mars 2011 l'activité services et gestion des licences commerciales de Qt à la société Digia . Le 9 août 2012, elle cède intégralement la gestion du framework Qt à Digia pour une somme de (à comparer aux de 2008). Digia annonce vouloir étendre le support de Qt à Android, iOS et Windows 8.

Qt 5.0 est sorti le . Bien que marquant des changements majeurs sur bien des points (rôle important de QML et de JavaScript pour la création des interfaces graphiques avec Qt Quick, séparation en modules indépendants pour faciliter les livraisons, couche d'abstraction pour faciliter les portages, etc.), le passage à Qt5 casse au minimum la compatibilité au niveau des sources. De cette façon, le passage est bien plus facile que pour Qt4.

Les versions Qt 5.x utilisent des " nécessaires au déploiement du programme.

Qt Quick est un framework libre développé et maintenu par Digia faisant partie de la bibliothèque Qt. Il fournit la possibilité de créer des interfaces utilisateur personnalisables et dynamiques avec des effets de transition fluides de manière déclarative. Ce type d'interface dynamique est de plus en plus commune, notamment sur les smartphones. Qt Quick inclut un langage de script déclaratif appelé QML comparable au XAML créé par Microsoft pour sa bibliothèque WPF. 

Qt Quick et QML sont officiellement supportés depuis Qt 4.7 (avec Qt Creator 2.1).

Le projet d'environnement graphique KDE a dès le début utilisé la bibliothèque Qt. Mais avec le succès de cet environnement, une certaine partie de la communauté du logiciel libre a critiqué la licence de Qt qui était propriétaire et incompatible avec la GNU GPL utilisée par KDE. Ce problème fut résolu par la société Trolltech qui mit les versions GNU/Linux et UNIX de Qt sous licence GNU GPL lorsque l'application développée était également sous GNU GPL. Pour le reste, c'est la licence commerciale qui entre en application. Cette politique de double licence a été appliquée uniquement pour GNU/Linux et UNIX dans un premier temps, mais depuis la version 4.0 de Qt, elle est appliquée pour tous les systèmes.

Créée en juin 1998, la fondation " est chargée de s'assurer de la disponibilité de Qt pour le développement de logiciels libres. Dans le cadre d'un accord avec Trolltech, cette fondation a le droit de diffuser Qt sous une licence de style BSD dans le cas où Trolltech cesserait le développement de la version libre pour diverses raisons, y compris un dépôt de bilan. Le rachat de Trolltech par Nokia le ne remet pas en cause la politique de double licence, l'entreprise finlandaise soutient même KDE.

Le , Trolltech annonce que la version 3 et 4 de Qt sont à partir de cette date sous licence GPLv2 et GPLv3. Ce changement de version ne s'applique que pour les versions libres de Qt. Ainsi la version 3 de Qt pour Windows, qui n'est pas libre, ne voit pas sa licence changer. Ce changement s'inscrit dans le désir de KDE de passer également en version 3 de la GPL, en plus de la version 2 déjà utilisée.

Un an plus tard, le , Trolltech annonce qu'à partir de Qt 4.5, Qt sera également disponible sous licence LGPL v2.1. Cette nouvelle licence permet ainsi des développements de logiciels propriétaires, sans nécessiter l'achat d'une licence commerciale auprès de Qt Development Frameworks. Ce changement, voulu par Nokia pour faire en sorte que Qt soit utilisé par un maximum de projets, est rendu possible par le fait que Nokia peut se passer des ventes des licences commerciales, contrairement à Trolltech qui ne pouvait pas se priver de cette source de revenus.

L'API Qt est constituée de classes aux noms préfixés par "Q" et dont chaque mot commence par une majuscule (ex: codice_1), c'est la typographie "camel case". Ces classes ont souvent pour attributs des types énumérés déclarés dans l'espace de nommage codice_2. Mis à part une architecture en pur objet, certaines fonctionnalités basiques sont implémentées par des macros (chaîne de caractères à traduire avec codice_3, affichage sur la sortie standard avec codice_4…).

Les conventions de nommage des méthodes sont assez semblables à celles de Java : le "lower camel case" est utilisé, c'est-à-dire que tous les mots sauf le premier prennent une majuscule (ex: codice_5), les modificateurs sont précédés par codice_6, en revanche les accesseurs prennent simplement le nom de l'attribut (ex : codice_7) ou commencent par codice_8 dans le cas des booléens (ex : codice_9).

Les objets Qt (ceux héritant de codice_10) peuvent s'organiser d'eux-mêmes sous forme d'arbre. Ainsi, lorsqu'une classe est instanciée, on peut lui définir un objet parent. Cette organisation des objets sous forme d'arbre facilite la gestion de la mémoire car avant qu'un objet parent ne soit détruit, Qt appelle récursivement le destructeur de tous les enfants .

Cette notion d'arbre des objets permet également de débugger plus facilement, "via" l'appel de méthodes comme codice_11 et codice_12 .

Le "moc" (pour "") est un préprocesseur qui, appliqué avant compilation du code source d'un programme Qt, génère des "meta-informations" relatives aux classes utilisées dans le programme. Ces meta-informations sont ensuite utilisées par Qt pour fournir des fonctions non disponibles en C++, comme les signaux et slots et l'introspection.

L'utilisation d'un tel outil additionnel démarque les programmes Qt du langage C++ standard. Ce fonctionnement est vu par Qt Development Frameworks comme un compromis nécessaire pour fournir l'introspection et les mécanismes de signaux. À la sortie de Qt 1.x, les implémentations des templates par les compilateurs C++ n'étaient pas suffisamment homogènes.

Les signaux et slots sont une implémentation du patron de conception observateur. L'idée est de connecter des objets entre eux "via" des signaux qui sont émis et reçus par des slots. Du point de vue du développeur, les signaux sont représentés comme de simples méthodes de la classe émettrice, dont il n'y a pas d'implémentation. Ces « méthodes » sont par la suite appelées, en faisant précéder « emit », qui désigne l'émission du signal. Pour sa part, le slot connecté à un signal est une méthode de la classe réceptrice, qui doit avoir la même signature (autrement dit les mêmes paramètres que le signal auquel il est connecté), mais à la différence des signaux, il doit être implémenté par le développeur. Le code de cette implémentation représente les actions à réaliser à la réception du signal.

C'est le MOC qui se charge de générer le code C++ nécessaire pour connecter les signaux et les slots.

Qt Designer est un logiciel qui permet de créer des interfaces graphiques Qt dans un environnement convivial. L'utilisateur, par glisser-déposer, place les composants d'interface graphique et y règle leurs propriétés facilement. Les fichiers d'interface graphique sont formatés en XML et portent l'extension ".ui" .

Lors de la compilation, un fichier d'interface graphique est converti en classe C++ par l'utilitaire codice_13. Il y a plusieurs manières pour le développeur d'employer cette classe  :

Qt se voulant un environnement de développement portable et ayant le MOC comme étape intermédiaire avant la phase de compilation/édition de liens, il a été nécessaire de concevoir un moteur de production spécifique. C'est ainsi qu'est conçu le programme codice_17.

Ce dernier prend en entrée un fichier (avec l'extension codice_18) décrivant le projet (liste des fichiers sources, dépendances, paramètres passés au compilateur, etc.) et génère un fichier de projet spécifique à la plateforme. Ainsi, sous les systèmes UNIX codice_17 produit un Makefile qui contient la liste des commandes à exécuter pour génération d'un exécutable, à l'exception des étapes spécifiques à Qt (génération des classes C++ lors de la conception d'interface graphique avec Qt Designer, génération du code C++ pour lier les signaux et les slots, ajout d'un fichier au projet, etc.).

Le fichier de projet est fait pour être très facilement éditable par un développeur. Il consiste en une série d'affectations de variables. En voici un exemple pour un petit projet:

Ces déclarations demandent que l'exécutable soit nommé "monAppli", donne la liste des fichiers sources, en-têtes et fichiers d'interface graphique. La dernière ligne déclare que le projet requiert le module SQL de Qt.

Qt intègre son propre système de traduction, qui n'est pas foncièrement différent dans le principe de la bibliothèque gettext. Selon le manuel de Qt Linguist, l'internationalisation est assurée par la collaboration de trois types de personnes : les développeurs, le chef de projet et les traducteurs .

Dans leur code source, les développeurs entrent des chaînes de caractères dans leur propre langue. Ils doivent permettre la traduction de ces chaînes grâce à la méthode codice_20. En cas d'ambiguïté sur le sens d'une expression, ils peuvent également indiquer des commentaires destinés à aider les traducteurs.

Le chef de projet déclare les fichiers de traduction (un pour chaque langue) dans le fichier de projet. L'utilitaire codice_21 parcourt les sources à la recherche de chaînes à traduire et synchronise les fichiers de traduction avec les sources. Les fichiers de traductions sont des fichiers XML portant l'extension ".ts".

Les traducteurs utilisent Qt Linguist pour renseigner les fichiers de traduction. Quand les traductions sont finies, le chef de projet peut compiler les fichiers ".ts" à l'aide de l'utilitaire codice_22 qui génère des fichiers binaires portant l'extension ".qm", exploitables par le programme. Ces fichiers sont lus à l'exécution et les chaînes de caractères qui y sont trouvées remplacent celles qui ont été écrites par les développeurs.

La bibliothèque embarque divers thèmes de widgets qui lui donnent une bonne intégration visuelle sur toutes les plateformes. Sur les environnements de bureau GNOME, Mac OS X et Windows les applications Qt ont ainsi l'apparence d'applications natives.

Qt permet de personnaliser l'apparence des différents composants d'interface graphique en utilisant le principe des feuilles de style en cascade (CSS) .

Qt Development Frameworks fournit un ensemble de logiciels libres pour faciliter le développement d'applications Qt :

Même si Qt Creator est présenté comme l'environnement de développement de référence pour Qt, il existe des modules Qt pour les environnements de développement Eclipse et Visual Studio . Il existe d'autres EDI dédiés à Qt et développés indépendamment de Nokia, comme QDevelop et Monkey Studio.

Des "bindings" existent afin de pouvoir utiliser Qt avec d'autres langages que le C++. Ainsi les langages Ada, C# et Visual Basic (Qt#), D, F# (Qt#), Java (Qt Jambi), OCaml, Perl, PHP, Python, Ruby (QtRuby), Scheme peuvent être utilisés.

De plus en plus de développeurs utilisent Qt, y compris parmi de grandes entreprises. On peut notamment citer : Google, Adobe Systems, Skype ou encore la NASA. Le site de Digia recense les entreprises utilisant Qt et les applications basées sur Qt.

Le classique "Hello World" dans différents langages : C++, Java (avec QtJambi), Python (avec PyQt et PySide) et C# (avec Qyoto) :

Étudions ces lignes.

codice_23 : inclut la bibliothèque codice_24, il faut dans le .pro rajouter codice_25 ou bien remplacer codice_24 par codice_27.

codice_28: Idem qu'en haut, avec la bibliothèque codice_29 (en l'utilisant, on peut ne pas inclure codice_16, car codice_29 l'utilise).

codice_32 : type_fonction nom_fonction(argument1, argument2) { (début de la fonction).

codice_33 : On crée un objet codice_24 ayant pour nom codice_35, avec les arguments codice_36 et codice_37.

codice_38 : on crée un objet codice_29 ayant pour nom bouton et pour argument codice_40.

codice_41 : on appelle la méthode show de l'objet bouton (par défaut le bouton est invisible).

codice_42 : on appelle la méthode codice_43 de codice_35, et on retourne son résultat.






</doc>
<doc id="14083" url="https://fr.wikipedia.org/wiki?curid=14083" title="Base d'or">
Base d'or

En mathématiques, la base d'or est le système de numération utilisant le nombre d'or, à savoir formula_1 comme base. Ce système de est également désigné plus rarement comme « développement phinaire » (car le symbole pour le nombre d'or est la lettre grecque « phi »), mais aussi « système de numération de Bergman ». Tout nombre réel positif possède une représentation standard en base φ où seuls les chiffres 0 et 1 sont utilisés, et où la suite « 11 » est évitée. Une représentation non standard en base φ avec ces deux chiffres (ou avec d'autres chiffres) peut toujours être réécrite en forme standard, en utilisant les propriétés algébriques du nombre φ — c'est-à-dire que φ + 1 = φ.
Par exemple 11 = 100. Malgré l'usage d'une base irrationnelle, tous les entiers naturels possèdent une représentation unique en développement fini dans la base φ. Les réels positifs qui possèdent une représentation finie (avec une quantité finie de 0 et 1) dans la base phinaire sont les entiers de ℚ() positifs.

Les autres nombres positifs possèdent des représentations standards infinies en base φ, les nombres rationnels positifs ayant des représentations récurrentes. Ces représentations sont uniques, excepté celles des nombres qui ont un développement fini ainsi qu'un développement non fini (de la même manière qu'en base dix : 2,2 = 2,199999… ou 1 = 0,999…).

Cette base est présentée en 1957 par . À cette époque, George Bergman entrevoit peu d'utilisations pratiques de son système mais pense ouvrir un nouveau champ d'investigation en théorie des nombres mais depuis, l'étude de la base d'or a produit des fruits en informatique, notamment pour la conception de convertisseurs analogiques digitaux et de processeurs tolérants au bruit.

Par la suite, par analogie avec l'écriture décimale positionnelle, on notera 
formula_2
Si, pour tout "i" inférieur ou égal à "n", "a" appartient à {0, 1} et ("a", "a") est différent de (1, 1), l'écriture sera appelé écriture phinaire standard ou minimale, ou la plus simple. 

On utilisera parfois, de manière intermédiaire, la notation dans le cas où, pour tout "i" inférieur ou égal à "n", "a" appartient à {0, 1} mais où la série 11 peut apparaitre ou même dans le cas où les "a" sont choisis dans un autre ensemble que {0, 1}. On parlera alors de représentations phinaires non standards.

Certaines de ces représentations non standards sont également étudiées : celle ne comportant que des 0 et des 1 mais sans 0 consécutifs, ou bien celle ne comportant que des 0 et 1 mais dont la longueur est minimale.

Tout nombre ayant une représentation phinaire finie composée de 0 et de 1 possède également un développement phinaire standard obtenu en "chassant" successivement les couples 11, par la gauche, en remplaçant 11 par 100

Exemple : 110101,1101 = 1000101,1101 = 1000110,0101 = 1001000,0101.

Dans une représentation phinaire standard, il est possible de remplacer un chiffre 1 par un chiffre 0, ou un chiffre 0 par un chiffre 1 tout en conservant une représentation phinaire non standard ne comportant que des 0 et de 1.


Ces deux techniques offrent un moyen simple d'ajouter ou retrancher une puissance de φ à un nombre écrit en représentation standard en passant provisoirement en représentation non standard.


Elles permettent de montrer que l'ensemble des nombres possédant une représentation phinaire standard finie est stable par addition et par soustraction.

Bergman utilise la technique de remplacement de 1 par 0 pour démontrer successivement que chaque entier strictement positif possède un développement phinaire fini et déterminer les développements des premiers entiers

Une multiplication par φ consistant seulement à décaler tous les chiffres d'un cran vers la gauche, tout nombre s'écrivant bφ où b est un entier strictement positif possède aussi un développement phinaire fini.

La stabilité par addition et soustraction prouve que tout nombre strictement positif s'écrivant "a" + "b"φ , avec "a" et "b" entiers relatifs, possède un développement phinaire standard fini. Tout élément de ℤ[φ], c'est-à-dire tout entier de ℚ() possède donc un développement phinaire standard fini.

Comme, d'autre part, on peut démontrer que, pour tout entier relatif "n", φ = "F" + "F"φ, où ("F") désigne la suite de Fibonacci généralisée aux indices négatifs, toute puissance de φ est élément de ℤ[φ], et tout nombre possédant un développement phinaire standard fini est élément de ℤ[φ].

Les nombres possédant un développement phinaire standard fini sont donc les éléments strictement positifs de ℤ[φ].

Les techniques de substitutions décrites précédemment permettent, dans une addition, d'éviter d'ajouter deux chiffres 1 dans une même colonne, ou d'avoir à soustraire 1 dans une colonne où figure un 0. Par exemple

Mais on peut également utiliser un développement phinaire intermédiaire non standard utilisant des 2 et des –1 (notés ) que l'on transformera ensuite sous forme standard en utilisant les techniques 

Pour l'addition de deux nombres en base φ, il suffit d'ajouter chaque paire de chiffres, sans retenue, puis de convertir le nombre en forme standard. Pour la soustraction, on déduit chaque paire de chiffres sans retenue, puis on convertit le nombre en forme standard. Pour la multiplication, on peut multiplier de façon habituelle, sans retenue, puis convertir le nombre en forme standard.

Par exemple :

Bergman propose une méthode pas à pas décrite ci-dessus pour déterminer les développements phinaires des entiers de 1 à "N" mais fait aussi remarquer que les techniques opératoires décrites ci-dessus offrent des moyens d'arriver plus rapidement au résultat.

Exemple : 70 = 10 × 7 = 10100,0101 × 10000,0001 = 101000101 +1,01000101= 101000101 +0,11110101= 101000101,11110101=101001000,10010101

Il existe également un algorithme glouton pour trouver le développement phinaire de "N" (dans la suite on notera E("x") la partie entière de "x" et {"x"} sa partie fractionnaire ou décimale) : 
Cet algorithme est équivalent à celui consistant à ôter à "N" la plus grande puissance de φ inférieure à "N" et à recommencer sur le nombre obtenu. 
On peut démontrer que cet algorithme s'arrête si "N" est un entier.

La procédure ci-dessus ne donnera jamais la suite « 11 », puisque 11φ = 100φ, donc obtenir un « 11 » signifie que l'on a manqué un « 1 » auparavant.

On peut enfin se servir des nombres de Lucas ("L"). Pour un entier "N" supérieur à 3, on cherche le plus grand entier "k" tel que "L" ≤ "N", puis on recommence le processus sur "N – L" tant que le nombre obtenu est supérieur à 3. On écrit ainsi "N" comme somme de nombres de Lucas. Comme "L" = φ + (–φ), on obtient un développement phinaire non standard comportant des 1 et des –1 séparés par des 0, développement qu'il suffit de standardiser.

Exemple : 70 = 47 + 18 + 4 + 1 = "L" + "L" + "L" + "L" = 101001010,000101 = 101001000,10010101.

Dans une écriture phinaire finie, formée de 0 et de 1, le nombre de 1 indique le nombre de puissances de φ utilisées pour écrire le nombre en question. L'écriture phinaire standard est celle qui minimise le nombre de 1.

La suite qui donne, pour chaque entier "N", le nombre de puissances de φ nécessaires à son écriture est la . On suppose, sans l'avoir encore démontré que ce nombre est toujours supérieur ou égal au nombre de termes de la représentation de Zeckendorf de "N".

L'algorithme glouton précédent appliqué à un réel positif quelconque fournit un développement phinaire standard éventuellement illimité, pour tout réel "x" positif. 

Réciproquement, tout développement phinaire converge vers un réel car la somme des termes pour les indices négatifs correspond à une série dont le terme général, positif, est inférieur ou égal à (1/φ), terme général d'une série convergente.

Comme pour le développement décimal où le développement de 0,99999… converge vers le nombre 1, on observe que tout développement phinaire qui se conclut par une alternance infinie de 0 et de 1 converge vers un nombre qui possède également un développement phinaire fini. Ce phénomène provient du fait que 0,1010101… = 1. Il existe plusieurs manières de justifier cette affirmation : 

Il est possible de diviser un entier a par un entier b écrits tous deux en base d'or en utilisant la méthode de la division longue. Les quotients successifs étant de 0 ou 1, le travail consiste seulement à savoir effectuer des soustractions. 

Exemple : division de 100 par 1001

Ici, on obtient le même reste ; la division se poursuit indéfiniment et le quotient de 100 par 1001 est 0,01001001001...

Puisqu'il existe un nombre fini de restes possibles, le quotient d'un entier "a" par un entier "b" a un développement phinaire fini (si le quotient est entier) ou périodique.

De même, comme tout élément positif du corps ℚ() — égal à ℚ(φ) — est le quotient d'un élément de ℤ[φ] (de développement fini) par un entier (de développement fini), il est de développement fini ou périodique.

Réciproquement, tout développement périodique correspond à un élément de ℚ(). En effet, si la période est de longueur "k" et commence à l'indice "j", le réel en question s'écrit 
où "A" et "B" sont à développements finis. Il s'agit de sommes, produits et quotients d'éléments de ℚ(). Le réel "x" est bien élément de ℚ().

La représentation phinaire est liée aux suites de Fibonacci ("F") et de Lucas ("L"). En effet, la suite des puissances de φ, la suite de Fibonacci et les nombres de Lucas vérifient tous trois la relation de récurrence : "u" = "u" + "u". Les termes des suites de Fibonacci et de Lucas s'expriment à l'aide de puissances de φ. Les puissances de φ s'expriment à l'aide de la suite de Fibonacci généralisée éventuellement aux nombres négatifs :
formula_8
Il en est de même des nombres de Lucas :
formula_9

De ces deux relations naissent deux théorèmes concernant la représentation phinaire standard des nombres entiers :

Soit "N" un nombre dont la représentation phinaire standard est finie :
formula_10

"N" est un entier si et seulement si formula_11

"N" est un entier si et seulement si formula_12

Ces propriétés sont utiles pour le contrôle des informations dans un système informatisé.



</doc>
<doc id="14084" url="https://fr.wikipedia.org/wiki?curid=14084" title="Mot-valise">
Mot-valise

Un mot-valise est un mot formé par la fusion d'au moins deux mots existant dans la langue de telle sorte qu'un de ces mots au moins y apparaisse tronqué, voire méconnaissable. Il peut s'agir d'une haplologie : une même syllabe constitue à la fois la fin d'un mot et le début d'un autre, et le procédé consiste alors à les accoler sans répéter cette partie commune, d'autres fois, un seul des mots se voit amuï.

Le mot-valise se distingue du mot composé et du mot dérivé par la troncation (abrègement de mots par la suppression d'au moins une syllabe). Il se distingue également de l'amalgame sémantique des éléments des mots d'origine, et par le fait que ceux-ci ne sont plus, par conséquent, immédiatement identifiables. Il peut également se distinguer par une lettre ou (de préférence) une séquence de lettres ou une syllabe commune aux deux éléments : la charnière de la valise.

Le but du mot-valise est de faire un jeu de mots ou d'enrichir la langue. C'est un phénomène proche de l’"orthographe fantaisiste".

Le terme « mot-valise » (traduction de l'anglais "portmanteau word") semble résulter de la transposition en français du jeu inventé par l'écrivain anglais Lewis Carroll pour montrer, dans son célèbre roman "De l'autre côté du miroir" (1871), l'intérêt des mots télescopés. Il utilise à cet effet l'image du "portmanteau", désignant dans l'anglais d'alors une valise rigide, en cuir, s'ouvrant en deux compartiments à la façon d'un livre : un seul mot suffit pour dire deux choses à la fois.

Au chapitre 6, Humpty Dumpty (l'œuf Gros Coco) explique à Alice la signification du mot « "slithy" » (« slictueux ») qu'elle a lu au début du poème "Jabberwocky" : 
Le mot-valise, appelé techniquement amalgame lexical, est connu depuis le (Rabelais a par exemple créé le mot « sorbonnagre » en amalgamant « sorbonne » et « onagre »). De nombreux mots-valise sont entrés dans le langage courant, mais il est courant d'en créer de nouveaux par jeu (amalgames fantaisistes).

Plusieurs termes existent pour nommer un amalgame lexical : mot porte-manteau, mot-centaure, mot-tiroir, mot-gigogne, etc. La définition du mot-valise varie selon les linguistes ; dans son acception la plus large, c'est un assemblage d'au moins deux léxèmes dont un au moins perd une partie de son signifiant.

Sur cette base, il est possible d'ajouter plusieurs contraintes. La plus courante, la contrainte morphologique, impose d'assembler deux mots sur une syllabe commune, appelée « charnière », avec apocope du premier et aphérèse du second : le mot-valise calligramme est un assemblage de calligraphie et d'idéogramme, les deux mots partageant la syllabe « gra ». La contrainte morpho-phonologique requiert un segment commun, qui peut être une seule lettre (comme Bollywood, croisement de Bombay et Hollywood par la lettre « o »).

La contrainte sémantique impose que les mots assemblés aient un sens commun : par exemple, le mot-valise « infobulle », assemblage d'information et de bulle dans le sens de phylactère.

La construction d'un mot-valise se fait par troncation d'un mot existant puis composition avec d'autres mots ou d'autres troncations. Les termes linguistiques qui se rapportent à la troncation sont : l'apocope (suppression de phonèmes à la fin du mot), l'aphérèse (suppression de phonèmes au début du mot) et la syncope (suppression de phonèmes au milieu du mot).

Comme tout néologisme, les mots-valises peuvent fournir une solution alternative aux emprunts lexicaux, notamment aux anglicismes :

Les mots-valises ne sont pas tous des créations récentes :

En linguistique, le terme peut être utilisé comme synonyme plaisant de "forme contractée" (forme unique issue de deux lexèmes qu'on ne peut plus reconnaître : "à" + "le" → "au", "de" + "les" → "des" en français, "in" + "dem" → "im" en allemand, etc.). De la même manière, un morphème porte-manteau est un morphème qui porte simultanément plusieurs significations : par exemple, le morphème anglais "-s" porte les significations : indicatif + présent + troisième personne + singulier.

Cette forme de néologisme créée par contraction d'expressions n'est pas propre au français et existe dans de nombreuses langues.

La création de mots-valises permet un nombre illimité de combinaisons, ce qui ne peut manquer de séduire les écrivains et les passionnés de jeux de langage :

Lewis Carroll a ouvert la voie pour les poètes et la poésie, qu’emprunteront en France aussi bien Raymond Roussel et Antonin Artaud que Michel Leiris (avec son ), et les oulipiens dont, bien sûr, Marcel Duchamp et Raymond Queneau. Ce dernier, dans les "Fleurs bleues" fait ainsi dire à Lalix : .
Boris Vian inventa de même le « pianocktail » de "L'Écume des jours", objet onirique qui unit deux plaisirs sensuels, le gustatif et l'auditif, grâce à l'ivresse de l'alcool et celle du jazz.

Le jeu peut alors devenir définitionnel :

De nombreux auteurs littéraires créent des mots-valises :

Certains l'emploient de façon ludique :

Dans son roman "1984", George Orwell a élaboré le novlangue (déjà un mot-valise), dont l'objectif était, grâce à la simplification lexicale et syntaxique de la langue, d'asservir la pensée elle-même. Exemples de mots-valises en novlangue :

Ce procédé littéraire, comme de nombreux autres, est souvent utilisé pour nommer des produits ou des marques.
Ceci sert à évoquer deux idées dans l'esprit du consommateur, et à améliorer l'image du produit par cette association d'images. Exemple : « Craquotte » (Craquante + biscotte) ou « Pom'pote » (Pomme + compote).

Dans les jeux vidéo, bandes dessinées, livres fantastiques et autres œuvres se passant dans un monde imaginaire fantastique, des mot-valises sont régulièrement utilisés dans la nomenclature du bestiaire et pour certains objets. 

L'exemple le plus frappant est celui de Pokémon (lui-même mot-valise issu de Pocket et Monsters, pour « monstre de poche »), où la plus grande partie du bestiaire est nommée de la sorte, de manière plus ou moins reconnaissable, d'après le physique ou le caractère :

Les "keypers" sont des jouets renfermant une cachette fermée à clé : le terme est un composé des mots anglais "key" (clé) et "keeper" (gardien).





</doc>
<doc id="14086" url="https://fr.wikipedia.org/wiki?curid=14086" title="Acrostiche">
Acrostiche

Un acrostiche, du grec "akrostikhos" ("akros", haut, élevé et "stikhos", le vers), est un poème, une strophe ou une série de strophes fondés sur une forme poétique consistant en ce que, lues verticalement de haut en bas, la première lettre ou, parfois, les premiers mots d'une suite de vers composent un mot ou une expression en lien avec le poème.

egis ussu antio t eliqua anonica rte esoluta c’est-à-dire : 

en dédicace de son « Offrande musicale » dédiée au roi de Prusse mélomane Frédéric II. Le mot formé, RICERCAR, désigne une forme archaïque de la fugue, forme des pièces dont se compose l'œuvre en question.

Un acrostiche a été découvert dans une pièce de Pierre Corneille : "Horace" () :

Il n'est pas possible de déterminer avec certitude si ce message est volontaire ou non de la part du dramaturge, le fait que le numéro du vers à partir duquel il est observable soit uniforme (444) tend cependant à prouver son intentionnalité. Si toutefois ce n'était pas le cas, on pourrait parler de kakemphaton.

François Villon signait parfois ses ballades en mettant un acrostiche dans l’envoi. C’est le cas de la "Ballade de la Grosse Margot", de la "Ballade de bon conseil", de la "Ballade des contre vérités", du "Débat du cœur et du corps de Villon". Ou de la "Ballade pour prier Notre Dame", tirée du :

Certains commentateurs s'appuient même sur l'acrostiche un peu défectueux de l'une des ballades en jargon du manuscrit de Stockholm pour attribuer la paternité de celle-ci à Villon, tandis que d'autres ne trouvent pas l'argument probant.

Ce poème et sa réponse sont des acrostiches. L'attribution à Alfred de Musset et George Sand est contestée par "les amis de George Sand" (association déclarée au J.O. 16 - 17 juin 1975 et placée sous le patronage de la Société des Gens de lettres)

La réponse :

La réponse :

Ou :

Auguste Mangeot publia dans "Le Monde musical" le sonnet suivant qu'il trouvait admirable bien qu'adressé par un correspondant anonyme : 

<poem>
Musique, tu me fus un palais enchanté
Au seuil duquel menaient d'insignes avenues
Nuit et jour, des vitraux aux flammes continues,
Glissait une adorable et vibrante clarté.
Et des chœurs alternant, – dames de volupté,
Oréades, ondins, faunes, prêtresses nues, –
Toute la joie ardente essorait vers les nues,
Et toute la langueur et toute la beauté.
Sur un seul vœu de moi, désir chaste ou lyrique,
Ta fertile magie a toujours, ô musique :
Bercé mon tendre songe ou mon brillant désir.
Et quand viendra l'instant ténébreux et suprême,
Tu sauras me donner le bonheur de mourir,
En refermant les bras sur le Rêve que j'aime !
</poem>

Mal lui en prit car le poème contenait un acrostiche : « Mangeot est bête » et son auteur n'était autre que Willy avec lequel il s'était violemment querellé.

Acrostiche du poète et ménestrel Adenet le Roi intitulé ""La Roysne de France Marie et Madame Blanche"". 

Les deux personnages sont : La reine de France Marie de Brabant (1254-1321) épouse du roi Philippe III le Hardi et Blanche de France (1253-1320) fille de Saint-Louis.

<poem>
Les dames qui ce me contèrent
Afaire ceft livre monstrèrent
Royaurnent leur humilité.
Or me doinst Diex que à leur gré
Yaie ma paine emploié.
Se li pri qu'il m'y aie;
Nommer les vueil, qu'en couvent l'ai,
En celt livre, & je le ferai.
Dont me convient bien aviser
En ce que l'en ne puist trouver
Fourme ne voie qui enseigne
Riens nule qui leur nons enfeigne
A ceux qui querre les voudront,
Ne dons riens jà n'en trouveront
Chose escripte, n'en ai pas soigne,
En quoi l'on me truist en mençoigne
Mès en vérité le plaisant.
Ace fait bon estre entendant,
Riens ne vaut chofe mençoinable :
Ie me tiens à la véritable.
E Diex! donnez-moi sens par quoi
Nommer les puisse si com doi,
Maintenant, se Diex me conssaut,
Ai nommée une qui mulit vaut,
Dont me convient l'autre nommer.
A Diex! tant parfont à amer,
Mult est chescune bonne & sage
En fais, en dis & en usage 
Bien doivent à Dieu obéir
Liement, & cuer & cors offrir.
A dès mouteplieront en bien ;
Ne croi qu'en ele faille rien.
Cel don leur donna Diex sans doute :
Haïr leur sist mauvestié toute.
En leur cuers mist, ainssi le croy,
Amours pour lui amer en foy.
Nommées les ai, ce sachiez :
Ne cuit pas qu'entendu l'aiez,
Ne je ne quier ne ne l' voudroie.
</poem>
En 1228, sous la minorité de Louis IX, Blanche de Castille décide de fortifier la cité d'Angers, dont la position stratégique face à la Bretagne et à leurs alliés anglais lui vaut le qualificatif de ""Clé du Royaume"". 

En 1598, l’Édit de Nantes est préparé à Angers par Henri IV, du 6 mars au 12 avril 1598. Henri IV fait d’Angers sa capitale d’un moment. Face à la Bretagne longtemps indépendante, Angers, bien située aux marches du royaume, était une place forte d’importance tenu par son fidèle gouverneur Puycharic.

L’acrostiche accolé sur le nom d’Angers témoigne de l’importance de la cité angevine :
<poem>
Antique clef de France, 
Necteté de souffrance, 
Garant contre ennemys, 
Estappe d'asseurance, 
Recours de secourance, 
Seccurité d’amys.
</poem>

Il existe différentes formes d'acrostiches, suivant la place des lettres choisies :


L'acrostiche est employé en cryptographie : on parle alors de la stéganographie.




</doc>
<doc id="14087" url="https://fr.wikipedia.org/wiki?curid=14087" title="Identités logarithmiques">
Identités logarithmiques

Voici une liste d'identités utiles lorsqu'on travaille avec les logarithmes. Toutes sont valables à condition que les réels utilisés (formula_1, formula_2, formula_3 et formula_4) soient strictement positifs. En outre, les bases des logarithmes doivent être différentes de 1.



Ces trois identités nous permettent d'utiliser des tables de logarithme et des règles à calcul ; connaissant le logarithme de deux nombres, nous pouvons les multiplier et diviser rapidement, ou aussi bien calculer des puissances ou des racines de ceux-ci.

Formules de G. G. Gendre :
Ces formules permettent dans certains cas de calculer numériquement formula_14 en fonction de formula_15 et formula_16 en évitant des dépassements des limites numériques.



Les formules précédentes sont utilisées pour résoudre des équations dont les inconnues sont en exposant.

Cette identité est utile pour calculer des logarithmes avec des machines à calculer, car la plupart de ces dernières ne proposent que les logarithmes décimaux et naturels.

CQFD

La dernière limite est souvent interprétée comme « en l'infini le logarithme croît plus lentement que toute puissance (strictement positive) de la variable ».

Dans le cas particulier de la base e :
Où formula_42.


</doc>
<doc id="14090" url="https://fr.wikipedia.org/wiki?curid=14090" title="Groupe frère">
Groupe frère

En systématique phylogénétique, deux taxons formant ensemble un taxon monophylétique entier, sont dits groupe frère l'un de l'autre. 

Par exemple, dans cette classification les Panini (chimpanzés) et les Hominini (hommes) sont deux groupes frères et forment ensemble le taxon des homininae.

Autre exemple, les rongeurs (souris, écureuils, castors...) ont pour groupe frère les lagomorphes (lapins, lièvres), et constituent à eux deux le clade des glires. 

On peut avoir trois groupes frères ou plus, à condition qu'il y ait entre chacun un même degré de parenté. Mais dans l'absolu, ce cas ne survient que si au cours de l'évolution, une seule espèce ancêtre a donné naissance à trois espèces filles, ou plus. En vérité, il s'agit le plus souvent d'une lacune dans la connaissance des phylogénies.


</doc>
<doc id="14092" url="https://fr.wikipedia.org/wiki?curid=14092" title="Aluette">
Aluette

L’aluette, le jeu de la vache ou la vache est un jeu de cartes pratiqué dans l'ouest de la France. 

C'est un jeu de cartes par levées, pratiqué par quatre personnes – deux contre deux – avec quarante-huit cartes aux enseignes espagnoles. Il se joue avec des signes codifiés, qui permettent aux coéquipiers de se communiquer des informations sur leurs cartes durant la partie. 

On dit « jouer à l'aluette » (forme écrite), ou plus souvent encore « jouer à la vache » (forme orale), d'après le nom de l'une des cartes du jeu.

La forme la plus ancienne du mot « aluette » est « luette », dont l'origine reste incertaine. Un « jeu des luettes » est mentionné trois fois par Rabelais dans son œuvre : une première fois dans "Pantagruel" (1532) puis dans "Gargantua" (1534), enfin dans le "Cinquième livre" (qui n'est qu'attribué à Rabelais), au chapitre 22 (1564)
sans qu'il soit possible de déterminer sans ambiguïté qu'il s'agit du jeu de cartes. Et ce d’autant plus que le dictionnaire français-anglais de Randle Cotgrave (1611), très attentif au vocabulaire rabelaisien, indique, au mot « luettes » : « Little bundle of peeces of Ivorie cast loosse upon a table ; the play is to take up one without shaking the rest, or else the taker looseth. » 

L'évolution par fausse-coupe de « la luette » aurait ensuite donné « l'aluette », et les explications de type « alouette » ou « sans luette » – l'utilisation des signes rendrait le jeu muet, ce qui est faux – ne semblent pas être vraisemblables.

Le "Code des Jeux" indique : « Le jeu de l'aluette doit son nom au participe celtique "al luet", le trompé. » 
Mais l'auteur Claude Aveline ne mentionne aucune référence à l'appui de cette hypothèse.

Les cartes utilisées sont les cartes aux enseignes espagnoles, telles qu'on les faisait à Thiers, en Auvergne, jusqu'au , pour le marché espagnol. Les enseignes espagnoles sont les deniers, les coupes, les bâtons et les épées. Ces cartes sont attestées en France aux , époque à laquelle les cartiers français, surtout de Thiers, les exportaient vers l'Espagne via Nantes. Après 1700, des cartiers installés à Nantes les fabriquent aussi. Elles sont au nombre de quarante-huit : du 1 (As) au 9, le valet, la cavalière (ou reine) et le roi. 

Le dessin des cartes a suivi une longue évolution, pour être fixé au début du . Les plus fortes cartes du jeu (les luettes, les doubles et les as) ainsi que quelques faibles cartes présentent des portraits et des symboles caractéristiques, ce qui fait que le jeu de cartes est spécifique à la règle de l'aluette et est donc vendu sous ce nom. Toutefois, rien n'interdit de jouer avec un jeu espagnol si les cartes sont suffisamment bien connues des joueurs. Et comme le fait remarquer le "Code des jeux", on peut à la rigueur jouer avec un jeu aux enseignes françaises en retirant les 10 et en convenant d'une correspondance entre enseignes.

L'origine des règles du jeu d'aluette reste inconnue. Deux hypothèses s'opposent :

Les règles du jeu d'Aluette ont évolué au cours des siècles. La caractéristique la plus fondamentale est qu'il est un jeu de levées pour les levées, sans atout et où la couleur est indifférente (proche, ainsi de la bataille). L'utilisation de mimiques est le trait le plus visible du jeu mais pas le plus essentiel. Des jeux de cartes aux règles très différentes les emploient:
Toutefois, le trut ou le truc, jeu signalé dans l'ouest de la France dès le , connu aussi en Catalogne et en Amérique du Sud (truco), partage avec l'aluette le même mécanisme et la même structure de règles. Il est possible que ces deux jeux aient un ancêtre commun.

L'aluette est traditionnellement pratiquée dans les zones rurales et côtières entre la Gironde et l'estuaire de la Loire, c'est-à-dire dans la partie occidentale de la zone d'influence des patois saintongeais et poitevin, et particulièrement en son centre, dans le département de la Vendée et au Pays de Retz jusqu'à Saint-Nazaire et en Bretagne. 

Elle est jouée en famille, en tournois, dans des associations, ou abondamment dans les cafés jusque dans les années 1960. On y jouait alors encore autour de la Brière et en presqu'île guérandaise. Il était beaucoup joué également dans les ports du Cotentin, où sa pratique a disparu.


Les quatre couleurs (deniers, coupe, épée, bâton) sont de même force et il n'y a pas d'atout.

Les quarante-huit cartes, de la plus forte à la moins forte, se classent en quatre catégories :

Les « luettes » :

Les « doubles » :

16 figures : as, rois, dames (cavalières), valets.

24 cartes faibles (les « bigailles ») : du 9 au 3 (sauf le 3 de denier, le 3 de coupe, le 9 de coupe et le 9 de denier). A noter que « bigaille » signifie menue monnaie en poitevin-saintongeais et en gallo.

Le 5 de denier est appelé « bise-dur », sans que la carte ait une valeur particulière. Il représente un couple s'embrassant ou s'enlaçant, selon les époques.

À chacune de ces cartes est associée une mimique (il existe des variantes régionales) destinée à faire connaitre son jeu à son partenaire :

Les signes les plus utilisés sont souvent l'As (ouvrir la bouche ou donner autant de coup de langue qu'on dispose d'As), la Vache (moue), le Borgne (clin d'œil) et Monsieur (sourcils levés). Les autres sont dits avec « au-dessus », ou « au-dessous ». On peut utiliser « au-dessus de là-dessus » et « en dessous de là-dessous » pour signifier une différence de deux niveaux avec le signe annoncé, parfois combiné avec « après ». Certains ferment les yeux pour indiquer qu'ils possèdent Monsieur et Madame.

Exemples : as, roi, Deux de chêne : (coup de langue), et annoncer « en dessous et au-dessus de là-dessus ». Deux d'écrit et un as : petit doigt (Petit Neuf), et annoncer « en dessous de là-dessous et en dessous après ».

Un jeu faible ou très faible se signale par le signe « misère », qui consiste à lever plus ou moins l'épaule ou faire une grimace. Si le partenaire n'est pas mieux loti, l'équipe peut décider de donner le point sans jouer.



Dans le film "Habemus Papam" (2011) de Nanni Moretti, les cardinaux jouent aux cartes avec un jeu d'aluette.

Les images suivantes proviennent d'un jeu d'aluette édité par Grimaud dans la du :





</doc>
<doc id="14094" url="https://fr.wikipedia.org/wiki?curid=14094" title="Flûte">
Flûte

Terme générique, une flûte est un instrument de musique à vent dont le son est créé par l' oscillation d'un jet d’air autour d'un "biseau" droit, en encoche ou en anneau. Ce souffle peut être dirigé par un conduit ou par les lèvres de l'instrumentiste ou provenir d'une soufflerie mécanique (voir « Flûte : jeu d'orgue ») . Le plus souvent de forme tubulaire mais parfois globulaire, en graminée, en bois, en os ou en corne, mais aussi en pierre, en terre cuite, en plastique, en métal (or, argent…), en ivoire et même en cristal, la flûte peut être formée d'un ou de plusieurs tuyaux, avec ou sans trous, ou posséder une coulisse.

Dès la Préhistoire, elle se retrouve partout dans le monde sous toutes sortes de formes. En septembre 2008, plusieurs morceaux d'une flûte datant du Paléolithique supérieur (environ ans) ont été découverts dans la grotte d'Hohle Fels par M. Aymeric Brias, lui-même pratiquant, au sud-ouest de l'Allemagne, dans le Jura souabe . Cette flûte avait été fabriquée dans un radius de vautour fauve et témoigne du fait que les tout premiers Homo sapiens jouaient déjà de la musique.

La flûte de pan était utilisée en Grèce dès le . Le tin whistle est apparu au , la flûte à bec au .
Certaines, à l'époque baroque, se virent ajouter un système de clés permettant d'obstruer les trous. Cette invention, dont il est impossible de tracer l'origine, fut notamment développée par Theobald Boehm au .

On distingue :


Peu prisée pendant les 40 premières années de l'histoire du jazz en raison d'un volume sonore modeste vite étouffé par les sections de cuivres et d'autre part en concurrence directe avec la clarinette, ce n'est qu'à partir des années 1950 qu'elle éveille l'intérêt des jazzmen.

Des musiciens comme James Moody, Gigi Gryce, Frank Wess, Eric Dolphy, Herbie Mann, des chefs d'orchestre comme Count Basie, Quincy Jones et Gil Evans ont su l'imposer comme un instrument de jazz à part entière. Roland Kirk élargira les possibilités expressives de l'instrument et nombre de musiciens l'adoptent dès lors comme instrument principal alors qu'au début elle n'était que le "bonus" des saxophonistes;

John Coltrane ne s'y sera essayé qu'une seule fois dans "To be". Longtemps utilisée par la musique classique pour son caractère pastoral et poétique la flûte jazz revendique sa place à part entière dans l'espace musical de la modernité.

Ian Anderson, du groupe de rock progressif Jethro Tull, a utilisé la flûte dans ses compositions et sur scène, influencé par la technique de Roland Kirk, en pratiquant l'Over-blowing, technique consistant à chanter en superposition du son de la flûte. Le groupe de folk metal Ithilien allie des instruments traditionnels, tels que la flûte, avec une touche de metal moderne.



</doc>
<doc id="14097" url="https://fr.wikipedia.org/wiki?curid=14097" title="Règle à calcul">
Règle à calcul

La règle à calcul (ou règle à calculer) est un instrument mécanique qui permet le calcul analogique et sert à effectuer facilement des opérations arithmétiques de multiplication et de division par simple déplacement longitudinal d’un coulisseau gradué. Elle utilise pour cela la propriété des fonctions logarithmes qui transforment un produit en "somme" et une division en "différence". Elle permet également la réalisation d'opérations plus complexes, telles que la détermination de racines carrées, de cubiques, des calculs logarithmiques ou trigonométriques.

Depuis le jusqu'à l'époque moderne, les règles à calcul sont largement utilisées par les scientifiques et les ingénieurs pour les calculs approchés car elles apportent une aide appréciable pour :

Simples de conception et de fabrication, bon marché, elles sont faciles d'utilisation et apportent une précision suffisante aux calculs triviaux (typiquement 2 décimales) pourvu qu'on lui consacre le soin et la rigueur d'utilisation nécessaires. Son usage se répand au jusqu'en 1972, année ou apparaît la HP-35 suivi des premières calculatrices électroniques portables. La règle à calcul perd alors de son intérêt et devient, en quelques années, un objet obsolète.

Aujourd'hui, des règles à calcul circulaires restent parfois utilisées pour la navigation aérienne, ainsi que sur les cadrans à lunette tournante de certaines montres.


La composition des règles à calcul est variable. Il convient donc à l'utilisateur de repérer les échelles.


Pour son utilisation la plus courante (la multiplication et la division), la règle à calcul utilise des échelles logarithmiques et le principe selon lequel la somme des logarithmes de deux nombres est égale au logarithme du produit des deux nombres :

Cela se traduit par le fait que, pour multiplier deux valeurs, il suffit d’additionner leurs longueurs représentées sur la règle, et de les retrancher pour faire une division.

Pour multiplier 2 par 3, on positionne donc le 1 de la règle mobile en regard du 2 de la règle fixe, et on lit le résultat 6 sur l'échelle fixe en face du 3 de la règle mobile.

Cette opération est très facile à effectuer, mais a l’inconvénient de ne pas donner les exposants de 10 (la position de la virgule), qui doivent être trouvés par une autre méthode (généralement un calcul mental approché).

Un autre inconvénient est que le résultat est souvent hors échelle (par exemple, 2 × 6 est impossible sur le premier exemple). Dans ce cas, on procède comme sur le deuxième exemple, en alignant le nombre à multiplier, non pas avec le 1, mais avec le 10 (deuxième exemple).
Pour limiter cet inconvénient, certaines règles proposent un petit prolongement au bout de chaque échelle, ou bien des échelles décalées notées CF et DF, allant de racine de 10 à racine de 10, avec le 1 au milieu. Dans ce cas, on commence le calcul sur l'échelle classique C-D, et on le finit sur CF-DF.

Pour la division, la position des règles est la même que pour la multiplication. L'exemple illustré ci-dessus concerne aussi bien la division de 6 par 3 : en retranchant la longueur (log) de 3 à la longueur 6, on obtient la longueur 2.
Lorsque l'on vient de faire une division le 1 de l'échelle C est placé en face du résultat, et est idéalement positionné pour faire une multiplication de ce nombre par un autre.

Une fois ce nouveau nombre trouvé, on le repère grâce au curseur mobile, et on déplace la règle mobile pour placer le nouveau diviseur en face du repère pour obtenir une nouvelle division, et ainsi de suite.

On constate donc qu'on peut alterner à l'infini des multiplications et des divisions avec un minimum de déplacements des éléments de la règle.

Les règles à calcul servent aussi à trouver les carrés, les cubes, ainsi que les racines.

Le maniement est très simple. Généralement, il suffit d'utiliser le curseur et de chercher la correspondance sur l'échelle adaptée.

Pour trouver le carré d'un nombre, on place le curseur sur ce nombre sur l'échelle des unités, et on cherche son correspondant sur l'échelle des carrés. En procédant à l'inverse, on trouve, sur l'échelle des unités, la racine carrée d'un nombre lu sur l'échelle des carrés. L'exemple ci-contre montre aussi bien que la racine carrée de 2,1 (échelle A) est proche de 1,45 (échelle D), que l'inverse.

Le principal piège est de ne pas se tromper dans le choix du nombre pour une racine carrée : la racine carrée de 9 est 3, tandis que celle de 90 est d'environ 9,5. En revanche, si l'on cherche la racine de 900, on doit bien trouver 30. En pratique, il faut donc trouver combien de fois on peut retirer deux zéros pour arriver à un nombre entre 1 et 100 pour choisir la position sur l'échelle.

On procède exactement pareil pour les cubes et racines cubiques, simplement en utilisant l'échelle des cubes au lieu de l'échelle des carrés.

L'échelle CI donne les inverses de l'échelle C (ou D). Il est facile de constater, en déplaçant le curseur que 5 est en face de 2 (1/2 = 0,5) et inversement.

Cette échelle semble faire double emploi au premier abord avec les échelles C et D. En effet, pour trouver l'inverse de 5, il suffit de diviser 10 par 5 pour trouver 2.

En fait, cette échelle permet un gain de temps notable pour les calculs à la chaîne. En effet, nous avons vu qu'il est très rapide d'alterner des multiplications et divisions avec un minimum de déplacements. Dans le cas où l'on aurait plusieurs multiplications à enchaîner, il suffit de considérer une multiplication sur deux comme une division par l'inverse.

Les sinus sont de lecture simple : après repérage de l'échelle des sinus (souvent au dos de la règle mobile), on place le curseur sur l'angle désiré, et on trouve le sinus sur l'échelle D (penser à placer la virgule, en pensant qu'il s'agit d'un nombre entre 0 et 1, ce qui pose d'ailleurs un problème, puisque la règle donne des résultats de 0,1 à 1). Par exemple, le sinus de 45 devrait se trouver proche du chiffre 7.

Les sinus des petits angles (<6°, soit un sinus < 0,1, représentant le début de l'échelle D) nécessitent une échelle supplémentaire ST.

Les cosinus sont les sinus des angles complémentaires. Par exemple, le cosinus de 60° est le sinus de 30°. On se passe donc d'échelle des cosinus moyennant un calcul simple.

Les tangentes s'utilisent comme les sinus, à ceci près que l'échelle des tangentes s'arrête à 45° (la tangente de 45° est 1, ce qui est la limite de l'échelle D). N'oublions pas que les tangentes tendent vers l'infini lorsque les angles approchent de 90°. Certaines règles proposent une échelle T2 pour les grands angles 

Enfin, les cotangentes sont de même valeur que les tangentes des angles complémentaires à 90°.

Les règles à calcul comportent le plus souvent une échelle de logarithmes en base 10, sur la face avant ou arrière de la règle selon les cas.

Après identification de l'échelle (une échelle de 0 à 1 sur laquelle les chiffres sont régulièrement espacés et étiquetée L), on repère la correspondance entre l'échelle de base (notée D en général) avec l'échelle des logs.

Pour mémoire, le log est la partie décimale du nombre qu'il faut mettre en exposant à 10 pour obtenir une certaine valeur.

Par exemple, 10 vaut environ 2. Le 2 de l'échelle D doit donc correspondre au 3 de l'échelle des logs.

La lecture des échelles est un peu déroutante pour les débutants.

En effet, le nombre de graduations entre les chiffres n'est généralement pas constant d'un bout à l'autre de l'échelle, car les espaces changent, et on ne peut tasser indéfiniment les graduations au fur et à mesure que les chiffres se resserrent.

De plus, certaines échelles se lisent de gauche à droite, tandis que les autres se lisent de droite à gauche.

Comme pour compliquer tout cela, les zéros sont souvent sous-entendus, ce qui fait que, par exemple, sur l'échelle des cubes, parfois les puissances de 10 ne sont pas notées 10-100-1000, mais 1-1-1.

Enfin, il y a peu d'indications sur l'usage des échelles.

L'utilisateur doit donc utiliser son bon sens pour 

La précision d’une règle dépend de sa longueur mais aussi de la qualité de la gravure.

Les règles de 30 cm donnent une précision de l’ordre de 1/200, ce qui se traduit par deux décimales au voisinage de la valeur 2. La différence du nombre de décimales visibles en fonction de la valeur n’est qu’un simple effet d’échelle.

La qualité de gravure est primordiale pour la précision : les traits doivent être d'épaisseur identique sur toute la longueur de l'échelle, les plus fins possibles.

Certaines règles sont fausses, ce qui peut être facile à démontrer, comme dans le cas où les échelles C et D ne sont pas strictement superposables.

Avant d'utiliser une règle inconnue pour des calculs importants, il peut être utile de la tester sur quelques calculs dont les résultats sont connus et tombent juste de préférence.

Anciennement, les règles à calcul sont fabriquées en bois d’ébénisterie : buis, poirier, acajou ou ébène afin d'assurer la régularité du glissement, la stabilité de la forme et la longévité nécessaires à une utilisation répétée. L'os et l’ivoire sont réservés aux versions luxueuses. Au , le buis recouvert de celluloïd s'impose et le le métal apparait quelquefois. L’époque moderne utilise principalement des matériaux plastiques, ainsi les réglettes sont en acryliques ou en polycarbonate glissant sur des paliers en téflon. Le bambou, pour ses propriétés de stabilité dimensionnelle et de bon glissement est utilisé en Orient. Le marquage est peint, ou mieux gravé, ce qui offre une solution à la fois précise et durable mais plus onéreuse.

John Neper invente en 1614 les logarithmes, bases mathématiques de certaines fonctions des règles à calcul. 

Edmund Gunter (1581-1626) enseignait alors l’astronomie au collège de Gresham. On lui doit l’invention de plusieurs instruments géométriques, tels que le secteur à l’aide duquel on trace les lignes parfaites des cadrans solaires. Il invente l’échelle dite « de Gunter » ou règle logarithmique en 1620, qui simplifie les opérations de calcul : sur cette règle, il suffisait d'ajouter ou retirer un écart à l'aide d'un compas pour multiplier ou diviser un nombre par un facteur.

Pour simplifier cette opération, Edmond Wingate, en 1627, a l'idée de faire coulisser deux échelles séparées, l'une contre l'autre, donnant naissance au concept de la règle à calcul.

L’anglais William Oughtred invente en 1630 une règle à calcul circulaire, en transposant l'idée sous la forme de deux échelles logarithmiques dessinées sur deux cercles concentriques.

M. Milburne, vers 1670, trace les premières spirales logarithmiques. Une version moderne et aboutie est réalisée et commercialisée en France par Léon Appoullot vers 1930.

En 1654, Robert Bissaker fait prendre à l’instrument sa forme classique (baguette coulissante dans une forme fixe).

Certains attribuent le montage des deux règles à Seth Partridge. Une description de la version Partridge est donnée dans "The description and use of an instrument called the double scale of proportion", ouvrage de Partridge, Londres, 1671, existant à la Bibliothèque Nationale.

Amédée Mannheim, officier puis professeur à l'École polytechnique lui adjoint (1850) un pointeur mobile (curseur) permettant une lecture plus aisée et de « stocker » un résultat intermédiaire. La règle de type Mannheim est la première règle moderne.

L’enroulement de deux longues échelles logarithmiques sur un cylindre permit d’obtenir une précision de calcul théoriquement supérieure - Otis King en Angleterre, A. Lafay en France, tous deux vers 1921, puis Fuller. L'aspect confus et peu lisible de ces hélices logarithmiques a été cause de leur insuccès.

Vers 1950, André Séjourné, professeur en classe préparatoire aux Arts et Métiers au lycée Voltaire à Paris, perfectionne la règle à calcul normale en lui adjoignant les échelles LL1, LL2, LL3. C'est la règle à calcul Log-Log. Il est conseil auprès de la société Graphoplex pour la création de ses premières règles.

Les échelles Log-Log étaient déjà connues dans l'entre-deux-guerres, règle « Electro » avec LL2 et LL3 dès les années 1920, règle « Darmstadt » avec LL1, LL2 et LL3 en 1935. André Séjourné diffusa l'« Electro Log Log » (Graphoplex 640), qui ne fut utilisée pratiquement qu'en France.

L'usage de la règle à calcul se généralisa en France à partir de la fin de la Seconde Guerre mondiale, les marques françaises les plus répandues étaient Tavernier-Gravet, Graphoplex et parmi les règles importées, les Nestler, Aristo et Faber-Castell allemandes, les Sun Hemmi japonaises en bambou et les Pickett américaines en aluminium. Son règne se poursuivit jusqu’au milieu des années 1980 malgré l’apparition des premières calculatrices, la règle étant le seul instrument autorisé lors des examens et concours (apparition des calculatrices à mémoire). La circulaire n°86-228 du , autorisant et recommandant l’emploi des calculettes pendant les épreuves des examens, la relégua finalement au fond des tiroirs. Elle est cependant toujours autorisée en 2016 au Concours commun Mines-Ponts et au concours de l'École polytechnique.

Les règles à calcul subsistent encore dans certains métiers, comme la navigation aérienne. Certains appareils de mesures analogiques spécialisés (par exemple les posemètres) sont également équipés d’un cercle à calcul intégré pour faciliter l’utilisation des mesures.





</doc>
<doc id="14098" url="https://fr.wikipedia.org/wiki?curid=14098" title="Éco-évolution">
Éco-évolution

L'éco-évolution désigne une composante à l'intersection de l'écologie et de l'évolution. Les dynamiques éco-évolutives prennent en compte l'évolution des individus, les conséquences sur les populations et sur l'environnement qui en retour modifient l'évolution des individus. Elles peuvent être désignées par la notion de boucle de rétroaction éco-évolutive (ou feedback éco-évolutif) et sont définies par Post et Palkovacs (2009) comme étant "l’interaction cyclique entre l'écologie et l'évolution tel que les changements dans les interactions écologiques conduisent les changements évolutifs des traits des organismes qui, en retour, altère la forme des interactions écologiques, et ainsi de suite."

L'évolution des espèces se limite à considérer comme évolution, la seule évolution des espèces, ou mutation génétique, et ne prend pas en compte l'évolution des conditions de sélection qui se modifient en fonction de l'évolution des écosystèmes. Elle néglige également le fait que ce ne sont pas uniquement des espèces aptes à survivre dans des conditions données qui sont sélectionnées, mais également tout ce qui favorise la régulation et la réorganisation des écosystèmes, bouleversés par de nombreuses mutations écologiques (submersions, émersions, plissements, surrections, érosions, tropicalisations, glaciations, migrations, surgissements d'espèces nouvelles). Ce ne sont pas uniquement des organismes qui sont sélectionnés, mais des rétroactions positives et négatives qui en se stabilisant deviennent de nouvelles sources de sélection vis-à-vis des espèces et des individus.

Si les principes de l'éco-évolution semble prendre sa source dès "l'origine des espèces" de Charles Darwin (1859) lorsque Darwin considère que la sélection naturelle résulte des interactions entre les individus. Cependant il faut attendre la fin du pour considérer à nouveau conjointement l'écologie et l'évolution. En effet, lors du développement de ces deux disciplines l'un des postulats majeur a été la différence d'échelle temporelle avec un "temps écologique" court et un "temps évolutif" rapide. Cependant les nombreuses évolutions "observées" (parfois sous forme d'évolution expérimentale) sur des micro- (bactéries) et macro- (papillons, lézard) organismes ont remis en question ce postulat et ont donc poussé à étudier l'effet conjoint de l'écologie et de l'évolution, et par conséquent les effets d'interaction et de rétroaction.

 


</doc>
<doc id="14099" url="https://fr.wikipedia.org/wiki?curid=14099" title="Ido">
Ido

L’ido est une langue construite dérivée de l'espéranto ("ido" signifie « fils/fille, descendant » en espéranto, on utilise aussi le terme d'« espérantide »), proposé en 1907 par une commission à l'initiative du Français Léopold Leau. Elle est également inspirée de l'Idiom Neutral.

L'élaboration de la langue dura plusieurs années et ce n'est qu'en 1912 qu'elle arriva à maturité. Par la suite, la langue ne subit que des modifications mineures. L'élan des premières années fut brutalement interrompu par la Première Guerre mondiale. Une nouvelle impulsion fut donnée au début des années 1920 puis au début des années 1930.

L’ido fut créé au début du . De nombreux projets de langues construites précédèrent la création de l'ido. Parmi les précurseurs dans ce domaine, citons Galien, Blaise Pascal, René Descartes, Gottfried Wilhelm Leibniz, mais surtout Johann Martin Schleyer, inventeur du volapük et Ludwik Lejzer Zamenhof, père de l'espéranto. Il y eut également de nombreux projets qui firent suite à la création de l'ido tels que l'italico, le latin-ido, le weltdeutsch... 

Au début du , le besoin d'une langue internationale était ressenti par de nombreuses personnalités, notamment parmi les scientifiques et les philosophes. Cette période correspond aussi à une floraison de nouveaux projets de langue internationale. À l'initiative du mathématicien français Léopold Leau, se mit en place à partir du 17 janvier 1901 une Délégation pour l'adoption d'une langue auxiliaire internationale, qui reçut le soutien de nombreux savants. En 1906, la Délégation avait reçu le soutien de plus de  membres d'académies et d'universités de différents pays et de plus de 300 sociétés savantes. 

En mai 1907, la délégation soumit la question à l'Association internationale des Académies à Vienne, laquelle se déclara incompétente par 12 voix contre 8 et une abstention. En conséquence, la délégation forma un comité de travail dont les membres furent élus par 242 voix sur un total de 253. Ce Comité comprenait des scientifiques de renom tels que les linguistes Jespersen, Schuchardt et Baudouin de Courtenay ou encore le chimiste Ostwald. Par cooptation, d'autres personnalités furent admises, comme le mathématicien italien Peano. Léopold Leau et Louis Couturat furent les secrétaires du comité.

Le comité se réunit au Collège de France à Paris durant le mois d'octobre 1907 et examina de nombreux projets de langue internationale, présentés la plupart du temps par leurs auteurs. Il parvint rapidement à la conclusion qu'il n'existait que deux projets de langue internationale dignes d'intérêt. Le premier était l'espéranto, inchangé depuis son apparition en 1887 ; le second était l'Idiom Neutral, développé par l'ancienne Académie du Volapük. La délégation décida finalement de choisir l'espéranto, mais en y appliquant des réformes définies par le projet connu sous le nom d'«ido». 

Ce projet, dont l'auteur était anonyme au moment de sa présentation, était une sorte de synthèse entre l'espéranto et l'idiom neutral. Ainsi que le rapporte le linguiste danois Otto Jespersen, membre du comité de la délégation : 
Il est peu probable que Wilhelm Ostwald fût satisfait de l'ido, car il se retira de la présidence de la commission et créa en 1916 sa propre langue construite, le "Weltdeutsch". Le 29 septembre 1926, dans le quotidien "Vossische Zeitung", il s’expliqua sur la nécessité de créer une nouvelle langue internationale pour remplacer l'ido.

La paternité du projet "ido" est controversée. Cependant, le témoignage d'Otto Jespersen, membre du comité de travail, puis de la commission permanente, établit clairement qu'il s'agissait de Louis de Beaufront. Le philosophe français Louis Couturat connaissait probablement l'identité de l'auteur du projet. Comme le projet ido reprenait bon nombre de ses thèses sur la dérivation, l'idée se répandit qu'il en était l'auteur.

Selon Otto Jespersen, le projet ido fut présenté à l'assemblée lors de sa dernière séance par Couturat à la place de l'auteur. Personne, parmi les membres du comité, ne savait quoi que ce fût au sujet de l'auteur, si ce n'est que ce n'était ni Couturat, ni Leau, ni aucun autre membre du comité. 

La relation entre Beaufront et l'espéranto est complexe :

Reconnu comme le représentant de Zamenhof, Beaufront fut l'un des pionniers de l'espéranto en France. Il déclara :

En 1901, Zamenhof s’était fait représenter par Beaufront auprès des éditions Hachette pour publier des ouvrages d’espéranto. Cependant, la rancœur le gagna lorsque Carlo Bourlet et Théophile Cart avertirent Zamenhof du caractère excessif des pouvoirs qu’il avait accordés à Beaufront et Hachette. 

Zamenhof se serait trouvé définitivement lié à Hachette tandis que Beaufront, de concert avec l’éditeur, aurait disposé d’un droit de regard absolu sur tous les ouvrages en espéranto ou concernant cette langue, quels qu'en soient les auteurs. En faisant capoter cette affaire, Cart et Bourlet évincèrent Beaufront de son rôle de premier plan et évitèrent ainsi que l’édition en espéranto ne devienne un monopole de Hachette.

Beaufront est considéré comme un traître par les espérantophones dans la mesure où il était censé représenter l'espéranto et où il présenta l'ido le dernier jour, « à la sauvette ».

Jespersen devint président de l’Académie de l'ido et, dans la revue "Progreso", participa activement aux discussions qui visaient une amélioration constante de la langue. Toutefois, après quelques années, son activité cessa subitement, en partie parce qu’il était mécontent de la manière dont Couturat et les autres voulaient faire évoluer l’ido, mais surtout parce qu’il suspectait que Couturat — dont le rôle intrigant durant la période du comité ne lui apparut clairement que par la suite — exploitait de façon éhontée son autorité et ne le considérait que comme une marionnette.

Le comité de la délégation décida de nommer, le 24 octobre 1907, une "commission permanente" chargée « d'étudier et de fixer les détails de la langue qui sera adoptée ». Les membres de cette commission furent Louis Couturat, Wilhelm Ostwald, Otto Jespersen, Baudouin de Courtenay et Léopold Leau. Louis de Beaufront fut ultérieurement coopté « en raison de sa compétence spéciale ». 

Une Union des amis de la langue internationale ("Uniono di la Amiki di la Linguo Internaciona") comprenant une Académie et un comité directeur fut fondée et une revue mensuelle, "Progreso", fut lancée en 1908 pour publier les discussions linguistiques et les décisions de l'Académie de l'ido. Ce sont les travaux de la commission permanente qui développèrent la langue, laquelle prit le nom d'« ido » après que Zamenhof eut refusé toutes les dénominations utilisant le mot « espéranto ». 

Pour l'essentiel, l'élaboration de la langue fut achevée en 1910 avec la publication des premiers manuels et dictionnaires d'ido « conformément aux décisions prises par le comité et par la commission », comme l'atteste une déclaration de la commission signée par tous ses membres. Sa mission achevée, la délégation s'est dissoute régulièrement le 31 juillet 1910 après avoir fondé l"'Uniono por la linguo internaciona".

Les principales modifications apportées à l'espéranto dans l'élaboration de l'ido sont les suivantes (telles qu'elles ont été énoncées par le Comité de la Délégation) :


Ces points sont discutés dans Critiques de l'espéranto.

Les "idistes" se trouvèrent vite confrontés à un grave dilemme : certes, la poursuite des travaux linguistiques améliorait la qualité de la langue mais, en même temps, elle entraînait des changements incessants qui rendaient difficile sa diffusion auprès du grand public. 

Un débat opposa les partisans de la stabilité pour diffuser la langue et ceux qui souhaitaient d'abord achever le travail linguistique. C'est dans ce contexte que Louis Couturat défendit dans la revue "Progreso" une voie médiane : 
Finalement, après plusieurs années de travail intense, une période de stabilité de dix ans fut décidée en 1914 afin de faire connaître la langue le plus largement possible.

C'est en pleine croissance que le mouvement idiste fut frappé de plein fouet par deux événements tragiques. Le 3 août 1914, Louis Couturat mourut dans un accident de voiture, le jour même où l'Allemagne déclara la guerre à la France. La disparition du plus actif défenseur de l'ido, suivie du déchaînement des nationalismes et des destructions de la première guerre mondiale, porta un coup terrible à l'ido et à l'idée même de langue internationale. Le mouvement idiste ne survécut véritablement que dans les pays neutres, tels que la Suisse (avec notamment l'activité de Schneeberger, secrétaire de "l'Uniono por la Linguo Internaciona Ido") et la Suède (avec Ahlberg, éditeur de la revue idiste "Mondo"). La revue "Progreso", dont Couturat était à la fois l'éditeur et le rédacteur en chef, cessa de paraître en 1914.

Après la guerre, le mouvement idiste se reconstitue lentement. En 1918, le linguiste français Antoine Meillet, professeur au Collège de France, loue l'ido, et plus généralement l'idée de langue internationale, dans son livre "Les Langues dans l'Europe nouvelle" :
En 1920, Schneeberger, président de l'Académie de l'ido, annonce la reprise des travaux de l'Académie. Louis de Beaufront publie en 1925 sa « Grammaire complète » ("Kompleta Gramatiko Detaloza"), qui demeure encore au début du une œuvre de référence sur la grammaire de l'ido. Plusieurs congrès idistes sont organisés : Vienne (1921), Dessau (1922), Kassel (1923), Luxembourg (1924), Turin (1925) et Prague (1926). Plusieurs revues idistes paraissent.

En 1927, de sérieuses dissensions divisent le mouvement idiste. L'apparition en 1922 d'une autre langue auxiliaire, l'Occidental, et le renouveau du dilemme entre stabilité et changement affaiblissent l'ido. De vifs débats opposent les conservateurs aux réformateurs. De son côté, Otto Jespersen, qui s'était détaché du mouvement idiste, publie en 1928 son propre projet linguistique, le novial. Jespersen explique les raisons de son éloignement de l'ido : 
Le novial, qui reprend de nombreux traits de l'ido tout en éliminant les caractères les plus marquants de l'espéranto (tels que les finales -o des substantifs, -a des adjectifs ou encore la conjugaison en -is, -as, -os), attira un certain nombre d'idistes comme Ahlberg, dont la revue idiste "Mondo" se transforma en revue novialiste.

Le congrès idiste de Zurich de 1928 marque le début de la réunification du mouvement idiste. "L'Uniono por la Linguo internaciona Ido" est réactivé et un bulletin officiel est édité. La revue "Progreso" reparaît à partir de 1931 et sans interruption jusqu'à aujourd'hui. Le secrétaire et rédacteur Matejka déclare à ce propos : 
Le premier numéro du nouveau "Progreso" comprenait des articles de quelques-uns des fondateurs de l'ido tels que Wilhelm Ostwald ou Léopold Leau. Des débats sur l'opportunité de nouveaux changements eurent lieu, jusqu'à ce qu'en 1934, une nouvelle période de stabilité de dix ans soit déclarée. Par ailleurs, la situation internationale était redevenue défavorable aux langues internationales avec la montée du fascisme, du nazisme et du stalinisme, suivie de la Seconde Guerre mondiale. La première préoccupation du mouvement idiste devint la simple survie.

Le mouvement idiste survécut à la Seconde Guerre mondiale. La revue "Progreso" n'a jamais cessé de paraître, même pendant la guerre. Ensuite, le travail linguistique reprit, quoiqu'à un rythme relativement lent. L'essentiel de la langue était en effet fixé. Le besoin principal se limitait essentiellement à l'adoption de mots nouveaux, notamment pour suivre le développement des sciences et des techniques. 

La plupart des articles de "Progreso" portaient sur des sujets variés, autres que linguistiques. Une production poétique importante s'est développée en ido grâce à de nombreux auteurs, dont le principal reste le poète idiste belge Andréas Juste. Il domine le mouvement idiste dans les années 1960-1998, non seulement pour l'importance de sa production littéraire en ido, mais aussi pour le dynamisme qu'il a su insuffler au mouvement. Le fonds Andreas-Juste comprend une bibliothèque spécialisée rassemblant plus de 250 ouvrages sur l'ido ou en ido ainsi qu'un grand nombre de textes en ido. En 2009, il est géré par l'association "Juste & Co".

En ce début du XXIe siècle, le mouvement idiste est présent sur Internet. Il existe plusieurs sites, plusieurs listes de discussion en ido ou sur l'ido et une Wikipédia en ido. Les revues idistes "Progreso", "Kuriero Internaciona" et "Ido-Saluto" paraissent régulièrement. Chaque année, une rencontre idiste rassemble une quinzaine de participants. 

Le débat prit vite la forme d'un dialogue de sourds entre les idistes et les espérantistes. Cette opposition fondamentale entre deux conceptions marque encore les différences entre les deux langues : l'espéranto a l'avantage du nombre et de la notoriété, fruits du travail intense des militants, alors que l'ido est plus orienté vers la linguistique et l'amélioration constante de la langue.

L'ido a été initialement développé par modification de l'espéranto en accord avec la déclaration finale du comité de la Délégation : 
L'entente avec le Comité linguistique espérantiste n'eut pas lieu. Au contraire, cette décision donna lieu à des débats passionnés entre partisans et adversaires de la réforme de l'espéranto qui conduisit à l'ido. Ces débats, dans une certaine mesure, durent encore au début du .

Dans les années qui suivirent la naissance de l'ido, de nombreux espérantistes, notamment parmi les « cadres », adoptèrent l'ido mais les militants restèrent dans leur majorité fidèles à l'espéranto. Cependant, très peu de ces personnes restèrent fidèle à l'ido après les années 1910

Beaucoup d'espérantistes considèrent la réforme idiste comme une trahison, mot qui revient fréquemment sous la plume des espérantistes pour désigner les promoteurs de l'ido.

Cette attitude n'est pas comprise par les idistes pour qui la langue est un objet d'étude nécessitant approche scientifique, liberté d'opinion et progrès. C'est cette conception que Louis Couturat développa dans la revue "Progreso" :
Les statuts de l'Union pour la Langue internationale Ido ("Uniono por la Linguo internaciona [IDO]") déclarent notamment que l' 

Le contraste est remarquable avec le "fundamento" de l'espéranto (qui rassemble les règles de l'espéranto) qui stipule que « le fundamento doit rester rigoureusement intangible avec ses imperfections ». De nombreux théoriciens de l'espéranto, comme Théophile Cart, craignent les risques de réformes successives, notamment le risque de schisme entre les pratiquants et la quête, vaine, d'une « langue parfaite ».

La composition en ido obéit à des règles plus strictes qu'en espéranto, en particulier la formation de noms, des adjectifs et des verbes à partir d'un radical d'une autre classe. Le principe de réversibilité suppose que pour chaque règle de composition (ajout d'un affixe), la règle de décomposition (retrait de l'affixe) est valide.

Ainsi, alors qu'en espéranto un adjectif (par exemple "papera", formé sur le radical nominal "paper"o) peut signifier un attribut ("papera enciklopedio" : « encyclopédie en papier ») et une relation ("papera fabriko" : « usine à papier »), l'ido distinguera par construction l'attribut "papera" de la relation "paperala". 

Similairement, "krono" signifie en espéranto et en ido « couronne » ; là où l'espéranto autorise la formation de « couronner » par simple "kroni" (« couronnement » se dit "kronado"), l'ido impose un affixe pour que la composition soit réversible : "kronizar" (« couronnement » se dit "kronizo").

Selon Claude Piron, certaines modifications apportées par l'ido sont en pratique inutilisables et ruinent l'expression spontanée : 

Le vocabulaire de l'ido est plus proche de celui des langues occidentales que celui de l'espéranto et notamment du français, de plus celui de l'ido est plus naturaliste.

Par exemple, les « corrélatifs » de l'espéranto, construits selon la logique d'un tableau à deux entrées (schématisme) dans lequel seulement 14 éléments sont à connaître, ont été remplacés en ido par 45 mots considérés comme plus « naturels » .

D'autre part, l'utilisation de mots dits « internationaux » a contribué, plus qu'en espéranto, à l'apparition de "pseudo-préfixes", comme "bi"- pour « deux », "pre"- pour « avant ». L'ido utilise également le préfixe "mi" pour moitié.

L'ido utilise les 26 lettres de base de l'alphabet latin, sans signes diacritiques, ainsi que plusieurs digraphes. Son écriture n'est pas phonémique : outre les digrammes, certaines lettres peuvent avoir des prononciations variées. Ainsi, alors que les lettres « eu » forment généralement une diphtongue [eŭ], elles seront prononcées séparément [eu] dans "neutila" (décomposé en "ne"-"utila").

L'ido a donc rejeté les six lettres à diacritiques de l'espéranto (ĉ, ĝ, ĥ, ĵ, ŝ, ŭ) et introduit les lettres q, w, x, y inexistantes en espéranto. En 1894, Zamenhof proposait une réforme qui devait, entre autres, supprimer ces lettres accentuées. Cette réforme fut rejetée par la majorité des espérantistes lors d'un vote.

Au contraire de l'espéranto, qui pratique l'accord en nombre et en cas entre le nom et l'adjectif, l'adjectif est invariable en ido.

L'accord de l'adjectif en espéranto permet souvent une plus grande souplesse dans l'ordre des mots du groupe nominal :

Ce type de construction est ambigu en ido.

L'accord de l'adjectif permet en espéranto, parfois mais pas toujours, de lever certaines ambiguïtés. Par exemple, la phrase "mi vidis ruĝan aŭton kaj kamionon" signifie « j'ai vu une voiture rouge et un camion », alors que "mi vidis ruĝajn aŭton kaj kamionon" signifie « j'ai vu une voiture et un camion rouges ». Cependant, l'accord de l'adjectif ne permet pas de lever l'ambiguïté si les deux substantifs sont au pluriel. Dans tous les cas, l’ido et l'espéranto peuvent résoudre le problème par la répétition de l'adjectif avant chaque substantif.

L'accord en cas de l'adjectif en espéranto permet également de distinguer l'adjectif épithète d'un complément d'objet direct et l'adjectif attribut du complément d'objet direct. En espéranto, l'adjectif épithète du COD, est à l'accusatif, mais l'adjectif attribut du COD est au nominatif. Par exemple, "mi kredis fidelan amikon" signifie « j'ai cru un fidèle ami » (épithète) alors que "mi kredis fidela amikon" signifie « j'ai cru fidèle un ami » (attribut du complément d'objet direct).

L'ido résout ce point par l'introduction facultative de l'attribut grâce à la préposition "kom" : "me kredis amiko kom fidela". Dans ce cas précis, l'ido peut également préciser le sens du verbe. En effet, en espéranto comme en français, l'ambiguïté de la phrase « j'ai cru un ami fidèle » joue sur le double sens du verbe « croire » qui s'interprète comme « avoir confiance en » dans le premier cas et « estimer » dans le second. L'ido dispose ici de deux verbes "kredar" et "evaluar".

L'accord de l'adjectif en nombre et en cas comme en espéranto n'est pas toujours indispensable, ni toujours suffisant, pour éliminer les ambiguïtés de sens. L'ido y parvient aussi par d'autres moyens. 

L'espéranto possède un accusatif obligatoire marqué par l'ajout d'un -"n" à la fin du mot (nom, pronom ou adjectif, à quelques exceptions près). Il introduit tous les compléments directs (sans préposition) des verbes. Il est principalement utilisé pour introduire le complément d'objet direct (COD) et le complément circonstanciel de mouvement. Il autorise une grande liberté dans l'ordre des mots puisque les fonctions de complément d'objet direct et de sujet sont marqués par déclinaison et non seulement par position. Il distingue également le lieu vers lequel il y a mouvement du lieu où se situe l'action.

L'ido s'appuie sur un ordre commun sujet-verbe-complément, dans lequel il considère la marque de l'accusatif comme facultative. Celle-ci n'est conservée que dans les cas d'inversion du sujet et du COD (lorsque le COD précède le sujet).

En espéranto, des phrases complexes peuvent ne pas distinguer un sujet d'un COD par déclinaison ; ainsi, dans "mi vidis homon mortigi hundon" (« j'ai vu un homme tuer un chien »), le sujet de la subordonnée est COD de la principale et donc marqué de l'accusatif.

Les mots en ido sont du genre neutre par défaut. Par exemple, "frato" exprime à la fois un frère ou une sœur. En ido, c'est le suffixe "-ino" qui précise le féminin ("fratino", « sœur ») et "-ulo" pour préciser le masculin ("fratulo", « frère »).

De même en espéranto, la plupart des termes sont neutres. Le féminin se forme par l'adjonction du suffixe "-ino", le masculin par l'adjonction du préfixe "vir-", la réunion des sexes par l'adjonction du préfixe "ge-". Ainsi à partir de la racine "kat-", on forme les mots suivants :
Toutefois, certains radicaux sont sexués. Ainsi "frato" veut dire « frère » et "fratino" veut dire « sœur » (le féminin se forme par l'ajout du suffixe "-ino"). Sur ce point, certains qualifient l'espéranto de sexiste, puisque dans certaines formes le masculin l'emporte sur le féminin. De fait, l'espéranto n'a fait qu'importer les formes linguistiques des langues européennes dominantes à l'époque où l'espéranto s'est formé. Depuis différentes projets de réformes visant à introduire une symétrie totale entre les genres (utilisation du suffixe "-iĉo" pour le masculin, riisme) ont été proposés ; mais du moins jusqu'à présent, aucun de ces projets ne s'est imposé chez les espérantophones.

Lorsqu'il créa l'espéranto, Zamenhof prit le parti d'une économie maximale des radicaux. Les différentes nuances y sont rendues par l'emploi massif de compositions lexicales (assemblage de racines).

Par exemple, le mot espéranto "kulpigi" est formé sur l'adjectif "kulpa" (« coupable », « fautif ») grâce au suffixe "-igi" (« rendre », « faire devenir »). Le sens littéral du mot serait donc « rendre coupable », « faire devenir coupable ». En réalité, "kulpigi" signifie « accuser » ; le sens ne se déduit pas de l'analyse des différentes parties du mot. De même, "almiliti" (littéralement « faire la guerre en direction de ») signifie « conquérir » ("almiliti" est maintenant un archaïsme. On utilise "konkeri").

De telles définitions constituent autant d'idiotismes à mémoriser. L'économie de radicaux, voulue pour économiser l'effort de mémoire, a ici ses limites. Pour éviter cet écueil, l'ido s'est d'emblée muni d'un nombre supérieur de radicaux pour éviter ambiguïtés et idiotismes.

La différence entre un espéranto à peu de racines et un ido riche en racines différentes s'est considérablement réduite avec le temps. En effet, l'espéranto s'est progressivement enrichi pour passer officiellement de racines dans le "Fundamento" à dans le "Baza Radikaro Oficiala" du début du . Par exemple, à l'imitation de l'ido "akuzar", le verbe "akuzi" a été introduit à côté de "kulpigi" pour rendre « accuser ». 

Ainsi, l'espéranto forme de nombreux termes comme « mauvais » ou « gauche » par leur contraire préfixés par "mal"- : "malbona" = « contraire de bon », « mauvais ». À l'inverse l'ido, bien que pouvant aussi définir ces mêmes mots par leur contraire ("desgranda", "desbona", etc.), possède aussi des mots spécifiques pour définir positivement ces notions (ainsi "petit" se dit "mikra", "mal" se dit "mala", etc.). C'est ainsi que plusieurs espérantistes ont introduit des racines simples en concurrence des formes en "mal"- "ovri" est venu concurrencer "malfermi" pour signifier « ouvrir ». 

Il existe d'autre part une différence dans le traitement de certains mots. En ido, "genitoro" veut dire « parent ». Bien qu'il soit possible d'utiliser "genitorulo" et "genitorino", il est plus courant d'utiliser "patro" pour « père » et "matro" pour « mère ». C'est une voie que ne prend pas l'espéranto qui utilise un seul radical dans ce cas : "patro" pour « père », "patrino" pour « mère » (féminin de père) et "gepatroj" pour « parents ». Le préfixe "ge-" exprime "les deux genres" en espéranto comme en ido.

L'ido, à cause de sa tendance naturaliste, double la quantité de vocabulaire ainsi que le temps d'apprentissage et le risque d'oubli s'en trouve également accrue.

L'ido ne transcrit pas les noms personnels. Par exemple, "John Lennon" reste écrit de la même manière. Pour les noms utilisant un alphabet autre, la transcription se fait en respectant au plus proche la phonétique originale. L'espéranto, à l'origine, tendait à transcrire les noms personnels et les lieux. Les prénoms et les noms de lieu étaient de plus adaptés au besoin de la grammaire espéranto (-"o" final). Les prénoms étaient parfois même traduits par un équivalent à consonance généralement slave. Par exemple, "John" deviendra "Johano" en espéranto.

Cette manière de faire de l'ido a le mérite de permettre de reconnaître instantanément les noms internationaux. Mais elle pose le problème des mots dérivés de ces noms personnels, ce qui fait qu'en ido tous les mots dérivés de noms personnels auront leur orthographe d'origine qu'il faudra savoir prononcer.

En 2009, la tendance en espéranto est de conserver l'orthographe initiale, généralement mutilée par l'absence de diacritiques, voire d'employer une transcription différente (pīnyīn sans diacritiques pour le chinois, par exemple).

Il est fixe en espéranto (toujours sur l'avant-dernière syllabe). En ido, il est également toujours porté par l'avant-dernière syllabe, sauf pour les infinitifs des verbes (c'est-à-dire les mots finissant en "ar", "ir" et "or"). 

L'alternance en ido d'oxytons (les infinitifs) et de paroxytons (les autres mots) est appréciée par de nombreux locuteurs de l'ido, d'une part parce qu'elle permet de repérer plus facilement les infinitifs à l'oral () et d'autre part parce qu'elle diminue nettement la d'une accentuation absolument régulière.

Le cheminement de l'ido a été jalonné par une abondante floraison de projets de langues visant à l'améliorer :


Il existe au moins une vingtaine de descendants visant à améliorer l'ido.

René de Saussure proposa plusieurs projets visant à améliorer l'ido, qu'il regroupa sous le nom de "global Konkordio" :

L'alphabet de l'ido comporte 26 lettres.
Chaque lettre a un seul son, sauf dans les cas particuliers des digrammes. Deux consonnes ou deux voyelles identiques accolées dans un même mot se prononcent séparément (exemples : ek-kurar, ne-eleganta). Il n'y a pas de lettre muette.



Les consonnes suivantes ont la même prononciation qu'en français : b, d, f, j, k, l, m, n, p, q, t, v, z. Les autres consonnes se prononcent comme suit :



L'association de leurs deux lettres modifie la prononciation de l'ensemble et constitue un seul phonème :


Ce sont des syllabes qu'on prononce en faisant entendre, d'une seule émission de voix, le son de deux voyelles :


Important : si l'assemblage du a et du u ou du e et du u résulte du rapprochement d'un préfixe ou d'un suffixe avec un radical, les lettres doivent être prononcées séparément (exemples : neutila [ne-utila], et kreuro [kre-uro].

Dans les couples de lettres suivants, la lettre u est prononcée 'w' :


Certaines paires de voyelles peuvent être fondues en une seule syllabe si elles terminent un mot (cela reste facultatif mais l'accent tonique porte sur la voyelle antépénultième si elle existe) :



L'accent tonique se situe :
excepté si le mot se termine par une paire de voyelles (éventuellement suivi d'un s final) dont la première est i ou u, dans ce cas l'accent tonique est reportée sur la syllabe précédente (exemples : RAdio, akaDEmio, REvuo, STUdias).

La grammaire de l'ido est régulière, simple et comporte très peu d'exceptions (voir le paragraphe sur l'accent tonique ci-dessus).

Les mots en ido sont formés à partir d'un radical et d'une terminaison. La terminaison indique la nature du mot. Ainsi tous les noms se terminent par "-o", les adjectifs par "-a", les verbes à l'infinitif par "-ar", etc. Les mots peuvent être combinés à des préfixes ou des suffixes. 

Les noms propres peuvent ne pas suivre ces règles, comme dans le cas des pays : "Kanada, Corea, Francia, Haiti" …

Voici quelques terminaisons, incluant les principaux temps des verbes :

Les verbes en ido ont une terminaison régulière. Ainsi la terminaison -"as" sera la même pour toutes les personnes : je, tu, il / elle, nous, vous, ils / elles.

Vu s'emploie par politesse ou par déférence, comme pronom de la seconde personne du singulier, au lieu de tu. 

À la troisième personne, on peut utiliser ol(u) au singulier et oli au pluriel pour désigner une (des) chose(s) ou un (des) animal(aux).

L'ido possède un pronom sans distinction de genre qui n'existe pas en français (l'emploi du masculin prévalant dans l'ambiguïté en français). Ainsi en ido, si l'on ne désire pas spécifier le genre (ou s'il est inconnu), on utilisera le pronom lu au singulier et li au pluriel (non spécifique ou groupe mixte).

L'ordre des mots dans une phrase est en général : sujet, verbe, complément(s).

La négation est indiqué par le mot "ne" en avant du verbe. "Me ne havas libro" se traduit par « Je n'ai pas de livre », et ainsi de suite pour : "Me ne…" (Je ne…), "Il ne" (Il ne…), et "Li ne" (Ils ne…). Et de même qu'au passé et au futur : "Me ne iris" (Je ne suis pas allé), "Me ne iros" (Je n'irai pas).

Une question commence par "ka" : "Ka me havas libro ?" (Est-ce que j'ai un livre ?). La question se termine par un point d'interrogation, comme en français.

Les racines de la langue ido proviennent en grande partie des six langues de base : anglais, allemand, espagnol, français, italien et russe. Elles ont été sélectionnées selon le principe de l'internationalité maximale. Selon une statistique faite sur racines de l'ido (qui englobent la quasi-totalité du lexique à l'exception du vocabulaire technique ou spécialisé), elles sont réparties comme suit :


Présenté autrement :


Plus des trois-quarts des racines appartiennent à 4, 5 ou 6 des 6 langues de base. Si les racines techniques et spécialisées sont ajoutées, cette proportion serait encore plus grande, étant donné la grande internationalité de celles-ci dans les langues nationales.

Le groupe des langues slaves est relativement peu représenté en ido. 52 % des racines appartiennent aussi au russe, mais il s'agit pour l'essentiel d'emprunts du russe aux langues latines et anglo-saxonnes, non de racines slaves.

Voici un tableau de comparaison avec ces six langues : 


"Kuriero Internaciona" est un magazine publié en France. "Adavane!" est un magazine bimestriel publié en Espagne. "Progreso" est la revue officielle du mouvement idiste.




</doc>
<doc id="14100" url="https://fr.wikipedia.org/wiki?curid=14100" title="Hadîth">
Hadîth

Un hadith ou hadîth (en arabe : / "", , pluriel "ʾaḥādīṯ" ) est une communication orale du prophète de l'islam Mahomet et, par extension, un recueil qui comprend l'ensemble des traditions relatives aux actes et aux paroles de Mahomet et de ses compagnons, précédées chacune d'une chaîne de transmetteurs remontant jusqu'à Mahomet. Considérés comme des principes de gouvernance personnelle et collective pour certains courants musulmans, ils sont aussi désignés sous le nom de « la tradition du Prophète ». Les hadiths auraient été rapportés par près de compagnons.

Le muhaddith est un savant de l'islam spécialiste de la science du hadith.

Le râwî est le transmetteur de hadith, c'est l'un des chaînons de l"'isnâd". En étudiant l'isnad et la fiabilité des rouwwât (pluriel de râwî) le composant, un muhaddith peut évaluer l'authenticité d'un hadith.

En dehors de quelques hadiths « sacrés », considérés comme les paroles de Dieu adressées directement à Mahomet et rapportées par celui-ci, les hadiths sont les paroles et actions attribuées au prophète et non une parole divine.

Avec les préceptes du Coran, les hadiths forment la sunna d'où le nom d'islam sunnite pour le courant orthodoxe. Les hadiths ont été rapportés dans divers recueils par des musulmans fidèles. Certains auteurs en ont recensé plus de . Beaucoup de ces citations étant suspectes, leur crédit est fonction de l'étude du contenu et de la chaîne de transmission. Cette chaîne des témoins est appelée "isnad". Ces différents recueils alimentent en partie l'opposition entre chiites et sunnites en particulier. Il existe à ce jour environ hadiths "sahîhs", c'est-à-dire reconnus comme « authentiques » (voir plus bas).

Les Hadiths ont été mis à l'écrit à la fin du après Mahomet. À la fin du , une chaîne de transmission devient un élément essentiel du hadith. Au , un corpus officiel prend forme. Les bibliographies n'apparaissent qu'à partir du . Cette différence temporelle rend difficile une critique historique.

Plusieurs chercheurs ont démontré que certains hadiths sont composés d'éléments plus récents que Mahomet et qui lui ont été attribués postérieurement.

Schacht considère que, de manière générale, plus une chaîne de transmission paraît « parfaite », plus le hadith est tardif. En particulier, les transmissions familiales sont des « indications positives que la tradition en question n'est pas authentique ».

Pour Amir Moezzi, « afin de justifier ces exactions, le pouvoir califal mit au point un système complexe de propagande, de censure et de falsification historique. Il altéra tout d'abord le texte coranique et forgea tout un corpus de tradition attribuée faussement au prophète ».

Les spécialistes musulmans de la science du hadith ont mis en place plusieurs types de classifications. Parmi les plus connues la classification selon la fiabilité, la classification selon la référence d'une autorité particulière, la classification selon « l’étendue ». Celles-ci ne sont pas applicables dans le cadre d'une critique historique.




Il s'agit d'une troisième classification promulguée dans l'école Hanafite. . Cette classification tient compte de « l’étendue » des Hadiths dans le monde : Mach'hur qui veut dire « connu de tous ».

Certains recueils furent compilés précocement et d'autres plus tardivement ainsi "Al Sahifah al Sahihah" de Hammam bin Munabbih compte parmi les ouvrages rédigés par les compagnons de Mahomet, ici rédigé, avant 678 (an 58 du calendrier hégirien), sous la dictée d'Abu Huraira par un disciple du compagnon. Le al-Muwatta d'imam Malik (715-795) et le "al-Musnad" d'Ahmed ben Hanbal (780-855) comptent parmi les ouvrages intégraux les plus anciens.


Il y a six principaux recueils de hadiths considérés comme références chez les sunnites ; on les appelle les « six livres » ("kutûb al-sitta") ou les « six (les plus) authentiques » ("sihha al sitta"). Deux d'entre eux sont considérés comme intégralement authentiques (les "sahihayn") et les hadiths répertoriés et acceptés par ces deux imams reçoivent parfois la qualification « d'opinion jointe » ("Muttafaqun 'alaih").


Les quatre autres livres sont:


Parmi les autres traditions considérées comme références :


Les zaydites reconnaissent bon nombre de hadiths sunnites mais disposent également de leurs propres ouvrages de hadîths relatés prioritairement par les Ahl al-Kisa que sont Fâtimah, 'Alî, Hasan et Husayn. Voici quelques-uns des ouvrages considérés comme rapportant exclusivement des hadîths authentiques :


Autres recueils :


Il faut noter qu'al-Bukhari était perse, et donc que ses hadiths sont largement admis par la communauté chiite, majoritairement perse en islam. Les chiites ont également des recueils de hadiths plus tardifs, recueillant principalement les paroles des imams de la lignée de Mahomet par Ali et Fatima :


Les ibadites (dissidence des kharidjites) reconnaissent bon nombre de hadiths sunnites. Cependant, le principal recueil accepté par ces derniers est le suivant :






</doc>
<doc id="14101" url="https://fr.wikipedia.org/wiki?curid=14101" title="Chophar">
Chophar

Le chophar ou chofar (fréquemment écrit shofar suivant la translittération anglophone, en hébreu : שופר) est un instrument de musique à vent en usage dans le rituel israélite depuis l'Antiquité. Le livre de Josué décrit notamment son utilisation par les Hébreux contre les murailles de Jéricho lors de la conquête du pays de Canaan.

On le classe dans les cornes puisqu'il est fabriqué avec une corne de bélier ou de grand koudou, selon les régions et les rites. 

Son extrémité étroite sert d'embouchure.

Le chofar est utilisé, à la fin de l'office du matin, du dimanche au vendredi inclus durant le mois d'Eloul et lors des fêtes de Roch Hachana et de Yom Kippour. Il est de coutume chez certains juifs de tradition algérienne d'en sonner avant « "véhou ra'houm vé'hanoun" » après le qaddich de la prière du soir, après le jeûne de Tich'a BéAv. 

Les sonneries du chofar prennent place, juste avant et pendant les offices de Moussaf, les deux jours de Roch Hachana, avec quatre types de sonneries distinctes :

Le jour du Yom Kippour, cet instrument est destiné à annoncer la fin du jeûne dans chaque synagogue au son d'une grande "Tequiya".



</doc>
<doc id="14102" url="https://fr.wikipedia.org/wiki?curid=14102" title="Format de papier">
Format de papier

Le format d’une feuille de papier rectangulaire est le couple formé par sa largeur et sa longueur. Ce format peut varier en fonction de l’usage de la feuille, de l’époque, et de la zone géographique. Certains de ces formats ont un nom : A4, raisin, " Certains formats font l’objet d'une normalisation internationale (ISO) ou nationale (DIN, AFNOR, ANSI). Il faut également ajouter le format des photographes, à savoir le format A3+ qui est de : 483 x 329 mm. Il est utilisé pour les imprimantes des photographes, ce format étant presque homothétique au format 2/3 ou 24x36 mm.

Pour les usages courants (création de brochures, dépliant, prospectus, etc.), notamment en bureautique, le format A4 () est aujourd’hui très largement répandu dans le monde, à l’exception de l’Amérique du Nord, où le format le plus utilisé reste le format ", soit (, à ne pas confondre avec l'ancien format français « » utilisé pour la correspondance commerciale jusqu'à la généralisation du format A4 en 1967).

Ces formats sont conçus pour que les proportions de la feuille soient conservées lorsqu’on la plie ou coupe en deux dans sa longueur, permettant ainsi le massicotage sans perte, la confection de livres par pliage, ainsi que l’assemblage, l’agrandissement et la réduction par un facteur deux. Le rapport entre longueur et largeur doit pour cela être égal à la racine carrée de deux, √2 soit environ 1,414. 
Chacun de ces formats est désigné par un nom formé d’une lettre (A, B, C) suivie d’un chiffre. Ce chiffre indique le nombre de fois où le format de base a été divisé en deux : une division en moitiés d’une feuille A0 donne deux feuilles A1, dont la division par deux donne deux feuilles A2 On notera cependant que la division successive du format supérieur en deux format inférieurs n'est vrai qu'à 1 mm près dans la pratique pour certains formats. Par exemple : deux largeurs de A1 ( = 1188 mm) sont inférieurs d'un millimètre à la longueur d'un A0 (1189 mm). Ceci est le résultat de l'arrondi au millimètre près de dimensions qui s'expriment mathématiquement avec un nombre infini de décimales (la longueur étant un facteur de √2 qui est un nombre irrationnel). On prendra ainsi avec précaution les représentations schématiques ci-dessous, qui pourraient induire en erreur.


Cette norme est utilisée dans tous les pays du monde mais est moins répandue aux États-Unis et au Canada. Bien que la norme ISO ait été adoptée au Mexique, en Colombie, au Brésil et aux Philippines, le format ' y est encore très utilisé.

Ces formats sont normalisés par l’AFNOR. Leurs noms sont hérités des filigranes qu’ils portaient quand ces papiers étaient fabriqués à la main, ce qui reste le cas pour certains papiers d’art. Ces formats existent généralement en double et dans ce cas, la petite dimension est multipliée par deux, ou en quadruple et alors les deux dimensions sont multipliées par deux.

Le format de papier américain est dérivé de formats de papier traditionnels et est officiellement utilisé aux États-Unis, au Canada ainsi qu’au Mexique.

Au Canada, le format de papier américain est le standard utilisé ', même si officiellement le gouvernement canadien utilise une combinaison de formats de papier ISO et de la norme « Formats de papier pour la correspondance » (') qui spécifie les formats P1 à P6, qui sont identiques à des formats de papier américains arrondis au demi-centimètre le plus proche.

Dans le tableau ci-dessous, les formats ' (« Lettre », « LTR » ou « 8½ par 11 ») (le plus près du ), ' (« Légal » ou « 8 ½ par 14 ») et " (« 11 par 17 ») sont les plus couramment utilisés.

Il existe également des standards pour les plan architecturaux. 

Le format ID-X définit quatre formats pour des cartes d'identité ou d'identification : ID-1, ID-2, ID-3 et ID-000.


Pour la même raison, ce même format est appelé « Super B » dans les formats américains, car variant de 12 x 18 à 13 × , il permet l’impression d’un B : 11 × avec des marges de .

Toujours pour coller au mieux aux largeurs d’imprimantes sur le marché, on peut aussi rencontrer un format 17 × soit 432 × parfois appelé « A2+ ».

Dans la plupart des imprimantes ou copieurs professionnels, les formats dits « B » sont en fait des formats JB ( japonais).

Les formats standards proposés par les fabricants de papier pour les imprimeurs sont :

Les formats 45 × 64, 52 × 74 et 70 × sont les plus répandus et correspondent aux formats les plus courants des machines d’imprimerie. Les formats allant de 63 × 88 à 72 × servent aux machines dites « » (toujours exprimé en A4), les formats 45 × 64 à 52 × 74 servent, plus généralement aux machines dites « ». Les formats intermédiaires tels que le 58 × 78 ou le 65 × sont plus généralement utilisés pour la réalisation de produits spécifiques.

Dans l’imprimerie numérique, on peut rencontrer aussi le format SRA3 (pour « " ») qui permet lui aussi d’imprimer un A3 à bords francs. Ces dimensions sont , ce qui a la particularité de correspondre à la moitié du , format de papier utilisé par les papetiers professionnels.

Le format traditionnel des cartes de visite, appelé « » ou « format postal », est 126 × .

Le format le plus couramment utilisé aujourd’hui est 85 × , format approximatif des cartes bancaires (86 × ).

En comparaison, le format des cartes de visite nord-américaines diffère légèrement en raison de l’utilisation des mesures impériales. Il s’établit à 3½ × (ou 89 × ).

Il existe de nombreux formats qui n'ont pas forcément de nom ou de standard et qui pourtant ont leur utilité.




</doc>
<doc id="14104" url="https://fr.wikipedia.org/wiki?curid=14104" title="Uppsala">
Uppsala

Uppsala ou Upsal (en français), est une ville de Suède située à 70 kilomètres au nord de Stockholm, en Uppland, célèbre pour sa prestigieuse université, la plus ancienne de Scandinavie. Avec environ habitants dans la commune dont elle est le chef-lieu et habitants dans l'agglomération, c'est la quatrième ville de Suède après Stockholm, Göteborg et Malmö (2012).

Elle est située au bout d'un "fjärd" (« baie ») du lac Mälar. La rivière Fyrisån traverse le centre de la ville.

À l'origine, Uppsala était située à quelques kilomètres au nord de la ville actuelle, à "Gamla Uppsala" (« Vieil Uppsala »). L'endroit où se trouve la ville d'aujourd'hui était alors appelé "Östra Aros". Selon Adam de Brême, le vieil Upsal était le centre du paganisme des Suédois et à cet endroit se serait trouvé un temple païen où les rois sacrifiaient aux divinités de la vieille religion d'Asar.

En 1164, environ un siècle après la christianisation de la Suède, Uppsala est devenue la ville diocésaine de l'archevêché catholique de Suède, et en 1274, la ville a été reconstruite à son endroit actuel.

Après la Réforme, Uppsala est restée le centre de l'Église de Suède ; l'archevêque de l'église luthérienne suédoise réside toujours dans la ville. C'est là qu'eut lieu en 1593 le synode du même nom au cours duquel la Suède adopta officiellement le luthéranisme.

"Domkyrkan", la cathédrale, est avec celle de Trondheim la plus grande église des pays scandinaves. Ses tours en briques rouges atteignent . L'archevêque d'Uppsala, , avait fait venir un certain Étienne de Bonneuil, tailleur de pierre français qui travaillait à la construction de la cathédrale Notre-Dame de Paris. En 1287, Bonneuil obtint la permission de Philippe le Bel de s'embarquer pour la Suède, où il se fit accompagner d'une dizaine d'ouvriers. Ils travaillèrent aux sculptures du portail sud du transept. Ici reposent le roi Gustave et Carl von Linné. De loin, la silhouette d'Uppsala est dominée par les hautes flèches de sa cathédrale, ainsi que par son château, situé sur une colline.

Fondée en 1477, l'université d'Uppsala est la plus vieille de Scandinavie encore en existence (le "studium generale" à Lund était actif de 1425 jusqu'à 1536 et la Réforme danoise). Il existe également à Uppsala une autre université, l'université suédoise des Sciences Agricoles, spécialisée dans les sciences naturelles (SLU, "Sveriges Lantbruksuniversitet"). En 2005, la population est constituée de 40 % d'étudiants, venus de tout le pays et du monde entier.




</doc>
<doc id="14106" url="https://fr.wikipedia.org/wiki?curid=14106" title="François Schuiten">
François Schuiten

François Schuiten (prononcer ""s’cueille-teun""), né le 26 avril 1956 à Bruxelles, est un dessinateur de bande dessinée et scénographe belge. Rendu célèbre par la série de bande dessinée fantastique "Les Cités obscures" réalisée en collaboration avec le scénariste Benoît Peeters.

Son père, Robert Schuiten, était un architecte très en vue à Bruxelles dans les années 1950-1960.

François Schuiten a publié sa première histoire, intitulée "Mutation", dans l’édition belge de "Pilote", alors qu’il n’avait que 16 ans. Il a étudié à l’atelier bande dessinée de l’Institut Saint-Luc, animé par Claude Renard. En 1977-1980, il a collaboré aux trois volumes du "9ème Rêve" d’où émergèrent les principaux artisans du renouveau de la bande dessinée belge.

En collaboration avec son frère aîné Luc Schuiten, il publie ses premiers récits dans "Métal hurlant" à partir de 1977. Ils sont recueillis en 1981 dans l’album "Carapaces". Parallèlement, il lance dans la même revue en 1979 "Aux médianes de Cymbiola", en collaboration avec Claude Renard, avec lequel il produit également "Le Rail" en 1981. Les albums sortent respectivement en 1980 et 1982.

Dès ses débuts en albums, Schuiten réussit à « imposer un univers fantasmatique d’une rare cohérence », variation autour de motifs invariables (la construction, le vol, etc.), témoignant « de l’impérieuse nécessité d’une œuvre qui ne doit rien à l’opportunisme et qui se développe selon une logique interne plus ou moins consciemment maîtrisée ».

En 1983, il entame une fructueuse collaboration avec son ami Benoît Peeters lorsque paraît dans "(À suivre)" "Les Murailles de Samaris", la première histoire de la série "Les Cités obscures". Cette série est située dans un univers parallèle au nôtre mais avec de nombreux passages vers le monde réel. Certains artistes de notre monde sont d’ailleurs célèbres dans "Les Cités obscures", tels que l’architecte Victor Horta ou l’écrivain Jules Verne. Le goût du détail a poussé les auteurs à décrire dans "Le Guide des Cités" les disciplines surprenantes de cet univers (comme la cryptozoologie ou la cartographie compulsive), sa faune et sa flore étranges, ses us et coutumes décalés et, surtout, son architecture obsédante, composante fondamentale de la série. Chaque cité est en effet dépeinte dans un style architectural précis ; la psychologie des personnages et le récit en sont même fortement influencés (comme l’Art déco monumental, sa rigueur et sa stabilité pour "La Fièvre d'Urbicande", l’Art nouveau, ses tentacules végétaux et sa folie pour "Les Murailles de Samaris", etc.). Traduite en une dizaine de langues, la série "Les Cités obscures" a obtenu de nombreux prix, dont le Grand prix Manga au Japan Media Arts Festival en 2013.

Mais son univers s’étend bien au-delà de la planche à dessin pour prendre vie dans l’architecture, le théâtre, la télévision et le cinéma (voir les différentes sections ci-bas).

Il a été élevé au rang de baron par le roi Albert II le 21 juillet 2002 .





François Schuiten a dessiné d'innombrables affiches, illustrations, sérigraphies et lithographies. Il a aussi réalisé une dizaine de timbres-poste pour La Poste belge. Il a également illustré un coffret long box de Gérard Manset. Il est l'auteur de la lithographie réalisée sur commande des Chœurs de l'Union européenne, une allégorie sur les thèmes de la musique et de l'Europe.

Depuis 2015, les éditions d'illustrations sont réalisées par Atlantic 12, la société détentrice des droits de reproduction des œuvres de François Schuiten

Prenant place dans le cycle des "Cités obscures" sans être des bandes dessinées, ces ouvrages, parfois édités à tirage limité, ont été écrits par Benoît Peeters et illustrés par François Schuiten

De 1989 à 1993, François Schuiten a travaillé avec Maurice Benayoun sur "Les Quarxs", une des toutes premières séries animées en images de synthèse 3D.

Il a également collaboré à la conception visuelle de six films :

Avec Benoît Peeters, il a coscénarisé deux documentaires-fiction :

En 2012, François Schuiten a achevé la conception visuelle du film "Mars et Avril" de Martin Villeneuve, tiré des photo-romans du même nom. La première mondiale de ce film a eu lieu au Festival international du film de Karlovy Vary en République tchèque dans la catégorie « Another View », qui présente des films faisant preuve d’une approche artistique hors du commun. Il travaille actuellement, avec son collègue Benoît Sokal, à la conception visuelle d’un long métrage d’animation fantaisiste, "Aquarica", dont la réalisation a été confiée à Martin Villeneuve.

François Schuiten a réalisé plusieurs scénographies, dont "La ville imaginaire" (Cités-Ciné Montréal), "Le Musée des Ombres" (présenté successivement à Angoulême, Sierre, Bruxelles et Paris), le Pavillon du Grand-Duché de Luxembourg à l'Exposition Universelle de Séville, le gigantesque "Pavillon des Utopies" ("A planet of visions") qui a accueilli cinq millions de visiteurs à l'Exposition Universelle d'Hanovre en l'an 2000, de même que le pavillon belge à l'Exposition de Aichi en 2005.

Il a scénographié l'opéra de Rossini, "La Cenerentola", présenté à La Monnaie à Bruxelles et à l'Opéra de Lyon, de même qu'un spectacle itinérant de chevaux autour des performances de Mario Luraschi.

François Schuiten a conçu la décoration des stations de métro Porte de Hal à Bruxelles et Arts et Métiers sur la ligne 11 du réseau parisien : recouverte de cuivre avec des hublots montrant des inventions.

En 2005, il orne la tour de la maison de Jules Verne à Amiens d'une sphère armillaire et réalise une grande fresque en trompe-l'œil qui habille le mur mitoyen, fresque évoquant les voyages autour du monde.

Avec Benoît Peeters et l'architecte Francis Metzger, il s'est également occupé de l'aménagement du premier édifice Art nouveau du grand architecte belge Victor Horta : la maison Autrique, devenue lieu d'expositions.

En collaboration avec Expoduo, il a travaillé sur la scénographie du musée Train World dans la gare de Schaerbeek (Bruxelles), inauguré le 25 septembre 2015. Ce musée nous transporte dans l'univers des chemins de fer du passé, du présent mais aussi de l'avenir. Un voyage sensoriel dans une ambiance inédite y est présenté et agrémenté d'histoires personnelles et passionnantes. Un projet qui a d'ailleurs inspiré l'album "La Douce", le premier que Schuiten a réalisé en solo.

"François Schuiten, Lumières sur les Cités", au Centre de la gravure et de l'Image imprimée à La Louvière (B) du 3 octobre 2015 au 7 février 2016






</doc>
<doc id="14108" url="https://fr.wikipedia.org/wiki?curid=14108" title="Festival international de la bande dessinée d'Angoulême">
Festival international de la bande dessinée d'Angoulême

Le festival international de la bande dessinée d'Angoulême, plus communément appelé festival d'Angoulême, est le principal festival de bande dessinée francophone et le plus important d'Europe en termes de notoriété et de rayonnement culturel. Créé en 1974, il a lieu à Angoulême tous les ans au mois de janvier et associe expositions, débats, rencontres et nombreuses séances de dédicace, les principaux auteurs francophones étant présents. 

Plusieurs prix y sont décernés, dont le Grand Prix de la ville d'Angoulême, qui récompense un auteur pour l'ensemble de son œuvre, et le Fauve d'or, récompensant un album paru l'année précédente.

Ville industrielle sur le déclin, Angoulême n'a aucune relation particulière à la bande dessinée avant les années 1970. À cette époque, la bande dessinée commence à avoir une image plus adulte, les grands médias se mettent à en parler et les premières expositions dédiées à ce support apparaissent.

Fin 1972, Francis Groux, conseiller municipal de la ville et passionné de bande dessinée, fait venir Claude Moliterni pour monter l'exposition « Dix millions d'images : l'âge d'or de la BD » organisée par la Socerlid. L'exposition est un succès et Groux renouvelle l'expérience l'année suivante, dans le cadre d'une quinzaine sur la littérature organisée par le maire adjoint Jean Mardikian, en invitant plusieurs dessinateurs pour deux journées, le jeudi et le samedi. Sont introduits deux mercredi consacrés à la bande dessinée au cours desquels des artistes de renom viennent animer ces journées.

En 1973, la première édition du salon national de la bande dessinée se déroule à Toulouse et obtient un certain succès. Comme le succès des manifestations à Angoulême se confirme, Groux propose à Moliterni d'organiser l'année suivante un salon de la bande dessinée similaire au festival de bande dessinée de Lucques en Italie, alors le plus grand d'Europe. Groux, Moliterni et Mardikian partent donc voir le festival en Italie pour s'en inspirer.

La première édition du festival d'Angoulême, alors nommé salon international de la bande dessinée, se déroule du 25 au 27 janvier 1974 dans l'aile désaffectée d'une partie du musée d'Angoulême. L'association organisatrice est présidée par Groux, Mardikian en est le secrétaire général, tandis que le festival lui-même est dirigé par Pierre Pascal. Hugo Pratt signe la première affiche et Burne Hogarth, Harvey Kurtzman, Maurice Tillieux, André Franquin, Claire Bretécher, Gotlib, Fred, Tibet, Peyo, Roba, Jean Giraud sont présents. Cette première édition est un succès immédiat et accueille dix mille visiteurs.

Au fur et à mesure des années, le festival multiplie les « choix souvent judicieux » : ouverture à toutes les bandes dessinées, décentralisation des activités, multiplication des colloques et conférences. À partir de 1976, chaque édition a un thème, idée aux résultats mitigés, ceux-ci étant trop restreints ou trop larges. L'édition de 1977 marque la consécration du festival avec la présence d'Hergé, qui accepte de présider le salon et d'en réaliser l'affiche. L'arrivée d'Hergé, le samedi 22 janvier, déplace les foules et donne une couverture médiatique nationale au festival.

En 1977, à la suite du changement d'équipe municipale, les subventions ne sont pas renouvelées. Le festival craint pour sa survie mais finalement le député-maire Jean-Michel Boucheron, amateur de bandes dessinées soucieux d'améliorer l'image de sa ville sinistrée par la désindustrialisation apporte à partir de l'édition de 1979 tout son soutien au festival. La même année, à la suite d'un conflit entre Pascal et l'administrateur du festival Mardikian, Groux se retire. Alain Beauregard est président par intérim de l'édition de 1980 avant que Boucheron ne le devienne courant 1980, ce qui suscite les critiques de Groux.

En 1981, deux ministres sont présents, Boucheron voulant montrer que le festival a dépassé le stade de l'amateurisme. Il veut également qu'Angoulême devienne une « capitale permanente de l'image en France », au-delà de la seule bande dessinée. Ainsi, un atelier-école de bande dessinée et la Maison de la bande dessinée (centre de documentation et de recherche) sont ouverts en 1982, le dépôt légal des bandes dessinées à la bibliothèque municipale est instauré en juillet de la même année. En mai 1983, le musée des Beaux-Arts municipal ouvre la Galerie Saint-Ogan afin d'exposer une sélection des planches qu'il a acquise depuis le milieu de la décennie précédente. Lors du festival 1984, Jack Lang annonce la création d'une Centre national de la bande dessinée et de l'image, à la fois musée, médiathèque et centre de recherche. Rapidement, les retombées économiques à long terme se font ressentir : en 1983, deux sociétés de dessin animé et de vidéopostes s'installent, créant 300 emplois.

Cette professionnalisation est accompagnée d'une hausse du budget (quatre millions de francs en 1984, soit 1,1 million d'euros de 2013). Elle implique également une certaine marchandisation du festival, qui se marque dans la croissance du nombre d'éditeurs présents et la diminution des conférences et tables rondes (de 20 en 1975 à 2 en 1984), tandis que le nombre d'expositions reste stable autour de la vingtaine.

En 1988, Jacques Glénat soutient Pierre Pascal pour déplacer le salon à Grenoble, où se trouve le siège de sa maison d'édition. Craignant qu'Angoulême perde son festival, le maire Boucheron décide d'augmenter le budget de l'édition 1989. En 1989, le successeur de Boucheron, Georges Chavanes, tranche en proposant d'alterner chaque année entre Angoulême et Grenoble malgré les protestations de Francis Groux. La subvention du salon à Angoulême est alors divisée en deux mais le financement est complété par un partenariat avec E.Leclerc.

En 1996, le salon international de la bande dessinée change de nom pour devenir le festival international de la bande dessinée (FIBD).

Dès sa première édition, le festival d'Angoulême a remis des prix à des auteurs de bande dessinée. Ces prix ont changé de nombreuses fois d’appellation et maintiennent un équilibre en distinguant à la fois des œuvres élitistes, plus expérimentales et bandes dessinées plus accessibles au grand public. Si ces changements permanents ont pu limiter leur lisibilité et diminuer leur impact, notamment sur les ventes, ce procédé d'attribution de prix, à l'imitation des grandes manifestations cinématographiques, a participé à la légitimation de la bande dessinée

Le plus notable est le Grand prix de la ville d'Angoulême qui récompense pour l'ensemble de son œuvre un auteur le plus souvent francophone, malgré le caractère officiellement international du festival. De 1989 à 2012 le prix a été remis par les anciens Grand Prix regroupés en Académie. À partir de 2013, ce système est progressivement abandonné pour un vote de l'ensemble des auteurs ayant publié un album de français. 

Les prix récompensant les albums sont décernés par divers jury. Les principaux prix sont remis par un « Grand Jury » composé de professionnels du secteur et jusqu'en 2014 présidé "ex officio" par le Grand Prix du festival précédent, qui devenait président du festival. Depuis 2015 et le refus de Bill Watterson de s'impliquer dans le festival, ce n'est plus le cas. Depuis 2015, les Présidents du jury ont été respectivement : Gwen de Bonneval, Antonin Baudry, Posy Simmonds et Guillaume Bouzard. Le Grand Jury distingue des albums à partir d'une sélection officielle d'une trentaine d'album, et d'une sélection spécifique d'albums patrimoniaux. Ces listes sont établies par un comité de sélection choisi par le festival. Ces prix sont en 2015 :
Huit titres de la sélection officielle sont par ailleurs isolés de la sélection officielle pour être proposés au vote du public, qui élit le Prix du public Cultura. Le comité de sélection prépare élection une sélection d'albums destinés aux enfants et pré-adolescents, et un jury d'enfants de huit à douze ans décerne le Prix jeunesse. Un Fauve Polar SNCF est également remis par un jury de personnalités à partir d'une liste de cinq albums. Enfin, un jury fixe composé de membre de la scène alternative remet le prix de la bande dessinée alternative, récompensant un fanzine ou un projet de micro-édition.

D'autres prix sont également décernés durant le festival, comme le Prix de l'École de l'image, le prix Tournesol ou le .

À la suite de l'attentat de "Charlie Hebdo" le 7 janvier 2015, les organisateurs du salon ont décidé, en collaboration avec l'hebdomadaire satirique, de décerner dès 2015 le Prix Charlie de la Liberté d'Expression.

Lors des premières éditions, sept prix récompensent dessinateurs, scénaristes et éditeurs français et étrangers. De 1976 à 1978, quatre nouvelles récompenses sont introduites pour consacrer les meilleures œuvres réalistes et comiques françaises et étrangères. Ces prix sont toutefois suspendus en 1979 et 1980.

En 1981, la refonte du festival entraîne un renommage des prix qui deviennent les « Alfred », en hommage au pingouin d'Alain Saint-Ogan dans "Zig et Puce". En 1989, les prix sont rebaptisés les « Alph-Art », une référence à l'album "Tintin et l'Alph-Art" inachevé par Hergé. Lors de l'édition 2002, la distinction entre bandes dessinées françaises et étrangères pour le prix du meilleur album disparaît.

De 2004 à 2006, les prix prennent simplement le nom de « Prix », puis d'« Essentiels » en 2008 et 2009.

En 2007, le président Lewis Trondheim crée le « Fauve », nouvelle mascotte du festival, qui conduit à appeler le meilleur album le « Fauve d'or » à partir de 2008. Tous les prix officiels deviennent ensuite des « Fauves d'Angoulême » à partir de 2010.

Depuis 1973, en même temps que le festival international, se tient le prix international de la BD chrétienne d'Angoulême.






</doc>
<doc id="14109" url="https://fr.wikipedia.org/wiki?curid=14109" title="Zig et Puce">
Zig et Puce

Zig et Puce est une série de bande dessinée créée en 1925 par Alain Saint-Ogan.

Cette série paraît initialement dans "Le Dimanche illustré", supplément hebdomadaire pour la jeunesse du quotidien "L’Excelsior".

Alain Saint-Ogan crée, à la demande de Henry de Weindel et Camille Ducray, responsables d'édition, les personnages de Zig et Puce pour remplacer au dernier moment une page de publicité manquante à la dernière page dans "Le Dimanche illustré", supplément dominical au journal "L’Excelsior".

La série "Zig et Puce" fait ainsi, par hasard, ses débuts dans le n° 11 du 3 mai 1925.

D'abord une suite de gags, la série se transforme graduellement en récits d'aventure plus structurés.

Dans le n° 148 du 27 décembre 1925, Zig et Puce font la connaissance du pingouin Alfred qu'ils adoptent.

Les lecteurs écrivent en masse et réclament leur présence plus régulière ; ce qui sera fait dès le n° 202 du 9 janvier 1927.

Zig et Puce sont les premiers héros d'expression française à s'exprimer par bulles de façon régulière.

À partir de 1927, le pingouin Alfred déclenche un phénomène de mode sans précédent, faisant l'objet, peut-être pour la première fois, de produits dérivés.

Il devient la mascotte de différentes personnalités, dont :

La série fait par ailleurs l'objet de traduction dès cette époque, comme en néerlandais dans "De Humorist" de 1930 à 1933.

La chanteuse Chantal Goya leur consacre une chanson en 2006.

Dans la même période, chansons et pièces de théâtre mettent en scène les personnages de la série "Zig et Puce".

La réplique « T'as le bonjour d'Alfred », dite régulièrement après que Zig et Puce ont défait un adversaire, devient une expression du langage courant utilisée lorsqu'on vient de donner une leçon à quelqu'un.

La série "Zig et Puce" sert de modèle à Hergé, jeune débutant, créateur de Tintin, qui vient rendre visite à Alain Saint-Ogan à Paris en 1931 pour lui demander conseil. On peut reconnaître l'influence de "Zig et Puce" dans la série "Quick et Flupke" d'Hergé.

La série "Zig et Puce" connaît ainsi un grand succès jusqu'à un premier arrêt en 1956.

Elle est reprise, en accord avec Alain Saint-Ogan, par Greg, qui la modernise, entre 1963 et 1970 dans le "Journal de Tintin".

En 1974, Alfred devient la mascotte du Festival international de la bande dessinée d'Angoulême.

Zig et Puce cherchent par tous les moyens à atteindre l'Amérique pour y devenir millionnaires, mais leur voyage est souvent contrarié soit par manque d'argent soit par accident. Il s'ensuit qu'ils voyagent partout dans le monde en cherchant toujours à atteindre l'Amérique.

C'est lors d'un de ces voyages, où ils se retrouvent au Pôle Nord, qu'ils rencontrent Alfred qui les accompagnera ensuite dans leurs pérégrinations. Originaire de l’hémisphère nord Alfred est donc bien un pingouin (et plus exactement un petit pingouin, la seule espèce subsistante du genre "Alca"), contrairement à la majorité des personnages dessinés animaliers ultérieurs de la même apparence, qui sont généralement des manchots. Certes, ayant des ailes atrophiées, Alfred ne peut voler, (sauf quand un savant lui greffe des ailes de cigogne !), alors que les pingouins en sont parfaitement capables. Il ne peut pourtant pas être un manchot, car ceux-ci vivent uniquement dans l'hémisphère sud, et de plus il ne ressemble à aucune des dix-huit espèces de manchots. Notons cependant qu'il n'a pas non plus vraiment l'aspect du petit pingouin, ayant un curieux bec spatulé qui n'est ni celui d'un pingouin, ni celui d'un manchot. L'artiste Alain Saint-Ogan semble donc avoir imaginé un drôle de pingouin, unique en son genre...

Sous Alain Saint-Ogan, Zig et Puce font la connaissance de la charmante Dolly et de son oncle richissime, de même que du bandit Musgrave.

Greg s'inspirera de ces personnages quand il reprendra la série, changeant le nom de Dolly en Sheila, et l'oncle de celle-ci devenant son père, qu'il nommera Poprocket.





</doc>
<doc id="14110" url="https://fr.wikipedia.org/wiki?curid=14110" title="Liste des pays par population">
Liste des pays par population

La liste des pays par population comprend 203 pays : 197 États reconnus par l'Organisation des Nations unies (dont 193 sont membres de l'ONU), et les six pays reconnus comme États par au moins un membre de l'ONU. Les données sont tirées des instituts officiels nationaux, du "World Factbook" de la CIA ou de certaines estimations.



</doc>
<doc id="14111" url="https://fr.wikipedia.org/wiki?curid=14111" title="Liste des pays et territoires par superficie">
Liste des pays et territoires par superficie

Cet article est une liste des États souverains classés en fonction de la surface totale de leur territoire, incluant les entités listées dans les standards ISO ISO 3166-1. Les chiffres représentent leur superficie totale, couvrant les terres et les eaux intérieures. Certaines entrées peuvent également comprendre les eaux marines internes (eaux côtières). Les mers territoriales, zones contiguës et les zones économiques exclusives ne sont pas incluses. Le total de toutes les terres du monde est de (près de 29,1 % de la surface de la Terre). 

Tous les pays souverainement reconnus sont sur cette liste. Les revendications territoriales en Antarctique (qui fait ) et des entités comme l'Union européenne (qui fait km) n'ont pas été prises en compte dans la superficie totale des pays concernés. Les pays avec une reconnaissance limitée qui ne sont pas listés dans les ISO standard ISO 3166-1 n'ont pas été inclus, mais sont compris dans le pays dans lequel ils sont reconnus et sont indiqués dans les notes.

La superficie moyenne est de et si l'on compte l'Antarctique.

La plupart des sources concernent, pour la Chine, le territoire contrôlé par la République populaire de Chine, sans inclure Taïwan, et, pour les États-Unis, le territoire couvert par les cinquante États et le District de Columbia. 

Dans ce cadre, le classement respectif de la Chine et des États-Unis dépend du mode de calcul. D'une part, le territoire contrôlé par la Chine inclut des territoires contestés tels que l'Aksai Chin et la vallée de Shaksgam, dont la prise en compte peut influer sur le calcul de la superficie du pays. D'autre part et surtout, le classement dépend de l'inclusion ou non des eaux côtières et territoriales :

Il apparaît donc que :



</doc>
<doc id="14112" url="https://fr.wikipedia.org/wiki?curid=14112" title="Temps universel coordonné">
Temps universel coordonné

Le temps universel coordonné, ou UTC, est une échelle de temps adoptée comme base du temps civil international par la majorité des pays du globe. 

UTC est une échelle de temps comprise entre le temps atomique international (TAI) qui est stable mais déconnecté de la rotation de la Terre et le temps universel (TU), directement lié à la rotation de la Terre et donc lentement variable. Le terme « coordonné » indique que le temps universel coordonné est identique au temps atomique international dont il a la stabilité et l’exactitude à un nombre entier de secondes près, ce qui lui permet de coller au temps universel à moins de près.

Le problème du temps universel (UT) est qu’il définit le jour comme la durée moyenne annuelle de rotation de la Terre autour de son axe (jour solaire moyen). Or cette rotation moyenne n’est pas constante, elle ralentit lentement sous l’effet des marées et, de plus, présente des irrégularités imprévisibles : la durée des jours UT augmente donc très lentement en moyenne. Mais dans les années 1960 et jusqu’à ces dernières années, plusieurs activités dont la navigation astronomique et le suivi de sondes spatiales, avaient toujours besoin du temps universel, c’est-à-dire se référaient toujours à la rotation terrestre, tout en nécessitant une échelle de temps la plus stable possible.

Initialement, avant l’instauration du TAI, le temps atomique délivré par les horloges atomiques était modifié en fréquence pour suivre la rotation terrestre et faire en sorte que la différence UT − UTC reste dans une limite fixée. Ce système devint vite lourd et trop compliqué à mettre en œuvre. C’est pour remédier à tous ces problèmes qu’en 1972 on instaura un temps atomique international (TAI) intangible et on lia UTC à ce TAI.

UTC a la même marche et la même fréquence que le TAI mais en diffère par un nombre entier de secondes. Pour faire en sorte que la différence entre UT et UTC reste inférieure à , tout en assurant un écart d’un nombre entier de secondes par rapport au temps atomique, UTC est occasionnellement incrémenté ou décrémenté d’ seconde atomique entière.

Le TAI est établi par le Bureau international des poids et mesures, à partir de plus de 400 horloges atomiques réparties dans plus de 70 laboratoires dans le monde. Ces horloges de référence sont majoritairement des horloges atomiques au césium, mais des masers à hydrogène et quelques horloges atomiques au rubidium sont aussi utilisés.

Afin de garder le temps universel coordonné synchronisé avec la rotation de la Terre, une seconde intercalaire ("" en anglais) est donc parfois rajoutée, ou retranchée, à la fin des mois de juin ou de décembre. Jusqu’à présent ces secondes intercalaires ont toujours été ajoutées, jamais retranchées : le temps UTC retarde donc sur le temps TAI. Ces insertions ne sont pas systématiques, elles sont décidées par le Service international de la rotation terrestre et des systèmes de référence (IERS), basé en particulier à l’Observatoire de Paris, au vu de l’évolution de la rotation terrestre.

Pour ajouter ou retrancher une seconde, le décompte du temps affiché par les horloges atomiques est simplement modifié :
La modification de l’affichage se fait automatiquement. Il suffit de programmer la date à laquelle le saut de seconde doit avoir lieu.

Le , ajout d’une seconde. Le décalage entre UTC et TAI passe à , après avoir été de durant (depuis 1999).

Le , ajout d’une seconde. Le décalage passe à .

Le , une autre seconde a été ajoutée, ce qui a porté le décalage entre UTC et TAI à .

Le une autre seconde a été ajoutée. Le décalage est donc porté à .

Le une autre seconde a été ajoutée. Le décalage est donc porté à .

L’utilisation de l’appellation standard temps moyen de Greenwich (sigle : GMT, de l’anglais ) s’était imposée par la prépondérance de la marine britannique durant le . Elle est désormais déconseillée parce que sa définition est ambiguë. L’utilisation de la nouvelle appellation standard temps universel coordonné (ou son abréviation UTC) doit lui être préférée. 



</doc>
<doc id="14115" url="https://fr.wikipedia.org/wiki?curid=14115" title="Darwin">
Darwin

Employé seul, Darwin désigne généralement le naturaliste anglais Charles Darwin (1809-1882).


Plusieurs espèces ont été nommées en hommage à Charles Darwin :



En physique les concepts portant le nom de Darwin font référence à Charles Galton Darwin, petit-fils de Charles :








Darwin est un prénom, notamment porté par :











</doc>
<doc id="14117" url="https://fr.wikipedia.org/wiki?curid=14117" title="Bicamérisme">
Bicamérisme

Le bicamérisme (ou bicaméralisme) est un système d'organisation politique qui divise le Parlement en deux chambres distinctes, une chambre haute et une chambre basse. Le mot, introduit au , est constitué de « bi » ("deux") et de « camera » ("chambre" en latin).

Ce système a pour but de modérer l'action de la Chambre basse, élue au suffrage direct et représentant donc directement le peuple, en soumettant toutes ses décisions à l'examen de la Chambre haute, élue généralement au suffrage indirect et représentant souvent des départements, des régions ou des États.

Les fédérations adoptent presque toujours un système de représentation bicaméral. Une chambre représente alors la population ; l'autre, les entités fédérées.

Les adjectifs consacrés, en français, pour désigner les deux types de système politique, sont : monocaméraliste, ou monocamériste ou unicaméral, et bicaméral. Monocamériste et bicamériste, quant à eux, sont des substantifs utilisés pour désigner les partisans des systèmes monocaméral et bicaméral.

Le bicamérisme est le fruit de l'histoire constitutionnelle anglaise. Pour bien comprendre les conséquences des évolutions du droit britannique, il faut remonter au milieu du Moyen Âge.

Lorsque Guillaume Le Conquérant envahit l'Angleterre au , il s'entoure afin de gouverner d'un Magnum Concilium, ou Grand Conseil, qui se charge de l'orienter pour ses décisions en matières fiscales et politiques. Ce même Conseil se révolte en 1215 en imposant au Roi Jean-Sans-Terre l'obligation de rendre compte de ses décisions financières et de demander l'autorisation du Concilium.

À la fin du , le roi d'Angleterre Édouard I entreprend une réforme sur la composition du Conseil : il appelle les représentants des bourgs et des comtés pour siéger aux côtés des nobles. Il espère ainsi pouvoir financer ses guerres.

Mais ce n'est qu'au , avec le roi Édouard III, que le Grand Conseil est divisé en deux chambres : celles des Communes et celles des Lords. Cependant, l'exécutif et le législatif sont encore largement confondus, et les révolutions de 1640 et 1688 excluront les rois de la puissance législative conformément au Bill of Rights. La formation de la loi passera désormais selon un Parlement bicaméral, composé de deux chambres, aux intérêts divers.

Cette forme de bicamérisme aristocratique est aujourd'hui tombée en désuétude. Plusieurs réformes (notamment les deux "Parliament Acts" de 1911 et 1949, le "Life Peerages Act" de 1958 et le "House of Lords Act" de 1999) de la Chambre des lords ont progressivement contribué à démocratiser sa composition et à lui donner un rôle de plus en plus symbolique, car certaines dispositions permettent de se passer de leur consentement, mais cette procédure est peu utilisée. À cet égard, certains constitutionnalistes vont même jusqu'à présenter le Royaume-Uni comme un régime monocaméral.

De manière générale, on peut observer le déclin du bicamérisme tant quantitatif (institution d'un parlement monocaméral en Suède) que qualitatif (phénomène du bicamérisme fonctionnel, avec une chambre haute purement réflexive en Norvège et en Finlande).

Mais le bicamérisme résiste dans les pays où la chambre haute a un rôle de représentation des collectivités, ainsi le Bundesrat autrichien et le Sénat français.

Dans plusieurs États fédéraux, la chambre haute représente, au moins théoriquement, les États constituants que ce soit sur une base égalitaire, comme aux États-Unis ou régionale, comme au Canada.

S'il n'existe qu'en Italie et en Suisse des cas de bicamérisme parfait ou égalitaire, la Pologne, la Croatie et la République tchèque ont réhabilité le bicamérisme. Son déclin n'apparaît donc pas inéluctable.

C'est Montesquieu qui théorise la division du pouvoir législatif. Dans "De l'Esprit des Lois" de 1748, il y décrit la Constitution anglaise : le peuple élit les membres de la Chambre des communes (NB : seul 10 % de la population peut voter), qui vient concurrencer la Chambre des lords, dont les membres sont héréditaires. Le fait que chaque Chambre défend des intérêts différents, et que chacune a la « faculté d'empêcher » l'initiative de l'autre, fait que le pouvoir législatif est dans l'incapacité d'abuser de son autorité. De plus, du fait de l'élection des Communes, la corruption a peu de chance de s'installer. Enfin, Montesquieu souligne l'importance du bicamérisme, dans le sens où l'exécutif peut occuper une place plus importante en échappant à la tyrannie d'un législatif trop fort.

Pour Montesquieu, le bicamérisme est une condition essentielle à la théorie de l'équilibre des pouvoirs, c'est-à-dire lorsque « le pouvoir arrête le pouvoir ».

L'histoire constitutionnelle anglaise peut être vue comme « accidentelle », dans le sens où ce sont de purs hasards historiques qui ont déterminé la séparation du pouvoir législatif en deux chambres. S'inspirant de Montesquieu et habitués au système anglais, les Américains appliquent un bicamérisme dès les premiers jours de leur indépendance. La chambre haute du système législatif américain est le Sénat.

Dès 1787, le Congrès se compose d'un Sénat et d'une Chambre des Représentants. En effet, il regroupe les députés des 13 territoires américains dans la première chambre, et la seconde accueille les représentants élus au suffrage universel par États (notion de fédéralisme). Pour former la loi, l'une des deux chambres propose un projet à l'autre. S'il n'y a pas d'accord, on crée une commission de conciliation, qui se charge de trouver un consensus. Si malgré tout, les deux Chambres ne sont pas d'accord, le projet est abandonné. Le bicamérisme américain constitue un véritable succès dans la première démocratie moderne.

L'Allemagne a également un parlement bicaméral : la chambre basse, le Bundestag, élit le chancelier. Elle vote les lois conjointement avec la chambre haute, le Bundesrat, qui est censée représenter les 16 États-régions (Land) composent la République fédérale d'Allemagne. Si les députés du Bundestag sont élus au suffrage universel, les membres du Bundesrat ne sont que les émissaires des différents gouvernements régionaux, le gouvernement de chaque État-région disposant de trois à six voix en fonction du nombre d'habitants. Ainsi la Bavière dispose-t-elle actuellement de six voix alors que la Sarre n'en compte que trois.

La Belgique possède également un parlement bicaméral constitué d'une part par la Chambre des représentants, la chambre basse, et d'autre part par le Sénat, la chambre haute. La composition de ces chambres est établie par la constitution coordonnée du 17 février 1994 en vertu des articles 61 et suivants pour la Chambres des représentants ainsi qu'en vertu des articles 67 et suivants pour le Sénat.

Ancien Dominion de l'Empire britannique, sa structure législative est directement héritée de celle de la Grande-Bretagne. Elle se compose de la Chambre des Communes, dont les membres, les députés, sont élus au suffrage universel, et du Sénat, dont les membres, les sénateurs sont nommés par le Gouverneur Général (représentant de la reine), sur recommandation du Premier Ministre, selon une répartition numérique par région.

Les parlements de chaque province sont en revanche tous monocamérales depuis 1968. Afin de mettre fin aux nominations partisanes, depuis de nombreuses années, les Provinces réclament sans succès que le choix des Sénateurs soit effectué obligatoirement à partir d'une liste fournie par chacune des provinces.

L’Espagne a un parlement de type bicaméral imparfait. Les "Cortes Generales" sont le nom des deux chambres du Parlement qui représentent le peuple espagnol. Elles sont formées par le Congrès des députés et le Sénat.

Le Congrès américain est composé de deux chambres : le Sénat et la Chambre des représentants. 
Il correspond ainsi à un bicamérisme. 
Le Sénat représente la chambre haute tandis que la Chambre des représentants figure la chambre basse.

L'objectif du bicamérisme en France est de modérer l'action de la Chambre basse (aujourd'hui l'Assemblée nationale qui est élue au suffrage universel direct), en soumettant toutes ses décisions à l'examen d'une seconde chambre, la Chambre haute (aujourd'hui le Sénat qui est élu au suffrage universel indirect), plus conservatrice.

Le bicamérisme fut introduit en France dans la Constitution du 5 fructidor an III (le Directoire), en 1795, avec deux assemblées élues : le Conseil des Anciens et le Conseil des Cinq-Cents. Le rapporteur du projet de cette constitution Boissy d'Anglas déclare devant la Constitution : 

Le Conseil des Anciens partageait le pouvoir avec l'exécutif représenté par cinq Directeurs, et concourait, avec le Conseil des Cinq-Cents, à l'élaboration des lois. Le Conseil des Anciens comptait 250 membres renouvelés par tiers tous les ans. Ces membres devaient avoir 40 ans au moins, être mariés ou veufs, et domiciliés depuis 15 ans sur le territoire de la République. Siégeant au palais des Tuileries, ils approuvaient ou rejetaient les résolutions prises par le Conseil des Cinq-Cents, et élisaient les directeurs du pouvoir exécutif.

Les constitutions du Consulat (1799) puis du Premier Empire (1804) en conserveront le principe, avec le Corps législatif et le Tribunat.

En 1814, la Restauration de la dynastie des Bourbons s'inspira des institutions britanniques en créant une "Chambre des pairs" sur le modèle britannique, dont une partie des membres était nommée à vie et l'autre à titre héréditaire. La Révolution de Juillet 1830 supprima l'hérédité des pairs.

La Constitution du (la Deuxième République) supprima brièvement le principe du bicaméralisme, en ne prévoyant qu'une "Assemblée nationale", seule détentrice du pouvoir législatif.

Toutefois, le coup d'État du « prince président », Napoléon III, le , permettait la promulgation de la Constitution du , qui prévoyait deux chambres dont le "Sénat", ce qui ne fut pas modifié par la restauration de la dignité impériale le (le Second Empire).

Les lois constitutionnelles de 1875 (la Troisième République), qui restauraient formellement la République, conservèrent le principe d'un parlement (appelé "Assemblée Nationale") composé de deux chambres, sous le nom de "Sénat" et de "Chambre des députés". Ce bicaméralisme était égalitaire : les deux chambres avaient les mêmes pouvoirs. Le Sénat était alors « prix à payer » comme le disait le duc de Broglie. « Les républicains lui sont hostiles mais l’acceptent pour avoir la République et les royalistes pour consentir à la République, exigent des garanties quant à son caractère conservateur ». Les monarchistes ayant obtenu satisfaction, ils disposaient ainsi d’une assemblée qui pouvait contrer la chambre des députés élue au suffrage universel, et ce d’autant plus que les deux chambres disposaient de pouvoirs identiques.

Sous l'empire de la loi constitutionnelle du 10 juillet 1940 le Régime de Vichy proroge et ajourne les chambres via l'acte constitutionnel du 11 juillet 1940.

Le projet de constitution du 19 avril 1946 qui prévoyait l'existence d'une seule chambre repoussée par référendum, la Constitution du (la Quatrième République) restaura le principe de deux chambres, en leur donnant les noms d"'Assemblée nationale" et de "Conseil de la République". La première avait un pouvoir prépondérant sur la seconde. Le Conseil de la République n'avait alors qu'un rôle consultatif.

La Constitution du (la Cinquième République) a maintenu ce système, tout en réintroduisant l'usage du nom "Sénat", disparu depuis juillet 1940.

L'Italie connaît la "Camera dei deputati" (Chambre des députés) et le "Senato della repubblica" (Sénat).

La Confédération suisse connaît également un Parlement fédéral bicaméral : la chambre basse, le Conseil national, répartit les députés par canton en fonction de leur population, la chambre haute, le Conseil des États, donne deux députés à chaque canton, et un à chaque demi-canton, quelle que soit sa taille géographique et sa population. Ce système permet d'éviter une domination des grands cantons sur les petits cantons et "vice versa".




</doc>
<doc id="14118" url="https://fr.wikipedia.org/wiki?curid=14118" title="Préfecture d'Okayama">
Préfecture d'Okayama

Lors de la restauration Meiji les provinces de Bitchū, Bizen et Mimasaka furent associées pour créer la préfecture d'Okayama.

Elle est entourée des préfectures de Hyōgo, Tottori et Hiroshima.

Elle fait face à la Kagawa qui se trouve de l'autre côté de la mer intérieure de Seto et compte plus de 90 îles et îlots.

La ville historique de Kurashiki s'y trouve.

La préfecture compte 15 :
La préfecture comporte également 12 et , répartis dans 10 :

La préfecture d’Okayama possède un pôle d’activités industrielles le district industriel de Mizushima situé dans la ville de Kurashiki. Ce pôle concentre des entreprises œuvrant dans les secteurs des produits pétroliers, de la chimie, de la sidérurgie et de la construction automobile.

Les industries locales fabriquent également des produits originaux dans les secteurs du textile, des matériaux réfractaires, de la machinerie et de la métallurgie.

La plus grande partie de la population est concentrée autour des villes de Kurashiki et Okayama. Elle contenaient lors du recensement de l'an 2000, 1'170'000 résidents. Ce qui représente environ 60 % de la population totale de cette région.
Les petits villages dans la région montagneuse du nord de la préfecture se dépeuple et la moyenne de la population y habitant vieillit.
Plus de la moitié des municipalités sont officiellement désignées comme dépeuplées.

La région d'Okayama possède un dialecte dénommé Okayama-ben.

La préfecture présente plusieurs spécialités culinaires. Les ', une variété d'okonomiyaki à base d'huîtres, se retrouve plus particulièrement dans le bourg de Hinase. Les pêches, en particulier les pêches blanches et la variété "shimizu-hakuto" se récoltent en juillet et août et sont fortement associées à la préfecture. Les ', pâtisserie à base de farines de millet et de riz se retrouvent, nature ou souvent aromatisés. Les sushi sont préparés dans la préfecture selon la méthode , sous forme de salade. Le , viande de très grande qualité comparable au bœuf de Kōbe, est une spécialité de la ville de Niimi.

Un jardin de style traditionnel japonais se trouve dans le centre de la ville de Okayama. Ce jardin, nommé "Koraku-en" (後楽園), est considéré comme l'un des trois plus beaux jardins japonais.

La préfecture d'Okayama est jumelée avec les municipalités ou régions suivantes :



</doc>
<doc id="14121" url="https://fr.wikipedia.org/wiki?curid=14121" title="Mer du Japon">
Mer du Japon

La mer du Japon (en japonais , en chinois , en russe ') est une mer située à l'ouest de l'océan Pacifique entourée à l'ouest par la péninsule coréenne, au nord par la Russie et les îles Sakhaline, enfin à l'est par les trois grandes îles japonaises de Hokkaidō, Honshū et Kyūshū. Elle est appelée « mer de l'Est » en Corée du Sud (en coréen ) et « mer orientale de Corée » en Corée du Nord ().

L'Organisation hydrographique internationale définit les limites de la mer du Japon de la façon suivante : 

dans le détroit de Corée :
depuis Nomo-saki à Kyūshū, jusqu’à la pointe sud de Fukue-jima, et à travers cette île jusqu’à Ose Zaki et jusqu’à Punam-got, la pointe sud de Jeju-do, à travers cette île jusqu’à son extrémité ouest ; de là, jusqu'au rocher Kan-Sǒ (34°13'N) dans l'archipel Mengoru ("Maenggol-gundo") ; de là jusqu'à la pointe nord de Ok-do (Oku To) (34°22'N), ensuite jusqu'à la pointe ouest de Sosǒngnam-do (Syo-Zyonan To) puis jusqu'à la pointe nord de Sǒngnam-do (Zyonan To) (34°24'N) ; de là, jusqu'à une pointe sur la côte de Chin-do (Jindo ou Tin Tō) (34°25'N), le long de la côte nord-ouest de cette île jusqu'à sa pointe nord ; et de là, une ligne en direction du nord-est jusqu'à la terre ferme de la République de Corée (en coréen : "Daehan-min’guk") ;

dans le détroit de Kanmon :
une ligne joignant Nagoya-Zaki (préfecture de Fukuoka, Japon) dans l’île de Kyūshū à travers les îles d’Uma shima et Mutsure jima jusqu’au Murasakino-Hana dans l’île de Honshū ;

dans le détroit de Tsugaru :
depuis l’extrémité du Shiriya-Zaki (Honshū) à l’extrémité d’Esan-misaki (Hokkaidō) ;

dans le détroit de La Pérouse (ou détroit de Sōya) :
une ligne joignant le cap Sōya ("Sōya-misaki"), en territoire japonais, au cap Crillon ("mys Krilon"), dans l'oblast de Sakhaline, en Russie ;

du cap Tyk ("mys Tyk"), dans l'oblast de Sakhaline (Russie), au cap Souchtchiov ("mys Souchtchiova"), dans le kraï de Khabarovsk (Russie).

Le point le plus profond est de au-dessous de niveau de la mer, la profondeur moyenne est de . La superficie de la mer est d'environ . La mer a trois bassins principaux : le bassin de Yamato se situe dans le sud-est de la mer du Japon/mer de l'Est ; le bassin du Japon dans le nord ; le dans le sud-ouest. Le bassin du Japon est la région la plus profonde de la mer, alors que les eaux les moins profondes peuvent être trouvées dans le bassin de Tsushima.

Sur les rivages orientaux, les plateaux continentaux de la mer sont larges, mais sur les rivages occidentaux, en particulier le long de la côte coréenne, ils sont étroits, faisant en moyenne trente kilomètres. L'eau chaude de la mer contribue au climat doux du Japon.

Le nord et le sud-est de la mer sont des secteurs de pêche riches. L'importance de la pêche en mer est bien illustrée par les réclamations continues du Japon sur les îles sud-coréennes de Dokdo. La mer est également importante pour ses dépôts de minerai, en particulier le sable de magnétite. On y trouve également du gaz naturel et quelques gisements de pétrole. Depuis la croissance des économies de l'Est asiatique, la mer du Japon/mer de l'Est est une voie commerciale importante.

Au cours des siècles, cette mer s’est appelée « mer de l’Est », « mer de Corée », « mer du Japon », ou encore « mer Orientale ». Dans la majorité des pays, la désignation usuelle est aujourd'hui « mer du Japon », et ce depuis au moins 1787. C'est le cas au Japon où elle est appelée , en Chine — "Rìběnhǎi" () — et en Russie — "Iaponskoié morié" (""). Cependant, en République de Corée elle est nommée « mer de l’Est » (, , "Tonghae"), et en République populaire démocratique de Corée, « mer orientale de Corée » (, , "Chosŏn Tonghae"). 

Le nom de « mer du Japon » était le nom officiel retenu par l’Organisation hydrographique internationale en 1928. Les Coréens dénoncent cette appellation décidée à l’occasion de la conférence de Monaco, déplorant de n’avoir pu y participer en tant que nation indépendante, le pays étant sous occupation japonaise à cette époque. L’appellation de la mer du Japon est l’un des thèmes de revendication des patriotes coréens, qui tentent depuis l’après-guerre de faire changer l'usage international.

Selon l’argumentation coréenne : 

Outre le fait que cette appellation « mer du Japon » puisse être considérée comme nationaliste, la demande sud-coréenne peut poser la question du parti-pris, puisque les cartes coréennes présentent cette mer comme étant celle « de l’Est », et la mer Jaune comme étant celle « de l’Ouest ». 

En 2006 lors d’un sommet APEC, la proposition faite par le président sud-coréen de l’époque Roh Moo-hyun au Premier ministre japonais Shinzo Abe de régler ce conflit en appelant cette mer « mer de la Paix » ou « mer de l’Amitié » avait fait un certain effet. Le premier ministre japonais avait alors demandé à reporter le débat à une date ultérieure et cette proposition est restée sans suite depuis.

En 2012 l’OHI, organisme international compétent en la matière, a décidé de continuer à employer la dénomination unique « mer du Japon » et de ne pas employer la dénomination double « mer du Japon/mer de l’Est ». Elle rejette ainsi les demandes de la République de Corée.




</doc>
<doc id="14122" url="https://fr.wikipedia.org/wiki?curid=14122" title="Shamisen">
Shamisen

Le est un instrument de musique traditionnel à cordes pincées utilisé en musique japonaise. C'est un luth à long manche à la touche lisse.

Le shamisen est dérivé d'un instrument chinois, le "sanxian", introduit dans l'île d'Okinawa au milieu du et très vite adapté à la musique de la cour du Royaume de Ryūkyū, alors tributaire de la dynastie Ming depuis l'Époque Sanzan puis l'unification de ce royaume. C'est au début de la période Edo (1603-1868) qu'il fit son apparition dans les autres îles de l'archipel japonais.

Le shamisen est un luth mesurant de 110 à , dont la caisse de résonance carrée est traditionnellement construite en bois de santal et recouverte de peau de chat ou de chien. À l'exception du shamisen de l'île d'Okinawa qui est traditionnellement recouvert de peau de serpent et est appelé ou . La table en peau fait qu'on donne parfois au shamisen le nom de « banjo japonais ».

Le manche est long et fin, sans frettes. Il est muni de trois cordes (d'où le nom de l’instrument, qui signifie littéralement « trois cordes du goût ») de soie ou de nylon.

Le "tsugaru shamisen" a un manche plus large et est destiné au style du même nom.

On joue du shamisen agenouillé sur un "zabuton" en pinçant les cordes à l'aide d'un large plectre en ivoire ("bâshô" ou bachi). La musique traditionnelle au shamisen intercale au milieu de la mélodie de longs silences qui donnent d'autant plus de force aux notes.

Il est utilisé avec des voix dans les chants populaires et comme instrument soliste ou d'ensemble (comme dans les orchestres de kabuki). Il devint l'instrument de prédilection des geishas.

Il existe des styles particuliers et régionaux : "min'yō", "tsugaru shamisen".

Des musiciens contemporains, comme les Yoshida Brothers, utilisent le shamisen pour produire des musiques très rythmées plus proches de ses origines okinawaïennes.




</doc>
<doc id="14124" url="https://fr.wikipedia.org/wiki?curid=14124" title="Biwa">
Biwa

Biwa représente plusieurs éléments :


</doc>
<doc id="14126" url="https://fr.wikipedia.org/wiki?curid=14126" title="Timbre">
Timbre








</doc>
<doc id="14127" url="https://fr.wikipedia.org/wiki?curid=14127" title="Timbre postal">
Timbre postal

Le timbre postal ou timbre-poste est un morceau de papier support d'un graphisme, généralement enduit d'un adhésif, qui apposé sur un courrier sert à indiquer que l'expéditeur a payé l'affranchissement.
La collection et l'étude des timbres postaux et fiscaux sont appelées la philatélie. Les timbres sont un marché très précieux et populaire pour des collectionneurs.

Le timbre-poste a été précédé et inspiré par le timbre fiscal (papier timbré et timbre fiscal mobile). Bien qu'étant, à l'origine, une idée de la toute jeune Belgique née en 1830 (en réalité, ce concept émerge dans l'entreprise postale des Turn und Taxis implantée à Bruxelles), le timbre-poste est une invention des Britanniques, Rowland Hill et James Chalmers dans le cadre d'une importante réforme postale. Le premier timbre mobile fut émis par les postes royales britanniques en 1840 sous la forme d'un portrait de la reine Victoria. De couleur noire et valant un penny il est appelé le « "Penny Black" ». Le premier timbre en couleur, la Colombe de Bâle, fut créé en Suisse en 1845 par le canton de Bâle.

Cette invention a été une révolution dans le système postal. Avant 1840, c'est habituellement le destinataire qui paye le coût de transport par les postes du pli qui lui est adressé, et à un prix très élevé. De ce fait, beaucoup de destinataires refusent les lettres trop coûteuses, tandis que des transporteurs privés à meilleur marché concurrencent indûment la poste. La petite histoire veut que Rowland Hill prit conscience des effets pervers de ce système pour les revenus des postes lorsqu'un jour, dans une auberge, il remarqua la serveuse recevant d'un facteur un pli de son fiancé. La jeune femme ne pouvant, semble-t-il, se permettre la dépense, Hill se proposa de la régler. Elle lui avoua que pour correspondre gratuitement, son fiancé et elle dessinaient de petits symboles sur l'enveloppe pour communiquer.

En Grande-Bretagne, pour éviter le transport à perte des plis, Hill proposa en 1837, dans son rapport intitulé « "Post Office reform, Its importance and practibility" » de faire payer l'expéditeur, mais au prix très réduit de (« "Penny postage" »). Cet expéditeur prouverait qu'il s'était bien acquitté de l'affranchissement en utilisant une enveloppe officielle à 1d, ou (suivant une proposition ultérieure de Chalmers) en collant un timbre postal sur l'enveloppe, que l'administration des postes annulerait avec un cachet encré pour éviter toute réutilisation.

La réforme postale demandée par Hill mis trois ans à aboutir en raison de l'opposition de l'administration postale qui ne prenait en considération que le manque à gagner initial pour la poste, sans tenir compte des effets stimulants de la réforme postale sur le développement du commerce et le développement de l'instruction.

Cette réforme entra enfin en vigueur le , six jours après la mise à la disposition du public, dès le , du premier timbre, le à l'effigie de Victoria, le « "Penny black" » dessiné par Henry Corbould, ainsi que de la première enveloppe de port payé dessinée par Mulready.

Cette substitution du « port payé » au « port dû », avec un abaissement considérable du prix perçu pour le transport, entraîna un accroissement immédiat, et chaque année de plus en plus important, du volume des correspondances.

Ce succès, immédiat au Royaume-Uni, fut imité dans le reste du monde. Les premiers pays à avoir réformé leur système postal et émis des timbres postaux ont été :



Dès 1837, l'adoption d'une réforme analogue au système britannique avait été proposée en France. Mais il faudra onze ans pour que la résistance de l'administration des postes soit surmontée par Étienne Arago, à la suite de la révolution de 1848, et que le port-payé à bon marché soit institué par un vote de l'Assemblée nationale, le 24 août 1848. Ce port-payé fut fixé à . C'est en application de cette réforme que fut émis, le janvier 1849, le premier timbre-poste français, le , au type Cérès de Barre.


Un timbre porte un certain nombre d'informations nécessaires à son utilisation postale. Une illustration décorative peut figurer, elle permet soit l'identification du pays émetteur (cas des allégories et des portraits royaux), soit de plaire à l'expéditeur, au destinataire ou au collectionneur (voir timbre commémoratif par exemple).

Les mentions nécessaires sont :

Des mentions peuvent apparaître sur les timbres de certains pays :

La quasi-totalité des timbres sont imprimés sur du papier, même si certaines administrations postales ont pu user, par souci de promotion parfois, d'autres matériaux.

Les gommes adhésives à l'arrière des timbres peuvent être assez variées. 
En France, on peut signaler une variété de gomme tropicale qui a parfois été utilisée.
En Suisse, et maintenant aussi en France, les timbres autocollants ont la particularité de se décoller facilement dans l'eau grâce à une invention brevetée, la colle du timbre reste sur l'enveloppe.

Pendant la Première Guerre mondiale, les pièces de monnaie (en argent et en bronze) furent thésaurisées. Pour pallier le manque de monnaie, des villes, des chambres de commerce, des commerçants firent frapper des monnaies de nécessité. Certaines entreprises firent alors fabriquer des timbres-monnaie : mi-timbres mi-monnaies, c'étaient des jetons circulaires constitués d'un petit boîtier rond en métal enchâssant le timbre-poste entre le disque métallique et un feuillet transparent ; ce disque métallique servait en outre de support publicitaire.

En France, la taille-douce a commencé à être utilisée à partir de 1928 pour les timbres grand-format de la Caisse d'amortissement puis pour ceux de la série touristique de 1930-31.

La connaissance des techniques d'impression est fort utile non seulement aux imprimeurs des timbres officiels, mais aussi aux faussaires, qu'ils visent à tromper le fisc, la poste ou les collectionneurs.

Les administrations postales font preuve d'originalité dans l'impression des timbres, en utilisant les techniques modernes d'impression :

Lors de l'impression des timbres, les poinçons en métal utilisés s'usent progressivement et sont régulièrement remplacés. Il arrive que l'usure d'une planche de poinçons ne soit pas remarquée à temps et que des défauts s'impriment sur un ou plusieurs timbres de la planche finale. Les philatélistes parlent de variété" "pour désigner des timbres légèrement différents de la version normale. Ils peuvent être très recherchés selon le côté spectaculaire du défaut et leur rareté.

Les variétés sont recherchées par les philatélistes en raison de leur rareté puisqu'elles sont accidentelles et que les contrôles à l'imprimerie devraient les détruire toutes. Il ne faut pas confondre « variété » et « curiosité ». La variété est répétitive, tandis que la curiosité est accidentelle et non nécessairement répétitive. La variété est répertoriée (la position du cliché d'impression sur la planche est bien déterminée).

Par contre, si la différence n'est pas accidentelle, mais est due à une différence systématique entre deux jeux de poinçons secondaires (ceux issus du poinçon originel), les collectionneurs parlent de "types" puisqu'il y a deux dessins différents existants pour un timbre. Leur valeur philatélique, cette fois-ci, va dépendre du souci du collectionneur de distinguer tous les aléas du processus d'impression et du nombre de timbres tirés des différents types.

Exemples de variétés :

Les différentes administrations postales ont fait preuve d'originalité au long de leur histoire dans la forme de leurs émissions de timbres postaux, surtout avec le développement de la philatélie.

En général, chaque timbre est une unité d'image, ne débordant pas sur le timbre le voisinant sur la feuille d'impression.










Pour assurer la promotion de leurs émissions et tirer des revenus supplémentaires, les administrations postales organisent des manifestations.

Ils ne sont pas toujours officiellement connus. Le nombre d'exemplaires réellement vendus peut être inférieur.
Quelques exemples extraits du catalogue France Yvert et Tellier 2008 :


Certains, devenus très coûteux du fait de leur rareté, ont été imprimés à un très petit nombre d'exemplaires : c'est le cas en France du type Pasteur surchargé , émis à seulement en 1928 par un bureau de Poste spécial du paquebot Île-de-France pour le courrier aérien envoyé depuis le paquebot. 

Depuis 1840, près de postaux ont été tirés dans le monde (dont ) par les pays qui ont le droit d'émettre des timbres (droit établi par l'Union postale universelle).

En France, malgré le passage à l'euro, les timbres postaux libellés en francs — y compris en anciens francs — sont encore valables, hormis ceux qui pour des raisons techniques ou politiques ont été démonétisés. Il suffit d'effectuer la conversion pour s'assurer que le montant corresponde au tarif d'envoi de la lettre ou du colis. Il est recommandé d'indiquer le prix de conversion en euros sur l'enveloppe ou le paquet, pour simplifier le travail des postiers et éviter ainsi d'éventuels retards ou pertes.

Parmi les timbres prisés par les collectionneurs et les investisseurs, l'on trouve :




</doc>
<doc id="14133" url="https://fr.wikipedia.org/wiki?curid=14133" title="Bjarne Stroustrup">
Bjarne Stroustrup

Bjarne Stroustrup ([ˈbjɑːnə ˈsdʁʌʊ̯ˀsdʁɔb]), né le à Aarhus, est un informaticien, écrivain et professeur de sciences informatiques danois. Il est connu pour être l'auteur du langage de programmation C++, l'un des plus utilisés dans le monde. 

Stroustrup obtient une maîtrise en mathématiques et en sciences informatiques à l'université d'Aarhus en 1975. Il prépare ensuite un doctorat en informatique à l'université de Cambridge en Angleterre, qu'il obtient en 1979. Il part alors vivre dans le New Jersey aux États-Unis, où il travaille pour les laboratoires Bell jusqu'en 2002. Après avoir été professeur à l'université Texas A&M aux États-Unis, où il était titulaire de la chaire de science informatique du collège d'ingénierie jusqu'en janvier 2014, il est maintenant « managing director » chez Morgan Stanley à New York. Il est « fellow » de l'ACM (1994) et de l'IEEE.



Une liste exhaustive de ses ouvrages et de ses publications est également disponible sur son site personnel.



</doc>
<doc id="14134" url="https://fr.wikipedia.org/wiki?curid=14134" title="National Aeronautics and Space Administration">
National Aeronautics and Space Administration

La , en français lAdministration nationale de l'aéronautique et de l'espace, plus connue sous son acronyme NASA, est l'agence gouvernementale qui est responsable de la majeure partie du programme spatial civil des États-Unis. La recherche aéronautique relève également du domaine de la NASA. Depuis sa création à la fin des années 1950, la NASA joue mondialement un rôle dominant dans le domaine du vol spatial habité, de l'exploration du Système solaire et de la recherche spatiale. Parmi les réalisations les plus marquantes de l'agence figurent les programmes spatiaux habités Apollo, la navette spatiale américaine, la station spatiale internationale (en coopération avec plusieurs pays), les télescopes spatiaux comme Hubble, l'exploration de Mars par les sondes spatiales Viking et MER, ainsi que celle de Jupiter et Saturne par les sondes Pioneer, Voyager, Galileo et Cassini-Huygens.

La NASA a été créée le pour administrer et réaliser les projets relevant de l'astronautique civile, jusque-là pris en charge par les différentes branches des forces armées des États-Unis, afin de rattraper l'avance prise par l'Union soviétique. La NASA reprend à cette époque les centres de recherche du NACA, jusque-là tourné vers la recherche dans le domaine de l'aéronautique. Elle est aujourd'hui dotée d'un budget de de dollars (2015) et emploie directement environ ( avec le Jet Propulsion Laboratory) ainsi qu'un grand nombre de sous-traitants répartis entre dix centres spatiaux situés principalement dans les États du Texas, de Californie et de Floride, de l'Alabama, de Virginie et de Washington. Les missions marquantes en cours sont l'achèvement et l'exploitation de la station spatiale internationale, l'utilisation et la réalisation de plusieurs télescopes spatiaux dont le James Webb Space Telescope, les sondes spatiales OSIRIS-REx , Mars 2020, New Horizons et Mars Science Laboratory déjà lancées ou sur le point d'être lancées. La NASA joue également un rôle fondamental dans les recherches en cours sur le changement climatique.

Le programme spatial habité de la NASA est depuis 2009 en cours de restructuration à la suite du retrait de la navette spatiale américaine programmé pour 2011 et de la remise en cause du programme Constellation confronté à des problèmes de conception et de financement. L'administration Obama, suivant les recommandations de la commission Augustine, a décidé d'abandonner le projet de retour d'astronautes sur le sol lunaire à l'horizon 2020 au profit d'une démarche d'exploration plus progressive qui doit être précédée par des recherches poussées notamment dans le domaine de la propulsion. Dans cette optique ont été mis en chantier le développement du lanceur lourd Space Launch System et de la capsule associée Orion, Pour pallier l'absence de système de desserte de la station spatiale après le retrait de la navette spatiale, la NASA s'appuie au cours de la décennie 2010 sur le secteur privé qui doit prendre en charge la desserte en orbite basse de la station spatiale internationale.

En 1956, les États-Unis et l'URSS ont annoncé, chacun de leur côté, qu'ils lanceront un satellite artificiel dans le cadre des travaux scientifiques prévus pour l'Année géophysique internationale (juillet 1957 — décembre 1958).

Aux États-Unis, le développement du satellite et de son lanceur est pris en charge par le programme Vanguard, confié à une équipe de l'US Navy, mais le projet lancé tardivement et trop ambitieux enchaîne les échecs. Le 4 octobre 1957, l'Union soviétique est le premier pays à placer en orbite le satellite Spoutnik 1. C'est un choc pour les responsables et l'opinion publique américains, jusqu'alors persuadés de leur supériorité technique. L'armée de l'Air et l'armée de Terre américaine ont à cette époque également des programmes spatiaux qui exploitent les travaux réalisés autour des missiles balistiques intercontinentaux : c'est l'équipe de Wernher von Braun, travaillant pour le compte de l'Armée de Terre, qui parvient finalement à lancer le premier satellite américain, Explorer 1, le grâce au lanceur Juno I improvisé à partir d'un missile balistique Redstone. Bien que réticent à investir massivement dans le spatial civil, le président américain Dwight D. Eisenhower décide par un décret en date du (le "National Aeronautics and Space Act") la création d'une agence spatiale civile. Celle-ci, baptisée NASA, doit fédérer les efforts américains pour mieux contrer les réussites soviétiques : la course à l'espace est lancée.

La NASA reprend les centres de recherche du NACA, jusque-là tourné vers la recherche dans le domaine de l'aéronautique mais qui depuis quelques années travaillent également sur les projets de lanceur développés par l'Armée américaine notamment dans le domaine de l'aérodynamisme et de la propulsion. Les projets militaires et leurs équipes, dont les ingénieurs commandés par Wernher von Braun, sont rapidement transférés à la NASA. Le premier projet de vol habité développé par la NASA est le programme Mercury, démarré en 1958 avant même la création de l'agence, qui doit permettre le lancement du premier américain dans l'espace. Le , Alan Shepard effectue un premier vol de quinze minutes dans le capsule 7 : mais ce n'est qu'un simple vol suborbital car la NASA ne dispose pas à l'époque d'une fusée suffisamment puissante. Le président John F. Kennedy annonce le lancement du programme Apollo le 25 mai 1961, essentiellement pour reconquérir le prestige américain mis à mal par les succès de l'astronautique soviétique, à une époque où la guerre froide entre les deux superpuissances bat son plein. La NASA mandatée par le président, doit poser un homme sur la Lune avant la fin de la décennie. Il fallut attendre la mission du 20 février 1962 pour que John Glenn devienne le premier astronaute américain à boucler une orbite autour de la Terre. Trois autres vols habités ont lieu en 1962 et en 1963.

Lorsque le programme Mercury s'achève en 1963, des aspects importants du vol spatial, nécessaires pour mener à bien les vols lunaires, ne sont toujours pas maîtrisés. Les dirigeants de la NASA lancent le programme Gemini destiné à acquérir ces techniques sans attendre la mise au point du vaisseau très sophistiqué de la mission lunaire. Ce programme intermédiaire doit remplir trois objectifs :
Le vaisseau spatial Gemini, qui devait initialement être une simple version améliorée de la capsule Mercury, devient un vaisseau sophistiqué de (contre environ pour le vaisseau Mercury), capable de voler avec deux astronautes durant deux semaines. La capsule Gemini est lancée par une fusée Titan II, missile de l'armée de l'air américaine reconverti en lanceur. Le programme rencontre toutefois des problèmes de mise au point. Mais fin 1963, tout est rentré dans l'ordre et deux vols sans équipage ont lieu en 1964 et début 1965. Le premier vol habité Gemini 3 emporte les astronautes Virgil Grissom et John Young le 23 mars 1965. Au cours de la mission suivante, l'astronaute Edward White réalise la première sortie dans l'espace américaine. Huit autres missions, émaillées d'incidents sans conséquence, s'échelonnent jusqu'en novembre 1966 : elles permettent de mettre au point les techniques de rendez-vous spatial et d'amarrage, de réaliser des vols de longue durée (Gemini 7 reste près de 14 jours en orbite) et d'effectuer de nombreuses autres expériences. À l'issue du programme Gemini, les États-Unis ont rattrapé leur retard sur l'URSS.

Dans le domaine des lanceurs, la NASA développe pour le programme Apollo la famille de lanceurs lourds Saturn. Le modèle le plus puissant, Saturn V, permet de placer en orbite basse, un record jamais égalé depuis. Il est conçu pour lancer les deux vaisseaux de l'expédition lunaire : le vaisseau Apollo et le module lunaire Apollo chargé de transporter les astronautes à la surface de la Lune. Une partie de la réussite du programme Apollo a pour origine la mise au point d'un nouveau type de propulsion utilisant l'hydrogène liquide dont la mise au point a débuté à la fin des années 1950 dans le cadre du développement de l'étage Centaur.

Deux accidents graves surviennent au cours du programme Apollo : l'incendie au sol du vaisseau spatial Apollo 1 dont l'équipage périt brûlé et qui entraîna un report de près de deux ans du calendrier et l'explosion d'un réservoir à oxygène du vaisseau spatial Apollo 13 dont l'équipage survécut en utilisant le module lunaire comme vaisseau de secours. Pour atteindre la Lune, une méthode audacieuse de rendez-vous orbital lunaire est retenue, qui nécessite de disposer de deux vaisseaux spatiaux dont le module lunaire destiné à l'alunissage. La fusée géante Saturn V de est développée pour lancer les véhicules de l'expédition lunaire. Le programme utilise un budget considérable (135 milliards de dollars US valeur 2005) et mobilise jusqu'à personnes.

Le , l'objectif est atteint par deux des trois membres d'équipage de la mission Apollo 11, Neil Armstrong et Buzz Aldrin. Cinq autres missions se posent par la suite sur d'autres sites lunaires et y séjournent jusqu'à trois jours. Ces expéditions permettent de rapporter 382 kilogrammes de roche lunaire et de mettre en place plusieurs batteries d'instruments scientifiques. Les astronautes ont effectué des observations "in situ" au cours d'excursions sur le sol lunaire d'une durée pouvant atteindre 8 heures, assistés à partir d'Apollo 15 par un véhicule tout-terrain, le rover lunaire. Les six missions qui ont aluni ont rapporté de nombreuses données scientifiques.

Parallèlement au programme Apollo, la NASA lance plusieurs programmes pour affiner sa connaissance du milieu spatial et du terrain lunaire. Ces informations sont nécessaires pour la conception des engins spatiaux et préparer les atterrissages sur la Lune. En 1965, trois satellites Pegasus sont placés en orbite par une fusée Saturn I afin d'évaluer le danger représenté par les micrométéorites ; les résultats seront utilisés pour dimensionner la protection des vaisseaux Apollo. Les sondes spatiales Ranger (1961–1965), après une longue série d'échecs, ramènent à compter de fin 1964, une série de photos de bonne qualité de la surface lunaire qui permettent d'identifier des sites propices à l'atterrissage. Le programme Lunar Orbiter, composé de cinq sondes qui sont placées en orbite autour de la Lune en 1966–1967, complète ce travail : une couverture photographique de 99 % du sol lunaire est réalisée, la fréquence des micrométéorites dans la banlieue lunaire est déterminée et l'intensité du rayonnement cosmique est mesurée. Le programme permet également de valider le fonctionnement du réseau de télémétrie. Les mesures effectuées indiquent que le champ gravitationnel lunaire est beaucoup moins homogène que celui de la Terre rendant dangereuses les orbites à basse altitude. Le phénomène, sous-estimé par la suite, réduira à l'altitude de l'orbite du Lem d'Apollo 15 dont l'équipage était endormi, alors que la limite de sécurité avait été fixée à pour disposer d'une marge suffisante par rapport aux reliefs. Le 2 juin 1966, la sonde Surveyor 1 effectue le premier alunissage en douceur sur la Lune fournissant des informations précieuses et rassurantes sur la consistance du sol lunaire (le sol est relativement ferme) ce qui permet de dimensionner le train d'atterrissage du module lunaire.

Malgré la priorité accordée au programme Apollo et à l'exploration de la Lune, la NASA lance également à cette époque plusieurs missions vers les autres planètes du Système solaire. Les sondes spatiales dans les années 1960 sont de petite taille et rudimentaires et il faudra attendre la décennie suivante pour disposer de sondes capables d'investigations scientifiques approfondies. Leur fiabilité est faible, aussi sont-elles généralement envoyées par paire. En 1962 la mission Mariner 2 devient la première sonde spatiale à effectuer un survol d'une autre planète (Vénus). Mariner 4 réussit le premier survol de la planète Mars en 1964. Trois autres sondes Mariner réussissent un survol de Vénus en 1967 et 1969.

Dans le domaine du vol habité la période de compétition acharnée avec l'URSS prend fin au début des années 1970 avec la dernière mission Apollo et l'abandon par les Soviétiques de leur programme lunaire habité. Un réchauffement des relations avec l'URSS est scellé symboliquement par le vol soviético-américain du projet Apollo-Soyouz en 1975. Dans ce nouveau contexte, en l'absence d'enjeu international, le président américain Nixon et le Congrès américain refusent de prolonger l'effort financier consenti pour le programme Apollo : le budget de l'agence spatiale qui avait culminé à 4,4 % du budget fédéral en 1965 va rapidement retomber. La station spatiale Skylab, un projet de station spatiale conçu à moindre frais en recyclant des composants du programme Apollo, est lancée. Trois équipages vont l'occuper successivement en 1973-1974 en ayant recours pour leur lancement au stock restant de lanceurs Saturn IB et de vaisseaux Apollo. Mais la station est ensuite abandonnée faute de budget et sera détruite en rentrant dans l'atmosphère en 1979.

La NASA qui plaide pour un programme spatial habité ambitieux doit se limiter au projet de la navette spatiale, un engin réutilisable dont l'objectif est d'abaisser fortement le coût de la mise en orbite. Le feu vert est arraché aux décideurs en 1972 en intégrant dans le cahier des charges de la navette les besoins du département de la défense des États-Unis et en révisant à la baisse les ambitions initiales du programme. Le développement, plus long que prévu, va se prolonger jusqu'au début de la décennie suivante.

Columbia, première des quatre navettes spatiales, effectue son premier vol le . Le projet est un grand succès technique mais les coûts opérationnels des navettes s'avèrent beaucoup plus élevés que ce qui était prévu. La catastrophe de Challenger le remet en cause le dogme du tout navette et les lanceurs classiques, qui avaient été abandonnés, doivent être remis en fonction. La navette abandonne en particulier le lancement des satellites commerciaux.

Alors que les relations avec l'Union soviétique se dégradent à nouveau, le président Ronald Reagan demande en à la NASA de lancer un projet de station spatiale consacrée à la recherche scientifique et qui soit occupée en permanence. Il annonce le , au cours de son discours annuel sur l'état de l'Union, la volonté des États-Unis d'entreprendre sa construction en coopération avec d'autres pays. Le coût du projet est alors estimé à huit milliards de dollars.

La course à l'espace entre les deux puissances spatiales touche également l'exploration planétaire. L'Union soviétique réussit avec la sonde Venera 7 (1970) le premier atterrissage sur une autre planète du Système solaire. La NASA de son côté choisit de privilégier pour son programme d'exploration la planète Mars qui, contrairement à Vénus, abrite peut-être la vie et qui pourrait faire l'objet dans le futur d'une mission habitée. Alors que l'URSS consacre tout un programme à Vénus, la NASA ne lance au cours de la décennie qu'une seule mission double vers cette planète : le projet Pioneer Venus, à l'étude depuis 1965, subit plusieurs reports dus aux réductions budgétaires avant de recevoir le feu vert en 1975 et d'être lancé en 1978. Le projet, qui sera une réussite, comporte d'une part 4 sondes atmosphériques d'autre part un orbiteur qui transmettra des données jusqu'en 1992.

Au milieu des années 1960, la NASA travaille sur une mission ambitieuse vers la planète Mars, le projet Voyager, qui se révèlera trop complexe et trop cher. À la place sont développées les sondes spatiales Mariner 8 et Mariner 9 qui sont lancées en 1971. La fusée de Mariner 8 a une défaillance mais Mariner 9 atteint Mars en 1972 et devient la première sonde spatiale à se placer en orbite autour d'une autre planète. Mais pour répondre à la question de la vie sur Mars il faut faire parvenir une sonde jusqu'au sol martien pour que celle-ci puisse effectuer des mesures directes. Les deux sondes programme Viking sont lancées vers Mars : le programme comporte deux atterrisseurs et deux orbiteurs et constitue le premier projet d'exploration planétaire. Le lancement planifié en 1973 est reporté à 1975 en raison de restrictions budgétaires et de dépassements des coûts de développements. Les deux atterrisseurs parviennent sur le sol martien en 1976 et transmettent des données jusqu'en 1982. De leur côté les orbiteurs fonctionneront bien au-delà de la durée de vie prévue jusqu'en 1980.

Dans le cadre du plan d'exploration à long terme de Mars, le projet Viking devait être suivi d'un orbiteur chargé d'étudier le climat de Mars et d'un rover mobile (astromobile). Pour des raisons à la fois financières et politiques, ces projets ne seront débloqués que dans les années 1990 avec l'orbiteur Mars Observer et dans les années 2000 avec les rovers Spirit et Opportunity.

La seule planète interne à ne pas avoir été explorée au début des années 1970 est Mercure. La NASA décide de développer Mariner 10 dans ce but. La sonde est lancée en 1973 et achève sa mission en 1975 après avoir effectué comme prévu trois survols de la planète. Mariner 10 est la première sonde spatiale à utiliser la technique de l'assistance gravitationnelle.

À la fin des années 1960 la NASA envisage également de lancer des sondes vers les planètes externes. Un alignement de ces planètes, très rare, doit se produire à la fin des années 1970 permettant à une seule sonde spatiale d'effectuer un survol des quatre planètes externes. Cet événement est à l'origine du projet ' ou ' qui prévoit le lancement de quatre à cinq sondes. Mais ce projet trop coûteux est abandonné en 1970 et remplacé début 1972 par le programme Voyager (qui n'a rien de commun avec le programme éponyme vers Mars). À l'époque les astronomes ignorent si une sonde peut franchir intacte la ceinture d'astéroïdes située entre Mars et Jupiter et si le champ magnétique de Jupiter, particulièrement puissant, présente un risque pour le fonctionnement d'un engin spatial. Pour répondre à ces interrogations le projet des sondes Pioneer 10 et Pioneer 11 est mis sur pied dès 1968. Pioneer 10 est lancée en 1972 et est la première sonde spatiale à survoler Jupiter en décembre 1973. Une année après la sonde jumelle Pioneer 11 quitte à son tour la Terre en avril 1973 et survole Jupiter fin 1974 avant d'effectuer le premier survol de Saturne en 1979. La reconnaissance effectuée par les sondes Pioneer a préparé la voie pour les sondes Voyager 1 et Voyager 2 toutes les deux lancées en 1977. Voyager 1 atteint Jupiter en 1979, Saturne en 1980 et collecte énormément de données inédites. Voyager 2 survole ces deux planètes en 1979 et 1981 et parvient à boucler le Grand Tour en passant près d'Uranus en 1986 et de Neptune en 1989. Les sondes Voyager comptent parmi les projets les plus réussis de la NASA.

À la fin des années 1970, la situation de la NASA se dégrade fortement. Après l'achèvement du programme Apollo de nombreux salariés doivent quitter l'agence et les moyens financiers qui subsistaient sont en grande partie absorbés par le projet de la navette spatiale. Les responsables politiques ne s'intéressent pas au programme spatial. Dans ces conditions peu de missions nouvelles voient le jour.

En 1974 un projet appelé initialement "Jupiter Orbiter/Probe (JOP)" et rebaptisé plus tard Galileo est proposé mais il ne commence à être financé qu'en 1977. La sonde doit être lancée en 1982 par la navette spatiale mais le retard pris dans la mise au point de la navette repousse son lancement jusqu'en 1986 ; le gouvernement Reagan envisage à un moment d'annuler le programme alors que l'engin est achevé à 90 % et il faudra des pressions officielles très importantes pour le sauver. L'accident de Challenger repousse son lancement jusqu'en 1989 et la sonde atteint le système de Jupiter en 1995 où elle démarre sa mission qui s'achèvera en 2003. La seconde mission conçue à la fin des années 1970 et au début des années 1980 est la sonde VOIR (Venus Orbiting Imaging Radar) qui doit effectuer une cartographie de la planète Vénus grâce à son radar. De nouvelles réductions budgétaires aboutissent à son annulation. Une autre sonde scientifique à destination du Soleil "International Solar Polar Mission" est annulée à la même époque. Pour les remplacer des expériences scientifiques américaines sont placées sur la sonde européenne jumelle Ulysses. En 1979 la sonde de la NASA qui devait être lancée vers la comète de Halley en même temps que la sonde européenne Giotto est également annulée.
En 1983 une nouvelle stratégie reposant sur la réalisation de sondes à coûts modérés est mise en place par la NASA. Quatre missions sont proposées : une mission VOIR simplifiée, un orbiteur martien, la sonde "Comet Rendezvous Asteroid Flyby (CRAF)" et la sonde "Saturn Orbiter/Titan Probe (SOTP)". La sonde VOIR est reconfigurée avec une charge utile réduite à un unique instrument et utilisant des pièces de rechange des sondes précédentes. La nouvelle sonde qui a été renommée Magellan doit être lancée en 1988 mais ne le sera finalement qu'en 1989 à la suite de l'accident de Challenger. Magellan remplit avec succès sa mission en effectuant une cartographie à haute résolution du sol de Vénus entre 1990 et 1992.

Ronald Reagan annonce en 1983 le lancement de l'Initiative de défense stratégique puis en 1984 la construction de la station spatiale , noyau de la future Station spatiale internationale. Dans les années qui suivent le budget consacré aux sondes spatiales est en hausse. Au titre du budget 1984 est lancé le développement de "Mars Geoscience/Climatology Orbiters (MGCO)", qui deviendra plus tard Mars Observer et qui doit prendre la suite du programme Viking et de la sonde Mariner 9. Le lancement programmé pour 1990 est repoussé à 1992 à cause de l'accident de Challenger. Malheureusement le contact avec la sonde est perdu au moment où celle-ci va s'insérer en orbite autour de Mars. À cette date, c'est l'erreur la plus coûteuse du programme des sondes spatiales de la NASA et c'est la première sonde qui subit un échec depuis 1967. Sa mission est en grande partie reprise par les sondes Mars Global Surveyor et 2001 Mars Odyssey lancées à la fin des années 1990 et au début des années 2000. Une troisième sonde, Mars Climate Orbiter, qui devait compléter la couverture des deux engins précédents, est un échec.

Dans le cadre du budget 1990, des fonds sont dégagés pour les projets Cassini-Huygens (ancien SOTP) et la sonde spatiale CRAF à destination d'une comète. L'augmentation des coûts de la station spatiale et de fortes contraintes budgétaires obligent en 1991 à restreindre la charge utile de CRAF à deux instruments puis la sonde elle-même est annulée en 1993. Cassini est par contre construite et lancée en 1997. La sonde réalise avec succès sa collecte de données dans le système de Saturne qu'elle atteint en 2004. Une autre mission marquante de cette époque est le télescope spatial Hubble qui avait été construit dès 1977 et devait initialement être lancé en 1986.

Le changement politique en Russie permet de mettre en place un accord de coopération spatiale entre les États-Unis et la Russie ratifié fin 1992 par les présidents George Bush et Boris Eltsine : des astronautes américains pourront effectuer des séjours de longue durée dans la station Mir. La NASA, qui met en application l'accord comme une répétition des vols vers la future station spatiale, règle 400 millions de dollars de coût de séjour à l'agence spatiale russe. Plusieurs missions se succèdent entre 1995 et 1998 au cours desquelles onze astronautes américains passent 975 jours à bord de la station Mir vieillissante. À neuf reprises, les navettes spatiales américaines ravitaillent la station Mir et assurent la relève des équipages.

Fin 1993, la Russie devient également un acteur majeur du programme de la Station spatiale internationale qui jusqu'à présent n'a pu démarrer faute de consensus sur son financement. L'agence spatiale russe doit fournir quatre modules pressurisés tandis que ses vaisseaux participeront au ravitaillement et à la relève des équipages. La nouvelle mouture de la station spatiale doit comporter deux sous-ensembles : la partie américaine héritée du projet et la partie russe basée sur « Mir 2 » successeur prévu de Mir. Le feu vert pour le lancement de la construction est donné en 1998.

La NASA avec Lockheed Martin développe un prototype de navette à l'échelle ½. Le X-33 est un engin mono-étage, entièrement réutilisable. Il incorpore un moteur aerospike sans divergent. Mais en février 2001, après avoir dépensé 1,3 de $, le projet est abandonné.

Au début des années 1990 deux sondes spatiales de la NASA très coûteuses (près d'un milliard de dollars chacune) essuient des échecs. La mission Mars Observer échoue complètement, tandis qu'un problème d'antenne limite fortement le volume des données transmises par la sonde Galileo. Dans les sphères politiques, les projets d'exploration solaire qui nécessitent de longs développements et comportent une part de risque non négligeable sont désormais considérés avec méfiance et il est demandé à la NASA de réduire le budget consacré à chaque mission. L'administrateur de la NASA Daniel Goldin adopte à cette époque le slogan "« »" ("plus vite, mieux, moins cher") qui se traduit notamment par la mise sur pied du programme Discovery : les missions sont plus petites et plus spécialisées, emportent moins d'instruments scientifiques mais en contrepartie sont moins chères, moins complexes et sont donc développées plus rapidement. Les deux premières sondes de ce programme sont lancées en 1996 : NEAR doit approcher une comète et Mars Pathfinder est un démonstrateur technologique. Au cours de la même décennie seront également lancées la sonde lunaire Lunar Prospector en 1998 et en 1999.

Le nouveau slogan est également appliqué aux programmes existants. À la suite de l'échec de Mars , il a été décidé d'envoyer de nouvelles sondes vers Mars. À compter de 1994 et pour les 10 années à venir une nouvelle sonde devait partir tous les 26 mois. Mars Global Surveyor qui reprend une grande partie des instruments de "Mars Observer" est la première à être lancée en 1996 : la mission est un succès et la sonde fournira des données jusqu'en 2006. Mais les missions suivantes "Mars Climate Orbiter" (1998) et "Mars Polar Lander" (1999) sont toutes deux des échecs. Le dogme du «faster, better, cheaper» est remis en cause. La mission suivante 2001 Mars Odyssey (2001) sera un succès mais désormais les sondes spatiales seront mieux financées.

À la fin des années 1980, la NASA tente de lancer à côté de la station spatiale Freedom d'autres projets importants bloqués depuis longtemps. Les critiques des autorités vis-à-vis de la NASA à la suite de l'accident de la navette Challenger sont en partie contrebalancées dans le public par le rôle joué par l'agence spatiale dans la confirmation du trou dans la couche d'ozone qui avait été découvert en 1985. Dans ce contexte, la NASA décide de faire de l'observation de la Terre une composante majeure de son programme : le projet « » ("Mission pour la planète Terre") est proposé en 1987 et mis en place officiellement en 1990. Son noyau est constitué par l'Earth Observing System (EOS) ; celui-ci doit débuter par le lancement de deux gros satellites sophistiqués. Pour des raisons budgétaires, le planning est revu au début des années 1990 : trois satellites de taille moyenne doivent désormais constituer le cœur d'EOS. Le satellite Terra est lancé en 1999, Aqua en 2002 et Aura en 2004. Toutefois, le premier engin spatial dont la mission répond aux préoccupations environnementales à l'origine de ' est le satellite UARS. Lancé en 1991, il permet à la NASA de fournir des données clés sur la destruction de la couche d'ozone et est chargé de vérifier l'application par les États du protocole de Montréal, qui proscrit l'utilisation des gaz destructeurs. Les autres missions importantes d'EOS sont les satellites TOPEX/Poseidon, lancé en 1992, et Tropical Rainfall Measuring Mission, lancé en 1997 et dont les contributions confirment la place essentielle du satellite dans la prévision des phénomènes météorologiques et en particulier de ses manifestations les plus violentes. Au début des années 1990, les préoccupations concernant le réchauffement climatique prennent le pas sur les travaux qui concernent la couche d'ozone. Le point de départ des recherches sur le sujet est une série de travaux effectués au début des années 1970 pour répondre aux préoccupations environnementales soulevées par la fréquence prévisionnelle très élevée des lancements de la navette spatiale américaine qui étaient susceptibles d'affecter la composition de la stratosphère. Des lois sont passées au Congrès américain en 1975 et 1977 élargissant le domaine d'intervention de la NASA à la recherche environnementale. Une nouvelle classe de satellites d'observation de la Terre est mise en œuvre à compter de 1972 avec le lancement d", rebaptisé plus tard Landsat 1. Les sondes Viking avaient cartographié en 1976 pratiquement l'ensemble de la planète Mars pour identifier des sites propices à l'atterrissage. La méthode de recherche utilisée, qui jusque-là n'avait été appliquée qu'aux autres planètes, va être mise en œuvre pour la première fois pour l'observation de la Terre avec le satellite Seasat lancé en 1978. Dans les années 1980, émergent de nouvelles théories, qui assimilent la Terre à un système global. Émergent également des travaux de recherche comparative entre les planètes, effectués dans le cadre des missions robotiques sur le sol martien et des survols de Vénus par les sondes Mariner dans les années 1960. Il apparaît alors essentiel de réaliser des missions d'exploration scientifique de la Terre pour définir des modèles globaux, ce qui conduit à la mise sur pied de l<nowiki>'</nowiki>"« »" ("Programme de Science de la Terre")

Pour explorer l'univers proche et lointain, la NASA lance un certain nombre de satellites scientifiques et de télescopes spatiaux dont OAO (1972-81), HEAO (1977-79 ), IRAS (1983), FUSE (1999-2007) et STEREO (depuis 2006). L'étude du fond diffus cosmologique est au cœur des missions lancées vers 1989 avec COBE (1989-93) et WMAP (depuis 2001).

Dans le cadre de son plan "Great Observatory Programs", la NASA lance quatre télescopes spatiaux pour étudier l'univers lointain dans toutes les gammes d'ondes importantes. Le télescope spatial Hubble lancé en 1990 couvre la lumière visible, l'ultraviolet et le rayonnement infrarouge. Le Compton Gamma-Ray Observatory spécialisé dans l'astronomie gamma est lancé en 1991, suivi par le télescope à rayons X Chandra en 1999 et enfin le télescope infrarouge télescope spatial Spitzer en 2003. Ces derniers sont en cours de remplacement par des télescopes encore plus puissants : le Fermi Gamma-ray Space Telescope (2008) et le James Webb Space Telescope (vers 2018).

La décennie 2000 est exceptionnelle pour l'activité d'exploration du Système solaire par les engins de la NASA avec le lancement de 12 sondes interplanétaires et la préparation de trois autres missions qui seront lancées en 2011. Cela résulte en partie de la décision prise au cours de la décennie précédente de réaliser des missions plus modestes mais plus nombreuses. L'exploration de Mars est au cœur de cette activité : l'orbiteur 2001 Mars Odyssey (2001) est suivi par les deux rovers MER (Spirit et Opportunity) (2003), l'orbiteur MRO (2005), l'atterrisseur Phoenix (2007) tandis que le rover de Mars Science Laboratory, le plus gros budget de la décennie initialement programmé en 2009, est repoussé en 2011. Toutes les missions sont des succès et font progresser de manière significative notre connaissance de la planète Mars. L'orbiteur MESSENGER (2004) est chargé d'étudier pour la première fois de manière détaillée la planète Mercure. Les petits corps ne sont pas oubliés avec l'impacteur Deep Impact (2004) lancé vers une comète et l'orbiteur Dawn (2007) qui est chargé d'explorer les deux plus grands corps de la ceinture d'astéroïdes. Le seul échec de la décennie est à imputer à la petite sonde CONTOUR (2002) chargée de survoler plusieurs comètes et sans doute victime d'une défaillance de son système de propulsion. Pour les planètes extérieures, la mission de la sonde Cassini-Huygens envoyée vers le système saturnien la décennie précédente est un succès total. New Horizons (2006) est lancée dans un voyage à très long cours qui doit l'amener à proximité de Pluton en 2014. Enfin dans le cadre du programme Constellation deux missions de reconnaissance sont lancées vers la Lune, l'orbiteur Lunar Reconnaissance Orbiter (2009) et l'impacteur LCROSS (2009).

La navette spatiale "Columbia" se désintègre le entraînant le décès de son équipage et une interruption de 29 mois des missions des navettes spatiales. Les problèmes logistiques engendrés par cet arrêt conduisent à un arrêt temporaire des travaux d'assemblage de la station spatiale internationale et à une réduction de l'équipage permanent qui l'occupe.

En réaction à cet accident le président des États-Unis George W. Bush rend public les nouveaux objectifs à long terme assignés au programme spatial américain dans le domaine de l'exploration du Système solaire et des missions habitées qui est formalisé à travers le plan programme Vision for Space Exploration. La définition de cette stratégie est dictée par 2 motivations :
La NASA a décidé parallèlement au programme Constellation de faire appel au privé pour le ravitaillement et la relève des équipages de la station spatiale internationale en attendant la disponibilité des composants du programme Constellation : deux sociétés sont sélectionnées en 2006 et 2008 dans le cadre du programme COTS. Mais leur engagement porte uniquement sur le ravitaillement de la station. Le relève des équipages repose toujours sur le lanceur Ares I et du vaisseau Orion dont la date de disponibilité recule de plus en plus. La viabilité du programme Constellation et les choix techniques effectués sont de plus en plus contestés. Le président Barack Obama nouvellement élu en 2008 demande à la commission Augustine, créée pour la circonstance, d'évaluer le programme spatial habité américain. Celle-ci souligne le manque d'ambition du programme Constellation, dont les objectifs sont proches du programme Apollo. Le financement n'est manifestement pas suffisant (il manque 3 milliards de dollars par an). Le lanceur Ares I, disponible trop tardivement, est jugé de peu d'intérêt. Le comité estime que la NASA doit s'appuyer de manière plus importante sur les opérateurs privés pour tout ce qui relève de l'orbite basse - lanceur, vaisseau cargo et capsule habitée - et se concentrer sur les objectifs situés au-delà de l'orbite basse. Le comité suggère de prolonger l'utilisation de la navette spatiale au-delà de 2010. Prenant le contre-pied du plan lancé par le président Bush, le comité recommande la prolongation jusqu'à 2020 de la durée de vie de la station spatiale internationale pour rentabiliser l'investissement effectué. En matière d'objectifs, le rapport confirme l'intérêt de l'exploration de Mars en tant que but du programme spatial habité mais approuve la nécessité d'une étape intermédiaire qui pourrait être l'exploration de la Lune ou un certain nombre de destinations intermédiaires comme les points de Lagrange, les lunes de Mars, le survol d'un objet géocroiseur ("flexible path"). Enfin le comité fait un certain nombre de constats sur l'organisation de la NASA, suggérant des améliorations dans ce domaine. Le président Obama et la NASA prennent en compte les conclusions du comité et décident pratiquement l'annulation du programme Constellation début 2010 avec des aménagements destinés à limiter l'incidence sur l'emploi au sein de la NASA. Cet abandon est confirmé par le président le 11 octobre 2010 dans le cadre de la validation du « NASA Authorization Act 2010 ».

Le début des années 2010 est marqué par la crise économique mondiale qui touche sévèrement les États-Unis. Le budget de la NASA régresse fortement entre 2011 et 2013 avant d'entamer un rétablissement à compter de 2014.

Le budget du télescope spatial infrarouge JWST, un projet très ambitieux démarré en 2004, accumule les dépassements. Évalué à 3 milliards US$ en 2005, son cout atteint 6,5 milliards US$ en 2010 puis 8,8 milliards US$ en 2013. Malgré l'allongement de la durée des développements (le lancement est repoussé de 2015 à 2018 au début de la décennie), ces surcouts viennent réduire les sommes disponibles pour les autres missions scientifiques. L'impact est aggravé par la crise économique mondiale qui entraine une réduction budgétaire sensible en 2013. L'agence spatiale doit renoncer dès 2011 à un premier projet ambitieux vers la lune Europe Jupiter Europa Orbiter et ne donne pas de suite à OSIRIS-REx dans le cadre du programme New Frontiers. La cadence des lancements des missions à bas cout du programme Discovery, qui est théoriquement de moins de 2 ans, est elle-même ralentie : la mission martienne InSight sélectionnée en 2012 doit être lancée en 2016 alors que la précédente mission l'avait été en 2011. La dernière mission lancée vers les planètes externes (Juno en 2011) n'a aucune successeur en chantier, même si une étude de mission, baptisée Europa Clipper, reçoit un certain soutien du Sénat américain. Mars reste toutefois épargné par cette récession. Après avoir étudié un projet commun avec l'Agence spatiale européenne, la NASA trouve les moyens de lancer le développement d'un successeur à Mars Science Laboratory, baptisé Mars 2020 dont le cout prévu doit être de 1,2 milliards US$ et qui doit recueillir des carottes du sol martien pour une future mission de retour d'échantillons qui n'est ni planifiée ni financée.

La NASA consacre environ un quart de ses ressources financières aux activités purement scientifiques. Celles-ci se répartissent entre quatre thèmes qui par ordre de budget décroissant sont :
Environ 20 % du budget est consacré aux activités de support : gestion des centres spatiaux, maintenance et réalisation d'équipements. La recherche aéronautique, activité d'origine de l'agence, pèse relativement peu (quelque pour-cents du budget). Enfin près de 50 % du budget est consacrée directement ou indirectement au vol spatial habité. Cette partie de l'activité est particulièrement fluctuante.

Le budget de l'année 2015 s'élève à 17,46 milliards de dollars (15,37 mds €) et doit progresser légèrement au cours des années suivantes dans un climat économique qui devrait se rétablir.

L'activité spatiale américaine civile et militaire est répartie entre plusieurs agences. Parmi celles-ci la NASA ne dispose que du deuxième budget par ordre d'importance :

Le programme spatial habité de la NASA est début 2010 en pleine restructuration après l'annulation du programme Constellation et l'arrêt confirmé des navettes spatiales fin 2010. La NASA va devoir durant quelques années s'appuyer lourdement sur ses partenaires pour poursuivre le programme de la station spatiale internationale et en particulier sur l'agence spatiale russe. Le programme COTS n'a pas encore débouché et ne sera manifestement pas prêt à temps pour ravitailler la station spatiale internationale fin 2010. Prenant acte des problèmes de développement rencontrés par le vaisseau Orion, la NASA a décidé de confier début 2010 à des opérateurs privés la relève des équipages : elle a sélectionné le février dans le cadre du programme CCDev les sociétés Boeing et Sierra Nevada Corporation : ces deux sociétés doivent développer un moyen de transport (vaisseau spatial et lanceur) permettant d'amener les astronautes à bord de la station spatiale internationale et d'assurer leur retour sur Terre. La construction des lanceurs du programme Constellation est arrêté mais le développement du vaisseau Orion se poursuit début 2010.

Le programme scientifique représente 26 % du budget de 2011 soit un peu plus de 5 milliards de dollars.

Pour 2014 la NASA consacre millions de dollars soit 7,6 % de son budget aux missions d'exploration du Système solaire. Début 2015 l'essentiel de ce budget est consacré aux 10 sondes spatiales en opération ou en transit et aux trois missions en cours de développement. Ce budget est ventilé entre :

Le programme des planètes extérieures (Outer Planets Program) se limite début 2015 à la mission Cassini-Huygens, lancée en 1997 qui étudie Saturne et ses lunes depuis 2004. Cette mission très ambitieuse (3,3 milliards de dollars dont 2,6 pris en charge par la NASA) menée en coopération avec l'Agence spatiale européenne a été prolongée jusqu'en 2017. Une autre mission extrêmement sophistiquée, Europa Clipper est aujourd'hui en phase de pré-étude et son financement estimé à 2,1 milliards de dollars n'est pas encore bouclé. Son objectif est l'étude de la lune Europe.

La planète Mars fait l'objet d'un programme distinct. Pas moins de cinq missions sont en cours. Mars Odyssey est un orbiteur qui étudie depuis 2002 la géologie de Mars et recherche en particulier la présence de traces d'eau. Mars Reconnaissance Orbiter est un orbiteur lourd (plus de ), embarquant une caméra particulièrement puissante, qui est entrée en service en 2006 et dont la mission principale est d'établir une cartographie détaillée de Mars. Les rover MER, Spirit et Opportunity poursuivent leur mission d'exploration au sol entamée en 2004 qui a été prolongée de nombreuses fois. Mars Science Laboratory emporte le rover Curiosity de (contre pour les rovers MER) qui arpente depuis 2012 le cratère Gale avec d'instruments scientifiques. C'est le projet le plus complexe et le plus coûteux (2,5 milliard de dollars) des dix dernières années. Il doit aider les scientifiques à déterminer si la vie a pu exister sur Mars et à affiner l'étude du climat et de la géologie de la planète. MAVEN (Mars Atmosphere and Volatile EvolutioN) est un orbiteur en orbite autour de Mars depuis 2014 pour étudier son atmosphère. Le rover Mars 2020 qui reprend l'architecture de Curiosity doit être lancé en 2020. Sa mission sera de choisir et collecter des échantillons pour une future mission de retour d'échantillons actuellement ni planifiée ni budgetée.

Le programme New Frontiers regroupe des missions ambitieuses dont le coût est néanmoins inférieur à 700 millions USD. La première mission de ce programme, New Horizons, a été lancée en 2006 afin d'étudier Pluton qu'elle a atteint en 2015, avant de survoler un objet de Kuiper aux confins du Système solaire. Juno, dont le lancement a eu lieu 2011, doit se placer sur une orbite polaire autour de Jupiter pour étudier son champ magnétique. La mission de retour d'échantillon d’astéroïde OSIRIS-REx est en cours de développement et doit être lancée en 2016. Faute de budget, aucune autre mission n'a été sélectionnée. Un appel d'offres devrait être lancé fin 2016 pour une nouvelle mission.

À côté des missions complexes, coûteuses et longues à mettre au point mais de ce fait rares, la NASA développe dans le cadre du programme Discovery des missions dont le coût doit être inférieur à 425 millions USD et dont le délai de développement ne doit pas excéder 36 mois. Le nombre d'instruments scientifiques est réduit et le développement est confié à une seule équipe. Les missions Discovery opérationnelles sont la sonde MESSENGER, lancée en 2008, qui achève sa mission autour de Mercure en 2015, Dawn lancé en 2007 qui s'est placé successivement en orbite autour des astéroïdes Vesta puis Cérès pour les étudier et l'orbiteur lunaire LRO lancé en 2009. Les missions en cours de développement sont l'atterrisseur martien InSight qui doit être lancé en 2016 et sondera l'intérieur de cette planète et l'instrument
STROFIO embarquée à bord de la sonde spatiale BepiColombo de l'Agence spatiale européenne à destination de Mercure. La prochaine mission devrait être sélectionnée en 2016.

Début 2015, la NASA prévoit de lancer Transiting Exoplanet Survey Satellite en 2017, JWST en 2018 et un instrument embarqué sur la sonde japonaise ASTRO-H en février 2016. Les fonds affectés à l'astronomie spatiale en 2014 représentent 7,5 % du budget soit millions de dollars. Ils se répartissent entre plusieurs programmes :

Le télescope spatial Hubble est le plus connu des télescopes spatiaux de la NASA : bien que lancé en 1990 il doit rester en activité encore plusieurs années grâce à la dernière opération de maintenance effectuée à l'aide de la navette spatiale en 2009. Pour les études portant sur l'histoire de l'univers il est assisté par le télescope infrarouge Spitzer lancé en 2003 qui doit être rejoint en 2018 par le JWST : ce télescope infrarouge doté d'un miroir primaire de de diamètre est un projet international lourd de 8,8 milliards de $. Celle-ci a également une participation majeure dans le télescope européen Herschel lancé en 2009. Le deuxième projet en cours, SOFIA, est un télescope infrarouge aéroporté développé avec l'agence spatiale allemande et installé à bord d'un Boeing 747.

Plusieurs observatoires spatiaux de la NASA recueillent des données permettant de répondre à des questions fondamentales sur les origines de l'univers : Chandra télescope à rayons X lancé en 1999 et GLAST observatoire de rayons gamma développé avec plusieurs autres agences spatiales et lancé en 2008. La NASA a également participé à l'observatoire européen Planck lancé en 2009 qui étudie le fond diffus cosmologique dans le domaine des micro-ondes. La NASA évalue en 2015 la mission WFIRST.

Le télescope Kepler, lancé en 2009 est consacré à la recherche d'exoplanètes. La NASA utilise également pour cette recherche le télescope terrestre Keck dont elle est l'un des propriétaires. Deux autres missions sont à l'étude : SIM observatoire spatial utilisant les techniques d'interférométrie et un instrument spécial qui équipe le télescope terrestre Large Binocular Telescope.

Plusieurs télescopes toujours actifs ont contribué à la mise au point de nouvelles technologies : SWIFT est un observatoire en ondes gamma lancé en 2004. WMPA étudie depuis 2001 le fond diffus cosmologique dans le domaine des micro-ondes. GALEX est un télescope ultraviolet lancé en 2003. Enfin la NASA est un coparticipant du télescope rayons X japonais Suzaku lancé en 2005. WISE, lancé en décembre 2009 pour une mission de 6 mois, effectue une cartographie des sources infrarouges à la recherche des galaxies les moins lumineuses, des étoiles froides situées dans la banlieue terrestre et des astéroïdes qui se trouvent dans le Système solaire. NuSTAR pour la détection des trous noirs par observation du rayonnement X doit a été lancé en 2012. La NASA doit fournir le spectromètre du télescope japonais ASTRO-H dont le lancement est planifié en 2015.

Début 2010 la NASA dispose de 17 satellites opérationnels consacrés à l'étude du Soleil, de l'héliosphère et de la magnétosphère en comptant MMS en 2015. Le budget 2014 représente 641 millions de dollars soit 3,6 % du budget total.

L'observatoire solaire ACE lancé en 1993 étudie l'ensemble des radiations et participe à la surveillance de l'activité solaire. SOHO mission conjointe avec l'ESA lancée en 1995 est le principal observatoire utilisé pour la météorologie spatiale et doit rester en activité jusqu'en 2013. Installé au point de Lagrange L1 ce satellite a également découvert un grand nombre de comètes. GEOTAIL n'est plus opérationnel mais ses données sont en cours d'analyse. Le satellite WIND lancé en 1994 étudie le vent solaire et la magnétosphère depuis le point de Lagrange L1 et doit rester en exploitation jusqu'en 2013. TIMED lancé en 2001 étudie l'influence du Soleil sur la thermosphère et la mésosphère terrestres, doit rester en activité jusqu'en 2014. RHESSI lancé en 2002 est réservé pour l'étude des éruptions solaires en activité jusqu'en 2013. Les sondes Voyager participent également à l'étude de l'héliosphère.

Plusieurs missions regroupées sous l'intitulé "Live with a star" sont principalement chargées d'étudier l'interaction entre l'activité solaire et l'atmosphère terrestre. L'observatoire solaire SDO a été lancé début 2010. Les satellites jumeaux RBSP lancés en 2012 doivent étudier les mécanismes à l'œuvre dans les ceintures de Van Allen. Deux missions sont en cours de développement dans le cadre du programme : SPP, dont le lancement est programmé pour 2018, étudie le Soleil à faible distance (10 rayons solaires) tandis que Solar Orbiter, projet mené par l'agence spatiale européenne, doit étudier les interactions entre la surface solaire, la couronne solaire et l'héliosphère intérieure depuis une distance de 45 rayons solaires. DSX est un petit satellite destiné à mettre au point des méthodes permettant de minorer l'influence des éruptions solaires sur les satellites. BARREL désigne un ensemble d'expériences scientifiques embarquées sur des ballons-sonde prévu en 2013 pour compléter les données recueillies par les satellites RBSP.
Le comportement du plasma solaire est étudié par plusieurs missions. Les deux satellites jumeaux STEREO en activité depuis 2007 étudient notamment les éjections de masse coronale. La NASA a embarqué trois instruments sur le satellite japonais Hinode (Solar B) lancé en 2006 qui étudie la relation entre la couronne solaire et le champ magnétique du Soleil. Le satellite MMS qui doit être lancé en 2014 étudiera les reconnexions du champ magnétique à proximité de la magnétosphère terrestre.

La thématique comporte également des missions caractérisées par un cycle de développement court (Small et Medium Explorer). IBEX, lancé en 2008, étudie l'interaction entre le vent solaire et les vents solaires des autres étoiles. TWINS B complète depuis 2008 les observations réalisées par le satellite jumeau TWINS A lancé précédemment et fournit une image tridimensionnelle de la magnétosphère terrestre. Les cinq petits satellites THEMIS (Time History of Events and Macroscale Interactions during Substorms) lancés en 2007 ont permis de mieux comprendre les mécanismes à l'œuvre dans les tempêtes de la magnétosphère. CINDI (Coupled Ion-Neutral Dynamics Investigation) est une expérience scientifique embarquée sur un satellite de l'Armée de l'air qui étudie le rôle des ions neutres sur la formation des champs électriques dans la haute atmosphère terrestre. AIM lancé en 2007 étudie la formation des nuages de haute altitude dans les régions polaires. Deux petites missions sont programmées IRIS étudie le transfert d'énergie entre la couronne solaire et le vent solaire et doit être lancé en 2013.

Début 2010 la NASA dispose de 18 satellites opérationnels consacrés à l'étude de la Terre et du climat. L'agence doit développer et lancer les satellites CYGNSS en 2016, GRACE-FO en 2017 et ICESat-2 en 2018. D'autre part les satellites SWOT (2019), PACE (vers 2020) et NISAR (vers 2020) sont en phase de spécifications. Le budget 2014 représente millions $ soit 10,4 % du budget total.

La NASA dispose d'importantes équipes de chercheurs dont les travaux portent sur la modélisation du système Terre et qui exploitent les données recueillies par les différents moyens spatiaux et aéroportés mis en œuvre par la NASA. Celle-ci possède le plus important système de stockage informatique de données scientifiques de la planète qui doit absorber les plusieurs téraoctets de données produits chaque jour par les satellites. Pour ses activités la NASA dispose de trois super-ordinateur (Pleiades, Merope et Endeavour) comportant en tout processeurs. Dans le cadre de ces recherches des campagnes de mesure sont menées avec des engins aériens avec et sans équipage. Les activités principales portent sur le cycle du carbone, la modélisation du système Terre, l'évolution de la couche d'ozone, la fourniture de références géodésiques Ces activités de recherche et la logistique associée représente 25 % du budget de la NASA (457 millions $) consacré aux sciences de la Terre.

Les missions du programme Earth Systematic Missions ont pour objectif d'effectuer des collectes systématiques de données qui sont ensuite redistribuées à un grand nombre d'utilisateurs internes et externes. Une quinzaine de satellites sont actuellement opérationnels :


Plusieurs satellites et instruments sont à des stades divers de développement. ICESat-2 doit prendre le relais en 2018 de ICESat tombé en panne en 2010 pour la mesure des calottes de glace polaires. Des campagnes de mesures aéroportées seront assurées par la NASA pour assurer la continuité entre la fin de vie de et le lancement du nouveau satellite. Le spectromètre Stratospheric Aerosol and Gas Experiment (SAGE III), qui mesurera la distribution verticale de l'ozone et des aérosols dans l'atmosphère terrestre doit être installé en 2016 dans la station spatiale internationale. GRACE-Follow-On prend la suite du projet germano-américain GTACE. Les deux satellites de cette mission, dont le lancement est prévu en 2018, doivent poursuivre la mesure des variations du champ de gravité terrestre. SWOT (lancement prévu en 2019) est une mission franco-américaine qui à l'aide de mesures altimétriques doit permettre de mesurer avec une très haute résolution la circulation océanique et effectuer un recensement détaillé des eaux de surface sur les continents. La NASA travaille sur trois projets relatifs à l'inventaire des ressources terrestres par l'imagerie : l'instrument TIR-FF doit assurer la continuité des mesures de l'instrument équivalent embarqué sur Landsat 8 ; des améliorations sont à l'étude pour les instruments embarqués sur Landsat 9 (2023) ; un projet de recherche plus fondamental est entamé pour les instruments de Landsat 10. Deux satellites doivent être également développés. PACE (vers 2020) doit mesurer la couleur de l'océan ce qui permet de déterminer les caractéristiques biologiques et biochimiques de celui-ci et ainsi mieux maitriser le cycle du carbone et la réponse des perturbations sur le climat de la Terre. NISAR (lancement vers 2020) est un satellite américano-indien équipé de deux radars qui doit fournir des informations très précises sur des processus complexes comme les perturbations écologiques, l'effondrement de la banquise, les tremblements de terre,etc...

La Station spatiale internationale est utilisée comme support pour différents instruments : "Hyperspectral Imager for the Coastal Ocean" (HICO) installé en 2014 est un spectromètre imageur utilisé pour étudier les eaux côtières, RapidScat également installé en 2014 remplace en partie l'instrument du satellite QuikSCAT qui mesurait la vitesse des vents au-dessus des océans. "Cloud Aerosol Transport System" (CATS) qui fonctionne depuis février 2015 est un lidar expérimental qui mesure la distribution verticale des aérosols dans l'atmosphère terrestre. "Lightning Imaging Sensor" (LIS) qui doit être placé en orbite début 2016 prend le relais de l'instrument équivalent embarqué à bord du satellite TRMM pour l'observation des éclairs dans l'atmosphère terrestre. L'instrument TSIS-1 (lancement en 2018) doit poursuivre la mesure de l'irradiance du Soleil actuellement prise en charge par un instrument équipant le satellite SORCE.

D'autres instruments doivent être lancés à bord de satellites commerciaux ou d'engins spatiaux développés par d'autres agences spatiale. Ce sont TSIS-2 qui doit prendre la suite de TSIS-1 vers 2020, "Radiation Budget Instrument" (RBI) qui doit être installé à bord du satellite JPSS-2 lancé vers 2021 pour mesurer le budget radiatif de la Terre, "Ozone Mapping and Profiler Suite-Limb Profiler" (OMPS-LIMB) également lancé sur ce satellite, CLARREO doit permettre de détecter rapidement les variations climatiques. D'autres missions recommandées par le rapport scientifique annuel sont en cours d'évaluation : "Active Sensing of CO2 Emissions over Nights, Days, and Seasons" (ASCENDS), "GEOstationary Coastal and Air Pollution Events" (GEO-CAPE);ACE et HyspIR.

Le programme Earth System Science Pathfinder qui dispose d'un budget de 267,7 millions US$ regroupe des missions à cout modéré et aux objectifs scientifiques plus ciblés que le programme Earth Systematic Missions. Ce programme comprend les projets en développement suivants :

Par ailleurs plusieurs satellites de ce programme sont opérationnels :

Les données collectées par les satellites d'observation de la Terre sont traitées, stockées et redistribuées dans le cadre du projet Earth Observing System Data and Information System (EOSDIS) auquel est alloué en 2014 un budget de 179 millions US$. Par ailleurs la NASA dispose d'un programme de recherche pour la mise au point de nouveaux instruments (60 millions US$) et le développement d'applications reposant sur les données collectées par ses satellites (35 millions US$).

Le programme Space Technology a pour objectif de mettre au point des concepts avancés applicables au spatial. Ces recherches sont lancées et financées, entre autres, par plusieurs programmes d'encouragement à l'innovation qui concernent des équipes internes ou des partenaires ou sociétés externes. Le programme est doté d'un budget en 2014 de 575 millions $ soit 3,3 % du total.

Pour ses missions d'exploration du Système solaire la NASA a plusieurs projets consacrés à la propulsion spatiale dont le financement est rattaché à celui des sondes spatiales. L'agence met au point le moteur ionique à xénon NEXT (NASA's Evolutionary Xenon Thruster) dans le cadre d'un programme qui devrait aboutir en 2013. L'agence a également un programme d'étude sur le propulseur à effet Hall. Le générateur thermoélectrique à radioisotope est une alternative aux cellules photovoltaïques utilisée lorsque l'énergie solaire n'est pas suffisante (mission vers les planètes extérieures). La NASA étudie une version beaucoup plus efficace grâce à l'utilisation du cycle de Stirling (programme Advanced Stirling Radioisotope Generator ASRG) qui pourrait être utilisé par des sondes spatiales lancées à compter de 2014-2016.

La NASA est le principal centre de recherche aérospatiale américain. Le budget affecté en 2011 à cette activité est de 566 millions de $ soit 3,2 % du budget total. Ces fonds sont répartis entre cinq programmes de recherche :
La NASA travaille sur le futur système de contrôle du trafic aérien américain NextGen qui doit permettre de faire face à l'augmentation du nombre de vols dans l'espace aérien américain. L'agence spatiale travaille, entre autres, sur les dispositifs capables de détecter automatiquement les situations dangereuses (risque de collision en vol) et la conception d'un poste de pilotage d'avion optimisant le travail de l'équipage et sa capacité à faire face rapidement aux événements à risque.
Ce programme concerne la recherche des stratégies d'automatisation du choix des routes aériennes dans le cadre du futur système de contrôle du trafic aérien américain NextGen.
Plusieurs thèmes font partie de ce programme : mise au point des techniques de voilure tournante, mise au point d'une boîte à outils permettant de concevoir la voilure des futurs avions volant à vitesse subsonique en optimisant les émissions sonores et les performances. Outils de conception du fuselage et de la voilure des avions supersoniques. Recherche sur le vol hypersonique (vitesse supérieure à Mach 5) avec des applications dans le domaine spatial (rentrée atmosphérique, atterrissage sur Mars).
Ce programme concerne la mise à disposition de moyens d'essais : souffleries, bancs d'essais aéronautiques.
Recherche sur de nouveaux concepts d'aéronefs permettant de réduire simultanément la quantité de carburant consommé, le bruit et les émissions de gaz. Insertion des avions sans pilote dans le trafic aérien.

L'administrateur de la NASA est désigné par le président des États-Unis, après consultation et accord du Sénat américain. L'administrateur de la NASA en poste depuis mai 2009 est Charles F. Bolden, un ancien astronaute. Le siège de la NASA se trouve à Washington (district de Columbia). Quatre directions (directorate) sont chargées de définir la politique spatiale et de contrôler sa mise en œuvre par les différents centres de la NASA : la direction de la Recherche Aéronautique (directorate ARMD), celle des Sciences (SMD) chargée de l'exploration scientifique de la Terre, du Système solaire et de l'univers, la direction des Systèmes d'Exploration (EMSD) qui réalise les développements nécessaires aux vols humains et robotiques et enfin la direction des opérations Spatiales (SOMD) responsable des lancements et du suivi des missions.

La NASA comporte 10 centres spatiaux qui emploient directement personnes auxquels s'ajoutent 4500 personnes au Jet Propulsion Laboratory fin 2014 ainsi qu'un grand nombre de sous-traitants sur site :

Le Centre spatial Lyndon B. Johnson (ou MSC Manned Spacecraft Center) situé près de Houston au Texas, est chargé de la conception et la qualification des engins spatiaux habités (station spatiale, vaisseaux spatiaux), l'entraînement des astronautes et le suivi des missions à partir de leur décollage. Parmi les installations présentes sur le site, on trouve le centre de contrôle des missions habitées (station spatiale internationale, navette spatiale), les simulateurs de vol et des équipements destinés à simuler les conditions spatiales et utilisés pour tester les composants livrés par les fournisseurs de la NASA. Le centre gère l'établissement de White Sands au Nouveau-Mexique qui est utilisé pour tester différents équipements faisant essentiellement partie du programme de la navette spatiale.

Le Centre de vol spatial Marshall (George C. Marshall Space Flight Center ou MSFC) situé près de Huntsville dans l'Alabama est spécialisé dans la propulsion des lanceurs civils et des vaisseaux spatiaux. Aujourd'hui le centre Marshall est responsable de la propulsion de la navette spatiale américaine et des charges utiles de celle-ci ainsi que des lanceurs et vaisseaux du programme Constellation. Le centre est responsable de l'établissement de Michoud où est assemblé le réservoir externe de la navette spatiale. Il gère également le programme des sondes lunaires. Cette ancienne installation de l'Armée de Terre (Redstone Arsenal) autrefois dirigée par Wernher von Braun a mis au point la famille de lanceurs Saturn.
Le Jet Propulsion Laboratory (JPL), situé près de Los Angeles en Californie, est responsable du développement et de la gestion opérationnelle de la majorité des sondes spatiales de la NASA, de certains satellites d'observation de la Terre ainsi que d'instruments embarqués sur les satellites scientifiques en orbite terrestre. Le JPL gère également les 3 groupes d'antennes situés en Australie, Espagne et Californie du Deep Space Network qui est utilisé pour les communications avec les sondes spatiales. Créé dans les années 1930 pour étudier la propulsion des fusées, à l'origine de son appellation, c'est une coentreprise entre la NASA et le Caltech.

Le Centre de recherche Ames est un établissement ancien (1939) situé en Californie au cœur de la Silicon Valley. Initialement connu pour ses souffleries utilisées notamment pour mettre au point la forme de la capsule Apollo, l'établissement est aujourd'hui spécialisé dans l'informatique embarquée sur les vaisseaux et sondes, les supercalculateurs, la gestion du trafic aérien ainsi que l'exobiologie. Le centre est responsable de quelques programmes spatiaux comme les sondes lunaires LCROSS, LADEE, le télescope spatial Kepler et le télescope aéroporté SOFIA.

Le Centre de recherche de Langley situé en Virginie est le plus ancien des centres de la NASA (1917). On y effectue des recherches sur l'aérodynamisme des avions à l'aide de plusieurs souffleries. Les chercheurs de Langley travaillent également sur le changement climatique.

Le Centre du vol spatial Goddard, situé à environ au nord-est de Washington DC dans l'État du Maryland, est le plus important centre de recherche de la NASA et emploie environ personnes en incluant les sous-traitants. L'établissement est responsable du développement et de la gestion des télescopes et observatoires spatiaux développés par la NASA ainsi que sur la plupart des satellites d'observation de la Terre. L'établissement gère également le Centre de Wallops Island consacré au lancement de ballons, de fusées-sondes ou de petits satellites scientifiques (à l’aide notamment de fusées Scout).

Le Centre de recherche Glenn, situé près de Cleveland dans l'Ohio, est traditionnellement spécialisé dans la mise au point des techniques de propulsion (cryogénique, électrique).
Le Centre de recherche en vol de Dryden situé dans le désert des Mojaves en Californie est utilisé pour effectuer des tests en vol atmosphérique. C'est également le site d'atterrissage de secours pour la navette spatiale lorsque les conditions atmosphériques ne sont pas favorables à Kennedy.

Le Centre spatial John C. Stennis situé dans le Mississippi rassemble plusieurs bancs d'essais utilisés pour tester les moteurs-fusées développés pour les différents programmes.

Le Centre spatial Kennedy (KSC), situé sur l'île Meritt en Floride, est le site d'où sont lancées les navettes spatiales. Au cœur du centre spatial, le complexe de lancement 39 comporte 2 aires de lancement et un immense bâtiment d'assemblage, le VAB (hauteur ), dans lequel la navette spatiale est préparée. Plusieurs plates-formes de lancement mobile permettent de transporter la navette jusqu'au site de lancement. Le centre spatial Kennedy jouxte la base de lancement de Cap Canaveral d'où sont lancés les sondes spatiales de la NASA.





</doc>
<doc id="14139" url="https://fr.wikipedia.org/wiki?curid=14139" title="Bactérie">
Bactérie

Le terme bactérie est un nom vernaculaire qui désigne certains organismes vivants microscopiques et procaryotes présents dans tous les milieux. Le plus souvent unicellulaires, elles sont parfois pluricellulaires (généralement filamenteuses), la plupart des espèces bactériennes ne vivant pas ìndividuellement en suspension, mais en communautés complexes adhérant à des surfaces au sein d'un gel muqueux (biofilm).

Les bactéries les plus grosses mesurent plus de 2 μm et, jusqu'au début du , les spécialistes considéraient que les plus petites mesuraient 0,2 μm, mais il existe des « ultramicrobactéries », y compris en eau douce.
Les bactéries présentent de nombreuses formes : sphériques (coques), allongées ou en bâtonnets (bacilles), des formes plus ou moins spiralées. L’étude des bactéries est la bactériologie, une branche de la microbiologie.

Il existe environ espèces connues à ce jour, mais la diversité réelle du groupe est probablement supérieure. L'estimation du nombre des espèces oscillerait entre 5 et 10 millions.

Les bactéries sont ubiquitaires et sont présentes dans tous les types de biotopes rencontrés sur Terre. Elles peuvent être isolées du sol, des eaux douces, marines ou saumâtres, de l’air, des profondeurs océaniques, des déchets radioactifs, de la croûte terrestre, sur la peau et dans l’intestin des animaux. Les bactéries ont une importance considérable dans les cycles biogéochimiques comme le cycle du carbone et la fixation de l’azote de l’atmosphère.

Chez l'humain, il a été calculé que 10 bactéries colonisent la peau, 10 bactéries colonisent la bouche et 10 bactéries habitent dans l'intestin, ce qui fait qu'il y a dix fois plus de cellules bactériennes que de cellules humaines dans le corps humain. La plupart de ces bactéries sont inoffensives ou bénéfiques pour l’organisme. Il existe cependant de nombreuses espèces pathogènes à l'origine de beaucoup de maladies infectieuses comme le choléra, la syphilis, la peste, l’anthrax, la tuberculose.

Les bactéries peuvent être très utiles à l’humain lors des processus de traitement des eaux usées, dans l’agroalimentaire lors de la fabrication des yaourts ou du fromage et dans la production industrielle de nombreux composés chimiques.

Les bactéries étant microscopiques, elles ne sont donc visibles qu'avec un microscope. Antoine van Leeuwenhoek fut le premier à observer des bactéries, grâce à un microscope de sa fabrication, en 1668. Il les appela « animalcules » et publia ses observations dans une série de lettres qu'il envoya à la Royal Society.

Au , les travaux de Louis Pasteur ont révolutionné la bactériologie. Il démontra en 1859 que les processus de fermentation sont causés par des microorganismes et que leur croissance n’était pas due à la génération spontanée. Il démontra aussi le rôle des microorganismes comme agents infectieux. Pasteur conçut également des milieux de culture, des procédés de destruction des microorganismes comme l’autoclave et la pasteurisation.

Le médecin allemand Robert Koch et ses collaborateurs mirent au point les techniques de culture des bactéries sur milieu solide. Robert Koch est un des pionniers de la microbiologie médicale, il a travaillé sur le choléra, la maladie du charbon (anthrax) et la tuberculose. Il démontra de façon claire qu’une bactérie pouvait être l’agent responsable d’une maladie infectieuse et il proposa une série de postulats (les postulats de Koch, toujours utilisés aujourd'hui) confirmant le rôle étiologique d’un microorganisme dans une maladie. Il obtient le prix Nobel de physiologie ou médecine en 1905.

Si les bactéries étaient connues au , il n’existait pas encore de traitement antibactérien. En 1909, Paul Ehrlich mit au point un traitement contre la syphilis avant l’utilisation de la pénicilline en thérapeutique suggérée par Ernest Duchesne en 1897 et étudiée par Alexander Fleming en 1929. Ehrlich reçut le prix Nobel pour ses travaux sur l'immunologie en 1908, et fut un pionnier de l'usage de colorants pour détecter et identifier les bactéries, son travail étant la base de la coloration de Gram et de la coloration de Ziehl-Neelsen.

Les microbiologistes Martinus Beijerinck et Sergei Winogradsky initièrent les premiers travaux de microbiologie de l’environnement et d’écologie microbienne en étudiant les relations entre ces microorganismes au sein de communautés microbiennes du sol et de l’eau.

Le mot « bactérie » apparaît pour la première fois avec le microbiologiste allemand Christian Gottfried Ehrenberg en 1838. Ce mot dérive du grec , qui signifie « bâtonnet ». Parallèlement Haeckel inventa en 1866 l'embranchement Monera pour regrouper au sein de son règne Protista tous les microorganismes sans structure interne (bien qu'excluant les cyanobactéries, alors classées parmi les plantes). Ferdinand Cohn utilisa à son tour le terme Bacteria comme taxon en 1870 et tenta le premier de les classer rigoureusement selon leur morphologie. Pour Cohn, les bactéries étaient des plantes primitives non chlorophylliennes. À la suite des travaux de Cohn, Haeckel révisa la circonscription de ses "monères" pour y inclure les cyanobactéries. Les termes de "monère" et de "bactérie" devinrent alors synonymes.

En 1938 Herbert Copeland éleva les monères au rang de règne, à un niveau désormais égal aux animaux, plantes et protistes. Ce n'est qu'en 1957 qu'André Lwoff distingua avec clarté les concepts de bactérie et de virus grâce à des arguments biochimiques et structuraux. Enfin Roger Stanier et Cornelis van Niel définirent pour la première fois rigoureusement en 1962 le concept de bactérie par l’absence d’organite membrané (et en particulier de véritable noyau, donc de mitose).

Liste alphabétique de noms vulgaires ou de noms vernaculaires attestés en français. 


En 1977, Carl Woese grâce à ses travaux de phylogénie moléculaire divisa les procaryotes en deux domaines : les Eubacteria et les Archaebacteria ; il les renomma respectivement Bacteria et Archaea lors de la révision de sa nomenclature en 1990. Le mot "bactérie" faisant référence à l'ensemble des procaryotes avant 1990, ce renommage a provoqué une certaine ambigüité dans l'utilisation de ce terme. Ce renommage n'a donc pas été accepté par tous les biologistes.

Certains biologistes pensent que cette tentative de renommage tient davantage de la propagande (de la part de Carl Woese, afin d'accréditer ses idées) que de la science :
Et plus loin dans le même article :

Dans un cadre kuhnien la théorie des trois domaines qui sous-tend ce changement de nomenclature est parfois analysé comme un paradigme de la bactériologie moderne, ce qui expliquerait les résistances (principalement de nature sociologiques) contre sa remise en cause.

Les bactéries présentent une grande diversité de tailles et de formes. Les cellules bactériennes typiques ont une taille comprise entre 0,5 et de longueur, cependant, quelques espèces comme "Thiomargarita namibiensis" et "Epulopiscium fishelsoni" peuvent mesurer jusqu’à () de long et être visibles à l’œil nu. Parmi les plus petites bactéries, les mycoplasmes mesurent , soit une taille comparable à certains gros virus.

La plupart des bactéries sont soit sphériques soit en forme de bâtonnets. Dans le premier cas elles sont appelées coques (du grec "kókkos", grain) et dans le second bacilles (du latin "baculus", bâton). Il existe aussi des formes intermédiaires : les coccobacilles. Quelques bactéries en forme de bâtonnets sont légèrement incurvées comme les "Vibrio". D’autres bactéries sont hélicoïdales. Ce sont des spirilles si la forme est invariable et rigide, des spirochètes si l’organisme est flexible et peut changer de forme. La grande diversité de formes est déterminée par la paroi cellulaire et le cytosquelette. Les différentes formes de bactéries peuvent influencer leur capacité d’acquérir des nutriments, de s’attacher aux surfaces, de nager dans un liquide et d’échapper à la prédation.

Beaucoup d’espèces bactériennes peuvent être observées sous forme unicellulaire isolée alors que d’autres espèces sont associées en paires comme les "Neisseria" ou en chaînette, caractéristique des Streptocoques. Dans ces cas, les coques se divisent selon un axe unique et les cellules restent liées après la division. Certains coques se divisent selon un axe perpendiculaire et s’agencent de façon régulière pour former des feuillets. D’autres se divisent de façon désordonnée et forment des amas comme les membres du genre "Staphylococcus" qui présentent un regroupement caractéristique en grappe de raisins. D'autres bactéries peuvent s’élonger et former des filaments composés de plusieurs cellules comme les actinobactéries. 

En dépit de leur apparente simplicité, elles peuvent former des associations complexes. Des capteurs leur permettent de détecter d'autres bactéries ou une surface (ce qui induit souvent chez elle un changement de comportement ; ainsi "Pseudomonas aeruginosa" ne devient virulente et active ses gènes de résistance que quand son « sens du toucher » l'informe qu'elle entre en contact avec une surface ; muqueuse pulmonaire par exemple).

Les cyanobactéries forment des chaînes appelées trichomes où les cellules sont en relation étroite, grâce à des échanges physiologiques. Certaines bactéries forment des colonies pouvant solidement s’attacher aux surfaces. Ces « biofilms » sont un arrangement complexe de cellules et de composants extracellulaires, formant des structures secondaires comme des microcolonies, au sein desquelles se forme un réseau de canaux facilitant la diffusion des nutriments.

Une caractéristique importante des bactéries est la paroi cellulaire. La paroi donne à la bactérie sa forme et la protège contre l’éclatement sous l’effet de la très forte pression osmotique du cytosol. Les bactéries peuvent être structuralement divisées en deux groupes : les bactéries à paroi unimembranée (ne contenant qu'une seule membrane, la membrane plasmique) et les bactéries à paroi bimembranée (constituée de deux membranes superposées, la membrane interne et la membrane externe). La coloration de Gram est un critère empirique, quoique imparfait, permettant de déterminer la structure de la paroi bactérienne.

Certains organites extracellulaires comme les flagelles ou les poils peuvent être enchâssés dans la paroi cellulaire. Quelques bactéries peuvent fabriquer de fines couches externes à la paroi cellulaire, généralement essentiellement constituées de polysaccharides (des sucres). D'autres bactéries peuvent s’envelopper d’une couche protéique appelée la couche S.

En tant que procaryote (organisme sans noyau), les bactéries sont des cellules relativement simples, caractérisées par une absence de noyau et d’organites comme les mitochondries et les chloroplastes, elles n'ont pas non plus de réticulum endoplasmique ou d'appareil de Golgi.

Les bactéries possèdent un chromosome généralement unique et circulaire (mais il y a des exceptions) qui porte la majorité des gènes. Certains gènes ayant des fonctions particulières (résistance à un antibiotique, un prédateur, adaptation physiologique au milieu, etc.) sont cependant localisés sur des petites sections d'ADN circulaire libres appelées plasmides.

Il existe une grande diversité de métabolismes par rapport aux eucaryotes. D'ailleurs la phototrophie et l'autotrophie chez les eucaryotes sont toujours le résultat d'une symbiose avec des bactéries (certains lichens par exemple) et/ou d'une symbiogenèse impliquant une cyanobactérie (chloroplaste).

Source de matière : Hétérotrophie vs Autotrophie

Source d'énergie : Phototrophie vs Chimiotrophie

Les bactéries, avec les autres micro-organismes, participent pour une très large part à l’équilibre biologique existant à la surface de la Terre. Elles colonisent en effet tous les écosystèmes et sont à l’origine de transformations chimiques fondamentales lors des processus biogéochimiques responsables du cycle des éléments sur la planète.

Au sein des biofilms des relations s'établissent entre bactéries, conduisant à une réponse cellulaire intégrée. Les molécules de la communication cellulaire ou « lang » sont soit des homosérines lactones pour les bactéries à Gram négatif, soit des peptides courts pour les bactéries à Gram positif. De plus au sein de biofilms établis, les caractéristiques physico-chimiques (pH, oxygénation, métabolites) peuvent être néfastes au bon développement bactérien et constituer donc des conditions stressantes. Les bactéries mettent en place des réponses de stress qui sont autant d'adaptation à ces conditions défavorables. En général les réponses de stress rendent les bactéries plus résistantes à toute forme de destruction par des agents mécaniques ou des molécules biocides.

L'étude des canaux ioniques bactériens a permis à une équipe de chercheurs de mettre en évidence, en 2015, une synchronisation du métabolisme de certaines bactéries au sein des communautés de biofilms bactériens par des vagues d'ions potassium. Celles-ci résultent d'une boucle de rétroaction positive, dans laquelle un déclencheur métabolique induit la libération d'ions potassium intracellulaire, qui à son tour dépolarise les bactéries voisines. Cette vague de dépolarisation coordonne les états métaboliques entre les bactéries à l'intérieur et à la périphérie du biofilm. La suppression ou le blocage de l'activité des canaux potassium supprime cette réponse.

Les eaux naturelles comme les eaux marines (océans) ou les eaux douces (lacs, mares, étangs, rivières, etc.) sont des habitats microbiens très importants. Les matières organiques en solution et les minéraux dissous permettent le développement des bactéries. Les bactéries participent dans ces milieux à l’autoépuration des eaux. Elles sont aussi la proie des protozoaires. Les bactéries composant le plancton des milieux aquatiques sont appelées le bactérioplancton.

Il y a environ 40 millions de cellules bactériennes dans un gramme de sol et 1 million de cellules bactériennes dans un millilitre d’eau douce. On estime qu'il y aurait (à un instant donné) quatre à six quintillions ( à ), soit entre quatre et six mille milliards de milliards de milliards de bactéries dans le monde, représentant une grande partie de la biomasse du monde. Cependant, un grand nombre de ces bactéries ne sont pas encore caractérisées car non cultivables en laboratoire.

Le sol est composé de matière minérale provenant de l’érosion des roches et de matière organique (l’humus) provenant de la décomposition partielle des végétaux. La flore microbienne y est très variée. Elle comprend des bactéries, des champignons, des protozoaires, des algues, des virus, mais les bactéries sont les représentants les plus importants quantitativement. On peut y retrouver tous les types de bactéries, des autotrophes, des hétérotrophes, des aérobies, des anaérobies, des mésophiles, des psychrophiles, des thermophiles. Tout comme les champignons, certaines bactéries sont capables de dégrader des substances insolubles d’origine végétale comme la cellulose, la lignine, de réduire les sulfates, d’oxyder le soufre, de fixer l’azote atmosphérique et de produire des nitrates. Les bactéries jouent un rôle dans le cycle des nutriments des sols, et sont notamment capables de fixer l’azote. Elles ont donc un rôle dans la fertilité des sols pour l’agriculture. Les bactéries abondent au niveau des racines des végétaux avec lesquels elles vivent en mutualisme. 
À la différence des milieux aquatiques, l’eau n’est pas toujours disponible dans les sols. Les bactéries ont mis en place des stratégies pour s’adapter aux périodes sèches. Les "Azotobacter" produisent des cystes, les "Clostridium" et les "Bacillus" des endospores ou d’autres types de spores chez les Actinomycètes.

Les bactéries peuvent aussi être rencontrées dans des environnements plus extrêmes. Elles sont qualifiées d’extrémophiles. Des bactéries halophiles sont rencontrées dans des lacs salés, des bactéries psychrophiles sont isolées d’environnements froids comme des océans Arctique et Antarctique, des banquises. Des bactéries thermophiles sont isolées des sources chaudes ou des cheminées hydrothermales.

Le 4 septembre 2007, un forage dans le pergélisol du nord-ouest Canadien a permis à des scientifiques de l'université de Californie dirigée par le professeur Eske Willerslev (Université de Copenhague) de mettre au jour une bactérie vieille d'environ et toujours vivante.

En 2000, une équipe scientifique a annoncé avoir découvert une bactérie demeurée endormie dans un cristal de sel pendant 250 millions d'années. De nombreux scientifiques sont très réservés vis-à-vis de ce résultat, qui serait plutôt dû à une colonisation récente du cristal.

Dans l'espace, les bactéries deviendraient presque trois fois plus virulentes. C'est du moins le cas de Salmonella typhimurium, une bactérie responsable d'intoxication alimentaire. Celles-ci ont fait un voyage à bord de la navette Atlantis en 2006. À leur retour, les bactéries qui avaient été conservées dans un récipient étanche, ont été transmises à des souris. Il n'a fallu que le tiers de la dose habituelle pour tuer la moitié du groupe de souris qui avait été infecté.

On cherche actuellement à savoir s'il a existé une vie bactérienne sur la planète Mars. Certains éléments d'analyse du sol martien semblent s'orienter en ce sens, et la présence abondante d'eau sur Mars jadis a peut-être pu constituer un terrain extrêmement favorable au développement de la vie bactérienne, si elle est apparue. Si la chose venait à être confirmée, ce serait un élément important en faveur de l'hypothèse de panspermie. Des chercheurs écossais ont mis en évidence en juin 2017 que le sol de mars éliminait la moindre bactérie. C’est l’interaction entre le rayonnement ultraviolet, les substances oxydantes du sol de Mars, et surtout les perchlorates qui confère à la surface de la Planète rouge sa capacité à éliminer toute bactérie . D'autres recherches s'intéressent aussi aux glaces de la lune jovienne Europe qui abritent de l'eau liquide sous leur surface.

En dépit de leur apparente simplicité, les bactéries peuvent entretenir des associations complexes avec d’autres organismes. Ces associations peuvent être répertoriées en parasitisme, mutualisme et commensalisme. En raison de leurs petites tailles, les bactéries commensales sont ubiquitaires et sont rencontrées à la surface et à l’intérieur des plantes et des animaux.

Dans le sol, les bactéries de la rhizosphère (couche de sol fixée aux racines des plantes) fixent l’azote et produisent des composés azotés utilisés par les plantes (exemple de la bactérie "Azotobacter" ou "Frankia"). En échange, la plante excrète au niveau des racines des sucres, des acides aminés et des vitamines qui stimulent la croissance des bactéries. D’autres bactéries comme "Rhizobium" sont associées aux plantes légumineuses au niveau de nodosités sur les racines.

Il existe de nombreuses relations symbiotiques ou mutualistes de bactéries avec des invertébrés. Par exemple, les animaux qui se développent à proximité des cheminées hydrothermales des fonds océaniques comme les vers tubicoles "Riftia pachyptila", les moules "Bathymodiolus" ou la crevette "Rimicaris exoculata" vivent en symbiose avec des bactéries chimiolitho-autotrophes. 
"Buchnera" est une bactérie endosymbiote des aphides (puceron). Elle vit à l'intérieur des cellules de l'insecte et lui fournit des acides aminés essentiels. La bactérie "Wolbachia" est hébergée dans les testicules ou les ovaires de certains insectes. Cette bactérie peut contrôler les capacités de reproduction de son hôte.
Des bactéries sont associées aux termites et lui apportent des sources d'azote et de carbone.

Des bactéries colonisant la panse des herbivores permettent la digestion de la cellulose par ces animaux. La présence de bactéries dans l’intestin de l’Homme contribue à la digestion des aliments mais les bactéries fabriquent également des vitamines comme l’acide folique, la vitamine K et la biotine.
Des bactéries colonisent le jabot d'un oiseau folivore (consommateur de feuilles), le Hoazin ("Opisthocomus hoazin"). Ces bactéries permettent la digestion de la cellulose des feuilles, de la même manière que dans le rumen des ruminants.

Des bactéries bioluminescentes comme "Photobacterium" sont souvent associées à des poissons ou des invertébrés marins. Ces bactéries sont hébergées dans des organes spécifiques chez leurs hôtes et émettent une luminescence grâce à une protéine particulière : la luciférase. Cette luminescence est utilisée par l'animal lors de divers comportements comme la reproduction, l'attraction de proies ou la dissuasion de prédateurs.

Le plus souvent, les maladies bactériennes mortelles sont les infections respiratoires : la tuberculose à elle seule tue environ 2 millions de personnes par an, principalement en Afrique subsaharienne. Des bactéries peuvent entraîner des troubles respiratoires ou intestinaux alors que d’autres peuvent être responsables de l’infection d'une blessure. Les infections bactériennes peuvent être traitées grâce aux antibiotiques, qui le plus souvent inhibent une de leurs fonctions vitales (par exemple, la pénicilline bloque la synthèse de la paroi cellulaire).

Les bactéries pathogènes sont responsables de maladies humaines et causent des infections. Les organismes infectieux peuvent être distingués en trois types : les pathogènes obligatoires, accidentels ou opportunistes.
Un pathogène obligatoire ne peut survivre en dehors de son hôte. Parmi les bactéries pathogènes obligatoires, "Corynebacterium diphtheriae" entraîne la diphtérie, "Treponema pallidum" est l’agent de la syphilis, "Mycobacterium tuberculosis" provoque la tuberculose, "Mycobacterium leprae" la lèpre, "Neisseria gonorrhoeae" la gonorrhée. Les "Rickettsia" à l’origine du typhus sont des bactéries parasites intracellulaires.
Un pathogène accidentel présent dans la nature peut infecter l’Homme dans certaines conditions. Par exemple, "Clostridium tetani" provoque le tétanos en pénétrant dans une plaie. "Vibrio cholerae" entraîne le choléra à la suite de la consommation d’une eau contaminée. 
Un pathogène opportuniste infecte des individus affaiblis ou atteints par une autre maladie. Des bactéries comme "Pseudomonas aeruginosa", des espèces de la flore normale, comme des "Staphylococcus" de la flore cutanée, peuvent devenir des pathogènes opportunistes dans certaines conditions. On rencontre ce type d’infection surtout en milieu hospitalier.

La capacité d’une bactérie à provoquer une maladie est son pouvoir pathogène. L’intensité du pouvoir pathogène est la virulence. L’aboutissement de la relation bactérie-hôte et l’évolution de la maladie dépendent du nombre de bactéries pathogènes présentes dans l’hôte, de la virulence de cette bactérie, des défenses de l’hôte et de son degré de résistance.

Pour déclencher une maladie, les bactéries infectieuses doivent d’abord pénétrer dans l’organisme et adhérer à un tissu. Des facteurs d’adhésion permettent la fixation des bactéries à une cellule. Le pouvoir invasif est la capacité de la bactérie à se répandre et à se multiplier dans les tissus de l’hôte, soit par un processus d'endocytose permettant leur pénétration intracellulaire, soit pour certaines bactéries en passant entre les cellules des muqueuses afin de coloniser la sous-jacente. Les bactéries peuvent produire des substances lytiques leur permettant de se disséminer dans les tissus. Certaines bactéries présentent aussi un pouvoir toxinogène qui est la capacité de produire des toxines, substances chimiques portant préjudice à l’hôte. On peut distinguer les exotoxines libérées lors de la multiplication des bactéries et les endotoxines fixées dans la membrane des bactéries.

Les bactéries pathogènes tentant d’envahir un hôte rencontrent toutefois de nombreux mécanismes de défense assurant à l’organisme une protection aux infections. Une bonne alimentation et une hygiène de vie correcte constituent une première protection. La peau, les muqueuses forment une première ligne de défense contre la pénétration d’organismes pathogènes. Les bactéries de la flore normale constituent aussi une barrière de protection. Lorsqu’un micro-organisme a pénétré ces premières lignes de défense, il rencontre des cellules spécialisées qui se mobilisent contre l’envahissement : ce sont les phagocytes. L’inflammation est une réaction défensive non spécifique. Un second système de défense très efficace est le système immunitaire spécifique, capable de reconnaître des antigènes portés ou sécrétés par les bactéries, et d’élaborer des anticorps et des cellules immunitaires spécifiques de ces antigènes.

En milieu hospitalier, le personnel soignant doit suivre des protocoles de protection (port de la blouse, gants, lunettes en chirurgie...). En cas de contact avec un élément à risque (sang, liquide...), le personnel soignant doit impérativement et au plus tôt se laver les mains avec un produit désinfectant et aseptisant.

Les bactéries pathogènes pour les plantes sont connues du grand public pour leur responsabilité dans la dévastation de cultures agricoles. En 2001, les vergers du midi de la France étaient victimes d'une vague d'infection par une bactérie du genre "Xanthomonas".

En biotechnologie végétale, la bactérie du sol, "Agrobacterium tumefaciens", est utilisée pour sa capacité à transmettre un fragment d'ADN à la plante cible lors de son cycle infectieux.

Les Procaryotes sont d'importants outils dans le domaine de la biorestauration: on se sert d'organismes pour éliminer des polluants du sol, de l'eau et de l'air. 
Exemple: Les archées décomposent la matière organique contenue dans les eaux usées pour la transformer en substance qui peut servir d'engrais.
Dans l'industrie minière, les Procaryotes aident à retirer les métaux contenus dans le minéral.
L'utilité des Procaryotes provient en grande partie de la diversité de leurs formes de nutrition et de métabolisme.

L’origine de la microbiologie industrielle date de l’époque préhistorique. Les premières civilisations ont utilisé sans le savoir des micro-organismes pour produire des boissons alcoolisées, du pain et du fromage.

Les bactéries comme "Lactobacillus", "Lactococcus" ou "Streptococcus", combinées aux levures et moisissures interviennent dans l’élaboration d’aliments fermentés comme les fromages, les yaourts, la bière, le vin, la sauce de soja, le vinaigre, la choucroute.

Les bactéries acétiques ("Acetobacter", "Gluconobacter") peuvent produire de l'acide acétique à partir de l'éthanol. Elles sont rencontrées dans les jus alcoolisés et sont utilisées dans la production du vinaigre. Elles sont également exploitées pour la production d'acide ascorbique (vitamine C) à partir du sorbitol transformée en sorbose.

La capacité des bactéries hétérotrophes à dégrader une large variété de composés organiques est exploitée dans des processus de traitement des déchets comme la bioremédiation ou le traitement des eaux usées. Des bactéries sont également utilisées dans les fosses septiques pour en assurer l'épuration. Des bactéries, capables de dégrader des hydrocarbures du pétrole, peuvent être utilisées lors du nettoyage d'une marée noire. Le processus de nettoyage de milieux pollués par des micro-organismes est la bioremédiation.

Des bactéries peuvent être utilisées pour récupérer des métaux d'intérêts économiques à partir de minerais. C'est la biolixiviation. L'activité de bactéries est ainsi exploitée pour la récupération du cuivre.

Des bactéries peuvent être utilisées à la place de pesticides en lutte biologique pour combattre des parasites des plantes. Par exemple, "Bacillus thuringiensis" produit une protéine Bt qui est toxique pour certains insectes. Cette toxine est utilisée en agriculture pour combattre des insectes qui se nourrissent de plantes.

En raison de leur capacité à se multiplier rapidement et de leur relative facilité à être manipulées, certaines bactéries comme "Escherichia coli" sont des outils très utilisés en biologie moléculaire, génétique et biochimie. Les scientifiques peuvent déterminer la fonction de gènes, d’enzymes ou identifier des voies métaboliques nécessaires à la compréhension fondamentale du vivant et permettant également de mettre en œuvre de nouvelles applications en biotechnologie.

De nombreuses enzymes utilisées dans divers processus industriels ont été isolées de micro-organismes. Les enzymes des détergents sont des protéases de certaines souches de "Bacillus". Des amylases capables d’hydrolyser l’amidon sont très utilisées dans l’industrie alimentaire. La Taq polymérase utilisée dans les réactions de polymérisation en chaîne (PCR) pour l’amplification de l’ADN provient d’une bactérie thermophile "Thermus aquaticus".

Les bactéries génétiquement modifiées sont très utilisées pour la production de produits pharmaceutiques. C’est le cas par exemple de l’insuline, l’hormone de croissance, certains vaccins, des interférons… Certaines bactéries comme "Streptomyces" sont très employées pour la production d’antibiotiques.

Certaines bactéries peuvent provoquer une dégradation d'installation (biocorrosion), en particulier les bactéries sulfato-réductrices.





</doc>
<doc id="14143" url="https://fr.wikipedia.org/wiki?curid=14143" title="Traduction">
Traduction

La traduction (dans son acception principale de traduction interlinguale) est le fait de faire passer un texte rédigé dans une langue (« langue source », ou « langue de départ ») dans une autre langue (« langue cible », ou « langue d'arrivée »). Elle met en relation au moins deux langues et deux cultures, et parfois deux époques. 

Une traduction ("translation" en ancien français) représente toujours un texte original (ou « texte source », ou « texte de départ ») ; en cela, elle comporte un certain degré d'équivalence, bien que le concept d'équivalence stricte entre les langues soit désormais dépassé en traductologie.
Le concept de traduction repose depuis longtemps sur des dichotomies telles que « fidélité » "versus" « liberté », « fidélité à la lettre » "versus" « fidélité à l'esprit », « traduction sourcière » "versus" « traduction cibliste », etc.

La traduction tient compte d'un certain nombre de paramètres (contexte, grammaire, etc.), afin de se rendre compréhensible pour des personnes n'ayant pas de connaissance de la langue source et n'ayant pas la même culture ou le même bagage de connaissances. Traduire implique maîtriser la langue source mais aussi la langue cible (ou "destinataire"), qui est généralement la langue maternelle. Le bon traducteur possède plus que des compétences linguistiques : il doit être capable d'analyser le texte, et lui-même posséder des qualités d'écriture. Pour traduire les textes scientifiques et techniques, il doit également posséder de solides connaissances techniques et maîtriser le jargon dans les deux langues.

La traduction est encore essentiellement humaine, mais des outils informatiques de traduction automatique apparaissent (traduction assistée par ordinateur).

La discipline qui s'intéresse à la traduction se nomme la traductologie.

Dans l'espace et dans le temps, l'intensité de la communication interculturelle et des échanges interlinguistiques dépend en grande partie de la quantité et qualité des informations traduites d'une langue à l'autre, mais l’Histoire a montré que la circulation et la « "notoriété" » des idées ne se confondent pas quantitativement avec les langues les plus parlées. En particulier, le nombre de locuteurs d’une langue parlée n’est pas un bon prédicteur de l’aptitude d’un message créé dans cette langue (ou circulant dans cette langue) à être ensuite traduit et à circuler dans le monde entier ; selon le linguiste David Crystal, "". Le réseau des locuteurs bilingues et des traducteurs a donc une grande importance de ce point de vue.

Depuis le et avec la globalisation et la réglementation du « droit de propriété intellectuelle » et de traduction, un certain nombre de langues et de cultures sont plus ou moins bien "« traduites »", voire s’éteignent plus rapidement qu’auparavant ou sont déjà mortes ou oubliées (une langue morte comme le latin peut continuer à être traduite). 
Certains auteurs décrivent l'émergence d'un nouveau réseau et système mondial de langages, où l’anglais joue un rôle devenu prépondérant et central. L’hégémonie culturo-linguistique de l'anglais pourrait toutefois être peu à peu contenue par l'amélioration et la généralisation des logiciels de traduction automatique sur l'Internet et par l'approche inédite wikimédienne qui encourage et facilite « les traductions et échanges inter-linguistiques dans Wikipédia et ses projets-frères (en 287 langues possibles fin 2013, dont langues dites « mortes » et espéranto, avec plusieurs grands projets linguistiques bilatéraux).

Analyser la situation relative des langues du monde est longtemps restée impossible faute de données pertinentes, note Mark Davis (président et co-fondateur du Consortium Unicode qui produit des standards d'encodage de caractères pour tous les ordinateurs et interfaces mobiles de la planète utilisant l'écriture), alors que l’on pressent pourtant l’importance de la structure de ce réseau ; il est longtemps resté impossible d’étudier quantitativement la structure du réseau mondial des échanges entre langues mais cela devient plus facile grâce à la constitution de grandes bases de données ouvertes de « "lieux" » d’échanges mondiaux tels que Wikipédia ou Twitter et alors qu’on connaît de mieux en mieux la proportion des langues parlées sur l’Internet.

En 2014, une équipe internationale américano-française a utilisé la Science des réseaux pour créer des cartographies permettant de visualiser comment des informations et des idées circulent dans le monde (selon la langue du message d’origine, selon le PIB moyen des pays où cette langue est parlée), selon la langue des premières traductions et celles qui vont véhiculer l’information ou selon le médium (livre, Wikipédia, Twitter). Pour dresser cette « "carte" » ces chercheurs ont étudié d’une part les données disponibles sur la traduction littéraire (en se basant sur 2,2 millions de traductions de livres publiés dans plus de 1000 langues) et d’autre part les deux grands réseaux mondiaux d’échanges par le langage que sont : 
L’analyse de ces données montre que :

La transmission culturelle passe aussi par le langage parlé, localement et à distance (via le téléphone ou Skype), ce qui pourrait accélérer la diffusion de certaines idées et informations.

On ne dispose pas de statistiques exhaustives du nombre de textes traduits dans le monde, en raison notamment d'un grand nombre de traductions faites dans la littérature grise ou diffusées via l'internet sans passer par les réseaux classiques.

Dans le monde, le document le plus traduit serait, d'après le livre des records, la Déclaration universelle des droits de l'homme avec 406 traductions, bien que la Bible soit réputée traduite dans plus de 2000 langues et la prière Notre Père dans 1698 langues.

En France, dans les années 2000-2010, pour environ titres annuels, 10 à 13 % sont des traductions Les francophones ont donc accès à environ 7000 à 9100 titres étrangers traduits en français et publiés.

Ces deux notions diffèrent : le traducteur traduit des idées exprimées à l'écrit d'une langue à une autre tandis que l'interprète traduit des idées exprimées oralement ou par l'utilisation de parties du corps (langue des signes) d'une langue à une autre. L'interprétation peut être considérée comme un sous-domaine de la traduction au regard des processus mis en œuvre (études en traduction), mais en pratique ces activités requièrent des aptitudes très différentes et ont un rapport au temps également différent.

Traduire présuppose de maîtriser deux langues au moins, mais aussi d'avoir un accès au texte à traduire (ou à sa copie) durant le temps nécessaire à sa traduction, et si possible à un original (éventuellement annoté) plutôt qu'à une traduction déjà faite dans une autre langue. 

Or les ouvrages ont longtemps été copiés et recopiés à la main. Ils sont parfois rares ou uniques. 

Les bibliothèques et lieux d'archives (municipales, royales, religieuses, industrielles, etc.) abritent des ouvrages rares et des collections patrimoniales, des collections d'enseignement et/ou de recherche et des ouvrages de lecture publique.
Les bibliothèques et lieux d'archives sont des lieux importants pour les traducteurs. Certains des ouvrages qui y sont conservés ne peuvent être empruntés à domicile ni même directement consultés, d'autres ne peuvent être ni photographiés ni microfilmés. Les étudiants, enseignants, chercheurs et autres professionnels viennent y traduire sur place des éléments d'ouvrages ou des ouvrages anciens entiers. Pour le philosophe Robert Damien, au-delà d'un lieu de juxtaposition des auteurs et des langues, des textes et des savoirs, la traduction est – comme la bibliothèque – un "".

Concernant les textes antiques, on distingue aujourd'hui principalement deux courants de traductions :

Le principe de la traduction littérale ou formelle reste centrée sur les mots et syntaxes originels, les privilégiant avant tout, au risque de les rendre peu intelligibles dans un contexte historique différent.

En matière de théories contemporaines de traduction, on constate généralement l'existence de six courants dominants :

Courant interprétatif : théorie du sens de l'E.S.I.T, basée principalement sur la pratique de l’interprétation de conférences. Dans leur ouvrage "Interpréter pour traduire" , D. Seleskovitch et M. Lederer soutiennent qu'il faut traduire le sens et non pas la langue. Celle-ci n'est qu'un simple transporteur du message. La langue peut être un obstacle à la compréhension. C'est pourquoi il faut toujours éviter de transcoder et procéder à la déverbalisation lors de toute opération traduisante. 
Parmi ces travaux, qui jouent un rôle important au sein des efforts de théorisation jalonnant l'histoire, on retiendra le projet d'un ouvrage général de la langue française d'Estienne Dolet qui aboutira à la publication, en 1540, de "La manière de bien traduire d'une langue à une autre."

Ce courant considère que la traduction n'est pas une opération linguistique mais plutôt une opération littéraire (Edmond Cary). En d'autres termes : pour traduire de la poésie, il faut être poète (Ezra Pound, Walter Benjamin, Henri Meschonnic, Antoine Berman).

Le concept d’énergie dans la langue : Les mots sont, en quelque sorte, une cristallisation du vécu historique d'une culture, ce qui leur donne une force et c'est justement cette énergie qu'il faut traduire. Chez Leopardi, plusieurs pages du "Zibaldone" vont déjà dans le même sens (c'est l"'enérgeia", voir lecture de Vegliante, qu'il faut restituer dans le texte destinataire). 

C'est le moule social qui détermine ce qui est traduisible ou pas, ce qui est acceptable ou pas (sélection, filtration, censure...). Le traducteur est le produit d'une société et l'on traduit selon son propre bagage socio-culturel (école de Tel-Aviv : Even Zohar, Guideon Toury). 

Structuralisme, linguistique, pragmatique, linguistique du texte. C'est un courant qui considère le mot, le syntagme et la phrase comme unités de traduction. (Georges Mounin, Vinay et Darbelnet, J.I Austin, J.-Ch. Vegliante - voir par ex. "Méthode", en ligne).

Le chef de file de ce courant est George Steiner. Le vrai traducteur doit être capable de se mettre dans la peau d'un écrivain afin de capter et de saisir l'intention (le « vouloir dire ») de l'auteur du texte de départ. Il voit l’opération traduisante comme un mouvement en quatre temps : trust (confiance / conviction), agression, incorporation et restitution.

La sémiotique est l’étude des signes et des systèmes de signification. Pour Peirce : Le processus de signification (ou sémiosis) est le résultat de la coopération de trois éléments : un signe, un objet et son interprétant. Aussi, d'un point de vue sémiotique, toute traduction est envisagée comme une forme d’interprétation qui porte sur des textes ayant un contenu encyclopédique différent et un contexte socioculturel particulier.

Le processus de traduction peut être découpé en trois phases successives : 

Sur le marché du travail, on distingue deux types de traduction : la traduction de textes techniques et la traduction littéraire. La majorité des traducteurs professionnels traduisent des textes techniques. Les traducteurs littéraires sont attachés à une maison d'édition ou auto-entrepreneurs. 

La traduction technique concerne les documents tels que les manuels, feuillets d'instructions, notes internes, procès-verbaux, rapports financiers, et autres documents destinés à un public limité (celui qui est directement concerné par le document) et dont la durée de vie utile est souvent limitée. 

Par exemple, un guide d'utilisation pour un modèle particulier de réfrigérateur n'a d'utilité que pour le propriétaire du réfrigérateur, et restera utile tant que ce modèle de réfrigérateur existera. De même, la documentation logicielle s'adresse généralement à un logiciel particulier, dont les applications concernent une catégorie d'utilisateurs.

La traduction de textes techniques exige souvent des connaissances spécialisées dans un domaine particulier. On compte parmi les textes techniques :

La traduction technique est un type de traduction souvent « anonyme » dans lequel le nom du traducteur peut ne pas être associé au document traduit, tout comme certaines entreprises ne font pas mention des auteurs des guides d'utilisation des produits. Cependant, dans le cas de la traduction de livres à contenu informatif, le traducteur sera mentionné dans la section de responsabilité primaire de l'item bibliographique du livre.

En général, la traduction technique est plus accessible et rapporte un salaire plus élevé que la traduction littéraire. Cette dernière est effectuée avant tout par amour de la langue et du texte original, ou par volonté de faire connaître toutes les subtilités d'un texte admirable écrit en langue étrangère.

Selon l'école de pensée "cibliste", il est nécessaire de privilégier l'exactitude des propos au détriment de la stylistique, lorsque cela s'impose. Pour « faire passer son message », la traduction devra parfois remplacer les éléments culturels du texte original par des exemples équivalents, mais mieux connus des lecteurs de la culture d'arrivée. Le plus important demeure le « sens » du message que tente de véhiculer l'auteur. Le traducteur doit d'abord faire passer ce message de manière idiomatique et naturelle pour le lecteur en langue d'arrivée, tout en demeurant fidèle au langage, au registre et au ton employé par l'auteur du texte dans la langue de départ.

Selon l'école de pensée "sourcière", le traducteur a la responsabilité de demeurer strictement fidèle à la forme du texte original. Le traducteur devra donc reproduire tous les éléments stylistiques de l'original, employer le même ton, laisser tous les éléments culturels intacts et même (à l'extrême) contraindre la langue d'arrivée à prendre la forme dictée par le texte de départ. Le traducteur sourcier veillera en premier lieu à ne pas trahir le véhicule employé par l'auteur, et tâchera ensuite de bien restituer le sens du message.

"(Voir Critiques de la traduction infra.)"

Il faut déjà savoir que la plupart des traducteurs freelance indiquent se spécialiser dans à peu près tout, ce qui contredit, évidemment, le terme de spécialisation ; il est évident que leur comportement est la recherche du maximum de travaux de traduction.

Il ne faut cependant pas sous-estimer la capacité à apprendre correctement un sujet au fil des traductions effectuées ; les sources d'explications, comme Wikipédia et autres dictionnaires terminologiques, sans oublier les sites Internet de sociétés traitant des sujets concernés, étant nombreuses.
Il peut donc être utile de faire appel à un traducteur présentant 20 spécialisations sur son CV.

Cela dit, pour réaliser des traductions pragmatiques utiles, il est nécessaire de maîtriser le jargon du domaine et de savoir employer les bons termes ; une traduction qui ne reflète pas l'usage courant et l'évolution de la langue de spécialité ne saurait intéresser ses lecteurs, au même titre qu'on n'écrit plus comme en 1750.

Certains domaines (comme l'informatique) évoluent à une vitesse vertigineuse, au point que le jargon spécialisé de la langue d'arrivée (par exemple le français) n'arrive pas à s'enrichir assez rapidement pour suivre l'évolution de la langue d'origine (par exemple l'anglais). Dans cette situation, le traducteur peut être confronté à l'absence d'équivalent français (donc à la nécessité de créer un néologisme) ; à plusieurs néologismes à peu près équivalents ou à une alternative entre un terme relativement général et bien connu, et un terme plus précis, mais moins employé.

La traduction de logiciels (qui comporte deux phases distinctes, l'internationalisation et la régionalisation) est un processus qui diffère de la simple traduction textuelle à divers degrés.

Ce type de traduction concerne les romans, poèmes et autres genres du domaine littéraire.

La traduction littéraire demande des aptitudes en stylistique, une bonne imagination et des connaissances culturelles étendues. Il s'agit de reproduire l'effet intégral du texte original chez le lecteur en langue d'arrivée, autant que le sens des mots. La traduction doit être aussi plaisante à lire et susciter les mêmes émotions que l'original, suivant l'adage de Cervantès : « ne rien mettre, ne rien omettre ». Les grands traducteurs, quelle que soit la langue, ont une formation très exigeante, études littéraires et universitaires, dans la langue dans laquelle ils traduisent mais aussi et surtout dans leur langue maternelle, langue vers laquelle ils traduisent. L'écriture du texte de destination devient alors primordiale.

En poésie, la traduction offre une double difficulté s'il faut rendre compte à la fois du sens et de la métrique (de la forme en général), voire de procédés rhétoriques. Si l'on se limite au sens (sémantique), un exercice de traduction de haïkus, après passage dans plusieurs langues et retour final au français, a permis de démontrer une assez grande robustesse du contenu sémantique.

Une difficulté bien connue des traducteurs est le fait que le texte à traduire est parfois déjà une traduction, pas nécessairement fidèle, et qu'il faut, dans la mesure du possible, essayer de la dépasser pour remonter à l'original. De nos jours, le phénomène s'est amplifié et se présente sous des formes diverses. Il y a d'abord l'utilisation consciente d'une langue-pont. S'il faut traduire en grec moderne un texte écrit en estonien, on pourra avoir du mal à dénicher un traducteur connaissant à la fois les deux langues et le sujet traité. C'est d'une traduction, généralement en anglais, que partira le traducteur. L'imprécision de cette langue peut créer des difficultés, comme le fait remarquer Claude Piron avec cette phrase dont il avait dû vérifier la traduction française : "« He could not agree with the amendments to the draft resolution proposed by the delegation of India. »" Le premier traducteur ne pouvait savoir si "proposed" se rapportait à "amendments" ou à "resolution" et avait choisi la mauvaise solution. Claude Piron, qui avait sous les yeux l'ensemble du rapport, put rectifier.

Marc Bloch a posé la question en écrivant dans "Apologie pour l'histoire" :

Il existe une autre critique, moins facile à argumenter, qui s'appuie sur une phrase italienne à la formulation particulièrement vigoureuse : « Traduttore, traditore ». Cette critique soutient que toute traduction revient trop à trahir l'auteur, son texte, l'esprit de celui-ci, son style... à cause des choix à faire de toute part. L’écrivain Julien Green, parfaitement bilingue, qui a lui-même traduit certaines de ses œuvres du français en anglais, déclare : « L’écrivain qu’on traduit aurait certainement employé d’autres mots et dit des choses différentes s’il avait écrit dans la langue du traducteur » . 

Le traducteur Pierre Leyris (qui a entre autres traduit l'œuvre d'Herman Melville) répond à cette critique en affirmant : « "Traduire, c’est avoir l’honnêteté de s’en tenir à une imperfection allusive" » .

Voir l'article "Régionalisation de logiciel".

La norme de qualité NF EN 15038:2006 est une norme européenne spécifique pour les services de traduction qui « a pour objet d'établir et de définir les exigences relatives à la prestation de services de traduction de qualité ». Elle spécifie les exigences relatives aux fournisseurs de services de traduction (FST) en matière de ressources humaines et techniques, de management de la qualité et de gestion de projets, de cadre contractuel et de procédures de service.

Conditions et étapes d'une traduction selon la Norme NF EN 15038.

La Direction générale de la traduction de la Commission européenne définit comme "Industrie langagière" les activités de traduction, d’interprétariat, de sous-titrage, ainsi que le doublage, l’internationalisation de logiciels et de sites web, le développement d’outils technologiques linguistiques, l’organisation de conférences internationales, l’enseignement des langues et l'expertise linguistique

Il peut être dans certains cas obligatoire de traduire un texte.

Dans les pays polyglottes ou les organismes internationaux, il peut être obligatoire de traduire les textes législatifs et réglementaire ou les formulaires administratifs.

La loi d'un pays peut imposer que certaines informations soient disponibles dans la langue nationale ou une des langue nationales. Par exemple, en France, la loi du 31 décembre 1975 puis après elle la loi Toubon impose que la publicité écrite ou parlée, ainsi que les modes d'emploi et manuels d'utilisation, soient disponibles en français.

Des cursus spécifiques diplômants ont été créés au niveau Master tel le Master Traduction spécialisée.

Un baccalauréat universitaire de trois ans en traduction existe dans la province. 
Il permet l'admission à l'Ordre des traducteurs, terminologues et interprètes agréés (OTTIAQ).

En Suisse, des formations en traduction sont proposées aux niveaux du bachelor et du master, par la Faculté de traduction et d'interprétation à Genève et par la à Winterthour.



</doc>
<doc id="14144" url="https://fr.wikipedia.org/wiki?curid=14144" title="Mer d'Irlande">
Mer d'Irlande

La Mer d’Irlande ("Irish Sea" en anglais, "Muir Éireann" en irlandais et "Môr Iwerddon" en gallois) est un bras de mer des Îles Britanniques séparant l'Irlande à l'ouest de la Grande-Bretagne à l'est. S'étendant sur km², elle communique avec la Mer d'Écosse par le Canal du Nord au nord-ouest et avec la Mer Celtique par le Canal Saint-Georges au sud, lequel débouche sur la Baie de Cardigan à l'est. Comptant en particulier les îles de Man et d'Anglesey, elle est entourée des quatre nations constitutives du Royaume-Uni.

L’Organisation Hydrographique Internationale définit les limites de la mer d'Irlande de la façon suivante :






</doc>
<doc id="14145" url="https://fr.wikipedia.org/wiki?curid=14145" title="Mer Noire">
Mer Noire

La mer Noire est située entre l’Europe, le Caucase et l’Anatolie. Large d'environ d’ouest en est et de du nord au sud, elle s’étend sur une superficie de . L'adjectif correspondant est « », qui vient du nom antique de cette mer, le "Pont Euxin".

Le terme océanographique d'« euxinisme » y fait également référence, il désigne une anoxie des eaux profondes, plus salées qu'en surface et provenant de la Méditerranée via la mer de Marmara par un courant de fond inverse de celui des eaux plus douces de la surface alimentées par les fleuves se jetant dans la mer Noire.

Elle communique au sud-ouest avec la mer Méditerranée par le Bosphore, la mer de Marmara et le détroit des Dardanelles. Sur ses côtes ouest et nord, elle communique avec de nombreux limans (lagunes navigables dont la salinité et la turbidité varient avec la saison, et qui servaient de frayères pour le poisson). Au nord-est, la mer d'Azov, reliée par le détroit de Kertch, est considérée comme le plus grand des limans. Son climat spécifique doux et humide, aux épais brouillards aux saisons intermédiaires, subit des influences méditerranéennes au sud-ouest et en été (chaud, sec et ensoleillé), continentales au nord et en hiver (froid glacial, la mer peut geler, les chutes de neige sont fréquentes), et subtropicales au sud-est. Pendant les tempêtes, surtout hivernales, les vagues sont courtes, mais hautes, et peuvent venir de plusieurs directions à la fois, rendant la navigation difficile.

Depuis 1996, le 31 octobre est la « journée internationale pour la protection de la mer Noire ».

L'Organisation Hydrographique Internationale détermine les limites de la mer Noire de la façon suivante :



La mer Noire a une superficie comprise entre et et un volume compris entre et .

Une autre source donne une superficie de .

Ces données ne prennent pas en compte la mer d'Azov dont la surface est de .

Le bassin pontique a une profondeur maximale de . Sa formation fait l'objet de deux hypothèses :

Quoi qu'il en soit, les sédiments déposés au fond du bassin sont essentiellement Pléistocènes et Holocènes, de faciès détritique et dulçaquicole en profondeur (témoignant d'importants apports fluviaux lors des périodes de dégel interglaciaires), et marin au-dessus (sédiments de moins de 8000 ans). Les sédiments détritiques et dulçaquicoles correspondent à une période dite « "sarmatique" » commencée il y a 5 millions d'années, durant laquelle une mer intérieure d'eau douce recouvrait les actuelles Hongrie, Roumanie, mer Noire, Ukraine littorale, Russie méridionale, mer Caspienne et Asie centrale. Le niveau de cette étendue d'eau a beaucoup varié, et à l'Holocène récent (durant la dernière glaciation, dite Würmienne), il était plus bas que le niveau actuel des mers, de sorte que seuls les bassins profonds pontique et caspien étaient encore en eau.

Dans les années 1960, en analysant au carbone 14 des coquillages d'eau douce trouvés dans les carottages des sédiments de la mer Noire sous les sédiments actuels marins, les chercheurs bulgares, roumains et soviétiques avaient découvert que l'actuelle mer Noire a été il y a près de un lac d'eau douce appelé « lac Pontique » qui se trouvait à au-dessous du niveau général des mers. À l'époque, le Bosphore n'était pas un détroit mais un isthme qui séparait ce grand lac de la mer de Marmara, elle-même isolée de la mer Égée par l'isthme des Dardanelles. Après la chute du rideau de fer et avec le développement d'internet, les géologues américains Walter Pitman et William Ryan découvrent en 1997 les publications bulgares, roumaines et soviétiques modélisant les effets de la déglaciation post-würmienne qui, élevant le niveau de la mer Méditerranée, finit par entraîner le déversement d'eaux salées en mer de Marmara puis dans la mer Noire, mais sans donner d'opinion sur la vitesse du phénomène, ni sur son caractère répétitif ou unique.

Pitman et Ryan rapprochèrent ces faits du mythe de l'arche de Noé, de la légende de Gilgamesh dans le royaume de Sumer, du déluge de Deucalion et du mythe de l'Atlantide dans la Grèce antique. Selon eux, le remplissage a dû être unique, brutal et catastrophique, une cascade gigantesque se serait formée par érosion hydraulique au débouché du Bosphore, et le niveau de la mer Noire serait monté de en seulement quelques semaines, ses rives reculant d'un kilomètre par jour ou plus. Or, les rives de ce lac étaient déjà peuplées d'agriculteurs, car, en Anatolie et en Europe orientale, l'agriculture a commencé très tôt. Ryan et Pitman pensent que ces agriculteurs, chassés par la montée des eaux, se seraient dispersés en Anatolie et en Mésopotamie, véhiculant le mythe du Déluge. Ils en firent des livres et des documentaires.

L'hypothèse de Pitman et Ryan n'a toutefois pas convaincu la majorité des chercheurs : des études géologiques publiées en 2007 récusent l'idée d'un déversement catastrophique unique, pour modéliser une série d'oscillations des niveaux des bassins pontique, marmarien et égéen, avec des périodes de déversements multiples, graduels et pas toujours dans le même sens. Actuellement, trois reconstructions très différentes de l'histoire de la mer Noire coexistent donc : l'hypothèse catastrophiste de Pitman et Ryan, une hypothèse gradualiste à déversement unique mais lent, et l'hypothèse des déversements multiples, qui recueille l'assentiment de la majorité des auteurs.

Quoi qu'il en soit, en devenant salée, la mer Noire désormais reliée à la Méditerranée, reste une mer particulière : la mort du biotope lacustre a provoqué une séparation des eaux profondes et des eaux superficielles (voir ci-dessous) et la salinité est restée très en dessous de la moyenne mondiale : 12 à de sel par litre au lieu de 35. De ce fait, un courant d'eau salée coule toujours en profondeur à travers le Bosphore (la « cascade » d'eau marine ne s'est jamais arrêtée) tandis qu'en surface, les eaux moins salées de la mer Noire coulent vers la mer de Marmara. Les sous-mariniers notamment soviétiques et américains connaissent bien le phénomène et ont essayé d'en profiter, mais l'étroitesse du Bosphore (un demi-mille à peine à son point le plus étroit) et l'intense circulation de navires rend l'exercice extrêmement dangereux (et il y eut des accidents).

Les eaux de cette mer, au-delà de de profondeur, sont anoxiques, c’est-à-dire sans dioxygène dissout. L'eau profonde concentre assez de sulfure d'hydrogène pour que les bois, cuirs et tissus des épaves soient préservés de l'action bactérienne, au profit des chercheurs d'épaves. Ce phénomène, également présent en mer Caspienne, en mer Baltique et dans le lac Tanganyika, est appelé "euxinisme".

De 2005 à 2009, le projet européen "Hermes" explore les écosystèmes marins sur de marge continentale profonde pour notamment mesurer les formes du méthane mer Noire et Baltique. On devrait ainsi mieux comprendre les écosystèmes microbiens anoxiques, et leurs bilans énergétiques et en termes de puits/sources de carbone et GES.

On a ainsi pu explorer le méiobenthos (de taille moyenne, c'est-à-dire de à ou ) et les espèces d'une zone active de production naturelle de gaz méthane et de HS toxique, ses variations (de -182 à , dans le canyon sous-marin du Dniepr au nord-ouest de la mer Noire). Le méiobenthos était essentiellement constitué de nématodes et foraminifères (Ciliophora notamment), cohabitant avec des polychètes, mais aussi de bivalves, gastéropodes, amphipodes, et Acarina. On a aussi trouvé dans des sédiments des stades juvéniles de Copépodes et Cladocères probablement d'origine planctonique. L'abondance du méiobenthos variait de à par mètre carré (plus nombreux dans la couche superficielle de sédiment pour les nématodes et foraminifères d'une zone permanente HS à des profondeurs de 220 à ). Cette forte concentration de méiobenthos a été trouvée dans un secteur d'intenses émanations de méthane, associées à un tapis microbien ("biofilm" méthanotrophe ou méthane-oxydant). L'étude suggère que le méthane et de ses produits d'oxydation microbienne expliqueraient la survie de nombreuses espèces benthiques adaptées à ce milieu extrême, et la bioproductivité élevée dans des zones fortement "sulfurées". Une corrélation inverse a été trouvée entre la densité en méiofaune et les taux de méthane des couches superficielles de sédiments. Les chercheurs supposent que le taux de nématodes et de foraminifères des zones enrichies en méthane est un compromis entre les exigences écologiques et les besoins alimentaires de ces organismes et leurs adaptations à l'environnement rendu toxique par l'HS.

La mer Noire abrite un pic de la biodiversité planétaire avec par exemple 42 espèces d'amphipodes benthiques relevées dans la région, où l'on découvre encore de nouvelles espèces mais elle est très menacée par la pollution et par des « espèces invasives ».

Le pourtour de la mer Noire est caractérisé par un climat quasi endémique appelé « climat pontique » : on parlera donc d'écorégion à son propos. C'est une variante transitionnelle du climat tempéré, avec des caractéristiques méditerranéennes, mais aussi continentales au nord (climat "drossopontique" à l'été méditerranéen et à l'hiver continental) et subtropicales humides au sud (climat "eupontique"). Le climat "drossopontique" est assez frais et sec en Bulgarie, Roumanie, Ukraine et nord-ouest de la Crimée ; le climat "eupontique" est plus doux et humide dans le sud-est de la Crimée (péninsule de Kertch), autour de Sotchi en Russie, en Abkhazie, en Colchide (Géorgie) et surtout dans la région pontique de la Turquie. Ce climat pontique est propice à une forte productivité végétale et c'est pourquoi dès les aux les pourtours de la mer Noire ont été densément colonisés par les Grecs antiques, la région devenant le « grenier à blé » des cités grecques dans sa partie "drossopontique" propice aux cultures céréalières, et la « réserve de bois » de la marine grecque dans sa partie "eupontique" en grande partie recouverte de forêts, aujourd'hui encore assez préservées. Cette abondance forestière a tant impressionné les anciens grecs qu'ils nommèrent les forêts pontiques "Amarante", soit, littéralement, « qui ne peut se corrompre ».

On cherche à mieux modéliser la cinétique environnementale de ces polluants, dont les polychlorobiphényles, via des modèles numériques tridimensionnels. 

Le tableau suivant donne le nom de la mer Noire dans les langues riveraines ; s'il n'y a pas de traduction, c'est que le terme signifie seulement « mer noire ».

L’étymologie du nom grec antique « » - Pòntos, signifiant « large mer », est la même que pour les îles Pontines de la Mer Tyrrhénienne, (Italie). Dans l’Antiquité, les Grecs la désignèrent d’abord par Skythikos Pontos (la « mer Scythique »). Les Scythes, peuple de langue iranienne, la désignaient comme "axšaēna "(« indigo »). Les Grecs comprirent d’abord ce terme comme "axeinos" (de "a-" privatif et "xeinos" « étranger ») signifiant dans leur langue : « inamicale aux étrangers ». Plus tard, quand ses courants et ses vents leur devinrent familiers, elle fut désignée comme Pontos ("Pontos" signifiant « la mer », « le flot ») Euxeinos ("eu-" « bien » et "xeinos" « étranger » c’est-à-dire mer « amicale » ou « accueillante », traduit en français par "Pont-Euxin").

Les Romains l'appelèrent "Pontus Euxinus" ou "Mare Scythicum" et les grecs byzantins "καικίας" : kaikías, mot désignant le « vent du nord », terme repris par les Bulgares en « mer Cécile » (« »).

Au , les portulans des génois (qui avaient alors des comptoirs tout autour de ses rives), ainsi que dans les chroniques de Wavrin et de Villehardouin l'appellent mer Majoure c'est-à-dire « grande mer » (' en italien, ' en roumain).

Pour expliquer le nom de Noire, terme apparu dans les textes et les cartes à partir du , il existe trois théories : la plus populaire est que ce serait sa couleur lors des tempêtes, mais c'est le cas de toute mer. On avance parfois que son appauvrissement en oxygène et sa richesse en sulfures, dont certains sont noirs ou très sombres, lui donnerait cette couleur, mais en réalité, ces caractéristiques physico-chimiques ne concernent que les eaux profondes, et en surface la mer « noire » est bleue comme les autres mers.

Des deux théories scientifiques, la plus ancienne est que ce nom de « noire » serait une traduction de l'adjectif "axaïna" (« sombre ») donné par les Scythes, mais le problème, c’est qu’entre la disparition des Scythes et le , il y a un millénaire pendant lequel seul "Pont-Euxin" est utilisé, dans le sens grec du terme. Selon l’autre hypothèse, ce nom lui aurait été donné par les Turcs Selçuks puis Osmanlıs installés en Anatolie à partir du . Chez ces derniers, les points cardinaux sont désignés par des couleurs avec différentes variantes. Ainsi, dans le cas présent :

Le Pont-Euxin étant situé au nord de la Turquie, aurait donc été désigné en turc : ", « mer Noire », sombre, alors que la Méditerranée, au sud, a été appelée "mer Blanche", claire () (qui ne doit pas être confondue avec la mer Blanche des Russes). Les savants turcs eux-mêmes sont divisés sur le sujet, car chez les anciens turcophones de la steppe, le nord était désigné par (blanc comme la neige) et le sud par (rouge comme la chaleur). La logique désignant le nord (obscur) par le noir, le sud (la clarté) par le blanc et l'ouest (soleil couchant) par le rouge, ne serait apparue que tardivement, en Asie mineure.











</doc>
<doc id="14146" url="https://fr.wikipedia.org/wiki?curid=14146" title="Limite">
Limite

Le mot limite peut désigner :

</doc>
<doc id="14153" url="https://fr.wikipedia.org/wiki?curid=14153" title="Gramme">
Gramme

Le gramme est une unité de masse du système CGS et une unité dérivée du Système international dont l'unité de masse est le kilogramme.

Le symbole du gramme est g (sans point, sauf si le symbole se trouve en fin de phrase).

Gramme est emprunté au grec ancien , égale à un vingt-quatrième de l'once romaine.

Le gramme a pour origine le gravet, unité de poids créée par la Convention nationale, par décret du , et défini comme le « poids du centimètre cubique d'eau », égal à grave.

Il représentait un centimètre cube d'eau distillée à densité maximale, soit .

Le apparaît le kilogramme, multiple du gramme ( = ).

La loi du définit le kilogramme par un cylindre en platine « matérialisant » la masse du décimètre cube d'eau à .

En 1889, à la première Conférence générale des poids et mesures (CGPM) le kilogramme-étalon (ainsi que le mètre-étalon) sont déposés au pavillon de Breteuil à Sèvres, près de Paris, France. Il est constitué d'un cylindre en platine iridié de hauteur égale au diamètre ().

L'artefact adopté lors de la première Conférence générale des poids et mesures de 1889 pose un souci : son inconstance.

Des propositions de substitutions sont à l'étude (cf kilogramme).



Les masses suivantes sont des ordres de grandeur :

Par rapport aux unités de masse anglo-saxonnes traditionnelles :



</doc>
<doc id="14154" url="https://fr.wikipedia.org/wiki?curid=14154" title="Kilogramme">
Kilogramme

Le kilogramme, dont le symbole est kg (en minuscules), est l'unité de base de masse dans le Système international d'unités (SI). Il est défini comme étant égal à la masse du prototype international du kilogramme.

Le kilogramme est la seule unité SI de base possédant un préfixe (« kilo », symbole « k » utilisé pour désigner le millier d'une unité) dans son nom. Il s'agit également de la seule unité SI de base qui soit toujours directement définie par un objet matériel plutôt que par une propriété physique fondamentale pouvant être reproduite dans différents laboratoires. Quatre des sept unités de base du Système international sont définies par rapport au kilogramme et sa stabilité est donc importante.

Le gramme est originellement défini en 1795 comme la masse d'un centimètre cube "« d'eau pure »" à , faisant du kilogramme l'égal de la masse d'un litre d'eau. Le prototype du kilogramme, fabriqué en 1799 et sur lequel est basé le kilogramme actuel, possède une masse égale à celle d' d'eau pure.

Le prototype international du kilogramme est commandé par la Conférence générale des poids et mesures (CGPM) sous l'autorité de la Convention du Mètre (1875), et est sous la garde du Bureau international des poids et mesures (BIPM) qui le conserve au nom de la CGPM. Après la constatation que la masse du prototype varie au cours du temps, le Comité international des poids et mesures (CIPM) recommande en 2005 de redéfinir le kilogramme en termes de constante fondamentale de la nature. Dans sa session de 2011, la CGPM convient que le kilogramme devrait être redéfini en fonction de la constante de Planck, mais reporte la décision finale à 2014, puis, constatant que les travaux existant à cette date ne permettent pas de mettre en œuvre le changement, à la CGPM, qui devrait se tenir en 2018 à Paris. La conférence est convoquée pour les 13 au 16 novembre 2018. Elle devrait figer quatre constantes physiques, redéfinir un nouveau système d'unités et donc la redéfinition effective du kilogramme

Le prototype international du kilogramme est rarement utilisé ou manipulé. Des copies sont conservées par les laboratoires nationaux de métrologie autour du globe et lui ont été comparées en 1889, 1948 et 1989 pour des besoins de traçabilité.

Le mot « kilogramme » est formé du préfixe « kilo », dérivant du grec ancien ("chílioi") signifiant « mille », et de « gramme », du grec ancien ("grámma") signifiant « petit poids ». Le mot « kilogramme » est écrit dans la loi française en 1795. L'apocope « kilo » est une abréviation courante qui apparaît dès le .

Le symbole du kilogramme est « kg ».

Le kilogramme est une unité de masse. Du point de vue physique, la masse est une propriété inertielle, décrivant la tendance d'un objet à conserver la même vitesse en l'absence d'une force extérieure. Selon les lois du mouvement de Newton, un objet de masse accélère d' quand on lui applique une force d'.

Si le poids d'un système dépend de la force locale de la gravité, sa masse est invariante (tant qu'il ne se déplace pas à des vitesses relativistes). En conséquence, pour un astronaute en micropesanteur, aucun effort n'est nécessaire pour maintenir un objet au-dessus du plancher : il est sans poids. Toutefois, comme les objets en micropesanteur conservent leur masse et donc leur inertie, un astronaute doit exercer une force dix fois plus importante pour donner la même accélération à un objet de qu'à un objet d'.

Comme sur Terre, le poids d'un objet est proportionnel à sa masse, sa masse en kilogramme est généralement mesurée en comparant son poids à celui d'un objet standard dont la masse est connue en kilogramme, à l'aide d'une balance. Le rapport de la force de gravitation exercée sur les deux objets est égal au rapport de leur masse.

Le kilogramme sous-tend une grande partie du Système international d'unités tel qu'il est actuellement défini et structuré. Par exemple :
Cette chaîne de dépendance se succède sur plusieurs unités de mesure SI. Par exemple :
La magnitude des unités principales d'électricité (coulomb, volt, tesla et weber) est donc déterminée par le kilogramme, tout comme celle des unités de lumière, la candela étant définie grâce au watt et définissant à son tour le lumen et le lux. Si la masse du prototype international du kilogramme venait à changer, toutes ces unités varieraient en conséquence.

Comme la magnitude de nombreuses unités SI est définie par la masse d'un objet de métal de la taille d'une balle de golf et vieux de plus de , la qualité du prototype international est protégée avec application afin de préserver l'intégrité du système. Cependant, en dépit de la meilleure intendance, la masse moyenne de l'ensemble des prototypes et du prototype international a vraisemblablement divergé de plus de depuis la troisième vérification périodique en 1989. De plus, les laboratoires de métrologie nationaux doivent attendre la quatrième vérification périodique pour confirmer cette tendance historique.

La définition des unités SI est toutefois différente de leur réalisation pratique. Par exemple, le mètre est défini comme la distance parcourue par la lumière pendant 1/299792458 de seconde. Sa réalisation pratique prend typiquement la forme d'un laser hélium-néon et la longueur du mètre est délinéée comme d'onde de la lumière de ce laser. Si, par hasard, on réalisait que la mesure officielle de la seconde avait dérivé de quelques parties par million (elle est en réalité extrêmement stable, avec une reproductibilité de quelques parties pour 10), cela n'aurait aucun effet automatique sur le mètre car la seconde — et donc la longueur du mètre — est absorbée par le laser qui en assume la réalisation pratique. Les scientifiques calibrant les appareils continueraient à mesurer le même nombre de longueurs d'onde du laser jusqu'à ce qu'un accord soit conclu pour procéder différemment. Dans le cas de la dépendance du monde extérieur à la valeur du kilogramme, si on déterminait que la masse du prototype international avait changé, cela n'aurait aucun effet automatique sur les autres unités de mesure, leur réalisation pratique fournissant un niveau d'abstraction les isolant. Si la variation de masse était définitivement prouvée, une solution consisterait à redéfinir le kilogramme comme égal à la masse du prototype plus une valeur de compensation.

Sur le long terme, la solution consiste à libérer le système SI du prototype international en développant une réalisation pratique du kilogramme qui puisse être reproduite dans différents laboratoires en suivant une spécification définie. Les unités de mesure dans ces réalisations pratiques possèdent leur magnitude précisément définie et exprimée en termes de constantes physiques fondamentales. Le kilogramme serait ainsi basé sur une constante universelle invariante. Actuellement, aucune alternative n'a encore atteint l'incertitude de par milliard (environ ) requise pour faire mieux que le prototype. Toutefois, la balance du watt du "National Institute of Standards and Technology" approche de ce but, avec une incertitude démontrée de .
Le système métrique est créé en France à l'initiative de Charles-Maurice de Talleyrand-Périgord. Le , le gouvernement français ordonne à l'Académie des sciences de déterminer précisément la magnitude des unités de base du nouveau système. L'Académie partage la tâche en cinq commissions ; celle chargée de la détermination de la masse comprend initialement Antoine Lavoisier et René Just Haüy ; Lavoisier est guillotiné le et Haüy est temporairement emprisonné, ils sont remplacés à la commission par Louis Lefèvre-Gineau et Giovanni Fabbroni.

Le concept d'utiliser une unité de volume d'eau pour définir une unité de masse est proposée par le philosophe anglais John Wilkins en 1668, afin de lier la masse et la longueur. Le système métrique ayant par ailleurs défini le mètre, « qui a été adopté pour l'unité fondamentale de tout le système des mesures », l'unité de poids qui en découle pouvait alors être le mètre cube d'eau d'une tonne (dont l'ordre de grandeur est celui des déplacements des navires), le décimètre cube d'un kilogramme (du même ordre de grandeur que la livre, d'usage courant sur les marchés pour peser les marchandises), le centimètre cube d'un gramme (du même ordre que le denier dans le système des poids de marc, poids des pièces monétaires courantes), ou le millimètre cube d'un milligramme (de l'ordre de la prime, utilisée pour les mesures de précision).

Le gramme est introduit par la loi du 18 germinal an () ; il est défini comme « le poids absolu d'un volume d'eau pure égal au cube de la centième partie du mètre, et à la température de la glace fondante ». Le choix de l'unité de base se porte donc sur le centimètre cube d'eau, le même décret prévoyant également dans ce système métrique universel une unité mesure monétaire, « l’unité des monnaies prendra le nom de franc, pour remplacer celui de livre usité jusqu'aujourd'hui » : le choix du gramme comme unité de poids préparant la voie à un franc métrique universel.

Comme le commerce met en jeu des objets nettement plus massifs qu'un gramme, et comme un standard de masse constitué d'eau serait instable, un étalon provisoire est réalisé en métal, d'une masse plus grande que le gramme : le kilogramme. Cet étalon provisoire est fabriqué en accord avec une mesure imprécise de la densité de l'eau réalisée auparavant par Lavoisier et Haüy, qui estiment que l'eau distillée à a une masse de dans l'ancien système des poids de marc.

Dans le même temps, une commission est nommée pour déterminer précisément la masse d'un litre d'eau. Bien que le décret mentionne spécifiquement de l'eau à , les études de Lefèvre-Gineau et Fabbroni montrent que l'eau est au plus dense à et qu'un litre pèse à cette température , 99,9265 % de la valeur imprécise mesurée précédemment par Lavoisier et Haüy.

Le , un étalon en platine d'un kilogramme (nom originel, le grave), soit la masse d'un litre d'eau, est déposé (ainsi qu'un étalon du mètre) aux Archives de France. Le , l'étalon est ratifié officiellement comme « kilogramme des Archives » et le kilogramme est défini comme égal à sa masse.

Le , la Convention du Mètre formalise un peu plus le système métrique. L'unité de masse est redéfinie comme « kilogramme » (et non « gramme »), qui devient ainsi la seule unité de base incluant un préfixe multiplicateur. Un nouvel étalon en platine iridié de masse pratiquement identique au kilogramme des Archives doit être réalisé dès cette année, mais la coulée est rejetée car la proportion d'iridium, 11,1 %, se situe en dehors des 9 - 11 % spécifiés. Le prototype international du kilogramme est l'un des trois cylindres réalisés en 1879. En 1883, sa masse est mesurée comme indifférenciable de celle du kilogramme des archives. Ce n'est qu'en 1889, lors de la première CGPM, que le prototype international du kilogramme définit la magnitude du kilogramme ; il est conservé depuis au pavillon de Breteuil en France.

Les mesures modernes de la "Vienna Standard Mean Ocean Water", une eau distillée pure avec une composition isotopique représentative de la moyenne des océans, montre qu'elle possède une masse volumique de à sa densité maximale () sous une atmosphère standard (). Ainsi, un décimètre-cube d'eau dans ces conditions n'est que moins massif que le prototype international du kilogramme (). La masse du kilogramme des Archives, réalisé il y a plus de deux siècles, est donc égale à celle d'un décimètre-cube d'eau à à un grain de riz près.

La Convention du Mètre, signée le , formalise le système métrique (prédécesseur du Système international d'unités actuel) ; depuis 1889, il définit la magnitude du kilogramme comme égale à la masse du prototype international du kilogramme (PIK en abrégé, ou IPK pour ""), surnommé le « grand K ».

Le PIK est constitué d'un alliage de 90 % de platine et 10 % d'iridium (proportions massiques), nommé « Pt-10Ir ». Il prend la forme d'un cylindre de de hauteur et de diamètre afin de minimiser sa surface totale. L'ajout d'iridium augmente fortement la dureté du platine tout en conservant certaines de ses propriétés : forte résistance à l'oxydation, très haute masse volumique (presque deux fois plus dense que le plomb et plus que l'eau), conductivités électrique et thermique satisfaisantes, et faible susceptibilité magnétique. Le "PIK" et ses six copies sont stockés au Bureau international des poids et mesures, protégés chacun par trois cloches de verre scellées dans un coffre-fort spécial à « l'environnement contrôlé » dans la cave la plus basse du pavillon de Breteuil à Sèvres, dans la banlieue de Paris. Trois clés indépendantes sont nécessaires pour ouvrir ce coffre. Des copies officielles du PIK sont réalisées pour les États afin de servir de standards nationaux. Le PIK n'est extrait de son coffre que pour en réaliser des étalonnages tous les environ (cette opération n'a eu lieu que trois fois depuis sa création), afin de fournir une traçabilité des mesures locales.

Le Bureau international des poids et mesures fournit à ses États membres des copies du PIK de forme et composition quasi-identiques, destinées à servir de standards de masse nationaux. Par exemple, les États-Unis possèdent quatre prototypes nationaux :

Aucune des copies ne possède une masse exactement égale à celle du PIK : leur masse est calibrée et documentée avec des valeurs de décalage. Par exemple, en 1889, la masse du prototype américain K20 est déterminée comme égale à de moins que le PIK ( donc = ). Lors d'une vérification en 1948, sa masse est mesurée égale à . La dernière vérification en 1999 lui détermine une masse identique à sa valeur initiale de 1889.

La masse de K4 a constamment décliné par rapport à celle du PIK, car les standards de vérification étant plus souvent manipulés, ils sont plus sujets aux éraflures et autre usure. En 1889, K4 est délivré avec une masse officielle de . En 1989, il est calibré à et en 1999, à ; c'est-à-dire qu'en , K4 a perdu par rapport au PIK.

Le kilogramme est la dernière unité de base du Système international d'unités à être définie au moyen d'un étalon matériel fabriqué par l'homme. Par définition, l'erreur dans la valeur mesurée de la masse du « PIK » est exactement zéro. Toutefois, tout changement dans sa masse peut être déduite en la comparant avec ses copies officielles stockées autour du monde, périodiquement retournées au Bureau international des poids et mesures pour vérification.

Malgré les précautions d'utilisation et de conservation, la masse "théorique" (entendu dans ce sens, la masse qu'il ferait si on trouvait une autre définition au kilogramme) du prototype a déjà varié de quelques microgrammes par rapport aux masses de copies. Il est souvent incorrectement dit que la masse "théorique" du prototype aurait diminué de l'équivalent d'un grain de sable de de diamètre. En fait, lorsqu'on mesure les copies par rapport à l'étalon on note que les masses des copies ont augmenté relativement au prototype (ce qui peut laisser croire que la masse du prototype a diminué par sa manipulation (éraflure microscopique par exemple). En plus, il est probable que la masse "théorique" du prototype a aussi augmenté (par ajout de poussière, de traces de doigts, de caoutchouc par exemple), mais moins que celles des copies. Il est aussi possible que les masses des copies et la masse "théorique" du prototype aient diminué mais que la masse "théorique" du prototype ait diminué plus rapidement que les masses des copies. En tout état de cause, par définition, la masse réelle du prototype est elle toujours restée immuable à .

Selon James Clerk Maxwell (1831 - 1879) :

Au delà de la simple usure qu'un prototype peut rencontrer, sa masse peut varier pour un certain nombre de raisons, certaines connues et d'autres inconnues. Comme le PIK et ses répliques sont stockés à l'air libre (bien que sous deux cloches ou plus), ils gagnent de la masse par adsorption et contamination atmosphérique à leur surface. Par conséquent, ils sont nettoyés selon un procédé mis au point par le BIPM entre 1939 et 1946, qui consiste à les frotter légèrement avec une peau de chamois imbibée à parts égales d'éther-oxyde et d'éthanol, suivi d'un nettoyage à la vapeur d'eau deux fois distillée, avant de laisser les prototypes reposer 7 à . Ce nettoyage retire de de contaminants, selon la date du nettoyage précédent. Un deuxième nettoyage peut retirer jusqu'à de plus. Après le nettoyage, et même s'ils sont stockés sous leurs cloches, le PIK et ses copies commencent immédiatement à gagner de la masse à nouveau pour les mêmes raisons. . Comme les standards de vérification comme K4 ne sont pas nettoyés pour les calibrations de routine d'autres standards — une précaution minimisant leur usure potentielle — ce modèle est utilisé comme facteur correctif.

Comme les copies sont réalisées dans le même alliage que le PIK et stockées dans des conditions similaires, des vérifications périodiques permettent de contrôler sa stabilité. Il est devenu clair après la périodique réalisée entre 1988 et 1992 que les masses de tous les prototypes divergent lentement mais inexorablement les unes des autres. Il est également clair que la masse du PIK a perdu environ en un siècle, et peut-être plus, en comparaison de ses copies officielles. La raison de cette divergence n'est pas connue. Aucun mécanisme plausible n'a été proposé pour l'expliquer.

De plus, aucun moyen technique ne permet de déterminer si l'ensemble des prototypes souffre d'une tendance à plus long terme ou non, car leur masse « relative à un invariant de la nature est inconnue en dessous de ou sur une période de 100 ou même ». Comme on ne sait pas quel prototype a été le plus stable dans l'absolu, il est tout aussi valable de dire que l'ensemble du premier lot de copies, en tant que groupe, a gagné en moyenne environ en sur le PIK.

On sait en revanche que le PIK présente une instabilité à court terme d'environ sur une période d'un mois après nettoyage. La raison précise de cette instabilité n'est pas connue, mais on suppose qu'elle est liée à des effets de surface : des différences microscopiques entre les surfaces polies des prototypes, peut-être aggravées par l'absorption d'hydrogène par catalyse des composés organiques volatils qui se déposent lentement sur les prototypes et des solvants à base d'hydrocarbures utilisés pour les nettoyer.

Il est possible d'exclure certaines explications sur les divergences observées. Le BIPM explique, par exemple, que la divergence dépend plus du temps écoulé entre les mesures que du nombre de fois où les prototypes ont été nettoyés ou d'un changement possible dans la gravité locale ni de l'environnement. Un rapport publié en 2013 par Cumpson de l'université de Newcastle upon Tyne, basé sur la spectrométrie photoélectronique X d'échantillons stockés à côté de plusieurs prototypes, suggère qu'une source de divergence pourrait remonter à du mercure absorbé par les prototypes situés à proximité d'instruments utilisant ce métal. Une autre source provient d'une contamination carbonacée. Les auteurs de ce rapport suggèrent que ces contaminants pourraient être enlevés en utilisant une lumière ultraviolette et un lavage à l'ozone.

Les scientifiques constatent une plus grande variabilité des prototypes que ce qui était estimé à la base. La divergence croissante des masses des prototypes et l'instabilité à court terme du PIK ont initié des recherches pour améliorer les méthodes d'obtention d'une surface lisse à l'aide d'usinage au diamant sur les nouvelles répliques, et ont intensifié les recherches d'une nouvelle définition du kilogramme.

En 2013, le kilogramme est la "dernière unité SI toujours définie par un artéfact", mais des études sont en cours pour trouver une définition s'appuyant sur des constantes physiques fondamentales.

En 1960, le mètre, précédemment défini par une simple barre de platine iridié avec deux marques gravées, est redéfini en termes de constantes physiques fondamentales et invariantes (la longueur d'onde de la lumière émise par un atome de krypton, puis plus tard la vitesse de la lumière) afin que le standard puisse être reproduit dans différents laboratoires en suivant des spécifications précises. Afin d'assurer la stabilité à long terme du Système international d'unités, la Conférence générale des poids et mesures, en 2000, a recommandé que En 2005, lors de la du Comité international des poids et mesures (CIPM), une recommandation similaire est émise pour le kilogramme.

En octobre 2010, le CIPM vote pour soumettre une résolution à la Conférence générale des poids et mesures (CGPM), afin de les notifier de l'intention de définir le kilogramme à l'aide de la constante de Planck, "h". Cette résolution est acceptée par la du CGPM en octobre 2011 ; en outre, la date de la est avancée de 2015 à 2014. Cette définition permet théoriquement à n'importe quel appareil de , dès qu'il possède une précision et une stabilité suffisantes. La balance du watt pourrait être capable de répondre à cette demande. Si la CGPM adopte cette nouvelle proposition, et si la nouvelle définition du kilogramme est retenue dans le SI, la constante de Planck, qui lie l'énergie des photons à leur fréquence, aurait une valeur fixe déterminée. Après accord international, le kilogramme ne serait plus défini par la masse du PIK. Toutes les unités SI dépendant du kilogramme et du joule auraient également leur magnitude définie au bout du compte, en termes d'oscillations de photons. En fixant la constante de Planck, la définition du kilogramme ne dépendrait que de celle de la seconde et du mètre. La définition de la seconde ne dépend que d'une seule constante physique : « la seconde est la durée de de la radiation correspondant à la transition entre les deux niveaux hyperfins de l’état fondamental de l’atome de césium 133 ». Le mètre dépend de la seconde et de la vitesse de la lumière "c".

Afin de remplacer le dernier artéfact en usage, une variété de techniques et d'approches très diverses ont été considérées et explorées. Certaines sont fondées sur des équipements et procédures permettant la production à la demande de nouveaux prototypes (moyennant toutefois un effort considérable), à l'aide de techniques de mesure et de propriétés de matériaux basées au bout du compte sur des constantes fondamentales. D'autres font usage d'appareils mesurant l'accélération ou le poids de masses test, exprimant leur magnitude en termes électriques permettant là encore de remonter à des constantes fondamentales. Toutes les approches dépendent de la conversion d'une mesure de poids en une masse et nécessitent donc une mesure précise de la force de la gravitation dans les laboratoires. Toutes fixent également une ou plusieurs constantes physiques à une valeur déterminée. À ce titre le Canada semble avoir pris une longueur d'avance avec son projet de définition du kilogramme 

La balance du watt est une balance à plateau simple qui mesure la puissance électrique nécessaire pour s'opposer au poids d'une masse test d'un kilogramme dans le champ de gravitation terrestre. Il s'agit d'une variation de la qui emploie une étape de calibration supplémentaire qui annule l'effet de la géométrie. Le potentiel électrique de la balance du watt est délinéé par tension Josephson standard, qui permet à la tension électrique d'être liée à une constante physique avec une grande précision et une haute stabilité. La partie résistive du circuit est calibrée par rapport à une résistance standard Hall quantique. La balance du watt nécessite une mesure précise de l'accélération locale de la gravitation, "g", à l'aide d'un gravimètre.

En avril 2007, l'implémentation de la balance du watt par le "National Institute of Standards and Technology" (NIST) démontre une incertitude standard relative combinée de et une résolution à court terme de 10 à . La balance du watt du "National Physical Laboratory" possède une incertitude de en 2007. En 2009, Cette balance est désassemblée et transférée à l'institut canadien pour les standards de mesure nationaux (membre du Conseil national de recherches Canada), où la recherche et le développement de l'appareil se poursuit.

Dans la balance du watt, qui fait osciller une masse de test de haut en bas contre l'accélération gravitationnelle locale "g", la puissance mécanique requise est comparée à la puissance électrique, qui correspond au carré de la tension divisé par la résistance électrique. Cependant, "g" varie de façon significative — près de 1 % — suivant l'endroit de la terre où est effectuée la mesure. Il existe également des variations saisonnières subtiles de "g" à cause du changement des nappes d'eau souterraines, et des variations bimensuelles et journalières due aux forces de marée de la lune. Bien que "g" n'intervienne pas dans la nouvelle définition du kilogramme, elle intervient dans sa délinéation. "g" doit donc être mesurée avec autant de précision que les autres termes et doit donc être identifiable à des constantes physiques. Pour les mesures les plus précises, "g" est mesurée à l'aide de gravimètres absolus à chute de masse contenant un interféromètre à laser hélium-néon stabilisé par iode. Le signal d'interférence de sortie est mesuré par une horloge atomique à rubidium. Comme ce type de gravimètre dérive sa précision et sa stabilité de la constance de la vitesse de la lumière et des propriétés des atomes d'hélium, de néon et de rubidum, "g" est mesurée en termes de constantes physiques avec une très haute précision. Par exemple, dans le sous-sol de l'établissement du NIST de Gaithersburg en 2009, la valeur mesurée était typiquement contrainte à de .

L'utilisation d'une balance du watt pour délinéer le kilogramme dépend de sa précision et de sa concordance avec la précision améliorée de la mesure de la masse d'une mole de silicium très pur, ce qui dépend de la précision du mètre « rayons X », qui pourra s'améliorer via les travaux du physicien Theodor W. Hänsch. En outre, une telle balance nécessite un ensemble de technologies suffisamment complexes pour ne pas pouvoir être produite en grand nombre. Si le kilogramme est redéfini à l'aide de la constante de Planck, il n'y aura au mieux que quelques balances de watt en opération dans le monde.

Bien que n'offrant pas de réalisation pratique, il est possible de redéfinir la magnitude du kilogramme à l'aide d'un certain nombre d'atomes de carbone 12. Le (C) est un isotope du carbone. La mole est actuellement définie comme « la quantité d'entités (particules élémentaires ou molécules) égale au nombre d'atomes dans de ». Cette définition implique que (83⅓) moles de C ont exactement une masse d'un kilogramme. Le nombre d'atomes dans une mole, une quantité connue comme le nombre d'Avogadro, est déterminé expérimentalement et sa meilleure estimation actuelle est atomes. La nouvelle définition du kilogramme proposerait de fixer la constante d'Avogadro à précisément , le kilogramme étant défini comme la masse égale à • atomes de C.

La précision dans la valeur mesurée de la constante d'Avogadro est actuellement limitée par l'incertitude sur celle de la constante de Planck, depuis 2006. En fixant la constante d'Avogadro constante, l'incertitude sur la masse d'un atome de C — et la magnitude du kilogramme — ne pourrait être meilleure que . En adoptant cette définition, la magnitude du kilogramme serait sujet à des affinages ultérieurs, lorsqu'une meilleure valeur de la constante de Planck serait disponible.

Une variation de la définition propose de définir la constante d'Avogadro comme précisément égale à () atomes. Une réalisation imaginaire en serait un cube de C d'exactement de côté. Le kilogramme serait alors la masse égale à  × 83⅓ atomes de C.

Une autre approche basée sur la constante d'Avogadro, le « projet Avogadro », propose de définir et délinéer le kilogramme par une sphère de silicium de de diamètre. Le silicium a été retenu car il existe une infrastructure commerciale mature permettant de créer du silicium monocristalin ultra-pur et sans défaut, pour l'industrie des semi-conducteurs. Pour réaliser un kilogramme, une boule de silicium serait produite. Sa composition isotopique serait mesurée avec un spectromètre de masse afin de déterminer sa masse atomique relative moyenne. La boule serait coupée et polie en sphères. La taille d'une sphère serait mesurée par interférométrie optique avec une erreur de sur son rayon, environ une unique couche d'atomes. L'espacement cristalin entre les atomes (environ ) serait mesurée par interférométrie aux rayons X, avec une incertitude d'environ 3 parties par milliards. Avec la taille de la sphère, sa masse atomique moyenne et son espacement atomique connus, le diamètre requis peut être calculé avec suffisamment de précision pour permettre de finir de la polir à un kilogramme.

De telles sphères ont été réalisées pour le projet Avogadro et sont parmi les objets artificiels les plus ronds jamais réalisés. À l'échelle de la Terre, le point culminant de la meilleure de ces sphères — une zone de la taille d'un continent — s'écarterait de d'une sphère parfaite.

Des tests sont en cours sur les sphères de silicium du projet Avogadro afin de déterminer si leur masse est la plus stable quand elles sont stockées dans le vide, dans un vide partiel ou à pression ambiante. Dans tous les cas, aucun moyen technique n'existe actuellement qui permet de prouver que leur stabilité à long terme est meilleure que celle du PIK, car les mesures de masse les plus précises et les plus sensibles sont réalisées avec des balances à deux plateaux, qui ne peuvent comparer la masse d'une sphère de silicium qu'avec une masse de référence (les balances à un seul plateau mesurent le poids par rapport à une constante physique et ne sont pas suffisamment précises, l'incertitude nécessaire étant de par milliard). D'après ce que l'on sait de l'absence de stabilité du PIK et de ses copies, il n'existe aucun artefact d'une masse parfaitement stable permettant cette comparaison. De plus, le silicium s'oxyde pour former une fine couche (de l'ordre de ) de silice et de monoxyde de silicium. Cette couche augmente légèrement la masse de la sphère, un effet qu'il faut prendre en compte lors du polissage final.

Toutes les approches basées sur le silicium fixeraient la constante d'Avogadro, mais conduiraient à des définitions différentes pour le kilogramme. Une approche ferait usage de silicium avec ses trois isotopes naturels présents. Environ 7,78 % du silicium est formé de deux isotopes plus lourds, Si et Si. Comme pour l'approche au C, cette méthode définirait la magnitude du kilogramme en fixant la constante d'Avogadro à un certain nombre d'atomes de C ; la sphère de silicium en serait la réalisation pratique. Cette approche pourrait définir précisément la magnitude du kilogramme car les masses des trois nucléides de silicium relativement à celle du C sont connues avec précision (incertitudes relatives de 1 partie par milliard, ou mieux). Une méthode alternative utiliserait des techniques de séparation isotopique afin d'enrichir le silicium en un Si quasiment pur, qui possède une masse atomique relative de . Avec cette approche, la constante d'Avogadro serait fixée, mais également la masse atomique du Si. Le kilogramme serait alors défini comme × atomes de Si. Mais même avec une telle définition, une sphère de Si dévierait nécessairement du nombre de moles requis pour compenser ses diverses impuretés isotopiques et chimiques, ainsi que prendre en compte l'oxydation en surface.

Une autre approche basée sur la constante d'Avogadro et depuis abandonnée, l'accumulation d'ions, aurait défini et décliné le kilogramme en créant des prototypes de métal à la demande. Ils auraient été créés en accumulant des ions d'or ou de bismuth (des atomes auquel il manque un électron) et en les comptant en mesurant le courant électrique nécessaire pour les neutraliser. L'or (Au) et le bismuth (Bi) ont été choisis car ils peuvent être manipulés sans danger et possèdent la masse atomique la plus élevée parmi les éléments non-radioactifs (bismuth) ou parfaitement stables (or).

Avec une définition basée sur l'or, la masse atomique relative de l'or aurait été fixée à exactement , au lieu de sa valeur actuelle de . La constante d'Avogadro aurait là encore été fixée. Le kilogramme aurait été défini comme la masse égale à exactement × atomes d'or.

En 2003, des expériences avec de l'or et un courant de mettent en évidence une incertitude relative de 1,5 %. Des expériences ultérieures avec des ions bismuth et un courant de espéraient accumuler une masse de en six jours et avoir une incertitude relative meilleure qu'. Au bout du compte, cette approche par accumulation d'ions s'est révélée inadapté. Les mesures nécessitent des mois et les données sont trop erratiques pour pouvoir servir de remplacement au PIK.

Un autre approche définirait le kilogramme comme .

Dans les faits, le kilogramme serait défini comme dérivé de l'ampère plutôt que la situation actuelle, où l'ampère est un dérivé du kilogramme. Cette redéfinition fixe la charge élémentaire ("e") à exactement .

Une réalisation pratique basée sur cette définition délinée la magnitude du kilogramme directement dans ce qui définit la nature même de la masse : une accélération due à une force appliquée. Cependant, il est très difficile de concevoir une réalisation pratique basée sur l'accélération de masses. Des expériences ont été réalisées sur des années au Japon avec une masse de supraconductive supporté par lévitation diamagnétique et n'ont jamais atteint une incertitude meilleurs que dix parties par million. L'hystérésis était l'un des facteurs limitants. D'autres groupes ont effectué des recherches similaires à l'aide de différentes techniques pour faire léviter la masse.

Comme l'unité de base « kilogramme » comporte déjà un préfixe, les préfixes SI sont ajoutés par exception au mot « gramme » ou à son symbole g, bien que le gramme ne soit qu'un sous-multiple du kilogramme ( = ).

Par exemple :

Dans les anciens livres, seuls les multiples et sous-multiples du kilogramme sont utilisés :

Dans la pratique, seuls les multiples du kilogramme sont utilisés :

Dans la pratique, seuls les sous-multiples du kilogramme sont utilisés (les unités qui ne sont pas en italiques sont peu usitées) :

On utilise également des noms d'unités anciennes, mais arrondies à des valeurs « exactes »
À ne pas confondre avec

le quintal "français ancien" : environ ou avec

le quintal "court" d'Amérique du Nord : environ ou avec

le quintal "long" du système impérial anglais : environ.


Les unités anglo-saxonnes sont assez largement utilisées de par le monde. On utilise couramment les unités du système "avoirdupois" (av), et, dans certains cas spécifiques, les unités du système "troy" (t) : médicaments et métaux précieux.

La table ci-dessous indique les correspondances entre les unités ; les valeurs en italiques indiquent les croisements entre les systèmes anglo-saxons.

Le carat est une autre unité de masse.




</doc>
<doc id="14155" url="https://fr.wikipedia.org/wiki?curid=14155" title="Masse">
Masse

En physique, la masse est une grandeur physique positive intrinsèque d'un corps. En physique newtonienne, c'est une grandeur extensive, c'est-à-dire que la masse d'un corps formé de parties est la somme des masses de ces parties. Elle est conservative, c'est-à-dire qu'elle reste constante pour un système isolé n'échangeant pas de matière avec son environnement.

La masse se manifeste à travers deux propriétés fondamentales. En mécanique statique, c'est tout d'abord et historiquement une grandeur immédiatement accessible à la mesure, à travers la pesée, qui permet de comparer la masse d'une certaine quantité de matière à une masse étalon. C'est ce que l'on appelle la « masse pesante ». La masse est ainsi directement liée à la quantité de matière que contient un corps. En mécanique dynamique, cette grandeur intervient directement dans le principe fondamental de la dynamique, comme exprimant la « résistance de la matière au changement de vitesse » : plus la masse du corps est importante, plus la force pour changer la direction ou la grandeur de sa vitesse doit être importante. C'est ce que l'on appelle la « masse inerte ». Cet aspect de la masse joue un rôle essentiel dans toutes les branches de la dynamique. C'est donc une notion présente dans de nombreuses relations de la physique classique et dans les calculs qui en découlent. On constate une proportionnalité stricte entre masse inerte et masse pesante, indépendamment de la nature du matériau : cette équivalence des deux notions a été érigée en un principe d'équivalence.

L'unité de masse est le kilogramme dans le Système international d'unités (S.I.).

La masse est facilement confondue avec le poids, qui, dans le vocabulaire de la physique, est la force exercée par la gravité sur un corps pesant.

Historiquement, la masse a été mesurée au moyen d'une balance (balance romaine, balance Roberval). La mesure de la masse s'appelle le pesage, bien que ce terme provienne du mot « poids ». Dans sa forme la plus simple, deux plateaux supportés par des bras de mêmes longueurs sont équilibrés, l'un portant la matière dont on cherche à mesurer la masse, et l'autre portant divers poids étalons de masse convenue. L'égalité des masses est démontrée par l'équilibre de la balance (du latin, "aequalis"=égal, et "libra"=balance) ; et (surtout) par le fait que cet équilibre est maintenu lorsqu'on échange le contenu des plateaux, chose trop souvent oubliée par des clients naïfs. La masse ainsi mesurée est celle d'une quantité de matière, c'est la « masse pesante ».

Avec l'émergence de la mécanique céleste au , il est devenu courant de mesurer les masses relatives des corps célestes. La masse n'est plus alors directement mesurée, mais calculée, à travers l'effet qu'elle exerce sur la trajectoire des autres corps. Cette masse, qui est statiquement à l'origine de la force de gravitation, est toujours une « masse pesante ». On peut de même estimer une masse par la perturbation du champ de gravité qu'elle induit. Cette mesure par gravimétrie n'est utilisable que pour les objets extrêmement lourds, et est utilisée en géologie pour estimer la taille d'une formation rocheuse, ainsi qu'en archéologie.

Mais par ailleurs, la cinématique a permis d'étudier les transferts de quantité de mouvement dus à des forces et se traduisant par des variations de vitesse. De même, avec la physique des particules du , la masse d'une particule chargée est calculée à partir de l'accélération qu'elle subit dans un champ électrique, ou de la courbure de sa trajectoire dans un champ magnétique. Cette masse qui apparaît en cinématique traduit la résistance de la matière à des changements de vitesse, et est la « masse inerte ». Il s'agit toujours d'une masse calculée à travers d'autres grandeurs physiques mesurées.

Au quotidien, la masse inerte en tant que « résistance de la matière aux variations de mouvement » se perçoit facilement à travers l'effet de forces continues, lorsque par exemple on souffle avec une paille sur une balle de tennis (de l'ordre de ) et sur une boule de pétanque (de l'ordre de ) : la force exercée dans les deux cas est sensiblement la même, mais il est beaucoup plus difficile de mettre en mouvement la boule de pétanque, parce que sa masse est beaucoup plus importante. De même, cette résistance se perçoit facilement à travers l'effet des chocs : un footballeur tapant dans un ballon de football (de l'ordre de ) lui donne facilement une impulsion qui le fait partir à grande vitesse, sans que la quantité de mouvement de sa jambe n'ait sensiblement varié ; le même exercice sur un boulet de canon de même taille (dont la masse est d'une soixantaine de kilogrammes) n'ébranlera guère le boulet, mais arrêtera net la jambe (et très probablement, fracassera le pied).

La mécanique classique a dégagé un certain nombre de propriétés de la masse :
Toutefois, la physique moderne montre que ces propriétés ne sont vérifiées que dans les conditions de l'expérience courante ; mais peuvent ne plus l'être en physique quantique ou en mécanique relativiste.

La masse est à la racine de deux grandeurs conservatoires fondamentales en mécanique :
Ces deux lois de conservation sont fondamentales tant en physique newtonienne qu'en physique moderne. Sur cette base, il est possible de proposer une nouvelle définition de la masse, qui permet de rendre compte des cinq propriétés ci-dessus sans en présupposer aucune ; en particulier sans avoir à recourir à la notion de force, critiquée au par des physiciens comme Ernst Mach, Gustav Kirchhoff, Heinrich Hertz parmi d'autres.

Chaque culture a eu ses unités de masse, et des unités différentes étaient souvent utilisées pour des produits différents. Dans les anciennes unités de mesure françaises on peut relever les poids de marc, dont le quintal reste d'usage courant. Les unités de mesure anglo-saxonnes sont beaucoup plus variées, et la livre y reste d'usage courant. Il faut se rappeler que la livre, en France, n'avait pas la même valeur sur tout le territoire : la provençale, la parisienne ou encore la bretonne n'avaient pas tout à fait la même valeur et aujourd'hui encore, la livre tout comme le gallon n'ont pas la même valeur aux États-Unis et au Royaume-Uni. Beaucoup de marchandises se vendaient au volume, par boisseaux ou encore par barils, soit 18 boisseaux (235 litres) — différent du baril pétrolier qui ne fait que 158,98 litres.

L'unité SI de masse est le kilogramme (kg) et non pas le gramme (g). On utilise également la tonne (t), égale à .

Dans l'Union européenne, de nombreuses masses (et volumes) sur les produits de consommation sont indiquées en quantité estimée. Ils sont marqués comme tel, d'un « e » minuscule.

Dans le domaine scientifique, on utilise l'unité de masse atomique. Du fait de l'équivalence masse-énergie révélée par la fameuse formule E=mc, les physiciens spécialistes des particules utilisent la même unité de mesure pour la masse et l'énergie, en général un multiple d'électron-volt/c², ce qui est rendu indispensable par l'observation quotidienne, dans les accélérateurs de particules, de la transformation de l'énergie en ses différentes formes : masse, énergie cinétique, énergie de liaison, lumière.

On peut aussi estimer indirectement la masse à partir du poids, c'est-à-dire que l'on mesure la force qu'exerce l'objet à peser ; le dispositif est en fait un dynamomètre. C'est le cas le plus courant des pèse-personne et des balances électroniques. Cette méthode ne donne pas le même résultat de mesure sur la Terre et sur la Lune, parce que le dynamomètre compare le poids à une force indépendante de la gravitation (celle du ressort), alors que le poids, qui est la force de gravitation s'exerçant sur un corps donné, dépend du lieu, et sera différent sur Terre et sur la Lune. Pour mesurer correctement la masse d'un corps à partir d'un dynamomètre, il faut donc d'abord étalonner le dynamomètre avec une masse de référence.

Dans le langage courant, la masse est fréquemment confondue avec le « poids ». La confusion est d'autant plus facile que la graduation du peson est (incorrectement) donnée en kilogrammes, que c'est par une « pesée » que l'on mesure la masse, que cette pesée est (de fait) effectuée par une comparaison des poids, et que ce que les corps « pèsent » est donc alors (correctement) exprimé en kilogrammes.

Dans le vocabulaire de la physique, le mot « poids » désigne spécifiquement la force exercée par la gravité sur un corps pesant, dont la valeur dépend de la pesanteur, et dont l'unité est le newton (Symbole N). Considérons par exemple un objet de masse "m" suspendu à un dynamomètre. La Terre exerce sur cet objet une force formula_1, appelée "poids de l'objet", qui est donnée par la loi universelle de la gravitation : en supposant la terre parfaitement sphérique, le poids de l'objet est proportionnel à sa masse et inversement proportionnel au carré de sa distance au centre de la terre. Donc, suivant l'altitude son poids est variable. Par exemple, à Paris, où "g" = 9,81 N/kg, une masse de 10 kg a donc pour poids 98 N (même si l'on dit couramment, dans la vie quotidienne, qu'un objet "pèse" 10 kg) ; et au sommet de l'Everest son poids est légèrement plus petit.

L'accélération de la pesanteur étant sensiblement constante sur la surface de la Terre, la mesure du poids rend compte de la masse pour les besoins pratiques, même s'il existe de légères différences d'un lieu à l'autre, de l'ordre du pour-cent. Pour cette raison, une unité pratique de poids a longtemps été le kilogramme-force, simplement défini comme le poids du kilogramme. La distinction entre poids et masse a conduit dans un souci de rigueur à remplacer dans l'affichage officiel le « kilogramme-force » par le « décanewton », qui n'est que 2 % plus grand. Pour cette raison, les limites d'utilisation des engins de chantiers et des ponts roulants est alors affiché en kilodécanewton (kdaN), les opérateurs sachant que « en pratique » le kdaN correspond à une tonne (ou pour le dire en toute rigueur : c'est à 2 % près le poids d'une masse d'une tonne, dans des conditions normales de pesanteur).

À strictement parler, la masse se mesure donc avec une balance, tandis que le poids se mesure avec un peson. La meilleure manière de percevoir la différence est de se rappeler que la masse est une grandeur scalaire, un nombre sans orientation, tandis que le poids est une grandeur vectorielle, qui a une orientation vers le bas (et qui définit la verticale en un lieu). L'autre manière de visualiser la différence est de se rappeler la démarche embarrassée et flottante des astronautes sur la Lune : leur masse est la même que sur Terre, et se manifeste toujours par la même résistance aux variations de vitesse ; mais leur poids est alors six fois plus faible, à cause de la faible gravité lunaire.

En sciences physiques, on peut distinguer de nombreux aspects à travers lesquels apparaît la notion de masse, ou traduisant ce concept. À ce jour, l'expérience montre que tous ces aspects conduisent à des valeurs identiques, l'ensemble construisant progressivement ce qu'est le concept abstrait de « masse » en physique.

Depuis que le commerce existe, le « poids » d'un objet est ce qui permet d'apprécier une « quantité de matière », principe à la base de la vente par pesée : plus il y a de matière, et plus le prix à payer est fort ; plus le poids d'or est grand, et plus grande est sa valeur.

C'est cette notion qui conduit historiquement au premier concept de « masse », en tant que grandeur essentiellement additive (en termes modernes, une grandeur extensive). Pour l'expérience quotidienne, le champ gravitationnel terrestre est une donnée constante ; et il n'y a pas de raison de distinguer entre le poids concret, accessible aux sens, et une masse abstraite qui en serait la cause.

Pour la pensée scolastique, gouvernée par les enseignements d'Aristote, le poids est une qualité intrinsèque de la matière, laquelle est par nature attirée vers le bas parce que c'est son lieu de repos ; mais c'est une propriété des seuls corps lourds susceptibles de chuter : l'air n'a pas de poids et reste à son emplacement, de même que les nuages. La loi empirique du mouvement est que tout corps tend vers son lieu de repos naturel, soit en tombant quand il est en l'air, soit en s'arrêtant progressivement quand il roule au sol : le mouvement résulte d'un déséquilibre, et le rétablissement de cet équilibre entraîne l'absence de mouvement. De plus, l'expérience courante montre que les objets lourds comme une enclume tombent rapidement, tandis que les objets plus légers, comme des plumes ou de la flanelle, tombent plus lentement ; ce qui conforte l'idée informelle que la vitesse de chute est qualitativement fonction du poids.

Nonobstant toutes sortes de transformations physiques et chimiques, la conservation de la masse a longtemps été expérimentalement observée, puis admise comme une grandeur fondamentale et confondue avec « la quantité de matière » (Isaac Newton l'a définie comme telle dans ses "Principia Mathematica"). 
La nature phénoménologique de la masse se confond dans cette approche avec celle du poids : c'est la qualité additive de la matière qui peut faire l'objet de comparaisons par pesée. L'opération de pesée conduira à terme à la notion de « masse pesante » (plus tard vue en mécanique newtonienne comme une « masse grave » puis une « masse gravitationnelle passive »), qui se mesure à travers la force appliquée à un objet lorsqu'il est passivement plongé dans un champ gravitationnel. Le rapport de cette « masse pesante » à celle d'une masse étalon peut être déterminé lors d'opérations de pesées classiques, parce que dans l'histoire de l'humanité, cette pesée est toujours effectuée dans le champ gravitationnel terrestre.

Avec la physique galiléenne, au début du , l'étude de la chute des corps et les premières études de dynamique permettent de dégager ce qui deviendra la notion de « masse inerte », qui est la mesure de la résistance interne d'un objet à un changement de mouvement lorsqu'une force lui est appliqué.

La révolution introduite par Galilée est celle de la "mesure" des lois du mouvement. En pratique, dans ces premières expériences, la force en question est celle de la pesanteur, ou une fraction de celle-ci dans les expériences sur plan incliné ; et le mouvement étudié est celui d'un pendule ou d'un corps en chute libre. Expérimentalement, et contrairement à l'intuition de l'époque, la loi du mouvement ne dépend pas du poids — qui est pourtant de toute évidence la force à laquelle l'objet est soumis — et ne dépend pas non plus de la nature du corps ou de sa densité. Cette indépendance surprenante du résultat par rapport au poids "a priori" générateur du mouvement conduit à identifier dans la matière une qualité d'inertie, capacité du corps à s'opposer à la variation de vitesse en créant ce qui semble être une « force d'inertie » équilibrant l'effet du poids.

Par rapport à la vision scolastique, Galilée ne change donc pas radicalement la nature phénoménologique de la masse / quantité de matière, mais lui rajoute un caractère se manifestant dans les lois du mouvement : celui de pouvoir engendrer une « force d'inertie ». Pour Galilée, cette « force d'inertie », étant capable de rendre le mouvement indépendant du poids, se manifeste donc nécessairement de manière proportionnelle à celui-ci.

Galilée étudie des situations où le frottement est négligeable, ce qui condamne l'idée d'une tendance naturelle au repos. De toute évidence il n'y a pas une telle force d'inertie sur un corps au repos. Mais par ailleurs, cette « force d'inertie » se manifestant comme une opposition à toute "variation" de vitesse, le déplacement dans un plan horizontal, qui n'est pas influencé par le poids, doit donc également se faire sans variation de vitesse : ce déplacement idéal à vitesse constante, identifiée par Galilée, est celui d'un référentiel galiléen.

L'approche de Galilée était obscurcie par la confusion, traduisant les conceptions de son époque, entre la masse et le poids, ce dernier était compris comme une propriété (vectorielle) intrinsèque à la matière, et conduisant à l'idée d'une force compensatrice émergeant à l'occasion des variations de vitesse.

De nos jours, on dira plutôt que le poids est la manifestation d'une qualité de « masse pesante », tandis que la force d'inertie manifeste de son côté celle « masse inerte », l'une et l'autre propriétés scalaires intrinsèques de la matière, et que l'on trouve expérimentalement en proportion constante - ce qui permet d'assimiler l'une à l'autre. En termes modernes, l’expérience de Galilée est la première manifestation de l'identité entre masse inerte et masse pesante. Il y a cependant une différence conceptuelle très importante entre ces deux, parce qu'elles diffèrent radicalement dans leur manifestation. Conceptuellement, la « masse inerte » n'apparaît qu'en dynamique. Elle est définie en appliquant une force à un objet, et en mesurant l'accélération qui en résulte : à force égale, l'accélération d'un objet est inversement proportionnelle à la masse de cet objet.

Avec la mécanique newtonienne, à la fin du , la logique causale ayant conduit à imaginer une « force d'inertie » est inversée : dans la loi du mouvement il n'y a pas une « force d'inertie » découlant d'une qualité d'inertie (scalaire), qui s'oppose à la variation de vitesse que tend à créer la « masse pesante » (comprise comme vectorielle). L'approche analytique du calcul différentiel traite avant tout de position, de vitesse et d'accélération, et sépare alors ces questions de toute préoccupation relative à la masse.

Celle ci étant introduite, il apparaît que d'une manière générale, il est nécessaire d'appliquer une « force » pour créer une variation de vitesse en proportion de la quantité de matière, laquelle est alors mesurée par une « masse inerte » formula_2 (comprise comme scalaire) ainsi mise en mouvement. Autrement dit, le changement de la quantité de mouvement d'un corps, provoqué par une force agissant sur lui, est proportionnel à cette force en grandeur et direction ; ou inversement, une force est ce qui est capable de produire une variation de la quantité de mouvement d'un corps matériel. C'est le principe fondamental de la dynamique :

La nature phénoménologique de la masse inerte est donc de se manifester par le rapport entre une force appliquée et une variation de quantité de mouvement. Prise littéralement, cette définition que l'on note à présent formula_4 est cependant circulaire, dans la mesure où la mesure d'une force suppose par ailleurs que la masse soit définie d'une manière ou d'une autre. Ce n'est qu'avec Ernst Mach que la masse inerte recevra une définition phénoménologique rigoureuse, à travers le principe d'action et de réaction.

Il n'y a pas non plus une qualité de poids (vectoriel) intrinsèque à la matière. L'approche newtonienne est que le poids (vectoriel) est également une force, qui traduit l'attraction qu'exerce la Terre, laquelle est proportionnelle à une certaine « masse grave » ; et seule cette « masse grave » (également comprise comme un scalaire) est une qualité intrinsèque à la matière, parce que la force, elle, doit dépendre de la distance à la terre (ce que montrent les lois de Kepler). Pour la physique newtonienne, si les lois de la chute des corps sont les mêmes que celles de la mécanique céleste, et si donc la Lune « tombe » en permanence sur la Terre comme le ferait une pomme, c'est qu'il s'agit d'un seul et même phénomène : l'attraction subie par la Lune —ce qui mesure sa « masse pesante », mais dépend de sa distance— n'est autre que sa réponse gravitationnelle à la présence de la Terre (désignée dans ce contexte comme sa « masse grave », ou « masse gravitationnelle passive »). Ce changement de perspective conduit donc à identifier « masse grave » et « masse pesante ».

En séparant ainsi poids vectoriel et masse scalaire (qu'elle soit inertielle ou gravitationnelle), c'est Newton qui est ainsi à l'origine du concept moderne de masse scalaire. Par ailleurs, la force exercée par la terre sur une « masse grave » céleste est donc égale à celle nécessaire à vaincre la résistance à l'accélération de sa « masse inerte », ce qui conduit à supposer que « masse grave » et « masse inerte » sont de même nature, voire (dans la mesure où il n'y a pas de raison que les lois de la gravitation dépendent de la nature des corps) qu'elles sont égales.

Formalisé ensuite dans le cadre général du potentiel newtonien, la « masse gravitationnelle passive » devient une réinterprétation de la « masse pesante », qui prend sa source dans la loi universelle de la gravitation.

De son côté, la « masse gravitationnelle active » est la « charge massique » responsable du potentiel gravitationnel créé par un corps, et dont on constate la présence en mécanique céleste : pour Newton, « tous les corps ont en propre un pouvoir de gravité, proportionnel aux quantités de matière que chacun d'eux contient ». Autrement dit, la nature phénoménologique d'une masse (pesante), sur le plan de la mécanique céleste, est sa capacité à imposer une accélération centripète à son espace environnant ; cette capacité de « masse gravitationnelle active » se traduit par le « paramètre gravitationnel standard » formula_5, qui donne l'accélération gravitationnelle γ exercée par ce corps à une distance "r" : 

Par ailleurs, la distinction entre masses gravitationnelles active et passive n'a pas lieu d'être en mécanique classique, puisque la « charge massique » est aussi bien à l'origine du potentiel (donc de la masse active) qu'à l'origine de la force subie par une particule chargée qui y est placée (donc de la masse passive ou inertielle). Une différence entre ces deux notions n'apparaît qu'en relativité générale, dans certains problèmes de cosmologie.

Pour Newton encore, la masse représente essentiellement une quantité de matière, qui est définie par le produit d'un volume (caractéristique géométrique) par une densité (propriété intrinsèque dépendant de la nature du matériau considéré). Un siècle plus tard, à la fin du , Antoine Lavoisier découvre ensuite expérimentalement la loi de conservation de la masse: . De ce fait, c'est bien la masse qui est une qualité intrinsèque et conservatoire de la matière, et non la masse volumique.

Cette loi, qui se révéla approximative, confirmait la définition de la masse et permit de s'en servir comme constante dans les transformations chimiques (d'où la classification des éléments chimiques en fonction de leur masse atomique dans le tableau de Mendeleiev) et fut un élément de mesure permettant, entre autres, de mettre en évidence l'existence des atomes. La notion de « quantité de matière » s'en est progressivement dégagée pour les besoins de la chimie. L'analyse précise des masses impliquées dans les transformations chimiques conduira à la loi des proportions définies, puis à la loi des proportions multiples, conduisant enfin à la notion d'atome et de masse atomique. La nature phénoménologique de la quantité de matière contenue dans un corps (chimiquement pur), qui se manifeste par la pesée, est alors essentiellement le nombre d'atomes (ou de molécules) impliqués, dont le nombre immense est appréhendé par le nombre d'Avogadro ; et la masse de ce corps n'est autre que la masse totale de ces atomes, agissant sur le plan pondéral par leur masse atomique.

Le Système international d'unités, continuellement amélioré depuis le , établit une distinction fondamentale entre la quantité de matière, mesurée en mole, et la masse, mesurée en kilogramme.

La quantité de matière peut être déterminée avec précision dans certains cas, en comptant le nombre d'atomes d'un échantillon, obtenu par dépôt électrolytique ou par usinage d'ultra-haute précision d'un monocristal sans défaut. Une mesure précise montre que la masse d'un tel échantillon est « presque » intégralement due au nombre et à la nature des molécules qui le constitue. L'écart à cette valeur théorique est dû à l'énergie de liaison qui assure la cohésion de l'ensemble, et qui contribue à un léger « déficit de masse » de l'échantillon par rapport à la somme de ses constituants : c'est un effet abordé par la relativité restreinte.

Dans la vie de tous les jours, à notre échelle et lors de processus de basse énergie, on considère volontiers que la masse est une grandeur additive : si l'on prend deux paquets de sucre de 1 kg, on obtient 2 kg de sucre.

En relativité restreinte, cependant, la résistance d'un corps à une variation de vitesse devient d'autant plus grande que cette vitesse se rapproche de celle de la lumière : le principe fondamental de la dynamique reste valable sous sa forme formula_8, mais la « masse inerte » qui est définie ainsi ne peut plus être considérée comme constante ; elle ne se limite pas à la « masse au repos » de la matière, parce que l'énergie elle-même correspond à une masse inertielle, suivant le principe d'équivalence entre masse et énergie.

La masse d'inertie relativiste est alors définie par rapport à la masse au repos formula_9 et la vitesse formula_10, par :

La masse au carré est l'invariant relativiste (la pseudo-norme) du quadrivecteur impulsion ou quadri-moment, formula_12, ce qui permet d'écrire la relation formula_13, où "m" est la masse, "E" l'énergie totale du corps (énergie de masse + énergie cinétique) et "p" sa quantité de mouvement. Aussi, dans cette optique, il n'y a qu'une seule masse, la masse invariante, liée au quadri-vecteur énergie-impulsion. Dès lors, on peut considérer la masse comme une forme d'énergie, appelée énergie de masse, et il apparaît que la notion véritablement invariante au cours des transformations physiques n'est pas la masse mais l'énergie qui se manifeste successivement sous différentes formes : sous forme de masse, d'énergie cinétique, d'énergie de liaison entre particules.

La conséquence de la théorie est que toute énergie possède également une masse inerte ; et inversement, toute masse au repos représente une énergie interne, susceptible d'être libérée. Dans les phénomènes usuels, les échanges d'énergie sont suffisamment faibles pour que la masse puisse être considérée comme sensiblement constante, mais ce n'est plus le cas pour la physique atomique et pour la cosmologie. La création de paires de particules élémentaires, et la fusion nucléaire, sont des exemples où des quantités non négligeables de masse sont converties en énergie, ou inversement. De même, la déviation gravitationnelle des rayons lumineux montre que des photons, particules sans masse mais porteur d'énergie, ont un comportement similaire à celui d'une masse gravitationnelle passive.

La relativité restreinte et la physique atomique révèlent que masse et quantité de matière sont deux quantités qui ne sont pas exactement proportionnelles ; bien que pour une substance inerte donnée (le carbone par exemple), la masse est directement proportionnelle à la quantité de matière, aux forces de liaison près : dix grammes de carbone contiennent bel et bien dix fois plus de matière qu'un seul gramme de carbone. De ce fait, la masse n'est pas réellement une quantité extensive et conservatoire, mais doit être appréhendée dans un ensemble plus vaste. La relativité restreinte montre que la masse (inertielle) constitue une forme d'énergie du corps qui, dès lors, n'est pas strictement conservée : par exemple, la dissipation d'énergie sous forme lumineuse se traduit par une perte de masse qui n'est pas envisagée par la physique classique. La connaissance de la constitution de la matière offre d'autres exemples de pertes de masse par l'utilisation de l'énergie sous forme de liaisons atomiques.

L'énergie nucléaire, qu'elle provienne de la fusion ou de la fission, résulte de la transformation d'une certaine quantité de masse en énergie :

Ce lien entre énergie et masse permet d'utiliser la même unité de mesure pour la masse et l'énergie : une unité de mesure de l'énergie, par exemple l'électron-volt est souvent utilisé pour exprimer la masse des particules élémentaires. Les accélérateurs de particules permettent de transformer physiquement de l'énergie en masse. Dans ce cadre, exprimer la masse en électron-volt permet de voir plus facilement dans quel régime d'énergie on se trouve et de savoir si l'on peut s'attendre à l'apparition de nouvelles particules. Par exemple, quand on accélère un électron jusqu'à 99,999 % de la vitesse de la lumière, on peut considérer que sa masse devient environ 224 fois plus grande qu'au repos. On peut alors affirmer que lors d'une collision d'un électron et d'un positron accélérés à 99,999 % de la vitesse de la lumière, on peut produire des muons, qui sont effectivement 206 fois plus massifs que des électrons.

La relativité générale posera comme principe qu'il n'est pas possible de distinguer une accélération reflétant un changement de vitesse d'une accélération gravitationnelle, ce qui revient à poser que par nature, la « masse inerte » est égale à la « masse grave ». La relativité générale dérive entre autres du principe d'équivalence qu'Einstein présente comme une « interprétation » de l'égalité de la masse inerte et de la masse grave en termes de relativité du mouvement accéléré.

La courbure de l'espace-temps est une manifestation relativiste de l'existence de la masse, et l'on peut considérer que la nature phénoménologique de la masse est sa capacité à courber l'espace-temps. Dans la métrique de Schwarzschild, la présence d'une masse formula_2 se caractérise par la courbure qu'elle impose à l'espace, dont la déformation est donnée par le « rayon de Schwarzschild », ou rayon gravitationnel (formule où le facteur formula_15 est la masse linéique de Planck) :
Pour les masses usuelles, cependant, cette courbure est très faible et difficilement mesurable ; c'est la raison pour laquelle elle n'a pas été découverte avant d'être prédite par la théorie de la relativité générale. À un facteur formula_17 près, le rayon de Schwarzschild est lié au paramètre gravitationnel standard, noté formula_18, égal au produit de la constante gravitationnelle formula_19 par la masse formula_2 de l'objet correspondant. Là où le paramètre gravitationnel standard traduit la capacité d'une masse à imposer une accélération dans son environnement, le rayon de Schwarzschild traduit la capacité d'une masse à courber l'espace, parce qu'en relativité générale il n'y a pas de différence de nature entre ces deux manières de se placer dans un repère non galiléen.

La "masse grave" n'a pas sa place en relativité restreinte car la gravitation n'a pu y être incluse en respectant à la fois les principes relativistes et les observations. Toutefois, pour l'élaboration d'une gravitation relativiste, Einstein est parti du constat de l'égalité entre "masse grave" et "masse inerte" pour en tirer une « interprétation » sous la forme d'un nouveau principe : son « principe d'équivalence ». Ensuite, dans la théorie de la relativité générale, le rôle de la "masse grave" est tenu par l'énergie du corps, exprimée sous la forme du tenseur énergie-impulsion, prolongeant ainsi l'identité liant la masse inerte et l'énergie établie en relativité restreinte.

La masse des particules élémentaires (leptons et quarks) est une propriété intrinsèque de ces particules (qu'elle soit due ou non au boson de Higgs). Autrement dit, les particules élémentaires ont chacune une masse bien définie. Mais si macroscopiquement, la masse est associée à la matière, dans le détail la « matière » n'est pas un concept aussi bien défini que celui de « masse ». À l'échelle subatomique, non seulement les fermions (qui sont les particules usuellement associées à la notion de « matière ») ont une masse au repos, mais une masse au repos est également associée à quelques bosons, qui sont les particules vecteurs de force et qui servent de « colle » pour lier la matière. Un autre problème gênant pour assimiler masse et matière est qu'une grande partie de la masse au repos de la matière ordinaire provient de l'équivalent en masse d'énergies cinétiques, et de la contribution de particules qui n'ont pas par elles-mêmes de masse au repos. L'un dans l'autre, il n'y a que 1 % de la masse de la matière ordinaire qui peut être considéré comme provenant effectivement de la masse au repos de quarks fermioniques et d'électrons.

La masse, dans le cadre de la physique des hautes énergies, n'est pas une quantité extensive (additive). La masse de trois quarks pris individuellement, n'est pas égale à la masse d'un baryon contenant ces mêmes types de quarks : la masse du baryon résultant est égale à la somme des masses des trois quarks qui le constituent moins l'équivalent masse de l'énergie de liaison par la relation d'Einstein. C'est ainsi que les protons et les neutrons, ont une masse (env. 940 MeVformula_21) bien différente de la somme des masses des quarks qui les composent (quark up et quark down) (d'env. 10 MeVformula_21). Dans cet exemple, la grande différence de masse indique que la force nucléaire entre les quarks est très grande : c'est l'interaction forte.

Sur le plan quantique, la masse se manifeste comme la différence entre la fréquence quantique d'une particule et son nombre d'onde. La masse quantique d'un électron, la longueur d'onde de Compton, peut être déterminée de diverses manières et est liée à la constante de Rydberg, le rayon de Bohr, et le rayon classique de l'électron. La masse quantique d'objets plus grand peut être mesurée directement au moyen d'une balance du watt.

La physique quantique utilise l'équivalence masse-énergie pour caractériser les particules virtuelles, responsables des interactions entre particules.

En mécanique quantique, l'action observable ne peut varier que par un nombre entier d'action élémentaire, la constante de Planck. Ce principe de base conduit à de nombreuses conséquences étranges sur les limites de l'observabilité, et au fait que les lois habituelles de la physique classique ne sont plus respectées à l'échelle quantique. Il existe en particulier une relation d'incertitude portant sur l'énergie d'une particule et la variable temps : la durée formula_23 nécessaire à la détection d'une particule d'énergie formula_24 à formula_25 près vérifie la relation :
Inversement, donc, une fluctuation d'énergie formula_27 ne peut pas être détectée physiquement si elle apparaît et disparaît en un intervalle de temps inférieur : le vide est constamment l'objet de fluctuations d'énergie extrêmement brèves. Ces fluctuations peuvent se matérialiser par la création de paires de particules / antiparticules, du moment que leur masse-énergie est inférieure à celle de la fluctuation, et que la paire s’annihile dans un temps inférieur à formula_23 :

De même, une particule virtuelle peut être émise par une particule et capturée par une autre, à condition que ce soit fait suffisamment rapidement pour que la relation d'incertitude soit respectée. Ces particules virtuelles sont ainsi responsables des interactions entre particules (réelles), et de la propagation des champs ; ceci d'autant plus facilement que dans le monde étrange des particules virtuelles, la vitesse de la lumière n'est plus une limitation, du moment que leur durée de vie est négligeable. Ce concept de particule virtuelle est primordial en théorie quantique des champs.

En mécanique quantique relativiste, la masse est l'un des représentations irréductibles et unitaires d'énergie positive du groupe de Poincaré.

En particulier, le boson de Higgs, qui semble avoir été découvert le par l'expérience CMS et ATLAS au CERN, est, dans la théorie du modèle standard, considéré comme responsable de l'acquisition de masse par les particules.

En mécanique classique, la masse inerte apparaît dans l'équation d'Euler-Lagrange comme un paramètre "m" :
En remplaçant le vecteur "x" par une fonction d'onde pour quantifier cette relation, ce paramètre "m" apparaît dans l'opérateur d'énergie cinétique :
Dans la forme covariante (invariante par transformation relativiste) de l'équation de Dirac, et en unités naturelles, l'équation devient :
Bref, la masse "m" apparaît à présent comme une constante associée au quantum que décrit la fonction d'onde ψ associée à la particule.

Dans le modèle standard de la physique des particules élaboré à partir des années 1960, il a été proposé que ce terme constant pouvait provenir du couplage entre le champ ψ et un champ additionnel Φ, le Champ de Higgs électrofaible. Dans le cas de fermions, le mécanisme de Higgs (et al.) conduit à remplacer dans le lagrangien le terme "m"ψ par un terme de la forme formula_33.

Avec cette transformation, l’"explicandum" de pourquoi une masse est constatée sur diverses particules élémentaires se simplifie, la question étant alors celle de la valeur des couplages inconnus "G". Dans le modèle standard, un couplage des particules élémentaires au champ de Higgs permet alors d'expliquer l'origine de la masse de ces particules, une particule ayant fondamentalement une masse nulle par elle-même.

Du coup, le, ou les, boson(s) de Higgs serai(en)t responsable de la masse de toutes les particules élémentaires, ainsi que de celle de certains bosons d'échange des interactions. Au niveau des particules élémentaires, la masse des bosons de jauge de l'interaction faible (boson W et boson Z), est due au boson de Higgs, leur donnant ainsi des propriétés différentes de celles du boson de l'électromagnétisme, le photon.

L'existence du boson de Higgs a été confirmée de manière expérimentale en 2012 grâce à l'utilisation du LHC et a conduit à l'attribution d'un prix Nobel de physique en 2013. Cette particule élémentaire constitue l'une des clefs de voûte du modèle standard de la physique des particules.

Cette découverte probable d'un boson de Higgs massif est considérée comme une confirmation forte de la théorie. Mais il y a de toute manière des arguments forts en faveur de la rupture de symétrie électrofaible telle que décrite par le mécanisme de Higgs (et al.) ; et la non-existence d'un tel boson de Higgs ne conduirait qu'à une de ce mécanisme.

La masse d'un corps physique renvoie donc à deux natures phénoménologiques distinctes.

La masse grave (du latin "gravis", lourd) est une propriété de la matière qui se manifeste par l'attraction universelle des corps, et au quotidien, par leur poids. Concrètement, en présence d'un même champ de gravité extérieur (celui de la Terre par exemple), la masse de 20 kg subira une force (le poids) deux fois plus grande que la masse de 10 kg ; par ailleurs, une masse de 20 kg crée autour d'elle un champ de gravité deux fois plus intense qu'une masse de 10 kg. La masse grave (gravifique, gravitationnelle) d'un corps est définie par Isaac Newton comme une mesure de la quantité de matière de ce corps, c'est la grandeur physique intervenant dans le calcul de la force de gravitation créée ou subie par un corps : c'est ainsi qu'il l'a introduite dans la loi universelle de la gravitation, et qu'elle a été utilisée jusqu'à la relativité générale. La force de gravitation est donc proportionnelle à la quantité de matière. En physique classique, la masse grave est aussi supposée extensive.

La masse inerte est une propriété de la matière qui se manifeste par l'inertie des corps. Concrètement, une masse de 20 kg résiste deux fois plus à l'accélération qu'une masse de 10 kg. La masse inerte (inertielle) d'un corps est la grandeur physique utilisée pour calculer la force nécessaire pour qu'un corps acquière une accélération, en fonction de celle-ci. C'est la quantification de la résistance du corps aux accélérations. Mathématiquement, cela s'exprime par l'égalité formula_34, où formula_35 est l'accélération acquise et formula_36 est la force nécessaire à l'obtention de cette accélération. Isaac Newton a défini la masse inerte comme une autre mesure de la quantité de matière, et a considéré que pour imprimer à une quantité de matière doublée une même accélération, il fallait le double de force. En physique classique, la masse inerte est ainsi supposée extensive : en mêlant deux corps, on obtient un troisième corps dont la masse est la somme des masses des deux corps initiaux.

Il n'y a aucune raison fondamentale imposant que masse inerte et masse grave soient identiques, ce n'est qu'un fait empirique. À part le fait d'être toutes les deux proportionnelles à la quantité de matière (proportionnalité approximative comme cela a été montré à partir du début du ), la masse grave et la masse inerte semblent "a priori" n'avoir aucun lien entre elles, et constituer deux propriétés de la matière tout à fait indépendantes l'une de l'autre. Mais inversement, et bien que les deux soient conceptuellement distincte, aucune expérience n'a jamais pu mettre en évidence une quelconque différence entre les deux.

Le fait de désigner ces deux manifestations sous le même terme de « masse » présuppose qu'il s'agit de la même grandeur physique, ce qui est effectivement la conception usuelle, mais obscurcit le caractère extraordinaire de cette proportionnalité. Pour mieux toucher du doigt cette différence, on peut discuter de deux qualités scalaires conservatoires et extensives de la matière, l'« inertie » qui se manifeste en dynamique, et le « pondéral » qui se manifeste par l'attraction gravitationnelle.

On peut en effet imaginer deux corps de natures différentes, ayant même inertie et des pondéraux différents. Le pondéral (qui intervient dans la loi de la gravitation de Newton) est formellement l'analogue de la charge électrique (qui intervient dans la loi de Coulomb) : le pondéral est en quelque sorte une "charge gravitationnelle" de la matière. De même qu'une charge électrique est à l'origine d'un potentiel électrostatique, le pondéral est à l'origine d'un potentiel gravitationnel. De même qu'une charge électrique placée dans un champ électrique subit une force électrostatique formula_37, de même une charge pondérale placée dans un champ gravitationnel subit une force de pesanteur formula_38. La particularité du pondéral est que contrairement à la charge électrique, ce scalaire est toujours positif, et la force d'attraction entre deux « charges gravitationnelles » de même signe est toujours positive.

Lorsqu'un corps subit une force électrostatique, du fait de sa charge électrique, il y répond par une accélération en raison inverse de son inertie ; mais il n'y a aucun rapport nécessaire entre cette inertie et sa charge électrique. Pour quelle raison l'inertie serait-elle alors "toujours" proportionnelle au pondéral, indépendamment de la nature des corps? Puisque la masse inerte n'a aucun lien avec la charge électrique, pour quelle raison en aurait-elle un avec la masse grave ?

L'équivalence entre masse inerte et masse grave est parfois appelé le « principe d'équivalence galiléen », ou encore la version faible du principe d'équivalence. La conséquence la plus directe de ce principe se rencontre en effet dans la loi de la chute libre, dont l'étude par Galilée l'avait conduit à dégager la notion d'inertie, étude à l'occasion de laquelle il avait pu constater que la loi de la chute était indépendante de la masse des corps "et de leur nature". En termes modernes, si une masse pesante formula_39 est placée dans un champ gravitationnel formula_40, elle subit une force formula_41, et y répond par une accélération formula_42, impliquant cette fois ci sa masse inerte formula_2. Si l'expérience montre que la loi du mouvement est la même pour tous les corps, indépendamment de leur nature, c'est donc que l'accélération formula_44 est la même pour tous, et donc :
Cette équation signifie que dire : « le rapport de la masse grave à la masse inerte est une constante », est équivalent à dire que : « ils tombent suivant la même loi dans un champ gravitationnel donné ». Ce constat expérimental est effectivement ce qui permet d'énoncer une loi universelle de la gravitation.

Des expériences bien plus précises ont été réalisées par le baron Loránd Eötvös, en 1889, à partir d'une balance de torsion. L'idée à la base de son expérience est que le champ de pesanteur en un lieu sur terre est (au premier ordre) la somme de deux composantes : une composante gravitationnelle, dépendant de la masse pesante et dirigée vers le centre de la terre, et une composante centrifuge, dépendant de la masse inerte et dirigée perpendiculaire à son axe de rotation. Si donc masse inerte et masse pesante ne sont pas toujours strictement proportionnelle, la direction de la verticale doit être légèrement différente pour deux corps de nature différente. Dans ce cas, une balance de torsion dont le bras est orienté en est-ouest subira un couple tendant à le tourner au contraire dans l'orientation nord-sud, cet effet étant maximum aux latitudes de l'ordre de 45°. Avec cette méthode, Eötvös a pu montrer l'égalité des deux masses à 10 près.

Une autre démonstration de cette égalité se fonde sur la remarque que la loi du mouvement des corps en orbite dépend à la fois du paramètre gravitationnel standard et du rapport entre masse inerte et masse grave du satellite. Le fait que tous les satellites autour de la terre suivent le même mouvement démontre également l'égalité des deux masses ; de même, une inégalité se traduirait par un couple tendant à faire tourner les satellites hétérogènes, qui n'est pas non plus observé.

Même si ces deux grandeurs sont "a priori" conceptuellement distinctes, tous les résultats expérimentaux indiquent donc qu'elles sont toujours directement proportionnelles entre elles, avec un même coefficient de proportionnalité, pour toutes les matières expérimentées. À notre échelle, cette équivalence semble évidente, et l'égalité formula_46 est aujourd'hui démontrée expérimentalement à 10 près.

En outre, dans ce cas, il n'y a pas de raison de considérer que masse inerte et masse grave sont deux grandeurs physiques indépendantes. Puisque le rapport entre elles est constant, c'est en fait la proportionnalité de ces grandeurs qui est vérifiée, indépendamment de la nature du corps. Dès lors, il s'agit d'une grandeur physique unique se manifestant par deux phénomènes différents, et un choix d'unité approprié permet de poser que l'« inertie » est égale au « pondéral », c'est-à-dire que masse inerte et masse grave sont identiques. On se permet dès lors de parler de masse d'un corps : en choisissant la même unité de mesure pour les deux masses, leur universelle proportionnalité (expérimentale) se traduit par leur égalité.

Ce fait d'expérience constitue le principe d'équivalence entre masse inerte et masse grave. Albert Einstein l'admit tel quel, et en donna une interprétation en termes de relativité du mouvement. Ce fut une avancée fondamentale vers la formulation des lois de la relativité générale. Albert Einstein développa la relativité générale en partant du principe que la correspondance entre masse inertielle et masse gravitationnelle (passive) n'était pas accidentelle, et que jamais aucune expérience ne pourrait détecter une quelconque différence entre les deux (c'est la version faible du principe d'équivalence). Cependant, dans le modèle théorique qui en résulte, la gravité n'est pas réellement une force, et ne répond pas au principe d'action et de réaction, si bien que .

Pourtant, certaines théories scientifiques comme la théorie des cordes prédisent qu'elle pourrait cesser d'être vérifiée à des échelles beaucoup plus fines.

Même si masse inerte et masse grave sont expérimentalement égales, il est parfois utile, dans des problèmes d'analyse dimensionnelle, de faire comme si ces deux quantités pouvaient varier indépendamment, et correspondaient donc à des dimensions différentes.

Comme signalé par ailleurs, la constante de gravitation "G" peut être vue comme un facteur d'échelle entre masse inertielle et masse grave ; et on peut chercher comment un problème varie en fonction du rapport entre les forces de gravité (qui se traduisent par des mesures pondérales statiques) et les forces d'inertie. De manière imagée, cette analyse consiste à « faire varier la constante de gravitation » dans le problème. Plus pragmatiquement, elle consiste à distinguer, dans les équations aux dimensions, entre masse inerte et masse pesante.

Sur le plan des dimensions, la constante gravitationnelle est alors en formula_47. D'une manière générale, les unités « pondérales » se traduisent alors par des dimensions en masse pesante, alors que les unités « dynamiques » ou « énergétiques » se traduisent par de la masse inerte. L'analyse permet de proche en proche de distinguer entre ces dimensions pondérales et inertielles :

Dans l'histoire du concept de masse inerte, le chapitre le plus important est celui de la reformulation due à Ernst Mach, qui visait à éliminer de la définition les éléments qu'il qualifiait de « métaphysique » pour ne plus reposer que sur des phénomènes observables. En effet, dans la mécanique newtonienne, la force est définie par le produit de l'accélération et de la masse inerte, mais cette dernière n'est elle-même définie qu'à travers la force. La reformulation claire qu'il en donna est une définition considérée à présent comme « classique ». C'est à partir de cette définition que Albert Einstein a tenté de définir la masse dans sa théorie de la relativité générale, mais à son grand regret, l'approche de Mach ne peut pas se transposer en mécanique relativiste. L'approche de Mach est fondée sur le principe d'action et réaction, en appliquant le principe de proportionnalité entre accélérations pour définir le rapport entre masses sans avoir à passer par les forces en présence.

Considérons un système isolé, constitué de deux corps (ponctuels) indicés « 1 » et « 2 », qui interagissent l'un sur l'autre. Quelle que soit la force agissant entre les deux corps, on peut observer expérimentalement que les accélérations subies par les deux corps sont toujours proportionnelles et dans un rapport constant de l'un à l'autre :
Le point important est que ce rapport de proportionnalité formula_52 est une constante qui ne dépend ni du temps, ni de l'état initial du système. La constante de proportionnalité est donc une propriété physique intrinsèque qui ne dépend que de ces deux corps ; et qui est modifiée lorsqu'on remplace l'un des deux corps par un troisième. On peut également noter que par définition,

Introduisons à présent un troisième corps « 3 », et refaisons les trois expériences correspondantes sur les trois couples possibles de masses ponctuelles (en supposant toujours le système isolé). On peut alors mesurer les trois constantes formula_52, formula_55 et formula_56. On constate expérimentalement que l'on a toujours formula_57 ; ou autrement dit, que les coefficients vérifient une relation de transitivité formula_58. Cependant, dans cette dernière forme, on voit que le coefficient formula_59 est donc le produit de deux termes, dont le premier terme formula_52 ne dépend pas de la nature du corps « 3 », et le second formula_55 ne dépend pas de celle du corps « 1 ». On en déduit donc que chaque coefficient formula_62 s'exprime d'une manière générale comme le produit de deux termes, chaque terme ne dépendant que de la nature d'un des deux corps. Posons alors formula_63. Mais il faut alors que l'on ait de même, pour tout couple de corps "a" et "b", et à chaque instant :

Par conséquent, on peut réécrire la proportionnalité des accélérations sous la forme suivante :
La quantité "m" ainsi définie (à un facteur constant près, qui correspond au choix de l'unité de mesure) est appelée par définition « masse inertielle » de ce corps.

Il est donc possible de comparer la masse inertielle de deux corps, en mesurant les accélérations auxquelles ils sont soumis suite à leurs interactions, sans avoir besoin de passer par les « forces » agissant sur ces deux corps (du moins, à condition de pouvoir supposer que le système est isolé, c'est-à-dire qu'il n'est pas soumis à des forces extérieures). Le rapport entre les deux masses est alors donné par le rapport des accélérations :

Que ce soit la « masse inerte » ou la « masse pesante », la masse est une grandeur physique qui apparaît toujours positive dans l'expérience courante. Cet état de fait empirique n'exclut pas que l'on puisse rencontrer un jour une masse négative, et n'interdit donc pas d'en explorer les propriétés sur le plan de la physique théorique, à défaut de pouvoir faire de la physique expérimentale. Des scientifiques se sont penchés sur la question car rien n'impose "a priori" que toute masse devrait être positive. En envisageant le concept de masse négative, il est important de considérer lequel des concepts de masse est négatif.

En physique théorique, une masse pesante négative est un concept hypothétique postulant l'existence de masse de « charge » négative, tout comme il existe des charges électriques positives et négatives. Comme rappelé ci-dessus, il n'y a pas de raison impérative, sur le plan théorique, pour que la masse inerte et la masse pesante soient systématiquement égale ; et la masse pesante peut être vue comme une « charge grave » gouvernant le mouvement de la matière dans un champ gravitationnel, de même qu'une « charge électrique » gouverne le mouvement de la matière dans un champ électrique. Dans ce cadre, et à partir du moment où l'on admet qu'une « charge grave » puisse être négative tout en maintenant une « inertie » positive, il ne peut rien y avoir d'intrinsèquement contradictoire dans les équations du mouvement.

Cependant, l'expérience montre que les « charges graves » de même signe s'attirent, la loi universelle de la gravitation imposant alors symétriquement que des « charges graves » de signe contraire se repoussent. Contrairement au cas électrique, une matière composite formée de « charges graves » positives et négatives ne peut donc pas maintenir sa cohérence, et les charges de différent signe tendent à la fois à se regrouper entre elles, et à se placer le plus loin possible de corps de charge contraire. Une « charge grave » de signe opposé ne peut donc que fuir un centre de masse homogène comme la Terre, le Soleil, la Galaxie... Dans le champ gravitationnel terrestre, par exemple, une « charge grave » de signe opposé sera soumise à une accélération donnée par :

De son côté, une masse inerte négative aurait en revanche des propriétés gravitationnelle et inertielle différentes, mais possiblement symétriques, de la masse « normale ». Toutefois, on constate rapidement qu'une telle matière ne respecterait pas une, voire plusieurs conditions sur l'énergie et posséderait certaines propriétés ambiguës comme une accélération dont l'orientation est opposée à la force à laquelle elle est soumise, ou encore une courbure inversée de l'espace-temps.

Cependant, l'hypothèse d'une masse inertielle négative implique des formes contre-intuitives de mouvement. La principale caractéristique de la masse inertielle est en effet de permettre à la matière de stocker de l'énergie cinétique à travers une augmentation de la vitesse. Une masse inertielle négative signifierait inversement qu'il faut fournir de l'énergie au système pour le ralentir, ou symétriquement, que le système fournit de l'énergie à son environnement en accélérant. Par exemple, un objet avec une masse inertielle négative accélérerait dans la direction opposée à celle vers laquelle il est poussé ou freiné. Une telle particule de masse inertielle négative serait par conséquent un projectile précieux : il fournit de l'énergie lorsqu'on lui donne une impulsion au départ, accélère sous l'effet des frottements de l'air, et en heurtant un obstacle tendrait donc à accélérer au travers de celui-ci, d'autant plus violemment que la résistance serait importante : un tel projectile serait donc irrésistible.

Une masse intégralement négative respectant le principe d'équivalence et de conservation de la quantité de mouvement serait alors le négatif d'une masse normale, tant sur le plan de la masse inerte que sur celui de la masse grave. D'après Hermann Bondi, l'idée de masse négative n'entraîne aucune contradiction logique tant que les trois formes de masse sont négatives.

En assumant que les trois formes de masse décrites plus haut soient équivalentes, les interactions gravitationnelles entre elles peuvent être explorées. En se basant sur l'Équation d'Einstein, soit la relativité générale :

L'idée initiale de tachyon dérive directement de l'équation donnant la masse relativiste : si une particule dépasse la vitesse de la lumière, son terme en formula_69 devient un nombre imaginaire pur. Et, « par conséquent », une particule se déplaçant plus rapidement que la vitesse de la lumière « doit » avoir une masse imaginaire pure, le quotient des deux permettant de retrouver les lois usuelles sur l'énergie. Une telle approche littérale, cependant, n'a jamais été prise au sérieux, que ce soit en mécanique relativiste ou en mécanique quantique.

En mécanique quantique, on définit effectivement un champ tachyonique (ou plus simplement, un tachyon) comme un champ quantique associé à une masse imaginaire. De fait, et bien que des tachyons (en tant que particules se déplaçant plus rapidement que la vitesse de la lumière) ne sont que des particules purement hypothétiques, et qui n'ont probablement pas d'existence réelle, l'idée d'un champ associé à une masse imaginaire est cependant un concept important pour la physique moderne des particules, et ce concept est abordé dans des livres de vulgarisation de physique quantique.

Dans ce cadre théorique, cependant, une excitation ne se propage jamais plus rapidement que la vitesse de la lumière. Qu'il puisse ou non exister des masses tachyoniques n'a aucune conséquence possible sur la vitesse de propagation de l'information, et il ne peut y avoir aucune violation du principe de causalité. Même si le champ quantique comporte dans ce cas un terme qui peut être interprété comme une « masse imaginaire », aucune description de particule n'hérite d'une telle masse. Ce qui se traduit apparemment par une « masse imaginaire » montre par ailleurs que le système devient instable, et que cette instabilité conduit à une transition de phase, conduisant à un condensat de tachyons, très proche d'une transition de second ordre qui conduit à une symétrie brisée dans le modèle standard.





</doc>
<doc id="14158" url="https://fr.wikipedia.org/wiki?curid=14158" title="Newton (unité)">
Newton (unité)

Le newton (symbole : N) est l'unité de mesure de la force nommée ainsi en l'honneur d'Isaac Newton pour ses travaux en mécanique classique.

Il équivaut à un kilogramme-mètre par seconde carrée ()

Un newton est la force capable de communiquer à une masse de une accélération de . Il faut donc pour augmenter la vitesse d'une masse de de à chaque seconde. Cette unité dérivée du Système international s'exprime en unités de base ainsi :

formula_1

Le Système international impose d'écrire le nom de l'unité (newton) en minuscule et le symbole (N) avec une majuscule.

L'usage du newton comme unité de force a été rendu obligatoire à partir de 1948, lors de la quatrième séance de la neuvième Conférence générale des poids et mesures. La 9e CGPM établit d'autre-part le joule, comme le travail produit par un newton, dont le point d'application se déplace de un mètre dans la direction de la force (Le watt, la puissance qui produit 1 joule par seconde). Le pascal, la pression qui s'exerçant uniformément sur une surface de 1 mètre carré produit une force de 1 newton; et d'autres unité comme la décapoise, unité de viscosité dynamique ou la myriastokes, unité de viscosité cinématique.

Le poids est une mesure de la force entre deux objets due à la gravité, le poids s'exprime en newtons.

Par abus de langage, le poids est pourtant souvent exprimé en kilogrammes (unité de masse). Sur Terre, une masse de génère une force (poids) de (valeur qui varie légèrement en fonction de la pesanteur à l'endroit où l'on se trouve). La pesanteur « normale » (définie en 1901 lors de la Conférence générale des poids et mesures) a été fixée à .

Dans les unités de mesure anglo-saxonnes, la livre-force est utilisée.

On utilise généralement un dynamomètre ou une jauge de déformation pour évaluer les forces.




</doc>
<doc id="14161" url="https://fr.wikipedia.org/wiki?curid=14161" title="Inclinaison orbitale">
Inclinaison orbitale

En mécanique céleste et en mécanique spatiale, l'inclinaison d'orbite (en anglais : ') ou inclinaison est un élément orbital d'un corps en orbite autour d'un autre. Il décrit l'angle dièdre entre le plan de l'orbite et le plan principal du système de référence (généralement le plan de l'écliptique, c'est-à-dire le plan moyen de l'orbite de la Terre, ou le plan équatorial). L'inclinaison est couramment notée formula_1 (lettre i minuscule de l'alphabet latin).

Lorsque l'inclinaison est non nulle, l'orbite est dite inclinée. Le plan de l'orbite et le plan de référence sont alors deux plans sécants. La droite d'intersection des deux plans est la ligne des nœuds. L'orbite la coupe en deux points d'intersection dits nœuds. Le nœud ascendant est celui que le corps en orbite franchit en trajectoire ascendante ; le nœud descendant, celui qu'il franchit en trajectoire descendante.

L'inclinaison se compte de 0 à 90° dans le cas d'une orbite directe et de 90° à 180° dans celui d'une orbite rétrograde.

Dans le Système solaire, l'inclinaison de l'orbite d'un corps céleste en orbite autour du Soleil (planètes, astéroïdes, etc.) est définie comme l'angle entre son plan orbital et celui de l'écliptique. Elle pourrait être mesurée par rapport à un autre plan, tel que le plan équatorial du Soleil ou le plan orbital de Jupiter, mais le plan de l'écliptique est le plus pratique pour des observateurs terrestres. La plupart des orbites des corps du système solaire possèdent une faible inclinaison, que ce soit par rapport à l'écliptique ou au plan équatorial solaire. Parmi les exceptions notables, on trouve les planètes naines Pluton (17°) et Éris (44°) et l'astéroïde (2) Pallas (34°).

Pour un satellite naturel ou artificiel, l'inclinaison est mesurée relativement au plan équatorial autour duquel l'objet orbite, s'il en est assez proche (le plan équatorial étant le plan perpendiculaire à l'axe de rotation du corps central). Dans ce cas :

Pour les objets situés loin du corps central, il est possible d'utiliser le plan de Laplace. Celui-ci est confondu avec le plan équatorial près du corps central et s'incline progressivement avec la distance pour finir par se confondre avec le plan orbital.

Si la rotation du corps central n'est pas connue avec précision, l'inclinaison d'un satellite sera donnée par rapport à l'écliptique ou parfois par rapport au plan de la sphère céleste (ou, de façon équivalente, comme l'angle entre l'axe de l'orbite et la direction de l'observateur).

Cette dernière mesure est utilisée pour les objets externes au système solaire, comme les étoiles doubles. Elle dépend donc de la direction dans laquelle regarde l'observateur, c'est-à-dire de la région de la sphère céleste dans laquelle est situé l'objet. Les étoiles doubles possédant une inclinaison proche de 90° sont souvent des binaire à éclipses.

Dans le cas de la Lune, mesurer l'inclinaison par rapport au plan équatorial de la Terre conduit à une valeur variant avec une période de 18 ans entre 18,29° et 28,58°. Il est plus utile de la mesurer par rapport à l'écliptique, ce qui donne une valeur relativement constante (5,145°).

En astrodynamique, l'inclinaison formula_1 peut être calculée de la manière suivante :

où :




</doc>
<doc id="14162" url="https://fr.wikipedia.org/wiki?curid=14162" title="Pollution">
Pollution

La pollution est la dégradation d'un écosystème par l'introduction, généralement humaine, de substances ou de radiations altérant de manière plus ou moins importante le fonctionnement de cet écosystème. Par extension, le mot désigne aussi parfois les conséquences de phénomènes géologiques comme une éruption volcanique.

La pollution a des effets importants sur la santé et la biosphère, comme en témoigne l'exposition aux polluants et le réchauffement climatique qui transforme le climat de la Terre et son écosystème, en entraînant l'apparition de maladies inconnues jusqu'alors dans certaines zones géographiques, des migrations de certaines espèces, voire leur extinction si elles ne peuvent s'adapter à leur nouvel environnement biophysique.

La Seconde Guerre mondiale est suivie d'une prise de conscience des répercussions des activités humaines sur l'environnement et la santé, parallèlement à l'approfondissement de l'écologisme et de l'écologie théorisée dès 1886 par Ernest Haeckel. Les préoccupations de santé environnementale conduisent les gouvernements à prendre des mesures pour limiter l'empreinte écologique des populations humaines et pour contrer des activités humaines contaminantes.
En 2012 selon l'OMS, plus de de personnes sont mortes prématurément à cause de la pollution de l'air (extérieur et domestique) ; l'Asie et le Pacifique étant les régions les plus touchées. 

En 2017, le journal "The Lancet" a estimé qu'au moins de personnes sont prématurément mortes en 2015 à cause de la pollution (soit une mort « "prématurée" », c'est-à-dire avant , sur six).

Pollution vient du latin "polluere" (= por + luo) qui signifie « souiller en mouillant », « salir » et surtout « profaner ».

Historiquement, la pollution est la profanation ou la souillure d'un objet ou d'une demeure sacrée par des substances impures. C'est clairement un mot d'origine cultuelle.

Au sens large, la pollution peut être anthropique (c'est-à-dire induite par l'Homme) ou d'origine non-humaine (rejet de méthane des ruminants).

Le "Dictionnaire de l'environnement. Les termes normalisés" de l'AFNOR définit le polluant comme un altéragène biologique, physique ou chimique, qui au-delà d'un certain seuil, et parfois dans certaines conditions (potentialisation), développe des impacts négatifs sur tout ou partie d'un écosystème ou de l'environnement en général.

Il est question de « pollution diffuse », lorsque les sources d'un polluant sont multiples (pots d'échappement, épandage de pesticides...) et de « pollution chronique » lors d'émissions répétées ou constantes de polluant, et parfois lorsqu'un polluant est très rémanent.

La notion de pollution appelle donc celle de d'un ou plusieurs composants des écosystèmes (air, eau, sol), d'un organisme (qui peut être l'être humain) ou d'un groupe d'organismes, ou ayant une incidence sur l'écosystème, au-delà d'un "seuil" ou "norme". La contamination peut notamment s'étendre ou se modifier via le réseau trophique (chaîne alimentaire) (bioconcentration, bioturbation).

La science qui étudie les pollutions est la molysmologie.

Une certaine pollution de l'air a toujours accompagné les progrès de la civilisation.

La pollution commence dès la préhistoire, avec la maîtrise du feu : « la suie trouvée sur le plafond des grottes préhistoriques est une preuve évidente de ce que les foyers entraînaient un niveau élevé de pollution du fait d'une ventilation insuffisante ».

La métallurgie de l'âge du bronze puis de l'âge du fer a marqué un tournant dans la pollution de l'environnement extérieur. Les carottages des glaciers du Groenland ont révélé un accroissement de la pollution associée à la métallurgie des grecs, des romains et des chinois. Mais à cette époque, la pollution était comparativement faible, et n'avait pas d'impact environnemental significatif.

Les concentrations urbaines ont constitué la source majeure de pollution tout au long de notre histoire. Les villes concentraient la présence et les déjections de nombreux hommes et de chevaux, conduisant à des pollutions de l'air et de l'eau. La nécessité de les évacuer (dans l'eau courante du fleuve) a conduit aux premiers systèmes d'égouts comme le Cloaca Maxima. C'est à cause de la puanteur qu'elles dégagent que les tanneries ont de tout temps été excentrées et placées en aval des villes. La combustion massive de bois et de charbon conduit également à des pollutions de l'air. Ainsi, en Angleterre, Édouard édicta en 1272 une proclamation interdisant l'usage de la houille bitumineuse à Londres, alors d'usage très courant, après que la fumée que produisait son usage massif soit devenu insupportable.

Le développement des métropoles aggrava le problème. Londres connu ainsi l'un des pires cas de pollution de l'eau avec la Grande Puanteur de 1858, qui entraîna la construction d'égouts à grande échelle et une nouvelle politique appelée « révolution sanitaire », et le mouvement hygiéniste. Berlin était dans une situation similaire en 1870, comme en témoigne August Bebel :

C'est la révolution industrielle qui a conduit la pollution aux niveaux connus de nos jours. La combustion massive de charbon amena la pollution de l'air à des niveaux sans précédents, les industries déchargèrent leurs effluents chimiques et leurs déchets sans traitements particuliers, polluant les cours d'eau, les nappes phréatiques et les sources d'eau potable.

En Amérique, Chicago et Cincinnati furent les deux premières villes à passer des réglementations pour lutter contre la pollution de l'air. Vers le milieu du , le smog provoqué par les échappements automobiles était devenu un problème majeur dans des villes comme Los Angeles, ou Donora Londres connu son pire épisode de pollution atmosphérique avec le Grand Smog de 1952, dont on estime qu'il a pu faire .

D'autres catastrophes environnementales dues à de la pollution chimique massive conduisirent à une sensibilisation croissante de l'opinion : scandale de Love Canal, intoxications massives au mercure de Minamata au Japon, etc.

C'est à la suite de tels événements que la préoccupation environnementaliste se développa, et que des lois et conventions internationales furent développées pour lutter contre la pollution.

Les pollutions d'origine humaine, dites aussi anthropiques, ont de nombreuses formes en pouvant être locales, culturelles, ponctuelles, accidentelles, diffuses, chroniques, génétiques, volontaires, involontaires, etc.

Cette pollution est une diffusion directe ou indirecte dans l'environnement de polluants. Ce sont souvent des "sous-produits involontaires" d'une activité humaine, comme les émissions des pots d'échappement ou des installations de combustion. Les déchets de produits de consommation courante (emballages, batteries usagées) jetés sans précautions dans l'environnement biophysique et dans l'environnement humain, constituent également une source de pollution très fréquente. Il peut aussi s'agir de phénomènes physiques (comme la chaleur, la lumière, la radioactivité, l'électromagnétisme, etc.).

Le caractère "impur" ou "malsain" est généralement relatif car dépendant de la dose, de la durée d'exposition, d'éventuelles synergies, etc. Il est relatif : 

Des pollutions d'origine environnementale peuvent être dues : 

Parmi tous les polluants existants, il faut annoter que certains d'entre eux sont beaucoup plus nocifs que les autres, soit :
Parmi ces substances nocives, on y retrouve généralement des composés tels que les POP (polluants organiques persistants), les PCB (polychlorobiphényls) et les métaux lourds.

La pollution de l'air, provoquée par des polluants dits "atmosphériques" est souvent diffuse et donc plus délicate à réglementer efficacement dans un cadre local ou national que beaucoup d'autres formes de pollutions (de même pour les pollutions marines). Des conventions mondiales visent les polluants destructeurs de la couche d'ozone ou les gaz à effet de serre (tous capables de modifier le fonctionnement planétaire du monde vivant). Elle intègre la "pollution biologique" induite par des taux anormaux ou anormalement allergènes de microbes, virus, pollens ou de spores fongiques. Les effets allergènes (rhinite, conjonctivite, asthme) de ces particules biologiques sont en augmentation, et ils semblent souvent exacerbés par les polluants urbains, routiers et de l'industrie.

Une mauvaise qualité de l'air peut tuer de nombreux d'organismes polluo-sensibles et causer des morts prématurées, via notamment des complications respiratoires, des maladies cardiovasculaires. Elle cause aussi une inflammation de la trachée, des douleurs abdominales et une congestion. Les enfants, les personnes âgées et les personnes ayant des problèmes pulmonaires ou cardiovasculaires y sont beaucoup plus vulnérables. Ainsi les enfants exposés aux pollutions automobiles développeraient plus facilement asthme, infections ORL, allergies respiratoires et cancers, les enfants en poussette étant particulièrement exposés à ce type de pollution.

Des études estiment à le nombre de victimes de la pollution de l'air aux États-Unis. En Europe, la pollution de l'air est à l'origine de plus de par an.

En mars 2014, une grande partie de la France () est en état d'alerte maximale, et Paris est plongé dans un épaisse brume de pollution, au point que la Tour Eiffel n'est presque plus visible. En 2017 l’AEE (Agence européenne pour l’environnement) concluait que mourraient chaque année prématurément (avant ) à cause de la pollution de l’air et une autre étude, de la revue médicale "The Lancet" a porté cette estimation à de morts pour la planète en 2015, ce bilan étant selon les auteurs sous-estimé en raison du fait que beaucoup de produits potentiellement toxiques mis sur le marché n'ont jamais subi de tests de toxicité/écotoxicité et d’évaluation en termes de santé environnementale.

La pollution de l'eau peut avoir diverses origines parmi lesquelles :

La pollution des eaux cause par jour, pollution principalement la conséquence de mauvais traitements des eaux usées dans les pays en voie de développement. Il est estimé que d'indiens n'ont aucun accès à l'hygiène et qu'un millier d'enfants meurt chaque jour de diarrhée infectieuse. Près de de Chinois n'ont aucun accès à de l'eau potable.

En 2009, l'Association Santé Environnement France et le WWF ont mené une étude sur l'imprégnation aux PCB des riverains du Rhône. Les conclusions du rapport ont mis en évidence un lien entre la consommation de poissons ainsi que le lieu de vie et le niveau d’imprégnation aux PCB.

La pollution du sol peut être diffuse ou locale, d'origine industrielle, agricole (utilisation excessive d'engrais, de pesticides qui s'infiltrent dans les sols). Ces pollutions agricoles peuvent avoir plusieurs impacts sur la santé humaine en contaminant par bioaccumulation ou diffusion par ruissellement.

Le mercure est lié à des déficits développementaux chez les enfants et à des symptômes neurologiques.

La pollution sonore cause une perte d'audition, de l'hypertension, du stress et des troubles du sommeil. 

Selon les estimations de l'Organisation mondiale de la santé, de personnes sont décédées en 2012 du fait d’avoir vécu ou travaillé dans un environnement insalubre, soit près d’un quart des décès dans le monde. Les facteurs de risque environnementaux, tels que la pollution de l’air ( de décès), de l’eau et des sols, l’exposition aux substances chimiques, le changement climatique ou le rayonnement ultraviolet, contribuent à la survenue de plus de 100 maladies ou traumatismes. Les accidents vasculaires cérébraux ( de décès par an), les cardiopathies (), les cancers () et les affections respiratoires chroniques () représentent aujourd’hui près des deux tiers des décès liés à des causes environnementales. On constate une baisse du nombre de décès entraînés par des maladies infectieuses, telles que les maladies diarrhéiques et le paludisme, souvent liées au manque d’eau, au défaut d’assainissement et à la mauvaise gestion des déchets. Cette baisse s’explique principalement par une amélioration de l’accès à l’eau potable et aux moyens d’assainissement. Ces décès sont surtout concentrés dans les régions de l'Asie du Sud-Est (), du Pacifique occidental () et de l'Afrique ().

Un rapport publié en octobre 2017 dans la revue The Lancet évalue le bilan des maladies dues à la pollution à de morts prématurées, soit 16 % de l'ensemble des décès survenus dans le monde en 2015, soit plus que les décès dus aux conflits qui ont sévi sur la planète cette année-là. La pollution de l'air est responsable de de décès (maladies cardiaques, AVC, cancers du poumon et broncho-pneumopathies chroniques) ; la pollution de l'eau causerait pour sa part la mort de de personnes par maladies gastro-intestinales et infections parasitaires, et la pollution sur le lieu de travail abrègerait la vie d'environ , du fait de leur exposition à des substances toxiques ou cancérigènes, chiffre probablement en-dessous de la réalité, selon le rapport. À elles seules, l'Inde et la Chine représentent près de la moitié du total mondial des morts par pollution, avec respectivement et de décès.

Globalement, plus de de morts étaient attribuables en 2012 aux effets des pollutions de l'air extérieure et domestique, et les régions de l'Asie et du Pacifique sont les plus touchées. Au moins meurent prématurément chaque année en Chine à cause de la pollution de l'air. En Inde, elle causerait par an. 

Il est estimé que d'indiens n'ont aucun accès à l'hygiène et qu'un millier d'enfants meurt chaque jour de diarrhée infectieuse.

Les hydrocarbures aromatiques polycycliques (produits de la combustion des hydrocarbures) seraient responsables d'un ralentissement de l'activité cérébrale (réduction de la substance blanche dans le cerveau des enfants).

En 2017, une équipe de chercheurs chinois et taïwanais met en évidence un lien entre l'exposition aux particules fines présentes dans l'air et la qualité des spermatozoïdes humains. L'étude est selon les chercheurs qui l'on menée peu fiable, car comportant de nombreux biais environnementaux.

Plusieurs conventions internationales portent sur les pollutions marines, animées par les commissions OSPAR et HELCOM notamment.

La Commission européenne a présenté le un projet de directive visant à condamner de manière uniforme au sein de l'Union européenne les crimes environnementaux. Actuellement (février 2007), la définition varie fortement d'un État membre à l'autre, avec des sanctions jugées souvent "insuffisantes" par la Commission. Franco Frattini, le Commissaire chargé de la Justice, à la liberté et à la sécurité a déclaré que 73 % des sont causés par les entreprises, il fallait donc les pénaliser plus fortement. C'est ainsi que des amendes allant de euros à 1,5 million d'euros peuvent être infligées, ainsi que pour les personnes, des peines de prison allant de 5 à 10 ans.

Les crimes pris en compte par ce projet sont notamment : émissions illicites de substances dangereuses, transport illicite de déchets et commerce illicite d'espèces menacées.

D'un point de vue législatif, dans la plupart des pays, le mot « "pollution" » qualifie la contamination d'un milieu par un agent polluant au-delà d'une norme, seuil, loi, ou hypothèse ; il peut s'agir de la présence d'un élément, de chaleur ou rayonnement dans un milieu ou dans un contexte où il est normalement absent à l'état naturel. Généralement, néanmoins, ce n'est pas simplement la présence mais plutôt la surabondance de l'élément dans un milieu où il est naturellement en équilibre (par exemple un métal lourd fixé dans les complexes argilohumiques et peu biodisponible) ou présent en plus faible quantité qui crée la pollution. 

Selon l'article 1 de la Convention internationale OSPAR : 

La législation européenne définit la pollution comme et un polluant comme . Ces définitions abordent le problème de l'eau et évitent celui des sols qui sera traité par le biais de la directive sol.

De ce point de vue, en l'absence d'impact sur la santé ou sur le fonctionnement des écosystèmes marins, il n'y a pas de pollution au sens légal du terme (mais l'environnement peut être plus ou moins « marqué », de manière détectable, par des substances dont on sait par ailleurs qu'elles sont potentiellement polluantes à forte dose).

En France, dans le domaine juridique, pour les produits soumis à des normes ou seuils, on ne devrait donc théoriquement parler de pollution que dans le cas de dépassement des seuils ou normes, ces seuils étant eux-mêmes fixés en fonction de l'impact biologique que les substances considérées peuvent avoir. Ceux-ci sont listés dans un rapport de l'Institut national de l'environnement industriel et des risques (INERIS) qui rapporte des valeurs dans un même milieu avec des unités identiques, ce qui n'est pas toujours le cas dans les textes réglementaires. Les valeurs, en vigueur au , y sont données pour information. Il convient donc après cette date de vérifier qu'elles n'ont pas été modifiées ou abrogées, et de systématiquement se référer aux textes originaux.

Inversement, en France, en l'absence de loi ou de normes spécifiques aux pollutions anciennes liées aux séquelles de guerre, des territoires que l'on sait très fortement contaminés (les forêts de la Zone rouge de Verdun par exemple) . préfectorales ou ministérielles. Ceci vaut pour les champignons qui peuvent fortement accumuler les métaux lourds, mais aussi pour les sangliers.

Depuis très longtemps, la justice ou les autorités cherchent en cas de pollution grave ou chronique à identifier les causes et les responsables.

Au , les méthodes d'Investigation environnementale, parallèlement à l'évaluation environnementale se sont développées (en France, souvent sous l'égide des DRIREs (devenues DREALs) et des Agences de l'Eau depuis que ces entités existent.

Alors que le droit de l'environnement se développe, et sur le modèle anglophone du mot , on parle maintenant de « forensie environnementale » pour décrire les enquêtes et méthodes mobilisées par les experts appelés à chercher des preuves et des faits scientifiques utilisables devant un tribunal.

Selon le type de pollution, il existe différentes associations qui agissent au quotidien : soit par des études scientifiques, soit par des mesures quotidiennes, soit par des actions locales, ou soit par de la prévention.

On peut citer le projet CERPA, de l'AASQA qui mesure la qualité de l'air, et qui publie régulièrement des études scientifiques sur le sujet. Ou encore, l'Association française de protection des plantes qui délivre des conseils recommandant l'utilisation de certains herbicides face à ceux composés de glyphosate.

Il arrive que des associations dont le but premier n'est pas l'environnement ou la pollution effectue ce genre de taches, comme l'Association Française des Capitaines de Navires, qui effectue des mesures de la pollution liée aux Marée Noire, et aux déchets d'hydrocarbures lors de transports maritimes.

Au niveau des actions locales, des nettoyages citoyens sont régulièrement organisés par des associations plus ou moins grandes, tels que Surfrider Foundation Europe et Let's do it! World.

L'Association santé environnement France (ASEF) donne des conseils pour lutter contre la pollution interieure et protéger sa santé avec son petit guide vert du bio-air intérieur.

Des "atlas" ou "cadastres des pollutions" se mettent peu à peu en place aux échelles communales à mondiales pour certains polluants, concernant les émissions et/ou les pollutions de stock. 

L'Europe dispose ainsi d'un registre européen des émissions polluantes (Eper) couvrant cinquante polluants (eau et air uniquement), émis par les principales (grandes et moyennes) installations industrielles. Il a permis de conclure mi 2007 à un . Si on observe une diminution de deux tiers des cinquante polluants industriels suivis, notamment azotés dans l'eau (-14,5 % dans l'eau), phosphore (-12 % dans l'eau) et dioxines et furanes (-22,5 % dans l'air) ; ces améliorations sont contrebalancées par une hausse des émissions de certains polluants dont le que la commission espérait réduire grâce à l'introduction du système communautaire d'échange de quotas d'émission.

L'Eper sera en 2009 remplacé par un "registre européen des rejets et des transferts de polluants" (PRTR européen) construit à partir des données de 2007, cette fois pour plus de 91 substances d'industries dans 65 domaines d'activité. Et les émissions diffuses du trafic autoroutier, chauffage domestique et l'agriculture y seront ajoutées.

En France, il existe un régime de déclaration annuelle obligatoire de certaines émissions polluantes et des déchets (par exemple pour les installations classées pour la protection de l'environnement et les exploitants de station d'épuration d'eaux urbaines.

Au niveau local, des Samu de l'environnement se créent en France, dont l'objectif principal est de fournir des laboratoires mobiles capables de mesurer rapidement et sur site pollué plusieurs centaines de paramètres physico-chimiques et biologiques.

L'étude de l'impact d'un polluant relève du domaine de l'écotoxicologie. Il est cependant difficile de mesurer l'impact de polluants multiples agissant en synergies, comme cela est le cas par exemple pour le syndrome d'effondrement des colonies d'abeilles. L'application de l'écotaxe ou du principe pollueur-payeur a nécessité que l'on crée des indices de pollution. L'une des unités retenues en France est le métox, mais uniquement pour huit polluants de type métaux et métalloïdes (arsenic, cadmium, chrome, cuivre, mercure, nickel, plomb et zinc).

La Croix verte internationale, en collaboration avec le Blacksmith Institute, a rendu un rapport en 2013, concernant les 10 sites les plus pollués au monde, se trouvant dans 8 pays. Ces lieux pollués menacent gravement la santé de centaines de milliers de personnes par inhalation directe, ingestion d'aliments ou contact cutané. Parmi ces sites, figurent :


Selon "L'Atlas de la France toxique" (2016), les villes françaises les plus polluées sont Marseille, Paris et Lyon. "Marseille est la ville la plus polluée en ce qui concerne les particules fines", "Lyon prend la tête du classement des sites sensibles et contaminés, avec représentant un danger sérieux pour la population", "Paris est la ville la plus radioactive avec de stockage des déchets nucléaires. Lyon et Marseille se partagent la deuxième place de ce classement, avec de stockage des déchets nucléaires".

Les villes les moins polluées se situent généralement dans l'ouest de la France : Vannes, Limoges, Brest.

Les zones rurales ne sont pas épargnées du fait notamment de leur utilisation intensive de pesticides.

La pollution de l'air a deux origines, l'une interne, l'autre externe :

Les émissions de particules fines en France proviennent du chauffage domestique (34 %), de l'industrie (31 %), à de l'agriculture (21 %), et des transports (14 %). Une partie des particules provient aussi de rejet de zones industriels et de centrales à charbon de l'étranger.

En 2000, les particules fines provoquaient prématurés chaque année dans la population de plus de .

En 2016, "les décès provoqués par cette pollution liée aux activités humaines (transports ; industrie ; chauffage avec des énergies fossiles comme le fuel ; agriculture) correspondent à 9 % de la mortalité en France continentale (hors Corse et outre-mer, soit près de d'habitants), d'après une étude de Santé publique en France.

« Le fardeau (le poids sanitaire) de la pollution de l'air ( par an) se situe au troisième rang, derrière celui du tabac ( par an) et de l'alcool () », souligne le professeur François Bourdillon, directeur général de cet organisme public, selon lequel il s'agit d'une 

Cette pollution représente , souligne l'étude. La perte d'espérance de vie est, en moyenne, plus élevée dans les grandes villes ( et plus), mais elle n'épargne pas les zones rurales (neuf mois).

Le docteur Gilles Dixsaut met en garde contre la pratique du jogging en milieu urbain, en raison de l'hyperventilation pendant l'effort : , estime le médecin, membre du comité stratégique de la Fondation du Souffle contre les maladies respiratoires. .

Cependant, une étude de 2016 permet de déterminer le seuil en deçà duquel les bienfaits de l'exercice physique restent supérieurs aux méfaits de la pollution de l'air. Ainsi, à Paris, il faudrait pédaler pendant plus de pour dépasser ce seuil.

Depuis le , les bâtiments accueillant du public doivent contrôler régulièrement les moyens de ventilation et la qualité de l'air intérieur. De nombreuses avancées ont vu le jour notamment dans les habitations grâce à la ventilation mécanique par insufflation qui permettent de réduire considérablement la pollution de l'air intérieur.

La Base de données du ministère de l'Écologie et du Développement durable sur les sites pollués permet d'établir une carte des sols pollués en France.


La loi sur l'eau du vise une gestion globale de la ressource en eau et des milieux aquatiques. Elle s'appuie sur des principes de partage de cette ressource entre les usagers et de protection des écosystèmes. Elle soumet à un régime de déclaration et d'autorisation (selon le même principe que la réglementation sur les I.C.P.E) certaines installations, ouvrages et travaux entraînants un prélèvement sur les eaux superficielles ou souterraines, une modification du niveau ou du mode d'écoulement des eaux ou un rejet. La Mission Inter-Service de l'Eau (MISE), regroupement départementale des services de l'État (DDASS, DDAF, DDE, DRIRE, DIREN…) est chargée d'assurer la "police de l'eau".

Afin de permettre une gestion équilibrée de l'eau, la France a été découpée en six bassins versants hydrogéographiques principaux. Sur chacun de ces bassins les modalités de cette gestion sont définies dans un Schéma directeur d'aménagement et de gestion des eaux (SDAGE). Ce document se développe en trois points : un état des lieux des milieux aquatiques, et des ressources ; les objectifs de gestion, de qualité et de quantités à atteindre ; et les mesures à prendre pour satisfaire ces objectifs.

Afin de permettre une gestion plus proche des exigences locales, un outil à l'échelle de plus petites unités hydrogéographiques (sous-bassins) a été mis en place : le Schéma d'aménagement et de gestion des eaux (SAGE).

Dans de nombreux pays, une réglementation sur certaines "installations classées" vise les installations susceptibles de présenter un danger pour l'environnement, le voisinage ou la personne. Ces installations appelées en France ICPE (installations classées pour la protection de l'environnement), répertoriées dans une nomenclature, sont tenues avant leur mise en activité ou avant un changement ou une diversification de leur activité de présenter au préfet un dossier répertoriant toutes les nuisances qu'elles sont susceptibles de provoquer et les moyens qu'elles comptent mettre en œuvre pour les prévenir et les réparer le cas échéant. Ces activités répertoriées soit simplement déclarées (dépôt du dossier avec récépissé attestant que le dossier est complet et conforme à la législation, soit soumises à autorisation (pour les installations présentant les risques les plus importants).

Des taxes et redevances sont dues pour certaines pollutions, en vertu du principe du pollueur-payeur, qui fait assumer la charge financière de la prévention, de la réduction et de la lutte contre la pollution au pollueur.

Dans cette optique, les équipements et produits polluants sont plus taxés (par des écotaxes) que des produits dits "écologiques".

Une redevance pour pollutions diffuses est par exemple due par les distributeurs de pesticides et de semences pré-enrobées. Son assiette est basée sur une liste de substances (actualisée annuellement en fonction des évolutions de la connaissance ou de la réglementation).

Les nouvelles listes sont mises en consultation publique par le ministère de l'Environnement, avec le projet d'arrêté d'actualisation. Par exemple en 2016, de nouveaux pesticides (Métobromuron, l'Ethoprophos et leFenpyrazamine) entrent dans la liste alors que le Phosphure d'hydrogène en sort et que d'autres évoluent dans le classement (ex : le Fluopyram classé CMR passe dans la liste des substances classées en raison de leur danger pour l'environnement alors que L'Imazalil (enilconazole) et le Valifenalate subissent le chemin inverse, selon le projet qui devrait entrer en vigueur le .

Des incitations financières, comme des réductions d'impôts encouragent le développement des énergies renouvelables. Et lors d'une catastrophe écologique (comme une marée noire), le pollueur est censé assumer le nettoyage des zones contaminées.

En Chine, la qualité de l’air ne respecte pas les normes de l’Organisation mondiale de la santé dans 495 des 500 plus grandes villes du pays ; un cinquième des terres cultivables sont polluées, selon un chiffre officiel longtemps caché par l’État, et très probablement sous-évalué ; la qualité de l’eau est aussi très mauvaise : près d’un tiers des rivières sondées par le ministère de l’Environnement contient des eaux considérées comme dangereuses pour le simple contact avec la peau. Une nouvelle loi de protection de l’environnement a été créée en 2015, avec des amendes quotidiennes, et nettement plus dissuasives qu’auparavant, pour les pollueurs, ainsi que des inspections pour vérifier les émissions de polluants des usines ; 180 sociétés, souvent de grands groupes d’État, se sont vu intimer l’ordre de publier quotidiennement leurs niveaux d’émission de polluants.

Le patriarche Bartholomée Ier de Constantinople s’est exprimé à plusieurs reprises pour inviter les êtres humains à reconnaître les péchés contre la création : « Que les hommes dégradent l’intégrité de la terre en provoquant le changement climatique, en dépouillant la terre de ses forêts naturelles ou en détruisant ses zones humides ; que les hommes portent préjudice à leurs semblables par des maladies en contaminant les eaux, le sol, l’air et l’environnement par des substances polluantes, tout cela, ce sont des péchés ».

La pollution de l'environnement a été évoquée comme une forme moderne du péché par , régent de la Pénitencerie apostolique, le 9 mars 2008. Ces nouvelles formes modernes de péché qu'il a citées ne sont néanmoins pas de nouveaux péchés capitaux, a notamment insisté sur la définition collective du péché, alors que l'accent est traditionnellement mis sur la dimension individuelle : « Alors que le péché concernait jusqu’à présent plutôt l’individu, aujourd’hui, il a une résonance sociale, en raison de la mondialisation ».

Pour , évêque de Cigoma en Tanzanie, le « péché contre la terre », est un péché social ou structurel.




</doc>
<doc id="14163" url="https://fr.wikipedia.org/wiki?curid=14163" title="Aldo van Eyck">
Aldo van Eyck

Aldo van Eyck (Driebergen, - Loenen aan de Vecht, ) est un architecte néerlandais.

Il passe sa jeunesse en Angleterre, à Golders Green au nord de Londres. Puis il part étudier l’architecture à l’École polytechnique fédérale de Zurich. 

À partir de 1947, il voyage en compagnie de son épouse à travers le monde, à la recherche des cultures archaïques. De retour aux Pays-Bas, il travaille au département des travaux publics de la ville d'Amsterdam, sous la direction de van Eesteren, figure emblématique du modernisme néerlandais. Il ouvre sa propre agence à Amsterdam en 1951, en association avec Jan Rietveld, fils de Gerrit Rietveld.

En 1946, il est membre de « De 8 en Opbouw ». Puis à partir de 1947 il est délégué pour son pays aux Congrès internationaux d'architecture moderne jusqu’à la fin de ceux-ci en 1959, à Otterlo.

Il fréquente de nombreux intellectuels et artistes d’avant-garde comme les membres du groupe "CoBrA" dont il fait partie de 1948 à 1951. Puis en 1953 il se joint à Jacob Bakema, Georges Candilis et les Smithson (Margareth Alison et Peter Denham) pour fonder le groupe "Team X". Il dirige également la revue d’architecture "Forum" de 1959 à 1967.

Il enseigne dans de nombreuses universités aux États-Unis, aux Pays-Bas (université de technologie de Delft) et en Suisse (École polytechnique fédérale de Zurich).

Ses œuvres les plus célèbres sont l’orphelinat municipal (1955-1960) et la maison Hubertus (1973-1978), tous deux situés à Amsterdam.





</doc>
<doc id="14166" url="https://fr.wikipedia.org/wiki?curid=14166" title="Hollande">
Hollande

La Hollande est une région et une ancienne province des Pays-Bas. En 1840, elle est divisée en deux entités distinctes : la Hollande-Septentrionale et la Hollande-Méridionale.

Le nom de cette région historique provient du moyen néerlandais "Holtland" désignant une « terre boisée » ( de "holt" : « bois », et de "land" : « terre » ).

Du fait de l'importance historique de celle-ci, le terme "Hollande" est aussi utilisé, par synecdoque, pour désigner l'ensemble des Pays-Bas, bien que cette appellation ne soit pas officielle. En 1806, la République batave est transformée en un royaume de Hollande confié à Louis Bonaparte ; c'est la seule occasion où le terme « Hollande » est utilisé officiellement pour désigner l'ensemble des Pays-Bas.
Ses habitants sont les Hollandais et les Hollandaises.

La Hollande est successivement :





</doc>
<doc id="14167" url="https://fr.wikipedia.org/wiki?curid=14167" title="Limite (mathématiques)">
Limite (mathématiques)

En mathématiques, la limite d'une suite ou d'une fonction en un point est, le cas échéant, la valeur particulière dont elle « s'approche » lorsque la variable ou l'indice « s'approche » du point en question. Cette valeur et ce point peuvent être un réel ou infini. Dans cette définition très intuitive, la notion de « s'approcher » reste à définir avec précision.

formula_1 ou formula_2, ou encore formula_3. La notion de proximité est liée à une distance qui dans ℝ est définie par la valeur absolue d'une différence, mais cette notion peut se généraliser à tout espace métrique. Plus tard, la notion s'est étendue aux espaces topologiques et « être proche » signifie alors « être dans un voisinage arbitrairement choisi ».

Ensuite est intervenue la notion de limite de fonction, initialement rattachée à la limite de suite. Pour chercher la limite d'une fonction quand la variable s'approche de "a", on cherchait à déterminer la limite de la suite ("f"("u")) pour toute suite ("u") dont la limite était "a". Si cette limite existe, on écrit que formula_4. La complexité de cette approche et la multiplicité des cas ont conduit à définir la notion de limite de fonction indépendamment de celle de limite de suite. Pour pouvoir manipuler la notion de limite et l'exploiter sans erreur, il a été nécessaire de la définir de manière plus précise et plus formelle. C'est ainsi que cet article présente une définition formelle de la limite d'une suite convergente, de la limite d'une fonction à valeurs dans ℝ, la notion de limite infinie, et enfin le cas des espaces métriques et des espaces topologiques.

Voir aussi, pour une présentation plus abordable, l'article « Limite (mathématiques élémentaires) » dans la série Mathématiques élémentaires.

Supposons que ("x" ; "x" ; …) soit une suite de nombres réels.
On dit que cette suite est "convergente" si :

il existe un réel "L" tel que pour tout réel ε > 0 il existe un entier naturel "n" tel que pour tout entier "n" > "n" on ait |"x – L"| < ε, ce qui s'écrit :
ou bien encore, ce qui revient au même :

Intuitivement, cela signifie que tous les termes de la suite deviennent aussi proches que l'on veut d'un réel "L", dès que "n" est assez grand ; la valeur absolue |"x" – "L"| peut être interprétée comme la distance entre "x" et "L".

On démontre que, pour une suite convergente, le réel "L" de la définition est unique.

Ce réel "L" est appelé la "limite" de cette suite et l'on écrit :

Lorsque la suite est identifiée par un nom global, comme dans la notation formula_8, on note souvent simplement formula_9 et l'on dit que la suite "u" converge vers "L".

Toutes les suites ne sont pas convergentes. Dans le cas où une suite n'est pas "convergente", elle est dite "non convergente" ou "divergente". Certains préfèrent réserver le mot "divergent" aux suites "non convergentes" "non bornées".


Supposons que "f" : "U" → R soit une application définie sur un sous-ensemble "U" de l'ensemble R des réels. Si "p" est un réel, n'appartenant pas nécessairement à "U" mais tel que "f" soit « définie au voisinage de "p" », on dit que "f" admet une limite (finie) au point "p", s'il existe un réel "L" vérifiant

On démontre que le réel "L" de la définition, lorsqu'il existe, est unique et on l'appelle limite de "f" au point "p". On le note :

On peut démontrer que ceci est équivalent à
Remarquons qu'une fonction peut admettre une limite en "p" sans être définie en "p" mais :

On dit dans ce cas que "f" est continue en "p".

Définissons maintenant la limite épointée (ou limite par valeurs différentes) :

On dit que "f" admet une limite épointée (finie) au point "p" si sa restriction à "U"\{"p"} admet une limite (finie), c'est-à-dire s'il existe un réel "L" vérifiant

De même ce nombre "L" est alors unique et on note :

Occasionnellement, il peut être utile de n'approcher le point "p" que d'un seul côté.

On dit que "f" admet une limite à droite (finie) au point "p", s'il existe un réel "L" vérifiant

Ce nombre "L" est alors unique et on le note :

Les limites à gauche s'obtiennent en remplaçant "x – p" dans la dernière définition par "p – x".

Il est possible aussi de considérer des limites où "p" ou "L" sont égaux à plus l'infini () ou moins l'infini ().

On dit que "f"("x") tend vers quand "x" tend vers "p" (ou que "f" a pour limite en "p") si

On dit que "f"("x") tend vers "L" quand "x" tend vers (ou que "f" a pour limite "L" en ) si

Enfin, on dit que "f"("x") tend vers quand "x" tend vers (ou que "f" a pour limite en ) si
Les définitions pour moins l'infini sont analogues.

En remplaçant ε par "S" comme précédemment, on peut aussi définir les limites infinies d'un seul côté (à droite ou à gauche).


Si "p" est un point de "U", alors les conditions suivantes sont équivalentes :

Si "p" n'appartient pas "U", alors les conditions suivantes sont équivalentes :

Ces propriétés sont aussi valables pour les limites à droite et à gauche, pour le cas "p" = , et aussi pour les limites infinies en utilisant les règles suivantes :
(Voir l'article « Droite réelle achevée ».)

Remarquons qu'il n'y a pas de règle générale pour le cas "q" / 0 : cela dépend de la façon dont on s'approche de 0. Certains cas, comme 0/0, 0×, ou , ne sont pas non plus couverts par ces règles.

Il existe certaines formes de limite où il est n'est pas possible de conclure directement en utilisant des opérations sur les limites, ce sont les formes dites « indéterminées » :


La règle de L'Hôpital permet souvent de lever ces indéterminations.

Les nombres réels forment un espace métrique pour la fonction distance définie par la valeur absolue : "d"("x" ; "y") = |"x – y"|. Il en est de même des nombres complexes avec le module. De plus, l'espace euclidien ℝ forme un espace métrique avec la distance euclidienne. Voici quelques exemples motivant une généralisation des définitions de limite données précédemment.

Si ("x") est une suite dans un espace métrique ("M" ; "d"), alors on dit que la suite a pour limite "L" si pour tout réel ε > 0, il existe un entier naturel "n" tel que pour tout entier "n" > "n" on ait "d"("x" ; "L") < ε.

Si l'espace métrique ("M", "d") est complet (ce qui est le cas pour l'ensemble des nombres réels ou complexes et l'espace euclidien, et tout autre espace de Banach), alors toute suite de Cauchy de "M" converge. Ceci permet de montrer que la suite est convergente sans nécessairement connaître la limite.

Si "M" est un espace vectoriel normé réel ou complexe, alors l'opération de passage à la limite est linéaire, comme dans le cas des suites de nombres réels.

Maintenant supposons que "M" et "N" sont deux espaces métriques, "A" une partie de "M", "p" un élément de "M" adhérent à "A", "L" un élément de "N" et "f" une application de "A" dans "N". On dit que la limite de "f"("x") quand "x" tend vers "p" est égale à "L" et l'on écrit :

si :

ce qui est équivalent à la caractérisation séquentielle de la limite d'une fonction sur un espace métrique .

Si l'espace d'arrivée est complet, on peut, de même que dans le cas particulier d'une suite, démontrer l'existence d'une limite pour "f" en "p" sans nécessairement connaître cette limite :

Une application "f" de "M" dans "N" est continue en "p" si et seulement si la limite de "f"("x") quand "x" tend vers "p" existe (elle est alors égale à "f"("p")). De manière équivalente, "f" transforme toute suite de "M" convergeant vers "p" en une suite de "N" convergeant vers "f"("p").

À nouveau, si "N" est un espace vectoriel normé, alors l'opération de passage à la limite est linéaire dans le sens suivant : si la limite de "f"("x") quand "x" tend vers "p" est égale à "L" et la limite de "g"("x") quand "x" tend vers "p" est égale à "P", alors la limite de "f"("x") + "g"("x") quand "x" tend vers "p" est égale à "L" + "P". Si "a" est un scalaire du corps de base, alors la limite de "af"("x") quand "x" tend vers "p" est égale à "aL".

Si "N" est égal à ℝ, alors on peut définir des limites infinies ; si "M" est égal à ℝ, alors on peut définir des limites à droite et à gauche de manière analogue aux définitions précédentes.


Toute sous-suite d'une suite convergente converge vers la même limite.

L'opération de passage à la limite est linéaire dans le sens suivant :
si ("x") et ("y") sont des suites réelles convergentes et que lim "x" = "L" et lim "y" = "P", alors la suite ("x" + "y") est aussi convergente et a pour limite "L" + "P". Si "a" est un nombre réel, alors la suite ("a" "x") est convergente de limite "aL". Ainsi, l'ensemble "c" de toutes les suites réelles convergentes est un espace vectoriel réel et l'opération de passage à la limite est une forme linéaire sur "c" à valeurs réelles.

Si ("x") et ("y") sont des suites réelles convergentes de limites respectives "L" et "P", alors la suite ("x""y") est convergente de limite "LP". Si ni "P" ni aucun des termes "y" n'est nul, alors la suite ("x"/"y") est convergente de limite "L"/"P".

Toute suite convergente est une suite de Cauchy et est ainsi bornée.
Si ("x") est une suite de réels, "bornée" et "croissante" ("i. e." pour tout entier "n", "x" ≤ "x"), alors elle est nécessairement convergente.

Une suite de nombres réels est convergente si et seulement si ses limites inférieure et supérieure sont finies et égales.

Toutes les notions de limite ci-dessus peuvent être unifiées et généralisées encore à des espaces topologiques "M" et "N" arbitraires : si "A" est une partie de "M", "p" un élément de "M" adhérent à "A", "L" un élément de "N" et "f" une application de "A" dans "N", on dit que
(Il suffit pour cela que cette propriété soit vérifiée pour tout "V" d'une base de voisinages de "L", par exemple pour tout "V" ouvert contenant "L".)

Si "N" est séparé (ou même seulement T), alors "f" possède au plus une limite au point "p".

La définition de limite d'une suite est un cas particulier de la définition précédente :

Si "M" est métrisable (ou plus généralement : héréditairement séquentiel), on dispose de la "caractérisation séquentielle des limites de fonctions" :
Si de plus "N" est T (ou même seulement à unique limite séquentielle), formula_17 admet une limite en formula_19 si (et seulement si) pour toute suite formula_26 dans formula_27 de limite formula_19, la suite formula_35 admet une limite.

D'autres généralisations de cette notion, permettant par exemple de parler de limites « à l'infini » pour un espace métrique quelconque, ou de dire qu'une intégrale est une limite de sommes de Riemann, ont été définies ; les plus puissantes utilisent la notion de filtre. On en trouvera des exemples aux divers articles traitant de "convergence" : convergence simple, convergence uniforme, convergence normale, convergence presque sûre, convergence en moyenne, etc.


</doc>
<doc id="14168" url="https://fr.wikipedia.org/wiki?curid=14168" title="Domrémy-la-Pucelle">
Domrémy-la-Pucelle

Domrémy-la-Pucelle — ou Domremy-la-Pucelle — est une commune française située dans le département des Vosges en Lorraine. La commune de l'arrondissement de Neufchâteau fait aujourd'hui partie de la région administrative Grand Est.

Domremy-la-Pucelle est connue pour être la patrie de Jeanne d'Arc.
Le territoire de la commune est limitrophe de cinq communes, dont une, Les Roises, est située dans le département voisin de la Meuse.

La commune est située dans la vallée de la Meuse au nord de Coussey. Le territoire communal englobe vers l'ouest une éminence boisée culminant à (le bois de Domrémy) et qui domine Les Roises, une petite commune meusienne.

Le nom de la localité est attesté au sous la forme latinisée "Domnum Remigium", puis "Dompremy la Pucelle" en 1578.

S'il existait un toponyme gaulois antérieur, il a disparu comme plusieurs milliers d'autres en Gaule sans laisser aucune trace. La formation toponymique actuelle date du Moyen Âge. Il s'agit d’un composé en "Dom-", élément fréquemment rencontré dans la toponymie médiévale et signifiant « saint » au sens de dédicace de la paroisse, suivi du nom de saint Remy auquel est d'ailleurs consacré la paroisse, d'où la forme latinisée "Remigius" qui est précisément le nom de Remi ("Remy" ou "Rémy") en latin. Le terme masculin "dom", issu du latin "dom[i]nus" a disparu de l'usage commun, alors que sa forme féminine "" s'est perpétuée dans le français moderne. Le "a" de "dame", vocable issu du latin "dom[i]na", est lié au phénomène de l'haplologie. On trouve également en toponymie la forme altérée "Dam-" comme dans par exemple.

Domremy est un type toponymique fréquent puisqu'on trouve Domremy-Landéville (Haute-Marne, "Domnus Remigius" siècle) ; Domremy-aux-Bois (Marne, "Domnus Remigius" en 1047) ; Domremy-la-Canne (Meuse, "Domnus Remigius" en 1064) ; ainsi que Dompremy (Marne, "Damremigius" en 1161). En outre "Dom-" se retrouve dans les nombreux , Domjean, Domptail, Domprix, etc., tous formés avec un nom de saint.

La prononciation rémoise parfaitement constante depuis des siècles est « Remi » (et non « Rémi »). Remy (rarement Remi) est par ailleurs un nom de baptême et un patronyme très fréquemment attesté depuis un millénaire. Dans une monographie sur Jeanne d'Arc, Jérôme Estrada écrit : « Il est inexact d’écrire Domrémy avec un accent. L’« e » initial de "Remigius" étant libre — c’est-à-dire suivi d’une seule consonne — s’est affaibli en « e » sourd en français ». En réalité ce "e" intervocalique était devenu muet en français, d'où la prononciation ancienne « r'nard » pour "renard", « s'cret » pour "secret", « p'tit » pour "petit". L'articulation du "e" [ø] est une réaction moderne, on devait dire « r'mi ». C'est pourquoi il faut écrire Remy comme "renard" ou "secret" sans accent aigu. Alain Litaize, de l’université de Nancy, pense que « la règle qui prévaut veut que l’on retienne la prononciation locale ». En la matière, les Domremois, à commencer par leur maire, Daniel Coince, et l’ancien recteur de la basilique Jean Mengin prononcent « dom-re-mi ».

Le sénateur Albert Voilquin fit passer au pilon tous les timbres de la maison de Jeanne d’Arc édités en 1970 parce qu’il y avait un accent aigu sur le "e".

Le lieu était habité à l'époque celte comme le montrent certaines murailles et "tumuli" antiques.

Au , du vivant de Jeanne d'Arc, la paroisse était divisée en deux parties : l'une dépendait du comté de Champagne, française, l'autre du Barrois mouvant. La jeune Jeanne d'Arc aimait se rendre en la chapelle de Bermont, près de Greux, pour prier, comme à l'église de Domrémy où elle avait reçu le baptême. "Ses voix", qui l'initièrent à sa mission et l'accompagnèrent dans son action – les saintes Catherine d'Alexandrie, Marguerite d'Antioche et saint Michel Archange – étaient pour elle des figures familières du voisinage, voire familiales, ce qui contribua à ouvrir la psychologie de la jeune adolescente à la vocation hors norme qui fut la sienne.

Domrémy – ou du moins la partie dans laquelle se trouvait la maison de Jeanne d'Arc, à savoir la partie nord du village – fut exempté d'impôts par Charles VII après son couronnement lors de l'anoblissement de Jeanne d'Arc. En 1571, le village de Domrémy fut officiellement rattaché à la Lorraine et perdit le privilège (le duché de Lorraine relevait du Saint-Empire romain germanique). Il fut rattaché au royaume de France près de deux siècles plus tard sous Louis XV. En revanche, le village de Greux demeura territoire français et conserva le privilège jusqu'en 1766. La paroisse de Domrémy devint en 1578 Domrémy-la-Pucelle. Elle passa au statut de commune à la Révolution française.

La commune a connu trois manifestations de masse en l'honneur de Jeanne d'Arc entre 1937 et 1939, organisées par le député Marcel Boucher et les Compagnons de Jeanne d'Arc.

En 2014, le budget de la commune était constitué ainsi :

Avec les taux de fiscalité suivants :







</doc>
<doc id="14172" url="https://fr.wikipedia.org/wiki?curid=14172" title="Récursivité">
Récursivité

La récursivité est une démarche qui fait référence à l'objet même de la démarche à un moment du processus. En d'autres termes, c'est la propriété de pouvoir appliquer une même règle plusieurs fois en elle-même. Ainsi, les cas suivants constituent des cas concrets de récursivité :

En informatique et en logique, une fonction ou plus généralement un algorithme qui contient un ou des appel(s) à lui-même est dit récursif. Ce procédé est employé dans la conception d'algorithme basée sur le paradigme diviser pour régner. Deux fonctions peuvent s'appeler l'une l'autre, on parle alors de récursivité croisée. La définition de certaines structures de données, comme les listes, les arbres est récursive. Par exemple (voir la figure) un arbre binaire est soit fait deux arbres binaires sur lequel on enracine un nœud, soit est un arbre binaire vide. 
La récursivité est un point délicat dans l'enseignement de l'informatique, car son appropriation par l'apprenant demande une dose d'abstraction.

La grammaire du sanskrit de Pānini utilise déjà la récursivité au tandis que les constructions des langues sont essentiellement récursives, par exemple, la construction des groupes nominaux : "la clé de la serrure de la porte d'entrée de la maison de la rue du bout du village". Des travaux menés par le professeur Daniel Everett tendraient cependant à montrer l'absence de récursivité dans la langue Pirahã. 

Certains auteurs ont considéré que la capacité à construire des structures récursives est propre aux systèmes de communication humaine, mais cette affirmation est aujourd'hui remise en cause par des travaux de cognition animale.

Un dictionnaire (dictionnaire de définition) est un exemple de récursivité : chaque mot du dictionnaire est défini par d'autres mots eux-mêmes définis par d'autres mots dans ce même dictionnaire. 

Dans le domaine des arts, le procédé récursif est appelé mise en abîme et c'est l'artiste Maurits Cornelis Escher qui en fait le plus grand usage ; il est connu pour ses œuvres inspirées par la récursivité. De son côté, la publicité a aussi utilisé la récursivité, rendant célèbres en France La vache qui rit et Dubonnet.

La récursivité est particulièrement présente en biologie, notamment dans les motifs de végétaux et les processus de développement. Les diatomées présentent en particulier de belles structures récursives.

Une fonction peut être définie en matière d'elle-même. Un exemple familier est la suite de Fibonacci vue comme une fonction "F" : ℕ → ℕ à savoir "F"("n") = "F"("n" - 1) + "F"("n" - 2). Pour qu'une telle définition ait un sens, elle doit conduire à des valeurs immédiatement évaluables, dans le cas de la suite de Fibonacci : "F"(0) = 0 et "F"(1) = 1.

Le fait de définir un concept à partir de lui-même a été appelé par les logiciens et les mathématiciens, l'imprédicativité et cela ne doit pas être confondu avec la récursivité, bien que cela s'y apparente. On parle aussi d'auto-référence. Il existe des théories logiques imprédicatives (comme le système F dû à Jean-Yves Girard), mais elles doivent être définies avec précautions si l'on veut préserver leur cohérence, car les paradoxes ne sont pas loin. Ainsi en théorie des ensembles, le paradoxe de Russell montre qu'il ne peut pas y avoir d'ensemble constitué des ensembles qui ne se contiennent pas eux-mêmes (popularisé comme le paradoxe du barbier, en effet « si le barbier est celui qui rase ceux qui ne se rasent pas eux-mêmes, qui rase le barbier ? »). Toujours en théorie des ensembles, l'axiome de fondation proscrit les ensembles qui se contiennent eux-mêmes. 

C'est pour jouer sur ces principes que des informaticiens facétieux ont défini des acronymes récursifs qui ne définissent rien puisqu'ils sont imprédicatifs et incohérents. De même, l'aphorisme suivant : « Pour comprendre le principe de récursivité, il faut d'abord comprendre le principe de récursivité », est imprédicatif et peut être considéré comme une pétition de principe.

Edgar Morin a très souvent utilisé le concept de récursivité, qu'il appelle "boucle récursive", notamment dans ses ouvrages constituant "la Méthode".
La boucle récursive est à causalité circulaire : la conséquence agit sur la cause de l'effet. La plasticité cérébrale, composée de la plasticité neuronale et de la plasticité synaptique, est un exemple de boucle récursive. Par exemple : le cerveau a la capacité de piloter l'enchaînement des différents muscles de commande lors du premier apprentissage d'un mouvement complexe (swing du golf). La répétition du geste modifie les réseaux neuronaux et synaptiques qui deviennent ainsi aptes à de nouvelles capacités : l'apprentissage des gestes pour les effets donnés à la balle.

Dans le 6ème opus de "La Méthode" Edgar Morin propose la récursion éthique. Il déclare que . 


</doc>
<doc id="14173" url="https://fr.wikipedia.org/wiki?curid=14173" title="GNU Debugger">
GNU Debugger

, également appelé "gdb", est le débogueur standard du projet GNU. Il est portable sur de nombreux systèmes type Unix et fonctionne pour plusieurs langages de programmation, comme le C, le C++ et le Fortran. Il fut écrit par Richard Stallman en 1988. gdb est un logiciel libre, distribué sous la licence GNU GPL.

GDB a été écrit en premier par Richard Stallman en 1986 en parallèle de son système GNU, après que GNU Emacs soit "stable" de manière raisonnable. GDB est un logiciel libre sorti sous Licence publique générale GNU (GPL). Il a été modelé d'après le débogueur DBX, qui était avec la distribution Unix de Berkeley.

De 1990 à 1993 il a été maintenu par John Gilmore. Maintenant, il est maintenu par le comité de direction GDB qui a été créé par la Free Software Foundation.

Gdb fonctionne sur de nombreuses architectures de processeur différentes, et permet le débogage distant (par l'intermédiaire d’une liaison série ou d’une connexion IP) d’application tournant sur une plateforme cible distincte de la plateforme de développement. Ceci éventuellement sur deux types de processeurs différents.

Gdb permet de déboguer un programme en cours d’exécution (en le déroulant instruction par instruction ou en examinant et modifiant ses données), mais il permet également un débogage "post-mortem" en analysant un fichier "" qui représente le contenu d’un programme terminé anormalement.

L’interface de gdb est une simple ligne de commande, mais il existe des applications frontales qui lui offrent une interface graphique beaucoup plus conviviale. L’utilitaire ddd par exemple permet de cliquer sur une ligne de code directement dans le listing pour y placer un point d’arrêt alors que gdb seul nécessite la saisie du numéro de ligne. Notons également que gdb est souvent invoqué en arrière-plan par les environnements de développement intégré comme Eclipse.




</doc>
<doc id="14176" url="https://fr.wikipedia.org/wiki?curid=14176" title="Lieux touristiques au Japon">
Lieux touristiques au Japon

Ci-après une liste non exhaustive des principaux sites intéressants à voir ou à visiter au Japon par région :


Le cakidju est la région de plaine où se trouve la capitale Tōkyō.





</doc>
<doc id="14183" url="https://fr.wikipedia.org/wiki?curid=14183" title="Préfecture de Nagano">
Préfecture de Nagano

La est une préfecture du Japon située sur l'île de Honshū.

Avant la mise en place du système des préfectures en 1871, la préfecture de Nagano était occupée par la province de Shinano.

En 1998, la ville de Nagano a accueilli les Jeux olympiques d'hiver.

Caractérisée par un relief élevé, cette préfecture n'a pas de débouché maritime.
Elle est entourée des préfectures de Niigata, Gunma, Saitama, Yamanashi, Shizuoka, Aichi, Gifu et Toyama.

Les villes principales sont Nagano, Matsumoto et Ueda.

Liste des 19 villes de la préfecture :

Liste des 14 districts de la préfecture, ainsi que de leurs 22 bourgs et 37 villages (en italique):

Le poney Kiso est originaire de la région de Kiso de la préfecture.

La préfecture de Nagano est jumelée avec les municipalités ou régions suivantes :



</doc>
<doc id="14191" url="https://fr.wikipedia.org/wiki?curid=14191" title="FastTrack">
FastTrack

FastTrack est un réseau "peer-to-peer" utilisé entre autres par les clients KaZaA, iMesh et Grokster.

Contrairement au logiciel de partage Napster qui est basé sur une architecture centralisée, les logiciels clients du réseau FastTrack reposent sur une architecture décentralisée : les fichiers sont téléchargeables directement depuis un dossier spécifique chez chaque utilisateur du logiciel. Ce dernier type d'architecture rend presque impossible le contrôle des contenus diffusés (il n 'y a pas d 'administrateur).

Ce type de réseau n'ayant pas de serveur central, l'indexation des informations disponibles par tel ou tel utilisateur n'en est que plus difficile. Pour cela, on utilise des superpeers, éléments choisis du réseaux permettant - entre autres fonctions - cette indexation. Ces superpeers ont une forte puissance de calcul et une large bande passante et assurent la recherche d'informations.


</doc>
<doc id="14192" url="https://fr.wikipedia.org/wiki?curid=14192" title="Audiogalaxy">
Audiogalaxy

Audiogalaxy était un système de partage de fichier centralisé sur Internet très populaire. Le site met aujourd'hui à disposition de la musique en ligne via la plateforme Rhapsody.

Créé par Michael Merhej et David McArthur, le système initial reposait sur l'installation d'un logiciel minimaliste qui indexait les fichiers MP3 présents sur les ordinateurs : le "satellite". Les fichiers présents sur les disques durs des utilisateurs étaient alors listés sur le site web d'Audiogalaxy. Un moteur de recherche permettait de trouver les fichiers qui pouvaient alors être téléchargés, toujours via le "satellite". Accessible en permanence, avec un système de reprise des téléchargements interrompus, le site a très vite attiré les utilisateurs quittant la plateforme Napster (2001). Cependant, la structure centralisée d'AudioGalaxy rendait le système encore plus vulnérable que Napster aux poursuites judiciaires.

La mission d'AudioGalaxy était de faciliter le transfert de musique. Le système était réputé pour sa communauté, notamment grâce aux fonctions de chat dans les groupes et de forums par artiste. On pouvait non seulement recevoir, mais aussi envoyer un fichier à un autre utilisateur.

Même si le système était conçu pour le partage de musique, les fichiers de n'importe quel type pouvaient également être partagés, simplement en les renommant. Pour partager foobar.zip, il suffisait de le renommer foobar zip .mp3.

Audiogalaxy a très tôt mis en place une version payante en plus de la version gratuite : les abonnés payants avaient accès à :


Face aux menaces de la RIAA qui estimait que le site proposait des fichiers en téléchargement illégalement, Audiogalaxy a commencé à mettre en place un premier filtrage. Cependant, ces premiers filtres se sont révélés être peu efficaces dans leur sélection : bien qu'AudioGalaxy prétende coopérer avec l'industrie musicale pour bloquer les chansons sous "copyright", le système continue à permettre l'échange de MP3 illégaux.

En mai 2001, AudioGalaxy implémente des « groupes » qui permettent à leurs membres de s'échanger des chansons. Certains bidouilleurs utilisèrent cette « "backdoor" » pour contourner les règles filtrant les restrictions sur certains titres de chansons (bloqués par AudioGalaxy pour des raisons de droit d'auteur).

Le 9 mai 2002, AudioGalaxy impose la présence des fichiers dans le dossier partagé de l'utilisateur pour pouvoir être envoyés. Il était auparavant possible d'envoyer n'importe quelle chanson en éditant les paramètres du CGI. La protection est vite contournée en créant un dossier partagé fantôme et en lançant une chanson avec le même nom. Le système de hachage permettait tout de même de lancer le fichier correct malgré le fichier fantôme.

Devant l'inefficacité du filtrage, la RIAA décide de poursuivre Audiogalaxy en justice le 24 mai 2002. Ce jour, le site bloque la possibilité d'envoyer des fichiers identifiés comme protégés par le droit d'auteur. 

Le 17 juin 2002, AudioGalaxy obtient un accord à l'amiable avec la RIAA. L'accord autorise AudioGalaxy à utiliser un système de filtre qui nécessite que pour chaque fichier mis à disposition, l'auteur, l'éditeur et la société d'enregistrement doivent donner leur accord. Le partage de la quasi-totalité des fichiers est bloqué.

Après la fermeture de son service d'échange de fichiers, une partie des inscrits sont partis pour Mediaseek ou GLT Poliane, deux clones d'Audiogalaxy, mais ceux-ci n'ont jamais atteint la popularité de l'original.

Le 8 septembre 2002, AudioGalaxy reprit à listen.com un système de streaming payant nommé Rhapsody et cessa son service de P2P basé sur une interface web.

Bien que les fichiers ne soient plus partagés, certains forums sont toujours actifs (exemples : Radiohead, Rush et General Discussion).
Il revient en 2012.
Ce nouveau service permet à des utilisateurs de streamer leurs musiques personnelles présentes sur un ordinateur distant par une interface web ou bien par différentes applications Android ou iOS.
Ce service fut racheté en 2012 par DropBox qui a pris la décision de fermer ce site. L'inscription était impossible depuis le 31 décembre 2012 puis l'utilisation impossible à partir du 31 janvier 2013



</doc>
<doc id="14194" url="https://fr.wikipedia.org/wiki?curid=14194" title="Kazaa">
Kazaa

Kazaa est un client pair à pair développé par Ahti Heinla, Priit Kasesalu et Jaan Tallinn. Les trois compagnons sont aussi à l'origine de la création de Skype. Kazaa se connecte sur le réseau FastTrack, caractérisé par son architecture décentralisée. Il y a quelques années, FastTrack était considéré comme le réseau P2P le plus utilisé au monde (environ 3 millions de connectés en moyenne).

Kazaa est souvent critiqué pour différentes raisons :

Sharman Networks, société éditrice de Kazaa, a obtenu gain de cause contre les accusations des grandes maisons de disques et de la RIAA, pour violation de copyright : cette victoire est due à cette décentralisation du réseau qu'utilise Kazaa : les fichiers qui circulent de poste-à-poste ne sont pas contenus dans un serveur principal (contrairement au célèbre client Napster, qui perdit justement son procès pour cette raison). Le contrôle des informations étant "presque" impossible, Niklas Zennström (fondateur de Kazaa, et également développeur de Skype et du The Venice Project) n'a donc aucune responsabilité face au type de contenu (libre de droit ou sous licence de droits d'auteur) circulant grâce à sa création.

Il existe une version de Kazaa sans logiciel espion dénommée Kazaa Lite. Cette version n'étant pas produite par Sharman Networks, ces derniers attaquèrent les auteurs de Kazaa Lite en justice et obtinrent gain de cause.

Malgré cela, Kazaa prit dès 2001 la suite de Napster comme référence incontournable dans le grand public ou la presse, quand on souhaite décrire un moyen gratuit d'échanger des fichiers musicaux ou vidéo. Cependant, les attaques judiciaires contre le téléchargement illégal sur Kazaa poussèrent de plus en plus les utilisateurs à le délaisser pour d'autres logiciels, tel eMule.

Il ferme en à la suite d'une condamnation en justice.

En 2007, la base d'utilisateurs de Kazaa était quasiment nulle.

En , le projet revient sous forme d'un plateau de distribution légal par l'intermédiaire d'un abonnement mensuel permettant de télécharger des morceaux de façon illimité. Le catalogue comprend plus d'un million de titres issus des répertoires des majors tel que EMI, Universal Music et Warner Music.

Depuis août 2012, le site est fermé et n'offre plus de musique.


</doc>
<doc id="14203" url="https://fr.wikipedia.org/wiki?curid=14203" title="L'Appel de Cthulhu (jeu de rôle)">
L'Appel de Cthulhu (jeu de rôle)

L'Appel de Cthulhu ("") est un jeu de rôle créé aux États-Unis en 1981 par Sandy Petersen. Le titre vient de la nouvelle du même nom écrite par l'écrivain fantastique américain H. P. Lovecraft.

Le jeu est édité par Chaosium et était publié en France par Jeux Descartes jusqu'en 2005. La licence française du jeu a été reprise en 2008 par les Éditions Sans-Détour.

Ce jeu emprunte à l’univers de H. P. Lovecraft son ambiance mystérieuse et oppressante, ainsi que sa cosmogonie hétéroclite et originale — le Mythe de Cthulhu — qui décrit un ensemble de puissances maléfiques, incarnations de forces cosmiques et primitives : les Grands Anciens. Les joueurs sont confrontés à des situations où ils doivent déjouer les complots de ces créatures et de leurs adorateurs.

Les personnages évoluent dans les années 1890, 1920 ou bien dans un monde contemporain alternatif, mais le principe est en fait utilisable dans quasiment n'importe quelle époque ou lieu.
Le système de jeu utilisé — "Basic Role-Playing" — est rapide et simple d'utilisation. Il utilise presque tous les dés polyédriques dont notablement trois dés à six faces pour définir la plupart des caractéristiques physiques et mentales des personnages et un dé à cent faces (réalisé en combinant deux dés à dix faces) pour effectuer les jets de compétence. Une version "D20 System", basée sur un système développé par les créateurs de "Donjons et Dragons", est également disponible.

La gamme de suppléments disponible dans le commerce, que ce soit en version originale ou dans sa traduction française, est très riche et comprend de nombreux scénarios et campagnes ainsi que des suppléments de règles et de background. Il s'agit également du jeu de rôle ayant bénéficié du plus grand nombre d'éditions.

À l’origine, Sandy Petersen contacta Chaosium en proposant un projet de supplément pour "RuneQuest" consacré aux "Contrées du Rêve" décrites par H. P. Lovecraft. L'équipe de Chaosium travaillait à l'époque sur la conception d’un jeu nommé "Dark Worlds" et, convaincue par l’enthousiasme de Sandy Pertersen, lui confia le projet.

En mars 2015, l'éditeur français Sans Détour a lancé une campagne de financement participatif pour la traduction de la édition qui a remporté .

Malgré ses sept éditions, les règles initiales du jeu (le "Basic Role-Playing", dérivées d’une version simplifiée du système de "RuneQuest") ont peu évolué au cours du temps.

À l’époque de sa sortie (1981 aux États-Unis), le monde du jeu de rôle est encore dominé par les mécanismes de jeu introduits par « l'ancêtre » "Donjons et Dragons" : 

"L’Appel de Cthulhu" prend le contre-pied de ces mécanismes :

"L'Appel de Cthulhu" a été le premier jeu dans lequel les personnages pouvaient devenir fous (ce qui est peut-être encore plus frustrant que de les voir mourir).

L'originalité du système de jeu repose notamment sur les règles de gestion de la santé mentale qui permet de faire une corrélation entre le surnaturel de l’univers de H. P. Lovecraft et les conséquences psychologiques des événements auxquels vont être confrontés les joueurs. La découverte de vérités terrifiantes se traduit par un jet de dés qui déterminera combien de points de santé mentale perdra le personnage. Le maximum de santé mentale autorisé décroit donc inexorablement.

Cette Santé Mentale a permis d'instaurer un élément qui disparaissait des jeux de rôles classiques : la gestion de la peur. La règle de la Santé Mentale permet d’injecter de la prudence dans le comportement du joueur. Certains d'entre eux préfèreront même s'enfuir, ou se cacher la vue, plutôt que d'affronter la réalité. La sixième édition met cependant en place l"'aplomb", qui permet de diminuer les pertes de Santé Mentale reçues pendant la partie. Mais l'aplomb est également édité pour ne pas durer éternellement et ainsi rendre les joueurs imprudents (un point d'aplomb peut-être utilisé pour augmenter les degrés de réussite).


Le jeu est édité en France, en Espagne, en Pologne, en Italie, au Japon, en Allemagne et bien entendu aux États-Unis.

Depuis 1981, le jeu a connu sept éditions et plus de 90 suppléments. exemplaires des règles du jeu ont été vendus toutes éditions confondues.

Voici, à titre d'indication, une liste des parutions à dater de mars 2005 (x indique nombre de scénarios disponibles dans la parution). Des listes exhaustives des suppléments publiés sont disponibles sur les sites référencés dans les liens externes.
La première édition du jeu est très rapidement suivie par la publication de plusieurs campagnes de longue haleine. Les plus notables :
Autres campagnes notables publiées par la suite :

Cette série de suppléments édité par Chaosium dans les années 1990 et supervisée par Keith Herber décrivent une série de villes de Nouvelle-Angleterre imaginées par Lovecraft.

Une série de suppléments consacrée aux métropoles et aux pays les plus notables dans le cadre du jeu :

Bien que des règles de contexte soient établies pour l'époque moderne, la grande majorité du matériel publié pour "l'Appel de Cthulhu" se déroule pendant les années 1920. Précédée de quelques suppléments et campagnes ayant pour contexte les années 1990 (en particulier "Cthulhu 90") cette période n'est exploitée réellement qu'avec la publication de "Delta Green" : cette série de suppléments éditée par a fortement contribué au renouvellement du jeu au milieu des années 1990. Ils sont maintenant tous traduits par les Éditions Sans-Détour :






</doc>
<doc id="14206" url="https://fr.wikipedia.org/wiki?curid=14206" title="Stefan Wul">
Stefan Wul

Stefan Wul, de son vrai nom Pierre Pairault, né le à Paris et mort le , est un écrivain de science-fiction français, également connu sous un autre nom de plume : Lionel Hudson. Principalement connu pour ses romans, il a également écrit des nouvelles et des recueils de poèmes.

Pierre Pairault naît en 1922 dans le quatrième arrondissement de Paris et suit des études classiques au Collège Rocroy Saint-Léon. Il obtient son baccalauréat de philosophie en 1940. Dans son enfance, Pierre Pairault écrivait déjà des histoires dont il vendait les chapitres à ses camarades de classe auxquels il les lisait à la récréation contre la modique somme d'un sou. Mais au moment de choisir sa filière d'étude, son père lui déconseilla de faire des études de littérature. À la fin de la Seconde Guerre mondiale, il obtint son diplôme de chirurgien dentiste.
Il se marie en 1951. Il quitte bientôt Paris pour s'installer dans la campagne normande à partir de 1952.

Parallèlement à son métier de chirurgien-dentiste, Pierre Pairault écrit sous le pseudonyme de Stefan Wul à partir de 1956. Le pseudonyme "Stefan Wul" fait référence au nom d'un ingénieur atomiste de l'Oural découvert dans une revue spécialisée. Après quelques essais infructueux dans le domaine du roman policier, Pierre Pairault choisit la science-fiction un peu par hasard, simplement parce que sa femme s'était plainte d'un roman de S.F. qu'elle venait de lire. Convaincu de pouvoir mieux faire, Pierre Pairault se lance dans l'aventure de l'anticipation et publiera onze romans entre 1956 et 1959, tous parus dans la célèbre collection « Anticipation » des éditions Fleuve noir, et ce malgré la mauvaise réputation littéraire de la science-fiction à cette époque. Stefan Wul participa également à la création des dessins de couverture de ses romans en envoyant à l'illustrateur René Brantonne plusieurs croquis de travail. Du point de vue de la méthode, Stefan Wul a toujours déclaré travailler sans plan ni ligne directrice, partant d'une simple idée de départ développée peu à peu au fil de l'écriture. Nombre de ses chutes surprenantes ne lui sont venues qu'en cours de rédaction, sans préméditation. Parlant de science-fiction, Stefan Wul avait une approche artistique proche de la peinture, affirmant ne s'intéresser qu'aux univers sensibles, faits d'odeurs, de couleurs, de formes, de paysages et d'animaux merveilleux :

Après une longue période de silence de dix-huit ans, Stefan Wul livre son dernier roman en 1977 avec "Noô", créant ainsi une dernière fois l'événement.

Ces romans sont aujourd'hui considérés comme des classiques dans le monde de la science-fiction française et furent maintes fois réédités. Son plus grand succès français reste le roman post-apocalyptique "Niourk" qui retrace l'histoire d'un enfant noir, rejeté par sa tribu, parti en quête de la ville mythique de "Niourk".

Stefan Wul participa brièvement au fandom "On dirait" de Pierre Versins et publia quelques nouvelles et recueils de poésie qui puisent leur inspiration dans l'univers de la science-fiction.

Les romans de Stefan Wul sont présentés par ordre de parution :

Les nouvelles qui suivent sont présentées avec les références de leurs premières publications :




D'autres romans sont en cours d'adaptation en BD chez Ankama dans la collection "Les univers de Stefan Wul"



</doc>
<doc id="14208" url="https://fr.wikipedia.org/wiki?curid=14208" title="Groupe musical">
Groupe musical

Un groupe musical, groupe de musique, ou plus simplement, un groupe, est un petit ensemble musical comprenant en général moins de dix musiciens, chanteurs ou instrumentistes.

L'origine du groupe de musique est souvent associée à l'arrivée de la musique jazz au début du , néanmoins les petites formations de musique dite « classique » tels les quatuors à cordes, par exemple, étaient déjà des groupes musicaux à part entière.

Ensuite le rock 'n' Roll entérina cette notion primordiale de groupe en présentant des orchestres, pourtant assez réduits (et donc propices au vedettariat flattant les égos), mais dont le public ignorait les noms des musiciens ; (exceptionnellement parfois informé seulement des prénoms, comme ceux des membres des Beatles, groupe très médiatisé dès le début). Pendant longtemps seule semblait compter le nom de l'entité collective.
Dans certains cas le nom de cette formation musicale n'est même énoncé qu'au singulier, (exemples : The Who, The Pink Floyd, etc.)"

A l'inverse du monde du jazz, où chaque C.V. se construit fréquemment sur un cursus fait de réunions fameuses lors de rencontres éphémères (les big-bands ne tournant plus beaucoup), rares furent les groupes de rock additionnant des individus déjà célèbres (ex.: Cream, Blind Faith, etc.) et ayant une existence durable.

Avec l'arrivée des nouvelles technologies musicales et du suivi plus soutenu que possèdent les groupes actuels, des fonctions telles que « vidéo-jockey » (VJ) sont apparues et les membres d'un groupe ne se limitent plus à leurs uniques musiciens. Un groupe de musique rock possède en grande majorité un batteur, un bassiste, deux guitaristes et un chanteur. Un groupe de musique "indie" possède les mêmes composantes que le groupe rock mais avec un ou plusieurs multi-instrumentistes.



</doc>
<doc id="14215" url="https://fr.wikipedia.org/wiki?curid=14215" title="Ariane 5">
Ariane 5

Ariane 5 est un lanceur de l'Agence spatiale européenne (ESA), développé pour placer des satellites sur orbite géostationnaire et des charges lourdes en orbite basse. Il fait partie de la famille des lanceurs Ariane et a été développé à compter de 1995 pour remplacer Ariane 4, dont les capacités limitées ne permettaient plus de lancer de manière concurrentielle les satellites de télécommunications de masses croissantes, alors que ce secteur était auparavant le point fort du lanceur européen.

Le programme Ariane 5 a été lancé en 1987, par les ministres européens des affaires spatiales réunis à La Haye. Il est dirigé par l'ESA, mais sa réalisation est assurée par le CNES français. Environ participent au projet.

Le premier lancement a eu lieu le et s'est soldé par un échec. Le lanceur a souffert de débuts difficiles, avec deux échecs (Vol 517 en 2002) totaux et deux échecs partiels sur les quatorze premiers lancements. En 2009, Ariane 5 détient plus de 60 % du marché mondial des satellites commerciaux en orbite géostationnaire.

En décembre 2016, il est prévu que le dernier tir d'une Ariane 5 ait lieu en 2023.

Commercialisée par la société Arianespace, la fusée effectue de cinq à sept lancements par an, en général doubles (deux satellites), depuis le centre de lancement de Kourou, en Guyane. Par rapport à Ariane 4, Ariane 5 est capable d’emporter des charges particulièrement lourdes en orbite basse : la version ECA, la plus récente, peut placer jusqu'à de charge utile en orbite de transfert géostationnaire et en orbite terrestre basse. Ariane 5 est construite par un consortium d'entreprises européennes, placées sous la maîtrise d’œuvre d'Airbus Defence and Space.

Ariane 5 a été développée pour franchir un saut qualitatif par rapport à Ariane 4. Il était prévu au début de sa conception qu'elle puisse mettre en orbite la navette européenne Hermès et assurer des lancements tous les quinze jours. C'est un lanceur complètement nouveau dans sa conception, à l'architecture simplifiée, et conçu pour constituer la base d'une famille évolutive, dont les performances pourront être augmentées progressivement de façon que le lanceur reste pleinement opérationnel, au moins jusqu'en 2020 :


Suivant les modèles, la capacité d’emport d’Ariane 5 se décide entre Arianespace et ses clients (en général des grands opérateurs satellites).


Selon la terminologie de son constructeur, Ariane 5 comprend :

Les « étages d'accélération à poudre » (EAP, ou P230) sont composés d'un tube métallique contenant le propergol solide (la poudre), réalisé dans l'usine Guyanaise REGULUS, et d'une tuyère. Les deux EAP sont identiques, ils entourent l'EPC (« étage principal cryogénique »). Ces propulseurs mesurent chacun de haut pour de diamètre. D'une masse à vide de , ils embarquent de poudre et délivrent 92 % de la poussée totale du lanceur au décollage (poussée moyenne : , poussée maximale : ).

Comparés au moteur Vulcain de l'EPC, les deux EAP ne peuvent être éteints une fois allumés, d'où leur danger en cas de défaillance. Ils assurent le support du lanceur au sol, leur séparation du lanceur, la transmission des mesures pendant le vol et leur neutralisation, sur séparation intempestive provoquée par l'EAP ou l'EPC. Chaque EAP est équipé d'un moteur MPS, qui assure la propulsion du booster en délivrant au sol une poussée de . La courbe de poussée est calculée pour minimiser les efforts aérodynamiques et optimiser les performances : elle est maximale durant les vingt premières secondes avec un long palier de 80 secondes.

L'EAP est composé de trois segments. Le segment avant S1 est fabriqué en Italie, tandis que les deux autres, S2 et S3, sont directement fabriqués en Guyane dans l'usine UPG (Usine de Propergol de Guyane). Ils sont ensuite acheminés par la route sur le fardier (une remorque à roues multiples conçue pour cet usage), depuis l'usine jusqu'au Bâtiment d'Intégration Propulseurs (BIP). Ils y sont préparés, assemblés en position verticale sur leurs palettes (dont ils resteront solidaires pendant toute la phase de préparation jusqu'au décollage), et tirés par un transbordeur (table mobile de ). Ces opérations de préparation sont réalisées par la société franco-italienne Europropulsion. Le segment S1, le plus haut, mesure de long et contient de poudre. Le segment central, S2, mesure de long et contient de poudre. Le dernier segment, S3, mesure de long et contient de poudre. Il donne directement sur la tuyère, par l'intermédiaire du moteur MPS.

L'enveloppe des segments est en acier de d'épaisseur, dont l'intérieur est recouvert d'une protection thermique à base de caoutchouc. Ils sont séparés par des lignes inter-segments d'isolation. Ces joints sont placés entre les segments. Ces segments sont chargés en poudre de manières différentes, avec un creux en forme d'étoile sur le segment supérieur (S1) et une empreinte quasi cylindrique sur les deux autres segments. Le chargement des segments en propergol est réalisé sous vide. La poudre contenue est composée de :

La tuyère, à la base du propulseur, est chargée d'évacuer les gaz de propulsion à raison de deux tonnes par seconde. Fixée sur le segment 3, elle peut s'orienter à et au maximum . Elle mesure de long pour un diamètre de et une masse de . Elle est conçue dans un alliage métallique et composite (avec de la silice) pour résister à la très haute température dégagée. La pression de combustion dans l'EAP est de . Au sommet des segments de poudre se trouve l'allumeur, mesurant de long pour un diamètre de et une masse de , dont de poudre. Il va permettre d'allumer le booster en amorçant la combustion de la poudre, qui va générer la combustion de tous les segments de manière progressive. L'allumeur constitue, en lui-même, un petit propulseur. Déclenché par une charge pyrotechnique, il se comporte comme une charge relais qui allume la charge principale. C'est un bloc étoilé qui donne un débit important de gaz chauds pendant une demi-seconde.

Après épuisement de la poudre, 129 à après leur allumage, ils sont séparés du lanceur à environ d'altitude pour retomber dans l'Océan Atlantique. Pour cela, on amorce 8 fusées d'éloignement réparties ainsi : 4 à l'avant (en haut) et 4 à l'arrière (en bas). Ces fusées contiennent chacune de poudre et fournissent entre 66 et de poussée pendant une demi-seconde. Si ces propulseurs sont parfois récupérés, ils ne sont toutefois jamais réutilisés, contrairement à ce qui se faisait avec les SRB de la navette spatiale.

Une version améliorée des EAP est en cours de préparation. Le , un tir d'essai sur banc de test a montré une poussée moyenne de () durant .

L'« étage principal cryogénique » (EPC) est composé principalement des deux réservoirs d'ergols liquides et du moteur cryogénique Vulcain (Vulcain II pour Ariane 5 évolution (ECA)). Cet étage est mis à feu dès le décollage et assure seul la propulsion du lanceur durant la deuxième phase de vol du lanceur, après le largage des étages d'accélération à poudre. Il fonctionne en tout durant neuf minutes, pendant lesquelles il fournit une poussée de pour un poids total de .

D'une hauteur de pour un diamètre de et une masse à vide de , il contient d'ergols, répartis entre l'hydrogène liquide (LH2 - ) et l'oxygène liquide (LOX - ). Ces réservoirs sont respectivement d'une capacité de et . Ils stockent les ergols refroidis respectivement à et . L'épaisseur de leur enveloppe est de l'ordre de , avec une protection thermique en polyuréthane expansé de d'épaisseur.

Les deux réservoirs sont mis sous pression environ 4 h 30 min avant le décollage avec de l'hélium. Cet hélium provient d'une sphère situé à côté du moteur Vulcain. Elle est isolée thermiquement par une poche d'air. Elle contient d'hélium, pressurisé à au décollage puis 17 au cours du vol. Cet hélium va pressuriser les réservoirs à pour l'oxygène et pour l'hydrogène. Au cours du vol, l'oxygène sera pressurisé à 3,7 puis . Le débit moyen d'hélium dans le réservoir est de l'ordre de . L'hydrogène liquide sera maintenu sous pression par de l'hydrogène gazeux. Cet hydrogène gazeux est prélevé en bas de l'étage avant le moteur, puis réchauffé et transformé en gaz (à environ ), pour être finalement réinjecté dans le réservoir d'hydrogène liquide. En moyenne, cela représente un débit de . Il y a donc tout un jeu de valves et de vannes pour commander les différentes pressions.

La turbopompe à hydrogène du moteur cryogénique Vulcain tourne à , développant une puissance de , soit (la puissance de deux rames de TGV). Elle fait l'objet d'études très poussées sur la résistance des matériaux, et la conception des roulements et le centrage des masses en mouvement se doivent d'être les plus proches possible de la perfection. La turbopompe à oxygène tourne à et développe une puissance de . Sa conception est essentiellement axée sur l'emploi de matériaux qui n'entreront pas en combustion avec l'oxygène qu'elle brasse. Le moteur Vulcain reçoit de ces pompes d'oxygène et d'hydrogène par seconde.

Le composite supérieur comprend la case à équipements et, en fonction de la charge utile emportée, un étage supérieur à moteur à ergols stockables (dans le cas d’une Ariane 5 avec étage supérieur EPS) ou à ergols cryogéniques (dans le cas d’une Ariane 5 avec étage supérieur ESC).

Le composite supérieur assure la propulsion du lanceur après l'extinction et le largage de l'étage EPC. Il fonctionne durant la troisième phase de vol, qui dure environ 25 minutes.

La case à équipements accueille le système de contrôle et de guidage du lanceur. Elle est située directement au-dessus de l'EPC dans le cas d'une Ariane 5 Générique ou en version A5E/S et entoure alors le moteur Aestus de l'EPS. Dans le cas d'une Ariane 5E/CA, la case à équipements est située au-dessus de l'ESC. La case à équipements est le véritable poste de pilotage du lanceur. Il orchestre l'ensemble des contrôles et des commandes de vol, les ordres de pilotage étant donnés par les calculateurs de bord via des équipements électroniques, à partir des informations fournies par les centrales de guidage. Ces calculateurs envoient également au lanceur tous les ordres nécessaires à son fonctionnement, tels que l'allumage des moteurs, la séparation des étages et le largage des satellites embarqués. Tous les équipements sont doublés (redondance), pour qu'en cas de défaillance de l'un des deux systèmes, la mission puisse se poursuivre.

La Case à équipements mesure de diamètre à sa base et au sommet, pour permettre d'y fixer soit la structure SPELTRA (Structure Porteuse Externe pour Lancements Multiples), soit la coiffe. Sa hauteur est , pour une masse de . L'interface avec l'EPS qui va se glisser dans l'anneau mesure au sommet de diamètre. L'anneau porteur sur lequel reposent les instruments mesure alors de large. Voici les principaux instruments qu'il contient :


La case à équipements abrite également le Système (propulsif) de Contrôle d'Attitude, plus fréquemment désigné par ses initiales SCA, qui comprend deux blocs de tuyères alimentées en hydrazine (). Elles permettent notamment le contrôle en roulis du lanceur, pendant les phases propulsées, et le contrôle d'attitude du composite supérieur, pendant la phase de largage des charges utiles. La durée de fonctionnement maximale spécifiée de la case est de l'ordre de , cette durée d'utilisation maximale étant généralement observée lors des missions en orbite basse. Le SCA permet également de pallier les irrégularités du moteur Vulcain, tandis qu'il permet de positionner des satellites en 3D. Il intègre deux réservoirs sphériques en titane, contenant chacun au décollage d'hydrazine, pressurisée à par de l'azote. Le système inclut également deux modules à trois propulseurs de de poussée (au niveau de la mer).

Durant la première phase du vol, le roulis du lanceur est géré par les deux EAP, dont les tuyères orientables permettent de diriger la fusée sur tous les axes. Le lanceur ne doit pas se mettre en rotation, car il perdrait alors de l'énergie et cela entraînerait un des ergols de l'EPC sur leurs parois, conséquence de la force centrifuge qui ferait alors apparition. Comme les canalisations et les sondes qui mesurent la quantité d'ergols restants sont placées au milieu du réservoir, cela pourrait occasionner un arrêt prématuré des moteurs, à la suite d'un désamorçage des turbopompes. Ce cas de figure s'est déjà produit sur le deuxième vol de qualification de la fusée (vol 502).

Une fois les EAP largués, il ne reste plus qu'un seul moteur, le Vulcain, et il n'est donc alors plus possible de jouer sur l'inclinaison des tuyères pour stopper le roulis de la fusée. C'est là que le SCA trouve toute son utilité, car avec ses trois propulseurs il va pouvoir stopper cette rotation. Ces trois moteurs sont braqués de la manière suivante : un vers la droite, un vers la gauche, et le dernier vers le bas. À la suite de l'échec du vol 502, il fut déterminé que le nombre de propulseurs n'était pas suffisant pour contrer le phénomène et les responsables ont préféré prendre leurs précautions en renforçant le système : Dorénavant, le système contient six sphères et dix propulseurs, ce qui porte par-ailleurs la masse totale de la case à équipements à .

Réalisé sous la responsabilité d'Astrium EADS, l'« étage à propergols stockables » (EPS, appelé plus rarement L9) a pour mission d'ajuster la satellisation des charges utiles selon l'orbite visée et d'assurer leur orientation et leur séparation. Situé à l'intérieur du lanceur, il ne subit pas les contraintes de l'environnement extérieur. Sa conception est très basique, se limitant à de simples réservoirs pressurisés dépourvus de turbopompes. Il est constitué d'une structure en nid d'abeilles, du moteur, des réservoirs, des équipements, de raidisseurs disposés en croix et de dix biellettes supportant les réservoirs d'hélium de mise en pression des réservoirs principaux.

De forme tronconique, il s'intercale entre la case à équipements et l'adaptateur de charge utile et mesure de haut (avec la tuyère) pour un diamètre de au niveau de la case à équipements. Au niveau de l'adaptateur de la charge utile, son diamètre est de . D'une masse à vide de , il est doté de quatre réservoirs en aluminium contenant au total d'ergols, répartis entre de monométhylhydrazine (MMH) et de peroxyde d'azote ().

Pressurisés par deux bouteilles en fibre de carbone gonflées à et contenant d'hélium, ces réservoirs alimentent un moteur Aestus (Daimler-Benz Aerospace) qui développe une poussée de pendant (18 min 30 s). Sa particularité est d'être ré-allumable en vol deux fois, afin d'optimiser certaines charges utiles. Sa tuyère est articulée sur deux axes (9.5°). Dans le cas de missions en orbite basse, l'allumage de l'EPS est précédé d'une phase de vol balistique, qui permet également de libérer l'orbite d'une charge utile après sa séparation.

L’« étage supérieur cryogénique » (ESC) utilise, comme son nom l’indique, un moteur cryogénique : le HM-7B.
Il fournit une poussée de pendant , pour un poids de ( à vide) et une hauteur de .

La charge utile est constituée des satellites qui doivent être placés sur orbite. Pour permettre les lancements de plusieurs satellites, ceux-ci sont disposés sous la coiffe dans un module SPELTRA (Structure Porteuse Externe pour Lancements Multiples) ou SYLDA (SYstème de Lancement Double Ariane). Fonctionnant un peu comme une étagère, ces modules permettent de placer en orbite deux satellites distincts, l'un après l’autre : un des satellites est positionné sur le module SPELTRA/SYLDA, l'autre à l'intérieur.

Les charges utiles et le séparateur sont largués durant la quatrième phase de vol : la phase balistique. Selon les caractéristiques de la mission, les largages peuvent être faits immédiatement ou plusieurs dizaines de minutes après le début de cette phase. Les actions effectuées sont des mises en rotation, des éloignements, etc.

Dans le cas d'un lancement simple, le satellite est directement placé sur l'EPS, mais lorsqu'il s'agit d'un lancement double, le satellite du bas est installé sous la cloche formée par la SPELTRA ou le SYLDA et le deuxième satellite vient ensuite prendre appui sur la structure porteuse. Toutes les interfaces de charge utile utilisent un diamètre de , qu'elles soient sur l'EPC ou les modules de lancement multiples. Les installations de satellites peuvent donc parfois nécessiter l'emploi d'adaptateurs de charge utile, s'ils ne peuvent pas utiliser directement ce diamètre pour être installés dans la coiffe. Afin d'améliorer l'offre commerciale proposée par le lanceur, trois adaptateurs seront développés, contenant des interfaces d'un diamètre compris entre et , et supportant des charges utiles d'un masse allant de 2 à . Ils incluront les boulons de fixation, les ressorts du système de séparation et un système d'alimentation électrique pour le satellite concerné.

La SPELTRA est une structure en nid d'abeilles de forme cylindrique avec une partie supérieure tronconique (6 panneaux). Construite en composite de type d'une épaisseur de , elle comporte de une à six portes d'accès et une prise ombilicale pour relier la charge utile au mât de lancement. Elle est utilisée depuis le premier vol d'Ariane 5.

Contrairement au SYLDA, qui est logé dans la coiffe, la SPELTRA se place entre la case à équipements et la coiffe, comme c'était déjà le cas pour la SPELTRA d'Ariane 4. Elle a donc un diamètre extérieur de , pour un diamètre intérieur de . La partie inférieure se pose sur la case à équipements, tandis que la partie supérieure cylindrique sert de cadre de liaison pour la coiffe. La partie tronconique sert d'adaptateur pour les charges utiles.

Elle existe en deux versions : une courte et une longue. La première mesure , auxquels s'ajoutent les de la partie conique coupée en haut, ce qui donne une hauteur totale de , pour une masse de . De la même manière, la grande version mesure de haut pour une masse de .

De sa vraie désignation SYLDA 5, cette structure est interne à la coiffe, et ne la soutient pas, contrairement à la SPELTRA. Conçue par le groupe industriel Daimler-Benz Aerospace, elle mesure de haut pour une masse de .

Le cône du bas mesure d'épaisseur pour un diamètre à la base de . Il est surmonté par la structure cylindrique, d'un diamètre de pour une hauteur de , qui est elle-même surmonté par un cône de avec un diamètre final de au niveau de la zone d'interface avec la charge utile.

Le SYLDA 5 a été utilisé pour la première fois lors du d'Ariane 5 (vol V128) en (satellites Insat 3B et AsiaStar).

Fabriquée en Suisse par Contraves Space, la coiffe protège les charges utiles durant le vol dans l'atmosphère et est larguée dès qu'elle n'est plus utile, afin d'alléger le lanceur. Ce largage est effectué peu après le largage des EAP, à une altitude d'environ , après être restée sur la fusée.

C'est une structure d'un diamètre extérieur de pour un diamètre intérieur utile de . Elle existe en deux longueurs : la , mesurant de haut pour une masse de , et la , mesurant de haut pour une masse de . Elle est équipée d'une prise ombilicale pour le satellite, d'une porte d'accès de de diamètre et d'une protection acoustique, constituée d'un assemblage de boudins en plastique absorbant les vibrations. , installés sur à base de mousse polyamide, recouvrent la paroi interne sur . Le bruit présent à l'intérieur reste toutefois d'un niveau très élevé, atteignant plus de , ce qui est au-delà du maximum supportable par une oreille humaine. Ce bruit se manifeste essentiellement dans les basses fréquences.

la coiffe courte a été utilisé depuis le et la longue à partir du , en (vol V145).

Plusieurs versions du lanceur ont été fabriquées, dont certaines ne sont plus produites.

Treize lanceurs Ariane 5 G (pour ) ont été lancés entre le et le . Cette version n'est plus commercialisée.

Cette version d'Ariane 5 G a un second étage amélioré, avec une charge possible de . Trois lanceurs de ce type ont été tirés, entre le et le . Cette version n'est plus commercialisée.

Cette version dispose des mêmes EAP que l'Ariane 5 ECA et d'un premier étage modifié avec un moteur Vulcain 1B. Charge possible de en orbite de transfert géostationnaire (GTO). Six tirs ont eu lieu entre le et le . Cette version n'est plus commercialisée.

Cette version est conçue pour placer en orbite basse le vaisseau cargo automatique ATV, ravitaillant la Station spatiale internationale. Elle peut lancer jusqu'à de charge utile sur cette orbite.

Ariane 5 ES assure trois allumages de l'étage supérieur, pour répondre aux besoins très spécifiques de la mission. Par ailleurs, ses structures ont été renforcées pour soutenir la masse imposante de l'ATV ().

Son premier lancement a eu lieu le .

Afin d'accélérer le déploiement de la constellation Galileo, Arianespace annonce, le 20 août 2014, le lancement de 12 satellites par un lanceur Ariane 5 ES. Ils seront lancés par quatre à partir de 2015.
Ce programme est partiellement réalisé.

Ariane 5 ECA, aussi appelée Ariane 5 , en référence à sa capacité proche de de mise en orbite de transfert géostationnaire. Son premier étage EPC est motorisé par le Vulcain 2, plus puissant que le Vulcain 1, et son second étage ESC utilise le moteur cryotechnique HM-7B, déjà utilisé pour le troisième étage d'Ariane 4.

Depuis fin 2009, c'est la seule version utilisée pour lancer des satellites commerciaux. Elle a été tirée 62 fois entre le et le et n'a connu qu'un échec, lors du vol V157 (). Malgré des contestations sociales en Guyane provoquant un retard, le projet a abouti à une double mise en orbite.


Pour pallier ces limitations, il était prévu de développer une version ME, initialement appelée Ariane 5 ECB. Celle-ci devait comporter un nouvel étage supérieur cryotechnique et réallumable, qui devait utiliser un nouveau moteur Vinci plus puissant, en cours de développement chez Snecma (Safran). Grâce à cet étage, Ariane 5 ME aurait alors été capable de lancer jusqu'à de charge utile en orbite de transfert géostationnaire (GTO). Le premier vol était prévu en 2017 ou 2019.

Le développement de cette version, avec un financement pour 2 ans jusqu'en 2014, décidé lors de la session ministérielle du Conseil de l'ESA en , n'est plus d'actualité, elle est remplacée par la future Ariane 6.

<nowiki>*</nowiki> Situé dans la case à équipement de de diamètre

La fusée Ariane 5 est lancée depuis le Centre spatial guyanais, construit par le CNES en Guyane française (Amérique du Sud) près de la ville de Kourou. Des installations adaptées à Ariane 5 ont été construites sur cette base qui a lancé les versions précédentes du lanceur Ariane.

L'ensemble de lancement de la fusée Ariane 5 (ELA-3, acronyme d'Ensemble de Lancement Ariane 3), qui occupe une superficie de , est utilisé pour lancer les fusées Ariane 5 et a été de 2003 jusqu'en 2009 le seul site actif après l'arrêt des lancements d'Ariane 4. Il comprend :
Les bâtiments d'assemblage (BIL, BAF) ainsi que la zone de lancement sont reliés par une double voie ferrée sur laquelle circule la table de lancement mobile portant la fusée. L'aménagement permet huit lancements par an.

Une partie du lanceur Ariane 5 est fabriquée sur place. Une unité de production fabrique et coule le propergol solide de deux des trois segments de chaque propulseur à poudre (EAP) de la fusée (le troisième est coulé en Italie). Le site dispose d'un banc d'essai pour les EAP.

Le centre Jupiter est le centre de contrôle qui permet de piloter l'ensemble des opérations de préparation et de lancement.

Durant cette phase, on met aussi les systèmes hydrauliques sous pression, afin de tester le circuit.
Sur le modèle Ariane 5ES ATV la dernière phase comporte trois réallumages successifs.

Les débuts d'Ariane 5 furent caractérisées par plusieurs échecs. La fiabilisation du lanceur nécessita un important effort financier, réalisé au détriment du développement de versions plus puissantes.

Le premier tir eut lieu le à Kourou, mais le lanceur fut détruit après 37 secondes de vol. L'échec était dû à une erreur informatique, intervenue dans un programme de gestion de gyroscopes conçu pour la fusée Ariane 4, et qui n'avait pas été testé dans la configuration d'Ariane 5.
Le défaut informatique avait pris sa source dans une erreur de transcription de spécifications. Lors des échanges entre l'ESA et le fabricant de la centrale inertielle (dite également IRS), les spécifications fonctionnelles ont été recopiées plusieurs fois et c'est lors de ces recopies qu'une erreur fut introduite. Les spécifications initiales définissaient une durée maximum admissible de 60 secondes pour l'alignement du gyroscope. La durée d'alignement est le temps qu'il faut pour qu'un gyroscope atteigne sa vitesse de rotation opérationnelle, et permette ainsi de situer l'objet et son orientation dans l'espace. valeur erronée provoquant un dysfonctionnement du programme chargé de gérer les données gyroscopiques.

Il existait une méthode de gestion de cette erreur, mais cette dernière avait été désactivée sur Ariane 4, considérant que sur ce modèle on pouvait prouver que l'occurrence du dépassement qui allait être produit par le programme était nulle compte tenu des trajectoires de vol possibles. Or les spécifications d'Ariane 5, notamment en phase de décollage, diffèrent notablement de celles d’Ariane 4. Le programme de la centrale inertielle, bien que redondant, produisit deux dépassements de trajectoire et finit par signaler la défaillance des systèmes gyroscopiques. Le calculateur de pilotage de la fusée (spécifiquement mis au point pour Ariane 5), en interprétant les valeurs d'erreurs (probablement négatives) fournies par le second gyroscope, déduisit que la fusée s'était mise à pointer vers le bas. La réaction du calculateur de pilotage fut de braquer les tuyères au maximum pour redresser la fusée, ce qui augmenta considérablement l'incidence du lanceur et provoqua des efforts aérodynamiques qui le détruisirent. Il s'agit certainement là de l'une des erreurs informatiques les plus coûteuses de l'histoire ().

Il a été souligné que le programme de gestion d'alignement gyroscopique, source de l'accident, était totalement inutile. Il était en effet conçu pour réajuster rapidement le calibrage des gyroscopes dans le cas d'un court retard de tir (de l'ordre de quelques minutes), afin de permettre une reprise rapide du compte à rebours – par exemple en raison de variations rapides des conditions météo du site de lancement à Kourou. Or ce cas de figure, envisagé initialement pour Ariane 3, était depuis longtemps exclu des procédures de tir.

Le second vol eut lieu le .

La mission parvint à son terme mais l'orbite désirée ne fut pas atteinte, par suite d'un mouvement de rotation du lanceur sur lui-même (mouvement de roulis, comme une toupie) qui a conduit à un arrêt prématuré de la propulsion du premier étage EPC. Après cette fin de propulsion du premier étage, et malgré la mise en route correcte de l'étage supérieur EPS, celui-ci n'a pas pu rattraper l'intégralité du déficit de poussée de la première phase du vol, conduisant donc la mission sur une orbite légèrement dégradée.

Ce mouvement en roulis était dû à un couple généré par l'écoulement des gaz dans la tuyère du moteur Vulcain 1, couple dont l'intensité avait été sous-estimée. Dès lors, et malgré la mise en œuvre du système de pilotage en roulis SCA, le lanceur a subi durant tout le vol du premier étage une mise en rotation excessive. Cette mise en rotation aurait pu n'avoir que peu de conséquences, les algorithmes de vol – relativement efficaces – contrôlant malgré tout la trajectoire. Cependant, en fin de propulsion, et sous l'effet de la vitesse en roulis atteinte, la surface des ergols (oxygène et hydrogène liquides) dans les réservoirs s'est incurvée en son centre (à la manière d'un siphon, lorsque le liquide se plaque contre les parois). Ce phénomène a été interprété par les capteurs de niveau (« jauges » des réservoirs) comme l'indication de l'imminence d'une « panne sèche », ce qui a conduit l'ordinateur de bord à commander l'arrêt de propulsion de l'EPC prématurément.

Le couple en roulis généré par le moteur Vulcain 1 fut maîtrisé dès le vol suivant par la mise en place, en extrémité, de divergents d'échappement légèrement inclinés corrigeant le roulis naturel engendré par le moteur. Les responsables de la conception d'Ariane 5 ont tout de même préféré prendre leurs précautions en renforçant le système SCA : il contient désormais six sphères de propergol et dix propulseurs de contrôle, au lieu des trois propulseurs du début.

Ce problème a touché d'autres lanceurs, dont le H-IIA japonais.

Le troisième essai eut lieu le . Ce fut une réussite totale.

La mission emportait la capsule de démonstration de rentrée atmosphérique Atmospheric Reentry Demonstrator (ARD) (capsule européenne de type Apollo), qui effectua une rentrée atmosphérique parfaite, et la maquette technologique MAQSAT.

Aux deux premiers échecs de début de carrière s'ajoutent ceux survenus sur des vols commerciaux, en 2001, 2002 et 2018.

Sur ce vol, effectué le , pas de panne franche ni d'erreur de pilotage. Le problème vient du moteur du dernier étage qui a fonctionné moins longtemps (1 minute et 20 secondes de moins) et avec une puissance inférieure de à celle qui avait été prévue, ne permettant pas d'atteindre la vitesse nécessaire à l'injection visée (apogée à au lieu de ). Ce vol est un demi-échec, car la satellisation a été réussie, mais avec des paramètres d'injection qui n'étaient pas optimaux.

La cause semble être la présence d'eau résiduelle dans l'infrastructure du moteur, provenant de tests réalisés au sol. Mélangée au carburant, elle aurait entraîné une baisse notable de la puissance et une surconsommation de l'un des ergols, ce qui pourrait expliquer la perte de puissance et l'arrêt prématuré.

Pour combler ces différences, le satellite Artemis a utilisé sa propre propulsion afin d’atteindre son orbite géostationnaire cible. Il a été reconfiguré à distance pour atteindre sa position souhaitée, par le biais d'une nouvelle procédure. D'abord par une série de mises à feu, utilisant la plus grande partie de son carburant, pour le mettre sur une orbite circulaire plus élevée. Puis par ses moteurs ioniques, prévus initialement seulement pour corriger son orbite, grâce à une trajectoire en spirale, qui lui a fait gagner par jour et atteindre, en 18 mois, son altitude de . Le second satellite, BSAT 2B a, lui, été définitivement perdu car il ne possédait pas les ressources suffisantes pour combler cette différence d'orbite.

Le , ce vol inaugural de la version ECA d'Ariane 5 s'est terminé dans l'océan Atlantique, à la suite d'une défaillance du moteur Vulcain 2, équipant l'étage principal de la fusée.

Une fuite dans le système de refroidissement a entraîné une déformation de la tuyère, ce qui a créé un déséquilibre dans la poussée du moteur et rendu le lanceur impossible à piloter. Face à une perte de contrôle insurmontable par la fusée, le contrôle au sol a pris ses précautions et commandé la destruction de la fusée en vol. Les deux satellites français de télécommunications présents à bord, "Hot Bird 7" et "Stentor", ont été détruits. L'échec de ce lancement a causé la perte de deux satellites d'une valeur totale de .

Le décollage de la fusée, s'est effectué comme prévu le à , mais à la , peu après la séparation du , alors que la fusée se trouvait dans l'espace, les différentes stations au sol n'ont pas reçu les signaux de télémesure du second étage, qui est resté « muet » pendant , jusqu'à la fin de la mission.

L'origine de l'incident est une erreur humaine. Des paramètres de vol erronés ont été programmés dans l'ordinateur de bord de la fusée. La station au sol de Galliot, suivant la fusée depuis le décollage, a constaté la déviation de la trajectoire mais n'a pas transmis cette information aux stations suivantes. Celles-ci, pointant leurs antennes sur la trajectoire prévue, n'ont pu établir le contact. La mission s'est poursuivie jusqu'à son achèvement de façon entièrement automatique. 

Les deux satellites ont été déployés, mais sur de mauvaises orbites. En effet si le périgée () et l'apogée () sont conformes aux attentes, l'inclinaison de l'orbite obtenue est de au lieu des visés. Le satellite pourra atteindre l'orbite prévue au bout d'un mois, sans réduction significative de sa durée de vie grâce au très bon rendement de sa propulsion électrique. Quant au satellite Al Yah 3, il est en route vers son orbite finale, mais ni sa position, ni ses réserves de carburant n'ont été communiquées.

L'important écart de trajectoire subi par la fusée a soulevé de nombreuses questions quant à la sécurité des vols. Car si l'erreur de programmation n'aurait théoriquement jamais du passer entre les mailles du filet des nombreuses étapes de vérification entreprises avant un lancement, un autre fait inquiète les divers acteurs de l'exploitation spatiale européenne. En effet, du fait de sa déviation de près de , la fusée a survolé la commune de Kourou, ce qui n'était jamais arrivé auparavant. Si un incident grave avait eu lieu à ce moment-là, les conséquences auraient pu être très lourdes pour les habitants de la commune survolée par la fusée.

La commission d'enquête a établi que la cause de la déviation de la trajectoire était une erreur d'alignement des deux centrales inertielles, l'azimut requis spécifiquement pour ce vol étant de au lieu des habituels. Elle a recommandé le renforcement du contrôle des données utilisées lors de la préparation des missions. La mise en œuvre de ces mesures correctives permettra la reprise des vols selon le calendrier prévu, dès le mois de .

Le premier vol commercial eut lieu le , avec la mise en orbite du satellite d’observation en rayons X XMM-Newton.

Un échec partiel eut lieu le : à nouveau, deux satellites ne purent être placés sur l'orbite désirée. Artémis, le satellite de communication de l'ESA, atteignit son orbite définitive par ses propres moyens, en utilisant son combustible destiné aux corrections d'orbite, ainsi qu'une unité de propulsion ionique qui n'avait pas été prévue pour cet usage. Ceci nécessita une modification complète du programme de bord depuis le sol et raccourcit la durée de vie du satellite.

Le vol suivant n'eut lieu que le , avec la mise en orbite réussie du satellite environnemental de ENVISAT, à une altitude de .

Au cours des années suivantes, Ariane 5 a pu conserver la position acquise par la version Ariane 4 (part de marché supérieure à ) sur le segment du lancement des satellites commerciaux en orbite géostationnaire, qui représente entre 20 et 25 satellites par an (sur une centaine de satellites lancés annuellement). La concurrence est représentée par les lanceurs à la capacité beaucoup moins importante, mais qui bénéficient d'un prix au kilogramme de charge utile nettement inférieur. Les deux principaux concurrents actuels sont :

Au , 98 tirs d'Ariane 5 ont été effectués, toutes versions confondues, et les 82 derniers lancements ont été réussis (dont 64 d'affilée pour la version ECA), ce qui constitue un record pour les lanceurs de la famille Ariane. Le taux de fiabilité s'établit à % (deux échecs complets et deux échecs partiels, considérés dans le calcul comme des demi-échecs). Ce taux de fiabilité se décline en fonction des versions de la manière suivante :

Ariane 5 est souvent utilisée pour placer en orbite géostationnaire des satellites de télécommunications lourds : le record est détenu par "TerreStar-1" () lancé le ; la charge utile la plus importante placée en orbite de transfert géostationnaire est constituée par les deux satellites "ViaSat‐2" et "EUTELSAT 172B", lancés le par le vol VA237 et qui représentaient une masse totale de au lancement. En orbite basse, la charge la plus lourde mise en orbite par Ariane 5 est le cargo spatial européen ATV "Georges Lemaître" de , destiné à ravitailler la station spatiale internationale (orbite de 250 - ) et lancé le par le vol VA219. Le satellite d'observation de la Terre "ENVISAT" de , placé sur une orbite héliosynchrone ( d’altitude) le par le vol 145, est le plus gros satellite d'observation placé en orbite basse par Ariane 5. Le nombre total de satellites lancés par Ariane 5 est de 361.

<br>





</doc>
<doc id="14217" url="https://fr.wikipedia.org/wiki?curid=14217" title="Tanakh">
Tanakh

Tanakh (en hébreu ), est l'acronyme 
de l’hébreu « - - », en français : « », formé à partir de l'initiale du titre des trois parties constitutives de la Bible hébraïque : 


On écrit aussi "Tanak" (sans "h" à la fin). Le Tanakh est aussi appelé "" , 

Terminologie : "Tanakh", "Ancien Testament" et "Bible hébraïque".

La division que reflète l'acronyme Tanakh est bien attestée dans des documents de l'époque du Second Temple, dans le "Nouveau Testament" chrétien et dans la littérature rabbinique, à ceci près qu'au cours de cette période l'acronyme en question n'était pas utilisé ; le terme correct était ' (« Lecture », renvoyant à une fonction liturgique du texte), par opposition à ' (« Enseignement », « Répétition ») ou ' (« Exégèse »). Le terme ' continue à être utilisé aux côtés de "Tanakh" pour dénommer les Écritures hébraïques. En hébreu moderne parlé, "" possède néanmoins une connotation plus formelle que "Tanakh".

Les livres inclus dans le "Tanakh" étant pour la plupart écrits en hébreu, on l'appelle également la Bible hébraïque. Bien que l'araméen se soit introduit en bonne partie dans les livres de Daniel et d'Esdras, ainsi que dans une phrase du Livre de Jérémie et un toponyme de deux mots dans le ("Livre de la Genèse"), ces passages sont écrits dans la même écriture hébraïque. Les passages en araméen sont les suivants : Esdras 4.8, 4.7 et 12.26 ; Jérémie 10.11 ; Daniel 2.4 à 7.28

Selon la tradition juive, le "Tanakh" est constitué de vingt-quatre livres : la "Torah" contenant cinq livres, les ' huit, et les ' onze.

La "Bible hébraïque" a exactement le même contenu que l’"Ancien Testament" protestant mais les livres sont présentés et classés différemment, les protestants comptant trente-neuf livres, et non vingt-quatre. 
Ceci est dû au fait que les Chrétiens ont choisi de subdiviser certains livres de la religion juive. 

Cependant, l'expression Ancien Testament, utilisée dans la tradition chrétienne, est souvent perçue comme péjorative par les Juifs. 
D'une part elle ferait l'objet d'une volonté de s'approprier arbitrairement les textes de la religion juive et d'autre part selon la foi juive il ne saurait exister de Nouveau Testament puisque celui-ci n'est pas reconnu. 
L'expression « Premier Testament » est parfois considérée comme plus respectueuse vis-à-vis de la tradition juive.

En tant que telle, une distinction technique peut être tracée entre le Tanakh et le corpus similaire mais non identique que les Chrétiens protestants nomment "Ancien Testament". L'expression de "Bible hébraïque" est donc préférée par certains érudits, car elle recouvre les aspects communs du Tanakh et de l'Ancien Testament en évitant les biais partisans.

L’"Ancien Testament" catholique et orthodoxe contient sept Livres non inclus dans le "Tanakh". Ils sont appelés "Livres deutérocanoniques" ( « canonisés secondairement » c'est-à-dire canonisés ultérieurement). Ils sont tirés de la "Septante", version grecque étendue du "Tanakh".

Dans les Bibles chrétiennes, les "Livres de Daniel" et "d'Esther" peuvent contenir des textes deutérocanoniques, n'ayant été inclus ni dans le canon juif ni dans le canon protestant.

Le texte hébreu ne consistait originellement qu'en consonnes, avec des lettres utilisées de façon inconstante comme des voyelles (""). Au cours du Haut Moyen Âge, les Massorètes codifièrent la tradition orale de lecture du Tanakh en ajoutant deux types spéciaux de symboles au texte : les signes de "" (ponctuation à fonction de voyelles) et de cantillation, ces derniers indiquant la syntaxe, l'accent tonique et la mélodie pour la lecture.

Les Livres de la "Torah" ont des noms d'usage basés sur le premier mot significatif de chaque livre. Les noms en français n'en sont pas la traduction : ils sont basés sur les noms grecs créés pour la , lesquels étaient eux-mêmes basés sur les noms rabbiniques décrivant le contenu thématique des Livres.

Les noms entre parenthèses sont ceux sous lesquels les Livres sont connus dans le monde chrétien.

La Torah ( « Loi ») également connue sous le nom de Pentateuque se constitue de :

Les (, « Prophètes ») sont :


Les (, « Écrits ») consistent en :

Alors que les Chrétiens lisent la Bible dans des livres, les Juifs la lisent (du moins pour l'usage rituel) dans un rouleau. La division en chapitres et versets n'a donc aucune signification dans la tradition juive, qui divise la Torah en "" (péricopes, sections), elles-mêmes divisées en sept parties thématiques, et les autres Livres selon les épisodes narratifs. Elle a néanmoins été ajoutée dans la plupart des éditions modernes du Tanakh, afin de faciliter la localisation et la citation de ceux-ci. La division de Samuel, Rois, et Chroniques en et est également indiquée sur chaque page de ces livres, afin d'éviter toute confusion dans la capitation de ces Livres, celle-ci suivant la tradition textuelle chrétienne. 

L'adoption de la capitation chrétienne par les Juifs commença en Espagne, aux alentours du , en partie du fait des disputations, des débats œcuméniques forcés dans le contexte de l'Inquisition espagnole naissante. Les débats requéraient en effet un système de citation biblique commun. Du point de vue de la tradition textuelle juive, la division en chapitres est non seulement une innovation étrangère sans aucun fondement dans la "", mais elle est également fort critiquable car :

Néanmoins, comme leur utilité — voire leur indispensabilité — a été prouvée pour les citations, elles continuèrent à être incluses par les Juifs dans la plupart des éditions hébraïques des textes bibliques, et même de textes sacrés non bibliques y ayant fréquemment recours, comme le Talmud. Pour plus d'informations sur l'origine des divisions, voir capitation biblique.

Les nombres des chapitres et des versets étaient souvent indiqués de façon proéminente dans les anciennes éditions, comme dans la Bible du Rabbinat, au point de recouvrir les divisions massorétiques traditionnelles. Cependant, dans de nombreuses éditions juives du Tanakh publiées au cours des quarante dernières années, il s'est produit une tendance notable à en minimiser l'impact sur les pages imprimées. 

La plupart des éditions réalisent ce but en reléguant la numération en marge des Textes. Le Texte de ces éditions est ininterrompu tout au long des chapitres (dont le début est uniquement notifié en marge). L'absence de capitation dans ces éditions renforce également l'impact visuel créé par les espaces et « paragraphes » des pages, qui indiquent la division traditionnelle juive en "".

Ces éditions modernes présentent les Livres de Samuel, des Rois, des Chroniques et d'Ezra comme un seul livre dans leur table des matières, et ne font aucune mention dans le texte de leur division en deux parties (bien qu'elle soit notée dans les marges supérieures et latérales). Le texte de , par exemple, suit celui de sur la même page, sans espacement particulier entre eux dans le flux du texte, et peut même continuer sur la même ligne de texte.


Le judaïsme rabbinique enseigne que la Torah fut transmise en parallèle avec une tradition orale qui la complète. Cette croyance n'est pas partagée par les Juifs karaïtes, les Beta Israël, les Samaritains, ainsi que la majorité des Chrétiens, à l'exception de certains groupes messianiques. 

Selon les tenants de la loi orale, de nombreux termes et définitions utilisés dans la loi écrite ne sont pas définis dans la Torah elle-même, ce qui suppose de la part du lecteur une familiarité avec le contexte et le détail, lesquels ne pourraient être connus que via une antique tradition orale.

Les opposants à la tradition orale objectent que, de l'important corpus des travaux rabbiniques, seule une partie sert à clarifier effectivement le contexte. Ces travaux rabbiniques, collectivement connus comme « la Loi orale » [], incluent la , la , les deux Talmuds (de Babylone et de Jérusalem), ainsi que les premières compilations du .





</doc>
<doc id="14219" url="https://fr.wikipedia.org/wiki?curid=14219" title="Vie politique en France depuis 1958">
Vie politique en France depuis 1958

La vie politique en France se déroule sous le régime de la Cinquième République depuis l'adoption de la Constitution française du 4 octobre 1958.

Depuis 1958, la France est une république constitutionnelle et un régime parlementaire. Concrètement, cela signifie que le pouvoir exécutif est détenu essentiellement par le président de la République et qu'il partage avec le Premier ministre et le gouvernement qu'il a nommé. On peut aussi parler de régime semi-présidentiel. C'est un régime politique typiquement français car très rare dans le monde ; ce régime a la réputation d'être à la fois très stable et très souple.

Après que Charles de Gaulle a fait adopter la Constitution de 1958, la France a été gouvernée par des gouvernements de droite successifs jusqu'en 1981. Durant les années 1960, les partis de gauche avaient des résultats plutôt médiocres aux élections nationales. Les gouvernements successifs appliquaient généralement le programme gaulliste d'indépendance nationale, et de modernisation d'une manière interventionniste. Le gouvernement gaulliste, pourtant, a été critiqué pour sa brutalité: tandis que les élections étaient libres, l'État avait le monopole et le contrôle des émissions de radio et des émissions de télévision et cherchait à imposer son point de vue sur l'actualité (cependant ce monopole n'était pas absolu, puisqu'il ne pouvait s'exercer qu'à l'intérieur du territoire français et qu'il y avait des radios qui émettaient depuis les pays voisins). La politique sociale de De Gaulle était foncièrement conservatrice.

Pendant les événements de Mai 1968, une série de grèves de travailleurs et de révoltes d'étudiants agita la France. Mais cette agitation n'eut pas pour effet un changement immédiat de gouvernement, la droite étant largement réélue à l'élection de juin 1968. L'électorat bascula en 1969 au référendum sur la réforme du Sénat et la régionalisation, dans un mouvement généralement considéré comme une lassitude des Français pour De Gaulle.

En 1981, François Mitterrand, candidat du parti socialiste, fut élu président avec un programme de réformes de grande envergure, le Programme commun. Après s'être assuré une majorité au parlement à l'issue des élections législatives de la même année, son gouvernement mena un programme de réformes économiques et sociales.

En 1983, la forte inflation et la crise économique menèrent à un revirement de la politique économique, connue sous le terme de « tournant de la rigueur » – le gouvernement de gauche s'est alors engagé dans des réformes de politique fiscale et de contrôle des dépenses, et de privatisation des principales banques françaises.

Bien que la majorité des nationalisations aient été annulées dès 1984, ou par les gouvernements suivants (de gauche comme de droite), les réformes sociales entreprises ont été maintenues. Depuis lors, le gouvernement alterna entre une coalition de gauche (composée du parti socialiste et du parti communiste, et plus récemment Les Verts), et une coalition de droite (composé par l'Union pour la démocratie française et le Rassemblement pour la République de Jacques Chirac, plus tard remplacé par l'Union pour un mouvement populaire).

Les années 1980 et années 1990 ont vu aussi l'émergence du Front national de Jean-Marie Le Pen, un parti accusant l'immigration, particulièrement l'immigration provenant des pays d'Afrique du Nord tels que l'Algérie de l'augmentation du chômage et de la criminalité. Depuis les années 1980, le chômage est resté élevé, à environ 10 % de la population active, quelles que soient les politiques menées pour le combattre. En outre, la criminalité a changé durant cette période, avec une très forte augmentation de la délinquance juvénile et des actes d'incivilité, bien que la mesure de son augmentation soit sujette à débat. Les problèmes dans les banlieues – un euphémisme décrivant les zones d'habitations périurbaines défavorisées, souvent à forte proportion de population issue de l'immigration – restent préoccupants. La présence de Jean-Marie Le Pen au second tour de l'élection présidentielle de 2002 a été attribuée, en grande partie, au sentiment d'insécurité.







Le système politique français est marqué par la bipolarisation de ses forces politiques sur la base du clivage gauche/droite. Aux débuts de la Cinquième République, la vie politique s'organisait à gauche autour du Parti communiste français (PCF) et de la gauche réformiste de gouvernement, au sein de laquelle le Parti socialiste (PS) deviendra hégémonique dès sa fondation en 1969. À droite, les partis de la mouvance gaulliste, unifiés en 1967 au sein de l'Union démocratique pour la Ve République (UDR) occupèrent longtemps une position largement dominante, mais régulièrement contrebalancée par la droite libérale principalement incarnée par les Républicains indépendants et de petits mouvements du centre droit, héritiers du Mouvement républicain populaire (MRP), comme le Centre démocrate ou Progrès et démocratie moderne. Les rapports entre ces formations au sein même des deux blocs sont souvent conflictuels, mais le recours au scrutin uninominal majoritaire à deux tours pour l'élection des députés et la présidentialisation du régime avec, dès 1962, l'élection du Président de la République au suffrage universel direct contribue au maintien de la bipolarisation. La droite menée par la sensibilité gaulliste dominera assez largement la vie politique jusqu'au début des années 1970, Charles de Gaulle et Georges Pompidou étant successivement largement élus à la présidence de la République, avec le soutien constant de l'Assemblée Nationale, où le parti gaulliste reste la première force jusqu'en 1981.

Les années 1970 marqueront le premier grand bouleversement du paysage politique français. Les élections législatives de 1973 marquent un net reflux du gaullisme au profit du centre-droit, largement concrétisé lorsque Valéry Giscard d'Estaing, chef des Républicains indépendants, accède à la présidence de la République aux dépens du gaulliste Jacques Chaban-Delmas. C'est la fin de "l'État-UDR". À gauche, le PS s'allie au PCF et au petit Mouvement des radicaux de gauche (MRG) dans le cadre de l'Union de la gauche en 1972. Cette alliance permettra à la gauche de se renforcer, son candidat unique à l'élection présidentielle de 1974, le Premier secrétaire du PS François Mitterrand, manquant de peu la victoire au second tour (49,2 % des voix). À droite, les rapports de force se rééquilibrent avec la fondation par M. Giscard d'Estaing en 1978 de l'Union pour la démocratie française (UDF), parti fédérant les forces de la droite non-gaulliste et du centre-droit. Ce nouveau parti met fin à l'hégémonie gaulliste à droite, l'UDR, transformée en Rassemblement pour la République (RPR) en décembre 1976 sous l'impulsion de Jacques Chirac, restant toutefois légèrement plus forte électoralement parlant. Le système partisan s'organise dès lors sur la base d'une "quadrille bipolaire", avec quatre partis d'importance équivalente : à gauche, le PCF et le PS, à droite, le RPR et l'UDF. Cette nouvelle situation est particulièrement bien illustrée par les résultats des élections législatives de 1978, à l'occasion desquelles la droite conserve de justesse sa majorité à l'Assemblée nationale malgré d'éclatantes victoires de la gauche lors d'élections locales.

En 1981, l'élection de François Mitterrand à la Présidence de la République bouleverse à nouveau les rapports de forces au sein du système partisan français. Au cours de cette élection et des législatives qui suivront, le Parti communiste perd une grande partie de son audience à gauche au bénéfice du Parti socialiste, qui devient très largement majoritaire de seul à l'Assemblée nationale. Les années 1980 marqueront l'avènement d'une nouvelle ère dans la vie politique française. Si à droite les rapports de forces restent inchangés, le PCF amorce de manière brutale un inexorable déclin, le PS devenant progressivement hégémonique dans le camp de la gauche de gouvernement. Les déceptions vis-à-vis des politiques menées par les premiers gouvernements socialistes, qui abandonnent dès 1982 leur plan de relance keynésien au profit d'un plan de rigueur économique nettement moins ambitieux, mécontente une bonne partie de l'électorat qui se réfugie dans le vote protestataire. À l'occasion des Élections européennes de 1984, le Front national (FN), formation populiste classée à l'extrême droite de l'échiquier politique, effectue une percée phénoménale, passant d'un niveau proche de zéro à plus de 10 % des suffrages exprimés. Ce succès est concrétisé en 1986 lorsque le FN envoi 32 députés siéger à l'Assemblée nationale, alors élue à la représentation proportionnelle. Rejeté par la droite traditionnelle, le FN est exclu du jeu des alliances et ne sera jamais en mesure d'obtenir plus d'un siège à l'Assemblée après le rétablissement du scrutin majoritaire dès les législatives de 1988. Cela n'empêchera pas son audience électorale et médiatique de croitre, Jean-Marie Le Pen obtenant plus de 14 % des suffrages exprimés au premier tour de l'élection présidentielle de 1988. Le FN s'impose peu à peu comme une troisième force politique alternative, en dehors de la bipolarisation traditionnelle entre droite et gauche.

Les années 1990 débutent sous le signe de l'alternance. Alors que le second mandat du président Mitterrand touche à sa fin, la gauche est laminée par l'alliance RPR-UDF lors des élections législatives de 1993. Cette période est marquée par une consolidation du FN et par la percée significative des mouvements écologistes, en particulier Les Verts et Génération écologie, qui font leur entrée dans plusieurs conseils régionaux à l'issue des élections régionales de 1992 et obtiennent d'excellents scores aux législatives de 1993. Ce succès sera de courte durée, et seuls Les verts parviendront à s'imposer durablement dans le paysage politique avec des scores tournant autour des 4 %. La classe politique traditionnelle perd encore du terrain lors de l'élection présidentielle de 1995, au cours de laquelle Jean-Marie Le Pen atteint les 15 %, tandis que l'extrême gauche, représentée par la candidate de Lutte ouvrière Arlette Laguiller, effectue une percée avec plus de 5 % des voix. Cette nouvelle recomposition des forces politiques oblige le Parti socialiste à composer avec des formations concurrentes à gauche, notamment sa petite scission du Mouvement des citoyens (MDC) mené par Jean-Pierre Chevènement. Lors des élections législatives de 1997, la Gauche plurielle menée par le socialiste Lionel Jospin, rassemblant PS, PCF, verts, MDC et radicaux-socialistes, investit une étroite majorité de 55 % des sièges. Le PS, doté d'une majorité relative, doit composer avec les autres forces de la majorité plurielle et Jospin forme dans cet esprit un gouvernement de coalition intégrant toutes ses composantes. Malgré des résultats plutôt satisfaisants, l'alliance volera en éclats, en 2002, à la fin de la législature.

À droite, l'UDF doit faire face dès 1998 à une scission orchestrée par une quarantaine de députés de sa tendance libérale, qui partent fonder Démocratie libérale (DL). Malgré son groupe parlementaire conséquent, ce parti restera marginal, dépendant de ses alliances avec le RPR et une UDF recentrée. Le Mouvement pour la France (MPF), autre scission de l'UDF intervenue en 1994 à l'initiative du député Philippe de Villiers, restera dans l'ombre jusqu'à sa fusion, en 1999, avec le Rassemblement pour la France (RPF) de l'ex RPR Charles Pasqua, à l'occasion des élections européennes, au cours desquelles leur liste obtient 13 % des voix, devant la liste RPR-DL de Nicolas Sarkozy. Ce sera un succès sans lendemain, le mouvement éclatant dès 2000 lorsque M. de Villiers décide de faire scission pour refonder le MPF, qui retrouve son audience antérieure, tandis que ce qui reste du RPF sombre dans la marginalisation. Un autre petit parti fondé par d'anciens membres des partis de droite traditionnels, Chasse, pêche, nature et traditions (CPNT), connaîtra à la fin des années 1990 un succès éphémère, en obtenant 6 élus lors des européennes de 1999 avec près de 7 % des voix et faisant son entrée dans plusieurs conseils régionaux lors des régionales de 1992 et de 1998. Malgré le score honorable de son chef Jean Saint-Josse à l'élection présidentielle de 2002 (4,2 %), CPNT connaîtra un brutal reflux lors des scrutins qui suivront, au cours desquels le parti perdra tous ses élus régionaux et européens.

Les années 2000 seront le théâtre de profonds bouleversements. L'élection présidentielle de 2002 sera marquée par une fragmentation du paysage politique tenant du jamais vu. Au premier tour, Jean-Marie Le Pen, bénéficiant d'un nouvel apport de voix (près de 17 % des voix) devance ainsi Lionel Jospin, victime de l'éparpillement des voix de gauche. L'extrême-gauche, dont les candidats Arlette Laguiller et Olivier Besancenot capitalisent 10 % des voix, monte en puissance. Le PCF, qui s'était jusqu'ici maintenu aux alentours de 9 %, s'effondre à 3,3 % avec la candidature de Robert Hue. Avec les candidatures de Jean-Pierre Chevènement pour le MDC et de Christiane Taubira pour le Parti radical de gauche, l'électorat socialiste s'éparpille aux dépens du candidat Jospin. Au second tour, Jacques Chirac, bénéficiant du rejet de l'extrême-droite, écrase Jean-Marie Le Pen avec 82 % des voix. Forte de ce nouveau souffle inattendu, la droite se réorganise, avec le lancement entre les deux tours de l'Union pour la majorité présidentielle (UMP) à l'initiative de Jacques Chirac. L'UMP, fusion du RPR et de Démocratie libérale (scission de l'UDF intervenue en 1998 à l'initiative d'une quarantaine de députés) et ayant reçu le renfort d'une majorité de cadres et d'élus de l'UDF devient le premier grand parti de la droite gouvernementale française. Lors des élections législatives de 2002, l'UMP, largement victorieuse, prend l'hégémonie à droite, la "Nouvelle UDF" de François Bayrou se sauvant qu'une vingtaine de sièges. Les scrutins locaux qui suivront verront le retour de la domination du Parti socialiste sur l'ensemble de la gauche, contribuant à faire converger peu à peu le système partisan vers une situation de bipartisme, bien que l'audience conservée par les petits partis (le PCF, les verts et le PRG à gauche, l'UDF à droite), oblige les deux grandes formations à continuer d'évoluer dans le cadre de la bipolarisation.

Lors de l'élection présidentielle de 2007, l'UMP et le PS se renforcent et leurs candidats s'affrontent au second tour dans un schéma plus traditionnel qu'en 2002. Les petits candidats à la gauche du PS sont laminés, obtenant des scores très inférieurs à ceux qu'ils avaient obtenu en 2002. Jean-Marie Le Pen, concurrencé par la campagne très marquée à droite du candidat de l'UMP Nicolas Sarkozy, s'effondre à 10,4 %, tandis que François Bayrou, en axant son discours sur une volonté de former une formation centriste indépendante de la droite, atteint plus de 18 % des voix. La transformation de l'UDF en Mouvement démocrate (MoDem) ne permettra toutefois pas de concrétiser ce succès, le nouveau parti obtenant un score nettement plus faible lors des législatives qui suivront (7,6 %). En revanche le FN s'effondre à nouveau et, avec 4,3 %, n'est définitivement plus en mesure de peser sur le débat politique. Les scrutins locaux qui suivront confirmeront cette tendance. Le paysage politique français s'organise actuellement sur la base d'une opposition gauche/droite très marquée, avec deux grands partis forts, l'UMP et le PS, qui composent avec plusieurs petits partis proches de leur sensibilité. Les dernières élections municipales ont été le théâtre d'une marginalisation du FN, de l'obtention de bons scores pour des listes menées par des partis de gauche autre que le PS (PCF, verts et surtout Ligue communiste révolutionnaire (LCR) dans plusieurs grandes villes) ainsi que de l'échec de la stratégie d'autonomie du MoDem, contraint de composer avec le PS ou l'UMP lorsque c'est possible, battu lorsqu'il présente des listes autonomes.

Les élections européennes de 2009, marquées par un taux de participation historiquement faible de 40,6 %, ont été l'occasion de plusieurs reclassements sur l'échiquier politique français. L'UMP s'est imposée avec près de 28 % des voix, loin devant le PS qui, à 16,5 %, subit la concurrence d'Europe écologie, rassemblement écologiste initié par les verts (16,3 %). Ce scrutin fut également l'occasion d'une nouvelle déconvenue pour le MoDem qui n'obtient qu'un piètre score de 8,5 %, très en dessous de la performance réalisée par l'UDF en 2004. À gauche, le Front de gauche initié par le Parti communiste et le nouveau Parti de gauche fondé par d'anciens membres de la minorité du PS obtient 6,5 % des voix, tandis que l'extrême-gauche se renforce considérablement, avec la percée du Nouveau Parti anticapitaliste (NPA), successeur de la LCR, qui obtient 5 % des voix (Lutte ouvrière est à 1,2 %). Le FN, à qui profite normalement ce type d'élection, demeure faible à seulement 6,3 % des voix, devant les listes Libertas alliant le Mouvement pour la France et CPNT, à 4,8 %, là aussi en net reflux par rapport aux scores obtenus par le seul MPF lors des scrutins précédents. Des courants minoritaires, en particulier l'Alliance écologiste indépendante et la formation gaulliste « Debout la République », obtiennent en outre des scores significatifs (respectivement 3,6 et 1,8 %).

Les élections régionales de 2010 sont remportées par la gauche qui gagne 21 des 22 régions métropolitaines. Cela est principalement dû à un retour du Front national qui réalise 11,4 % des suffrages et s'est maintenu dans douze régions. Les élections cantonales de 2011 sont aussi gagnées par la gauche qui totalise 49 % des voix. L'UMP est la grande perdante de cette élection, puisqu'elle ne représente que 17 % des voix. Le Front national, quant à lui poursuit son ascension et réalise 15 % des voix. Désormais, tous les regards sont portés vers l'élection présidentielle de 2012.

On observe une participation de plus en plus faible des électeurs aux différents scrutins, particulièrement depuis les dernières législatives de 2007, municipales de 2008, européennes de 2009, régionales de 2010 ou cantonales de 2011 où l'abstention a connu des chiffres jamais atteints sous la République. Seule l'élection présidentielle semble résister à ce phénomène récurrent.

Aux élections sénatoriales de 2011, alors que le Sénat est désormais renouvelé de moitié, la gauche progresse de vingt-cinq sièges, et détient désormais la majorité absolue à la haute assemblée, avec 177 sénateurs contre 171 à la droite.

Le premier tour de l'élection présidentielle de 2012 voit le candidat du PS François Hollande arriver en tête devant le président sortant Nicolas Sarkozy ; les observateurs notent également les 18 % de la candidate FN Marine Le Pen, qui maintient son parti comme troisième force politique nationale. Au second tour, François Hollande l'emporte sur Nicolas Sarkozy, par 51,6 %. Le 17 juin suivant, à la suite des élections législatives, le Parti socialiste devient majoritaire à l'Assemblée nationale. Ainsi, pour la première fois sous la République la gauche est majoritaire dans toutes les institutions (Présidence, Parlement, Conseils régionaux, Conseil généraux, Conseils municipaux).

La forte poussée du FN lors des scrutins suivants conduit les observateurs à évoquer, surtout à partir des élections européennes de 2014, une « tripartition » du système politique, expression créée dès 1997 par Gérard Grunberg et Étienne Schweisguth à propos de l’élection présidentielle de 1995 ; le politologue Joël Gombin remet en cause cette notion en soulignant que , et lui préfère celle de .

Au pouvoir, le PS devient très impopulaire et lors des élections de 2017, marquées par des affaires politico-financières et par l'émergence de nouvelles forces politique comme La France Insoumise (gauche radicale) ou La République en Marche ! (centre), Emmanuel Macron devient président de la République et forme un gouvernement contenant des ministres venus de gauche, de centre et de droite. 




Sous l'Ancien Régime, c'est-à-dire avant la Révolution française, la France était une monarchie.

Dynasties royales :

Régimes politiques en France depuis la chute de la monarchie absolue :




</doc>
<doc id="14221" url="https://fr.wikipedia.org/wiki?curid=14221" title="Démographie de la France">
Démographie de la France

La démographie de la France est l'ensemble des données et études concernant la population de la France à toutes les époques. Ces données sont notamment calculées par l'Institut national de la statistique et des études économiques (Insee).

Au , le nombre de personnes habitant en France est estimé à 67,2 millions, dont 65 millions en France métropolitaine et 2,2 millions dans les départements et régions d'outre-mer (DROM). Ce chiffre n'inclut pas les quelques des collectivités d'outre-mer (COM) et de Nouvelle-Calédonie.

En mars 2017, la population de la France dépasse officiellement la barre des 67 millions d'habitants. La barre des 66 millions a été dépassée au début de l'année 2014.

Entre les années 2010 et 2017, la population française est passée de à , soit une augmentation d'environ personnes sur une période de , faisant de la France l'un des pays européens les plus dynamiques. La population de la France croit d'environ un million de personnes tous les . Pour une croissance annuelle moyenne de , soit un taux de croissance de . 

Au , 11,6 % d’entre eux, soit , sont nés à l’étranger, 8,9 %, soit , sont immigrés et 6,4 %, soit , sont de nationalité étrangère. D'autre part, près de de personnes nées en France vivraient à l’étranger.

L'indicateur conjoncturel de fécondité est de 1,93 en 2016, contre 2,9 en 1950 et 1,8 en 1990. Quoique supérieur à la moyenne européenne, le taux de fécondité est insuffisant pour assurer le renouvellement des générations. La population continue toutefois d'augmenter en raison de la forme de la pyramide des âges, de l'immigration et de l'augmentation de l'espérance de vie, qui s'élève en 2017 à pour les femmes et pour les hommes, soit une hausse d'une dizaine d'années en cinquante ans.

La France compte d'habitants au (). 

En 2015 la population a augmenté de , et en 2016 de en comptant le solde migratoire. Ces deux dernières années la population a augmenté à un rythme moins soutenu en comparaison aux années précédentes, en 2013 et en 2014. En 2006, la population a augmenté de 

L'évolution probable de la taille et de la structure de la population fait l'objet d'une projection tenant compte des tendances actuelles de l'évolution de la population avec comme année de référence 2015 :

La population totale de la France (DROM inclus) devrait atteindre au et d'habitants en 2080.

Les chiffres suivants concernent la France y compris les DROM (Mayotte depuis 2014) :
En 2015, sont nées en France métropolitaine ou dans les départements d'outre mer. L'indicateur de fécondité se situe à par femme. À nouveau en baisse, la fécondité atteint par femme en 2016. Ce chiffre est nettement supérieur à la moyenne de l'Union européenne (1,6), et a augmenté depuis le début des années 1990, période où il avait atteint son minimum de 1,6. Toutefois, l'indice de fécondité reste nettement inférieur à ce qu'il était de 1946 à la fin des années 1960 et a baissé en 2015 : il était proche de 3 en 1960. Il est aussi légèrement insuffisant pour assurer le renouvellement des générations. Les femmes nées hors de l'Union européenne présentent un indice de fécondité de 3,2, très supérieur à celui des femmes nées en France.

Les chiffres suivants concernent la France y compris les DROM (Mayotte depuis 2014) :
Ces chiffres sont globalement stables depuis la fin des années 1980. 17,2 % des femmes sexuellement actives avaient recours à l'IVG au cours de leur vie en 2007.

En 2016, 30,4 % des nouveaux-nés en France métropolitaine ont au moins un parent né à l'étranger (22,4 % en 2000), quelle que soit sa nationalité, dont 26,5 % un parent né hors de l'Europe des Vingt-Huit (18,3 % en 2000) et 2,5 % ont au moins un parent né dans un DOM-COM.

Avec en 2013, le taux brut de mortalité était en de , en augmentation par rapport aux années précédentes.

Le taux de mortalité infantile, c'est-à-dire le taux d'enfants décédés avant un an, était de en 2013 ( pour la France métropolitaine).

En France, les maladies infectieuses sont la troisième cause de mortalité, derrière le cancer et les maladies cardio-vasculaires. Parmi ces maladies infectieuses :

En 2014, l'espérance de vie à la naissance est supérieure à , avec une différence relativement importante entre les sexes : pour les femmes contre pour les hommes. Entre 1994 et 2014, l'espérance de vie à la naissance d'une personne de sexe masculin née et vivant en France a gagné un peu plus de six ans, tandis que celle d'une personne de sexe féminin née et vivant en France a gagné quatre ans. L'écart entre les espérances de vie à la naissance des deux sexes, qui avait tendu à s'accroître de 1949 ( en France métropolitaine) au début des années 1990 ( en France métropolitaine) tend donc à se réduire : il était de en 2014.

Il existe des différences significatives entre les régions : en 2009, l'espérance de vie s'élevait à près de en Île-de-France contre dans le Nord-Pas-de-Calais.

Il existe également des différences significatives entre les classes sociales : un ouvrier vit de moins qu'un cadre supérieur.

Selon Cris Beauchemin, chercheur à l'INED, on peut estimer en 2018 que deux personnes sur cinq (soit 40 % de la population vivant en France) sont issues de l’immigration sur trois générations .

Selon la définition du Haut Conseil à l'Intégration (HCI) de 1991, est immigrée . Cette définition est spécifique à la France car la plupart des autres pays, ainsi que des organisations internationales (OCDE, Commission européenne) considèrent comme immigrée toute personne née à l'étranger quelle que soit sa nationalité de naissance. Dans cette définition internationale, les Français de naissance nés à l'étranger (rapatriés d'Algérie, Harkis), qui représentent une part importante des personnes nées à l'étranger, sont considérés comme immigrés. Dans la définition française, la population non-immigrée est composée, d'une part, des Français de naissance, quel que soit leur lieu de naissance, d'autre part, des étrangers de naissance nés en France.

Par sa situation géographique qui en fait un lieu de croisement des commerces et des populations, puis par son histoire d'ancienne puissance coloniale, la France est un pays de migration de longue date. À partir du milieu du , la part des étrangers dans la population s'accroît, passant de 1 % en 1851 à 2,9 % en 1886, puis se stabilisant jusqu'à la Première Guerre mondiale (3 % en 1911). Le mouvement s'accélère après 1918, en raison des besoins en main-d'œuvre consécutifs au traumatisme démographique de la guerre, qui a décimé les jeunes adultes alors qu'arrivent sur le marché du travail les classes d'âge peu nombreuses nées à la fin du (4 % d'étrangers en 1921, 6,6 % en 1931), se tasse des années 1930 au début des années 1950, puis reprend à partir de la fin des années 1950. Selon l'INED, près de 14 millions de personnes vivaient en France en 1999 en étant immigré ou en ayant un parent ou grand-parent immigré, soit 23 % de la population. Gérard Noiriel estimait en 2002 cette proportion à environ un tiers si l'on remonte jusqu'aux arrière-grands parents. En 2015, selon Pascal Blanchard, entre 12 et 14 millions de Français, soit entre 18 et 22 % de la population totale, ont au moins un de leurs grands-parents né dans un territoire non européen.

En 2010, la France accueille, selon la définition internationale des Nations unies (), 7,2 millions d’immigrés soit 11,1 % de la population dont (7,8 %) nés hors de l'Union européenne. Elle se classe au sixième rang mondial, derrière les États-Unis (), la Russie (12,3), l'Allemagne (9,8), l'Arabie saoudite (7,3), le Canada (7,2) mais elle devance en revanche le Royaume-Uni (7,0), l'Espagne (6,4) et l'Italie (4,8). En proportion de sa population totale, la France (11,1 %) se situe au de l'UE 27, derrière le Luxembourg (32,5 %), Chypre (18,8 %), l'Estonie (16,3 %), la Lettonie (15,3 %), l'Autriche (15,2 %), la Suède (14,3 %), l'Espagne (14 %), l'Irlande (12,7 %), la Slovénie (12,4 %), l'Allemagne (12 %), le Royaume-Uni (11,3 %), et à égalité avec la Grèce (11,1 %) et les Pays-Bas (11,1 %). 

La France, en raison de l'histoire plus que séculaire de l'immigration vers son territoire, est également l'un des pays de l'Union européenne qui compte proportionnellement le plus de personnes issues de l'immigration ( et ) parmi les personnes âgées de avec 13,1 % d'immigrés et 7,7 % d'enfants de couple mixte et 5,3 % de deux parents immigrés, soit un total de 26,6 %, devant notamment le Royaume-Uni (24,4 %), les Pays-Bas (23,5 %), la Belgique (22,9 %), l'Allemagne (21,9 %) et l'Espagne (20,2 %), mais moins que le Luxembourg (61,9 %), l'Estonie (35,6 %) et la Lettonie (29,2 %).

Selon la définition française (), la France métropolitaine comptait en 2008, d’immigrés, soit de plus qu’en 1999 et 8,3 % de la population totale. 40 % d’entre eux avaient la nationalité française, qu’ils ont pu acquérir par naturalisation ou par mariage. D'autre part, de personnes nées françaises à l'étranger (dont les rapatriés des anciennes colonies) ne sont pas incluses dans ce total d'où la différence avec le chiffre de donné par Eurostat qui inclut également ces personnes.

Au début de l'année 2013 les immigrés vivants en France métropolitaine venaient principalement d'Afrique (43,8 % dont 30,2 % du Maghreb), de l'Europe (37,4 % dont 32,8 % de l'UE27) et enfin de 14,3 % du reste du monde.

Sur l'année 2012, sont arrivés en France. Ils venaient d'Europe (46 %), d'Afrique (30 %), d'Asie (14 %) et enfin d'Amérique et d'Océanie (10 %). 

Les enfants d’immigrés, descendants directs d’un ou de deux immigrés, représentaient, en 2008, 6,7 millions de personnes, soit 11 % de la population. Trois millions d’entre eux avaient leurs deux parents immigrés.

Au total, immigrés et enfants d'immigrés (seconde génération) sont donc près de 12 millions en 2008, soit 19 % de la population. 

Sur trois générations, près de 40 % des nouveau-nés entre 2006 et 2008 ont au moins un grand-parent immigré dont 16 % un grand-parent immigré originaire du Maghreb. La moitié sont nés de couples mixtes.

Une étude conjointe de l'INSEE et de l'INED, précise le parcours familial des populations qui ont une histoire avec les migrations. La définition de celle-ci est encore différente de la définition des immigrés, car elle intègre en plus les Français nés à l'étranger, dans les DROM, les rapatriés ainsi que tous leurs descendants. Cette étude révèle que 30 % de la population métropolitaine âgée de 18 à 50 ans a un lien avec la migration sur deux générations (12 % des 18-50 ans sont nés à l'étranger ou dans un DROM et 18 % sont des descendants directs). On peut aussi y voir que seul 16 % de cette population n'a pas d’ascendance française.
Le tableau suivant montre la répartition par origine des immigrés et leurs enfants en 2008 selon l'Insee.

Le tableau suivant montre la répartition par classe d'âges des immigrés et leurs enfants en 2008 selon l'Insee.


Le nombre de personnes de nationalité française résidant hors de France est mal connu. Nombre d'entre eux sont enregistrés auprès d'un consulat, mais beaucoup ne le sont pas. Inversement, les personnes enregistrées auprès d'un consulat peuvent avoir quitté le pays ou être décédées sans que les autorités françaises n'en aient été informées.

Selon le ministère des Affaires étrangères, entre 2 et 2,5 millions de Français seraient établis à l'étranger. Environ Français étaient enregistrés auprès d'un consulat à l'étranger au 31 décembre 2015, dont près de la moitié (49,7 %) dans un autre pays d'Europe, notamment en Suisse (), au Royaume-Uni (), en Belgique (), en Allemagne () et en Espagne (). 19,7 % résidaient en Amérique, en particulier aux États-Unis () et au Canada (), 14,9 % en Afrique, 8,1 % au Proche et Moyen-Orient (dont en Israël et dans les Territoires palestiniens) et 8,1 % en Asie-Océanie.

La France – métropole et départements et régions d'Outre-mer (DROM) – est peuplée de 67 millions d'habitants en 2017.
La densité de population en métropole est de 118 habitants au km², ce qui est légèrement supérieur à la moyenne de l'Union européenne (114), mais assez faible comparé aux pays limitrophes (Espagne exceptée). Les espaces à très faible densité ont moins de 20 habitants au km².

La population française est inégalement répartie. Les régions de montagne – Alpes du Sud, Corse, Pyrénées et Massif central en particulier – mais aussi les plaines et les plateaux du Nord-Est (Ardenne, plateaux champenois et lorrains) et du centre de la France, ainsi que les régions rurales du Sud-Ouest, qui forment la « diagonale des faibles densités », ont des densités inférieures à 50 /km². Les grandes vallées fluviales de la Seine, du Rhône, de la Garonne, les franges de la mégalopole européenne (régions frontalières du Nord et de l'Est) et les littoraux concentrent les populations. Les départements insulaires d'outre-mer ont également de fortes densités (près de 250 /km² en Guadeloupe, plus de 300 en Martinique, à Mayotte et à La Réunion). De plus, dans chacun de ces espaces, les Français vivent majoritairement dans les villes.

La population aussi est de plus en plus mobile. En effet, on constate que les migrations entre les régions s'intensifient. L'Île-de-France a un taux de migration négatif. Les vieilles régions industrielles du Nord et de l'Est ont également un solde migratoire négatif. Les régions du Sud (notamment autour de Toulouse, Montpellier, Lyon), de l'Ouest (notamment autour de Rennes, Nantes, Bordeaux) et alpines ont un solde migratoire positif.

Cette répartition de la population s'explique par une concentration des activités économiques et des richesses dans les aires urbaines et sur les littoraux. L'urbanisation de la population française date du . La littoralisation est plus récente. La littoralisation est la concentration des activités et des populations sur le littoral. Le climat plus ensoleillé, l'attrait de la mer et l'image positive des littoraux atlantiques et méditerranéens et des régions alpines expliquent en partie ces dynamiques.

La métropolisation et la littoralisation sont donc aujourd'hui des caractéristiques principales de la répartition de la population française.

82 % des Français vivent dans une aire urbaine. Avec près de 12,5 millions d'habitants en 2014, l'aire urbaine de Paris domine largement les autres aires urbaines. Suivent les aires de Lyon, Marseille, Toulouse, Bordeaux, Lille et Nice, dont la population est supérieure à un million d'habitants. L'urbanisation du territoire s'accompagne d'un étalement urbain. En effet, la majorité des urbains vivent désormais dans les banlieues ou les couronnes périurbaines.

La périurbanisation a entraîné un essor important des mobilités. Les migrations pendulaires correspondent au déplacement de population matin et soir pour se rendre et revenir de leur lieu de travail souvent situé dans le centre-ville. Il a donc fallu aménager un réseau de transport pour répondre aux besoins des populations. Ces aménagements ne sont plus entièrement financés par les pouvoirs publics : les entreprises privées participent de plus en plus à leur réalisation. Les transports en commun sont moins polluants et permettent d'assurer un développement plus durable.

L'étalement urbain a aussi un impact sur les espaces ruraux. Les campagnes proches des villes sont devenues des espaces d'habitation et de loisirs pour les urbains. Par contre, les espaces ruraux éloignés des villes, peu desservis par le réseau autoroutier ou ferroviaire, voient leur population diminuer et ont des difficultés à développer des activités économiques pour assurer leurs revenus.

L'évolution de la structure de la population de la France métropolitaine depuis 1950 est la suivante :

En 2014, la France comptait environ de femmes et d'hommes, soit 51,45 % de femmes pour 48,55 % d'hommes.

Le tableau ci-dessous indique le lieu de naissance des personnes résidant en France métropolitaine en 2012 avec également leur nationalité à la naissance (française ou étrangère). 
"* : Nombre d'immigrés selon la définition de l'Insee (né étranger à l'étranger)

"Note: Ce tableau inclut aussi bien des Français de naissance nés à l'étranger, par exemple les Pieds-noirs nés en Algérie française, ou dans un DOM (colonne « Né français ») que des immigrés, nés étranger à l'étranger selon la définition de l'Insee (colonne « Né avec une autre nationalité
»). Il est donc différent des tableaux liés à l'immigration qui n'incluent que les immigrés (nés étranger à l'étranger)."

"Lecture : 11,6 % des personnes résidant en France métropolitaine en 2012 sont nées à l'étranger. Parmi ces personnes 2,7 % sont nées française et 8,8 % sont nées avec une autre nationalité. Ce dernier chiffre qui représente de personnes correspond au nombre d'immigrés selon la définition de l'INSEE (né étranger à l'étranger). 8,4 % des personnes résidant en France métropolitaine en 2012 sont nées hors d'Europe ou dans un Dom. Parmi ces personnes 2,9 % sont nées française et 5,5 % sont nées avec une autre nationalité."

Les chiffres suivants concernent la France y compris les DROM (Mayotte depuis 2014), hormis pour le taux de nuptialité qui concerne uniquement la France métropolitaine :

En 2016, mariages ont été célébrés, dont entre personnes de sexe différent et entre personnes de même sexe. Le nombre de mariages, qui baissait de manière quasi continue depuis le pic de l’an 2000, malgré quelques années de pause (2005, 2010 ou 2012), est relativement stable depuis 2013.

En 2015, pacs ont été conclus, soit de plus qu’en 2014. Après avoir atteint un niveau record en 2010, le nombre de pacs avait baissé en 2011, pour la première fois depuis sa création fin 1999, de façon concomitante avec l’aménagement de la fiscalité. Depuis, le nombre de pacs conclus augmente continûment. En 2015, quatre pacs ont été conclus pour cinq mariages célébrés, contre un pacs pour cinq mariages dix ans plus tôt.

En 2015, divorces ont été prononcés, en baisse de plus de 8 % par rapport à 2010. Plus de la moitié l’ont été par consentement mutuel. Entre 1950 et 1970, le nombre de divorces était relativement stable et s’établissait à par an en moyenne. Durant les quinze ans qui ont suivi, de plus en plus de couples mariés ont divorcé. La nouvelle législation de 1975 sur le divorce, qui introduit notamment le divorce par consentement mutuel, a accéléré le mouvement déjà en cours. En 1986, divorces ont été prononcés. S’en est ensuivi une période de relative stabilité, jusqu’au début des années 2000. Une nouvelle augmentation du nombre de divorces s’est amorcée en 2003, avec un pic en 2005 ( divorces). 2005 est en effet l’année qui suit l’adoption de la loi visant à simplifier les procédures de divorce. Le phénomène est à peu près stabilisé en 2007, année où l’on enregistre autant de divorces qu’en 2004. Par la suite, le nombre annuel de divorces tend à diminuer légèrement, de divorces en 2010 à en 2014. En 2015, la baisse du nombre de divorces s’interrompt.

D'après les deux études les plus récentes, réalisées par les instituts Ifop en 2011 et CSA en 2012 :

Le recensement de la population en France permet d'établir le nombre d'habitants légal de chaque commune française. Ce nombre est indispensable à l'application de 351 articles législatifs de 28 codes différents. En particulier, il permet le calcul :
Ce nombre est également nécessaire à la gestion des communes pour :





</doc>
<doc id="14222" url="https://fr.wikipedia.org/wiki?curid=14222" title="Téléphonie en France">
Téléphonie en France

La téléphonie en France est le réseau de télécommunication téléphonique français.
L'autorité administrative indépendante qui régule le marché est l'ARCEP.

Vers 1987 le premier livre vert pose les bases de la future ouverture à la concurrence des télécommunications en France. Il insiste notamment sur :


Les recommandations sont arrêtées en 1988 par l'Europe et sont répercutées en France dans les deux ans.

Dès 1989 les réflexions commencent pour ouvrir les télécommunications. Une enquête publique (le rapport Prévost) est conduite sur les PTT. Le les PTT sont scindées en deux entités distinctes : La Poste (pour le courrier) et France Télécom (les télécommunications). FT est effectivement créé le , c'est une société à conseil d'administration sans patrimoine donc non privatisable. Les fonctionnaires conservent leur statut et les clients doivent s'adresser au tribunal de commerce au lieu du tribunal administratif.

En 1990 l'open network provision est créée par une directive européenne et établit les bases du marché des télécommunications :


Le est créée la direction générale à la règlementation (DGR) qui est l'embryon de la future Autorité de régulation des communications électroniques et des postes (ARCEP).

En 1994, la directive européenne sur le satellite a peu d'impact, car la concurrence est déjà ouverte. Elle est suivie en 1996 de la directive sur la téléphonie mobile qui, là encore, a peu d'impact, puisque les sociétés Cegetel et Bouygues mettent déjà en place leur réseau (respectivement, SFR et Bouygues Telecom), en concurrence directe avec France Télécom (et son réseau Itinéris).
Enfin, la directive d'ouverture à la concurrence impose l'ouverture complète du marché au , permettant de réellement ouvrir les marchés (notamment les communications nationales et locales). Elle est mise en place par la loi du et dispose que les activités de télécommunications doivent s'exercer librement (bien que toujours soumises à des licences). Elle crée l'ART devenue ARCEP (mise en place le ) et l'Autorité nationale des fréquences.

Les opérateurs construisent leur réseaux de différentes façons :

La SNCF et Cegetel créent Télécom Développement. La société ferroviaire apporte ses fibres optiques et Cegetel les compléments de financements pour étendre ce réseau. La commercialisation grand public se fera par l'intermédiaire du "7 de Cegetel" dès le . 

Bouygues panache son réseau avec différentes offres puis s'associe à la société LDCom.

La France a prévu de fermer progressivement son réseau de téléphonie RTC, pour le remplacer par un réseau internet basé sur la fibre optique FTTH et sur l'ADSL, et par de la téléphonie sur Internet (VoIP).

La desserte permettant d'atteindre l'utilisateur final est essentiellement composée du réseau cuivre appartenant à France Télécom. Ce gigantesque réseau représente la plus grande valeur de la société. Toutefois, à sa privatisation, elle a conservé sa dette.

Des licences pour la Boucle Locale Radio (technologie WiMAX) sont attribuées par l'ART en 1999 à deux opérateurs américains. Le succès est mitigé, en tout cas restreint à certains usages et à certaines zones. Bolloré avait gagné des licences dans douze régions, Maxtel dans 13, France Telecom dans deux, et des conseils régionaux dans six.

Source Arcep

Le secteur des télécommunications a perdu emplois de 1998 à 2004 et emplois de 2004 à 2015, représentant environ emplois perdus en 17 années sur un secteur qui en comptait , soit une perte de l'ordre de 30 %.

Le secteur des Télécommunications en France a transféré une partie de ses revenus depuis le secteur de la téléphonie, vers le secteur de l'Internet, ce qui a permis de faire baisser les factures téléphoniques.

Avec l'ouverture à la concurrence, le coût des communications téléphoniques varie d'un opérateur à l'autre et pour un même opérateur en fonction du type d'abonnement.

Les communications de mobile à mobile nationales les moins chères étaient en 2012/2013 d'environ trois centimes la minute (Free Mobile et quelques MVNO), soit un prix proche de celui d'une communication émise depuis un téléphone fixe. Certains opérateurs facturaient les mêmes communications jusqu'à quarante centimes la minute.

Exemple de tarifs à la minute pour les particuliers en métropole :



</doc>
<doc id="14225" url="https://fr.wikipedia.org/wiki?curid=14225" title="Kourou">
Kourou

Kourou est une commune française, située dans le département de la Guyane. Avec habitants en 2010, Kourou est la quatrième commune la plus peuplée de ce département d’outre-mer (DOM) derrière Cayenne, Saint-Laurent-du-Maroni et Matoury. 

Autrefois connue pour son bagne, elle est actuellement surtout réputée pour abriter le Centre spatial guyanais (CSG), locomotive économique de toute la Guyane.

Commune située au nord-est de l’Amérique du Sud, sur le littoral guyanais, Kourou est localisée à l’embouchure de son fleuve éponyme. Derrière la ville se trouvent quatre monts : la Carapa, le Pariacabo, la montagne Café et la montagne Lombard. Elle est parsemée de trois lacs : le Bois Diable, le Marie-Claire, et le Bois Chaudat. Les environs sont un mélange de savane sèche et inondée (cette dernière appelée "pripri"), ainsi que de forêt tropicale. Elle est localisée à au nord-ouest de la préfecture du département, Cayenne.

De longues plages de sable bordent la côte Atlantique ; elles sont délimitées au nord par des mangroves et au sud par le fleuve Kourou. Ces plages ne sont pas orientées nord-ouest comme sur le reste du littoral (à l’exception de celle à l’embouchure du Mahury) à cause des rochers de la Pointe des Roches. Le reste de la côte, comme ailleurs en Amazonie, voit un apport constant de sédiments dû aux nombreux fleuves amazoniens (dont les eaux sont d’ailleurs brunes pour cette même raison), surtout de l’Amazone lui-même. Les sédiments s’accumulent sur la côte, formant de longues étendues de vase qui sont colonisées par les palétuviers, formant des mangroves ; les étendues de vase et les mangroves reculent et avancent selon les orages de la saison des pluies, les marées, les houles, et les nouveaux apports de sédiments. La configuration de la côte change donc d’année en année.

Kourou est située dans une zone sismique d’intensité faible ; des tremblements de terre peuvent survenir dans le Nord-Est de l’Amérique du Sud et dans les Guyanes causés par les contacts entre les plaques tectoniques sud-américaine et caraïbe. Une secousse d’une magnitude de 5,2 a été ressentie le 8 juin 2006. Kourou, possédant le port le plus proche, est le point de départ des excursions aux îles du Salut, situées à dix kilomètres de la côte.

Le climat de Kourou est équatorial humide avec alternance de saisons sèches et humides : la petite saison des pluies de la mi-décembre à mars, la petite saison sèche en mars, la grande saison des pluies de la fin mars à juillet, et la grande saison sèche de juillet à la mi-décembre. La pluviométrie annuelle moyenne est de mm, et l’insolation peut atteindre les heures annuelles, voire plus. L’année 2003 fut très sèche en Guyane et en Amazonie en général ; la saison des pluies fut déficiente mais les années postérieures virent une pluviométrie dans la moyenne, diminuant les effets de la sécheresse.

La température moyenne est de (avec un minimum d’environ et un maximum de , voire plus durant la saison sèche). L’humidité moyenne oscille entre 80 et 90 % ; elle varie de 75 à 98 % pendant la saison des pluies. Pendant la saison sèche elle est d’environ 50 % en début d’après-midi et de 100 % tôt le matin vers 6 h.

En raison de la situation de la zone intertropicale de convergence qui se positionne bien plus au nord que le département, il n'y a aucun risque qu'un ouragan touche la côte guyanaise. Pas plus que dans le reste de la Guyane, les vents n’y sont violents. La vitesse maximale jamais enregistrée par la station météo du CSG depuis son installation en 1968 est de /h (soit /s). Les alizés y sont fréquents, diminuant la présence des moustiques omniprésents dans l’intérieur du département.

Kourou étant une ville nouvelle, construite avec beaucoup d’espaces verts et d’immeubles ne dépassant pas les trois étages, la faune et la flore amazonienne sont souvent vues en ville, au contraire d’autres villes guyanaises plus urbanisées, dont la capitale, Cayenne.

La faune de Kourou est très variée et typique du littoral amazonien : on peut voir dans les environs des agoutis, des tatous, des tapirs du Brésil, des porcs-épics brésiliens, des cabiaïs, des pakiras, des jaguars.

Les mangroves et les étendues de vase abritent de nombreuses espèces de crustacés, dont de nombreuses variétés de crevettes, dont la pêche est une industrie importante sur toute la côte guyanaise. Elles abritent aussi des crabes, des urubus noirs, des aigrettes (aigrettes neigeuses, grandes aigrettes et plusieurs autres espèces), des hérons (hérons bihoreaux...), des ibis rouges et ibis blancs, des spatules rosées, des bécasseaux. Les palétuviers les plus souvent rencontrés sont ceux des genres "Avicennia" ("Avicennia germinans"), "Rhizophora" ("Rhizophora mangle" et "Rhizophora racemosa"), et "Laguncularia" ("Laguncularia racemosa"). On peut voir des lamantins des Caraïbes aux alentours de la Pointe des Roches lors de la marée haute.

En ville on trouve surtout des iguanes communs, des lézards, des caïmans dans les lacs, parfois des serpents à sonnettes, des matoutous ("avicularia versicolor"), des couleuvres, et de nombreuses espèces de tortues ainsi que d’oiseaux (dont les ibis blancs, souvent présents sur les rives des lacs). La ville ayant été autrefois entièrement entourée de mangroves et construite sur un marais, de temps en temps elle se voit envahie de papillons de cendre, qui de leurs ailes blanchâtres dégagent une fine poussière très urticante qui peut déclencher de fortes réactions allergiques, appelées papillonite. Quand ces nuées de papillons apparaissent, on éteint tout l’éclairage public pour ne pas les attirer sur la ville. Il n’y a que les lampes rouges qui ne les attirent pas. On allume alors de forts projecteurs sur un bassin ou un lac pour y noyer les papillons. Les épisodes de papillonite sont beaucoup moins fréquents depuis que les légionnaires ont abattu beaucoup de palétuviers des mangroves.

L'histoire de Kourou est bien longue et commence des milliers d’années avant l’arrivée des Européens. Les Amérindiens ayant une culture orale, il est pratiquement impossible de retracer les événements antérieurs à la colonisation. Même après, la région étant colonisée et abandonnée plusieurs fois tout au long de son histoire, les sources restent clairsemées, fragmentaires et se contredisent parfois. Les rares chercheurs étudiant l’histoire de la région s’appuient sur les fouilles archéologiques et sur les quelques documents d’époque, dont des récits de voyage et des documents officiels rédigés pour la plupart dans la lointaine "« Métropole »".

Les amérindiens Kali'na prédominaient dans la région avant la venue des Français, jusqu’à la fin du . Les fouilles archéologiques menées avant la construction du barrage de Petit-Saut sur la Sinnamary, à quelques kilomètres au nord de Kourou, ont permis la découverte de traces de présence amérindienne vieilles de deux mille ans.

Il existe un site près de la ville, appelé "« les Roches Gravées »" où l’on peut voir des exemples d’art rupestre amérindien. Il se situe à quelques centaines de mètres du pied du mont Carapa, à l’arrière de la zone industrielle de Pariacabo que l’on traverse pour rejoindre le pont du Kourou sur la route de Cayenne. Ce sont les explorateurs français Henri Coudreau et Jules Crevaux qui sont les premiers à mentionner les gravures amérindiennes en Guyane, au début du , mais on doit attendre 1955 pour qu’un chasseur de papillons, Eugène Le Moult, ne redécouvre les roches de Carapa. Sa trouvaille sera à nouveau oubliée pendant plusieurs années, jusqu’à ce qu’un chercheur du CSG, Yves Dejean, tombe sur une vieille carte de la région mentionnant les roches. Il les retrouve, et le mont Carapa étant sur propriété du CSG, celui-ci décide de mettre en valeur le site et y construit des carbets pour les protéger des intempéries.

En 1500, l’explorateur espagnol Vicente Yañez Pinzon longe les côtes guyanaises et passe devant l’emplacement actuel de Kourou. En 1645 deux frères capucins s’installent à Kourou avec un domestique. Les frères servent de médiateurs avec les Amérindiens Palikour, qui sont alors en guerre contre les Français. Cette année-là, le petit établissement est attaqué par des guerriers palikours. Le fort de Cépérou à Cayenne n’a que 25 hommes ; quand les renforts (forts de 40 hommes) de la Compagnie de Rouen du sieur Poncet de Brétigny arrivent, la petite colonie guyanaise est presque entièrement décimée. La plus grande partie des survivants partent aux îles du Salut, inhabitées et à l’époque encore connues sous le nom des « îles du Diable », mais 16 d’entre eux, dont les deux frères capucins, pour des raisons aujourd’hui inconnues, décident de s’installer au Mahury. Ils sont massacrés par les Amérindiens six semaines plus tard ; il n’y a que deux survivants, des jeunes qui se réfugient dans le fort Cépérou, y sont retrouvés par des Amérindiens onze jours après le massacre et bien traités. L’un d’eux, appelé « Le Vendangeur », sert plus tard d’interprète entre les Blancs et les Amérindiens.

En février 1665 le navire "« La Suzanne »", sous le commandement du capitaine Baron de la Compagnie de la France équinoxiale, arrive à Cayenne. La colonie compte alors habitants y compris la petite garnison, 40 femmes blanches et 200 esclaves. Les Français, alors en paix avec les Amérindiens, construisent un poste à « Caourou » (Kourou) et des forts à Sinnamary et Camoripo (aujourd’hui la Montagne d’Argent) à l’embouchure de l’Oyapock. Le , le roi de France déclare la guerre aux Anglais ; la petite colonie de Guyane, faiblement peuplée et mal défendue, est sous la menace d’attaques des Anglais et des Hollandais, tous les deux ayant des colonies proches.

En 1744, le physicien et géographe La Condamine, chargé de conduire une expédition au Pérou afin de mesurer la longueur d’un arc de méridien d’un degré à proximité de l’équateur, passe par la région kouroucienne et donne son nom à l’un des monts derrière Kourou, avant de rejoindre Cayenne.

Le père jésuite Pierre Aimé Lombard, né à Lyon en 1678, part pour Cayenne le après avoir enseigné les humanités en France ; il débarque à Cayenne le 12 juin et y prononce ses vœux en 1711. Il devient, avec le père Simon Ramette, l’un des plus importants personnages de la colonie. Les deux hommes décident d’évangéliser les Amérindiens ; ils se mettent à apprendre le kali'na, langue de la tribu du même nom, avec un dictionnaire et un livre sur la grammaire de cette langue. Ils installent leur petite mission sur le Carouabo à côté d’un important village amérindien. Le premier contact est, selon Lombard, « pénible » ; ils n’avancent guère dans leur travail pendant huit mois.

Ils changent alors de méthode : choisissant les Amérindiens qu’ils jugeaient être les plus « intelligents », ils les instruisent dans leur propre langue, le kali'na. Les pères jésuites font baptiser quinze Kali'na à l’église Saint-Nicolas de Cayenne en décembre 1710, en grande pompe (le gouverneur, Rémy Guillouet d’Orvilliers, ainsi que d’importants officiers, en sont les témoins). Cette stratégie rencontre plus de succès, puisque l’année suivante 80 Kali'na demandent à être baptisés. Ramette est appelé à Cayenne en 1712. Lombard reste à Kourou pour y éduquer les enfants ; à cette époque il a 300 fidèles. Il fait construire une petite église et met en valeur les terrains de Guatémala (petit bourg situé sur l’autre rive du fleuve Kourou). En 1730 le père Gaspard du Molard, coadjuteur et architecte, construit un très grand bâtiment incluant deux infirmeries (une pour les femmes, une autre pour les hommes), pour y soigner les Amérindiens, dont beaucoup d’agriculteurs et ouvriers.

Lombard décède près de sa mission en 1748, ayant éduqué et évangélisé environ Amérindiens. La Compagnie de Jésus est dissoute en 1762, la petite mission prospère est abandonnée et Kourou reste un petit hameau jusqu’à l’arrivée de l'expédition. Les bâtiments de la mission servent d’abri aux colons.

En 1763, la Grande-Bretagne prend possession de la Nouvelle-France en vertu du traité de Paris qui met fin à la guerre de Sept Ans. La Guyane apprend la nouvelle le avec l’arrivée de la frégate « La Diligente », qui met un temps record pour traverser l’Atlantique : seulement 32 jours. En route « La Diligente » passe devant un navire corsaire anglais mais ne s’arrête pas pour lui donner les nouvelles de la paix. Le 31 janvier un corsaire anglais, peut-être le même, attaque le petit hameau de Kourou à six heures du matin. Les Amérindiens s’enfuient, permettant aux Anglais de piller l’église, le presbytère, et quelques maisons. Les Amérindiens reviennent avec des fusils peu après, tuant un corsaire et faisant battre en retraite les autres. Le détachement envoyé sur place revient à Cayenne le 7 février avec un missionnaire à demi nu qui loue le courage des Amérindiens et demande des armes et munitions pour mieux défendre Kourou.

Ayant perdu une très grande partie de ses terres en Amérique, le gouvernement français décide d’envoyer une grande expédition en Guyane. Ce sera la seule tentative massive de colonisation volontaire de la Guyane. La naissance de ce projet tient à de multiples facteurs. Le premier est cartographique, puisque le gouvernement a la volonté de trouver une terre continentale, plus aisément défendable que les îles. La seconde, à l'absence d'une forte colonie en place.

L'exemple des colons de Guadeloupe, qui s'étaient ralliés aux Anglais figure à de nombreuses reprises dans la correspondance. Enfin, coïncidence heureuse, l'un des colons de Guyane, Brulletout de Préfontaine, se trouve depuis 1762 en France (exilé en raison de son comportement en Guyane), et propose un projet de colonisation de la partie nord, sur les rives du fleuve Saint Laurent. Cette proposition n'est pas retenue par le gouvernement qui comprend que le projet doit être un profond renouvellement des colonies existantes ; et principalement de l'esclavage et du commerce. C'est ainsi qu'il fait appel au gouverneur Turgot, mais aussi à l'ancien directeur de l'Académie de Bordeaux, l'intendant Chanvalon, à travers les nombreux mémoires qui discutent du statut de la colonie, la volonté d'une tolérance religieuse, d'une suppression du droit d'aubaine, d'une liberté de commerce avec les colonies voisines, et surtout de l'implantation d'écoles qui fixerait en le peuplement sont absolument novateurs et démontre l'innovation intellectuelle. Surtout, l'esclavage est banni. Ces mesures entrainent à terme la mort des colons de Guyane, mais surtout, remettent en cause toutes les pratiques de commerce des ports de l'atlantique français, Nantes, Bordeaux, versés dans le commerce transatlantique.

Près de personnes se seraient présentées à Strasbourg. Le gouvernement décidant de stopper les recrutements à partir de âmes. Environ à personnes, tentées par les légendes de l’Eldorado véhiculées par la propagande du gouvernement, débarquent à Kourou, après une longue et difficile traversée de 51 jours. Les ports de départ choisis sont Rochefort, Le Havre et Marseille ; le premier convoi quitte Rochefort en octobre 1763 et arrive à Kourou le 20 décembre. Les autres le suivent peu après. De décembre 1763 à février 1765 débarquent environ colons. Ils viennent de toute l'Europe : il y a des Rhénans, des Prussiens, des Autrichiens, des Suisses, des Néerlandais, des Belges, des Français (dont beaucoup d’Alsaciens), et même des Acadiens des colonies françaises de l’Amérique du Nord. Ils sont artisans, paysans et même prêtres (la seule religion officiellement permise étant le Catholicisme). Ces gens du Nord débarquent en Amazonie en pleine petite saison des pluies, aux embouchures du Kourou et de la Sinnamary. Le système de recrutement est particulièrement intéressant. Il s'appuie sur une pratique courante des états de recruter dans l'ancien Saint-Empire romain germanique des colons. Cette pratique est commune à George II puis George III, tout comme à Catherine II de Russie.

L'intendant nouvellement nommé pour l'expédition est Jean-Baptiste Thibault de Chanvalon, et le nouveau gouverneur de Guyane est le chevalier Étienne-François Turgot, frère de l’intendant et physiocrate lui aussi. Cependant, à leur arrivée une épidémie se propage d'abord parmi les enfants, puis les adultes, à toute la colonie qu'il soit colons venus d'Europe ou non. L'épidémie a de multiples sources : la saison des pluies, le retards des départs en raison des mauvais vents en France, la non vérification du contenu des embarquements (vivres, matériel, médecine) par les commissaires des ports, et aussi la volonté des autorités de l'expédition (l’intendant Chanvalon/ le commandant Préfontaine) de n'alerter que trop tard le Bureau des colonies de l'échec de l'entreprise. On estime que les trois quarts des colons sont décimés en peu de temps par les fièvres et autres maladies, l'épidémie toucha également l'ancienne colonie bien qu'il n'existe aucune liste des décès qui permette une analyse fine. Sur colons, en incluant les soldats et les naissances sur la terre guyanaise, décèdent avant le et de 2 à sont rapatriés. Ce taux de mortalité très élevé ne prend pas en compte les esclaves et les Amérindiens morts à cause des maladies que les Européens leur auraient transmises.

Jacques-François Artur, le médecin du roi en Guyane de 1738 à 1771, raconte soigner, dans toute la colonie, surtout des petites fièvres non identifiées jugées bénignes, des « fluxions de poitrine », des « maux hépatiques » (l’alcoolisme étant un grave problème dans toutes les strates de la société guyanaise), des maladies de peau (les irritants étant nombreux aux tropiques), et à Kourou surtout du paludisme (qui reste encore aujourd’hui un problème dans les climats tropicaux). La fièvre jaune emporte elle aussi beaucoup de colons, ainsi que la petite vérole, la typhoïde, le typhus et la dysenterie. Certains se réfugient aux îles du Salut (d’où leur nom ; avant, elles se nommaient les "« îles du Diable »"). Les îles, grâce aux alizés quasi permanents, n’ont presque pas de moustiques, porteurs de maladies. Ces îles servent également de déchargement pour les navires en provenance de France, car l’embouchure du Kourou ne permet pas à ceux ayant un trop fort tirant d’eau d’y entrer. Très peu de colons choisissent de rester en Guyane, on estime ce nombre à seulement . Le désastre notoire de cette expédition aboutit en France à un scandale qui se conclut par l’édiction de lettres patentes en 1767, dont l’une des mesures est l'emprisonnement de l’intendant Chanvalon dans différentes forteresses de France (Bastille, Mont Saint-Michel...), et l’exil de Turgot. Brûletout de Préfontaine, quant à lui, s’installe sur ses terres à Kourou. Le gouvernement de Louis XVI qui réhabilitera l'intendant plus de quinze années après les faits.

L'échec de l’expédition signe l'abandon de tout projet d'Amérique française et a nourri la légende noire de la Guyane. Le pays prend le surnom d"'« Enfer Vert »", dont il essaie encore aujourd’hui de se débarrasser et qui fut utilisé comme argument pour la construction du bagne. Aucune autre colonisation « de masse » ne fut jamais tentée, les politiques du étant il est vrai tournées vers l’Afrique et l’Asie, et ce dans une lutte tactique avec l’Angleterre. Aussi, la population blanche fut toujours très faible par rapport à celle des esclaves. Ce n’est qu’au milieu du avec l’abolition de l’esclavage, et l’installation du bagne, que la population augmente.

De 1795 à 1798 on envoie en Guyane des révolutionnaires Billaud-Varenne, Collot d'Herbois, puis, à la suite du coup d'État du 18 fructidor an V (4 septembre 1797) des prêtres réfractaires et des opposants politiques (dont Barbé-Marbois, Pichegru, Laffon de Ladebat...), les prêtres d'abord à Counanama et les seconds à Sinnamary. 328 hommes sont déportés, 187 morts sur place ou pendant leur évasion, 22 évadés, 11 établis en Guyane et 108 rapatriés par Bonaparte en 1800.

Les déportations de fructidor an V à Sinnamary et à Counanama ne sont pas du tout assimilables aux envois au bagne de Guyane qui leur ont succédé au . D'une part les déportés n’étaient pas des criminels condamnés légalement par un tribunal, mais seulement des ennemis politiques du moment déportés là sans aucun jugement ; d'autre part ils n'étaient pas incarcérés mais jouissaient d'une certaine liberté sous réserve de ne pas s’éloigner de leurs lieux de résidence forcée. Enfin à l’époque il existait pour les forçats de vrais bagnes installés dans des grands ports de France (Brest, Cherbourg, Le Havre, Lorient, Marseille, Toulon, Rochefort notamment) Il n’y avait alors aucun bagne en Guyane. Les bagnes portuaires parfois qualifiés de « galères à terre » avaient en effet remplacé les navires galères qu’on utilisait plus. Les déportations de fructidor servirent ensuite de modèle pour d’autres exils politiques puis pour des tentatives de « colonisation pénitentiaire » ; ce n’est qu’en 1854 qu’une loi créa officiellement le « bagne de la transportation » qui ne sera supprimé définitivement qu’en 1938. Il y eut un petit épisode curieux au début des années 1820 : le gouverneur Laussat, autrefois préfet colonial de la Louisiane, fait venir de cette région 20 Américains, issus de sept familles d’origine irlandaise ; ils débarquent le . Il rebaptise un petit coin de la région de Kourou "« Laussatdelphie »" en son honneur ; il s’agit de la savane Ollivier sur la crique Passoura, au nord de la ville actuelle. Les colons irlandais refusent de devenir agriculteurs (Laussat dira qu’ils manquaient de vocation agricole) et sont rapatriés en Louisiane un an plus tard. Laussat fera de même avec des Chinois et Malais envoyés à Kaw y faire cultiver du thé, projet qui se soldera aussi par un échec.

À l'époque les établissements à l’ouest de Kourou, Sinnamary, Iracoubo, à l’exception de Mana, sont sous le contrôle des Sœurs de Saint-Joseph de Cluny et abritaient des habitations (plantations) de deux à cinq personnes se consacrant à l’élevage et aux cultures vivrières. Kourou fut l’un des seuls établissements ayant un bourg où les habitants pouvaient se rassembler pour des fêtes ou d’autres évènements ; celui-ci se situe au port de Kourou, un peu avant l’embouchure du Kourou et de la Pointe des Roches. Le canal de Kourou est construit pour améliorer le transport de marchandises et la communication entre les habitations, souvent isolées par les marais ou la boue pendant la saison des pluies. L'éruption de la montagne Pelée en 1902 pousse un grand nombre de réfugiés à s’installer ailleurs, ravivant les espoirs de repeupler la Guyane, toujours beaucoup moins peuplée que les Antilles françaises. Les notables guyanais prévoient d’héberger 900 agriculteurs martiniquais ; le gouverneur était plus ambitieux, en prévoyant dans la région de Kourou. Finalement, seulement 572 choisissent de s’installer en Guyane, à Montjoly (à dix kilomètres de Cayenne) ; 317 individuellement et 255 en convoi organisé par le gouvernement. Des immigrants de Sainte-Lucie, de la Dominique, du Liban, de la Chine, et de l’Indonésie s’installent ailleurs en Guyane en petit nombre.

Kourou sert de lieu de déportation : elle est un pénitencier à vocation agricole, ce qui fait revivre le petit hameau. La ferme de Kourou produit de l'huile de coco et abrite une porcherie de 200 porcs, qui sert principalement à varier la nourriture donnée aux bagnards et au personnel pénitentiaire car on ne pouvait pas faire envoyer de la viande depuis la métropole.

Le pénitencier de Kourou, situé sur la Pointe des Roches, est inauguré en 1856. Trois bagnes forestiers en dépendent, à Pariacabo (aujourd’hui la zone industrielle), et les lieux-dits « Léandre » et « les Trois-Carbets ». Un chantier dans le quartier de Passoura occupe trente bagnards. Il y eut un projet de bagne pour femmes sur l’îlet le Père, au nord de Kourou, qui ne se réalisa jamais. Le pénitencier de Kourou, comme ailleurs en Guyane, a des problèmes de vol de vivres et une comptabilité mal tenue. Toutefois, les autorités semblent être plutôt tolérantes envers cette situation, peut-être parce que des Guyanais (généralement opposés au bagne) étaient à l’origine d’une partie des pertes. Il n'y a pas de médecin (à une époque c'est un Chinois transporté qui soignera les malades ; quand il n’y avait personne les malades étaient envoyés à l’hôpital de l'île Royale), et le personnel est composé surtout de bagnards Arabes âgés. Les vivres sont mal stockés : parfois, on ne trouve même pas les clés qui ouvrent les portes des magasins. On y récolte peu de légumes ; le principal produit devient le combustible, l’huile de coco. L'administration du bagne de Kourou est tellement désorganisée qu'elle oublie d'envoyer les actes de décès en 1885, chose grave puisque les familles en métropole en avaient besoin en cas d'héritages, de remariages...

À Kourou, on ne commence à chercher les bagnards en fuite que trente-six heures après le début de leur absence ; ce délai va de trente heures aux îles à soixante heures aux bagnes forestiers. Les bâtiments consacrés au bagne, dans le quartier des Roches, sont démolis peu après la fermeture de celui-ci et remplacés par l’Hôtel des Roches. Les derniers vestiges de cette partie de l’histoire de la ville sont la Tour Dreyfus (utilisée pour communiquer avec les îles en code Morse), les ruines du four du bagne (maintenant propriété privée, mais visible dans une petite impasse non loin de la tour), et une petite fontaine près des palmiers moucayas. Au large de Kourou, il y avait des bagnes aux îles du Salut, ouverts en 1852, quand ils furent désignés comme tels. Les premiers déportés y sont arrivés en avril 1862. Une loi ferme le bagne en 1938, mettant un terme aux transports de prisonniers, mais il ne fut véritablement fermé qu’en 1946, et les tout derniers rapatriements ne se feront qu’en 1953.

Le , le gouvernement français décide d’installer une base spatiale en Guyane, et c’est avec la construction de la base, en 1965 que Kourou, jusqu'alors simple village, va devenir une véritable ville, les besoins du Centre spatial guyanais (CSG) provoquant une vigoureuse croissance démographique. La base est gérée conjointement par le CNES (son propriétaire), Arianespace et l'Agence spatiale européenne.

Jusque-là, Kourou avait été divisé en deux parties distinctes : le « bourg » (200 habitants), aujourd’hui les rues les plus près du port de plaisance presque à l’embouchure du Kourou, et les plaines appelées la « savane » et l'« anse », qui abritaient 400 personnes. Celles-ci n’allaient au bourg que les jours de marché ou en cas de nécessité. Le bourg était le centre de la communauté : le lieu de culte (l’église Sainte-Catherine), et le marché s’y situaient, et le bourg était le point de rassemblement lors des fêtes, dont le Carnaval. Les enfants habitant la savane devaient aller à pied à l’école au bourg ; un trajet de deux heures le matin et deux le soir. Un parent leur donnait à manger le midi. Le village ne connut l’électricité qu’en 1948 et les réfrigérateurs dans les années 1955-1960 ; la viande (de production locale) et le poisson (du Kourou ou de l'océan) étaient alors salés ou fumés pour éviter qu’ils ne pourrissent.

Le CSG lui-même est construit sur l’emplacement d’un village appelé Malmanoury et s’étend sur à partir de la rive gauche du Kourou et pénètre, dans ce qui était la forêt vierge, sur une distance de . Cent cinq familles (651 personnes) sont expropriées de leurs terres pour faire de la place pour le CSG et relogées dans des logements en ville. Pour construire la base on embauche des milliers d’ouvriers dès la première phase de la construction, de 1965 à 1970. Beaucoup s’installent ensuite en Guyane, surtout dans la région kouroucienne. En 1976 les personnes travaillant sur le site comprenaient, à part des Guyanais, 330 Sud-Américains (Colombiens, Brésiliens, Surinamiens...), 206 Européens, et 72 Antillais (essentiellement de Martinique et de Guadeloupe). On construit, pour ces personnes ainsi que pour les employés du CSG (ingénieurs, astronomes...) plusieurs petits quartiers d’immeubles collectifs de quatre ou cinq étages, près de la plage : Simarouba, Diamant, place de l’Europe... À Simarouba on trouve le cinéma de la ville et de nombreux petits commerces.

Le Centre spatial est inauguré avec le lancement de la fusée-sonde "Véronique" le .

Le Centre Médico Chirurgical de Kourou (CMCK), sous l’égide de la Croix-Rouge française, est présent non loin de Simarouba depuis 1965, quand il est construit pour soigner les habitants de la ville en pleine expansion. Il y a trois hôpitaux en Guyane, à Kourou, à Cayenne, et à St-Laurent ; dans les autres communes, les soins sont assurés par des centres de santé qui, notamment pour les communes isolées, envoient les malades les plus graves en hélicoptère aux grands hôpitaux du littoral. Le port industriel de Pariacabo est également construit pour les livraisons de matériel nécessaire à la construction des fusées, le port de Cayenne n’étant pas bien équipé et les routes impraticables avec de lourdes et encombrantes charges. La station de traitement des eaux de Kourou, la seule en Guyane à respecter les normes européennes, est construite en 1965. Le étranger d’infanterie, le régiment le plus décoré de la Légion étrangère, est présent au quartier Forget de Kourou depuis l’année 1973, date de son transfert depuis Diégo Suarez à Madagascar. Environ 600 hommes, dont 280 légionnaires permanents, composent ce régiment spécialisé dans le combat en forêt équatoriale. L’unité a activement préparé l’aménagement du CSG et a comme mission principale d’en assurer sa surveillance. Elle participe régulièrement aux opérations de démantèlement de sites d’orpaillage clandestins. La de l’unité est engagée dans le cadre de l’opération Carbet, au sein du bataillon français de la force multinationale intérimaire en Haïti.

Des heurts ont opposé les légionnaires protégeant le centre spatial aux habitants de Kourou en 1985 et 2006. La ville change continuellement pour preuves, la construction de nouveaux édifices comme le marché du bourg ou encore la rénovation du quartier 205. Les programmes de construction de logements et d'établissement scolaires ne sont pas en reste, le Lycée d'enseignement professionnel Elie Castor à ouvert ses portes début 2012.

La mairie de Kourou se situe entre l'avenue des Roches et la petite allée du Bac. La caserne de police est située derrière la mairie et la caserne des pompiers est près du port, au Vieux Bourg, sur l'avenue Charles-de-Gaulle.

Lors du dernier recensement en 2007, la population s’élevait à habitants, toutes origines confondues. Il y a davantage d'hommes que de femmes dans toutes les tranches d'âge dû au grand nombre de légionnaires et d'ingénieurs du CSG installés en ville. Les jeunes de 20 à 24 ans sont peu nombreux car la plupart partent faire leurs études supérieures en métropole ou aux Antilles, la Guyane abritant peu de sections de l'Université Antilles-Guyane. Pendant longtemps Kourou ne fut qu’un simple hameau, plusieurs fois colonisée et abandonnée au long de son histoire, jusqu’à l’implantation du Centre spatial guyanais, qui fit venir des milliers d’ouvriers pour construire les bâtiments ainsi que des milliers d’astronomes, physiciens, ingénieurs, etc., travaillant au sein du CSG même. Cette immigration massive se voit clairement dans les graphes d’évolution de la population. (Voir la section "Économie" pour plus de précisions sur l’émigration massive en Guyane en général après l’implantation du CSG.)

Les Noirs-Marrons sont représentés par les Bonis et les Saramacas. Un quartier saramaca est d’ailleurs implanté sur les bords du fleuve Kourou, près de l’embouchure. Ce quartier a néanmoins partiellement brûlé en avril 2006 et ne s’en est pas encore relevé. Il existe également un quartier amérindiens (nommé ). La coexistence entre les habitants de Kourou et en Guyane en général n’est pas toujours paisible. Le taux de vols violents avec arme est de 40 % dans le département, qui dépasse aisément les taux d'autres délits comparés à la métropole et les autres DOM-TOM. Le taux d’atteintes à l’intégrité physique y atteint 17 faits pour habitants alors qu’il est au maximum de 11,7 ‰ en Île-de-France, par exemple.

Le 27 octobre 2006 se déroule une manifestation contre l’insécurité dans la ville et en Guyane en général. Les commerçants d’origine chinoise, en particulier, sont souvent victimes de cambriolages dans leurs magasins.

Dans la première moitié du on voit l’implantation de cotonneries sur les hautes terres de Kourou (ainsi qu’à Sinnamary, sur l’île de Cayenne, sur l’Oyapock et à Macouria, où cette plante fleurit le mieux). Par contre, le roucou, « spécialité » de la Guyane coloniale, n’y trouva pas du succès, étant cultivé surtout sur à Kaw, sur l’Approuague, et à Montsinéry. Ce fut également le cas pour la culture du sucre, qui se limita en grande partie à l’est du littoral. Cette extrême dépendance de toute la colonie sur trois, parfois deux, cultures la rendit très dépendante des fluctuations du marché ; ainsi, quand le prix du coton baissait, la région de Kourou se trouvait économiquement affaiblie. Ce fut le cas à partir de 1844, quand la petite colonie ne put plus concurrencer l’énorme production des États-Unis, entre autres. La colonie en général vécut une crise agricole de 1830 à 1855. L’économie de la Guyane reposait essentiellement sur la production ou la culture d’un produit à la fois, chacun échouant tôt ou tard, plongeant les guyanais dans la misère jusqu’à la monoculture suivante : tabac, coton, sucre, roucou, balata, bois de rose, rhum, banane... se succédèrent.

La ruée vers l’or, qui commença en 1855 et ne finit qu’à la fin de la Seconde Guerre mondiale, détruit presque entièrement l’activité agricole à Kourou et en Guyane en général. L’effondrement du prix du coton, presque la monoculture de la colonie, et l’abolition de l’esclavage l’avait déjà énormément appauvrie. La colonie dépend presque entièrement de l’or, dont la production atteint un maximum de presque kilos par an en moyenne dans la décennie allant de 1904 à 1914. La valeur des exportations d’or par rapport aux exportations totales atteint une moyenne record de 96 % de 1884 à 1903. Kourou et tous les autres villages et villes de la Guyane se sont vidés de presque tous leurs hommes à l’exception des plus vieux, les plus jeunes et les infirmes. Beaucoup d’agriculteurs quittèrent leurs terres pour travailler dans les mines aurifères. L’économie guyanaise souffre aussi de la rupture des liens commerciaux avec la métropole pendant la guerre.

L’implantation, après la guerre, d’une ferme-pilote à Pariacabo (aujourd’hui la zone industrielle) utilisant des techniques modernes, échoua parce qu’il fut difficile de former les agriculteurs guyanais aux nouvelles machines et techniques. L’expérience fut annulée en 1957 pour cause de manque de budget.

Le Centre spatial guyanais y est implanté depuis 1965. C’est de ce site que sont lancées toutes les fusées Ariane, et d’ici 2008 les Soyouz russes ainsi que le petit lanceur italien Vega. Le CSG est la plus grande entreprise de la Guyane. Elle est considérée comme la « locomotive » de l’économie du département : en 1990, elle comptait pour 49,8 % de la production totale de la Guyane et pour 28,3 % du PIB ; un cinquième de la population vit de l’industrie spatiale, soit personnes. La Guyane se voit dans la même situation économique que pendant la période coloniale : la production est presque entièrement limitée au spatial.

L’importance du CSG et de Kourou en général est tellement grande que les villes proches, dont Sinnamary, Macouria et Roura, de petits villages en 1965, se sont énormément développés depuis, en particulier Sinnamary, où habitent de nombreux employés de la base spatiale. L’impulsion économique du centre spatial développa l’ouest et centre du littoral guyanais, l’est étant en déclin depuis l’abolition de l’esclavage. L’activité économique suscitée par le CSG attire de nombreux immigrants de la région fuyant des troubles économiques ou la guerre dans leurs pays d’origine, dont les Haïtiens, les plus nombreux (ils seront dans tout le département en 1985). L’estimation de la population d’origine brésilienne est plus difficile à cause des nombreux immigrants clandestins ; on en comptait officiellement en 1985. Viendront aussi plusieurs milliers de Chinois dans le département (déjà 329 en 1977), et des milliers de réfugiés Hmong du Laos fuyant la guerre ( personnes à Javouhey près de Mana et à Cacao près de Roura). Pendant la guerre au Surinam on trouve Surinamiens dans la région de Mana et de Saint-Laurent, surtout dans des camps de réfugiés. La plupart retourneront chez eux lors de la paix. La population antillaise, quant à elle, augmente de près de personnes. On compte également "« Métros »" en 1990, la plus grande partie installés à Kourou. Si en 1961 la Guyane comptait habitants, en 1990 ce nombre grimpe à et en 1999 on en recense . Ce spectaculaire essor est presque entièrement dû à l’implantation du CSG dans le département.

Un centre commercial carrefour devrait voir le jour en 2014 au bord du lac du Bois Chaudat, avec lui des boutiques, un cinéma et un hôtel. Le permis de construire est déposé en octobre 2013.

Kourou est à environ à l'ouest de Cayenne et à à l'est de Saint-Laurent.

Les Kourouciens se déplacent surtout à vélo, puis en voiture ou à moto. Pour aller en dehors de Kourou les particuliers n’ayant pas de moyen de transport personnel sont contraints à utiliser un service de navette informel, nommée localement « taxi co » (pour taxis collectifs).

La route nationale 1, à deux voies, relie les grandes villes côtières. Elle fut déviée plus loin de la côte pour contourner le CSG et comprend un pont sur le fleuve Kourou. Ce dernier fut ouvert le 25 décembre 1968, la traversée du fleuve en direction de Cayenne se faisant auparavant grâce à un bac situé au lieu-dit Guatemala. Il y a un aérodrome à une piste dans l’enceinte du CSG, desservant de petits avions, essentiellement des amateurs de l’aéro-club local, et des vols touristiques en hélicoptère ou avions légers.

On y trouve plusieurs écoles primaires et secondaires, ainsi qu’une crèche municipale, un IUT de génie électrique et un DUT Réseaux et télécommunications, toujours à l'IUT . Il y a huit groupes scolaires (Olivier-Compas, Michel-Lohier, Eustase-Rimane, Émile-Nézès, Maximilien-Saba, Raymond-Cresson, Olive-Palmot et Solange-Patient), quatre collèges (Henri-Agarande, Victor-Schœlcher, Omeba-Tobo et Joseph Ho-Ten-You), et deux lycées (Gaston-Monnerville et Élie-Castor).

L'école d'ingénieur AgroParisTech y possède aussi un centre spécialisé dans les forêts tropicales humides et les bois tropicaux.

La médiathèque, ou « pôle culturel », de la ville, située sur la rive sud du lac Bois Chaudat, abrite la bibliothèque municipale, une petite salle de cinéma, une salle d'informatique et une salle pour expositions d'art, ainsi que de nombreuses salles où se déroulent des cours pour enfants et adultes. Il existe également une petite bibliothèque associative payante située derrière la mairie, à côté de la caserne de police. Kourou possède également un petit cinéma de deux salles, l'Urania, au quartier Simarouba.

Le plus grand évènement culturel de l'année est le "Vaval", mot créole désignant le Carnaval. Chaque dimanche pendant un mois avant le jour de Vaval, on peut voir des défilés composés de 4 à 8 groupes d'une trentaine de carnavaliers défiler sur l'avenue des Roches. Le jour même du Vaval, plusieurs centaines de personnes de toute la Guyane et des Antilles vont à Kourou voir la grande parade, appelée « Grande Parade du Littoral », qui dure trois heures ou plus. Toute la ville s'y rend : les femmes y sont le plus souvent danseuses ou "touloulous", les hommes dansent ou jouent d'un instrument de musique, et les enfants se déguisent en petits diablotins ou jouent d'un instrument (le plus souvent des tambours en plastique). Accessoirement, à Kourou on élit le "Prince Vaval", tandis que les Cayennais élisent le "Roi Vaval" ; ceux-ci animent une partie de la parade dans leurs villes respectives.

Plus petite mais néanmoins importante, la fête patronale de Kourou se déroule le 25 novembre, la patronne de la ville étant sainte Catherine Labouré. En ce jour, une petite fête foraine est installée près de la place du marché, au Vieux Bourg, et les enfants y vont jouer toute la journée. Quelques semaines avant Noël se déroule le marché de Noël sur la place du parvis de la médiathèque.

Le culte chrétien catholique est très majoritaire à Kourou. L'église catholique dispose de deux lieux de culte : l’église Sainte-Catherine, située sur l’avenue Charles-de-Gaulle au Vieux Bourg, et l'église Notre-Dame, derrière la mairie. Il y a également une église œcuménique sur l'avenue de France, ainsi qu'un centre dédié aux adventistes du septième jour.

Le stade Bois Chaudat, de places, se situe sur la rive nord du lac éponyme avec un camp d’entraînement à côté. Sur les rives, on trouve aussi la piscine publique de la ville, des courts de tennis et un boulodrome. Au nord est basé le club hippique à côté du lac Bois Diable. Il y a aussi plusieurs équipes de sports nautiques, d’arts martiaux, d'athlétisme...

Sous le nom de "Le Geldar", il y a des sections d’escrime, de handball, de volle-ball et de football. La section de foot, l'ASC Le Geldar, a remporté la Coupe D.O.M. (1-0 contre l’AS Sada de Mayotte) en 2005.

D'autres clubs existent comme le SC Kourou, le Kourou FC en football et le VCK (Vélo Club de Kourou) en cyclisme.

Une équipe de football américain est en cours de création (2012), les Anacondas, d'autres viendront à Cayenne et Macouria dans l'optique d'un championnat à l'instar de ce qui existe déjà en métropole. Ce club comprend des sections seniors et juniors en football américain et flag football. Une équipe de cheerleaders viendra s'ajouter.

Il existe plusieurs petites compétitions se déroulant en ville : en octobre, deux jours de courses en pirogue à la mer, départ et fin sur la plage de Kourou, et en mars le Marathon de l'Espace.



L'expédition de Kourou est l'objet de plusieurs études scientifiques, Marion Godfroy, docteur en Histoire a consacré sa thèse à cette expédition, à travers un examen exhaustif de toutes les sources en Europe et en Guyane (Godfroy, "Kourou (...)"; Véndémaire, 2011). Elle a également confronté ses résultats à la communauté scientifique par deux articles dans des revues stricts comité de lecture ("French Historical Studies" ,2 / Annales de démographie française 2012). Cependant, on ne peut omettre les travaux d'Emma Rotschild, professeur à Harvard (article dans Past and Présents). De même, parmi les études plus anciennes, on doit citer ceux de Jacques Michel et dans une autre mesure ceux de P Thibauldaut . Les articles de Jean Chäia, Carol Blum, JF Tarrade ou K Heinz s'attèlent à certains aspects de l'expédition.




</doc>
<doc id="14228" url="https://fr.wikipedia.org/wiki?curid=14228" title="Ariane 4">
Ariane 4

Ariane 4 était un lanceur civil de l'Agence spatiale européenne (ESA), développé pour placer des satellites en orbite autour de la Terre. Il fait partie de la famille des lanceurs européens Ariane, conçue en collaborations par différents industriels européens, commercialisé par Arianespace et lancé depuis la base de Kourou (Guyane française).

L'agence spatiale européenne décide en octobre 1981, sur proposition de la France, de développer une version plus puissante du lanceur Ariane, permettant de placer en orbite géostationnaire et d'effectuer des lancements doubles. L'objectif est de mettre en service cette nouvelle version en 1986 de manière à pouvoir répondre aux besoins de la nouvelle génération de satellites de télécommunications. 

Le CNES est maître d'œuvre et l'Aérospatiale l'architecte industriel. La plupart des composants sont confiés à l'Aérospatiale et DASA dont les activités spatiales ont été intégrées en l'an 2000 au sein d'EADS Astrium. Les moteurs sont fournis par la SEP intégrée à SNECMA en 1997, à l’exception des propulseurs à poudre PAP fournis par SNIA-BPD. La case à équipement est réalisée par Matra.

Les installations de lancement à Kourou sont agrandies : en effet, d'une part le premier complexe de lancement ne permet d'effectuer que 6 tirs par an, alors qu'il est prévu dans le futur une moyenne de 10 tirs par an ; d'autre part les propulseurs d'appoint liquide ne peuvent être montés dans les installations existantes. L'ESA autorise en août 1981 l'édification d'un deuxième complexe de lancement, ELA 2, pour un coût de 153 millions d'euros. Avec ELA 2 le déroulement du montage de la fusée et son lancement sont profondément modifiés pour limiter les conséquences d'une explosion au décollage et surtout réduire le délai entre deux tirs, le bâtiment d'assemblage est construit à près d' de l'aire de lancement et le lanceur est amené sur le lieu de décollage posé sur une table de lancement qui se déplace sur des rails. Grâce à ces nouvelles installations, le délai entre deux tirs peut être réduit théoriquement de 28 à 18 jours.

Arianespace commande un premier lot de 23 lanceurs en 1987 pour la somme de , puis en 1989 un second lot de 50 lanceurs pour une commande record de . Quatre autres commandes suivront en 1995 (5 lanceurs), 1996 (10 lanceurs), et deux en 1997 (10 et 20 lanceurs).

Le premier lancement d'Ariane 4 a lieu le (vol V-22) avec un Ariane 44LP embarquant 3 satellites : un satellite météo Meteosat-3, un satellite de télécommunications PAS-1 et un satellite radioamateur AMSAT-P3C.

En 1990, Ariane 4 réalise sa première mission héliosynchrone (vol V-35) et connait son premier échec (vol V-36), qui sera suivi par deux autres en 1994.

Le record de transfert en orbite géostationnaire est établi en 1998 par un Ariane 44L (V-113) avec AFRISTAR et GE-5 pour une masse .

Il termine sa carrière le 15 février 2003 avec un Ariane 44L (V-159), remplacée par Ariane 5 qui permet d'emporter des charges plus lourdes.

Ariane 4 fut tirée 116 fois entre 1988 et 2003 et mit en orbite plus de 180 satellites, et réalisa 10 mises en orbite héliosynchrone et 106 mises en orbite de transfert géostationnaire.

Il y eut 3 échecs : le Vol 36, où un technicien a oublié un chiffon dans une conduite d'eau d'un moteur Viking du premier étage, le Vol 63 lorsque la turbopompe d'oxygène liquide a surchauffé et s'est rompue, et le Vol 70 quand une fuite ou une obstruction (non déterminé par l'enquête) de la canalisation d'oxygène liquide en amont de la turbopompe entraîna une baisse de puissance du moteur. Malgré ces ennuis, ce lanceur eut un taux de fiabilité supérieur à 97 %.

Ariane 4 est la version la plus produite des lanceurs Ariane. C'est cette version qui permit à l'Europe de devenir un acteur majeur des lancements de satellites commerciaux détenant jusqu'à 60 % du marché mondial. De 1995 à 2003, Ariane 4 a enchaîné 74 lancements successifs réussis, ce qui constituait un record pour un lanceur commercial jusqu’au vol VA 233 en novembre 2016, qui permis à Ariane 5 de battre ce record.

Le lanceur était commercialisé dans 6 configurations différentes permettant de lancer un ou deux satellites en orbite de transfert géostationnaire (GTO) avec une masse totale de à . Il possédait selon les versions des propulseurs supplémentaires (zéro, deux ou quatre) de types variables : à propergol liquide (symbolisés par la lettre L), ou bien à propergol solide (également qualifiés de « à poudre » ; symbolisés par la lettre P). Il en résultait ainsi les versions :

Ariane 4 possède une architecture à trois étages, similaire aux précédentes versions des lanceurs Ariane (Ariane 1, Ariane 2 et Ariane 3)

Des propulseurs d'appoint à carburant liquide ou solide pouvaient être associés au premier étage, offrant une souplesse supplémentaire avec différentes versions utilisables selon la charge à emporter.

Les propulseurs d'appoint liquide (PAL) utilisaient un moteur Viking 6 consommant les mêmes carburants que les premiers et deuxièmes étages :
Les propulseurs d'appoint à poudre (PAP) d'Ariane 4 étaient dérivés de ceux d'Ariane 3, allongés de :

Le premier étage était construit par l'Aerospatiale, et les moteurs par la SEP. Par rapport aux versions précédentes d'Ariane, le premier étage fut allongé lui permettant d'emporter jusqu'à d'ergols. La puissance des quatre moteurs Viking fut légèrement augmentée délivrant une poussée totale au niveau de la mer de .

Différentes configurations existaient suivant le modèle d'Ariane 4 :
Dimensions :

Le deuxième étage était intégré par DASA en Allemagne, les moteurs fournis par la SEP et les réservoirs par Dornier. Il fut peu modifié par rapport aux précédentes versions d'Ariane. Sa structure fut renforcée pour supporter les efforts supplémentaires liés à l'augmentation de la charge utile et à la coiffe plus grande. Quelques équipements furent également modifiés. Il était propulsé par un moteur Viking 4B.

Caractéristiques du L33 :

Ariane 4 reprend l'étage H10 des Ariane 2 et 3. Il est construit par Aérospatiale, les moteurs fournis par SEP et les réservoirs par Air liquide. Sa structure fut renforcée de la même manière que l'étage L33. Il était propulsé par un moteur cryogénique HM-7B (désormais utilisé au niveau de l'étage supérieur du lanceur Ariane 5 ECA).

Différentes versions ont équipé Ariane 4 :
Par rapport aux précédents lanceurs Ariane, la coiffe était entièrement nouvelle pour permettre les lancements doubles. Elle se décomposait en deux parties, la partie basse appelée SPELDA (pour Structure porteuse externe de lancement double Ariane) et d'une partie haute simplement appelée coiffe. Chaque partie existait en 3 tailles différentes pour s'adapter aux différents satellites. Les deux parties se séparaient en utilisant un dispositif pyrotechnique. En cas de lancement simple, seule la coiffe était utilisée.

La partie SPELDA était construite par BAe et la coiffe par Contraves .




</doc>
<doc id="14230" url="https://fr.wikipedia.org/wiki?curid=14230" title="William Hogarth">
William Hogarth

William Hogarth, né à Londres le et mort à Chiswick le , est un peintre, graveur et philanthrope anglais.

Enfant de la Glorieuse Révolution, très tôt reconnu par la critique et identifié en France dès 1753 par Denis Diderot comme un brillant esprit, Hogarth est un artiste complet, qui embrassa plusieurs modes d'expression, et dont l'influence se perpétue jusqu'au début du siècle. Premier artiste libre et singulier de l'école anglaise de peinture, il n'hésite pas à utiliser la presse et ses réseaux d'amis pour défendre ses idées, tout en exprimant, tant par la plume que le pinceau, les errances, les plaisirs et les contradictions morales de son époque.

William Hogarth est né dans le quartier de Bartholomew Close, à Londres, de Richard Hogarth, un professeur de latin et rédacteur de manuels d'enseignement, et Ann Gibbons, le couple a trois enfants, dont deux filles, Mary et Ann. Le père, originaire du Westmorland, fait modestement vivre sa famille de son métier de maître d'école. Il décide d'ouvrir en 1703 une sorte de "coffee-house" réservé aux adeptes de la langue latine, mais il fait faillite en 1707 et écope de cinq ans de prison à Fleet pour dettes : son épouse Ann et ses trois enfants sont contraints de vivre à proximité de la prison dans un local réservé aux familles de débiteurs. En 1712, Richard est libéré, et devient correcteur d'épreuves d'imprimerie et la famille emménage sur Long Lane, quartier de Smithfields (celui des graveurs et des orfèvres).

L'année suivante, à seize ans, l'adolescent entre comme apprenti dans l'atelier d'un graveur sur argent métal, Ellis Gamble (actif de 1712 à 1733), membre de la Merchant Taylor's Company, sur Blue Cross Street (Leicester Fields), où il cisèle des emblèmes sur des pièces d'orfèvrerie jusqu'en 1720, ainsi que des cartes-adresses ("trade card"), petite pièce de gravure reportée sur papier épais destinée aux artisans et fournisseurs londoniens, et des ex-libris.

En 1718, le père de William meurt. Le voici chargé de famille.

Décidé à gagner de l'argent par ses propres moyens et à aider ses proches, Hogarth se lance à son compte à partir d'avril 1720, date à laquelle il édite sa propre carte de visite commerciale : installé chez sa mère, il se présente comme graveur. Ses premiers clients sont des commerçants et des éditeurs. La même année, il entre à l'académie de peinture de St Martin's Lane fondée par Louis Chéron et John Vanderbank, au moment où Londres est en proie à une fièvre financière qui conduit à l'effondrement durant l'automne de la South Sea Company, un désastre ruineux qui va traumatiser la capitale.

Il exécute en 1721 sa première pièce gravée à connaître un certain succès, "Emblematical Print on the South Sea Scheme", d'une part, parce qu'il inaugure là un nouveau genre avec une originalité qui trouve son public, la satire, et d'autre part, parce qu'il va tenter de rompre le monopôle du marché des estampes détenu par la , et, s'associant avec ses amis graveurs, réveille le milieu londonien de l'illustration, plutôt conformiste. Durant les quatre années suivantes, il produit de nombreuses estampes satiriques, comme "The Bad Taste of the Town" (1724), dont l'un des points d'ancrage est le théâtre (la scène mais aussi celui de la vie londonienne, de ses manies et ses hypocrisies).

Vers cette époque, il commence à fréquenter le Rose and Crown Club, et se lie aux artistes George Vertue, Peter Tillemans, et Michael Dahl, entre autres membres.

En 1724, William tient boutique à l'enseigne de la Boule d'or, à l'angle de Cranbourne Alley et Little Newport Street. Il découvre que sa gravure "Masquerades & Operas" a été piratée, et il achève la commande de dix-sept gravures pour un ouvrage, le "Hudibras", de Samuel Butler, qui lui fournit également le sujet de douze estampes publiées séparément. Parallèlement, le jeune homme entre cette même année dans l'académie de dessin fondée trois ans auparavant par James Thornhill à Covent Garden. Peu de temps après, Hogarth tombe amoureux et s'enfuit avec la fille du maître, Jane, qui du coup n'aura point de dot. De leurs côtés, les sœurs de William ouvrent une boutique de modiste sur Long Walk près de l'hôpital de St Bartholomew.

Entre temps, il renonce à une commande de cartonnier en tapisserie en 1727, au prétexte qu'il est : l'affaire tourne mal, et le conduit à un procès, qu'il remporte le 28 mai 1728.

Nullement courtisan, il produit des planches satiriques sur le roi George II, sur Henri VIII et Anne Boleyn, sur Horace Walpole (1726-1727).

Jane Thornhill et William se marient le 23 mars 1729 et emménagent non loin des Thornhill, à Little Piazza, face à Covent Garden.

En fait, depuis déjà une année, le jeune homme, qui continue de graver, s'affirme aussi comme peintre. Pour gagner de l'argent, il s'essaye d'abord à partir de 1727-1728 au tableau de conversation de petits formats, des scènes de genre à la mode, dans la lignée de Joseph Highmore. Sa première commande provient du vicomte de Castelmaine, Richard Child (1680-1750), pour un portrait de groupe en conversation commémorant sa réception à .

Comme ses gravures, certaines de ses premières toiles — qui d'ailleurs seront souvent portées en gravure par lui et ses assistants — témoignent de ses qualités de satiriste. D'autres manifestent une sensibilité particulière à la puissance d'expression du théâtre qui ne le quittera jamais. Ainsi illustre-t-il dès 1728 par deux toiles "The Beggar's Opera" ("L'Opéra des gueux"), pièce de John Gay qui connaissait à Londres le succès, puis, vers 1737-38, "Scene from The Tempest" ("Une scène de « La Tempête »") de Shakespeare, et en 1745, "David Garrick in the Character of Richard III" ("L'acteur Garrick dans le rôle de Richard III"). « Ma peinture est ma scène, écrira Hogarth, et mes personnages sont des acteurs qui y donnent une pantomime silencieuse ».

C'est sans doute dans la représentation de gens simples ou bourgeois fraichement arrivés qu'il parvient à la plus grande expressivité. Hogarth atteint à une grande virtuosité dans les sujets contemporains et moraux qu'il appelait ses « pièces morales », dont on pourraît trouver l'origine conceptuelle chez Jacques Callot et certains maîtres hollandais du siècle précédent. Sous la forme satirique que connaît alors la littérature anglaise avec Daniel Defoe ou Jonathan Swift, le peintre fustige les mœurs de la société britannique. Il est à ce titre considéré comme le père de l'estampe satirique anglaise et un précurseur de la caricature. Il s'intéresse aux réformes sociales, il est l'ami d'écrivains comme Tobias Smollett et Henry Fielding, dont il partage le mépris pour la corruption politique. Aussi, malgré le succès de ses portraits, Hogarth, comme il l'écrira dans ses "autobiographical notes" ("notes autobiographiques"), tourne ses pensées « vers un genre encore plus original : la peinture et la gravure de sujets moraux modernes, un champ qui [n'a] encore été exploité à aucune époque et dans aucun pays ».

En 1731, les William et son épouse emménage dans la maison des Thornhill, à Great Piazza (Covent Garden). En 1733, quelques jours avant son exécution par pendaison, il rend visite en prison à Sarah Malcolm, en compagnie de son beau-père James Thornhill : de ses esquisses, il tire une gravure puis un tableau. C'est aussi cette année-là qu'il reçoit ses premières commandes royales mais où il se heurte à quelques ennemis et intriguants, dont l'architecte William Kent et Charles FitzRoy, duc de Grafton (1683-1757). Il finit par déménager, laissant la maison de James Thornhill à son beau-frère John, en sa propre maison située sur Leicester Fields, où il a son atelier et son échoppe à l'enseigne du « "Golden Head" ». Il constate que ses gravures sont de plus en plus piratées, signe également de leur succès.

Ayant peint une toile représentant le lever d'une prostituée, Hogarth la montre à ses amis, qui l'en félicitent. Peut-être inspiré par un roman contemporain de Daniel Defoe ("Heurs et malheurs de la fameuse Moll Flanders"), il décide alors de lui donner un pendant, et enfin de l'intégrer dans un ensemble de six tableaux qui content l'histoire malheureuse, mais au dénouement édifiant, d'une fille de la campagne : "A Harlot's Progress" ("La Carrière d'une prostituée") est achevée à la fin de 1731, son succès encourage l'artiste à mettre en chantier dès la fin 1733 une nouvelle série, "A Rake's Progress" ("La Carrière d'un libertin"), achevée en 1735.

Au début des années 1730, il se lie à Mary Edwards (1705–43), l'une des femmes les plus riches et indépendantes du pays, qui avait été répudiée par son mari, lequel avait déclaré leur fils illégitime. Elle devient durant dix ans, la plus proche amie d'Hogarth et son principal mécène. Le peintre exécute en 1742 un grand portrait d'elle, l'un de ses plus accomplis.

En 1735, l'artiste est l'un des signataires d'une pétition qui aboutit quelques semaines plus tard au vote par le Parlement de la « Loi Hogarth » ("Engraver Copyright Act"), laquelle interdit de tirer des estampes d'une œuvre d'art sans l'accord contractuel de l'auteur. Le souci de toucher le plus de monde possible, et dans toutes les couches de la société, pousse également le graveur à varier le style de ses estampes. Cette même année, il rouvre la St Martin's Lane Academy, qui avait fait faillite, et où il accueille entre autres le français Hubert-François Gravelot dont le style fera école, ainsi que le sculpteur Roubillac. 1735 est aussi l'année où il refonde avec le comédien John Rich et le peintre George Lambert la « "Sublime Society of Beefsteaks" », qui deviendra le : leur principale « tête de turc » est Robert Walpole.

Si Hogarth réussit pleinement comme « peintre d'histoire comique » ainsi que le qualifie Henry Fielding à la fois dans sa gazette, "The Champion", et dans la préface à son roman "Joseph Andrews", il s'essaye également à la grande peinture d'histoire et à la peinture religieuse : répondant à des commandes qu'il estime acceptable, il réalise notamment en 1735-1736 "Le Bon Samaritain" et "La Piscine de Béthesda" pour l'escalier d'honneur du St. Bartholomew's Hospital, le "Saint Paul devant Félix" (1746) pour le Lincoln'Inn, et plus tard, en 1756, le grand triptyque pour St. Mary Redcliffe, à Bristol. Mais dans la protestante Angleterre, la peinture religieuse est peu prisée.

En 1739, étant sans enfant, il s'intéresse aux orphelins du Foundling Hospital : devenu l'ami du capitaine philanthrope et fondateur du lieu, Thomas Coram, il exécute les costumes et le blason de l'orphelinat, puis persuade Coram d'y accueillir des expositions de peintures afin de transformer le produit des ventes en donation pour l'éducation et le soin des enfants. La gestion des expositions est confiée bientôt à la Société des Dilettanti dont Hogarth est membre. Le salon de charité du Foundling Hospital, qui se doubla de concerts, entre autres offerts par George Frideric Handel lui-même en 1749 et 1750, est le premier espace d'exposition dédié aux arts en Angleterre.

Hogarth voyagea peu : il ne fit pas le "Grand Tour" en Italie, contrairement à nombre de ses collègues, méprisant ce qu'il considérait comme une perte de temps. En 1743, toutefois, il se rend à Paris pour un voyage assez bref — peut-être conseillé par Gravelot et Jacques-Philippe Le Bas — afin de solliciter « les plus grands maîtres de Paris », comme il l'annonce sur le bulletin de souscription de six gravures sur cuivre du "Marriage A-la-Mode" [sic], d'après John Dryden, achevées en 1745, description satirique mais raffinée d'« une aventure moderne dans la plus haute société ». Aucun artiste n'est en fait venu de France, la guerre l'empêchant.

En 1748, durant un voyage d'étude en compagnie des peintres Francis Hayman
et Thomas Hudson qui les mènent en Flandre, il effectue juste un petit saut à Calais, la France et l'Angleterre sont toujours en guerre, on l'accuse d'espionnage, on l'emprisonne, le voyage est donc mouvementé, et lui donne l'idée d'une nouvelle toile satirique surnommée "La Porte de Calais" (1748), suivie par "Le Départ de la garde pour Finchley" (1749), une toile qui se moque des soldats partant la fleur au fusil, tout en manifestant son nationalisme, certes ambigu. C'est l'époque où apparaissent certains tableaux plus intimistes, possédant une palette à la fois éclaircie et intense : d'abord son "Autoportrait au chien" (1745), contemporain du "L'Acteur David Garrick en Richard III", puis ses sœurs, puis, avec une grande force expressive, les traits de "Lord Salvat" (1746), qui fut le dernier condamné à la décapitation en Angleterre, tendance qui s'affirme encore avec la toile "Serviteurs du peintre" ("Hogarth's Servants", vers 1754) qui touche le spectateur par sa lumineuse humanité.

Hogarth se rapproche de plus en plus de l'estampe populaire pour opposer les effets bienfaisants de la bière aux désastres provoqués par le gin ("Beer Street and Gin Lane" - "La Rue de la Bière et la Ruelle du Gin", 1751) ou pour « écrire » les douze chapitres truculents d'une fable où s'opposent les carrières de deux apprentis ("Industry and Idleness" - "Le Zèle et la Paresse", 1747). Les vertus de l'un l'amènent à devenir lord-maire de Londres, les vices de l'autre sont sanctionnés par l'échafaud.

Dans son essai "Analysis of Beauty, written with the view of fixing the fluctuating ideas of taste" ("Analyse de la beauté", 1753), Hogarth affirme que le principe de la beauté réside dans la ligne ondulée ou "serpentine" baptisée par lui du nom de « ligne de beauté ». Dans la préface, Hogarth fait part des grandes motivations qui animent son projet : s'inscrivant dans la logique des Lumières, il s'agit, dans ce domaine comme dans tant d'autres, de sortir le discours sur la beauté et sur la grâce des brumes élitistes (le je-ne-sais-quoi, le « mystère de la création ») et des hiérarchies académiques (seule la copie des modèles anciens permet d'atteindre le beau idéal). Le traité est ensuite divisé en 17 chapitres : 6 consacrés aux grands principes de la composition picturale, 5 à une théorie des lignes, 3 à l'éclairage et la couleur, et 3 au visage, à l'attitude et enfin à l'action.

En 1755, son essai, en dépit de quelques critiques, lui vaut d'être élu membre de la Royal Society of Arts, dont il démissionnera deux ans plus tard.

En 1757, il est nommé "" par le roi George II, une charge honorifique, relativement différente de celle de peintre de cour, ou de celle de premier peintre du roi en France, en remplacement de son beau-frère, John Thornhill, mort la même année. Depuis quelque temps, il s'oppose publiquement à l'idée d'une académie officielle des arts, qui remplacerait St Martin's Lane. Le peintre Joshua Reynolds publie une satire de Hogarth dans "The Idler" (1759). Ayant renoncé à la peinture d'histoire, il accepte néanmoins des commandes de James Caulfeild of Charlemont (1728-1799). Il se fatigue beaucoup et tombe malade, subissant une première attaque au début de 1760 qui l'immobilise pendant près d'une année.

Au début des années 1760, ses plus proches amis sont, outre Garrick, Francis Hayman, Henry Fielding, et Laurence Sterne. En 1761, il devient membre de la Society of Artists et expose avec eux à Spring Gardens, soit sept tableaux. Hogarth publie encore des gravures satiriques contre la nouvelle guerre qui oppose Britanniques et Français, et se heurte au parti des bellicistes, et à ceux qui attaquent la Society of Artists. Fin 1761, il subit une seconde crise, sa santé vacille. Il ne peut achever sa suite prévue, "L'Époque" ("The Times" I et II, 1762), qui vilipendait les tueurs de la paix et le climat anti-français et de paranoïa qui jetait en prison le moindre opposant aux va-t-en-guerre, mais surtout, qui montre que son auteur demeure profondément un libre esprit, ne rejoignant au fond aucun parti, saut celui de la lucidité. En découvrant "The Times I", des journalistes s'en prennent à lui, dont le polémiste John Wilkes — qui se veut le champion de la liberté mais que certains jugent comme un laquais de William Pitt l'Ancien —, auquel Hogarth répond par une gravure en mai 1763 qui se vendit à plus de exemplaires. Hogarth fustige dans la foulée l'esprit de crédulité et de superstition, avec une nouvelle gravure. Le climat politico-social de Londres n'est pas bon : amer, il s'installe dans sa maison de campagne à Chiswick. En 1763, toujours, le prélat et journaliste Charles Churchill attaque Hogarth via une épître, et le peintre répond par la gravure "L'Homme de main" (qui aurait prétendument tué Churchill). Épuisé par ces joutes souvent violentes, Hogarth est victime d'une paralysie à la fin de cette année-là.

Hogarth meurt le 26 octobre 1764, après une année passée à lutter contre la maladie. Il est enterré le 2 novembre dans l'église de Chiswick. Sa maison, près de la route nationale A4 Londres-Bristol, très fréquentée, est devenue un musée qui lui est consacré. Garrick composa des vers de circonstances, une épitaphe. Il est mort sans descendance ; son épouse Jane (1709-1789) lui survit et durant vingt-cinq ans, gère son œuvre.

Hogarth fréquentait une certaine élite intellectuelle de Londres, mais aussi pas mal d'excentriques, d'iconoclastes, et des gens du peuple ; il passait beaucoup de temps avec des dramaturges, des acteurs et des artistes, au sein de nombreux "gentlemen's clubs" où l'on pouvait débattre, faire de la musique, lire, boire et manger en toute liberté.

Dès les années 1730, le poète John Bancks loue la « grâce variée » des images d'Hogarth, et George Vertue, graveur à succès et commentateur assidu de la scène artistique londonienne, souligne la « grande variété » de ses tableaux. À la fin du siècle, les mêmes expressions seront reprises pour décrire l'œuvre peint et gravé de l'artiste. Ainsi, en 1792, Jonathan Richardson célèbre-t-il « la variété infinie qui s'exprime dans les œuvres d'Hogarth ».

En 1935, Gavin Gordon (1901-1970) créé le balet "The Rake's Progress", avec Ninette de Valois, qui s'inspire directement de la suite d'Hogarth. En 1951, Igor Stravinsky compose l'opéra "The Rake's Progress", sur un livret de W. H. Auden, plus ou moins inspiré des images d'Hogarth.

Premier peintre anglais à permettre à son pays de s'émanciper de l'influence de la peinture flamande et française, Hogarth apparaît, avec le recul, comme une figure majeure de l'Europe artistique du . Par son œuvre de théoricien, Hogarth ouvre la voie à une reconnaissance des genres mineurs que sont le portrait des humbles, le paysage urbain et la gravure satirique, qui joueront un rôle essentiel dans l'affirmation d'une école anglaise, celle de Joshua Reynolds et Thomas Gainsborough, jusqu’à Joseph Mallord William Turner et John Constable, en passant par Thomas Rowlandson, William Blake et James Gillray.

En octobre 2006, s'ouvre au musée du Louvre, en partenariat avec la Tate Britain, la première grande exposition jamais organisée en France sur Hogarth, où l'on découvre des artiste contemporains qui s'inspirent du maître, comme Paula Rego, David Hockney, Yinka Shonibare ou Jake et Dinos Chapman.




William Hogarth s'est peint lui-même plusieurs fois — au moins trois huiles sur toile — et a été le sujet de représentations, dont une miniature attribuée à Jean-André Rouquet et un buste signé Roubiliac. En regardant son autoportrait de 1745 — celui au carlin nommé Trump —, on distingue sur son front une cicatrice dont on ignore l'origine mais dont il était fier. Une analyse du tableau au rayon X démontre que l'esquisse originelle prévoyait une perruque et une tenue de ville. Quant au chien, il est en avant, et veut symboliser, comme dans une vanité, que le caractère principal d'Hogarth est la ténacité et la fidélité. Physiquement, Hogarth était plutôt de petite taille et assez râblé.





</doc>
<doc id="14231" url="https://fr.wikipedia.org/wiki?curid=14231" title="Déville-lès-Rouen">
Déville-lès-Rouen

Déville-lès-Rouen est une commune française située dans le département de la Seine-Maritime, en Normandie .
Elle fait partie de la banlieue nord de Rouen.

La commune est traversée par le Cailly.

La gare de Maromme sur la ligne de Paris-Saint-Lazare au Havre est située sur les communes de Notre-Dame-de-Bondeville et de Déville-lès-Rouen.

L'histoire de Déville-lès-Rouen a été écrite par l'historien rouennais Robert Eude qui a également dessiné son blason. Une petite rue porte son nom aujourd'hui.





Dans un important arrêt "Compagnie du gaz de Déville-lès-Rouen", du , le Conseil d'État a admis la possibilité pour la commune de modifier unilatéralement la convention qui la lie à une compagnie de gaz, consacrant ainsi la mutabilité du contrat administratif.





</doc>
<doc id="14232" url="https://fr.wikipedia.org/wiki?curid=14232" title="Capital social">
Capital social

Le capital économique, appelé souvent plus simplement le capital, désigne, en économie, une vision comptable des ressources financières mobilisées pour effectuer une production économique de valeur ajoutée. En comptabilité il désigne les ressources (le capital au sens général du terme) apportées à une société par ses associés lors de sa création ou d'augmentations de capital ultérieures. 
Le capital est la représentation au passif des engagements de l'entreprise envers ses associés ou actionnaires (les capitaux propres) pour la partie venant de leurs apports. Il est la contrepartie des apports en biens ou numéraires réalisés par eux.

C'est une garantie générale du créancier (fournisseur, banquier…) et est donc soumis à un régime juridique strict. Ainsi les réductions du capital social sont soumises à des mesures de publicité pour que les créanciers de la société en soient informés.




</doc>
<doc id="14236" url="https://fr.wikipedia.org/wiki?curid=14236" title="Technologies de l'information et de la communication">
Technologies de l'information et de la communication

Technologies de l'information et de la communication (TIC : transcription de l'anglais "", "ICT") est une expression, principalement utilisée dans le monde universitaire, pour désigner le domaine de la "télématique", c'est-à-dire les techniques de l'informatique, de l'audiovisuel, des multimédias, d'Internet et des télécommunications qui permettent aux utilisateurs de communiquer, d'accéder aux sources d'information, de stocker, de manipuler, de produire et de transmettre l'information sous toutes les formes : texte, musique, son, image, vidéo et interface graphique interactive (IHM). Les textes juridiques et réglementaires utilisent la locution communications électroniques.

Les nouvelles technologies de l'information et de la communication (NTIC) ouvrent des problématiques résultant de l'intégration de ces techniques au sein des systèmes institutionnels, recouvrant notamment les produits, les pratiques et les procédés potentiellement générés par cette intégration.

Après les premiers pas vers une société de l'information qu'ont été l'écriture puis l'imprimerie, de grandes étapes ont été le télégraphe électrique, puis le téléphone et la radiotéléphonie. L'informatique a pris son essor grâce aux circuits imprimés les constructeurs d'informatique décentralisée innovant rapidement. La télévision, le Minitel et l'Internet puis les télécommunications mobiles ont associé l'image au texte et à la parole, "sans fil", l'Internet et la télévision devenant accessibles sur le téléphone portable qui fait aussi office d'appareil photo.Le rapprochement de l'informatique, de l'audiovisuel et des télécommunications, dans la dernière décennie du a bénéficié de la miniaturisation des composants, permettant de produire des appareils « multifonctions » à des prix accessibles, dès les années 2000. L'augmentation rapide du nombre d'accès à internet à haut débit (par exemple avec l'ADSL ou via les réseaux de la télévision par câble) et d'accès à internet à très haut débit (avec les réseaux de lignes d'abonnés en fibre optique) a favorisé la diffusion de contenus audiovisuels à des prix abordables puisque cela a fait baisser les prix des TIC en deux ans.

Avec le développement d'Internet et du WEB 2.0, les usages des TIC se sont développés et la grande majorité des citoyens des pays industrialisés utilise ces outils pour accéder à l'information. Par contre, une fracture numérique géographique s'est développée avec les pays en développement où l'accès à internet à haut débit est hors de la portée de la plupart des ménages. Un grand nombre d'internautes, via des sites, des blogs ou des projets tels que le projet encyclopédique Wikipédia ajoutent de l'information à l'internet.

Le nombre de services disponibles explose, et génère des emplois liés à ces technologies, pour 3,2 % du PIB français vers 2010 et 5,5 attendu en 2015.

Les emplois de la filière nécessitent de plus en plus de compétences de communication, de marketing et de vente, la technique n'étant qu'un support de la communication et d'organisation. Cela entraîne de nombreuses modifications dans les profils professionnels recherchés par les entreprises selon l'Observatoire International des Métiers Internet, qui analyse les profils et les compétences recherchés par le marché de l'emploi en Europe.

Les usages des TIC ne cessent de s'étendre, surtout dans les pays développés, au risque d'accentuer localement la fracture numérique et sociale ainsi que le fossé entre les générations. De l'agriculture de précision et de la gestion de la forêt (traçabilité des bois pour lutter contre le trafic), au contrôle global de l'environnement planétaire ou de la biodiversité, à la démocratie participative (TIC au service du développement durable) en passant par le commerce, la télémédecine, l'information, la gestion de multiples bases de données, la bourse, la robotique et les usages militaires, sans oublier l'aide aux handicapés (dont les aveugles qui utilisent des synthétiseurs vocaux avancés ainsi que des plages braille éphémère), les TIC tendent à prendre une place croissante dans la vie humaine et le fonctionnement des sociétés.

De 2007 à 2010, la proportion de sociétés équipées d'un extranet est passée de 17 % début 2007 à 35 % début 2010.

Certains craignent aussi une perte de liberté individuelle (effet "Big Brother", intrusion croissante de la publicité ciblée et non-désirée…). Les prospectivistes s'accordent à penser que les TIC devraient prendre une place croissante et pourraient être à l'origine d'un nouveau paradigme civilisationnel, avec peut être une évolution des TIC vers les NBIC (Nanotechnologies, biotechnologies, informatique et sciences cognitives).

L'expression « technologies de l'information et de la communication » est la transcription d'une locution anglaise utilisée dans diverses instances internationales qui correspond à peu près au domaine de la "télématique". Il fait l'objet de différentes définitions selon le point de vue de la source utilisée ou selon l'époque de la définition en raison du brouillage progressif des frontières des domaines concernés et de l'évolution rapide des techniques avec la convergence numérique.

La définition des TIC reste particulièrement floue : le terme technologie qui signifie est utilisé à la place de , qui serait à la fois plus simple et plus exact. Les technologies de l'information et de la communication sont des outils de support au traitement de l'information et à la communication, le traitement de l'information et la communication de l'information restant l'objectif, et la technologie, le moyen.

Le dictionnaire Larousse définit les technologies de l'information et de la communication comme étant un ""ensemble des techniques et des équipements informatiques permettant de communiquer à distance par voie électronique (câble, téléphone, Internet, etc.)"". Mais cette définition se limite à la convergence de l'informatique et des télécommunications en vue de communiquer et ne tient pas compte de l'impact de la convergence numérique dans les multimédias et l'audiovisuel.

Le "Grand dictionnaire terminologique de l'OQLF" définit les technologies de l'information et de la communication comme étant un « Ensemble des technologies issues de la convergence de l'informatique et des techniques évoluées du multimédia et des télécommunications, qui ont permis l'émergence de moyens de communication plus efficaces, en améliorant le traitement, la mise en mémoire, la diffusion et l'échange de l'information ». Cette définition est beaucoup plus complète que la précédente en tenant compte de la convergence numérique dans son ensemble. Elle reflète davantage le point de vue des institutions internationales qui considèrent les technologies de l'information et de la communication comme étant l'intégration des techniques des télécommunications, de l'informatique, des multimédias et de l'audiovisuel. La diffusion rapide des accès à l'Internet à haut débit a permis une explosion des usages des services audiovisuels qui prennent une importance accrue dans le concept des TIC, non seulement au niveau de la communication, mais aussi au niveau de la gestion des informations et des connaissances et au niveau de leur diffusion. Cette extension du concept des TIC est à l'origine de nombreux débats en raison de l'importance de son impact sur la société.

Selon une convention internationale fixée par l'OCDE, les technologies de l'information et de la communication (TIC) englobent les secteurs économiques suivants :

L'avènement de l'Internet et principalement du Web comme média de masse et le succès des blogs, des réseaux sociaux, des wikis ou des technologies Peer to Peer confèrent aux TIC une dimension sociétale. Gérard Ayache dans "La Grande confusion", parle d'« hyper information » pour souligner l'impact anthropologique des nouvelles technologies.
De nombreux internautes, quant à eux, considèrent l'Internet comme une technologie de la relation (TR) : Joël de Rosnay a repris cette expression dans "La révolte du pronétariat : des mass média aux média des masses". Le Web 2.0 est permis par les TIC.
Le concept de technologies de l'information et de la communication est à rapprocher de celui de société de l'information en raison de leur impact sur le fonctionnement de la société.

Le terme NTIC (nouvelles technologies de l'information et de la communication) a souvent été utilisé dans la littérature francophone au cours des années 1990 et au début des années 2000 pour caractériser certaines technologies dites « nouvelles ». Mais les définitions fournies sont généralement floues ou équivalentes à celles des TIC. La qualification de « nouvelles » est ambigüe, car le périmètre des technologies dites nouvelles n'est pas précisé et varie d'une source à l'autre. En raison de l'évolution rapide des technologies et du marché, des innovations déclarées « nouvelles » se retrouvent obsolètes une décennie plus tard. Parfois il s'agit de distinguer les technologies basées sur l'Internet par opposition aux télécommunications traditionnelles. Parfois, il s'agit de distinguer les plateformes du web 2.0 par opposition aux premières technologies de l'Internet qui ont maintenant trois décennies et sont tout à fait obsolètes. Parfois, il s'agit de caractériser les services issus de la convergence des télécommunications et des multimédias utilisant des accès à haut débit car les applications utilisant seulement les accès à bas débit peuvent difficilement être qualifiées de « nouvelles ». Parfois, les NTIC incluent aussi la téléphonie mobile, mais les premières technologies mobiles qui ont plus de trois décennies peuvent-elles être qualifiées de « nouvelles » ? Le sigle NTIC est source de confusion car il ne fait l'objet d'aucune définition officielle par les institutions internationales responsables de ce domaine alors que le terme de TIC (ou ICT en anglais) y est défini comme étant l'intégration des technologies des télécommunications, de l'informatique et des multimédias. L'utilisation des moteurs de recherche montre que le sigle "NICT", traduction de NTIC en anglais, est très rarement utilisé (en dehors de la traduction de documents d'origine francophone) et qu'il est plutôt fait mention de l'évolution rapide de ce domaine en permanence. Cela montre qu'il n'est pas utile d'établir des catégories rigides pour distinguer ce qui est nouveau de ce qui ne l'est pas.

La désignation « communications électroniques », largement utilisée dans les textes juridiques et réglementaires européens, correspond aux TIC à l'exception de certains systèmes de diffusion de télévision et de radio qui ne sont pas soumis aux mêmes droits et obligations réglementaires. Cette distinction n'est pas sans poser des problèmes aux autorités chargées de la réglementation et de la régulation du fait de la convergence des technologies des télécommunications et de l'Internet. Cette convergence permet un usage croissant de la diffusion de la télévision et de la radio par Internet grâce aux technologies à haut débit.

L'ARCEP évite l'utilisation du sigle TIC dans ses documents et utilise systématiquement le terme de Communications électroniques qui est défini ainsi: "On entend par communications électroniques les émissions, transmissions ou réceptions de signes, de signaux, d'écrits, d'images ou de sons, par voie électromagnétique". À noter que dans le document cité en référence, les termes de TIC ou Technologies de l'Information et de la Communication ne sont pas mentionnés.

Les sigles anglais correspondant sont "IT", pour "information technology" et "ICT" pour "information communication technology" , le sigle "NICT", pour "new information and communication technology" étant beaucoup plus rarement utilisé.

Le terme infocommunications (ou info-com) a surtout été utilisé dans les années 1990 dans les pays anglo-saxons et les pays de l’Europe de l’Est pour désigner le concept de convergence entre télécommunications et informatique avec une approche équivalente au terme communications électroniques, mais en considérant particulièrement les questions de traitement de l’information et de manipulation de contenus avec l’utilisation d’Internet .

Dans le système éducatif français, on évoque plutôt les technologies usuelles de l'information et de la communication (TUIC) et les technologies de l'information et de la communication pour l'enseignement (TICE).

Les TIC jouent un rôle majeur dans la compétitivité des entreprises et dans l'efficacité des administrations et des services publics (santé, éducation, sécurité). Les TIC sont devenus également un enjeu crucial pour la production et la diffusion des biens culturels. Selon le rapport « Technologies Clés 2015 », "le secteur des technologies de l’information et de la communication (TIC) est devenu un segment majeur de l’économie des principaux pays industrialisés avec une contribution directe de 5,9 % du PIB en Europe (et 7,5 % aux États-Unis). Au-delà du secteur lui-même, les TIC contribuent au développement de tous les autres secteurs économiques, les TIC représentant en effet plus de 50 % de la croissance de la productivité en Europe (source: Commission Européenne), les perspectives de croissances du secteur STIC (Sciences et Technologies de l’Information et de la Communication) sont par ailleurs considérables avec +8 % de croissance par an pour l’économie d’internet d’ici 2016 comme le rapporte le Boston Consulting Group".

Le phénomène le plus caractéristique des TIC est le brouillage des frontières entre télécommunications, informatique et audiovisuel/ multimédias. Un rapport des Nations unies insiste sur l'impact de la convergence des TIC :
""La convergence a supprimé nombre de distinctions entre les secteurs des TIC, notamment la radiotélédiffusion, l’informatique et les télécommunications, et a favorisé l’innovation dans des secteurs autres que les TIC, tels que les services financiers. Dès lors, les utilisateurs ont accès à du contenu, à des services et à des applications sur de multiples plates-formes, ce qui accroît la versatilité et la sophistication de l’accès à l’information et de l’utilisation des communications"."

La fertilisation croisée des produits issus de domaines autrefois bien séparés et maintenant confondus est à l’origine d’une multitude de services innovants. Les quelques exemples suivants montrent qu’il est de plus en plus difficile d'étudier l'un des domaines des TIC sans tenir compte des autres.

Exemple 1 : Les opérateurs de télécommunications ne sont plus seulement des exploitants de réseaux de télécommunications, ils deviennent des fournisseurs de services Internet en s’appuyant sur les accès aux abonnés qu’ils détiennent et la téléphonie n’est plus qu’un segment de service parmi bien d’autres . Par exemple France Telecom est devenue également un distributeur de chaînes de télévision et de musique.

Exemple 2 : les réseaux des TIC.
Dans les années 1980 et 90, les réseaux étaient spécialisés par domaine et il fallait différents systèmes de transmission (et parfois différentes infrastructures) pour la téléphonie, la transmission de données entre ordinateurs, la radio et la télévision. Maintenant les réseaux en technologie IP à haut débit sont communs pour toutes les formes de service et les octets de la téléphonie sont acheminés par les mêmes routeurs sur les mêmes artères que les octets des consultations des sites Internet, des transferts de fichiers, du streaming de vidéos ou du courrier électronique .

Exemple 3 : les ordinateurs personnels.
Dans les années 1980, un ordinateur servait essentiellement au traitement de texte et au calcul avec un tableur. Puis dans les années 1990 il a servi aussi au courrier électronique et aux présentations par diapositives. Un ordinateur était donc clairement un équipement informatique. Mais avec la convergence numérique, un ordinateur personnel devient un outil de base pour le domaine de l’audiovisuel et des multimédias, car il est aussi de façon routinière :

Exemple 4 : les terminaux téléphoniques mobiles.
Au début des années 2000, un terminal téléphonique servait essentiellement aux communications téléphoniques et la facture était composée d’un abonnement et d’un montant proportionnel aux durées des communications par catégorie en fonction de la distance. La convergence numérique est particulièrement illustrée par les nouveaux terminaux téléphoniques appelés smartphones. Ils contiennent plus de capacité en mémoire et de puissance de calcul que les ordinateurs personnels des années 1980 et même 90. Ils permettent de prendre des photos et des vidéos avec des performances équivalentes à de bons appareils de photos ou caméscopes des années 1990. Ils sont utilisés comme des baladeurs pour écouter de la musique enregistrée ou voir des images ou des vidéos enregistrées.

Exemple 5 : les industriels producteurs de biens électroniques.
Un fabricant d’ordinateur (Apple) devient l’un des plus grands fabricants de terminaux téléphoniques et devient un intermédiaire de premier plan dans la vente de produits et services audiovisuels avec le logiciel multimédia iTunes. Apple développe aussi sa propre suite bureautique iWork, en concurrence directe avec Microsoft Office.

Les technologies de l'information et de la communication regroupent un ensemble de ressources techniques nécessaires à la mise en œuvre des services de l'information et de la communication pour produire, manipuler, convertir, stocker, gérer, transmettre et retrouver l'information et pour communiquer.

On peut regrouper ces techniques par catégories suivantes :

Les services de l'information et de la communication sont regroupés en différentes catégories dont les plus connues sont :








L’investissement dans les TIC serait l’un des principaux moteurs de compétitivité des entreprises. En effet, selon des études de l'OCDE, les TIC seraient un facteur important de croissance économique aux États-Unis.


La mondialisation des TIC, permettant un accès banalisé et 24h/24 depuis n'importe quel point du globe à un ensemble de ressources (données, puissance informatique), a aussi des effets pervers sur le plan de l'environnement :
Selon France Télécom, une utilisation judicieuse (pour le télétravail par exemple des NTIC) permettrait de réduire de 7 % les émissions des gaz à effet de serre entre 2010 et 2020 (1/3 de l'objectif français de 20 % selon les protocoles actuels), mais en réalité :

D'après une étude publiée lors du colloque Colloque EJC - ICT 2012, les « Technologies de l'Information et de la Communication - émettent autant de (2 % à 5 % des émissions globales annuelles, selon les pays) que l'industrie aéronautique tout en affichant une croissance de 20 % par an ».
Selon un rapport "Votre cloud est-il Net?" (avril 2012), .

En 2010, en France, une charte a engagé les opérateurs développer le réseau tout en économisant l'électricité, et améliorer la récupération et le recyclage des matériels informatiques. De 2005 à 2008, les unités centrales desktop vendues ont légèrement diminué leur consommation d'énergie, de même que les écrans, alors que la consommation des portables augmentaient légèrement.

L'empreinte environnementale des TIC est très importante, puisqu'elles nécessitent pour leur fabrication une très grande quantité de matériaux, en particulier de métaux. Le nombre de métaux de la table de Mendeleïev sollicités pat la fabrication des TIC est ainsi passé de 10 dans les années 1980 à 60 dans les années 2010. Pendant cette période, la demande de l'industrie des TIC en métaux a triplé. Le PNUE a publié en 2013 la composition d'un ordinateur personnel fixe, et l'Öko Institute a fait une étude équivalente en 2012 pour les ordinateurs portables.

Le développement des activités tertiaires au détriment des activités primaires et secondaires dans les pays industrialisés a augmenté les besoins de traitement de l'information et de communication des entreprises.

Selon l’étude publiée dans le chapitre 4 du rapport IC4D06, Information and Communications for Development 2006: Global Trends and Policies, de la Banque Mondiale portant sur entreprises de 26 secteurs dans 56 pays en développement, les entreprises qui ont davantage recours aux TIC font preuve d’une plus grande productivité, d’une croissance plus rapide, d’investissements plus conséquents et d’une plus grande rentabilité. De nombreuses petites et moyennes entreprises (PME) ont participé à cette étude.

"La diffusion des technologies de l’information représente l’exemple le plus fragrant de la pénétration intersectorielle des progrès", voir: Croissance, emplois et productivité dans le secteur tertiaire : controverses théoriques et réalités suisses.

L'augmentation du niveau d'éducation a favorisé l'utilisation d'ordinateurs personnels et de logiciels plus ou moins complexes par un pourcentage croissant de la population.

Les collectivités locales investissent dans la formation sur les TIC pour améliorer la compétitivité des entreprises de leurs territoires.

Les mêmes outils de TIC sont utilisés aussi bien dans le domaine professionnel que dans le domaine privé, ce qui entraîne à la fois un brouillage de la frontière entre ces deux domaines et une stimulation pour se doter de ces outils de TIC.

La distribution des dépenses par les ménages a sensiblement évolué avec une part croissante de dépenses pour les TIC, même dans les milieux populaires, avec en conséquence une réduction d'autres types de dépenses (par exemple, réduction d'achats de journaux, de magazines, de CD...). Ce phénomène a été analysé par l'agence Aravis à partir des statistiques sur les dépenses des ménages. Les résultats montrent que: "les services de communication dont le poids a quintuplé depuis 1960 dans le budget : augmentation de la consommation en volume nettement plus forte (+ 8,1 % par habitant) que les autres postes (+ 2,5 %), principalement dans les années 1970 (diffusion de la téléphonie fixe) et depuis 1995 (téléphonie mobile et internet)".

À la question : « Si vous ne deviez regarder que deux médias dans votre vie, lesquels choisiriez-vous ? », les enfants du millénaire répondent : Internet à 61 %, la télévision à 49 %, le cinéma à 35 %, la radio à 29 %, la presse quotidienne à 17 % et les magazines à 9 %.

Les terminaux des TIC à haut débit (ordinateurs personnels et téléphones 3G) sont de plus en plus utilisés pour la radio et la télévision en lieu et place des terminaux traditionnels spécifiques à chaque catégorie d'audiovisuel. La possibilité de voir des émissions de télévision en mode différé (catch-up TV ou replay) et d'écouter des émissions de radio en mode différé par le téléchargement (podcast) augmente considérablement la gamme de ce qu'on peut voir ou entendre. En particulier, les jeunes de 14 à 24 ans veulent pouvoir consommer leurs émissions préférées n'importe quand, n'importe où et sur n'importe quel écran.

Le streaming est de plus en plus utilisé, ce qui fait reculer le téléchargement illégal.

Des projets sont réalisés pour utiliser les TIC pour lutter contre l'isolement des séniors, par exemple le projet Monalisa.

La possibilité d'intervenir en temps réel dans les débats et forums apporte un nouveau degré de participation des consommateurs. Grâce au web 2.0, les réseaux sociaux et les services à base de contenus générés par les utilisateurs ont connu un essor spectaculaire et ont transformé les relations sociales pour des centaines de millions de personnes.

La rapide augmentation de la capacité des processeurs (loi de Moore) et la rapide augmentation de la capacité des artères de transmission (avec la généralisation des fibres optiques) ont entraîné une chute des coûts unitaires des services de communications et surtout une généralisation progressive des systèmes d'accès à Internet à haut débit.

La numérisation de toutes les informations de toute nature: textes, images, photos, musique, films, etc. a permis de tirer parti de la convergence numérique dans les TIC en mutualisant de couteuses infrastructures de commutation et de transmission au lieu d'avoir des infrastructures spécifiques pour chaque catégorie de service (téléphone, transmission de données, diffusion de l'audiovisuel). Cette caractéristique est d'autant plus importante que ces infrastructures sont caractérisées par des coûts fixes élevés et des coûts marginaux faibles. Cela a entraîné un profond bouleversement des modèles économiques des opérateurs de télécommunications.

En quinze ans, le marché des TIC a été bouleversé, d'abord avec l'essor phénoménal de la téléphonie mobile et de l'Internet fixe à haut débit, puis récemment avec l'Internet mobile à haut débit, ce qui caractérise l'étape majeure de la convergence des technologies du traitement de l'information et de la communication.

L'évolution des coûts des systèmes des TIC et le développement de la concurrence avec l'émergence nouveaux fournisseurs de service ont favorisé l'essor spectaculaire des offres commerciales des services de l'information et de la communication. Cet essor est caractérisé par des bouquets de service avec des tarifs forfaitaires abordables incluant généralement des appels téléphoniques illimités, un accès à l'Internet à haut débit et un accès à des chaînes de télévision

L'objectif des opérateurs de télécommunications n'est plus de vendre des minutes de communications, mais de la bande passante et des services à valeur ajoutée. La part des revenus des opérateurs de télécommunications venant de la téléphonie vocale est en baisse constante alors que la part des revenus venant des services de données et de l'Internet est en hausse constante.

Dans les années 1980 et 1990, le développement des TIC s’est mesuré essentiellement par l’augmentation du nombre de lignes téléphoniques fixes dans un contexte de privatisation et d’ouverture du marché à la concurrence. Dans les années 2000, il s’est mesuré par le nombre d'abonnés à la téléphonie mobile et à l’Internet, d’abord en bas débit, puis en haut débit. Actuellement, la diffusion de la téléphonie ayant atteint des niveaux spectaculaires même dans les pays en développement, le développement des TIC se mesure surtout par le nombre d'accès aux services Internet à haut débit et à leur utilisation effective dans le cadre de la mise en œuvre de la société de l’information. Au nivaux économique, les indicateurs des TIC sont divisés en quatre grandes catégories : (I) infrastructure et accès au TIC, (II) accès aux TIC et leur utilisation par les ménages et les particuliers, (III) utilisation des TIC par les entreprises et, (IV) secteur des TIC et commerce des biens TIC. Ces quatre grandes catégories mentionnées sont l'objet des résultats de recherche obtenus par le Partenariat sur la mesure des TIC au service du développement lancé en juin 2004.

Il est donc important que le développement des TIC soit mesuré par un ensemble d'indicateurs prenant en compte tous les éléments essentiels des TIC et l'accès aux services Internet à haut débit en particulier.

Globalement, selon les estimations de l'UIT, le nombre d'abonnements au large bande fixe aura dépassé les 688 millions avant la fin de 2013, soit un taux de pénétration de 9,8 %. Parallèlement, le nombre d'abonnements activés au large bande mobile augmentera de 21 % entre 2010 et 2013 pour atteindre 2,1 milliards fin 2013 - chiffre qui sera presque le triple du nombre d'abonnements au large bande fixe, mais restera nettement inférieur à celui des abonnements au cellulaire mobile, qui devrait atteindre les 6,84 milliards fin 2013. Le nombre total d'internautes dans le monde passera, toujours selon les estimations, le cap des 2,7 milliards fin 2013. Dans les pays en développement, il aura plus que triplé entre 2007 et 2013 pour dépasser le chiffre de 1,8 milliard.

Stimulé par la croissance du nombre de connexions Internet sur des plates-formes fixes et mobiles, le trafic utilisant le protocole Internet (IP) a littéralement explosé, passant de tout juste 1 pétaoctet par mois il y a vingt ans à pétaoctets en 2012, selon les estimations. Et rien ne semble devoir arrêter la croissance future du trafic IP: en 2013, il devrait augmenter de quelque pétaoctets par mois, soit l'équivalent du double du trafic mondial total entre 1994 et 2003. Cette croissance phénoménale est alimentée par l'augmentation du nombre de personnes et d'appareils connectés et par la généralisation de contenus en ligne abondants, diversifiés et, dans la plupart des cas, gratuits.

Parallèlement, à l'heure actuelle, l'internet haut-débit reste financièrement inaccessible pour les pays en développement. De même que l'accès à internet reste aussi inaccessible aux personnes en situation d'illectronisme, ceci pose le problème de la fracture numérique. 

L'UIT (Union Internationale des Télécommunications) est l'institution spécialisée des Nations unies pour les questions relatives aux technologies de l'information et de la communication (TIC). L'UIT fournit le système de statistiques sur les TIC qui est le plus complet et le plus fiable en raison des contributions des ministères et autorités de régulation de tous les pays membres.
Les statistiques collectées et distribuées par l'UIT sont relatives aux domaines suivants:

Le site de l'UIT fournit un ensemble de définitions des concepts et des indicateurs caractérisant les Technologies de l'Information et de la Communication .

L'UIT fournit gratuitement des données historiques annuelles de 2000 à 2012 de tous les pays / économies membres de l'UIT pour les indicateurs suivants au niveau mondial et par région.


Par ailleurs, l'UIT commercialise une base de données (sur support informatique ou sur support papier) fournissant les données historiques annuelles pour une centaine d'indicateurs caractérisant les TIC pour les pays membres de l'UIT.

En 2013, il y avait plus de 2,4 milliards de téléphones cellulaires, tablettes et PC vendus dans le monde. 

De plus, au cours de la même année, 476 millions de tablettes ont été achetées et 271 millions d'ordinateurs portables.

Une liste d'indicateurs fondamentaux relatifs aux TIC a été mise au point par une structure spécialisée internationale appelée « Partenariat sur la mesure des TIC au service du développement ». Cette structure a été lancée en juin 2004, et se compose des membres suivants : Eurostat, l’Union internationale des télécommunications (UIT), l’Organisation de coopération et de développement économiques (OCDE), la Conférence des Nations unies sur le commerce et le développement (CNUCED), quatre commissions régionales de l’ONU — la Commission économique des Nations unies pour l’Afrique (UNECA), la Commission économique pour l’Amérique latine et les Caraïbes (CEPALC), la Commission économique et sociale pour l’Asie et le Pacifique (CESAP) et la Commission économique et sociale pour l’Asie occidentale (CESAO) - -, l’Organisation des Nations unies pour l’éducation, la science et la culture (UNESCO), l’Institut de statistique de l’UNESCO, le Groupe d’étude des technologies de l’information et des communications et la Banque mondiale. L'objectif de ce Partenariat est d'homogénéiser le choix des indicateurs fondamentaux relatifs aux TIC et de proposer des définitions standard de façon à éviter l'incohérence des appellations et des définitions des indicateurs publiés précédemment à travers le monde et selon les différentes institutions. Cette homogénéisation est indispensable pour garantir des comparaisons pertinentes des différents pays à une date donnée et une standardisation permet de garantir un suivi pertinent des données historiques d'un pays sur une période donnée.

Liste des indicateurs fondamentaux relatifs aux TIC

"Indicateurs fondamentaux sur l’accès et l’infrastructure"


"Indicateurs fondamentaux sur l’accès aux TIC et leur utilisation par les ménages et les particuliers"


"Indicateurs fondamentaux sur l’accès l'utilisation des TIC par les entreprises"


"Indicateurs fondamentaux sur le secteur des TIC "

"Indicateurs fondamentaux sur l'utilisation des TIC par les particuliers, les entreprises"

"Utilisation des TIC par les administrations et d’autres organismes du secteur public"

L'indice de développement des TIC ou IDI est un indice composite visant à caractériser le développement des TIC de chaque pays. Cet indice a été mis au point par un ensemble d'institutions internationales et un rapport annuel est publié par l'UIT pour actualiser les résultats. On y trouve la valeur de l'IDI de chaque pays et ses composantes essentielles. On y trouve aussi un autre indicateur fondamental sur le TIC, le coût et l'accessibilité du large bande. Le résumé analytique de 2012 fournissait également un autre indicateur fondamental : le panier de prix des TIC qui est une valeur composite des prix d'éléments représentatifs de services des TIC. Des résultats plus complets, et une définition complète du calcul de l'IDI et de ses composantes ainsi qu'une définition du calcul du panier de prix des TIC avec la définition des sous-paniers sont disponibles dans la version anglaise du rapport de 2013 "Measuring the Information Society".

Chaque année, le Forum économique mondial publie le "Networked Readiness Index", un indice défini en fonction de la place, de l'usage et du bénéfice que peut tirer un pays des technologies de l'information et des communications. Cet indice prend en considération une centaine de pays (133 en 2009-2010) et permet d'établir un classement mondial. Cet indice reste encore sous le niveau souhaité si l'on tient compte des besoins et des limitations imposées par les situations actuelles.

"Source : Forum économique mondial, 2010"
Nombre de pays : 133

"Source : Forum économique mondial, 2012"
Nombre de pays : 142





</doc>
<doc id="14237" url="https://fr.wikipedia.org/wiki?curid=14237" title="TIC">
TIC

TIC, sigle de trois lettres, peut signifier :


</doc>
<doc id="14241" url="https://fr.wikipedia.org/wiki?curid=14241" title="Irlande (pays)">
Irlande (pays)

L’Irlande (en irlandais ' et en anglais ') est un pays d'Europe de l'Ouest occupant la majeure partie de l'île d'Irlande dans l'océan Atlantique nord. Elle occupe 26 des 32 comtés traditionnels, les six autres formant l'Irlande du Nord, l'une des quatre nations constitutives du Royaume-Uni.

L'Irlande est une république parlementaire dont les deux langues officielles sont l'irlandais et l'anglais. Son hymne national est "Amhrán na bhFiann" et son drapeau est constitué de trois bandes verticales verte, blanche et orange. Le pays a pour capitale Dublin () et a pour monnaie l'euro. L'Irlande est membre de l'Union européenne depuis 1973. Afin de la différencier de l'île d'Irlande, on la nomme parfois "République d'Irlande" (en irlandais "" et en anglais ) mais ce n'est pas son nom officiel (le "Republic of Ireland Act" de 1949 en fait sa « description officielle »).

Le pays s'étend sur une superficie de , et sa population s'élève en 2016 à . La plus grande ville est la capitale Dublin, les autres principales villes étant Cork, Limerick et Galway. L'Irlande revendique son identité celtique, si bien que, sur le plan culturel, elle représente habituellement l'île d'Irlande considérée comme l'une des six « nations celtiques ». Le symbole officiel du pays est la harpe celtique (qui figure sur les pièces d'euro), mais le trèfle irlandais est souvent utilisé comme autre symbole, notamment par l'équipe irlandaise de rugby.

Colonisée par les Celtes durant la préhistoire qui constituent quatre royaumes qui donneront naissance aux provinces actuelles (Ulster, Connacht, Munster et Leinster), l'Irlande est christianisée au par saint Patrick. Au Moyen Âge, l'Irlande est envahie par les Anglo-Normands. Cette conquête aboutit à l'union de l'Irlande et de la Grande-Bretagne en 1800. Au , l'Irlande est ravagée par la Grande famine et connaît une forte émigration vers le Nouveau Monde. Le nationalisme commence à se développer à la fin du siècle, et l'échec du projet de "Home Rule" aboutit à la guerre d'indépendance irlandaise et à la partition de l'Irlande. Avec le traité anglo-irlandais, l'Irlande accède à l'indépendance en 1922 mais l'Irlande du Nord quitte le nouvel État libre d'Irlande le lendemain de sa création. En 1937, la Constitution républicaine de l'Irlande est adoptée, en 1949, l'Irlande se déclare officiellement république.

Adepte de la concurrence fiscale, de la déréglementation et considéré comme un paradis fiscal, le pays a connu une forte croissance depuis le début des années 1990 avant de subir l'explosion de la bulle spéculative immobilière et d'être l'un des plus touchés par la crise financière de 2008, ce qui entraîna une crise économique et politique majeure pour le pays jusqu'en 2011. À titre d'exemple, en 2010, son déficit public s'établissait à 32,4 % du PIB pour un taux de chômage supérieur à 14 % de la population active. Le pays a depuis renoué avec la croissance et le PIB a connu une croissance de 7,8 % en 2015, soit le niveau le plus élevé de l'Union européenne, un chiffre depuis révisé à 26,3 %, soit la croissance la plus élevée au monde.

Selon la Constitution de 1937, le nom officiel de l'État est simplement « Irlande » ("Éire" en irlandais et "Ireland" en anglais).

Depuis l'entrée en vigueur du "Republic of Ireland Act" en 1949, la « description officielle » du pays est « République d'Irlande » ("Poblacht na hÉireann" en irlandais ou "Republic of Ireland" en anglais), mais le nom de l'État reste « Irlande ». Ainsi, par exemple, le chef d'État est le "président d'Irlande" et non pas "président de la République d'Irlande". 

Chaque État de l'Union européenne porte un nom propre dans chacune des langues officielles. Le nom de l'État est donc « Éire » en irlandais, « Ireland » en anglais et « Irlande » en français. Mais les appellations « Poblacht na hÉireann », « Republic of Ireland » ou « République d’Irlande », n'ont aucun caractère officiel pour l'Union.

L'Irlande est située sur l'île éponyme, au nord-ouest du continent européen dans l’océan Atlantique nord. Elle s'étend sur plus de 85 % de la superficie de cette île, soit (seul l'Irlande du Nord, au nord-est de l'île, est restée sous souveraineté britannique). Le pays est divisé en vingt-six comtés depuis la partition du pays en 1921.

La capitale de l'Irlande est Dublin, mais d'autres villes possèdent : Galway, Cork, Waterford et Limerick.

La morphologie de l'île comprend une plaine centrale entourée de montagnes et de collines, particulièrement dans le Donegal et le Wicklow. Mais les sommets les plus hauts se trouvent au sud-ouest avec les Macgillycuddy's Reeks qui comprend le point culminant du pays, le Carrauntuohil avec ses .

Les paysages sont faits de tourbières, de lacs et de rivières. L'Irlande possède d'ailleurs dans la totalité de son cours, le plus long fleuve des îles Britanniques (appelées ici îles Anglo-Celtes) : le Shannon, qui parcourt du nord au sud-ouest avant de se jeter dans l'océan Atlantique. Deux autres cours d'eau sont également particulièrement importants : la Barrow et la Blackwater.

Au sud-ouest, les falaises de Moher surplombent l'océan Atlantique. Non loin de là, les îles d'Aran font face à la baie de Galway. À l'est du pays, au nord de Dublin, le rivage est assez plat. Mais tout autour de l'île s'étendent de nombreuses plages de sable fin.

Le climat est de type tempéré océanique, ou Cfb selon le système de classification de climat de Köppen. Les étés sont généralement doux, la température ne dépassant que très rarement les .
Les hivers sont frais et pluvieux. Les températures minimales ne descendent qu'exceptionnellement en dessous de .

Le Centre et l'Est de l'île connaissent des températures un peu plus basses en hiver que l'Ouest du pays, qui bénéficie énormément de l'effet du Gulf Stream. À Dublin, les mois les plus frais sont janvier et février des moyennes de 3° pour les minimales et de 8° pour les maximales ; en été, les températures moyennes oscillent entre 12° pour les minimales et 19° pour les maximales. Les précipitations sont assez importantes ( de précipitations annuels en moyenne à Dublin). Elles tombent rarement sous forme de neige (quatre jours de neige par an en moyenne à Dublin).

Les écarts de température dans une même journée sont faibles : il n'est pas rare que la température soit constante du matin au soir, ce qui contraste avec les brusques changements de temps durant les saisons intermédiaires (printemps et automne), au cours desquelles la rapidité d'évolution du temps est étonnante avec des épisodes de « giboulées » très fréquents.

Les premiers occupants de l'Irlande sont arrivés sur l'île environ au . Ensuite sont venues des peuplades mégalithiques, puis néolithiques, qui ont développé l'agriculture et l'élevage. Au , les Celtes vont imposer leur civilisation laténienne.
C'est une société parfaitement organisée en clans, et parlant le gaélique dans lequel les premiers poètes composent des chants et légendes qui constituent le patrimoine autochtone.

Les traces les plus anciennes du peuplement de l'île remontent au , elles ont été retrouvées dans le comté de Waterford. Une migration de populations mésolithiques est attestée vers le , il semble qu'il s'agisse de peuples venus d'Europe du Nord, passés par l'Écosse. Ils se sont installés dans la région de l'actuelle Ulster. Leur mode de vie est une économie de prédation (chasse et pêche) qui va progressivement évoluer vers l'agriculture et l'élevage. Ils connaissent l'usage d'outils en pierre polie, cultivent la terre et ont du bétail. C'est une civilisation mégalithique qui érige des dolmens à vocation funéraire. De nouveaux arrivants entre apportent l'âge du bronze. C'est de cette époque que date l'exploitation de mines de cuivre dans les régions de Cork et Kerry et d'or dans le Wicklow.

C'est vers - 500 que les Celtes font leur apparition en Irlande. Leur civilisation sur l'île va durer près de mille ans. Leur arrivée s'est faite par deux routes différentes : par l'actuelle Grande-Bretagne et par l'Espagne. La mythologie celtique a d’ailleurs conservé le souvenir de cette "origine espagnole" puisque, selon le "Lebor Gabála Érenn (Livre des Conquêtes d’Irlande)", les Milesiens (c’est-à-dire les Gaels) sont dits fils de Míl Espáine. Avant l'introduction de la civilisation de La Tène (second âge du fer), l’histoire d’Erin est largement mythique.

La structure de la société celtique reprend le schéma de la structure sociale tripartite des Indo-européens au sommet de laquelle on trouve une classe sacerdotale composée des druides, des bardes et des vates. Les druides ont en charge la religion, le sacrifice, la justice, l’enseignement, la poésie, la divination ; les bardes sont spécialisés dans la poésie orale et chantée et doivent faire la louange, la satire ou le blâme ; les vates sont des devins qui se consacrent plus particulièrement à la divination et à la médecine. En Irlande, les "filid" (bardes) vont devenir les membres les plus influents de cette classe sacerdotale, dont une des prérogatives est de conseiller le roi. Dans la civilisation celtique, le rôle du roi est non seulement de mener la guerre, mais surtout de redistribuer les richesses et de dire la justice, inspirée par les druides. Longtemps ces rois n’ont été que des chefs au pouvoir incertain et aux successions problématiques. La deuxième classe de la société est celle des guerriers et la troisième est celle des producteurs, artisans, agriculteurs et éleveurs.
Ces clans vont progressivement fusionner pour constituer quatre royaumes (ou provinces) : l’Ulster, le Leinster, le Munster et le Connacht (Connaught). Au début du , un Ard rí Érenn (roi suprême d’Irlande) étend son pouvoir sur toute l’île, il siège à Tara capitale du Royaume de Mide. Les autres rois lui doivent le « boroma », tribut payable en bétail – son non-paiement entraîne des guerres dont la mythologie se fait l’écho, tout autant que les razzias.

Les Romains, qui occupent la "Bretagne" (Grande-Bretagne actuelle), n'occuperont jamais l'Irlande (qu'ils appellent "Hibernia", « Hibernie »), peuplée de populations trop difficilement assimilables et loin du centre de l'Empire.

La christianisation de l’Irlande marque la fin de la religion celtique, du moins en ce qui concerne sa mythologie, car la structure de la société s’est maintenue, avec une classe sacerdotale prédominante. Les circonstances exactes de l’introduction du christianisme dans l’île sont mal connues, d’autant que les textes relatifs à son initiateur, saint Patrick, sont largement hagiographiques.

Padraig serait né en 390 en un lieu incertain de l’île de Bretagne. Il était le fils d’un fonctionnaire britto-romain. En 405, il aurait été victime d’une razzia de Gaels et aurait été emmené comme esclave en Irlande, sous le règne du Ard rí Érenn, Niall Noigiallach. Pendant six années de captivité, passées à surveiller les troupeaux, sa foi en Dieu se serait affermie et une fois évadé, il aurait poursuivi ses études théologiques en Gaule. Les dates de son retour en Irlande sont incertaines (entre 432 et 490 selon les différentes thèses) mais la conversion de l’île, probablement commencée avant lui, aurait connu un moment décisif sous le règne du roi Lóegaire, fils de Niall. Patrick est souvent décrit conversant avec les druides pour tenter de les convaincre que le Dieu unique est plus puissant que la magie druidique. La légende rapporte aussi qu’il a chassé tous les serpents de l'île et qu'il avait l'habitude d'expliquer le principe de la Trinité en montrant les feuilles du trèfle. Le concept de "triades" était en effet très répandu dans la mythologie celtique. Patrick est mort vers 461.

Dans ce contexte, la conversion du pays ne s’est faite que par celle des "filid", qui sont devenus les porteurs de la nouvelle religion, de manière pacifique. Le rite celte est imprégné par les usages monastiques. Des moines des pays celtiques sont nombreux venus dans plusieurs pays d'Europe occidentale pour évangéliser et fonder des monastères. Les monastères de Clonard, de Clonmacnoise, ou de Glendalough sont des centres importants de culture et de spiritualité. L’église d’Armagh est fondée par saint Patrick vers 445.
Au , fondation du monastère de Bangor (558) par Comgall.

Ce n’est qu’au que le synode de Whitby (664) préconise l'abandon des rites celtes au profit du rite romain mais certains usages celtiques se maintiennent jusqu'au .

Au , la croissance démographique et des guerres de succession entraînent, semble-t-il, des peuples scandinaves, Norvégiens et Danois, à sortir de leurs territoires. Les Varègues partent vers l’est et fondent en Russie des embryons d’États, les Vikings (du norrois "fara í víkingu" : partir en expédition et "víkingar" qui désigne ceux qui partent) déferlent sur les îles britanniques à l’ouest et le continent au sud.

L’Irlande vit un âge d’or intellectuel par le dynamisme de ses institutions religieuses, mais sur le plan politique l’île est divisée entre cent à cent cinquante "tuatha" (les clans), à la tête de chacun desquels se trouve un "rí" (roi). Ces chefs sont eux-mêmes assujettis au roi d’une des cinq provinces (Ulster, Connacht, Munster, Leinster et Mide). Le "Ard rí" (roi suprême) porte un titre honorifique : son titulaire ne bénéficie pas d'une réelle autorité.

C’est dans ce contexte d’instabilité que les Vikings arrivent dans l’île. Les premières expéditions attestées sont de 795, ils brûlent l’église de l’île Lambay ainsi que les monastères d’Inisbifin et d’Inismurray ; ce dernier subira un nouvel assaut en 807. Dès 812 les raids se concentrent sur la côte ouest, puis sur les rivages de la mer d'Irlande. Au début des années 820 le tour de l’île est accompli. Pendant une quarantaine d’années, les Vikings vont multiplier les raids et les razzias, privilégiant les monastères, non pour des raisons religieuses, mais parce que plus riches en trésors. Durant les années 830, ils remontent les fleuves et pénètrent à l’intérieur des terres qu’ils ravagent. En 836, ils empruntent la rivière Shannon et pillent le Connaught. L’année suivante, deux flottes d’une soixantaine de drakkars chacune, reconnaissent la Liffey et La Boyne, les territoires sont systématiquement ravagés, les habitants massacrés. Nombreux sont les exemples de leurs méfaits. L’hiver 840-841 marque une étape, puisque pour la première fois les Vikings passent la saison dans l’île et s’installent dans des places fortifiées qui deviennent aussi des lieux de commerce : Dublin, Annagassan, puis par la suite, Wexford, Cork, Limerick, pour ne citer que quelques établissements. Ce sont autant de bases retranchées qui permettent des expéditions vers l’intérieur, dont le point culminant semble être l’année 845, à tel point que l’on parle d’invasion. Le revers de la médaille est que les rois celtes peuvent parfois les contenir et les assiéger.

Après la victoire de Brian Boru à la bataille de Clontarf, en 1014, l'Irlande connaît une brève période d'indépendance. Mais à la fin du , l'Angleterre entreprend sa conquête. Après s'être limitée à l'ouest de l'île, en 1494, la couronne anglaise déclare sa domination sur toute l'île (loi Poynings). En 1541, Henri VIII prend le titre de roi d'Irlande. La colonisation par les confiscations de terres se développe alors ( Plantations en Irlande).

Une grande révolte éclate en 1641, brisée par Oliver Cromwell en 1649 (massacres de Drogheda et Wexford). Les Irlandais ont profité de la Première révolution anglaise pour tenter de retrouver leur indépendance. Cromwell débarque à Dublin (durant l'été 1649) avec ses soldats, les « Côtes de Fer », entreprend la reconquête de l'île et organise un véritable massacre. Selon les sources, entre le tiers et la moitié de la population de l'île est massacrée. La sauvagerie de l'armée de Cromwell, très anti-catholique, contribuera à créer de profonds clivages entre catholiques et protestants en Irlande. Après sa défaite, l'Irlande est soumise à l'autorité et aux lois de l'Angleterre et les terres du Nord du pays sont confisquées et attribuées à des colons venus d'Écosse et d'Angleterre.

Les Irlandais sont par la suite soumis à une répression et à des discriminations très fortes de la part des autorités anglaises, qui continuent les décennies suivantes. Des lois leur interdisent notamment de pratiquer leur religion catholique.

Jacques II, roi catholique chassé du trône de l'Angleterre protestante, tente de reprendre pied en Irlande et y est défait à la bataille de la Boyne (1690), le sort de l'Irlande s'aggrave encore. En 1695, Guillaume promulgue des « lois pénales » anti-catholiques. Celles-ci interdisent notamment l'enseignement en langue irlandaise, excluent les catholiques de l'administration, de l'armée, de l'enseignement dans les écoles, les empêchent d'être propriétaires terriens, et leur interdisent d'exercer des professions libérales.

Un nouveau soulèvement a lieu en 1798, nourri aussi bien par l'émancipation des États-Unis que par l'exemple de la Révolution française (il est commémoré par la chanson "The Wind That Shakes the Barley"). Une éphémère République du Connaught a été proclamée après la bataille de Castlebar et John Moore, chef de la Société des Irlandais unis, a été déclaré son président. La répression est terrible et le août 1800, en réaction, la Grande-Bretagne proclame un « acte d'union » unissant totalement l'Irlande au Royaume-Uni.

Le est marqué par une émigration massive des Irlandais (plusieurs millions) en direction de l'Amérique, émigration accrue par les conséquences de la terrible famine qui sévit en Irlande entre 1846 et 1848. Celle-ci a fait un million de morts, qui, cumulés à l'exode des habitants, ont provoqué une réduction de la population de l'île de 25 % en à peine dix ans. 

Cette famine est aggravée par le fait que le Royaume-Uni forcera les Irlandais à continuer leurs exportations de nourriture vers l'Angleterre, alors que la population irlandaise mourait de faim.

Mais à la fin du , le mouvement pour l'indépendance reprend de la force, les élus irlandais au parlement britannique s'en font l'écho. Une suite de réformes agraires commence à restituer des terres aux Irlandais. En 1905, le Sinn Féin indépendantiste est fondé. De son côté, James Connolly fonde le premier journal socialiste irlandais : "Workers' Republic". Des syndicats irlandais se développent.

En 1914, le « Home Rule » est voté, donnant une autonomie relative à l'île. Néanmoins le pouvoir suspensif de la Chambre des lords puis le déclenchement de la Première Guerre mondiale l'empêcheront d'être mis en œuvre.

Durant la guerre, en 1916, sous la direction de l'Irish Republican Brotherhood du Sinn Féin et de l'Irish Citizen Army de James Connolly, éclate l'insurrection de Pâques 1916 à Dublin, qui proclame la République au nom de Dieu et des générations disparues. Elle est écrasée au bout d'une semaine. Mais le Sinn Féin en retire une popularité accrue : il remporte triomphalement les élections de décembre 1918, constitue un parlement irlandais (le "Dáil Éireann") et proclame l'indépendance. Le pouvoir britannique dissout le parlement. Un nouveau soulèvement éclate, qui va durer trois ans.

Le , des négociations entre le gouvernement britannique et les dirigeants nationalistes irlandais aboutissent au traité de Londres, qui fait de l'Irlande, amputée des deux tiers de l'Ulster, un dominion au sein de l'empire britannique, l'"Irish free state", qui se dota d'une constitution en octobre 1922. En fait, la partition de l'île s'est faite sur des critères économiques. L'Est de l'Ulster était la région la plus développée à l'époque. C'est pourquoi six des neuf comtés de l'Ulster restèrent britanniques dont deux avec une faible majorité catholique. Ce traité fut ratifié de peu par le Dáil Éireann en décembre 1921, mais fut rejeté par une large majorité de la population. Cela entraîna la guerre civile d'Irlande qui dura jusqu'en 1923, opposant les adeptes d'une poursuite de la lutte pour obtenir l'indépendance complète de l'île et les partisans du compromis de 1921.

Durant ses premières années, ce nouvel État fut gouverné par les vainqueurs de la guerre civile. Cependant, en 1932, "Fianna Fáil", le parti des opposants au traité, dirigé par Éamon de Valera, remporta les élections (il resta au pouvoir jusqu'en 1948). En 1933, De Valera, devenu président du conseil, fit abolir le serment au souverain du Royaume-Uni.

En 1937, il fait adopter une nouvelle constitution qui renomme l'État en "Éire" ou en anglais "Ireland". Un traité conclu en 1938 avec le Royaume-Uni, lui laissait ses bases navales en Irlande, et entérinait cette indépendance. L'Irlande resta neutre durant la Seconde Guerre mondiale, interdisant même officiellement au Royaume-Uni l'usage militaire de ses ports et aéroports.

En février 1948, c'est le parti "Fine Gael" qui remporte les élections. Le gouvernement est une coalition constituée avec le parti travailliste. En 1948, l'Oireachtas proclame le Republic of Ireland Act qui prend effet le et qui déclare que l’Irlande est officiellement une république. Le pays quitte le Commonwealth. Paradoxalement il ne nomme pas le nouvel État comme étant la « République d'Irlande », mais considère que cela en est la description.

L'Irlande rejoint la Communauté économique européenne en 1973, suivant le Royaume-Uni, pays dont elle est restée très dépendante (90 % des exportations de l'époque). Mais les problèmes économiques mondiaux des années 1970, aggravés par une mauvaise politique économique des gouvernements suivants, dont celui de Premier ministre Jack Lynch, entraînent une stagnation de l'économie. Les troubles en Irlande du Nord découragent les investissements étrangers. 

Toutefois, les réformes économiques des années 1980, aidées par les investissements de la Communauté européenne, conduisent à l'émergence de l'un des taux de croissance économique les plus élevés du monde avec une immigration massive (en particulier de personnes en provenance d'Asie et d'Europe orientale) comme caractéristique de la fin des années 1990. Cette période sera connue comme celle du « Tigre celtique ».

Mais le pays est frappé peu après de plein fouet par la crise économique de 2008. En 2013, l'Irlande quitte la tutelle mise en place en 2010 par le FMI et l'UE à la suite de la bulle immobilière de 2007.

Depuis 1949, l'Irlande est une république parlementaire.

Le président d'Irlande (' en irlandais, ' en anglais) est élu pour sept ans au suffrage universel direct.
Le Dáil Éireann est le nom irlandais de la chambre basse du Parlement (l'Oireachtas) de l'Irlande. Il est directement élu tous les cinq ans selon un système de représentation proportionnelle (selon la méthode du vote transférable). Il a le pouvoir de voter les lois, de nommer et de remplacer le Taoiseach (Premier ministre). La chambre haute s'appelle Seanad Éireann.

L’île d'Irlande est divisée entre 32 comtés et 4 provinces : Ulster, Connacht, Munster et Leinster. Les provinces suivent approximativement les anciennes provinces d'Irlande, les six comtés d'Irlande du Nord, tous en Ulster, ne sont pas inclus. Les provinces ne sont pas utilisées par l'administration, elles sont utilisées seulement pour les sports, le rugby à XV ou les sports gaéliques.

Les comtés sont utilisés pour l'administration locale. Ils ont été créés par les Britanniques après la conquête de l'Irlande. Il y a 26 comtés traditionnels dans la république plus les comtés de Nord-Tipperary et de Sud-Tipperary et les comtés issus de la partition du Comté de Dublin (Dublin Sud, Fingal et Dún Laoghaire-Rathdown).
Certaines villes ont aussi un pouvoir de décision du même niveau que le comté au travers d'un conseil de Cité (Dublin, Cork, Galway, Limerick et Waterford).

De 1995 à 2007, l'économie irlandaise croît en moyenne de 6 % par an, lui valant le surnom de « tigre celtique ».

Comme quinze autres pays européens, l'Irlande a abandonné sa monnaie nationale, la livre irlandaise, pour adopter l'euro le janvier 1999 (les pièces et billets ayant été introduits le janvier 2002).

Le taux de chômage est de 14,8 % en 2012 - contre 4,3 % en 2005. En juin 2012, travaillaient en République d'Irlande, soit de moins que début 2012. La plus forte baisse a eu lieu dans le secteur public, les industries de la finance, de l'assurance et de la construction.

En 2009, le PIB par habitant de l'Irlande est le second plus élevé de l'Union européenne, après celui du Luxembourg. Il était de 64 % de la moyenne européenne lors de son adhésion en 1973.

Au milieu des années 1980, une prise de conscience collective a abouti en 1987 à un consensus national visant à sortir du sous-développement chronique qui caractérisait l'Irlande depuis la Grande Famine de 1851.

En effet, avant les années 1980, comparativement aux autres pays européens, l'économie irlandaise pouvait être qualifiée d'« économie de subsistance » ; avec une industrie archaïque et un niveau de vie très en deçà des standards occidentaux (le PIB (PPA) par habitant était inférieur de plus de 50 % à celui d'un Américain en 1987). Bien que la croissance économique eût été de 4 % par an entre 1970 et 1985 (contre 2,7 % en Europe et 3,2 % aux États-Unis), cela s'avérait encore insuffisant pour que les Irlandais atteignent un niveau de vie comparables aux autres pays européens.

L'économie de l'Irlande était alors très dépendante de son voisin le Royaume-Uni, qui recevait encore plus de la moitié des exportations au début des années 1980. Les exportations irlandaises étaient alors constituées pour beaucoup de produits agro-alimentaires, de textile artisanal et de produits industriels à faible valeur ajoutée.

Ce pays, ancré dans ses traditions et aux paysages exceptionnels, a toujours attiré de nombreux touristes. Le tourisme a toujours été un secteur phare de l'économie irlandaise, de nombreux descendants d'émigrés irlandais (américains notamment) venant fouler la terre de leurs ancêtres.

Ce retard de développement caractéristique de l'Irlande se traduisait par une émigration massive (plus de par an, soit 10 % de la population chaque décennie), un chômage endémique (17 % en 1987) et une misère autant rurale qu'urbaine, particulièrement visible dans les quartiers nord de Dublin.

Les indicateurs sociaux (espérance de vie, mortalité infantile, pauvreté primaire) étaient alors notoirement mauvais.

Il est cependant à noter que, hormis à Dublin, la société irlandaise était alors jusque dans les années 1980 relativement égalitaire.

Les réformes d'inspiration libérale menées depuis 1987 ont cependant été conduites de manière pragmatique par les gouvernements successifs quelle que soit leur couleur politique, Fianna Fáil (centre droit national) ou Fine Gael (centre droit libéral).

La volonté des gouvernements visant à ouvrir l'Irlande sur le monde ont dopé l'attractivité du pays pour les multinationales avec des taux d'imposition sur les sociétés très bas (12,5 % contre 33,3 % en France) et une fiscalité directe sur les particuliers elle aussi très favorable. Le taux de prélèvements obligatoires est particulièrement bas (30 % du PIB en 2007 en Irlande, contre 44 % en France ou 37 % au Royaume-Uni).

Une meilleure compétitivité et l'implantation de nombreuses compagnies étrangères ont permis la création de centaines de milliers d'emplois, dans tout le secteur de l'industrie et des services. La population étant anglophone et d'un bon niveau de formation, les firmes informatiques et pharmaceutiques, ainsi que les centres d'appel, ont trouvé en Irlande dans les années 1990 un lieu idéal où s'installer en Europe. À la suite de cela, le tourisme, le commerce de détail, et la construction ont connu un développement exceptionnel jusqu'au crash de 2008.

Les lotissements de maisons à l'américaine ont poussé comme des champignons dans les années 1990 et 2000 à la périphérie de Dublin, Galway et Cork.
Les prix de l'immobilier, tant à la location qu'à l'acquisition, ont ainsi atteint des sommets à Dublin (le prix du mètre carré étant plus cher qu'à Paris en 2007).

Ainsi, l'économie irlandaise a décollé de manière remarquable. Les taux de croissance économique ont été exceptionnels (+9 % par an entre 1995 et 2000), et la croissance des échanges fulgurante. Le niveau de vie des Irlandais était en 2007 parmi les plus élevés au monde (voir liste des pays par PIB par habitant), entraînant le retour au pays d'émigrés américains.

Les salaires élevés ainsi que le plein emploi (le taux de chômage n'était que de 4 % en 2007) ont engendré une immigration importante au cours des années 2000.
L'immigration en provenance des pays de l'Est (Pologne, Estonie…), de France (près de vivent en Irlande à la fin de 2009), et d'Asie (de très nombreux Chinois vivent à Dublin) entraîne une augmentation de la population de 2,5 % par an durant les années 2000. En raison de la crise économique, l’émigration a néanmoins fortement augmenté à partir de 2010.

Le taux de chômage était très faible (4 %) durant les années du tigre celtique. Le manque de main-d'œuvre criant dans certains secteurs tels que le BTP provoqua des fortes hausses de salaires. L'immobilier était alors en surchauffe, avec une augmentation démesurée des prix à cause d'une forte demande due à l'immigration.

À la veille de la crise de 2008, les déséquilibres paraissaient évidents : l'endettement des Irlandais par les « mortgages » (prêts hypothécaires) était démesuré, alors que leur taux d'épargne demeurait très faible.

La crise de 2008-2009 a balayé de plein fouet l'économie celtique, les économistes parlant même pour l'Irlande d'une crise financière de 2007-2010.

La récession est particulièrement violente avec une contraction du PIB de 3 % en 2008, 7 % en 2009 et 0,4 % en 2010 .
Ainsi le taux de chômage était de 14,8 % de la population active en 2012. Les années 2011 et 2012 semblent ouvrir la porte à une légère embellie, avec une hausse du PIB de 0,7 % et 0,5 % (estimation) respectivement. Ce retour à la croissance peut s'expliquer en grande partie par la bonne tenue des exportations (multinationales installées en Irlande), la demande intérieure demeurant faible.

Le déficit budgétaire, à cause de l'effondrement des recettes fiscales et de la hausse des dépenses sociales, explose. Les plans de sauvetage des banques irlandaises coûtent une fortune au gouvernement. Le déficit public annoncé pour 2010 avoisinera 32 % du PIB, ce qui est tout à fait hors normes. La dette publique avoisine les 100 % du PIB.

Les prix à la consommation baissent fortement en 2009. Les prix de l'immobilier s'effondrent.
Les pancartes "to let" (à louer) et "for sale" (à vendre) fleurissent dans toutes les rues des villes du pays.
Nombre de ménages connaissent des défauts de paiements sur leurs crédits immobiliers.

Les activités bancaires et financières qui avaient connu un essor sans égal à Dublin sur les bords de la Liffey dans les années 2000 ont été particulièrement touchées par la crise financière d'octobre 2008, mettant en grandes difficultés les principales banques du pays et obligeant le gouvernement à intervenir en recapitalisant ou nationalisant les établissements financiers et éviter une faillite générale du système. Le coût de ces plans de sauvetage est énorme (près de 20 % du PIB en 2010), expliquant le niveau astronomique du déficit public à 32 % du PIB (à titre de comparaison, la France connait un déficit public de 7,8 % du PIB en 2010).

La délocalisation d'une partie des activités de l'informatique Dell de Limerick vers la Pologne, entraînant la perte de près de 2000 emplois, ce qui est énorme à l'échelle irlandaise, est ressentie comme une catastrophe économique et sociale par l'Irlande tout entière pour cette région qui avait connu une renaissance économique dans les années 2000. L'ampleur de cette crise est inédite.

Les difficultés financières s'accumulant depuis plusieurs mois, le gouvernement de Brian Cowen s'est résigné, sous la pression des dirigeants européens, à accepter l'aide financière du FMI pilotée par l'Union Européenne.
Ce « plan de sauvetage » sous la forme d'un emprunt de 85 milliards d'euros au taux de 6,7 %, est largement décrié par la population du fait de son coût prohibitif et du plan d'austérité qui y est associé.

La dette continue néanmoins d'augmenter, passant de 91,2 % du PIB en 2010 à 124,8 % (204,696 milliards d’euros) fin 2013, avec un dernier trimestre de cette même année en récession de 2,3%. Proportionnellement au nombre d'habitants, l'Irlande est en 2016 le deuxième pays le plus endetté au monde après le Japon. 

Tous les comtés du pays ont su développer leur infrastructure afin d'attirer les touristes du monde entier. Le pays offre ainsi de nombreuses activités, telles que la pêche, l'équitation, le golf, la randonnée pédestre… afin de découvrir les richesses du pays.

L'Irlande comptait d'après le recensement de 2016. La densité reste assez faible : avec 67.7 hab/km, elle est presque deux fois moindre que la moyenne de l'Union européenne, ensemble dont l'Irlande regroupe environ 1 % de la population.

Le dynamisme démographique est relativement nouveau dans le pays. Il est dû à la relative jeunesse de la population, à un taux de natalité élevé pour l'Europe et surtout à une forte immigration. Il s'agit d'un phénomène assez nouveau : la croissance démographique n'a commencé que dans les années 1960. Auparavant, la région s'était fortement dépeuplée. Elle comptait 6,5 millions d'habitants en 1841, et passe à 5,1 millions en 1850 du fait d'une grande famine accompagnée d'une émigration massive. L'émigration s'est poursuivie pendant le et une bonne partie du , et a fait plus que contrebalancer l'excédent naturel. La population a donc globalement continué à décroître jusque dans les années 1960 : 3,2 millions d'habitants en 1901 et 2,8 millions en 1961.
À partir de cette date la population a crû de nouveau. Dans les années 1990, et plus encore les années 2000, la population immigrée a fortement augmenté. En 2011, vivaient en Irlande, soit une augmentation de 143 % par rapport au recensement de 2002. La plupart viennent d'Europe, du Royaume-Uni pour plus de la moitié, d'Europe de l'Est pour une part importante et en rapide augmentation.

Villes principales en 2016 :

Il existe deux langues officielles en Irlande. La constitution dispose que la première langue nationale est l'irlandais, et que l'anglais est une langue annexe. Toutefois, l'anglais est fortement majoritaire, et l'irlandais, bien qu'enseigné obligatoirement à l'école, n'est plus pratiqué dans la vie courante que par peu de personnes (environ 1,8 millions d'Irlandais ont une connaissance de la langue, et 538 283 la parlent tous les jours), essentiellement dans les "Gaeltachtai" , qui sont surtout éparpillées le long de la côte occidentale. Pourtant, même dans ces zones, l'irlandais devient une langue minoritaire, et il est estimé que d'ici à 2025, l'irlandais ne sera quasiment plus parlé dans la vie courante, même dans les "Gaeltachtai".

D'après le recensement de 2016, 78,3 % des habitants se déclarent catholiques et 9,8 % n'ont pas de religion. Les 11,9 % restants sont protestants, musulmans, etc..

L'Irlande est le pays occidental possédant la plus forte pratique religieuse (entre 35 et 50 % de pratiquants réguliers), même si ce taux a sensiblement baissé depuis vingt ans (près de 90 % de pratiquants jusque dans les années 1980). La religion catholique occupe de fait un rôle prédominant dans la culture et l'identité irlandaise, celle-ci ayant été utilisée pour se démarquer du Royaume-Uni. 

L'Église catholique a longtemps conservé une influence sur les questions de société, comme pour le divorce, légalisé en 1995, ou pour l'avortement, interdit dans tous les cas jusqu'en 2013, et depuis légalisé uniquement en cas de danger pour la mère de l'enfant. Les femmes qui ont subi un viol et qui sont tombées enceintes n'ont toujours pas le droit d'avorter. 

En vue du référendum de 2015 sur le mariage homosexuel en Irlande, l’Église catholique mène une campagne pour le "non". Le "oui" l'emporte finalement avec plus de 62 % des voix.


Parmi les écrivains irlandais, on compte Jonathan Swift, Oscar Wilde, James Joyce, George Bernard Shaw, Samuel Beckett, William Butler Yeats, Eoin Colfer, John Millington Synge.






</doc>
<doc id="14243" url="https://fr.wikipedia.org/wiki?curid=14243" title="Macédoine (cuisine)">
Macédoine (cuisine)

La macédoine est un plat chaud ou une salade, mélange de différents légumes, ou parfois de différents fruits, taillés en cubes d'approximativement de côté (« taille en macédoine »).

Ce nom est apparu en français à la fin du par allusion à la Macédoine, dont les peuples apparaissaient, sur les cartes de l'époque, comme un mélange de couleurs d'éléments de nature différente. En Espagne, Italie, Serbie et Bulgarie, on la nomme "salade russe" alors qu'ailleurs dans les Balkans on l'appelle "salade française" et en Roumanie et Moldavie, "salade orientale". En anglais, le nom est "Oliver salad". C'est généralement un mélange de dés de carottes, de petits pois et d'autres légumes.

Avec le développement de l'alimentation industrielle, on peut acheter de la macédoine de légumes ou de fruits en conserve.

En Belgique, la macédoine de légumes est un mets consommé, à l'origine, pendant le "cwarmê" : le carnaval de Malmedy (Province de Liège). Cette salade est le plus souvent appelée « salade russe ». Elle a une couleur rose due à la présence de betteraves rouges parmi les ingrédients. Elle a l'avantage d'être préparée avant le "cwarmê", ce qui débarrasse des soucis de préparation culinaire durant les jours du carnaval. En principe, elle se consomme pendant les quatre jours qui précèdent le carnaval puis pendant les quatre jours du "cwarmê" proprement dit. Autre avantage : son apport calorique est utile aux participants du carnaval en période d'hiver. La salade russe se mange froide et à tout moment de la journée. Aujourd'hui, la consommation de la salade russe ne se limite plus à Malmedy et à ses fêtes carnavalesques. On en trouve de plus en plus en province de Liège surtout pendant la période hivernale.



</doc>
<doc id="14244" url="https://fr.wikipedia.org/wiki?curid=14244" title="Macédoine (région)">
Macédoine (région)

La Macédoine est une région géographique et historique de l'Europe du Sud et de la péninsule des Balkans qui tire son nom du royaume antique de Macédoine et qui est actuellement répartie sur plusieurs pays : la Grèce, la république de Macédoine, la Bulgarie, mais aussi, selon certaines cartes, quelques petits territoires en Albanie orientale et en Serbie méridionale, le long de leurs frontières.

La région macédonienne a toujours été multiethnique. Dans l'Antiquité déjà, les Macédoniens étaient issus de populations thraco-illyriennes, hellénisées dans le centre et le sud du pays, notamment autour de la capitale Pella, mais aussi celtisées dans le nord par le mélange avec les Scordices. À l'ouest, les Illyriens conservent leur langue d'origine. Plus tard, une partie des habitants sont latinisés sous la domination romaine. De l'Antiquité, la Macédoine actuelle a hérité les langues albanaise, grecque et aromane. À partir du des populations slavophones s'installent, de plus en plus nombreuses, assimilant une partie des autres habitants, et de cette période, la région a hérité la langue slave dite macédonienne, proche du bulgare. Au arrivent les Turcs, et au les Roms. Au , il était impossible de trouver en Macédoine un district où les habitants ne parlent qu'une langue et les cartes linguistiques présentaient une mosaïque de couleurs, au point qu'un mélange de légumes fut appelé, pour évoquer cette variété : « macédoine de légumes ». Depuis la seconde moitié du , les transferts de populations (Grecs vers le sud, Slaves vers le nord, Albanais vers l'ouest, Aromans vers la Dobrogée roumaine et Turcs (peuple) vers la Turquie) ainsi que la scolarisation dans les langues de chaque état, ont simplifié la carte, et au la région de Macédoine a une population majoritairement albanaise sur ses marges occidentales, slave en république de Macédoine et en Bulgarie et grecque en Macédoine grecque, avec des minorités de chaque groupe des deux côtés des frontières politiques.

Sur un territoire initialement peuplé de tribus Thraces (Almopes, Bottiens, Edones, Eordes, Péoniens et Pières), un royaume est fondé en 729 (ou 700) avant notre ère par le roi Perdicas : c'est l'actuelle Macédoine égéenne, au Nord de la Grèce. Ce royaume apparaît dans l'histoire grecque sous le nom de "Makedonia" ( "Μακεδονία") à l'époque de l'Empire perse dont il devient vassal de 513 à 480 avant notre ère. Alexandre (498–454), qui s'est allié avec les autres Grecs dans leur guerre contre les Perses, parvient à agrandir son royaume. Après -480, les Bottiens et les Pières (tribus thraces hellénisées), fondent le petit royaume de Pella. Philippe II de Macédoine (359–336) en hérite, réunit toutes les tribus en 357 et agrandit son royaume de la Thessalie, de l’Épire et de toute la Thrace méridionale en 340. Profitant de la déliquescence de l'Empire perse, Alexandre le Grand (356–323), fils de Philippe II, étendra les conquêtes, avec l’aide d'autres troupes grecques, des Illyriens et des Thraces, jusqu'au haut-Nil et à l'Indus. À l'époque, tout le Sud et l'Est des Balkans sont fortement hellénisés jusqu'à la Ligne Jireček (c'est-à-dire jusqu'à l'Haemos dans l'actuelle Bulgarie, et tout le long des côtes du Pont Euxin). À la mort d'Alexandre le Grand, ses généraux se partagent son empire ; la Macédoine revient à la dynastie des Antigonides et ses dimensions ne varieront plus : elle englobe l'actuelle république de Macédoine, le département de Blagoevgrad en Bulgarie, et la Macédoine grecque. Après la défaite de Pydna en 168 avant notre ère, la Macédoine est incorporée dans l’Empire romain et divisée en deux parties : "Macedonia Prima" et "Macedonia Salutarus".

La domination romaine ne s'achève pas ici en 476 et dure sept siècles, car l'Empire romain ne s'effondre pas : il se transforme en Empire romain d'Orient (dit « byzantin »). Durant cette période, la romanisation des Thraces qui n'avaient pas été hellénisés donne le peuple Aroumain dit « Valaque ». La Macédoine est donc hellénophone au Sud, et latinophone au Nord. Toutefois, si la puissance romaine se maintient bien le long des côtes, elle décline dans l'intérieur des terres à partir de l'arrivée des slaves au .

En 378 la Macédoine est ravagée par les Wisigoths et à partir de 518, les Slavons, des Slaves venant de la région de Dniepr, s'installent dans la région : les Grecs restent groupés près de la côte au Sud, et les Aroumains dans les montagnes du Pinde à l'Ouest. Les turcophones Avars envahissent le pays en 626, mais toutes ces invasions n'empêchent pas le maintien de l'Empire romain d'Orient, dit « byzantin », jusqu'à ce que le roi des Bulgares entame une campagne de conquête de la Macédoine en 806 (qui ne prend cependant pas Thessalonique, restée byzantine).

L'Empire byzantin réussit l'évangélisation orthodoxe de la population slave ou « barbare », bientôt hellénisée à son tour le long des côtes. Cyrille et Méthode, deux moines de Salonique, adaptent l'alphabet grec à la langue slavonne : c'est la naissance de l'alphabet cyrillique. Englobée de 893 à 927 dans l'empire bulgare de Siméon (sauf sa capitale Salonique), la Macédoine revient entièrement à l'Empire byzantin vers 1015. Vers 1200 se répand l’« hérésie » des « Bogomiles » (prêchée par le pope Bogomil) ou « Cathares » (« purs » en grec). De 1204 à 1229, les Croisés s'emparent de la Macédoine et fondent le « Royaume de Salonique », repris par les Byzantins d'Épire en 1230. Jusqu'en 1281, la Macédoine est à nouveau incluse dans l’Empire byzantin. En 1282, le roi de Serbie Stefan Uroš II Milutin conquiert une grande partie de la région, mais ne la garde que quelques années. Le tsar serbe Stefan Uroš IV Dušan s'empare à son tour de la Macédoine (1331-1355).

En 1389, les Turcs, qui ont envahi les Balkans et encerclé Constantinople et Salonique, battent les Serbes au Kosovo. La Macédoine devient une province de l’Empire ottoman intégrée au pachalik de Roumélie. Jusqu'en 1453, l'Empire byzantin se réduit désormais à sa capitale, à Salonique, à Mistra et à quelques îles de l'Égée. La Macédoine ottomane est divisée en deux départements : Monastir/Bitola (noms aroumain/slave) et Salonique. Des rébellions ont lieu entre 1564 et 1565 à Mariovo-Prilep et en 1689 avec l’. 

Plusieurs insurrections contre l'empire ottoman ont encore lieu au cours de la deuxième moitié du , dont celle de Razlovci, en 1876. La lutte pour la Macédoine oppose les partisans des différents mouvements nationalistes : Bulgares, qui obtiennent en 1872 l'instauration d'un exarchat orthodoxe, Grecs partisans de la Grande Idée et Serbes créent leurs écoles, leurs institutions, puis leurs groupes armés.

Après la défaite ottomane de 1878, la Russie impose, lors du traité de San Stefano le 3 mars 1878, la création d'une « grande Bulgarie » incluant la majeure partie de la Macédoine géographique. Mais le congrès de Berlin (juin-juillet 1878) replace la Macédoine sous l'emprise ottomane, ce qui provoque l'insurrection de Kresna en 1878-1879. À ce moment, les Macédoniens slaves se définissent encore comme « Bulgares » et sont ainsi identifiés sur les cartes ethnographiques du temps. Mais peu après, une identité locale commence à se dessiner chez les "komitadjis" (insurgés bulgarophones) de Macédoine, et en 1893 est créée l’Organisation révolutionnaire intérieure macédonienne, qui réclame l’autonomie du pays (et non plus son rattachement à la Bulgarie). Un des animateurs du mouvement est Damé Grouev. L’insurrection d'Ilinden en conduit à la proclamation de la république de Kruševo. Mais l’année 1903 est également marquée par des représailles sanglantes des Turcs qui provoquent une forte émotion en Europe. Sous la pression des puissances occidentales, le sultan doit accepter une mission internationale chargée de superviser la gendarmerie ottomane et d'empêcher les exactions. L'Empire ottoman se maintient la région jusqu'en 1912.
Pour les Turcs de l'époque, la Macédoine est la « Roumélie occidentale » ("Rum-ili" : le « pays pris aux Roumis ») et forme les trois vilayets de Salonique, du Kosovo (qui comprend Skopje) et de Monastir. Mais sur toutes les cartes européennes, le nom de "Macédoine" se conserve car jusqu'au , les érudits d’Europe pensent la géographie des Balkans selon les conceptions de Strabon et Ptolémée . Cette identification nourrit aussi les revendications grecques de la Grande Idée.

Au début du , la population de la Macédoine était fort bigarrée: Grecs, Slavons bulgarophones, Serbes, Albanais, Aroumains, Turcs et Roms s'y côtoyaient. L’"Encyclopædia Britannica" de 1911 indique les statistiques linguistiques suivantes :

À cette époque, la Grèce, la Serbie, la Bulgarie et les Albanais revendiquent tous la Macédoine, partiellement ou intégralement ; les puissances soutiennent les uns ou les autres (la Russie soutient la Serbie et la Bulgarie ; l'Autriche-Hongrie soutient les Albanais ; la France soutient la Grèce, tandis que la Grande-Bretagne soutient le "statu quo", c'est-à-dire l'Empire ottoman).

Les guerres balkaniques (1912-1913) marquent la fin de la domination ottomane et le partage de la région entre la Grèce (sud), la Serbie (centre) et la Bulgarie (est). Par deux fois, durant les Première et Seconde guerres mondiales, la Bulgarie s'allie à l'Allemagne pour tenter (vainement) de récupérer la Macédoine centrale, où la population la plus nombreuse est formée de Slavons bulgarophones. Après la Première Guerre mondiale, la Macédoine slave est incluse dans le Royaume des Serbes, des Croates et des Slovènes qui prend, en 1930, l’appellation de « Yougoslavie ». Après la Seconde Guerre mondiale, avec le communisme, la Bulgarie renonce définitivement à ses revendications, tandis que la Yougoslavie se divise en six républiques fédérées : l'actuelle république de Macédoine sera l'une d'elles. Les Slavons bulgarophones y sont officiellement redéfinis comme « Macédoniens ». La « bigarrure » linguistique et religieuse se simplifie par le départ des Turcs, des Grecs et des Aroumains.

Finalement le partage de 1913 reste en vigueur... à ceci près que la Macédoine centrale, serbe puis yougoslave, devient indépendante en 1991.

Historiquement, à très peu de choses près, l'actuelle Macédoine grecque, issue de ces partages, correspond au Royaume antique de Macédoine à ses débuts.

Après la Seconde Guerre mondiale, la « Macédoine du Vardar » devient une république fédérée de la nouvelle république fédérale socialiste de Yougoslavie. La guerre civile grecque de 1945 à 1948 cause un exode important des minorités slaves (mais aussi de Grecs communistes) de Macédoine égéenne vers la république socialiste de Macédoine. Sous contrôle communiste mais pour la première fois autonome, le régime de la Macédoine yougoslave cultive l'identité macédonienne à partir de 1945. Un groupe de linguistes dont Blaže Koneski commencent à travailler autour de la codification et de la standardisation de la langue macédonienne, selon les prescriptions de Krste Petkov-Misirkov. C'est ainsi qu'on commence la grammaticalisation de la langue macédonienne. À la fin des années 1960, des tensions entre les nationalités du Kosovo émergent, notamment avec les demandes d'une autonomie accrue de la minorité albanaise. À la mort du maréchal Tito en 1980, le mécontentement grandit dans les républiques de la Yougoslavie contre le Gouvernement fédéral, dominé par des Serbes. De même, la minorité albanaise exprime ses revendications face au gouvernement fédéral tant au Kosovo qui, peu à peu, devient majoritairement albanais, qu'en Macédoine (région de Tetovo (Macédoine)).

La Macédoine connut un développement limité au sein de la Yougoslavie communiste, et fut une des républiques les plus pauvres de la fédération.

La république de Macédoine devient un État indépendant et souverain en 1991, lors de la dissolution de la république fédérale socialiste de Yougoslavie. La république socialiste de Macédoine organisa le , soit deux mois après les déclarations d'indépendance de la Croatie et de la Slovénie, un référendum sur l'indépendance, qui l'emporta avec 68 % des voix. Elle proclama son indépendance en octobre et devint la république de Macédoine. Contrairement à ce qui s'était passé ailleurs (où il y avait de fortes minorités Serbes), en Macédoine l'armée fédérale se retira pacifiquement et il n'y eut pas de guerre. Kiro Gligorov fut le premier président de la nouvelle république de Macédoine, tandis que Nikola Kljušev fut le premier premier-ministre du pays. En revanche, si la Grèce avait bien accepté la dénomination de "Macédoine" dans le cadre de la Yougoslavie, elle ne l'accepta pas pour la république indépendante en raison de l'utilisation par ce nouvel état slave de l'histoire et des symboles identitaires de la Macédoine hellénistique antique, qui, pour la Grèce, font partie de ses racines à elle. Un contentieux apparut ainsi, avec des conséquences politiques et économiques (blocages des frontières, difficultés d'accès aux instances et traités internationaux...), la nouvelle République étant reconnue par la Grèce et par la communauté internationale sous le nom d'ARYM (ancienne république yougoslave de Macédoine) ou FYROM (Former Yugoslav Republic Of Macedonia), conformément à la Résolution 817 du Conseil de Sécurité des Nations unies. Toutefois la même résolution permet à chaque pays en particulier de reconnaître l'État macédonien sous le nom qui lui convient, pour son usage intérieur, et l'immense majorité des États adoptèrent le nom de « Macédoine » en dépit des propositions alternatives (d'inspiration grecque) du type « "Macédoslavie" » ou « "Vardarie" ».
Définir la Macédoine dans ses limites géographiques est quelque chose de particulièrement difficile car il y a autant de descriptions que de pays qui se partagent cette zone. Cependant, la Macédoine dans sa plus grande acception se réfère à une zone située entre la rivière Nestos et les Rhodopes à l'est, les montagnes d'Osgorske, de la Crna Gora et de la Stara planina au nord, les monts albanais de Jablanica et les lacs d'Ohrid et de Prespa à l'ouest et les montagnes du Grammos et l’Olympe au sud.

L'aire géographique macédonienne est aujourd'hui divisée entre la Grèce (52,4 %), la république de Macédoine ou « ancienne république yougoslave de Macédoine », ARYM ou FYROM, (35,8 %), la Bulgarie (10,1 %) et, selon les cartes, l'Albanie (1,4 %) et de la Serbie (0,3 %).


La Macédoine se trouve à cheval sur le grand corridor qui mène d'Europe orientale vers la Méditerranée le long de la Morava et du Vardar/Axios, une voie à fonction stratégique qui a été témoin du passage d'innombrables armées grecques, romaines, slaves et turques.

En mai 2012, les autorités de Pella indiquent une invasion de sauterelles.

L'art médiéval de la Macédoine est marqué par les icônes, souvent réalisées sur des supports en bois, décorant les églises ou bien utilisées dans des lieux publics ou privés de al vie quotidienne. Les plus remarquables datent des .

Au début du , la langue littéraire de la partie slave évolue sur la base du parler populaire slave de la région. Krste Petkov Misirkov, linguiste macédonien, publie une œuvre majeure, qui détermine les normes de la langue littéraire slave macédonienne. L'œuvre littéraire macédonienne se caractérise principalement par la poésie, avec des auteurs tels que Slavko Janevski et Gane Todorovski.

Dans la partie grecque, où ont été installés plus d'un million de grecs chassés d'Asie Mineure, d'Imbros, de Constantinople et de la mer Noire, et où se sont croisés durant des siècles des Saracatsanes, des Valaques, des Pomaques, des Romaniotes et des Roms, les influences de toutes ces populations ont créé ou apporté des traditions, une culture, un vocabulaire, des musiques, des costumes et des cuisines spécifiques qui font aujourd'hui partie de l'identité de la Macédoine hellénique, même si, entre-temps, les populations en question se sont fondues dans le creuset grec ou bien ont été anéanties (Romaniotes, par les nazis).

En Macédoine, la musique traditionnelle puise ses influences dans les musiques surtout grecques, bulgares puis turques et rom. De tonalité joyeuse et entraînante, la musique traditionnelle est jouée lors de fêtes telles que les mariages ou les festivals. Les fanfares comprennent de nombreux cuivres, des accordéons, ainsi que des instruments à cordes pincées. Les costumes, richement colorés mais variant d'une région à l'autre, sont quant à eux marqués par une influence byzantine et bulgare.




</doc>
<doc id="14248" url="https://fr.wikipedia.org/wiki?curid=14248" title="Féminisme">
Féminisme

Le féminisme est un ensemble de mouvements et d'idées politiques, philosophiques et sociales, qui partagent un but commun : définir, établir et atteindre l'égalité politique, économique, culturelle, personnelle, sociale et juridique entre les femmes et les hommes. Le féminisme a donc pour objectif d'abolir, dans ces différents domaines, les inégalités homme-femme dont les femmes sont les principales victimes, et ainsi de promouvoir les droits des femmes dans la société civile et dans la vie privée.

Si le terme « féminisme » ne prend son sens actuel qu'à la fin du , les idées de libération de la femme prennent leurs racines dans le siècle des Lumières et se réclament de mouvements plus anciens ou de combats menés dans d'autres contextes historiques. L’objectif principal de la première vague féministe est de réformer les institutions, de sorte que les hommes et les femmes deviennent égaux devant la loi : droit à l'éducation, droit au travail, droit à la maîtrise de leurs biens et droit de vote des femmes constituent les revendications principales de cette période.

Le mouvement féministe a produit une grande diversité d'analyses sociologiques et philosophiques. La deuxième vague féministe, qui intervient à la fin des années 1960 avec la naissance du Mouvement de libération des femmes (MLF) et du "Women's Lib", a ainsi élaboré plusieurs concepts qui entendent rendre compte de la spécificité du rapport de domination exercé sur les femmes. C'est à cette période qu'est reformulé le concept de patriarcat, élaboré celui de sexisme et que l'accent est mis sur la sphère privée comme lieu privilégié de la domination masculine : le « privé est politique ». Les revendications touchant au contrôle de leur corps par les femmes (avortement, contraception) sont placées au premier plan mais, plus largement, c'est à la construction de nouveaux rapports sociaux de sexe qu'appellent les féministes de cette deuxième vague. Dans cette perspective, la notion de « genre » entend « dénaturaliser » les rapports entre les sexes.

Sous le nom de troisième vague féministe, on désigne à partir des années 1990, un large ensemble de revendications exprimées par des militantes féministes issues de groupes minoritaires, dans le sillage du "Black feminism".

Le terme « féminisme » a longtemps été attribué à tort à Fourier. Il pourrait être emprunté à Alexandre Dumas fils, qui écrivait en 1872 dans "L'Homme-femme" : ; mais il ne prend son sens actuel qu’à la fin du . C'est au cours de la Révolution française, avec l’affirmation des droits naturels, que naît le mouvement de revendication sociale et politique qu'il désigne. Dans la première moitié du , le mouvement féministe apparaît en pointillé, sans parvenir à fédérer d’organisations durables. Il épouse les grandes secousses politiques du siècle, à l’occasion desquelles resurgissent ses revendications. L’objectif large de cette « première vague du féminisme » est de réformer les institutions, de sorte que les hommes et les femmes deviennent égaux devant la loi : droit à l'éducation, droit au travail, droit à la maîtrise de leurs biens et droit de vote des femmes constituent les revendications principales de cette période.

Avec l’émergence des démocraties occidentales, le mouvement féministe s’incarne progressivement dans des groupes organisés, sans jamais présenter un visage monolithique, au point que les études contemporaines mettent l’accent sur la diversité des féminismes.

Les configurations nationales imposent souvent leurs cadres et leurs calendriers ; les objectifs et les méthodes varient selon les groupes constitués et les débats sont constants pour définir les orientations stratégiques et les étapes intermédiaires à atteindre en priorité. Les féministes se trouvent en particulier confrontés à un dilemme : doivent-elles pour mener leur combat mettre en avant les qualités spécifiques qui sont attribuées aux femmes ou au contraire affirmer l’universalité des propriétés humaines ? La première position au risque de figer la nature des femmes ; la seconde au risque de choquer l’évidence de la différence des sexes sur laquelle s’appuient les représentations et la structure sociale.

Certains auteurs affirment que le féminisme existe depuis tout temps : ils parlent de proto-féminisme, même si d'autres pensent qu'il s'agit bien d'un même féminisme qui apparaît puis disparaît de manière cyclique.

Lors du quatrième concile du Latran organisé à l'initiative du pape Innocent III, le mariage est déclaré comme étant l'objet de deux volontés plutôt que de deux corps, ce qui a notamment pour objectif d'empêcher les mariages clandestins et de s'assurer que le mariage est consenti par les deux mariés.

En 1906 le pape aurait déclaré: «...Il est bon que les femmes se libèrent du joug pesant sous lequel les courbe, depuis des siècles, la société. Il est bon qu'elles sachent conquérir leurs moyens d'existence...».

Lors de la révolution anglaise de 1688-1689, les femmes de L’Église anglicane proclamèrent que si Dieu aime les femmes en tant que telles, le Parlement devait agir de même.

Malgré les contributions féminines à la rédaction des cahiers de doléances et le rôle que jouent les femmes du peuple parisien —notamment lors des manifestations d’octobre 1789 pour demander du pain et des armes —, les femmes ne se voient pas attribuer de droit particulier dans la "Déclaration des droits de l'homme et du citoyen" ; et si le nouveau régime leur reconnaît une personnalité civile, elles n'auront pas le droit de vote à cette époque.

Elles n'en continuent pas moins à investir l'espace public, organisées en clubs mixtes ou féminins et en sociétés d’entraide et de bienfaisance, et participent avec passion — à l'instar des hommes — à toutes les luttes politiques de l'époque. Parmi les personnalités féminines notoires des débuts de la Révolution, il faut retenir Olympe de Gouges qui publie en 1791 la "Déclaration des droits de la femme et de la citoyenne" et Théroigne de Méricourt qui appela le peuple à prendre les armes, et participant à la prise de la Bastille, ce dont elle sera récompensée par le don d'une épée par l'Assemblée nationale. C’est par des femmes comme Claire Lacombe, Louison Chabry ou Renée Audou que fut organisée la marche sur Versailles qui finit par ramener Louis XVI dans la capitale.

Toutes deux proches des Girondins, elles connurent une fin tragique : Théroigne de Méricourt devenant folle après avoir été fouettée nue par des partisanes de leurs adversaires, Olympe de Gouges guillotinée. Si les femmes ont été privées du droit de vote, cela ne les a pas préservées des châtiments réservés aux hommes, et nombreuses connurent la prison ou l'échafaud à la suite de leurs actions publiques ou politiques.

À partir de 1792, l'entrée en guerre de la France conduit certaines à se battre aux frontières, tandis qu'en 1793 se développe à Paris un militantisme féminin, porté par des femmes du peuple parisien proches des sans-culottes. Les deux cents femmes du Club des citoyennes républicaines révolutionnaires créé le 10 mai 1793 par Claire Lacombe et Pauline Léon, les « tricoteuses », occupent les tribunes publiques de la Constituante et apostrophent les députés, entendant représenter le peuple souverain. Claire Lacombe propose d’armer les femmes. Leurs appels véhéments à la Terreur et à l'égalité, leur participation à la chute des Girondins, ainsi que les autres manifestations spectaculaires des « enragées », allaient leur valoir une image de furies sanguinaires qui entretiendrait longtemps les répulsions du pouvoir masculin.
Cependant, plus que les excès d'une violence largement partagée à l'époque, ce sont d'abord les réticences des hommes au pouvoir qui excluent les femmes de la sphère politique. La plupart des députés partagent les conceptions exposées dans "Émile ou De l'éducation" de Rousseau d'un idéal féminin restreint au rôle de mères et d'épouses, rares étant ceux qui, comme Condorcet, revendiquent le droit de vote des femmes en vertu des droits naturels inhérents au genre humain, lesquels, à la même époque, inspirent la lutte contre le despotisme et l’esclavage.

En novembre 1793, toute association politique féminine est interdite par la Convention, un seul député s'y oppose Louis Joseph Charlier, mais les femmes vont continuer à jouer un rôle jusqu'à l’insurrection du printemps 95, dont le mot d’ordre est « du pain et la Constitution de 93 », avant que la répression généralisée qui marque la fin de la Révolution ne mette un terme provisoire à cette première prise de parole politique, pour les femmes comme pour les hommes.

En 1792, une femme de lettres britannique, Mary Wollstonecraft fait paraître « "Vindication of the Rights of Woman" », un ouvrage traduit en français la même année sous le titre de « "Défense du droit des femmes" ». L'auteure, qui participe aux débats passionnés suscités outre-Manche par la Révolution en France, n'hésite pas à assimiler le mariage à la prostitution. Elle oppose et rapproche l'exploitation dont sont victimes les femmes les plus pauvres, contraintes au travail salarié ou à la rémunération de leurs services sexuels, au sort des jeunes femmes de la petite et moyenne bourgeoisie privées de toutes perspectives professionnelles par les préjugés et le défaut d'éducation, et réduites à faire un beau parti.

Mary Wollstonecraft sera vite oubliée en France, avant d'être redécouverte par Flora Tristan en 1840.
Éteintes sous l’Empire et la Restauration, les revendications féministes renaissent en France avec la Révolution de 1830. Un féminisme militant se développe à nouveau dans les milieux socialistes de la génération romantique, en particulier chez les saint-simoniens et les fouriéristes de la capitale. Les féministes participent à l'abondante littérature de l'époque, favorisée par la levée de la censure sur la presse. "La Femme Libre" et "La Tribune des femmes" paraissent en 1832 ; "Le Conseiller des femmes", édité à Lyon par Eugénie Niboyet, est le premier journal féministe de province.

Sur le plan politique, la constitution de la Monarchie de Juillet privant de ses droits la majorité de la population française, le combat des femmes rejoint celui des premiers défenseurs des ouvriers et des prolétaires, mais les femmes se mobilisent également contre le statut civil de la femme, soumise en matière juridique et financière à son mari — affirme le Code civil —, et pour le rétablissement du divorce interdit sous la Restauration en 1816.

Certaines femmes revendiquent le droit à l’amour libre, au scandale de l'opinion publique. Claire Démar se livre ainsi dans son "Appel au peuple sur l'affranchissement de la femme" (1833) à une critique radicale du mariage dans lequel elle dénonce une forme de prostitution légale. Elle n’est toutefois pas suivie par l’ensemble des saint-simoniennes qui tiennent à se démarquer des accusations d’immoralisme qui frappent le mouvement.

Les débuts du régime laissent entrevoir quelques espoirs d’évolution. Les pétitions en faveur du rétablissement du divorce placent ce sujet sur l’agenda politique : en 1831 et 1833, les députés votent par deux fois en faveur de la loi, laquelle est toutefois repoussée par la Chambre des pairs. Les revendications féministes deviennent inaudibles. Quand Louise Dauriat adresse en 1837 aux députés une demande en révision des articles du Code civil qui lui paraissent contraires aux droits des femmes, elle ne récolte en retour que les rires de l’assemblée.

Comme en 1789, les femmes participent activement aux journées révolutionnaires de février 1848. Elles s'expriment publiquement par le biais d’associations et de journaux. Les lois proclamant la liberté de la presse profitent ainsi à nouveau à la presse féministe : Eugénie Niboyet crée, le 20 mars, "La Voix des femmes" qui est dans un premier temps le principal relais des revendications féminines, écartées de la presse traditionnelle. Puis viendront en juin "La Politique des Femmes" de Désirée Gay ou encore "L’Opinion des femmes" publiée en janvier 1849 par Jeanne Deroin.

À la suite de leurs protestations, les femmes se voient accorder le droit au travail au même titre que les hommes ; les ateliers nationaux leur sont ouverts, avec retard, le 10 avril. Elles goûtent aux prémices d’une participation citoyenne en élisant des déléguées à la Commission du Luxembourg, en proposant des réformes pour leurs conditions de travail, la création de crèches ou de restaurants collectifs.

Le droit de vote pour l’élection de la future Assemblée nationale constituante est au centre de leurs préoccupations : Jenny d'Héricourt, la fondatrice de la Société pour l’émancipation des femmes imagine que, une fois conquis, ce droit permettra d’agir par la voix législative sur l’ensemble des revendications au nombre desquelles figurent toujours l’abrogation du Code civil et le droit au divorce. Elles lancent des pétitions, sont reçues par les instances politiques. Le Comité des droits des femmes présidé par Allix Bourgeois se voit répondre, par la voix d’Armand Marrast, le maire de Paris, que la décision ne pourra être prise que par la future instance législative.

Les pétitions en faveur du rétablissement du divorce ne rencontrent pas plus de succès que celles de leurs devancières des années 1830 : la proposition du Ministre de la Justice Adolphe Crémieux à la Chambre en mai 1848 est accueillie sous les quolibets. On s’inquiète notamment de la menace que la parole libérée des femmes pourrait faire peser sur la famille. Le Club des femmes, ouvert en avril 1848, est un lieu de débat qui provoque de virulentes réactions ; certaines de ses séances tournent à l’émeute et sa présidente — Eugénie Niboyet — est âprement caricaturée dans la presse. Le Club des femmes sera finalement fermé pour ne pas troubler l’ordre public.

En Allemagne, un premier courant féministe trouve son origine dans les idées libérales du Vormärz et émerge véritablement à la faveur de la Révolution de mars 1848. Louise Aston ou Louise Dittmar tentent de lancer les premiers journaux dédiés à la cause des femmes. Louise Otto, élevée dans un milieu bourgeois qui aspire à des réformes libérales, est la première à pouvoir pérenniser son entreprise ; le "Frauen-Zeitung" (1849-1852), lequel s’adresse prioritairement à la classe moyenne, relaie des revendications essentiellement économiques, insistant sur l’éducation des femmes, leur indépendance économique et le refus des mariages arrangés. Le retour à l’ordre freinera pour quinze ans ce premier élan.

Si la première manifestation collective du féminisme américain coïncide chronologiquement avec le Printemps des peuples européens, ses origines intellectuelles diffèrent sensiblement. Les sectes protestantes dissidentes, en particulier celle des quakers, sont le principal vecteur des idées favorables à l’émancipation des femmes. Mouvement abolitionniste et mouvement du droit des femmes ("Women’s right movement") sont étroitement imbriqués ; les sœurs Angelina et Sarah Grimké, Lucretia C. Mott ou Elisabeth Cady Stanton figurent en première ligne sur ces deux fronts. Mott et Stanton organisent de concert en 1848 la Convention de Seneca Falls dont le texte final — la « déclaration de sentiments » —, calqué sur le modèle de la déclaration d'indépendance des États-Unis, est traditionnellement considérée comme l’acte fondateur du féminisme américain.

Au Royaume-Uni, l’enseignement est dispensé aux jeunes filles de la bourgeoisie par des préceptrices, un des seuls métiers socialement acceptables pour les veuves et les jeunes filles issues de la bonne société. Outre les connaissances de base en matière de lecture, d’écriture et de calcul, il est focalisé sur les activités d’agréments qui fondent « l’art de plaire » et exclut les disciplines scientifiques telles que le grec et le latin, alors indispensables pour poursuivre un cursus dans l’enseignement supérieur.

Éduquées et indépendantes, les femmes qui s’improvisent institutrices fournissent historiquement une part importante des effectifs militants féministes. Elles souffrent néanmoins d’un déficit de formation, provenant de leur exclusion de l’université. Le "Queen’s College for women" puis le "Bedford College" d’ sont créés à la fin des années 1840 pour permettre aux éducatrices de bénéficier d’une formation de niveau supérieur. Les nouvelles diplômées sont à la pointe du mouvement pour l’éducation des femmes. Le "North London Collegiate School" (1850) puis le "Cheltenham Ladies' College" (1853), dirigés par deux anciennes élèves de Bedford, Frances Buss et , proposent une pédagogie révisée, alignée sur les standards masculins.

Les féministes se tournent alors progressivement vers l’université. Conduit par Emily Davies, le Comité pour l’accès des femmes aux examens universitaires revendique l’ouverture aux filles des examens de fin d’études secondaires ("The Cambridge and Oxford Local Examination") ; après une première expérimentation en 1863, il obtient l’autorisation officielle du Sénat de l'Université de Cambridge en 1865.

L’étape suivante est l’ouverture de l’accès aux examens d’entrée à l’université (Matriculation Examinations). Face au refus des instances universitaires, Davies inaugure, malgré de nombreuses difficultés matérielles, un établissement féminin conçu sur le modèle des "colleges" masculins à Hitchin dans le Hertfordshire (1869), avant de se rapprocher de Cambridge en s’installant à Girton l’année suivante. Un autre projet du même type voit le jour peu après, toujours à Cambridge, avec la création du Newnham College sous le patronage d’Henry Sidgwick et d’Anne Clough.

Le Second Empire est le théâtre de plusieurs avancées dans le domaine de l'éducation des femmes. Sous la République, la loi Falloux avait fixé en mars 1850 l'objectif d'une école primaire pour filles dans chaque commune de plus de 800 habitants. La loi Duruy de 1867 aligne ce seuil sur les standards masculins en le fixant à 500.

Les programmes restent définis en fonction des rôles sociaux assignés aux femmes (y figurent les travaux ménagers et la puériculture) ; les couvents et congrégations prennent majoritairement en charge l’éducation des jeunes filles. La mobilisation pour l’éducation des femmes trouve appui dans l’opposition libérale au régime, notamment dans les milieux saint-simoniens. Elisa Lemonnier crée en 1862 les premières écoles professionnelles pour jeunes filles. Julie-Victoire Daubié sollicite, avec le soutien de François Barthélemy Arlès-Dufour, influent capitaine d’industrie saint-simonien, l’autorisation de se présenter à l’épreuve du baccalauréat, qu’elle obtient à Lyon en 1861, à l’âge de 37 ans. Madeleine Brès doit, quant à elle, son inscription en faculté de médecine à sa pugnacité, à l’intervention de l’impératrice Eugénie et du ministre de l'instruction publique, Victor Duruy. Ces pionnières restent toutefois encore isolées : la deuxième bachelière française, Emma Chenu, obtient son diplôme en 1863, deux ans après Daubié. L’amélioration de l’enseignement des femmes reste un leitmotiv des féministes françaises : en 1866, André Léo crée ainsi une association dédiée spécifiquement à cette question.

Les réformes de structure dans l’enseignement secondaire et supérieur interviennent sous la République. Les collèges pour filles, dont les programmes restent spécifiques, sont institués par la loi Sée (1880). Les femmes se voient également garantir une formation à l’enseignement : les écoles normales féminines, rendues obligatoires dans chaque département en 1879, et l’école normale supérieure de Sèvres (1881) forment institutrices et professeurs.

La Troisième République se caractérise en France par la constitution d’organisations féministes réformistes, plus durables et structurées. La Société pour l’amélioration du sort des femmes, présidée par Maria Deraismes, voit le jour en 1878 ; la Ligue française pour le droit des femmes, d’orientation modérée, est créée en 1882 par Léon Richer. En 1891, la Fédération française des sociétés féministes symbolise l’entrée du terme « féminisme » dans le vocabulaire militant.

Le Conseil national des femmes françaises, fondé dans le sillage de la loi sur les associations de 1901, se veut apolitique et laïque. Ses militantes, issues principalement de la bourgeoisie, sont des républicaines, des socialistes ou des protestantes, initiées à l’action publique à travers les activités sociales et philanthropiques. L’Union française pour le suffrage des femmes fédère en 1909 les féministes favorables au droit de vote des femmes.

Refusant l’activisme des suffragettes britanniques, ces grandes fédérations réformistes entendent prouver la responsabilité des femmes et s’intègrent dans le modèle républicain en tissant des liens avec le monde politique masculin (le Parti radical notamment), avec l’objectif d’influer sur l’activité législative.

Au Royaume-Uni, un mouvement pour le droit de vote des femmes se développe à partir de 1866, date du dépôt de la première pétition adressée au Parlement, pour en faire la requête ; le philosophe John Stuart Mill en est le principal relais dans l’enceinte parlementaire. À l’initiative de Barbara Bodichon et Emily Davies, un "Women’s suffrage committee" (Comité pour le droit de vote des femmes) est constitué ; il est rapidement décliné en de multiples comités locaux, coordonnés au niveau national par la "National society for women’s suffrage" (1867). Un mouvement de masse s’organise rapidement ; lors de la pétition initiale de 1866, les féministes sont capables de réunir signataires en 1894.

Proche d’aboutir à plusieurs reprises, mais bloqué par la frange conservatrice du Parlement, le mouvement se radicalise en 1903 avec la création de la "Women's Social and Political Union" par Emmeline et Christabel Pankhurst. Ses militantes, désignées sous le nom de « suffragettes », optent pour de nouvelles formes d’action, parfois violentes et illégales (incendies volontaires, bris de vitres, grèves de la faim…). La popularité du mouvement s'accroît encore, et en 1908, les organisations suffragistes réunissent personnes lors d’une manifestation à Hyde Park. Le bras de fer engagé avec les autorités dure jusqu’au début de la Première Guerre mondiale. Pendant la guerre, des négociations sont ouvertes par le gouvernement Asquith avec les représentantes de la "National Union of Women's Suffrage Societies" de Millicent Fawcett, qui présentent une orientation plus modérée. Elles aboutissent au "" qui autorise le vote des femmes de plus de trente ans.

Aux États-Unis, le front commun entre féministes et antiesclavagistes s’effrite progressivement après la guerre de Sécession. Alors qu’on s’oriente vers un amendement pour le droit de vote des Noirs, une partie des féministes souhaiterait y voir également associées les femmes qu’elles estiment laissées pour compte par les leaders masculins du mouvement. Deux organisations rivales naissent en 1869 des désaccords survenus au sein de l’"American Equal Rights Association". Susan B. Anthony et Elisabeth Cady Stanton constituent la "National Woman Suffrage Association", qui milite pour un amendement à la Constitution qui garantirait le vote des femmes. Ses revendications, qui dépassent le cadre des droits politiques, s’inspirent du texte élaboré lors de la Convention de Senecca Falls. L’organisation rivale — l’"American Woman Suffrage Association" créée par Lucy Stone — est plus modérée et préfère concentrer son action sur le seul droit de vote, délaissant le niveau fédéral pour agir au niveau des États. En 1890, les deux associations finissent par se regrouper dans la "National American Woman Suffrage Association". Dans l’intervalle, en 1869 et 1870, les territoires du Wyoming et de l’Utah autorisent le vote des femmes blanches.
En 1920, le est ratifié au niveau fédéral : toutes les Américaines blanches obtiennent le droit de vote.

Des féministes sont actives dans d'autres pays, particulièrement en Europe du Nord, par exemple Emilie Mundt et Marie Luplau au Danemark.

La question de l’amour libre et du contrôle des naissances divise profondément les féministes de la seconde partie du .

Au Royaume-Uni, une partie du mouvement féministe s'est engagé, durant la période victorienne dans un combat, pour la régénération morale de la nation. À partir de 1869, elle se mobilise contre une série de lois visant à lutter contre les maladies vénériennes — les "Contagious Diseases Acts" — qui imposent un examen gynécologique aux prostituées. Bien que d’orientation conservatrice, ce mouvement, mené notamment par Josephine Butler, prend parti pour les prostituées et réclame la criminalisation des clients et la fermeture des maisons de prostitution. Il entend plus largement rétablir la pureté des mœurs et la moralité publique, et défendre la famille. Le point d’orgue de cette mobilisation constitue un meeting réunissant personnes dans Hyde Park en 1885.

Face à ce mouvement, les militantes favorables à l’amour libre et au contrôle des naissances sont isolées. Quelques-unes adhèrent au mouvement néomalthusien, très actif en Grande-Bretagne mais aussi en France. Annie Besant est ainsi condamnée en 1877 pour avoir publié "The Fruits of Philosophy", un pamphlet de Charles Knowlton, sans avoir reçu le soutien qu’elle réclamait des féministes conservatrices. À la fin du siècle, les écrits d’Edward Carpenter ou d’Havelock Ellis contribuent cependant à répandre plus largement ces idées. Elles trouvent parmi les féministes un relais dans la revue "The Freewoman" (1911), qui réunit les signatures de , Stella Browne ou Marie Stopes.

Si le mot d’ordre « À travail égal, salaire égal » remporte l’adhésion de l’ensemble des composantes du mouvement féministe, l’idée d’une protection spécifique des femmes sur le marché du travail divise. En 1906, la Convention de Berne, ratifiée par quatorze pays, prononce l’interdiction du travail industriel nocturne des femmes. Déjà en vigueur dans certains pays, comme la France où elle s'applique depuis 1892, cette législation rencontre l’opposition des féministes égalitaristes. Menées par la Hollandaise Marie Rutgers-Hoitsem, elles se regroupent dans le réseau Correspondance internationale qui recrute principalement parmi les laïques et les libre-penseuses. Après-guerre, le Bureau International du Travail reprend le mot d’ordre de protection des travailleuses. Toujours minoritaires dans les fédérations féministes internationales, les partisanes de l’égalité constituent l’"Open Door Council" autour de la personnalité de Chrystal MacMillan. Mouvement d’avant-garde qui réunit des intellectuelles de l’ensemble de l’Europe, il élabore un argumentaire qui s’oppose au « féminisme maternaliste » alors dominant : il marque notamment son refus de voir la maternité devenir « une sorte de domaine clos où les femmes se trouveraient parquées d'office, en marge de l'ensemble de la vie sociale et culturelle… ».
La première manifestation internationale des femmes a lieu le 8 mars 1911, à la suite d'une proposition de Clara Zetkin. La revendication principale est le droit de vote. Le premier livre historique féministe est écrit par Mathilde Laigle : "Le livre des trois vertus de Christine de Pisan et son milieu historique et littéraire", 1912. Auparavant, la première grande manifestation des femmes avait été celle pour la paix organisée en marge de la Première conférence de La Haye de 1899 par Margarete Lenore Selenka.

Durant la Première Guerre mondiale, la grande majorité des organisations féministes des pays belligérants soutient l’effort de guerre. Certaines espèrent tirer parti de ce loyalisme : à l’issue du conflit, les féministes britanniques se verront ainsi récompensées par l’obtention partielle du droit de vote. L’opposition à la guerre est surtout le fait de militantes des pays neutres et de quelques groupes isolés des pays engagés dans le conflit.

Aux États-Unis, le "Women Peace Party" de Jane Addams revendique adhérentes mais ne résiste pas à l’entrée en guerre du pays en 1917. Le Congrès international pour la paix future est organisé par Addams et la physicienne Aletta Jacobs à la Haye. Parmi les femmes, principalement hollandaises, qui se réunissent à cette occasion, 9 nationalités sont représentées dont une délégation allemande menée par Anita Augspurg. Les Françaises en sont absentes.

Les milieux socialistes, et leurs organisations féminines, se sont également rangés derrière leurs nations respectives. Des voix discordantes se font néanmoins entendre : en France, Hélène Brion, Madeleine Vernet ou Louise Saumoneau. Cette dernière est présente en mars 1915 à la conférence internationale des femmes socialistes, qui réunit à Berne, à l’initiative de Clara Zetkin, les militantes restées fidèles à l’internationalisme.

À l’issue de la guerre, deux grandes tendances, héritières des débats du début du siècle, s’opposent : un « féminisme maternaliste » ou « social » et un « féminisme de l’égalité », universaliste ou « intégral ».

La première tendance, dominante sur le continent et en particulier en France, réclame des évolutions législatives qui protègent la spécificité des femmes. Elle s’ajuste aux impératifs des politiques natalistes qui se renforcent encore après la saignée démographique de la Première Guerre mondiale. La valorisation de la participation des femmes à l’équilibre de la nation, à travers notamment l’exercice de la « fonction maternelle », occupe ainsi une place centrale dans l’argumentation des réformistes et des sociaux-démocrates. Pour les représentantes de l’Union française pour le suffrage des femmes, « détruire le prestige de la maternité, c’est atteindre le plus sûr prestige de la femme...C’est au nom de la maternité, non point contre elle, que doit se faire la réforme indispensable de la condition féminine ». Les féministes radicales qui entendent abolir la différence entre les sexes ou lutter en faveur de la contraception et de l’avortement sont plus isolées et ont du mal à faire entendre leur voix au sein des grandes coordinations réformistes.

Alors que le chef de famille détenait la puissance paternelle et avait priorité dans la signature des contrats, les féministes obtiennent, par la loi du 18 février 1938, la suppression de la puissance maritale, de l'incapacité juridique de la femme mariée ainsi que de son devoir d'obéissance.

En Allemagne, le féminisme se scinda en deux mouvements. Le premier – proche des mouvances libérales et socio-démocrates – défendait le principe d'égalité des individus, tandis que le second proche du mouvement völkisch, défendait la thèse antisémite d'un « complot judéo-patriarcal », l'homme Juif étant accusé d'avoir inventé « la religion qui devait annihiler la grande force créatrice féminine en lui déniant toute reconnaissance, en la privant de toute possibilité d'action hors d'un cercle se réduisant peu ou prou à la famille ». Les officiels nazis n'apportèrent pas leur soutien à ce féminisme völkisch, qui cessa d'exister en 1937.

La littérature militante connaît un nouvel essor, notamment en France grâce à la parution en 1949 de l'essai "Le Deuxième Sexe" par Simone de Beauvoir. L’ouvrage rencontre un énorme succès dès sa sortie mais fait également scandale, dû en grande partie à son chapitre sur l’avortement qui reste considéré comme un homicide à l’époque. À l’instar de Mary Wollstonecraft et Claire Démar, Simone de Beauvoir assimile le mariage à une forme de prostitution lorsque la femme est dominée par son mari et dans l’incapacité de s’en échapper. À la suite de cette publication, elle devient une figure emblématique du féminisme.

À partir des années 1960, aux États-Unis, l'égalité des droits progresse. En 1963, la loi sur l'égalité des salaires ("Equal Pay Act") est votée. Le 2 juillet 1964, la loi sur les droits civiques ("Civil Rights Act") abolit théoriquement toute forme de discrimination aux États-Unis.

À la fin des années 1960, une nouvelle vague militante féministe émerge aux États-Unis et en Europe de l'Ouest au sein de l’espace politique ouvert par le mouvement étudiant. Le Mouvement de libération des femmes en France et le "Women's Lib" dans les pays anglo-saxons désignent ce mouvement au périmètre fluctuant.

Aux États-Unis, la recomposition qui fait suite au « creux de la vague » des années 1950 débute avec la fondation en 1966 d’une organisation réformiste, la "National Organization for Women" (NOW) par Betty Friedan. Mais c’est principalement en réaction à la division sexuelle du travail militant qui, au sein même des organisations de la Nouvelle Gauche, relègue les femmes aux positions subalternes que se constitue une multitude de groupes féministes radicaux de petites tailles ("New York radical feminists", "Redstockings", "WITCH", "Radicalesbians"…).

Refusant l’organisation verticale et l’orientation réformiste de la NOW, elles ont recours à des formes de mobilisation volontairement provocatrices qui visent à attirer l’attention des médias. En Islande, c'est par une grève générale le que les femmes obtiennent l'égalité en droits en 1976. Se développent également des formes d’organisations originales, comme les groupes d’éveil de la conscience ("consciousness-raising groups"). Par le partage de l’expérience individuelle, ces groupes de discussion entendent faire prendre conscience de la communauté de condition des femmes, de la spécificité de leur oppression et de la dimension politique inscrite dans les éléments les plus banals de la vie quotidienne.

La période est marquée par une intense activité de théorisation de la condition féminine. Si un courant, mené en France par Antoinette Fouque avec son groupe Psychanalyse et politique, défend des positions différentialistes et, selon certaines critiques, essentialistes, le mouvement est majoritairement constructiviste. Il approfondit la voie esquissée en 1949 par Simone de Beauvoir avec "Le Deuxième Sexe" et étudie les modalités de la construction sociale de la différence des sexes, c'est-à-dire la manière par laquelle la socialisation impose des rôles sociaux différents aux personnes des deux sexes. Le terme de sexisme se répand et les féministes radicales et matérialistes élaborent le concept de patriarcat pour définir le système social d’oppression des femmes. Se refusant à subordonner leur combat à la lutte des classes, elles affirment que le domaine de la reproduction (maternité, corps, famille, travail domestique…) est un espace d'exploitation privilégié des femmes. Elles rejettent l’objectif réformiste d'égalité "dans" le système qui a prédominé jusqu’alors. Pour elles, aucune égalité entre les sexes ne peut être obtenue à l'intérieur du système « patriarcal », sinon quelques compromis temporaires qui seraient perpétuellement menacés. Elles préconisent de renverser ce système et d'instaurer de nouveaux rapports entre les sexes.

Une tendance séparatiste s’affirme également, notamment parmi les groupes militants lesbiens des grandes métropoles que sont Londres ou New York.

La maîtrise de leur corps est placée au centre des préoccupations des féministes de la deuxième vague. Longtemps sujet de division, le contrôle des naissances devient l’une de ses revendications les plus visibles. Le libre accès à la contraception mais surtout le droit à l’avortement concentrent leurs efforts. En France, le Mouvement pour la liberté de l’avortement et de la contraception (MLAC) est fondé en 1973. Il s’appuie notamment sur l’aile la plus radicale du Mouvement français pour le planning familial qui se prononce peu après « en faveur de l’avortement et de la contraception libres et remboursés par la Sécurité sociale » et ouvre des cliniques d'interruption volontaire de grossesse (IVG).

La dissociation de la sexualité et de la reproduction s’inscrit dans le cadre plus large de la révolution sexuelle qui traduit une demande sociale pour plus de liberté dans le domaine de la sexualité. Les féministes en font cependant leur propre lecture qui passe par la critique de la normativité de la psychanalyse ou de la sexologie qui auraient défini sexuellement les femmes « en fonction de ce qui fait jouir les hommes », minorant par exemple le plaisir clitoridien. La sexualité est ainsi analysée comme un domaine où s’exerce la domination masculine. Le viol fait l’objet de nombreuses mobilisations : des manifestations citadines nocturnes ("Reclaim the night") entendent regagner un espace dont la peur de l’agression maintient les femmes exclues. Sur le plan juridique, les féministes françaises luttent pour que la loi de 1832 soit appliquée à des faits qui sont jusqu'alors déqualifiés en « coups et blessures ».

Dans le sillage de l’effort de théorisation de la condition féminine inhérent à la deuxième vague, les études féministes pénètrent dans le monde académique à partir des années 1970. L’ensemble des champs du savoir sont ainsi progressivement envisagés sous l’angle de la critique féministe : philosophie féministe, anthropologie féministe, histoire des femmes, critique de la psychanalyse se développent . À la fin des années 1970 et au début des années 1980, la critique féministe des sciences prend également son essor (, , Evelyn Fox Keller, ).

L’ancrage institutionnel le plus fort a lieu aux États-Unis où sont créés des départements de "Women’s Studies" ou de "Feminists Studies" dont l’approche est souvent interdisciplinaire. Avec le développement de l’usage du concept de genre se développent par la suite des départements d'études de genre. En 2003, on dénombrait ainsi 600 départements de ce type aux États-Unis.

Au-delà de cette conquête de l'espace géographique universitaire, Francine Descarries, professeure de sociologie à l'UQAM, constate en 2004 la difficulté des "Women’s Studies" au Québec « à s'extraire de la périphérie, de la marge du champ scientifique pour convaincre de sa légitimité et de la compatibilité de ses approches théoriques et méthodologiques avec l'esprit scientifique ». D'après cette sociologue, peu de recherches sont parvenues à pénétrer le « mainstream scientifique ».

Désormais les femmes votent dans la plupart des pays industrialisés, dont la majorité des parlements ont voté des lois sur le divorce. La légalisation de la contraception et de l'avortement n'est pas effective pour l'ensemble des pays industrialisés, les situations sont donc très variables d'un pays (voire d'une région) à un autre. Ces droits sont fréquemment remis en cause par des courants conservateurs et des institutions religieuses, telle que l'Église catholique et en particulier la mouvance traditionaliste en son sein, et le courant fondamentaliste des protestants évangéliques.

Depuis la fin des années 1990, divers groupements, se réclamant ou non du féminisme, ont été créés. Parmi les plus médiatisés, on peut citer :

En 2010 en Australie, c'est la première fois dans l'histoire d'un État que le chef d'État (Élisabeth II), le chef de gouvernement (Julia Gillard) et le gouverneur général (Quentin Bryce) sont toutes des femmes.

Le féminisme contemporain, dans la plupart des pays occidentaux, se diversifie et change de visage, du fait que les revendications féministes initiales ont été traduites dans les systèmes juridiques, et font partie du périmètre conventionnel des droits de l'homme. La réflexion et l'action féministes sont donc amenées d'une part à s'attacher davantage à l'analyse critique des pratiques sociales réelles (souvent décalées des principes) et à reformuler l'expression de leurs enjeux et de leurs objectifs. Elles doivent aussi tenir compte de la résurgence de débats ethniques, communautaires ou religieux qui compliquent la donne — certaines associations réfutent ainsi la dichotomie Occident féministe contre Orient sexiste. Ce changement de paysage entraîne inévitablement des divergences de vues qui divisent les courants féministes.

Le féminisme libéral épouse les principes du libéralisme politique dont il réclame l’application aux femmes, au même titre qu’à tous les hommes. À ce titre, il se fixe comme horizon l’indifférence aux différences de sexe dans le cadre de l’espace public.
Sur le plan politique, sa méthode est réformiste ; il cherche à obtenir une modification des dispositions légales par la voix législative, le lobbying ou l’action militante à destination de l’opinion publique (presse, pétitions…). Confiant dans les valeurs du progrès et les vertus de l’éducation, il entend également agir sur les mentalités, sans développer, à la manière du féminisme marxiste ou radical, une analyse systémique du capitalisme ou du patriarcat.

Historiquement, il se structure dans la seconde moitié du où il s’incarne dans des groupes organisés, militants pour l’égalité civile et politique ainsi que pour l’égalité des droits dans les domaines de l’éducation ou du travail. L’ensemble de ces droits doivent être à même de garantir l’autonomie des femmes en tant que sujet.

Sur le plan théorique, la tradition marxiste puise principalement ses sources concernant la question des femmes dans "L’origine de la famille, de la propriété privée et de l’État" (1884) de Friedrich Engels et dans "La femme dans le passé, le présent et l’avenir" (1879) d’August Bebel. Clara Zetkin ou Alexandra Kollontaï constituent les représentantes les plus marquantes de cette tradition marxiste de défense des droits des femmes qui a néanmoins refusé le qualificatif de « féministe », jugé « individualiste » et « bourgeois ».

Contre une représentation fixiste de la famille et du rôle qu’y tiennent les femmes, le marxisme affirme l’historicité des structures familiales dont les formes évoluent avec la structure économique. S’inspirant de l’anthropologue évolutionniste Lewis Henry Morgan, Engels définit ainsi une origine historique à l’oppression des femmes : il fait coïncider l’apparition de la propriété privée avec la fin d’une période historique où le droit maternel et la filiation en ligne féminine auraient réglé les modalités de l’héritage.

Avec l’instauration du système patriarcal et du mariage monogamique qui marquent « la grande défaite historique du sexe féminin », les femmes sont victimes d’une double oppression : assignées aux seules fonctions reproductives, elles sont maintenues par leurs maris hors du champ productif et de la vie publique ; quand elles accèdent au marché du travail, elles subissent, comme les autres travailleurs, les effets néfastes du mode de production capitaliste.

Les féminismes marxistes de la fin du militent pour l’accès des femmes au marché du travail : leur entrée dans la sphère productive doit permettre l’éveil d’une conscience de classe et la participation des femmes à la lutte des classes. La doctrine reste attachée au respect de ce qui est défini comme la « double tâche sociale de la femme » : production et reproduction. Stigmatisant les revendications égalitaristes de certaines féministes, il affirme ainsi respecter la spécificité biologique des femmes. Alexandra Kollontaï met ainsi l’accent sur la nécessaire adaptation du droit du travail pour les femmes et aux mesures de protections légales des mères.

Sur le plan stratégique, les mobilisations des femmes doivent rester subordonnées à la lutte des classes. Seul le renversement du capitalisme peut en effet mettre un terme définitif à l’oppression des femmes. La question de l’alliance avec des groupes féministes est posée à la fin du . Des organisations féminines, rattachées aux structures socialistes nationales, s’organisent en effet dans la majorité des pays d’Europe ; elles sont regroupées en 1907 dans l’Internationale socialiste des femmes, à l’occasion de la première Conférence internationale des femmes socialistes qui se tient à Stuttgart. Clara Zetkin en prend la tête et parvient notamment à imposer le principe du refus de toute alliance avec le « féminisme bourgeois » et réformiste.

À la fin des années 1960, la réflexion marxiste sur l’oppression des femmes s’est considérablement renouvelée en questionnant notamment l’articulation entre patriarcat et capitalisme.

Le féminisme radical est un courant du féminisme qui apparaît à la fin des années 1960 et qui voit en l'oppression des femmes au bénéfice des hommes (ou patriarcat) le fondement du système de pouvoir sur lequel les relations humaines dans la société sont organisées. Il se démarque des mouvements féministes qui visent à l'amélioration de la condition féminine par des aménagements de législation (réformisme) sans mettre en cause le système patriarcal, bien que certaines féministes radicales (Catharine MacKinnon et Andrea Dworkin) aient précisément centré leur lutte sur des réformes législatives.

Le féminisme différentialiste d'auteures comme Julia Kristeva, Luce Irigaray ou Antoinette Fouque et, dans le monde anglo-saxon, de Mary Daly et Adrienne Rich (voir aussi ) postule que le patriarcat est si profondément enraciné dans les mentalités qu'il impose un système de valeurs qui empêche l'existence d'une différence authentique entre hommes et femmes, les femmes étant sans cesse définies, construites comme antithèses (idéalisées ou démonisées) des hommes. Le féminisme de la différence a mis en valeur la parole des femmes, les relations mères-filles, l'importance révolutionnaire de la création de groupes de femmes, et a critiqué le logocentrisme de la pensée occidentale (en particulier), y compris chez certaines féministes. Qualifié d'antiféminisme par certaines féministes radicales, ce mouvement se définit par sa valorisation des différences, la différence sexuelle étant la principale, sans éclipser les autres.

Ce mouvement, contemporain de l'apparition du féminisme radical français, a eu un profond impact à l'époque :
De fait, le féminisme de la différence a ensuite reçu davantage d'attention dans le monde anglo-saxon, jusqu'à être appelé , sans égards pour le fait que le féminisme français s'est graduellement opposé au féminisme différentialiste. Carol Gilligan a ravivé le féminisme différentialiste anglosaxon avec la publication d', dans les années 1980. Cet ouvrage met en évidence des trajectoires de développement moral qui se distinguent de celles, réputées plus masculines, de Lawrence Kohlberg. L'éthique de la sollicitude est un développement contemporain du féminisme de la différence.

La théologie féministe est un ensemble de courants féministes qui se fondent sur une étude des textes sacrés pour affirmer l'égalité des genres.

Le féminisme islamique, ou féminisme musulman, est un mouvement féministe proche de l'islam libéral, qui revendique un féminisme interne à l'islam et vise à une modification des rapports entre hommes et femmes au sein de la communauté musulmane.

Le féminisme pro-sexe est un courant du féminisme, issu du milieu queer, qui apparaît dans les années 1980 aux États-Unis et qui voit en la sexualité un domaine qui doit être investi par les femmes et les minorités sexuelles. En faisant , il s'oppose au féminisme radical.

Dans la mouvance pro-sexe, on trouve des organisations comme la SlutWalk ou Marche des Salopes en français, dont le slogan est : .

Des écrivaines comme Virginie Despentes ont contribué à la vulgarisation des thèses pro-sexes, avec notamment des livres comme "Baise-moi" et plus tard "King Kong Théorie". Un artiste queer Lazlo Pearlman a produit un film intitulé "Fake Orgasm", et la productrice de films pornographiques Erika Lust se revendique également d'une mouvance pro-sexe qui considère le potentiel libératoire dans l'éclatement de la norme genrée des pratiques sexuelles. La réalisatrice Ovidie en est également un exemple, qui s'investit dans ce mouvement à la fois comme actrice et réalisatrice de films pornographiques, et comme réalisatrice et autrice de documentaires qui développent une pensée théorique et critique. On trouvera également une approche plus théorique dans les ouvrages de Paul B. Preciado.

L'anarcha-féminisme ou féminisme libertaire, qui combine féminisme et anarchisme, considère la domination des hommes sur les femmes comme l'une des premières manifestations de la hiérarchie dans nos sociétés. Le combat contre le patriarcat est donc pour les anarcha-féministes partie intégrante de la lutte des classes et de la lutte contre l'État, comme l'a formulé Susan Brown : 

En 1896 et 1897 paraît en Argentine "La Voz de la Mujer" ("La Voix de la Femme"), première publication anarcha-féministe au monde. En épigraphe : (soit « Ni dieu, ni patron, ni mari »). La figure de proue en est Virginia Bolten, féministe révolutionnaire et communiste libertaire. Ce n’est pas le premier journal féminin en Amérique latine, mais c'est le premier journal féministe et révolutionnaire au sein de la classe ouvrière.

En Espagne, à partir de 1922, "Estudios" est à l'avant-garde d'une campagne en faveur de l'éducation sexuelle et de l'émancipation féminine. Ouverte aux débats sur les sexualités, cette revue éclectique et libertaire aborde le nudisme, l'amour libre et l'éducation sexuelle. Elle a une influence décisive sur la classe ouvrière espagnole en contribuant à faire évoluer radicalement les mentalités.

Fondée par Lucía Sánchez Saornil, Mercedes Comaposada et Amparo Poch y Gascón lors de la Révolution sociale espagnole de 1936, la fédération des Mujeres Libres, proche de la Confédération nationale du travail, se revendique d'un « féminisme prolétarien » et défend à la fois les idées anarchistes et féministes.

Aux États-Unis, Emma Goldman, Voltairine de Cleyre, Lucy Parsons et Kate Austin en sont les principales théoriciennes. D'autres figures marquantes de ce courant sont les Françaises Madeleine Vernet et Nelly Roussel ou la Suisse Paulette Brupbacher et la Suédoise Elise Ottesen-Jensen qui résume son combat en une phrase : 

Aussi appelé féminisme inclusif, le féminisme intersectionnel s'appuie sur les travaux de la féministe américaine Kimberlé Williams Crenshaw, la première à avoir popularisé le terme d'intersectionnalité, mais aussi en France sur les écrits de Christine Delphy, Éric Fassin ou Elsa Dorlin. Il a pour objectif de mieux prendre en compte les problèmes des femmes subissant d'autres discriminations en plus du sexisme, c'est-à-dire les personnes qui subissent plusieurs oppressions en même temps. Ce courant cherche principalement à porter les revendications des femmes non blanches victimes de racisme afin de lutter contre ce qu'il considère être le détournement du féminisme à des fins racistes.

Les féministes intersectionnelles ne s'intéressent pas à chaque discrimination de façon séparée, mais cherchent à comprendre comment les différentes discriminations se conjuguent et forment une oppression spécifique. Elles reprochent aux associations féministes plus traditionnelles de parler de problèmes qui ne les concernent pas directement, à la place des femmes qui vivent réellement ces situations. Elles déplorent également le caractère excluant de ces associations qui, selon elles, n'incluent pas suffisamment les femmes non blanches dans leur luttes.

En France, le mouvement est principalement porté par des militantes noires, se revendiquant de l'afroféminisme, qui critiquent notamment l'invisibilité médiatique des femmes noires et les diktats de beauté qu'elles subissent, comme l'estime Rokhaya Diallo : 
Comme la majorité des courants féministes, l'afroféminisme critique les normes de beauté imposées par la société. Mais il explique que les femmes noires – et également celles issues des autres minorités ethniques – subissent une double peine car l'idéal de beauté féminin occidental est destiné à des femmes blanches et correspond aux caractéristiques physiques de ces dernières (peau claire, nez fin et cheveux clairs). Ainsi Myriam Keita Brunet estime que le budget consacré à la beauté par les femmes non blanches est neuf fois plus élevé que celui des femmes blanches. Elles dépensent leur argent dans des produits éclaircissants, des défrisages, voire des opérations de chirurgie esthétique afin de ressembler au modèle occidental.

Les afroféministes critiquent également le manque de représentation des femmes noires dans les médias. Peu de fictions françaises mettent en scène des femmes noires dans des rôles principaux et, lorsque c'est le cas, c'est souvent pour incarner des jeunes femmes issues des quartiers populaires. Les militantes noires réclament, par conséquent, une représentation plus positive et réaliste des noires dans les fictions télévisuelles. De même, selon le magazine "Slate", seules 5 % des mannequins qui ont fait la couverture de "Vogue" en 2013 étaient noires ou métisses, 9 % asiatiques, 1 % issus d'autres minorités ethniques, contre 75 % de mannequins blancs. En ce qui concerne le domaine politique, trois des députés qui siègent à l'assemblée sont des femmes noires : George Pau-Langevin, Seybah Dagoma et Hélène Geoffroy. Les afroféministes critiquent cette quasi-absence des femmes noires et militent en faveur d'une présence accrue de celles-ci dans l'espace publique. Enfin, elles luttent contre les stéréotypes racistes que subissent les femmes noires, souvent associées, selon elles, à l'animalité et à un fantasme sexuel exotique.

Le féminisme radical est critiqué, avec des arguments très divers. Ces critiques sont notamment détaillées dans des ouvrages comme "Fausse Route" d'Élisabeth Badinter, "La Nécessaire Compréhension entre les sexes" de Paul-Edmond Lalancette, "Le Grand Mensonge du féminisme" de Jean-Philippe Trottier, "Ainsi soit-il, Sans de vrais hommes, point de vraies femmes" d'Hélène Vecchiali, ou encore "Le Premier Sexe" d'Éric Zemmour.

Sur le plan politique, le féminisme a parfois été qualifié par des marxistes-léninistes de « diversion » car dans cette analyse, toutes les classes sociales sont composées de femmes et d'hommes et les premières ne constituent pas une caste ou une classe particulière caractérisée par une réelle solidarité d'intérêts. De ce fait, l'invocation d'un conflit d'intérêts entre sexes ou la lutte pour l'émancipation d'un sexe à l'égard de l'autre serait un « artifice » ayant pour conséquence (voire pour but) de « masquer les vrais rapports de domination et les vraies lignes de fracture sociale ».

Dans un ordre d'idées différent, le féminisme est aussi relativisé parce qu'il minimiserait l'importance des critères de différenciation physique entre individus (sexe, âge, état de santé, couleur de peau, morphologie) qui sont pourtant autant facteurs essentiels de discrimination sociale et d'exclusion.

Dans la mouvance de la critique de Simone de Beauvoir, certaines auteurs comme Julia Kristeva, Sylviane Agacinski, Luce Irigaray ou Antoinette Fouque pensent que l'égalitarisme abstrait efface les différences sexuelles et prolonge ainsi l'androcentrisme de l'universalisme masculin. Ce discours critique différentialiste, de l'intérieur du mouvement féministe, est fréquemment reçu comme un recul essentialiste en France, moins au Québec (où les différentes mouvances sont exprimées, notamment au sein de l'Institut de Recherches et d'Études Féministes (UQÀM), au "Simone de Beauvoir Institute" (Université Concordia) et grâce à diverses politiques d'état), et s'exprime dans plusieurs féminismes hors de l'Occident (voir aussi ).

Le terme « virilisme » est parfois employé pour qualifier l'alignement de certaines féministes sur les droits et les mœurs masculines au détriment d'une véritable promotion du féminin dans l'humanité.

La féministe américaine bell hooks affirme que les hommes souffrent également d'un système patriarcal étouffant et déshumanisant. Elle appelle les hommes à refuser les codes du patriarcat qui les encouragent à devenir froids, violents et à refouler tous sentiments. Obtenir un tel changement passe par la prise de conscience de la souffrance masculine mais également par l'arrêt de la prolifération du patriarcat dans la culture populaire notamment.

En 2010, l'Organisation mondiale de la santé a émis des recommandations sur la place des hommes et des garçons dans le processus d'accession à l'égalité. L'OMS donne des exemples de politiques qui ont aidé les hommes de façon significative à contribuer à la condition des femmes :

Des mouvements de défense des intérêts spécifiques des hommes se sont créés en réaction au féminisme : le masculinisme et l'hominisme. Ils font pendant aux mouvements féministes « radicaux », en réduisant souvent le féminisme à ces courants, et se focalisent sur l'existence d'inégalités défavorables aux hommes ou perçues comme telles (par exemple la garde des enfants accordée plus souvent à la mère). Alors que les prises de position masculinistes relèvent souvent de l'antiféminisme, l'hominisme se dit favorable au féminisme et à l'égalité des genres mais préoccupé par certaines problématiques liées spécifiquement aux garçons.

Voir aussi la .

Voir aussi les liste de féministes et liste de féministes musulmanes.



















</doc>
<doc id="14249" url="https://fr.wikipedia.org/wiki?curid=14249" title="Rayon X">
Rayon X

Les rayons X sont une forme de rayonnement électromagnétique à haute fréquence constitué de photons dont la longueur d'onde est comprise approximativement entre 0,001 nanomètre et 10 nanomètres ( et ), correspondant à des fréquences de 30 pétahertz à 300 exahertz (3× à 3×). L'énergie de ces photons va d'une centaine d'ev (électron-volt), à environ un MeV.

Les rayons X ont été découverts en 1895 par le physicien allemand Wilhelm Röntgen, qui a reçu pour cela le premier prix Nobel de physique ; il leur donna le nom habituel de l'inconnue en mathématiques, X.

Les rayons X et les rayons gamma sont de même nature, mais sont produits différemment : les rayons X sont produits par des transitions électroniques alors que les rayons gamma sont produits lors de la désintégration radioactive des noyaux des atomes ou d'autres processus nucléaires ou subatomiques.

Les rayons X sont une des modalités principales de l'imagerie médicale et du contrôle non destructif où ils sont émis de manière non naturelle. Ils sont également utilisés en cristallographie. En astrophysique contemporaine, on mesure les rayonnements X de l'espace pour l'étudier.

À la fin du , Wilhelm Conrad Röntgen, comme de nombreux physiciens de l'époque, se passionne pour les rayons cathodiques qui ont été découverts par Hittorf en 1869 ; ces nouveaux rayons avaient été étudiés par Crookes. À cette époque, tous les physiciens savent reproduire l'expérience de Crookes mais personne n'a encore d'idée d'application de ces rayonnements.

En 1895, Wilhelm Conrad Röntgen reproduit l'expérience à de nombreuses reprises en modifiant ses paramètres expérimentaux (types de cibles, tensions différentes, etc.). Le , il parvient à rendre luminescent un écran de platinocyanure de baryum. Röntgen décide alors de faire l'expérience dans l'obscurité en plongeant son tube de Crookes dans un caisson opaque. Le résultat est identique à la situation normale. Röntgen place ensuite différents objets de différentes densités entre l'anode et l'écran fluorescent, et en déduit que le rayonnement traverse la matière d'autant plus facilement que celle-ci est peu dense et peu épaisse. Lorsqu'il place des objets métalliques entre le tube et une plaque photographique, il parvient à visualiser l'ombre de l'objet sur le négatif.

Röntgen en déduit que les rayons sont produits perpendiculairement à la direction d'émission des électrons du tube et que ce rayonnement est invisible et très pénétrant.

Faute de trouver une dénomination adéquate, Röntgen les baptise « rayons X ». Ce rayonnement est encore souvent appelé "" (littéralement : « rayons de Röntgen ») en Allemagne et dans toute l'Europe (sauf en France). L'autre nom de la radiologie est encore aujourd'hui la "röntgenologie".

Le premier cliché célèbre est celui de la main d'Anna Bertha Röntgen réalisé le (, pose de 25 minutes) ; il s'agit de la première radiographie. Un mois plus tard, Bergonié reproduit à Bordeaux l'expérience de Röntgen, avant que ce dernier publie officiellement.

Le , Röntgen publie sa découverte dans un article intitulé "« Über eine neue Art von Strahlen »" (en français : « À propos d'une nouvelle sorte de rayons ») dans le bulletin de la Société physico-chimique de Wurtzbourg.

C'est cette découverte qui lui vaudra le premier prix Nobel de physique en 1901.

Il tire quatre conclusions dans son article :

La recherche de Röntgen est rapidement développée en dentisterie, puisque deux semaines plus tard, le Dr Otto Walkhoff réalise à Brunswick la première radiographie dentaire. 

Il faut 25 minutes d'exposition. Il utilise une plaque photographique en verre, recouverte de papier noir et d'une digue (champ opératoire) en caoutchouc.
Six mois après paraît le premier livre consacré à ce qui va devenir la radiologie dont les applications se multiplient - dans le cadre de la physique médicale, pour le diagnostic des maladies puis leur traitement (radiothérapie qui donne une expansion extraordinaire à ce qui était jusque-là l'électrothérapie). Avant la fin de l'année 1896, Otto Walkhoff et Fritz Giesel ouvrent le premier laboratoire de radiologie dentaire. 

Les temps d'exposition sont alors longs et les effets secondaires notables. Les cas de brûlures sont nombreux. Certains praticiens, ne voyant pas le lien entre l'exposition aux rayons X et les brûlures, concluent qu'elles sont dues aux rayons ultra-violets. Certains hésitent toutefois à réaliser des clichés radiologiques sans nécessité. De plus, tous les praticiens ne relèvent pas les mêmes effets : le Williams, en 1897, indique que sur 250 patients exposés aux rayons X, il n'a noté aucun effet secondaire indésirable. Toutefois il ne faudra pas longtemps pour qu'un lien causal soit établi entre l'exposition prolongée aux rayons X et les brûlures : en 1902 le EA Codman, recensant les préjudices causés par les rayons X, note qu'ils ont pratiquement disparus.

Röntgen laissa son nom à la première unité de mesure utilisée en radiologie pour évaluer une exposition aux rayonnements. Le symbole des röntgens est R.

La découverte de Röntgen fit rapidement le tour de la Terre. Elle suscita des expériences y compris en dehors des cercles scientifiques et marqua l'imaginaire de la culture populaire. Des chercheurs tels que Thomas A. Edison, Nicola Tesla, A.A. Campbell Swinton firent immédiatement des expériences avec les rayons Röntgen. En novembre 1896, un inventeur américain, le Dr. Robert D'Unger proposa un "telephot" aux rayons-X, censé permettre transmettre les images par fil télégraphique. 

En 1897, Antoine Béclère, pédiatre et clinicien réputé, créa, à ses frais, le premier Laboratoire hospitalier de radiologie. 

Tout le monde voulait faire photographier son squelette. Mais pendant longtemps, les doses étaient trop fortes. Par exemple, Henri Simon, photographe amateur, a laissé sa vie au service de la radiologie. Chargé de prendre les radiographies, les symptômes dus aux radiations ionisantes apparurent après seulement deux ans de pratique. On lui amputa d'abord la main (qui était constamment en contact avec l'écran fluorescent) mais ensuite, un cancer généralisé se déclara.

Au début de la radiologie, les rayons X étaient utilisés à des fins multiples : dans les fêtes foraines où on exploitait le phénomène de fluorescence, dans les magasins où l'on étudiait l'adaptation d'une chaussure au pied des clients grâce au rayonnement et on les utilisait pour la radiographie médicale. Encore là, on fit quelques erreurs, par exemple en radiographiant les femmes enceintes.

Le premier « Congrès international de radiologie », qui réunit des scientifiques de la Grande-Bretagne, des États-Unis, de la France, de l’Allemagne, de l’Italie et de la Suède, émet en 1925 des recommandations portant sur les « rayonnements ionisants ».

Un monument a été inauguré en 1936 au voisinage du pavillon Roentgen de l’hôpital St-Georg, à Hambourg, à l’initiative du professeur allemand Hans Meyer, .

Avec les années, en médecine pour la fluoroscopie ou radioscopie, on diminua la durée des examens et les quantités administrées. En 1948, notamment, par la découverte de la « crête de Tavernier » par le physicien belge Guy Tavernier qui correspond à un accroissement de la dose d'irradiation dans les tissus avant leur décroissance avec la profondeur, ce qui mena à une réduction de la dose d'exposition de 1,2 Roentgen à 0,3 Roentgen par semaine au niveau international dès 1950. Cette valeur sera encore divisée par 3 dès 1958 pour tenir compte des risques potentiels d'effets génétiques.

Cent ans après leur découverte, on se sert encore des rayons X en radiographie moderne. On les utilise aussi dans les scanners, pour effectuer des coupes du corps humain, et dans les densitomètres pour détecter ou suivre l'ostéoporose. Plusieurs autres techniques sont actuellement utilisées en imagerie médicale : l'échographie (qui utilise les ultrasons), l'imagerie par résonance magnétique nucléaire, la scintigraphie ou encore la tomographie par émission de positons.

Mais l'utilisation des rayons X ne se limite pas au seul domaine de la médecine : les services de sécurité les utilisent pour examiner le contenu des valises ou des conteneurs aériens et maritimes sur écran. Les policiers les exploitent afin d'analyser les fibres textiles et les peintures se trouvant sur le lieu d'un sinistre. L'industrie agroalimentaire les utilise pour détecter les corps étrangers dans les produits finis. En cristallographie, on peut identifier divers cristaux à l'aide de la diffraction des rayons X. Enfin, depuis peu, il est possible d'étudier des fossiles piégés à l'intérieur d'un matériau (type ambre) et d'en voir des coupes virtuelles.

Les rayons X sont un rayonnement électromagnétique comme les ondes radio, la lumière visible, ou les infra-rouge. Cependant, ils peuvent être produits de deux manières très spécifiques :

Lors de la production de rayons X avec un tube à rayons X, le spectre est composé d'un rayonnement continu "()" auquel se superposent des raies spécifiques à l'anode utilisée et qui sont dues au phénomène de fluorescence.

Anecdotiquement, des rayons X peuvent être produits par triboluminescence, décollement d'un ruban adhésif sous vide, et la quantité émise peut alors suffire pour faire une radiographie (de mauvaise qualité) d'un doigt.

Historiquement, les rayons X étaient connus pour faire briller certains cristaux (fluorescence), ioniser les gaz et impressionner les plaques photographiques.

Les principales propriétés des rayons X sont les suivantes :

On distingue les rayons X durs et rayons X mous selon leur énergie, pour les premiers supérieure à environ (longueur d'onde inférieure à 0,1-). À l'inverse des rayons X durs, les rayons X mous sont facilement absorbés par une épaisseur millimétrique de matière solide.

Les rayons X sont des radiations ionisantes. Une exposition prolongée aux rayons X, ou une exposition répétée avec des répits trop courts pour l'organisme peut provoquer des brûlures (radiomes) mais aussi des cancers et des anomalies chez le nourrisson et l'enfant de moins de un an.

Les personnels travaillant avec des rayons X doivent suivre une formation spécifique, se protéger et être suivis médicalement (ces mesures peuvent être peu contraignantes si l'appareil est bien « étanche » aux rayons X).

Les rayons X sont invisibles à l'œil, mais ils impressionnent les pellicules photographiques. Si l'on place un film vierge protégé de la lumière (dans une chambre noire ou enveloppée dans un papier opaque), la figure révélée sur le film donne l'intensité des rayons X ayant frappé la pellicule à cet endroit. C'est ce qui a permis à Röntgen de découvrir ces rayons. Ce procédé est utilisé en radiographie médicale ainsi que dans certains diffractomètres (clichés de Laue, chambres de Debye-Scherrer). Il est aussi utilisé dans les systèmes de suivi des manipulateurs : ceux-ci doivent en permanence porter un badge, appelé « film dosimètre », enfermant une pellicule vierge ; ce badge est régulièrement changé et développé par des laboratoires spécialisés pour contrôler que le manipulateur n'a pas reçu de dose excessive de rayons X.

Comme tous les rayonnements ionisants, les rayons X sont détectés par les compteurs Geiger-Müller (ou compteur G-M). Si l'on diminue la tension de polarisation du compteur, on obtient un compteur dit « proportionnel » (encore appelé « compteur à gaz » ou « compteur à flux gazeux ») ; alors que le compteur G-M travaille à saturation, dans le compteur proportionnel, les impulsions électriques générées sont proportionnelles à l'énergie des photons X.

Les rayons X provoquent aussi de la fluorescence lumineuse sur certains matériaux, comme l'iodure de sodium NaI. Ce principe est utilisé avec les « compteurs à scintillation » (ou « scintillateurs ») : on place un photodétecteur après un cristal de NaI ; les intensités des impulsions électriques récoltées par le photomultiplicateur sont elles aussi proportionnelles aux énergies des photons.

De même qu'ils peuvent ioniser un gaz dans un compteur G-M ou proportionnel, les rayons X peuvent aussi ioniser les atomes d'un cristal semi-conducteur et donc générer des paires électron-trou de charges. Si l'on soumet un semi-conducteur à une haute tension de prépolarisation, l'arrivée d'un photon X va libérer une charge électrique proportionnelle à l'énergie du photon. Ce principe est utilisé dans les détecteurs dits « solides », notamment pour l'analyse dispersive en énergie ("EDX" ou "EDS"). Pour avoir une résolution correcte, limitée par l'énergie de seuil nécessaire à la création de charges, les détecteurs solides doivent être refroidis, soit avec une platine Peltier, soit à l'azote liquide. Les semi-conducteurs utilisés sont en général du silicium dopé au lithium Si(Li), ou bien du germanium dopé au lithium Ge(Li). 
Il existe cependant des détecteurs semi-conducteurs non refroidis à base de silicium ou de tellurure de cadmium. L'utilisation de structures à diodes bloquantes permet en particulier de réduire le bruit associé au courant d'obscurité.

La faible température n'a pas d'effet direct sur la valeur de l'énergie de seuil, mais sur le bruit de fond. Il est possible en revanche d'utiliser des supraconducteurs maintenus à très basse température afin de faire usage d'énergie de seuil vraiment petite. Par exemple l'énergie de seuil nécessaire à la création de charges « libres » dans le silicium est de l'ordre de , alors que dans le tantale supraconducteur, disons au-dessous de 1 kelvin, elle est de , soit fois plus faible. La diminution de la valeur de seuil a pour effet d'augmenter le nombre de charges créées lors de la déposition d'énergie, ce qui permet d'atteindre une meilleure résolution. Cette dernière est en effet limitée par les fluctuations statistiques du nombre de charge créées. L'amplitude de ces fluctuations peut s'estimer avec la loi de Poisson.
Des expériences récentes de détection d'un photon X à l'aide d'un calorimètre maintenu à très basse température () permettent d'obtenir une excellente résolution en énergie. Dans ce cas, l'énergie du photon absorbé permet de chauffer un absorbeur, la différence de température est mesurée à l'aide d'un thermomètre ultra sensible. 

Afin de comparer les approches : les détecteurs basés sur le silicium permettent une précision de la mesure de l'ordre de 150 eV pour un photon de eV. Un senseur supraconducteur au tantale permet d'approcher 20 eV, et un calorimètre maintenu à 0,1 K a récemment démontré une résolution d'environ 5 eV, soit un pouvoir de résolution de l'ordre de 0,1 %. 
Il est utile de mentionner que les méthodes de détection cryogéniques ne permettent pas encore de fabriquer des capteurs possédant un grand nombre d'éléments d'images (pixel), alors que les capteurs basés sur les semi-conducteurs offrent des « caméras » à rayons X avec plusieurs milliers d'éléments. De plus, les taux de comptage obtenus par les senseurs cryogéniques sont limités, à cps par pixel.

L'analyse des cristaux par diffraction de rayons X est aussi appelée radiocristallographie. Ceci permet soit de caractériser des cristaux et de connaître leur structure (on travaille alors en général avec des monocristaux), soit de reconnaître des cristaux déjà caractérisés (on travaille en général avec des poudres polycristallines).

Pour travailler avec un monocristal, on utilise l'appareil ci-contre :

Utilisé en géologie et en métallurgie, c'est aussi un outil de biophysique, très utilisé en biologie pour déterminer la structure des molécules du vivant, notamment en cristallogenèse (c'est l'art de fabriquer des monocristaux avec une molécule pure) ;
dans ce cadre, un monocristal de la molécule est mis dans un faisceau de rayons X monochromatiques et la diffraction observée pour différentes positions du cristal dans le faisceau de rayons X (manipulé par un goniomètre) permet de déterminer non seulement la structure du cristal, mais aussi et surtout la structure de la molécule. C'est notamment par radiocristallographie que Rosalind Franklin, puis James Watson, Francis Crick, Maurice Wilkins et leurs collaborateurs ont pu déterminer la structure hélicoïdale de l'ADN en 1953.

Dans l'Union européenne, l'utilisation des rayons X est soumise aux normes Euratom 96/29 et 97/43. La directive 97/43/ Euratom du 30 juin 1997 aurait dû être transposée en Droit interne français au plus tard le 30 mai 2000.

En France, il faut se référer :

L'organisme chargé du contrôle est l'Autorité de sûreté nucléaire (ASN), créée par la loi du 13 juin 2006 relative à la transparence et à la sécurité en matière nucléaire (loi TSN). L'ASN a remplacé la DGSNR, la Direction générale de la sûreté nucléaire et de la radioprotection, créée par le décret du 22 février 2002, modifiant le décret du décembre 1993,qui avait elle-même remplacé la DSIN (Direction de la sûreté des installations nucléaires).

« X-ray » est aussi l'appellation de la lettre X dans l'alphabet radio international.



</doc>
<doc id="14253" url="https://fr.wikipedia.org/wiki?curid=14253" title="Hanyu pinyin">
Hanyu pinyin

Le Hanyu Pinyin () est un système de romanisation du chinois mandarin, promu officiellement par la République populaire de Chine, puis la République de Chine (Taïwan). C'est le système de transcription de cette langue le plus répandu de nos jours dans les ouvrages modernes. En 1979, l'Organisation internationale de normalisation a adopté le pinyin comme système de romanisation du mandarin. Elle porte aujourd'hui l'identifiant "ISO 7098". Elle a également été adoptée par la République de Chine (Taïwan) pour la romanisation dans les instances gouvernementales en 2009, tout en n'étant pas imposée dans l'éducation ou les méthodes de saisie informatique.

Le projet de transcription de la langue chinoise aboutissant au pinyin a été lancé par le linguiste Zhou Youguang et approuvé le , pendant la cinquième session plénière de l'Assemblée populaire nationale de la République populaire de Chine. Le pinyin a été adopté en 1979 par le gouvernement chinois. Il supplanta des transcriptions plus anciennes comme le système Wade-Giles (1859 ; modifié en 1912) ou le Bopomofo, qui reste utilisé en République de Chine. Des systèmes similaires ont été conçus pour les autres dialectes chinois et les minorités non "han" de la Chine. Le terme pinyin signifie littéralement « assembler les sons » en mandarin standard et se réfère au hanyu pinyin (, soit dans le contexte : « assembler les sons de la langue des Hans »).

Le Pinyin est devenu la méthode dominante pour écrire le chinois sur ordinateur en chine continentale, alors que c'est le Bopomofo qui est le plus utilisé à Taïwan. Depuis 1958, le Pinyin a été activement utilisé dans l'éducation des adultes, ce qui rend plus facile pour les personnes autrefois analphabètes de continuer à s'auto-éduquer après une courte période d'instruction en Pinyin. Le Pinyin est devenu un outil pour beaucoup d'étrangers cherchant à apprendre la prononciation du Mandarin, et il est utilisé pour expliquer à la fois la grammaire et le Mandarin parlé, couplé avec l'étude des Caractère chinois. Les livres contenant des caractères chinois et des transcriptions Pinyin sont donc souvent utilisés par des apprenants étrangers. Par simplification, les marques diacritiques qui marquent les 4 tonalités sont souvent omises dans les grands journaux chinois, et même dans les travaux scolaires. Il en résulte un certain degré d'ambiguïté quant aux mots représentés. Le contexte devient alors très important pour bien comprendre le mot.

Le mandarin étant une langue tonale à quatre tons, ceux-ci sont représentés en pinyin par les diacritiques suivants : 

Lorsqu'on ne dispose pas des caractères accentués du pinyin, on peut utiliser des chiffres écrits après les syllabes, correspondant au numéro du ton, le 0 indiquant l'absence de ton.

Le pinyin utilise l'alphabet latin. La prononciation du chinois est souvent décrite par les mots "attaque" et "finale". L'attaque est la consonne au début d'une syllabe, alors que la finale est la combinaison d'une "médiane" éventuelle (semi-voyelle avant une voyelle), d'un "noyau" et éventuellement d'une "coda" (la voyelle ou consonne à la fin de la syllabe). En mandarin, la coda est toujours une voyelle ou une consonne nasale, et, quelquefois, un "-r", qui est attaché comme un suffixe grammatical. 

La première ligne indique l'API, la deuxième le pinyin.

"Voir aussi "

Dans chaque cellule, la première ligne indique l'API, la deuxième le pinyin pour une syllabe sans initiale, et la troisième le pinyin pour une syllabe avec initiale. Ce qui suit est un tableau de toutes les formes de syllabes possibles en mandarin. 
² "ü" s'écrit "u" après "j", "q", ou "x".
³ "uo" s'écrit "o" après "b", "p", "m", ou "f".

Les premiers diagrammes connus de voyelles chinoises ont été publiés en 1920 par le linguiste Yi Tso-lin. Le diagramme suivant se réfère au chinois standard ; la première ligne indique l'API, la deuxième le pinyin.

Cette table donne les caractères Unicode correspondant aux lettres accentuées utilisées en pinyin.

"Voir l'article : Méthode d'encodage pinyin"

Il est possible de saisir des caractères chinois sur un clavier alphabétique, en frappant le pinyin, avec ou sans le ton exprimé par un chiffre (1 à 4, le zéro correspondant à l'absence de ton). Une liste de sinogrammes est proposée et l'opérateur choisit. Des mécanismes d'anticipation basés sur un vocabulaire permettent de proposer en premier les sinogrammes les plus probables, notamment le suivant d'un mot polysyllabique, et une mémoire des frappes antérieures propose en premier les sinogrammes et les mots polysyllabiques déjà utilisés. 

Les systèmes d'exploitation les plus courants Windows, GNU/Linux et Mac OS X possèdent en standard des fonctions de saisie du chinois en pinyin.


Il est également possible sous Linux de taper du pinyin avec ton, afin d'écrire des translittérations pinyin.

Pour cela il y deux méthodes :

1. On utilise le module SCIM zh-pinyin (taper la lettre suivi du numéro du ton) (attention aux touches "a" et "q", qui sont parfois inversées).
2. On utilise la touche compose, combinée avec :

Dans certains cas, le quatrième ton ne marche pas. Par contre, "à", "è" et "ù" sont déjà présents sur les claviers azerty français, ou bien il est possible d'utiliser l'« input pad » de SCIM (clavier virtuel, touches composées), puis de combiner grave + caractère (a/e/i/o/(")u).
Il est également possible sous Windows de taper du pinyin avec ton en utilisant un pilote clavier français enrichi comme celui du projet libre FrElrick. Dans ce dernier, les tons sont gérés comme des accents, ainsi :
"À noter que les caractères ü et Ü dans ce dispositif doivent être saisis "directement" sans passer par une frappe muette d’accent, et ces caractères ont donc été ajoutés à la touche de la lettre U, respectivement en (AltGr) et en (AltGr)+(Cap) pour permettre de les accentuer avec un ton."
Enfin, il est possible de taper du pinyin sous Linux, Windows et Mac, en utilisant la disposition de clavier bépo, qui est une disposition Dvorak. Cette disposition permet entre autres d'écrire énormément de caractères, grâce aux touches mortes. Voici les exemples pour la voyelle "u" (pour les autres voyelles, on procède par analogie) :

La composition des messages, et des noms dans les répertoires, sur un clavier numérique dont chaque touche correspond à plusieurs caractères latins, tire parti du nombre réduit de syllabes existantes (420 environ). Les syllabes possibles du pinyin s'affichent à la frappe des touches, par prédiction, comme dans les méthodes avec clavier alphabétique. Le choix par curseur de la syllabe affiche une liste de caractères, qu'on choisit, donc, par curseur. La composition d'un message en sinogrammes est plus rapide que la composition d'un message équivalent en caractères latins correctement orthographié. Toutes les grandes marques distribuent la variante de logiciel de leurs appareils qui offre ce mode de saisie, en plus du mode alphabétique et des modes spécialisés. 

Les smartphones sont généralement équipés d'écrans numériques. Des claviers spécialisés sont donc affichés sur la surface de l'écran. Un clavier de type QWERTY ou AZERTY peut être utilisé pour taper en pinyin avec prédiction de caractères, sur Android : Sogou pinyin, Google pinyin, 国笔GB输入法 (qui propose aussi zhuyin, méthode cāngjié et écriture manuscrite), etc.



</doc>
<doc id="14255" url="https://fr.wikipedia.org/wiki?curid=14255" title="Macao">
Macao

Macao ( ; portugais : "Macau"), officiellement Région administrative spéciale de Macao de la République populaire de Chine, est une région administrative spéciale (RAS) de la République populaire de Chine depuis le . Auparavant, Macao a été colonisé et administré par le Portugal durant plus de et est considéré comme le dernier comptoir ainsi que la dernière colonie européenne en Chine et en Asie.

La création de cette administration remonte au milieu du , lorsque Macao a été colonisé et "occupé graduellement" par les Portugais. Ils ont rapidement apporté la prospérité à la zone, ce qui en a fait une grande ville et un intermédiaire important dans les échanges entre la Chine, l'Europe et le Japon, en atteignant son apogée à la fin du et au début du . À partir de 1887, la Chine reconnaît officiellement la souveraineté et l'occupation perpétuelle du Portugal à Macao à travers le "Traité sino-portugais de Pékin", un des traités inégaux, signés entre les puissances impériales occidentales et les empires colonisés d'Extrême-Orient. En 1967, à la suite de l'émeute soulevée par les pro-communistes chinois résidents à Macao le , le Portugal renonce à son occupation perpétuelle du territoire. En 1987, après d'intenses négociations entre le Portugal et la Chine, les deux pays convinrent que Macao allait revenir à la souveraineté chinoise le . Aujourd'hui, Macao connaît une croissance économique rapide, basée sur le fort développement du tourisme et des jeux d'argent. Le chiffre d'affaires des casinos y est d'ailleurs quatre fois plus élevé qu'à Las Vegas, ce qui fait de Macao l'une des villes les plus riches du monde. Orson Welles l'a d'ailleurs qualifié de « ville la plus pervertie au monde » et le poète W. H. Auden de « ville de l'indulgence ».

Macao se compose de la péninsule de Macao et de deux îles, Taipa et Coloane, qui sont reliées ensemble par des terres gagnées sur la mer (l'isthme de Cotai), totalisant une superficie de . Macao est situé sur la côte sud de la République populaire de Chine, à l'ouest de l'embouchure de la rivière des Perles et à de Hong Kong, qui se trouve approximativement en face de Macao. Les frontières Nord et Ouest communiquent avec la zone économique spéciale de Zhuhai qui fait partie de la province de Guangdong.

Sa population est de en 2016, la majorité de la population étant constituée de Chinois de souche.

Depuis le , le nom officiel de Macao est « Région administrative spéciale de Macao de la République populaire de Chine » (RASM). Après la création de la RASM, Macao est régi selon les principes du gouvernement central de la République populaire de Chine, soit « un pays, deux systèmes », ainsi que de « l'administration de Macao par le peuple de Macao » avec un « haut degré d'autonomie », profitant ainsi d'un régime spécial, similaire à celui de Hong Kong. Par conséquent le gouvernement local gère tout, sauf les relations extérieures et la défense. La République populaire de Chine a garanti le maintien de son système économico-financier et de ses spécificités pour au moins , c'est-à-dire au moins jusqu'en 2049.

Le nom portugais, "Macau", provient de l’approximation phonétique de la réponse donnée par les habitants quand les marins portugais demandèrent comment s’appelait ce lieu : « "A-Ma Kong" » (baie de A-Má en cantonais), nom qui doit son existence au temple en l'honneur de la déesse A-Má qui s’y trouvait. "A-Ma Kong" est devenu "Amacao", dont la première mention est faite en 1555 par l’écrivain et explorateur portugais Fernão Mendes Pinto, "Macao" et, en portugais, "Macau".

Son nom chinois, 澳門 (pinyin : "àomén"), qui signifie littéralement « porte de la baie », apparaît pour la première fois en 1564 et semble provenir de la description faite de la partie du sud de la péninsule de Macao où les Portugais furent autorisés à s’établir ; la baie y est dite entourée de deux collines qui sont « comme les jambages d’une porte ». Par ailleurs, nombre de toponymes de villes côtières chinoises contiennent le caractère 門 qui signifie « porte ». 

Ce nom est prononcé Omoune mn en cantonais et a été transcrit "oumun" ou "oumun" selon le système deromanisation cantonaise Jyutping et "ou mún" selon le système Yale.

Macao apparaît dans les documents chinois anciens sous différents noms dont , et . L’origine incertaine de ces noms est peut-être celle attribuée par l'historien chinois Fei Chengkang et reprise par Philippe Pons : les deux larges rades au nord et au sud de la partie enflée de la péninsule étaient rondes comme des miroirs.

Macao est située au sud du tropique du Cancer, sur la côte sud de la République populaire de Chine bordant la mer de Chine méridionale. Établie à l'ouest du delta de la rivière des Perles, elle est à de Canton et à de Hong Kong qui se trouve de l'autre côté du delta. Macao possède des frontières au nord et à l'ouest avec la zone économique spéciale de Zhuhai qui fait partie de la province de Guangdong. La longueur de son littoral est de .

En un siècle, le territoire de Macao s'est considérablement étendu sur la mer passant de en 1912 à en 2007. Les premiers grands travaux de remblaiement des eaux de faible profondeur du delta de la rivière des Perles eurent lieu durant les années 1920 (port extérieur, "Ilha verde") et 1930 ("Praia grande"). Les anciennes villas et fortifications portugaises qui jadis jalonnaient la côte se trouvent depuis le long de l’« avenue de la grande plage » en pleine ville.

Les îles qui formaient Macao ont été peu à peu reliées entre elles, soit par des ponts, soit par des terre-pleins. L’isthme reliant Taipa à Coloane date ainsi de 1968 et le premier pont entre la péninsule et Taipa, le pont du gouverneur Nobre de Carvalho, de 1974.

Mais c’est surtout à partir de 1991, que le rythme s’accélera (68 % de croissance entre 1991 et 2007). C'est sur ces terrains que furent notamment construits l’Aéroport international de Macao en 1995 et la « Tour Macao » en 1998. À partir de 1997, les eaux peu profondes qui séparaient Taipa et Coloane furent à leur tour « enterrées ». C’est sur ces terrains appelés Cotai, contraction des premières syllabes de Coloane et Taipa, que furent construits les casinos du "Cotai strip", le premier étant le "Venetian".

Aujourd'hui, Macao a une superficie de et est composé de la péninsule de Macao (), des îles de Taipa () et Coloane () et de la zone de Cotai (). C'est dans la péninsule de Macao que se concentrent l'activité principale, l'essentiel des organes politiques et administratifs, la plupart de l'industrie et les principaux services d'équipement culturels.

Le Conseil des affaires de l'État de la République populaire de Chine a approuvé en 2009 un plan qui vise à gagner supplémentaires principalement pour créer des logements pour . Pour cela, 5 terre-pleins, appelés zones A, B, C, D, E sont en cours de création.
Superficie de Macao (km)

Source : Direction des services de cartographie et des cadastres

La surface de Macao est essentiellement composée de plaines (70 % de la surface) constituées par la sédimentation naturelle puis par les terrains gagnés sur la mer. Cependant, elle possède plusieurs collines de granite abruptes (20 % du total) témoins de la masse rocheuse originelle : la colline de Guia (), la colline de Barra (), la colline de Mong-Ha (), la colline de Penha (), la colline de l'Île-Verte (), la colline Monte () et la colline Dona Maria II (). Ces points culminants sont maintenant souvent masqués par les grands bâtiments construits récemment.

La colline de Coloane () est le point culminant de la RAS, tandis que Taipa culmine à avec la colline de "Taipa grande" et à avec la colline de "Taipa pequena".

De 1928 à la rétrocession en 1999, Macao était divisé en deux municipalités : la municipalité de Macao et la municipalité des Îles, qui étaient chacune administrées par un organe exécutif appelé « chambre municipale », elle-même supervisée par une assemblée municipale.

À son tour, la municipalité de Macao, qui couvrait toute la péninsule de Macao, était divisée en cinq "freguesias" :

La municipalité des Îles (créée en 1928), qui couvrait les îles de Taipa et de Coloane, était divisée en deux "freguesias" :


Mais, après une période transitoire, les municipalités et leurs organes municipaux provisoires ont finalement été dissous le et un nouvel organe administratif, l'Institut pour les affaires civiques et municipales (IACM) couvre depuis toute la RAS. Les "freguesias" n'ont plus de statut administratif et sont seulement reconnues par le gouvernement comme de simples divisions symboliques de Macao. Cotai, terre gagnée sur la mer entre Taipa et Coloane, n'a ainsi pas été affecté à une "freguesia".

Macao est situé dans la zone de mousson et son climat est subtropical humide : chaud et humide en été, frais et sec en hiver.

Durant l’été, la plus longue saison de l'année à Macao, entre mai et octobre, il y a souvent de fortes pluies, des orages, des tempêtes tropicales (typhons), et des températures élevées. Lorsque les vents atteignent des vitesses de , le code local des tempêtes tropicales est déclenché coupant les liaisons maritimes et aériennes. L'une de ces tempêtes ayant causé beaucoup de dégâts récemment, du fait de la conjonction avec les marées d'équinoxe, date du , lors du passage du typhon Hagupit.

La plus agréable saison de l'année est l'automne, qui commence à la mi-octobre, lorsque la température de l'intérieur de la Chine se réduit, et se termine mi-décembre. Le temps y est généralement chaud et le ciel y est clair.

En janvier et février (les mois d'hiver), Macao est touché par des vagues de froid et de vents secs provenant du nord de la Sibérie, refroidissant ses températures, avec des minima qui tombent au-dessous de .

Le printemps se situe aux mois de mars et d'avril, le vent souffle d'est en ouest, augmentant la température et l'humidité. À cette période de l'année, les bruines humides et une faible visibilité surviennent relativement fréquemment.

Les variations climatiques de Macao, qui sont relativement importantes entre l'été et l'hiver, sont principalement causées par la mousson. En 2006, les valeurs absolues de la température au maximum (en été) et minimale (en hiver) de l'air ont été de et de , respectivement. La température moyenne annuelle entre 1971 et 2000 y est de , la moyenne des précipitations annuelles est de et la moyenne de l'humidité relative de 83 %.

Macao a, selon le recensement intermédiaire de 2016, une population de , ce qui en fait le territoire le plus densément peuplé du monde devant Monaco et Singapour, avec (sur une surface de ). La densité de population a cependant déjà été plus élevée. Elle était de en 1985 avant les travaux de terrassement permettant d’agrandir le territoire sur la mer (population de sur une surface deux fois plus petite : ) et a atteint en 1940-1941 pendant la Seconde Guerre mondiale et l’occupation japonaise de Guangzhou et Hong Kong (population de sur une surface de ).

Courbe d'évolution démographique de Macao depuis 1825 

Actuellement, la croissance de la population, notamment de la population active, est principalement soutenue par l'immigration venant de Chine continentale, des Philippines et d'autres pays d'Asie. En février 2011, la main d'œuvre importée se chiffrait à , soit 24,5 % de la population active occupée.

Le taux d'accroissement démographique annuel pour la période 2004-2009 est en effet de 4,7 % en moyenne, mais le taux de accroissement naturel n'est que de 0,5 % pour la même période, la différence représentant le solde migratoire. Le taux de natalité est l'un des plus faibles au monde : estimé à en 2011, celui-ci est cependant en hausse depuis 2002 quand il avait atteint après 14 années consécutives en baisse. Macao est cependant l'un des endroits où l'espérance de vie à la naissance (84,4 ans en 2010) est la plus forte et le taux de mortalité infantile le plus bas (environ 3,18 décès pour ). Plus concrètement, environ sont nés à Macao, y sont mortes et y ont immigré en 2010.
La population est en état de vieillissement démographique même si celui-ci est en partie caché depuis 2003 par la forte immigration de personnes en âge de travailler. De 1996 à 2010, le pourcentage de personnes âgées de plus de 65 ans est passé de 6,9 à 8,0 tandis que celui de personnes âgées de moins de 15 ans est passé de 25,7 à 12,2.

La population de cette région est composée principalement de Chinois (94 %) nés en Chine continentale (47 %) selon le dernier recensement intercalaire de 2006. 42,5 % de la population est née à Macao, 3,7 % à Hong Kong et seulement 0,3 % au Portugal. De même pour la nationalité, seulement 1,7 % de la population a la nationalité portugaise. La notion de « nationalité » dans les recensements de Macao a cependant radicalement changé depuis la rétrocession, ce qui explique la différence entre les nombres ci-dessus et ceux de l'estimation de population de 1988 qui dénombrait 20,69 % de « citoyens portugais ». Celle-ci se basait en effet sur la loi portugaise de la nationalité, elle-même basée sur le principe du « droit du sol » jusqu'en 1976. Ainsi toutes les personnes nées à Macao avant 1976 avaient automatiquement la nationalité portugaise et de même pour toute personne née entre 1976 et 1999 et dont au moins un des parents avaient la nationalité en vertu du « droit du sang » appliqué ensuite. Depuis 1999, la loi sur la nationalité de la République populaire de Chine, ne reconnaissant pas la double nationalité, s'applique et les personnes nées à Macao d'ascendance chinoise acquièrent automatiquement la nationalité chinoise.

Les personnes qui ont une ascendance mixte portugaise et asiatique (chinoise mais aussi malaise, indienne, sri lankaise) nées ou vivant à Macao ainsi que certains Chinois convertis au catholicisme et parlant le portugais sont appelés les Macanais ou les « fils de la terre », formant jusqu'à la rétrocession une « minorité influente ». Nombre d'entre eux ont cependant émigré avant la rétrocession et très peu savent encore parler le patois macanais, basé sur le créole portugais, qui est menacé de disparition.

Les langues officielles sont le portugais et le chinois. Le cantonais est la langue la plus parlée, en 2006, avec 91,9 % de la population. Le mandarin est parlé par 38,5 % de la population et le portugais par 2,4 %. L' Anglais est parlé par 12 % de la population (surtout chez les plus jeunes). 

Même si la langue portugaise est très isolée et largement supplantée par l'anglais dans la région, elle est toutefois maintenue à Macao en accord avec la loi fondamentale de la région administrative spéciale de Macao. Le gouvernement chinois y voit aussi une occasion pour Macao de servir de plate-forme de contacts avec le monde lusophone.

En effet le portugais est parlé par plus de 200 millions de personnes dans le monde et certains pays de langue portugaise sont soit, comme l'Angola, des partenaires économiques importants, soit, comme le Brésil, des pays émergents aux perspectives économiques prometteuses.

Selon des études archéologiques, la région était déjà habitée il y a à ans. Au cours du , un certain nombre de Chinois fidèles à la cour Song du Sud fuient les envahisseurs mongols et s'installent dans la région après la bataille de Yamen qui a lieu dans l'estuaire de la rivière des Perles non loin de Macao le 12 mars 1279. Ils établirent de nouvelles colonies de peuplement dans ce qui était le district de "Xiangshan" () dont dépendait "Haojing" (ancien nom de Macao). Le « vieux temple Yongfu » (dont il ne reste rien), aurait été construit à cette époque à Patane au nord de la péninsule de Macao, mais cela n'a pas été prouvé.

À la fin du , la répression contre les familles de généraux de la dynastie Yuan vaincue par la dynastie Ming, conduit certains pêcheurs de la province de Fujian à fuir vers le sud. Un rapport écrit en 1787 par les chefs du village de Mongha (), soulignant que leurs ancêtres vivaient dans ce village depuis au moins 300 ans, sert de base à l'histoire officielle qui veut que Macao était habité avant l'arrivée des Portugais.

Après la découverte de la route des Indes par Vasco de Gama en 1498, les Portugais continuent leurs explorations maritimes le long de la côte de l'océan Indien. Ils « découvrent » ainsi Cochin en 1500 (fondant l’Inde portugaise en 1505), Goa en 1510, Malacca en 1511, les fameuses « îles aux épices », les Moluques, en 1512 et arrivent dans le delta de la rivière des Perles en 1513 sous le commandement de Jorge Álvares. En 1517, Fernão Pires de Andrade se rend à Canton et obtient la permission que l'ambassadeur Tomé Pires se rende à Pékin pour y rencontrer l'empereur. La construction illégale d'un fort sur ce que les portugais appellent l'île de Tamao () et la supposée abduction d'enfants chinois par le frère de Fernão Pires de Andrade changent cependant l'attitude des Chinois vis-à-vis de l'ambassade portugaise. L'empereur décrète alors l'interdiction de tout commerce avec les "Folangji" (, nom venant de « français », donné par les traducteurs musulmans à tous les étrangers). En 1521, l'Empereur Ming envoie la marine reprendre le port lors de la .

Sous l'impulsion du roi Jean III de Portugal, et d'une diplomatie plus pragmatique et tolérante, réussit à inverser l'édit impérial en 1554 et les marchands portugais retournent à Canton, tout en étant autorisés à s'établir sur les îles de Sancian (, où François Xavier meurt en 1552) puis de Lampacao ().

Cherchant un port plus proche de Canton sur le continent proprement dit, les Portugais s'établissent de manière saisonnière dans la rade de Macao entre 1553 et 1554 et demandent à rester sur la terre ferme en prétextant qu'ils doivent sécher leur cargaison. En 1557, les autorités chinoises accordent finalement l'autorisation aux Portugais de s'y établir de façon permanente, en leur donnant un degré considérable d'autonomie. En échange, les Portugais payent une sorte de loyer annuel (près de 500 taels d'argent) et certaines taxes, qui ont fait valoir que Macao faisait encore partie intégrante de l'Empire chinois. Les autorités chinoises, exprimant de la crainte et du mépris pour les étrangers, suivent de près les activités des Portugais de Macao et exercent, jusqu'au milieu du , une grande influence dans l'administration de leur comptoir commercial.

Depuis, Macao a été conçu comme un comptoir de type triangulaire entre la Chine, le Japon et l'Europe à un moment où les autorités chinoises ont interdit les échanges directs avec l'archipel nippon pendant plus d'une centaine d'années. Ce commerce lucratif a introduit une grande prospérité à Macao, ce qui en fit une grande ville commerciale et l'a aidée à atteindre son apogée entre la fin du et le début du .

En plus d'être un comptoir commercial, Macao a également joué un rôle actif et central dans la propagation du catholicisme à l'étranger, jusqu'à devenir un haut lieu de formation de missionnaires catholiques de différents pays de l'Extrême-Orient, principalement pour la Chine. Pour cette raison, le pape Grégoire XIII crée en 1576 le diocèse de Macao. Ces missionnaires ont également joué un rôle important dans l'échange culturel, scientifique et technique entre la Chine et l'Occident, et le développement de la culture et de l'éducation à Macao.

En 1583 fut créé le Leal Senado, le siège et le symbole du pouvoir et du gouvernement local, par des résidents portugais et plus précisément par les commerçants de Macao. Cet organe politique, considéré comme le premier conseil municipal de Macao, fut fondé dans le but de protéger le commerce contrôlé par Macao, d'établir l'ordre et la sécurité de cette ville et de résoudre les questions et les problèmes quotidiens. Bien qu'en 1623 Macao se dote d'un gouverneur portugais, le Leal Senado a continué à maintenir une grande autonomie et un rôle clé dans l'administration de la ville jusqu'à la première moitié du .
En raison de sa prospérité, Macao a souvent été attaqué par les Néerlandais au cours de la première moitié du . La plus célèbre attaque a eu lieu le , où près de 800 soldats néerlandais débarquent dans une tentative de conquérir la ville. Après deux jours de combat, le 24 juin les envahisseurs sont vaincus. Le bilan des victimes est élevé, environ . Cependant, seulement une dizaine de soldats portugais sont tués. Pour Macao, non préparé, cette victoire a été considérée comme un miracle.
De 1638 à 1853, le commerce portugais avec le Japon a pris fin en raison de la politique d'isolement ("Sakoku"), menées par l'ancien Shogun japonais, Ieyasu Tokugawa. Cette décision a gravement affecté l'économie de Macao, qui déclina rapidement.
Le , Jean-François de La Pérouse y débarque dans le cadre de son expédition, comptant y vendre ses fourrures achetées en Alaska. Il décrit la dureté du mandarin chinois et la faiblesse de la réaction du gouvernement portugaise.

Dans le contexte de la Guerre péninsulaire, en septembre 1808, le territoire est occupé par les troupes de la force expéditionnaire sous le commandement de l'amiral William O'Brien Drury, commandant en chef des forces navales britanniques dans les mers asiatiques, prétextant une protection contre la menace française. Cet effectif fut finalement rappelé à la fin de cette même année, en raison de la présence d'environ de l'armée chinoise aux portes de la ville.

Depuis le milieu du , Macao, même en ayant perdu de nombreux marchés commerciaux à partir de la fin des échanges avec le Japon et vivant avec une certaine fréquence dans la pauvreté et la misère, a tout de même réussi à conserver son importance économique et stratégique en tant que port européen en Chine. Toutefois, cette importance a été fortement réduite avec la Première guerre de l'opium en 1841 lorsque Hong Kong est devenu le port le plus important de l'Ouest de la Chine.

En 1844, par le biais d'un décret royal, Macao a finalement été intégré dans le système ultramarin portugais. Toutefois, le décret n'a pas été reconnu par la Chine. Cet acte a par ailleurs redéfini le pouvoir à Macao affirmant que le gouverneur est le principal organe politique et administratif, et non plus le Leal Senado, qui avait perdu son importance et son influence politique depuis 1834.

En 1845, le Portugal déclare la ville port franc. Le gouverneur João Ferreira do Amaral, en fonction de 1846 à 1849, ordonne la fin du loyer annuel et des taxes chinoises, l'expulsion des mandarins de Macao et l'abolition, en 1849, de la douane chinoise (le "Ho-pu").

Au cours du , les Portugais occupèrent la partie Nord de la péninsule de Macao (alors occupée par les Chinois), les îles de Taipa (en 1851) et Coloane (en 1864). Ils ont également commencé à étendre leur influence sur les îles voisines de Lapa, Dom João e Montanha.

En 1887, le Portugal organisa avec le faible gouvernement chinois de l'époque le "Traité d'amitié et de commerce sino-portugais", qui reconnaît et légitime l'occupation perpétuelle de Macao et de ses dépendances par le Portugal.

Le gouvernement de Macao, souhaitant créer sa propre monnaie officielle, autorise, en 1901, la Banco Nacional Ultramarino (BNU) à émettre des billets sous le nom de patacas. Les premiers billets imprimés pour distribution sont créés en 1906 et 1907.

Le Portugal n'a pas officiellement participé à la Seconde Guerre mondiale (1939-1945), Macao devint alors l'un des seuls endroits en Asie à rester neutre durant le conflit mondial. Pour cette raison, un grand nombre de réfugiés chinois fuyant l'occupation japonaise se sont réfugiés provisoirement à Macao, ce qui a doublé sa population durant cette période. Cet afflux de réfugiés a causé de nombreux problèmes, en particulier ceux relatifs à la surpopulation et au manque de nourriture.

Le Japon respecta la neutralité du Portugal, et donc celle de Macao. Mais les Japonais occupèrent la colonie Portugaise du Timor oriental en Asie. Cependant, même en n'occupant pas le territoire, les forces japonaises exercèrent une énorme pression sur le gouvernement de Macao, le menaçant à plusieurs reprises. Par exemple, en 1941 les îles de Lapa, Dom João e Montanha, officiellement occupées par les Portugais en 1938, ont été abandonnées en raison d'une menace émise par l'armée japonaise. En conséquence, les Japonais les occupèrent, mais avec la fin de la Seconde Guerre mondiale, en 1945, les rendirent à la Chine en raison de l'incapacité des Portugais à les réoccuper.

1949 marque la fondation de la République populaire de Chine (RPC) à caractère communiste et anticolonialiste. La nouvelle République lista le "Traité de l'amitié et du commerce sino-portugais " parmi les nombreux traités inégaux imposés par les puissances européennes à la Chine et déclarés invalides en conséquence. Mais le nouveau régime n'étant pas encore prêt à aborder la question historique de ces traités, le "statu quo" de Macao fut provisoirement maintenu.

Le , la célèbre révolte dite émeute 1-2-3 éclate. Elle est le fait de Chinois pro-communistes insatisfaits et fortement influencés par la Révolution culturelle chinoise de Mao Zedong. Lors de cette journée de protestation, sont tuées et environ blessées. Ces évènements exigèrent la mobilisation de soldats pour contrôler la situation. L'émeute généra la tension et la terreur à Macao jusqu'à sa résolution le avec une demande d'excuses humiliantes faites par le gouvernement de Macao à la communauté chinoise. Cette émeute a également causé la renonciation du Portugal à l'occupation perpétuelle de Macao et fit reconnaître la puissance et le contrôle "de facto" des Chinois de Macao aux Portugais, marquant le début de la fin de la période coloniale de la ville.

Après la Révolution des Œillets, en 1974, le Portugal déclara l'indépendance immédiate de toutes ses provinces ultramarines. Mais la Chine rejeta une fois de plus ce transfert immédiat et appela à la mise en place de négociations visant à permettre un transfert sans heurt.

Au cours des négociations, le statut de Macao fut changé pour "Territoire chinois sous administration portugaise" et le transfert de souveraineté de Macao à la Chine fut prévu pour le par la "Déclaration commune sino-portugaise sur la question de Macao". Ce projet bilatéral et international, signé le , établit une série d'engagements et de garanties conclues entre le Portugal et la Chine qui accorde à Macao un haut degré d'autonomie et la conservation de ses caractéristiques uniques, y compris son mode de vie et son système économique capitaliste jusqu'en 2049.

Le territoire redevient finalement chinois le , sans susciter d'émotion au sein du peuple portugais, qui en avait une assez mauvaise image.

Après le transfert, le nouveau gouvernement de Macao, dirigé par Edmund Ho Hau-wah, a lutté avec acharnement et avec succès contre le crime organisé par les Triades avec le précieux soutien de l'administration centrale de la République populaire de Chine. Macao a été remilitarisé par une garnison de troupes chinoises. Ces troupes furent un moyen pour la Chine d'affirmer son autorité sur le territoire, et un soutien pour la lutte contre la criminalité. De 2001 à 2002, la libéralisation partielle du secteur des jeux eut lieu en raison de l'expiration du monopole de ce secteur d'une grande importance pour la société des casinos de Stanley Ho. Cette libéralisation et l'assouplissement des restrictions sur les résidents chinois continentaux par le gouvernement central provoqua le développement du tourisme à Macao et une croissance économique rapide.

Mais, derrière cette croissance, de graves problèmes sociaux apparurent, comme l'inflation, la main-d'œuvre illégale ou un excès d'importations (légales) de main-d'œuvre bon marché et l'écart économique entre les riches et les pauvres. En 2006, le coefficient de Gini à Macao a augmenté de 0,48, et donc l'inégalité de la distribution des revenus fut plus forte que, par exemple, à Singapour, en Corée du Sud ou même en Chine continentale. Ces problèmes, ainsi que la découverte en 2006 de cas de corruption impliquant le secrétaire aux Travaux publics, Ao Man Long et le manque de transparence du gouvernement de la RAS de Macao, ont engendré plusieurs protestations, dont les plus récentes datent du (date de la Fête du Travail) et du lors du de la création de la Région administrative spéciale. Au cours de la dernière manifestation, près de à marchèrent dans les rues pour obtenir un système politique plus démocratique et une plus grande transparence, en exigeant du gouvernement la mise en œuvre intégrale du suffrage universel direct pour les élections de l'Assemblée législative de Macao et du chef de l'exécutif. Les citoyens manifestaient aussi pour une plus grande indépendance des recettes de jeu et l'introduction de mesures visant à réduire l'écart entre les riches et les pauvres. La violence a refait surface lors des manifestations du .

Macao est une petite économie de marché très ouverte et libérale, avec la libre circulation des capitaux, résultat de sa longue histoire en tant que port franc. La monnaie officielle utilisée à Macao est la pataca qui est liée au dollar de Hong Kong. Macao fait partie de l'Organisation mondiale du commerce depuis le .

L'économie de Macao est largement fondée sur le secteur tertiaire, en particulier dans les "jeux de chance et de hasard", avec plusieurs grands casinos situés dans des hôtels très luxueux et entourés de boutiques spécialisées dans les hypothèques, rassemblant ainsi trois activités très lucratives liées les unes aux autres, et le tourisme. D'autres activités importantes sont l'industrie du textile et la production de feux d'artifice, de jouets, de produits électroniques, de fleurs artificielles, de transactions bancaires et de construction civile.

Le PIB de Macao, en 2007, était de 19,1 milliards de dollars. Le PIB par habitant en 2007, était lui de . En 2006, l'économie de la région a subi une augmentation de l'inflation de 5,2 % à une inflation de 5,57 % en 2007. Cependant, un économiste de Macao indique que la valeur réelle de l'inflation est bien au-dessus de 5,57 %, voire supérieure à la valeur de 7,5 %.

Le gouvernement de Macao a toujours réussi à équilibrer ses finances, c'est pourquoi en 2006, le solde budgétaire s'élevait à plus de 50 milliards de patacas.

Mais cet équilibre budgétaire est actuellement très dépendant des taxes perçues par le jeu, objet d'une libéralisation partielle, depuis la fin du monopole de la société des casinos de Stanley Ho (la STDM) qui a pris fin le .

À Macao, l'activité dans ce secteur vital pour l'économie repose sur les subventions de droit administratif. Il existe maintenant trois concessionnaires et trois sous-concessionnaires de jeux de chance et de hasard. Grâce à son grand nombre de casinos, Macao est appelé le "Las Vegas de l'Orient".

En 2005, les sommes de jeu à Macao égalent pour la première fois Las Vegas (chacune d'environ 5,6 milliards de dollars) ce qui fait de Macao le principal centre mondial de l'industrie du jeu.

En 2006, l'Organisation mondiale du tourisme a classé Macao comme étant la touristique mondiale et la en nombre de touristes reçus.

En 2007, le revenu brut du secteur des jeux à Macao a été de près de 83,8 milliards de patacas, c'est-à-dire environ 10,5 milliards de dollars. Macao a une balance commerciale négative (exportations moins importations) d'environ -2,805 milliards de dollars. Plus de 27 millions de touristes ont choisi Macao comme destination de voyage, dont 55,08 % provenant de Chine continentale. Cette croissance est principalement due à la réduction des restrictions de Voyage par le gouvernement central. Après la baisse progressive de ces restrictions, les Chinois ont réussi à obtenir des visas de voyage, pouvant aller librement dans d'autres pays et régions, principalement vers Macao et Hong Kong, ce qui contribua à développer le domaine du tourisme.

En 2009, Fernando Chui Sai On a toutefois manifesté la volonté de diversifier les activités du territoire afin de moins dépendre de l'industrie du jeu.

Macao est une ville très densément peuplée.
Plus de 160 gratte-ciel y ont été construits depuis les années 1980. 
Le statut actuel de Macao (Région administrative spéciale) est défini dans la Déclaration commune sino-portugaise sur la question de Macao et dans la Loi fondamentale de la région administrative spéciale de Macao. La Déclaration commune et la Loi fondamentale précisent que Macao bénéficie d'une forte autonomie territoriale, et que son système économique, financier, social, fiscal, de sécurité et de contrôle de l'immigration et des frontières, le mode de vie, les droits et libertés de ses citoyens, telles que les libertés d'expression, médias, d'édition de la libre sortie et retour à Macao, d'association, de réunion, de cortège et de manifestation, de culte, d'organisation et de participation aux grèves, et, finalement, de ses spécificités, continueront d'être préservés et inchangés au moins jusqu'en 2049, 50 ans après le transfert de souveraineté.

À titre d'exemple de son autonomie, la Région administrative spéciale, peut d'elle-même, établir des relations, des célébrations et des accords avec les pays et les régions et les organisations internationales sous le nom de « Macao, la Chine ». Avec cette désignation, Macao peut également participer, par elle-même, dans les organisations et conférences internationales et aux événements sportifs par exemple.

Elle est administrée par les habitants et non par les officiers de la République populaire de Chine. Elle possède une grande autonomie dans tous les aspects et les questions liées à celle-ci, sauf dans les questions relatives à la défense et dans les affaires étrangères (politique étrangère), et, dans ce dernier domaine, Macao a encore une certaine autonomie. Cette petite région maintient sa propre monnaie (pataca), son propre système d'immigration et de la police des frontières et de sa propre police.

Le pouvoir est divisé, comme dans la plupart des systèmes politiques, en 3 parties distinctes : pouvoir exécutif, législatif et judiciaire. Pour la magistrature, Macao possède une cour de cassation, appelée "tribunal de última instância", conformément aux dispositions de la Loi fondamentale.

Le Chef de l'exécutif, le plus important organe politique à Macao, est toujours occupé par un résident chinois éminent citoyen de Macao. Il est conseillé par le Conseil exécutif, composé de 7 à 11 administrateurs. L'actuel chef de l'exécutif est Fernando Chui Sai On, élu le .

Il est choisi par une « Commission électorale », composée de nommés par les associations ou organisations représentant les intérêts des divers secteurs de la société de Macao, dûment enregistrés et régulièrement interrogés par des membres de l'Assemblée législative de Macao, par les députés de l'Assemblée nationale populaire et par les représentants de Macao, à la Conférence consultative politique du peuple chinois. Après son élection, le chef de l'exécutif n'a pas encore été officiellement désigné, il doit encore être accepté par le gouvernement central chinois. M. Edmund Ho Hau-wah, dirigeant de la communauté, homme d'affaires et ancien banquier, fut le premier chef de l'exécutif de Macao, en remplacement du gouverneur portugais, le général Vasco Joaquim Rocha Vieira (désigné directement par le Portugal) dans les premières heures de la rétrocession.

L'organe législatif de Macao est l'Assemblée législative (AL), composée de élus ou désignés pour quatre ans de différentes façons : 14 sont élus directement selon un mode de scrutin proportionnel plurinominal de liste au plus fort reste dans une unique circonscription, 12 sont élus par des organisations ou associations représentant les intérêts de différents secteurs de la société locale, et 7 sont nommés par le chef de l'exécutif. L'assemblée peut élaborer des lois, examiner et approuver le budget et a le pouvoir de charger le président de la Cour de cassation de former une commission d'enquête indépendante puis de présenter une motion de censure contre le chef de l'exécutif, et de la soumettre au Gouvernement populaire central. Elle a également le pouvoir de modifier le mode d'élection du chef de l'exécutif si les modifications proposées reçoivent l'approbation des deux tiers des députés et du chef de l'exécutif.

Les élections sont organisées tous les quatre ans. Elles sont réservées à tous les citoyens ayant leur résidence permanente à Macao et étant âgés de plus de 18 ans. Le terme de « parti » n'existe pas à Macao, c'est pourquoi, les élections indirectes sont uniquement réservées aux organismes ou aux associations locales représentant les intérêts des divers secteurs de la société, qui ont acquis la personnalité juridique depuis au moins trois ans, et ont été officiellement enregistrées et régulièrement recensées. Ceci signifie que 17 députés sur 29 sont désignés et non pas élus.

Les élections de 2009, les troisièmes dans l'histoire du territoire (les premières s'étaient déroulées en 2001), furent les plus participatives avec de Macao (environ 59,9 % du total des électeurs inscrits) s'étant déplacés pour voter donnant les résultats suivants :

Selon la Loi fondamentale de Macao, la région possède un degré élevé d'autonomie dans la gestion de ses affaires. Le système judiciaire de Macao est autonome et indépendant du gouvernement ou du gouvernement populaire central de la République populaire de Chine, et il a même le pouvoir de jugement en dernier ressort.

Les organes judiciaires de la RAS sont : le "Ministère public"; les "Tribunaux de première instance", qui sont subdivisés en tribunal judiciaire de base, tribunal administratif, tribunal de seconde instance et cour de cassation.

Le système juridique de Macao est basé sur le concept d' "État de droit", à l'indépendance du pouvoir judiciaire, la défense des droits de l'homme et les droits et libertés fondamentales des citoyens. Ce système est essentiellement fondé sur le modèle de la loi portugaise, et donc en partie de la famille des systèmes juridiques continentaux de la racine (romain germanique).

De 1987 à 1999, le système juridique a été complètement modernisé en vue du transfert de la souveraineté de Macao à la République populaire de Chine. Ainsi a été adoptée une série de nouvelles lois et codes, y compris le Code pénal (1995), le Code civil (1999), le Code du commerce (1999), le Code de procédure pénale (1996) et le Code de procédure civile (1999). Après la transition, le système a continué d'être réformé, avec des ajouts tels que l'utilisation du chinois dans les tribunaux et les lois.

Bien que, depuis la création de Macao, ce système ait subi plusieurs changements, des améliorations et des adaptations pour répondre à la Loi fondamentale de Macao et le nouveau statut de Macao en tant que région administrative spéciale, il est maintenu pendant au moins 50 ans à compter de la date du transfert de souveraineté (1999), avec le principe de "un pays, deux systèmes". Pour cette raison, toutes les lois en vigueur avant 1999 ont été maintenues, sauf une petite partie qui entrait en contradiction avec la Loi fondamentale.

D'un point de vue constitutionnel, le système juridique de Macao se caractérise par l'existence d'un texte constitutionnel en conformité avec le droit interne de Macao, la Loi fondamentale de la région administrative spéciale de Macao, promulgué par Assemblée nationale populaire de la République populaire de Chine en 1993. En général, les lois nationales de la Chine ne s'appliquent pas à Macao, à l'exception de celles expressément énoncées à l'annexe III de la Loi fondamentale. Actuellement, elles sont 11 et traitent des questions qui ne figurent pas dans le statut autonome du territoire, telles que la défense nationale et les relations extérieures.

Depuis le début des années 1990, l'éducation juridique relève de l'université de Macao, qui propose des cours de licence et de maîtrise en droit en portugais et en chinois, et certains cours de master en anglais. Cette université offre également des cours de doctorat et Postgraduate également en droit. La formation de magistrats, d'autres officiers de justice ou des professionnels de la justice proviennent principalement du Centre pour la formation juridique et judiciaire.

Le secteur du jeu, qui est une activité fondamentale de Macao, est soumise à une réglementation assez bien développée et un droit du jeu en conséquence. Le droit pénal ne comporte ni peine de mort ni de prison à vie.

Environ 78 pays et territoires sont convenus d'accorder un traitement d'exemption de visa pour les titulaires de passeports macanais au 24 décembre 2008:

La détention d’un visa n’est pas obligatoire pour les ressortissants des pays suivants voulant se rendre à Macao :

Tout voyageur doit être muni d'un passeport ou d'un titre de voyage valable depuis au moins trente jours à partir de la date d'entrée du territoire.

Le passeport de Macao est délivré par la République populaire de Chine aux résidents permanents de nationalité chinoise. Depuis 2009, la « Direcção dos Serviços de Identificação » (Direction des services d'identification) de Macao émet des passeports biométriques.

Depuis plusieurs décennies, les crimes violents étaient un risque sérieux pour le tourisme, car la ville n'arrivait pas à contrôler la criminalité.

Les groupes du crime organisé, connus localement sous le nom de « Triades » ou de « sectes » sont des transformations des organisations politico-révolutionnaires, qui existaient depuis l'époque de la dynastie Qing. Au fil du temps, ces organisations étaient en train de perdre leur identité et, aujourd'hui, sont surtout connues comme des sociétés secrètes, ou en chinois, « "Hák Sé Wui" ». Parmi elles, les plus connues sont les « "14 Kilates" » ("Sap Sei Kei") et les « "gazeuses" » ("Soi Fong").

Les sources de revenu des triades sont : les commissions pour ne pas déstabiliser l'activité des casinos, des boutiques et autres activités commerciales, notamment des prêts à de très hautes commissions pour les joueurs de casinos, « protection » pour les commerçants qui les paient, la drogue et le blanchiment d'argent.
Dans les années 1990, il y a eu un grand nombre d'assassinats, mais de telles choses n'ont pas nui à la vie des personnes civiles.

En , Wan Kuok-koi, le célèbre et redouté chef de la puissante triade « 14 Kilates » a été arrêté. En , débute son historique jugement et en novembre, un mois avant le transfert, il est condamné à 15 ans de prison et à la confiscation de tous ses biens illégaux.

Après le transfert de souveraineté, le nouveau gouvernement de Macao, soutenu par l'administration centrale de la République populaire de Chine, lutta avec succès contre le crime organisé. Une garnison de l'Armée populaire de libération fut mise en place à Macao et fut considérée comme un plus pour lutter contre la criminalité. Le nombre de crimes a été réduit considérablement, en particulier les crimes violents, qui ont diminué de 70 % au cours de l'année 2000, puis de 45 % au cours de l'année 2001. Macao est devenu beaucoup plus sûr, ce qui redonne confiance aux touristes. Cette évolution est également propice à la réanimation de l'économie. Malgré la diminution du nombre de crimes organisés, les triades sont toujours présentes et gardent une influence sur la société.

En 2006, la criminalité, surtout non organisée, est repartie à la hausse, avec plus de crimes contre la vie en société, mais moins de crimes violents.

En 2009, infractions ont été commises sur le territoire, soit une diminution de 11,15 % par rapport à 2008.

Macao est souvent caractérisé comme un point de rencontre de la coexistence harmonieuse et des échanges multiculturels (notamment entre la culture chinoise et celle de l'occident) et, par conséquent, un site où convergent de nombreuses valeurs, de croyances religieuses, coutumes, habitudes, traditions et styles architecturaux, contribuant à l'émergence d'une culture propre et unique de Macao, qui est l'une des spécificités de cette région administrative spéciale. Cela est principalement dû à l'emplacement du territoire de Macao, en territoire chinois et étant une ancienne colonie portugaise.

Le Portugal a envoyé de nombreux missionnaires catholiques à Macao, y compris le plus célèbre occidental de la Chine, Matteo Ricci. Mais l'art, la littérature classique, la médecine et la philosophie chinoise ont atteint l'Europe "via" Macao.

La population de Macao est composée principalement par des Chinois de l'ethnie Han, et aussi par une minorité (environ 1,7 %) de Portugais et de Macanais. Les langues officielles sont le cantonais et le portugais, mais la première langue est le cantonais. Les Macanais, qui ont une ascendance portugaise ou chinoise (et d'autres personnes d'origine asiatique, par exemple, malaise, indienne et sinhala), ont leurs propres culture et leur mode de vie, ainsi que leur propre créole, ou patois macanais. Ce type de créole est basé sur le portugais, et est fortement influencé par le cantonais, par le malais et de nombreuses autres langues. C'est le fruit d'une longue et historique vie commune, de coexistence et d'échanges entre les cultures occidentales et orientales.

Cette réunion harmonieuse des cultures est également montrée, dans le calendrier des jours fériés à Macao, en particulier comme le Nouvel an lunaire, la Journée de Bouddha, Noël et Pâques.

Le gouvernement, en particulier l’"Instituto cultural" et quelquefois l’"Instituto dos Assuntos Cívicos e Municipais", organisent de nombreux spectacles, concerts et activités, évènements récréatifs et manifestations culturelles, en mettant en évidence la compétition "Jeunes musiciens de Macao" (qui se tient en été), l"'Exposition des arts visuels", le "Festival international de la musique" (en octobre) et le "Festival des arts de Macao "(en mars).

Macao possède également de nombreux musées, en particulier le "Musée de Macao" et le "Musée maritime" et un centre culturel ("le Centre culturel de Macao"), d'une superficie de mètres carrés, constituant un lieu approprié pour la tenue d'expositions, de spectacles, d'activités et de manifestations culturelles.

La cuisine de Macao est aussi un mélange de cultures. Si toutes les variations régionales de la cuisine chinoise sont représentées du fait de l'origine variée des habitants de la région, la cuisine cantonaise a bien sûr le plus d'adeptes. Un des plaisirs de cette nourriture est le fameux "dimsum" (), qui est un ensemble de différents petits plats, servi principalement dans les restaurants où l'on va "yam tcha" ().

Lorsque l'on parle de la cuisine de Macao, il faut aussi considérer la cuisine macanaise, comme unique au monde, née lorsque les épouses des explorateurs portugais essayèrent de faire des repas avec les ingrédients locaux (en particulier ceux d'origine chinoise), mais aussi avec plusieurs ingrédients issus de lieux (par exemple Malacca, Inde et Mozambique) visités par les Portugais au cours de leurs découvertes. Les traditions culinaires de certaines de ces épouses, elles-mêmes d'origine orientale, ont influencé certains plats, créant la cuisine de Macao, considérée par beaucoup comme une véritable cuisine fusion. Lacassá de talharins et porc fumé, "Minchi" (viande hachée accompagnée d'un oeuf au plat, de pommes de terre frites et de riz), "Tacho" (ragoût de viande et de légumes), riz au lait, abats de canard, crevettes larges farcies, igname "chau-chau" avec "lap-Yock", poulet rôti, jus de racines de lotus et poulet au curry sont quelques-uns des plats populaires de Macao.

La protection, la valorisation et la préservation des richesses historiques, architecturales et culturelles de Macao sont des priorités majeures du gouvernement de Macao, qui a élaboré plusieurs lois, mesures, directives et des politiques claires et efficaces dans ce domaine, qui constituent une attraction touristique de grande importance.

Des dizaines de bâtiments et de lieux historiques, en raison de leur valeur unique et universelles, ont même été inclus dans la liste du Patrimoine mondial de l'Unesco, le . À partir de ce moment, cet ensemble historique et architectural, fut nommé "Centre historique de Macao".

Liste des jours fériés de Macao :
La première diffusion d'un « film » à Macao eut lieu en 1893 (il ne s'agissait alors que d'images animées) et le premier cinéma, le « Victoria », ouvrit quant à lui ses portes en 1910. 
Ici suit une liste des principaux films se déroulant ou ayant été tournés à Macao:

La musique de Macao est un mélange de musique chinoise et de musique portugaise.

Ce genre de musique hybride a connu son essor au début du , notamment avec des groupes surnommés « Tunas ». Au Portugal, ces groupes étaient composés de jeunes hommes qui se réunissaient dans les universités pour former des groupes estudiantina-like, mais à Macao, ce genre de groupe de musique a pris une orientation différente, il a été mélangé avec les célébrations de salle de bal de Carnaval et fêtes de rue.
Le répertoire se compose de marches de carnaval, de ballades, de valses, de ballades d'inspiration cantonaise, fados, polkas, etc.

Beaucoup de chansons de cette période ont été traduites en patois macanais, comme la chanson brésilienne « Mamãe Eu Quero », qui a donné « Mama Sa Filo », la chanson populaire portugaise « Oh Careca tira a bóina » (en français : Oh le chauve, enlève ton béret) qui a donné « Sium Careca », la chanson de fado « Uma Casa Portuguesa », renommée en « Unga Casa Macaísta » ou encore les chansons « Bastiana » et « Assi Sa Macau ». Cette dernière est une composition de l'adepte du patois macanais sur le territoire, le poète, compositeur et metteur en scène José dos Santos Ferreira, surnommé « Adé ».

Le groupe macanais « Tuna Macaense » a été créé en 1935 et existe toujours de nos jours. Il se compose de huit membres (António Lopes - mandoline, guitare, chant et percussions ; Rui Coelho - mandoline, basse, guitare solo et chant ; Filomena Jorge - basse, guitare rythme, guitare solo et chant ; Carlos Pereira - batterie et percussions ; Judas Manhão Jorge - guitare solo ; Pedro Santos - basse et percussions ; Isabel Carvalhal - chant et percussions ; Un Yiu - clavier).

Les institutions musicales de la ville sont le "Macao Orchestra" et le "Macao Conservatoire", qui a joué un rôle important dans l'éducation musicale dans la région depuis sa fondation en 1991. Le « Festival international de musique de Macao » est un événement annuel important.

L'un des plus importants musiciens chinois, Xian Xinghai, est né à Macao.

La musique pop à Macao en est encore à ses débuts. Toutefois, le boys band Soler est allé à Hong Kong développer sa carrière musicale.

Christophe Masson, ""L'Empereur de Macao"", 2016, (Éditions Revoir) 

En tant que point de rencontre et d'échange entre l'Occident et l'Extrême-Orient, Macao est doté d'une grande diversité de religions comme le bouddhisme, le taoïsme, le catholicisme, le protestantisme, l'islam et le bahaïsme.

La principale et la plus pratiquée dans cette région administrative spéciale est le bouddhisme. Mais beaucoup des habitants considèrent le bouddhisme comme une conception générique, en incorporant plusieurs éléments dans cette religion des valeurs du confucianisme, du taoïsme, de la mythologie chinoise et d'autres coutumes, des philosophies, des croyances et des pratiques traditionnelles chinoises. Une de ces pratiques correspond aux cultes ancestraux, considérés par les chinois comme très importants et faisant partie intégrante de la tradition et de la culture chinoise. Cet ensemble syncrétique de croyances, de valeurs religieuses et de pratiques adoptées par les Chinois est communément appelé "religions populaires chinoises" ou "croyances populaires chinoises" ou encore "croyances traditionnelles chinoises".

Il existe également à Macao une forte communauté de chrétiens, la majorité étant membres de l'Église catholique romaine qui est hiérarchiquement structurée et organisée dans le diocèse de Macao. Cette circonscription catholique, créée le par l'édit du pape , ne gère plus actuellement que le territoire de Macao, et est directement dépendante du Saint-Siège. Depuis 2003, l'évêque de ce diocèse est D. José Lai Hung-Seng.

Il existe en plus de l'Église catholique, une communauté significative de protestants qui comptait en 2006, près de et environ protestantes contrastant avec les existantes dans les années 1950. L'introduction du protestantisme a Macao a débuté avec l'arrivée de Robert Morrison au cours de l'année 1807, pour se dédier à la traduction de la Bible en chinois et une compilation du dictionnaire anglais-chinois, ce qui en fait le premier missionnaire protestant à Macao.

Conformément à l'article 34 de la Loi fondamentale de la région administrative spéciale de Macao, « les résidents de Macao bénéficient de liberté de croyance religieuse et de la liberté de prêcher, de promouvoir les activités religieuses en public et d'y participer ». Et, conformément à l'article 128, « le gouvernement de la Région administrative spéciale de Macao n'interfère pas dans les affaires internes des organisations religieuses, ou dans le maintien et le développement des relations des organisations religieuses et des croyants avec les organisations religieuses et les croyants de l'extérieur de la région de Macao. Il n'impose aucune restriction sur les activités religieuses qui ne contreviennent pas aux lois de la Région administrative spéciale de Macao. À Macao, toutes les religions sont égales devant la loi et conformément à la "loi 5/98/M", les relations entre son gouvernement et les groupes religieux se fondent « sur les principes de séparation et de neutralité ».

Selon les statistiques gouvernementales, Macao comptait en 2009, (en moyenne pour ), (moyenne de 2,8 infirmiers pour ) et d'hôpitaux disponibles (soit près de 2,0 lits pour ). Bien que la moyenne pour le nombre de médecins et d'infirmières par tête soit au-dessus de la moyenne mondiale (mais encore loin de la moyenne européenne), le nombre moyen de lits d'hôpitaux par tête est en revanche bien en dessous de la moyenne mondiale, qui est d'environ pour .

En 2005, le gouvernement de Macao a dépensé de patacas, avec une augmentation de 20 % par rapport à l'année précédente (environ de patacas) dans le système de santé. Mais en dépit de tous ces investissements le système de santé de Macao, en plus du manque de lits d'hôpitaux, est également confronté à plusieurs problèmes tels que la lenteur ou le retard des soins pour les patients et le manque de personnel qualifié dans certains départements et zones spécifiques de médecine.

Le gouvernement offre divers types de services de santé, notamment les premiers, qui sont actuellement fournis par les 7 « centres de santé », les de santé publique et surtout le centre hospitalier "Conde de São Januário", l'unique hôpital public de Macao. Ce centre hospitalier, fondé en 1874, est actuellement composé de liés et un héliport et fournit des services médicaux gratuits pour tous les résidents de Macao. Plus concrètement, en 2005, il a administré des consultations externes à près de , les urgences à plus de , et l'internement de plus de . En 2005, il était composé de , et , 476 appartenant aux services d'hébergement.

Outre le secteur public, il existe également de nombreuses entités privées telles que l'hôpital "Kiang Wu", "le dispensaire du commerce", "l'Association des cliniques de bienfaisance Tung Sin Tong" ainsi que des cliniques privées, qui fournissent des services de santé. Plusieurs de ces entités, comme l'hôpital Kiang Wu, reçoivent le soutien financier du gouvernement et des associations locales. L'hôpital privé a été fondé par les Chinois en 1871, et continue d'être administré par des Chinois, il est dépendant de l'Association de bienfaisance de l'hôpital de Kiang Wu. En 2005, il était composé de , , et à d'autres fonctions. L'hôpital dispose également d'un centre médical à Taipa, construit en octobre 2005 qui sert principalement les habitants de Taipa.

À Macao, les principales causes de décès sont, en 2008, les tumeurs (31 %), les maladies cardiaques (27,6 %) et les maladies respiratoires (13,7 %).

En 2005, les dépenses du gouvernement de Macao pour les services sociaux s'élevaient à plus de de patacas, en augmentation de 38,88 % par rapport à l'année précédente. La même année, le gouvernement voulant verser une aide spéciale aux familles monoparentales, aux handicapés et aux malades chroniques, a réuni plus de de patacas et, avec l'intention d'aider les résidents âgés de plus de , créa une subvention pour les personnes âgées, qui octroie chaque année patacas par an à cette tranche d'âge. Ces subventions annuelles, même critiquées par de nombreux habitants de Macao pour cette petite somme délivrée chaque année aux personnes âgées, a coûté au gouvernement près de de patacas rien qu'en 2005.

En 1989, Le gouvernement de Macao a créé un régime contributif de sécurité sociale pour protéger les salariés. Ce régime est placé sous la responsabilité du "Fonds de sécurité Sociale" (FSS), créé le . Cette institution, placée sous la tutelle du Secrétariat à l'économie et aux finances, possède une autonomie administrative et financière, ses principales sources de revenus provenant des bénéfices des investissements privés et en particulier les contributions des employeurs, des salariés, du gouvernement (1 % de ses recettes courantes) et du secteur du jeu (un certain pourcentage de ses revenus bruts).

Le Fonds de sécurité sociale octroie aux salariés contribuant plusieurs types de prestations, tels que la pension de vieillesse, la pension d'invalidité, la pension sociale, les allocations chômage, l'allocation maladie, l'allocation de naissance, l'allocation de mariage, l'allocation de funérailles et des crédits émergeant des relations industrielles. En 2005, toutes prestations confondues, le FSS a versé environ de patacas.

Conformément à la loi, tous les employeurs ont la responsabilité d'inscrire et de payer leurs contributions, afin que leurs salariés puissent profiter des avantages offerts par le FSS. Les salariés avec un paiement volontaire des contributions et les travailleurs indépendants bénéficient d'un autre régime de cotisations.

En 2005, plus de contribuant travaillaient pour le compte d'un autre (y compris les salariés occasionnels et les fonctionnaires), plus de payaient volontairement leurs cotisations et plus de étaient indépendants.

Macao est couvert par un réseau routier, dont la longueur totale atteint en 2004. Cette région possède également un système relativement efficace de transports publics, couvrant une grande partie du territoire, en dépit de nombreux problèmes graves et urgents à résoudre, tels que le niveau élevé de saturation du système et le manque préoccupant de conducteurs de bus qui, par conséquent, a provoqué une diminution du nombre de bus en circulation dans cette région. Les deux moyens prédominant de transports publics sont les bus et les taxis.

Actuellement, les "Transports urbains de Macao SARL" (Transmac) et la "Société des transports collectifs de Macao" (STCM) sont les deux seules sociétés qui sont autorisées à exploiter les services de bus à Macao. Pour les taxis, on comptait, fin 2005, environ 760 véhicules ce qui reste relativement faible.

Fin 2010, il y avait à Macao plus de et motos et plus de particulières en circulation ayant été enregistrées et plus de neuves (montrant une augmentation de 1 % par rapport à 2009). La circulation routière est aussi le fait de véhicules provenant de Chine. En 2010, ceux-ci augmentèrent de 9 % par rapport à 2009, se montant à . Pour le stationnement des véhicules, qui est un problème qu'il convient de résoudre, il existait à Macao, en 2005, en concession à usage public, qui ont une capacité totale de , , ou camions et , et près de 2931 parcmètres. Tout ceci contribue à l'aggravation des problèmes relativement urgents liés au trafic à Macao.

La péninsule de Macao est reliée à l'île de Taipa par trois ponts (le "pont de l'amitié", le " pont du gouverneur Nobre de Carvalho", et le "pont Sai Van"). Les îles de Taipa et de Coloane, sont reliées par la « route de l'isthme », construite sur l'isthme de Cotai. Cotai à son tour, est reliée à l'île chinoise de Hengqin par un pont-route (le "pont Fleur de Lotus") et recevra une future liaison ferroviaire.

Pour établir des liens avec le monde extérieur, Macao possède un aéroport international, un héliport et divers ports, qui servent à l'embarquement ou le débarquement des passagers (notamment le "Terminal maritime du port extérieur" également appelé "Terminal maritime et héliport de Macao") ou pour le chargement et le déchargement de marchandises et de carburant. Le transport maritime et par hélicoptère contribuent de façon significative à l'établissement de liens entre Hong Kong, Shenzhen, Zhuhai et d'autres villes de la région de l'embouchure de la rivière des Perles.

Au contraire de Hong Kong, Macao n'a pas développé de réseau de télécommunications local particulièrement compétitif. Pour cette raison, ce réseau fait face à une forte concurrence avec d'autres réseaux régionaux, notamment ceux de Hong Kong et de Chine continentale, qui sont, dans la plupart des cas, de meilleure qualité. Ainsi, de nombreux résidents de Macao choisissent souvent, au détriment ou en complémentarité avec le réseau local, les dizaines de journaux et de magazines publiés et les différentes chaînes et programmes de radio et de télévision émis à Hong Kong et en Chine.

Pour l'audiovisuel, il existe à Macao :

La presse écrite est apparue au cours du à Macao, mais ce n'est qu'à partir des années 1930, que les journaux de Macao ont commencé à se développer.
Actuellement, plus de de différents journaux sont publiés quotidiennement à Macao. Il y a neuf journaux (quotidiens) chinois, le plus ancien étant le "Tai Chung Pou" (fondé en juillet 1933) et le plus récent le "Hou Kong Iat Pou" (fondé en 2008). Le plus grand journal de Macao est le "Ou Mun Iat Pou", fondé le , qui exerce une grande influence, représentant environ 80 % du tirage total de tous les journaux de Macao. Il est détenu par une société privée qui entretient des liens étroits avec le parti communiste chinois. Le "Va Kio Pou", fondé le , est le deuxième plus grand journal chinois à Macao.

Parmi les sept hebdomadaires chinois existants, le plus ancien reste le "Jornal Si-Si", fondé en 1972. Tous, sauf le "Semanário Desportivo de Macau", qui est un journal sportif, traitent principalement des questions d'intérêt général de la population locale.

De plus en plus de personnes critiquent la presse chinoise, car elle est pro-chinoise et pro-gouvernementale et parce qu'elle pratique l'autocensure en évitant certaines questions sensibles, craignant des représailles de la Chine.

Pour la presse en portugais, il existe trois quotidiens : le "Jornal Tribuna de Macau", le "Hoje Macau" et le "Ponto Final"; et un hebdomadaire : "O Clarim".

Quant à la presse en anglais, il y a deux quotidiens : "The Macau Post Daily" et le "Macau Daily Times"; et deux mensuels : "Macau Business" et "Macau Closer".

Il y a également plusieurs types de magazines, comme la "Revista MACAU".

Ici suit la liste des journaux diffusés à Macao :


De 1927 à 1981, les services de télécommunication étaient assurés par le service public : « Correios, Telégrafos e Telefones ».

Depuis 1981, les réseaux de télécommunications fixes et les services de télécommunication avec l'extérieur sont sous la concession exclusive de la "Companhia de Telecomunicações de Macau (CTM)", et ce jusqu’en décembre 2011. Il y a plus de de téléphone fixe en 2008.

La CTM a initié les services de téléphonie mobile numérique en 1995, et à la fin 2008, plus de ont utilisé les services de télécommunications mobiles (dont ) offerts par les quatre entreprises opérant à Macao ("Hutchison - Telefone (Macau)", "SmarTone - Comunicações Moveis" et "Companhia de Telecomunicações de Macau" depuis 2001 à la norme GSM, et "Companhia de China Unicom (Macau)" depuis 2006 à la norme CDMA), ce qui représente un taux de pénétration du marché de la téléphonie mobile de 167 %.

L'indicatif téléphonique de Macao est le 853.

En 1995 les services d'Internet furent lancés à Macao et en 2000, le service d’accès à Internet à haut débit, fut lancé par la "CTM". En 2001, le gouvernement de Macao a commencé à délivrer des licences de fournisseur de services Internet à plusieurs opérateurs et, en 2005, accorda des licences définitives à (dont 4 fournisseurs d'accès). En 2002, le règlement d'administration relatif à la fourniture de ces services a finalement été publié.

En 2007, plus de utilisaient Internet, soit environ 22 % de la population de Macao, dont environ étaient abonnés en "haut débit". Le domaine Internet de Macao est «.mo ».

En 2006, le taux d'alphabétisation de la population résidente âgée de 15 ans ou plus était seulement de 93,5 %, avec toutefois une hausse d'environ 2,2 % par rapport à 2001. Mais ce nombre est dû au fait que le taux d'alphabétisation de la population âgée de plus de 65 ans est seulement de 60,1 %. Pour la population âgée de 15 à 19 ans, le taux d'alphabétisation était de 99,7 %, ce qui est beaucoup plus encourageant.

Cependant, même ainsi, Macao reste en dessous de la moyenne des pays riches et développés. En 2006 seul 16,4 % de la population active a achevé l'enseignement supérieur. C'est inquiétant pour Macao, qui se développe à un rythme rapide et qui a besoin de main-d'œuvre qualifiée et spécialisée.

À Macao, l'enseignement obligatoire est appliqué sous une forme universelle et obligatoire à tous les enfants entre 5 et 15 ans. La scolarité gratuite, qui est plus répandue, englobe l'enseignement infantile ou pré-scolaire, dont l'accès est autorisé lorsque l'enfant atteint l'âge de 3 ans, pour une durée de 3 ans; l'enseignement primaire, dont l'accès est autorisé lorsque l'enfant est âgé de 6 ans et l'âge maximum de fréquentation est de 15 ans, pour une durée de 6 ans; le secondaire général, dont l'âge limite de fréquentation est de 18 ans, pour une durée de 3 ans; l'enseignement secondaire supplémentaire, dont l'âge maximum de fréquentation est de 21 ans, a une durée de 3 ans.

Macao ne dispose pas d'un système éducatif propre et universel, c'est pourquoi l'on utilise le système éducatif britannique, chinois ou portugais. Les langues chinoises (le cantonais et le Mandarin) et anglaises sont enseignées dans la quasi-totalité des écoles locales, alors que le portugais est délaissé depuis le transfert de souveraineté, à l'exception évidente de l'École portugaise de Macao qui est la seule école à offrir des programmes similaires à ceux du Portugal.

Au cours de l'année 2005/2006, il existait à Macao 86 écoles (13 sont publiques ou officielles, 60 sont privées mais gratuites et 13 sont privées et payantes), qui composaient l'enseignement non-supérieur, avec plus de et professeurs.

Pour l'année scolaire 2004/2005, il existait à Macao d'enseignement supérieur, et . Ces institutions, (la plus ancienne étant l'université de Macao) avec environ et , offrent un total de de différents diplômes : baccalauréat, licence, maîtrise et doctorat.

Pour promouvoir le sport, le gouvernement de Macao organise de nombreuses activités et événements, telles que "la Journée de sport en famille", la "Journée du sport pour tous", le "Festival sportif des entités publiques", le "Festival sportif des femmes de Macao" et la "Journée internationale du défi".

Macao possède de nombreux terrains, pavillons, centres et installations sportives de grande qualité, notamment le "complexe olympique de Macao", la "piscine olympique de Macao" et le "complexe sportif de Macao", presque toujours ouverts au public.

Cette région administrative spéciale dispose d'un réseau de chemins de randonnée (qui se trouvent sur les îles de Taipa et Coloane), piscines et plages publiques, et un nombre significatif de parcs et jardins publics grand public de parcs et de jardins (compte tenu de la petite région de Macao), offrant au public un lieu de pratiques physiques ou matinale ou simplement par pur plaisir.

Le hockey sur gazon et la pelote basque ont été des sports qui ont eu de l'importance à Macao, mais qui ont disparu au fil du temps. Le rink hockey, très influencé par la communauté portugaise, l'emporte encore à Macao comme une bonne alternative au sport pour de nombreux jeunes, en plus du tennis de table, badminton, basket-ball et volley-ball.

Les activités liées aux arts martiaux tels que le kung-fu, le karaté, le judo et le taekwondo sont très populaires et possèdent de nombreux fans. La plupart de la population âgée, principalement chinoise, opte pour le tai chi comme sport d'entretien.

Macao n’a pas de comité national olympique mais a un comité national paralympique et a envoyé ses premiers athlètes aux Jeux paralympiques d'été de 1988 à Séoul. Macao accueille de nombreuses compétitions sportives au niveau régional et international. Le Grand Prix de Macao de Formule 3 fait partie du calendrier annuel mais Macao a aussi organisé les Jeux d'Asie orientaux en 2005, les Jeux de la lusophonie en 2006 et les Jeux asiatiques d'intérieur en 2007.

Le football est le sport le plus pratiqué du territoire et celui qui possède le plus d'amateurs. Étant donné qu'il y a peu de stades de taille officielle pour (deux équipes) pour répondre aux besoins du grand nombre de joueurs de football local, le football se joue à sept à Macao, il est appelé "Bolinha" et est pratiqué dans les stades de petites dimensions comme celui du collège D. Bosco. Malgré cela, Macao possède sa propre équipe de football et sa propre fédération de football fondée en 1939 et affiliée à la FIFA depuis 1978 et de l'AFC depuis 1976.

Les clubs de football de petites dimensions, comme celui de l"'Infirmerie", constitué d'athlètes chinois, macanais et portugais, est un exemple d'union inter-communautaire que le sport illustrant l'esprit de tolérance de Macao. Le football en salle "(futsal)" est également très pratiqué par les jeunes de Macao.

Une des attractions de Macao est le Grand Prix, une série de spectaculaires courses de voitures et de motos dont celle comptant pour le championnat du monde des voitures de tourisme et la Formule 3.

Tous les mois de novembre, depuis 1954, les meilleurs pilotes au monde sont invités à participer dans l'un des circuits les plus dangereux du monde. Il s'agit d'un circuit qui traverse la ville, entrecoupé de longues lignes droites (Port extérieur) avec les courbes sinueuses de la montagne de la Guia. La courbe de l'hôtel Lisboa est le lieu où la plupart des accidents sont enregistrés. Le circuit n'accueille toutefois la Formule 3 que depuis 1983.

Les plus célèbres pilotes qui ont réussi sur ce circuit sont: Vern Schuppan, Riccardo Patrese, Geoff Lees, Roberto Moreno, John Nielsen, Ayrton Senna, Michael Schumacher, Mika Häkkinen, Rubens Barrichello, David Coulthard et Ralf Schumacher. Beaucoup ont utilisé le circuit de Macao comme un tremplin pour la Formule 1. Le pilote chinois Michael Kwan, dans l'épreuve de "Super Cars" est celui qui a remporté le plus souvent le Grand Prix.

Toutefois, la difficulté du circuit et l'absence d'échappatoires ont provoqué des morts depuis la création du concours en particulier dans les motocycles, comme le Français Bruno Bonhuil.

Pendant les deux jours d’entraînement, et en particulier pendant les deux jours de compétition, un nombre important de résidents de Macao et de touristes assistent à ces courses spectaculaires.

En 2003, on a célébré la du Grand Prix, avec de nombreuses célébrations et festivités, dont une démonstration de Formule 1.

Depuis quelques années, Macao organise une exhibition de tennis, avec de grand spécialistes en la matière, on a ainsi pu voir jouer en 2007 Roger Federer et Pete Sampras, en 2008 Roger Federer, Björn Borg, John McEnroe et James Blake et en 2009 Pete Sampras et Andre Agassi sur un central de , à titre de comparaison le court central de Roland-Garros fait .

L'événement a eu lieu à Macao le pour la dernière étape du relais de la flamme olympique. C'était la première fois que la flamme olympique voyageait à Macao. Une cérémonie a eu lieu aux docks des pêcheurs. Ensuite, la torche a voyagé à travers Macao, en passant par un certain nombre de repères comprenant le temple d'A-Ma, la tour Macau, le pont du gouverneur Nobre de Carvalho, le pont de Sai Van, le centre culturel de Macao, le stade de Macao, puis retour aux docks pour la clôture de la cérémonie. Certaines parties de la route, près des ruines de Saint-Paul et de Taipa ont été raccourcies à cause de la foule de supporters qui bloquaient les rues étroites. Un total de 120 relayeurs ont participé à cet événement, y compris le propriétaire de casinos Stanley Ho. Leong Hong Man et Leong Heng Teng ont été respectivement les premiers et les derniers relayeurs. Un article publié sur le "Macao Daily News" critique le fait que la liste des relayeurs ne pouvait pas représenter pleinement les Macanais et qu'il y avait trop de non-athlètes parmi les relayeurs (dont certains avaient déjà été relayeurs d'autres événements sportifs).

Un syndicat avait prévu de protester contre le relais pour de meilleures conditions de vie. Le législateur hongkongais Michael Mak Kwok-Fung et le militant Chan Cheong, tous deux membres de la Ligue des sociaux-démocrates de Hong Kong, n'ont pas été autorisés à entrer à Macao.

Un résident de Macao, a été arrêté le 26 avril pour avoir posté un message sur cyberctm.com qui encourageait la population à perturber le relais. Les sites Internet de forum orchidbbs.com et cyberctm.com ont été temporairement fermés du 2 au 4 mai. Des spéculations affirment que les fermetures ont été des ciblages des discours contre le relais. Le chef du Bureau de régulation des télécommunications a nié le fait que la fermeture des sites Internet ait un but politique. Près de 2200 policiers ont été déployés dans les rues, il n'y a pas eu d'interruption du relais.

Les premiers timbres de Macao sont émis le mars 1884, utilisant les figures de la « couronne portugaise » et allant de 5 à 300 réaux. Les pénuries de valeurs ont perduré jusqu'en 1887, résultant en une variété de surcharges à la fois sur les timbres-poste et les timbres fiscaux.

En janvier 1888, de nouveaux timbres furent émis à l'effigie de avec un profil en relief. Louis mourut peu après, et le une nouvelle série de 12 valeurs fut émise avec le portrait de . Toujours en 1894, la monnaie changea pour les avos et la roupie, faisant une roupie. Cela changera à pour une pataca en 1913 et en conséquence, les timbres ont été surchargés de valeurs en avos différentes, en alphabet latin et, pour la première fois, en caractères chinois, ainsi que du mot « PROVISORIO ».

Dans le sillage de l’instauration de la république en 1910 au Portugal, le gouvernement local surchargea ses stocks de timbres à l'effigie de Charles avec le mot « REPUBLICA ». La série Cérès du 17 novembre 1913 (Macao) a été un nouveau départ pour tous les territoires portugais, totalisant 29 combinaisons de couleurs et de valeurs jusqu'en 1924. En 1948, une nouvelle série d'usage courant comprenait 12 valeurs avec différents paysages et bâtiments locaux.

Le janvier 1976, le statut de Macao fut changé pour "Territoire chinois sous administration portugaise", lui donnant plus d'autonomie dans sa gestion de la poste. La mention « REPÚBLICA PORTUGUESA » disparut, ne laissant plus que « MACAU ».

Depuis le 20 décembre 1999, Macao conserve son indépendance philatélique et postale au sein de la République populaire. La mention bilingue sur les timbres est « MACAU, CHINA » et « 中國澳門 ».

Macao a pour codes :


Macao a développé des associations de jumelage avec :

Macao a également conclu des pactes d'amitié et de coopération avec plusieurs villes :





</doc>
<doc id="14256" url="https://fr.wikipedia.org/wiki?curid=14256" title="Amhrán na bhFiann">
Amhrán na bhFiann

Bien que d'habitude chantée en irlandais, la version originale de cette chanson composée en 1907 est en anglais, sous son titre "A Soldier's Song" ("La chanson d'un soldat"), aussi bien que sous l'appellation "Hymne national de l'Irlande" (""). 

Les paroles en anglais ont été écrites par Peadar Kearney et la musique a été composée par Patrick Heeney en 1907. Elle a été publiée dans le journal ("Liberté irlandaise") en 1912. La traduction en irlandais a été le travail de Liam Ó Rinn.

Ce chant a été adopté immédiatement par les Irish Volunteers (milice irlandaise nationaliste) et le refrain a été déclaré hymne national officiel en 1926, en remplacement du "God Save Ireland".

L’"Ireland's Call" est joué lors des matchs de rugby internationaux ; cet hymne est joué lors des matchs qui réunissent les quatre provinces d'Irlande, y compris l'Irlande du Nord sous législation britannique. "" est joué uniquement lorsque l'équipe d'Irlande joue à Dublin.



</doc>
<doc id="14258" url="https://fr.wikipedia.org/wiki?curid=14258" title="Joule">
Joule

Le joule (symbole : J) est une unité dérivée du Système international (SI) pour quantifier l'énergie, le travail et la quantité de chaleur. Le joule étant une très petite quantité d'énergie par rapport à celles mises en jeu dans certains domaines, on utilise plutôt les kilojoules (kJ) ou les calories en nutrition et dans les tableaux de valeur nutritive, et le kilowatt-heure pour mesurer l'énergie électrique ou thermique.

Un kilojoule vaut , et une calorie vaut 180/43 = .
Un watt-heure vaut joules, et un kilowatt-heure vaut kilojoules.

L'unité doit son nom au physicien anglais James Prescott Joule.

On définit cette unité comme étant le travail d'une force motrice d'un newton dont le point d'application se déplace d'un mètre dans la direction de la force :

L'expression du joule en unités de base du Système international est donc le kilogramme mètre carré par seconde au carré. Il est facile de retrouver ce résultat à partir de la formule de l'énergie cinétique , étant exprimé en joules (J), en kilogrammes (kg) et en mètres par seconde (m⋅s).

Bien que le joule soit homogène au newton-mètre, cette dernière appellation est réservée au moment d'une force afin que l'unité rende compte de la façon dont cette grandeur est définie.

Le concept d'énergie étant utilisé dans de nombreux domaines scientifiques, un certain nombre de définitions pratiques du joule sont possibles. Par exemple, un joule est le travail fourni par un circuit électrique pour faire circuler un courant d'un ampère à travers une résistance d'un ohm pendant une seconde :
Un joule est aussi l'énergie fournie par une puissance de 1 watt pendant une seconde.

D'autres unités d'énergie sont exprimables en joules :

Un joule vaut exactement :

Un joule est approximativement égal à :

Le térajoule (symbole : TJ), le pétajoule (PJ) et l'exajoule (EJ) sont souvent utilisés dans les brochures relatives aux consommations énergétiques nationales ou mondiales. Le préfixe téra correspond à 10 (mille milliards) :

le préfixe péta correspond à 10 (un million de milliards) :

et le préfixe exa correspond à 10 (un milliard de milliards) :

Dans la vie de tous les jours et approximativement :






</doc>
<doc id="14261" url="https://fr.wikipedia.org/wiki?curid=14261" title="Watt">
Watt

Le watt, de symbole W, est l'unité internationale de puissance ou de flux énergétique (dont le flux thermique). Un watt équivaut à un joule par seconde.

Le nom "watt" rend hommage à l'ingénieur écossais James Watt (1736-1819), qui a contribué au développement de la machine à vapeur. Comme tous les noms d'unités du Système international, s'écrit en minuscules ; en revanche, comme ce nom provient d'un nom propre de personne, le symbole associé W s'écrit avec une majuscule.

Le watt est la puissance d'un système dans lequel une énergie d'un joule est transférée uniformément pendant une seconde. Il est donc égal à un joule par seconde. Comme le joule est le produit d'un newton par un mètre, et le newton celui d'un kilogramme par un mètre par seconde carrée, le watt est égal à un newton mètre par seconde ou encore un kilogramme mètre carré par seconde au cube : 


De même que la notion d'accélération mesurée en m/s ne doit pas être confondue avec celle de vitesse mesurée en m/s, la notion de "puissance" (mesurée en watts) ne doit pas être confondue avec celle d"'énergie" (mesurée en joules). La puissance est le quotient d'une énergie par un temps :

Ainsi, si un travail de est produit, il s'effectuera, par exemple :

Le watt peut être considéré comme une petite unité :

En mécanique, le watt est la puissance développée par une force de se déplaçant sur une distance de pendant . Si le point d'application d'une force de se déplace à la vitesse de , la puissance vaut :
avec :

En électricité, le watt est l'unité de puissance d'un système débitant ou absorbant une intensité de sous une tension de . La puissance instantanée est le produit de la tension par l'intensité :
avec :

En intégrant cette puissance sur un temps de , on obtient une quantité d'énergie dont l'unité pratique courante est le kilowatt-heure équivalent à .

Pour les courants alternatifs on définit aussi la puissance moyenne et la puissance efficace. Elles s'expriment également en watts.

Les termes techniques (We) et (Wt ou Wth) correspondent à la puissance produite sous forme électrique et sous forme thermique, respectivement. Leurs multiples sont le "mégawatt électrique" (MWe) et le "gigawatt électrique" (GWe), et le "mégawatt thermique" (MWt ou MWth) et le "gigawatt thermique" (GWt ou GWth).

Cette précision est utilisée couramment pour distinguer la production électrique de la dissipation thermique d'une centrale. La puissance d'une centrale est généralement exprimée sous forme de puissance électrique. La puissance thermique d'une centrale nucléaire est typiquement trois fois sa puissance électrique. La différence correspondant au rendement thermodynamique (directement lié à la température de fonctionnement) et aux pertes de conversion, étant donné que la transformation d'énergie thermique en énergie électrique ne peut se faire qu'avec des pertes (rendement de l'ordre de ). Elle explique le besoin de refroidissement important des centrales thermiques. Par exemple, la centrale nucléaire d'Embalse en Argentine génère de chaleur () pour seulement d'électricité ().

Cet usage (de symboles munis d'indices) n'est pas recommandé par le Bureau international des poids et mesures (BIPM), qui considère qu'il n'y a qu'un seul watt : c'est la quantité mesurée qui change, pas l'unité utilisée pour la mesure. De façon générale, .

Le watt peut avoir deux significations en matière de caractérisation de la lumière :


</doc>
<doc id="14262" url="https://fr.wikipedia.org/wiki?curid=14262" title="Pascal (unité)">
Pascal (unité)

Le pascal, de symbole Pa, est l'unité de pression ou de contrainte du Système international d'unités (SI).

Il tient son nom de Blaise Pascal et, conformément aux règles du SI, le nom commence par une minuscule (« pascal »), mais comme il provient d'un nom propre, le symbole commence par une .

Le pascal est une unité dérivée du Système international, qui s’exprime en unités de base du Système international de la façon suivante :

Une pression d’un pascal est une contrainte uniforme qui, agissant sur une surface plane de , exerce perpendiculairement à cette surface une force totale de .

La correspondance entre le pascal et les unités de pression n'appartenant pas au SI passe par la valeur standard de la pression atmosphérique :

En Europe, le pascal est préconisé pour les mesures de pression et .

Le pascal étant une unité relativement petite par rapport aux valeurs usuelles, on utilise souvent ses multiples :

Les conversions vers des unités hors du SI :
Soit : = = ≈ (mètres de colonne d'eau) ≈ .



</doc>
<doc id="14263" url="https://fr.wikipedia.org/wiki?curid=14263" title="Volt">
Volt

Le volt (symbole : V) est une unité, de force électromotrice et de différence de potentiel (ou tension) et dérivée du SI.

Ce nom a été donné en hommage à Alessandro Volta, inventeur italien de la pile voltaïque en 1800.

Il correspond à la différence de potentiel électrique qui existe entre deux points d'un circuit parcouru par un courant constant de lorsque la puissance dissipée entre ces deux points est égale à .

Autrement dit, le volt est la différence de potentiel électrique qui accélère une charge électrique de en lui donnant une énergie de . Il en résulte que le volt est ce qui déplace une charge de avec une force de sur une longueur de . 

Il peut être défini à partir des unités de base du Système international, dont il est une des unités dérivées :

avec :

Selon la norme du Système international d'unités, tous les noms d'unités sont des noms communs et s'écrivent donc en minuscules ; le symbole associé est en minuscules, sauf quand le nom de l'unité provient d'un nom de personne, auquel cas le symbole commence par une majuscule (d'où : « volt », nom commun écrit en minuscules ; mais comme ce nom dérive de celui d'Alessandro Volta, le symbole est « V » majuscule).

En 1800, Alessandro Volta développe ce qu'on appelle la pile voltaïque, un précurseur de la pile alcaline, qui produit une tension électrique continue. Volta a déterminé que la meilleure paire de différents métaux pour produire de l'électricité est une paire de zinc et d'argent. Dans les années 1880, le Congrès international d'électricité, actuellement la Commission électrotechnique internationale (CEI), a approuvé le volt comme unité de force électromotrice. Dans le même temps, la tension fut définie comme .

Dans un circuit, la tension s'explique par la différence de potentiel entre les deux pôles électriques, c'est-à-dire par un surplus de charges négatives. Cette différence crée un champ électrique qui entraîne le déplacement des charges jusqu'à ce que celles-ci se soient équilibrées des deux côtés.

Le volt est l'unité de différence de potentiel et de tension.

L'analogie avec l'hydraulique est souvent utilisée pour expliquer la tension et l'intensité dans les circuits électriques ; ils sont comparés à des tuyaux remplis d'eau. Pour avoir une meilleure image de ce qu'est la pression de l'eau, on peut imaginer de l'eau sortant d'un robinet, d'un tuyau d'arrosage ou une chute d'eau causée par un barrage : plus la pression est élevée et plus l'eau qui sort du tuyau a un débit élevé. 

La pression correspond d'après cette analogie à la tension électrique. 

Les tensions nominales de différentes sources :

Dans tous les réseaux électriques, du fait de la résistance de chaque matériau qui les composent, en particulier des conducteurs, il existe une chute de tension proportionnelle à la longueur des câbles. Les lignes à haute tension permettent de réduire relativement les pertes. Lesquelles sont principalement dues à l'effet Joule (chaleur dissipée) et sont proportionnelles au produit de la résistance par le carré de l'intensité (P = RI) ; la puissance acheminée étant égale au produit de la tension par l'intensité (P = UI), élever la tension permet, pour une puissance donnée, de réduire significativement les pertes dans les réseaux câblés. La recherche continue sur les matériaux supraconducteurs qui ont une résistance nulle. Ils permettraient en effet d'éliminer non seulement les pertes dans le câble mais aussi le besoin d'équipement de transformation pour élever et abaisser la tension, avec les pertes qui y sont associées.

Le tableau ci-dessous détaille les multiples et sous-multiples du volt dans le Système international ; toutes ces unités ne sont pas utilisées.




</doc>
<doc id="14264" url="https://fr.wikipedia.org/wiki?curid=14264" title="Tesla (unité)">
Tesla (unité)

Le tesla (symbole : T), nommé en l'honneur du physicien serbe Nikola Tesla, est l'unité dérivée d'induction électromagnétique (appelé parfois densité de flux magnétique ou champ magnétique) du Système international d'unités (SI).

Il est défini comme l'induction magnétique qui, répartie normalement et uniformément sur une surface de 1 mètre carré, produit à travers cette surface un flux d'induction électromagnétique total de 1 weber : on a = .

Dans les unités dérivées du système international, un tesla peut être donné par les expressions suivantes :

Unités utilisées :


Dans le système CGS, l'unité d'induction magnétique est le gauss (G) ou maxwell par centimètre carré (Mx/cm).

Le gauss : 1 G équivaut à 0,1 mT = 100 µT.

Le gamma est le nom particulier d'un sous-multiple du gauss.


</doc>
<doc id="14265" url="https://fr.wikipedia.org/wiki?curid=14265" title="Weber (unité)">
Weber (unité)

Le weber (symbole : Wb) est l'unité dérivée de flux d'induction magnétique du Système international (SI). Il est nommé en l’honneur du physicien allemand Wilhelm Eduard Weber.

C'est le flux d'induction magnétique qui, traversant un circuit d'une seule spire, y produit une force électromotrice de 1 volt si on l'annule en 1 seconde par une décroissance uniforme ( = ).



</doc>
<doc id="14266" url="https://fr.wikipedia.org/wiki?curid=14266" title="Mètre carré">
Mètre carré

Le mètre carré, de symbole m, est l'unité d'aire du Système international. C'est notamment l'aire d'un carré d'un mètre de côté.

Cependant le Système international recommande les multiples, et sous-multiples, de 10.

Le format de papier A0, dont le rapport est de 1,414 à 1, est défini par une surface de 1 m. Le format A1 est la moitié du A0 et ainsi de suite. Le format standard A4, très utilisé en 2018, a donc une surface de 1/16ème de mètre carré.



</doc>
<doc id="14267" url="https://fr.wikipedia.org/wiki?curid=14267" title="Degré Celsius">
Degré Celsius

Le degré Celsius (symbole °C) est l’unité de l’échelle de température Celsius, qui est une unité dérivée du Système international d'unités, introduite le . Son nom est une référence à l’astronome et physicien suédois Anders Celsius, inventeur en 1742 d’une des premières échelles centigrades de température. Cette unité de mesure est d'usage courant à travers le monde, à l'exception des États-Unis, de Belize et des Îles Caïman, qui utilisent toujours l'échelle Fahrenheit.

La température Celsius est définie comme étant la température thermodynamique en kelvins, moins , afin que zéro soit la température de congélation.

L’unité de température Celsius est le degré Celsius (symbole °C), égale en magnitude au kelvin par définition : pour exprimer une "différence de température", les unités kelvin et degré Celsius sont équivalentes et un "intervalle de température" en degrés Celsius ou en kelvins a la même valeur numérique : par exemple, la différence 37 (°C) − 25 (°C) = = . Mais les deux échelles de température ne sont pas équivalentes, dans l’absolu : = .

Le degré Celsius est une unité de l’échelle internationale de température.

Le degré Celsius est défini par un seul point fixe de à .

Autrement dit, l’échelle Celsius n’est pas strictement définie par , car cette définition est celle du degré centigrade. Cependant, la différence relative est très faible, 1/, la température d’ébullition de l’eau étant de .

Pour l’historique des échelles centigrades qui ont précédé le degré Celsius, voir degré centigrade.

C’est lors de la Conférence générale des poids et mesures (CGPM), en 1948, que le Comité international trancha entre les trois termes « degré centigrade », « degré centésimal », et « degré Celsius » en faveur de ce dernier. Le degré centésimal désigne aussi un angle plan égal au 1/400 d’un cercle ; c’est un synonyme de grade ou gon (son symbole est gon).

Le degré Celsius est la seule unité métrique dont le nom comporte une majuscule. Les préfixes lui sont appliqués normalement ; on peut donc, par exemple, parler de « millidegré Celsius (symbole m°C) ».



</doc>
<doc id="14268" url="https://fr.wikipedia.org/wiki?curid=14268" title="Lanceur (astronautique)">
Lanceur (astronautique)

Dans le domaine astronautique, un lanceur est une fusée capable de placer une charge utile en orbite autour de la Terre ou de l'envoyer dans l'espace interplanétaire. La charge utile peut être un satellite artificiel, placé en orbite terrestre basse ou en orbite géostationnaire, ou une sonde spatiale qui quitte l’attraction terrestre pour explorer le système solaire. Pour y parvenir un lanceur doit pouvoir imprimer à sa charge utile une vitesse horizontale d'environ 8 km/s et l'élever au-dessus des couches denses de l'atmosphère terrestre (environ 200 km). Pour répondre aux différents besoins des lanceurs de toute taille ont été construits depuis le lanceur SS-520 de 2,6 tonnes capable de placer 4 kg en orbite basse jusqu'à la fusée Saturn V de tonnes pouvant placer 130 tonnes sur la même orbite.

Un lanceur est un engin complexe nécessitant la maitrise d'un grand nombre de technologies touchant aux domaines de la métallurgie, de la chimie et de l'électronique. À la suite de la première satellisation d'un engin spatial réussie en 1957 à l'aide d'une fusée "Semiorka", l'espace est devenu un enjeu politique puis économique et militaire majeur et les nations les plus avancées sur le plan technique ont progressivement développé leurs propres lanceurs. En 2017 une dizaine de pays (États-Unis, Russie, Europe, Japon, Chine, Inde, Israël, Iran, Corée du Nord) disposent de leur propre lanceur. Mais leur coût élevé, compris entre 10 millions € pour un lanceur léger (1 tonne placée en orbite basse) et 200 millions € pour un lanceur lourd (25 tonnes en orbite basse), limite leur usage. Depuis une vingtaine d'années il y a entre 50 et 100 lancements annuels. Les tirs sont effectués depuis des bases de lancement comprenant de nombreuses installations spécialisées (bâtiment d'assemblage, aire de lancement, centre de contrôle) et situées dans des emplacements choisis en fonction de contraintes de logistique, de sécurité et d'optimisation des performances des lanceurs.

Le lanceur est de manière standard non réutilisable c'est-à-dire que ses composants sont perdus après usage. La perte du lanceur après chaque tir constitue un frein important au développement de l'activité spatiale dans la mesure ou il contribue à augmenter son coût de manière significative. Pour abaisser ceux-ci plusieurs techniques permettant de réutiliser tout ou partie du lanceur ont fait l'objet de développements plus ou moins poussés. Le premier lanceur partiellement réutilisable, la navette spatiale américaine, s'est révélée à l'usage plus coûteuse que les lanceurs classiques. La piste du lanceur orbital monoétage utilisant une propulsion classique (X-33) est aujourd'hui abandonnée car elle nécessite de réduire la masse à vide du lanceur dans des proportions qui ne peuvent être atteintes avec les technologies existantes. L'avion spatial utilisant des moteurs aérobies (Skylon) n'a pas dépassé le stade de la planche à dessins. La seule réussite début 2017 est la récupération du premier étage du lanceur Falcon 9 dont l'intérêt économique, compte tenu des coûts inhérents à la technique utilisée (réduction de la charge utile, coût de récupération et de remise en état, surcoût assurance), n'a pas encore été démontré.

Durant la décennie 1950, la tension très forte entre par l'Union soviétique et les États-Unis et le développement de l'arme atomique conduisent au développement d'engins capables de lancer à grande distance une bombe atomique. Des études sont menés en parallèle autour d'engins ailés non pilotés et de fusées inspirés du missile V2 développé par les Allemands durant la Seconde Guerre mondiale. C'est cette deuxième technique qui l'emporte et très rapidement les deux pays mettent au point une série de missiles balistiques à longue portée. L'utilisation de de ce type d'engin pour la mise en orbite est rapidement identifié et les premiers lanceurs opérationnels capables de placer une charge utile en orbite sont mis au point immédiatement après la réalisation des premiers missiles balistiques opérationnels. Le premier lanceur est la fusée soviétique R-7 Semiorka qui place en orbite le 4 octobre 1957 le premier satellite artificiel Spoutnik 1. Le lanceur très lourd pour l'époque (plus de 250 tonnes) a une carrière très brève en tant que missile balistique intercontinental mais en tant que lanceur a par contre une carrière particulièrement longue puisqu'elle se poursuit encore aujourd'hui avec la fusée Soyouz.

Aux États-Unis les différents corps militaires ont chacun développé au milieu des années 1950 des missiles balistiques à courte, moyenne ou longue portée. Plusieurs d'entre eux donnent naissance à plusieurs familles de lanceurs à la durée de vie particulièrement longue : ainsi le lanceur Delta II, qui est la dernière version d'une famille de lanceurs développée à partir du missile Thor à la fin des années 1950, ne prend sa retraite qu'en 2017. Les missiles balistiques développés après 1961 ne font plus l'objet de conversions en lanceur à l'exception très marginale du lanceur Minotaur. Durant la décennie 1950 les ingénieurs américains multiplient les innovations grâce aux investissements massifs suscités par la tension entre les deux superpuissances de l'époque plongés en pleine Guerre froide Si les premiers lanceurs Juno/Mercury-Redstone sont encore très proches sur le plan technique du missile allemand V2, les lanceurs mis au point quelques années plus tard n'ont plus grand-chose de commun avec la fusée de Van Braun. La poussée et l'impulsion spécifique des moteurs-fusées est fortement accrue, l'électronique joue un rôle décisif dans le pilotage, de nouvelles combinaisons d'ergols sont mises au point et la masse structurelle est allégée de manière spectaculaire (Atlas). À côté des lanceurs issus de missiles convertis, deux lanceurs légers sans filiation militaire sont développés pour lancer principalement des satellites scientifiques. La fusée Vanguard dont le premier vol a lieu en 1956, et qui possède une charge utile de 45 kg a une courte carrière avec des résultats mitigés. Le lanceur Scout (premier vol 1961, charge utile de 50 à 150 kg) a une carrière qui se prolonge jusqu'en 1984. 

En Union soviétique tous les lanceurs du début de l'ère spatiale sont également dérivés de missiles balistiques développés dans les années 1950. Contrairement à ce qui se passe aux États-Unis ce mouvement de conversion se poursuit par la suite lorsque de nouveaux modèles de missiles apparaissent.

La Course à l'espace entre l'Union soviétique et les États-Unis pousse ces deux pays à développer des lanceurs de plus en plus puissants. Il faut notamment placer en orbite des vaisseaux spatiaux habités de plus en plus lourds, des missions d'exploration du système solaire plus complexes et des satellites de télécommunications (orbite géostationnaire) qui gagnent en capacité.

D'un point de vue technique, le lanceur est une fusée dont la principale spécificité est d'être suffisamment puissante pour pouvoir atteindre la vitesse de satellisation minimale qui sur Terre est de 7,9 km/s (vitesse horizontale). Il y a très peu de différences entre un lanceur et les autres types de fusée telles que la fusée-sonde utilisée pour sonder la haute atmosphère dans le cadre d'une mission scientifique ou le missile balistique intercontinental capable d'emporter une charge nucléaire à quelques milliers de kilomètres. Ainsi presque tous les lanceurs du début de l'ère spatiale sont dérivés de missiles balistiques reconvertis : "Semiorka" (Voskhod, Soyouz, Vostock, Molnia) , Cosmos, Juno , Longue Marche 2/3/4, Atlas, Delta, Titan, Thor. D'autres sont des fusées-sondes améliorées : : ce dernier est d'ailleurs parfois reconverti en lanceur en ne modifiant que la programmation du vol et en réduisant la masse de la charge utile. Le lanceur comme le missile est propulsé par des moteurs-fusées pouvant fonctionner en mode anaérobie. Il comporte plusieurs étages, qui sont largués au fur et à mesure, pour lui permettre d'atteindre la vitesse nécessaire à la mise en orbite. La charge utile qui doit être envoyée dans l'espace est placée au sommet du lanceur sous une coiffe qui est larguée dès que les couches plus denses de l'atmosphère ont été traversées.

Le système de propulsion constitue la caractéristique la plus importante d'un lanceur comme pour toute fusée (fusée-sonde, missile balistique). Toutefois le lanceur a des contraintes spécifiques :

Les premiers missiles balistiques à l'origine des familles de lanceurs qui vont longtemps occuper une place prépondérante utilisent principalement la combinaison d'ergols liquides kérosène/oxygène qui constitue un bon compromis entre performance, masse volumique et complexité de mise en œuvre. Les lanceurs Soyouz, Delta, Atlas utilisent cette technique pour le premier étage. 

Pour les missiles balistiques le mélange Kérosène / Oxygène présente l'inconvénient de ne pouvoir être stocké en permanence dans les réservoirs de l'engin et donc de nécessiter une phase de remplissage avant le lancement trop longue (jusqu'à plusieurs heures) pour répondre aux contraintes opérationnelles qui nécessitent un délai de mise à feu de quelques secondes. La deuxième génération de missiles utilise des ergols stockables qui présentent toutefois l'inconvénient d'être très toxiques et moins performants. Les lanceurs dérivés de cette génération de missile sont la fusée américaine Titan, les lanceurs soviétiques Proton, Dnepr, Rockot, ainsi que les lanceurs issus de missiles d'inspiration soviétique : Longue Marche 2/3/4 chinois, Safir iranien, Unha nord-coréen. C'est également ces ergols qui sont utilisés par les lanceurs européens Ariane 1, 2, 3 et 4. La toxicité des ergols et leurs performances réduites ont conduit à l'abandon de ce mode de propulsion au fur et à mesure du renouvellement des familles de lanceurs. En 2017 les principaux lanceurs qui utilisent encore ce mélange sont les Proton en cours de remplacement par l'Angara et les Longue Marche 2/3/4 en cours de remplacement par les Longue Marche 5 et 7.

La capacité d'un lanceur se mesure selon plusieurs critères. Le principal est la masse qu'il peut satelliser. La charge utile peut occuper un volume important ou nécessiter une accélération et un régime de vibrations réduits que tous les lanceurs ne peuvent pas offrir.

La performance d'un lanceur se mesure d'abord par sa capacité à placer une charge utile plus ou moins lourde en orbite. On range ainsi les lanceurs dans des grandes catégories reposant sur la masse satellisable : depuis le lanceur léger capable de placer environ 1 à 2 tonnes en orbite basse (par exemple Vega), au lanceur lourd pouvant lancer 20 à 25 tonnes (Ariane 5) en passant par lanceur de taille intermédiaire pouvant emporter une charge d'une dizaine de tonnes (Soyouz). Deux lanceurs plus puissants ont été développés par la passé dans le cadre de la course à la Lune : le lanceur américain Saturn V (130 tonnes en orbite basse dans sa dernière version) et le lanceur soviétique N-1 (95 tonnes). Ces lanceurs très couteux ont été retirés du service au début des années 1970 après l'arrêt du programme Apollo. Mi 2010 deux lanceurs capables de placer en orbite plus de 25 tonnes sont en phase de développement Falcon Heavy (53 tonnes en orbite basse) et Space Launch System (70 à 130 tonnes). 

La taille de la coiffe joue un rôle important car elle conditionne le volume des charges utiles emportées. En général la taille de la coiffe est corrélée avec le diamètre du lanceur. Pour permettre l'emport de charges utiles volumineuses, elle a souvent un diamètre plus important que le lanceur mais le rapport de diamètre ne doit pas être trop important pour ne pas générer de contraintes trop fortes sur la structure durant la traversée des couches basses de l'atmosphère. 

Presque tous les lanceurs sont aujourd'hui qualifiés pour effectuer des lancements multiples c'est-à-dire larguer plusieurs charges utiles sur des orbites différentes. 

Selon les missions, le lanceur peut placer sa charge utile sur des orbites différentes. Celles-ci sont caractérisées par leur altitude, la forme de l'orbite (circulaire ou plus ou moins fortement elliptique) et l'inclinaison orbitale. Le type d'orbite visé et la position du site de lancement influent sur la puissance nécessaire pour l'atteindre. La masse que peut placer en orbite un lanceur donné dépend donc de sa destination. Les principales orbites terrestres sont dans l'ordre de puissance nécessaire croissante :

Enfin le lanceur peut placer une charge utile sur une orbite interplanétaire c'est-à-dire qui lui permet d'échapper à l'attraction terrestre. Parmi celles-ci, les destinations les plus couramment visées sont 

Le lanceur est tiré depuis une base de lancement qui comprend de nombreuses installations spécialisées : bâtiment d'assemblage, aire de lancement, centre de contrôle. La base de lancement est située dans des emplacements choisis en fonction de contraintes de logistique, de sécurité et d'optimisation des performances des lanceurs. Un carneau d’environ vingt mètres de profondeur reçoit les gaz produit par le fonctionnement des moteurs au décollage.

"En gras figurent les lanceurs ou familles de lanceurs qui sont actuellement (mai 2014) en service. Les autres étant soit abandonnés, soit toujours en développement."


Orbital ATK

SpaceX

Blue Origin

















</doc>
<doc id="14272" url="https://fr.wikipedia.org/wiki?curid=14272" title="Fusée (astronautique)">
Fusée (astronautique)

Une fusée, dans le domaine de l'astronautique, est un véhicule capable d'échapper à l'attraction terrestre et de se déplacer dans l'espace proche, grâce à un moteur-fusée de grande puissance. Une fusée doit ainsi atteindre la vitesse de libération en emportant à la fois le combustible et le comburant nécessaires à son fonctionnement, mais aussi une charge utile. Elle est pour cela dotée de plusieurs étages mis à feu successivement. Les plus grosses fusées construites, comme Saturn V, permettent de placer jusqu'à en orbite basse. 

La science des fusées a été théorisée par le Russe Constantin Tsiolkovski à la fin du et mise en pratique dès 1935 par Hermann Oberth, puis par les chercheurs allemands durant la Seconde Guerre mondiale, pour la conception des premiers missiles balistiques V2. À compter de la fin des années 1950, les fusées ont été utilisées pour mettre en orbite des satellites à des fins commerciales, militaires, de télécommunication ou de recherche, et envoyer des sondes spatiales vers les autres planètes du système solaire, ou des hommes dans l'espace proche, ainsi que sur la Lune. La technologie des fusées n'évolue pratiquement plus depuis la fin des années 1960. 

La fusée utilise le principe des actions réciproques pour accélérer en éjectant derrière elle de la matière, à l'aide d'un (ou de plusieurs) moteur-fusée(s).

C'est la pression interne exercée contre la paroi située du côté opposé à celui où se trouve l'orifice de sortie (tuyère) qui, étant supérieure à la pression ambiante fait se déplacer le corps de la fusée dans le sens de la poussée la plus forte.

Comme dans un moteur de voiture, la propulsion par un moteur-fusée utilise l'énergie dégagée par la combustion d'un carburant avec un comburant. Le moteur-fusée comporte deux éléments essentiels : la chambre de combustion et la tuyère. Dans les moteurs à propergol liquide, ceux-ci sont injectés en grande quantité et sous forte pression dans la chambre de combustion, grâce à des pompes surpuissantes : les turbopompes. 

À la différence d'un moteur classique :

La combustion est une réaction chimique qui fait intervenir un réducteur (le carburant) et un oxydant (le comburant). Elle est fortement exothermique c'est-à-dire qu'elle dégage de la chaleur et porte les gaz résultant de la combustion à des températures de plusieurs milliers de degrés. L'énergie thermique est transformée en énergie cinétique avec un rendement qui est bon puisqu'il atteint 50 %.

L'accélération obtenue est régie par la loi de la conservation de mouvement : elle est proportionnelle au logarithme de la masse de gaz expulsée divisé par la masse de la fusée. Elle est aussi proportionnelle à la vitesse d'éjection du gaz.

Plus précisément la loi s'énonce ainsi :
formula_1
dans laquelle :

Cette équation est établie en intégrant l'équation de conservation de la quantité de mouvement entre le début et la fin de la phase propulsée sous les hypothèses suivantes :

L'impulsion spécifique (notée généralement I) est le quotient de la poussée d'un propulseur, par le produit du débit massique de propergol par la valeur normale de l'accélération de la pesanteur (ou débit-poids du propergol éjecté). 
À poussée égale, plus l'I d'un propulseur est grande, moins il consomme d'ergols. L'impulsion spécifique, homogène à un temps, s'exprime en unités de temps (le plus souvent en secondes). Elle indique la durée pendant laquelle un kilogramme de propergol produit une poussée de 1 kilogramme-force, c'est-à-dire 9,81 N :

formula_6

avec F la poussée, q le débit massique d'éjection des gaz et g l'accélération de la pesanteur. 

Impulsions spécifiques des propergols les plus utilisés ou étudiés :

On démontre qu'une fusée composée d'un seul étage ne pourrait placer en orbite une charge utile même si elle utilise les ergols les plus performants et que son indice constructif est particulièrement faible. 

Pour optimiser ses performances, une fusée doit donc être multiétages : chaque étage est doté de son ou de ses propres moteurs-fusées et est largué lorsque le carburant est épuisé. Le moteur de l'étage suivant est alors allumé.

Le premier étage des lanceurs modernes est souvent constitué d'un étage principal flanqué d'étages appelés accélérateurs dont le rôle est de fournir une poussée additionnelle durant les premières minutes du vol. Ces accélérateurs qui sont généralement à poudre peuvent avoir une poussée supérieure au premier étage (Ariane 5) proprement dit mais sont largués longtemps avant que le premier étage ait épuisé son carburant. 

Traditionnellement les lanceurs spatiaux ont 3 étages (Ariane 1 et 4, Saturn V) ou 2 étages + accélérateurs accolés au (Ariane 5…). Le dernier étage propulsif communique la part la plus importante de la vitesse horizontale au satellite. Pour augmenter ses performances, on choisit souvent une propulsion cryogénique. Cet étage dans les lanceurs les plus sophistiqués peut être éteint et rallumé plusieurs fois ce qui donne plus de souplesse pour mettre en place les charges utiles sur leurs orbites.

La propulsion est utilisée par deux types d'engins :

Le moteur-fusée comprend :

La chambre de combustion est le lieu où se réalise la combustion des ergols. Pour réduire la taille et donc le poids du moteur-fusée la pression dans la chambre de combustion doit être la plus élevée possible. Généralement les ergols sont pulvérisés dans des proportions qui assurent une combustion complète (mélange stœchiométrique) ce qui suppose que le mélange soit homogène. L'injecteur qui envoie carburant et comburant dans la chambre de combustion prend des formes variables selon les modèles de moteur : injecteur en pomme de douche (jets parallèles), jets concourants, etc. L'instabilité de la combustion est un des problèmes les plus graves et les plus fréquents qui affecte les fusées.

S'il n'est pas hypergolique le mélange doit être enflammé par un dispositif dont la fiabilité est un critère essentiel. L'allumage du mélange peut être déclenché par l'introduction d'un produit hypergolique avec un des deux ergols, une résistance parcourue par un courant de forte intensité, un catalyseur, une petite charge pyrotechnique, une chambre d'allumage qui communique avec la chambre de combustion.

La tuyère permet d'accélérer les gaz résultant de la combustion portés à des pressions et des températures très élevés en leur imprimant une vitesse suivant l'axe de la fusée (lorsque celle-ci ne braque pas). La tuyère a la forme d'un cône convergent puis divergent qui permet aux gaz de franchir la vitesse du son : en amont du col la vitesse du gaz est subsonique et en aval supersonique. En présence d'atmosphère la poussée est optimale lorsque la pression des gaz en sortie de tuyère est égale à la pression ambiante. Les tuyères de premier étage sont donc plus courtes que celles des étages devant fonctionner dans le vide. Pour limiter l'encombrement, la tuyère des moteurs fusées des étages supérieurs peut être en partie déployable.

Les parois de la chambre de combustion ainsi que la tuyère sont portées à des températures très élevées (plusieurs milliers de degrés) et doivent être refroidies. La méthode la plus courante consiste à faire circuler un des ergols dans la paroi qui à cet effet est creuse ou constituée de tubes jointifs. Le liquide utilisé pour le refroidissement peut être réinjecté dans la chambre de combustion (refroidissement par circulation d'ergol) ou moins performant être éjecté en extrémité de tuyère (refroidissement par fluide perdu).

Les moteurs à propergol solide ont des caractéristiques et un mode de fonctionnement différents des moteurs à propergol liquide. Carburant et comburant sont stockés sous forme solide intimement mélangés. Le réservoir est en même temps la chambre de combustion : celle-ci est située dans le canal percé au centre du bloc de poudre sur toute sa longueur. Au fur et à mesure de la combustion, le canal s'élargit. Le diamètre du bloc de poudre détermine la durée de la combustion. La surface exposée à la combustion détermine la poussée. En donnant une géométrie donnée au canal (souvent en forme d'étoile) on peut créer une poussée croissante, décroissante ou constante (on parle de bloc progressif, dégressif ou neutre).

Le moteur est allumé par un système d'allumage placé au fond du canal. Les gaz résultant de la combustion sont chassés vers l'extrémité inférieure : au bout du réservoir, une tuyère canalise et accélère les gaz brulés. La tuyère peut-être orientée par des vérins pour modifier l'axe de la poussée. Sur certaines fusées un autre système d'orientation est utilisé reposant sur l'injection d'un jet de gaz dans la tuyère.

Le moteur à propergol solide est de conception simple car il ne comporte pas de pièces mobiles. Les ergols peuvent être conservés longtemps sans précautions particulières et mis en œuvre rapidement ce qui fait qu'il est systématiquement utilisé pour les missiles balistiques. Contrairement aux moteurs à ergols liquides il est relativement facile de concevoir un moteur doté d'une poussée très importante (accélérateurs de la Navette spatiale et d'Ariane 5). Mais les performances (ISP) sont beaucoup plus faibles : le mélange perchlorate d'ammonium/aluminium/polybutadiène (liant), qui est utilisé dans 90 % des cas, a une impulsion spécifique de 273. De plus l'enveloppe de l'étage qui subit de fortes contraintes thermiques doit être en acier ce qui accroit la masse de la structure. Le moteur à propergol solide une fois allumé ne peut plus être éteint puis rallumé. Il existe parfois un dispositif d'arrêt de poussée. La tuyère qui n'est pas refroidie doit être conçue dans des matériaux résistant à des températures élevées.

Les principaux éléments d'une fusée à propergol liquide sont :

L'indice de structure d'une fusée est le rapport entre la masse à vide d'un étage de fusée (réservoirs, structure, moteur…) et sa masse au décollage. Plus cet indice est faible, plus la fusée est performante. Pour y parvenir, la fusée est construite avec des matériaux légers et la structure est optimisée en particulier par la mise en œuvre de réservoirs structuraux.

La paroi latérale des réservoirs des étages principaux constitue en même temps la structure de la fusée. Dans le cas d'étage à ergols liquides, les réservoirs sont constitués de plusieurs viroles de faible épaisseur ( pour l'étage cryogénique de la fusée Ariane 5) soudés entre elles. La tenue aux efforts mécaniques est assurée en grande partie par la mise en pression des réservoirs. Les parties de la fusée non pressurisés (inter-étages, inter-réservoirs et les bâtis-moteurs) sont constitués de structures raidies donc plus lourdes.

Les principaux matériaux utilisés pour la construction d'une fusée sont des alliages d'aluminium qui ont de bonnes caractéristiques mécaniques, sont relativement légers, peu coûteux et assez faciles à travailler. Les alliages d'acier, malgré leur densité très pénalisante, sont utilisés principalement pour l'enveloppe des propulseurs à poudre qui subissent des fortes pressions ; le recours à l'acier entraine un indice de structure élevé (11,5 % pour les propulseurs à poudre d'Ariane 5 contre 7,3 % pour l'étage cryotechnique). Les composites (fibres de carbone, kevlar, verre), plus coûteux, ont d'excellentes caractéristiques mécaniques et sont utilisés dans la partie haute de la fusée pour la coiffe, la structure porteuse des charges utiles et pour les petits réservoirs.

Une fusée comprend différents systèmes qui permettent son fonctionnement. Les boitiers de commande de ces systèmes sont regroupés dans la case à équipement généralement logée juste sous la charge utile sur la périphérie d'un anneau faisant la jonction avec les étages propulsifs. Les capteurs, les actuateurs, les charges pyrotechniques sont eux répartis sur l'ensemble de la fusée.

La charge utile est positionnée au sommet de la fusée au-dessus de tous les étages propulsifs. Elle est constituée d'un ou plusieurs satellites qui sont recouverts d'une coiffe à la forme aérodynamique qui les protège tant que la fusée traverse l'atmosphère et qui est larguée par la suite pour réduire la masse propulsée.

Lorsque la fusée transporte des astronautes, elle doit pouvoir préserver la vie des passagers au cas où le vol se passe mal. Si au-dessus d'une certaine altitude il suffit que la capsule qui transporte les passagers se sépare de la fusée à l'aide de charges pyrotechniques puis entame la phase de descente prévue initialement pour le retour, ce dispositif ne peut pas fonctionner lorsque la fusée est trop basse. 

La tour de sauvetage placée au sommet du lanceur est une fusée miniature qui, en cas de problème, est mise à feu et arrache la capsule du corps de la fusée en l'éloignant de la trajectoire fusée tout en lui faisant prendre une hauteur minimum pour que le parachute puisse être ouvert et dispose de suffisamment de temps pour freiner le vaisseau spatial avant qu'il atteigne le sol. Initialement, pour les premiers vols spatiaux habités (Gemini, Vostok), le sauvetage de l'équipage en cas d'explosion de la fusée était confié à un siège éjectable. Ce dispositif était lourd (la surcharge est conservée tout au long du vol) et ne permettait pas d'écarter suffisamment les cosmonautes de la zone dangereuse lorsque la fusée utilisait des carburants hypergoliques (oxygène/hydrogène).

La fusée suit une trajectoire précise qui doit lui permettre de placer sa charge utile sur une orbite calculée adaptée à sa mission. Cette trajectoire doit répondre à plusieurs contraintes dont celle critique de la consommation de carburant. Un système de guidage et de pilotage embarqué calcule en permanence en temps réel la position et l'attitude de la fusée, corrige l'orientation et déclenche la séparation des étages.

Avant l'envol de la fusée une trajectoire dite nominale est calculée pour permettre de placer la charge utile sur l'orbite désirée (vitesse horizontale, direction). Cette trajectoire optimise la consommation de carburant et répond à un certain nombre d'autres contraintes.

La trajectoire réelle diffère de la trajectoire nominale pour différentes raisons :

Le système de guidage fait en sorte que la trajectoire nominale soit respectée. Il doit corriger les déviations en réorientant la fusée et éventuellement en prolongeant le temps de combustion des étages.

Le système de guidage détermine l'écart avec la trajectoire nominale à l'aide d'accéléromètres qui mesurent les accélérations et de gyromètres qui mesurent les vitesses de rotation angulaire. Il envoie des instructions au système de pilotage.

Le système de pilotage corrige la trajectoire en modifiant l'orientation de la poussée du ou des moteur de quelques degrés ce qui entraine le pivotement du lanceur autour de son centre de masse. La plupart des moteurs-fusées (propergol liquide) sont orientables à l'aide de vérins électriques (petits moteurs de quelques kilogrammes) ou des vérins hydrauliques. Le pilotage est asservi c'est-à-dire que le résultat des corrections est constamment contrôlé et éventuellement corrigé à nouveau.

Le pilotage doit également prendre en compte les phénomènes suivants : 

La précision de la trajectoire obtenue par ce pilotage peut alors être inférieure à et quelques centaines de mètres en périgée sur Ariane 5 (sur , donc une précision de 10).

La campagne de lancement d'une fusée comprend les étapes suivantes :

La latitude de la base de lancement a une incidence importante sur l'orbite qui peut être atteinte par la charge utilisée :
Pour ces deux raisons les bases de lancement situées près de l'équateur sont avantagées pour le lancement des satellites géostationnaire par rapport aux bases spatiales situées à des latitudes plus septentrionales (à l'origine de la décision de lancer de fusées Soyouz depuis la base spatiale de Kourou).

Le lanceur place la charge utile sur une orbite initiale qui dépend de plusieurs paramètres :
L'heure de lancement est donc un facteur souvent important. Pour certains satellites héliosynchrones, la fenêtre de lancement est réduite à quelques minutes par jour. D'autres critères peuvent être pris en compte en particulier la position du soleil lorsque la charge utile entame son orbite : celle-ci a une incidence sur les capteurs pilotant le contrôle de l'orientation et sur l'éclairement des panneaux solaires. 

Pour une sonde spatiale qui doit être mise en orbite ou survoler une autre planète, il est nécessaire de prendre en compte les positions relatives de la Terre et de la planète visée : pour des raisons de coût, ces sondes sont généralement conçues pour emporter une quantité de carburant correspondant aux configurations les plus favorables. Celles-ci peuvent n'apparaître qu'à des intervalles de temps éloignées (créneau d'environ huit mois tous les deux ans pour Mars). Le calendrier de réalisation du satellite tient évidemment compte de la fenêtre de tir mais à la suite de retard dans le développement ou de problèmes avec le lanceur, il est arrivé que, la fenêtre de tir ayant été manquée, le lancement soit reporté de plusieurs mois sinon de plusieurs années.

La trajectoire d'une fusée est initialement verticale durant 10 à 20 secondes pour dégager la fusée des installations au sol.

Durant la traversée de l'atmosphère la fusée est basculée dans le plan de sa future orbite avec un angle qui doit minimiser les efforts mécaniques qui s'exercent sur sa structure en réduisant au minimum la pression aérodynamique. Durant cette phase les rafales de vent doivent être amorties. Durant cette phase la pression aérodynamique, qui est fonction de la vitesse et de la densité de l'atmosphère, passe par un maximum (la PD max ou max Q). La structure du lanceur doit être dimensionnée pour pouvoir supporter ces forces. Pour la fusée Ariane 5, la PD max est atteinte à une altitude de alors que la fusée est à une vitesse relative d'environ Mach 2. 

La séparation des étages est effectuée à l'aide de charges pyrotechniques. Certains lanceurs comportent de petites fusées de séparation qui ralentissent l'étage largué pour éviter que celui-ci ne vienne percuter le reste de la fusée car l'extinction du moteur de l'étage largué n'est généralement pas totale (queue de poussée) tandis que l'allumage de l'étage suivant n'est pas immédiat. Après séparation, des fusées de tassement ( en anglais) de faible puissance peuvent être allumées pour plaquer les ergols liquides au fond du réservoir et permettre une alimentation des moteurs principaux au démarrage malgré l'apesanteur.

Au-delà de l'atmosphère est suffisamment raréfiée pour que la pression aérodynamique devienne quasi nulle : il n'y a plus de contrainte sur l'orientation de la poussée. Si le vol est un lancement de satellites, arrivé à une altitude qui se situe selon les lanceurs entre 100 et d'altitude, la coiffe, dont le poids réduit la performance du lanceur, est larguée car la charge utile ne subit plus qu'une pression aérodynamique très faible.

Selon le type de mission, le lanceur place la charge utile immédiatement sur son orbite définitive (satellites en orbite basse) ou sur une orbite d'attente ou de transfert (satellite géostationnaire, sonde spatiale à destination d'une autre planète). Le lanceur après avoir décollé prend un azimut de manière à ce que le vecteur vitesse se rapproche le plus possible du plan d'orbite cible à l'extinction des moteurs du lanceur. 

Lorsque le moteur du lanceur s'éteint la charge utile entame sa première orbite : c'est le point d'injection. Si par suite d'une défaillance partielle du lanceur, la vitesse de satellisation n'est pas atteinte, la charge utile effectue un vol balistique et retombe vers le sol. Si la composante verticale de sa vitesse par rapport au sol est nulle au point d'injection ce dernier se confond avec le périgée de l'orbite sinon le périgée se trouve à une altitude inférieure. Il subsiste toujours de petits écarts par rapport à l'orbite visée (les dispersions) qui sont corrigées dans les phases suivantes.

Avant le largage de la charge utile le lanceur modifie son orientation conformément au besoin de celle-ci. Le lanceur imprime une vitesse de rotation plus ou moins importante à la charge utile pour lui donner une certaine stabilité. Celle-ci se sépare alors du lanceur. Le lanceur peut répéter cette opération plusieurs fois s'il s'agit d'un lancement multiple. la charge utile libérée met en service ses panneaux solaires en les déployant si nécessaire (manœuvre parfois source de défaillances). Elle utilise ses senseurs pour définir son orientation dans l'espace et corrige celle-ci à l'aide de ses moteurs d'attitude de manière à pointer ses panneaux solaires et ses instruments dans la bonne direction.

La trajectoire est calculée de manière à ce qu'après la séparation les étages retombent dans une zone dépourvue d'habitations. Ces règles ne sont pas toujours respectées en Russie et en Chine.

Une fusée-sonde est une fusée décrivant une trajectoire sub-orbitale permettant d'effectuer des mesures et des expériences. Lancée verticalement, une fusée-sonde peut emporter des centaines de kilogrammes d’instruments ou d’expériences scientifiques à une altitude comprise entre une centaine et un millier de kilomètres selon les modèles. Sa charge utile, abritée dans la pointe de l’engin, est récupérée avec un parachute. Les recherches effectuées avec les fusées-sondes portent essentiellement sur 2 thèmes :

Un missile balistique est une fusée transportant une charge militaire généralement nucléaire dont la trajectoire est balistique. La phase balistique est précédée par une phase d'accélération alimentée par un moteur-fusée donnant à l'engin l'impulsion nécessaire pour atteindre sa cible.

On distingue :

Le premier usage documenté de fusées remonte à 1232. Pendant la bataille de Kaifeng, les Chinois repoussèrent les Mongols à l'aide de « flèches de feu volantes ». Les fusées utilisées à l'époque bien que n'étant pas très destructrices par elles-mêmes permettaient de désorganiser l'armée adverse en provoquant la panique de ses chevaux.

Après la bataille de Kaifeng, les Mongols produisirent leurs propres fusées, et pourraient avoir été responsables de leur introduction en Europe. Entre les XIII et s, on relève des mentions éparses de fusées.

Au le livre arabe "Kitâb al-furussia wal munassab al-harbiya" illustre l'utilisation de fusées lors de combats. Il se peut qu'elles aient été utilisées par les Arabes lors des septièmes croisades, contre les troupes du roi Saint Louis ; ces fusées comportait une charge utile explosive sous la forme d'un sac de poudre.

Elles auraient été utilisées en Normandie contre les Anglais vers 1450. En Angleterre, le moine Roger Bacon améliora la poudre augmentant la portée des fusées. En France, Jean Froissart découvrit que la précision de ces armes était améliorée, si on les lançait à partir de tubes (c'est l'ancêtre du bazooka). En Italie, Joanes de Fontana conçut une torpille de surface, propulsée par fusée, dont le but était de bouter (transmettre) le feu aux navires ennemis.

Au , les fusées tombèrent en défaveur en tant qu'engins de guerre (en partie à cause de l'amélioration en puissance et en précision de l'artillerie), même si elles continuèrent à être utilisées pour les feux d'artifice.

Un artificier allemand, , inventa la fusée gigogne, un engin à multiples étages allumés séquentiellement et permettant de faire atteindre au feu d'artifice une plus grande altitude. C'est l'ancêtre des fusées à multiples étages utilisées aujourd'hui.

Lors de la Première Guerre mondiale, elles refirent surface avec les ancêtres des roquettes, utilisées par les aviateurs pour toucher les ballons d'observation ennemis.

Les plus grands progrès de la fin du à la Seconde Guerre mondiale sont dus à Constantin Tsiolkovski à qui l'on doit la découverte de la loi fondamentale du rapport de masse impliquant le découpage en plusieurs tronçons des fusées et les calculs et dessins d'une chambre de combustion à refroidissement pour deux combustibles, devenant un véritable visionnaire de l'astronautique. 

La technique progressa encore dans l'entre-deux-guerres grâce à divers pionniers comme Pedro Paulet, réalisateur du premier moteur à propergols liquides, Louis Damblanc qui lança la première fusée à étages, et Robert Goddard, spécialiste du développement de moteurs de fusées.

Même si on pensait dès le début du à un usage pacifique pour des voyages interplanétaires, c'est uniquement les militaires qui ont développé les fusées pour en faire des missiles à longue portée. Notamment les Allemands avec les travaux de Werner von Braun, avec le fameux V2. Après la guerre, les États-Unis et l'URSS récupérèrent le matériel et les ingénieurs allemands pour leur propre compte (voir Opération Paperclip). Le premier « V2 » américain, sur lequel travaillait von Braun, décolla le 14 mars 1946. Le « V2 » soviétique, lui, décolla le 18 octobre 1947 sous la direction de Sergueï Korolev et Valentin Glouchko. La course vers l'espace était commencée.

Indépendamment de sa taille deux paramètres suffisent pratiquement pour définir les performances d'une fusée :
Space X 


Les fusées sont particulièrement présentes dans le domaine de la science-fiction où elles sont associés au vol spatial. On se souviendra notamment de l'album de BD "On a marché sur la Lune" où l'auteur belge Hergé faisait poser le pied sur la Lune à son équipage dirigé par Tintin en 1954, soit quinze ans avant Neil Armstrong. Ils disposaient d'une fusée restée célèbre.

Principales sources utilisées pour la rédaction de l'article :




</doc>

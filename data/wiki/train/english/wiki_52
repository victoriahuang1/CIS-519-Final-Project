<doc id="6907" url="https://en.wikipedia.org/wiki?curid=6907" title="Chakra">
Chakra

Chakras (Sanskrit: चक्र, IAST: cakra, Pali: cakka, lit. "wheel, circle"), are focal points in the subtle body used in a variety of meditation techniques in the esoteric traditions of Indian religions and used in new age medicine and psychology.

The concept is found particularly in the tantric traditions of Hinduism, Buddhism and Jainism. They are treated as focal points, or psychic nodes in the subtle body of the practitioner. The concept of the cakras is an integral part of kundalini yoga, but not all systems of the chakras rely on kundalini. These theories differ between the Indian religions, with many esoteric Buddhist texts mentioning five Chakras, while esoteric Hindu texts will often describe seven. They are believed to be part of the subtle body, not the physical body, and connected by energy channels called Nadi. In kundalini yoga breath exercises, visualizations, mudras, bandhas, kriyas, and mantras are focused on channeling kundalini energy through chakras.

The word "Chakra" (चक्र) derives from the Sanskrit word meaning "wheel," as well as "circle" and "cycle". One of the Hindu scriptures "Rigveda" mentions "Chakra" with the meaning of "wheel", with "ara" (spokes). According to Frits Staal, "Chakra" has Indo-European roots, is "related to Greek "Kuklos" (from which comes English "cycle"), Latin "circus", Anglo-Saxon "hveohl" and English "wheel"." However, the Vedic period texts use the same word as a simile in other contexts, such as the "wheel of time" or "wheel of dharma", such as in "Rigveda" hymn verse 1.164.11.

In Buddhism, the Sanskrit term "cakra" (Pali "cakka") also means "wheel", but it is used in the additional sense of "circle" connoting rebirth in six realms of existence where a being is reborn after each death.

In Jainism, the term "Chakra" also means "wheel" and appears in various context in its ancient literature. Like other Indian religions, "Chakra" in esoteric theories in Jainism such as those by Buddhisagarsuri means yogic-energy centers.

The term "Chakra" already appears in Vedic literature, the earliest stratum of Hindu scripture, but not in the sense of psychic energy centers, rather as "chakravartin" or the king who "turns the wheel of his empire" in all directions from a center, representing his influence and power. The iconography popular in representing the "Chakras", states White, trace back to the five symbols of yajna, the Vedic fire altar: "square, circle, triangle, half moon and dumpling".

The hymn 10.136 of the "Rigveda" mentions a loner yogi ascetic with a female named "kunamnama". Literally, it means "she who is bent, coiled", and it probably is either a minor goddess or one of many embedded puzzles and hidden references within the "Rigveda". Some scholars, such as David Gordon White and Georg Feuerstein interpret this might be related to kundalini shakti, and a prelude to the terms such as chakra that emerged later.

Breath channels (nāḍi) of Yoga practices are mentioned in the classical Upanishads of Hinduism dated to 1st millennium BCE, but not psychic-energy Chakra theories. The latter, states White, were introduced about 8th-century CE in Buddhist texts as hierarchies of inner energy centers, such as in the "Hevajra Tantra" and "Caryāgiti". These are called by various terms such as "cakka", "padma" (lotus) or "pitha" (mound). These medieval Buddhist texts mention only four chakras, while later Hindu texts such as the "Kubjikāmata" and "Kaulajñānanirnaya" expanded the list to many more.

In contrast to White, according to Feuerstein, early Upanishads of Hinduism do mention "cakra" in the sense of "psychospiritual vortices", along with other terms found in tantra: "prana" or "vayu" (life energy) along with "nadi" (energy carrying arteries). According to Galvin Flood, the ancient texts do not present "chakra" and kundalini-style yoga theories although these words appear in the earliest Vedic literature in many contexts. The "chakra" in the sense of four or more vital energy centers appear in the medieval era Hindu and Buddhist texts.

Chakra is a part of the esoteric medieval era theories about physiology and psychic centers that emerged across Indian traditions. The theory posited that human life simultaneously exists in two parallel dimensions, one "physical body" ("sthula sarira") and other "psychological, emotional, mind, non-physical" it is called the "subtle body" ("suksma sarira"). This subtle body is energy, while the physical body is mass. The psyche or mind plane corresponds to and interacts with the body plane, and the theory posits that the body and the mind mutually affect each other. The subtle body consists of nadi (energy channels) connected by nodes of psychic energy it called "chakra". The theory grew into extensive elaboration, with some suggesting 88,000 cakras throughout the subtle body. The chakra it considered most important varied between various traditions, but they typically ranged between four and seven.
The important chakras are stated in Buddhist and Hindu texts to be arranged in a column along the spinal cord, from its base to the top of the head, connected by vertical channels. The tantric traditions sought to master them, awaken and energize them through various breathing exercises or with assistance of a teacher. These chakras were also symbolically mapped to specific human physiological capacity, seed syllables (bija), sounds, subtle elements (tanmatra), in some cases deities, colors and other motifs.

The chakra theories of Buddhism and Hinduism differs from the historic Chinese system of meridians in acupuncture. Unlike the latter, the "chakra" relates to subtle body, wherein it has a position but no definite nervous node or precise physical connection. The tantric systems envision it as a continually present, highly relevant and a means to psychic and emotional energy. It is useful in a type of yogic rituals and meditative discovery of radiant inner energy ("prana" flows) and mind-body connections. The meditation is aided by extensive symbology, mantras, diagrams, models (deity and mandala). The practitioner proceeds step by step from perceptible models, to increasingly abstract models where deity and external mandala are abandoned, inner self and internal mandalas are awakened.

These ideas are not unique to Buddhist and Hindu traditions. Similar and overlapping concepts emerged in other cultures in the East and the West, and these are variously called by other names such as subtle body, spirit body, esoteric anatomy, sidereal body and etheric body. According to Geoffrey Samuel and Jay Johnston, professors of Religious studies known for their studies on Yoga and esoteric traditions:

Chakra and related theories have been important to the esoteric traditions, but they are not directly related to mainstream yoga. According to Edwin Bryant and other scholars, the goals of classical yoga such as spiritual liberation (freedom, self-knowledge, moksha) is "attained entirely differently in classical yoga, and the "cakra / nadi / kundalini" physiology is completely peripheral to it."

The classical eastern traditions, particularly those that developed in India during the 1st millennium AD, primarily describe "nadi" and "cakra" in a "subtle body" context. To them, they are the parallel dimension of psyche-mind reality that is invisible yet real. In the "nadi" and "cakra" flow the "prana" (breath, life energy). The concept of "life energy" varies between the texts, ranging from simple inhalation-exhalation to far more complex association with breath-mind-emotions-sexual energy. This essence is what vanishes when a person dies, leaving a gross body. Some of it, states this subtle body theory, is what withdraws within when one sleeps. All of it is believed to be reachable, awake-able and important for an individual's body-mind health, and how one relates to other people in one's life. This subtle body network of "nadi" and "chakra" is, according to some later Indian theories and many new age speculations, closely associated with emotions.

Different esoteric traditions in Hinduism mention numerous numbers and arrangements chakras, of which a classical system of seven is most prevalent. This seven-part system, central to the core texts of hatha yoga, is one among many systems found in Hindu tantric literature. These texts teach many different Chakra theories.

The Chakra methodology is extensively developed in the goddess tradition of Hinduism called Shaktism. It is an important concept along with yantras, mandalas and kundalini yoga in its practice. Chakra in Shakta tantrism means circle, a "energy center" within, as well as being a term of group rituals such as in "chakra-puja" (worship within a circle) which may or may not involve tantra practice. The cakra-based system is one part of the meditative exercises that came to be known as laya yoga.

Beyond its original Shakta milieu, various sub-traditions within the Shaiva and Vaishnava schools of Hinduism also developed texts and practices on Nadi and Chakra systems. Certain modern Hindu groups also utilize a technique of circular energy work based on the chakras known as "kriya yoga". Followers of this practice include the Bihar School of Yoga and Self Realization Fellowship, and practitioners are known as "kriyaban". Although Paramahansa Yogananda claimed this was the same technique taught as kriya yoga by Patañjali in the Yoga Sūtras and by Krishna in the Bhagavad Gita (as karma yoga), Swami Satyananda of the Bihar school disagreed with this assessment and acknowledged the similarities between kriya and taoist inner orbit practices. Both schools claim the technique is taught in every age by an avatar of god known as Babaji. The historicity of its techniques in India prior to the early twentieth century are not well established. It believed by its practitioners to activate the chakras and stimulate faster spiritual development.

The esoteric traditions in Buddhism generally teach four chakras. These are the Manipura, the Anahata, the Visuddha and the Usnisa Kamala. In another version, these four are the Nirmana, the Dharma, the Sambhoga and the Mahasukha (respectively corresponding to the Shaiva tantra school's following four of seven chakra: Svadhisthana, the Anahata, the Visuddha and the Sahasrara). However, depending on the meditational tradition, these vary between three and six.

Chakras play an important role in the Tibetan Buddhism in completion stage practices. It is practiced to bring the subtle winds of the body into the central channel, to realise the clear light of bliss and emptiness, and to attain Buddhahood.

According to Geoffrey Samuel, the Tibetan and esoteric Buddhist traditions developed cakra and nadi as "central to their soteriological process". The theories were coupled with a tradition of physical exercises, now sometimes called "yantra yoga", but traditionally referred to a "'phrul 'khor" in Tibetan. This style of yoga emphasizes visualizations and internal practices, somewhat similar to the "kriya yoga" practices in some sub-traditions of Hinduism. The differences between the two styles, according to Geoffrey, has been that the Tibetan tradition focussed more on "offering rituals to benign deities" already prevalent in Tibet, while the Indic traditions focussed more on the internal practices linked to subtle body concepts. The "yantra yoga" at the Completion Stage of esoteric Buddhism typically followed its "deity-yoga" practices of the Generation Stage.

The Chakra in the Tibetan practice are considered psycho-physical centers, each associated with a cosmic Buddha.

Chakras, according to the Bon tradition, influence the quality of experience, because movement of vayu cannot be separated from experience. Each of the six major chakras is linked to experiential qualities of one of the six realms of existence.

The tsa lung practices such as those embodied in Trul khor lineages open channels so "lung" (the Tibetan term for vayu) may move without obstruction. Yoga opens chakras and evokes positive qualities associated with a particular chakra. In the hard drive analogy, the screen is cleared and a file is called up that contains positive, supportive qualities. A bīja (seed syllable) is used both as a password that evokes the positive quality and the armour that sustains the quality.

Tantric practice is said to eventually transform all experience into bliss. The practice aims to liberate from negative conditioning and leads to control over perception and cognition.

Qigong () also relies on a similar model of the human body as an esoteric energy system, except that it involves the circulation of qì (, also "ki") or life-energy. The qì, equivalent to the Hindu prana, flows through the energy channels called meridians, equivalent to the nadi, but two other energies are also important: "jīng", or primordial essence, and "shén", or spirit energy.

In the principle circuit of qì, called the microcosmic orbit, energy rises up a main meridian along the spine, but also comes back down the front torso. Throughout its cycle it enters various dantian (elixir fields) which act as furnaces, where the types of energy in the body (jing, qi and shen) are progressively refined. These dantian play a very similar role to that of chakras. The number of dantian varies depending on the system; the navel dantian is the most well-known, but there is usually a dantian located at the heart and between the eyebrows. The lower dantian at or below the navel transforms essence, or jīng, into qì. The middle dantian in the middle of the chest transforms qì into shén, or spirit, and the higher dantian at the level of the forehead (or at the top of the head), transforms shen into wuji, infinite space of void.

Traditional spirituality in the Malay Archipelago borrows heavily from Hindu-Buddhist concepts. In Malay and Indonesian metaphysical theory, the chakras' energy rotates outwards along diagonal lines. Defensive energy emits outwards from the centre line, while offensive energy moves inwards from the sides of the body. This can be applied to energy-healing, meditation, or martial arts. Silat practitioners learn to harmonise their movements with the chakras, thereby increasing the power and effectiveness of attacks and movements.

The more common and most studied esoteric system incorporates six major chakras along with a seventh center generally not regarded as a chakra. These points are arranged vertically along the axial channel (sushumna nadi in Hindu texts, Avadhuti in some Buddhist texts). It was this chakra system that was translated in early 20th century by Sir John Woodroffe (also called Arthur Avalon) in the text "The Serpent Power". Avalon translated the Hindu text "Ṣaṭ-Cakra-Nirūpaṇa" meaning the examination (nirūpaṇa) of the six (ṣaṭ) chakras (cakra).

The Chakras are traditionally considered meditation aids. The yogi progresses from lower chakras to the highest chakra blossoming in the crown of the head, internalizing the journey of spiritual ascent. In both the Hindu and Buddhist kundalini or candali traditions, the chakras are pierced by a dormant energy residing near or in the lowest chakra. In Hindu texts she is known as Kundalini, while in Buddhist texts she is called Candali or Tummo (Tibetan: "gtum mo", "fierce one").

Below is a common new age description of these six chakras and the seventh point known as sahasrara. This new age version incorporates the newtonian colors that were completely unknown when these systems were created. The actual colors for the chakras vary from text to text and do not conform to the newtonian spectrum:

Many systems include additional chakras including "talu" "bindu" "manas" and "dvadashanta" chakra.

In 1918, the translation of two Indian texts: the "Ṣaṭ-Cakra-Nirūpaṇa" and the "Pādukā-Pañcaka", by Sir John Woodroffe, alias Arthur Avalon, in a book titled "The Serpent Power" introduced the shakta theory of seven main chakras in the West. This book is extremely detailed and complex, and later the ideas were developed into the predominant Western view of the chakras by C. W. Leadbeater in his book "The Chakras". Many of the views which directed Leadbeater's understanding of the chakras were influenced by previous theosophist authors, in particular Johann Georg Gichtel, a disciple of Jakob Böhme, and his book "Theosophia Practica (1696)", in which Gichtel directly refers to inner "force centres", a concept reminiscent of the chakras.

A completely separate contemplative movement within the Eastern Orthodox Church is Hesychasm, a form of Christian meditation. Comparisons have been made between the Hesychastic centres of prayer and the position of the chakras. Particular emphasis is placed upon the heart area. However, there is no talk about these centres as having any sort of metaphysical existence. Far more than in any of the cases discussed above, the centres are simply places to focus the concentration during prayer.

In "Anatomy of the Spirit" (1996), Caroline Myss describes the function of chakras as follows: "Every thought and experience you've ever had in your life gets filtered through these chakra databases. Each event is recorded into your cells...". The chakras are described as being aligned in an ascending column from the base of the spine to the top of the head. New Age practices often associate each chakra with a certain colour. In various traditions, chakras are associated with multiple physiological functions, an aspect of consciousness, a classical element, and other distinguishing characteristics. They are visualised as lotuses or flowers with a different number of petals in every chakra.

The chakras are thought to vitalise the physical body and to be associated with interactions of a physical, emotional and mental nature. They are considered of life energy or prana (which New Age belief equates with "shakti", "qi" in Chinese, "ki "in Japanese, "koach-ha-guf" in Hebrew, "bios "in Greek, and "aether"" "in both Greek and English), which is thought to flow among them along pathways called nadi. The function of the chakras is to spin and draw in this energy to keep the spiritual, mental, emotional and physical health of the body in balance.

Rudolf Steiner considered the chakra system to be dynamic and evolving. He suggested that this system has become different for modern people than it was in ancient times and that it will, in turn, be radically different in future times. Steiner described a sequence of development that begins with the upper chakras and moves down, rather than moving in the opposite direction. He gave suggestions on how to develop the chakras through disciplining thoughts, feelings, and will.

According to Florin Lowndes, a "spiritual student" can further develop and deepen or elevate thinking consciousness when taking the step from the "ancient path" of schooling to the "new path" represented by Steiner's "The Philosophy of Freedom".

Chakras and their importance are posited to reside in the psyche. However, there are those who believe that chakras have a physical manifestation as well. Gary Osborn, for instance, has described the chakras as metaphysical counterparts to the endocrine glands, while Anodea Judith noted a marked similarity between the positions of the two and the roles described for each. Stephen Sturgess also links the lower six chakras to specific nerve plexuses along the spinal cord as well as glands. C.W. Leadbeater associated the Ajna chakra with the pineal gland, which is a part of the endocrine system. These associations remain speculative, however, and have yet to be empirically validated.



</doc>
<doc id="6910" url="https://en.wikipedia.org/wiki?curid=6910" title="Cloning">
Cloning

In biology, cloning is the process of producing similar populations of genetically identical individuals that occurs in nature when organisms such as bacteria, insects, plants or animals reproduce asexually. Cloning in biotechnology refers to processes used to create copies of DNA fragments (molecular cloning), cells (cell cloning), or organisms (organism cloning). The term also refers to the production of multiple copies of a product such as digital media or software.

The term clone, invented by J. B. S. Haldane, is derived from the Ancient Greek word κλών "klōn", "twig", referring to the process whereby a new plant can be created from a twig. In horticulture, the spelling "clon" was used until the twentieth century; the final "e" came into use to indicate the vowel is a "long o" instead of a "short o". Since the term entered the popular lexicon in a more general context, the spelling "clone" has been used exclusively.

In botany, the term lusus was traditionally used.

Cloning is a natural form of reproduction that has allowed life forms to spread for more than 50 thousand years. It is the reproduction method used by plants, fungi, and bacteria, and is also the way that clonal colonies reproduce themselves. Examples of these organisms include blueberry plants, hazel trees, the Pando trees, the Kentucky coffeetree, "Myrica"s, and the American sweetgum.

Molecular cloning refers to the process of making multiple molecules. Cloning is commonly used to amplify DNA fragments containing whole genes, but it can also be used to amplify any DNA sequence such as promoters, non-coding sequences and randomly fragmented DNA. It is used in a wide array of biological experiments and practical applications ranging from genetic fingerprinting to large scale protein production. Occasionally, the term cloning is misleadingly used to refer to the identification of the chromosomal location of a gene associated with a particular phenotype of interest, such as in positional cloning. In practice, localization of the gene to a chromosome or genomic region does not necessarily enable one to isolate or amplify the relevant genomic sequence. To amplify any DNA sequence in a living organism, that sequence must be linked to an origin of replication, which is a sequence of DNA capable of directing the propagation of itself and any linked sequence. However, a number of other features are needed, and a variety of specialised cloning vectors (small piece of DNA into which a foreign DNA fragment can be inserted) exist that allow protein production, affinity tagging, single stranded RNA or DNA production and a host of other molecular biology tools.

Cloning of any DNA fragment essentially involves four steps

Although these steps are invariable among cloning procedures a number of alternative routes can be selected; these are summarized as a "cloning strategy".

Initially, the DNA of interest needs to be isolated to provide a DNA segment of suitable size. Subsequently, a ligation procedure is used where the amplified fragment is inserted into a vector (piece of DNA). The vector (which is frequently circular) is linearised using restriction enzymes, and incubated with the fragment of interest under appropriate conditions with an enzyme called DNA ligase. Following ligation the vector with the insert of interest is transfected into cells. A number of alternative techniques are available, such as chemical sensitivation of cells, electroporation, optical injection and biolistics. Finally, the transfected cells are cultured. As the aforementioned procedures are of particularly low efficiency, there is a need to identify the cells that have been successfully transfected with the vector construct containing the desired insertion sequence in the required orientation. Modern cloning vectors include selectable antibiotic resistance markers, which allow only cells in which the vector has been transfected, to grow. Additionally, the cloning vectors may contain colour selection markers, which provide blue/white screening (alpha-factor complementation) on X-gal medium. Nevertheless, these selection steps do not absolutely guarantee that the DNA insert is present in the cells obtained. Further investigation of the resulting colonies must be required to confirm that cloning was successful. This may be accomplished by means of PCR, restriction fragment analysis and/or DNA sequencing.

Cloning a cell means to derive a population of cells from a single cell. In the case of unicellular organisms such as bacteria and yeast, this process is remarkably simple and essentially only requires the inoculation of the appropriate medium. However, in the case of cell cultures from multi-cellular organisms, cell cloning is an arduous task as these cells will not readily grow in standard media.

A useful tissue culture technique used to clone distinct lineages of cell lines involves the use of cloning rings (cylinders). In this technique a single-cell suspension of cells that have been exposed to a mutagenic agent or drug used to drive selection is plated at high dilution to create isolated colonies, each arising from a single and potentially clonal distinct cell. At an early growth stage when colonies consist of only a few cells, sterile polystyrene rings (cloning rings), which have been dipped in grease, are placed over an individual colony and a small amount of trypsin is added. Cloned cells are collected from inside the ring and transferred to a new vessel for further growth.

Somatic-cell nuclear transfer, known as SCNT, can also be used to create embryos for research or therapeutic purposes. The most likely purpose for this is to produce embryos for use in stem cell research. This process is also called "research cloning" or "therapeutic cloning." The goal is not to create cloned human beings (called "reproductive cloning"), but rather to harvest stem cells that can be used to study human development and to potentially treat disease. While a clonal human blastocyst has been created, stem cell lines are yet to be isolated from a clonal source.

Therapeutic cloning is achieved by creating embryonic stem cells in the hopes of treating diseases such as diabetes and Alzheimer's. The process begins by removing the nucleus (containing the DNA) from an egg cell and inserting a nucleus from the adult cell to be cloned. In the case of someone with Alzheimer's disease, the nucleus from a skin cell of that patient is placed into an empty egg. The reprogrammed cell begins to develop into an embryo because the egg reacts with the transferred nucleus. The embryo will become genetically identical to the patient. The embryo will then form a blastocyst which has the potential to form/become any cell in the body.

The reason why SCNT is used for cloning is because somatic cells can be easily acquired and cultured in the lab. This process can either add or delete specific genomes of farm animals. A key point to remember is that cloning is achieved when the oocyte maintains its normal functions and instead of using sperm and egg genomes to replicate, the oocyte is inserted into the donor’s somatic cell nucleus. The oocyte will react on the somatic cell nucleus, the same way it would on sperm cells.

The process of cloning a particular farm animal using SCNT is relatively the same for all animals. The first step is to collect the somatic cells from the animal that will be cloned. The somatic cells could be used immediately or stored in the laboratory for later use. The hardest part of SCNT is removing maternal DNA from an oocyte at metaphase II. Once this has been done, the somatic nucleus can be inserted into an egg cytoplasm. This creates a one-cell embryo. The grouped somatic cell and egg cytoplasm are then introduced to an electrical current. This energy will hopefully allow the cloned embryo to begin development. The successfully developed embryos are then placed in surrogate recipients, such as a cow or sheep in the case of farm animals.

SCNT is seen as a good method for producing agriculture animals for food consumption. It successfully cloned sheep, cattle, goats, and pigs. Another benefit is SCNT is seen as a solution to clone endangered species that are on the verge of going extinct. However, stresses placed on both the egg cell and the introduced nucleus can be enormous, which led to a high loss in resulting cells in early research. For example, the cloned sheep Dolly was born after 277 eggs were used for SCNT, which created 29 viable embryos. Only three of these embryos survived until birth, and only one survived to adulthood. As the procedure could not be automated, and had to be performed manually under a microscope, SCNT was very resource intensive. The biochemistry involved in reprogramming the differentiated somatic cell nucleus and activating the recipient egg was also far from being well understood. However, by 2014 researchers were reporting cloning success rates of seven to eight out of ten and in 2016, a Korean Company Sooam Biotech was reported to be producing 500 cloned embryos per day.

In SCNT, not all of the donor cell's genetic information is transferred, as the donor cell's mitochondria that contain their own mitochondrial DNA are left behind. The resulting hybrid cells retain those mitochondrial structures which originally belonged to the egg. As a consequence, clones such as Dolly that are born from SCNT are not perfect copies of the donor of the nucleus.

Organism cloning (also called reproductive cloning) refers to the procedure of creating a new multicellular organism, genetically identical to another. In essence this form of cloning is an asexual method of reproduction, where fertilization or inter-gamete contact does not take place. Asexual reproduction is a naturally occurring phenomenon in many species, including most plants (see vegetative reproduction) and some insects. Scientists have made some major achievements with cloning, including the asexual reproduction of sheep and cows. There is a lot of ethical debate over whether or not cloning should be used. However, cloning, or asexual propagation, has been common practice in the horticultural world for hundreds of years.

The term "clone" is used in horticulture to refer to descendants of a single plant which were produced by vegetative reproduction or apomixis. Many horticultural plant cultivars are clones, having been derived from a single individual, multiplied by some process other than sexual reproduction. As an example, some European cultivars of grapes represent clones that have been propagated for over two millennia. Other examples are potato and banana. Grafting can be regarded as cloning, since all the shoots and branches coming from the graft are genetically a clone of a single individual, but this particular kind of cloning has not come under ethical scrutiny and is generally treated as an entirely different kind of operation.

Many trees, shrubs, vines, ferns and other herbaceous perennials form clonal colonies naturally. Parts of an individual plant may become detached by fragmentation and grow on to become separate clonal individuals. A common example is in the vegetative reproduction of moss and liverwort gametophyte clones by means of gemmae. Some vascular plants e.g. dandelion and certain viviparous grasses also form seeds asexually, termed apomixis, resulting in clonal populations of genetically identical individuals.

Clonal derivation exists in nature in some animal species and is referred to as parthenogenesis (reproduction of an organism by itself without a mate). This is an asexual form of reproduction that is only found in females of some insects, crustaceans, nematodes, fish (for example the hammerhead shark), the Komodo dragon and lizards. The growth and development occurs without fertilization by a male. In plants, parthenogenesis means the development of an embryo from an unfertilized egg cell, and is a component process of apomixis. In species that use the XY sex-determination system, the offspring will always be female. An example is the little fire ant ("Wasmannia auropunctata"), which is native to Central and South America but has spread throughout many tropical environments.

Artificial cloning of organisms may also be called "reproductive cloning".

Hans Spemann, a German embryologist was awarded a Nobel Prize in Physiology or Medicine in 1935 for his discovery of the effect now known as embryonic induction, exercised by various parts of the embryo, that directs the development of groups of cells into particular tissues and organs. In 1928 he and his student, Hilde Mangold, were the first to perform somatic-cell nuclear transfer using amphibian embryos – one of the first steps towards cloning.

Reproductive cloning generally uses "somatic cell nuclear transfer" (SCNT) to create animals that are genetically identical. This process entails the transfer of a nucleus from a donor adult cell (somatic cell) to an egg from which the nucleus has been removed, or to a cell from a blastocyst from which the nucleus has been removed. If the egg begins to divide normally it is transferred into the uterus of the surrogate mother. Such clones are not strictly identical since the somatic cells may contain mutations in their nuclear DNA. Additionally, the mitochondria in the cytoplasm also contains DNA and during SCNT this mitochondrial DNA is wholly from the cytoplasmic donor's egg, thus the mitochondrial genome is not the same as that of the nucleus donor cell from which it was produced. This may have important implications for cross-species nuclear transfer in which nuclear-mitochondrial incompatibilities may lead to death.

Artificial "embryo splitting" or "embryo twinning", a technique that creates monozygotic twins from a single embryo, is not considered in the same fashion as other methods of cloning. During that procedure, a donor embryo is split in two distinct embryos, that can then be transferred via embryo transfer. It is optimally performed at the 6- to 8-cell stage, where it can be used as an expansion of IVF to increase the number of available embryos. If both embryos are successful, it gives rise to monozygotic (identical) twins.

Dolly, a Finn-Dorset ewe, was the first mammal to have been successfully cloned from an adult somatic cell. Dolly was formed by taking a cell from the udder of her 6-year old biological mother. Dolly's embryo was created by taking the cell and inserting it into a sheep ovum. It took 434 attempts before an embryo was successful. The embryo was then placed inside a female sheep that went through a normal pregnancy. She was cloned at the Roslin Institute in Scotland by British scientists Sir Ian Wilmut and Keith Campbell and lived there from her birth in 1996 until her death in 2003 when she was six. She was born on 5 July 1996 but not announced to the world until 22 February 1997. Her stuffed remains were placed at Edinburgh's Royal Museum, part of the National Museums of Scotland.

Dolly was publicly significant because the effort showed that genetic material from a specific adult cell, programmed to express only a distinct subset of its genes, can be reprogrammed to grow an entirely new organism. Before this demonstration, it had been shown by John Gurdon that nuclei from differentiated cells could give rise to an entire organism after transplantation into an enucleated egg. However, this concept was not yet demonstrated in a mammalian system.

The first mammalian cloning (resulting in Dolly the sheep) had a success rate of 29 embryos per 277 fertilized eggs, which produced three lambs at birth, one of which lived. In a bovine experiment involving 70 cloned calves, one-third of the calves died young. The first successfully cloned horse, Prometea, took 814 attempts. Notably, although the first clones were frogs, no adult cloned frog has yet been produced from a somatic adult nucleus donor cell.

There were early claims that Dolly the sheep had pathologies resembling accelerated aging. Scientists speculated that Dolly's death in 2003 was related to the shortening of telomeres, DNA-protein complexes that protect the end of linear chromosomes. However, other researchers, including Ian Wilmut who led the team that successfully cloned Dolly, argue that Dolly's early death due to respiratory infection was unrelated to deficiencies with the cloning process. This idea that the nuclei have not irreversibly aged was shown in 2013 to be true for mice.

Dolly was named after performer Dolly Parton because the cells cloned to make her were from a mammary gland cell, and Parton is known for her ample cleavage.

The modern cloning techniques involving nuclear transfer have been successfully performed on several species. Notable experiments include:

Human cloning is the creation of a genetically identical copy of a human. The term is generally used to refer to artificial human cloning, which is the reproduction of human cells and tissues. It does not refer to the natural conception and delivery of identical twins. The possibility of human cloning has raised controversies. These ethical concerns have prompted several nations to pass legislature regarding human cloning and its legality. As of right now, scientists have no intention of trying to clone people and they believe their results should spark a wider discussion about the laws and regulations the world needs to regulate cloning.

Two commonly discussed types of theoretical human cloning are "therapeutic cloning" and "reproductive cloning". Therapeutic cloning would involve cloning cells from a human for use in medicine and transplants, and is an active area of research, but is not in medical practice anywhere in the world, as of 2014. Two common methods of therapeutic cloning that are being researched are somatic-cell nuclear transfer and, more recently, pluripotent stem cell induction. Reproductive cloning would involve making an entire cloned human, instead of just specific cells or tissues.

There are a variety of ethical positions regarding the possibilities of cloning, especially human cloning. While many of these views are religious in origin, the questions raised by cloning are faced by secular perspectives as well. Perspectives on human cloning are theoretical, as human therapeutic and reproductive cloning are not commercially used; animals are currently cloned in laboratories and in livestock production.

Advocates support development of therapeutic cloning in order to generate tissues and whole organs to treat patients who otherwise cannot obtain transplants, to avoid the need for immunosuppressive drugs, and to stave off the effects of aging. Advocates for reproductive cloning believe that parents who cannot otherwise procreate should have access to the technology.

Opponents of cloning have concerns that technology is not yet developed enough to be safe and that it could be prone to abuse (leading to the generation of humans from whom organs and tissues would be harvested), as well as concerns about how cloned individuals could integrate with families and with society at large.

Religious groups are divided, with some opposing the technology as usurping "God's place" and, to the extent embryos are used, destroying a human life; others support therapeutic cloning's potential life-saving benefits.

Cloning of animals is opposed by animal-groups due to the number of cloned animals that suffer from malformations before they die, and while food from cloned animals has been approved by the US FDA, its use is opposed by groups concerned about food safety.

Cloning, or more precisely, the reconstruction of functional DNA from extinct species has, for decades, been a dream. Possible implications of this were dramatized in the 1984 novel "Carnosaur" and the 1990 novel "Jurassic Park". The best current cloning techniques have an average success rate of 9.4 percent (and as high as 25 percent) when working with familiar species such as mice, while cloning wild animals is usually less than 1 percent successful. Several tissue banks have come into existence, including the "Frozen Zoo" at the San Diego Zoo, to store frozen tissue from the world's rarest and most endangered species.

In 2001, a cow named Bessie gave birth to a cloned Asian gaur, an endangered species, but the calf died after two days. In 2003, a banteng was successfully cloned, followed by three African wildcats from a thawed frozen embryo. These successes provided hope that similar techniques (using surrogate mothers of another species) might be used to clone extinct species. Anticipating this possibility, tissue samples from the last "bucardo" (Pyrenean ibex) were frozen in liquid nitrogen immediately after it died in 2000. Researchers are also considering cloning endangered species such as the giant panda and cheetah.

In 2002, geneticists at the Australian Museum announced that they had replicated DNA of the thylacine (Tasmanian tiger), at the time extinct for about 65 years, using polymerase chain reaction. However, on 15 February 2005 the museum announced that it was stopping the project after tests showed the specimens' DNA had been too badly degraded by the (ethanol) preservative. On 15 May 2005 it was announced that the thylacine project would be revived, with new participation from researchers in New South Wales and Victoria.

In 2003, for the first time, an extinct animal, the Pyrenean ibex mentioned above was cloned, at the Centre of Food Technology and Research of Aragon, using the preserved frozen cell nucleus of the skin samples from 2001 and domestic goat egg-cells. The ibex died shortly after birth due to physical defects in its lungs.

One of the most anticipated targets for cloning was once the woolly mammoth, but attempts to extract DNA from frozen mammoths have been unsuccessful, though a joint Russo-Japanese team is currently working toward this goal. In January 2011, it was reported by Yomiuri Shimbun that a team of scientists headed by Akira Iritani of Kyoto University had built upon research by Dr. Wakayama, saying that they will extract DNA from a mammoth carcass that had been preserved in a Russian laboratory and insert it into the egg cells of an African elephant in hopes of producing a mammoth embryo. The researchers said they hoped to produce a baby mammoth within six years. It was noted, however that the result, if possible, would be an elephant-mammoth hybrid rather than a true mammoth. Another problem is the survival of the reconstructed mammoth: ruminants rely on a symbiosis with specific microbiota in their stomachs for digestion.

Scientists at the University of Newcastle and University of New South Wales announced in March 2013 that the very recently extinct gastric-brooding frog would be the subject of a cloning attempt to resurrect the species.

Many such "de-extinction" projects are described in the Long Now Foundation's Revive and Restore Project.

After an eight-year project involving the use of a pioneering cloning technique, Japanese researchers created 25 generations of healthy cloned mice with normal lifespans, demonstrating that clones are not intrinsically shorter-lived than naturally born animals. Other sources have noted that the offspring of clones tend to be healthier than the original clones and indistinguishable from animals produced naturally.

In a detailed study released in 2016 and less detailed studies by others suggest that once cloned animals get past the first month or two of life they are generally healthy. However, early pregnancy loss and neonatal losses are still greater with cloning than natural conception or assisted reproduction (IVF). Current research endeavors are attempting to overcome this problem.

Discussion of cloning in the popular media often presents the subject negatively. In an article in the 8 November 1993 article of "Time", cloning was portrayed in a negative way, modifying Michelangelo's "Creation of Adam" to depict Adam with five identical hands. "Newsweek"'s 10 March 1997 issue also critiqued the ethics of human cloning, and included a graphic depicting identical babies in beakers.

The concept of cloning has featured a wide variety of science fiction works. An early fictional depiction of cloning is Bokanovsky's Process which features in Aldous Huxley's 1931 dystopian novel "Brave New World". The process is applied to fertilized human eggs "in vitro", causing them to split into identical genetic copies of the original. Following renewed interest in cloning in the 1950s, the subject was explored further in works such as Poul Anderson's 1953 story "UN-Man", which describes a technology called "exogenesis", and Gordon Rattray Taylor's book "The Biological Time Bomb", which popularised the term "cloning" in 1963.

Cloning is a recurring theme in a number of contemporary science fiction films, ranging from action films such as "Jurassic Park" (1993), "Alien Resurrection" (1997), "The 6th Day" (2000), "Resident Evil" (2002), "" (2002) and "The Island" (2005), to comedies such as Woody Allen's 1973 film "Sleeper".

The process of cloning is represented in different ways in fiction. Many works depict the artificial creation of humans by a method of growing cells from a tissue or DNA sample; the process may instantaneous, or take place through a slow process of growing human embryos in artificial wombs. Science fiction films such as "The Matrix" and "Star Wars: Episode II – Attack of the Clones" have featured scenes of human foetuses being cultured on an industrial scale in mechanical tanks. In the long-running British television series "Doctor Who", the Fourth Doctor and his companion Leela were cloned in a matter of seconds from DNA samples ("The Invisible Enemy", 1977) and then — in an apparent homage to the 1966 film "Fantastic Voyage" — shrunk to microscopic size in order to enter the Doctor's body to combat an alien virus. The clones in this story are short-lived, and can only survive a matter of minutes before they expire.

Cloning humans from body parts is also a common theme in science fiction. Cloning features strongly among the science fiction conventions parodied in Woody Allen's "Sleeper", the plot of which centres around an attempt to clone an assassinated dictator from his disembodied nose. In the 2008 "Doctor Who" story "Journey's End", a duplicate version of the Tenth Doctor spontaneously grows from his severed hand, which had been cut off in a sword fight during an earlier episode.

After the death of her beloved dog in late 2017, Barbra Streisand announced that she had cloned her 14-year old Coton de Tulear dog Samantha, and was now "waiting for [the two cloned pups] to get older so [she] can see if they have [Samantha's] brown eyes and her seriousness." The whole operation cost $50,000, however the name of the company she worked with has not been released.

Science fiction has used cloning, most commonly and specifically human cloning, due to the fact that it brings up controversial questions of identity. "A Number" is a 2002 play by English playwright Caryl Churchill which addresses the subject of human cloning and identity, especially nature and nurture. The story, set in the near future, is structured around the conflict between a father (Salter) and his sons (Bernard 1, Bernard 2, and Michael Black) – two of whom are clones of the first one. "A Number" was adapted by Caryl Churchill for television, in a co-production between the BBC and HBO Films.

In 2012, a Japanese television series named "Bunshin" was created. The story's main character, Mariko, is a woman studying child welfare in Hokkaido. She grew up always doubtful about the love from her mother, who looked nothing like her and who died nine years before. One day, she finds some of her mother's belongings at a relative's house, and heads to Tokyo to seek out the truth behind her birth. She later discovered that she was a clone.

In the 2013 television series "Orphan Black", cloning is used as a scientific study on the behavioral adaptation of the clones. In a similar vein, the book "The Double" by Nobel Prize winner José Saramago explores the emotional experience of a man who discovers that he is a clone.

Cloning has been used in fiction as a way of recreating historical figures. In the 1976 Ira Levin novel "The Boys from Brazil" and its 1978 film adaptation, Josef Mengele uses cloning to create copies of Adolf Hitler.

In Michael Chrichton's 1990 novel "Jurassic Park", which spawned a series of "Jurassic Park" feature films, a bioengineering company develops a technique to resurrect extinct species of dinosaurs by creating cloned creatures using DNA extracted from fossils. The cloned dinosaurs are used to populate the eponymous Jurassic Park wildlife park for the entertainment of visitors. The scheme goes disastrously wrong when the dinosaurs escape their enclosures. Despite being selectively cloned as females to prevent them from breeding, the dinosaurs develop the ability to reproduce through parthenogenesis.

The use of cloning for military purposes has also been explored in several works. In "Doctor Who", an alien race of armour-clad, warlike beings called Sontarans was introduced in the 1973 serial "The Time Warrior". Sontarans are depicted as squat, bald creatures who have been genetically engineered for combat. Their weak spot is a "probic vent", a small socket at the back of their neck which is associated with the cloning process. The concept of cloned soldiers being bred for combat was revisited in "The Doctor's Daughter" (2008), when the Doctor's DNA is used to create a female warrior called Jenny.

The 1977 film "Star Wars" was set against the backdrop of a historical conflict called the Clone Wars. The events of this war were not fully explored until the prequel films "" (2002) and "" (2005), which depict a space war waged by a massive army of heavily armoured clone troopers that leads to the foundation of the Galactic Empire. Cloned soldiers are "manufactured" on an industrial scale, genetically conditioned for obedience and combat effectiveness. It is also revealed that the popular character Boba Fett originated as a clone of Jango Fett, a mercenary who served as the genetic template for the clone troopers.

Cloning has appeared in many video games. In Metal Gear Solid, the characters Solid Snake and Liquid Snake were born in a secret project as cloned soldiers. In Halo, cloning technology is shown to recreate organs. In addition, the UNSC uses cloning when it abducts children to train as supersoldiers. Here, non-clone children are trained as soldiers while the clones covertly replace the abducted children at home.

A recurring sub-theme of cloning fiction is the use of clones as a supply of organs for transplantation. The 2005 Kazuo Ishiguro novel "Never Let Me Go" and the 2010 film adaption are set in an alternate history in which cloned humans are created for the sole purpose of providing organ donations to naturally born humans, despite the fact that they are fully sentient and self-aware. The 2005 film "The Island" revolves around a similar plot, with the exception that the clones are unaware of the reason for their existence. In Raymond Han's 2017 novel, The Mind Clones Trilogy, a dictator who suffered a terminal illness sought to implant his mind clone into his son's mind so that he could continue to rule the country. In another part of the trilogy, usurpers plotted to replace members of the Chinese Politburo Standing Committee using look-alike human clones.

The exploitation of human clones for dangerous and undesirable work was examined in the 2009 British science fiction film "Moon". In the futuristic novel "Cloud Atlas" and subsequent film, one of the story lines focuses on a genetically-engineered fabricant clone named Sonmi~451 who is one of millions raised in an artificial "wombtank," destined to serve from birth. She is one of thousands of clones created for manual and emotional labor; Sonmi herself works as a server in a restaurant. She later discovers that the sole source of food for clones, called 'Soap', is manufactured from the clones themselves.




</doc>
<doc id="6911" url="https://en.wikipedia.org/wiki?curid=6911" title="Cellulose">
Cellulose

Cellulose is an organic compound with the formula , a polysaccharide consisting of a linear chain of several hundred to many thousands of β(1→4) linked -glucose units. Cellulose is an important structural component of the primary cell wall of green plants, many forms of algae and the oomycetes. Some species of bacteria secrete it to form biofilms. Cellulose is the most abundant organic polymer on Earth. The cellulose content of cotton fiber is 90%, that of wood is 40–50%, and that of dried hemp is approximately 57%.

Cellulose is mainly used to produce paperboard and paper. Smaller quantities are converted into a wide variety of derivative products such as cellophane and rayon. Conversion of cellulose from energy crops into biofuels such as cellulosic ethanol is under investigation as an alternative fuel source. Cellulose for industrial use is mainly obtained from wood pulp and cotton.

Some animals, particularly ruminants and termites, can digest cellulose with the help of symbiotic micro-organisms that live in their guts, such as "Trichonympha". In human nutrition, cellulose is a non-digestible constituent of insoluble dietary fiber, acting as a hydrophilic bulking agent for feces and potentially aiding in defecation.

Cellulose was discovered in 1838 by the French chemist Anselme Payen, who isolated it from plant matter and determined its chemical formula. Cellulose was used to produce the first successful thermoplastic polymer, celluloid, by Hyatt Manufacturing Company in 1870. Production of rayon ("artificial silk") from cellulose began in the 1890s and cellophane was invented in 1912. Hermann Staudinger determined the polymer structure of cellulose in 1920. The compound was first chemically synthesized (without the use of any biologically derived enzymes) in 1992, by Kobayashi and Shoda.

Cellulose has no taste, is odorless, is hydrophilic with the contact angle of 20–30 degrees, is insoluble in water and most organic solvents, is chiral and is biodegradable. It was shown to melt at 467 °C in 2016. It can be broken down chemically into its glucose units by treating it with concentrated mineral acids at high temperature.

Cellulose is derived from -glucose units, which condense through β(1→4)-glycosidic bonds. This linkage motif contrasts with that for α(1→4)-glycosidic bonds present in starch and glycogen. Cellulose is a straight chain polymer: unlike starch, no coiling or branching occurs, and the molecule adopts an extended and rather stiff rod-like conformation, aided by the equatorial conformation of the glucose residues. The multiple hydroxyl groups on the glucose from one chain form hydrogen bonds with oxygen atoms on the same or on a neighbor chain, holding the chains firmly together side-by-side and forming "microfibrils" with high tensile strength. This confers tensile strength in cell walls, where cellulose microfibrils are meshed into a polysaccharide "matrix".

Compared to starch, cellulose is also much more crystalline. Whereas starch undergoes a crystalline to amorphous transition when heated beyond 60–70 °C in water (as in cooking), cellulose requires a temperature of 320 °C and pressure of 25 MPa to become amorphous in water.

Several different crystalline structures of cellulose are known, corresponding to the location of hydrogen bonds between and within strands. Natural cellulose is cellulose I, with structures I and I. Cellulose produced by bacteria and algae is enriched in I while cellulose of higher plants consists mainly of I. Cellulose in regenerated cellulose fibers is cellulose II. The conversion of cellulose I to cellulose II is irreversible, suggesting that cellulose I is metastable and cellulose II is stable. With various chemical treatments it is possible to produce the structures cellulose III and cellulose IV.

Many properties of cellulose depend on its chain length or degree of polymerization, the number of glucose units that make up one polymer molecule. Cellulose from wood pulp has typical chain lengths between 300 and 1700 units; cotton and other plant fibers as well as bacterial cellulose have chain lengths ranging from 800 to 10,000 units. Molecules with very small chain length resulting from the breakdown of cellulose are known as cellodextrins; in contrast to long-chain cellulose, cellodextrins are typically soluble in water and organic solvents.

Plant-derived cellulose is usually found in a mixture with hemicellulose, lignin, pectin and other substances, while bacterial cellulose is quite pure, has a much higher water content and higher tensile strength due to higher chain lengths.

Cellulose is soluble in Schweizer's reagent, cupriethylenediamine (CED), cadmiumethylenediamine (Cadoxen), "N"-methylmorpholine "N"-oxide, and lithium chloride / dimethylacetamide. This is used in the production of regenerated celluloses (such as viscose and cellophane) from dissolving pulp. Cellulose is also soluble in many kinds of ionic liquids.

Cellulose consists of crystalline and amorphous regions. By treating it with strong acid, the amorphous regions can be broken up, thereby producing nanocrystalline cellulose, a novel material with many desirable properties. Recently, nanocrystalline cellulose was used as the filler phase in bio-based polymer matrices to produce nanocomposites with superior thermal and mechanical properties.

Given a cellulose-containing material, the carbohydrate portion that does not dissolve in a 17.5% solution of sodium hydroxide at 20 °C is "α cellulose", which is true cellulose. Acidification of the extract precipitates "β cellulose". The portion that dissolves in base but does not precipitate with acid is "γ cellulose".

Cellulose can be assayed using a method described by Updegraff in 1969, where the fiber is dissolved in acetic and nitric acid to remove lignin, hemicellulose, and xylosans. The resulting cellulose is allowed to react with anthrone in sulfuric acid. The resulting coloured compound is assayed spectrophotometrically at a wavelength of approximately 635 nm.

In addition, cellulose is represented by the difference between acid detergent fiber (ADF) and acid detergent lignin (ADL).

Luminescent conjugated oligothiophenes can also be used to detect cellulose using fluorescence microscopy or spectrofluorometric methods.

In vascular plants cellulose is synthesized at the plasma membrane by rosette terminal complexes (RTCs). The RTCs are hexameric protein structures, approximately 25 nm in diameter, that contain the cellulose synthase enzymes that synthesise the individual cellulose chains. Each RTC floats in the cell's plasma membrane and "spins" a microfibril into the cell wall.

RTCs contain at least three different cellulose synthases, encoded by "CesA" genes, in an unknown stoichiometry. Separate sets of "CesA" genes are involved in primary and secondary cell wall biosynthesis. There are known to be about seven subfamilies in the "CesA" superfamily. These cellulose synthases use UDP-glucose to form the β(1→4)-linked cellulose.

Cellulose synthesis requires chain initiation and elongation, and the two processes are separate.
"CesA" glucosyltransferase initiates cellulose polymerization using a steroid primer, sitosterol-beta-glucoside, and UDP-glucose. Cellulose synthase utilizes UDP-D-glucose precursors to elongate the growing cellulose chain. A cellulase may function to cleave the primer from the mature chain.

Cellulose is also synthesised by animals, particularly in the tests of ascidians (where the cellulose was historically termed "tunicine") although it is also a minor component of mammalian connective tissue.

Cellulolysis is the process of breaking down cellulose into smaller polysaccharides called cellodextrins or completely into glucose units; this is a hydrolysis reaction. Because cellulose molecules bind strongly to each other, cellulolysis is relatively difficult compared to the breakdown of other polysaccharides. However, this process can be significantly intensified in a proper solvent, e.g. in an ionic liquid.

Most mammals have limited ability to digest dietary fiber such as cellulose. Some ruminants like cows and sheep contain certain symbiotic anaerobic bacteria (like "Cellulomonas") in the flora of the rumen, and these bacteria produce enzymes called cellulases that help the microorganism to digest cellulose; the breakdown products are then used by the bacteria for proliferation. The bacterial mass is later digested by the ruminant in its digestive system (stomach and small intestine). Horses use cellulose in their diet by fermentation in their hindgut via symbiotic bacteria which produce cellulase to digest cellulose. Similarly, some termites contain in their hindguts certain flagellate protozoa producing such enzymes, whereas others contain bacteria or may produce cellulase.

The enzymes used to cleave the glycosidic linkage in cellulose are glycoside hydrolases including endo-acting cellulases and exo-acting glucosidases. Such enzymes are usually secreted as part of multienzyme complexes that may include dockerins and carbohydrate-binding modules.

At temperatures above 350 °C, cellulose undergoes thermolysis (also called ‘pyrolysis’), decomposing into solid char, vapors, aerosols, and gases such as carbon dioxide. Maximum yield of vapors which condense to a liquid called "bio-oil" is obtained at 500 °C.

Semi-crystalline cellulose polymers react at pyrolysis temperatures (350–600 °C) in a few seconds; this transformation has been shown to occur via a solid-to-liquid-to-vapor transition, with the liquid (called "intermediate liquid cellulose" or "molten cellulose") existing for only a fraction of a second. Glycosidic bond cleavage produces short cellulose chains of two-to-seven monomers comprising the melt. Vapor bubbling of intermediate liquid cellulose produces aerosols, which consist of short chain anhydro-oligomers derived from the melt.

Continuing decomposition of molten cellulose produces volatile compounds including levoglucosan, furans, pyrans, light oxygenates and gases via primary reactions. Within thick cellulose samples, volatile compounds such as levoglucosan undergo ‘secondary reactions’ to volatile products including pyrans and light oxygenates such as glycolaldehyde.

Hemicellulose is a polysaccharide related to cellulose that comprises about 20% of the biomass of most plants. In contrast to cellulose, hemicellulose is derived from several sugars in addition to glucose, especially xylose but also including mannose, galactose, rhamnose, and arabinose. Hemicellulose consists of shorter chains – between 500 and 3000 sugar units. Furthermore, hemicellulose is branched, whereas cellulose is unbranched.

The hydroxyl groups (-OH) of cellulose can be partially or fully reacted with various reagents to afford derivatives with useful properties like mainly cellulose esters and cellulose ethers (-OR). In principle, though not always in current industrial practice, cellulosic polymers are renewable resources.

Ester derivatives include:

The cellulose acetate and cellulose triacetate are film- and fiber-forming materials that find a variety of uses. The nitrocellulose was initially used as an explosive and was an early film forming material. With camphor, nitrocellulose gives celluloid.

Ether derivatives include:

The sodium carboxymethyl cellulose can be cross-linked to give the croscarmellose sodium (E468) for use as a disintegrant in pharmaceutical formulations.

Cellulose for industrial use is mainly obtained from wood pulp and cotton. The kraft process is used to separate cellulose from lignin, another major component of plant matter.





</doc>
<doc id="6913" url="https://en.wikipedia.org/wiki?curid=6913" title="Cortez">
Cortez









</doc>
<doc id="6916" url="https://en.wikipedia.org/wiki?curid=6916" title="Colony">
Colony

In politics and history, a colony is a territory under the immediate political control of a state, distinct from the home territory of the sovereign. For colonies in antiquity, city-states would often found their own colonies. Some colonies were historically countries, while others were territories without definite statehood from their inception.

The metropolitan state is the state that rules the colony. In Ancient Greece, the city that founded a colony was known as the metropolis. "Mother country" is a reference to the metropolitan state from the point of view of citizens who live in its colony. There is a United Nations list of Non-Self-Governing Territories.

Unlike a puppet state or satellite state, a colony has no independent international representation, and its top-level administration is under direct control of the metropolitan state.

The term "informal colony" is used by some historians to refer to a country under the "de facto" control of another state, although this term is often contentious.

The word "colony" comes from the Latin word "colōnia". This in turn derives from the word "colōnus", which means colonist but also implies a farmer. Cologne is an example of a settlement preserving this etymology. Other, less obvious settlements that began as Roman colonia include cities from Belgrade to York. A tell-tale sign of a settlement once being a Roman Colony is a city centre with a grid pattern. The terminology is taken from architectural analogy, where a column pillar is beneath the (often stylized) head capital, which is also a biological analog of the body as subservient beneath the controlling head (with 'capital' coming from the Latin word "caput", meaning 'head'). So colonies are not independently self-controlled, but rather are controlled from a separate entity that serves the capital function.

Roman colonies first appeared when the Romans conquered neighbouring Italic peoples. These were small farming settlements that appeared when the Romans had subdued an enemy in war. A colony could take many forms, as a trade outpost or a military base in enemy territory. Its original definition as a settlement created by people migrating from a central region to an outlying one became the modern definition.



The Special Committee on Decolonization maintains the United Nations list of Non-Self-Governing Territories, which identifies areas the United Nations (though not without controversy) believes are colonies. Given that dependent territories have varying degrees of autonomy and political power in the affairs of the controlling state, there is disagreement over the classification of "colony".




</doc>
<doc id="6918" url="https://en.wikipedia.org/wiki?curid=6918" title="Rod (optics)">
Rod (optics)

"Rods" (sometimes known as "skyfish", "air rods", or "solar entities") is a term used in cryptozoology, ufology, and outdoor photography to refer to elongated artifacts in the form of light-rods produced by cameras. Videos of rod-shaped objects moving quickly through the air were claimed by some ufologists and cryptozoologists to be alien life forms, "extradimensional" creatures, or very small UFOs. Subsequent experiments showed that these rods appear in film because of an optical illusion/collusion (especially in interlaced video recording), and are typically traces of a flying insect's wingbeats.

Various paranormal interpretations appeared in the popular culture, and one of the more outspoken proponents of rods as alien life forms is Jose Escamilla, who claims to have been the first to film them on March 19, 1994 at Roswell, New Mexico, while attempting to film a UFO. Since then, Escamilla has made additional videos and embarked on lecture tours to promote his claims.

The Straight Dope columnist Cecil Adams called rods a hoax "where unscrupulous people are exploiting a gullible public for profit", and said that investigators have shown that rods are mere tricks of light which result from how images (primarily video images) of flying insects are recorded and played back. In particular, the fast passage before the camera of an insect flapping its wings has been shown to produce rodlike effects, due to motion blur, if the camera is shooting with relatively long exposure times.

In August 2005, China Central Television (CCTV) aired a two-part documentary about flying rods in China. It reported the events from May to June of the same year at Tonghua Zhenguo Pharmaceutical Company in Tonghua City, Jilin Province, which debunked the flying rods. Surveillance cameras in the facility's compound captured video footage of flying rods identical to those shown in Jose Escamilla's video. Getting no satisfactory answer to the phenomenon, curious scientists at the facility decided that they would try to solve the mystery by attempting to catch these airborne creatures. Huge nets were set up and the same surveillance cameras then captured images of rods flying into the trap. When the nets were inspected, the "rods" were no more than regular moths and other ordinary flying insects. Subsequent investigations proved that the appearance of flying rods on video was an optical illusion created by the slower recording speed of the camera.

After attending a lecture by Jose Escamilla, UFO investigator Robert Sheaffer wrote that "some of his “rods” were obviously insects zipping across the field at a high angular rate" and others appeared to be “appendages” which were birds' wings blurred by the camera exposure.




</doc>
<doc id="6920" url="https://en.wikipedia.org/wiki?curid=6920" title="Column">
Column

A column or pillar in architecture and structural engineering is a structural element that transmits, through compression, the weight of the structure above to other structural elements below. In other words, a column is a compression member. The term column applies especially to a large round support (the "shaft" of the column) with a capital and a "base" or pedestal which is made of stone, or appearing to be so. A small wooden or metal support is typically called a post, and supports with a rectangular or other non-round section are usually called piers. For the purpose of wind or earthquake engineering, columns may be designed to resist lateral forces. Other compression members are often termed "columns" because of the similar stress conditions. Columns are frequently used to support beams or arches on which the upper parts of walls or ceilings rest. In architecture, "column" refers to such a structural element that also has certain proportional and decorative features. A column might also be a decorative element not needed for structural purposes; many columns are "engaged", that is to say form part of a wall.

All significant Iron Age civilizations of the Near East and Mediterranean made some use of columns. In Ancient Egyptian architecture as early as 2600 BC the architect Imhotep made use of stone columns whose surface was carved to reflect the organic form of bundled reeds; in later Egyptian architecture faceted cylinders were also common. Egyptian columns are famously present in the Great Hypostyle Hall of Karnak (ca. 1224 BC), where 134 columns are lined up in 16 rows, with some columns reaching heights of 24 metres.

Some of the most elaborate columns in the ancient world were those of the Persians, especially the massive stone columns erected in Persepolis. They included double-bull structures in their capitals. The Hall of Hundred Columns at Persepolis, measuring 70 × 70 metres, was built by the Achaemenid king Darius I (524–486 BC). Many of the ancient Persian columns are standing, some being more than 30 metres tall.

The Minoans used whole tree-trunks, usually turned upside down in order to prevent re-growth, stood on a base set in the stylobate (floor base) and topped by a simple round capital. These were then painted as in the most famous Minoan palace of Knossos. The Minoans employed columns to create large open-plan spaces, light-wells and as a focal point for religious rituals. These traditions were continued by the later Mycenaean civilization, particularly in the megaron or hall at the heart of their palaces. The importance of columns and their reference to palaces and therefore authority is evidenced in their use in heraldic motifs such as the famous lion-gate of Mycenae where two lions stand each side of a column. Being made of wood these early columns have not survived, but their stone bases have and through these we may see their use and arrangement in these palace buildings.

The Egyptians, Persians and other civilizations mostly used columns for the practical purpose of holding up the roof inside a building, preferring outside walls to be decorated with reliefs or painting, but the Ancient Greeks, followed by the Romans, loved to use them on the outside as well, and the extensive use of columns on the interior and exterior of buildings is one of the most characteristic features of classical architecture, in buildings like the Parthenon. The Greeks developed the classical orders of architecture, which are most easily distinguished by the form of the column and its various elements. Their Doric, Ionic, and Corinthian orders were expanded by the Romans to include the Tuscan and Composite orders (see below).

Columns, or at least large structural exterior ones, became much less significant in the architecture of the Middle Ages. The classical forms were abandoned in both Byzantine architecture and the Romanesque and Gothic architecture of Europe in favour of more flexible forms, with capitals often using various types of foliage decoration, and in the West scenes with figures carved in relief. Renaissance architecture was keen to revive the classical vocabulary and styles, and the informed use and variation of the classical orders remained fundamental to the training of architects throughout Baroque, Rococo and Neo-classical architecture.

Early columns were constructed of stone, some out of a single piece of stone. Monolithic columns are among the heaviest stones used in architecture. Other stone columns are created out of multiple sections of stone, mortared or dry-fit together. In many classical sites, sectioned columns were carved with a centre hole or depression so that they could be pegged together, using stone or metal pins. The design of most classical columns incorporates entasis (the inclusion of a slight outward curve in the sides) plus a reduction in diameter along the height of the column, so that the top is as little as 83% of the bottom diameter. This reduction mimics the parallax effects which the eye expects to see, and tends to make columns look taller and straighter than they are while entasis adds to that effect.

Most classical columns arise from a basis, or base, that rests on the stylobate, or foundation, except for those of the Doric order, which usually rest directly on the stylobate. The basis may consist of several elements, beginning with a wide, square slab known as a plinth. The simplest bases consist of the plinth alone, sometimes separated from the column by a convex circular cushion known as a torus. More elaborate bases include two toruses, separated by a concave section or channel known as a scotia or trochilus. Scotiae could also occur in pairs, separated by a convex section called an astragal, or bead, narrower than a torus. Sometimes these sections were accompanied by still narrower convex sections, known as annulets or fillets.

At the top of the shaft is a capital, upon which the roof or other architectural elements rest. In the case of Doric columns, the capital usually consists of a round, tapering cushion, or echinus, supporting a square slab, known as an abax or abacus. Ionic capitals feature a pair of volutes, or scrolls, while Corinthian capitals are decorated with reliefs in the form of acanthus leaves. Either type of capital could be accompanied by the same moldings as the base. In the case of free-standing columns, the decorative elements atop the shaft are known as a finial.

Modern columns may be constructed out of steel, poured or precast concrete, or brick, left bare or clad in an architectural covering, or veneer. Used to support an arch, an impost, or pier, is the topmost member of a column. The bottom-most part of the arch, called the springing, rests on the impost.

As the axial load on a perfectly straight slender column with elastic material properties is increased in magnitude, this ideal column passes through three states: stable equilibrium, neutral equilibrium, and instability. The straight column under load is in stable equilibrium if a lateral force, applied between the two ends of the column, produces a small lateral deflection which disappears and the column returns to its straight form when the lateral force is removed. If the column load is gradually increased, a condition is reached in which the straight form of equilibrium becomes so-called neutral equilibrium, and a small lateral force will produce a deflection that does not disappear and the column remains in this slightly bent form when the lateral force is removed. The load at which neutral equilibrium of a column is reached is called the critical or buckling load. The state of instability is reached when a slight increase of the column load causes uncontrollably growing lateral deflections leading to complete collapse.

For an axially loaded straight column with any end support conditions, the equation of static equilibrium, in the form of a differential equation, can be solved for the deflected shape and critical load of the column. With hinged, fixed or free end support conditions the deflected shape in neutral equilibrium of an initially straight column with uniform cross section throughout its length always follows a partial or composite sinusoidal curve shape, and the critical load is given by

formula_1

where "r" = radius of gyration of [column]cross-section which is equal to the square root of (I/A), "K" = ratio of the longest half sine wave to the actual column length, and "KL" = effective length (length of an equivalent hinged-hinged column). From Equation (2) it can be noted that the buckling strength of a column is inversely proportional to the square of its length.

When the critical stress, "F" ("F" ="P"/"A", where "A" = cross-sectional area of the column), is greater than the proportional limit of the material, the column is experiencing inelastic buckling. Since at this stress the slope of the material's stress-strain curve, "E" (called the tangent modulus), is smaller than that below the proportional limit, the critical load at inelastic buckling is reduced. More complex formulas and procedures apply for such cases, but in its simplest form the critical buckling load formula is given as Equation (3),

formula_2

where "E" = tangent modulus at the stress "F"

A column with a cross section that lacks symmetry may suffer torsional buckling (sudden twisting) before, or in combination with, lateral buckling. The presence of the twisting deformations renders both theoretical analyses and practical designs rather complex.

Eccentricity of the load, or imperfections such as initial crookedness, decreases column strength. If the axial load on the column is not concentric, that is, its line of action is not precisely coincident with the centroidal axis of the column, the column is characterized as eccentrically loaded. The eccentricity of the load, or an initial curvature, subjects the column to immediate bending. The increased stresses due to the combined axial-plus-flexural stresses result in a reduced load-carrying ability.

Column elements are considered to be massive if their smallest side dimension is equal to or more than 400 mm. Massive columns have the ability to increase in carrying strength over long time periods (even during periods of heavy load). Taking into account the fact, that possible structural loads may increase over time as well (and also the threat of progressive failure), massive columns have an advantage compared to non-massive ones.

When a column is too long to be built or transported in one piece, it has to be extended or spliced at the construction site. A reinforced concrete column is extended by having the steel reinforcing bars protrude a few inches or feet above the top of the concrete, then placing the next level of reinforcing bars to overlap, and pouring the concrete of the next level. A steel column is extended by welding or bolting splice plates on the flanges and webs or walls of the columns to provide a few inches or feet of load transfer from the upper to the lower column section. A timber column is usually extended by the use of a steel tube or wrapped-around sheet-metal plate bolted onto the two connecting timber sections.

A column that carries the load down to a foundation must have means to transfer the load without overstressing the foundation material. Reinforced concrete and masonry columns are generally built directly on top of concrete foundations. When seated on a concrete foundation, a steel column must have a base plate to spread the load over a larger area, and thereby reduce the bearing pressure. The base plate is a thick, rectangular steel plate usually welded to the bottom end of the column.

The Roman author Vitruvius, relying on the writings (now lost) of Greek authors, tells us that the ancient Greeks believed that their Doric order developed from techniques for building in wood. The earlier smoothed tree-trunk was replaced by a stone cylinder.

The Doric order is the oldest and simplest of the classical orders. It is composed of a vertical cylinder that is wider at the bottom. It generally has neither a base nor a detailed capital. It is instead often topped with an inverted frustum of a shallow cone or a cylindrical band of carvings. It is often referred to as the masculine order because it is represented in the bottom level of the Colosseum and the Parthenon, and was therefore considered to be able to hold more weight. The height-to-thickness ratio is about 8:1. The shaft of a Doric Column is almost always fluted.

The Greek Doric, developed in the western Dorian region of Greece, is the heaviest and most massive of the orders. It rises from the stylobate without any base; it is from four to six times as tall as its diameter; it has twenty broad flutes; the capital consists simply of a banded necking swelling out into a smooth echinus, which carries a flat square abacus; the Doric entablature is also the heaviest, being about one-fourth the height column. The Greek Doric order was not used after c. 100 B.C. until its “rediscovery” in the mid-eighteenth century.

The Tuscan order, also known as Roman Doric, is also a simple design, the base and capital both being series of cylindrical disks of alternating diameter. The shaft is almost never fluted. The proportions vary, but are generally similar to Doric columns. Height to width ratio is about 7:1.

The Ionic column is considerably more complex than the Doric or Tuscan. It usually has a base and the shaft is often fluted (it has grooves carved up its length). The capital features a volute, an ornament shaped like a scroll, at the four corners. The height-to-thickness ratio is around 9:1. Due to the more refined proportions and scroll capitals, the Ionic column is sometimes associated with academic buildings. Ionic style columns were used on the second level of the Colosseum.

The Corinthian order is named for the Greek city-state of Corinth, to which it was connected in the period. However, according to the architectural historian Vitruvius, the column was created by the sculptor Callimachus, probably an Athenian, who drew acanthus leaves growing around a votive basket. In fact, the oldest known Corinthian capital was found in Bassae, dated at 427 BC. It is sometimes called the feminine order because it is on the top level of the Colosseum and holding up the least weight, and also has the slenderest ratio of thickness to height. Height to width ratio is about 10:1.

The Composite order draws its name from the capital being a composite of the Ionic and Corinthian capitals. The acanthus of the Corinthian column already has a scroll-like element, so the distinction is sometimes subtle. Generally the Composite is similar to the Corinthian in proportion and employment, often in the upper tiers of colonnades. Height to width ratio is about 11:1 or 12:1.

A Solomonic column, sometimes called "barley sugar", begins on a base and ends in a capital, which may be of any order, but the shaft twists in a tight spiral, producing a dramatic, serpentine effect of movement. Solomonic columns were developed in the ancient world, but remained rare there. A famous marble set, probably 2nd century, was brought to St Peter's, Rome by Constantine I, and placed round the saint's shrine, and was thus familiar throughout the Middle Ages, by which time they were thought to have been removed from the Temple of Jerusalem. The style was used in bronze by Bernini for his spectacular St. Peter's baldachin, actually a ciborium (which displaced Constantine's columns), and thereafter became very popular with Baroque and Rococo church architects, above all in Latin America, where they were very often used, especially on a small scale, as they are easy to produce in wood by turning on a lathe (hence also the style's popularity for spindles on furniture and stairs).

Pillar tombs are monumental graves, which typically feature a single, prominent pillar or column, often made of stone. A number of world cultures incorporated pillars into tomb structures. In the Ancient Greek colony of Lycia in Anatolia, one of these edifices is located at the tomb of Xanthos. In the town of Hannassa in southern Somalia, ruins of houses with archways and courtyards have also been found along with other pillar tombs, including a rare octagonal tomb.



</doc>
<doc id="6921" url="https://en.wikipedia.org/wiki?curid=6921" title="Carmilla">
Carmilla

Carmilla is a Gothic novella by Joseph Sheridan Le Fanu and one of the early works of vampire fiction, predating Bram Stoker's "Dracula" (1897) by 26 years. First published as a serial in "The Dark Blue" (1871–72), the story is narrated by a young woman preyed upon by a female vampire named Carmilla, later revealed to be Mircalla, Countess Karnstein (Carmilla is an anagram of Mircalla). The story is often anthologized and has been adapted many times in film and other media.

"Carmilla," serialized in the literary magazine "The Dark Blue" in late 1871 and early 1872, was reprinted in Le Fanu's short story collection "In a Glass Darkly" (1872). Comparing the work of the two illustrators, David Henry Friston and Michael Fitzgerald, whose work appears in the magazine but not in modern printings of the book, reveals inconsistencies in the characters' depictions. Consequently, confusion has arisen relating the pictures to the plot. Isabella Mazzanti illustrated the book's 2014 anniversary edition, published by Editions Soleil and translated by Gaid Girard.

Le Fanu presents the story as part of the casebook of Dr. Hesselius, whose departures from medical orthodoxy rank him as the first occult detective in literature.

Laura, the teenage protagonist, narrates, beginning with her childhood in a "picturesque and solitary" castle amid an extensive forest in Styria, where she lives with her father, a wealthy English widower retired from service to the Austrian Empire. When she was six, Laura had a vision of a beautiful visitor in her bedchamber. She later claims to have been punctured in her breast, although no wound was found.

Twelve years later, Laura and her father are admiring the sunset in front of the castle when her father tells her of a letter from his friend, General Spielsdorf. The General was supposed to bring his niece, Bertha Rheinfeldt, to visit the two, but the niece suddenly died under mysterious circumstances. The General ambiguously concludes that he will discuss the circumstances in detail when they meet later.

Laura, saddened by the loss of a potential friend, longs for a companion. A carriage accident outside Laura's home unexpectedly brings a girl of Laura's age into the family's care. Her name is Carmilla. Both girls instantly recognize the other from the "dream" they both had when they were young.

Carmilla appears injured after her carriage accident, but her mysterious mother informs Laura's father that her journey is urgent and cannot be delayed. She arranges to leave her daughter with Laura and her father until she can return in three months. Before she leaves, she sternly notes that her daughter will not disclose any information whatsoever about her family, past, or herself, and that Carmilla is of sound mind. Laura comments that this information seems needless to say, and her father laughs it off.

Carmilla and Laura grow to be very close friends, but occasionally Carmilla's mood abruptly changes. She sometimes makes romantic advances towards Laura. Carmilla refuses to tell anything about herself, despite questioning by Laura. Her secrecy is not the only mysterious thing about Carmilla; she never joins the household in its prayers, she sleeps much of the day, and she seems to sleepwalk outside at night.

Meanwhile, young women and girls in the nearby towns have begun dying from an unknown malady. When the funeral procession of one such victim passes by the two girls, Laura joins in the funeral hymn. Carmilla bursts out in rage and scolds Laura, complaining that the hymn hurts her ears.

When a shipment of restored heirloom paintings arrives, Laura finds a portrait of her ancestor, Mircalla, Countess Karnstein, dated 1698. The portrait resembles Carmilla exactly, down to the mole on her neck. Carmilla says she might be descended from the Karnsteins even though the family died out centuries before.

During Carmilla's stay, Laura has nightmares of a large cat-like beast entering her room and biting her on the chest. The beast then takes the form of a female figure and disappears through the door without opening it. In another nightmare, Laura hears a voice say, "Your mother warns you to beware of the assassin," and a sudden light reveals Carmilla standing at the foot of her bed, her nightdress drenched in blood. Laura's health declines, and her father has a doctor examine her. He finds a small blue spot on her chest and speaks privately with her father, only asking that Laura never be unattended.

Her father then sets out with Laura, in a carriage, for the ruined village of Karnstein, three miles distant. They leave a message behind asking Carmilla and one of the governesses to follow once the perpetually late-sleeping Carmilla wakes. En route to Karnstein, Laura and her father encounter General Spielsdorf. He tells them his own ghastly story:

At a costume ball, Spielsdorf and his niece Bertha had met a young woman named Millarca and her enigmatic mother. Bertha was immediately taken with Millarca. The mother convinced the General that she was an old friend of his and asked that Millarca be allowed to stay with them for three weeks while she attended to a secret matter of great importance.

Bertha fell mysteriously ill, suffering the same symptoms as Laura. After consulting with a specially-ordered priestly doctor, the General realized that Bertha was being visited by a vampire. He hid with a sword and waited until a large black creature crawled onto his niece's bed and to her neck. He leapt from his hiding place and attacked the beast, which took the form of Millarca. She fled through the locked door, unharmed. Bertha died immediately afterward.

Upon arriving at Karnstein, the General asks a woodman where he can find the tomb of Mircalla Karnstein. The woodman says the tomb was relocated long ago by the hero who vanquished the vampires that haunted the region.

While the General and Laura are alone in the ruined chapel, Carmilla appears. The General and Carmilla both fly into a rage upon seeing each other, and the General attacks her with an axe. Carmilla disarms the General and disappears. The General explains that Carmilla is also Millarca, both anagrams for the original name of the vampire Mircalla, Countess Karnstein.

The party is joined by Baron Vordenburg, the descendant of the hero who rid the area of vampires long ago. Vordenburg, an authority on vampires, has discovered that his ancestor was romantically involved with the Countess Karnstein before she died and became one of the undead. Using his forefather's notes, he locates Mircalla's hidden tomb. An Imperial Commission exhumes the vampire's body. Immersed in blood, it seems to be breathing faintly, its heart beating, its eyes open. A stake is driven through its heart, and it gives a corresponding shriek; then the head is struck off. The body and head are burned to ashes, which are thrown into a river.

Afterward, Laura's father takes his daughter on a year-long tour through Italy to regain her health and recover from the trauma, which she never fully does.

As with "Dracula", critics have looked for the sources used in the writing of "Carmilla". One source used was from a dissertation on magic, vampires and the apparitions of spirits written by Dom Augustin Calmet entitled "Traité sur les apparitions des esprits et sur les vampires ou les revenants de Hongrie, de Moravie, &c." (1751). This is evidenced by a report analyzed by Calmet, from a priest who learned information of a town being tormented by a vampiric entity 3 years earlier. Having traveled to the town to investigate and collecting information of the various inhabitants there, the priest learned that a vampire had tormented many of the inhabitants at night by coming from the nearby cemetery and would haunt many of the residents on their beds. An unknown Hungarian traveler came to the town during this period and helped the town by setting a trap at the cemetery and decapitating the vampire that resided there, curing the town of their torment. This story was retold by LeFanu and adapted into the thirteenth chapter of Carmilla 

According to Matthew Gibson, the Reverend Sabine Baring-Gould's "The Book of Were-wolves" (1863) and his account of Elizabeth Báthory, Coleridge's "Christabel" (Part 1, 1797 and Part 2, 1800), and Captain Basil Hall's "Schloss Hainfeld; or a Winter in Lower Styria" (London and Edinburgh, 1836) are other sources for Le Fanu's "Carmilla". Hall's account provides much of the Styrian background and, in particular, a model for both Carmilla and Laura in the figure of Jane Anne Cranstoun, Countess Purgstall.

Carmilla, the title character, is the original prototype for a legion of female and lesbian vampires. Though Le Fanu portrays his vampire's sexuality with the circumspection that one would expect for his time, it is evident that lesbian attraction is the main dynamic between Carmilla and the narrator of the story:

When compared to other literary vampires of the 19th century, Carmilla is a similar product of a culture with strict sexual mores and tangible religious fear. While Carmilla selected exclusively female victims, she only becomes emotionally involved with a few. Carmilla had nocturnal habits, but was not confined to the darkness. She had unearthly beauty, and was able to change her form and to pass through solid walls. Her animal alter ego was a monstrous black cat, not a large dog as in "Dracula". She did, however, sleep in a coffin. "Carmilla" works as a Gothic horror story because her victims are portrayed as succumbing to a perverse and unholy temptation that has severe metaphysical consequences for them.

Some critics, among them William Veeder, suggest that "Carmilla", notably in its outlandish use of narrative frames, was an important influence on Henry James' "The Turn of the Screw" (1898).

Although "Carmilla" is a lesser known and far shorter Gothic vampire story than the generally considered master work of that genre, "Dracula", the latter is heavily influenced by Le Fanu's novella. Stoker's posthumously published short story "Dracula's Guest" (1914), known as the deleted first chapter to "Dracula", shows a more obvious and intact debt to "Carmilla":








(chronological) 






</doc>
<doc id="6922" url="https://en.wikipedia.org/wiki?curid=6922" title="Clitoridectomy">
Clitoridectomy

Clitoridectomy or clitorectomy is the surgical removal, reduction, or partial removal of the clitoris. It is rarely used as a therapeutic medical procedure, such as when cancer has developed in or spread to the clitoris. It is often performed on intersex newborns. Commonly, non-medical removal of the clitoris is performed during female genital mutilation (FGM). 

A clitoridectomy is often done to remove malignancy or necrosis of the clitoris. This is sometimes done along with a radical complete vulvectomy. Surgery may also become necessary due to therapeutic radiation treatments to the pelvic area.

Female infants born with a 46,XX genotype but have genitalia affected by congenital adrenal hyperplasia and are treated surgically with vaginoplasty that often reduces the size of the clitoris without its total removal. The atypical size of the clitoris is due to an endocrine imbalance in utero. This treatment raises human rights concerns, see below. Other reasons for the surgery include issues involving a microphallus and those who have Mayer-Rokitansky-Kustner disorder. Removal of the clitoris may be due to malignancy or trauma.

Clitoridectomy surgical techniques are used to remove an invasive malignancy that extends to the clitoris. Standard surgical procedures are followed in these cases. This includes evaluation and biopsy. Other factors that will affect the technique selected are age, other existing medical conditions, and obesity. Other considerations are the probability of extended hospital care and the development of infection at the surgical site.
The surgery proceeds with the use of general anesthesia, and prior to the vulvectomy/clitoridectomy an inguinal lymphyadenectomy is first done. The extent of the surgical site extends one to two centimeters beyond the boundaries of malignancy. Superficial lymph nodes may also need to be removed. If the malignancy is present in muscular tissue in the region, it is also removed. In some cases, the surgeon is able to preserve the clitoris though the malignancy may be extensive. The cancerous tissue is removed and the incision is closed.

Post operative care may employ the use of suction drainage to allow the deeper tissues to heal toward the surface. Follow up after surgery includes the stripping of the drainage device to prevent blockage. A typical hospital stay can be up to two weeks. The site of the surgery is left unbandaged to allow for frequent examination.
Complications can be the development of lymphedema though not removing the saphenous vein during the surgery will help prevent this. In some instances, foot elevation, diuretic medication and compression stockings can reduce the build up of fluid.

In a clitoridectomy for intersex infants, the clitoris is often reduced instead of removed. The surgeon cuts the shaft of the elongated phallus and sews the glans and preserved nerves back onto the stump. In a less common surgery called clitoral recession, the surgeon hides the clitoral shaft under a fold of skin so only the glans remains visible.

In the 19th century, a clitoridectomy was thought to curb female masturbation. Isaac Baker Brown (1812–1873), an English gynaecologist who was president of the Medical Society of London believed that the "unnatural irritation" of the clitoris caused epilepsy, hysteria, and mania, and he worked "to remove [it] whenever he had the opportunity of doing so", according to his obituary in the "Medical Times and Gazette". Peter Lewis Allen writes that Brown's views caused outrage, and he died penniless after being expelled from the Obstetrical Society.

For a time female circumcision was done as a cure for insanity. Some practitioners of medicine in the Victorian era believed that mental and emotional disorders were related to female reproductive organs. Some thought that removing the clitoris would cure the neurosis. This treatment was discontinued in 1867.

Aesthetics may determine clitoral norms. A lack of ambiguity of the genitalia is seen as necessary in the assignment of a sex to infants and therefore whether a child's genitalia is normal, but what is ambiguous or normal can vary from person to person.

Sexual behavior is another reason for clitoridectomies. Author Sarah Rodriguez stated that the history of medical textbooks has indirectly created accepted ideas about the female body. Medical and gynecological textbooks are also at fault in the way that the clitoris is described in comparison to a male's penis. The importance and originality of a female's clitoris is underscored because it is seen as "a less significant organ, since anatomy texts compared the penis and the clitoris in only one direction." Rodriguez said that a male's penis created the framework of the sexual organ.

Clitoridectomies are the most common form of female genital mutilation. The World Health Organization (WHO) estimates that clitordectomies have been performed on 200 million girls and women that are currently alive. The regions that most clitodectomies take place are Asia, the Middle East and west, north and east Africa. The practice also exists in migrants originating from these regions. Most of the surgeries are for cultural or religious reasons.

Clitoridectomy of women with intersex conditions is controversial when it takes place during childhood or under duress. Intersex women exposed to such treatment have spoken of their loss of physical sensation, and loss of autonomy. In recent years, multiple human rights institutions have criticized early surgical management of such characteristics.

In 2013, it was disclosed in a medical journal that four unnamed elite female athletes from developing countries were subjected to gonadectomies and partial clitoridectomies after testosterone testing revealed that they had an intersex condition. In April 2016, the United Nations Special Rapporteur on health, Dainius Pūras, condemned this treatment as a form of female genital mutilation "in the absence of symptoms or health issues warranting those procedures."




</doc>
<doc id="6924" url="https://en.wikipedia.org/wiki?curid=6924" title="Cabal">
Cabal

A cabal is a group of people united in some close design together, usually to promote their private views or interests in an ideology, state, or other community, often by , usually unbeknown to persons outside their group. The use of this term usually carries strong connotations of shadowy corners, back rooms and insidious influence. The term is frequently used in conspiracy theories.

The term "cabal" derives from Cabala (a word that has numerous spelling variations), the Jewish mystical interpretation of the Hebrew scripture. In Hebrew it means "reception" or "tradition", denoting the "sod" (secret) level of Jewish exegesis. In European culture (Christian Cabala, Hermetic Qabalah) it became associated with occult doctrine or a secret.

There is a theory that the term took on its present meaning from a group of ministers ("Cabal ministry") of King Charles II of England (Sir Thomas Clifford, Lord Arlington, the Duke of Buckingham, Lord Ashley, and Lord Lauderdale), whose initial letters coincidentally spelled CABAL, and who were the signatories of the Secret Treaty of Dover that allied England to France in a prospective war against the Netherlands. The theory that the word originated as an acronym from the names of the group of ministers is a folk etymology, although the coincidence was noted at the time and could possibly have popularized its use. The group came to prominence after the fall of Charles' first Chief Minister, Lord Clarendon, in 1667. The Cabal was later called by Lord Macauley "the first germ of the present system of government by a Cabinet".


</doc>
<doc id="6925" url="https://en.wikipedia.org/wiki?curid=6925" title="Cytochrome">
Cytochrome

Cytochromes are iron containing hemeproteins central to which are heme groups that are primarily responsible for the generation of ATP via electron transport.

They are found either as monomeric proteins (e.g., cytochrome c) or as subunits of larger enzymatic complexes that catalyze redox reactions.

Cytochromes were initially described in 1884 by MacMunn as respiratory pigments (myohematin or histohematin). In the 1920s, Keilin rediscovered these respiratory pigments and named them the cytochromes, or “cellular pigments”, and classified these heme proteins, on the basis of the position of their lowest energy absorption band in the reduced state, as
cytochromes "a" (605 nm), "b" (~565 nm), and "c" (550 nm). The ultra-violet (UV) to visible spectroscopic signatures of hemes are still used to identify heme type from the reduced bis-pyridine-ligated state, i.e., the pyridine hemochrome method. Within each class, cytochrome "a", "b", or "c", early cytochromes are numbered consecutively, e.g. cyt "c", cyt "c", and cyt "c", with more recent examples designated by their reduced state R-band maximum, e.g. cyt "c".

The heme group is a highly conjugated ring system (which allows its electrons to be very mobile) surrounding a metal ion, which readily interconverts between the oxidation states. For many cytochromes, the metal ion present is that of "iron", which interconverts between Fe (reduced) and Fe (oxidized) states (electron-transfer processes). Cytochromes are, thus, capable of performing oxidation and reduction. Because the cytochromes (as well as other complexes) are held within membranes in an organized way, the redox reactions are carried out in the proper sequence for maximum efficiency.

In the process of oxidative phosphorylation, which is the principal energy-generating process undertaken by organisms, other membrane-bound and -soluble complexes and cofactors are involved in the chain of redox reactions, with the additional net effect that protons (H) are transported across the mitochondrial inner membrane. The resulting transmembrane proton gradient (protonmotive force) is used to generate ATP, which is the universal chemical energy currency of life. ATP is consumed to drive cellular processes that require energy (such as synthesis of macromolecules, active transport of molecules across the membrane, and assembly of flagella).

Many scientists believe that cytochrome c has evolved at a fairly constant rate. The constant rate of change in cytochrome c may be the basis for a molecular "clock". The molecular clock can be a helpful tool in trying to determine when various organisms may have diverged from a common ancestor. 

Several kinds of cytochrome exist and can be distinguished by spectroscopy, exact structure of the heme group, inhibitor sensitivity, and reduction potential.

Three types of cytochrome are distinguished by their prosthetic groups:

The definition of cytochrome c is not defined in terms of the heme group. There is no "cytochrome e," but there is a cytochrome f, which is often considered a type of cytochrome c.

In mitochondria and chloroplasts, these cytochromes are often combined in electron transport and related metabolic pathways:

A distinct family of cytochromes is the cytochrome P450 family, so named for the characteristic Soret peak formed by absorbance of light at wavelengths near 450 nm when the heme iron is reduced (with sodium dithionite) and complexed to carbon monoxide. These enzymes are primarily involved in steroidogenesis and detoxification.



</doc>
<doc id="6927" url="https://en.wikipedia.org/wiki?curid=6927" title="Crowded House">
Crowded House

Crowded House are a rock band formed in Melbourne, Australia, in 1985. The founding members were New Zealander Neil Finn (vocalist, guitarist, primary songwriter) and Australians Paul Hester (drums) and Nick Seymour (bass). Later band members included Neil Finn's brother, Tim Finn, and Americans Mark Hart and Matt Sherrod.

Originally active from 1985 to 1996, the band had consistent commercial and critical success in Australia and New Zealand and international chart success in two phases, beginning with their self-titled debut album, which reached number twelve on the US Album Chart in 1987 and provided the Top Ten hits "Don't Dream It's Over" and "Something So Strong". Further international success came in the UK, Europe and South Africa with their third and fourth albums, "Woodface" and "Together Alone" and the compilation album "Recurring Dream", which included the hits "Fall at Your Feet", "Weather with You", "Distant Sun", "Locked Out", "Instinct" and "Not the Girl You Think You Are". Neil and Tim Finn were each awarded an OBE in June 1993, for their contribution to the music of New Zealand.

Founding drummer Hester left in May 1994 citing family reasons, but briefly returned for their "Farewell to the World" concerts in Melbourne and Sydney in 1996. Neil Finn had decided to end the band to concentrate on his solo career and the Finn Brothers project with Tim. On 26 March 2005 Hester died by suicide, aged 46. In 2006 the group re-formed with a new drummer Matt Sherrod and released two further albums (in 2007 and 2010), both of which reached number one on Australia's album chart. As of July 2010 the group has sold 10 million albums. In November 2016 they were inducted into the ARIA Hall of Fame.

Neil Finn (vocals, guitar, piano) and drummer Paul Hester (ex-The Cheks, Deckchairs Overboard) were former members of New Zealand band Split Enz, which spent part of 1975–6 in Australia and several years in England. Neil Finn is the younger brother of Split Enz founding member Tim Finn, who joined Crowded House in 1990 on vocals, guitars and keyboards for the album "Woodface". Bassist Nick Seymour (ex-Plays with Marionettes, Bang, The Horla) is the younger brother of singer-songwriter and guitarist Mark Seymour of the now defunct Australian rock group Hunters & Collectors.

Finn and Hester decided to form a new band during the first Split Enz farewell tour, "Enz with a Bang", in late 1984. Seymour approached Finn during the after party for the Melbourne show and asked if he could audition for the new band. The Mullanes formed in Melbourne in early 1985 with Finn, Hester, Seymour and guitarist Craig Hooper (ex-The Reels) and first performed on 11 June. They secured a record contract with Capitol Records, but Hooper left the band before the remaining trio moved to Los Angeles to record their debut album. At Capitol's behest, the band's name was changed to Crowded House, which alluded to the lack of space at the small Hollywood Hills house they shared during the recording of the album "Crowded House". Former Split Enz keyboardist Eddie Rayner produced the track "Can't Carry On" and was asked to join the band. He toured with them in 1988, but was unable to become a full member due to family commitments.

Thanks to their Split Enz connection, the newly formed Crowded House had an established Australasian fanbase. They began by playing at festivals in Australia and New Zealand and released their debut album, "Crowded House", in June 1986. Capitol Records initially failed to see the band's potential and gave them only low-key promotion, forcing the band to play at small venues to try and gain attention. The album's first single, "Mean to Me", reached the Australian Kent Music Report Singles Chart top 30 in June. It failed to chart in the US, but moderate American airplay introduced US listeners to the group.

A single, "Don't Dream It's Over", was released in December 1986 and proved an international hit, reaching number two on the US "Billboard" Hot 100 and number one in Canada. New Zealand radio stations initially gave the song little support until months later when it became successful internationally. Ultimately, the song reached number one on the New Zealand singles chart and number eight in Australia. It remains the group's most commercially successful song.

In March 1987, the group were awarded "Best New Talent", along with "Song of the Year" and "Best Video" awards for "Don't Dream It's Over" at the inaugural ARIA Music Awards. The video also earned the group the MTV Video Music Award for Best New Artist that year. The song has often been covered by other artists and gave Paul Young a hit single in 1991. It was also used for a New Zealand Tourism Board advertisement in its "100% Pure New Zealand" worldwide promotion from October 2005. In May 2001, "Don't Dream it's Over" was voted seventh in a poll of the best Australian songs of all time by the Australasian Performing Rights Association.

In June 1987, a year after its release, "Crowded House" finally reached number one on the Kent Music Report Album Charts. It also reached number three in New Zealand and number twelve on the US Billboard album chart. The follow-up to "Don't Dream it's Over", "Something So Strong", was another global smash, reaching the Top 10 in New Zealand, America, and Canada. "World Where You Live" and "Now We're Getting Somewhere" were also released as singles with chart success.

As the band's primary songwriter, Neil Finn was under pressure to create a second album to match their debut and the band joked that one potential title for the new release was "Mediocre Follow-Up". Eventually titled "Temple of Low Men", their second album was released in July 1988 with strong promotion by Capitol Records. The album did not fare as well as their debut in the US, only reaching number 40, but it achieved Australasian success, reaching number one in Australia and number two in New Zealand. The first single "Better Be Home Soon" peaked at number two on both Australian and New Zealand singles charts and reached top 50 in the US, though the following four singles were less successful. Crowded House undertook a short tour of Australia and Canada to promote the album, with Eddie Rayner on keyboards. Multi-instrumentalist Mark Hart, who would eventually become a full band member, replaced Rayner in January 1989. After the tour, Finn fired Seymour from the band. Music journalist Ed Nimmervoll claimed that Seymour's temporary departure was because Finn blamed him for causing his writer's block; however, Finn cited "artistic differences" as the reason. Seymour said that after a month he contacted Finn and they agreed that he would return to the band.

Crowded House took a break after the Canadian leg of the "Temple of Low Men" tour. Neil Finn and his brother Tim recorded songs they had co-written for their own album, "Finn". Following the recording sessions with Tim, Neil began writing and recording a third Crowded House album with Hester and Seymour, but these tracks were rejected by the record company, so Neil asked Tim if Crowded House could use the "Finn" songs. Tim jokingly agreed on the proviso that he become a member, which Neil apparently took literally. With Tim as an official member, the band returned to the studio. The new tracks, as well as some from the previously rejected recordings were combined to make "Woodface", which was released in July 1991. The album features eight tracks co-written by Neil and Tim, which feature the brothers harmonising on lead vocals, except on the sombre "All I Ask" on which Tim sang lead. The track was later used on AIDS awareness commercials in Australia. Five of the album's tracks were Neil's solo compositions and two were by Hester, the exuberant "Italian Plastic", which became a crowd favourite at concerts and the hidden track "I'm Still Here".

"Chocolate Cake", a humorous comment on American excesses that was not taken well by some US critics and sections of the American public, was released in June 1991 as the first single. Perhaps unsurprisingly it failed to chart in the US, however it reached number two on Billboard's Modern Rock Tracks chart. The song peaked at number seven in New Zealand and reached the top 20 in Australia. The second single, "Fall at Your Feet", was less successful in Australia and New Zealand but did at least reach the US Hot 100. The album reached number one in New Zealand, number two in Australia, number six in the UK and made the top 20 in several European countries. The third single from "Woodface", "Weather With You", peaked at No. 7 in early 1992 giving the band their highest UK chart placement. By contrast, the album had limited success in the US, only reaching number 83 on the Billboard 200 Album Chart.

Tim Finn left Crowded House during the "Woodface" tour in November 1991, part-way through the UK leg. Performances on this tour, at the Town and Country Club in London, were recorded live and given a limited release in Australia, while individual songs from those shows were released as B-sides of singles in some countries. In June 1993 the New Zealand Government recommended that the Queen award an OBE to Neil and Tim Finn for their contribution to the music of New Zealand.

For their fourth album, "Together Alone", Crowded House used producer Martin Glover (aka "Youth") and invited touring musician Mark Hart (guitar and keyboards) to become a permanent band member. The album was recorded at Karekare Beach, New Zealand, which gave its name to the opening track, "Kare Kare". The album was released in October 1993 and sold well internationally on the strength of lead single "Distant Sun" and followup "Private Universe". It topped the New Zealand Album Chart, reached number 2 in Australia and number 4 in the UK. "Locked Out" was the album's first US single and received airplay on MTV and VH1. This track and "My Sharona" by The Knack, which were both included the soundtrack of the film "Reality Bites", were bundled together on a jukebox single to promote the film soundtrack.

Crowded House were midway through a US tour when Paul Hester quit the band on 15 April 1994. He flew home to Melbourne to await the birth of his first child and indicated that he required more time with his family. Wally Ingram, drummer for support act Sheryl Crow, temporarily filled in until a replacement, Peter Jones (ex-Harem Scarem, Vince Jones, Kate Ceberano's Septet) was found. After the tour, the Finn Brothers released their album "Finn" in November 1995. In June 1996, at a press conference to announce the release of their greatest hits album "Recurring Dream", Neil revealed that Crowded House were to disband. The June 1996 concerts in Europe and Canada were to be their final performances.

"Recurring Dream" contained four songs from each of the band's studio albums, along with three new songs. The album debuted at number one in Australia, New Zealand and the UK in July 1996. Early copies included a bonus CD of live material. The album's three new songs, which were released as singles, were "Instinct", "Not the Girl You Think You Are" and "Everything Is Good for You", which featured backing vocals from Pearl Jam's Eddie Vedder. Paul Hester returned to the band to play drums on the three new tracks.

Worried that their goodbye had been too low-key and had disregarded their home fans, the band performed the "Farewell to the World" concert on the steps of the Sydney Opera House on 24 November 1996, which raised funds for the Sydney Children's Hospital. The concert featured the line-up of Neil Finn, Nick Seymour, Mark Hart and Paul Hester. Tim Finn and Peter Jones both made guest appearances. Support bands on the day were Custard, Powderfinger and You Am I. The concert had one of the highest live audiences in Australian history with the crowd being estimated at between 120,000 and 250,000 people. "Farewell to the World" was released on VHS in December 1996. In 2007, a double CD and a DVD were issued as to commemorate the concert's tenth anniversary. The DVD featured newly recorded audio commentary by Finn, Hart and Seymour and other new bonus material.

Following the 1996 break-up of Crowded House, the members embarked upon a variety of projects. Neil Finn released two solo studio albums, "Try Whistling This" (1998) and "One Nil" (2001), as well as two live albums, "Sessions at West 54th" (2000) and "7 Worlds Collide" (2001). "7 Worlds Collide" saw him performing with guest musicians including Eddie Vedder, Johnny Marr, Ed O'Brien and Phil Selway of Radiohead, Tim Finn, Sebastian Steinberg, Lisa Germano and Betchadupa (featuring his son Liam Finn). A double CD and DVD of the shows were released in November 2001.

Tim Finn had resumed his solo career after leaving the group in 1992 and he also worked with Neil on a second Finn Brothers album, "Everyone Is Here", which was released in 2004. Paul Hester joined The Finn Brothers on stage for three songs at their Palais Theatre show in Melbourne at the end of 2004. Nick Seymour also joined them on stage in Dublin, where he was living, in 2004. Peter Jones and Nick Seymour joined Australian group Deadstar for their second album, "Milk", in 1997. Seymour later worked as a record producer in Dublin, producing Irish group Bell X1's debut album, "Neither Am I" in 2000. Mark Hart rejoined Supertramp in the late 1990s and later toured with Ringo Starr & His All-Starr Band. In 2001 he released a solo album, "Nada Sonata".

Paul Hester worked with children's entertainers The Wiggles, playing "Paul the Cook". He also had his own ABC show "Hessie's Shed" in Australia from late 1997. He formed the band Largest Living Things, which was the name rejected by Capitol Records in favour of Crowded House. It was on "Hessie's Shed" that Finn, Hester and Seymour last shared a stage, on an episode filmed as part of Finn's promotion for his solo album "Try Whistling This" in 1998. Finn and Hester performed "Not the Girl You Think You Are" with Largest Living Things, before being joined by Seymour for "Sister Madly" and a version of Paul Kelly's "Leaps and Bounds", which also featured Kelly on vocals. In late 2003, Hester hosted the series "Music Max's Sessions". Hester and Seymour were reunited when they both joined singer-songwriter Matt O'Donnell's Melbourne-based group Tarmac Adam. The band released one album, 2003's "Handheld Torch", which was produced by Seymour.

In May 1999 Crowded House issued a compilation of unreleased songs, "Afterglow", which included the track "Recurring Dream", recorded when the group were still called The Mullanes and included Craig Hooper on guitar. The album's liner notes included information about the songs, written by music journalist David Hepworth. Some limited-release versions included a second CD with songwriting commentary by Finn. The liner notes confirmed that Crowded House had no plans to reunite at that time. A 2003 compilation album, "Classic Masters", was released only in the US, while 2005 saw the release of the album "She Will Have Her Way", a collection of cover versions of Crowded House, Split Enz, Tim Finn and Finn Brothers songs by Australasian female artists. The album reached the top 5 in Australia and New Zealand.

On 26 March 2005 Paul Hester was found dead, after hanging himself from a tree in a park near his home in Melbourne. He was 46 years old. His obituary in "The Sydney Morning Herald" stated that he had fought "a long battle with depression." Following the news of Hester's death, Nick Seymour joined The Finn Brothers on stage at the Royal Albert Hall in London, where the three played in memory of Paul. A snare drum with a top hat on it stood at the front of the stage as a tribute. Writing in 2010 Neil Finn said, "When we lost Paul it was like someone pulled the rug out from underneath everything, a terrible jolt out of the dark blue. He was the best drummer I had ever played with and for many years, my closest friend."

In 2006 Neil Finn asked Nick Seymour to play bass on his third solo album. Seymour agreed and the two joined up with producer and multi-instrumentalist Ethan Johns to begin recording. As the recording sessions progressed it was decided that the album would be issued under the Crowded House band name, rather than as a Neil Finn solo album. In January 2007, the group publicly announced their reformation and on 23 February, after 20 days of auditions, former Beck drummer Matt Sherrod joined Finn, Seymour and Mark Hart to complete the new line up. As Sherrod and Hart had not participated in the initial sessions, four new tracks were recorded with producer Steve Lillywhite including the album's first single "Don't Stop Now".

On 17 March 2007 the band played a live show at their rehearsal studio in front of around fifty fans, friends and family. The performance was streamed live as a webcast. The two-and-a-half-hour set included some new tracks, including "Silent House" co-written by Finn with the Dixie Chicks. A concert onboard "The Thekla", moored in Bristol, followed on 19 March. Crowded House played at the Marquee Theatre in Tempe, Arizona on 26 April as a warm-up for their appearance at the Coachella Festival on 29 April in Indio, California. They also played at the Australian Live Earth concert in Sydney on 7 July. The next day, Finn and Seymour were interviewed on "Rove Live" and the band, with Hart and Sherrod, performed "Don't Stop Now" to promote the new album, which was titled "Time on Earth". The single was a minor hit in Australia and the UK. The album was released worldwide in June and July. It topped the album chart in New Zealand and made number 2 in Australia and number 3 in the UK.

On 6 December 2008 Crowded House played the Homebake festival in Sydney, with warm up gigs at small venues in Hobart, Melbourne and Sydney. For these shows the band were augmented by multi-instrumentalist Don McGlashan and Neil's younger son, Elroy Finn, on guitar. On 14 March 2009 the band joined Neil's older son, Liam Finn, on stage for three songs at the Sound Relief concert in Melbourne.

Crowded House began recording their follow-up to "Time on Earth" in April 2009, at Finn's own Roundhead Studios. The album, "Intriguer", was produced by Jim Scott who had worked on "The Sun Came Out" by Neil's 7 Worlds Collide project. In August 2009, Finn travelled to Los Angeles to record some overdubs at Jim Scott's Los Angeles studio before they began mixing tracks. The album was released in June 2010, in time for the band's appearance at the West Coast Blues & Roots Festival near Perth. Finn stated that the album contains some, "Unexpected twists and turns" and some songs that, "Sound like nothing we've done before." "Intriguer" topped the Australian album chart, reached number 3 in New Zealand and number 12 in the UK.

Crowded House undertook an extensive world tour in 2010 in support of "Intriguer". This was the first album where the band regularly interacted with fans via the internet on their own re-launched website, Twitter and Facebook. The band sold recordings of the shows on the "Intriguer" tour on USB flash drives and made individual live tracks available for free download. The band's final concert was at the A Day on the Green festival in Auckland on 27 February 2011.

A new compilation album, The Very Very Best of Crowded House, was released in October 2010 to celebrate the band's 25th anniversary. It includes 19 of the band's greatest hits and is also available in a box set with a 25 track DVD of their music videos. A deluxe digital version, available for download only, has 32 tracks including a rare 1987 live recording of the band's version of the Hunters & Collectors song "Throw Your Arms Around Me". No mention of this album has been made on the band's official website or Twitter page, which suggests that they are not involved with its release.

Following the success of the album "She Will Have Her Way" in 2005, a second album of cover versions of Finn Brothers songs (including numerous Crowded House songs) was released on 12 November 2010. Entitled "He Will Have His Way", all tracks on this album are performed by Australasian male artists. In November 2011, there was an Australian tour by various artists involved with the "She Will Have Her Way" and "He Will Have His Way" projects, including Paul Dempsey, Clare Bowditch, Seeker Lover Keeper (Sarah Blasko, Sally Seltmann and Holly Throsby), Alexander Gow (Oh Mercy) and Lior.

Former Crowded House drummer Peter Jones died from brain cancer on 18 May 2012 aged 49. A statement issued by the band described him as, "A warm-hearted, funny and talented man, who was a valuable member of Crowded House."

In September 2015, the song "Help Is Coming" from the "Afterglow" album, was released as a download and limited edition 7" single to raise money for the charity Save the Children. The B-side, "Anthem", was a previously unreleased track that was initially recorded in 1995, with the final vocal added in 2015. The money will be used to provide shelter, water, sanitation and hygiene for refugees in Syria, Lebanon and Iraq. Neil Finn said of "Help Is Coming"..."It was always a song about refugees, even if at the time I was thinking about the immigrants setting off on ships from Europe to America, looking for a better life for their families. There is such a huge scale and urgency to the current refugee crises that barely a day goes by without some crushing image or news account to confront us. We can't be silent any more."

In 2016, Neil Finn mentioned in an interview with the Dutch newspaper "Volkskrant" that Crowded House are on hiatus. Later that year, he and Seymour announced a series of concerts at the Sydney Opera House to mark the 20th anniversary of the "Farewell to the World" show (24 Nov 1996). The band performed four shows, 24–27 November 2016. Around the same time, each of the band's 7 studio albums (including the rarities collection "Afterglow") was reissued in deluxe 2-CD format with bonus tracks including demos, live recordings, alternate mixes, b-sides and outtakes.

In 2018, Neil Finn joins Fleetwood Mac, along with Mike Campbell of Tom Petty and the Heartbreakers, for upcoming tour in the wake of Lindsey Buckingham's departure. 

As the primary songwriter for the band, Neil Finn has always set the tone for the band's sound. Allmusic said that Finn "has consistently proven his knack for crafting high-quality songs that combine irresistible melodies with meticulous lyrical detail." Neil's brother Tim was an early and important musical influence. Neil first saw Tim play with Split Enz in 1972, and said "that performance and those first songs made a lasting impression on me." His mother was another significant musical influence, encouraging him to listen to a variety of genres, including Irish folk music and Māori music. She would play piano at family parties and encourage Neil and Tim to accompany her.

Bassist Nick Seymour, who is also an artist, designed or co-designed all of the band's album covers and interior artwork. He also designed some of the costumes worn by the group, notably those from the cover of the group's debut album "Crowded House". Seymour collaborated with Finn and Hester on the set design of some of their early music videos, including "Don't Dream It's Over" and "Better Be Home Soon". Since the band reunited, Seymour has again designed their album covers.

The majority of the covers for the band's singles were not designed by Seymour. The artwork for "Pineapple Head" was created by Reg Mombassa of Mental As Anything. For the first four albums Mombassa and Noel Crombie, who had been the main designer of Split Enz's artwork, assisted Seymour in creating sets and costumes. For the "Farewell to the World" concerts Crombie designed the set, while Mombassa and Seymour designed promotional materials and artwork.






Crowded House has won several national and international awards. In Australia, the group has won eleven ARIA Awards from 26 nominations, including the inaugural "Best New Talent" award in 1987. The majority of their ARIAs were awarded for their first two albums, "Crowded House" and "Temple of Low Men". They won eight APRA Awards from eleven nominations and were nominated for "The New Zealand Silver Scroll" for "Don't Stop Now" in 2007. "Don't Dream It's Over" was named the seventh best Australian song of all time in 2001. In 1987, Crowded House won the American MTV Video Music Award for "Best New Artist" for their song "Don't Dream It's Over", which was also nominated for three other awards. In 1994, the group was named "International Group of the Year" at the BRIT Awards. In 2009, "Don't Dream It's Over" was ranked number fifty on the Triple J "Hottest 100 of All Time", voted by the Australian public.

In November 2016 Crowded House were inducted into the ARIA Hall of Fame, 30 years after their formation.







</doc>
<doc id="6928" url="https://en.wikipedia.org/wiki?curid=6928" title="Colette">
Colette

Colette (; Sidonie-Gabrielle Colette, 28 January 1873 – 3 August 1954) was a French novelist nominated for the Nobel Prize in Literature in 1948. Her best known work, the novella "Gigi" (1944), was the basis for the film and Lerner and Loewe stage production of the same name. She was also a mime, an actress, and a journalist.

Sidonie-Gabrielle Colette was born on January 28, 1873, to war hero and tax collector Jules-Joseph Colette and his wife Adèle Eugénie Sidonie ("Sido"), "nėe" Landoy, in the village of Saint-Sauveur-en-Puisaye in the "département" of Yonne, Burgundy. The family was initially well off, but by the time she was of school age poor financial management had substantially reduced her father's income and she attended a public school from the ages of 6 to 17 — this was, nevertheless, a fairly extensive education for a girl of the period.

In 1893 she married Henry Gauthier-Villars (1859–1931) or "Willy", his nom-de-plume, a well-known author and publisher, and her first four novels — the four Claudine stories, "Claudine à l'école" (1900), "Claudine à Paris" (1901), "Claudine en ménage" (1902), and "Claudine s'en va" (1903) — appeared under his name. They chart the coming of age of their heroine, Claudine, from an unconventional fifteen-year-old in a Burgundian village to the literary salons of turn-of-the-century Paris. (The four are published in English as "Claudine at School", "Claudine in Paris", "Claudine Married", and "Claudine and Annie"). The story they tell is semi-autobiographical, but not entirely — most strikingly, Claudine, unlike Colette, is motherless.

Willy, fourteen years older than his wife and one of the most notorious libertines in Paris, introduced Colette into avant-garde intellectual and artistic circles while engaging in sexual affairs and encouraging her own lesbian alliances. It was he who chose the titillating subject-matter of the Claudine novels, "the secondary myth of Sappho...the girls' school or convent ruled by a seductive female teacher" (Ladimer, p. 53). Colette later said that she would never have become a writer if it had not been for Willy.

Colette and Willy separated in 1906, although it was not until 1910 that the divorce became final. She had no access to the sizable earnings of the Claudine books — the copyright belonged to Willy — and until 1912 she followed a stage career in music halls across France, sometimes playing Claudine in sketches from her own novels, earning barely enough to survive and often hungry and unwell. This period of her life is recalled in "La Vagabonde" (1910), which deals with women's independence in a male society, a theme to which she would regularly return in future works. During these years she embarked on a series of relationships with other women, notably with Mathilde de Morny, Marquise de Belbeuf ("Missy"), with whom she sometimes shared the stage. On January 3, 1907, an onstage kiss between Missy and Colette in a pantomime entitled "Rêve d'Égypte" caused a near-riot, and as a result they were no longer able to live together openly, although their relationship continued for another five years.

In 1912 she married Henry de Jouvenel, the editor of "Le Matin". A daughter, Colette de Jouvenel, nicknamed "Bel-Gazou", was born in 1913. During the war she devoted herself to journalism, but marriage allowed her to devote her time to writing.

In 1920 Colette published "Chéri", portraying love between an older woman and a much younger man. Chéri is the lover of Léa, a wealthy courtesan; Léa is devastated when Chéri marries a girl his own age, and delighted when he returns to her, but after one final night together she sends him away again.

The marriage to Jouvenel ended in divorce in 1924, partly due to Jouvenel's infidelities and partly to Colette's own affair with her sixteen-year-old stepson, Bertrand de Jouvenel. In 1925 she met Maurice Goudeket, who became her final husband (the couple stayed together until her death).

Already an established writer ("The Vagabond" had received three votes for the prestigious "Prix Goncourt"), the decades of the 1920s and 1930s were Colette's most productive and innovative period. Set mostly in Burgundy or Paris during the "Belle Époque", her work treated married life and sexuality. It was frequently quasi-autobiographical: "Chéri" (1920) and "Le Blé en herbe" (1923) both deal with love between an aging woman and a very young man, a situation reflecting her relationship with Bertrand de Jouvenel and even Goudeket, who was sixteen years her junior. "La Naissance du Jour" (1928) is her explicit criticism of the conventional lives of women, expressed in a meditation on age and the renunciation of love through the character of her mother, Sido.

By this period Colette was frequently acclaimed as France's greatest woman writer. "It... has no plot, and yet tells of three lives all that should be known", wrote Janet Flanner of "Sido". "Once again, and at greater length than usual, she has been hailed for her genius, humanities and perfect prose by those literary journals which years ago... lifted nothing at all in her direction except the finger of scorn."

Colette was 67 years old at the fall of France, and remained in Paris, in her apartment in the Palais Royal. Her husband Maurice Goudeket, a Jew, was arrested by the Gestapo in December 1941, and although he was released after a few months through the intervention of the French wife of the German ambassador, Colette lived through the rest of the war years with the anxiety of a possible second arrest. During the Occupation she produced two volumes of memoirs, "Journal à rebours" (1941) and "De ma fenêtre" (1942 — the two issued in English in 1975 as "Looking Backwards").

In 1944 she published what became perhaps her most famous work, "Gigi", telling the story of sixteen year old Gilberte ("Gigi") Alvar. Born into a family of demimondaines, Gigi is being trained as a courtesan to captivate a wealthy lover, but breaks with tradition by marrying him instead. In 1949 it was made into a French film starring Danièle Delorme and Gaby Morlay, then in 1951 adapted for the stage with the then-unknown Audrey Hepburn in the title role, picked by Colette personally; the 1958 Hollywood musical, starring Leslie Caron and Louis Jourdan, with a screenplay by Alan Jay Lerner and a score by Lerner and Frederick Loewe, won the Academy Award for Best Picture.

In the postwar years, she became a famous public figure, crippled by arthritis and cared for by Goudeket, who supervised the preparation of her collected works, or "Œuvres completes" (1948–1950). She continued to write during these years, bringing out "L'Etoile vesper" (1944) and "Le Fanal bleu" (1949), in which she reflected on the problems of a writer whose inspiration is primarily autobiographical. On her death on August 3, 1954, she was refused a religious funeral by the Catholic Church on account of her divorces, but was given a state funeral, the first French woman of letters to be granted this honour, and interred in Père-Lachaise cemetery.

Colette was elected to the Belgian Royal Academy (1935), the Académie Goncourt (1945, and President in 1949), and a Chevalier (1920) and Grand Officer (1953) of the Légion d'honneur.

Initially considered a limited if talented novelist (despite the outspoken admiration in her lifetime of figures such as André Gide and Henri de Montherlant), she has been increasingly recognised as an important voice in women's writing.

Singer-songwriter Rosanne Cash paid tribute to the writer in the song, "The Summer I Read Colette", on her 1996 album "10 Song Demo".

Truman Capote wrote a short story about her (1970) called "The White Rose".

"Lucette Stranded on the Island" by Julia Holter, from her 2015 album "Have You in My Wilderness", is based on a minor character from Colette's short story "Chance Acquaintances".

In the 2018 film "Colette" the novelist is played by Keira Knightley.

Source: 






</doc>
<doc id="6932" url="https://en.wikipedia.org/wiki?curid=6932" title="Charles Alston">
Charles Alston

Charles Henry Alston (November 28, 1907 – April 27, 1977) was an African-American painter, sculptor, illustrator, muralist and teacher who lived and worked in the New York City neighborhood of Harlem. Alston was active in the Harlem Renaissance; Alston was the first African-American supervisor for the Works Progress Administration's Federal Art Project. Alston designed and painted murals at the Harlem Hospital and the Golden State Mutual Life Insurance Building. In 1990 Alston's bust of Martin Luther King, Jr. became the first image of an African American displayed at the White House.

Charles Henry Alston was born on November 28, 1907, in Charlotte, North Carolina, to Reverend Primus Priss Alston and Anna Elizabeth Miller Alston, and was the youngest of five children. Only three survived past infancy: Charles, his sister Rousmaniere and his brother Wendell. His father was born into slavery in 1851 in Pittsboro, North Carolina; after the Civil War, he graduated from St. Augustine's College and became a prominent minister and founder of St. Michael's Episcopal Church. He was described as a "race man": an African American who dedicated his skills to the furtherance of the black race. Reverend Alston met his wife when she was a student at his school. Charles was nicknamed "Spinky" by his father, and kept the nickname as an adult. In 1910, when Charles was three, his father died suddenly of a cerebral hemorrhage. Locals described him in admiration as the "Booker T. Washington of Charlotte".

In 1913 Anna Alston married Harry Bearden. Through the marriage, the future artist Romare Bearden became Charles’ cousin. The two Bearden families lived across the street from each other; the friendship between Romare and Charles would last a lifetime. As a child Alston was inspired by his older brother Wendell's drawings of trains and cars, which the young artist copied. Charles also played with clay, creating a sculpture of North Carolina. As an adult he reflected on his memories of sculpting with clay as a child: "I’d get buckets of it and put it through strainers and make things out of it. I think that's the first art experience I remember, making things." His mother was a skilled embroiderer and took up painting at the age of 75. His father was also good at drawing, wooing Alston's mother with small sketches in the medians of letters he wrote her.

In 1915 the family moved to New York, as many African-American families did during the Great Migration. Alston's step-father, Henry Bearden, left before his wife and children to secure a job overseeing elevator operations and the newsstand staff at the Bretton Hotel in the Upper West Side. The family lived in Harlem and was considered middle-class. During the Great Depression, the people of Harlem suffered economically. The "stoic strength" seen within the community was later expressed in Charles’ fine art. At Public School 179 in Manhattan, the boy's artistic abilities were recognized and he was asked to draw all of the school posters during his years there.

Harry and Anna Bearden had a dauther, Aida C. Bearden (1917–2007) who, on June 9, 1943, in Manhattan, married operatic baritone Lawrence Whisonant.

Alston graduated from DeWitt Clinton High School, where he was nominated for academic excellence and was the art editor of the school's magazine, "The Magpie". He was a member of the Arista - National Honor Society and also studied drawing and anatomy at the Saturday school of the National Academy of Art . In high school he was given his first oil paints and learned about his aunt Bessye Bearden's art salons, which stars like Duke Ellington and Langston Hughes attended. After graduating in 1925, he attended Columbia University, turning down a scholarship to the Yale School of Fine Arts.

Alston entered the pre-architectural program only to lose interest upon seeing the lack of success many African-American architects had in the field. After also experimenting with pre-med, he decided that math, physics and chemistry "was not just my bag" and he entered the fine arts program. During his time at Columbia he joined Alpha Phi Alpha, worked on the university's "Columbia Daily Spectator" and drew cartoons for the school's magazine "Jester". He also hung out in Harlem restaurants and clubs, where his love for jazz and black music would be fostered. In 1929 he graduated and received a fellowship to study at Teachers College, where he obtained his Master's in 1931.

For the years 1942–43 Alston was stationed in the army at Fort Huachuca in Arizona. Upon returning to New York on April 8, 1944, he married Dr. Myra Adele Logan, then an intern at the Harlem Hospital. They met when he was working on a mural project at the hospital. Their home, including his studio, as on Edgecombe Avenue near Highbridge Park. The couple lived close to family; at their frequent gatherings Alston enjoyed cooking and Myra played piano. During the 1940s Alston also took occasional art classes studying under Alexander Kostellow.

In January 1977 Myra Logan died. Months later on April 27, 1977, Charles "Spinky" Alston died after a long bout with cancer. His memorial service was held at St. Martins Episcopal Church on May 21, 1977, in New York City.

While obtaining his master's degree, Alston was the boys’ work director at the Utopia Children's House, started by James Lesesne Wells. He also began teaching at the Harlem Community Art Center, founded by Augusta Savage in the basement of what is now the Schomburg Center for Research in Black Culture. Alston's teaching style was influenced by the work of John Dewey, Arthur Wesley Dow, and Thomas Munro. During this period, Alston began to teach the 10-year-old Jacob Lawrence, whom he strongly influenced. Alston was introduced to African art by the poet Alain Locke. In the late 1920s Alston joined Bearden and other black artists who refused to exhibit in William E. Harmon Foundation shows, which featured all-black artists in their traveling exhibits. Alston and his friends thought the exhibits were curated for a white audience, a form of segregation which the men protested. They did not want to be set aside but exhibited on the same level as art peers of every skin color.

In 1938 the Rosenwald Fund provided money for Alston to travel to the South, which was his first return there since leaving as a child. His travel with Giles Hubert, an inspector for the Farm Security Administration, gave him access to certain situations and he photographed many aspects of rural life. These photographs serves as the basis for a series of genre portraits depicting southern black life. In 1940 he completed "Tobacco Farmer", the portrait of a young black farmer in white overalls and a blue shirt with a youthful yet serious look upon his face, sitting in front of the landscape and buildings he works on and in. That same year he received a second round of funding from the Rosenwald Fund to travel South, and he spent extended time at Atlanta University.

During the 1930s and early 1940s, Alston created illustrations for magazines such as "Fortune", "Mademoiselle", "The New Yorker", "Melody Maker" and others. He also designed album covers for artists such as Duke Ellington and Coleman Hawkins. Alston became staff artist at the Office of War Information and Public Relations in 1940, creating drawings of notable African Americans. These images were used in over 200 black newspapers across the country by the government to "foster goodwill with the black citizenry."

Eventually Alston left commercial work to focus on his own artwork. In 1950, he became the first African-American instructor at the Art Students League, where he remained on faculty until 1971. In 1950, his "Painting" was exhibited at the Metropolitan Museum of Art and his artwork was one of few purchased by the museum. He landed his first solo exhibition in 1953 at the John Heller Gallery, which represented artists such as Roy Lichtenstein. He exhibited there five times from 1953 to 1958.

In 1956, he became the first African-American instructor at the Museum of Modern Art, where he taught for a year before going to Belgium on behalf of MOMA and the State Department. He coordinated the children's community center at Expo 58. In 1958 he was awarded a grant from and was elected as a member of the American Academy of Arts and Letters.

In 1963, Alston co-founded Spiral with Romare Bearden and Hale Woodruff. Spiral served as a collective of conversation and artistic exploration for a large group of artists who "addressed how black artists should relate to American society in a time of segregation." Artists and arts supporters gathered for Spiral, such as Emma Amos, Perry Ferguson and Merton Simpson. 

In 1968, Alston received a presidential appointment from Lyndon Johnson to the National Council of Culture and the Arts. Mayor John Lindsay appointed him to the New York City Art Commission in 1969. He was made full professor at City College of New York in 1973 where he had taught since 1968. In 1975 he was awarded the first Distinguished Alumni Award from Teachers College. The Art Student's League created a 21-year merit scholarship in 1977 under Alston's name to commemorate each year of his tenure.

Alston shared studio space with Henry Bannarn at 306 W. 141st Street, which served as an open space for artists, photographers, musicians, writers and the like. Other artists held studio space at 306, such as Jacob Lawrence, Addison Bate and his brother Leon. During this time Alston founded the Harlem Artists Guild with Savage and Elba Lightfoot to work towards equality in WPA art programs in New York. During the early years of 306, Alston focused on mastering portraiture. Early works such as "Portrait of a Man" (1929) show Alston's detailed and realistic style depicted through pastels and charcoals, inspired by the style of Winold Reiss. In his "Girl in a Red Dress" (1934) and "The Blue Shirt" (1935), he used modern and innovative techniques for his portraits of young individuals in Harlem. "Blue Shirt" is thought to be a portrait of Jacob Lawrence. During this time he also created "Man Seated with Travel Bag" (c. 1938–40), showing the seedy and bleak environment, contrasting with work like the racially charged "Vaudeville" (c. 1930) and its caricature style of a man in blackface.

Inspired from his trip south, Alston began his "family series" in the 1940s. Intensity and angularity come through in the faces of the youth in his portraits "Untitled (Portrait of a Girl)" and "Untitled (Portrait of a Boy)". These works also show the influence that African sculpture had on his portraiture, with "Portrait of a Boy" showing more cubist features. Later family portraits show Alston's exploration of religious symbolism, color, form and space. His family group portraits are often faceless, which Alston states is the way that white America views blacks. Paintings such as "Family" (1955) show a woman seated and a man standing with two children – the parents seem almost solemn while the children are described as hopeful and with a use of color made famous by Cézanne. In "Family Group" (c. 1950) Alston's use of gray and ochre tones brings together the parents and son as if one with geometric patterns connecting them together as if a puzzle. The simplicity of the look, style and emotion upon the family is reflective and probably inspired by Alston's trip south. His work during this time has been described as being "characterized by his reductive use of form combined with a sun-hued". During this time he also started to experiment with ink and wash painting seen in work such as "Portrait of a Woman" (1955) as well as creating portraits to illustrate the music surrounding him in Harlem. "Blues Singer #4" shows a female singer on stage with a white flower on her shoulder and a bold red dress, reminiscent of Ella Fitzgerald. "Girl in a Red Dress" is thought to be Bessie Smith, for whom he drew many times when she was recording and performing. Jazz was an important influence in Alston's work and social life, representing itself in other works like "Jazz" (1950) and "Harlem at Night".

The 1960s civil rights movement influenced his work heavily with artworks influenced by inequality and race relations in the United States. One of his few religious artworks was created in 1960, "Christ Head", with an angular "Modiglianiesque" portrait of Jesus Christ. Seven years later he created "You never really meant it, did you, Mr. Charlie?" which, in a similar style as "Christ Head" shows a black man standing against a red sky "looking as frustrated as any individual can look", according to Alston.

Experimenting with the use of negative space and organic forms in the late 1940s, by the mid-1950s Alston began creating notably modernist style paintings. "Woman with Flowers" (1949) has been described as a tribute to Modigliani and African art makes another strong appearance in "Ceremonial" (1950). Untitled works during the era show his use of color overlay using muted colors to create simple layered abstracts of still live. "Symbol" (1953) relates to Picasso's "Guernica", which was a favorite work of Alston's. His final work of the 1950s, "Walking", was inspired by the Montgomery Bus Boycott and has come to represent "the surge of energy among African Americans to organize in their struggle for full equality." About the artwork, Alston is quoted "The idea of a march was growing...It was in the air...and this painting just came. I called it "Walking" on purpose. It wasn't the militancy that you saw later. It was a very definite walk-not going back, no hesitation."

The civil rights movement of the 1960s was a major influence on Alston. Considered to be one of his most powerful and impressive periods in the late 1950s he began working in black and white up until the mid-1960s. Some of the works are simple abstracts of black ink on white paper, similar to a Rorschach test. "Untitled" (c. 1960s) shows a boxing match in great simplicity with an attempt to express the drama of the fight through few brushstrokes. Alston worked with oil-on-Masonite during this period as well, utilizing impasto, cream and ochre to create a moody cave-like artwork. "Black and White #1" (1959) is one of Alston's more "monumental" works. Gray, white and black come together to fight for space on an abstract canvas, in a softer form than the more harsh Franz Kline. Alston continued to explore the relationship between monochromatic hues throughout the series which Wardlaw describes as "some of the most profoundly beautiful works of twentieth-century American art."

In the beginning Charles Alston's mural work was inspired by the work of Aaron Douglas, Diego Rivera and José Clemente Orozco, the latter who he met when they did mural work in New York. In 1943 Alston was elected to the board of directors of the National Society of Mural Painters. He created murals for the Harlem Hospital, Golden State Mutual, American Museum of Natural History, Public School 154, the Bronx Family and Criminal Court and the Abraham Lincoln High School in Brooklyn, New York.

Originally hired as an easel painter, in 1935 Alston became the first African-American supervisor to work for the WPA's Federal Art Project (FAP) in New York, which would also serve as his first mural work. At this time he was awarded WPA Project Number 1262 – an opportunity to oversee a group of artists creating murals and to supervise their painting for the Harlem Hospital. The first government commission ever awarded to African-American artists including Beauford Delaney, Seabrook Powell and Vertis Hayes. He also had the chance to create and paint his own contribution to the collection: "Magic in Medicine" and "Modern Medicine". These paintings were part of a diptych completed in 1936 depicting the history of medicine in the African-American community and Beauford Delaney served as assistant. When creating the murals Alston was inspired by the work of Aaron Douglas, who a year earlier had created the public art piece "Aspects of Negro Life" for the New York Public Library, and researched traditional African culture, including traditional African medicine. "Magic in Medicine", which depicts African culture and holistic healing, is considered one of "America's first public scenes of Africa". All of the murals sketches submitted were accepted by the FAP, however, four were denied creation by the hospital superintendent Lawrence T. Dermody and commissioner of hospitals S.S. Goldwater due to the excessive amount of African-American representation in the works. The artists fought the response through letter writing and four years later succeeded in gaining the right to complete the murals. The sketches for "Magic in Medicine" and "Modern Medicine" were exhibited in the Museum of Modern Art's "New Horizons in American Art".

Alston's murals were hung in the Women's Pavilion of the hospital over uncapped radiators which caused the paintings to deteriorate from the steam. Plans failed to recap the radiators. In 1959 Alston estimated, in a letter to the Department of Public Works, that the conservation would cost $1,500 but the funds were never acquired. In 1968, after Martin Luther King Jr.'s death, Alston was asked to create another mural for the hospital to be placed in a pavilion named after the assassinated civil rights leader titled "Man Emerging from the Darkness of Poverty and Ignorance into the Light of a Better World." One year after Alston's death in 1977, a group of artists and historians, including the renowned painter and collagist Romare Bearden and art historian Greta Berman, together with administrators from the hospital, and from the NYC Art Commission, examined the murals, and presented a proposal for their restoration to then-mayor Ed Koch. The request was approved, and conservator Alan Farancz set to work in 1979, rescuing the murals from further decay. Many years passed, and the murals began to deteriorate again – especially the Alston works, which continued to suffer effects from the radiators. In 1991 the Municipal Art Society's Adopt-a-Mural program was launched and the Harlem Hospital murals were chosen for further restoration (Greta Berman. Personal experience). A grant from Alston's sister Rousmaniere Wilson and step-sister Aida Bearden Winters assisted in completing a restoration of the works in 1993. In 2005 Harlem Hospital announced a $2 million project to conserve Alston's murals and three other pieces in the original commissioned project as part of a $225 million hospital expansion.

In the late 1940s Alston became involved in a mural project commissioned by Golden State Mutual Life Insurance Company which asked the artists to create work involving African-American contributions to the settling of California. Alston worked with Hale Woodruff on the murals in a large studio space in New York where they utilized ladders to reach the upper parts of the canvas. The artworks, which are considered "priceless contributions to American narrative art", consists of two panels: "Exploration and Colonization" by Alston and "Settlement and Development" by Woodruff. Alston's piece covers the post-colonial period of 1527 to 1850. Images of James Beckwourth, Biddy Mason, and William Leidesdorff are portrayed in the well detailed historical mural. While both artists kept in contact with African Americans on the West Coast during its creation, influencing the content and depictions. The murals, which were unveiled in 1949, have been on display in the lobby of the Golden State Mutual Headquarters. Due to economic downturn Golden State was forced to sell their entire art collection to ward off its mounting debts and as of spring 2011 the National Museum of African American History and Culture had offered $750,000 to purchase the artworks which led to a controversy regarding the importance of the artworks which have been estimated to be worth at least $5 million. It was requested that the murals be covered by city landmark protections by the Los Angeles Conservancy. The state of California had declined philanthropic proposals to keep the murals in their original location and the Smithsonian withdrew their offer. The murals are currently awaiting their fate in California courts.

Alston also created sculptures. "Head of a Woman" (1957) shows his move towards a "reductive and modern approach to sculpture...where facial features were suggested rather than fully formulated in three dimensions,". In 1970 Alston was commissioned by the Community Church of New York to create a bust of Martin Luther King Jr. for $5,000, with only five copies produced. In 1990 Alston's bronze bust of Martin Luther King Jr. (1970), became the first image of an African American displayed in the White House.

Art critic Emily Genauer stated that Alston "refused to be pigeonholed", regarding his varied exploration in his artwork. Patron Lemoine Pierce said of Alston's work: "Never thought of as an innovative artist, Alston generally ignored popular art trends and violated many mainstream art conventions; he produced abstract and figurative paintings often simultaneously, refusing to be stylistically consistent, and during his 40-year career he worked prolifically and unapologetically in both commercial and fine art." Romare Bearden described Alston as "...one of the most versatile artists whose enormous skill led him to a diversity of styles..." Bearden also describes the professionalism and impact that Alston had on Harlem and the African-American community: "'was a consummate artist and a voice in the development of African American art who never doubted the excellence of all people's sensitivity and creative ability. During his long professional career, Alston significantly enriched the cultural life of Harlem. In a profound sense, he was a man who built bridges between Black artists in varying fields, and between other Americans." Writer June Jordan described Alston as "an American artist of first magnitude, and he is a Black American artist of undisturbed integrity."







</doc>
<doc id="6933" url="https://en.wikipedia.org/wiki?curid=6933" title="Chromatin">
Chromatin

Chromatin is a complex of macromolecules found in cells, consisting of DNA, protein, and RNA. The primary functions of chromatin are 1) to package DNA into a more compact, denser shape, 2) to reinforce the DNA macromolecule to allow mitosis, 3) to prevent DNA damage, and 4) to control gene expression and DNA replication. The primary protein components of chromatin are histones that compact the DNA. Chromatin is only found in eukaryotic cells (cells with defined nuclei). Prokaryotic cells have a different organization of their DNA (the prokaryotic chromosome equivalent is called genophore and is localized within the nucleoid region). 

Chromatin's structure is currently poorly understood despite being subjected to intense investigation. Its structure depends on several factors. The overall structure depends on the stage of the cell cycle. During interphase, the chromatin is structurally loose to allow access to RNA and DNA polymerases that transcribe and replicate the DNA. The local structure of chromatin during interphase depends on the genes present on the DNA. That DNA which codes genes that are actively transcribed ("turned on") is more loosely packaged and associated with RNA polymerases (referred to as euchromatin) while that DNA which codes inactive genes ("turned off") is more condensed and associated with structural proteins (heterochromatin). Epigenetic chemical modification of the structural proteins in chromatin also alters the local chromatin structure, in particular chemical modifications of histone proteins by methylation and acetylation. As the cell prepares to divide, i.e. enters mitosis or meiosis, the chromatin packages more tightly to facilitate segregation of the chromosomes during anaphase. During this stage of the cell cycle this makes the individual chromosomes in many cells visible by optical microscope.

In general terms, there are three levels of chromatin organization:
There are, however, many cells that do not follow this organisation. For example, spermatozoa and avian red blood cells have more tightly packed chromatin than most eukaryotic cells, and trypanosomatid protozoa do not condense their chromatin into visible chromosomes for mitosis.

Chromatin undergoes various structural changes during a cell cycle. Histone proteins are the basic packer and arranger of chromatin and can be modified by various post-translational modifications to alter chromatin packing (Histone modification). Most of the modifications occur on the histone tail. The consequences in terms of chromatin accessibility and compaction depend both on the amino-acid that is modified and the type of modification. For example, Histone acetylation results in loosening and increased accessibility of chromatin for replication and transcription. Lysine tri-methylation can either be correlated with transcriptional activity (tri-methylation of histone H3 Lysine 4) or transcriptional repression and chromatin compaction (tri-methylation of histone H3 Lysine 9 or 27). Several studies suggested that different modifications could occur simultaneously. For example, it was proposed that a bivalent structure (with tri-methylation of both Lysine 4 and 27 on histone H3) was involved in mammalian early development.

Polycomb-group proteins play a role in regulating genes through modulation of chromatin structure.

For additional information, see Histone modifications in chromatin regulation and RNA polymerase control by chromatin structure.

In nature, DNA can form three structures, A-, B-, and Z-DNA. A- and B-DNA are very similar, forming right-handed helices, whereas Z-DNA is a left-handed helix with a zig-zag phosphate backbone. Z-DNA is thought to play a specific role in chromatin structure and transcription because of the properties of the junction between B- and Z-DNA.

At the junction of B- and Z-DNA, one pair of bases is flipped out from normal bonding. These play a dual role of a site of recognition by many proteins and as a sink for torsional stress from RNA polymerase or nucleosome binding.

The basic repeat element of chromatin is the nucleosome, interconnected by sections of linker DNA, a far shorter arrangement than pure DNA in solution.

In addition to the core histones, there is the linker histone, H1, which contacts the exit/entry of the DNA strand on the nucleosome. The nucleosome core particle, together with histone H1, is known as a chromatosome. Nucleosomes, with about 20 to 60 base pairs of linker DNA, can form, under non-physiological conditions, an approximately 10 nm "beads-on-a-string" fibre. (Fig. 1-2). .

The nucleosomes bind DNA non-specifically, as required by their function in general DNA packaging. There are, however, large DNA sequence preferences that govern nucleosome positioning. This is due primarily to the varying physical properties of different DNA sequences: For instance, adenine and thymine are more favorably compressed into the inner minor grooves. This means nucleosomes can bind preferentially at one position approximately every 10 base pairs (the helical repeat of DNA)- where the DNA is rotated to maximise the number of A and T bases that will lie in the inner minor groove. (See mechanical properties of DNA.)

With addition of H1, the beads-on-a-string structure in turn coils into a 30 nm diameter helical structure known as the 30 nm fibre or filament. The precise structure of the chromatin fibre in the cell is not known in detail, and there is still some debate over this.

This level of chromatin structure is thought to be the form of heterochromatin, which contains mostly transcriptionally silent genes. EM studies have demonstrated that the 30 nm fibre is highly dynamic such that it unfolds into a 10 nm fiber ("beads-on-a-string") structure when transversed by an RNA polymerase engaged in transcription.
The existing models commonly accept that the nucleosomes lie perpendicular to the axis of the fibre, with linker histones arranged internally.
A stable 30 nm fibre relies on the regular positioning of nucleosomes along DNA. Linker DNA is relatively resistant to bending and rotation. This makes the length of linker DNA critical to the stability of the fibre, requiring nucleosomes to be separated by lengths that permit rotation and folding into the required orientation without excessive stress to the DNA.
In this view, different lengths of the linker DNA should produce different folding topologies of the chromatin fiber. Recent theoretical work, based on electron-microscopy images
of reconstituted fibers supports this view.

The spatial arrangement of the chromatin within the nucleus is not random - specific regions of the chromatin can be found in certain territories. Territories are, for example, the lamina-associated domains (LADs), and the topological association domains (TADs), which are bound together by protein complexes. Currently, polymer models such as the Strings & Binders Switch (SBS) model and the Dynamic Loop (DL) model are used to describe the folding of chromatin within the nucleus.


Chromatin and its interaction with enzymes has been researched, and a conclusion being made is that it is relevant and an important factor in gene expression. Vincent G. Allfrey, a professor at Rockefeller University, stated that RNA synthesis is related to histone acetylation. The lysine amino acid attached to the end of the histones is positively charged. The acetylation of these tails would make the chromatin ends neutral, allowing for DNA access.

When the chromatin decondenses, the DNA is open to entry of molecular machinery. Fluctuations between open and closed chromatin may contribute to the discontinuity of transcription, or transcriptional bursting. Other factors are probably involved, such as the association and dissociation of transcription factor complexes with chromatin. The phenomenon, as opposed to simple probabilistic models of transcription, can account for the high variability in gene expression occurring between cells in isogenic populations

During metazoan spermiogenesis, the spermatid's chromatin is remodeled into a more spaced-packaged, widened, almost crystal-like structure. This process is associated with the cessation of transcription and involves nuclear protein exchange. The histones are mostly displaced, and replaced by protamines (small, arginine-rich proteins). It is proposed that in yeast, regions devoid of histones become very fragile after transcription; HMO1 an HMGB protein helps in stabilizing nucleosomes-free chromatin.

The packaging of eukaryotic DNA into chromatin presents a barrier to all DNA-based processes that require recruitment of enzymes to their sites of action. To allow the critical cellular process of DNA repair, the chromatin must be remodeled. In eukaryotes, ATP dependent chromatin remodeling complexes and histone-modifying enzymes are two predominant factors employed to accomplish this remodeling process.

Chromatin relaxation occurs rapidly at the site of a DNA damage. This process is initiated by PARP1 protein that starts to appear at DNA damage in less than a second, with half maximum accumulation within 1.6 seconds after the damage occurs. Next the chromatin remodeler Alc1 quickly attaches to the product of PARP1, and completes arrival at the DNA damage within 10 seconds of the damage. About half of the maximum chromatin relaxation, presumably due to action of Alc1, occurs by 10 seconds. This then allows recruitment of the DNA repair enzyme MRE11, to initiate DNA repair, within 13 seconds.

γH2AX, the phosphorylated form of H2AX is also involved in the early steps leading to chromatin decondensation after DNA damage occurrence. The histone variant H2AX constitutes about 10% of the H2A histones in human chromatin. γH2AX (H2AX phosphorylated on serine 139) can be detected as soon as 20 seconds after irradiation of cells (with DNA double-strand break formation), and half maximum accumulation of γH2AX occurs in one minute. The extent of chromatin with phosphorylated γH2AX is about two million base pairs at the site of a DNA double-strand break. γH2AX does not, itself, cause chromatin decondensation, but within 30 seconds of irradiation, RNF8 protein can be detected in association with γH2AX. RNF8 mediates extensive chromatin decondensation, through its subsequent interaction with CHD4, a component of the nucleosome remodeling and deacetylase complex NuRD.

After undergoing relaxation subsequent to DNA damage, followed by DNA repair, chromatin recovers to a compaction state close to its pre-damage level after about 20 min.


The term, introduced by Walther Flemming, has multiple meanings:

The following scientists were recognized for their contributions to chromatin research with Nobel Prizes:




</doc>
<doc id="6934" url="https://en.wikipedia.org/wiki?curid=6934" title="Condition number">
Condition number

In the field of numerical analysis, the condition number of a function with respect to an argument measures how much the output value of the function can change for a small change in the input argument. This is used to measure how sensitive a function is to changes or errors in the input, and how much error in the output results from an error in the input. Very frequently, one is solving the inverse problem – given formula_1 one is solving for "x," and thus the condition number of the (local) inverse must be used. In linear regression the condition number can be used as a diagnostic for multicollinearity.

The condition number is an application of the derivative, and is formally defined as the value of the asymptotic worst-case relative change in output for a relative change in input. The "function" is the solution of a problem and the "arguments" are the data in the problem. The condition number is frequently applied to questions in linear algebra, in which case the derivative is straightforward but the error could be in many different directions, and is thus computed from the geometry of the matrix. More generally, condition numbers can be defined for non-linear functions in several variables.

A problem with a low condition number is said to be well-conditioned, while a problem with a high condition number is said to be ill-conditioned. The condition number is a property of the problem. Paired with the problem are any number of algorithms that can be used to solve the problem, that is, to calculate the solution. Some algorithms have a property called backward stability. In general, a backward stable algorithm can be expected to accurately solve well-conditioned problems. Numerical analysis textbooks give formulas for the condition numbers of problems and identify known backward stable algorithms.

As a rule of thumb, if the condition number formula_2, then you may lose up to formula_3 digits of accuracy on top of what would be lost to the numerical method due to loss of precision from arithmetic methods. However, the condition number does not give the exact value of the maximum inaccuracy that may occur in the algorithm. It generally just bounds it with an estimate (whose computed value depends on the choice of the norm to measure the inaccuracy).

For example, the condition number associated with the linear equation
"Ax" = "b" gives a bound on how inaccurate the solution "x" will be after approximation. Note that this is before the effects of round-off error are taken into account; conditioning is a property of the matrix, not the algorithm or floating point accuracy of the computer used to solve the corresponding system. In particular, one should think of the condition number as being (very roughly) the rate at which the solution, "x", will change with respect to a change in "b". Thus, if the condition number is large, even a small error in "b" may cause a large error in "x". On the other hand, if the condition number is small then the error in "x" will not be much bigger than the error in "b".

The condition number is defined more precisely to be the maximum ratio of the relative error in "x" to the relative error in "b".

Let "e" be the error in "b". Assuming that "A" is a nonsingular matrix, the error in the solution "A""b" is "A""e". The ratio of the relative error in the solution to the relative error in "b" is

This is easily transformed to

The maximum value (for nonzero "b" and "e") is then seen to be the product of the two operator norms as follows:

The same definition is used for any consistent norm, i.e. one that satisfies

When the condition number is exactly one (which can only happen if "A" is a scalar multiple of a linear isometry), then a solution algorithm can find (in principle, meaning if the algorithm introduces no errors of its own) an approximation of the solution whose precision is no worse than that of the data.

However, it does not mean that the algorithm will converge rapidly to this solution, just that it won't diverge arbitrarily because of inaccuracy on the source data (backward error), provided that the forward error introduced by the algorithm does not diverge as well because of accumulating intermediate rounding errors.

The condition number may also be infinite, but this implies that the problem is ill-posed (does not possess a unique, well-defined solution for each choice of data -- that is, the matrix is not invertible), and no algorithm can be expected to reliably find a solution.

The definition of the condition number depends on the choice of norm, as can be illustrated by two examples.

If formula_9 is the norm (usually noted as formula_10) defined in the square-summable sequence space ℓ (which matches the usual distance in a standard Euclidean space), then
where formula_12 and formula_13 are maximal and minimal singular values of formula_14 respectively. Hence
where formula_17 and formula_18 are maximal and minimal (by moduli) eigenvalues of formula_15 respectively.
The condition number with respect to "L" arises so often in numerical linear algebra that it is given a name, the condition number of a matrix.

If formula_9 is the norm (usually denoted by formula_23) defined in the sequence space ℓ of all bounded sequences (which matches the maximum of distances measured on projections into the base subspaces), and formula_15 is lower triangular non-singular (i.e., formula_25) then
The condition number computed with this norm is generally larger than the condition number computed with square-summable sequences, but it can be evaluated more easily (and this is often the only practicably computable condition number, when the problem to solve involves a "non-linear algebra", for example when approximating irrational and transcendental functions or numbers with numerical methods).

If the condition number is not too much larger than one (but it can still be a multiple of one), the matrix is well conditioned which means its inverse can be computed with good accuracy. If the condition number is very large, then the matrix is said to be ill-conditioned. Practically, such a matrix is almost singular, and the computation of its inverse, or solution of a linear system of equations is prone to large numerical errors. A matrix that is not invertible has condition number equal to infinity.

Condition numbers can also be defined for nonlinear functions, and can be computed using calculus. The condition number varies with the point; in some cases one can use the maximum (or supremum) condition number over the domain of the function or domain of the question as an overall condition number, while in other cases the condition number at a particular point is of more interest.

The condition number of a differentiable function "f" in one variable as a function is formula_27 Evaluated at a point "x" this is:
Most elegantly, this can be understood as (the absolute value of) the ratio of the logarithmic derivative of "f," which is formula_29 and the logarithmic derivative of "x," which is formula_30 yielding a ratio of formula_27 This is because the logarithmic derivative is the infinitesimal rate of relative change in a function: it is the derivative formula_32 scaled by the value of "f." Note that if a function has a zero at a point, its condition number at the point is infinite, as infinitesimal changes in the input can change the output from zero to positive or negative, yielding a ratio with zero in the denominator, hence infinite relative change.

More directly, given a small change formula_33 in "x," the relative change in "x" is formula_34 while the relative change in formula_35 is formula_36 Taking the ratio yields:
The last term is the difference quotient (the slope of the secant line), and taking the limit yields the derivative.

Condition numbers of common elementary functions are particularly important in computing significant figures, and can be computed immediately from the derivative; see significance arithmetic of transcendental functions. A few important ones are given below:

Condition numbers can be defined for any function "ƒ" mapping its data from some domain (e.g. an "m"-tuple of real numbers "x") into some codomain [e.g. an "n"-tuple of real numbers "ƒ"("x")], where both the domain and codomain are Banach spaces. They express how sensitive that function is to small changes (or small errors) in its arguments. This is crucial in assessing the sensitivity and potential accuracy difficulties of numerous computational problems, for example polynomial root finding or computing eigenvalues.

The condition number of "ƒ" at a point "x" (specifically, its relative condition number) is then defined to be the maximum ratio of the fractional change in "ƒ"("x") to any fractional change in "x", in the limit where the change δ"x" in "x" becomes infinitesimally small:

where formula_55 is a norm on the domain/codomain of "ƒ"("x").

If "ƒ" is differentiable, this is equivalent to:

where "J(x)" denotes the Jacobian matrix of partial derivatives of "ƒ" at "x" and formula_57 is the induced norm on the matrix.




</doc>
<doc id="6936" url="https://en.wikipedia.org/wiki?curid=6936" title="Cheddar cheese">
Cheddar cheese

Cheddar cheese is a relatively hard, off-white (or orange if spices such as annatto are added), sometimes sharp-tasting (i.e., bitter), natural cheese. Originating in the English village of Cheddar in Somerset, cheeses of this style are produced beyond the region and in several countries around the world.

Cheddar is the most popular type of cheese in the UK, accounting for 51% of the country's £1.9 billion annual cheese market. It is the second-most popular cheese in the US (behind mozzarella), with an average annual consumption of per capita. The US produced approximately in 2014, and the UK in 2008.

The term "Cheddar cheese" is widely used, but has no protected designation of origin within the European Union. However, in 2007 a Protected Designation of Origin, "West Country Farmhouse Cheddar", was created and only Cheddar produced from local milk within Somerset, Dorset, Devon and Cornwall and manufactured using traditional methods may use the name. Outside Europe, the style and quality of cheeses labelled as cheddar may vary greatly; furthermore, cheeses that are more similar in taste and appearance to Red Leicester are sometimes popularly marketed as "Red Cheddar".

The cheese originates from the village of Cheddar in Somerset, south west England. Cheddar Gorge on the edge of the village contains a number of caves, which provided the ideal humidity and steady temperature for maturing the cheese. Cheddar cheese traditionally had to be made within of Wells Cathedral.

Cheddar has been produced since at least the 12th century. A pipe roll of King Henry II from 1170 records the purchase of at a farthing per pound (totalling £10.13s.4d).
Charles I (1600–1649) also bought cheese from the village.
Romans may have brought the recipe to Britain from the Cantal region of France.

Central to the modernisation and standardisation of Cheddar cheese was the 19th-century Somerset dairyman Joseph Harding.
For his technical innovations, promotion of dairy hygiene, and volunteer dissemination of modern cheese-making techniques, he has been dubbed "the father of Cheddar cheese".
Harding introduced new equipment to the process of cheese-making, including his "revolving breaker" for curd cutting, saving much manual effort.
The "Joseph Harding method" was the first modern system for Cheddar production based upon scientific principles. Harding stated that Cheddar cheese is "not made in the field, nor in the byre, nor even in the cow, it is made in the dairy". His wife and he were behind the introduction of the cheese into Scotland and North America. His sons, Henry and William Harding, were responsible for introducing Cheddar cheese production to Australia and facilitating the establishment of the cheese industry in New Zealand, respectively.

During the Second World War, and for nearly a decade after, most milk in Britain was used for the making of one single kind of cheese nicknamed "government Cheddar" as part of war economies and rationing. This almost resulted in wiping out all other cheese production in the country. Before the First World War, more than 3,500 cheese producers were in Britain; fewer than 100 remained after the Second World War.

According to a United States Department of Agriculture researcher, Cheddar cheese is the world's most popular variety of cheese, and the most studied type of cheese in scientific publications.

The curds and whey are separated using rennet, an enzyme complex normally produced from the stomachs of newborn calves (in vegetarian or kosher cheeses, bacterial, yeast or mould-derived chymosin is used).

"Cheddaring" refers to an additional step in the production of Cheddar cheese where, after heating, the curd is kneaded with salt, cut into cubes to drain the whey, and then stacked and turned. Strong, extra-mature Cheddar, sometimes called vintage, needs to be matured for up to 15 months. The cheese is kept at a constant temperature, often requiring special facilities. As with other hard cheese varieties produced worldwide, caves provide an ideal environment for maturing cheese; still, today, some Cheddar cheese is matured in the caves at Wookey Hole and Cheddar Gorge. Additionally, some versions of Cheddar cheese are smoked.

The ideal quality of the original Somerset Cheddar was described by Joseph Harding in 1864 as "close and firm in texture, yet mellow in character or quality; it is rich with a tendency to melt in the mouth, the flavour full and fine, approaching to that of a hazelnut".

Cheddar made in the classical way tends to have a sharp, pungent flavour, often slightly earthy. The "sharpness" of cheddar is associated with the levels of bitter peptides in the cheese. This bitterness has been found to be significant to the overall perception of the aged Cheddar flavour. The texture is firm, with farmhouse traditional Cheddar being slightly crumbly; it should also, if mature, contain large cheese crystals consisting of calcium lactate – often precipitated when matured for times longer than six months.

Cheddar can be a deep to pale yellow (off-white) colour, or a yellow-orange colour when certain plant extracts are added. One commonly used spice is annatto, extracted from seeds of the tropical achiote tree. Originally added to simulate the colour of high-quality milk from grass-fed Jersey and Guernsey cows, annatto may also impart a sweet, nutty flavour. The largest producer of Cheddar cheese in the United States, Kraft, uses a combination of annatto and oleoresin paprika, an extract of the lipophilic (oily) portion of paprika.

Cheddar cheese was sometimes (and still can be found) packaged in black wax, but was more commonly packaged in larded cloth, which was impermeable to contaminants, but still allowed the cheese to "breathe".

The Slow Food Movement has created a Cheddar Presidium, claiming that only three cheeses should be called "original Cheddar". Their specifications, which go further than the "West Country Farmhouse Cheddar" PDO, require that Cheddar cheese be made in Somerset and with traditional methods, such as using raw milk, traditional animal rennet, and a cloth wrapping.

The Cheddar cheese name is used internationally; its name does not have a PDO, but the use of the name "West Country Farmhouse Cheddar" does. In addition to the United Kingdom, Cheddar cheese is also made in Australia, Argentina, Belgium, Canada, Ireland, the Netherlands, New Zealand, South Africa, Sweden, Finland and the United States. Cheddars can be industrial or artisan cheeses. The flavour, colour, and quality of industrial cheese varies significantly, and food packaging will usually indicate a strength, such as mild, medium, strong, tasty, sharp, extra sharp, mature, old, or vintage; this may indicate the maturation period, or food additives used to enhance the flavour. Artisan varieties develop strong and diverse flavours over time.

As of 2013, Cheddar accounts for over 55% of the Australian cheese market, with average annual consumption around per person. Cheddar is so commonly found that the name is rarely used: instead, Cheddar is sold by strength alone as e.g. "mild", "tasty" or "sharp".

Following a wheat midge outbreak in Canada in the mid-19th century, farmers in Ontario began to convert to dairy farming in large numbers, and Cheddar cheese became their main exportable product, even being exported to England. By the turn of the 20th century, 1,242 Cheddar factories were in Ontario, and Cheddar had become Canada’s second-largest export after timber. Cheddar exports totalled in 1904, but by 2012, Canada was a net importer of cheese. James L. Kraft grew up on a dairy farm in Ontario, before moving to Chicago. According to the writer Sarah Champman, "Although we cannot wholly lay the decline of cheese craft in Canada at the feet of James Lewis Kraft, it did correspond with the rise of Kraft’s processed cheese empire." Most Canadian Cheddar is produced by a number of large companies in Ontario, though other provinces produce some and some smaller artisanal producers exist. The annual production is 120,000 tons. It is aged a minimum of three months, but much of it is held for much longer, up to 10 years.

Canadian Cheddar cheese soup is a featured dish at the Canada pavilion at Epcot, in Walt Disney World.

Much of the Cheddar cheese in New Zealand is factory produced. While most of it is sold young within the country, the Anchor dairy company ships New Zealand Cheddars to the UK, where the blocks mature for another year or so.

Only one producer of the cheese is now based in Cheddar itself, the Cheddar Gorge Cheese Co. The name "cheddar" is not protected by the European Union, though the name "West Country Farmhouse Cheddar" has an EU protected designation of origin, and may only be produced in Somerset, Devon, Dorset and Cornwall, using milk sourced from those counties. Cheddar is usually sold as mild, medium, mature, extra mature or vintage. Mature cheddar is the best-selling variety in the UK. Cheddar produced in Orkney is registered as an EU protected geographical indication under the name "Orkney Scottish Island Cheddar".

The state of Wisconsin produces the most Cheddar cheese in the US; other centres of production include: California, Idaho, upstate New York, Vermont, Oregon, Texas, and Oklahoma. It is sold in several varieties (mild, medium, sharp, extra-sharp, New York-style, white, and Vermont). New York-style Cheddar is particularly "sharp"/acidic, but tends to be somewhat softer than the milder-tasting varieties. Cheddar that does not contain annatto is frequently labelled "white Cheddar" or "Vermont Cheddar" (regardless of whether it was actually produced there). Vermont's three creameries produce what is regarded as first-class Cheddar cheeses: the Cabot Creamery, which produces the 16-month-old "Private Stock Cheddar", the Grafton Village Cheese Company, and Shelburne Farms.

Some processed cheeses or "cheese foods" are called "Cheddar flavored". Examples include Easy Cheese: a cheese-food packaged in a pressurised spray can; also, as packs of square, sliced, individually wrapped, "processed cheese" (sometimes also pasteurised).

Cheddar is one of several products used by the United States Department of Agriculture to track the status of America's overall dairy industry; reports are issued weekly detailing prices and production quantities.

U.S. President Andrew Jackson once held an open house party at the White House at which he served a block of Cheddar cheese.

A cheese of was produced in Ingersoll, Ontario, in 1866 and exhibited in New York and Britain; it was immortalised in the poem "Ode on the Mammoth Cheese Weighing over 7,000 Pounds" by James McIntyre, a Canadian poet.

In 1893, farmers from the town of Perth, Ontario, produced "The Mammoth Cheese", which weighed for the Chicago World's Fair. It was planned to be exhibited at the Canadian display, but the mammoth cheese fell through the floor and was placed on a reinforced concrete floor in the Agricultural Building. It received the most journalistic attention at the fair and was awarded the bronze medal. A larger, Wisconsin cheese of was made for the 1964 New York World's Fair. A cheese this size would use the equivalent of the daily milk production of 16,000 cows.

Oregon members of the Federation of American Cheese-makers created the largest Cheddar cheese in 1989. The cheese weighed .




</doc>
<doc id="6938" url="https://en.wikipedia.org/wiki?curid=6938" title="Classical order">
Classical order

An order in architecture is a certain assemblage of parts subject to uniform established proportions, regulated by the office that each part has to perform"."
Coming down to the present from Ancient Greek and Ancient Roman civilization, the architectural orders are the styles of classical architecture, each distinguished by its proportions and characteristic profiles and details, and most readily recognizable by the type of column employed. The three orders of architecture—the Doric, Ionic, and Corinthian—originated in Greece. To these the Romans added, in practice if not in name, the Tuscan, which they made simpler than Doric, and the Composite, which was more ornamental than the Corinthian. The architectural order of a classical building is akin to the mode or key of classical music, the grammar or rhetoric of a written composition. It is established by certain "modules" like the intervals of music, and it raises certain expectations in an audience attuned to its language.

Whereas the orders were essentially structural in Ancient Greek architecture, which made little use of the arch until its late period, in Roman architecture where the arch was often dominant, the orders became increasingly decorative elements except in porticos and similar uses. Columns shrank into half-columns emerging from walls or turned into pilasters. This treatment continued after the conscious and "correct" use of the orders, initially following exclusively Roman models, returned in the Italian Renaissance. Greek Revival architecture, inspired by increasing knowledge of Greek originals, returned to more authentic models, including ones from relatively early periods.

Each style has distinctive capitals at the top of columns and horizontal entablatures which it supports, while the rest of the building does not in itself vary between the orders. The column shaft and base also varies with the order, and is sometimes articulated with vertical hollow grooves known as fluting. The shaft is wider at the bottom than at the top, because its entasis, beginning a third of the way up, imperceptibly makes the column slightly more slender at the top, although some Doric columns, especially early Greek ones, are visibly "flared", with straight profiles that narrow going up the shaft.

The capital rests on the shaft. It has a load-bearing function, which concentrates the weight of the entablature on the supportive column, but it primarily serves an aesthetic purpose. The necking is the continuation of the shaft, but is visually separated by one or many grooves. The echinus lies atop the necking. It is a circular block that bulges outwards towards the top to support the abacus, which is a square or shaped block that in turn supports the entablature. The entablature consists of three horizontal layers, all of which are visually separated from each other using moldings or bands. In Roman and post-Renaissance work, the entablature may be carried from column to column in the form of an arch that springs from the column that bears its weight, retaining its divisions and sculptural enrichment, if any. There are names for all the many parts of the orders.

The height of columns are calculated in terms of a ratio between the diameter of the shaft at its base and the height of the column. A Doric column can be described as seven diameters high, an Ionic column as eight diameters high and a Corinthian column nine diameters high, although the actual ratios used vary considerably in both ancient and revived examples, but keeping to the trend of increasing slimness between the orders. Sometimes this is phrased as "lower diameters high", to establish which part of the shaft has been measured.

There are three distinct orders in Ancient Greek architecture: Doric, Ionic, and Corinthian. These three were adopted by the Romans, who modified their capitals. The Roman adoption of the Greek orders took place in the 1st century BC. The three Ancient Greek orders have since been consistently used in neo-classical European architecture.

Sometimes the Doric order is considered the earliest order, but there is no evidence to support this. Rather, the Doric and Ionic orders seem to have appeared at around the same time, the Ionic in eastern Greece and the Doric in the west and mainland.

Both the Doric and the Ionic order appear to have originated in wood. The Temple of Hera in Olympia is the oldest well-preserved temple of Doric architecture. It was built just after 600 BC. The Doric order later spread across Greece and into Sicily where it was the chief order for monumental architecture for 800 years. Early Greeks were no doubt aware of the use of stone columns with bases and capitals in Ancient Egyptian architecture, and that of other Near Eastern cultures, although there they were mostly used in interiors, rather than as a dominant feature of all or part of exteriors, in the Greek style. 

The Doric order originated on the mainland and western Greece. It is the simplest of the orders, characterized by short, faceted, heavy columns with plain, round capitals (tops) and no base. With a height that is only four to eight times its diameter, the columns are the most squat of all orders. The shaft of the Doric order is channeled with 20 flutes. The capital consists of a necking which is of a simple form. The echinus is convex and the abacus is square.

Above the capital is a square abacus connecting the capital to the entablature. The Entablature is divided into three horizontal registers, the lower part of which is either smooth or divided by horizontal lines. The upper half is distinctive for the Doric order. The frieze of the Doric entablature is divided into triglyphs and metopes. A triglyph is a unit consisting of three vertical bands which are separated by grooves. Metopes are the plain or carved reliefs between two triglyphs.

The Greek forms of the Doric order come without an individual base. They instead are placed directly on the stylobate. Later forms, however, came with the conventional base consisting of a plinth and a torus. The Roman versions of the Doric order have smaller proportions. As a result, they appear lighter than the Greek orders.

The Ionic order came from eastern Greece, where its origins are entwined with the similar but little known Aeolic order. It is distinguished by slender, fluted pillars with a large base and two opposed "volutes" (also called "scrolls") in the echinus of the capital. The echinus itself is decorated with an egg-and-dart motif. The Ionic shaft comes with four more flutes than the Doric counterpart (totalling 24). The Ionic base has two convex moldings called "tori" which are separated by a scotia.

The Ionic order is also marked by an entasis, a curved tapering in the column shaft. A column of the ionic order is nine times its lower diameter. The shaft itself is eight diameters high. The architrave of the entablature commonly consists of three stepped bands ("fasciae"). The frieze comes without the Doric "triglyph" and "metope". The frieze sometimes comes with a continuous ornament such as carved figures instead.

The Corinthian order is the most ornate of the Greek orders, characterized by a slender fluted column having an ornate capital decorated with two rows of acanthus leaves and four scrolls. It is commonly regarded as the most elegant of the three orders. The shaft of the Corinthian order has 24 flutes. The column is commonly ten diameters high.

The Roman writer Vitruvius credited the invention of the Corinthian order to Callimachus, a Greek sculptor of the 5th century BC. The oldest known building built according to this order is the Choragic Monument of Lysicrates in Athens, constructed from 335 to 334 BC. The Corinthian order was raised to rank by the writings of Vitruvius in the 1st century BC.

The Romans adapted all the Greek orders and also developed two orders of their own, basically modifications of Greek orders. However, it was not until the Renaissance that these were named and formalized as the Tuscan and Composite, respectively the plainest and most ornate of the orders. The Romans also invented the superposed order. A superposed order is when successive stories of a building have different orders. The heaviest orders were at the bottom, whilst the lightest came at the top. This means that the Doric order was the order of the ground floor, the Ionic order was used for the middle story, while the Corinthian or the Composite order was used for the top story.

The Colossal order was invented by architects in the Renaissance. The Colossal order is characterized by columns that extend the height of two or more stories.

The Tuscan order has a very plain design, with a plain shaft, and a simple capital, base, and frieze. It is a simplified adaptation of the Doric order by the Greeks. The Tuscan order is characterized by an unfluted shaft and a capital that only consists of an echinus and an abacus. In proportions it is similar to the Doric order, but overall it is significantly plainer. The column is normally seven diameters high. Compared to the other orders, the Tuscan order looks the most solid.

The Composite order is a mixed order, combining the volutes of the Ionic with the leaves of the Corinthian order. Until the Renaissance it was not ranked as a separate order. Instead it was considered as a late Roman form of the Corinthian order. The column of the Composite order is typically ten diameters high.

The Renaissance period saw renewed interest in the literary sources of the ancient cultures of Greece and Rome, and the fertile development of a new architecture based on classical principles. The treatise "De architectura" by Roman theoretician, architect and engineer Vitruvius, is the only architectural writing that survived from Antiquity. Rediscovered in the 15th century, Vitruvius was instantly hailed as the authority on architecture. However, in his text the word "order" is not to be found. To describe the four species of columns (he only mentions: Tuscan, Doric, Ionic and Corinthian) he uses, in fact, various words such as: "genus" (gender), "mos" (habit, fashion, manner), "opera" (work).

The term "order", as well as the idea of redefining the "canon" started circulating in Rome, at the beginning of the 16th century, probably during the studies of Vitruvius' text conducted and shared by Peruzzi, Raphael and Sangallo.
Ever since, the definition of the "canon" has been a collective endeavor that involved several generations of European architects, from Renaissance and Baroque periods, basing their theories both on the study of Vitruvius' writings and the observation of Roman ruins (the Greek ruins became available only after Greek Independence, 1821–23). What was added were rules for the use of the Architectural Orders, and the exact proportions of them down to the most minute detail. Commentary on the appropriateness of the orders for temples devoted to particular deities (Vitruvius I.2.5) were elaborated by Renaissance theorists, with Doric characterized as bold and manly, Ionic as matronly, and Corinthian as maidenly.

Following the examples of Vitruvius and the five books of the "Regole generali di architettura sopra le cinque maniere de gli edifici" by Sebastiano Serlio, published from 1537 onwards, Giacomo Barozzi da Vignola produced an architecture rule book that was not only more practical than the previous two treatises, but also was systematically and consistently adopting, for the first time, the term order to define each of the five different species of columns inherited from Antiquity. A first publication of the various plates, as separate sheets, appeared in Rome in 1562, with the title: "Regola delli Cinque Ordini di Architettura" ("Canon of the Five Orders of Architecture "). As David Watkin has pointed out, Vignola's book "was to have an astonishing publishing history of over 500 editions in 400 years in ten languages, Italian, Dutch, English, Flemish, French, German, Portuguese, Russian, Spanish, Swedish, during which it became perhaps the most influential book of all times".
The book consisted simply of an introduction followed by 32 annotated plates, highlighting the proportional system with all the minute details of the Five Architectural Orders. According to Christof Thoenes, the main expert of Renaissance architectural treatises, "in accordance with Vitruvius’s example, Vignola chose a “module” equal to a half-diameter which is the base of the system. All the other measurements are expressed in fractions or in multiples of this module. The result is an arithmetical model, and with its help each order, harmoniously proportioned, can easily be adapted to any given height, of a façade or an interior. From this point of view, Vignola’s Regola is a remarkable intellectual achievement".

In America, "The American Builder's Companion", written in the early 19th century by the architect Asher Benjamin, influenced many builders in the eastern states, particularly those who developed what became known as the Federal style. The last American re-interpretation of Vignola's "Regola", was edited in 1904 by William Robert Ware.

The break from the classical mode came first with the Gothic revival, then the development of modernism during the 19th century. The Bauhaus promoted pure functionalism, stripped of superfluous ornament, and that has become one of the defining characteristics of modern architecture. There are some exceptions. Postmodernism introduced an ironic use of the orders as a cultural reference, divorced from the strict rules of composition. On the other hand, a few practitioners e.g. Quinlan Terry and Stuart Martin still work in a traditional classical idiom.

Several orders, usually based upon the composite order and only varying in the design of the capitals, have been invented under the inspiration of specific occasions, but have not been used again. Thus they may be termed "nonce orders" on the analogy of nonce words. Robert Adam's brother James was in Rome in 1762, drawing antiquities under the direction of Clérisseau; he invented a British Order, of which his ink-and-wash rendering with red highlighting is at the Avery Library, Columbia University. Adam published an engraving of it. In its capital the heraldic lion and unicorn take the place of the Composite's volutes, a Byzantine/Romanesque conception, but expressed in terms of neoclassical realism. In 1789 George Dance invented an Ammonite Order, a variant of Ionic substituting volutes in the form of fossil ammonites for John Boydell's Shakespeare Gallery in Pall Mall, London. An adaptation of the Corinthian order by William Donthorne that used turnip leaves and mangelwurzel is termed the Agricultural Order.

In the United States Benjamin Latrobe, the architect of the Capitol building in Washington DC, designed a series of botanically American orders. Most famous is the order substituting corncobs and their husks, which was executed by Giuseppe Franzoni and employed in the small domed Vestibule of the Supreme Court. Only the Supreme Court survived the fire of August 24, 1814, nearly intact. With peace restored, Latrobe designed an American order that substituted for the acanthus tobacco leaves, of which he sent a sketch to Thomas Jefferson in a letter, November 5, 1816. He was encouraged to send a model of it, which remains at Monticello. In the 1830s Alexander Jackson Davis admired it enough to make a drawing of it. In 1809 Latrobe invented a second American order, employing magnolia flowers constrained within the profile of classical mouldings, as his drawing demonstrates. It was intended for "the Upper Columns in the Gallery of the Entrance of the Chamber of the Senate" (United States Capitol exhibit).
Edwin Lutyens, who from 1912 laid out New Delhi as the new seat of government for the British Empire in India, designed a Delhi Order having a capital displaying a band of vertical ridges, and with bells hanging at each corner as a replacement for volutes. His design for the new city's central palace, Viceroy's House, now the Presidential residence Rashtrapati Bhavan, was a thorough integration of elements of Indian architecture into a building of classical forms and proportions, and made use of the order throughout. The Delhi Order reappears in some later Lutyens buildings including Campion Hall, Oxford.

These nonce orders all express the "speaking architecture" ("architecture parlante") that was taught in the Paris courses, most explicitly by Étienne-Louis Boullée, in which sculptural details of classical architecture could be enlisted to speak symbolically, the better to express the purpose of the structure and enrich its visual meaning with specific appropriateness. This idea was taken up strongly in the training of Beaux-Arts architecture, ca 1875-1915. 





</doc>
<doc id="6941" url="https://en.wikipedia.org/wiki?curid=6941" title="Colin Kapp">
Colin Kapp

Derek Ivor Colin Kapp (3 April 1928 – 3 August 2007) was a British science fiction author; he was most often published under the byline Colin Kapp.

As an electronic engineer, he began his career with Mullard Electronics then specialised in electro-plating techniques, eventually becoming a freelance consultant engineer. This technical background probably enabled him to write semi-realistic science fiction stories.

He was born in Southwark, south London, 3 April 1928 to John L. F. Kapp and Annie M.A. (née Towner)

A contemporary of Brian Aldiss and James White, as an author, Kapp is best known for his stories about the Unorthodox Engineers.




Collected in "The Unorthodox Engineers" (1979)




</doc>
<doc id="6942" url="https://en.wikipedia.org/wiki?curid=6942" title="Catherine of Aragon">
Catherine of Aragon

Catherine of Aragon (; 16 December 1485 – 7 January 1536), was Queen of England from June 1509 until May 1533 as the first wife of King Henry VIII; she was previously Princess of Wales as the wife of Henry's elder brother Arthur.

The daughter of Isabella I of Castile and Ferdinand II of Aragon, Catherine was three years old when she was betrothed to Arthur, Prince of Wales, heir apparent to the English throne. They married in 1501, but Arthur died five months later. In 1507, she held the position of ambassador of the Aragonese Crown in England, the first female ambassador in European history. Catherine subsequently married Arthur's younger brother, the recently ascended Henry VIII, in 1509. For six months in 1513, she served as regent of England while Henry VIII was in France. During that time the English won the Battle of Flodden, an event in which Catherine played an important part with an emotional speech about English courage.

By 1525, Henry VIII was infatuated with Anne Boleyn and dissatisfied that his marriage to Catherine had produced no surviving sons, leaving their daughter, the future Mary I of England, as heir presumptive at a time when there was no established precedent for a woman on the throne. He sought to have their marriage annulled, setting in motion a chain of events that led to England's schism with the Catholic Church. When Pope Clement VII refused to annul the marriage, Henry defied him by assuming supremacy over religious matters. In 1533 their marriage was consequently declared invalid and Henry married Anne on the judgement of clergy in England, without reference to the Pope. Catherine refused to accept Henry as Supreme Head of the Church in England and considered herself the King's rightful wife and queen, attracting much popular sympathy. Despite this, she was acknowledged only as Dowager Princess of Wales by Henry. After being banished from court, she lived out the remainder of her life at Kimbolton Castle, and died there on 7 January 1536. English people held Catherine in high esteem, and her death set off tremendous mourning.

The controversial book "The Education of a Christian Woman" by Juan Luis Vives, which claimed women have the right to an education, was commissioned by and dedicated to her. Such was Catherine's impression on people that even her enemy, Thomas Cromwell, said of her, "If not for her sex, she could have defied all the heroes of History." She successfully appealed for the lives of the rebels involved in the Evil May Day, for the sake of their families. Catherine also won widespread admiration by starting an extensive programme for the relief of the poor. She was a patron of Renaissance humanism, and a friend of the great scholars Erasmus of Rotterdam and Thomas More.

Catherine was born at the Archbishop's Palace in Alcalá de Henares near Madrid, on the night of 16 December 1485. She was the youngest surviving child of King Ferdinand II of Aragon and Queen Isabella I of Castile. Catherine was quite short in stature with long red hair, wide blue eyes, a round face, and a fair complexion. She was descended, on her maternal side, from the English royal house; her great-grandmother Catherine of Lancaster, after whom she was named, and her great-great-grandmother Philippa of Lancaster were both daughters of John of Gaunt and granddaughters of Edward III of England. Consequently, she was third cousin of her father-in-law, Henry VII of England, and fourth cousin of her mother-in-law Elizabeth of York.

Catherine was educated by a tutor, Alessandro Geraldini, who was a clerk in Holy Orders. She studied arithmetic, canon and civil law, classical literature, genealogy and heraldry, history, philosophy, religion, and theology. She had a strong religious upbringing and developed her Roman Catholic faith that would play a major role in later life. She learned to speak, read and write in Spanish and Latin, and spoke French and Greek. She was also taught domestic skills, such as cooking, dancing, drawing, embroidery, good manners, lace-making, music, needlepoint, sewing, spinning, and weaving. Scholar Erasmus later said that Catherine "loved good literature which she had studied with success since childhood".

At an early age, Catherine was considered a suitable wife for Arthur, Prince of Wales, heir apparent to the English throne, due to the English ancestry she inherited from her mother. By means of her mother, Catherine had a stronger legitimate claim to the English throne than King Henry VII himself through the first two wives of John of Gaunt, 1st Duke of Lancaster: Blanche of Lancaster and Constance of Castile. In contrast, Henry VII was the descendant of Gaunt's third marriage to Katherine Swynford, whose children were born out of wedlock and only legitimised after the death of Constance and the marriage of John to Katherine. The children of John and Katherine, while legitimised, were barred from inheriting the English throne, a stricture that was ignored in later generations. Because of Henry's descent through illegitimate children barred from succession to the English throne, the Tudor monarchy was not accepted by all European kingdoms. At the time, the House of Trastámara was the most prestigious in Europe, due to the rule of the Catholic Monarchs, so the alliance of Catherine and Arthur validated the House of Tudor in the eyes of European royalty and strengthened the Tudor claim to the English throne via Catherine of Aragon's ancestry. It would have given a male heir an indisputable claim to the throne. The two were married by proxy on 19 May 1499 and corresponded in Latin until Arthur turned fifteen, when it was decided that they were old enough to be married.

When Catherine of Aragon travelled to London, she brought a group of her African attendants with her, including one identified as the trumpeter John Blanke. They are the first Africans recorded to have arrived in London at the time, and were considered luxury servants. They caused a great impression about the princess and the power of her family.

Catherine and Arthur met on 4 November 1501 at Dogmersfield in Hampshire. Little is known about their first impressions of each other, but Arthur did write to his parents-in-law that he would be "a true and loving husband" and told his parents that he was immensely happy to "behold the face of his lovely bride". The couple had corresponded in Latin, but found that they could not understand each other, since they had learned different pronunciations. Ten days later, on 14 November 1501, they were married at Old St. Paul's Cathedral. A dowry of 200,000 crowns had been agreed, and half was paid shortly after the marriage.

Once married, Arthur was sent to Ludlow Castle on the borders of Wales to preside over the Council of Wales and the Marches, as was his duty as Prince of Wales, and his bride accompanied him. The couple stayed at Castle Lodge, Ludlow. A few months later, they both became ill, possibly with the sweating sickness, which was sweeping the area. Arthur died on 2 April 1502; Catherine recovered to find herself a widow.

At this point, Henry VII faced the challenge of avoiding the obligation to return her 200,000 ducat dowry, half of which he had not yet received, to her father, as required by her marriage contract should she return home. Following the death of Queen Elizabeth in February 1503, King Henry VII initially considered marrying Catherine himself, but the opposition of her father and potential questions over the legitimacy of the couple's issue ended the idea. To settle the matter, it was agreed that Catherine would marry Henry VII's second son, Henry, Duke of York, who was five years younger than she was. The death of Catherine's mother, however, meant that her "value" in the marriage market decreased. Castile was a much larger kingdom than Aragon, and it was inherited by Catherine's mentally unstable elder sister, Joanna. Ostensibly, the marriage was delayed until Henry was old enough, but Ferdinand II procrastinated so much over payment of the remainder of Catherine's dowry that it became doubtful that the marriage would take place. She lived as a virtual prisoner at Durham House in London. Some of the letters she wrote to her father complaining of her treatment have survived. In one of these letters she tells him that "I choose what I believe, and say nothing. For I am not as simple as I may seem." She had little money and struggled to cope, as she had to support her ladies-in-waiting as well as herself. In 1507 she served as the Spanish ambassador to England, the first female ambassador in European history. While Henry VII and his councillors expected her to be easily manipulated, Catherine went on to prove them wrong.

Marriage to Arthur's brother depended on the Pope granting a dispensation because canon law forbade a man to marry his brother's widow (Lev. 18:16). Catherine testified that her marriage to Arthur was never consummated as, also according to canon law, a marriage was not valid until consummated.

Catherine's second wedding took place on 11 June 1509, seven years after Prince Arthur's death. She married Henry VIII, who had only just acceded to the throne, in a private ceremony in the church of the Observant Friars outside Greenwich Palace. She was 23 years of age. The king was just days short of his 18th birthday.

On Saturday 23 June 1509, the traditional eve-of-coronation procession to Westminster was greeted by a large and enthusiastic crowd. As was the custom, the couple spent the night before their coronation at the Tower of London. On Midsummer's Day, Sunday, 1509, Henry VIII and Catherine were anointed and crowned together by the Archbishop of Canterbury at a lavish ceremony at Westminster Abbey. The coronation was followed by a banquet in Westminster Hall. Many new Knights of the Bath were created in honour of the coronation.
In that month that followed, many social occasions presented the new Queen to the English public. She made a fine impression and was well received by the people of England.

Catherine was pregnant seven times altogether:

On 11June 1513, Henry appointed Catherine Regent in England with the titles "Governor of the Realm and Captain General," while he went to France on a military campaign.
When Louis d'Orléans, Duke of Longueville, was captured at Thérouanne, Henry sent him to stay in Catherine's household. She wrote to Wolsey that she and her council would prefer the Duke to stay in the Tower of London as the Scots were "so busy as they now be" and she added her prayers for "God to sende us as good lukke against the Scotts, as the King hath ther." The war with Scotland occupied her subjects, and she was "horrible busy with making standards, banners, and badges" at Richmond Palace. The Scots invaded and on 3 September 1513, she ordered Thomas Lovell to raise an army in the midland counties.

Catherine rode north in full armour to address the troops, despite being heavily pregnant at the time. Her fine speech was reported to the historian Peter Martyr d'Anghiera in Valladolid within a fortnight. Although an Italian newsletter said she was north of London when news of the victory at Battle of Flodden Field reached her, she was near Buckingham. From Woburn Abbey she sent a letter to Henry along with a piece of the bloodied coat of King James IV of Scotland, who died in the battle, for Henry to use as a banner at the siege of Tournai.

Catherine's religious dedication increased as she became older, as did her interest in academics. She continued to broaden her knowledge and provide training for her daughter, Mary. Education among women became fashionable, partly because of Catherine's influence, and she donated large sums of money to several colleges. Henry, however, still considered a male heir essential. The Tudor dynasty was new, and its legitimacy might still be tested. A long civil war (1135–54) had been fought the last time a woman, (Empress Matilda), had inherited the throne. The disasters of civil war were still fresh in living memory from the Wars of the Roses.

In 1520, Catherine's nephew, the Holy Roman Emperor Charles V, paid a state visit to England, and she urged Henry to enter an alliance with Charles rather than with France. Immediately after his departure, she accompanied Henry to France on the celebrated visit to Francis I, the so-called Field of the Cloth of Gold. Within two years, war was declared against France and the Emperor was once again welcome in England, where plans were afoot to betroth him to Catherine's daughter Mary.

In 1525, Henry VIII became enamoured of Anne Boleyn, a lady-in-waiting to Queen Catherine who was 11 years younger than Henry. Henry began pursuing her; Catherine was no longer able to bear children by this time. Henry began to believe that his marriage was cursed and sought confirmation from the Bible, which he interpreted to say that if a man marries his brother's wife, the couple will be childless. Even if her marriage to Arthur had not been consummated (and Catherine would insist to her dying day that she had come to Henry's bed a virgin), Henry's interpretation of that biblical passage meant that their marriage had been wrong in the eyes of God. Whether the Pope at the time of Henry and Catherine's marriage had the right to overrule Henry's claimed scriptural impediment would become a hot topic in Henry's campaign to wrest an annulment from the present Pope. It is possible that the idea of annulment had been suggested to Henry much earlier than this, and is highly probable that it was motivated by his desire for a son. Before Henry's father ascended the throne, England was beset by civil warfare over rival claims to the English crown, and Henry may have wanted to avoid a similar uncertainty over the succession.

It soon became the one absorbing object of Henry's desires to secure an annulment. Catherine was defiant when it was suggested that she quietly retire to a nunnery, saying: "God never called me to a nunnery. I am the King's true and legitimate wife". He set his hopes upon an appeal to the Holy See, acting independently of Cardinal Thomas Wolsey, whom he told nothing of his plans. William Knight, the King's secretary, was sent to Pope Clement VII to sue for an annulment, on the grounds that the dispensing bull of Pope Julius II was obtained by false pretences.

As the Pope was, at that time, the prisoner of Catherine's nephew, Emperor Charles V, following the Sack of Rome in May 1527, Knight had difficulty in obtaining access to him. In the end, Henry's envoy had to return without accomplishing much. Henry now had no choice but to put this great matter into the hands of Wolsey, who did all he could to secure a decision in Henry's favour.

When Henry decided to annul his marriage to Catherine, John Fisher became her most trusted counsellor and one of her chief supporters. He appeared in the legates' court on her behalf, where he shocked people with the directness of his language, and by declaring that, like John the Baptist, he was ready to die on behalf of the indissolubility of marriage. Henry was so enraged by this that he wrote a long Latin address to the legates in answer to Fisher's speech. Fisher's copy of this still exists, with his manuscript annotations in the margin which show how little he feared Henry's anger. The removal of the cause to Rome ended Fisher's role in the matter, but Henry never forgave him. Other people who supported Catherine's case included Thomas More; Henry's own sister Mary Tudor, Queen of France (though as a member of the Tudor family and of royal blood, she was safe from any punishment and execution); María de Salinas; Holy Roman Emperor Charles V; Pope Paul III and Protestant Reformers Martin Luther and William Tyndale.

Upon returning to Dover from a meeting with King Francis I of France in Calais, Henry married Anne Boleyn in a secret ceremony. Some sources speculate that Anne was already pregnant at the time (and Henry did not want to risk a son being born illegitimate) but others testify that Anne (who had seen her sister Mary Boleyn taken up as the king's mistress and summarily cast aside) refused to sleep with Henry until they were married. Henry defended the legality of their union by pointing out that Catherine had previously been married. If she and Arthur had consummated their marriage, Henry by canon law had the right to remarry. On 23 May 1533, Cranmer, sitting in judgement at a special court convened at Dunstable Priory to rule on the validity of Henry's marriage to Catherine, declared the marriage illegal, even though Catherine testified she and Arthur had never had physical relations. Cranmer ruled Henry and Anne's marriage valid five days later, on 28 May 1533.

Until the end of her life, Catherine would refer to herself as Henry's only lawful wedded wife and England's only rightful queen, and her servants continued to address her by that title. Henry refused her the right to any title but "Dowager Princess of Wales" in recognition of her position as his brother's widow.

Catherine went to live at The More castle in the winter of 1531/32. In 1535 she was transferred to Kimbolton Castle. There, she confined herself to one room (which she left only to attend Mass), dressed only in the hair shirt of the Order of St. Francis, and fasted continuously. While she was permitted to receive occasional visitors, she was forbidden to see her daughter Mary. They were also forbidden to communicate in writing, but sympathizers discreetly ferried letters between the two. Henry offered both mother and daughter better quarters and permission to see each other if they would acknowledge Anne Boleyn as the new queen. Both refused.

In late December 1535, sensing her death was near, Catherine made her will, and wrote to her nephew, the Emperor Charles V, asking him to protect her daughter. It has been alleged that she then penned one final letter to Henry, her "most dear lord and husband":
The authenticity of the letter itself has been questioned, but not Catherine's attitude in its wording, which has been reported with variations in different sources.

Catherine died at Kimbolton Castle on 1536. The following day, news of her death reached the king. At the time there were rumours that she was poisoned, possibly by Gregory di Casale. According to the chronicler Edward Hall, Anne Boleyn wore yellow for the mourning, which has been interpreted in various ways; Polydore Vergil interpreted this to mean that Anne did not mourn. Chapuys reported that it was King Henry who decked himself in yellow, celebrating the news and making a great show of his and Anne's daughter, Elizabeth, to his courtiers. This was seen as distasteful and vulgar by many. Another theory is that the dressing in yellow was out of respect for Catherine as yellow was said to be the Spanish colour of mourning. Certainly, later in the day it is reported that Henry and Anne both individually and privately wept for her death. On the day of Catherine's funeral, Anne Boleyn miscarried a boy. Rumours then circulated that Catherine had been poisoned by Anne or Henry, or both, as Anne had threatened to murder both Catherine and Mary on several occasions. The rumours were born after the apparent discovery during her embalming that there was a black growth on her heart that might have been caused by poisoning. Modern medical experts are in agreement that her heart's discolouration was due not to poisoning, but to cancer, something which was not understood at the time.

Catherine was buried in Peterborough Cathedral with the ceremony due to a Dowager Princess of Wales, not a queen. Henry did not attend the funeral and forbade Mary to attend.

Catherine was a member of the Third Order of Saint Francis and she was punctilious in her religious obligations in the Order, integrating without demur her necessary duties as queen with her personal piety. After her divorce, she was quoted "I would rather be a poor beggar’s wife and be sure of heaven, than queen of all the world and stand in doubt thereof by reason of my own consent."

The outward celebration of saints and holy relics formed no major part of her personal devotions, which she rather expressed in the Mass, prayer, confession and penance. Privately, however, she was aware of what she identified as the shortcomings of the papacy and church officialdom. Her doubts about Church improprieties certainly did not extend so far as to support the allegations of corruption made public by Martin Luther in Wittenberg in 1517, which were soon to have such far-reaching consequences in initiating the Protestant Reformation.

In 1523 Alfonso de Villa Sancta, a learned friar of the Observant (reform) branch of the Friars Minor and friend of the king's old advisor Erasmus, dedicated to the queen his book "De Liberio Arbitrio adversus Melanchthonem" denouncing Philip Melanchthon, a supporter of Luther. Acting as her confessor, he was able to nominate her for the title of "Defender of the Faith" for denying Luther's arguments.

Catherine was of a very fair complexion, had blue eyes, and had a hair colour that was between reddish-blonde and auburn like her mother and sister Joanna. In her youth she was described as "the most beautiful creature in the world" and that there was "nothing lacking in her that the most beautiful girl should have". Thomas More and Lord Herbert would reflect later in her lifetime that in regard to her appearance "there were few women who could compete with the Queen [Catherine] in her prime."

The controversial book "The Education of Christian Women" by Juan Luis Vives, which claimed women have the right to an education, was dedicated to and commissioned by her. Such was Catherine's impression on people, that even her enemy, Thomas Cromwell, said of her "If not for her sex, she could have defied all the heroes of History." She successfully appealed for the lives of the rebels involved in the Evil May Day for the sake of their families. Furthermore, Catherine won widespread admiration by starting an extensive programme for the relief of the poor. She was also a patron of Renaissance humanism, and a friend of the great scholars Erasmus of Rotterdam and Saint Thomas More. Some saw her as a martyr.

In the reign of her daughter Mary I of England, her marriage to Henry VIII was declared "good and valid". Her daughter Queen Mary also had several portraits commissioned of Catherine, and it would not by any means be the last time she was painted. After her death, numerous portraits were painted of her, particularly of her speech at the Legatine Trial, a moment accurately rendered in Shakespeare's play about Henry VIII.

Her tomb in Peterborough Cathedral can be seen and there is hardly ever a time when it is not decorated with flowers or pomegranates, her heraldic symbol. It bears the title "Katharine Queen of England".

In the 20th century, George V's wife, Mary of Teck, had her grave upgraded and there are now banners there denoting Catherine as a Queen of England. Every year at Peterborough Cathedral there is a service in her memory. There are processions, prayers, and various events in the Cathedral including processions to Catherine's grave in which candles, pomegranates, flowers and other offerings are placed on her grave. On the service commemorating the 470th anniversary of her death, the Spanish Ambassador to the United Kingdom attended. During the 2010 service a rendition of Catherine of Aragon's speech before the Legatine court was read by Jane Lapotaire. There is a statue of her in her birthplace of Alcalá de Henares, as a young woman holding a book and a rose.

Catherine has remained a popular biographical subject to the present day. The American historian Garrett Mattingly was the author of a popular biography "Katherine of Aragon" in 1942. In 1966, Catherine and her many supporters at court were the subjects of "Catherine of Aragon and her Friends", a biography by John E. Paul. In 1967, Mary M. Luke wrote the first book of her Tudor trilogy, "Catherine the Queen" which portrayed her and the controversial era of English history through which she lived.


Her baptismal name was "Catalina", but "Katherine" was soon the accepted form in England after her marriage to Arthur. Catherine herself signed her name "Katherine", "Katherina", "Katharine" and sometimes "Katharina". In a letter to her, Arthur, her husband, addressed her as "Princess Katerine". Her daughter Queen Mary I called her "Quene Kateryn", in her will. Rarely were names, particularly first names, written in an exact manner during the sixteenth century and it is evident from Catherine's own letters that she endorsed different variations.
Loveknots built into his various palaces by her husband, Henry VIII, display the initials "H & K", as do other items belonging to Henry and Catherine, including gold goblets, a gold salt cellar, basins of gold, and candlesticks. Her tomb in Peterborough Cathedral is marked "Katharine Queen of England".

Over the years, numerous artistic and cultural works have been dedicated to Catherine, have been written about her, or have mentioned her, including some by her husband Henry VIII, who wrote "Grene growth the holy" about and for her, and Juan Luis Vives, who dedicated "The Education of Christian Women" to her.

Catherine of Aragon has been portrayed in film, television, plays, novels, songs, poems, and other creative forms many times, and as a result she has stayed very much in popular memory. There has never been a film or television series in which she is the main character, the nearest is the first episode of "The Six Wives of Henry VIII", which is told from her point of view (and in which she is portrayed by Annette Crosbie). William Shakespeare's play "Henry VIII" succeeds in recreating with great accuracy Catherine's statement about the legitimacy of her marriage at the court in Blackfriars before King Henry, and Shakespeare's portrayal of Catherine is remarkably sympathetic; however, most of the rest of the play is an attempt to absolve many, especially Henry VIII, and the timing of key incidents (including Catherine's death) is changed and other events are avoided (the play makes Henry nearly an innocent pawn in the hands of a dastardly Cardinal Wolsey, and the play stops short of Anne Boleyn's execution).

Although Catherine is often portrayed in film and on stage as having possessed stereotypically Spanish dark hair and eyes and olive complexion, existing portraits and contemporary descriptions depict her with blue eyes, fair skin, and reddish-blonde hair, not uncommon for Spaniards from the northern regions of Spain, including her father's land of Aragon. And she was part English, through her ancestresses Katherine of Lancaster and Philippa of Lancaster, who were daughters of John of Gaunt, 1st Duke of Lancaster.

Catherine often is played with a Spanish accent. From most reports, this is accurate, as she never fully mastered the English language.

In January, 2013, the National Portrait Gallery in London revealed that its curators had recently discovered that a portrait at Lambeth Palace formerly believed to have been a portrait of Catherine Parr in fact shows Catherine of Aragon. The National Portrait Gallery announced that the painting, which had hung in a private sitting room of the Archbishop of Canterbury since at least the 19th century, would be paired with a portrait of Henry VIII already in the museum's collection, and would remain at the museum on loan.


Catherine is the main character in:

Catherine is a character in:

Catherine was portrayed by:







</doc>
<doc id="6943" url="https://en.wikipedia.org/wiki?curid=6943" title="Cathode ray">
Cathode ray

Cathode rays (also called an electron beam or e-beam) are streams of electrons observed in vacuum tubes. If an evacuated glass tube is equipped with two electrodes and a voltage is applied, glass behind the positive electrode is observed to glow, due to electrons emitted from and traveling away from the cathode (the electrode connected to the negative terminal of the voltage supply). They were first observed in 1869 by German physicist Johann Wilhelm Hittorf, and were named in 1876 by Eugen Goldstein "Kathodenstrahlen", or cathode rays. In 1897, British physicist J. J. Thomson showed that cathode rays were composed of a previously unknown negatively charged particle, which was later named the "electron". Cathode ray tubes (CRTs) use a focused beam of electrons deflected by electric or magnetic fields to create the image on a television screen.

Cathode rays are so named because they are emitted by the negative electrode, or cathode, in a vacuum tube. To release electrons into the tube, they first must be detached from the atoms of the cathode. In the early cold cathode vacuum tubes, called Crookes tubes, this was done by using a high electrical potential between the anode and the cathode to ionize the residual gas atoms in the tube; the ions were accelerated by the electric field and released electrons when they collided with the cathode. Modern vacuum tubes use thermionic emission, in which the cathode is made of a thin wire filament which is heated by a separate electric current passing through it. The increased random heat motion of the filament knocks electrons out of the surface of the filament, into the evacuated space of the tube.

Since the electrons have a negative charge, they are repelled by the negative cathode and attracted to the positive anode. They travel in straight lines through the empty tube. The voltage applied between the electrodes accelerates these low mass particles to high velocities. Cathode rays are invisible, but their presence was first detected in early vacuum tubes when they struck the glass wall of the tube, exciting the atoms of the glass and causing them to emit light, a glow called fluorescence. Researchers noticed that objects placed in the tube in front of the cathode could cast a shadow on the glowing wall, and realized that something must be travelling in straight lines from the cathode. After the electrons reach the anode, they travel through the anode wire to the power supply and back to the cathode, so cathode rays carry electric current through the tube.

The current in a beam of cathode rays through a vacuum tube can be controlled by passing it through a metal screen of wires (a grid) to which a small negative voltage is applied. The electric field of the wires deflects some of the electrons, preventing them from reaching the anode. The amount of current that gets through to the anode depends on the voltage on the grid. Thus, a small voltage on the grid can be made to control a much larger voltage on the anode. This is the principle used in vacuum tubes to amplify electrical signals. The triode vacuum tube was the first electronic device that could amplify, and is still used in some applications such as radio transmitters. High speed beams of cathode rays can also be steered and manipulated by electric fields created by additional metal plates in the tube to which voltage is applied, or magnetic fields created by coils of wire (electromagnets). These are used in cathode ray tubes, found in televisions and computer monitors, and in electron microscopes.

After the 1654 invention of the vacuum pump by Otto von Guericke, physicists began to experiment with passing high voltage electricity through rarefied air. In 1705, it was noted that electrostatic generator sparks travel a longer distance through low pressure air than through atmospheric pressure air.

In 1838, Michael Faraday passed a current through a rarefied air filled glass tube and noticed a strange light arc with its beginning at the cathode (negative electrode) and its end are at the anode (positive electrode). In 1857, German physicist and glassblower Heinrich Geissler sucked even more air out with an improved pump, to a pressure of around 10 atm and found that, instead of an arc, a glow filled the tube. The voltage applied between the two electrodes of the tubes, generated by an induction coil, was anywhere between a few kilovolts and 100 kV. These were called Geissler tubes, similar to today's neon signs.

The explanation of these effects was that the high voltage accelerated electrically charged atoms (ions) naturally present in the air of the tube. At low pressure, there was enough space between the gas atoms that the ions could accelerate to high enough speeds that when they struck another atom they knocked electrons off of it, creating more positive ions and free electrons in a chain reaction, known as a Townsend discharge.

The positive ions are attracted to the cathode. When they struck it, they knocked many electrons out of the metal. The free electrons were all attracted to the anode.

Geissler tubes had enough air in them that the electrons could only travel a tiny distance before colliding with an atom. The electrons in these tubes moved in a slow diffusion process, never gaining much speed, so these tubes didn't produce cathode rays. Instead, they produced a colorful glow discharge (as in a modern neon light), caused when the electrons or ions struck gas atoms, exciting their orbital electrons to higher energy levels. The electrons released this energy as light. This process is called fluorescence.

By the 1870s, British physicist William Crookes and others were able to evacuate tubes to a lower pressure, below 10 atm. These were called Crookes tubes. Faraday had been the first to notice a dark space just in front of the cathode, where there was no luminescence. This came to be called the "cathode dark space", "Faraday dark space" or "Crookes dark space". Crookes found that as he pumped more air out of the tubes, the Faraday dark space spread down the tube from the cathode toward the anode, until the tube was totally dark. But at the anode (positive) end of the tube, the glass of the tube itself began to glow.

What was happening was that as more air was pumped from the tube, the electrons could travel farther, on average, before they struck a gas atom. By the time the tube was dark, most of the electrons could travel in straight lines from the cathode to the anode end of the tube without a collision. With no obstructions, these low mass particles were accelerated to high velocities by the voltage between the electrodes.These were the cathode rays.

When they reached the anode end of the tube, they were traveling so fast that, although they were attracted to it, they often flew past the anode and struck the back wall of the tube. When they struck atoms in the glass wall, they excited their orbital electrons to higher energy levels, causing them to fluoresce. Later researchers painted the inside back wall with fluorescent chemicals such as zinc sulfide, to make the glow more visible.

Cathode rays themselves are invisible, but this accidental fluorescence allowed researchers to notice that objects in the tube in front of the cathode, such as the anode, cast sharp-edged shadows on the glowing back wall. In 1869, German physicist Johann Hittorf was first to realize that something must be traveling in straight lines from the cathode to cast the shadows. Eugen Goldstein named them "cathode rays".

At this time, atoms were the smallest particles known, and were believed to be indivisible. What carried electric currents was a mystery. During the last quarter of the 19th century, many experiments were done to determine what cathode rays were. There were two theories. Crookes and Arthur Schuster believed they were particles of "radiant matter," that is, electrically charged atoms. German scientists Eilhard Wiedemann, Heinrich Hertz and Goldstein believed they were "aether waves", some new form of electromagnetic radiation, and were separate from what carried the electric current through the tube.

The debate was resolved in 1897 when J. J. Thomson measured the mass of cathode rays, showing they were made of particles, but were around 1800 times lighter than the lightest atom, hydrogen. Therefore, they were not atoms, but a new particle, the first "subatomic" particle to be discovered, which he originally called ""corpuscle"" but was later named "electron", after particles postulated by George Johnstone Stoney in 1874. He also showed they were identical with particles given off by photoelectric and radioactive materials. It was quickly recognized that they are the particles that carry electric currents in metal wires, and carry the negative electric charge of the atom.

Thomson was given the 1906 Nobel prize for physics for this work. Philipp Lenard also contributed a great deal to cathode ray theory, winning the Nobel prize for physics in 1905 for his research on cathode rays and their properties.

The gas ionization (or cold cathode) method of producing cathode rays used in Crookes tubes was unreliable, because it depended on the pressure of the residual air in the tube. Over time, the air was absorbed by the walls of the tube, and it stopped working.

A more reliable and controllable method of producing cathode rays was investigated by Hittorf and Goldstein, and rediscovered by Thomas Edison in 1880. A cathode made of a wire filament heated red hot by a separate current passing through it would release electrons into the tube by a process called thermionic emission. The first true electronic vacuum tubes, invented in 1904, used this hot cathode technique, and they superseded Crookes tubes. These tubes didn't need gas in them to work, so they were evacuated to a lower pressure, around 10 atm (10 Pa). The ionization method of creating cathode rays used in Crookes tubes is today only used in a few specialized gas discharge tubes such as krytrons.

In 1906,Lee De Forest found that a small voltage on a grid of metal wires could control a much larger current in a beam of cathode rays passing through a vacuum tube. His invention, called the triode, was the first device that could amplify electric signals, and founded the field of "electronics". Vacuum tubes made radio and television broadcasting possible, as well as radar, talking movies, audio recording, and long distance telephone service, and were the foundation of consumer electronic devices until the 1960s, when the transistor brought the era of vacuum tubes to a close.

Cathode rays are now usually called electron beams. The technology of manipulating electron beams pioneered in these early tubes was applied practically in the design of vacuum tubes, particularly in the invention of the cathode ray tube (CRT) by Ferdinand Braun in 1897, which was used in television sets and oscilloscopes. It is today employed in sophisticated devices such as electron microscopes, electron beam lithography and particle accelerators.

Like a wave, cathode rays travel in straight lines, and produce a shadow when obstructed by objects. Ernest Rutherford demonstrated that rays could pass through thin metal foils, behavior expected of a particle. These conflicting properties caused disruptions when trying to classify it as a wave or particle. Crookes insisted it was a particle, while Hertz maintained it was a wave. The debate was resolved when an electric field was used to deflect the rays by J. J. Thomson. This was evidence that the beams were composed of particles because scientists knew it was impossible to deflect electromagnetic waves with an electric field. These can also create mechanical effects, fluorescence, etc.

Louis de Broglie later (1924) showed in his doctoral dissertation that electrons are in fact much like photons in the respect that they act both as waves and as particles in a dual manner as Einstein had shown earlier for light. The wave-like behaviour of cathode rays was later directly demonstrated using a crystal lattice by Davisson and Germer in 1927.






</doc>
<doc id="6944" url="https://en.wikipedia.org/wiki?curid=6944" title="Cathode">
Cathode

A cathode is the electrode from which a conventional current leaves a polarized electrical device. (This definition can be recalled by using the mnemonic "CCD" for "cathode current departs".) A conventional current describes the direction in which positive electronic charges move. Electrons have a negative electrical charge, so the movement of electrons is opposite to that of the conventional current flow (consequently, the mnemonic "cathode current departs" also means that electrons flow "into" the device's cathode).

Cathode polarity with respect to the anode can be positive or negative depending on how the device is being operated. Although positively charged cations always move towards the cathode (hence their name) and negatively charged anions move away from it, cathode polarity depends on the device type, and can even vary according to the operating mode. In a device which absorbs energy of charge (such as recharging a battery), the cathode is negative (electrons flow out of the cathode, and charge flows into it), and in a device which provides energy (such as battery in use), the cathode is positive (electrons flow into it and charge flows out):
°A battery or galvanic cell in use has a cathode that is the positive terminal since that is where the current flows out of the device. This outward current is carried internally by positive ions moving from the electrolyte to the positive cathode (chemical energy is responsible for this "uphill" motion). It is continued externally by electrons moving into the battery which constitutes positive current flowing outwards. For example, the Daniell galvanic cell's copper electrode is the positive terminal and the cathode.
°A battery that is recharging or an electrolytic cell performing electrolysis has its cathode as the negative terminal, from which current exits the device and returns to the external generator as charge enters the battery/ cell. For example, reversing the current direction in a Daniell galvanic cell converts it into an electrolytic cell where the copper electrode is the positive terminal and also the anode.
°In a diode, the cathode is the negative terminal at the pointed end of the arrow symbol, where current flows out of the device. Note: electrode naming for diodes is always based on the direction of the forward current (that of the arrow, in which the current flows "most easily"), even for types such as Zener diodes or solar cells where the current of interest is the reverse current.
°In vacuum tubes (including cathode ray tubes) it is the negative terminal where electrons enter the device from the external circuit and proceed into the tube's near-vacuum, constituting a positive current flowing out of the device.

An electrode through which current flows the other way (into the device) is termed an anode.

The word was coined in 1834 from the Greek κάθοδος ("kathodos"), 'descent' or 'way down', by William Whewell, who had been consulted by Michael Faraday over some new names needed to complete a paper on the recently discovered process of electrolysis. In that paper Faraday explained that when an electrolytic cell is oriented so that electric current traverses the "decomposing body" (electrolyte) in a direction "from East to West, or, which will strengthen this help to the memory, that in which the sun appears to move", the cathode is where the current leaves the electrolyte, on the West side: ""kata" downwards, "`odos" a way ; the way which the sun sets".

The use of 'West' to mean the 'out' direction (actually 'out' → 'West' → 'sunset' → 'down', i.e. 'out of view') may appear unnecessarily contrived. Previously, as related in the first reference cited above, Faraday had used the more straightforward term "exode" (the doorway where the current exits). His motivation for changing it to something meaning 'the West electrode' (other candidates had been "westode", "occiode" and "dysiode") was to make it immune to a possible later change in the direction convention for current, whose exact nature was not known at the time. The reference he used to this effect was the Earth's magnetic field direction, which at that time was believed to be invariant. He fundamentally defined his arbitrary orientation for the cell as being that in which the internal current would run parallel to and in the same direction as a hypothetical magnetizing current loop around the local line of latitude which would induce a magnetic dipole field oriented like the Earth's. This made the internal current East to West as previously mentioned, but in the event of a later convention change it would have become West to East, so that the West electrode would not have been the 'way out' any more. Therefore, "exode" would have become inappropriate, whereas "cathode" meaning 'West electrode' would have remained correct with respect to the unchanged direction of the actual phenomenon underlying the current, then unknown but, he thought, unambiguously defined by the magnetic reference. In retrospect the name change was unfortunate, not only because the Greek roots alone do not reveal the cathode's function any more, but more importantly because, as we now know, the Earth's magnetic field direction on which the "cathode" term is based is subject to reversals whereas the current direction convention on which the "exode" term was based has no reason to change in the future.

Since the later discovery of the electron, an easier to remember, and more durably technically correct (although historically false), etymology has been suggested: cathode, from the Greek "kathodos", 'way down', 'the way (down) into the cell (or other device) for electrons'.

The flow of electrons is almost always from anode to cathode outside of the cell or device, regardless of the cell or device type and operating mode. An exception is when a diode reverse-conducts, either by accident (breakdown of a normal diode) or by design (breakdown of a Zener diode, photo-current of a photodiode).

In chemistry, a cathode is the electrode of an electrochemical cell at which reduction occurs; a useful mnemonic to remember this is AnOx RedCat (Oxidation at the Anode = Reduction at the Cathode). Another mnemonic is to note the cathode has a 'c', as does 'reduction'. Hence, reduction at the cathode. Perhaps most useful would be to remember cathode corresponds to cation (acceptor) and anode corresponds to anion (donor). The cathode can be negative like when the cell is electrolytic (where electrical energy provided to the cell is being used for decomposing chemical compounds); or positive as when the cell is galvanic (where chemical reactions are used for generating electrical energy). The cathode supplies electrons to the positively charged cations which flow to it from the electrolyte (even if the cell is galvanic, i.e., when the cathode is positive and therefore would be expected to repel the positively charged cations; this is due to electrode potential relative to the electrolyte solution being different for the anode and cathode metal/electrolyte systems in a galvanic cell).

The cathodic current, in electrochemistry, is the flow of electrons from the cathode interface to a species in solution. The anodic current is the flow of electrons into the anode from a species in solution.

In an electrolytic cell, the cathode is where the negative polarity is applied to drive the cell. Common results of reduction at the cathode are hydrogen gas or pure metal from metal ions. When discussing the relative reducing power of two redox agents, the couple for generating the more reducing species is said to be more "cathodic" with respect to the more easily reduced reagent.

In a galvanic cell, the cathode is where the positive pole is connected to allow the circuit to be completed: as the anode of the galvanic cell gives off electrons, they return from the circuit into the cell through the cathode.

When metal ions are reduced from ionic solution, they form a pure metal surface on the cathode. Items to be plated with pure metal are attached to and become part of the cathode in the electrolytic solution.

In physics or electronics, a cathode is an electrode that emits electrons into the device. This contrasts with an anode, which accepts electrons.

In a vacuum tube or electronic vacuum system, the cathode is a metal surface which emits free electrons into the evacuated space. Since the electrons are attracted to the positive nuclei of the metal atoms, they normally stay inside the metal and require energy to leave it; this is called the "work function" of the metal. Cathodes are induced to emit electrons by several mechanisms:

Cathodes can be divided into two types:

A hot cathode is a cathode that is heated by a filament to produce electrons by thermionic emission. The filament is a thin wire of a refractory metal like tungsten heated red-hot by an electric current passing through it. Before the advent of transistors in the 1960s, virtually all electronic equipment used hot-cathode vacuum tubes. Today hot cathodes are used in vacuum tubes in radio transmitters and microwave ovens, to produce the electron beams in older cathode ray tube (CRT) type televisions and computer monitors, in x-ray generators, electron microscopes, and fluorescent tubes.

There are two types of hot cathodes:

In order to improve electron emission, cathodes are treated with chemicals, usually compounds of metals with a low work function. Treated cathodes require less surface area, lower temperatures and less power to supply the same cathode current. The untreated tungsten filaments used in early tubes (called "bright emitters") had to be heated to 1400 °C (~2500 °F), white-hot, to produce sufficient thermionic emission for use, while modern coated cathodes produce far more electrons at a given temperature so they only have to be heated to 425–600 °C (~800–1100 °F) () There are two main types of treated cathodes:

This is a cathode that is not heated by a filament. They may emit electrons by field electron emission, and in gas-filled tubes by secondary emission. Some examples are electrodes in neon lights, cold-cathode fluorescent lamps (CCFLs) used as backlights in laptops, thyratron tubes, and Crookes tubes. They do not necessarily operate at room temperature; in some devices the cathode is heated by the electron current flowing through it to a temperature at which thermionic emission occurs. For example, in some fluorescent tubes a momentary high voltage is applied to the electrodes to start the current through the tube; after starting the electrodes are heated enough by the current to keep emitting electrons to sustain the discharge.

Cold cathodes may also emit electrons by photoelectric emission. These are often called "photocathodes" and are used in phototubes used in scientific instruments and image intensifier tubes used in night vision goggles.

In a semiconductor diode, the cathode is the N–doped layer of the PN junction with a high density of free electrons due to doping, and an equal density of fixed positive charges, which are the dopants that have been thermally ionized. In the anode, the converse applies: It features a high density of free "holes" and consequently fixed negative dopants which have captured an electron (hence the origin of the holes).

When P and N-doped layers are created adjacent to each other, diffusion ensures that electrons flow from high to low density areas: That is, from the N to the P side. They leave behind the fixed positively charged dopants near the junction. Similarly, holes diffuse from P to N leaving behind fixed negative ionised dopants near the junction. These layers of fixed positive and negative charges are collectively known as the depletion layer because they are depleted of free electrons and holes. The depletion layer at the junction is at the origin of the diode's rectifying properties. This is due to the resulting internal field and corresponding potential barrier which inhibit current flow in reverse applied bias which increases the internal depletion layer field. Conversely, they allow it in forwards applied bias where the applied bias reduces the built in potential barrier.

Electrons which diffuse from the cathode into the P-doped layer, or anode, become what are termed "minority carriers" and tend to recombine there with the majority carriers, which are holes, on a timescale characteristic of the material which is the p-type minority carrier lifetime. Similarly, holes diffusing into the N-doped layer become minority carriers and tend to recombine with electrons. In equilibrium, with no applied bias, thermally assisted diffusion of electrons and holes in opposite directions across the depletion layer ensure a zero net current with electrons flowing from cathode to anode and recombining, and holes flowing from anode to cathode across the junction or depletion layer and recombining.

Like a typical diode, there is a fixed anode and cathode in a Zener diode, but it will conduct current in the reverse direction (electrons flow from anode to cathode) if its breakdown voltage or "Zener voltage" is exceeded.



</doc>
<doc id="6945" url="https://en.wikipedia.org/wiki?curid=6945" title="Chrominance">
Chrominance

Chrominance ("chroma" or C for short) is the signal used in video systems to convey the color information of the picture, separately from the accompanying luma signal (or Y for short). Chrominance is usually represented as two color-difference components: U = B′ − Y′ (blue − luma) and V = R′ − Y′ (red − luma). Each of these difference components may have scale factors and offsets applied to it, as specified by the applicable video standard. 

In composite video signals, the U and V signals modulate a color subcarrier signal, and the result is referred to as the chrominance signal; the phase and amplitude of this modulated chrominance signal correspond approximately to the hue and saturation of the color. In digital-video and still-image color spaces such as Y′CbCr, the luma and chrominance components are digital sample values.

Separating RGB color signals into luma and chrominance allows the bandwidth of each to be determined separately. Typically, the chrominance bandwidth is reduced in analog composite video by reducing the bandwidth of a modulated color subcarrier, and in digital systems by chroma subsampling.

The idea of transmitting a color television signal with distinct luma and chrominance components originated with Georges Valensi, who patented the idea in 1938. Valensi's patent application described:
The use of two channels, one transmitting the predominating color (signal T), and the other the mean brilliance (signal t) output from a single television transmitter to be received not only by color television receivers provided with the necessary more expensive equipment, but also by the ordinary type of television receiver which is more numerous and less expensive and which reproduces the pictures in black and white only.
Previous schemes for color television systems, which were incompatible with existing monochrome receivers, transmitted RGB signals in various ways.

In analog television, chrominance is encoded into a video signal using a subcarrier frequency. Depending on the video standard, the chrominance subcarrier may be either quadrature-amplitude-modulated (NTSC and PAL) or frequency-modulated (SECAM).

In the PAL system, the color subcarrier is 4.43 MHz above the video carrier, while in the NTSC system it is 3.58 MHz above the video carrier. The NTSC and PAL standards are the most commonly used, although there are other video standards that employ different subcarrier frequencies. For example, PAL-M (Brazil) uses a 3.58 MHz subcarrier, and SECAM uses two different frequencies, 4.250 MHz and 4.40625 MHz above the video carrier.

The presence of chrominance in a video signal is indicated by a color burst signal transmitted on the back porch, just after horizontal synchronization and before each line of video starts. If the color burst signal were visible on a television screen, it would appear as a vertical strip of a very dark olive color. In NTSC and PAL, hue is represented by a phase shift of the chrominance signal relative to the color burst, while saturation is determined by the amplitude of the subcarrier. In SECAM (R′ − Y′) and (B′ − Y′) signals are transmitted alternately and phase does not matter.

Chrominance is represented by the U-V color plane in PAL and SECAM video signals, and by the I-Q color plane in NTSC.

Digital video and digital still photography systems sometimes use a luma/chroma decomposition for improved compression. For example, when an ordinary RGB digital image is compressed via the JPEG standard, the RGB colorspace is first converted (by a rotation matrix) to a YCbCr colorspace, because the three components in that space have less correlation redundancy and because the chrominance components can then be subsampled by a factor of 2 or 4 to further compress the image. On decompression, the Y′CbCr space is rotated back to RGB.



</doc>
<doc id="6946" url="https://en.wikipedia.org/wiki?curid=6946" title="Chirality (disambiguation)">
Chirality (disambiguation)

Chirality ("handedness") is a property of asymmetry.

Chirality may also refer to:




</doc>
<doc id="6947" url="https://en.wikipedia.org/wiki?curid=6947" title="Campus">
Campus

A campus is traditionally the land on which a college or university and related institutional buildings are situated. Usually a college campus includes libraries, lecture halls, residence halls, student centers or dining halls, and park-like settings.

A modern campus is a collection of buildings and grounds that belong to a given institution, either academic or non-academic. Examples include the Googleplex and the Apple Campus.

The word derives from a Latin word for "field" and was first used to describe the large field adjacent Nassau Hall of the College of New Jersey (now Princeton University) in 1774. The field separated Princeton from the small nearby town.

Some other American colleges later adopted the word to describe individual fields at their own institutions, but "campus" did not yet describe the whole university property. A school might have one space called a campus, one called a field, and another called a yard.

The tradition of a campus began with the medieval European universities where the students and teachers lived and worked together in a cloistered environment. The notion of the importance of the setting to academic life later migrated to America, and early colonial educational institutions were based on the Scottish and English collegiate system.

The campus evolved from the cloistered model in Europe to a diverse set of independent styles in the United States. Early colonial colleges were all built in proprietary styles, with some contained in single buildings, such as the campus of Princeton University or arranged in a version of the cloister reflecting American values, such as Harvard's. Both the campus designs and the architecture of colleges throughout the country have evolved in response to trends in the broader world, with most representing several different contemporary and historical styles and arrangements.

The meaning expanded to include the whole institutional property during the 20th century, with the old meaning persisting into the 1950s in some places.

Sometimes the lands on which company office buildings sit, along with the buildings, are called campuses. The Microsoft Campus in Redmond, Washington is a good example. Hospitals, and even airports sometimes use the term to describe the territory of their facilities.

The word "campus" has also been applied to European universities, although most such institutions are characterized by ownership of individual buildings in urban settings rather than park-like lawns in which buildings are placed.




</doc>
<doc id="6948" url="https://en.wikipedia.org/wiki?curid=6948" title="Crossbow">
Crossbow

A crossbow is a type of ranged weapon based on the bow and consisting of a horizontal bow-like assembly mounted on a frame which is handheld in a similar fashion to the stock of a gun. It shoots arrow-like projectiles called bolts or quarrels. The medieval crossbow was called by many names, most of which were derived from the word "ballista", a torsion siege engine resembling a crossbow.

Historically, crossbows played a significant role in the warfare of East Asia and Medieval Europe. The earliest crossbows in the world were invented in ancient China and caused a major shift in the role of projectile weaponry. The traditional bow and arrow had long been a specialized weapon that required a considerable user training, physical strength and expertise to operate with any degree of practical efficiency. In many cultures, bowmen were considered a separate and superior caste, despite usually being drawn from the common class, as their archery skill-set was essentially developed from birth (similar to many horseman cultures) and was impossible to reproduce outside a pre-established cultural tradition, which many nations lacked. In contrast, the crossbow was the first projectile weapon to be simple, cheap and physically undemanding enough to be operated by large numbers of conscript soldiers, thus enabling virtually any nation to field a potent force of ranged crossbowmen with little expense beyond the cost of the weapons themselves.

In modern times, crossbows have been largely supplanted by firearms in most roles, but are still widely used for shooting sports, hunting and when shooting with relative silence is important.

A crossbow is essentially a bow mounted on an elongated frame (called a tiller or stock) with a built-in mechanism that holds the drawn bow string, as well as a trigger mechanism that allows the string to be released. The earliest designs featured a transverse slot in the top surface of the frame, down into which the string was placed. To shoot this design, a vertical rod is thrust up through a hole in the bottom of the notch, forcing the string out. This rod is usually attached perpendicular to a rear-facing lever called a "tickler". A later design implemented a rolling cylindrical pawl called a "nut" to retain the string. This nut has a perpendicular centre slot for the bolt, and an intersecting axial slot for the string, along with a lower face or slot against which the internal trigger sits. They often also have some form of strengthening internal "sear" or trigger face, usually of metal. These "roller nuts" were either free-floating in their close-fitting hole across the stock, tied in with a binding of sinew or other strong cording; or mounted on a metal axle or pins. Removable or integral plates of wood, ivory, or metal on the sides of the stock kept the nut in place laterally. Nuts were made of antler, bone, or metal. Bows could be kept taut and ready to shoot for some time with little physical straining, allowing crossbowmen to aim better without fatiguing.

The bow (called the "prod" or "lath" on a crossbow) of early crossbows was made of a single piece of wood, usually ash or yew. Composite bows are made from layers of different material, often wood, horn, and sinew glued together and bound with animal tendon. These composite bows made of several layers are much stronger and more efficient in releasing energy than simple wooden bows. As steel became more widely available in Europe around the 14th century, steel prods came into use.

The crossbow prod is very short compared to ordinary bows, resulting in a short draw length. This leads to a higher draw weight in order to store the same amount of energy. Furthermore, the thick prods are a bit less efficient at releasing energy, but more energy can be stored by a crossbow. Traditionally, the prod was often lashed to the stock with rope, whipcord, or other strong cording. This cording is called the "bridle".

The strings for a crossbow are typically made of strong fibres that would not tend to fray. Whipcord was very common; however linen, hemp, and sinew were used as well. In wet conditions, twisted mulberry root was occasionally used.

Very light crossbows can be drawn by hand, but heavier types need the help of mechanical devices. The simplest version of mechanical cocking device is a hook attached to a belt, drawing the bow by straightening the legs. Other devices are hinged levers, which either pulled or pushed the string into place, cranked rack-and-pinion devices called "cranequins" and multiple cord-and-pulley cranked devices called windlasses.
Crossbows exist in different variants. One way to classify them is the acceleration system, while another is the size and energy, degree of automation or projectiles.

A recurve crossbow is a bow that has tips curving away from the archer. The recurve bow's bent limbs have a longer draw length than an equivalent straight-limbed bow, giving more acceleration to the projectile and less hand shock. Recurved limbs also put greater strain on the materials used to make the bow, and they may make more noise with the shot.

Multiple bow systems have a special system of pulling the sinew via several bows (which can be recurve bows). The workings can be compared to a modern compound bow system. The weapon uses several different bows instead of one bow with a tackle system to achieve a higher acceleration of the sinew via the multiplication with each bow's pulling effect.

A compound crossbow is a modern crossbow and is similar to a compound bow. The limbs are usually much stiffer than those of a recurve crossbow. This limb stiffness makes the compound bow more energy efficient than other bows, but the limbs are too stiff to be drawn comfortably with a string attached directly to them. The compound bow has the string attached to the pulleys, one or both of which has one or more cables attached to the opposite limb. When the string is drawn back, the string causes the pulleys to turn. This causes the pulleys to pull the cables, which in turn causes the limbs to bend and thus store energy. Other types of compound bows use either (one or both) cam shaped or eccentrically mounted pulleys in order to provide a "let off", such that the archer is not holding against the maximum draw weight of the bow while trying to aim. But, in a crossbow, the string is held back mechanically, so there is no advantage in providing a let off. Therefore, compound crossbows generally only use pulleys that are both round and concentrically mounted, in order to capture the maximum available energy from the relatively short draw length.
The smallest crossbows are pistol crossbows. Others are simple long stocks with the crossbow mounted on them. These could be shot from under the arm. The next step in development was stocks of the shape that would later be used for firearms, which allowed better aiming. The arbalest was a heavy crossbow that required special systems for pulling the sinew via windlasses. For siege warfare, the size of crossbows was further increased to hurl large projectiles, such as rocks, at fortifications. The required crossbows needed a massive base frame and powerful windlass devices. Such devices include the oxybeles. The ballista has torsion springs replacing the elastic prod of the oxybeles, but later also developed into smaller versions. "Ballista" is still the root word for crossbow in Romance languages such as Italian ("balestra") and Spanish ("ballesta").

The repeating crossbow automated the separate actions of stringing the bow, placing the projectile and shooting. This way the task can be accomplished with a simple one-handed movement, while keeping the weapon stationary. As a result, it is possible to shoot at a faster rate compared to an unmodified version. The Greek Polybolos was an ancient repeating ballista reputedly invented by Dionysius of Alexandria in the 3rd century BC. The Chinese repeating crossbow, Chu Ko Nu, is a handheld crossbow that accomplishes the task with a magazine containing a number of bolts on top. The mechanism is worked by moving a rectangular lever forward and backward. The weapon was mainly used as a weapon against lightly armored soldiers, since it shot small bolts that were often dipped in poison.

A bullet crossbow is a type of handheld crossbow that, instead of arrows or bolts, shoots spherical projectiles made of stone, clay or lead. There are two variants; one has a double string with a pocket for the projectile, and the other has a barrel with a slot for the string.

A slurbow is a type of crossbow with a wood or metal barrel over the top of the stock that is arguably influenced by the emergence of the pistol.

The arrow-like projectiles of a crossbow are called bolts. These are much shorter than arrows, but can be several times heavier. There is an optimum weight for bolts to achieve maximum kinetic energy, which varies depending on the strength and characteristics of the crossbow, but most could pass through common mail. In ancient times, the bolts of a strong crossbow were usually several times heavier than arrows. Modern bolts are stamped with a proof mark to ensure their consistent weight. Bolts do not have fletching, i.e. feathered ends like those commonly seen on arrows. Crossbow bolts can be fitted with a variety of heads, some with sickle-shaped heads to cut rope or rigging; but the most common today is a four-sided point called a quarrel. A highly specialized type of bolt is employed to collect blubber biopsy samples used in biology research.

Most modern crossbow hunters incorrectly refer to the bolts as arrows, due to the similar appearance, but the physics of how a bolt finds its target are different than that of an arrow used in a vertical bow.

Crossbows can also be adapted to shoot lead bullets or rocks, in which case they are called stone-bows. Primarily used for hunting wildfowl, these usually have a double string with a pouch between the strings to hold the projectile.

Even relatively small differences in arrow weight can have a considerable impact on its drop and, conversely, its flight trajectory.

The ancient Chinese crossbow often included a metal (i.e. bronze or steel) grid serving as iron sights. Modern crossbow sights often use similar technology to modern firearm sights, such as red dot sights and telescopic sights. Many crossbow scopes feature multiple crosshairs to compensate for the significant effects of gravity over different ranges. In most cases, a newly bought crossbow will need to be sighted for accurate shooting.

Quivers can be mounted to hold ammunition. These are often made from plastic and usually hold the bolts in fixed positions along the structure. A popular detachable design consists of a main arm that is attached to the weapon, a plate on one end that secures four or more individual bolts at a point on their shafts and at the other end a cover that secures their heads. This kind of quiver is attached under the front of the crossbow, parallel to the string and is designed to be quickly detached and reattached. Other designs hold bolts underneath the crossbow parallel to the stock, sometimes on either side of the crossbow.

A major cause of the sound of shooting a crossbow is vibration of various components. Crossbow silencers are multiple components placed on high vibration parts, such as the string and limbs, to dampen vibration and suppress the sound of loosing the bolt.

The earliest evidence of crossbows comes from ancient China in the form of crossbow triggers dating back to the 6th century BC. According to Sir Joseph Needham in his "Science and Civilisation in China", it is not possible to pinpoint exactly which of the East Asian peoples invented the crossbow. However, there is unquestionable evidence that the crossbow was used for military purposes at least as far back as the Warring States period from the second half of the 4th century BC onwards.
In terms of archaeological evidence, bronze crossbow bolts dating from as early as the mid-5th century BC have been found at a Chu burial site in Yutaishan, Hubei. The earliest handheld crossbow stocks with bronze trigger, dating from the 6th century BC, were found in Tomb 3 and 12 at Qufu, Shandong, previously the capital of Lu, ancient China. Other early finds of crossbows were discovered in Tomb 138 at Saobatang, Hunan, dating to the mid-4th century BC. Ammunition for crossbows could have also been spherical. In discussing the astronomical topics such as solar and lunar eclipses, the Western-Han era mathematician and music theorist Jing Fang (78-37 BC) wrote that the moon, shaped like a ball, produced no light and was illuminated only by the sun, which he compared to the shape of a round crossbow bullet.

Repeating crossbows, first mentioned in the "Records of the Three Kingdoms", were discovered in 1986 in Tomb 47 at Qinjiazui, Hubei, and were dated to around the 4th century BC. The earliest Chinese document mentioning a crossbow were texts from the 4th to 3rd centuries BC attributed to the followers of Mozi. Sun Tzu's influential treatise on war, "The Art of War" (first appearance dated to sometime between 500 BC to 300 BC) refers in chapter five to the traits of crossbows and in chapter twelve, to the usage of crossbows. One of the earliest reliable descriptions of this weapon in warfare is of an ambush in 341 BC, the Battle of Ma-Ling. In the opinion of one authority, the crossbow () had become "nothing less than the standard weapon of the Han armies" by the 2nd century BC.

The earliest textual evidence of the "handheld" crossbow used in battle dates to the 4th century BC. Handheld crossbows with complex bronze trigger mechanisms have also been found with the Terracotta Army in the tomb of Qin Shihuang (r. 221–210 BC) that are similar to specimens from the subsequent Han Dynasty (202 BC–220 AD), while crossbowmen described in the Qin and Han Dynasty learned drill formations, some were even mounted as cavalry units, and Han Dynasty writers attributed the success of numerous battles against the Xiongnu to massed crossbow volleys. The bronze triggers were designed in such a way that they were able to store a large amount of energy within the bow when drawn, but was easily shot with little recoil when the trigger were pulled (this allowed it for precision shooting). The metal portions of the crossbow were also mass-produced with precision, with the bronze mechanisms being interchangeable. Finally, the Qin and Han Dynasties also developed crossbow shooting lines, with alternating rows of crossbowmen shooting and reloading in a manner similar to a musket firing line.

Different varieties of crossbows were also developed, such as the repeating crossbow, multi-shot crossbow, larger field artillery crossbows, and repeating multi-shot crossbow.

According to the Six Secrets Teachings by T'ai Kung from the Warring States era, an ideal army proportion was 30% crossbowmen:
"Armored soldiers, ten thousand. Strong crossbowmen, six thousand. Halberdiers with shields, two thousand. Spearmen with shields, two thousand. Skilled men to repair offensive weapons and sharpen them, three hundred." 
During the time of the Tang Dynasty, the use of crossbows fell out of favor and traditional recurve bows were more prioritized. Crossbows eventually made a resurgence during the Song Dynasty that succeeded the Tang Dynasty. 
"An 11th century writer remarks that the T’ang had so little confidence in the crossbow that they equipped its users with halberds for self-defence. They then tended to succumb to the temptation to throw down their crossbows and charge, so that other men had to be sent to follow them and pick up the discarded weapons. One source gives the ratio of bows to crossbows in the ideal army as five to one." 

In Vietnamese historical legend, general Thục Phán, who ruled over the ancient kingdom of Âu Lạc from 257 to 207 BC, is said to have owed his power to a magic crossbow, capable of shooting thousands of bolts at once.

Crossbow technology for multi-proded crossbows was transferred from the Chinese to Champa, which Champa used in its invasion of the Khmer Empire's Angkor in 1177. China transferred crossbow technology to Champa. When the Chams sacked Angkor they used the Chinese siege crossbow. Crossbows were given to the Chams by China. Crossbows and archery while mounted were instructed to the Cham by a Chinese in 1171.

A third Greek author, Biton of Pergamon (fl. 2nd century BC), whose reliability has been positively reevaluated by recent scholarship, described two advanced forms of the "gastraphetes", which he credits to Zopyros, an engineer from southern Italy. Zopyrus has been plausibly equated with a Pythagorean of that name who seems to have flourished in the late 5th century BC. He probably designed his bow-machines on the occasion of the sieges of Cumae and Milet between 421 BC and 401 BC. The bows of these machines already featured a winched pull back system and could apparently throw two missiles at once.

From the mid-4th century BC onwards, evidence of the Greek use of crossbows becomes more dense and varied: Arrow-shooting machines ("katapeltai") are briefly mentioned by Aeneas Tacticus in his treatise on siegecraft written around 350 BC. An Athenian inventory from 330–329 BC includes catapults bolts with heads and flights. Arrow-shooting machines in action are reported from Philip II's siege of Perinthos in Thrace in 340 BC. At the same time, Greek fortifications began to feature high towers with shuttered windows in the top, presumably to house anti-personnel arrow shooters, as in Aigosthena.

The transition to torsion catapults, which are not considered crossbows and came to dominate Greek and Roman artillery design, is first evident in inventories of the Athenian arsenal from between 338 and 326 BC.

The ancient world knew a variety of mechanical hand-held weapons similar to the later medieval crossbow. The exact terminology is a subject of continuing scholarly debate.
Roman authors like Vegetius (fl. 4th century) note repeatedly the use of arrow shooting weapons such as "arcuballista" and "manuballista" respectively "cheiroballista". While most scholars agree that one or more of these terms refer to handheld mechanical weapons, there is disagreement whether these were flexion bows or torsion powered like the recent Xanten find. According to R. Ernest Dupuy and Trevor N. Dupuy, in 36 BC a Han empire expedition into central Asia encountered and defeated a contingent of Roman legionaries, most likely leftovers of the army Crassus lead in his doomed invasion of Parthia (the Parthians most likely pitted them against the Chinese for their own amusement). Chinese victory was based on their crossbows, whose bolts & quarrels seem to "have penetrated Roman shields and armor." The Chinese crossbow was transmitted to the Roman world in this event.

The Roman commander Arrian (c. 86 – after 146) records in his "Tactica" Roman cavalry training for shooting some mechanical handheld weapon from horseback.
Sculptural reliefs from Roman Gaul depict the use of crossbows in hunting scenes. These are remarkably similar to the later medieval crossbow.

The crossbow is portrayed as a hunting weapon on four Pictish stones from early medieval Scotland (6th to 9th centuries): St. Vigeans no. 1, Glenferness, Shandwick, and Meigle. It was introduced to England by the invaders during the Norman conquest. After the steel crossbow likely made the weapon more popular around 1100, the Second Lateran Council of 1139 spoke out against using any form of archery (including crossbows) against other Christians, as well as jousts and tournaments; Pope Innocent III reiterated the ban later that century. While a trained longbowman can fire ten arrows in the time a crossbowman can fire three bolts, an untrained archer can learn to use the latter weapon well after three weeks of training but would still be a beginner with the former.

In the armies of Europe, mounted and unmounted crossbowmen, often mixed with slingers, javelineers and archers, occupied a central position in battle formations. Usually they engaged the enemy in offensive skirmishes before an assault of mounted knights. Crossbowmen were also valuable in counterattacks to protect their infantry. The rank of commanding officer of the crossbowmen corps was one of the highest positions in any army of this time. Along with polearm weapons made from farming equipment, the crossbow was also a weapon of choice for insurgent peasants such as the Taborites.

The Muslims called the crossbow "qaws Ferengi", or "Frankish bow", as the Crusaders used the crossbow against the Arab and Turkic horsemen with remarkable success. The adapted crossbow was used by the Islamic armies in defence of their castles. Later, footstrapped versions became very popular among the Muslim armies in Iberia. During the Crusades, Europeans were exposed to Muslim composite bows, made from layers of different material—often wood, horn and sinew—glued together and bound with animal tendon. These composite bows could be much more powerful than wooden bows, and were adopted for crossbow prods across Europe. Crossbow prods could be more easily waterproofed than hand bows, which was essential in the humid European climate.

In Western Africa and Central Africa, crossbows served as a scouting weapon and for hunting, with enslaved Africans bringing this technology to natives in America. In the American South, the crossbow was used for hunting and warfare when firearms or gunpowder were unavailable because of economic hardships or isolation. In the North of Northern America, light hunting crossbows were traditionally used by the Inuit. These are technologically similar to the African derived crossbows, but have a different route of influence.

The native Montagnards of Vietnam's Central Highlands were also known to have used crossbows, as both a tool for hunting, and later, an effective weapon against the Viet Cong during the Vietnam War. Montagnard fighters armed with crossbows proved a highly valuable asset to the US Special Forces operating in Vietnam, and it was not uncommon for the Green Berets to integrate Montagnard crossbowmen into their strike teams.

The French, and the British used a Sauterelle (French for grasshopper) in World War I. It was lighter and more portable than the Leach Trench Catapult, but less powerful. It weighed and could throw an F1 grenade or Mills bomb . The Sauterelle replaced the Leach Catapult in British service and was in turn replaced in 1916 by the 2 inch Medium Trench Mortar and Stokes mortar.

Crossbows are used for shooting sports and bowhunting in modern archery and for blubber biopsy samples in scientific research. In some countries such as Canada or the United Kingdom, they may be less heavily regulated than firearms, and thus more popular for hunting; some jurisdictions have bow and/or crossbow only seasons.

In modern times, crossbows are no longer used for assassinations, but there are still some applications. For example, in the Americas, the Peruvian army (Ejército) equips some soldiers with crossbows and rope, to establish a zip-line in difficult terrain. In Brazil the CIGS (Jungle Warfare Training Center) also trains soldiers in the use of crossbows. In the United States, SAA International Ltd manufacture a 150 ft·lb crossbow-launched version of the U.S. Army type classified Launched Grapnel Hook (LGH), among other mine countermeasure solutions designed for the middle-eastern theatre. It has been successfully evaluated in Cambodia and Bosnia. It is used to probe for and detonate tripwire initiated mines and booby traps at up to 50 meters. The concept is similar to the LGH device originally only fired from a rifle, as a plastic retrieval line is attached. Reusable up to 20 times, the line can be reeled back in without exposing oneself. The device is of particular use in tactical situations where noise discipline is important.
In Europe, British-based Barnett International supplied crossbows to Serbian forces which according to "The Guardian" were later used "in ambushes and as a counter-sniper weapon", against the Kosovo Liberation Army during the Kosovo War in the areas of Pec and Djakovica, south west of Kosovo. Whitehall launched an investigation, though the department of trade and industry established that not being "on the military list" crossbows were not covered by such export regulations. Paul Beaver of Jane's defence publications commented that, "They are not only a silent killer, they also have a psychological effect". On 15 February 2008, Serbian Minister of Defence Dragan Sutanovac was pictured testing a Barnett crossbow during a public exercise of the Serbian army's Special Forces in Nis, 200 km south of capital Belgrade. Special forces in both Greece and Turkey also continue to employ the crossbow. Spain's Green Berets still use the crossbow as well.

In Asia, some Chinese armed forces use crossbows, including the special force Snow Leopard Commando Unit of the People's Armed Police and the People's Liberation Army. One justification for this comes in the crossbow's ability to stop persons carrying explosives without risk of causing detonation. During the Xinjiang riots of July 2009, crossbows were used alongside modern military hardware to quell protests. The Indian Navy's Marine Commando Force were equipped until the late 1980s with crossbows supplied with cyanide-tipped bolts, as an alternative to suppressed handguns.

With a crossbow, archers could release a draw force far in excess of what they could have handled with a bow. Furthermore, the crossbow could hold the tension for a long time, whereas even the strongest longbowman could only hold a drawn bow for a short period of time. The ease of use of a crossbow allows it to be used effectively with little training, while other types of bows take far more skill to shoot accurately. The disadvantage is the greater weight and clumsiness compared to a bow, as well as the slower rate of shooting and the lower efficiency of the acceleration system, but there would be reduced elastic hysteresis, making the crossbow a more accurate weapon.

Crossbows have a much smaller draw length than bows. This means that for the same energy to be imparted to the arrow (or bolt), the crossbow has to have a much higher draw weight.

A direct comparison between a fast hand-drawn replica crossbow and a longbow show a 6:10 rate of shooting or a 4:9 rate within 30 seconds and comparable weapons.

Can. 29 of the Second Lateran Council under Pope Innocent II in 1139 banned the use of crossbows, as well as slings and bows, against Christians.

Today, the crossbow often has a complicated legal status due to the possibility of lethal use and its similarities to both firearms and archery weapons. While some jurisdictions regard crossbows the same as firearms, many others do not require any sort of license to own a crossbow. The legality of using a crossbow for hunting varies widely around the world, and even within different jurisdictions of some federal countries.

For example, in Canada you do not need a valid licence or registration certificate to possess any other type of bow, including a crossbow that is longer than 500 mm and that requires the use of both hands.




</doc>
<doc id="6949" url="https://en.wikipedia.org/wiki?curid=6949" title="Carbamazepine">
Carbamazepine

Carbamazepine (CBZ), sold under the tradename Tegretol, among others, is a medication used primarily in the treatment of epilepsy and neuropathic pain. It is not effective for absence seizures or myoclonic seizures. It is used in schizophrenia along with other medications and as a second-line agent in bipolar disorder. Carbamazepine appears to work as well as phenytoin and valproate.
Common side effects include nausea and drowsiness. Serious side effects may include skin rashes, decreased bone marrow function, suicidal thoughts, or confusion. It should not be used in those with a history of bone marrow problems. Use during pregnancy may cause harm to the baby; however stopping it in pregnant women with seizures is not recommended. Its use during breastfeeding is not recommended. Care should be taken in those with either kidney or liver problems.
Carbamazepine was discovered in 1953 by Swiss chemist Walter Schindler. It was first marketed in 1962. It is available as a generic medication. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. The wholesale cost in the developing world is between 0.01 and 0.07 USD per dose as of 2014.

Carbamazepine is typically used for the treatment of seizure disorders and neuropathic pain. It is used off-label as a second-line treatment for bipolar disorder and in combination with an antipsychotic in some cases of schizophrenia when treatment with a conventional antipsychotic alone has failed. It is not effective for absence seizures or myoclonic seizures.

In the United States, the FDA-approved medical uses are epilepsy (including partial seizures, generalized tonic-clonic seizures and mixed seizures), trigeminal neuralgia, and manic and mixed episodes of bipolar I disorder.

The drug is also claimed to be effective for ADHD.

As of 2014, a controlled release formulation was available for which there is tentative evidence showing fewer side effects and unclear evidence with regard to whether there is a difference in efficacy.

In the US, the label for carbamazepine contains warnings concerning:

Common adverse effects may include drowsiness, dizziness, headaches and migraines, motor coordination impairment, nausea, vomiting, and/or constipation. Alcohol use while taking carbamazepine may lead to enhanced depression of the central nervous system. Less common side effects may include increased risk of seizures in people with mixed seizure disorders, abnormal heart rhythms, blurry or double vision. Also, rare case reports of an auditory side effect have been made, whereby patients perceive sounds about a semitone lower than previously; this unusual side effect is usually not noticed by most people, and disappears after the person stops taking carbamazepine.

Carbamazepine has a potential for drug interactions; caution should be used in combining other medicines with it, including other antiepileptics and mood stabilizers. Lower levels of carbamazepine are seen when administrated with phenobarbital, phenytoin, or primidone, which can result in breakthrough seizure activity. Carbamazepine, as a CYP450 inducer, may increase clearance of many drugs, decreasing their concentration in the blood to subtherapeutic levels and reducing their desired effects. Drugs that are more rapidly metabolized with carbamazepine include warfarin, lamotrigine, phenytoin, theophylline, and valproic acid. Drugs that decrease the metabolism of carbamazepine or otherwise increase its levels include erythromycin, cimetidine, propoxyphene, and calcium channel blockers. Carbamazepine also increases the metabolism of the hormones in birth control pills and can reduce their effectiveness, potentially leading to unexpected pregnancies. As a drug that induces cytochrome P450 enzymes, it accelerates elimination of many benzodiazepines and decreases their action.

Valproic acid and valnoctamide both inhibit microsomal epoxide hydrolase (MEH), the enzyme responsible for the breakdown of carbamazepine-10,11 epoxide into inactive metabolites. By inhibiting MEH, valproic acid and valnoctamide cause a build-up of the active metabolite, prolonging the effects of carbamazepine and delaying its excretion.

Grapefruit juice raises the bioavailability of carbamazepine by inhibiting CYP3A4 enzymes in the gut wall and in the liver. Carbamazepine increases the processing of methadone resulting in lower blood levels.

Serious skin reactions such as Stevens–Johnson syndrome or toxic epidermal necrolysis due to carbamazepine therapy are more common in people with a particular human leukocyte antigen allele, HLA-B*1502. Odds ratios for the development of Stevens-Johnson syndrome or toxic epidermal necrolysis in people who carry the allele can be in the double, triple or even quadruple digits, depending on the population studied. HLA-B*1502 occurs almost exclusively in people with ancestry across broad areas of Asia, but has a very low or absent frequency in European, Japanese, Korean and African populations. However, the HLA-A*31:01 allele has been shown to be a strong predictor of both mild and severe adverse reactions, such as the DRESS syndrome form of severe cutaneous reactions, to carbamazepine among Japanese, Chinese, Korean, and Europeans.

Carbamazepine is relatively slowly but well absorbed after oral administration. Its plasma half-life is about 30 hours when it is given as single dose, but it is a strong inducer of hepatic enzymes and the plasma half-life shortens to about 15 hours when it is given repeatedly. 

Carbamazepine is a sodium channel blocker. It binds preferentially to voltage-gated sodium channels in their inactive conformation, which prevents repetitive and sustained firing of an action potential. Carbamazepine has effects on serotonin systems but the relevance to its antiseizure effects is uncertain. There is evidence that it is a serotonin releasing agent and possibly even a serotonin reuptake inhibitor.

Carbamazepine was discovered by chemist Walter Schindler at J.R. Geigy AG (now part of Novartis) in Basel, Switzerland, in 1953. It was first marketed as a drug to treat epilepsy in Switzerland in 1963 under the brand name "Tegretol"; its use for trigeminal neuralgia (formerly known as tic douloureux) was introduced at the same time. It has been used as an anticonvulsant and antiepileptic in the UK since 1965, and has been approved in the US since 1968.

In 1971, Drs. Takezaki and Hanaoka first used carbamazepine to control mania in patients refractory to antipsychotics (lithium was not available in Japan at that time). Dr. Okuma, working independently, did the same thing with success. As they were also epileptologists, they had some familiarity with the antiaggression effects of this drug. Carbamazepine was studied for bipolar disorder throughout the 1970s.

Carbamazepine has been detected in wastewater effluent. Field and laboratory studies have been conducted to understand the accumulation of carbamazepine in food plants grown in soil treated with sludge, which vary with respect to the concentrations of carbamazepine present in sludge and in the concentrations of sludge in the soil; taking into account only studies that used concentrations normally found, a 2014 review found that "the accumulation of carbamazepine into plants grown in soil amended with biosolids poses a "de minimis" risk to human health according to the approach."

Carbamazepine is available worldwide under many brand names.




</doc>
<doc id="6951" url="https://en.wikipedia.org/wiki?curid=6951" title="CCIR">
CCIR

CCIR is a four-letter abbreviation that may stand for:




</doc>
<doc id="6955" url="https://en.wikipedia.org/wiki?curid=6955" title="Chalcedonian Definition">
Chalcedonian Definition

The Chalcedonian Definition (also called the Chalcedonian Creed) was adopted at the Council of Chalcedon in AD 451. Chalcedon was an early centre of Christianity located in Asia Minor (modern Turkey). The council was the fourth of the Ecumenical Councils that are accepted by Chalcedonian churches which include the Eastern Orthodox, Catholic, and most Protestant churches. It was the first council not to be recognised by any Oriental Orthodox church; these churches may be classified as non-Chalcedonian.

The definition defines that Christ is "acknowledged in two natures", which "come together into one person and one hypostasis". The formal definition of "two natures" in Christ was understood by the critics of the council at the time, and is understood by many historians and theologians today, to side with western and Antiochene Christology and to diverge from the teaching of Cyril of Alexandria, who always stressed that Christ is "one". However, a modern analysis of the sources of the creed (by A. de Halleux, in Revue Theologique de Louvain 7, 1976) and a reading of the acts, or proceedings, of the council (recently translated into English) show that the bishops considered Cyril the great authority and that even the language of "two natures" derives from him.

Regarding the Person of Christ and the Hypostatic union, Chalcedonian Creed affirmed the notion that Christ is "One Person", having "One Hypostasis". The christological notions of "One Person" (monoprosopic - having one "prosopon" / Greek term for "person") and "One Hypostasis" (monohypostatic - having one hypostasis) were stated explicitly, in order to emphasize Council's anti-Nestorian positions.

The Chalcedonian Definition was written amid controversy between the Western and Eastern churches over the meaning of the Incarnation (see Christology), the ecclesiastical influence of the emperor, and the supremacy of the Bishop of Rome. The Western church readily accepted the creed, but some Eastern churches did not.

It became standard orthodox doctrine. However the Coptic Church of Alexandria dissented, holding to Cyril of Alexandria's preferred formula for the oneness of Christ's nature in the incarnation of God the Word as "out of two natures". Cyril's language is not consistent and he may have countenanced the view that it is possible to contemplate in theory two natures after the incarnation, but the Church of Alexandria felt that the Definition should have stated that Christ be acknowledged "out of two natures" rather than "in two natures".

This miaphysite position, historically characterised by Chalcedonian followers as "monophysitism" though this is denied by the dissenters, formed the basis for the distinction from other churches of the Coptic Church of Egypt and Ethiopia and the "Jacobite" churches of Syria, and the Armenian Apostolic Church (see Oriental Orthodoxy).

In one of the translations into English, the key section runs:<br>
Following, then, the holy Fathers, we all unanimously teach that our Lord Jesus Christ is to us One and the same Son, the Self-same Perfect in Godhead, the Self-same Perfect in Manhood; truly God and truly Man; the Self-same of a rational soul and body; co-essential with the Father according to the Godhead, the Self-same co-essential with us according to the Manhood; like us in all things, sin apart; before the ages begotten of the Father as to the Godhead, but in the last days, the Self-same, for us and for our salvation (born) of Mary the Virgin Theotokos as to the Manhood; One and the Same Christ, Son, Lord, Only-begotten; acknowledged in Two Natures unconfusedly, unchangeably, indivisibly, inseparably; the difference of the Natures being in no way removed because of the Union, but rather the properties of each Nature being preserved, and (both) concurring into One Person and One Hypostasis; not as though He were parted or divided into Two Persons, but One and the Self-same Son and Only-begotten God, Word, Lord, Jesus Christ; even as from the beginning the prophets have taught concerning Him, and as the Lord Jesus Christ Himself hath taught us, and as the Symbol of the Fathers hath handed down to us.
The full text of the definition which reaffirms the decisions of the Council of Ephesus, the pre-eminence of the Creed of Nicaea (325) and the further definitions of the Council of Constantinople (381) can be found here It also affirms the authority of two of Cyril of Alexandria's letters and the Tome of Leo written against Eutyches and sent to Archbishop Flavian of Constantinople in 449.



</doc>
<doc id="6956" url="https://en.wikipedia.org/wiki?curid=6956" title="Conservation law">
Conservation law

In physics, a conservation law states that a particular measurable property of an isolated physical system does not change as the system evolves over time. Exact conservation laws include conservation of energy, conservation of linear momentum, conservation of angular momentum, and conservation of electric charge. There are also many approximate conservation laws, which apply to such quantities as mass, parity, lepton number, baryon number, strangeness, hypercharge, etc. These quantities are conserved in certain classes of physics processes, but not in all.

A local conservation law is usually expressed mathematically as a continuity equation, a partial differential equation which gives a relation between the amount of the quantity and the "transport" of that quantity. It states that the amount of the conserved quantity at a point or within a volume can only change by the amount of the quantity which flows in or out of the volume.

From Noether's theorem, each conservation law is associated with a symmetry in the underlying physics.

Conservation laws are fundamental to our understanding of the physical world, in that they describe which processes can or cannot occur in nature. For example, the conservation law of energy states that the total quantity of energy in an isolated system does not change, though it may change form. In general, the total quantity of the property governed by that law remains unchanged during physical processes. With respect to classical physics, conservation laws include conservation of energy, mass (or matter), linear momentum, angular momentum, and electric charge. With respect to particle physics, particles cannot be created or destroyed except in pairs, where one is ordinary and the other is an antiparticle. With respect to symmetries and invariance principles, three special conservation laws have been described, associated with inversion or reversal of space, time, and charge.

Conservation laws are considered to be fundamental laws of nature, with broad application in physics, as well as in other fields such as chemistry, biology, geology, and engineering.

Most conservation laws are exact, or absolute, in the sense that they apply to all possible processes. Some conservation laws are partial, in that they hold for some processes but not for others.

One particularly important result concerning conservation laws is Noether's theorem, which states that there is a one-to-one correspondence between each one of them and a differentiable symmetry of nature. For example, the conservation of energy follows from the time-invariance of physical systems, and the conservation of angular momentum arises from the fact that physical systems behave the same regardless of how they are oriented in space.

A partial listing of physical conservation equations due to symmetry that are said to be exact laws, or more precisely "have never been proven to be violated:"

There are also approximate conservation laws. These are approximately true in particular situations, such as low speeds, short time scales, or certain interactions.


The total amount of some conserved quantity in the universe could remain unchanged if an equal amount were to appear at one point "A" and simultaneously disappear from another separate point "B". For example, an amount of energy could appear on Earth without changing the total amount in the Universe if the same amount of energy were to disappear from a remote region of the Universe. This weak form of "global" conservation is really not a conservation law because it is not Lorentz invariant, so phenomena like the above do not occur in nature. Due to Special Relativity, if the appearance of the energy at "A" and disappearance of the energy at "B" are simultaneous in one inertial reference frame, they will not be simultaneous in other inertial reference frames moving with respect to the first. In a moving frame one will occur before the other; either the energy at "A" will appear "before" or "after" the energy at "B" disappears. In both cases, during the interval energy will not be conserved. 

A stronger form of conservation law requires that, for the amount of a conserved quantity at a point to change, there must be a flow, or "flux" of the quantity into or out of the point. For example, the amount of electric charge in a volume is never found to change without an electric current into or out of the volume that carries the difference in charge. Since it only involves continuous "local" changes, this stronger type of conservation law is Lorentz invariant; a quantity conserved in one reference frame is conserved in all moving reference frames. This is called a "local conservation" law. Local conservation also implies global conservation; that the total amount of the conserved quantity in the Universe remains constant. All of the conservation laws listed above are local conservation laws. A local conservation law is expressed mathematically by a "continuity equation", which states that the change in the quantity in a volume is equal to the total net "flux" of the quantity through the surface of the volume. The following sections discuss continuity equations in general.

In continuum mechanics, the most general form of an exact conservation law is given by a continuity equation. For example, conservation of electric charge "q" is

where ∇⋅ is the divergence operator, "ρ" is the density of "q" (amount per unit volume), j is the flux of "q" (amount crossing a unit area in unit time), and "t" is time.

If we assume that the motion u of the charge is a continuous function of position and time, then

In one space dimension this can be put into the form of a homogeneous first-order quasilinear hyperbolic equation:

where the dependent variable "y" is called the "density" of a "conserved quantity", and "A(y)" is called the "current jacobian", and the subscript notation for partial derivatives has been employed. The more general inhomogeneous case:

is not a conservation equation but the general kind of balance equation describing a dissipative system. The dependent variable "y" is called a "nonconserved quantity", and the inhomogeneous term "s(y,x,t)" is the-"source", or dissipation. For example, balance equations of this kind are the momentum and energy Navier-Stokes equations, or the entropy balance for a general isolated system.

In the one-dimensional space a conservation equation is a first-order quasilinear hyperbolic equation that can be put into the "advection" form:

where the dependent variable "y(x,t)" is called the density of the "conserved" (scalar) quantity (c.q.(d.) = conserved quantity (density)), and "a(y)" is called the current coefficient, usually corresponding to the partial derivative in the conserved quantity of a current density (c.d.) of the conserved quantity "j(y)":

In this case since the chain rule applies:

the conservation equation can be put into the current density form:

In a space with more than one dimension the former definition can be extended to an equation that can be put into the form:

where the "conserved quantity" is "y(r,t)", "formula_11" denotes the scalar product, "∇" is the nabla operator, here indicating a gradient, and "a(y)" is a vector of current coefficients, analogously corresponding to the divergence of a vector c.d. associated to the c.q. j(y):

This is the case for the continuity equation:

Here the conserved quantity is the mass, with density "ρ"(r,t) and current density "ρ"u, identical to the momentum density, while u(r,t) is the flow velocity.

In the general case a conservation equation can be also a system of this kind of equations (a vector equation) in the form:

where y is called the "conserved" (vector) quantity, ∇ y is its gradient, 0 is the zero vector, and A(y) is called the Jacobian of the current density. In fact as in the former scalar case, also in the vector case A(y) usually corresponding to the Jacobian of a current density matrix J(y):

and the conservation equation can be put into the form:

For example, this the case for Euler equations (fluid dynamics). In the simple incompressible case they are:

where:

It can be shown that the conserved (vector) quantity and the c.d. matrix for these equations are respectively:

where "formula_19" denotes the outer product.

Conservation equations can be also expressed in integral form: the advantage of the latter is substantially that it requires less smoothness of the solution, which paves the way to weak form, extending the class of admissible solutions to include discontinuous solutions. By integrating in any space-time domain the current density form in 1-D space:

and by using Green's theorem, the integral form is:

In a similar fashion, for the scalar multidimensional space, the integral form is:

where the line integration is performed along the boundary of the domain, in an anticlock-wise manner.

Moreover, by defining a test function "φ"(r,"t") continuously differentiable both in time and space with compact support, the weak form can be obtained pivoting on the initial condition. In 1-D space it is:

Note that in the weak form all the partial derivatives of the density and current density have been passed on to the test function, which with the former hypothesis is sufficiently smooth to admit these derivatives.






</doc>
<doc id="6959" url="https://en.wikipedia.org/wiki?curid=6959" title="Chord">
Chord

Chord may refer to:


Chord may also refer to:

The Chords may refer to:

Chords may refer to:



</doc>
<doc id="6960" url="https://en.wikipedia.org/wiki?curid=6960" title="Car Talk">
Car Talk

Car Talk was a Peabody Award-winning radio talk show broadcast weekly on NPR stations and elsewhere. Its subjects were automobiles and automotive repair, discussed often in a humorous way. It was hosted by brothers Tom and Ray Magliozzi, known also as "Click and Clack, the Tappet Brothers". The show was produced from 1977 to October 2012, when the Magliozzi brothers retired. Edited reruns (which are introduced as "The Best of Car Talk") continued to be available for weekly airing on NPR's national schedule up through September 30, 2017, though some NPR affiliates have continued to broadcast reruns. Past episodes are otherwise available in a podcast format.

"Car Talk" was presented in the form of a call-in radio show: listeners called in with questions related to motor vehicle maintenance and repair. Most of the advice sought was diagnostic, with callers describing symptoms and demonstrating sounds of an ailing vehicle while the Magliozzis made an attempt to identify the malfunction over the telephone and give advice on how to fix it. While the hosts peppered their call-in sessions with jokes directed at both the caller and at themselves, the Magliozzis were usually able to arrive at a diagnosis. However, when they were stumped, they attempted anyway with an answer they claimed was ""unencumbered by the thought process"", the official motto of the show.

Edited reruns are carried on XM Satellite Radio via both the Public Radio and NPR Now channels.

The "Car Talk" theme music was "Dawggy Mountain Breakdown" by bluegrass artist David Grisman.

Throughout the program, listeners were encouraged to dial the toll-free telephone number, 1-888-CAR-TALK (1-888-227-8255), which connected to a 24-hour answering service. Although the approximately 2,000 queries received each week were screened by the "Car Talk" staff, the questions were unknown to the Magliozzis in advance as "that would entail researching the right answer, which is what? ... Work."

The show originally consisted of two segments with a break in between but was changed to three segments. After the shift to the three-segment format, it became a running joke to refer to the last segment as "the third half" of the program.

The show opened with a short comedy segment, typically jokes sent in by listeners, followed by eight call-in sessions. The hosts ran a contest called the "Puzzler", in which a riddle, sometimes car-related, was presented. The answer to the previous week's "Puzzler" was given at the beginning of the "second half" of the show, and a new "Puzzler" was given at the start of the "third half". The hosts gave instructions to listeners to write answers addressed to "Puzzler Tower" on some non-existent or expensive object, such as a "$26 bill" or an advanced digital SLR camera. This gag initially started as suggestions that the answers be written "on the back of a $20 bill". A running gag concerned Tom's inability to remember the previous week's "Puzzler" without heavy prompting from Ray. For each puzzler, one correct answer was chosen at random, with the winner receiving a $26 gift certificate to the "Car Talk" store, referred to as the "Shameless Commerce Division". It was originally $25, but was increased for inflation after a few years. Originally, the winner received a specific item from the store, but it soon changed to a gift certificate to allow the winner to choose the item they wanted (though Tom often made an item suggestion).

A recurring feature was "Stump the Chumps," in which the hosts revisited a caller from a previous show to determine the accuracy and the effect, if any, of their advice. A similar feature began in May 2001, "Where Are They Now, Tommy?" It began with a comical musical theme with a sputtering, backfiring car engine and a horn as a backdrop. Tom then announced who the previous caller was, followed by a short replay of the essence of the previous call, preceded and followed by harp music often used in other audiovisual media to indicate recalling and returning from a dream. The hosts then greeted the previous caller, confirmed that they had not spoken since their previous appearance and asked them if there had been any influences on the answer they were about to relate, such as arcane bribes by the NPR staff. The repair story was then discussed, followed by a fanfare and applause if the Tappet Brothers' diagnosis was correct, or a wah-wah-wah music piece mixed with a car starter operated by a weak battery (an engine which wouldn't start) if the diagnosis was wrong. The hosts then thanked the caller for their return appearance.

The brothers also had an official Animal-Vehicle Biologist and Wildlife Guru named Kieran Lindsey. She answered questions like "How do I remove a snake from my car?" and offered advice on how those living in cities and suburbs could reconnect with wildlife.

In addition to at least one on-orbit call, the Brothers once received a call asking advice on winterizing an electric car. When they asked what kind of car, the caller stated it was a "kit car", a $400 million "kit car". It was a joke call from the Jet Propulsion Laboratory concerning the preparation of the Mars rover for the oncoming Martian winter. Click and Clack have also been featured in editorial cartoons, including one where a befuddled NASA engineer called them to ask how to fix the Space Shuttle.

Humor and wisecracking pervaded the program. Tom and Ray are known for their self-deprecating humor, often joking about the supposedly poor quality of their advice and the show in general. They also commented at the end of each show: "Well, it's happened again—you've wasted another perfectly good hour listening to "Car Talk"."

At some point in almost every show, usually when giving the address for the Puzzler answers or fan mail, Ray mentioned Cambridge, Massachusetts (where the show originated), at which point Tom reverently interjected with a tone of civic pride, "Our fair city". Ray invariably mocked "'Cambridge, MA', the United States Postal Service's two-letter abbreviation for 'Massachusetts"', by pronouncing the "MA" as a word.

Preceding each break in the show, one of the hosts led up to the network identification with a humorous take on a disgusted reaction of some usually famous person to hearing that identification. The full line went along the pattern of, for example, "And even though Roger Clemens stabs his radio with a syringe whenever he hears "us" say it, this is NPR: National Public Radio" (later just "... this is NPR").

At one point in the show, often after the break, Ray usually stated that: "Support for this show is provided by," followed by an absurd fundraiser.

The ending credits of the show started with thanks to the colorfully nicknamed actual staffers: producer Doug "the subway fugitive, not a slave to fashion, bongo boy frogman" Berman; "John 'Bugsy' Lawlor, just back from the ..." every week a different eating event with rhyming foodstuff names; David "Calves of Belleville" Greene; Catherine "Frau Blücher" Fenollosa, whose name caused a horse to neigh and gallop (an allusion to a running gag in the movie "Young Frankenstein"); and Carly "High Voltage" Nix, among others. Following the real staff was a lengthy list of pun-filled fictional staffers and sponsors such as statistician Marge Innovera ("margin of error"), customer care representative Haywood Jabuzoff ("Hey, would ya buzz off"), meteorologist Claudio Vernight ("cloudy overnight"), optometric firm C. F. Eye Care ("see if I care"), Russian chauffeur Pikup Andropov ("pick up and drop off"), Leo Tolstoy biographer Warren Peace ("War and Peace"), hygiene officer and chief of the Tokyo office Oteka Shawa ("oh take a shower"), Swedish snowboard instructor Soren Derkeister ("sore in the keister"), law firm Dewey, Cheetham & Howe ("Do we cheat 'em? And how!"), Greek tailor Euripides Eumenades ("You rip-a these, you mend-a these"), cloakroom attendant Mahatma Coate ("My hat, my coat"), and many, many others, usually concluding with Erasmus B. Dragon ("Her ass must be draggin'"), whose job title varied, but who was often said to be head of the show's working mothers' support group. They sometimes advised that "our chief counsel from the law firm of Dewey, Cheetham, & Howe is Hugh Louis Dewey, known to [group of people] in Harvard Square as Huey Louie Dewey." Huey, Louie, and Dewey were the juvenile nephews being raised by Donald Duck in Walt Disney's Comics and Stories. Guest accommodations were provided by The Horseshoe Road Inn ("the horse you rode in").

At the end of the show, Ray warned the audience, "Don't drive like my brother!" to which Tom replied, "And don't drive like "my" brother!" The original tag line was "Don't drive like a knucklehead!" There were variations such as, "Don't drive like my brother ..." "And don't drive like his brother!" and "Don't drive like my sister ..." "And don't drive like "my" sister!" The tagline was heard in the Pixar film "Cars", in which Tom and Ray voiced anthropomorphized vehicles (Rusty and Dusty Rust-eze, respectively a 1963 Dodge Dart V1.0 and 1963 Dodge A100 van, as Lightning McQueen's racing sponsors) with personalities similar to their own on-air personae. Tom notoriously once owned a "convertible, green with large areas of rust!" Dodge Dart, known jokingly on the program by the faux-elegant name "Dartre".

In 1977, radio station WBUR-FM in Boston scheduled a panel of local car mechanics to discuss car repairs on one of its programs, but only Tom Magliozzi showed up. He did so well that he was asked to return as a guest, and he invited his younger brother Ray (who was actually more of a car repair expert) to join him. The brothers were soon asked to host their own radio show on WBUR, which they continued to do every week. In 1986, NPR decided to distribute their show nationally.

In 1992, "Car Talk" won a Peabody Award, saying "Each week, master mechanics Tom and Ray Magliozzi provide useful information about preserving and protecting our cars. But the real core of this program is what it tells us about human mechanics ... The insight and laughter provided by Messrs. Magliozzi, in conjunction with their producer Doug Berman, provide a weekly mental tune-up for a vast and ever-growing public radio audience."

In May 2007, the program, which previously had been available digitally only as a paid subscription from Audible.com, became a free podcast distributed by NPR, after a two-month test period where only a "call of the week" was available via podcast.

As of 2012, it had 3.3 million listeners each week, on about 660 stations. On June 8, 2012, the brothers announced that they would no longer broadcast new episodes as of October. Executive producer Doug Berman said the best material from 25 years of past shows would be used to put together "repurposed" shows for NPR to broadcast. Berman estimated the archives contain enough for eight years' worth of material before anything would have to be repeated. Ray Magliozzi, however, would occasionally record new taglines and sponsor announcements that were aired at the end of the show.

The show was inducted into the National Radio Hall of Fame in 2014.

Ray Magliozzi hosted a special "Car Talk" memorial episode for his brother Tom after he died in November 2014. However, Ray continued to write their syndicated newspaper column, saying that his brother would want him to.

The "Best of Car Talk" episodes ended their weekly broadcast on NPR on September 30, 2017, although past episodes would continue availability online and via podcasts. 120 of the 400 stations intended to continue airing the show. NPR announced one option for the time slot would be their new news-talk program "It's Been A Minute".

The Magliozzis were long-time auto mechanics. Ray Magliozzi has a bachelor of science degree in humanities and science from MIT, while Tom had a bachelor of science degree in economics from MIT and an MBA and DBA from the Boston University School of Management.

The duo, usually led by Tom, were known for rants on the evils of the internal combustion engine, people who talk on mobile phones while driving, Peugeots, women named Donna who always seem to drive Chevrolet Camaros, lawyers, puns and other clever uses of the English language, people who choose to live in Alaska (or similar snowy, icy climates), and practically anything else, including themselves. They had a relaxed and humorous approach to cars, car repair, cup holders, pets, lawyers, car repair mechanics, SUVs, and almost everything else. They often cast a critical, jaundiced insider's eye toward the auto industry. Tom and Ray were committed to the values of defensive driving and environmentalism.

The Magliozzis operated a do-it-yourself garage together in the 1970s which became more of a conventional repair shop in the 1980s. Ray continued to have a hand in the day-to-day operations of the shop for years, while his brother Tom semi-retired, often joking on "Car Talk" about his distaste for doing "actual work". The show's offices were located near their shop at the corner of JFK Street and Brattle Street in Harvard Square, marked as "Dewey, Cheetham & Howe", the imaginary law firm to which they referred on-air. DC&H doubled as the business name of Tappet Brothers Associates, the corporation established to manage the business end of "Car Talk". Initially a joke, the company was incorporated after the show expanded from a single station to national syndication.

The two were commencement speakers at MIT in 1999.

Executive producer Doug Berman said in 2012, "The guys are culturally right up there with Mark Twain and the Marx Brothers. They will stand the test of time. People will still be enjoying them years from now. They're that good."

Tom Magliozzi died on November 3, 2014, at age 77, due to complications from Alzheimer's disease.

The show was the inspiration for the short-lived "The George Wendt Show", which briefly aired on CBS in the 1994-1995 season- as a mid-season replacement.

In July 2007, PBS announced that it had green-lit an animated adaptation of "Car Talk", to air on prime-time in 2008. The show, titled "Click and Clack's As the Wrench Turns" is based on the adventures of the fictional "Click and Clack" brothers' garage at "Car Talk Plaza". The ten episodes aired in July and August 2008.

"Car Talk: The Musical!!!" was written and directed by Wesley Savick, and composed by Michael Wartofsky. The adaptation was presented by Suffolk University, and opened on March 31, 2011, at the Modern Theatre in Boston, Massachusetts. The play was not officially endorsed by the Magliozzis, but they participated in the production, lending their voices to a central puppet character named "The Wizard of Cahs".



</doc>
<doc id="6962" url="https://en.wikipedia.org/wiki?curid=6962" title="Council of Chalcedon">
Council of Chalcedon

The Council of Chalcedon () was a church council held from October 8 to November 1, AD 451, at Chalcedon. The council is numbered as the fourth ecumenical council by the Catholic Church, the Eastern Orthodox Church, and most Protestants. A minority of Christians, subsequently known as Oriental Orthodoxy, do not agree with the council's teachings.

Its most important achievement was to issue the Chalcedonian Definition, stating that Jesus is "perfect both in deity and in humanness; this selfsame one is also actually God and actually man." The council's judgments and definitions regarding the divine marked a significant turning point in the Christological debates.

Chalcedon was a city in Bithynia, on the Asian side of the Bosphorus; today the city is part of the Republic of Turkey and is known as Kadıköy (a district of Istanbul).

The Council of Chalcedon was convened by Emperor Marcian, with the reluctant approval of Pope Leo the Great, to set aside the 449 Second Council of Ephesus which would become known as the "Latrocinium" or "Robber Council". The Council of Chalcedon issued the Chalcedonian Definition, which repudiated the notion of a single nature in Christ, and declared that he has two natures in one person and hypostasis. It also insisted on the completeness of his two natures: Godhead and manhood. The council also issued 27 disciplinary canons governing church administration and authority. In a further decree, later known as canon 28, the bishops declared that the See of Constantinople (New Rome) had the same patriarchal status as the See of Rome.

The dogmatic definitions of the council are recognized as infallible by the Eastern Orthodox and Catholic Churches, as well by certain other Western Churches; also, most Protestants agree that the council's teachings regarding the Trinity and the Incarnation are orthodox doctrine which must be adhered to. The council, however, is rejected by the Assyrian Church of the East and the Oriental Orthodox Churches, the latter teaching rather that "The Lord Jesus Christ is God the Incarnate Word. He possesses the perfect Godhead and the perfect manhood. His fully divine nature is united with His fully human nature yet without mixing, blending or alteration." The Oriental Orthodox contend that this latter teaching has been misunderstood as monophysitism, an appellation with which they strongly disagree but, nevertheless, refuse to accept the decrees of the council.

Many Anglicans and most Protestants consider it to be the last authoritative ecumenical council. These churches, along with Martin Luther, hold that both conscience and scripture preempt doctrinal councils and generally agree that the conclusions of later councils were unsupported by or contradictory to scripture.

In 325, the first ecumenical council (First Council of Nicaea) determined that Jesus Christ was God, "consubstantial" with the Father, and rejected the Arian contention that Jesus was a created being. This was reaffirmed at the First Council of Constantinople (381) and the Council of Ephesus (431).

After the Council of Ephesus had condemned Nestorianism, there remained a conflict between Patriarchs John of Antioch and Cyril of Alexandria. Cyril claimed that John remained Nestorian in outlook, while John claimed that Cyril held to the Apollinarian heresy. The two settled their differences under the mediation of the Bishop of Beroea, Acacius, on April 12, 433. In the following year, Theodoret of Cyrrhus assented to this formula as well. He agreed to anathematize Nestorius as a heretic in 451, during the Council of Chalcedon, as the price to be paid for being restored to his see (after deposition at the Council of Ephesus of 449). This put a final end to Nestorianism within the Roman Empire.

About two years after Cyril of Alexandria's death in 444, an aged monk from Constantinople named Eutyches began teaching a subtle variation on the traditional Christology in an attempt (as he described in a letter to Pope Leo I in 448) to stop a new outbreak of Nestorianism. He claimed to be a faithful follower of Cyril's teaching, which was declared orthodox in the Union of 433.

Cyril had taught that "There is only one "physis", since it is the Incarnation, of God the Word." Cyril had apparently understood the Greek word "physis" to mean approximately what the Latin word "persona" (person) means, while most Greek theologians would have interpreted that word to mean "natura" (nature). Thus, many understood Eutyches to be advocating Docetism, a sort of reversal of Arianism—where Arius had denied the consubstantial divinity of Jesus, Eutyches seemed to be denying his human nature. Cyril's orthodoxy was not called into question, since the Union of 433 had explicitly spoken of two "physeis" in this context. 

Leo I wrote that Eutyches' error seemed to be more from a lack of skill on the matters than from malice. Further, his side of the controversy tended not to enter into arguments with their opponents, which prevented the misunderstanding from being uncovered. Nonetheless, due to the high regard in which Eutyches was held (second only to the Patriarch of Constantinople in the East), his teaching spread rapidly throughout the East. 

In November 448, during a local synod in Constantinople, Eutyches was denounced as a heretic by the Bishop Eusebius of Dorylaeum. Eusebius demanded that Eutyches be removed from office. Patriarch Flavian of Constantinople preferred not to press the matter on account of Eutyches' great popularity. He finally relented and Eutyches was condemned as a heretic by the synod. However, the Emperor Theodosius II and Pope Dioscorus I of Alexandria, rejected this decision ostensibly because Eutyches had repented and confessed his orthodoxy. Dioscorus then held his own synod which reinstated Eutyches. The competing claims between the Patriarchs of Constantinople and Alexandria led the Emperor to call a council which was held in Ephesus in 449. The emperor invited Pope Leo I to preside. He declined to attend on account of the invasion of Italy by Attila the Hun. However, he agreed to send four legates to represent him. Leo provided his legates, one of whom died en route, with a letter addressed to Flavian of Constantinople explaining Rome's position in the controversy. Leo's letter, now known as Leo's Tome, confessed that Christ had two natures, and was not of or from two natures. Although it could be reconciled with Cyril's Formula of Reunion, it was not compatible in its wording with Cyril's Twelve Anathemas. In particular, the third anathema reads: "If anyone divides in the one Christ the hypostases after the union, joining them only by a conjunction of dignity or authority or power, and not rather by a coming together in a union by nature, let him be anathema." This appeared to some to be incompatible with Leo's definition of two natures hypostatically joined. However, the council would determine (with the exception of 13 Egyptian bishops) that this was an issue of wording and not of doctrine; a committee of bishops appointed to study the orthodoxy of the Tome using Cyril's letters (which included the twelve anathemas) as their criteria unanimously determined it to be orthodox, and the council, with few exceptions, supported this.

On August 8, 449 the Second Council of Ephesus began its first session with Pope Dioscorus of Alexandria presiding by command of the Emperor. Dioscorus began the council by banning all members of the November 447 synod which had deposed Eutyches. He then introduced Eutyches who publicly professed that while Christ had two natures before the incarnation, the two natures had merged to form a single nature after the incarnation. Of the 130 assembled bishops, 111 voted to rehabilitate Eutyches. Throughout these proceedings, Hilary (one of the papal legates) repeatedly called for the reading of Leo's Tome, but was ignored. Dioscorus then moved to depose Flavian and Eusebius of Dorylaeum on the grounds that they taught the Word had been made flesh and not just assumed flesh from the Virgin and that Christ had two natures. When Flavian and Hilary objected, Dioscorus called for a pro-monophysite mob to enter the church and assault Flavian as he clung to the altar. Flavian was mortally wounded. Dioscorus then placed Eusebius of Dorylaeum under arrest and demanded the assembled bishops approve his actions. Fearing the mob, they all did. The papal legates refused to attend the second session at which several more orthodox bishops were deposed, including Ibas of Edessa, Irenaeus of Tyre (a close personal friend of Nestorius), Domnus of Antioch, and Theodoret. Dioscorus then pressed his advantage by having Cyril of Alexandria's Twelve Anathemas posthumously declared orthodox with the intent of condemning any confession other than one nature in Christ. Hilary, who later became pope and dedicated an oratory in the Lateran Basilica in thanks for his life, managed to escape from Constantinople and brought news of the council to Leo who immediately dubbed it a "synod of robbers"—Latrocinium—and refused to accept its pronouncements. The decisions of this council now threatened schism between the East and the West.

The situation continued to deteriorate, with Leo demanding the convocation of a new council and Emperor Theodosius II refusing to budge, all the while appointing bishops in agreement with Dioscorus. All this changed dramatically with the Emperor's death and the elevation of Marcian, an orthodox Christian, to the imperial throne. To resolve the simmering tensions, Marcian announced his intention to hold a new council. Leo had pressed for it to take place in Italy, but Emperor Marcian instead called for it to convene at Nicaea. Hunnish invasions forced it to move at the last moment to Chalcedon, where the council opened on October 8, 451. Marcian had the bishops deposed by Dioscorus returned to their dioceses and had the body of Flavian brought to the capital to be buried honorably.

The Emperor asked Leo to preside over the council, but Leo again chose to send legates in his place. This time, Bishops Paschasinus of Lilybaeum and Julian of Cos and two priests Boniface and Basil represented the western church at the council. The Council of Chalcedon condemned the work of the Robber Council and professed the doctrine of the Incarnation presented in Leo's Tome. The council was attended by about 520 bishops or their representatives and was the largest and best-documented of the first seven ecumenical councils. Paschasinus refused to give Dioscorus (who had excommunicated Leo leading up to the council) a seat at the council. As a result, he was moved to the nave of the church. Paschasinus further ordered the reinstatement of Theodoret and that he be given a seat, but this move caused such an uproar among the council fathers, that Theodoret also sat in the nave, though he was given a vote in the proceedings, which began with a trial of Dioscorus.

Marcian wished to bring proceedings to a speedy end, and asked the council to make a pronouncement on the doctrine of the Incarnation before continuing the trial. The council fathers, however, felt that no new creed was necessary, and that the doctrine had been laid out clearly in Leo's Tome. They were also hesitant to write a new creed as the Council of Ephesus had forbidden the composition or use of any new creed. The second session of the council ended with shouts from the bishops, "It is Peter who says this through Leo. This is what we all of us believe. This is the faith of the Apostles. Leo and Cyril teach the same thing." However, during the reading of Leo's Tome, three passages were challenged as being potentially Nestorian, and their orthodoxy was defended by using the writings of Cyril. Nonetheless due to such concerns, the council decided to adjourn and appoint a special committee to investigate the orthodoxy of Leo's Tome, judging it by the standard of Cyril's Twelve Chapters, as some of the bishops present raised concerns about their compatibility. This committee was headed by Anatolius, Patriarch of Constantinople, and was given five days to carefully study the matter; Cyril's Twelve Chapters were to be used as the orthodox standard. The committee unanimously decided in favor of the orthodoxy of Leo, determining that what he said was compatible with the teaching of Cyril. A number of other bishops also entered statements to the effect that they believed that Leo's Tome was not in contradiction with the teaching of Cyril as well.

The council continued with Dioscorus' trial, but he refused to appear before the assembly. As a result, he was condemned, but by an underwhelming amount (more than half the bishops present for the previous sessions did not attend his condemnation), and all of his decrees were declared null. Marcian responded by exiling Dioscorus. All of the bishops were then asked to sign their assent to the Tome, but a group of thirteen Egyptians refused, saying that they would assent to "the traditional faith". As a result, the Emperor's commissioners decided that a "credo" would indeed be necessary and presented a text to the fathers. No consensus was reached, and indeed the text has not survived to the present. Paschasinus threatened to return to Rome to reassemble the council in Italy. Marcian agreed, saying that if a clause were not added to the "credo" supporting Leo's doctrine , the bishops would have to relocate. The bishops relented and added a clause, saying that, according to the decision of Leo, in Christ there are two natures united, inconvertible, inseparable.

The Confession of Chalcedon provides a clear statement on the human and divine nature of Christ:

The full text of the definition which reaffirms the decisions of the Council of Ephesus, the pre-eminence of the Creed of Nicea (325) and the further definitions of the Council of Constantinople (381) can be found here It also canonises as authoritative two of Cyril of Alexandria's letters and the Tome of Leo written against Eutyches and sent to Archbishop Flavian of Constantinople in 449.

The work of the council was completed by a series of 30 disciplinary canons the Ancient Epitomes of which are:

While Canon 28 was in fact a resolution passed by the council at the 16th session to grant equal privileges ("") to Constantinople as of Rome because Constantinople was considered the New Rome it was rejected by the papal legates Paschasinus bishop of Lilybaeum, Bishop Lucentius, the priests Boniface and Basil, and Bishop Julian of Cos. Pope Leo confirmed all the canons with regards to doctrines of faith however; Canon 28, while retained in the Acts, was declared null and void since it violated the prerogatives of the Patriarchs of Alexandria, Antioch, and Jerusalem, and was contrary to Canons 6 and 7 of the Council of Nicaea. 

According to some ancient Greek collections, canons 29 and 30 are attributed to the council: canon 29, which states that an unworthy bishop cannot be demoted but can be removed, is an extract from the minutes of the 19th session; canon 30, which grants the Egyptians time to consider their rejection of Leo's "Tome", is an extract from the minutes of the fourth session.

In all likelihood an official record of the proceedings was made either during the council itself or shortly afterwards. The assembled bishops informed the pope that a copy of all the "Acta" would be transmitted to him; in March, 453, Pope Leo commissioned Julian of Cos, then at Constantinople, to make a collection of all the Acts and translate them into Latin. Most of the documents, chiefly the minutes of the sessions, were written in Greek; others, e.g. the imperial letters, were issued in both languages; others, again, e.g. the papal letters, were written in Latin. Eventually nearly all of them were translated into both languages.

The metropolitan of Jerusalem was given independence from the metropolitan of Antioch and from any other higher-ranking bishop, given what is now known as autocephaly, in the council's seventh session whose "Decree on the Jurisdiction of Jerusalem and Antioch" contains: "the bishop of Jerusalem, or rather the most holy Church which is under him, shall have under his own power the three Palestines". This led to Jerusalem becoming a patriarchate, one of the five patriarchates known as the pentarchy, when the title of "patriarch" was created in 531 by Justinian. The Oxford Dictionary of the Christian Church, s.v. "patriarch (ecclesiastical)", also calls it "a title dating from the 6th century, for the bishops of the five great sees of Christendom". Merriam-Webster's Encyclopedia of World Religions, says: "Five patriarchates, collectively called the pentarchy, were the first to be recognized by the legislation of the emperor Justinian (reigned 527–565)".

In a canon of disputed validity, the Council of Chalcedon also elevated the See of Constantinople to a position "second in eminence and power to the Bishop of Rome".

The Council of Nicaea in 325 had noted the primacy of the See of Rome, followed by the Sees of Alexandria and Antioch. At the time, the See of Constantinople was not yet of ecclesiastical prominence, but its proximity to the Imperial court gave rise to its importance. The Council of Constantinople in 381 modified the situation somewhat by placing Constantinople second in honor, above Alexandria and Antioch, stating in Canon III, that "the bishop of Constantinople... shall have the prerogative of honor after the bishop of Rome; because Constantinople is New Rome". In the early 5th century, this status was challenged by the bishops of Alexandria, but the Council of Chalcedon confirmed in Canon XXVIII:
In making their case, the council fathers argued that tradition had accorded "honor" to the see of older Rome because it was the first imperial city. Accordingly, "moved by the same purposes" the fathers "apportioned equal prerogatives to the most holy see of new Rome" because "the city which is honored by the imperial power and senate and enjoying privileges equaling older imperial Rome should also be elevated to her level in ecclesiastical affairs and take second place after her". The framework for allocating ecclesiastical authority advocated by the council fathers mirrored the allocation of imperial authority in the later period of the Roman Empire. The Eastern position could be characterized as being political in nature, as opposed to a doctrinal view. In practice, all Christians East and West addressed the papacy as the See of Peter and Paul or the Apostolic See rather than the See of the Imperial Capital. Rome understands this to indicate that its precedence has always come from its direct lineage from the apostles Peter and Paul rather than its association with Imperial authority.

After the passage of the Canon 28, Rome filed a protest against the reduction of honor given to Antioch and Alexandria. However, fearing that withholding Rome's approval would be interpreted as a rejection of the entire council, in 453 the pope confirmed the council's canons with a protest against the 28th.

The near-immediate result of the council was a major schism. The bishops that were uneasy with the language of Pope Leo's Tome repudiated the council, saying that the acceptance of two "physes" was tantamount to Nestorianism. Pope Dioscorus of Alexandria advocated miaphysitism and had dominated the Council of Ephesus. Churches that rejected Chalcedon in favor of Ephesus broke off from the rest of the Eastern Church in a schism, the most significant among these being the Church of Alexandria, today known as the Coptic Orthodox Church of Alexandria.

Justinian I attempted to bring those monks who still rejected the decision of the Council of Chalcedon into communion with the greater church. The exact time of this event is unknown, but it is believed to have been between 535 and 548. St Abraham of Farshut was summoned to Constantinople and he chose to bring with him four monks. Upon arrival, Justinian summoned them and informed them that they would either accept the decision of the council or lose their positions. Abraham refused to entertain the idea. Theodora tried to persuade Justinian to change his mind, seemingly to no avail. Abraham himself stated in a letter to his monks that he preferred to remain in exile rather than subscribe to a faith contrary to that of Athanasius. They were not alone, and the non-Chalcedon churches compose Oriental Orthodoxy, with the Church of Alexandria as their primus inter pares. Only in recent years has a degree of rapprochement between Chalcedonian Christians and the Oriental Orthodox been seen.

The Eastern Orthodox Church commemorates the "Holy Fathers of the 4th Ecumenical Council, who assembled in Chalcedon" on the Sunday on or after July 13;

For both of the above complete propers have been composed and are found in the Menaion.

For the former "The Office of the 630 Holy and God-bearing Fathers of the 4th ... Summoned against the Monophysites Eftyches and Dioskoros ..." was composed in the middle of the 14th century by Patriarch Philotheus I of Constantinople. This contains numerous hymns exposing the council's teaching, commemorating its leaders whom it praises and whose prayers it implores, and naming its opponents pejoratively. "e.g.", "Come let us clearly reject the errors of ... but praise in divine songs the fourth council of pious fathers."

For the latter the propers are titled "We Commemorate Six Holy Ecumenical Councils". This repeatedly damns those anathematized by the councils with such rhetoric as "Christ-smashing deception enslaved Nestorius" and "mindless Arius and ... is tormented in the fires of Gehenna ..." while the fathers of the councils are praised and the dogmas of the councils are expounded in the hymns therein.





</doc>
<doc id="6963" url="https://en.wikipedia.org/wiki?curid=6963" title="Canadian football">
Canadian football

Canadian football () is a sport played in Canada in which two teams of 12 players each compete for territorial control of a field of play long and wide attempting to advance a pointed prolate spheroid ball into the opposing team's scoring area (end zone).

In Canada, the term "football" may refer to Canadian football and American football collectively, or to either sport specifically, depending on context. The two sports have shared origins and are closely related but have some key differences.

Rugby football in Canada originated in the early 1860s, and over time, the game known as Canadian football developed. Both the Canadian Football League (CFL), the sport's top professional league, and Football Canada, the governing body for amateur play, trace their roots to 1880 and the founding of the Canadian Rugby Football Union. 

The CFL is the most popular and only major professional Canadian football league. Its championship game, the Grey Cup, is one of Canada's largest sporting events, attracting a broad television audience. In 2009, about 40% of Canada's population watched part of the game; in 2014, it was closer to 33%, peaking at 5.1 million viewers in the fourth quarter.

Canadian football is also played at the bantam, high school, junior, collegiate, and semi-professional levels: the Canadian Junior Football League, formed May 8, 1974, and Quebec Junior Football League are leagues for players aged 18–22, many post-secondary institutions compete in U Sports football for the Vanier Cup, and senior leagues such as the Alberta Football League have grown in popularity in recent years. Great achievements in Canadian football are enshrined in the Canadian Football Hall of Fame located in Hamilton, Ontario.

Other organizations across Canada perform senior league Canadian football during the summer.

The first documented football match was a practice game played on November 9, 1861, at University College, University of Toronto (approximately west of Queen's Park). One of the participants in the game involving University of Toronto students was Sir William Mulock, later Chancellor of the school. A football club was formed at the university soon afterward, although its rules of play at this stage are unclear.

The first written account of a game played was on October 15, 1862, on the Montreal Cricket Grounds. It was between the First Battalion Grenadier Guards and the Second Battalion Scots Fusilier Guards resulting in a win by the Grenadier Guards 3 goals, 2 rouges to nothing. In 1864, at Trinity College, Toronto, F. Barlow Cumberland, Frederick A. Bethune, and Christopher Gwynn, one of the founders of Milton, Massachusetts, devised rules based on rugby football. The game gradually gained a following, with the Hamilton Football Club formed on November 3, 1869, Montreal formed a team April 8, 1872, Toronto was formed on October 4, 1873, and the Ottawa FBC on September 20, 1876. (Of those clubs, only the Toronto club is still in continuous operation today.)

This rugby-football soon became popular at Montreal's McGill University. McGill challenged Harvard University to a game, in 1874 using a hybrid game of English rugby devised by the University of McGill.

The first attempt to establish a proper governing body and adopted the current set of Rugby rules was the Foot Ball Association of Canada, organized on March 24, 1873 followed by the Canadian Rugby Football Union (CRFU) founded June 12, 1880, which included teams from Ontario and Quebec. Later both the Ontario and Quebec Rugby Football Union (ORFU and QRFU) were formed (January 1883), and then the Interprovincial (1907) and Western Interprovincial Football Union (1936) (IRFU and WIFU). The CRFU reorganized into an umbrella organization forming the Canadian Rugby Union (CRU) in 1891. The original forerunners to the current Canadian Football League, was established in 1956 when the IRFU and WIFU formed an umbrella organization, The Canadian Football Council (CFC). In 1958 the CFC left the CRFU to become the CFL.

The Burnside rules closely resembling American football (which are similar rules developed by Walter Camp for that sport) that were incorporated in 1903 by the ORFU, was an effort to distinguish it from a more rugby-oriented game. The Burnside Rules had teams reduced to 12 men per side, introduced the Snap-Back system, required the offensive team to gain 10 yards on three downs, eliminated the Throw-In from the sidelines, allowed only six men on the line, stated that all goals by kicking were to be worth two points and the opposition was to line up 10 yards from the defenders on all kicks. The rules were an attempt to standardize the rules throughout the country. The CIRFU, QRFU and CRU refused to adopt the new rules at first. Forward passes were not allowed in the Canadian game until 1929, and touchdowns, which had been five points, were increased to six points in 1956, in both cases several decades after the Americans had adopted the same changes. The primary differences between the Canadian and American games stem from rule changes that the American side of the border adopted but the Canadian side did not (originally, both sides had three downs, goal posts on the goal lines and unlimited forward motion, but the American side modified these rules and the Canadians did not). The Canadian field width was one rule that was not based on American rules, as the Canadian game was played in wider fields and stadiums that were not as narrow as the American stadiums.

The Grey Cup was established in 1909 after being donated by Albert Grey, 4th Earl Grey, The Governor General of Canada as the championship of teams under the CRU for the Rugby Football Championship of Canada. Initially an amateur competition, it eventually became dominated by professional teams in the 1940s and early 1950s. The Ontario Rugby Football Union, the last amateur organization to compete for the trophy, withdrew from competition in 1954. The move ushered in the modern era of Canadian professional football, culminating in the formation of the present-day Canadian Football League in 1958.

Canadian football has mostly been confined to Canada, with the United States being the only other country to have hosted high-level Canadian football games. The CFL's controversial "South Division" as it would come to be officially known attempted to put CFL teams in the United States playing under Canadian rules between 1992 and 1995. The move was aborted after three years; the Baltimore Stallions were the most successful of the numerous Americans teams to play in the CFL, winning the 83rd Grey Cup. Continuing financial losses, a lack of proper Canadian football venues, a pervasive belief that the American teams were simply pawns to provide the struggling Canadian teams with expansion fee revenue, and the return of the NFL to Baltimore prompted the end of Canadian football on the American side of the border.

The CFL hosted the Touchdown Atlantic regular season game at Nova Scotia in 2005 and New Brunswick in 2010, 2011 and 2013. In 2013, Newfoundland and Labrador became the last province to establish football at the minor league level, with teams playing on the Avalon Peninsula and in Labrador City. The province however has yet to host a college or CFL game. Prince Edward Island, the smallest of the provinces, has also never hosted a CFL game.

Canadian football is played at several levels in Canada; the top league is the professional nine-team Canadian Football League (CFL). The CFL regular season begins in June, and playoffs for the Grey Cup are completed by mid-November. In cities with outdoor stadiums such as Edmonton, Winnipeg, Calgary, and Regina, low temperatures and icy field conditions can seriously affect the outcome of a game.

Amateur football is governed by Football Canada. At the university level, 26 teams play in four conferences under the auspices of U Sports (known from 2001–2016 as Canadian Interuniversity Sport); the U Sports champion is awarded the Vanier Cup. Junior football is played by many after high school before joining the university ranks. There are 20 junior teams in three divisions in the Canadian Junior Football League competing for the Canadian Bowl. The Quebec Junior Football League includes teams from Ontario and Quebec who battle for the Manson Cup.

Semi-professional leagues have grown in popularity in recent years, with the Alberta Football League becoming especially popular. The Northern Football Conference formed in Ontario in 1954 has also surged in popularity for former college players who do not continue to professional football. The Ontario champion plays against the Alberta champion for the "National Championship". The Canadian Major Football League is the governing body for the semi-professional game.

Women's football has gained attention in recent years in Canada. The first Canadian women's league to begin operations was the Maritime Women's Football League in 2004. The largest women's league is the Western Women's Canadian Football League.

The Canadian football field is long and wide, within which the goal areas are deep, and the goal lines are apart. Including the endzones, the total area of the field is .

At each goal line is a set of goalposts, which consist of two "uprights" joined by an crossbar which is above the goal line. The goalposts may be H-shaped (both posts fixed in the ground) although in the higher-calibre competitions the tuning-fork design (supported by a single curved post behind the goal line, so that each post starts above the ground) is preferred.

The sides of the field are marked by white sidelines, the goal line is marked in white, and white lines are drawn laterally across the field every from the goal line. These lateral lines are called "yard lines" and often marked with the distance in yards from and an arrow pointed toward the nearest goal line. In previous decades, arrows were not used and every yard line (in both multiples of 5 and 10) was usually marked with the distance to the goal line, including the goal line itself which was marked with either a "0" or "00"; in most stadiums today, only the yard markers in multiples of 10 are marked with numbers, with the goal line sometimes being marked with a "G". The centre (55-yard) line usually is marked with a "C" (or, more rarely, with a "55"). "Hash marks" are painted in white, parallel to the yardage lines, at intervals, from the sidelines.

On fields that have a surrounding running track, such as Molson Stadium and many universities, the end zones are often cut off in the corners to accommodate the track. Until 1986, the end zones were deep, giving the field an overall length of , and a correspondingly larger cutoff could be required at the corners. The first field to feature the shorter 20-yard endzones was Vancouver's BC Place (home of the BC Lions), which opened in 1983. This was particularly common among U.S.-based teams during the CFL's American expansion, where few American stadiums were able to accommodate the much longer and noticeably wider CFL field. The end zones in Toronto's BMO Field are only 18 yards instead of 20 yards.

Teams advance across the field through the execution of quick, distinct plays, which involve the possession of a brown, prolate spheroid ball with ends tapered to a point. The ball has two one-inch-wide white stripes.

At the beginning of a match, an official tosses a coin and allows the captain of the visiting team call heads or tails. The captain of the team winning the coin toss is given the option of having first choice, or of deferring first choice to the other captain. The captain making first choice may either choose a) to kick off or receive the kick at the beginning of the half, or b) which direction of the field to play in. The remaining choice is given to the opposing captain. Before the resumption of play in the second half, the captain that did not have first choice in the first half is given first choice. Teams usually choose to defer, so it is typical for the team that wins the coin toss to kick to begin the first half and receive to begin the second.

Play begins at the start of each half with one team place-kicking the ball from its own 35-yard line. Both teams then attempt to catch the ball. The player who recovers the ball may run while holding the ball, or lateral throw the ball to a teammate.

Play stops when the ball carrier's knee, elbow, or any other body part aside from the feet and hands, is forced to the ground (a "tackle"); when a forward pass is not caught on the fly (during a scrimmage); when a touchdown (see below) or a field goal is scored; when the ball leaves the playing area by any means (being carried, thrown, or fumbled out of bounds); or when the ball carrier is in a standing position but can no longer move forwards (called forward progress). If no score has been made, the next play starts from "scrimmage".

Before scrimmage, an official places the ball at the spot it was at the stop of clock, but no nearer than 24 yards from the sideline or 1 yard from the goal line. The line parallel to the goal line passing through the ball (line from sideline to sideline for the length of the ball) is referred to as the line of scrimmage. This line is similar to "no-man's land"; players must stay on their respective sides of this line until the play has begun again. For a scrimmage to be valid the team in possession of the football must have seven players, excluding the quarterback, within one yard of the line of scrimmage. The defending team must stay a yard or more back from the line of scrimmage.
On the field at the beginning of a play are two teams of 12 (unlike 11 in American football). The team in possession of the ball is the offence and the team defending is referred to as the defence. Play begins with a backwards pass through the legs (the snap) by a member of the offensive team, to another member of the offensive team. This is usually the quarterback or punter, but a "direct snap" to a running back is also not uncommon. If the quarterback or punter receives the ball, he may then do any of the following:

Each play constitutes a "down". The offence must advance the ball at least ten yards towards the opponents' goal line within three downs or forfeit the ball to their opponents. Once ten yards have been gained the offence gains a new set of three downs (rather than the four downs given in American football). Downs do not accumulate. If the offensive team completes 10 yards on their first play, they lose the other two downs and are granted another set of three. If a team fails to gain ten yards in two downs they usually punt the ball on third down or try to kick a field goal (see below), depending on their position on the field. The team may, however use its third down in an attempt to advance the ball and gain a cumulative 10 yards.

The ball changes possession in the following instances:

There are many rules to contact in this type of football. First, the only player on the field who may be legally tackled is the player currently in possession of the football (the ball carrier). Second, a receiver, that is to say, an offensive player sent down the field to receive a pass, may not be interfered with (have his motion impeded, be blocked, etc.) unless he is within one yard of the line of scrimmage (instead of in American football). Any player may block another player's passage, so long as he does not hold or trip the player he intends to block. The kicker may not be contacted after the kick but before his kicking leg returns to the ground (this rule is not enforced upon a player who has blocked a kick), and the quarterback, having already thrown the ball, may not be hit or tackled.

Infractions of the rules are punished with "penalties", typically a loss of yardage of 5, 10 or 15 yards against the penalized team. Minor violations such as "offside" (a player from either side encroaching into scrimmage zone before the play starts) are penalized five yards, more serious penalties (such as holding) are penalized 10 yards, and severe violations of the rules (such as face-masking) are typically penalized 15 yards. Depending on the penalty, the penalty yardage may be assessed from the original line of scrimmage, from where the violation occurred (for example, for a pass interference infraction), or from where the ball ended after the play. Penalties on the offence may, or may not, result in a loss of down; penalties on the defence may result in a first down being automatically awarded to the offence. For particularly severe conduct, the game official(s) may eject players (ejected players may be substituted for), or in exceptional cases, declare the game over and award victory to one side or the other. Penalties do not affect the yard line which the offence must reach to gain a first down (unless the penalty results in a first down being awarded); if a penalty against the defence results in the first down yardage being attained, then the offence is awarded a first down.

If the defence is penalized on a two-point convert attempt and the offence chooses to attempt the play again, the offence must attempt another two-point convert; it cannot change to a one-point attempt. Conversely, the offence can attempt a two-point convert following a defensive penalty on a one-point attempt.

Penalties may occur before a play starts (such as offside), during the play (such as holding), or in a dead-ball situation (such as unsportsmanlike conduct).

Penalties never result in a score for the offence. For example, a point-of-foul infraction committed by the defence in their end zone is not ruled a touchdown, but instead advances the ball to the one-yard line with an automatic first down. For a distance penalty, if the yardage is greater than half the distance to the goal line, then the ball is advanced half the distance to the goal line, though only up to the one-yard line (unlike American football, in Canadian football no scrimmage may start inside either one-yard line). If the original penalty yardage would have resulted in a first down or moving the ball past the goal line, a first down is awarded.

In most cases, the non-penalized team will have the option of "declining" the penalty; in which case the results of the previous play stand as if the penalty had not been called. One notable exception to this rule is if the kicking team on a 3rd down punt play is penalized before the kick occurs: the receiving team may not decline the penalty and take over on downs. After the kick is made, change of possession occurs and subsequent penalties are assessed against either the spot where the ball is caught, or the runback.

Canadian football distinguishes four ways of kicking the ball:


On any kicking play, all onside players (the kicker, and teammates behind the kicker at the time of the kick) may recover and advance the ball. Players on the kicking team who are not onside may not approach within five yards of the ball until it has been touched by the receiving team, or by an onside teammate.

The methods of scoring are:


Resumption of play following a score is conducted under procedures which vary with the type of score.

The game consists of two 30-minute halves, each of which is divided into two 15-minute quarters. The clock counts down from 15:00 in each quarter. Timing rules change when there are three minutes remaining in a half.
A short break interval of 2 minutes occurs after the end of each quarter (a longer break of 15 minutes at halftime), and the two teams then change goals.

In the first 27 minutes of a half, the clock stops when:

The clock starts again when the referee determines the ball is ready for scrimmage, except for team time-outs (where the clock starts at the snap), after a time count foul (at the snap) and kickoffs (where the clock starts not at the kick but when the ball is first touched after the kick).

In the last three minutes of a half, the clock stops whenever the ball becomes dead. On kickoffs, the clock starts when the ball is first touched after the kick. On scrimmages, when it starts depends on what ended the previous play. The clock starts when the ball is ready for scrimmage except that it starts on the snap when on the previous play

During the last three minutes of a half, the penalty for failure to place the ball in play within the 20-second play clock, known as "time count" (this foul is known as "delay of game" in American football), is dramatically different from during the first 27 minutes. Instead of the penalty being 5 yards with the down repeated, the base penalty (except during convert attempts) becomes loss of down on first or second down, and 10 yards on third down with the down repeated. In addition, as noted previously, the referee can give possession to the defence for repeated deliberate time count violations on third down.

The clock does not run during convert attempts in the last three minutes of a half. If the 15 minutes of a quarter expire while the ball is live, the quarter is extended until the ball becomes dead. If a quarter's time expires while the ball is dead, the quarter is extended for one more scrimmage. A quarter cannot end while a penalty is pending: after the penalty yardage is applied, the quarter is extended one scrimmage. Note that the non-penalized team has the option to "decline" any penalty it considers disadvantageous, so a losing team cannot indefinitely prolong a game by repeatedly committing infractions.

In the CFL, if the game is tied at the end of regulation play, then each team is given an equal number of offensive possessions to break the tie. A coin toss is held to determine which team will take possession first; the first team scrimmages the ball at the opponent's 35-yard line and conducts a series of downs until it scores or loses possession. If the team scores a touchdown, starting with the 2010 season, it is required to attempt a two-point conversion.
The other team then scrimmages the ball at the opponent's 35-yard line and has the same opportunity to score. After the teams have completed their possessions, if one team is ahead, then it is declared the winner; otherwise, the two teams each get another chance to score, scrimmaging from the other 35-yard line. After this second round, if there is still no winner, during the regular season the game ends as a tie. In a playoff game, the teams continue to attempt to score from alternating 35-yard lines, until one team is leading after both have had an equal number of possessions.

In U Sports football, for the Uteck Bowl, Mitchell Bowl, and Vanier Cup, the same overtime procedure is followed until there is a winner.

The offensive positions found in Canadian football have, for the most part, evolved throughout the years, and are not officially defined in the rules. However, among offensive players, the rules recognize three different types of players:


Specific offensive positions include:


The rules do not constrain how the defence may arrange itself (other than the requirement that they must remain one yard behind the line of scrimmage until the play starts).


"Special teams" generally refers to kicking plays, which typically involve a change in possession.





</doc>
<doc id="6966" url="https://en.wikipedia.org/wiki?curid=6966" title="Chinese calendar">
Chinese calendar

The traditional Chinese calendar is a lunisolar calendar which reckons years, months and days according to astronomical phenomena. It was developed by the Qin Dynasty. , the Chinese calendar is defined by GB/T 33661-2017 "Calculation and promulgation of the Chinese calendar," which the Standardization Administration of China issued on May 12, 2017.

The Chinese calendar governs traditional activities in China and in overseas-Chinese communities. It depicts and lists the dates of traditional Chinese holidays, and guides Chinese people in selecting the most auspicious days for weddings, funerals, moving, or beginning a business.

In the Chinese calendar the days begin and end at midnight. The months begin on the day with the dark (new) moon. The years begin with the second (or third) dark moon after winter solstice. The solar terms are the important components of the Chinese calendar. In a month there are one to three solar terms.

The currently used traditional Chinese calendar represents the end result of centuries of evolution. Ancient scientists added many astronomical and seasonal factors, and people can reckon the timing of natural phenomena such as the moon phase and tides based on the Chinese calendar. The Chinese calendar has over 100 variants, whose characteristics reflect the calendar's evolutionary path. As with Chinese characters, different variants are used in different parts of the Chinese cultural sphere.

Korea, Vietnam, and the Ryukyu Islands adopted the Chinese calendar completely - it evolved into Korean, Ryukyuan, and Vietnamese calendars, with the main difference being the use of different meridians, which leads to some astronomical events falling on different dates in different countries. Thus the same event may occasionally be assigned a different date in each of those calendars. The traditional Japanese calendar also derived from the Chinese calendar, based on a Japanese meridian, however its official use in Japan was abolished in the early 20th century and its usage has mostly disappeared since then. Calendars in Mongolia and Tibet have absorbed elements from the Chinese calendar and elements from other systems, but they are not direct descendants of the Chinese calendar.

The official calendar in China is the Gregorian calendar, but the traditional Chinese calendar still plays an important role there. The Chinese calendar is known officially as the "Rural Calendar" (),
but is often referred to by other names, such as the "Former Calendar" (), the "Traditional Calendar" (), or the "Lunar Calendar" (). The Chinese calendar preserves traditional East Asian culture, and is the root to many other East Asian calendars.

Although the solar term governs the month sequences of the traditional Chinese calendar, it is not an agricultural calendar.

In ancient China the calendars marked the name and stem – branch of the year; month names; month length flags (大/小=Long/Short); the stems and branches of the first, eleventh, and 21st days; and the date/stem-branch/time of the solar terms in the month.

The calendar has a year, month and date frame. The key elements are the day, synodic month and solar year. The Chinese calendar is a lunisolar calendar, similar to the Hindu and Hebrew calendars.

Elements:

The movements of the Sun, the Moon, Mercury, Venus, Mars, Jupiter, and Saturn are the key references for calendar calculations. These are known as the seven luminaries. 

The Big Dipper is regarded as the compass in the sky, and the handle's direction decides the season and solar month.

The stars are divided into Three Enclosures and 28 Mansions according to their locations in the sky relative to Ursa Minor at the centre. Each mansion is named with a character that describes the shape of the principal asterism it contains.
The moon moves about one mansion per day; the 28 mansions are thus used to count days too. In the Tang Dynasty, Yuan Tiangang () matched the 28 mansions, seven luminaries and yearly animal signs, yielding combinations such as “horn-wood-flood dragon” ().

Several coding systems are used for some special circumstances in order to avoid ambiguity, such as continuous day or year count. 

In Modern China, people use the Western hour-minute-second system to divide time. In Ancient China, people used the "shi-ke" system to divide the time during the day and the "geng-dian" system to divide the time during the night.. For example:
In the Chinese calendar, the day begins at midnight and ends at the next midnight, but people tend to regard the days as beginning at dawn.

The Chinese appear to have adopted the seven-day week from the Hellenistic system by the 4th century, although by which route is not entirely clear. It was again transmitted to China in the 8th century by Manichaeans, via the country of Kang (a Central Asian polity near Samarkand). It is the most predominantly used system in modern China.

Other than the seven-day week system, in ancient China, the days were grouped into 10-day weeks with the stems, 12-day weeks with the branches, or 9/10-day weeks () with the date in the month.

The ten-day week was used in antiquity (reportedly as early as in the Bronze Age Xia dynasty). In modern time, it is still used in counting special days including Three Fu Days ().

The law during the Han dynasty (206 BC – AD 220) required officials of the empire to rest every five days, called "mu" (沐), while it was changed into 10 days in the Tang dynasty (AD 618 – 907), called "huan" (澣/浣) or "xún" (旬).

Months were almost three weeks long (alternating 29 and 30 days to keep in line with the lunation). As a practice, the months are divided into 3 "xún". The first 10 days is the "early xún" (), the middle 10 days is the "mid xún" (), and the last 9 or 10 days is the "late xún" ().

Markets in Japan followed the Chinese "jun" (旬) system; see Japanese calendar. In Korea, it was called "Sun" (순,旬).

In winter, there is also a 9-day cycle counting start from the winter solstice, which would last for 9 cycles until 81 days later when it is deemed as the end of winter.

Month is the time between the dark moon. In the early days, the month length was estimated, and balanced. In general, 12-months-cycles and 13-months-cycles alternated for compliance with the synodic month. 
In different ages, the calendar use different major cycle, which contains several 12-months-cycles and 13-months-cycle. The synodic month of Taichu calendar is 29/ days.

In 7th century, the "Wùyín Yuán Calendar" of Tang dynasty in 7th century, the month length was determined by the real synodic month for the first time, instead of the cycling method, which mean month lengths is determined by observation and prediction starting from Tang dynasty, except a few brief period of time.

Because astronomical observation is used to determine month length, date of the Chinese calendar corresponds to the moon phase.

As the beginning of every month is determined by the time when the new moon occur, thus other countries who have adopted the calendar and use time standard that are different from China to calculate their own version of the calendar could result in deviation. For instance, the first new moon in the year 1968 in Gregorian calendar happened in UTC Jan 29 16:29, which would translate to Jan 29 23:29 in UTC+7 timezone (which is what North Vietnam used to calculate their Vietnamese calendar) while it would be Jan 30 00:15 based on the longitude of Beijing (as used by South Vietnam at the time), causing the two countries celebrate Tết holiday in different date that year and result in asynchronized attacks in Tet Offensive.

The solar year () is the time between the winter solstices. The solar year is divided into 24 solar terms.

In ancient China, the solar year and solar terms were estimated and balanced, and the solar term is just the / of the solar year, about 15/ days.

Starting from the 17th century, when the "Shixian Calendar" of Qing dynasty was adopted, the solar year was determined by the real tropical year instead. The solar terms correspond to intervals of 15° along the ecliptic.

Different version of traditional Chinese calendar might have different average year length. For instance, one solar year of "Taichu calendar", which were implemented in 1st century BC, is 365/ (365.25016) days, while one solar year of "Shoushi calendar", which were implemented in 13th century, is 365/ (365.24250) days, which is the same as the Gregorian calendar. The difference of 0.00766 days amounts to a one-day shift in 130.5 years.

Couples of solar terms are climate terms (solar months). The first of each couples is "pre-climate" (), and the second of the each couple is "mid-climate" ().

In general, there are 11 or 12 complete months and 2 incomplete months, which contains the winter solstice, in a solar year. The 11 mid-climates except the winter solstice are in the 11 or 12 complete months. The first month without a mid-climate is the leap month.

The complete months except the intercalary month, queues up from 0 to 10, and the incomplete months follows this queue, to be 11. The intercalary follows the queue number before by rule.

The civil year starts from the first spring month (1), and ends at the last winter month (0/0i). The first and last month is called as "Zhēngyuè" (, capital month) and "Làyuè" (, sacrificial month), and the other month is called according to the queue number (except that the 0th month is "Shi'eryue", if the "Layue" is a leap month).

There are 12 or 13 months in each year. The years with 12 months, or 353~355 days, are common years. The years with 13 months, or 383~385 days, are long years.

Years were numbered after the reign title in Ancient China, but the reign title was no longer used after the founding of PRC in 1949. People use the stem-branches to demarcate the years. For example, the year from "February 8, 2016" to "January 27, 2017" is a "Bǐngshēnnían", long.

To Encode the date in the Chinese calendar, the flag of the intercalary month should be considered. For example, "Run Liuyue 6, Dingyounian: 408-6i-06 (Timestamp: 40806106)"

In "Tang Dynasty", the earthly branches are used to mark the months for about 150 days (Dec, 761~May, 762). At that time, the year starts from the month with Winter Solstice, and the month from Zhengyue to Layue are named as: Yinyue, Maoyue, Chenyue, Siyue, Wuyue, Weiyue, Shenyue Youyue, Xuyue, Haiyue, Ziyue, and Chouyue.


A typical graphical representation of the Chinese calendar is the vernal cattle diagram (), which help people calculate the date. In the vernal cattle diagram: 

In China, age for official use is based on the Gregorian calendar. For traditional use, age is based on the Chinese calendar. For the first year from the birthday, the child is considered one year old. After each New Year's Eve, add one year. "Ring out the old age and ring in the new one ()" is the literary express of New Year Ceremony. For example, if one's birthday is "Làyuè" 29th 2013, he is 2 years old at "Zhēngyuè" 1st 2014. On the other hand, people say months old instead of years old, if someone is too young. It is that the age sequence is "1 month old, 2 months old, ... 10 months old, 2 years old, 3 years old...".

After the actual age () was introduced into China, the Chinese traditional age was referred to as the nominal age (). Divided the year into two halves by the birthday in the Chinese calendar, the nominal age is 2 older than the actual age in the first half, and the nominal age is 1 older than the actual age in the second half ().

Just as it is awkward to define the birthday of someone born on the 29th of February in the Gregorian calendar, special rules are used for birthdays or other anniversaries during the intercalary month or on the 30th day.

In the Ancient China, years were numbered from 1, beginning from the next year after a new emperor ascended the throne or the current emperor announced a new era name. The first reign title was "Jiànyuán" (, from 140 BCE), and the last reign title was "Xuāntǒng" (, from 1908 CE). The era system was abolished in 1912 CE, after which the Current Era or Republican era was used. The epoch of the Current Era is just the same as the era name of Emperor Ping of Han, "Yuánshí" ().


The 60 stem-branches were used to mark the date continually from Shang Dynasty. Before Han Dynasty, people knew the orbital period of Jupiter is about 4332 days, which is about 12*361 days. So, the orbital period of Jupiter was divided into 12 periods, which was used to number the year. The Jupiter was called as the star of age (), and the / Jupiter orbital period was called as the age ().

361 days is just 6 cycles of 60-stem-branches, so the stem-branches of the first day move forward one after each "sui". The first day of each "sui" was called as the "sui" capital ().

And the stem-branches of the "taisui" was used to mark the year. Obviously, there're two "taisui" in some year for the "sui" is shorter than solar rear. About after each 86 year, a "taisui" was leaped. The leaped of the "sui" was called as beyond the star ().

At the eastern Han Dynasty, the "chaochen" are abolished, and the 60 stem-branches are used to mark year continually without leap.

The Stem-branches year number system provided a solution for the defect of era system (unequal length of the reign titles)


Occasionally, nomenclature similar to that of the Christian era has been used, such as
No reference date is universally accepted. The most popular is the Christian Era, such as in "Today is "gongli" (GC) 1984 "nian" (year)... nongli (CC)...".

On January 2, 1912, Sun Yat-sen declared a change to the official calendar and era. In his declaration, January 1, 1912 is called "Shíyīyuè 13th, 4609 AH" which assumes an epoch (1st year) of 2698 BCE. This declaration was adopted by many overseas Chinese communities outside Southeast Asia such as San Francisco's Chinatown.

In the 17th century, the Jesuits tried to determine what year should be considered the epoch of the Han calendar. In his "Sinicae historiae decas prima" (first published in Munich in 1658), Martino Martini (1614–1661) dated the ascension of the Yellow Emperor to 2697 BC, but started the Chinese calendar with the reign of Fuxi, which he claimed started in 2952 BCE. Philippe Couplet's (1623–1693) "Chronological table of Chinese monarchs" ("Tabula chronologica monarchiae sinicae"; 1686) also gave the same date for the Yellow Emperor. The Jesuits' dates provoked great interest in Europe, where they were used for comparisons with Biblical chronology.

Modern Chinese chronology has generally accepted Martini's dates, except that it usually places the reign of the Yellow Emperor in 2698 BC and omits the Yellow Emperor's predecessors Fuxi and Shennong, who are considered "too legendary to include".

Starting in 1903, radical publications started using the projected date of birth of the Yellow Emperor as the first year of the Han calendar. Different newspapers and magazines proposed different dates. Jiangsu, for example, counted 1905 as year 4396 (use an epoch of 2491 BCE), whereas the newspaper "Ming Pao" () reckoned 1905 as 4603 (use an epoch of 2698 BCE). Liu Shipei (; 1884–1919) created the Yellow Emperor Calendar, now often used to calculate the date, to show the unbroken continuity of the Han race and Han culture from earliest times. Liu's calendar started with the birth of the Yellow Emperor, which he determined to be 2711 BC. There is no evidence that this calendar was used before the 20th century. Liu calculated that the 1900 international expedition sent by the Eight-Nation Alliance to suppress the Boxer Rebellion entered Beijing in the 4611th year of the Yellow Emperor.


There is an epoch for each version of the Chinese calendar, which is called "Lìyuán" (). The epoch is the optimal origin of the calendar, and it is a "Jiǎzǐrì", the first day of a lunar month, and the dark moon and solstice are just at the midnight (). And tracing back to a perfect day, such as that day with the magical star sign, there's a supreme epoch (). The continuous year based on the supreme epoch is "shàngyuán jīnián" (). More and more factors were added into the supreme epoch, and the "shàngyuán jīnián" became a huge number. So, the supreme epoch and "shàngyuán jīnián" were neglected from the "Shòushí" calendar.


Shao Yong ( 1011–1077), a philosopher, cosmologist, poet, and historian who greatly influenced the development of Neo-Confucianism in China, introduced a time system in his "The Ultimate which Manages the World" ()

In his time system, 1 "yuán" (), which contains 12'9600 years, is a lifecycle of the world. Each "yuán" is divided into 12 "huì" (). Each "huì" is divided into 30 "yùn" (), and each "yùn" is divided into 12 "shì" (). So, each "shì" is equivalent to 30 years. The "yuán-huì-yùn-shì" corresponds with "nián-yuè-rì-shí". So the "yuán-huì-yùn-shì" is called the "major tend" or the "numbers of the heaven", and the "nián-yuè-rì-shí" is called the "minor tend" or the "numbers of the earth".

The "minor tend" of the birth is adapted by people for predicting destiny or fate. The numbers of "nián-yuè-rì-shí" are encoded with stem-branches and show a form of "Bāzì". The "nián-yuè-rì-shí" are called the Four Pillars of Destiny. For example, the "Bāzì" of the Qianlong Emperor is "Xīnmǎo, Dīngyǒu, Gēngwǔ, Bǐngzǐ" (). Shào "Huángjíjīngshì" recorded the history of the timing system from the first year of the 180 "yùn" or 2149 "shì" ("HYSN 0630-0101", 2577 BC) and marked the year with the reign title from the "Jiǎchénnián" of the 2156 "shì" ("HYSN 0630-0811", 2357 BC, "Tángyáo 1", ). According to this timing system, 2014-1-31 is "HYSN/YR 0712-1001/0101".

The table below shows the kinds of year number system along with correspondences to the Western (Gregorian) calendar. Alternatively, see this larger table of the full 60-year cycle.

In the Sinosphere, the traditional festivals are calculated using the date or solar terms, and are considered auspicious.

Before the Zhou dynasty, the Chinese calendars used a solar calendar.
According to Ancient Chinese literature, the first version was the five-phases calendar (), which came from the tying knots culture. In the five-phases calendar, a year was divided into five phases which were expressed by five ropes. Each rope was folded into halves, and the day in the corner was the capital day (). They're three sections in each halves, and the Chinese Character of phase is the pictograph of the rope of the tying knots. The ten half-ropes were arranged into a row, and a man shape was engraved by the ropes. The part of man shape derived into 10 heaven stems. The days in each sections were recorded with 12 earthly branches. So, in the five-phases calendar, a year is fives phases or ten months, and a phase is six sections or 73 days. The remainder of each phases are marked in the Hetu, which is found in Song Dynasty.

The second version is the four-seasons calendar (). In the four-seasons calendar, the days were counting by ten, and three ten-days weeks were built into a month. There were 12 months in a year, and a week were intercalated in the hot month. In the age of four-seasons calendar, the 10 heaven stems and 12 earthly branches were used to mark days synchronously.

The third version is the balanced calendar () a year was defined into 365.25 days, and the month was defined into 29.5 days. And after each 16 months, a half-month was intercalated. There half-months were merged into months later, and the archetype of the Chinese calendar was brought out in the Spring and Autumn ages.

Oracle bone records indicate that the calendar of Shang Dynasty were a balanced calendar, and the 12, 13, even 14 months were packed into a year roughly. Generally, the month after the winter solstice was named as the capital month ().

In Zhou dynasty, the authority issued the official calendar, which is a primitive lunisolar calendar. The year beginning of Zhou's calendar () is the day with dark moon before the winter solstice, and the epoch is the Winter Solstice of a Dīngyǒu year.

Some remote vassal states issued their own calendars upon the rule of Zhou's calendar, such as:
During the Spring and Autumn period and Warring States period, Some vassal states got out of control of Zhou, and issues their own official calendar, such as:
These six calendars are called as the six ancient calendars (), and are the quarter remainder calendars (). The months of these calendars begin on the day with the darkmoon, and there are 12 or 13 month within a year. The intercalary month is placed at the end of the year, and called as 13th month.

The modern version of the "Zhuanxu's" calendar is the Chinese Qiang calendar and Chinese Dai calendar, which are the calendar of mountain peoples.

After Qin Shi Huang unified China under the Qin dynasty in 221 BCE, Qin's calendar () was promulgated. The Qin's calendar follows the rules of Zhuanxu's calendar, but the month order follows the Xia calendar. The months in the year are from the 10th month to the 9th month, and the intercalary month is called as the second Jiuyue (). In the early Han dynasty, the Qin calendar continued to be used.

Emperor Wu of the Han dynasty introduced reforms halfway through his administration. His Taichu or Grand Inception Calendar ( introduced 24 solar terms which determined the month names. The solar year was defined as 365 / days, and divided into 24 solar terms. Each couples of solar terms are associated into 12 climate terms. The lunar month was defined as 29 / days and named according to the closest climate term. The mid-climate in the month decides the month name, and a month without mid-climate is an intercalary month.

The Taichu calendar established the frame of the Chinese calendar, Ever since then, there have been over 100 official calendars in Chinese which are consecutive and follow the structure of "Tàichū" calendar both. There're several innovation in calendar calculation in the history of over 2100 years, such as:

The Chinese calendar lost the status of the official statutory calendar in China in the beginning of the 20th century, however it has been continually being used for various purposes.

Because the Republic of China adopted the UTC+8 timezone instead of using Beijing Mean Solar Time in 1928 CE, Chinese calendars produced in Mainland China have switched to use UTC+8 in the following year. However, the switch in time standard used in Chinese calendars has not been universally adopted in areas like Taiwan and Hong Kong, and some calendars were still follow the last calendar of Qing dynasty that was published in 1908. In 1978, this practice caused confusion on what date the 1978 Mid-autumn festival occur, and caused those areas to switch to the UTC+8-based Chinese calendar thereafter.

In the late Ming dynasty, Xu Guangqi and his colleagues worked out the new calendar based on western astronomical arithmetic. But the new calendar was not released before the end of the Ming dynasty. In the early Qing dynasty, Johann Adam Schall von Bell submitted the calendar to the Shunzhi Emperor. The Qing government released the calendar under the name the "Shíxiàn" calendar, which means seasonal charter.
In the "Shíxiàn" calendar, the solar terms each correspond to 15° along the ecliptic. It meant the Chinese calendar can be used as astronomical calendar. However, the length of the climate term near the perihelion is shorter than 30 days and there may be two mid-climate terms. The rule of the mid-climate terms decides the months, which is used for thousands years, lose its validity. The "Shíxiàn" calendar changed the rule to "decides the month in sequence, except the intercalary month."

The version of the traditional Chinese calendar currently being used follows the rules of the "Shíxiàn" calendar, except that: 

To optimize the Chinese calendar, astronomers have released many proposed changes. A typical proposal was released by Gao Pingzi (; 1888-1970), a Chinese astronomer who was one of the founders of Purple Mountain Observatory. In his proposal, the month numbers are calculated before the dark moons and the solar terms were rounded to the day. Under his proposal, the month numbers are the same for the Chinese calendar upon different time zones.

As the intercalary month is determined by the first month without mid-climate and the exact time when each mid-climate happen would vary according to time zone, countries that have adopted the calendar but calculate with their own time could vary from the one used in China because of this. For instance, the 2012 FTG happened in UTC May 20 15:15, which would translate to May 20 23:15 in UTC+8, making FTG the mid-climate for the fourth month of that traditional Chinese year [April 21 ~ May 20 in Gregorian calendar], but in Korea it happened in May 21 00:15 in UTC+9, and as new moon take place in May 21 in that month, therefore the month before that would only consist of the SC solar term, lacking mid-climate. As a result, the month starting at April 21 would be an intercalary month in Korean calendar, but not in Chinese Calendar, and the intercalary month in Chinese calendar would start in the month after, in the fifth month starting from May 21, which would only consist of the solar term STG, while the month in Korean Calendar would have both FTG and STG solar term in it.

Among the ethnic groups inhabiting the mountains and plateaus of southwestern China, and those living in the grasslands of northern China, their civil calendars show a diversity of practice based upon their characteristic phenology and culture, but they are based on the algorithm of the Chinese calendar of different periods, especially those of the Tang dynasty and pre-Qin dynasty period.







</doc>
<doc id="6968" url="https://en.wikipedia.org/wiki?curid=6968" title="Customer relationship management">
Customer relationship management

Customer relationship management (CRM) is an approach to manage a company's interaction with current and potential customers. It uses data analysis about customers' history with a company to improve business relationships with customers, specifically focusing on customer retention and ultimately driving sales growth.

One important aspect of the CRM approach is the systems of CRM that compile data from a range of different communication channels, including a company's website, telephone, email, live chat, marketing materials, and more recently, social media. Through the CRM approach and the systems used to facilitate it, businesses learn more about their target audiences and how to best cater to their needs.

The concept of customer relationship management started in the early 1970s, when customer satisfaction was evaluated using annual surveys or by front-line asking. At that time, businesses had to rely on standalone mainframe systems to automate sales, but the extent of technology allowed them to categorize customers in spreadsheets and lists. In 1982, Kate and Robert Kestnbaum introduced the concept of Database marketing, namely applying statistical methods to analyze and gather customer data. By 1986, Pat Sullivan and Mike Muhney released a customer evaluation system called ACT! based on the principle of digital rolodex, which offered a contact management service for the first time.

The trend was followed by numerous developers trying to maximize leads' potential, including Tom Siebel, who signed the first CRM product Siebel Systems in 1993. Nevertheless, customer relationship management popularized in 1997, due to the work of Siebel, Gartner, and IBM. Between 1997 and 2000, leading CRM products were enriched with enterprise resource planning functions, and shipping and marketing capabilities. Siebel introduced the first mobile CRM app called Siebel Sales Handheld in 1999. The idea of a cloud-hosted and moveable customer bases was soon adopted by other leading providers at the time, including PeopleSoft, Oracle, and SAP.

The first open-source CRM system was developed by SugarCRM in 2004. During this period, CRM was rapidly migrating to cloud, as a result of which it became accessible to sole entrepreneurs and small teams, and underwent a huge wave of price reduction. Around 2009, developers began considering the options to profit from social media's momentum, and designed tools to help companies become accessible on all users' favorite networks. Many startups at the time benefited from this trend to provide exclusively social CRM solutions, including Base and Nutshell. The same year, Gartner organized and held the first Customer Relationship Management Summit, and summarized the features systems should offer to be classified as CRM solutions. In 2013 and 2014, most of the popular CRM products were linked to business intelligence systems and communication software to improve corporate communication and end-users' experience. The leading trend is to replace standardized CRM solutions with industry-specific ones, or to make them customizable enough to meet the needs of every business.

In November 2016, "Forrester" released a report where it "identified the nine most significant CRM suites from eight prominent vendors," among them companies such as Infor, Microsoft, and NetSuite.

Strategic CRM is focused upon the development of a customer-centric business culture.

The primary goal of customer relationship management systems is to integrate and automate sales, marketing, and customer support. Therefore, these systems typically have a dashboard that gives an overall view of the three functions on a single customer view, a single page for each customer that a company may have. The dashboard may provide client information, past sales, previous marketing efforts, and more, summarizing all of the relationships between the customer and the firm. Operational CRM is made up of 3 main components: sales force automation, marketing automation, and service automation.

The role of analytical CRM systems is to analyze customer data collected through multiple sources, and present it so that business managers can make more informed decisions. Analytical CRM systems use techniques such as data mining, correlation, and pattern recognition to analyze the customer data. These analytics help improve customer service by finding small problems which can be solved, perhaps, by marketing to different parts of a consumer audience differently. For example, through the analysis of a customer base's buying behavior, a company might see that this customer base has not been buying a lot of products recently. After scanning through this data, the company might think to market to this subset of consumers differently, in order to best communicate how this company's products might benefit this group specifically.

The third primary aim of CRM systems is to incorporate external stakeholders such as suppliers, vendors, and distributors, and share customer information across organizations. For example, feedback can be collected from technical support calls, which could help provide direction for marketing products and services to that particular customer in the future.

A customer data platform (CDP) is a computer system used by marketing departments that assembles data about individual people from various sources into one database, with which other software systems can interact. As of February 2017 there were about twenty companies selling such systems and revenue for them was around US$300 million.

The main components of CRM are building and managing customer relationships through marketing, observing relationships as they mature through distinct phases, managing these relationships at each stage and recognizing that the distribution of value of a relationship to the firm is not homogenous. When building and managing customer relationships through marketing, firms might benefit from using a variety of tools to help organizational design, incentive schemes, customer structures, and more to optimize the reach of its marketing campaigns. Through the acknowledgement of the distinct phases of CRM, businesses will be able to benefit from seeing the interaction of multiple relationships as connected transactions. The final factor of CRM highlights the importance of CRM through accounting for the profitability of customer relationships. Through studying the particular spending habits of customers, a firm may be able to dedicate different resources and amounts of attention to different types of consumers.

Relational Intelligence, or awareness of the variety of relationships a customer can have with a firm, is an important component to the main phases of CRM. Companies may be good at capturing demographic data, such as gender, age, income, and education, and connecting them with purchasing information to categorize customers into profitability tiers, but this is only a firm's mechanical view of customer relationships. This therefore is a sign that firms believe that customers are still resources that can be used for up-sell or cross-sell opportunities, rather than humans looking for interesting and personalized interactions.

CRM systems include:


Customer satisfaction has important implications for the economic performance of firms because it has the ability to increase customer loyalty and usage behavior and reduce customer complaints and the likelihood of customer defection. The implementation of a CRM approach is likely to have an effect on customer satisfaction and customer knowledge for a variety of different reasons.

Firstly, firms are able to customize their offerings for each customer. By accumulating information across customer interactions and processing this information to discover hidden patterns, CRM applications help firms customize their offerings to suit the individual tastes of their customers. This customization enhances the perceived quality of products and services from a customer's viewpoint, and because perceived quality is a determinant of customer satisfaction, it follows that CRM applications indirectly affect customer satisfaction. CRM applications also enable firms to provide timely, accurate processing of customer orders and requests and the ongoing management of customer accounts. For example, Piccoli and Applegate discuss how Wyndham uses IT tools to deliver a consistent service experience across its various properties to a customer. Both an improved ability to customize and a reduced variability of the consumption experience enhance perceived quality, which in turn positively affects customer satisfaction. Furthermore, CRM applications also help firms manage customer relationships more effectively across the stages of relationship initiation, maintenance, and termination.

With CRM systems customers are served better on day to day process and with more reliable information their demand of self service from companies will decrease. If there is less need to interact with the company for different problems, customer satisfaction level increases. These central benefits of CRM will be connected hypothetically to the three kinds of equity that are relationship, value and brand, and in the end to customer equity. Eight benefits were recognized to provide value drivers.
In 2012, after reviewing the previous studies, someone selected some of those benefits which are more significant in customer's satisfaction and summarized them into the following cases:


Research has found a 5% increase in customer retention boosts lifetime customer profits by 50% on average across multiple industries, as well as a boost of up to 90% within specific industries such as insurance. Companies that have mastered customer relationship strategies have the most successful CRM programs. For example, MBNA Europe has had a 75% annual profit growth since 1995. The firm heavily invests in screening potential cardholders. Once proper clients are identified, the firm retains 97% of its profitable customers. They implement CRM by marketing the right products to the right customers. The firm's customers' card usage is 52% above industry norm, and the average expenditure is 30% more per transaction. Also 10% of their account holders ask for more information on cross-sale products.

Amazon has also seen great success through its customer proposition. The firm implemented personal greetings, collaborative filtering, and more for the customer. They also used CRM training for the employees to see up to 80% of customers repeat.

Consultants, such as Bain & Company, argue that it is important for companies establishing strong CRM systems to improve their relational intelligence. According to this argument, a company must recognize that people have many different types of relationships with different brands. One research study analyzed relationships between consumers in China, Germany, Spain, and the United States, with over 200 brands in 11 industries including airlines, cars and media. This information is valuable as it provides demographic, behavioral, and value-based customer segmentation. These types of relationships can be both positive and negative. Some customers view themselves as friends of the brands, while others as enemies, and some are mixed with a love-hate relationship with the brand. Some relationships are distant, intimate or anything in between.

Managers must understand the different reasons for the types of relationships, and provide the customer with what they are looking for. Companies can collect this information by using surveys, interviews, and more, with current customers. For example, Frito-Lay conducted many ethnographic interviews with customers to try and understand the relationships they wanted with the companies and the brands. They found that most customers were adults who used the product to feel more playful. They may have enjoyed the company's bright orange color, messiness and shape.

Companies must also improve their relational intelligence of their CRM systems. These days, companies store and receive huge amounts of data through emails, online chat sessions, phone calls, and more. Many companies do not properly make use of this great amount of data, however. All of these are signs of what types of relationships the customer wants with the firm, and therefore companies may consider investing more time and effort in building out their relational intelligence. Companies can use data mining technologies and web searches to understand relational signals. Social media such as Facebook, Twitter, blogs, etc. is also a very important factor in picking up and analyzing information. Understanding the customer and capturing this data allows companies to convert customer's signals into information and knowledge that the firm can use to understand a potential customer's desired relations with a brand.

It is also very important to analyze all of this information to determine which relationships prove the most valuable. This helps convert data into profits for the firm. Stronger bonds contribute to building market share. By managing different portfolios for different segments of the customer base, the firm can achieve strategic goals.

Many firms have also implemented training programs to teach employees how to recognize and effectively create strong customer-brand relationships. For example, Harley Davidson sent its employees on the road with customers, who were motorcycle enthusiasts, to help solidify relationships. Other employees have also been trained in social psychology and the social sciences to help bolster strong customer relationships. Customer service representatives must be educated to value customer relationships, and trained to understand existing customer profiles. Even the finance and legal departments should understand how to manage and build relationships with customers.

Applying new technologies while using CRM systems requires changes in infrastructure of the organization as well as deployment of new technologies such as business rules, databases and information technology.

Contact center CRM providers are popular for small and mid-market businesses. These systems codify the interactions between company and customers by using analytics and key performance indicators to give the users information on where to focus their marketing and customer service. This allows agents to have access to a caller's history to provide personalized customer communication. The intention is to maximize average revenue per user, decrease churn rate and decrease idle and unproductive contact with the customers.

Growing in popularity is the idea of gamifying, or using game design elements and game principles in a non-game environment such as customer service environments. The gamification of customer service environments includes providing elements found in games like rewards and bonus points to customer service representatives as a method of feedback for a job well done.
Gamification tools can motivate agents by tapping into their desire for rewards, recognition, achievements, and competition.

Contact center automation, the practice of having an integrated system that coordinates contacts between an organization and the public, is designed to reduce the repetitive and tedious parts of a contact center agent's job. Automation prevents this by having pre-recorded audio messages that help customers solve their problems. For example, an automated contact center may be able to re-route a customer through a series of commands asking him or her to select a certain number in order to speak with a particular contact center agent who specializes in the field in which the customer has a question. Software tools can also integrate with the agent's desktop tools to handle customer questions and requests. This also saves time on behalf of the employees.

Social CRM involves the use of social media and technology to engage and learn from consumers. Because the public, especially among young people, has increasingly using social networking sites, companies use these sites to draw attention to their products, services and brands, with the aim of building up customer relationships to increase demand.

Some CRM systems integrate social media sites like Twitter, LinkedIn and Facebook to track and communicate with customers. These customers also share their own opinions and experiences with a company's products and services, giving these firms more insight. Therefore, these firms can both share their own opinions and also track the opinions of their customers.

Enterprise feedback management software platforms, such as Confirmit, Medallia, and Satmetrix, combine internal survey data with trends identified through social media to allow businesses to make more accurate decisions on which products to supply.

CRM systems can also include technologies that create geographic marketing campaigns. The systems take in information based on a customer's physical location and sometimes integrates it with popular location-based GPS applications. It can be used for networking or contact management as well to help increase sales based on location.

Despite the general notion that CRM systems were created for the customer-centric businesses, they can also be applied to B2B environments to streamline and improve customer management conditions. For the best level of CRM operation in a B2B environment, the software must be personalized and delivered at individual levels.

The main differences between business-to-consumer (B2C) and business-to-business CRM systems concern aspects like sizing of contact databases and length of relationships. Business-to-business companies tend to have smaller contact databases than business-to-consumer, the volume of sales in business-to-business is relatively small. There are fewer figure propositions in business-to-business, but in some cases, they cost a lot more than business-to-consumer items and relationships in business-to-business environment are built over a longer period of time. Furthermore, business-to-business CRM must be easily integrated with products from other companies. Such integration enables the creation of forecasts about customer behavior based on their buying history, bills, business success, etc. An application for a business-to-business company must have a function to connect all the contacts, processes and deals among the customers segment and then prepare a paper. Automation of sales process is an important requirement for business-to-business products. It should effectively manage the deal and progress it through all the phases towards signing. Finally, a crucial point is personalization. It helps the business-to-business company to create and maintain strong and long-lasting relationship with the customer.

The overall CRM market grew by 12.3 percent in 2015. The following table lists the top vendors in 2012-2015 (figures in millions of US dollars) published in Gartner studies.

The four largest vendors with CRM system offerings are Salesforce, SAP, Oracle, and Microsoft, which represented 42 percent of the market in 2015. Other providers also are popular for small and mid market businesses. Splitting CRM providers into nine different categories (Enterprise CRM Suite, Midmarket CRM Suite, Small-Business CRM Suite, sales force automation, incentive management, marketing solutions, business intelligence, data quality, consultancies), each category has a different market leader. Additionally, applications often focus on professional fields such as healthcare, manufacturing, and other areas with branch-specific requirements.

In the Gartner CRM Summit 2010 challenges like "system tries to capture data from social networking traffic like Twitter, handles Facebook page addresses or other online social networking sites" were discussed and solutions were provided that would help in bringing more clientele. Many CRM vendors offer subscription-based web tools (cloud computing) and SaaS. Some CRM systems are equipped with mobile capabilities, making information accessible to remote sales staff.Salesforce.com was the first company to provide enterprise applications through a web browser, and has maintained its leadership position. 

Traditional providers have recently moved into the cloud-based market via acquisitions of smaller providers: Oracle purchased RightNow in October 2011 and SAP acquired SuccessFactors in December 2011.

The era of the "social customer" refers to the use of social media (Twitter, Facebook, LinkedIn, Google Plus, Pinterest, Instagram, Yelp, customer reviews in Amazon, etc.) by customers. CRM philosophy and strategy has shifted to encompass social networks and user communities.

Sales forces also play an important role in CRM, as maximizing sales effectiveness and increasing sales productivity is a driving force behind the adoption of CRM. Empowering sales managers was listed as one of the top 5 CRM trends in 2013.

Another related development is vendor relationship management (VRM), which provide tools and services that allow customers to manage their individual relationship with vendors. VRM development has grown out of efforts by ProjectVRM at Harvard's Berkman Center for Internet & Society and Identity Commons' Internet Identity Workshops, as well as by a growing number of startups and established companies. VRM was the subject of a cover story in the May 2010 issue of "CRM" Magazine.

Pharmaceutical companies were some of the first investors in sales force automation (SFA) and some are on their third- or fourth-generation implementations. However, until recently, the deployments did not extend beyond SFA—limiting their scope and interest to Gartner analysts.

Another trend worth noting is the rise of Customer Success as a discipline within companies. More and more companies establish Customer Success teams as separate from the traditional Sales team and task them with managing existing customer relations. This trend fuels demand for additional capabilities for more holistic understanding of the customer health, which is a limitation for many existing vendors in the space. As a result, a growing number of new entrants enter the market, while existing vendors add capabilities in this area to their suites. In 2017, artificial intelligence and predictive analytics were identified as the newest trends in CRM.

Companies face large challenges when trying to implement CRM systems. Consumer companies frequently manage their customer relationships haphazardly and unprofitably. They may not effectively or adequately use their connections with their customers, due to misunderstandings or misinterpretations of a CRM system's analysis. Clients who want to be treated more like a friend may be treated like just a party for exchange, rather than a unique individual, due to, occasionally, a lack of a bridge between the CRM data and the CRM analysis output. Many studies show that customers are frequently frustrated by a company's inability to meet their relationship expectations, and on the other side, companies do not always know how to translate the data they have gained from CRM software into a feasible action plan. In 2003, a Gartner report estimated that more than $2 billion had been spent on software that was not being used. According to CSO Insights, less than 40 percent of 1,275 participating companies had end-user adoption rates above 90 percent. Many corporations only use CRM systems on a partial or fragmented basis. In a 2007 survey from the UK, four-fifths of senior executives reported that their biggest challenge is getting their staff to use the systems they had installed. 43 percent of respondents said they use less than half the functionality of their existing systems. However, market research regarding consumers' preferences may increase the adoption of CRM among the developing countries' consumers.

Collection of customer data such as personally identifiable information must strictly obey customer privacy laws, which often requires extra expenditures on legal support.

Part of the paradox with CRM stems from the challenge of determining exactly what CRM is and what it can do for a company. The CRM paradox, also referred to as the "Dark side of CRM", may entail favoritism and differential treatment of some customers.

CRM technologies can easily become ineffective if there is no proper management, and they are not implemented correctly. The data sets must also be connected, distributed, and organized properly, so that the users can access the information that they need quickly and easily. Research studies also show that customers are increasingly becoming dissatisfied with contact center experiences due to lags and wait times. They also request and demand multiple channels of communications with a company, and these channels must transfer information seamlessly. Therefore, it is increasingly important for companies to deliver a cross-channel customer experience that can be both consistent as well as reliable.



</doc>
<doc id="6970" url="https://en.wikipedia.org/wiki?curid=6970" title="Chuck-a-luck">
Chuck-a-luck

Chuck-a-luck, also known as birdcage, is a game of chance played with three dice. It is derived from grand hazard and both can be considered a variant of sic bo, which is a popular casino game, although chuck-a-luck is more of a carnival game than a true casino game. The game is sometimes used as a fundraiser for charity.

Chuck-a-luck is played with three standard dice that are kept in a device shaped somewhat like an hourglass that resembles a wire-frame bird cage and pivots about its centre. The dealer rotates the cage end over end, with the dice landing on the bottom.

Wagers are placed based on possible combinations that can appear on the three dice. The possible wagers are usually fewer than the wagers that are possible in sic bo and, in that sense, chuck-a-luck can be considered to be a simpler game.

The wagers, and their associated odds, that are typically available are set out in the table below.

Chuck-a-luck is a game of chance. That is, on average, the players are expected to lose more than they win. The casino's advantage (house advantage or house edge) is greater than most other casino games and can be much greater.

For example, there are 216 (6 × 6 × 6) possible outcomes for a single throw of three dice. For a specific number:


At odds of 1 to 1, 2 to 1 and 10 to 1 respectively for each of these types of outcome, the expected loss as a percentage of the stake wagered is:

1 - ((75/216) × 2 + (15/216) × 3 + (1/216) × 11) = 4.6%

At worse odds of 1 to 1, 2 to 1 and 3 to 1, the expected loss as a percentage of the stake wagered is:

1 - ((75/216) × 2 + (15/216) × 3 + (1/216) × 4) = 7.9%

It should be noted that if the odds are adjusted to 1 to 1, 3 to 1 and 5 to 1 respectively, the expected loss as a percentage is:

1 - ((75/216) × 2 + (15/216) × 4 + (1/216) × 6) = 0%

However, commercially organised gambling games always have a house advantage which acts as a fee for the privilege of being allowed to play the game, so the last scenario does not represent real practice.


There is a reference to chuck-a-luck in the Abbott and Costello film "Hold That Ghost".

In Fritz Lang's 1952 film, "Rancho Notorious", chuck-a-luck is the name of the ranch run by Altar Keane (played by Marlene Dietrich) where outlaws hide from the law. Chuck-a-luck is featured in the lyrics to the theme song and in some plot points.

The game is played by Lazar in the James Bond movie "The Man with the Golden Gun".

The game is played by Freddie Rumsen in "Mad Men Season 2 Episode 9: Six-Month Leave".



</doc>
<doc id="6972" url="https://en.wikipedia.org/wiki?curid=6972" title="Chipmunk">
Chipmunk

Chipmunks are small, striped rodents of the family Sciuridae. Chipmunks are found in North America, with the exception of the Siberian chipmunk which is found primarily in Asia.

Chipmunks may be classified either as a single genus, "Tamias" (), or as three genera: "Tamias", which includes the eastern chipmunk; "Eutamias", which includes the Siberian chipmunk; and "Neotamias", which includes the 23 remaining, mostly western, species. These classifications are arbitrary, and most taxonomies over the twentieth century have placed the chipmunks in a single genus. However, studies of mitochondrial DNA show that the divergence between each of the three chipmunk groups is comparable to the genetic dissimilarity between "Marmota" and "Spermophilus".

The genus name "Tamias" is Greek for "treasurer", "steward", or "housekeeper", which is a reference to the animals' role in plant dispersal through their habit of collecting and storing food for winter use.

The common name originally may have been spelled "chitmunk," from the native Odawa (Ottawa) word "jidmoonh", meaning "red squirrel" ("cf." Ojibwe, "ajidamoo"). The earliest form cited in the "Oxford English Dictionary" (from 1842) is "chipmonk," however, "chipmunk" appears in several books from the 1820s and 1830s. Other early forms include "chipmuck" and "chipminck," and in the 1830s they were also referred to as "chip squirrels;" probably in reference to the sound they make. In the mid-1800s, John James Audubon and his sons included a lithograph of the chipmunk in their "Viviparous Quadrupeds of North America", calling it the "chipping squirrel [or] hackee." Chipmunks have also been referred to as "striped squirrels," "timber tigers," and "ground squirrels" (although the name "ground squirrel" usually refers to other squirrels, such as those of the genus "Spermophilus").

Chipmunks have an omnivorous diet primarily consisting of seeds, nuts and other fruits, and buds. They also commonly eat grass, shoots, and many other forms of plant matter, as well as fungi, insects and other arthropods, small frogs, worms, and bird eggs. Around humans, chipmunks can eat cultivated grains and vegetables, and other plants from farms and gardens, so they are sometimes considered pests. Chipmunks mostly forage on the ground, but they climb trees to obtain nuts such as hazelnuts and acorns. At the beginning of autumn, many species of chipmunk begin to stockpile nonperishable foods for winter. They mostly cache their foods in a larder in their burrows and remain in their nests until spring, unlike some other species which make many small caches of food. Cheek pouches allow chipmunks to carry food items to their burrows for either storage or consumption.

Eastern chipmunks mate in early spring and again in early summer, producing litters of four or five young twice each year. Western chipmunks breed only once a year. The young emerge from the burrow after about six weeks and strike out on their own within the next two weeks.

These small mammals fulfill several important functions in forest ecosystems. Their activities harvesting and hoarding tree seeds play a crucial role in seedling establishment. They consume many different kinds of fungi, including those involved in symbiotic mycorrhizal associations with trees, and are an important vector for dispersal of the spores of subterranean sporocarps (truffles) which have co-evolved with these and other mycophagous mammals and thus lost the ability to disperse their spores through the air.

Chipmunks construct expansive burrows which can be more than in length with several well-concealed entrances. The sleeping quarters are kept clean as shells and feces are stored in refuse tunnels.

The eastern chipmunk hibernates in the winter, while western chipmunks do not, relying on the stores in their burrows.

Chipmunks play an important role as prey for various predatory mammals and birds but are also opportunistic predators themselves, particularly with regard to bird eggs and nestlings, as in the case of eastern chipmunks and mountain bluebirds ("Siala currucoides").

Chipmunks typically live about three years although some have been observed living to nine years in captivity.

Chipmunks in captivity are said to sleep for an average of about 15 hours a day. It is thought that mammals which can sleep in hiding, such as rodents and bats, tend to sleep longer than those that must remain on alert.

Subgenus "Eutamias"

Subgenus "Tamias"

Genus "Neotamias", sometimes considered subgenus

Extinct:





</doc>
<doc id="6974" url="https://en.wikipedia.org/wiki?curid=6974" title="Computer music">
Computer music

Computer music is the application of computing technology in music composition, to help human composers create new music or to have computers independently create music, such as with algorithmic composition programs. It includes the theory and application of new and existing computer software technologies and basic aspects of music, such as sound synthesis, digital signal processing, sound design, sonic diffusion, acoustics, and psychoacoustics. The field of computer music can trace its roots back to the origins of electronic music, and the very first experiments and innovations with electronic instruments at the turn of the 20th century.

In the 2000s, with the widespread availability of relatively affordable home computers that have a fast processing speed, and the growth of home recording using digital audio recording systems ranging from Garageband to Protools, the term is sometimes used to describe music that has been created using digital technology.

Much of the work on computer music has drawn on the relationship between music theory and mathematics, a relationship which has been noted since the Ancient Greeks described the "harmony of the spheres".

Musical notes were first generated by a computer programmed by Alan Turing at the Computing Machine Laboratory of the University of Manchester in 1948. The first music proper, a performance of the British National Anthem was programmed by Christopher Strachey on the Mark II Manchester Electronic Computer at same venue, in 1951. Later that year, short extracts of three pieces were recorded there by a BBC outside broadcasting unit: the National Anthem, "Ba, Ba Black Sheep, and "In the Mood". Researchers at the University of Canterbury, Christchurch restored the acetate master disc in 2016 and the results may be heard on Soundcloud.

The world's first computer to play music publicly was CSIRAC, which was designed and built by Trevor Pearcey and Maston Beard in the 1950s. Mathematician Geoff Hill programmed the CSIRAC to play popular musical melodies from the very early 1950s. In 1951 it publicly played the "Colonel Bogey March" of which no known recordings exist.
However, CSIRAC played standard repertoire and was not used to extend musical thinking or composition practice which is current computer-music practice.

Two further major 1950s developments were the origins of digital sound synthesis by computer, and of algorithmic composition programs beyond rote playback. Max Mathews at Bell Laboratories developed the influential MUSIC I program and its descendents, further popularising computer music through a 1963 article in "Science". Amongst other pioneers, the musical chemists Lejaren Hiller and Leonard Isaacson worked on a series of algorithmic composition experiments from 1956-9, manifested in the 1957 premiere of the "Illiac Suite" for string quartet.

In Japan, experiments in computer music date back to 1962, when Keio University professor Sekine and Toshiba engineer Hayashi experimented with the computer. This resulted in a piece entitled "TOSBAC Suite", influenced by the "Illiac Suite". Later Japanese computer music compositions include a piece by Kenjiro Ezaki presented during Osaka Expo '70 and "Panoramic Sonore" (1974) by music critic Akimichi Takeda. Ezaki also published an article called "Contemporary Music and Computers" in 1970. Since then, Japanese research in computer music has largely been carried out for commercial purposes in popular music, though some of the more serious Japanese musicians used large computer systems such as the "Fairlight" in the 1970s.

Early computer-music programs typically did not run in real time. Programs would run for hours or days, on multimillion-dollar computers, to generate a few minutes of music. One way around this was to use a 'hybrid system', most notably the Roland MC-8 Microcomposer, where a microprocessor-based system controls an analog synthesizer, released in 1978. John Chowning's work on FM synthesis from the 1960s to the 1970s allowed much more efficient digital synthesis, eventually leading to the development of the affordable FM synthesis-based Yamaha DX7 digital synthesizer, released in 1983. In addition to the Yamaha DX7, the advent of inexpensive digital chips and microcomputers opened the door to real-time generation of computer music. In the 1980s, Japanese personal computers such as the NEC PC-88 came installed with FM synthesis sound chips and featured audio programming languages such as Music Macro Language (MML) and MIDI interfaces, which were most often used to produce video game music, or chiptunes. By the early 1990s, the performance of microprocessor-based computers reached the point that real-time generation of computer music using more general programs and algorithms became possible.
Interesting sounds must have a fluidity and changeability that allows them to remain fresh to the ear. In computer music this subtle ingredient is bought at a high computational cost, both in terms of the number of items requiring detail in a score and in the amount of interpretive work the instruments must produce to realize this detail in sound. 

Advances in computing power and software for manipulation of digital media have dramatically affected the way computer music is generated and performed. Current-generation micro-computers are powerful enough to perform very sophisticated audio synthesis using a wide variety of algorithms and approaches. Computer music systems and approaches are now ubiquitous, and so firmly embedded in the process of creating music that we hardly give them a second thought: computer-based synthesizers, digital mixers, and effects units have become so commonplace that use of digital rather than analog technology to create and record music is the norm, rather than the exception.

Despite the ubiquity of computer music in contemporary culture, there is considerable activity in the field of computer music, as researchers continue to pursue new and interesting computer-based synthesis, composition, and performance approaches. Throughout the world there are many organizations and institutions dedicated to the area of computer and electronic music study and research, including the ICMA (International Computer Music Association), Centre for Digital Music [C4DM, IRCAM, GRAME, SEAMUS (Society for Electro Acoustic Music in the United States), CEC (Canadian Electroacoustic Community), and a great number of institutions of higher learning around the world.

Computer-generated music is music composed by, or with the extensive aid of, a computer. Although any music which uses computers in its composition or realisation is computer-generated to some extent, the use of computers is now so widespread (in the editing of pop songs, for instance) that the phrase computer-generated music is generally used to mean a kind of music which could not have been created "without" the use of computers.

We can distinguish two groups of computer-generated music: music in which a computer generated the score, which could be performed by humans, and music which is both composed and performed by computers. There is a large genre of music that is organized, synthesized, and created on computers.

Later, composers such as Gottfried Michael Koenig had computers generate the sounds of the composition as well as the score. Koenig produced algorithmic composition programs which were a generalisation of his own serial composition practice. This is not exactly similar to Xenakis' work as he used mathematical abstractions and examined how far he could explore these musically. Koenig's software translated the calculation of mathematical equations into codes which represented musical notation. This could be converted into musical notation by hand and then performed by human players. His programs Project 1 and Project 2 are examples of this kind of software. Later, he extended the same kind of principles into the realm of synthesis, enabling the computer to produce the sound directly. SSP is an example of a program which performs this kind of function. All of these programs were produced by Koenig at the Institute of Sonology in Utrecht in the 1970s.

Procedures such as those used by Koenig and Xenakis are still in use today. Since the invention of the MIDI system in the early 1980s, for example, some people have worked on programs which map MIDI notes to an algorithm and then can either output sounds or music through the computer's sound card or write an audio file for other programs to play.

Some of these simple programs are based on fractal geometry, and can map midi notes to specific fractals, or fractal equations. Although such programs are widely available and are sometimes seen as clever toys for the non-musician, some professional musicians have given them attention also. The resulting 'music' can be more like noise, or can sound quite familiar and pleasant. As with much algorithmic music, and algorithmic art in general, more depends on the way in which the parameters are mapped to aspects of these equations than on the equations themselves. Thus, for example, the same equation can be made to produce both a lyrical and melodic piece of music in the style of the mid-nineteenth century, and a fantastically dissonant cacophony more reminiscent of the avant-garde music of the 1950s and 1960s.

Other programs can map mathematical formulae and constants to produce sequences of notes. In this manner, an irrational number can give an infinite sequence of notes where each note is a digit in the decimal expression of that number. This sequence can in turn be a composition in itself, or simply the basis for further elaboration.

Operations such as these, and even more elaborate operations can also be performed in computer music programming languages such as Max/MSP, Reaktor, SuperCollider, Csound, Pure Data (Pd), Keykit, and ChucK. These programs now easily run on most personal computers, and are often capable of more complex functions than those which would have necessitated the most powerful mainframe computers several decades ago.

There exist programs that generate "human-sounding" melodies by using a vast database of phrases. One example is Band-in-a-Box, which is capable of creating jazz, blues and rock instrumental solos with almost no user interaction. Another is Impro-Visor, which uses a stochastic context-free grammar to generate phrases and complete solos.

Another 'cybernetic' approach to computer composition uses specialized hardware to detect external stimuli which are then mapped by the computer to realize the performance. Examples of this style of computer music can be found in the middle-80's work of David Rokeby (Very Nervous System) where audience/performer motions are 'translated' to MIDI segments. Computer controlled music is also found in the performance pieces by the Canadian composer Udo Kasemets such as the Marce(ntennia)l Circus C(ag)elebrating Duchamp (1987), a realization of the Marcel Duchamp process piece "Erratum Musical" using an electric model train to collect a hopper-car of stones to be deposited on a drum wired to an Analog:Digital converter, mapping the stone impacts to a score display (performed in Toronto by pianist Gordon Monahan during the 1987 Duchamp Centennial), or his installations and performance works (e.g. Spectrascapes) based on his Geo(sono)scope (1986) 15x4-channel computer-controlled audio mixer. In these latter works, the computer generates sound-scapes from tape-loop sound samples, live shortwave or sine-wave generators.

Many systems for generating musical scores actually existed well before the time of computers. One of these was Musikalisches Würfelspiel "(Musical dice game"; 18th century), a system which used throws of the dice to randomly select measures from a large collection of small phrases. When patched together, these phrases combined to create musical pieces which could be performed by human players. Although these works were not actually composed with a computer in the modern sense, it uses a rudimentary form of the random combinatorial techniques sometimes used in computer-generated composition.

The world's first digital computer music was generated in Australia by programmer Geoff Hill on the CSIRAC computer which was designed and built by Trevor Pearcey and Maston Beard, although it was only used to play standard tunes of the day. Subsequently, one of the first composers to write music with a computer was Iannis Xenakis. He wrote programs in the FORTRAN language that generated numeric data that he transcribed into scores to be played by traditional musical instruments. An example is "ST/48" of 1962. Although Xenakis could well have composed this music by hand, the intensity of the calculations needed to transform probabilistic mathematics into musical notation was best left to the number-crunching power of the computer.

Computers have also been used in an attempt to imitate the music of great composers of the past, such as Mozart. A present exponent of this technique is David Cope. He wrote computer programs that analyse works of other composers to produce new works in a similar style. He has used this program to great effect with composers such as Bach and Mozart (his program "Experiments in Musical Intelligence" is famous for creating "Mozart's 42nd Symphony"), and also within his own pieces, combining his own creations with that of the computer.

Melomics, a research project from the University of Málaga, Spain, developed a computer composition cluster named Iamus, which composes complex, multi-instrument pieces for editing and performance. Since its inception, Iamus has composed a full album in 2012, appropriately named Iamus, which New Scientist described as "The first major work composed by a computer and performed by a full orchestra." The group has also developed an API for developers to utilize the technology, and makes its music available on its website.

Computer-aided algorithmic composition (CAAC, pronounced "sea-ack") is the implementation and use of algorithmic composition techniques in software. This label is derived from the combination of two labels, each too vague for continued use. The label "computer-aided composition" lacks the specificity of using generative algorithms. Music produced with notation or sequencing software could easily be considered computer-aided composition. The label "algorithmic composition" is likewise too broad, particularly in that it does not specify the use of a computer. The term computer-aided, rather than computer-assisted, is used in the same manner as computer-aided design.

Machine improvisation uses computer algorithms to create improvisation on existing music materials. This is usually done by sophisticated recombination of musical phrases extracted from existing music, either live or pre-recorded. In order to achieve credible improvisation in particular style, machine improvisation uses machine learning and pattern matching algorithms to analyze existing musical examples. The resulting patterns are then used to create new variations "in the style" of the original music, developing a notion of stylistic reinjection.
This is different from other improvisation methods with computers that use algorithmic composition to generate new music without performing analysis of existing music examples.

Style modeling implies building a computational representation of the musical surface that captures important stylistic features from data. Statistical approaches are used to capture the redundancies in terms of pattern dictionaries or repetitions, which are later recombined to generate new musical data. Style mixing can be realized by analysis of a database containing multiple musical examples in different styles. Machine Improvisation builds upon a long musical tradition of statistical modeling that began with Hiller and Isaacson's "Illiac Suite for String Quartet" (1957) and Xenakis' uses of Markov chains and stochastic processes. Modern methods include the use of lossless data compression for incremental parsing, prediction suffix tree and string searching by factor oracle algorithm (basically a "factor oracle" is a finite state automaton constructed in linear time and space in an incremental fashion).

Machine improvisation encourages musical creativity by providing automatic modeling and transformation structures for existing music. This creates a natural interface with the musician without need for coding musical algorithms. In live performance, the system re-injects the musician's material in several different ways, allowing a semantics-level representation of the session and a smart recombination and transformation of this material in real-time. In offline version, machine improvisation can be used to achieve style mixing, an approach inspired by Vannevar Bush's memex imaginary machine.

The first system implementing interactive machine improvisation by means of Markov models and style modeling techniques is the Continuator, developed by François Pachet at Sony CSL Paris in 2002 based on work on non-real time style modeling.
Matlab implementation of the Factor Oracle machine improvisation can be found as part of Computer Audition toolbox.

OMax is a software environment developed in IRCAM. OMax uses OpenMusic and Max. It is based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov and on researches on improvisation with the computer by G. Assayag, M. Chemillier and G. Bloch (a.k.a. the "OMax Brothers") in the Ircam Music Representations group.

Gerard Assayag (IRCAM, France),
Jeremy Baguyos (University of Nebraska at Omaha, US)
Tim Blackwell (Goldsmiths College, Great Britain),
George Bloch (Composer, France),
Marc Chemiller (IRCAM/CNRS, France),
Nick Collins (University of Sussex, UK),
Shlomo Dubnov (Composer, Israel / US),
Mari Kimura (Juilliard, New York City),
Amanuel Zarzowski (Composer Los Angeles/San Diego), 
George Lewis (Columbia University, New York City),
Bernard Lubat (Pianist, France),
François Pachet (Sony CSL, France),
Joel Ryan (Institute of Sonology, Netherlands),
Michel Waisvisz (STEIM, Netherlands),
David Wessel (CNMAT, California),
Michael Young (Goldsmiths College, Great Britain),
Pietro Grossi (CNUCE, Institute of the National Research Council, Pisa, Italy),
Toby Gifford and Andrew Brown (Griffith University, Brisbane, Australia),
Davis Salks (jazz composer, Hamburg, PA, US),
Doug Van Nort (electroacoustic improviser, Montreal/New York)

Live coding (sometimes known as 'interactive programming', 'on-the-fly programming', 'just in time programming') is the name given to the process of writing software in realtime as part of a performance. Recently it has been explored as a more rigorous alternative to laptop musicians who, live coders often feel, lack the charisma and pizzazz of musicians performing live.

Generally, this practice stages a more general approach: one of interactive programming, of writing (parts of) programs while they are interpreted. Traditionally most computer music programs have tended toward the old write/compile/run model which evolved when computers were much less powerful. This approach has locked out code-level innovation by people whose programming skills are more modest. Some programs have gradually integrated real-time controllers and gesturing (for example, MIDI-driven software synthesis and parameter control). Until recently, however, the musician/composer rarely had the capability of real-time modification of program code itself. This legacy distinction is somewhat erased by languages such as ChucK, SuperCollider, and Impromptu.

TOPLAP, an ad-hoc conglomerate of artists interested in live coding was formed in 2004, and promotes the use, proliferation and exploration of a range of software, languages and techniques to implement live coding. This is a parallel and collaborative effort e.g. with research at the Princeton Sound Lab, the University of Cologne, and the Computational Arts Research Group at Queensland University of Technology.




</doc>
<doc id="6978" url="https://en.wikipedia.org/wiki?curid=6978" title="Concept">
Concept

Concepts are the fundamental building blocks of our thoughts and beliefs. They play an important role in all aspects of cognition.

Concepts arise as abstractions or generalisations from experience; from the result of a transformation of existing ideas; or from innate properties. A concept is instantiated (reified) by all of its actual or potential instances, whether these are things in the real world or other ideas.

Concepts are studied as components of human cognition in the cognitive science disciplines of linguistics, psychology and philosophy, where an ongoing debate asks whether all cognition must occur through concepts. Concepts are used as formal tools or models in mathematics, computer science, databases and artificial intelligence where they are sometimes called classes, schema or categories. In informal use the word "concept" often just means any idea.

In metaphysics, and especially ontology, a concept is a fundamental category of existence. In contemporary philosophy, there are at least three prevailing ways to understand what a concept is:


Concepts can be organized into a hierarchy, higher levels of which are termed "superordinate" and lower levels termed "subordinate". Additionally, there is the "basic" or "middle" level at which people will most readily categorize a concept. For example, a basic-level concept would be "chair", with its superordinate, "furniture", and its subordinate, "easy chair".

Within the framework of the representational theory of mind, the structural position of concepts can be understood as follows: Concepts serve as the building blocks of what are called "mental representations" (colloquially understood as "ideas in the mind"). Mental representations, in turn, are the building blocks of what are called "propositional attitudes" (colloquially understood as the stances or perspectives we take towards ideas, be it "believing", "doubting", "wondering", "accepting", etc.). And these propositional attitudes, in turn, are the building blocks of our understanding of thoughts that populate everyday life, as well as folk psychology. In this way, we have an analysis that ties our common everyday understanding of thoughts down to the scientific and philosophical understanding of concepts.

A central question in the study of concepts is the question of what concepts "are". Philosophers construe this question as one about the ontology of concepts – what they are really like. The ontology of concepts determines the answer to other questions, such as how to integrate concepts into a wider theory of the mind, what functions are allowed or disallowed by a concept's ontology, etc. There are two main views of the ontology of concepts: (1) Concepts are abstract objects, and (2) concepts are mental representations.

Platonist views of the mind construe concepts as abstract objects,

There is debate as to the relationship between concepts and natural language. However, it is necessary at least to begin by understanding that the concept "dog" is philosophically distinct from the things in the world grouped by this concept – or the reference class or extension. Concepts that can be equated to a single word are called "lexical concepts".

Study of concepts and conceptual structure falls into the disciplines of linguistics, philosophy, psychology, and cognitive science.

In the simplest terms, a concept is a name or label that regards or treats an abstraction as if it had concrete or material existence, such as a person, a place, or a thing. It may represent a natural object that exists in the real world like a tree, an animal, a stone, etc. It may also name an artificial (man-made) object like a chair, computer, house, etc. Abstract ideas and knowledge domains such as freedom, equality, science, happiness, etc., are also symbolized by concepts. It is important to realize that a concept is merely a symbol, a representation of the abstraction. The word is not to be mistaken for the thing. For example, the word "moon" (a concept) is not the large, bright, shape-changing object up in the sky, but only "represents" that celestial object. Concepts are created (named) to describe, explain and capture reality as it is known and understood.

Kant declared that human minds possess pure or "a priori" concepts. Instead of being abstracted from individual perceptions, like empirical concepts, they originate in the mind itself. He called these concepts categories, in the sense of the word that means predicate, attribute, characteristic, or quality. But these pure categories are predicates of things "in general", not of a particular thing. According to Kant, there are twelve categories that constitute the understanding of phenomenal objects. Each category is that one predicate which is common to multiple empirical concepts. In order to explain how an "a priori" concept can relate to individual phenomena, in a manner analogous to an "a posteriori" concept, Kant employed the technical concept of the schema. He held that the account of the concept as an abstraction of experience is only partly correct. He called those concepts that result from abstraction "a posteriori concepts" (meaning concepts that arise out of experience). An empirical or an "a posteriori" concept is a general representation ("Vorstellung") or non-specific thought of that which is common to several specific perceived objects (Logic, I, 1., §1, Note 1)

A concept is a common feature or characteristic. Kant investigated the way that empirical "a posteriori" concepts are created.
In cognitive linguistics, abstract concepts are transformations of concrete concepts derived from embodied experience. The mechanism of transformation is structural mapping, in which properties of two or more source domains are selectively mapped onto a blended space (Fauconnier & Turner, 1995; see conceptual blending). A common class of blends are metaphors. This theory contrasts with the rationalist view that concepts are perceptions (or "recollections", in Plato's term) of an independently existing world of ideas, in that it denies the existence of any such realm. It also contrasts with the empiricist view that concepts are abstract generalizations of individual experiences, because the contingent and bodily experience is preserved in a concept, and not abstracted away. While the perspective is compatible with Jamesian pragmatism, the notion of the transformation of embodied concepts through structural mapping makes a distinct contribution to the problem of concept formation.

Plato was the starkest proponent of the realist thesis of universal concepts. By his view, concepts (and ideas in general) are innate ideas that were instantiations of a transcendental world of pure forms that lay behind the veil of the physical world. In this way, universals were explained as transcendent objects. Needless to say this form of realism was tied deeply with Plato's ontological projects. This remark on Plato is not of merely historical interest. For example, the view that numbers are Platonic objects was revived by Kurt Gödel as a result of certain puzzles that he took to arise from the phenomenological accounts.

Gottlob Frege, founder of the analytic tradition in philosophy, famously argued for the analysis of language in terms of sense and reference. For him, the sense of an expression in language describes a certain state of affairs in the world, namely, the way that some object is presented. Since many commentators view the notion of sense as identical to the notion of concept, and Frege regards senses as the linguistic representations of states of affairs in the world, it seems to follow that we may understand concepts as the manner in which we grasp the world. Accordingly, concepts (as senses) have an ontological status (Morgolis:7).

According to Carl Benjamin Boyer, in the introduction to his "The History of the Calculus and its Conceptual Development", concepts in calculus do not refer to perceptions. As long as the concepts are useful and mutually compatible, they are accepted on their own. For example, the concepts of the derivative and the integral are not considered to refer to spatial or temporal perceptions of the external world of experience. Neither are they related in any way to mysterious limits in which quantities are on the verge of nascence or evanescence, that is, coming into or going out of existence. The abstract concepts are now considered to be totally autonomous, even though they originated from the process of abstracting or taking away qualities from perceptions until only the common, essential attributes remained.

In a physicalist theory of mind, a concept is a mental representation, which the brain uses to denote a class of things in the world. This is to say that it is literally, a symbol or group of symbols together made from the physical material of the brain. Concepts are mental representations that allow us to draw appropriate inferences about the type of entities we encounter in our everyday lives. Concepts do not encompass all mental representations, but are merely a subset of them. The use of concepts is necessary to cognitive processes such as categorization, memory, decision making, learning, and inference.

Concepts are thought to be stored in long term cortical memory, in contrast to episodic memory of the particular objects and events which they abstract, which are stored in hippocampus. Evidence for this separation comes from hippocampal damaged patients such as patient HM. The abstraction from the day's hippocampal events and objects into cortical concepts is often considered to be the computation underlying (some stages of) sleep and dreaming. Many people (beginning with Aristotle) report memories of dreams which appear to mix the day's events with analogous or related historical concepts and memories, and suggest that they were being sorted or organised into more abstract concepts. ("Sort" is itself another word for concept, and "sorting" thus means to organise into concepts.)

The classical theory of concepts, also referred to as the empiricist theory of concepts, is the oldest theory about the structure of concepts (it can be traced back to Aristotle), and was prominently held until the 1970s. The classical theory of concepts says that concepts have a definitional structure. Adequate definitions of the kind required by this theory usually take the form of a list of features. These features must have two important qualities to provide a comprehensive definition. Features entailed by the definition of a concept must be both "necessary" and "sufficient" for membership in the class of things covered by a particular concept. A feature is considered necessary if every member of the denoted class has that feature. A feature is considered sufficient if something has all the parts required by the definition. For example, the classic example "bachelor" is said to be defined by "unmarried" and "man". An entity is a bachelor (by this definition) if and only if it is both unmarried and a man. To check whether something is a member of the class, you compare its qualities to the features in the definition. Another key part of this theory is that it obeys the "law of the excluded middle", which means that there are no partial members of a class, you are either in or out.

The classical theory persisted for so long unquestioned because it seemed intuitively correct and has great explanatory power. It can explain how concepts would be acquired, how we use them to categorize and how we use the structure of a concept to determine its referent class. In fact, for many years it was one of the major activities in philosophy – concept analysis. Concept analysis is the act of trying to articulate the necessary and sufficient conditions for the membership in the referent class of a concept. For example, Shoemaker's classic "Time Without Change" explored whether the concept of the flow of time can include flows where no changes take place, though change is usually taken as a definition of time.

Given that most later theories of concepts were born out of the rejection of some or all of the classical theory, it seems appropriate to give an account of what might be wrong with this theory. In the 20th century, philosophers such as Wittgenstein and Rosch argued against the classical theory. There are six primary arguments summarized as follows:

Prototype theory came out of problems with the classical view of conceptual structure. Prototype theory says that concepts specify properties that members of a class tend to possess, rather than must possess. Wittgenstein, Rosch, Mervis, Berlin, Anglin, and Posner are a few of the key proponents and creators of this theory. Wittgenstein describes the relationship between members of a class as "family resemblances". There are not necessarily any necessary conditions for membership, a dog can still be a dog with only three legs. This view is particularly supported by psychological experimental evidence for prototypicality effects. Participants willingly and consistently rate objects in categories like 'vegetable' or 'furniture' as more or less typical of that class. It seems that our categories are fuzzy psychologically, and so this structure has explanatory power. We can judge an item's membership to the referent class of a concept by comparing it to the typical member – the most central member of the concept. If it is similar enough in the relevant ways, it will be cognitively admitted as a member of the relevant class of entities. Rosch suggests that every category is represented by a central exemplar which embodies all or the maximum possible number of features of a given category. According to Lech, Gunturkun, and Suchan explain that categorization involves many areas of the brain, some of these are; visual association areas, prefrontal cortex, basal ganglia, and temporal lobe.

Theory-theory is a reaction to the previous two theories and develops them further. This theory postulates that categorization by concepts is something like scientific theorizing. Concepts are not learned in isolation, but rather are learned as a part of our experiences with the world around us. In this sense, concepts' structure relies on their relationships to other concepts as mandated by a particular mental theory about the state of the world. How this is supposed to work is a little less clear than in the previous two theories, but is still a prominent and notable theory. This is supposed to explain some of the issues of ignorance and error that come up in prototype and classical theories as concepts that are structured around each other seem to account for errors such as whale as a fish (this misconception came from an incorrect theory about what a whale is like, combining with our theory of what a fish is). When we learn that a whale is not a fish, we are recognizing that whales don't in fact fit the theory we had about what makes something a fish. In this sense, the Theory–Theory of concepts is responding to some of the issues of prototype theory and classic theory.

According to the theory of ideasthesia (or "sensing concepts"), activation of a concept may be the main mechanism responsible for creation of phenomenal experiences. Therefore, understanding how the brain processes concepts may be central to solving the mystery of how conscious experiences (or qualia) emerge within a physical system e.g., the sourness of the sour taste of lemon. This question is also known as the hard problem of consciousness. Research on ideasthesia emerged from research on synesthesia where it was noted that a synesthetic experience requires first an activation of a concept of the inducer. Later research expanded these results into everyday perception.

There is a lot of discussion on the most effective theory in concepts. Another theory is semantic pointers, which use perceptual and motor representations and these representations are like symbols.

The term "concept" is traced back to 1554–60 (Latin "" – "something conceived"), but what is today termed "the classical theory of concepts" is the theory of Aristotle on the definition of terms.



</doc>
<doc id="6979" url="https://en.wikipedia.org/wiki?curid=6979" title="Cell Cycle">
Cell Cycle

Cell Cycle is a biweekly peer-reviewed scientific journal covering cell biology. The editor-in-chief is Mikhail V. Blagosklonny (Roswell Park Cancer Institute). The journal is abstracted and indexed in Medline/PubMed and the Science Citation Index Expanded.



</doc>
<doc id="6982" url="https://en.wikipedia.org/wiki?curid=6982" title="List of classical music competitions">
List of classical music competitions

European Classical music has long relied on music competitions to provide a public forum that identifies the strongest players and contributes to the establishment of their professional careers. This is a list of current competitions in classical music, with each competition and reference link given only once. Many offer competitions across a range of categories and in these cases they are listed under "General/mixed". Competitions with age restrictions are listed under "Young musicians".



















</doc>
<doc id="6984" url="https://en.wikipedia.org/wiki?curid=6984" title="Colin Powell">
Colin Powell

Colin Luther Powell (; born April 5, 1937) is an American statesman and a retired four-star general in the United States Army. Powell was born in Harlem as the son of Jamaican immigrants. During his military career, Powell also served as National Security Advisor (1987–1989), as Commander of the U.S. Army Forces Command (1989) and as Chairman of the Joint Chiefs of Staff (1989–1993), holding the latter position during the Persian Gulf War. Powell was the first, and so far the only, African American to serve on the Joint Chiefs of Staff. He was the 65th United States Secretary of State, serving under U.S. President George W. Bush from 2001 to 2005, the first African American to serve in that position.

Powell was born in New York City in 1937 and was raised in the South Bronx. His parents, Luther and Maud Powell, immigrated to the United States from Jamaica. Powell was educated in the New York City public schools, graduating from the City College of New York (CCNY), where he earned a bachelor's degree in geology. He also participated in ROTC at CCNY and received a commission as an Army second lieutenant upon graduation in June 1958. His further academic achievements include a Master of Business Administration degree from George Washington University.

Powell was a professional soldier for 35 years, during which time he held myriad command and staff positions and rose to the rank of 4-star General. His last assignment, from October 1, 1989 to September 30, 1993, was as the 12th Chairman of the Joint Chiefs of Staff, the highest military position in the Department of Defense. During this time, he oversaw 28 crises, including Operation Desert Storm in the 1991 Persian Gulf War. He also formulated the Powell Doctrine.

Following his military retirement, Powell wrote his best-selling autobiography, "My American Journey". In addition, he pursued a career as a public speaker, addressing audiences across the country and abroad. Prior to his appointment as Secretary of State, Powell was the chairman of America's Promise - The Alliance for Youth, a national nonprofit organization dedicated to mobilizing people from every sector of American life to build the character and competence of young people. He was nominated by President Bush on December 16, 2000 as Secretary of State. After being unanimously confirmed by the U.S. Senate, he was sworn in as the 65th Secretary of State on January 20, 2001.

Powell is the recipient of numerous U.S. and foreign military awards and decorations. Powell's civilian awards include two Presidential Medal of Freedom, the President's Citizens Medal, the Congressional Gold Medal, the Secretary of State Distinguished Service Medal, and the Secretary of Energy Distinguished Service Medal. Several schools and other institutions have been named in his honor and he holds honorary degrees from universities and colleges across the country. Powell is married to the former Alma Vivian Johnson of Birmingham, Alabama. The Powell family includes son Michael (ex-chairman of the Federal Communications Commission); daughters Linda and Anne; daughter-in-law Jane; and grandsons Jeffrey and Bryan.

In 2016, while not a candidate for that year's election, Powell received three electoral votes for the office of President of the United States.

Powell was born on April 5, 1937, in Harlem, a neighborhood in the New York City borough of Manhattan, to Jamaican immigrant parents Maud Arial (née McKoy) and Luther Theophilus Powell. His parents were both of mixed African and Scots ancestry. Luther worked as a shipping clerk and Maud as a seamstress. Powell was raised in the South Bronx and attended Morris High School, from which he graduated in 1954. (This school has since closed.)

While at school, Powell worked at a local baby furniture store, where he picked up Yiddish from the eastern European Jewish shopkeepers and some of the customers. He also served as a Shabbos goy, helping Orthodox families with needed tasks on the Sabbath. He received a Bachelor of Science degree in Geology from the City College of New York in 1958 and has said he was a 'C average' student. He later earned an MBA degree from the George Washington University in 1971, after his second tour in Vietnam.

Despite his parents' pronunciation of his name as , Powell has pronounced his name since childhood, after the heroic World War II flyer Colin P. Kelly Jr. Public officials and radio and television reporters have used Powell's preferred pronunciation.

Powell was a professional soldier for 35 years, holding a variety of command and staff positions and rising to the rank of General.

Powell described joining the Reserve Officers' Training Corps (ROTC) during college as one of the happiest experiences of his life; discovering something he loved and could do well, he felt he had "found himself." According to Powell: It was only once I was in college, about six months into college when I found something that I liked, and that was ROTC, Reserve Officer Training Corps in the military. And I not only liked it, but I was pretty good at it. That's what you really have to look for in life, something that you like, and something that you think you're pretty good at. And if you can put those two things together, then you're on the right track, and just drive on. Cadet Powell joined the Pershing Rifles, the ROTC fraternal organization and drill team begun by General John Pershing. Even after he had become a general, Powell kept on his desk a pen set he had won for a drill team competition.

Upon graduation, he received a commission as an Army second lieutenant. After attending basic training at Fort Benning, Powell was assigned to the 48th Infantry, in West Germany, as a platoon leader.

In his autobiography, Powell said he is haunted by the nightmare of the Vietnam War and felt that the leadership was very ineffective.

Captain Powell served a tour in Vietnam as a South Vietnamese Army (ARVN) advisor from 1962 to 1963. While on patrol in a Viet Cong-held area, he was wounded by stepping on a punji stake. The large infection made it difficult for him to walk, and caused his foot to swell for a short time, shortening his first tour.

He returned to Vietnam as a major in 1968, serving in the 23rd Infantry Division, then as assistant chief of staff of operations for the Americal Division. During the second tour in Vietnam he was decorated for bravery after he survived a helicopter crash, single-handedly rescuing three others, including division commander Major General Charles M. Gettys, from the burning wreckage.

Powell was charged with investigating a detailed letter by 11th Light Infantry Brigade soldier Tom Glen, which backed up rumored allegations of the My Lai Massacre. He wrote: "In direct refutation of this portrayal is the fact that relations between American soldiers and the Vietnamese people are excellent." Later, Powell's assessment would be described as whitewashing the news of the massacre, and questions would continue to remain undisclosed to the public. In May 2004 Powell said to television and radio host Larry King, "I was in a unit that was responsible for My Lai. I got there after My Lai happened. So, in war, these sorts of horrible things happen every now and again, but they are still to be deplored."

Powell served a White House Fellowship under President Richard Nixon from 1972 to 1973. During 1975–1976 he attended the National War College, Washington, D.C.

In his autobiography, "My American Journey", Powell named several officers he served under who inspired and mentored him. As a lieutenant colonel serving in South Korea, Powell was very close to General Henry "Gunfighter" Emerson. Powell said he regarded Emerson as one of the most caring officers he ever met. Emerson insisted his troops train at night to fight a possible North Korean attack, and made them repeatedly watch the television film "Brian's Song" to promote racial harmony. Powell always professed that what set Emerson apart was his great love of his soldiers and concern for their welfare. After a race riot occurred, in which African American soldiers almost killed a White officer, Powell was charged by Emerson to crack down on black militants; Powell's efforts led to the discharge of one soldier, and other efforts to reduce racial tensions.

In the early 1980s, Powell served at Fort Carson, Colorado. After he left Fort Carson, Powell became senior military assistant to Secretary of Defense Caspar Weinberger, whom he assisted during the 1983 invasion of Grenada and the 1986 airstrike on Libya.
In 1986, Powell took over the command of V Corps in Frankfurt, Germany, from Robert Lewis "Sam" Wetzel.

Following the Iran Contra scandal, Powell became, at the age of 49, Ronald Reagan's National Security Advisor, serving from 1987 to 1989 while retaining his Army commission as a lieutenant general.

In April 1989, after his tenure with the National Security Council, Powell was promoted to four-star general under President George H. W. Bush and briefly served as the Commander in Chief, Forces Command (FORSCOM), headquartered at Fort McPherson, Georgia, overseeing all Army, Army Reserve, and National Guard units in the Continental U.S., Alaska, Hawaii, and Puerto Rico. He became the third general since World War II to reach four-star rank without ever serving as a division commander, joining Dwight D. Eisenhower and Alexander Haig.

Later that year, President George H. W. Bush selected him as Chairman of the Joint Chiefs of Staff.

Powell's last military assignment, from October 1, 1989, to September 30, 1993, was as the 12th Chairman of the Joint Chiefs of Staff, the highest military position in the Department of Defense. At age 52, he became the youngest officer, and first Afro-Caribbean American, to serve in this position. Powell was also the first JCS Chair who received his commission through ROTC.

During this time, he oversaw 28 crises, including the invasion of Panama in 1989 to remove General Manuel Noriega from power and Operation Desert Storm in the 1991 Persian Gulf War. During these events, Powell earned his nickname, "the reluctant warrior." He rarely advocated military intervention as the first solution to an international crisis, and instead usually prescribed diplomacy and containment.

As a military strategist, Powell advocated an approach to military conflicts that maximizes the potential for success and minimizes casualties. A component of this approach is the use of overwhelming force, which he applied to Operation Desert Storm in 1991. His approach has been dubbed the "Powell Doctrine". Powell continued as chairman of the JCS into the Clinton presidency but as a dedicated "realist" he considered himself a bad fit for an administration largely made up of liberal internationalists. He clashed with then-U.S. ambassador to the United Nations Madeleine Albright over the Bosnian crisis, as he opposed any military interventions that didn't involve US interests.

During his chairmanship of the JCS, there was discussion of awarding Powell a fifth star, granting him the rank of General of the Army. But even in the wake of public and Congressional pressure to do so, Clinton-Gore presidential transition team staffers decided against it.

First printed in the August 13, 1989 issue of "Parade" magazine, these are Colin Powell's 13 Rules of Leadership.

Powell's experience in military matters made him a very popular figure with both American political parties. Many Democrats admired his moderate stance on military matters, while many Republicans saw him as a great asset associated with the successes of past Republican administrations. Put forth as a potential Democratic Vice Presidential nominee in the 1992 U.S. presidential election or even potentially replacing Vice President Dan Quayle as the Republican Vice Presidential nominee, Powell eventually declared himself a Republican and began to campaign for Republican candidates in 1995. He was touted as a possible opponent of Bill Clinton in the 1996 U.S. presidential election, possibly capitalizing on a split conservative vote in Iowa and even leading New Hampshire polls for the GOP nomination, but Powell declined, citing a lack of passion for politics. Powell defeated Clinton 50–38 in a hypothetical match-up proposed to voters in the exit polls conducted on Election Day. Despite not standing in the race, Powell won the Republican New Hampshire Vice-Presidential primary on write-in votes.

In 1997 Powell founded America's Promise with the objective of helping children from all socioeconomic sectors. That same year saw the establishment of The Colin L. Powell Center for Leadership and Service. The mission of the Center is to "prepare new generations of publicly engaged leaders from populations previously underrepresented in public service and policy circles, to build a strong culture of civic engagement at City College, and to mobilize campus resources to meet pressing community needs and serve the public good." 

Powell was mentioned as a potential candidate in the 2000 U.S. presidential election, but decided against running. Once Texas Governor George W. Bush secured the Republican nomination, Powell endorsed him for president and spoke at the 2000 Republican National Convention. Bush eventually won, and Powell was appointed Secretary of State.

In the electoral college vote count of 2016, Powell received three votes from faithless electors from Washington.

As Secretary of State in the Bush administration, Powell was perceived as moderate. Powell was unanimously confirmed by the United States Senate. Over the course of his tenure he traveled less than any other U.S. Secretary of State in 30 years.
On September 11, 2001, Powell was in Lima, Peru, meeting with President Alejandro Toledo and US Ambassador John Hamilton, and attending the special session of the OAS General Assembly that subsequently adopted the Inter-American Democratic Charter. After the September 11 attacks, Powell's job became of critical importance in managing America's relationships with foreign countries in order to secure a stable coalition in the War on Terrorism.

Powell came under fire for his role in building the case for the 2003 Invasion of Iraq. In a press statement on February 24, 2001, he had said that sanctions against Iraq had prevented the development of any weapons of mass destruction by Saddam Hussein. As was the case in the days leading up to the Persian Gulf War, Powell was initially opposed to a forcible overthrow of Saddam, preferring to continue a policy of containment. However, Powell eventually agreed to go along with the Bush administration's determination to remove Saddam. He had often clashed with others in the administration, who were reportedly planning an Iraq invasion even before the September 11 attacks, an insight supported by testimony by former terrorism czar Richard Clarke in front of the 9/11 Commission. The main concession Powell wanted before he would offer his full support for the Iraq War was the involvement of the international community in the invasion, as opposed to a unilateral approach. He was also successful in persuading Bush to take the case of Iraq to the United Nations, and in moderating other initiatives. Powell was placed at the forefront of this diplomatic campaign.

Powell's chief role was to garner international support for a multi-national coalition to mount the invasion. To this end, Powell addressed a plenary session of the United Nations Security Council on February 5, 2003, to argue in favor of military action. Citing numerous anonymous Iraqi defectors, Powell asserted that "there can be no doubt that Saddam Hussein has biological weapons and the capability to rapidly produce more, many more." Powell also stated that there was "no doubt in my mind" that Saddam was working to obtain key components to produce nuclear weapons.

Most observers praised Powell's oratorical skills. However, Britain's "Channel 4 News" reported soon afterwards that a UK intelligence dossier that Powell had referred to as a "fine paper" during his presentation had been based on old material and plagiarized an essay by American graduate student Ibrahim al-Marashi.
A 2004 report by the Iraq Survey Group concluded that the evidence that Powell offered to support the allegation that the Iraqi government possessed weapons of mass destruction (WMDs) was inaccurate.

In an interview with Charlie Rose, Powell contended that prior to his UN presentation, he had merely four days to review the data concerning WMD in Iraq.

A Senate report on intelligence failures would later detail the intense debate that went on behind the scenes on what to include in Powell's speech. State Department analysts had found dozens of factual problems in drafts of the speech. Some of the claims were taken out, but others were left in, such as claims based on the yellowcake forgery. The administration came under fire for having acted on faulty intelligence, particularly what was single-sourced to the informant known as Curveball. Powell later recounted how Vice President Dick Cheney had joked with him before he gave the speech, telling him, "You've got high poll ratings; you can afford to lose a few points." Powell's longtime aide-de-camp and Chief of Staff from 1989–2003, Colonel Lawrence Wilkerson, later characterized Cheney's view of Powell's mission as to "go up there and sell it, and we'll have moved forward a peg or two. Fall on your damn sword and kill yourself, and I'll be happy, too."

In September 2005, Powell was asked about the speech during an interview with Barbara Walters and responded that it was a "blot" on his record. He went on to say, "It will always be a part of my record. It was painful. It's painful now."

Wilkerson said that he inadvertently participated in a hoax on the American people in preparing Powell's erroneous testimony before the United Nations Security Council.

Because Powell was seen as more moderate than most figures in the administration, he was spared many of the attacks that have been leveled at more controversial advocates of the invasion, such as Donald Rumsfeld and Paul Wolfowitz. At times, infighting among the Powell-led State Department, the Rumsfeld-led Defense Department, and Cheney's office had the effect of polarizing the administration on crucial issues, such as what actions to take regarding Iran and North Korea.

After Saddam Hussein had been deposed, Powell's new role was to once again establish a working international coalition, this time to assist in the rebuilding of post-war Iraq. On September 13, 2004, Powell testified before the Senate Governmental Affairs Committee, acknowledging that the sources who provided much of the information in his February 2003 UN presentation were "wrong" and that it was "unlikely" that any stockpiles of WMDs would be found. Claiming that he was unaware that some intelligence officials questioned the information prior to his presentation, Powell pushed for reform in the intelligence community, including the creation of a national intelligence director who would assure that "what one person knew, everyone else knew."

Additionally, Powell has been critical of other instances of U.S. foreign policy in the past, such as its support for the 1973 Chilean coup d'état. From two separate interviews in 2003, Powell stated in one about the 1973 event "I can't justify or explain the actions and decisions that were made at that time. It was a different time. There was a great deal of concern about communism in this part of the world. Communism was a threat to the democracies in this part of the world. It was a threat to the United States." In another interview, however, he also simply stated "With respect to your earlier comment about Chile in the 1970s and what happened with Mr. Allende, it is not a part of American history that we're proud of."

Powell announced his resignation as Secretary of State on November 15, 2004. According to "The Washington Post", he had been asked to resign by the president's chief of staff, Andrew Card. Powell announced that he would stay on until the end of Bush's first term or until his replacement's confirmation by Congress. The following day, Bush nominated National Security Advisor Condoleezza Rice as Powell's successor. News of Powell's leaving the Administration spurred mixed reactions from politicians around the world — some upset at the loss of a statesman seen as a moderating factor within the Bush administration, but others hoping for Powell's successor to wield more influence within the cabinet.

In mid-November, Powell stated that he had seen new evidence suggesting that Iran was adapting missiles for a nuclear delivery system. The accusation came at the same time as the settlement of an agreement between Iran, the IAEA, and the European Union.

On December 31, 2004, Powell rang in the New Year by pressing a button in Times Square with New York City Mayor Michael Bloomberg to initiate the ball drop and 60 second countdown, ushering in the year 2005. He appeared on the networks that were broadcasting New Year's Eve specials and talked about this honor, as well as being a native of New York City.

After retiring from the role of Secretary of State, Powell returned to private life. In April 2005, he was privately telephoned by Republican senators Lincoln Chafee and Chuck Hagel, at which time Powell expressed reservations and mixed reviews about the nomination of John R. Bolton as ambassador to the United Nations, but refrained from advising the senators to oppose Bolton (Powell had clashed with Bolton during Bush's first term). The decision was viewed as potentially dealing significant damage to Bolton's chances of confirmation. Bolton was put into the position via a recess appointment because of the strong opposition in the Senate.
On April 28, 2005, an opinion piece in "The Guardian" by Sidney Blumenthal (a former top aide to President Bill Clinton) claimed that Powell was in fact "conducting a campaign" against Bolton because of the acrimonious battles they had had while working together, which among other things had resulted in Powell cutting Bolton out of talks with Iran and Libya after complaints about Bolton's involvement from the British. Blumenthal added that "The foreign relations committee has discovered that Bolton made a highly unusual request and gained access to 10 intercepts by the National Security Agency. Staff members on the committee believe that Bolton was probably spying on Powell, his senior advisors and other officials reporting to him on diplomatic initiatives that Bolton opposed."

In July 2005, Powell joined Kleiner, Perkins, Caufield & Byers, a well-known Silicon Valley venture capital firm, with the title of "strategic limited partner."

In September 2005, Powell criticized the response to Hurricane Katrina. Powell said that thousands of people were not properly protected, but because they were poor rather than because they were black.

On January 5, 2006, he participated in a meeting at the White House of former Secretaries of Defense and State to discuss United States foreign policy with Bush administration officials. In September 2006, Powell sided with more moderate Senate Republicans in supporting more rights for detainees and opposing President Bush's terrorism bill. He backed Senators John Warner, John McCain and Lindsey Graham in their statement that U.S. military and intelligence personnel in future wars will suffer for abuses committed in 2006 by the U.S. in the name of fighting terrorism. Powell stated that "The world is beginning to doubt the moral basis of [America's] fight against terrorism."

Also in 2006, Powell began appearing as a speaker at a series of motivational events called "Get Motivated", along with former New York Mayor Rudy Giuliani. In his speeches for the tour, he openly criticized the Bush Administration on a number of issues. Powell has been the recipient of mild criticism for his role with "Get Motivated" which has been called a "get-rich-quick-without-much-effort, feel-good schemology."

In 2007 he joined the board of directors of Steve Case's new company Revolution Health. Powell also serves on the Council on Foreign Relations Board of directors.

Powell, in honor of Martin Luther King Day, dropped the ceremonial first puck at a New York Islanders ice hockey game at Nassau Coliseum on January 21, 2008. On November 11, 2008, Powell again dropped the puck in recognition of Military Appreciation Day and Veterans Day.

Recently, Powell has encouraged young people to continue to use new technologies to their advantage in the future. In a speech at the Center for Strategic and International Studies to a room of young professionals, he said, "That's your generation...a generation that is hard-wired digital, a generation that understands the power of the information revolution and how it is transforming the world. A generation that you represent, and you're coming together to share; to debate; to decide; to connect with each other." At this event, he encouraged the next generation to involve themselves politically on the upcoming Next America Project, which uses online debate to provide policy recommendations for the upcoming administration.

In 2008, Powell served as a spokesperson for National Mentoring Month, a campaign held each January to recruit volunteer mentors for at-risk youth.

Soon after Barack Obama's 2008 election, Powell began being mentioned as a possible cabinet member. He was not nominated.

In September 2009, Powell advised President Obama against surging US forces in Afghanistan. The president announced the surge the following December.

On March 14, 2014, Salesforce.com announced that Powell had joined its board of directors.

A liberal Republican, Powell is well known for his willingness to support liberal or centrist causes. He is pro-choice regarding abortion, and in favor of "reasonable" gun control. He stated in his autobiography that he supports affirmative action that levels the playing field, without giving a leg up to undeserving persons because of racial issues. Powell was also instrumental in the 1993 implementation of the military's don't ask, don't tell policy, though he later supported its repeal as proposed by Robert Gates and Admiral Mike Mullen in January 2010, saying "circumstances had changed".

The Vietnam War had a profound effect on Powell's views of the proper use of military force. These views are described in detail in the autobiography "My American Journey". The Powell Doctrine, as the views became known, was a central component of U.S. policy in the Persian Gulf War (the first U.S. war in Iraq) and U.S. invasion of Afghanistan (the overthrow of the Taliban regime in Afghanistan following the September 11 attacks). The hallmark of both operations was strong international cooperation, and the use of overwhelming military force.

Powell was the subject of controversy in 2004 when, in a conversation with British Foreign Secretary Jack Straw, he reportedly referred to neoconservatives within the Bush administration as "fucking crazies." In addition to being reported in the press (although the expletive was generally censored in the U.S. press), the quotation was used by James Naughtie in his book, "The Accidental American: Tony Blair and the Presidency", and by Chris Patten in his book, "Cousins and Strangers: America, Britain, and Europe in a New Century".

In a September 2006 letter to Sen. John McCain, General Powell expressed opposition to President Bush's push for military tribunals of those formerly and currently classified as enemy combatants. Specifically, he objected to the effort in Congress to "redefine Common Article 3 of the Geneva Convention." He also asserted: "The world is beginning to doubt the moral basis of our fight against terrorism."

Powell endorsed President Obama in 2008 and again in 2012. When asked why he is still a Republican on Meet the Press he said, "I’m still a Republican. And I think the Republican Party needs me more than the Democratic Party needs me. And you can be a Republican and still feel strongly about issues such as immigration, and improving our education system, and doing something about some of the social problems that exist in our society and our country. I don’t think there's anything inconsistent with this." 

While Powell was wary of a military solution, he supported the decision to invade Iraq after the Bush administration concluded that diplomatic efforts had failed. After his departure from the State Department, Powell repeatedly emphasized his continued support for American involvement in the Iraq War.

At the 2007 Aspen Ideas Festival in Colorado, Powell revealed that he had spent two and a half hours explaining to President Bush "the consequences of going into an Arab country and becoming the occupiers." During this discussion, he insisted that the U.S. appeal to the United Nations first, but if diplomacy failed, he would support the invasion: "I also had to say to him that you are the President, you will have to make the ultimate judgment, and if the judgment is this isn't working and we don't think it is going to solve the problem, then if military action is undertaken I'm with you, I support you."

In a 2008 interview on CNN, Powell reiterated his support for the 2003 decision to invade Iraq in the context of his endorsement of Barack Obama, stating: "My role has been very, very straightforward. I wanted to avoid a war. The president [Bush] agreed with me. We tried to do that. We couldn't get it through the U.N. and when the president made the decision, I supported that decision. And I've never blinked from that. I've never said I didn't support a decision to go to war."

Powell's position on the Iraq War troop surge of 2007 has been less consistent. In December 2006, he expressed skepticism that the strategy would work and whether the U.S. military had enough troops to carry it out successfully. He stated: "I am not persuaded that another surge of troops into Baghdad for the purposes of suppressing this communitarian violence, this civil war, will work." Following his endorsement of Barack Obama in October 2008, however, Powell praised General David Petraeus and U.S. troops, as well as the Iraqi government, concluding that "it's starting to turn around." By mid-2009, he had concluded a surge of U.S. forces in Iraq should have come sooner, perhaps in late 2003. Throughout this period, Powell consistently argued that Iraqi political progress was essential, not just military force.

Powell donated the maximum allowable amount to John McCain's campaign in the summer of 2007 and in early 2008, his name was listed as a possible running mate for Republican nominee McCain's bid during the 2008 U.S. presidential election. However, on October 19, 2008, Powell announced his endorsement of Barack Obama during a "Meet the Press" interview, citing "his ability to inspire, because of the inclusive nature of his campaign, because he is reaching out all across America, because of who he is and his rhetorical abilities", in addition to his "style and substance." He additionally referred to Obama as a "transformational figure". Powell further questioned McCain's judgment in appointing Sarah Palin as the vice presidential candidate, stating that despite the fact that she is admired, "now that we have had a chance to watch her for some seven weeks, I don't believe she's ready to be president of the United States, which is the job of the vice president." He said that Obama's choice for vice-president, Joe Biden, was ready to be president. He also added that he was "troubled" by the "false intimations that Obama was Muslim." Powell stated that "[Obama] is a Christian—he's always been a Christian... But the really right answer is, what if he is? Is there something wrong with being a Muslim in this country? The answer's no, that's not America." Powell then mentioned Kareem Rashad Sultan Khan, a Muslim American soldier in the U.S. Army who served and died in the Iraq War. He later stated, "Over the last seven weeks, the approach of the Republican Party has become narrower and narrower [...] I look at these kind of approaches to the campaign, and they trouble me." Powell concluded his Sunday morning talk show comments, "It isn't easy for me to disappoint Sen. McCain in the way that I have this morning, and I regret that [...] I think we need a transformational figure. I think we need a president who is a generational change and that's why I'm supporting Barack Obama, not out of any lack of respect or admiration for Sen. John McCain." Later in a December 12, 2008, CNN interview with Fareed Zakaria, Powell reiterated his belief that during the last few months of the campaign, Palin pushed the Republican party further to the right and had a polarizing impact on it.

In a July 2009 CNN interview with John King, Powell expressed concern over President Obama growing the size of the federal government and the size of the federal budget deficit. In September 2010, he criticized the Obama administration for not focusing "like a razor blade" on the economy and job creation. Powell reiterated that Obama was a "transformational figure." In a video that aired on CNN.com in November 2011, Colin Powell said in reference to Barack Obama, "many of his decisions have been quite sound. The financial system was put back on a stable basis."

On October 25, 2012, 12 days before the presidential election, he gave his endorsement to President Obama for re-election during a broadcast of CBS This Morning. He cited success and forward progress in foreign and domestic policy arenas under the Obama Administration, and made the following statement: "I voted for him in 2008 and I plan to stick with him in 2012 and I'll be voting for and for Vice President Joe Biden next month."

As additional reason for his endorsement, Powell cited the changing positions and perceived lack of thoughtfulness of Mitt Romney on foreign affairs, and a concern for the validity of Romney's economic plans.

In an interview with ABC's Diane Sawyer and George Stephanopoulos during ABC's coverage of President Obama's second inauguration, Powell criticized members of the Republican Party who "demonize[d] the president". He called on GOP leaders to publicly denounce such talk.

Powell has been very vocal on the state of the Republican party. Speaking at a Washington Ideas forum in early October 2015, he warned the audience that the Republican party had begun a move to the fringe right, lessening the chances of a Republican White House in the future. He also remarked on Republican presidential contender Donald Trump's statements regarding immigrants, noting that there were many immigrants working in Trump hotels.

In March 2016, Powell denounced the "nastiness" of the 2016 Republican primaries during an interview on CBS "This Morning". He compared the race to a "reality show", and stated that the campaign had gone "into the mud".

In August 2016, Powell accused the Clinton campaign of trying to pin Democratic presidential nominee Hillary Clinton's email controversy on him. Speaking to "People" magazine, Powell said, "The truth is, she was using [the private email server] for a year before I sent her a memo telling her what I did."

On September 13, 2016, emails were obtained that revealed Powell's private communications regarding both Donald Trump and Hillary Clinton. Powell privately reiterated his comments regarding Clinton's email scandal, writing, "I have told Hillary's minions repeatedly that they are making a mistake trying to drag me in, yet they still try," and complaining that "Hillary’s mafia keeps trying to suck me into it" in another email. In another email discussing Clinton's controversy, Powell noted that she should have told everyone what she did "two years ago", and said that she has not "been covering herself with glory." Writing on the 2012 Benghazi attack controversy surrounding Clinton, Powell said to then U.S. Ambassador Susan Rice, "Benghazi is a stupid witch hunt." Commenting on Clinton in a general sense, Powell mused that "Everything [Clinton] touches she kind of screws up with hubris", and in another email stated "I would rather not have to vote for her, although she is a friend I respect."

Powell referred to Donald Trump as a "national disgrace", with "no sense of shame". He wrote candidly of Trump's role in the birther movement, which he referred to as "racist". Powell suggested that the media ignore Trump, saying, "To go on and call him an idiot just emboldens him." The emails were obtained by the media as the result of a hack.

Powell endorsed Clinton on October 25, 2016, stating it was "because I think she's qualified, and the other gentleman is not qualified."

Despite not running in the election, Powell received three electoral votes for president from faithless electors in Washington who had pledged to vote for Clinton, coming in third overall. After Barack Obama, Powell was only the second African American to receive electoral votes in a presidential election. He was also the first Republican since 1984 to receive electoral votes from Washington in a presidential election, as well as the first Republican African American to do so.

Powell married Alma Johnson on August 25, 1962. Their son, Michael Powell, was the chairman of the Federal Communications Commission (FCC) from 2001 to 2005. His daughters are Linda Powell, an actress, and Annemarie Powell. As a hobby, Powell restores old Volvo and Saab cars. In 2013, he faced questions about a relationship with a Romanian diplomat, after a hacked AOL email account had been made public. He acknowledged a "very personal" email relationship but denied further involvement.

Powell's civilian awards include two Presidential Medals of Freedom (the second with distinction), the President's Citizens Medal, the Congressional Gold Medal, the Secretary of State Distinguished Service Medal, the Secretary of Energy Distinguished Service Medal, and the Ronald Reagan Freedom Award. Several schools and other institutions have been named in his honor and he holds honorary degrees from universities and colleges across the country.


Azure, two swords in saltire points downwards between four mullets Argent, on a chief of the Second a lion passant Gules. On a wreath of the Liveries is set for Crest the head of an American bald-headed eagle erased Proper. And in an escrol over the same this motto, "DEVOTED TO PUBLIC SERVICE."

The swords and stars refer to the former general's career, as does the crest, which is the badge of the 101st Airborne (which he served as a brigade commander in the mid-1970s). The lion may be an allusion to Scotland. The shield can be shown surrounded by the insignia of an honorary Knight Commander of the Most Honorable Order of the Bath (KCB), an award the General received after the first Gulf War.





</doc>
<doc id="6985" url="https://en.wikipedia.org/wiki?curid=6985" title="Chlorophyll">
Chlorophyll

Chlorophyll (also chlorophyl) is any of several related green pigments found in cyanobacteria and the chloroplasts of algae and plants. Its name is derived from the Greek words χλωρός, "chloros" ("green") and φύλλον, "phyllon" ("leaf"). Chlorophyll is essential in photosynthesis, allowing plants to absorb energy from light.

Chlorophylls absorb light most strongly in the blue portion of the electromagnetic spectrum as well as the red portion. Conversely, it is a poor absorber of green and near-green portions of the spectrum, which it reflects, producing the green color of chlorophyll-containing tissues. Two types of chlorophyll exist in the photosystems of green plants: chlorophyll a and b.

Chlorophyll was first isolated and named by Joseph Bienaimé Caventou and Pierre Joseph Pelletier in 1817.
The presence of magnesium in chlorophyll was discovered in 1906, and was the first time that magnesium had been detected in living tissue.

After initial work done by German chemist Richard Willstätter spanning from 1905 to 1915, the general structure of chlorophyll "a" was elucidated by Hans Fischer in 1940. By 1960, when most of the stereochemistry of chlorophyll "a" was known, Robert Burns Woodward published a total synthesis of the molecule. In 1967, the last remaining stereochemical elucidation was completed by Ian Fleming, and in 1990 Woodward and co-authors published an updated synthesis. Chlorophyll f was announced to be present in cyanobacteria and other oxygenic microorganisms that form stromatolites in 2010; a molecular formula of CHONMg and a structure of (2-formyl)-chlorophyll "a" were deduced based on NMR, optical and mass spectra.

Chlorophyll is vital for photosynthesis, which allows plants to absorb energy from light.

Chlorophyll molecules are arranged in and around photosystems that are embedded in the thylakoid membranes of chloroplasts. In these complexes, chlorophyll serves three functions. The function of the vast majority of chlorophyll (up to several hundred molecules per photosystem) is to absorb light. Having done so, these same centers execute their second function: the transfer of that light energy by resonance energy transfer to a specific chlorophyll pair in the reaction center of the photosystems. This pair effects the final function of chlorophylls, charge separation, leading to biosynthesis.
The two currently accepted photosystem units are photosystem II and photosystem I, which have their own distinct reaction centres, named P680 and P700, respectively. These centres are named after the wavelength (in nanometers) of their red-peak absorption maximum. The identity, function and spectral properties of the types of chlorophyll in each photosystem are distinct and determined by each other and the protein structure surrounding them. Once extracted from the protein into a solvent (such as acetone or methanol), these chlorophyll pigments can be separated into chlorophyll a and chlorophyll b.

The function of the reaction center of chlorophyll is to absorb light energy and transfer it to other parts of the photosystem. The absorbed energy of the photon is transferred to an electron in a process called charge separation. The removal of the electron from the chlorophyll is an oxidation reaction. The chlorophyll donates the high energy electron to a series of molecular intermediates called an electron transport chain. The charged reaction center of chlorophyll (P680) is then reduced back to its ground state by accepting an electron stripped from water. The electron that reduces P680 ultimately comes from the oxidation of water into O and H through several intermediates. This reaction is how photosynthetic organisms such as plants produce O gas, and is the source for practically all the O in Earth's atmosphere. Photosystem I typically works in series with Photosystem II; thus the P700 of Photosystem I is usually reduced as it accepts the electron, via many intermediates in the thylakoid membrane, by electrons coming, ultimately, from Photosystem II. Electron transfer reactions in the thylakoid membranes are complex, however, and the source of electrons used to reduce P700 can vary.

The electron flow produced by the reaction center chlorophyll pigments is used to pump H ions across the thylakoid membrane, setting up a chemiosmotic potential used mainly in the production of ATP (stored chemical energy) or to reduce NADP to NADPH. NADPH is a universal agent used to reduce CO into sugars as well as other biosynthetic reactions.

Reaction center chlorophyll–protein complexes are capable of directly absorbing light and performing charge separation events without the assistance of other chlorophyll pigments, but the probability of that happening under a given light intensity is small. Thus, the other chlorophylls in the photosystem and antenna pigment proteins all cooperatively absorb and funnel light energy to the reaction center. Besides chlorophyll "a", there are other pigments, called accessory pigments, which occur in these pigment–protein antenna complexes.

Chlorophylls are numerous types, but all are defined by the presence of a fifth ring beyond the four pyrrole-like rings. Most chlorophylls are classified as chlorins, which are reduced relatives to porphyrins (found in hemoglobin). They share a common biosynthetic pathway as porphyrins, including the precursor uroporphyrinogen III. Unlike hemes, which feature iron at the center of the tetrapyrrole ring, chlorophylls bind magnesium. For the structures depicted in this article, some of the ligands attached to the Mg center are omitted for clarity. The chlorin ring can have various side chains, usually including a long phytol chain. The most widely distributed form in terrestrial plants is chlorophyll "a".

The structures of chlorophylls are summarized below:

When leaves degreen in the process of plant senescence, chlorophyll is converted to a group of colourless tetrapyrroles known as nonfluorescent chlorophyll catabolites (NCC's) with the general structure:

These compounds have also been identified in several ripening fruits.

Measurement of the absorption of light is complicated by the solvent used to extract the chlorophyll from plant material, which affects the values obtained,

By measuring the absorption of light in the red and far red regions, it is possible to estimate the concentration of chlorophyll within a leaf.

Ratio fluorescence emission can be used to measure chlorophyll content. By exciting chlorophyll “a” fluorescence at a lower wavelength, the ratio of chlorophyll fluorescence emission at 705 nm +/- 10 nm and 735 nm +/-10 nm can provide a linear relationship of chlorophyll content when compared to chemical testing. The ratio F735/F700 provided a correlation value of r 0.96 compared to chemical testing in the range from 41 mg m up to 675 mg m. Gitelson also developed a formula for direct readout of chlorophyll content in mg m. The formula provided a reliable method of measuring chlorophyll content from 41 mg m up to 675 mg m with a correlation r value of 0.95.

In plants, chlorophyll may be synthesized from succinyl-CoA and glycine, although the immediate precursor to chlorophyll "a" and "b" is protochlorophyllide. In Angiosperm plants, the last step, the conversion of protochlorophyllide to chlorophyll, is light-dependent and such plants are pale (etiolated) if grown in darkness. Non-vascular plants and green algae have an additional light-independent enzyme and grow green even in darkness.

Chlorophyll itself is bound to proteins and can transfer the absorbed energy in the required direction. Protochlorophyllide occurs mostly in the free form and, under light conditions, acts as a photosensitizer, forming highly toxic free radicals. Hence, plants need an efficient mechanism of regulating the amount of chlorophyll precursor. In angiosperms, this is done at the step of aminolevulinic acid (ALA), one of the intermediate compounds in the biosynthesis pathway. Plants that are fed by ALA accumulate high and toxic levels of protochlorophyllide; so do the mutants with the damaged regulatory system.
Chlorosis is a condition in which leaves produce insufficient chlorophyll, turning them yellow. Chlorosis can be caused by a nutrient deficiency of iron — called iron chlorosis — or by a shortage of magnesium or nitrogen. Soil pH sometimes plays a role in nutrient-caused chlorosis; many plants are adapted to grow in soils with specific pH levels and their ability to absorb nutrients from the soil can be dependent on this. Chlorosis can also be caused by pathogens including viruses, bacteria and fungal infections, or sap-sucking insects.

Anthocyanins are other plant pigments. The absorbance pattern responsible for the red color of anthocyanins may be complementary to that of green chlorophyll in photosynthetically active tissues such as young "Quercus coccifera" leaves. It may protect the leaves from attacks by plant eaters that may be attracted by green color.

The chlorophyll maps show milligrams of chlorophyll per cubic meter of seawater each month. Places where chlorophyll amounts were very low, indicating very low numbers of phytoplankton, are blue. Places where chlorophyll concentrations were high, meaning many phytoplankton were growing, are yellow. The observations come from the Moderate Resolution Imaging Spectroradiometer (MODIS) on NASA's Aqua satellite. Land is dark gray, and places where MODIS could not collect data because of sea ice, polar darkness, or clouds are light gray.The highest chlorophyll concentrations, where tiny surface-dwelling ocean plants are thriving, are in cold polar waters or in places where ocean currents bring cold water to the surface, such as around the equator and along the shores of continents. It is not the cold water itself that stimulates the phytoplankton. Instead, the cool temperatures are often a sign that the water has welled up to the surface from deeper in the ocean, carrying nutrients that have built up over time. In polar waters, nutrients accumulate in surface waters during the dark winter months when plants cannot grow. When sunlight returns in the spring and summer, the plants flourish in high concentrations.

Chlorophyll is registered as a food additive (colorant), and its E number is E140. Chefs use chlorophyll to color a variety of foods and beverages green, such as pasta and absinthe. Chlorophyll is not soluble in water, and it is first mixed with a small quantity of vegetable oil to obtain the desired solution.



</doc>
<doc id="6986" url="https://en.wikipedia.org/wiki?curid=6986" title="Carotene">
Carotene

The term carotene (also carotin, from the Latin "carota", "carrot") is used for many related unsaturated hydrocarbon substances having the formula CH, which are synthesized by plants but in general cannot be made by animals (with the exception of some aphids and spider mites which acquired the synthesizing genes from fungi). Carotenes are photosynthetic pigments important for photosynthesis. Carotenes contain no oxygen atoms. They absorb ultraviolet, violet, and blue light and scatter orange or red light, and (in low concentrations) yellow light.

Carotenes are responsible for the orange colour of the carrot, for which this class of chemicals is named, and for the colours of many other fruits, vegetables and fungi (for example, sweet potatoes, chanterelle and orange cantaloupe melon). Carotenes are also responsible for the orange (but not all of the yellow) colours in dry foliage. They also (in lower concentrations) impart the yellow coloration to milk-fat and butter. Omnivorous animal species which are relatively poor converters of coloured dietary carotenoids to colourless retinoids have yellowed-coloured body fat, as a result of the carotenoid retention from the vegetable portion of their diet. The typical yellow-coloured fat of humans and chickens is a result of fat storage of carotenes from their diets.

Carotenes contribute to photosynthesis by transmitting the light energy they absorb to chlorophyll. They also protect plant tissues by helping to absorb the energy from singlet oxygen, an excited form of the oxygen molecule O which is formed during photosynthesis.

β-Carotene is composed of two retinyl groups, and is broken down in the mucosa of the human small intestine by β-carotene 15,15'-monooxygenase to retinal, a form of vitamin A. β-Carotene can be stored in the liver and body fat and converted to retinal as needed, thus making it a form of vitamin A for humans and some other mammals. The carotenes α-carotene and γ-carotene, due to their single retinyl group (β-ionone ring), also have some vitamin A activity (though less than β-carotene), as does the xanthophyll carotenoid β-cryptoxanthin. All other carotenoids, including lycopene, have no beta-ring and thus no vitamin A activity (although they may have antioxidant activity and thus biological activity in other ways).

Animal species differ greatly in their ability to convert retinyl (beta-ionone) containing carotenoids to retinals. Carnivores in general are poor converters of dietary ionone-containing carotenoids. Pure carnivores such as ferrets lack β-carotene 15,15'-monooxygenase and cannot convert any carotenoids to retinals at all (resulting in carotenes not being a form of vitamin A for this species); while cats can convert a trace of β-carotene to retinol, although the amount is totally insufficient for meeting their daily retinol needs.

Chemically, carotenes are polyunsaturated hydrocarbons containing 40 carbon atoms per molecule, variable numbers of hydrogen atoms, and no other elements. Some carotenes are terminated by hydrocarbon rings, on one or both ends of the molecule. All are coloured to the human eye, due to extensive systems of conjugated double bonds. Structurally carotenes are tetraterpenes, meaning that they are synthesized biochemically from four 10-carbon terpene units, which in turn are formed from eight 5-carbon isoprene units.

Carotenes are found in plants in two primary forms designated by characters from the Greek alphabet: alpha-carotene (α-carotene) and beta-carotene (β-carotene). Gamma-, delta-, epsilon-, and zeta-carotene (γ, δ, ε, and ζ-carotene) also exist. Since they are hydrocarbons, and therefore contain no oxygen, carotenes are fat-soluble and insoluble in water (in contrast with other carotenoids, the xanthophylls, which contain oxygen and thus are less chemically hydrophobic).

The following foods are particularly rich in carotenes (also see Vitamin A article for amounts):


Absorption from these foods is enhanced if eaten with fats, as carotenes are fat soluble, and if the food is cooked for a few minutes until the plant cell wall splits and the colour is released into any liquid. 6 μg of dietary β-carotene supplies the equivalent of 1 μg of retinol, or 1 RE (Retinol Equivalent). This is equivalent to 3⅓ IU of vitamin A.

The two primary isomers of carotene, α-carotene and β-carotene, differ in the position of a double bond (and thus a hydrogen) in the cyclic group at one end (the right end in the diagram at right).

β-Carotene is the more common form and can be found in yellow, orange, and green leafy fruits and vegetables. As a rule of thumb, the greater the intensity of the orange colour of the fruit or vegetable, the more β-carotene it contains.

Carotene protects plant cells against the destructive effects of ultraviolet light. β-Carotene is an antioxidant.

An article on the American Cancer Society says that The Cancer Research Campaign has called for warning labels on β-carotene supplements to caution smokers that such supplements may increase the risk of lung cancer.

The New England Journal of Medicine published an article in 1994 about a trial which examined the relationship between daily supplementation of β-carotene and vitamin E (α-tocopherol) and the incidence of lung cancer. The study was done using supplements and researchers were aware of the epidemiological correlation between carotenoid-rich fruits and vegetables and lower lung cancer rates. The research concluded that no reduction in lung cancer was found in the participants using these supplements, and furthermore, these supplements may, in fact, have harmful effects.

The Journal of the National Cancer Institute and The New England Journal of Medicine published articles in 1996 about a trial with a goal to determine if vitamin A (in the form of retinyl palmitate) and β-carotene (at about 30 mg/day, which is 10 times the Reference Daily Intake) supplements had any beneficial effects to prevent cancer. The results indicated an "increased" risk of lung and prostate cancers for the participants who consumed the β-carotene supplement and who had lung irritation from smoking or asbestos exposure, causing the trial to be stopped early.

A review of all randomized controlled trials in the scientific literature by the Cochrane Collaboration published in "JAMA" in 2007 found that synthetic β-carotene "increased" mortality by 1-8 % (Relative Risk 1.05, 95% confidence interval 1.01–1.08). However, this meta-analysis included two large studies of smokers, so it is not clear that the results apply to the general population. The review only studied the influence of synthetic antioxidants and the results should not be translated to potential effects of fruits and vegetables.

A recent report demonstrated that 50 mg of β-carotene every other day prevented cognitive decline in a study of over 4000 physicians at a mean treatment duration of 18 years.

Oral β-carotene is prescribed to people suffering from erythropoietic protoporphyria. It provides them some relief from photosensitivity.

Carotenemia or hypercarotenemia is excess carotene, but unlike excess vitamin A, carotene is non-toxic. Although hypercarotenemia is not particularly dangerous, it can lead to an oranging of the skin (carotenodermia), but not the conjunctiva of eyes (thus easily distinguishing it visually from jaundice). It is most commonly associated with consumption of an abundance of carrots, but it also can be a medical sign of more dangerous conditions.

β-Carotene and lycopene molecules can be encapsulated into carbon nanotubes enhancing the optical properties of carbon nanotubes. Efficient energy transfer occurs between the encapsulated dye and nanotube — light is absorbed by the dye and without significant loss is transferred to the single wall carbon nanotube (SWCNT). Encapsulation increases chemical and thermal stability of carotene molecules; it also allows their isolation and individual characterization.

Most of the world's synthetic supply of carotene comes from a manufacturing complex located in Freeport, Texas and owned by DSM. The other major supplier BASF also uses a chemical process to produce β-carotene. Together these suppliers account for about 85% of the β-carotene on the market. In Spain Vitatene produces natural β-carotene from fungus Blakeslea trispora, as does DSM but at much lower amount when compared to its synthetic β-carotene operation. In Australia, organic β-carotene is produced by Aquacarotene Limited from dried marine algae "Dunaliella salina" grown in harvesting ponds situated in Karratha, Western Australia. BASF Australia is also producing β-carotene from microalgae grown in two sites in Australia that are the world’s largest algae farms. In Portugal, the industrial biotechnology company Biotrend is producing natural all-"trans"-β-carotene from a non genetically modified bacteria of the "Sphingomonas" genus isolated from soil.

Carotenes are also found in palm oil, corn, and in the milk of dairy cows, causing cow's milk to be light yellow, depending on the feed of the cattle, and the amount of fat in the milk (high-fat milks, such as those produced by Guernsey cows, tend to be yellower because their fat content causes them to contain more carotene).

Carotenes are also found in some species of termites, where they apparently have been picked up from the diet of the insects.

There are currently two commonly used methods of total synthesis of β-carotene. The first was developed by the Badische Anilin- & Soda-Fabrik (BASF) and is based on the Wittig reaction with Wittig himself as patent holder:

The second is a Grignard reaction, elaborated by Hoffman-La Roche from the original synthesis of Inhoffen et al. They are both symmetrical; the BASF synthesis is C20 + C20, and the Hoffman-La Roche synthesis is C19 + C2 + C19.

Carotenes are carotenoids containing no oxygen. Carotenoids containing some oxygen are known as xanthophylls.

The two ends of the β-carotene molecule are structurally identical, and are called β-rings. Specifically, the group of nine carbon atoms at each end form a β-ring.

The α-carotene molecule has a β-ring at one end; the other end is called an ε-ring. There is no such thing as an "α-ring".

These and similar names for the ends of the carotenoid molecules form the basis of a systematic naming scheme, according to which:

ζ-Carotene is the biosynthetic precursor of neurosporene, which is the precursor of lycopene, which, in turn, is the precursor of the carotenes α through ε.

Carotene is also used as a substance to colour products such as juice, cakes, desserts, butter and margarine. It is approved for use as a food additive in the EU (listed as additive E160a) Australia and New Zealand (listed as 160a) and the US.




</doc>
<doc id="6988" url="https://en.wikipedia.org/wiki?curid=6988" title="Cyclic adenosine monophosphate">
Cyclic adenosine monophosphate

Cyclic adenosine monophosphate (cAMP, cyclic AMP, or 3',5'-cyclic adenosine monophosphate) is a second messenger important in many biological processes. cAMP is a derivative of adenosine triphosphate (ATP) and used for intracellular signal transduction in many different organisms, conveying the cAMP-dependent pathway. It should not be confused with 5'-AMP-activated protein kinase (AMP-activated protein kinase).

Earl Sutherland of Vanderbilt University won a Nobel Prize in Physiology or Medicine in 1971 "for his discoveries concerning the mechanisms of the action of hormones", especially epinephrine, via second messengers (such as cyclic adenosine monophosphate, cyclic AMP).

Cyclic AMP is synthesized from ATP by adenylate cyclase located on the inner side of the plasma membrane and anchored at various locations in the interior of the cell. Adenylate cyclase is "activated" by a range of signaling molecules through the activation of adenylate cyclase stimulatory G (G)-protein-coupled receptors. Adenylate cyclase is "inhibited" by agonists of adenylate cyclase inhibitory G (G)-protein-coupled receptors. Liver adenylate cyclase responds more strongly to glucagon, and muscle adenylate cyclase responds more strongly to adrenaline.

cAMP decomposition into AMP is catalyzed by the enzyme phosphodiesterase.

cAMP is a second messenger, used for intracellular signal transduction, such as transferring into cells the effects of hormones like glucagon and adrenaline, which cannot pass through the plasma membrane. It is also involved in the activation of protein kinases. In addition, cAMP binds to and regulates the function of ion channels such as the HCN channels and a few other cyclic nucleotide-binding proteins such as Epac1 and RAPGEF2.

cAMP and its associated kinases function in several biochemical processes, including the regulation of glycogen, sugar, and lipid metabolism.

In eukaryotes, cyclic AMP works by activating protein kinase A (PKA, or cAMP-dependent protein kinase). PKA is normally inactive as a tetrameric holoenzyme, consisting of two catalytic and two regulatory units (CR), with the regulatory units blocking the catalytic centers of the catalytic units.

Cyclic AMP binds to specific locations on the regulatory units of the protein kinase, and causes dissociation between the regulatory and catalytic subunits, thus enabling those catalytic units to phosphorylate substrate proteins.

The active subunits catalyze the transfer of phosphate from ATP to specific serine or threonine residues of protein substrates. The phosphorylated proteins may act directly on the cell's ion channels, or may become activated or inhibited enzymes. Protein kinase A can also phosphorylate specific proteins that bind to promoter regions of DNA, causing increases in transcription. Not all protein kinases respond to cAMP. Several classes of protein kinases, including protein kinase C, are not cAMP-dependent.

Further effects mainly depend on cAMP-dependent protein kinase, which vary based on the type of cell.

Still, there are some minor PKA-independent functions of cAMP, e.g., activation of calcium channels, providing a minor pathway by which growth hormone-releasing hormone causes a release of growth hormone.

However, the view that the majority of the effects of cAMP are controlled by PKA is an outdated one. In 1998 a family of cAMP-sensitive proteins with guanine nucleotide exchange factor (GEF) activity was discovered. These are termed Exchange proteins activated by cAMP (Epac) and the family comprises Epac1 and Epac2. The mechanism of activation is similar to that of PKA: the GEF domain is usually masked by the N-terminal region containing the cAMP binding domain. When cAMP binds, the domain dissociates and exposes the now-active GEF domain, allowing Epac to activate small Ras-like GTPase proteins, such as Rap1.

In the species "Dictyostelium discoideum", cAMP acts outside the cell as a secreted signal. The chemotactic aggregation of cells is organized by periodic waves of cAMP that propagate between cells over distances as large as several centimetres. The waves are the result of a regulated production and secretion of extracellular cAMP and a spontaneous biological oscillator that initiates the waves at centers of territories.

In bacteria, the level of cAMP varies depending on the medium used for growth. In particular, cAMP is low when glucose is the carbon source. This occurs through inhibition of the cAMP-producing enzyme, adenylate cyclase, as a side-effect of glucose transport into the cell. The transcription factor cAMP receptor protein (CRP) also called CAP (catabolite gene activator protein) forms a complex with cAMP and thereby is activated to bind to DNA. CRP-cAMP increases expression of a large number of genes, including some encoding enzymes that can supply energy independent of glucose.

cAMP, for example, is involved in the positive regulation of the lac operon. In an environment with a low glucose concentration, cAMP accumulates and binds to the allosteric site on CRP (cAMP receptor protein), a transcription activator protein. The protein assumes its active shape and binds to a specific site upstream of the lac promoter, making it easier for RNA polymerase to bind to the adjacent promoter to start transcription of the lac operon, increasing the rate of lac operon transcription. With a high glucose concentration, the cAMP concentration decreases, and the CRP disengages from the lac operon.

Some research has suggested that a deregulation of cAMP pathways and an aberrant activation of cAMP-controlled genes is linked to the growth of some cancers.

Recent research suggests that cAMP affects the function of higher-order thinking in the prefrontal cortex through its regulation of ion channels called hyperpolarization-activated cyclic nucleotide-gated channels (HCN). When cAMP stimulates the HCN, the channels open, closing the brain cell to communication and thus interfering with the function of the prefrontal cortex. This research, especially the cognitive deficits in age-related illnesses and ADHD, is of interest to researchers studying the brain.



</doc>
<doc id="6991" url="https://en.wikipedia.org/wiki?curid=6991" title="Cimabue">
Cimabue

Cimabue (; ; 1240 – 1302), also known as Cenni di Pepo or Cenni di Pepi, was an Italian painter and designer of mosaics from Florence.

Although heavily influenced by Byzantine models, Cimabue is generally regarded as one of the first great Italian painters to break from the Italo-Byzantine style. While medieval art then was scenes and forms that appeared relatively flat and highly stylized, Cimabue's figures were depicted with more-advanced lifelike proportions and shading than other artists of his time. According to Italian painter and historian Giorgio Vasari, Cimabue was the teacher of Giotto, the first great artist of the Italian Proto-Renaissance. However, many scholars today tend to discount Vasari's claim by citing earlier sources that suggest otherwise.

Little is known about Cimabue's early life. One source that recounts his career is Vasari's "Lives of the Most Excellent Painters, Sculptors, and Architects", but its accuracy is uncertain. 

He was born in Florence and died in Pisa. Hayden Maginnis speculates he could have trained in Florence under masters who were culturally connected to Byzantine art. Many scholars now discount Vasari's claim that he later had Giotto as his pupil; they cite earlier sources that suggest otherwise.

Italian art historian Pietro Toesca attributed the "Crucifixion" in the church of San Domenico in Arezzo to Cimabue, dating around 1270, making it the earliest known attributed work that departs from the Byzantine style. Cimabue's Christ is bent, and the clothes have the golden striations that were introduced by Coppo di Marcovaldo.

Around 1272, Cimabue is documented as being present in Rome, and a little later he made another "Crucifix" for the Florentine church of Santa Croce. Now restored, having been damaged by the 1966 Arno River flood, the work was larger and more advanced than the one in Arezzo, with traces of naturalism perhaps inspired by the works of Nicola Pisano. In the same period (c. 1280), Cimabue painted the "Maestà", originally displayed in the church of San Francesco at Pisa, but now at the Louvre. This work established a style that was followed subsequently by numerous artists, including Duccio di Buoninsegna in his "Rucellai Madonna" (in the past, wrongly attributed to Cimabue) as well as Giotto. Other works from the period, which were said to have heavily influenced Giotto, include a "Flagellation" (Frick Collection), mosaics for the Baptistery of Florence (now largely restored), the "Maestà" at the Santa Maria dei Servi in Bologna and the "Madonna" in the Pinacoteca of Castelfiorentino. A workshop painting, perhaps assignable to a slightly later period, is the "Maestà with Saints Francis and Dominic" currently housed in the Uffizi.

During the pontificate of Pope Nicholas IV, the first Franciscan pope, Cimabue worked in Assisi. At Assisi, in the transept of the Lower Basilica of San Francesco, he created a fresco named "Madonna with Child Enthroned, Four Angels and St Francis". The left portion of this fresco is lost, but it may have shown St Anthony of Padua (the authorship of the painting has been recently disputed for technical and stylistic reasons). Cimabue was subsequently commissioned to decorate the apse and the transept of the Upper Basilica of Assisi, in the same period of time that Roman artists were decorating the nave. The cycle he created there comprises scenes from the Gospels, the lives of the Virgin Mary, St Peter and St Paul. The paintings are now in poor condition because of oxidation of the brighter colours that were used by the artist.

The "Maestà of Santa Trinita", dated to c. 1290–1300, which was originally painted for the church of Santa Trinita in Florence, is now in the Uffizi Gallery. The softer expression of the characters suggests that it was influenced by Giotto, who was by then already active as a painter.

Cimabue spent the last period of his life, 1301 to 1302, in Pisa. There, he was commissioned to finish a mosaic of Christ Enthroned, originally begun by Maestro Francesco, in the apse of the city's cathedral. Cimabue was to create the part of the mosaic depicting St John the Evangelist, which remains the sole surviving work documented as being by the artist. Cimabue died around 1302.

According to Vasari, quoting a contemporary of Cimabue, "Cimabue of Florence was a painter who lived during the author's own time, a nobler man than anyone knew but he was as a result so haughty and proud that if someone pointed out to him any mistake or defect in his work, or if he had noted any himself... he would immediately destroy the work, no matter how precious it might be." 

His nickname, Cenni di Pepo, translates as ‘bull-head’ but also possibly as ‘one who crushes the views of others’, from the Latin word "cimare", meaning top, shear, and blunt. The conclusion for the second meaning is drawn from similar commentaries on Dante, who was also known "for being contemptuous of criticism".

History has long regarded Cimabue as the last of an era that was overshadowed by the Italian Renaissance. As early as 1543, Vasari wrote of Cimabue, "Cimabue was, in one sense, the principal cause of the renewal of painting," with the qualification that, "Giotto truly eclipsed Cimabue's fame just as a great light eclipses a much smaller one."

In Canto XI of his "Purgatorio", Dante laments Cimabue's quick loss of public interest in the face of Giotto's revolution in art:
O vanity of human powers,
how briefly lasts the crowning green of glory,
unless an age of darkness follows!
In painting Cimabue thought he held the field
but now it's Giotto has the cry,
so that the other's fame is dimmed.



</doc>
<doc id="6997" url="https://en.wikipedia.org/wiki?curid=6997" title="Corporatocracy">
Corporatocracy

Corporatocracy , a portmanteau of corporate and -ocracy (form of government), short form corpocracy, is a recent term used to refer to an economic and political system controlled by corporations or corporate interests. It is most often used today as a term to describe the current economic situation in a particular country, especially the United States. This is different from corporatism, which is the organisation of society into groups with common interests. Corporatocracy as a term is often used by observers across the political spectrum.

Economist Jeffrey Sachs described the United States as a corporatocracy in "The Price of Civilization" (2011). He suggested that it arose from four trends: weak national parties and strong political representation of individual districts, the large U.S. military establishment after World War II, large corporations using money to finance election campaigns, and globalization tilting the balance of power away from workers.

This collective is what author C Wright Mills in 1956 called the 'power elite', wealthy individuals who hold prominent positions in corporatocracies. They control the process of determining a society's economic and political policies.

The concept has been used in explanations of bank bailouts, excessive pay for CEOs, as well as complaints such as the exploitation of national treasuries, people, and natural resources. It has been used by critics of globalization, sometimes in conjunction with criticism of the World Bank or unfair lending practices, as well as criticism of "free trade agreements".

Edmund Phelps published an analysis in 2010 theorizing that the cause of income inequality is not free market capitalism, but instead is the result of the rise of corporatization. Corporatization, in his view, is the antithesis of free market capitalism. It is characterized by semi-monopolistic organizations and banks, big employer confederations, often acting with complicit state institutions in ways that discourage (or block) the natural workings of a free economy. The primary effects of corporatization are the consolidation of economic power and wealth, with end results being the attrition of entrepreneurial and free market dynamism.

His follow-up book, "Mass Flourishing", further defines corporatization by the following attributes: power-sharing between government and large corporations (exemplified in the U.S. by widening government power in areas such as financial services, healthcare, energy, law enforcement/prison systems, and the military through regulation and outsourcing), an expansion of corporate lobbying and campaign support in exchange for government reciprocity, escalation in the growth and influence of financial and banking sectors, increased consolidation of the corporate landscape through merger and acquisition (with ensuing increases in corporate executive compensation), increased potential for corporate/government corruption and malfeasance, and a lack of entrepreneurial and small business development leading to lethargic and stagnant economic conditions.

In the United States, several of the characteristics described by Phelps are apparent. With regard to income inequality, the 2014 income analysis of University of California, Berkeley economist Emmanuel Saez confirms that relative growth of income and wealth is not occurring among small and mid-sized entrepreneurs and business owners (who generally populate the lower half of top one per-centers in income), but instead only among the top .1 percent of income distribution ... whom Paul Krugman describes as "super-elites – corporate bigwigs and financial wheeler-dealers."... who earn $2,000,000 or more every year.

Corporate power can also increase income inequality. Joseph Stiglitz wrote in May 2011: "Much of today’s inequality is due to manipulation of the financial system, enabled by changes in the rules that have been bought and paid for by the financial industry itself—one of its best investments ever. The government lent money to financial institutions at close to zero percent interest and provided generous bailouts on favorable terms when all else failed. Regulators turned a blind eye to a lack of transparency and to conflicts of interest." Stiglitz explained that the top 1% got nearly "one-quarter" of the income and own approximately 40% of the wealth.

Measured relative to GDP, total compensation and its component wages and salaries have been declining since 1970. This indicates a shift in income from labor (persons who derive income from hourly wages and salaries) to capital (persons who derive income via ownership of businesses, land and assets). Wages and salaries have fallen from approximately 51% GDP in 1970 to 43% GDP in 2013. Total compensation has fallen from approximately 58% GDP in 1970 to 53% GDP in 2013.

To put this in perspective, five percent of U.S. GDP was approximately $850 billion in 2013. This represents an additional $7,000 in compensation for each of the 120 million U.S. households. Larry Summers estimated in 2007 that the lower 80% of families were receiving $664 billion less income than they would be with a 1979 income distribution (a period of much greater equality), or approximately $7,000 per family.

Not receiving this income may have led many families to increase their debt burden, a significant factor in the 2007–2009 subprime mortgage crisis, as highly leveraged homeowners suffered a much larger reduction in their net worth during the crisis. Further, since lower income families tend to spend relatively more of their income than higher income families, shifting more of the income to wealthier families may slow economic growth.

As another indication of U.S. corporate political influence, U.S. corporate effective tax rates have also fallen significantly, from 29% in 2000 to 17% in 2013. Corporate tax payments have not kept pace with profit growth.

Some large U.S. corporations have used a strategy called tax inversion to change their headquarters to a non-U.S. country to reduce their tax liability. About 46 companies have reincorporated in low-tax countries since 1982, including 15 since 2012. Six more also planned to do so in 2015.

One indication of increasing corporate power was the removal of restrictions on their ability to buy back stock, contributing to increased income inequality. Writing in the "Harvard Business Review" in September 2014, William Lazonick blamed record corporate stock buybacks for reduced investment in the economy and a corresponding impact on prosperity and income inequality. Between 2003 and 2012, the 449 companies in the S&P 500 used 54% of their earnings ($2.4 trillion) to buy back their own stock. An additional 37% was paid to stockholders as dividends. Together, these were 91% of profits. This left little for investment in productive capabilities or higher income for employees, shifting more income to capital rather than labor. He blamed executive compensation arrangements, which are heavily based on stock options, stock awards and bonuses for meeting earnings per share (EPS) targets. EPS increases as the number of outstanding shares decreases. Legal restrictions on buybacks were greatly eased in the early 1980s. He advocates changing these incentives to limit buybacks.

In the 12 months to March 31, 2014, S&P 500 companies increased their stock buyback payouts by 29% year on year, to $534.9 billion. U.S. companies are projected to increase buybacks to $701 billion in 2015 according to Goldman Sachs, an 18% increase over 2014. For scale, annual non-residential fixed investment (a proxy for business investment and a major GDP component) was estimated to be about $2.1 trillion for 2014.

Brid Brennan of the Transnational Institute explained how concentration of corporations increases their influence over government: ”It’s not just their size, their enormous wealth and assets that make the TNCs [transnational corporations] dangerous to democracy. It’s also their concentration, their capacity to influence, and often infiltrate, governments and their ability to act as a genuine international social class in order to defend their commercial interests against the common good. It is such decision making power as well as the power to impose deregulation over the past 30 years, resulting in changes to national constitutions, and to national and international legislation which has created the environment for corporate crime and impunity."

An example of such industry concentration is in banking. The top 5 U.S. banks had approximately 30% of the U.S. banking assets in 1998; this rose to 45% by 2008 and to 48% by 2010, before falling to 47% in 2011.

The Economist also explained how an increasingly profitable corporate financial and banking sector caused Gini coefficients to rise in the U.S. since 1980: "Financial services' share of GDP in America doubled to 8% between 1980 and 2000; over the same period their profits rose from about 10% to 35% of total corporate profits, before collapsing in 2007–09. Bankers are being paid more, too. In America the compensation of workers in financial services was similar to average compensation until 1980. Now it is twice that average."

The summary argument, considering these findings, is that if corporatization is the consolidation and sharing of economic and political power between large corporations and the state ... then a corresponding concentration of income and wealth (with resulting income inequality) is an expected by-product of such a consolidation.

Corporations have significant influence on the regulations and regulators that monitor them. For example, Senator Elizabeth Warren explained in December 2014 how an omnibus spending bill required to fund the government was modified late in the process to weaken banking regulations. The modification made it easier to allow taxpayer-funded bailouts of banking "swaps entities", which the Dodd-Frank banking regulations prohibited. She singled out Citigroup, one of the largest banks, which had a role in modifying the legislation. She also explained how both Wall Street bankers and members of the government that formerly had worked on Wall Street stopped bi-partisan legislation that would have broken up the largest banks. She repeated President Theodore Roosevelt's warnings regarding powerful corporate entities that threatened the "very foundations of Democracy."

Several companies that typify corporatocracy power structures are listed below by incorporation date:






Corporations have held the right to vote in some jurisdictions. For example, Livery Companies currently appoint most of the voters for the City of London Corporation, which is the municipal government for the area centered on the financial district.





</doc>
<doc id="6999" url="https://en.wikipedia.org/wiki?curid=6999" title="Culture of Canada">
Culture of Canada

The culture of Canada embodies the artistic, culinary, literary, humour, musical, political and social elements that are representative of Canada and Canadians. Throughout Canada's history, its culture has been influenced by European culture and traditions, especially British and French, and by its own indigenous cultures. Over time, elements of the cultures of Canada's immigrant populations have become incorporated into mainstream Canadian culture. The population has also been influenced by American culture because of a shared language, proximity, television and migration between the two countries.

Canada is often characterized as being "very progressive, diverse, and multicultural". Canada's federal government has often been described as the instigator of multicultural ideology because of its public emphasis on the social importance of immigration. Canada's culture draws from its broad range of constituent nationalities, and policies that promote a just society are constitutionally protected. Canadian Government policies—such as publicly funded health care; higher and more progressive taxation; outlawing capital punishment; strong efforts to eliminate poverty; an emphasis on cultural diversity; strict gun control; and most recently, legalizing same-sex marriage—are social indicators of Canada's political and cultural values. Canadians identify with the country's institutions of health care, military peacekeeping, the National park system and the Canadian Charter of Rights and Freedoms.

The Canadian government has influenced culture with programs, laws and institutions. It has created crown corporations to promote Canadian culture through media, such as the Canadian Broadcasting Corporation (CBC) and the National Film Board of Canada (NFB), and promotes many events which it considers to promote Canadian traditions. It has also tried to protect Canadian culture by setting legal minimums on Canadian content in many media using bodies like the Canadian Radio-television and Telecommunications Commission (CRTC).

For thousands of years Canada has been inhabited by indigenous peoples from a variety of different cultures and of several major linguistic groupings. Although not without conflict and bloodshed, early European interactions with First Nations and Inuit populations in what is now Canada were arguably peaceful. First Nations and Métis peoples played a critical part in the development of European colonies in Canada, particularly for their role in assisting European coureur des bois and voyageurs in the exploration of the continent during the North American fur trade. Combined with late economic development in many regions, this comparably nonbelligerent early history allowed indigenous Canadians to have a lasting influence on the national culture (see: The Canadian Crown and Aboriginal peoples). Over the course of three centuries, countless North American Indigenous words, inventions, concepts, and games have become an everyday part of Canadian language and use. Many places in Canada, both natural features and human habitations, use indigenous names. The name "Canada" itself derives from the St. Lawrence Iroquoian word meaning "village" or "settlement". The name of Canada's capital city Ottawa comes from the Algonquin language term "adawe" meaning "to trade".

The French originally settled New France along the shores of the Atlantic Ocean and Saint Lawrence River during the early part of the 17th century. Themes and symbols of pioneers, trappers, and traders played an important part in the early development of French Canadian culture. The British conquest of New France during the mid-18th century brought 70,000 Francophones under British rule, creating a need for compromise and accommodation. The migration of 40,000 to 50,000 United Empire Loyalists from the Thirteen Colonies during the American Revolution (1775–1783) brought American colonial influences. Following the War of 1812 a large wave of Irish, Scottish and English settlers arrived in Upper Canada and Lower Canada.

The Canadian Forces and overall civilian participation in the First World War and Second World War helped to foster Canadian nationalism; however, in 1917 and 1944, conscription crises highlighted the considerable rift along ethnic lines between Anglophones and Francophones. As a result of the First and Second World Wars, the Government of Canada became more assertive and less deferential to British authority. Canada until the 1940s saw itself in terms of English and French cultural, linguistic and political identities, and to some extent aboriginal.

Legislative restrictions on immigration (such as the Continuous journey regulation and Chinese Immigration Act) that had favoured British, American and other European immigrants (such as Dutch, German, Italian, Polish, Swedish and Ukrainian) were amended during the 1960s, resulting in an influx of diverse people from Asia, Africa, and the Caribbean. By the end of the 20th century, immigrants were increasingly Chinese, Indian, Vietnamese, Jamaican, Filipino, Lebanese and Haitian. As of 2006, Canada has grown to have thirty four ethnic groups with at least one hundred thousand members each, of which eleven have over 1,000,000 people and numerous others are represented in smaller numbers. 16.2% of the population self identify as a visible minority. The Canadian public as-well as the major political parties support immigration.

Canada has also evolved to be religiously and linguistically diverse, encompassing a wide range of dialects, beliefs and customs. The 2011 Canadian census reported a population count of 33,121,175 individuals of whom 67.3% identify as being Christians; of these, Catholics make up the largest group, accounting for 38.7 percent of the population. The largest Protestant denomination is the United Church of Canada (accounting for 6.1% of Canadians), followed by Anglicans (5.0%), and Baptists (1.9%). About 23.9% of Canadians declare no religious affiliation, including agnostics, atheists, humanists, and other groups. The remaining are affiliated with non-Christian religions, the largest of which is Islam (3.2%), followed by Hinduism (1.5%), Sikhism (1.4%) Buddhism (1.1%) and Judaism (1.0%). English and French are the first languages of approximately 60% and 20% of the population; however in 2011, nearly 6.8 million Canadians listed a non-official language as their mother tongue. Some of the most common non-official first languages include Chinese (mainly Cantonese with 1,072,555 first-language speakers); Punjabi (430,705); Spanish (410,670); German (409,200); and Italian (407,490).

French Canada's early development was relatively cohesive during the 17th and 18th centuries, and this was preserved by the Quebec Act of 1774, which allowed Roman Catholics to hold offices and practice their faith. In 1867, the Constitution Act was thought to meet the growing calls for Canadian autonomy while avoiding the overly strong decentralization that contributed to the Civil War in the United States. The compromises reached during this time between the English- and French-speaking Fathers of Confederation set Canada on a path to bilingualism which in turn contributed to an acceptance of diversity. The English and French languages have had limited constitutional protection since 1867 and full official status since 1969. Section 133 of the Constitution Act of 1867 (BNA Act) guarantees that both languages may be used in the Parliament of Canada. Canada adopted its first Official Languages Act in 1969, giving English and French equal status in the government of Canada. Doing so makes them "official" languages, having preferred status in law over all other languages used in Canada.

Prior to the advent of the Canadian Bill of Rights in 1960 and its successor the Canadian Charter of Rights and Freedoms in 1982, the laws of Canada did not provide much in the way of civil rights and this issue was typically of limited concern to the courts. Canada since the 1960s has placed emphasis on equality and inclusiveness for all people. For example, in 1995, the Supreme Court of Canada ruled in "Egan v. Canada" that sexual orientation should be "read in" to Section Fifteen of the Canadian Charter of Rights and Freedoms, a part of the Constitution of Canada guaranteeing equal rights to all Canadians. Following a series of decisions by provincial courts and the Supreme Court of Canada, on July 20, 2005, the Civil Marriage Act (Bill C-38) received Royal Assent, legalizing same-sex marriage in Canada. Canada thus became the fourth country to officially sanction same-sex marriage worldwide, after The Netherlands, Belgium, and Spain. Furthermore, sexual orientation was included as a protected status in the human-rights laws of the federal government and of all provinces and territories.

Today, Canada has a diverse makeup of ethnicities and nationalities and constitutional protection for policies that promote multiculturalism rather than cultural assimilation or a single national myth. In Quebec, cultural identity is strong, and many French-speaking commentators speak of a Quebec culture as distinguished from English Canadian culture and other French Canadian cultures. However, as a whole, Canada is in theory, a cultural mosaic—a collection of several regional, aboriginal, and ethnic subcultures. Celtic (Scottish and Irish) influences have allowed survival of non-English dialects in Nova Scotia and Newfoundland; Canada's Pacific trade has also brought a large Chinese influence into British Columbia and other areas. Multiculturalism in Canada was adopted as the official policy of the Canadian government during the prime ministership of Pierre Trudeau, and is enshrined in Section 27 of the Canadian Charter of Rights and Freedoms. In parts of Canada, especially the major cities of Montreal, Vancouver, Ottawa and Toronto, multiculturalism itself is the cultural norm in many urban communities.

Canadian values are the commonly shared ethical and human values of Canada. The major political parties have claimed explicitly that they uphold these values, but use generalities to specify them. Justin Trudeau after taking office as Prime Minister in 2015 tried to redefine what it means to be Canadian, saying that Canada lacks a core identity but does have shared values:

Numerous scholars, beginning with Seymour Martin Lipset in the 1940s, have tried to identify, measure and compare them with other countries, especially the United States. However, there are critics who say that such a task is practically impossible.

Canada's large geographic size, the presence of a significant number of indigenous peoples, the conquest of one European linguistic population by another and relatively open immigration policy have led to an extremely diverse society. As a result, the issue of Canadian identity remains under scrutiny. Journalist and author Richard Gwyn has suggested that "tolerance" has replaced "loyalty" as the touchstone of Canadian identity. Journalist and professor Andrew Cohen wrote in 2007:
The question of Canadian identity was traditionally dominated by three fundamental themes: first, the often conflicted relations between English Canadians and French Canadians stemming from the French Canadian imperative for cultural and linguistic survival; secondly, the generally close ties between English Canadians and the British Empire, resulting in a gradual political process towards complete independence from the imperial power; and finally, the close proximity of English-speaking Canadians to the United States. In the 20th century, immigrants from African, Caribbean and Asian nationalities have shaped the Canadian identity, a process that continues today with the ongoing arrival of large numbers of immigrants from non-British or non-French backgrounds, adding the theme of multiculturalism to the debate. Much of the debate over contemporary Canadian identity is argued in political terms, and defines Canada as a country defined by its government policies, which are thought to reflect deeper cultural values.

In general, Canadian nationalists are highly concerned about the protection of Canadian sovereignty and loyalty to the Canadian State, placing them in the civic nationalist category. It has likewise often been suggested that anti-Americanism plays a prominent role in Canadian nationalist ideologies. A unified, bi-cultural, tolerant and sovereign Canada remains an ideological inspiration to many Canadian nationalists. Alternatively French Canadian nationalism and support for maintaining French Canadian culture would inspire Quebec nationalists, many of whom were supporters of the Quebec sovereignty movement during the late-20th century.

Cultural protectionism in Canada has, since the mid-20th century, taken the form of conscious, interventionist attempts on the part of various Canadian governments to promote Canadian cultural production. Sharing a large border and (for the majority) a common language with the United States, Canada faces a difficult position in regard to American culture, be it direct attempts at the Canadian market or the general diffusion of American culture in the globalized media arena. While Canada tries to maintain its cultural differences, it also must balance this with responsibility in trade arrangements such as the General Agreement on Tariffs and Trade (GATT) and the North American Free Trade Agreement (NAFTA).

Official symbols of Canada include the maple leaf, beaver, and the Canadian Horse. Many official symbols of the country such as the Flag of Canada have been changed or modified over the past few decades to 'Canadianize' them and de-emphasise or remove references to the United Kingdom. Other prominent symbols include the Canada goose, common loon and more recently, the totem pole and inuksuk. Symbols of the Canadian monarchy continue to be featured in, for example, the Arms of Canada, the armed forces, and the prefix Her Majesty's Canadian Ship. The designation "Royal" remains for institutions as varied as the Royal Canadian Mounted Police and the Royal Winnipeg Ballet. During unification of the forces in the 1960s, a renaming of the branches took place, resulting in the abandonment of "royal designations" of the navy and air force. On August 16, 2011, the Government of Canada announced that the name "Air Command" was re-assuming the air force's original historic name, Royal Canadian Air Force; "Land Command" was re-assuming the name Canadian Army; and "Maritime Command" was re-assuming the name Royal Canadian Navy. These name changes were made to better reflect Canada's military heritage and align Canada with other key Commonwealth of Nations whose militaries use the royal designation.

Canadian humour is an integral part of the Canadian Identity. There are several traditions in Canadian humour in both English and French. While these traditions are distinct and at times very different, there are common themes that relate to Canadians' shared history and geopolitical situation in the Western Hemisphere and the world. Various trends can be noted in Canadian comedy. One trend is the portrayal of a "typical" Canadian family in an ongoing radio or television series. Other trends include outright absurdity, and political and cultural satire. Irony, parody, satire, and self-deprecation are arguably the primary characteristics of Canadian humour.

The beginnings of Canadian radio comedy date to the late 1930s with the debut of The Happy Gang, a long-running weekly variety show that was regularly sprinkled with corny jokes in between tunes. Canadian television comedy begins with Wayne and Shuster, a sketch comedy duo who performed as a comedy team during the Second World War, and moved their act to radio in 1946 before moving on to television. Second City Television, otherwise known as SCTV, Royal Canadian Air Farce, This Hour Has 22 Minutes, The Kids in the Hall and more recently Trailer Park Boys are regarded as television shows which were very influential on the development of Canadian humour. Canadian comedians have had great success in the film industry and are amongst the most recognized in the world.

Humber College in Toronto and the École nationale de l'humour in Montreal offer post-secondary programmes in comedy writing and performance. Montreal is also home to the bilingual (English and French) Just for Laughs festival and to the Just for Laughs Museum, a bilingual, international museum of comedy. Canada has a national television channel, The Comedy Network, devoted to comedy. Many Canadian cities feature comedy clubs and showcases, most notable, The Second City branch in Toronto (originally housed at The Old Fire Hall) and the Yuk Yuk's national chain. The Canadian Comedy Awards were founded in 1999 by the Canadian Comedy Foundation for Excellence, a not-for-profit organization.

Indigenous artists were producing art in the territory that is now called Canada for thousands of years prior to the arrival of European settler colonists and the eventual establishment of Canada as a nation state. Like the peoples that produced them, indigenous art traditions spanned territories that extended across the current national boundaries between Canada and the United States. The majority of indigenous artworks preserved in museum collections date from the period after European contact and show evidence of the creative adoption and adaptation of European trade goods such as metal and glass beads. Canadian sculpture has been enriched by the walrus ivory, muskox horn and caribou antler and soapstone carvings by the Inuit artists. These carvings show objects and activities from the daily life, myths and legends of the Inuit. Inuit art since the 1950s has been the traditional gift given to foreign dignitaries by the Canadian government.

The works of most early Canadian painters followed European trends. During the mid-19th century, Cornelius Krieghoff, a Dutch-born artist in Quebec, painted scenes of the life of the "habitants" (French-Canadian farmers). At about the same time, the Canadian artist Paul Kane painted pictures of indigenous life in western Canada. A group of landscape painters called the Group of Seven developed the first distinctly Canadian style of painting. All these artists painted large, brilliantly colored scenes of the Canadian wilderness.

Since the 1930s, Canadian painters have developed a wide range of highly individual styles. Emily Carr became famous for her paintings of totem poles in British Columbia. Other noted painters have included the landscape artist David Milne, the painters Jean-Paul Riopelle, Harold Town and Charles Carson and multi-media artist Michael Snow. The abstract art group Painters Eleven, particularly the artists William Ronald and Jack Bush, also had an important impact on modern art in Canada. Government support has played a vital role in their development enabling visual exposure through publications and periodicals featuring Canadian art, as has the establishment of numerous art schools and colleges across the country.

Canadian literature is often divided into French- and English-language literatures, which are rooted in the literary traditions of France and Britain, respectively. Canada’s early literature, whether written in English or French, often reflects the Canadian perspective on nature, frontier life, and Canada’s position in the world, for example the poetry of Bliss Carman or the memoirs of Susanna Moodie and Catherine Parr Traill. These themes, and Canada's literary history, inform the writing of successive generations of Canadian authors, from Leonard Cohen to Margaret Atwood.

By the mid-20th century, Canadian writers were exploring national themes for Canadian readers. Authors were trying to find a distinctly Canadian voice, rather than merely emulating British or American writers. Canadian identity is closely tied to its literature. The question of national identity recurs as a theme in much of Canada's literature, from Hugh MacLennan's "Two Solitudes" (1945) to Alistair MacLeod's "No Great Mischief" (1999). Canadian literature is often categorized by region or province; by the socio-cultural origins of the author (for example, Acadians, indigenous peoples, LGBT, and Irish Canadians); and by literary period, such as "Canadian postmoderns" or "Canadian Poets Between the Wars."

Canadian authors have accumulated numerous international awards. In 1992, Michael Ondaatje became the first Canadian to win the Man Booker Prize for "The English Patient". Margaret Atwood won the Booker in 2000 for "The Blind Assassin" and Yann Martel won it in 2002 for the "Life of Pi". Carol Shields's "The Stone Diaries" won the Governor General's Awards in Canada in 1993, the 1995 Pulitzer Prize for Fiction, and the 1994 National Book Critics Circle Award. In 2013, Alice Munro was the first Canadian to be awarded the Nobel Prize in Literature for her work as "master of the modern short story". Munro is also a recipient of the Man Booker International Prize for her lifetime body of work, and three-time winner of Canada's Governor General's Award for fiction.

Canada has had a thriving stage theatre scene since the late 1800s. Theatre festivals draw many tourists in the summer months, especially the Stratford Shakespeare Festival in Stratford, Ontario, and the Shaw Festival in Niagara-on-the-Lake, Ontario. The Famous People Players are only one of many touring companies that have also developed an international reputation. Canada also hosts one of the largest fringe festival the Edmonton International Fringe Festival.
Canada's largest cities host a variety of modern and historical venues. The Toronto Theatre District is Canada's largest, as well as being the third largest English-speaking theatre district in the world. In addition to original Canadian works, shows from the West End and Broadway frequently tour in Toronto. Toronto's Theatre District includes the venerable Roy Thomson Hall; the Princess of Wales Theatre; the Tim Sims Playhouse; The Second City; the Canon Theatre; the Panasonic Theatre; the Royal Alexandra Theatre; historic Massey Hall; and the city's new opera house, the Sony Centre for the Performing Arts. Toronto's Theatre District also includes the Theatre Museum Canada.

Montreal's theatre district ("Quartier des Spectacles") is the scene of performances that are mainly French-language, although the city also boasts a lively anglophone theatre scene, such as the Centaur Theatre. Large French theatres in the city include Theatre Saint-Denis, Theatre du Nouveau Monde, and EXcentris.

Quebec City is the permanent home to the world renowned Robert LePage theatre, opera and experimental artist.

Vancouver is host to, among others, the Vancouver Fringe Festival, the Arts Club Theatre Company, Carousel Theatre, Bard on the Beach, Theatre Under the Stars and Studio 58. It also home of Vancouver Theatresports League, the improvisational theatre company, world-known for providing an impetus for the present worldwide interest in theatresports at Expo in 1986.

Calgary is home to Theatre Calgary, a mainstream regional theatre; Alberta Theatre Projects, a major centre for new play development in Canada; the Calgary Animated Objects Society; and One Yellow Rabbit, a touring company.

There are three major theatre venues in Ottawa; the Ottawa Little Theatre, originally called the Ottawa Drama League at its inception in 1913, is the longest-running community theatre company in Ottawa. Since 1969, Ottawa has been the home of the National Arts Centre, a major performing-arts venue that houses four stages and is home to the National Arts Centre Orchestra, the Ottawa Symphony Orchestra and Opera Lyra Ottawa. Established in 1975, the Great Canadian Theatre Company specializes in the production of Canadian plays at a local level.

Canadian television, especially supported by the Canadian Broadcasting Corporation, is the home of a variety of locally produced shows. French-language television, like French Canadian film, is buffered from excessive American influence by the fact of language, and likewise supports a host of home-grown productions. The success of French-language domestic television in Canada often exceeds that of its English-language counterpart. In recent years nationalism has been used to prompt products on television. The "I Am Canadian" campaign by Molson beer, most notably the commercial featuring Joe Canadian, infused domestically brewed beer and nationalism.

Canada's television industry is in full expansion as a site for Hollywood productions. Since the 1980s, Canada, and Vancouver in particular, has become known as Hollywood North. The American TV series "Queer as Folk" was filmed in Toronto. Canadian producers have been very successful in the field of science fiction since the mid-1990s, with such shows as "The X-Files", "Stargate SG-1", "", the new "Battlestar Galactica", "My Babysitter's A Vampire", "Smallville", and "The Outer Limits", all filmed in Vancouver.

The CRTC's Canadian content regulations dictate that a certain percentage of a domestic broadcaster's transmission time must include content that is produced by Canadians, or covers Canadian subjects. These regulations also apply to US cable television channels such as MTV and the Discovery Channel, which have local versions of their channels available on Canadian cable networks. Similarly, BBC Canada, while showing primarily BBC shows from the United Kingdom, also carries Canadian output.

A number of Canadian pioneers in early Hollywood significantly contributed to the creation of the motion picture industry in the early days of the 20th century. Over the years, many Canadians have made enormous contributions to the American entertainment industry, although they are frequently not recognized as Canadians.

Canada has developed a vigorous film industry that has produced a variety of well-known films, actors, and auteurs. In fact, this eclipsing may sometimes be creditable for the bizarre and innovative directions of some works, such as auteurs Atom Egoyan ("The Sweet Hereafter", 1997) and David Cronenberg ("The Fly", "Naked Lunch", "A History of Violence") and the "avant-garde" work of Michael Snow and Jack Chambers. Also, the distinct French-Canadian society permits the work of directors such as Denys Arcand and Denis Villeneuve, while First Nations cinema includes the likes of "". At the 76th Academy Awards, Arcand's "The Barbarian Invasions" became Canada's first film to win the Academy Award for Best Foreign Language Film. James Cameron is a very successful Canadian filmmaker, having been nominated for and received many Academy Awards.

The National Film Board of Canada is 'a public agency that produces and distributes films and other audiovisual works which reflect Canada to Canadians and the rest of the world'. Canada has produced many popular documentaries such as "The Corporation", "Nanook of the North", "Final Offer", and "". The Toronto International Film Festival (TIFF) is considered by many to be one of the most prevalent film festivals for Western cinema. It is the première film festival in North America from which the Oscars race begins.

The music of Canada has reflected the multi-cultural influences that have shaped the country. Indigenous, the French, and the British have all made historical contributions to the musical heritage of Canada. The country has produced its own composers, musicians and ensembles since the mid-1600s. From the 17th century onward, Canada has developed a music infrastructure that includes church halls; chamber halls; conservatories; academies; performing arts centres; record companys; radio stations, and television music-video channels. The music has subsequently been heavily influenced by American culture because of its proximity and migration between the two countries. Canadian rock has had a considerable impact on the development of modern popular music and the development of the most popular subgenres.

Patriotic music in Canada date back over 200 years as a distinct category from British patriotism, preceding the first legal steps to independence by over 50 years. The earliest known song, "The Bold Canadian", was written in 1812. The national anthem of Canada, "O Canada" adopted in 1980, was originally commissioned by the Lieutenant Governor of Quebec, the Honourable Théodore Robitaille, for the 1880 St. Jean-Baptiste Day ceremony. Calixa Lavallée wrote the music, which was a setting of a patriotic poem composed by the poet and judge Sir Adolphe-Basile Routhier. The text was originally only in French, before English lyrics were written in 1906.

Music broadcasting in the country is regulated by the Canadian Radio-television and Telecommunications Commission (CRTC). The Canadian Academy of Recording Arts and Sciences presents Canada's music industry awards, the Juno Awards, which were first awarded in a ceremony during the summer of 1970.

Canada has one of the largest video-game industries in terms of employment numbers, right behind the USA and Japan, with 16,000 employees, 348 companies, and a direct annual economic impact of nearly $2 billion. Canada has grown from a minor player in the video-games industry to a major industry player. In part, this prominence is made possible by a large pool of university-educated talent and a high quality of life, but favourable government policies towards digital media companies also play a role in making Canada an attractive location for game development studios.

Canada has a well-developed media sector, but its cultural output—particularly in English films, television shows, and magazines—is often overshadowed by imports from the United States. Television, magazines, and newspapers are primarily for-profit corporations based on advertising, subscription, and other sales-related revenues. Nevertheless, both the television broadcasting and publications sectors require a number of government interventions to remain profitable, ranging from regulation that bars foreign companies in the broadcasting industry to tax laws that limit foreign competition in magazine advertising.

The promotion of multicultural media in Canada began in the late 1980s as the multicultural policy was legislated in 1988. In the Multiculturalism Act, the federal government proclaimed the recognition of the diversity of Canadian culture. Thus, multicultural media became an integral part of Canadian media overall. Upon numerous government reports showing lack of minority representation or minority misrepresentation, the Canadian government stressed separate provision be made to allow minorities and ethnicities of Canada to have their own voice in the media.

Sports in Canada consists of a variety of games. Although there are many contests that Canadians value, the most common are ice hockey, box lacrosse, Canadian football, basketball, soccer, curling, baseball and ringette. All but curling and soccer are considered domestic sports as they were either invented by Canadians or trace their roots to Canada.

Ice hockey, referred to as simply "hockey", is Canada's most prevalent winter sport, its most popular spectator sport, and its most successful sport in international competition. It is Canada's official national winter sport. Lacrosse, a sport with indigenous origins, is Canada's oldest and official summer sport. Canadian football is Canada's second most popular spectator sport, and the Canadian Football League's annual championship, the Grey Cup, is the country's largest annual sports event.

While other sports have a larger spectator base, association football, known in Canada as "soccer" in both English and French, has the most registered players of any team sport in Canada, and is the most played sport with all demographics, including ethnic origin, ages and genders. Professional teams exist in many cities in Canada — with a trio of teams in North America's top pro league, Major League Soccer — and international soccer competitions such as the FIFA World Cup, UEFA Euro and the UEFA Champions League attract some of the biggest audiences in Canada. Other popular team sports include curling, street hockey, cricket, rugby league, rugby union, softball and Ultimate frisbee. Popular individual sports include auto racing, boxing, karate, kickboxing, hunting, Sport shooting, fishing, cycling, golf, hiking, horse racing, ice skating, skiing, snowboarding, swimming, triathlon, disc golf, water sports, and several forms of wrestling.

As a country with a generally cool climate, Canada has enjoyed greater success at the Winter Olympics than at the Summer Olympics, although significant regional variations in climate allow for a wide variety of both team and individual sports. Great achievements in Canadian sports are recognized by Canada's Sports Hall of Fame, while the Lou Marsh Trophy is awarded annually to Canada's top athlete by a panel of journalists. There are numerous other Sports Halls of Fame in Canada.

Canadian cuisine varies widely depending on the region. The former Canadian prime minister Joe Clark has been paraphrased to have noted: "Canada has a cuisine of cuisines. Not a stew pot, but a smorgasbord." There are considerable overlaps between Canadian food and the rest of the cuisine in North America, many unique dishes (or versions of certain dishes) are found and available only in the country. Common contenders for the Canadian national food include poutine and butter tarts. Other popular Canadian made foods include indigenous fried bread bannock, French tourtière, Kraft Dinner, ketchup chips, date squares, nanaimo bars, back bacon, and the caesar cocktail. Canada is the birthplace and world's largest producer of maple syrup.

The three earliest cuisines of Canada have First Nations, English, and French roots, with the traditional cuisine of English Canada closely related to British and American cuisine, while the traditional cuisine of French Canada has evolved from French cuisine and the winter provisions of fur traders. With subsequent waves of immigration in the 18th and 19th century from Central, Southern, and Eastern Europe, and then from Asia, Africa and Caribbean, the regional cuisines were subsequently augmented. The Jewish immigrants to Canada during the late 1800s also play a significant role to foods in Canada. The Montreal-style bagel and Montreal-style smoked meat are both food items originally developed by Jewish communities living in Montreal.

In a 2002 interview with the Globe and Mail, Aga Khan, the 49th Imam of the Ismaili Muslims, described Canada as "the most successful pluralist society on the face of our globe", citing it as "a model for the world". A 2007 poll ranked Canada as the country with the most positive influence in the world. 28,000 people in 27 countries were asked to rate 12 countries as either having a positive or negative worldwide influence. Canada’s overall influence rating topped the list with 54 per cent of respondents rating it mostly positive and only 14 per cent mostly negative. A global opinion poll for the BBC saw Canada ranked the second most positively viewed nation in the world (behind Germany) in 2013 and 2014.

The United States is home to a number of perceptions about Canadian culture, due to the countries' partially shared heritage and the relatively large number of cultural features common to both the US and Canada. For example, the average Canadian may be perceived as more reserved than his or her American counterpart. Canada and the United States are often inevitably compared as sibling countries, and the perceptions that arise from this oft-held contrast have gone to shape the advertised worldwide identities of both nations: the United States is seen as the rebellious child of the British Crown, forged in the fires of violent revolution; Canada is the calmer offspring of the United Kingdom, known for a more relaxed national demeanour.





</doc>
<doc id="7000" url="https://en.wikipedia.org/wiki?curid=7000" title="List of companies of Canada">
List of companies of Canada

Canada is a country in the northern part of North America.
Canada is the world's tenth-largest economy , with a nominal GDP of approximately US$1.52 trillion. It is a member of the Organisation for Economic Co-operation and Development (OECD) and the Group of Eight (G8), and is one of the world's top ten trading nations, with a highly globalized economy. Canada is a mixed economy, ranking above the US and most western European nations on The Heritage Foundation's index of economic freedom, and experiencing a relatively low level of income disparity. The country's average household disposable income per capita is over US$23,900, higher than the OECD average. Furthermore, the Toronto Stock Exchange is the seventh-largest stock exchange in the world by market capitalization, listing over 1,500 companies with a combined market capitalization of over US$2 trillion .

For further information on the types of business entities in this country and their abbreviations, see "Business entities in Canada".

This list shows firms in the Fortune Global 500, which ranks firms by total revenues reported before March 31, 2017. Only the top five firms (if available) are included as a sample.

This list includes notable companies with primary headquarters located in the country. The industry and sector follow the Industry Classification Benchmark taxonomy. Organizations which have ceased operations are included and noted as defunct.



</doc>
<doc id="7003" url="https://en.wikipedia.org/wiki?curid=7003" title="Cauchy distribution">
Cauchy distribution

The Cauchy distribution, named after Augustin Cauchy, is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution (after Hendrik Lorentz), Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution. The Cauchy distribution formula_1 is the distribution of the "x"-intercept of a ray issuing from formula_2 with a uniformly distributed angle. It is also the distribution of the ratio of two independent normally distributed random variables if the denominator distribution has mean zero.

The Cauchy distribution is often used in statistics as the canonical example of a "pathological" distribution since both its expected value and its variance are undefined. (But see the section "Explanation of undefined moments" below.) The Cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist. The Cauchy distribution has no moment generating function.

In mathematics, it is closely related to the Poisson kernel, which is the fundamental solution for the Laplace equation in the upper half-plane. In spectroscopy, it is the description of the shape of spectral lines which are subject to homogeneous broadening in which all atoms interact in the same way with the frequency range contained in the line shape. Many mechanisms cause homogeneous broadening, most notably collision broadening.

It is one of the few distributions that is stable and has a probability density function that can be expressed analytically, the others being the normal distribution and the Lévy distribution.

Functions with the form of the Cauchy distribution were studied by mathematicians in the 17th century, but in a different context and under the title of the Witch of Agnesi. Despite its name, the first explicit analysis of the properties of the Cauchy distribution was published by the French mathematician Poisson in 1824, with Cauchy only becoming associated with it during an academic controversy in 1853. As such, the name of the distribution is a case of Stigler's Law of Eponymy. Poisson noted that if the mean of observations following such a distribution were taken, the mean error did not converge to any finite number. As such, Laplace's use of the Central Limit Theorem with such a distribution was inappropriate, as it assumed a finite mean and variance. Despite this, Poisson did not regard the issue as important, in contrast to Bienaymé, who was to engage Cauchy in a long dispute over the matter.

The Cauchy distribution has the probability density function (PDF)

where formula_4 is the location parameter, specifying the location of the peak of the distribution, and formula_5 is the scale parameter which specifies the half-width at half-maximum (HWHM), alternatively formula_6 is full width at half maximum (FWHM). formula_5 is also equal to half the interquartile range and is sometimes called the probable error. Augustin-Louis Cauchy exploited such a density function in 1827 with an infinitesimal scale parameter, defining what would now be called a Dirac delta function.

The maximum value or amplitude of the Cauchy PDF is formula_8, located at formula_9.

It is sometimes convenient to express the PDF in terms of the complex parameter formula_10

The special case when formula_12 and formula_13 is called the standard Cauchy distribution with the probability density function

In physics, a three-parameter Lorentzian function is often used:
where formula_16 is the height of the peak. The three-parameter Lorentzian function indicated is not, in general, a probability density function, since it does not integrate to 1, except in the special case where formula_17

The cumulative distribution function is:

and the quantile function (inverse cdf) of the Cauchy distribution is
It follows that the first and third quartiles are formula_20, and hence the interquartile range is formula_6.

For the standard distribution, the cumulative distribution function simplifies to arctangent function formula_22:

The entropy of the Cauchy distribution is given by:

The derivative of the quantile function, the quantile density function, for the Cauchy distribution is:

The differential entropy of a distribution can be defined in terms of its quantile density, specifically:

The Cauchy distribution is the maximum entropy probability distribution for a random variate formula_27 for which 

or, alternatively, for a random variate formula_27 for which 

In its standard form, it is the maximum entropy probability distribution for a random variate formula_27 for which 

The Cauchy distribution is an example of a distribution which has no mean, variance or higher moments defined. Its mode and median are well defined and are both equal to formula_4.

When formula_34 and formula_35 are two independent normally distributed random variables with expected value 0 and variance 1, then the ratio formula_36 has the standard Cauchy distribution.

If formula_37 is a formula_38 positive-semidefinite covariance matrix with strictly positive diagonal entries, then for independent and identically distributed formula_39 and any random formula_40-vector formula_41 independent of formula_27 and formula_43 such that formula_44 and formula_45 it holds that

If formula_47 are independent and identically distributed random variables, each with a standard Cauchy distribution, then the sample mean formula_48 has the same standard Cauchy distribution. To see that this is true, compute the characteristic function of the sample mean:

where formula_50 is the sample mean. This example serves to show that the hypothesis of finite variance in the central limit theorem cannot be dropped. It is also an example of a more generalized version of the central limit theorem that is characteristic of all stable distributions, of which the Cauchy distribution is a special case.

The Cauchy distribution is an infinitely divisible probability distribution. It is also a strictly stable distribution.

The standard Cauchy distribution coincides with the Student's "t"-distribution with one degree of freedom.

Like all stable distributions, the location-scale family to which the Cauchy distribution belongs is closed under linear transformations with real coefficients. In addition, the Cauchy distribution is closed under linear fractional transformations with real coefficients. In this connection, see also McCullagh's parametrization of the Cauchy distributions.

Let formula_27 denote a Cauchy distributed random variable. The characteristic function of the Cauchy distribution is given by

which is just the Fourier transform of the probability density. The original probability density may be expressed in terms of the characteristic function, essentially by using the inverse Fourier transform:

The "n"th moment of a distribution is the "n"th derivative of the characteristic function evaluated at formula_54. Observe that the characteristic function is not differentiable at the origin: this corresponds to the fact that the Cauchy distribution does not have well-defined moments higher than the zeroth moment.

If a probability distribution has a density function formula_55, then the mean is

The question is now whether this is the same thing as
for an arbitrary real number formula_58.

If at most one of the two terms in (2) is infinite, then (1) is the same as (2). But in the case of the Cauchy distribution, both the positive and negative terms of (2) are infinite. Hence (1) is undefined.

Note that the Cauchy principal value of the Cauchy distribution is:

which is zero, while:

is "not" zero, as can be seen easily by computing the integral.

Various results in probability theory about expected values, such as the strong law of large numbers, will not work in such cases.

The Cauchy distribution does not have finite moments of any order. Some of the higher raw moments do exist and have a value of infinity, for example the raw second moment:

By re-arranging the formula, one can see that the second moment is essentially the infinite integral of a constant (here 1). Higher even-powered raw moments will also evaluate to infinity. Odd-powered raw moments, however, are undefined, which is distinctly different from existing with the value of infinity. The odd-powered raw moments are undefined because their values are essentially equivalent to formula_62 since the two halves of the integral both diverge and have opposite signs. The first raw moment is the mean, which, being odd, does not exist. (See also the discussion above about this.) This in turn means that all of the central moments and standardized moments are undefined, since they are all based on the mean. The variance—which is the second central moment—is likewise non-existent (despite the fact that the raw second moment exists with the value infinity).

The results for higher moments follow from Hölder's inequality, which implies that higher moments (or halves of moments) diverge if lower ones do.

Because the parameters of the Cauchy distribution do not correspond to a mean and variance, attempting to estimate the parameters of the Cauchy distribution by using a sample mean and a sample variance will not succeed. For example, if an i.i.d. sample of size "n" is taken from a Cauchy distribution, one may calculate the sample mean as:

Although the sample values formula_64 will be concentrated about the central value formula_4, the sample mean will become increasingly variable as more observations are taken, because of the increased probability of encountering sample points with a large absolute value. In fact, the distribution of the sample mean will be equal to the distribution of the observations themselves; i.e., the sample mean of a large sample is no better (or worse) an estimator of formula_4 than any single observation from the sample. Similarly, calculating the sample variance will result in values that grow larger as more observations are taken.

Therefore, more robust means of estimating the central value formula_4 and the scaling parameter formula_5 are needed. One simple method is to take the median value of the sample as an estimator of formula_4 and half the sample interquartile range as an estimator of formula_5. Other, more precise and robust methods have been developed For example, the truncated mean of the middle 24% of the sample order statistics produces an estimate for formula_4 that is more efficient than using either the sample median or the full sample mean. However, because of the fat tails of the Cauchy distribution, the efficiency of the estimator decreases if more than 24% of the sample is used.

Maximum likelihood can also be used to estimate the parameters formula_4 and formula_5. However, this tends to be complicated by the fact that this requires finding the roots of a high degree polynomial, and there can be multiple roots that represent local maxima. Also, while the maximum likelihood estimator is asymptotically efficient, it is relatively inefficient for small samples. The log-likelihood function for the Cauchy distribution for sample size formula_74 is:

Maximizing the log likelihood function with respect to formula_4 and formula_5 produces the following system of equations:

Note that

is a monotone function in formula_5 and that the solution formula_5 must satisfy

Solving just for formula_4 requires solving a polynomial of degree formula_85, and solving just for formula_5 requires solving a polynomial of degree formula_74 (first for formula_88, then formula_4). Therefore, whether solving for one parameter or for both parameters simultaneously, a numerical solution on a computer is typically required. The benefit of maximum likelihood estimation is asymptotic efficiency; estimating formula_4 using the sample median is only about 81% as asymptotically efficient as estimating formula_4 by maximum likelihood. The truncated sample mean using the middle 24% order statistics is about 88% as asymptotically efficient an estimator of formula_4 as the maximum likelihood estimate. When Newton's method is used to find the solution for the maximum likelihood estimate, the middle 24% order statistics can be used as an initial solution for formula_4.

A random vector formula_94 is said to have the multivariate Cauchy distribution if every linear combination of its components formula_95 has a Cauchy distribution. That is, for any constant vector formula_96, the random variable formula_97 should have a univariate Cauchy distribution. The characteristic function of a multivariate Cauchy distribution is given by:

where formula_99 and formula_100 are real functions with formula_99 a homogeneous function of degree one and formula_100 a positive homogeneous function of degree one. More formally:

for all formula_105.

An example of a bivariate Cauchy distribution can be given by:
Note that in this example, even though there is no analogue to a covariance matrix, formula_107 and formula_108 are not statistically independent.

Analogous to the univariate density, the multidimensional Cauchy density also relates to the multivariate Student distribution. They are equivalent when the degrees of freedom parameter is equal to one. The density of a formula_109 dimension Student distribution with one degree of freedom becomes:

Properties and details for this density can be obtained by taking it as a particular case of the multivariate Student density.

where formula_58, formula_125, formula_126 and formula_127 are real numbers.

The Cauchy distribution is the stable distribution of index 1. The Lévy-Khintchine representation of such a stable distribution of parameter formula_131 is given, for formula_132 by:
and formula_135 can be expressed explicitly. In the case formula_136 of the Cauchy distribution, one has formula_137.

This last representation is a consequence of the formula


In nuclear and particle physics, the energy profile of a resonance is described by the relativistic Breit–Wigner distribution, while the Cauchy distribution is the (non-relativistic) Breit–Wigner distribution.






</doc>
<doc id="7011" url="https://en.wikipedia.org/wiki?curid=7011" title="Control engineering">
Control engineering

Control engineering or control systems engineering is an engineering discipline that applies automatic control theory to design systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering at many institutions around the world.

The practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.

Modern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering has an essential role in a wide range of control systems, from simple household washing machines to high-performance F-16 fighter aircraft. It seeks to understand physical systems, using mathematical modeling, in terms of inputs, outputs and various components with different behaviors; use control systems design tools to develop controllers for those systems; and implement controllers in physical systems employing available technology. A system can be mechanical, electrical, fluid, chemical, financial or biological, and the mathematical modeling, analysis and controller design uses control theory in one or many of the time, frequency and complex-s domains, depending on the nature of the design problem.

Automatic control systems were first developed over two thousand years ago. The first feedback control device on record is thought to be the ancient Ktesibios's water clock in Alexandria, Egypt around the third century BCE. It kept time by regulating the water level in a vessel and, therefore, the water flow from that vessel. This certainly was a successful device as water clocks of similar design were still being made in Baghdad when the Mongols captured the city in 1258 A.D. A variety of automatic devices have been used over the centuries to accomplish useful tasks or simply to just entertain. The latter includes the automata, popular in Europe in the 17th and 18th centuries, featuring dancing figures that would repeat the same task over and over again; these automata are examples of open-loop control. Milestones among feedback, or "closed-loop" automatic control devices, include the temperature regulator of a furnace attributed to Drebbel, circa 1620, and the centrifugal flyball governor used for regulating the speed of steam engines by James Watt in 1788.

In his 1868 paper "On Governors", James Clerk Maxwell was able to explain instabilities exhibited by the flyball governor using differential equations to describe the control system. This demonstrated the importance and usefulness of mathematical models and methods in understanding complex phenomena, and it signaled the beginning of mathematical control and systems theory. Elements of control theory had appeared earlier but not as dramatically and convincingly as in Maxwell's analysis.

Control theory made significant strides over the next century. New mathematical techniques, as well as advancements in electronic and computer technologies, made it possible to control significantly more complex dynamical systems than the original flyball governor could stabilize. New mathematical techniques included developments in optimal control in the 1950s and 1960s followed by progress in stochastic, robust, adaptive, nonlinear, and azid-based control methods in the 1970s and 1980s. Applications of control methodology have helped to make possible space travel and communication satellites, safer and more efficient aircraft, cleaner automobile engines, and cleaner and more efficient chemical processes.

Before it emerged as a unique discipline, control engineering was practiced as a part of mechanical engineering and control theory was studied as a part of electrical engineering since electrical circuits can often be easily described using control theory techniques. In the very first control relationships, a current output was represented by a voltage control input. However, not having adequate technology to implement electrical control systems, designers were left with the option of less efficient and slow responding mechanical systems. A very effective mechanical controller that is still widely used in some hydro plants is the governor. Later on, previous to modern power electronics, process control systems for industrial applications were devised by mechanical engineers using pneumatic and hydraulic control devices, many of which are still in use today.

There are two major divisions in control theory, namely, classical and modern, which have direct implications for the control engineering applications. The scope of classical control theory is limited to single-input and single-output (SISO) system design, except when analyzing for disturbance rejection using a second input. The system analysis is carried out in the time domain using differential equations, in the complex-s domain with the Laplace transform, or in the frequency domain by transforming from the complex-s domain. Many systems may be assumed to have a second order and single variable system response in the time domain. A controller designed using classical theory often requires on-site tuning due to incorrect design approximations. Yet, due to the easier physical implementation of classical controller designs as compared to systems designed using modern control theory, these controllers are preferred in most industrial applications. The most common controllers designed using classical control theory are PID controllers. A less common implementation may include either or both a Lead or Lag filter. The ultimate end goal is to meet requirements typically provided in the time-domain called the step response, or at times in the frequency domain called the open-loop response. The step response characteristics applied in a specification are typically percent overshoot, settling time, etc. The open-loop response characteristics applied in a specification are typically Gain and Phase margin and bandwidth. These characteristics may be evaluated through simulation including a dynamic model of the system under control coupled with the compensation model.

In contrast, modern control theory is carried out in the state space, and can deal with multiple-input and multiple-output (MIMO) systems. This overcomes the limitations of classical control theory in more sophisticated design problems, such as fighter aircraft control, with the limitation that no frequency domain analysis is possible. In modern design, a system is represented to the greatest advantage as a set of decoupled first order differential equations defined using state variables. Nonlinear, multivariable, adaptive and robust control theories come under this division. Matrix methods are significantly limited for MIMO systems where linear independence cannot be assured in the relationship between inputs and outputs. Being fairly new, modern control theory has many areas yet to be explored. Scholars like Rudolf E. Kalman and Aleksandr Lyapunov are well-known among the people who have shaped modern control theory.

Control engineering is the engineering discipline that focuses on the modeling of a diverse range of dynamic systems (e.g. mechanical systems) and the design of controllers that will cause these systems to behave in the desired manner. Although such controllers need not to be electrical many are and hence control engineering is often viewed as a subfield of electrical engineering. However, the falling price of microprocessors is making the actual implementation of a control system essentially trivial. As a result, focus is shifting back to the mechanical and process engineering discipline, as intimate knowledge of the physical system being controlled is often desired.
Control system is very crucial for daily life.Its applications are very vast and it have bring comfort in life from household devices defense system of a nation.wholly we can say it is a game changer of life in every aspect.
Electrical circuits, digital signal processors and microcontrollers can all be used to implement control systems. Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. 

In most of the cases, control engineers utilize feedback when designing control systems. This is often accomplished using a PID controller system. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system, which adjusts the motor's torque accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback. In practically all such systems stability is important and control theory can help ensure stability is achieved.

Although feedback is an important aspect of control engineering, control engineers may also work on the control of systems without feedback. This is known as open loop control. A classic example of open loop control is a washing machine that runs through a pre-determined cycle without the use of sensors.

At many universities around the world, control engineering courses are taught primarily in electrical engineering but some courses can be instructed in mechatronics engineering, mechanical engineering, and aerospace engineering. In others, control engineering is connected to computer science, as most control techniques today are implemented through computers, often as embedded systems (as in the automotive field). The field of control within chemical engineering is often known as process control. It deals primarily with the control of variables in a chemical process in a plant. It is taught as part of the undergraduate curriculum of any chemical engineering program and employs many of the same principles in control engineering. Other engineering disciplines also overlap with control engineering as it can be applied to any system for which a suitable model can be derived. However, specialised control engineering departments do exist, for example, the Department of Automatic Control and Systems Engineering at the University of Sheffield and the Department of Systems Engineering at the United States Naval Academy.

Control engineering has diversified applications that include science, finance management, and even human behavior. Students of control engineering may start with a linear control system course dealing with the time and complex-s domain, which requires a thorough background in elementary mathematics and Laplace transform, called classical control theory. In linear control, the student does frequency and time domain analysis. Digital control and nonlinear control courses require Z transformation and algebra respectively, and could be said to complete a basic control education.

Originally, control engineering was all about continuous systems. Development of computer control tools posed a requirement of discrete control system engineering because the communications between the computer-based digital controller and the physical system are governed by a computer clock. The equivalent to Laplace transform in the discrete domain is the Z-transform. Today, many of the control systems are computer controlled and they consist of both digital and analog components.

Therefore, at the design stage either digital components are mapped into the continuous domain and the design is carried out in the continuous domain, or analog components are mapped into discrete domain and design is carried out there. The first of these two methods is more commonly encountered in practice because many industrial systems have many continuous systems components, including mechanical, fluid, biological and analog electrical components, with a few digital controllers.

Similarly, the design technique has progressed from paper-and-ruler based manual design to computer-aided design and now to computer-automated design or CAutoD which has been made possible by evolutionary computation. CAutoD can be applied not just to tuning a predefined control scheme, but also to controller structure optimisation, system identification and invention of novel control systems, based purely upon a performance requirement, independent of any specific control scheme.

Resilient control systems extend the traditional focus of addressing only planned disturbances to frameworks and attempt to address multiple types of unexpected disturbance; in particular, adapting and transforming behaviors of the control system in response to malicious actors, abnormal failure modes, undesirable human action, etc.





</doc>
<doc id="7012" url="https://en.wikipedia.org/wiki?curid=7012" title="Chagas disease">
Chagas disease

Chagas disease, also known as American trypanosomiasis, is a tropical parasitic disease caused by the protist "Trypanosoma cruzi". It is spread mostly by insects known as Triatominae, or "kissing bugs". The symptoms change over the course of the infection. In the early stage, symptoms are typically either not present or mild, and may include fever, swollen lymph nodes, headaches, or local swelling at the site of the bite. After 8–12 weeks, individuals enter the chronic phase of disease and in 60–70% it never produces further symptoms. The other 30 to 40% of people develop further symptoms 10 to 30 years after the initial infection, including enlargement of the ventricles of the heart in 20 to 30%, leading to heart failure. An enlarged esophagus or an enlarged colon may also occur in 10% of people.
"T. cruzi" is commonly spread to humans and other mammals by the blood-sucking "kissing bugs" of the subfamily Triatominae. These insects are known by a number of local names, including: "vinchuca" in Argentina, Bolivia, Chile and Paraguay, "barbeiro" (the barber) in Brazil, "pito" in Colombia, "chinche" in Central America, and "chipo" in Venezuela. The disease may also be spread through blood transfusion, organ transplantation, eating food contaminated with the parasites, and by vertical transmission (from a mother to her fetus). Diagnosis of early disease is by finding the parasite in the blood using a microscope. Chronic disease is diagnosed by finding antibodies for "T. cruzi" in the blood.
Prevention mostly involves eliminating kissing bugs and avoiding their bites. This may involve the use of insecticides or bed-nets. Other preventive efforts include screening blood used for transfusions. A vaccine has not been developed as of 2017. Early infections are treatable with the medication benznidazole or nifurtimox. Medication nearly always results in a cure if given early, but becomes less effective the longer a person has had Chagas disease. When used in chronic disease, medication may delay or prevent the development of end–stage symptoms. Benznidazole and nifurtimox cause temporary side effects in up to 40% of people including skin disorders, brain toxicity, and digestive system irritation.
It is estimated that 6.6 million people, mostly in Mexico, Central America and South America, have Chagas disease as of 2015. In 2015, Chagas was estimated to result in 8,000 deaths. Most people with the disease are poor, and most do not realize they are infected. Large-scale population movements have increased the areas where Chagas disease is found and these include many European countries and the United States. These areas have also seen an increase in the years up to 2014. The disease was first described in 1909 by the Brazilian physician Carlos Chagas, after whom it is named. Chagas disease is classified as a neglected tropical disease. It affects more than 150 other animals.

The human disease occurs in two stages: an acute stage, which occurs shortly after an initial infection, and a chronic stage that develops over many years.

The acute phase lasts for the first few weeks or months of infection. It usually occurs unnoticed because it is symptom-free or exhibits only mild symptoms that are not unique to Chagas disease. These can include fever, fatigue, body aches, muscle pain, headache, rash, loss of appetite, diarrhea, nausea, and vomiting. The signs on physical examination can include mild enlargement of the liver or spleen, swollen glands, and local swelling (a chagoma) where the parasite entered the body.

The most recognized marker of acute Chagas disease is called Romaña's sign, which includes swelling of the eyelids on the side of the face near the bite wound or where the bug feces were deposited or accidentally rubbed into the eye. Rarely, young children, or adults may die from the acute disease due to severe inflammation/infection of the heart muscle (myocarditis) or brain (meningoencephalitis). The acute phase also can be severe in people with weakened immune systems.

If symptoms develop during the acute phase, they usually resolve spontaneously within three to eight weeks in approximately 90% of individuals. Although the symptoms resolve, even with treatment the infection persists and enters a chronic phase. Of individuals with chronic Chagas disease, 60–80% will never develop symptoms (called "indeterminate" chronic Chagas disease), while the remaining 20–40% will develop life-threatening heart and/or digestive disorders during their lifetime (called "determinate" chronic Chagas disease). In 10% of individuals, the disease progresses directly from the acute form to a symptomatic clinical form of chronic Chagas disease.

The symptomatic (determinate) chronic stage affects the nervous system, digestive system and heart. About two-thirds of people with chronic symptoms have cardiac damage, including dilated cardiomyopathy, which causes heart rhythm abnormalities and may result in sudden death. About one-third of patients go on to develop digestive system damage, resulting in dilation of the digestive tract (megacolon and megaesophagus), accompanied by severe weight loss. Swallowing difficulties (secondary achalasia) may be the first symptom of digestive disturbances and may lead to malnutrition.

20% to 50% of individuals with intestinal involvement also exhibit cardiac involvement. Up to 10% of chronically infected individuals develop neuritis that results in altered tendon reflexes and sensory impairment. Isolated cases exhibit central nervous system involvement, including dementia, confusion, chronic encephalopathy and sensory and motor deficits.

The clinical manifestations of Chagas disease are due to cell death in the target tissues that occurs during the infective cycle, by sequentially inducing an inflammatory response, cellular lesions, and fibrosis. For example, intracellular amastigotes destroy the intramural neurons of the autonomic nervous system in the intestine and heart, leading to megaintestine and heart aneurysms, respectively. If left untreated, Chagas disease can be fatal, in most cases due to heart muscle damage.

In Chagas-endemic areas, the main mode of transmission is through an insect vector called a triatomine bug. A triatomine becomes infected with "T. cruzi" by feeding on the blood of an infected person or animal. During the day, triatomines hide in crevices in the walls and roofs.

The bugs emerge at night, when the inhabitants are sleeping. Because they tend to feed on people's faces, triatomine bugs are also known as "kissing bugs". After they bite and ingest blood, they defecate on the person. Triatomines pass "T. cruzi" parasites (called trypomastigotes) in feces left near the site of the bite wound.

Scratching the site of the bite causes the trypomastigotes to enter the host through the wound, or through intact mucous membranes, such as the conjunctiva. Once inside the host, the trypomastigotes invade cells, where they differentiate into intracellular amastigotes. The amastigotes multiply by binary fission and differentiate into trypomastigotes, which are then released into the bloodstream. This cycle is repeated in each newly infected cell. Replication resumes only when the parasites enter another cell or are ingested by another vector. (See also: )

Dense vegetation (such as that of tropical rainforests) and urban habitats are not ideal for the establishment of the human transmission cycle. However, in regions where the sylvatic habitat and its fauna are thinned by economic exploitation and human habitation, such as in newly deforested areas, piassava palm culture areas, and some parts of the Amazon region, a human transmission cycle may develop as the insects search for new food sources.

"T. cruzi" can also be transmitted through blood transfusions. With the exception of blood derivatives (such as fractionated antibodies), all blood components are infective. The parasite remains viable at 4 °C for at least 18 days or up to 250 days when kept at room temperature. It is unclear whether "T. cruzi" can be transmitted through frozen-thawed blood components.

Other modes of transmission include organ transplantation, through breast milk, and by accidental laboratory exposure. Chagas disease can also be spread congenitally (from a pregnant woman to her baby) through the placenta, and accounts for approximately 13% of stillborn deaths in parts of Brazil.

Oral transmission is an unusual route of infection, but has been described. In 1991, farm workers in the state of Paraíba, Brazil, were infected by eating contaminated food; transmission has also occurred via contaminated açaí palm fruit juice and garapa. A 2007 outbreak in 103 Venezuelan school children was attributed to contaminated guava juice.

Chagas disease is a growing problem in Europe, because the majority of cases with chronic infection are asymptomatic and because of migration from Latin America.

The presence of "T. cruzi" is diagnostic of Chagas disease. It can be detected by microscopic examination of fresh anticoagulated blood, or its buffy coat, for motile parasites; or by preparation of thin and thick blood smears stained with Giemsa, for direct visualization of parasites. Microscopically, "T. cruzi" can be confused with "Trypanosoma rangeli", which is not known to be pathogenic in humans. Isolation of "T. cruzi" can occur by inoculation into mice, by culture in specialized media (for example, NNN, LIT); and by xenodiagnosis, where uninfected Reduviidae bugs are fed on the patient's blood, and their gut contents examined for parasites.

Various immunoassays for "T. cruzi" are available and can be used to distinguish among strains (zymodemes of "T.cruzi" with divergent pathogenicities). These tests include: detecting complement fixation, indirect hemagglutination, indirect fluorescence assays, radioimmunoassays, and ELISA. Alternatively, diagnosis and strain identification can be made using polymerase chain reaction (PCR).
There is currently no vaccine against Chagas disease. Prevention is generally focused on decreasing the numbers of the insect that spreads it ("Triatoma") and decreasing their contact with humans. This is done by using sprays and paints containing insecticides (synthetic pyrethroids), and improving housing and sanitary conditions in rural areas. For urban dwellers, spending vacations and camping out in the wilderness or sleeping at hostels or mud houses in endemic areas can be dangerous; a mosquito net is recommended. Some measures of vector control include:

A number of potential vaccines are currently being tested. Vaccination with "Trypanosoma rangeli" has produced positive results in animal models. More recently, the potential of DNA vaccines for immunotherapy of acute and chronic Chagas disease is being tested by several research groups.

Blood transfusion was formerly the second-most common mode of transmission for Chagas disease, but the development and implementation of blood bank screening tests has dramatically reduced this risk in the 21st century. Blood donations in all endemic Latin American countries undergo Chagas screening, and testing is expanding in countries, such as France, Spain and the United States, that have significant or growing populations of immigrants from endemic areas. In Spain, donors are evaluated with a questionnaire to identify individuals at risk of Chagas exposure for screening tests.

The US FDA has approved two Chagas tests, including one approved in April 2010, and has published guidelines that recommend testing of all donated blood and tissue products. While these tests are not required in US, an estimated 75–90% of the blood supply is currently tested for Chagas, including all units collected by the American Red Cross, which accounts for 40% of the U.S. blood supply. The Chagas Biovigilance Network reports current incidents of Chagas-positive blood products in the United States, as reported by labs using the screening test approved by the FDA in 2007.

There are two approaches to treating Chagas disease: antiparasitic treatment, to kill the parasite; and symptomatic treatment, to manage the symptoms and signs of the infection. Management uniquely involves addressing selective incremental failure of the parasympathetic nervous system. Autonomic disease imparted by Chagas may eventually result in megaesophagus, megacolon and accelerated dilated cardiomyopathy. The mechanisms that explain why Chagas targets the parasympathetic autonomic nervous system and spares the sympathetic autonomic nervous system remain poorly understood.

Antiparasitic treatment is most effective early in the course of infection, but is not limited to cases in the acute phase. Drugs of choice include azole or nitro derivatives, such as benznidazole or nifurtimox. Both agents are limited in their capacity to completely eliminate "T. cruzi" from the body (parasitologic cure), especially in chronically infected patients, and resistance to these drugs has been reported.

Studies suggest antiparasitic treatment leads to parasitological cure in more than 90% of infants but only about 60–85% of adults treated in the first year of acute phase Chagas disease. Children aged six to 12 years with chronic disease have a cure rate of about 60% with benznidazole. While the rate of cure declines the longer an adult has been infected with Chagas, treatment with benznidazole has been shown to slow the onset of heart disease in adults with chronic Chagas infections.

Treatment of chronic infection in women prior to or during pregnancy does not appear to reduce the probability the disease will be passed on to the infant. Likewise, it is unclear whether prophylactic treatment of chronic infection is beneficial in persons who will undergo immunosuppression (for example, organ transplant recipients) or in persons who are already immunosuppressed (for example, those with HIV infection).

In the chronic stage, treatment involves managing the clinical manifestations of the disease. For example, pacemakers and medications for irregular heartbeats, such as the anti-arrhythmia drug amiodarone, may be life saving for some patients with chronic cardiac disease, while surgery may be required for megaintestine. The disease cannot be cured in this phase, however. Chronic heart disease caused by Chagas disease is now a common reason for heart transplantation surgery. Until recently, however, Chagas disease was considered a contraindication for the procedure, since the heart damage could recur as the parasite was expected to seize the opportunity provided by the immunosuppression that follows surgery.

Chagas disease affects 8 to 10 million people living in endemic Latin American countries, with an additional 300,000–400,000 living in nonendemic countries, including Spain and the United States. An estimated 41,200 new cases occur annually in endemic countries, and 14,400 infants are born with congenital Chagas disease annually. in 2010 it resulted in approximately 10,300 deaths up from 9,300 in 1990.

The disease is present in 18 countries on the American continents, ranging from the southern United States to northern Argentina. Chagas exists in two different ecological zones. In the Southern Cone region, the main vector lives in and around human homes. In Central America and Mexico, the main vector species lives both inside dwellings and in uninhabited areas. In both zones, Chagas occurs almost exclusively in rural areas, where triatomines breed and feed on the more than 150 species from 24 families of domestic and wild mammals, as well as humans, that are the natural reservoirs of "T. cruzi".

Although Triatominae bugs feed on them, birds appear to be immune to infection and therefore are not considered to be a "T. cruzi" reservoir. Even when colonies of insects are eradicated from a house and surrounding domestic animal shelters, they can re-emerge from plants or animals that are part of the ancient, sylvatic (referring to wild animals) infection cycle. This is especially likely in zones with mixed open savannah, with clumps of trees interspersed by human habitation.

The primary wildlife reservoirs for "Trypanosoma cruzi" in the United States include opossums, raccoons, armadillos, squirrels, woodrats, and mice. Opossums are particularly important as reservoirs, because the parasite can complete its life cycle in the anal glands of this animal without having to re-enter the insect vector. Recorded prevalence of the disease in opossums in the U.S. ranges from 8.3% to 37.5%.

Studies on raccoons in the Southeast have yielded infection rates ranging from 47% to as low as 15.5%. Armadillo prevalence studies have been described in Louisiana, and range from a low of 1.1% to 28.8%. Additionally, small rodents, including squirrels, mice, and rats, are important in the sylvatic transmission cycle because of their importance as bloodmeal sources for the insect vectors. A Texas study revealed 17.3% percent "T. cruzi" prevalence in 75 specimens representing four separate small rodent species.

Chronic Chagas disease remains a major health problem in many Latin American countries, despite the effectiveness of hygienic and preventive measures, such as eliminating the transmitting insects. However, several landmarks have been achieved in the fight against it in Latin America, including a reduction by 72% of the incidence of human infection in children and young adults in the countries of the Southern Cone Initiative, and at least three countries (Uruguay, in 1997, and Chile, in 1999, and Brazil in 2006) have been certified free of vectorial and transfusional transmission. In Argentina, vectorial transmission has been interrupted in 13 of the 19 endemic provinces, and major progress toward this goal has also been made in both Paraguay and Bolivia.

Screening of donated blood, blood components, and solid organ donors, as well as donors of cells, tissues, and cell and tissue products for "T. cruzi" is mandated in all Chagas-endemic countries and has been implemented. Approximately 300,000 infected people live in the United States, which is likely the result of immigration from Latin American countries, and there have been 23 cases acquired from kissing bugs in the United States reported between 1955 and 2014. With increased population movements, the possibility of transmission by blood transfusion became more substantial in the United States. Transfusion blood and tissue products are now actively screened in the U.S., thus addressing and minimizing this risk.

The disease was named after the Brazilian physician and epidemiologist Carlos Chagas, who first described it in 1909. The disease was not seen as a major public health problem in humans until the 1960s (the outbreak of Chagas disease in Brazil in the 1920s went widely ignored). Dr Chagas discovered that the intestines of Triatomidae (now Reduviidae: Triatominae) harbored a flagellate protozoan, a new species of the "Trypanosoma" genus, and was able to demonstrate experimentally that it could be transmitted to marmoset monkeys that were bitten by the infected bug. Later studies showed squirrel monkeys were also vulnerable to infection.

Chagas named the pathogenic parasite as "Trypanosoma cruzi" and later that year as "Schizotrypanum cruzi", both honoring Oswaldo Cruz, the noted Brazilian physician and epidemiologist who successfully fought epidemics of yellow fever, smallpox, and bubonic plague in Rio de Janeiro and other cities in the beginning of the 20th century. Chagas was also the first to unknowingly discover and illustrate the parasitic fungal genus "Pneumocystis", later infamously linked to PCP ("Pneumocystis" pneumonia in AIDS victims). Confusion between the two pathogens' life-cycles led him to briefly recognize his genus "Schizotrypanum", but following the description of "Pneumocystis" by others as an independent genus, Chagas returned to the use of the name "Trypanosoma cruzi".

In Argentina, the disease is known as "mal de Chagas-Mazza", in honor of Salvador Mazza, the Argentine physician who in 1926 began investigating the disease and over the years became the principal researcher of this disease in the country. Mazza produced the first scientific confirmation of the existence of "Trypanosoma cruzi" in Argentina in 1927, eventually leading to support from local and European medical schools and Argentine government policy makers.

It has been hypothesized that Charles Darwin might have suffered from Chagas disease as a result of a bite of the so-called great black bug of the Pampas ("vinchuca") (see Charles Darwin's illness). The episode was reported by Darwin in his diaries of the Voyage of the Beagle as occurring in March 1835 to the east of the Andes near Mendoza. Darwin was young and generally in good health, though six months previously he had been ill for a month near Valparaiso, but in 1837, almost a year after he returned to England, he began to suffer intermittently from a strange group of symptoms, becoming incapacitated for much of the rest of his life. Attempts to test Darwin's remains at Westminster Abbey by using modern PCR techniques were met with a refusal by the Abbey's curator.

Several experimental treatments have shown promise in animal models. These include inhibitors of oxidosqualene cyclase and squalene synthase, cysteine protease inhibitors, dermaseptins collected from frogs in the genus "Phyllomedusa" ("P. oreades" and "P. distincta"), the sesquiterpene lactone dehydroleucodine (DhL), which affects the growth of cultured epimastigote-phase "Trypanosoma cruzi", inhibitors of purine uptake, and inhibitors of enzymes involved in trypanothione metabolism. Hopefully, new drug targets may be revealed following the sequencing of the "T. cruzi" genome.

Chagas disease has a serious economic impact on the United States and the world. The cost of treatment in the United States alone, where the disease is not indigenous, is estimated to be $900 million annually, which includes hospitalization and medical devices such as pacemakers. The global cost is estimated at $7 billion.

Megazol in a study seems more active against Chagas than benznidazole but has not been studied in humans. A Chagas vaccine (TcVac3) has been found to be effective in mice with plans for studies in dogs. It is hoped that it will be commercially available by 2018.





</doc>
<doc id="7015" url="https://en.wikipedia.org/wiki?curid=7015" title="Christiaan Barnard">
Christiaan Barnard

Christiaan Neethling Barnard (8 November 1922 – 2 September 2001) was a South African cardiac surgeon who performed the world's first human-to-human heart transplant on 3 December 1967 at Groote Schuur Hospital in Cape Town, South Africa. Growing up in Beaufort West, Cape Province, he studied medicine and practised for several years in his native country. As a young doctor experimenting on dogs, Barnard developed a remedy for the infant defect of intestinal atresia. His technique saved the lives of ten babies in Cape Town and was adopted by surgeons in Britain and the United States. In 1955, he travelled to the United States and was initially assigned further gastrointestinal work by Owen Harding Wangensteen. He was introduced to the heart-lung machine, and Barnard was allowed to transfer to the service run by open heart surgery pioneer Walt Lillehei. Upon returning to South Africa in 1958, Barnard was appointed head of the Department of Experimental Surgery at the Groote Schuur Hospital, Cape Town.

On 3 December 1967, Barnard transplanted the heart of Denise Darvall, who had just died from a head injury, into the chest of a 54-year-old Louis Washkansky. Washkansky regained full consciousness and lived for eighteen days, even spending time with his wife, before he died of pneumonia, with the suppression of his immune system by the anti-rejection drugs being a major contributing factor. Barnard had told Mr. and Mrs. Washkansky that the operation had an 80% chance of success, a claim which has been criticised as misleading.

Barnard's second transplant patient Philip Blaiberg, with the operation performed at the beginning of 1968, lived for nineteen months and was able to go home from the hospital.

He retired as Head of the Department of Cardiothoracic Surgery in Cape Town in 1983 after developing rheumatoid arthritis in his hands which ended his surgical career. He became interested in anti-aging research, and in 1986 his reputation suffered when he promoted "Glycel", an expensive "anti-aging" skin cream, whose approval was withdrawn by the United States Food and Drug Administration soon thereafter. During his remaining years, he established the Christiaan Barnard Foundation, dedicated to helping underprivileged children throughout the world. He died in 2001 at the age of 78 after an asthma attack.

Barnard grew up in Beaufort West, Cape Province, Union of South Africa. His father, Adam Barnard, was a minister in the Dutch Reformed Church. One of his four brothers, Abraham, was a "blue baby" who died of a heart problem at the age of three (Barnard would later guess that it was tetralogy of Fallot). The family also experienced the loss of a daughter who was stillborn and who had been the fraternal twin of Barnard's older brother Johannes, who was twelve years older than Chris. Barnard matriculated from the Beaufort West High School in 1940, and went to study medicine at the University of Cape Town Medical School, where he obtained his MB ChB in 1945.

His father served as a missionary to mixed-race peoples. His mother, the former Maria Elisabeth de Swart, instilled in the surviving brothers the belief that they could do anything they set their minds to.

Barnard did his internship and residency at the Groote Schuur Hospital in Cape Town, after which he worked as a general practitioner in Ceres, a rural town in the Cape Province. In 1951, he returned to Cape Town where he worked at the City Hospital as a Senior Resident Medical Officer, and in the Department of Medicine at the Groote Schuur Hospital as a registrar. He completed his master's degree, receiving Master of Medicine in 1953 from the University of Cape Town. In the same year he obtained a doctorate in medicine (MD) from the same university for a dissertation titled "The treatment of tuberculous meningitis".

Soon after qualifying as a doctor, Barnard performed experiments on dogs investigating intestinal atresia, a birth defect which allows life-threatening gaps to develop in the intestines. He followed a medical hunch that this was caused by inadequate blood flow to the fetus. After nine months and forty-three attempts, Barnard was able to reproduce this condition in a fetus puppy by tying off some of the blood supply to a puppy's intestines and then placing the animal back in the womb, after which it was born some two weeks later, with the condition of intestinal atresia. He was also able to cure the condition by removing the piece of intestine with inadequate blood supply. The mistake of previous surgeons had been attempting to reconnect ends of intestine which themselves still had inadequate blood supply. To be successful, it was typically necessary to remove between 15 and 20 centimeters of intestine (6 to 8 inches). Jannie Louw used this innovation in a clinical setting, and Barnard's method saved the lives of ten babies in Cape Town. This technique was also adapted by surgeons in Britain and the US. In addition, Barnard analyzed 259 cases of tubercular meningitis. 

Owen Wangensteen in Minnesota had been impressed by the work of Alan Thal, a young South African doctor working in Minnesota. He asked Groote Schuur Head of Medicine John Brock if he might recommend any similarly talented South Africans and Brock recommended Barnard. In December 1955, Barnard travelled to the University of Minnesota, Minneapolis, United States, to begin a two-year scholarship under Chief of Surgery Wangensteen, who assigned Barnard more work on the intestines, which Barnard accepted even though he wanted to move onto something new. Simply by luck, whenever Barnard needed a break from this work, he could wander across the hall and talk with Vince Gott who ran the lab for open-heart surgery pioneer Walt Lillehei. Gott had begun to develop a technique of running blood backwards through the veins of the heart so Lillehei could more easily operate on the aortic valve (McRae writes, "It was the type of inspired thinking that entranced Barnard"). In March 1956, Gott asked Barnard to help him run the heart-lung machine for an operation. Shortly thereafter, Wangensteen agreed to let Barnard switch to Lillehei's service. It was during this time that Barnard first became acquainted with fellow future heart transplantation surgeon Norman Shumway. Barnard also became friendly with Gil Campbell who had demonstrated that a dog's lung could be used to oxygenate blood during open-heart surgery. (The year before Barnard arrived, Lillehei and Campbell had used this procedure for twenty minutes during surgery on a 13-year-old boy with ventricular septal defect, and the boy had made a full recovery.) Barnard and Campbell met regularly for early breakfast. In 1958, Barnard received a Master of Science in Surgery for a thesis titled "The aortic valve – problems in the fabrication and testing of a prosthetic valve". The same year he was awarded a Ph.D. for his dissertation titled "The aetiology of congenital intestinal atresia". Barnard described the two years he spent in the United States as "the most fascinating time in my life."

Upon returning to South Africa in 1958, Barnard was appointed head of the Department of Experimental Surgery at Groote Schuur hospital, as well as holding a joint post at the University of Cape Town. He was promoted to full-time lecturer and Director of Surgical Research at the University of Cape Town. In 1960, he flew to Moscow in order to meet Vladimir Demikhov, a top expert on organ transplants (later he credited Demikhov's accomplishment saying that "if there is a father of heart and lung transplantation then Demikhov certainly deserves this title.") In 1961 he was appointed Head of the Division of Cardiothoracic Surgery at the teaching hospitals of the University of Cape Town. He rose to the position of Associate Professor in the Department of Surgery at the University of Cape Town in 1962. Barnard's younger brother Marius, who also studied medicine, eventually became Barnard's right-hand man at the department of Cardiac Surgery. Over time, Barnard became known as a brilliant surgeon with many contributions to the treatment of cardiac diseases, such as the Tetralogy of Fallot and Ebstein's anomaly. He was promoted to Professor of Surgical Science in the Department of Surgery at the University of Cape Town in 1972. In 1981, Barnard became a founding member of the World Cultural Council. Among the many awards he received over the years, he was named Professor Emeritus in 1984.

Following the first successful kidney transplant in 1953, in the United States, Barnard performed the second kidney transplant in South Africa in October 1967, the first being done in Johannesburg the previous year.

On 23 January 1964, James Hardy at the University of Mississippi Medical Center in Jackson, Mississippi, performed the world's first heart transplant and world's first cardiac xenotransplant by transplanting the heart of a chimpanzee into a desperately ill and dying man. This heart did beat in the patient's chest for approximately 60 to 90 minutes. The patient, Boyd Rush, died without ever regaining consciousness.

Barnard had experimentally transplanted forty-eight hearts into dogs, which was about a fifth the number that Adrian Kantrowitz had performed at Maimonides Medical Center in New York and about a sixth the number Norman Shumway had performed at Stanford University in California. Barnard had no dogs which had survived longer than ten days, unlike Kantrowitz and Shumway who had had dogs survive for more than a year.
With the availability of new breakthroughs introduced by several pioneers, also including Richard Lower at the Medical College of Virginia, several surgical teams were in a position to prepare for a human heart transplant. Barnard had a patient willing to undergo the procedure, but as with other surgeons, he needed a suitable donor.

During the Apartheid era in South Africa, non-white persons and citizens were not given equal opportunities in the medical professions. At Groote Schuur Hospital, Hamilton Naki was an informally taught surgeon. He started out as a gardener and cleaner. One day he was asked to help out with an experiment on a giraffe. From this modest beginning, Naki became principal lab technician and taught hundreds of surgeons, and assisted with Barnard's organ transplant program. Barnard said, "Hamilton Naki had better technical skills than I did. He was a better craftsman than me, especially when it came to stitching, and had very good hands in the theatre". A popular myth, propagated principally by a widely discredited documentary film called "Hidden Heart" and an erroneous newspaper article , maintains incorrectly that Naki was present during the Washkansky transplant.

Barnard performed the world's first human-to-human heart transplant operation in the early morning hours of Sunday 3 December 1967. Louis Washkansky, a 54-year-old grocer who was suffering from diabetes and incurable heart disease, was the patient. Barnard was assisted by his brother Marius Barnard, as well as a team of thirty persons. The operation lasted approximately five hours.

Barnard stated to Washkansky and his wife Ann Washkansky that the transplant had an 80% chance of success. This has been criticised by the ethicists Peter Singer and Helga Kuhse as making claims for chances of success to the patient and family which were "unfounded" and "misleading".
Barnard later wrote, "For a dying man it is not a difficult decision because he knows he is at the end. If a lion chases you to the bank of a river filled with crocodiles, you will leap into the water, convinced you have a chance to swim to the other side." The donor heart came from a young woman, Denise Darvall, who had been rendered brain dead in an accident on 2 December 1967, while crossing a street in Cape Town. On examination at Groote Schuur hospital, Darvall had two serious fractures in her skull, with no electrical activity in her brain detected, and no sign of pain when ice water was poured into her ear. Coert Venter and Bertie Bosman requested permission from Darvall's father for Denise's heart to be used in the transplant attempt. The afternoon before his first transplant, Barnard dozed at his home while listening to music. When he awoke, he decided to modify Shumway and Lower's technique. Instead of cutting straight across the back of the atrial chambers of the donor heart, he would avoid damage to the septum and instead cut two small holes for the venae cavae and pulmonary veins. Prior to the transplant, rather than wait for Darvall's heart to stop beating, at his brother Marius Barnard's urging, Christiaan had injected potassium into her heart to paralyse it and render her technically dead by the whole-body standard. Twenty years later, Marius Barnard recounted, "Chris stood there for a few moments, watching, then stood back and said, 'It works.'"

Washkansky survived the operation and lived for 18 days, having succumbed to pneumonia as he was taking immunosuppressive drugs.

Barnard was celebrated around the world for his accomplishment. He was photogenic, and enjoyed the media attention following the operation.

Worldwide, approximately 100 transplants were performed by various doctors during 1968. Only a third of these patients lived longer than three months. A U.S. National Institutes of Health publication states, "Within several years, only Shumway's team at Stanford was attempting transplants."

Barnard's second transplant operation was conducted on 2 January 1968, and the patient, Philip Blaiberg, survived for 19 months. Dirk van Zyl, who received a new heart in 1971, was the longest-lived recipient, surviving over 23 years.

Between December 1967 and November 1974 at Groote Schuur Hospital in Cape Town, South Africa, ten heart transplants were performed, as well as a heart and lung transplant in 1971. Of these ten patients, four lived longer than 18 months, with two of these four becoming long-term survivors. One patient lived for over thirteen years and another for over twenty-four years.

Full recovery of donor heart function often takes place over hours or days, during which time considerable damage can occur. Other deaths to patients can occur from preexisting conditions. For example, in pulmonary hypertension the patient's right ventricle has often adapted to the higher pressure over time and, although diseased and hypertrophied, is often capable of maintaining circulation to the lungs. Barnard designed the idea of the heterotopic (or "piggy back" transplant) in which the patient's diseased heart is left in place while the donor heart is added, essentially forming a "double heart". Barnard performed the first such heterotopic heart transplant in 1974.

From November 1974 through December 1983, 49 consecutive heterotopic heart transplants on 43 patients were performed at Groote Schuur. The survival rate for patients at one year was over 60%, as compared to less than 40% with standard transplants, and the survival rate at five years was over 36% as compared to less than 20% with standard transplants.

Many surgeons gave up cardiac transplantation due to poor results, often due to rejection of the transplanted heart by the patient's immune system. Barnard persisted until the advent of cyclosporine, an effective immunosuppressive drug, which helped revive the operation throughout the world. He also attempted xenotransplantation in a human patient, while attempting to save the life of a girl who was unable to leave artificial life support after her second aortic valve replacement.

Barnard was an outspoken opponent of South Africa's laws of apartheid, and was not afraid to criticise his nation's government, although he had to temper his remarks to some extent to travel abroad. Rather than leaving his homeland, he used his fame to campaign for a change in the law. Christiaan's brother, Marius Barnard, went into politics, and was elected to the legislature from Progressive Federal Party. Barnard later stated that the reason he never won the Nobel Prize in Physiology or Medicine was probably because he was a "white South African".

Shortly before his visit to Kenya in 1978, the following was written about his views regarding race relations in South Africa;
"While he believes in the participation of Africans in the political process of South Africa, he is opposed to a one-man-one-vote system in South Africa".

In answering a hypothetical question on how he would solve the race problem were he a "benevolent dictator in South Africa", Barnard stated the following in a long interview at the Weekly Review:

The interview ended with the following summary from he himself;
"I often say that, like King Lear, South Africa is a country more sinned against than sinning."

Barnard's first marriage was to Aletta Gertruida Louw, a nurse, whom he married in 1948 while practising medicine in Ceres. The couple had two children — Deirdre (born 1950) and Andre (1951–1984). International fame took a toll on his personal life, and in 1969, Barnard and his wife divorced. In 1970, he married heiress Barbara Zoellner when she was 19, the same age as his son, and they had two children — Frederick (born 1972) and Christiaan Jr. (born 1974). He divorced Zoellner in 1982. Barnard married for a third time in 1988 to Karin Setzkorn, a young model. They also had two children, Armin (born 1989) and Lara (born 1997), but this last marriage also ended in divorce in 2000.

Barnard described in his autobiography "The Second Life" a one-night extramarital affair with Italian film star Gina Lollobrigida, that occurred in January 1968. During that visit to Rome he received an audience from Pope Paul VI.

In October 2016, U.S. Congresswoman Ann McLane Kuster (D-NH) stated that Barnard sexually assaulted her when she was 23 years old. According to Kuster, he attempted to grope her under her skirt, while seated at a business luncheon with Rep. Pete McCloskey (R-CA), whom she worked for at the time.

Barnard retired as Head of the Department of Cardiothoracic Surgery in Cape Town in 1983 after developing rheumatoid arthritis in his hands which ended his surgical career. He had struggled with arthritis since 1956, when it was diagnosed during his postgraduate work in the United States. After retirement, he spent two years as the Scientist-In-Residence at the Oklahoma Transplantation Institute in the United States and as an acting consultant for various institutions.

He had by this time become very interested in anti-aging research, and his reputation suffered in 1986 when he promoted "Glycel", an expensive "anti-aging" skin cream, whose approval was withdrawn by the United States Food and Drug Administration soon thereafter. He also spent time as a research advisor to the Clinique la Prairie, in Switzerland, where the controversial "rejuvenation therapy" was practised.

Barnard divided the remainder of his years between Austria, where he established the Christiaan Barnard Foundation, dedicated to helping underprivileged children throughout the world, and his game farm in Beaufort West, South Africa.

Christiaan Barnard died on 2 September 2001, while on holiday in Paphos, Cyprus. Early reports stated that he had died of a heart attack, but an autopsy showed his death was caused by a severe asthma attack.

Christiaan Barnard wrote two autobiographies. His first book, "One Life", was published in 1969 () and sold copies worldwide. Some of the proceeds were used to set up the Chris Barnard Fund for research into heart disease and heart transplants in Cape Town. His second autobiography, "The Second Life", was published in 1993, eight years before his death ().

Apart from his autobiographies, Dr Barnard also wrote several other books including:




</doc>
<doc id="7016" url="https://en.wikipedia.org/wiki?curid=7016" title="Concubinage">
Concubinage

Concubinage () is an interpersonal and sexual relationship in which the couple are not or cannot be married. The inability to marry may be due to multiple factors such as differences in social rank status, an existing marriage, religious or professional prohibitions (for example Roman soldiers), or a lack of recognition by appropriate authorities. The woman in such a relationship is referred to as a concubine (), and occasionally so is a man in such a relationship.

The prevalence of concubinage and the status of rights and expectations of a concubine have varied among cultures, as have the rights of children of a concubine. Whatever the status and rights of the concubine, they were always inferior to those of the wife and typically neither she nor her children had rights of inheritance. Historically, concubinage was frequently entered into voluntarily (by the woman or her family) as it provided a measure of economic security for the woman. Involuntary or servile concubinage sometimes involved sexual slavery of one member of the relationship, usually the woman. Nevertheless, sexual relations outside marriage were not uncommon, especially among royalty and nobility, and the woman in such relationships was commonly described as a mistress. The children of such relationships were counted as illegitimate and were barred from inheriting the father's title or estates, even when there was an absence of legitimate heirs.

While forms of long-term sexual relationships and co-habitation short of marriage have become increasingly common in the Western world, these are generally not described as concubinage. The terms concubinage and concubine are used today primarily when referring to non-marital partnerships of earlier eras. In modern usage, a non-marital domestic relationship is commonly referred to as co-habitation (or similar terms), and the woman in such a relationship is generally referred to as a girlfriend, mistress, fiancée, lover or life partner.

Concubinage was highly popular before early 20th century all over Asia. The main function of concubinage was producing additional heirs, as well as bringing males pleasure. Children of concubines had lower rights in account to inheritance, which was regulated by the Dishu system.

In China, successful men often had concubines until the practice was outlawed after the Communist Party of China came to power in 1949. The standard Chinese term translated as "concubine" was "qiè" 妾, a term used since ancient times, which means "female slave". Concubines resembled wives () in that they were recognized sexual partners of a male family member and were expected to bear children from him. Unofficial concubines () are of lower status, and children of her are considered illegitimate. In English the term concubine is also used for what the Chinese refer to as "pínfēi" () "consorts of emperors", some of very high rank.

In premodern China, it was illegal and socially disreputable for a man to have more than one wife at a time, but he could have concubines. At first a man could have as many concubines as he could afford, however, from the Eastern Han (AD 25–220) onward, the maximal number of concubines a man could have was limited by law. The higher ranking and the more noble an identity a man possessed, the more concubines he was permitted to have.

A concubine's treatment and situation were highly variable and were influenced by the social status of the male to whom she was engaged, as well as the attitude of the wife. In the "Book of Rites" chapter on “The Pattern of the Family” () it says: “If there were betrothal rites, she became a wife; and if she went without these, a concubine.” Besides, wives were married with dowries but concubines were not. Concubines could be taken without any of the ceremonies used in marriages. And neither remarriage nor a return to her natal home in widowhood were allowed.

The position of the concubine was generally inferior to that of the wife. Although a concubine could produce heirs, her children would be inferior in social status to wife's children but were still better than illegitimate children. The child of a concubine had to show filial duty to two women, their biological mother and legal mother—the wife of their father. After the death of a concubine, her sons would make an offering to her, but these offerings were not continued by the concubine's grandsons, who only made offerings to their grandfather’s wife.

In ancient times, concubines were allegedly buried alive with their masters to "keep them company in the afterlife". Until the Song dynasty (960–1276), it was treated as a serious breach of social ethics to promote a concubine to a wife.

During the Qing China (1644–1911), the status of concubines improved. It became permissible to promote a concubine to wife, if the wife had died and the concubine was the mother of the only surviving sons. Moreover, the prohibition against forcing a widow to remarry was extended to widowed concubines. Tablets for concubine-mothers seem to have been more commonly placed in family ancestral altars and genealogies of some lineages listed concubine-mothers.

Imperial concubines, kept by emperors in the Forbidden City, had different ranks and were traditionally guarded by eunuchs to ensure that they could not be impregnated by anyone but the emperor. In Ming China (1368-1644), there was an official system to select concubines for the emperor. The age of the candidates ranged mainly from 14 to 16. Virtues, behavior, character, appearance and body condition were taken as selection criteria.

Despite the limitations imposed on Chinese concubines, there are several examples of concubines who achieved great power and influence in history and literature. Lady Yehenara, otherwise known as Empress Dowager Cixi, was arguably one of the most successful concubines in China's history. Cixi first entered the court as a concubine to the Xianfeng Emperor and gave birth to his only surviving son, who would become the Tongzhi Emperor. She eventually become the "de facto" ruler of Qing China for 47 years after her husband's death.

A display of concubinage is in one of the Four Great Classical Novels, "Dream of the Red Chamber" (believed to be a semi-autobiographical account of author Cao Xueqin's family life). Three generations of the Jia family are supported by one notable concubine of the emperor, Jia Yuanchun, the full elder sister of the male protagonist Jia Baoyu. In contrast, their younger half-siblings by Concubine Zhao, Jia Tanchun and Jia Huan, developed distorted personalities, being children of concubine. Tanchun insisted that the brother of her father's wife Madam Wang, instead of the brother of Concubine Zhao, is her uncle and strive to be excellent in the girls to overcome her inferiority. Wang Xifeng stated that occasionally nobles seeking marriage would value the bride from her Dishu (being born by wife or concubine) status. Jia Baoyu himself has an unofficial concubine Hua Xiren, whom he had first sexual encounter with, but remain deep spiritual love to his cousin Lin Daiyu and intend to marry her.

The concept of men having relationships with one or more concubines has seen a comeback since modern China has prospered. Mistresses are often viewed as concubines, inferior to the wife in status.

The women called er nai, unofficial concubines in the 21st century, typically say they feel fine about exploiting their youth, beauty and wombs for the sake of earning money and protection from their men, and not having to live with the primary wives any more as in the past. The one-child policy in Mainland China also pushed those men with power and wealth to pursue a male heir.

Emperors' concubines and harems are emphasized in 21st-century romantic novels written for female readers and set in ancient times. As a plot element, the children of concubines are depicted with a status much inferior to that in actual history. The zhai dou (residential intrigue) and gong dou (harem intrigue) genres show concubines and wives, as well as their children, scheming secretly to access power. Empresses in the Palace, a "gong dou" type novel and TV drama, has had great success in 21st-century China.

Hong Kong officially abolished the Great Qing Legal Code in 1971, which makes concubinage illegal. Stanley Ho of Macau took his "2nd wife" as his official concubine in 1957, and his "3rd and 4th wife" retain no official status.

Before monogamy was legally imposed in the Meiji period, concubinage was common among the nobility. Its purpose was to ensure male heirs. For example, the son of an Imperial concubine often had a chance of becoming emperor. Yanagihara Naruko, a high-ranking concubine of Emperor Meiji, gave birth to Emperor Taishō, who was later legally adopted by Empress Haruko, Emperor Meiji's formal wife. Even among merchant families, concubinage was occasionally used to ensure heirs. Asako Hirooka, an entrepreneur who was the daughter of a concubine, worked hard to help her husband's family survive after the Meiji Restoration. She lost her fertility giving birth to her only daughter, Kameko; so her husband—with whom she got along well—took Asako's maid-servant as a concubine and fathered three daughters and a son with her. Kameko, as the child of the formal wife, married a noble man and matrilineally carried on the family name.

Joseon monarchs had a harem which contains concubines of different ranks. Empress Myeongseong managed to have sons, preventing sons of concubines getting power.

Children of concubines often have lower value in account of marriage. A daughter of concubine cannot be the wife of a wife-born son of the same class. For example, Jang Nok-su is a concubine-born daughter of a mayor, who was initially married to a slave-servant, later a high-rank concubine of Yeonsangun.

Before 1935, the family law listed three kind of wives - official wife, minor wife and slave wife.

In Ancient Greece, the practice of keeping a slave concubine ( "pallakís") was little recorded but appears throughout Athenian history. The law prescribed that a man could kill another man caught attempting a relationship with his concubine for the production of free children, which suggests that a concubine's children were not granted citizenship. While references to the sexual exploitation of maidservants appear in literature, it was considered disgraceful for a man to keep such women under the same roof as his wife. Some interpretations of "hetaera" have held they were concubines when they had a permanent relationship with a single man.

Concubinage was an institution practiced in ancient Rome that allowed a man to enter into an informal but recognized relationship with a woman ("concubina", plural "concubinae") who was not his wife, most often a woman whose lower social status was an obstacle to marriage. Concubinage was "tolerated to the degree that it did not threaten the religious and legal integrity of the family". It was not considered derogatory to be called a "concubina", as the title was often inscribed on tombstones.

A "concubinus" was a young male slave sexually exploited by his master as a sexual partner (see homosexuality in ancient Rome). These relations, however, were expected to play a secondary role to marriage, within which institution an adult male demonstrated his masculine authority as head of the household ("pater familias"). In one of his epithalamiums, Catullus ("fl." mid-1st century BC) assumes that the young bridegroom has a "concubinus" who considers himself elevated above the other slaves, but who will be set aside as his master turns his attention to marriage and family life.

Among the Israelites, men commonly acknowledged their concubines, and such women enjoyed the same rights in the house as legitimate wives.

The concubine may not have commanded the exact amount of respect as the wife. In the Levitical rules on sexual relations, the Hebrew word that is commonly translated as "wife" is distinct from the Hebrew word that means "concubine". However, on at least one other occasion the term is used to refer to a woman who is not a wife specifically, the handmaiden of Jacob's wife. In the Levitical code, sexual intercourse between a man and a wife of a different man was forbidden and punishable by death for both persons involved. Since it was regarded as the highest blessing to have many children, wives often gave their maids to their husbands if they were barren, as in the cases of Sarah and Hagar, and Rachel and Bilhah. The children of the concubine often had equal rights with those of the wife; for example, King Abimelech was the son of Gideon and his concubine. Later biblical figures such as Gideon, and Solomon had concubines in addition to many childbearing wives. For example, the Books of Kings say that Solomon had 700 wives and 300 concubines.
The account of the unnamed Levite in Judges 19–20 shows that the taking of concubines was not the exclusive preserve of Kings or patriarchs in Israel during the time of the Judges, and that the rape of a concubine was completely unacceptable to the Israelite nation and led to a civil war. In the story, the Levite appears to be an ordinary member of the tribe dedicated to the worship of God, who was undoubtedly dishonored both by the unfaithfulness of his concubine and her abandonment of him. However, after four months, he decides to make her fall in love with him again at her father’s house; he brought a servant and two asses to show off what glory he has. Her father seeks to keep him there until one day he refuses to remain and leaves. hospitality he is offered at Gibeah, the way in which his host's daughter is offered to the townsmen and the circumstances of his concubine's death at their hands describe a lawless time where visitors are both welcomed and threatened in equal measure.The Levite and his (male) host seek to protect themselves by offering their womrmfolk, both the host’s virgin daughter and his companion’s concubine, to their aggressors for sex, in exchange for their own safety. In the morning, the Levite tries to wake her up, but then realizes that she is dead. He dismembers her body and distributes her (body parts) throughout the nation of Israel to remind them of the blessing that God gave them of liberating them from the likewise sexually vicious and sadistic land of Egypt, and to inform them of the horribleness of the land of Gibeah. The sadistic rape of the concubine is considered outrageous by the Israelite tribesmen, who then wreak total retribution on the men of Gibeah and the surrounding tribe of Benjamin when they support the Gibeans, killing them without mercy and burning all their towns. The inhabitants of (the town of) Jabesh Gilead are then slaughtered as a punishment for not joining the eleven tribes in their war against the Benjamites, and their four hundred unmarried daughters given in forced marriage to the six hundred Benjamite survivors. Finally, the two hundred Benjamite survivors who still have no wives are granted a mass marriage by abduction by the other tribes.

In Judaism, concubines are referred to by the Hebrew term pilegesh (). The term is a loanword from Ancient Greek , meaning "a mistress staying in house".

According to the Babylonian Talmud, the difference between a concubine and a full wife was that the latter received a ketubah and her marriage ("nissu'in") was preceded by an erusin ("formal betrothal"). Neither was the case for a concubine. One opinion in the Jerusalem Talmud argues that the concubine should also receive a "marriage contract", but without a clause specifying a divorce settlement.

Certain Jewish thinkers, such as Maimonides, believed that concubines were strictly reserved for kings, and thus that a commoner may not have a concubine. Indeed, such thinkers argued that commoners may not engage in any type of sexual relations outside of a marriage.

Maimonides was not the first Jewish thinker to criticise concubinage. For example, Leviticus Rabbah severely condemns the custom. Other Jewish thinkers, such as Nahmanides, Samuel ben Uri Shraga Phoebus, and Jacob Emden, strongly objected to the idea that concubines should be forbidden.

In the Hebrew of the contemporary State of Israel, "pilegesh" is often used as the equivalent of the English word "mistress"—i.e., the female partner in extramarital relations—regardless of legal recognition. Attempts have been initiated to popularise "pilegesh" as a form of premarital, non-marital or extramarital relationship (which, according to the perspective of the enacting person(s), is permitted by Jewish law).

Sexual slavery as concubinage in Islamic sexual jurisprudence is permitted in Islam which was not considered prostitution, and was very common during the Arab slave trade throughout the Middle Ages and early modern period, when women and girls from the Caucasus, Africa, Central Asia and Europe were captured and served as concubines in the harems of the Arab World. Ibn Battuta tells us several times that he was given or purchased female slaves. 

Concubinage is permitted and regulated in Islam. Al-Muminun 6 and Al-Maarij 30 both, in identical wording, draw a distinction between spouses and "those whom one's right hands possess" (concubine/sexual slaves), saying " أَزْوَاجِهِمْ أَوْ مَا مَلَكَتْ أَيْمَانُهُمْ" (literally, "their spouses or what their right hands possess"), while clarifying that sexual intercourse with either is permissible. Sayyid Abul Ala Maududi explains that "two categories of women have been excluded from the general command of guarding the private parts: (a) wives, (b) women who are legally in one's possession". "Concubine" ("surriyya") refers to the female slave ("jāriya"), whether Muslim or non-Muslim, with whom her master engages in sexual intercourse. The word ""surriyya"" is not mentioned in the Qur'an. However, the expression "Ma malakat aymanukum" (that which your right hands own), which occurs fifteen times in the sacred book, refers to slaves and therefore, though not necessarily, to concubines. Concubinage was a pre-Islamic custom that was allowed to be practiced under Islam with Jews and non-Muslim people to marry concubine after teaching her and instructing her well and then giving them freedom. Rationale given for recognition of concubinage in Islam is that "it satisfied the sexual desire of the female slaves and thereby prevented the spread of immorality in the Muslim community." Most schools restrict concubinage to a relationship where the female slave is required to be monogamous to her master (though the master's monogamy to her is not required), but according to Sikainga, "in reality, however, female slaves in many Muslim societies were prey for [male] members of their owners' household, their [owner's male] neighbors, and their [owner's male] guests." 
Concubines were common in pre-Islamic Arabia and when Islam arrived, it had a society with concubines. Islam introduced legal restrictions to the concubinage and encouraged manumission. In verse 23:6 in the Quran it is allowed to have sexual intercourse with concubines after marrying them, as Islam forbids sexual intercourse outside of marriage.
Children of former concubines were generally declared as legitimate as they were born in wedlock, and the mother of a free child was considered free upon the death of the male partner.

According to Shia Muslims, Muhammad sanctioned Nikah mut‘ah (fixed-term marriage, called muta'a in Iraq and sigheh in Iran) which has instead been used as a legitimizing cover for sex workers, in a culture where prostitution is otherwise forbidden. Some Western writers have argued that mut'ah approximates prostitution. Julie Parshall writes that mut'ah is legalised prostitution which has been sanctioned by the Twelver Shia authorities. She quotes the Oxford encyclopedia of modern Islamic world to differentiate between marriage (nikah) and Mut'ah, and states that while nikah is for procreation, mut'ah is just for sexual gratification. According to Zeyno Baran, this kind of temporary marriage provides Shi'ite men with a religiously sanctioned equivalent to prostitution. According to Elena Andreeva's observation published in 2007, Russian travellers to Iran consider mut'ah to be "legalized profligacy" which is indistinguishable from prostitution. Religious supporters of mut'ah argue that temporary marriage is different from prostitution for a couple of reasons, including the necessity of iddah in case the couple have sexual intercourse. It means that if a woman marries a man in this way and has sex, she has to wait for a number of months before marrying again and therefore, a woman cannot marry more than 3 or 4 times in a year.

In ancient times, two sources for concubines were permitted under an Islamic regime. Primarily, non-Muslim women taken as prisoners of war were made concubines as happened after the Battle of the Trench, or in numerous later Caliphates.
It was encouraged to manumit slave women who rejected their initial faith and embraced Islam, or to bring them into formal marriage.
According to the rules of Islamic Fiqh, what is "halal" (permitted) by Allah in the Quran cannot be altered by any authority or individual. Therefore, although the "concept" of concubinage is "halal", concubines are mostly no longer available in this modern era nor allowed to be sold or purchased in accordance with the latest human rights standards. However, as change of existing Islamic law is impossible, a concubine in this modern era, if existing, must be given all the due rights that Islam had preserved in the past.

It is further clarified that all domestic and organizational female employees are not concubines in this era and hence sex is forbidden with them unless Nikah (formal marriage) or Nikah mut‘ah (temporary marriage which only Shi'ah Islam permits; some Sunni Muslims practice Nikah Misyar, or "traveller's marriage") is committed through the proper channels.

When slavery became institutionalized in the North American colonies, white men, whether or not they were married, sometimes took enslaved women as concubines. Marriage between the races was prohibited by law in the colonies and the later United States. Many colonies and states also had laws against miscegenation, or any interracial relations. From 1662 the Colony of Virginia, followed by others, incorporated into law the principle that children took their mother's status, i.e., the principle of "partus sequitur ventrem". All children born to enslaved mothers were born into slavery, regardless of their father's status or ancestry. This led to generations of multiracial slaves, some of whom were otherwise considered legally white (one-eighth or less African, equivalent to a great-grandparent) before the American Civil War.

In some cases, men had long-term relationships with enslaved women, giving them and their mixed-race children freedom and providing their children with apprenticeships, education and transfer of capital. A purported relationship between Thomas Jefferson and Sally Hemings is an example of this. Such arrangements were more prevalent in the Southern states during the antebellum years.

In Louisiana and former French territories, a formalized system of concubinage called "plaçage" developed. European men took enslaved or free women of color as mistresses after making arrangements to give them a dowry, house or other transfer of property, and sometimes, if they were enslaved, offering freedom and education for their children. A third class of free people of color developed, especially in New Orleans. Many became educated, artisans and property owners. French-speaking and practicing Catholicism, these women combined French and African-American culture and created an elite between those of European descent and the slaves. Today, descendants of the free people of color are generally called Louisiana Creole people.



</doc>
<doc id="7017" url="https://en.wikipedia.org/wiki?curid=7017" title="Central Plaza (Hong Kong)">
Central Plaza (Hong Kong)

Central Plaza is a 78-storey, skyscraper completed in August 1992 at 18 Harbour Road, in Wan Chai on Hong Kong Island in Hong Kong. It is the third tallest tower in the city after 2 International Finance Centre in Central and the ICC in West Kowloon. It was the tallest building in Asia from 1992 to 1996, until the Shun Hing Square in neighbouring Shenzhen was built. Central Plaza surpassed the Bank of China Tower as the tallest building in Hong Kong until the completion of 2 IFC.

Central Plaza was also the tallest reinforced concrete building in the world, until it was surpassed by CITIC Plaza, Guangzhou. The building uses a triangular floor plan. On the top of the tower is a four-bar neon clock that indicates the time by displaying different colours for 15-minute periods, blinking at the change of the quarter.

An anemometer is installed on the tip of the building's mast, at above sea level. The mast has a height of . It also houses the world's highest church inside a skyscraper, Sky City Church.

The land upon which Central Plaza sits was reclaimed from Victoria Harbour in the 1970s. The site was auctioned off by the Hong Kong Government at City Hall Theatre on 25 January 1989. It was sold for a record HK$3.35 billion to a joint venture called "Cheer City Properties", owned 50 per cent by Sun Hung Kai Properties and 50 per cent by Sino Land and shareholders the Ng family. A third developer, Ryoden Development, joined the consortium afterward.

The first major tenant to sign a lease was the Provisional Airport Authority, who on 2 August 1991 agreed to lease the 24th to 26th floors. A topping-out ceremony, presided over by Sir David Ford, was held on 9 April 1992.

Central Plaza is made up of two principal components: a free standing office tower and a podium block attached to it. The tower is made up of three sections: a tower base forming the main entrance and public circulation spaces; a tower body containing 57 office floors, a sky lobby and five mechanical plant floors; and the tower top consist of six mechanical plant floors and a tower mast.

The ground level public area along with the public sitting out area form an landscaped garden with fountain, trees and artificial stone paving. No commercial element is included in the podium. The first level is a public thoroughfare for three pedestrian bridges linking the Mass Transit Railway, the Convention and Exhibition Centre and the China Resource Building. By turning these space to public use, the building got 20% plot ratio more as bonus. The shape of the tower is not truly triangular but with its three corners cut off to provide better internal office spaces.

Central Plaza was designed by the Hong Kong architectural firm Ng Chun Man and Associates and engineered by Arup. The main contractor was a joint venture, comprising the contracting firms Sanfield and Tat Lee, called Manloze Ltd.

The building was designed to be triangular in shape because it would allow 20% more of the office area to enjoy the harbour view as compared with a square or rectangular shaped buildings. From an architectural point of view, this arrangement provides better floor area utilisation, offering an internal column-free office area with a clear depth of and an overall usable floor area efficiency of 81%.
Nonetheless, the triangular building plan causes the air handling unit (AHU) room in the internal core to also assume a triangular configuration. With only limited space, this makes the adoption of a standard AHU not feasible. Furthermore, all air-conditioning ducting, electrical trunking and piping gathered inside the core area has to be squeezed into a very narrow and congested corridor ceiling void.

As the building is situated opposite to the HKCEC, the only way to get more sea view for the building and not be obstructed by the neighbouring high-rise buildings is to build it tall enough. However, a tall building brings a lot of difficulties to structural and building services design, for example, excessive system static pressure for water systems, high line voltage drop and long distance of vertical transportation. All these problems can increase the capital cost of the building systems and impair the safety operation of the building.

As a general practice, for achieving a clear height of , a floor-to-floor height of would be required. However, because of high windload in Hong Kong for such a super high-rise building, every increase in building height by a metre would increase the structural cost by more than HK$1 million (HK$304,800 per ft). Therefore, a comprehensive study was conducted and finally a floor height of was adopted. With this issue alone, an estimated construction cost saving for a total of 58 office floors, would be around HK$30 million. Yet at the same time, a maximum ceiling height of in office area could still be achieved with careful coordination and dedicated integration.


Steel structure is more commonly adopted in high-rise building. In the original scheme, an externally cross-braced framed tube was applied with primary/secondary beams carrying metal decking with reinforced concrete slab. The core was also of steelwork, designed to carry vertical load only. Later after a financial review by the developer, they decided to reduce the height of the superstructure by increasing the size of the floor plate so as to reduce the complex architectural requirements of the tower base which means a highstrength concrete solution became possible.

In the final scheme, columns at centres and floor edge beams were used to replace the large steel corner columns. As climbing form and table form construction method and efficient construction management are used in this project which make this reinforced concrete structure take no longer construction time than the steel structure. And the most attractive point is that the reinforced concrete scheme can save HK$230 million compared to that of steel structure. Hence the reinforced concrete structure was adopted and Central Plaza is now one of the tallest reinforced concrete buildings in the world.

In the reinforced concrete structure scheme, the core has a similar arrangement to the steel scheme and the wind shear is taken out from the core at the lowest basement level and transferred to the perimeter diaphragm walls. In order to reduce large shear reversals in the core walls in the basement, and at the top of the tower base level, the ground floor, basement levels 1 and 2 and the 5th and 6th floors, the floor slabs and beams are separated horizontally from the core walls.

Another advantage of using reinforced concrete structure is that it is more flexible to cope with changes in structural layout, sizes and height according to the site conditions by using table form system.



</doc>
<doc id="7018" url="https://en.wikipedia.org/wiki?curid=7018" title="Caravaggio">
Caravaggio

Michelangelo Merisi (Michele Angelo Merigi or Amerighi) da Caravaggio (, ; ; 28 September 1571 – 18 July 1610) was an Italian painter active in Rome, Naples, Malta, and Sicily from the early 1590s to 1610. His paintings combine a realistic observation of the human state, both physical and emotional, with a dramatic use of lighting, and they had a formative influence on Baroque painting.

Caravaggio employed close physical observation with a dramatic use of chiaroscuro that came to be known as tenebrism. He made the technique a dominant stylistic element, darkening shadows and transfixing subjects in bright shafts of light. Caravaggio vividly expressed crucial moments and scenes, often featuring violent struggles, torture and death. He worked rapidly, with live models, preferring to forego drawings and work directly onto the canvas. His influence on the new Baroque style that emerged from Mannerism was profound. It can be seen directly or indirectly in the work of Peter Paul Rubens, Jusepe de Ribera, Gian Lorenzo Bernini, and Rembrandt, and artists in the following generation heavily under his influence were called the "Caravaggisti" or "Caravagesques", as well as tenebrists or "tenebrosi" ("shadowists"). 

Caravaggio trained as a painter in Milan before moving in his twenties to Rome. He developed a considerable name as an artist, and as a violent, touchy and provocative man. A brawl led to a death sentence for murder and forced him to flee to Naples. There he again established himself as one of the most prominent Italian painters of his generation. He traveled in 1607 to Malta and on to Sicily, and pursued a papal pardon for his sentence. In 1609 he returned to Naples, where he was involved in a violent clash; his face was disfigured and rumours of his death circulated. Questions about his mental state arose from his erratic and bizarre behavior. He died in 1610 under uncertain circumstances while on his way from Naples to Rome. Reports stated that he died of a fever, but suggestions have been made that he was murdered or that he died of lead poisoning.

Caravaggio's innovations inspired Baroque painting, but the Baroque incorporated the drama of his chiaroscuro without the psychological realism. The style evolved and fashions changed, and Caravaggio fell out of favor. In the 20th century interest in his work revived, and his importance to the development of Western art was reevaluated. The 20th-century art historian André Berne-Joffroy stated, "What begins in the work of Caravaggio is, quite simply, modern painting."

Caravaggio (Michelangelo Merisi or Amerighi) was born in Milan, where his father, Fermo (Fermo Merixio), was a household administrator and architect-decorator to the Marchese of Caravaggio, a town not far from the city of Bergamo. His mother, Lucia Aratori (Lutia de Oratoribus), came from a propertied family of the same district. In 1576 the family moved to Caravaggio (Caravaggius) to escape a plague that ravaged Milan, and Caravaggio's father and grandfather both died there on the same day in 1577. It is assumed that the artist grew up in Caravaggio, but his family kept up connections with the Sforzas and with the powerful Colonna family, who were allied by marriage with the Sforzas and destined to play a major role later in Caravaggio's life.

Caravaggio's mother died in 1584, the same year he began his four-year apprenticeship to the Milanese painter Simone Peterzano, described in the contract of apprenticeship as a pupil of Titian. Caravaggio appears to have stayed in the Milan-Caravaggio area after his apprenticeship ended, but it is possible that he visited Venice and saw the works of Giorgione, whom Federico Zuccari later accused him of imitating, and Titian. He would also have become familiar with the art treasures of Milan, including Leonardo da Vinci's "Last Supper", and with the regional Lombard art, a style that valued simplicity and attention to naturalistic detail and was closer to the naturalism of Germany than to the stylised formality and grandeur of Roman Mannerism.

Following his initial training under Simone Peterzano, in 1592 Caravaggio left Milan for Rome, in flight after "certain quarrels" and the wounding of a police officer. The young artist arrived in Rome "naked and extremely needy ... without fixed address and without provision ... short of money." A few months later he was performing hack-work for the highly successful Giuseppe Cesari, Pope Clement VIII's favourite artist, "painting flowers and fruit" in his factory-like workshop.

In Rome there was demand for paintings to fill the many huge new churches and palazzos being built at the time. It was also a period when the Church was searching for a stylistic alternative to Mannerism in religious art that was tasked to counter the threat of Protestantism. Caravaggio's innovation was a radical naturalism that combined close physical observation with a dramatic, even theatrical, use of chiaroscuro that came to be known as tenebrism (the shift from light to dark with little intermediate value).

Known works from this period include a small "Boy Peeling a Fruit" (his earliest known painting), a "Boy with a Basket of Fruit", and the "Young Sick Bacchus", supposedly a self-portrait done during convalescence from a serious illness that ended his employment with Cesari. All three demonstrate the physical particularity for which Caravaggio was to become renowned: the fruit-basket-boy's produce has been analysed by a professor of horticulture, who was able to identify individual cultivars right down to "... a large fig leaf with a prominent fungal scorch lesion resembling anthracnose ("Glomerella cingulata")."

Caravaggio left Cesari, determined to make his own way after a heated argument.. At this point he forged some extremely important friendships, with the painter Prospero Orsi, the architect Onorio Longhi, and the sixteen-year-old Sicilian artist Mario Minniti. Orsi, established in the profession, introduced him to influential collectors; Longhi, more balefully, introduced him to the world of Roman street-brawls. Minniti served Caravaggio as a model and, years later, would be instrumental in helping him to obtain important commissions in Sicily. Ostensibly, the first archival reference to Caravaggio in a contemporary document from Rome is the listing of his name, with that of Prospero Orsi as his partner, as an 'assistante' in a procession in October 1594 in honour of St. Luke. The earliest informative account of his life in the city is a court transcript dated 11 July 1597, when Caravaggio and Prospero Orsi were witnesses to a crime near San Luigi de' Francesi.

An early published notice on Caravaggio, dating from 1604 and describing his lifestyle three years previously, recounts that "after a fortnight's work he will swagger about for a month or two with a sword at his side and a servant following him, from one ball-court to the next, ever ready to engage in a fight or an argument, so that it is most awkward to get along with him." In 1606 he killed a young man in a brawl, possibly unintentionally, and fled from Rome with a death sentence hanging over him.

"The Fortune Teller", his first composition with more than one figure, shows a boy, likely Minniti, having his palm read by a gypsy girl, who is stealthily removing his ring as she strokes his hand. The theme was quite new for Rome, and proved immensely influential over the next century and beyond. This, however, was in the future: at the time, Caravaggio sold it for practically nothing. "The Cardsharps" – showing another naïve youth of privilege falling the victim of card cheats – is even more psychologically complex, and perhaps Caravaggio's first true masterpiece. Like "The Fortune Teller", it was immensely popular, and over 50 copies survive. More importantly, it attracted the patronage of Cardinal Francesco Maria del Monte, one of the leading connoisseurs in Rome. For Del Monte and his wealthy art-loving circle, Caravaggio executed a number of intimate chamber-pieces – "The Musicians", "The Lute Player", a tipsy "Bacchus", an allegorical but realistic "Boy Bitten by a Lizard" – featuring Minniti and other adolescent models.

Caravaggio's first paintings on religious themes returned to realism, and the emergence of remarkable spirituality. The first of these was the "Penitent Magdalene", showing Mary Magdalene at the moment when she has turned from her life as a courtesan and sits weeping on the floor, her jewels scattered around her. "It seemed not a religious painting at all ... a girl sitting on a low wooden stool drying her hair ... Where was the repentance ... suffering ... promise of salvation?" It was understated, in the Lombard manner, not histrionic in the Roman manner of the time. It was followed by others in the same style: "Saint Catherine"; "Martha and Mary Magdalene"; "Judith Beheading Holofernes"; a "Sacrifice of Isaac"; a "Saint Francis of Assisi in Ecstasy"; and a "Rest on the Flight into Egypt". These works, while viewed by a comparatively limited circle, increased Caravaggio's fame with both connoisseurs and his fellow artists. But a true reputation would depend on public commissions, and for these it was necessary to look to the Church.

Already evident was the intense realism or naturalism for which Caravaggio is now famous. He preferred to paint his subjects as the eye sees them, with all their natural flaws and defects instead of as idealised creations. This allowed a full display of his virtuosic talents. This shift from accepted standard practice and the classical idealism of Michelangelo was very controversial at the time. Caravaggio also dispensed with the lengthy preparations traditional in central Italy at the time. Instead, he preferred the Venetian practice of working in oils directly from the subject – half-length figures and still life. "Supper at Emmaus", from c. 1600–1601, is a characteristic work of this period demonstrating his virtuoso talent.

In 1599, presumably through the influence of Del Monte, Caravaggio was contracted to decorate the Contarelli Chapel in the church of San Luigi dei Francesi. The two works making up the commission, the "Martyrdom of Saint Matthew" and "Calling of Saint Matthew", delivered in 1600, were an immediate sensation. Thereafter he never lacked commissions or patrons.

Caravaggio's tenebrism (a heightened chiaroscuro) brought high drama to his subjects, while his acutely observed realism brought a new level of emotional intensity. Opinion among his artist peers was polarized. Some denounced him for various perceived failings, notably his insistence on painting from life, without drawings, but for the most part he was hailed as a great artistic visionary: "The painters then in Rome were greatly taken by this novelty, and the young ones particularly gathered around him, praised him as the unique imitator of nature, and looked on his work as miracles."

Caravaggio went on to secure a string of prestigious commissions for religious works featuring violent struggles, grotesque decapitations, torture and death, most notable and most technically masterful among them "The Taking of Christ" of circa 1602 for the Mattei Family, recently rediscovered in Ireland after two centuries. For the most part each new painting increased his fame, but a few were rejected by the various bodies for whom they were intended, at least in their original forms, and had to be re-painted or find new buyers. The essence of the problem was that while Caravaggio's dramatic intensity was appreciated, his realism was seen by some as unacceptably vulgar.

His first version of "Saint Matthew and the Angel", featuring the saint as a bald peasant with dirty legs attended by a lightly clad over-familiar boy-angel, was rejected and a second version had to be painted as "The Inspiration of Saint Matthew". Similarly, "The Conversion of Saint Paul" was rejected, and while another version of the same subject, the "Conversion on the Way to Damascus", was accepted, it featured the saint's horse's haunches far more prominently than the saint himself, prompting this exchange between the artist and an exasperated official of Santa Maria del Popolo: "Why have you put a horse in the middle, and Saint Paul on the ground?" "Because!" "Is the horse God?" "No, but he stands in God's light!"

Other works included "Entombment", the "Madonna di Loreto" ("Madonna of the Pilgrims"), the "Grooms' Madonna", and the "Death of the Virgin". The history of these last two paintings illustrates the reception given to some of Caravaggio's art, and the times in which he lived. The "Grooms' Madonna", also known as "Madonna dei palafrenieri", painted for a small altar in Saint Peter's Basilica in Rome, remained there for just two days, and was then taken off. A cardinal's secretary wrote: "In this painting there are but vulgarity, sacrilege, impiousness and disgust...One would say it is a work made by a painter that can paint well, but of a dark spirit, and who has been for a lot of time far from God, from His adoration, and from any good thought..."

The "Death of the Virgin", commissioned in 1601 by a wealthy jurist for his private chapel in the new Carmelite church of Santa Maria della Scala, was rejected by the Carmelites in 1606. Caravaggio's contemporary Giulio Mancini records that it was rejected because Caravaggio had used a well-known prostitute as his model for the Virgin. Giovanni Baglione, another contemporary, tells us it was due to Mary's bare legs —a matter of decorum in either case. Caravaggio scholar John Gash suggests that the problem for the Carmelites may have been theological rather than aesthetic, in that Caravaggio's version fails to assert the doctrine of the Assumption of Mary, the idea that the Mother of God did not die in any ordinary sense but was assumed into Heaven. The replacement altarpiece commissioned (from one of Caravaggio's most able followers, Carlo Saraceni), showed the Virgin not dead, as Caravaggio had painted her, but seated and dying; and even this was rejected, and replaced with a work showing the Virgin not dying, but ascending into Heaven with choirs of angels. In any case, the rejection did not mean that Caravaggio or his paintings were out of favour. The "Death of the Virgin" was no sooner taken out of the church than it was purchased by the Duke of Mantua, on the advice of Rubens, and later acquired by Charles I of England before entering the French royal collection in 1671.

One secular piece from these years is "Amor Victorious", painted in 1602 for Vincenzo Giustiniani, a member of Del Monte's circle. The model was named in a memoir of the early 17th century as "Cecco", the diminutive for Francesco. He is possibly Francesco Boneri, identified with an artist active in the period 1610–1625 and known as Cecco del Caravaggio ('Caravaggio's Cecco'), carrying a bow and arrows and trampling symbols of the warlike and peaceful arts and sciences underfoot. He is unclothed, and it is difficult to accept this grinning urchin as the Roman god Cupid – as difficult as it was to accept Caravaggio's other semi-clad adolescents as the various angels he painted in his canvases, wearing much the same stage-prop wings. The point, however, is the intense yet ambiguous reality of the work: it is simultaneously Cupid and Cecco, as Caravaggio's Virgins were simultaneously the Mother of Christ and the Roman courtesans who modeled for them.

Caravaggio led a tumultuous life. He was notorious for brawling, even in a time and place when such behavior was commonplace, and the transcripts of his police records and trial proceedings fill several pages. On 29 May 1606, he killed, possibly unintentionally, a young man named Ranuccio Tomassoni from Terni (Umbria). The circumstances of the brawl and the death of Ranuccio Tomassoni remain mysterious. Several contemporary "avvisi" referred to a quarrel over a gambling debt and a tennis game, and this explanation has become established in the popular imagination. But recent scholarship has made it clear that more was involved. Good modern accounts are to be found in Peter Robb's "M" and Helen Langdon's "Caravaggio: A Life". A theory relating the death to Renaissance notions of honour and symbolic wounding has been advanced by art historian Andrew Graham-Dixon. Whatever the details, it was a serious matter. Previously his high-placed patrons had protected him from the consequences of his escapades, but this time they could do nothing. Caravaggio, outlawed, fled to Naples.

Following the death of Tomassoni, Caravaggio fled first to the estates of the Colonna family south of Rome, then on to Naples, where Costanza Colonna Sforza, widow of Francesco Sforza, in whose husband's household Caravaggio's father had held a position, maintained a palace. In Naples, outside the jurisdiction of the Roman authorities and protected by the Colonna family, the most famous painter in Rome became the most famous in Naples.

His connections with the Colonnas led to a stream of important church commissions, including the "Madonna of the Rosary", and "The Seven Works of Mercy". "The Seven Works of Mercy" depicts the seven corporal works of mercy as a set of compassionate acts concerning the material needs of others. The painting was made for, and is still housed in, the church of Pio Monte della Misericordia in Naples. Caravaggio combined all seven works of mercy in one composition, which became the church's altarpiece.. Alessandro Giardino has also established the connection between the iconography of "The Seven Works of Mercy" and the cultural, scientific and philosophical circles of the painting's commissioners.

Despite his success in Naples, after only a few months in the city Caravaggio left for Malta, the headquarters of the Knights of Malta. Fabrizio Sforza Colonna, Costanza's son, was a Knight of Malta and general of the Order's galleys. He appears to have facilitated Caravaggio's arrival in the island in 1607 (and his escape the next year). Caravaggio presumably hoped that the patronage of Alof de Wignacourt, Grand Master of the Knights, could help him secure a pardon for Tomassoni's death. De Wignacourt was so impressed at having the famous artist as official painter to the Order that he inducted him as a Knight, and the early biographer Bellori records that the artist was well pleased with his success.

Major works from his Malta period include a huge "Beheading of Saint John the Baptist", the only painting to which he put his signature, and a "Portrait of Alof de Wignacourt and his Page", as well as portraits of other leading Knights.. According to Andrea Pomella, "The Beheading of Saint John the Baptist" is widely considered "one of the most important works in Western painting.". Completed in 1608, the painting had been commissioned by the Knights of Malta as an altarpiece and was the largest altarpiece Caravaggio painted. It still hangs in St. John's Co-Cathedral, for which it was commissioned and where Caravaggio himself was inducted and briefly served as a knight.

Yet, by late August 1608, he was arrested and imprisoned, likely the result of yet another brawl, this time with an aristocratic knight, during which the door of a house was battered down and the knight seriously wounded. Caravaggio was imprisoned by the Knights at Valletta, but he managed to escape. By December, he had been expelled from the Order "as a foul and rotten member", a formal phrase used in all such cases.

Caravaggio made his way to Sicily where he met his old friend Mario Minniti, who was now married and living in Syracuse. Together they set off on what amounted to a triumphal tour from Syracuse to Messina and, maybe, on to the island capital, Palermo. In Syracuse and Messina Caravaggio continued to win prestigious and well-paid commissions. Among other works from this period are "Burial of St. Lucy", "The Raising of Lazarus", and "Adoration of the Shepherds". His style continued to evolve, showing now friezes of figures isolated against vast empty backgrounds. "His great Sicilian altarpieces isolate their shadowy, pitifully poor figures in vast areas of darkness; they suggest the desperate fears and frailty of man, and at the same time convey, with a new yet desolate tenderness, the beauty of humility and of the meek, who shall inherit the earth." Contemporary reports depict a man whose behaviour was becoming increasingly bizarre, which included sleeping fully armed and in his clothes, ripping up a painting at a slight word of criticism, and mocking local painters.

Caravaggio displayed bizarre behaviour from very early in his career. Mancini describes him as "extremely crazy", a letter of Del Monte notes his strangeness, and Minniti's 1724 biographer says that Mario left Caravaggio because of his behaviour. The strangeness seems to have increased after Malta. Susinno's early 18th century "Le vite de' pittori Messinesi" ("Lives of the Painters of Messina") provides several colourful anecdotes of Caravaggio's erratic behaviour in Sicily, and these are reproduced in modern full-length biographies such as Langdon and Robb. Bellori writes of Caravaggio's "fear" driving him from city to city across the island and finally, "feeling that it was no longer safe to remain", back to Naples. Baglione says Caravaggio was being "chased by his enemy", but like Bellori does not say who this enemy was.

After only nine months in Sicily, Caravaggio returned to Naples in the late summer of 1609. According to his earliest biographer he was being pursued by enemies while in Sicily and felt it safest to place himself under the protection of the Colonnas until he could secure his pardon from the pope (now Paul V) and return to Rome. In Naples he painted "The Denial of Saint Peter", a final "John the Baptist (Borghese)", and his last picture, "The Martyrdom of Saint Ursula". His style continued to evolve — Saint Ursula is caught in a moment of highest action and drama, as the arrow fired by the king of the Huns strikes her in the breast, unlike earlier paintings that had all the immobility of the posed models. The brushwork was also much freer and more impressionistic.
In October 1609 he was involved in a violent clash, an attempt on his life, perhaps ambushed by men in the pay of the knight he had wounded in Malta or some other faction of the Order. His face was seriously disfigured and rumours circulated in Rome that he was dead. He painted a "Salome with the Head of John the Baptist (Madrid)", showing his own head on a platter, and sent it to de Wignacourt as a plea for forgiveness. Perhaps at this time, he painted also a "David with the Head of Goliath", showing the young David with a strangely sorrowful expression gazing on the severed head of the giant, which is again Caravaggio. This painting he may have sent to his patron, the unscrupulous art-loving Cardinal Scipione Borghese, nephew of the pope, who had the power to grant or withhold pardons. Caravaggio hoped Borghese could mediate a pardon, in exchange for works by the artist.

News from Rome encouraged Caravaggio, and in the summer of 1610 he took a boat northwards to receive the pardon, which seemed imminent thanks to his powerful Roman friends. With him were three last paintings, the gifts for Cardinal Scipione. What happened next is the subject of much confusion and conjecture, shrouded in much mystery.

The bare facts seem to be that on 28 July an anonymous "avviso" (private newsletter) from Rome to the ducal court of Urbino reported that Caravaggio was dead. Three days later another "avviso" said that he had died of fever on his way from Naples to Rome. A poet friend of the artist later gave 18 July as the date of death, and a recent researcher claims to have discovered a death notice showing that the artist died on that day of a fever in Porto Ercole, near Grosseto in Tuscany. Human remains found in a church in Porto Ercole in 2010 are believed to almost certainly belong to Caravaggio. The findings come after a year-long investigation using DNA, carbon dating and other analyses.

Some scholars argue that Caravaggio was murdered by the same "enemies" that had been pursuing him since he fled Malta, possibly Wignacourt and/or factions of the Knights. Caravaggio might have died of lead poisoning. Bones with high lead levels were recently found in a grave likely to be Caravaggio's. Paints used at the time contained high amounts of lead salts, and Caravaggio is known to have indulged in violent behavior, as caused by lead poisoning.

Caravaggio never married and had no known children, and Howard Hibbard notes the absence of erotic female figures from the artist's oeuvre: "In his entire career he did not paint a single female nude." On the other hand, the cabinet-pieces from the Del Monte period are replete with "full-lipped, languorous boys ... who seem to solicit the onlooker with their offers of fruit, wine, flowers – and themselves" suggesting an erotic interest in the male form. At the same time, however, a connection with a certain Lena is mentioned in a 1605 court deposition by Pasqualone, where she is described as "Michelangelo's girl". According to G.B.Passeri this 'Lena' was Caravaggio's model for the "Madonna di Loreto"; and according to Catherine Puglisi, 'Lena' may have been the same person as the courtesan Maddalena di Paolo Antognetti, who named Caravaggio as an "intimate friend" by her own testimony in 1604. Caravaggio also probably enjoyed close relationships with other "whores and courtesans" such as Fillide Melandroni, of whom he painted a portrait. Andrew Graham-Dixon has also suggested that the infamous duel with Ranuccio Tommasoni may have been the result of the latter discovering an affair between his wife (Lavinia Giugioli) and the artist, which possibly resulted in an illegitimate daughter

Nevertheless, since the 1970s both art scholars and historians have debated the inferences of homoeroticism in Caravaggio's works as a way to better understand the man. The model of "Amor vincit omnia", for example, is known to have been Cecco di Caravaggio. Cecco stayed with Caravaggio even after he was obliged to leave Rome in 1606, and the two may have been lovers.

Caravaggio's sexuality also received early speculation due to claims about the artist by Honoré Gabriel Riqueti, Comte de Mirabeau. Mirabeau contrasted the personal life of Caravaggio directly with the writings of St Paul in the Book of Romans, arguing that "Romans" excessively practice sodomy or homosexuality. The twenty-sixth verse of the first chapter contains the Latin phrase: ""Et fœminæ eorum immutaverunt naturalem usum in eum usum qui est contra naturam."" The phrase, according to Mirabeau, entered Caravaggio's thoughts, and he claimed that such an "abomination" could be witnessed through a particular painting housed at the Museum of the Grand Duke of Tuscany - featuring a rosary of a blasphemous nature, in which a circle of thirty men ("turpiter ligati") are intertwined in embrace and presented in unbridled composition. Mirabeau notes the affectionate nature of Caravaggio's depiction reflects the voluptuous glow of the artist's sexuality. By the late nineteenth century, Sir Richard Francis Burton identified the painting as Caravaggio's painting of St. Rosario. Burton also identifies both St. Rosario and this painting with the practices of Tiberius mentioned by Seneca the Younger. The survival status and location of Caravaggio's painting is unknown. No such painting appears in his or his school's catalogues. 

Aside from the paintings, evidence also comes from the libel trial brought against Caravaggio by Giovanni Baglione in 1603. Baglione accused Caravaggio and his friends of writing and distributing scurrilous doggerel attacking him; the pamphlets, according to Baglione's friend and witness Mao Salini, had been distributed by a certain Giovanni Battista, a "bardassa," or boy prostitute, shared by Caravaggio and his friend Onorio Longhi. Caravaggio denied knowing any young boy of that name, and the allegation was not followed up. 

Baglione's painting of "Divine Love" has also been seen as a visual accusation of sodomy against Caravaggio. Such accusations were damaging and dangerous as sodomy was a capital crime at the time. Even though the authorities were unlikely to investigate such a well-connected person as Caravaggio, "Once an artist had been smeared as a pederast, his work was smeared too." Francesco Susinoo in his later biography additionally relates the story of how the artist was chased by a school-master in Sicily for spending too long gazing at the boys in his care. Susino presents it as a misunderstanding, but Caravaggio may indeed have been seeking sexual solace; and the incident could explain one of his most homoerotic paintings: his last depiction of St John the Baptist.

The art historian, Andrew Graham-Dixon has summarised the debate:
A lot has been made of Caravaggio's presumed homosexuality, which has in more than one previous account of his life been presented as the single key that explains everything, both the power of his art and the misfortunes of his life. There is no absolute proof of it, only strong circumstantial evidence and much rumour. The balance of probability suggests that Caravaggio did indeed have sexual relations with men. But he certainly had female lovers. Throughout the years that he spent in Rome he kept close company with a number of prostitutes. The truth is that Caravaggio was as uneasy in his relationships as he was in most other aspects of life. He likely slept with men. He did sleep with women. He settled with no one... [but] the idea that he was an early martyr to the drives of an unconventional sexuality is an anachronistic fiction.

Caravaggio "put the oscuro (shadows) into chiaroscuro." Chiaroscuro was practiced long before he came on the scene, but it was Caravaggio who made the technique a dominant stylistic element, darkening the shadows and transfixing the subject in a blinding shaft of light. With this came the acute observation of physical and psychological reality that formed the ground both for his immense popularity and for his frequent problems with his religious commissions. 

He worked at great speed, from live models, scoring basic guides directly onto the canvas with the end of the brush handle; very few of Caravaggio's drawings appear to have survived, and it is likely that he preferred to work directly on the canvas. The approach was anathema to the skilled artists of his day, who decried his refusal to work from drawings and to idealise his figures. Yet the models were basic to his realism. Some have been identified, including Mario Minniti and Francesco Boneri, both fellow artists, Minniti appearing as various figures in the early secular works, the young Boneri as a succession of angels, Baptists and Davids in the later canvasses. His female models include Fillide Melandroni, Anna Bianchini, and Maddalena Antognetti (the "Lena" mentioned in court documents of the "artichoke" case as Caravaggio's concubine), all well-known prostitutes, who appear as female religious figures including the Virgin and various saints. Caravaggio himself appears in several paintings, his final self-portrait being as the witness on the far right to the "Martyrdom of Saint Ursula".

Caravaggio had a noteworthy ability to express in one scene of unsurpassed vividness the passing of a crucial moment. "The Supper at Emmaus" depicts the recognition of Christ by his disciples: a moment before he is a fellow traveler, mourning the passing of the Messiah, as he never ceases to be to the inn-keeper's eyes; the second after, he is the Saviour. In "The Calling of St Matthew", the hand of the Saint points to himself as if he were saying "who, me?", while his eyes, fixed upon the figure of Christ, have already said, "Yes, I will follow you". With "The Resurrection of Lazarus", he goes a step further, giving us a glimpse of the actual physical process of resurrection. The body of Lazarus is still in the throes of rigor mortis, but his hand, facing and recognizing that of Christ, is alive. Other major Baroque artists would travel the same path, for example Bernini, fascinated with themes from Ovid's "Metamorphoses". 

The installation of the St. Matthew paintings in the Contarelli Chapel had an immediate impact among the younger artists in Rome, and Caravaggism became the cutting edge for every ambitious young painter. The first Caravaggisti included Orazio Gentileschi and Giovanni Baglione. Baglione's Caravaggio phase was short-lived; Caravaggio later accused him of plagiarism and the two were involved in a long feud. Baglione went on to write the first biography of Caravaggio. In the next generation of Caravaggisti there were Carlo Saraceni, Bartolomeo Manfredi and Orazio Borgianni. Gentileschi, despite being considerably older, was the only one of these artists to live much beyond 1620, and ended up as court painter to Charles I of England. His daughter Artemisia Gentileschi was also close to Caravaggio, and one of the most gifted of the movement. Yet in Rome and in Italy it was not Caravaggio, but the influence of his rival Annibale Carracci, blending elements from the High Renaissance and Lombard realism, which ultimately triumphed.

Caravaggio's brief stay in Naples produced a notable school of Neapolitan Caravaggisti, including Battistello Caracciolo and Carlo Sellitto. The Caravaggisti movement there ended with a terrible outbreak of plague in 1656, but the Spanish connection – Naples was a possession of Spain – was instrumental in forming the important Spanish branch of his influence.

A group of Catholic artists from Utrecht, the "Utrecht Caravaggisti", travelled to Rome as students in the first years of the 17th century and were profoundly influenced by the work of Caravaggio, as Bellori describes. On their return to the north this trend had a short-lived but influential flowering in the 1620s among painters like Hendrick ter Brugghen, Gerrit van Honthorst, Andries Both and Dirck van Baburen. In the following generation the effects of Caravaggio, although attenuated, are to be seen in the work of Rubens (who purchased one of his paintings for the Gonzaga of Mantua and painted a copy of the "Entombment of Christ"), Vermeer, Rembrandt, and Velázquez, the last of whom presumably saw his work during his various sojourns in Italy.

Caravaggio's innovations inspired the Baroque, but the Baroque took the drama of his chiaroscuro without the psychological realism. While he directly influenced the style of the artists mentioned above, and, at a distance, the Frenchmen Georges de La Tour and Simon Vouet, and the Spaniard Giuseppe Ribera, within a few decades his works were being ascribed to less scandalous artists, or simply overlooked. The Baroque, to which he contributed so much, had evolved, and fashions had changed, but perhaps more pertinently Caravaggio never established a workshop as the Carracci did, and thus had no school to spread his techniques. Nor did he ever set out his underlying philosophical approach to art, the psychological realism that may only be deduced from his surviving work.

Thus his reputation was doubly vulnerable to the critical demolition-jobs done by two of his earliest biographers, Giovanni Baglione, a rival painter with a personal vendetta, and the influential 17th-century critic Gian Pietro Bellori, who had not known him but was under the influence of the earlier Giovanni Battista Agucchi and Bellori's friend Poussin, in preferring the "classical-idealistic" tradition of the Bolognese school led by the Carracci. Baglione, his first biographer, played a considerable part in creating the legend of Caravaggio's unstable and violent character, as well as his inability to draw.

In the 1920s, art critic Roberto Longhi brought Caravaggio's name once more to the foreground, and placed him in the European tradition: "Ribera, Vermeer, La Tour and Rembrandt could never have existed without him. And the art of Delacroix, Courbet and Manet would have been utterly different". The influential Bernard Berenson agreed: "With the exception of Michelangelo, no other Italian painter exercised so great an influence."

Caravaggio's epitaph was composed by his friend Marzio Milesi. It reads:

He was commemorated on the front of the Banca d'Italia 100,000 lire banknote in the 1980s and 90s (before Italy switched to the Euro) with the back showing his "Basket of Fruit".

"See also Chronology of works by Caravaggio"
There is disagreement as to the exact size of Caravaggio's oeuvre, with counts as low as 40 and as high as 80. In his biography, Caravaggio scholar Alfred Moir writes "The forty-eight colorplates in this book include almost all of the surviving works accepted by every Caravaggio expert as autograph, and even the least demanding would add fewer than a dozen more". One, "The Calling of Saints Peter and Andrew", was recently authenticated and restored; it had been in storage in Hampton Court, mislabeled as a copy. Richard Francis Burton writes of a "picture of St. Rosario (in the museum of the Grand Duke of Tuscany), showing a circle of thirty men "turpiter ligati"" ("lewdly banded"), which is not known to have survived. The rejected version of "The Inspiration of Saint Matthew", intended for the Contarelli Chapel in San Luigi dei Francesi in Rome, was destroyed during the bombing of Dresden, though black and white photographs of the work exist. In June 2011 it was announced that a previously unknown Caravaggio painting of Saint Augustine dating to about 1600 had been discovered in a private collection in Britain. Called a "significant discovery", the painting had never been published and is thought to have been commissioned by Vincenzo Giustiniani, a patron of the painter in Rome.

A painting believed by some experts to be Caravaggio's second version of "Judith Beheading Holofernes", tentatively dated between 1600 and 1610, was discovered in an attic in Toulouse in 2014. An export ban was placed on the painting by the French government while tests were carried out to establish its provenance.

In October 1969, two thieves entered the Oratory of San Lorenzo in Palermo, Italy and stole Caravaggio's "Nativity with St. Francis and St. Lawrence" from its frame. Experts estimated its value at $20 million.

Following the theft, Italian police set up an art theft task force with the specific aim of re-acquiring lost and stolen art works. Since the creation of this task force, many leads have been followed regarding the "Nativity". Former Italian mafia members have stated that "Nativity with St. Francis and St. Lawrence" was stolen by the Sicilian mafia and displayed at important mafia gatherings. Former mafia members have said that the "Nativity" was damaged and has since been destroyed.

The whereabouts of the artwork are still unknown. A reproduction currently hangs in its place in the Oratory of San Lorenzo.


The main primary sources for Caravaggio's life are:
All have been reprinted in Howard Hibbard's "Caravaggio" and in the appendices to Catherine Puglisi's "Caravaggio".


Biography
Articles and essays

Art works

Music

Video


</doc>
<doc id="7019" url="https://en.wikipedia.org/wiki?curid=7019" title="Jean-Baptiste-Siméon Chardin">
Jean-Baptiste-Siméon Chardin

Jean-Baptiste-Siméon Chardin (; November 2, 1699 – December 6, 1779) was an 18th-century French painter. He is considered a master of still life, and is also noted for his genre paintings which depict kitchen maids, children, and domestic activities. Carefully balanced composition, soft diffusion of light, and granular impasto characterize his work.

Chardin was born in Paris, the son of a cabinetmaker, and rarely left the city. He lived on the Left Bank near Saint-Sulpice until 1757, when Louis XV granted him a studio and living quarters in the Louvre.

Chardin entered into a marriage contract with Marguerite Saintard in 1723, whom he did not marry until 1731. He served apprenticeships with the history painters Pierre-Jacques Cazes and Noël-Nicolas Coypel, and in 1724 became a master in the Académie de Saint-Luc.

According to one nineteenth-century writer, at a time when it was hard for unknown painters to come to the attention of the Royal Academy, he first found notice by displaying a painting at the "small Corpus Christi" (held eight days after the regular one) on the Place Dauphine (by the Pont Neuf). Van Loo, passing by in 1720, bought it and later assisted the young painter.

Upon presentation of "The Ray" in 1728, he was admitted to the Académie Royale de Peinture et de Sculpture. The following year he ceded his position in the Académie de Saint-Luc. He made a modest living by "produc[ing] paintings in the various genres at whatever price his customers chose to pay him", and by such work as the restoration of the frescoes at the Galerie François I at Fontainebleau in 1731.
In November 1731 his son Jean-Pierre was baptized, and a daughter, Marguerite-Agnès, was baptized in 1733. In 1735 his wife Marguerite died, and within two years Marguerite-Agnès had died as well.

Beginning in 1737 Chardin exhibited regularly at the Salon. He would prove to be a "dedicated academician", regularly attending meetings for fifty years, and functioning successively as counsellor, treasurer, and secretary, overseeing in 1761 the installation of Salon exhibitions.

His work gained popularity through reproductive engravings of his genre paintings (made by artists such as François-Bernard Lépicié and P.-L. Sugurue), which brought Chardin income in the form of "what would now be called royalties".
In 1744 he entered his second marriage, this time to Françoise-Marguerite Pouget. The union brought a substantial improvement in Chardin's financial circumstances. In 1745 a daughter, Angélique-Françoise, was born, but she died in 1746.

In 1752 Chardin was granted a pension of 500 livres by Louis XV. At the Salon of 1759 he exhibited nine paintings; it was the first Salon to be commented upon by Denis Diderot, who would prove to be a great admirer and public champion of Chardin's work. Beginning in 1761, his responsibilities on behalf of the Salon, simultaneously arranging the exhibitions and acting as treasurer, resulted in a diminution of productivity in painting, and the showing of 'replicas' of previous works. In 1763 his services to the Académie were acknowledged with an extra 200 livres in pension. In 1765 he was unanimously elected associate member of the Académie des Sciences, Belles-Lettres et Arts of Rouen, but there is no evidence that he left Paris to accept the honor. By 1770 Chardin was the 'Premier peintre du roi', and his pension of 1,400 livres was the highest in the Academy.

In 1772 Chardin's son, also a painter, drowned in Venice, a probable suicide. The artist's last known oil painting was dated 1776; his final Salon participation was in 1779, and featured several pastel studies. Gravely ill by November of that year, he died in Paris on December 6, at the age of 80.

Chardin worked very slowly and painted only slightly more than 200 pictures (about four a year) total.

Chardin's work had little in common with the Rococo painting that dominated French art in the 18th century. At a time when history painting was considered the supreme classification for public art, Chardin's subjects of choice were viewed as minor categories. He favored simple yet beautifully textured still lifes, and sensitively handled domestic interiors and genre paintings. Simple, even stark, paintings of common household items ("Still Life with a Smoker's Box") and an uncanny ability to portray children's innocence in an unsentimental manner ("Boy with a Top" [right]) nevertheless found an appreciative audience in his time, and account for his timeless appeal.

Largely self-taught, Chardin was greatly influenced by the realism and subject matter of the 17th-century Low Country masters. Despite his unconventional portrayal of the ascendant bourgeoisie, early support came from patrons in the French aristocracy, including Louis XV. Though his popularity rested initially on paintings of animals and fruit, by the 1730s he introduced kitchen utensils into his work ("The Copper Cistern", ca. 1735, Louvre). Soon figures populated his scenes as well, supposedly in response to a portrait painter who challenged him to take up the genre. "Woman Sealing a Letter" (ca. 1733), which may have been his first attempt, was followed by half-length compositions of children saying grace, as in "Le Bénédicité", and kitchen maids in moments of reflection. These humble scenes deal with simple, everyday activities, yet they also have functioned as a source of documentary information about a level of French society not hitherto considered a worthy subject for painting. The pictures are noteworthy for their formal structure and pictorial harmony. Chardin said about painting, "Who said one paints with colors? One "employs" colors, but one paints with "feeling"." 

A child playing was a favourite subject of Chardin. He depicted an adolescent building a house of cards on at least four occasions. The version at Waddesdon Manor is the most elaborate. Scenes such as these derived from 17th-century Netherlandish vanitas works, which bore messages about the transitory nature of human life and the worthlessness of material ambitions, but Chardin's also display a delight in the ephemeral phases of childhood for their own sake.

Chardin frequently painted replicas of his compositions—especially his genre paintings, nearly all of which exist in multiple versions which in many cases are virtually indistinguishable. Beginning with "The Governess" (1739, in the National Gallery of Canada, Ottawa), Chardin shifted his attention from working-class subjects to slightly more spacious scenes of bourgeois life.

In 1756 he returned to the subject of the still life. In the 1770s his eyesight weakened and he took to painting in pastels, a medium in which he executed portraits of his wife and himself (see "Self-portrait" at top right). His works in pastels are now highly valued. Chardin's extant paintings, which number about 200, are in many major museums, including the Louvre.

Chardin's influence on the art of the modern era was wide-ranging, and has been well-documented. Édouard Manet's half-length "Boy Blowing Bubbles" and the still lifes of Paul Cézanne are equally indebted to their predecessor. He was one of Henri Matisse's most admired painters; as an art student Matisse made copies of four Chardin paintings in the Louvre. Chaim Soutine's still lifes looked to Chardin for inspiration, as did the paintings of Georges Braque, and later, Giorgio Morandi. In 1999 Lucian Freud painted and etched several copies after "The Young Schoolmistress" (National Gallery, London).

Marcel Proust, in the chapter "How to open your eyes?" from "In Search of Lost Time" ("À la recherche du temps perdu"), describes a melancholic young man sitting at his simple breakfast table. The only comfort he finds is in the imaginary ideas of beauty depicted in the great masterpieces of the Louvre, materializing fancy palaces, rich princes, and the like. The author tells the young man to follow him to another section of the Louvre where the pictures of Jean-Baptiste Chardin are. There he would see the beauty in still life at home and in everyday activities like peeling turnips.




</doc>
<doc id="7021" url="https://en.wikipedia.org/wiki?curid=7021" title="Crookes radiometer">
Crookes radiometer

The Crookes radiometer, also known as a light mill, consists of an airtight glass bulb, containing a partial vacuum. Inside are a set of vanes which are mounted on a spindle. The vanes rotate when exposed to light, with faster rotation for more intense light, providing a quantitative measurement of electromagnetic radiation intensity. The reason for the rotation was a cause of much scientific debate in the ten years following the invention of the device, but in 1879 the currently accepted explanation for the rotation was published. Today the device is mainly used in physics education as a demonstration of a heat engine run by light energy.

It was invented in 1873 by the chemist Sir William Crookes as the by-product of some chemical research. In the course of very accurate quantitative chemical work, he was weighing samples in a partially evacuated chamber to reduce the effect of air currents, and noticed the weighings were disturbed when sunlight shone on the balance. Investigating this effect, he created the device named after him.

It is still manufactured and sold as an educational aid or curiosity.

The radiometer is made from a glass bulb from which much of the air has been removed to form a partial vacuum. Inside the bulb, on a low friction spindle, is a rotor with several (usually four) vertical lightweight vanes spaced equally around the axis. The vanes are polished or white on one side and black on the other.

When exposed to sunlight, artificial light, or infrared radiation (even the heat of a hand nearby can be enough), the vanes turn with no apparent motive power, the dark sides retreating from the radiation source and the light sides advancing.

Cooling the radiometer causes rotation in the opposite direction.

The effect begins to be observed at partial vacuum pressures of a few torr (several hundred pascals), reaches a peak at around 10 torr (1 pascal) and has disappeared by the time the vacuum reaches 10 torr (10 pascal) (see explanations note 1). At these very high vacuums the effect of photon radiation pressure on the vanes can be observed in very sensitive apparatus (see Nichols radiometer) but this is insufficient to cause rotation.

This can be done, for example, by visual means (e.g., a spinning slotted disk, which functions as a simple stroboscope) without interfering with the measurement itself.

Radiometers are now commonly sold worldwide as a novelty ornament; needing no batteries, but only light to get the vanes to turn. They come in various forms, such as the one pictured, and are often used in science museums to illustrate "radiation pressure" – a scientific principle that they do not in fact demonstrate.

When a radiant energy source is directed at a Crookes radiometer, the radiometer becomes a heat engine. The operation of a heat engine is based on a difference in temperature that is converted to a mechanical output. In this case, the black side of the vane becomes hotter than the other side, as radiant energy from a light source warms the black side by black-body absorption faster than the silver or white side. The internal air molecules are heated up when they touch the black side of the vane. The details of exactly how this moves the warmer side of the vane forward are given in the section below.

The internal temperature rises as the black vanes impart heat to the air molecules, but the molecules are cooled again when they touch the bulb's glass surface, which is at ambient temperature. This heat loss through the glass keeps the internal bulb temperature steady with the result that the two sides of the vanes develop a temperature difference. The white or silver side of the vanes are slightly warmer than the internal air temperature but cooler than the black side, as some heat conducts through the vane from the black side. The two sides of each vane must be thermally insulated to some degree so that the polished or white side does not immediately reach the temperature of the black side. If the vanes are made of metal, then the black or white paint can be the insulation. The glass stays much closer to ambient temperature than the temperature reached by the black side of the vanes. The external air helps conduct heat away from the glass.

The air pressure inside the bulb needs to strike a balance between too low and too high. A strong vacuum inside the bulb does not permit motion, because there are not enough air molecules to cause the air currents that propel the vanes and transfer heat to the outside before both sides of each vane reach thermal equilibrium by heat conduction through the vane material. High inside pressure inhibits motion because the temperature differences are not enough to push the vanes through the higher concentration of air: there is too much air resistance for "eddy currents" to occur, and any slight air movement caused by the temperature difference is damped by the higher pressure before the currents can "wrap around" to the other side.

When the radiometer is heated in the absence of a light source, it turns in the forward direction (i.e. black sides trailing). If a person's hands are placed around the glass without touching it, the vanes will turn slowly or not at all, but if the glass is touched to warm it quickly, they will turn more noticeably. Directly heated glass gives off enough infrared radiation to turn the vanes, but glass blocks much of the far-infrared radiation from a source of warmth not in contact with it. However, near-infrared and visible light more easily penetrate the glass.

If the glass is cooled quickly in the absence of a strong light source by putting ice on the glass or placing it in the freezer with the door almost closed, it turns backwards (i.e. the silver sides trail). This demonstrates black-body radiation from the black sides of the vanes rather than black-body absorption. The wheel turns backwards because the net exchange of heat between the black sides and the environment initially cools the black sides faster than the white sides. Upon reaching equilibrium, typically after a minute or two, reverse rotation ceases. This contrasts with sunlight, with which forward rotation can be maintained all day.

Over the years, there have been many attempts to explain how a Crookes radiometer works:


To rotate, a light mill does not have to be coated with different colors across each vane. In 2009, researchers at the University of Texas, Austin created a monocolored light mill which has four curved vanes; each vane forms a convex and a concave surface. The light mill is uniformly coated by gold nanocrystals, which are a strong light absorber. Upon exposure, due to geometric effect, the convex side of the vane receives more photon energy than the concave side does, and subsequently the gas molecules receive more heat from the convex side than from the concave side. At rough vacuum, this asymmetric heating effect generates a net gas movement across each vane, from the concave side to the convex side, as shown by the researchers' Direct Simulation Monte Carlo (DSMC) modeling. The gas movement causes the light mill to rotate with the concave side moving forward, due to Newton's Third Law.
This monocolored design promotes the fabrication of micrometer- or nanometer- scaled light mills, as it is difficult to pattern materials of distinct optical properties within a very narrow, three-dimensional space.

In 2010 researchers at the University of California, Berkeley succeeded in building a nanoscale light mill that works on an entirely different principle to the Crookes radiometer. A gold light mill, only 100 nanometers in diameter, was built and illuminated by laser light that had been tuned. The possibility of doing this had been suggested by the Princeton physicist Richard Beth in 1936. The torque was greatly enhanced by the resonant coupling of the incident light to plasmonic waves in the gold structure.





</doc>
<doc id="7022" url="https://en.wikipedia.org/wiki?curid=7022" title="Cold Chisel">
Cold Chisel

Cold Chisel are an Australian rock band that formed in Adelaide, Australia. They had chart success in the 70s, 80s and 90s, and again more recently since reforming in 2011, with nine albums making the Australian top ten. Cold Chisel's mainstream popularity is restricted to Australia, with their songs and musicianship exemplifying "pub rock" and highlighting working class life in their home country.

Originally named Orange, the band formed in Adelaide in 1973 as a heavy-metal cover-band comprising keyboardist Ted Broniecki and bassist Les Kaczmarek (died December 5, 2008), keyboard player Don Walker, guitarist Ian Moss and drummer Steve Prestwich (died 16 January 2011). Seventeen-year-old singer Jimmy Barnes— called Jim Barnes on the initial run of albums— joined in December 1973, taking leave from the band in 1975 for a brief stint as Bon Scott's replacement in Fraternity.

The group changed its name several times before settling on Cold Chisel in 1974 after writing a song with that title. Barnes' relationship with other band members was volatile; as a Scot he often came to blows with Liverpool-born Prestwich and he left the band several times. During these periods Moss would handle vocals until Barnes returned. Walker soon emerged as Cold Chisel's primary songwriter. Walker spent 1974 in Armidale, completing his studies and in 1975 Kaczmarek left the band and was replaced by Phil Small. Barnes' older brother John Swan was a member of Cold Chisel around this time, providing backing vocals and percussion but after several violent incidents he was fired.

In May 1976, Cold Chisel relocated to Melbourne but found little success, moving on to Sydney in November. Six months later, in May 1977, Barnes announced he would quit Cold Chisel to join Swan in Feather, a hard-rocking blues band that had evolved from an earlier group called Blackfeather. A farewell performance in Sydney went so well that the singer changed his mind. The following month the Warner Music Group picked up Cold Chisel.

In the early months of 1978, Cold Chisel recorded its self-titled debut album with producer Peter Walker. All tracks were written by Don Walker except "Juliet", for which Barnes wrote the melody and Walker the lyrics. "Cold Chisel" was released in April and featured appearances from harmonica player Dave Blight, who would become a regular on-stage guest, and saxophonists Joe Camilleri and Wilbur Wilde from Jo Jo Zep & The Falcons. The following month the song "Khe Sanh" was released as a single but was deemed too offensive for airplay on commercial radio because of the lyric ""Their legs were often open/But their minds were always closed"", although it was played regularly on Sydney rock station, Double J, which was not subject to such restrictions because it was part of the ABC. Despite that setback, it still reached #41 on the Australian singles chart and number four on the Adelaide charts thanks mainly to the band's rising popularity as a touring act and some local radio support in Adelaide where the single was aired in spite of the ban. "Khe Sanh" has since become Cold Chisel's signature tune and arguably its most popular among fans. The song was later remixed, with re-recorded vocals, for inclusion on the international version of 1980's "East".

The band's next release was a live EP titled "You're Thirteen, You're Beautiful, and You're Mine", in November. This had been recorded at a show at Sydney's Regent Theatre in 1977 that had featured Midnight Oil as one of the support acts. One of the EP's tracks, "Merry Go Round" was later recorded on the follow-up, "Breakfast at Sweethearts". This album was recorded between July 1978 and January 1979 with experienced producer Richard Batchens, who had previously worked with Richard Clapton, Sherbet and Blackfeather. Batchens smoothed out many of the band's rough edges and attempted to give their songs a sophisticated sound. This approach has made the album sit a little uncomfortably with the band ever since. Once again, the majority of the songs were penned by Walker, with Barnes collaborating with Walker on the first single "Goodbye (Astrid, Goodbye)" and Moss contributing to "Dresden". "Goodbye (Astrid, Goodbye)" became a live favourite for the band, and even went on to be performed by U2 during Australian tours in the 1980s.

By now the band stood at the verge of major national success, even without significant radio airplay or support from "Countdown", Australia's most important youth music program at the time. The band had become notorious for its wild behaviour, particularly from Barnes who was rumoured to have had sex with over 1000 women and who was known to consume more than a bottle of vodka every night during performances.

In late 1979, following their problematic relationship with Batchens, Cold Chisel chose Mark Opitz to produce the next single, "Choirgirl", a Don Walker composition dealing with a young woman's experience with abortion. In spite of the controversial subject matter, the track became a hit and paved the way for Cold Chisel's next album. Recorded over two months in early 1980, "East" reached #2 on the Australian album charts and was the second-highest selling album by an Australian artist for the year. Despite the continued dominance of Walker, during Cold Chisel's later career all four of the other members began to contribute songs to the band, and this was the first of their albums to feature songwriting contributions from each member of the band. Cold Chisel is the only Australian rock band to score hits with songs written by every member of the group.

Of the album's 12 tracks, two were written by Barnes, with Moss, Prestwich and Small contributing one song each. The songs ranged from straight ahead rock tracks such as "Standing on the Outside" and "My Turn to Cry" to rockabilly-flavoured work-outs ("Rising Sun", written about Barnes' relationship with his girlfriend Jane Mahoney) and pop-laced love songs ("My Baby", featuring Joe Camilleri on saxophone) to a poignant piano ballad about prison life, "Four Walls". The cover featured Barnes asleep in a bathtub wearing a kamikaze bandanna in a room littered with junk and was inspired by Jacques-Louis David's 1793 painting "The Death of Marat". The Ian Moss-penned "Never Before" was chosen as the first song to air by the ABC's radio station Triple J when it switched to the FM band that year.

Following the release of "East", Cold Chisel embarked on the Youth in Asia Tour, which took its name from a lyric in "Star Hotel". This tour saw the group play more than 60 shows in 90 days and would form the basis of 1981's double live album "Swingshift".

In April 1981 the band was nominated for all seven of the major awards at the joint "Countdown"/"TV Week" music awards held at the Sydney Entertainment Centre, and won them all. As a protest against the concept of a TV magazine being involved in a music awards ceremony, the band refused to accept its awards and finished the night by performing "My Turn to Cry". After only one verse and chorus, the band smashed up the set and left the stage.

"Swingshift" debuted at No. 1 on the Australian album charts, crystallizing the band's status as the biggest-selling act in the country. Overseas, however, Cold Chisel was unable to make an impact. With a slightly different track-listing, "East" had been issued in the United States and the band undertook its first (and only) US tour. But while it was popular as a live act, the American arm of their label did little to support the album. According to Barnes biographer Toby Creswell, at one point the band was ushered into an office to listen to the US master only to find it drenched in tape hiss and other ambient noise, making it almost unreleasable. The band was even booed off stage after a lacklustre performance in Dayton, Ohio in May 1981 opening for Ted Nugent, who at the time was touring with his guitar army a.k.a. the 'D.C. Hawks'. European audiences were more accepting of the band and the group developed a small but significant fan base in Germany.

In August 1981, the band began work on the album "Circus Animals", again with Opitz producing. The album opened with "You Got Nothing I Want", an aggressive Barnes-penned hard rock track that attacked the American industry for its handling of the band. The song would later cause problems for Barnes when he later attempted to break into the US market as a solo performer as senior music executives there continued to hold it against him. Like its predecessor, "Circus Animals" contained songs of contrasting styles, with harder-edged tracks like "Bow River" and "Hound Dog" in place beside more expansive ballads such as "Forever Now" and "When the War Is Over", both written by Prestwich. The latter track has proved to be the most popular Cold Chisel song for other artists to record -- Uriah Heep included a version on the 1989 album "Raging Silence" and John Farnham has recorded it twice, once while he and Prestwich were members of Little River Band in the mid-80s and again for his 1990 solo album "Age of Reason". The song was also a No. 1 hit for former "Australian Idol" contestant Cosima De Vito in 2004 and was also performed by Bobby Flynn during that show's 2006 season. "Forever Now" was also covered (as a country waltz) by Australian band The Reels.

To launch the album, the band performed under a circus tent at Wentworth Park in Sydney and toured heavily once more, including a show in Darwin that attracted more than 10 per cent of the city's population.

"Circus Animals" and its three singles, "You Got Nothing I Want", "Forever Now" and "When the War is Over" were all major hits in Australia during 1982 but further success overseas continued to elude the band and cracks began to appear. In early 1983 the band toured Germany but the shows went so badly that in the middle of the tour Walker up-ended his keyboard and stormed off stage during one show and Prestwich was fired. Returning to Australia, Prestwich was replaced by Ray Arnott, formerly of the 1970s progressive rock band Spectrum. After this, Barnes requested a large advance from management. Now married with a young child, exorbitant spending had left him almost broke. His request was refused however because there was a standing arrangement that any advance to one band member had to be paid to all the others. After a meeting on 17 August during which Barnes quit the band it was decided that Cold Chisel would split up. A final concert series known as The Last Stand was planned and a final studio album was also recorded. Prestwich returned for the tour, which began in October. Before the Sydney shows however, Barnes lost his voice and those dates were rescheduled for December. The band's final performance was at the Sydney Entertainment Centre on 12 December 1983, apparently precisely 10 years since its first live appearance. The Sydney shows formed the basis of the film "The Last Stand", the biggest-selling concert film of any Australian band. Several other recordings from the tour were used on the 1984 live album "Barking Spiders Live: 1983", the title of which was inspired by the name the group occasionally used to play warm-up shows before tours, and as b-sides for a three-CD singles package known as "Three Big XXX Hits", issued ahead of the release of the 1994 compilation album, "Teenage Love".
During breaks in the tour, "Twentieth Century" was recorded. It was a fragmentary process, spread across various studios and sessions as the individual members often refused to work together, but nonetheless successful. Released in February 1984, it reached No. 1 upon release and included the songs "Saturday Night" and "Flame Trees", both of which remain radio staples. "Flame Trees", co-written by Prestwich and Walker, took its title from the BBC series "The Flame Trees of Thika" although it was lyrically inspired by the organist's hometown of Grafton, New South Wales. Barnes later recorded an acoustic version of the song on his 1993 album "Flesh and Wood" and the track was also covered by Sarah Blasko in 2006.

Barnes launched a solo career in January 1984 that has produced nine Australian No. 1 albums and an array of hit singles. One of those, "Too Much Ain't Enough Love" also peaked at No. 1. He has recorded with INXS, Tina Turner, Joe Cocker, John Farnham and a long list of other Australian and international artists and is arguably the country's most popular male rock singer.

Prestwich joined Little River Band in 1984 and appeared on the albums "Playing to Win" and "No Reins" before departing in 1986 to join John Farnham's touring band. Walker, Moss and Small all took extended breaks from music. Small, the least prominent member of the band virtually disappeared from the scene for many years, playing in a variety of minor acts. Walker formed Catfish in 1988, ostensibly a solo band with floating membership that included Moss, Charlie Owen and Dave Blight at various times. The music had a distinctly modern jazz aspect and his recordings during this phase attracted little commercial success. During 1989 he wrote several songs for Moss including "Tucker's Daughter" and "Telephone Booth" that the guitarist recorded on his debut solo album "Matchbook". Both the album and "Tucker's Daughter" peaked at No. 1 on the chart in 1989 and won Moss five ARIA Awards. His other albums met with little success.

Throughout the '80s and most of the '90s, Cold Chisel was courted to re-form but refused, at one point reportedly turning down an offer of $5 million to play a single show in each of the major Australian state capitals. While Moss and Walker often collaborated on projects, neither would work with Barnes again until Walker wrote "Stone Cold" for the singer's "Heat" in 1993. The pair then recorded an acoustic version for "Flesh and Wood" later the same year. Thanks primarily to continued radio airplay and Barnes' solo success, Cold Chisel's legacy remained solidly intact and by the early 90s the group had surpassed 3 million album sales, most of which had been sold since 1983. The 1991 compilation album "Chisel" was re-issued and re-packaged several times, once with the long-deleted 1978 EP as a bonus disc and a second time in 2001 as a double album. The "Last Stand" soundtrack album was also finally released in 1992 and in 1994 a complete album of previously unreleased demo and rare live recordings also surfaced. "Teenage Love" spawned a string of hit singles.

Cold Chisel reunited in 1998 to record the album "The Last Wave of Summer" and supported it with a sold-out national concert tour. The album debuted at number one on the Australian album chart. In 2003, the band re-grouped once more for the "Ringside" tour and in 2005 again reunited to perform at a benefit for the victims of the Boxing Day tsunami at the Myer Music Bowl in Melbourne.

On 10 September 2009, two days after Barnes' 15th studio solo album "The Rhythm and the Blues" hit No. 1 on the Australian album charts, Cold Chisel announced it would reform for a one-off performance at the Sydney 500 V8 Supercars event on 5 December 2009. The reunion saw the band perform at ANZ Stadium to the largest crowd of its career, with more than 45,000 fans in attendance. Cold Chisel played a single live performance in 2010, at the Deniliquin ute muster in October. In December Ian Moss confirmed that Cold Chisel was working on new material for an album.

In January 2011, Steve Prestwich was diagnosed with a brain tumour. He underwent surgery on 14 January but never regained consciousness and died two days later, aged 56.

All six of Cold Chisel's studio albums were re-released in digital and CD formats in mid-2011. Three digital only albums were also released, "Never Before",
"Besides" and "Covered" as well as a new 'best of' compilation titled, "The Best of Cold Chisel: All for You", which peaked at number 2 on the ARIA Charts.
The thirty-date 'Light the Nitro Tour' was announced in July 2011 along with the news that former Divinyls and Catfish drummer Charley Drayton had been recruited to replace Steve Prestwich. Most shows on the tour sold out within days and new dates were later announced for early 2012. Midway through 2012, they had a short UK tour, as well as playing with Soundgarden and Mars Volta at Hard Rock Calling.

Influences from blues and early rock n' roll was broadly apparent, fostered by the love of those styles by Moss, Barnes and Walker. Small and Prestwich contributed strong pop sensibilities. This allowed volatile rock songs like "You Got Nothing I Want" and "Merry-Go-Round" to stand beside thoughtful ballads like "Choirgirl", pop-flavoured love songs like "My Baby" and caustic political statements like "Star Hotel", an attack on the late-70s government of Malcolm Fraser, inspired by the Star Hotel riot in Newcastle.

The songs were not overtly political but rather observations of everyday life within Australian society and culture, in which the members with their various backgrounds (Moss was from Alice Springs, Walker grew up in rural New South Wales, Barnes and Prestwich were working-class immigrants from the UK) were quite well able to provide.

Cold Chisel's songs were about distinctly Australian experiences, a factor often cited as a major reason for the band's lack of international appeal. "Saturday Night" and "Breakfast at Sweethearts" were observations of the urban experience of Sydney's Kings Cross district where Walker lived for many years. "Misfits", which featured on the b-side to "My Baby", was about homeless kids in the suburbs surrounding Sydney. Songs like "Shipping Steel" and "Standing on The Outside" were working class anthems and many others featured characters trapped in mundane, everyday existences, yearning for the good times of the past ("Flame Trees") or for something better from life ("Bow River").

Alongside contemporaries like The Angels and Midnight Oil, Cold Chisel was renowned as one of the most dynamic live acts of their day and from early in their career concerts routinely became sell-out events. But the band was also famous for its wild lifestyle, particularly the hard-drinking Barnes, who played his role as one of the wild men of Australian rock to the hilt, never seen on stage without at least one bottle of vodka and often so drunk he could barely stand upright. Despite this, by 1982 he was a devoted family man who refused to tour without his wife and daughter. All the other band members were also settled or married; Ian Moss had a long-term relationship with the actress Megan Williams (she even sang on "Twentieth Century") whose own public persona could have hardly been more different. Yet it was the band's public image that often saw them compared less favourably with other important acts like Midnight Oil, whose music and politics (while rather more overt) were often similar but whose image and reputation was far more clean-cut. Cold Chisel remained hugely popular however and by the mid-90s had continued to sell records at such a consistent rate they became the first Australian band to achieve higher sales after their split than during their active years. While repackages and compilations accounted for much of these sales, 1994's "Teenage Love" album of rarities and two of its singles were Top Ten hits and when the group finally reformed in 1998 the resultant album was also a major hit and the follow-up tour sold out almost immediately.

Cold Chisel was one of the first Australian acts to have become the subject of a major tribute album. In 2007, "Standing on the Outside: The Songs of Cold Chisel" was released, featuring a collection of the band's songs as performed by artists including The Living End, Evermore, Something for Kate, Pete Murray, Katie Noonan, You Am I, Paul Kelly, Alex Lloyd, Thirsty Merc and Ben Lee, many of whom were still only children when Cold Chisel first disbanded and some of whom, like the members of Evermore, had not even been born.









</doc>
<doc id="7023" url="https://en.wikipedia.org/wiki?curid=7023" title="Confederate States of America">
Confederate States of America

The Confederate States of America (CSA or C.S.), commonly referred to as the Confederacy, was an unrecognized country in North America that existed from 1861 to 1865. The Confederacy was originally formed by seven secessionist slave-holding states – South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas – in the Lower South region of the United States, whose regional economy was heavily dependent upon agriculture, particularly cotton, and a plantation system that relied upon the labor of African-American slaves.

Each state declared its secession from the United States following the November 1860 election of Republican candidate Abraham Lincoln to the U.S. presidency on a platform which opposed the expansion of slavery into the western territories. Before Lincoln took office in March, a new Confederate government was established in February 1861, which was considered illegal by the government of the United States. States volunteered militia units and the new government hastened to form its own Confederate States Army from scratch practically overnight. After the American Civil War began in April, four slave states of the Upper South – Virginia, Arkansas, Tennessee, and North Carolina – also declared their secession and joined the Confederacy. The Confederacy later accepted Missouri and Kentucky as members, although neither officially declared secession nor were they ever largely controlled by Confederate forces; Confederate shadow governments attempted to control the two states but were later exiled from them.

The government of the United States (the Union) rejected the claims of secession and considered the Confederacy illegitimate. The War began with the Confederate attack upon Fort Sumter on April 12, 1861, a Union fort in the harbor of Charleston, South Carolina. No foreign government officially recognized the Confederacy as an independent country, although Great Britain and France granted it belligerent status, which allowed Confederate agents to contract with private concerns for arms and other supplies. In early 1865, after four years of heavy fighting which led to over 620,000 military deaths, all the Confederate forces surrendered and the Confederacy vanished. The war lacked a formal end; nearly all Confederate forces had been forced into surrender or deliberately disbanded by the end of 1865, by which point the dwindling manpower and resources of the Confederacy were facing overwhelming odds. By 1865, Jefferson Davis lamented that the Confederacy had "disappeared".

On February 22, 1862, the Confederate Constitution of seven state signatories – Mississippi, South Carolina, Florida, Alabama, Georgia, Louisiana, and Texas – replaced the Provisional Constitution of February 8, 1861, with one stating in its preamble a desire for a "permanent federal government". Four additional slave-holding states – Virginia, Arkansas, Tennessee, and North Carolina – declared their secession and joined the Confederacy following a call by U.S. President Abraham Lincoln for troops from each state to recapture Sumter and other seized federal properties in the South. Missouri and Kentucky were represented by partisan factions from those states, while the legitimate governments of those two states retained formal adherence to the Union. Also fighting for the Confederacy were two of the "Five Civilized Tribes" – the Choctaw and the Chickasaw – located in Indian Territory and a new, but uncontrolled, Confederate Territory of Arizona. Efforts by certain factions in Maryland to secede were halted by federal imposition of martial law; Delaware, though of divided loyalty, did not attempt it. A Unionist government in western parts of Virginia organized the new state of West Virginia, which was admitted to the Union during the war on June 20, 1863.

Confederate control over its claimed territory and population in congressional districts steadily shrank from 73% to 34% during the course of the American Civil War due to the Union's successful overland campaigns, its control of the inland waterways into the South, and its blockade of the southern coast. With the Emancipation Proclamation on January 1, 1863, the Union made abolition of slavery a war goal (in addition to reunion). As Union forces moved southward, large numbers of plantation slaves were freed. Many joined the Union lines, enrolling in service as soldiers, teamsters and laborers. The most notable advance was Sherman's "March to the Sea" in late 1864. Much of the Confederacy's infrastructure was destroyed, including telegraphs, railroads and bridges. Plantations in the path of Sherman's forces were severely damaged. Internal movement became increasingly difficult for Southerners, weakening the economy and limiting army mobility.

These losses created an insurmountable disadvantage in men, materiel, and finance. Public support for Confederate President Jefferson Davis's administration eroded over time due to repeated military reverses, economic hardships, and allegations of autocratic government. After four years of campaigning, Richmond was captured by Union forces in April 1865. A few days later General Lee surrendered to Union General Ulysses S. Grant, effectively signalling the collapse of the Confederacy. President Davis was captured on May 10, 1865, and jailed in preparation for a treason trial that was ultimately never held.

The initial Confederacy was established in the Montgomery Convention in February 1861 by 7 states (SC, MS, AL, FL, GA, LA, adding TX in March before Lincoln's inauguration), expanded in May–July 1861 (with VA, AR, TN, NC), and was disintegrated in April–May 1865. It was formed by delegations from seven slave states of the Lower South that had proclaimed their secession from the Union. After the fighting began in April, four additional slave states seceded and were admitted. Later, two slave states (Missouri and Kentucky) and two territories were given seats in the Confederate Congress. Southern California, although having some pro-Confederate sentiment, was never organized as a territory.

Many southern whites had considered themselves more Southern than American and were prepared to fight for their state and their region to be independent of the larger nation. That regionalism became a Southern nationalism, or the "Cause". For the duration of its existence, the Confederacy underwent trial by war. The "Southern Cause" transcended the ideology of states' rights, tariff policy, or internal improvements. This "Cause" supported, or descended from, cultural and financial dependence on the South's slavery-based economy. The convergence of race and slavery, politics, and economics raised almost all South-related policy questions to the status of moral questions over way of life, commingling love of things Southern and hatred of things Yankee (the North). Not only did national political parties split, but national churches and interstate families as well divided along sectional lines as the war approached. According to historian John M. Coski,

Southern Democrats had chosen John Breckinridge as their candidate during the U.S. presidential election of 1860, but in no Southern state (other than South Carolina, where the legislature chose the electors) was support for him unanimous; all of the other states recorded at least some popular votes for one or more of the other three candidates (Abraham Lincoln, Stephen A. Douglas and John Bell). Support for these candidates, collectively, ranged from significant to an outright majority, with extremes running from 25% in Texas to 81% in Missouri. There were minority views everywhere, especially in the upland and plateau areas of the South, with western Virginia and eastern Tennessee of particular concentration.

Following South Carolina's unanimous 1860 secession vote, no other Southern states considered the question until 1861, and when they did none had a unanimous vote. All had residents who cast significant numbers of Unionist votes in either the legislature, conventions, popular referendums, or in all three. Voting to remain in the Union did not necessarily mean that individuals were northern sympathizers. Once hostilities began, many of these who voted to remain in the Union, particularly in the Deep South, accepted the majority decision, and supported the Confederacy.

The American Civil War became an American tragedy, what some scholars termed the "Brothers' War", pitting "brother against brother, father against son, kin against kin of every degree".

According to historian Avery O. Craven in 1950, the Confederate States of America was created by secessionists in Southern slave states who believed that the federal government was making them second-class citizens and refused to honor their belief that slavery was beneficial to the Negro. They judged the agent of change to be abolitionists and anti-slavery elements in the Republican Party, whom they believed used repeated insult and injury to subject them to intolerable "humiliation and degradation". The "Black Republicans" (as the Southerners called them) and their allies soon dominated the U.S. House, Senate, and Presidency. On the U.S. Supreme Court, Chief Justice Roger B. Taney (a presumed supporter of slavery) was 83 years old, and ailing.

During the campaign for president in 1860, some secessionists threatened disunion should Lincoln (who opposed the expansion of slavery into the territories) be elected, most notably William L. Yancey. Yancey toured the North calling for secession as Stephen A. Douglas toured the South calling for union in the event of Lincoln's election. To the Secessionists the Republican intent was clear: to contain slavery within its present bounds, and, eventually, to eliminate it entirely. A Lincoln victory presented them with a momentous choice (as they saw it), even before his inauguration – "the Union without slavery, or slavery without the Union".

The immediate catalyst for secession was the victory of the Republican Party and the election of Abraham Lincoln as president in the 1860 elections. American Civil War historian James M. McPherson suggested that, for the Southerners, the most ominous feature of the Republican victories in the Congressional and Presidential elections of 1860 was the magnitude of those victories. Republicans captured over 60 percent of the Northern vote, and won three-fourths of its Congressional delegations. The Southern press said that such Republicans represented the anti-slavery portion of the North, "a party founded on the single sentiment…of hatred of African slavery", and now the controlling power in national affairs. The "Black Republican party" could overwhelm conservative Yankees. "The New Orleans Delta" said of the Republicans, "It is in fact, essentially, a revolutionary party" to overthrow slavery.

By 1860, sectional disagreements between North and South relate primarily to the maintenance or expansion of slavery in the United States. Historian Drew Gilpin Faust observed that "leaders of the secession movement across the South cited slavery as the most compelling reason for southern independence". Although most white Southerners did not own slaves, the majority supported the institution of slavery and benefited in indirect ways from the slave society. For struggling yeomen and subsistence farmers, the slave society provided a large class of people ranked lower in the social scale than they. Secondary differences related to issues of free speech, runaway slaves, expansion into Cuba, and states' rights.

Historian Emory Thomas assessed the Confederacy's self-image by studying the correspondence sent by the Confederate government in 1861–62 to foreign governments. He found that Confederate diplomacy projected multiple contradictory self-images:

In what later became known as the Cornerstone Speech, C.S. Vice President Alexander H. Stephens declared that the "cornerstone" of the new government "rest[ed] upon the great truth that the negro is not equal to the white man; that slavery – subordination to the superior race – is his natural and normal condition. This, our new government, is the first, in the history of the world, based upon this great physical, philosophical, and moral truth". After the war Stephens made efforts to qualify his remarks, claiming they were extemporaneous, metaphorical, and intended to refer to public sentiment rather than "the principles of the new Government on this subject".

Four of the seceding states, the Deep South states of South Carolina,
Mississippi, Georgia, and Texas, issued formal declarations of causes, each of which identified the threat to slaveholders' rights as the cause of, or a major cause of, secession. Georgia also claimed a general Federal policy of favoring Northern over Southern economic interests. Texas mentioned slavery 21 times, but also listed the failure of the federal government to live up to its obligations, in the original annexation agreement, to protect settlers along the exposed western frontier. Texas resolutions further stated that governments of the states and the nation were established "exclusively by the white race, for themselves and their posterity". They also stated that although equal civil and political rights applied to all white men, they did not apply to those of the "African race", further opining that the end of racial enslavement would "bring inevitable calamities upon both [races] and desolation upon the fifteen slave-holding states".

Alabama did not provide a separate declaration of causes. Instead the Alabama ordinance stated "the election of Abraham Lincoln...by a sectional party, avowedly hostile to the domestic institutions and to the peace and security of the people of the State of Alabama, preceded by many and dangerous infractions of the Constitution of the United States by many of the States and people of the northern section, is a political wrong of so insulting and menacing a character as to justify the people of the State of Alabama in the adoption of prompt and decided measures for their future peace and security." The ordinance invited "the slaveholding States of the South, who may approve such purpose, in order to frame a provisional as well as a permanent Government upon the principles of the Constitution of the United States" to participate in a February 4, 1861 convention in Montgomery, Alabama.

The secession ordinances of the remaining two states, Florida and Louisiana, simply declared their severing of ties with the federal Union, without stating any causes. Afterward, the Florida secession convention formed a committee to draft a declaration of causes, but the committee was discharged before completion of the task. Only an undated, untitled draft remains.

Four of the Upper South states initially rejected secession until after the clash at Ft. Sumter (Virginia, Arkansas, Tennessee, and North Carolina). Virginia's ordinance stated a kinship with the slave-holding states of the Lower South, but did not name the institution itself as a primary reason for its course

Arkansas's secession ordinance primarily revolved around strong objection to the use of military force to maintain the Union as its motivating factor. Prior to the outbreak of war, the Arkansas Convention had on March 20 given as their first resolution: "The people of the Northern States have organized a political party, purely sectional in its character, the central and controlling idea of which is hostility to the institution of African slavery, as it exists in the Southern States; and that party has elected a President...pledged to administer the Government upon principles inconsistent with the rights and subversive of the interests of the Southern States."

North Carolina and Tennessee limited their ordinances to simply withdrawing, although Tennessee went so far as to make clear they wished to make no comment at all on the "abstract doctrine of secession".

In a message to the Confederate Congress on April 29, 1861 Jefferson Davis cited both the tariff and slavery for the South's secession.

The Fire-Eaters, calling for immediate secession, were opposed by two factions. "Cooperationists" in the Deep South would delay secession until several states went together, maybe in a Southern Convention. Under the influence of men such as Texas Governor Sam Houston, delay would have had the effect of sustaining the Union. "Unionists", especially in the Border South, often former Whigs, appealed to sentimental attachment to the United States. Southern Unionists' favorite presidential candidate was John Bell of Tennessee, sometimes running under an "Opposition Party" banner.

Many secessionists were active politically. Governor William Henry Gist of South Carolina corresponded secretly with other Deep South governors, and most southern governors exchanged clandestine commissioners. Charleston's secessionist "1860 Association" published over 200,000 pamphlets to persuade the youth of the South. The most influential were: "The Doom of Slavery" and "The South Alone Should Govern the South", both by John Townsend of South Carolina; and James D.B. De Bow's "The Interest of Slavery of the Southern Non-slaveholder".

Developments in South Carolina started a chain of events. The foreman of a jury refused the legitimacy of federal courts, so Federal Judge Andrew Magrath ruled that U.S. judicial authority in South Carolina was vacated. A mass meeting in Charleston celebrating the Charleston and Savannah railroad and state cooperation led to the South Carolina legislature to call for a Secession Convention. U.S. Senator James Chesnut, Jr. resigned, as did Senator James Henry Hammond.

Elections for Secessionist conventions were heated to "an almost raving pitch, no one dared dissent", said Freehling. Even once–respected voices, including the Chief Justice of South Carolina, John Belton O'Neall, lost election to the Secession Convention on a Cooperationist ticket. Across the South mobs expelled Yankees and (in Texas) executed German-Americans suspected of loyalty to the United States. Generally, seceding conventions which followed did not call for a referendum to ratify, although Texas, Arkansas, and Tennessee did, as well as Virginia's second convention. Kentucky declared neutrality, while Missouri had its own civil war until the Unionists took power and drove the Confederate legislators out of the state.

In the antebellum months, the Corwin Amendment was an unsuccessful attempt by the Congress to bring back the seceding states to the Union and to prevent the border slave states to remain. It was a proposed amendment to the United States Constitution by Ohio Congressman Thomas Corwin that would shield "domestic institutions" of the states (which in 1861 included slavery) from the constitutional amendment process and from abolition or interference by Congress.

It was passed by the 36th Congress on March 2, 1861. The House approved it by a vote of 133 to 65 & the United States Senate adopted it, with no changes, on a vote of 24 to 12. It was then submitted to the state legislatures for ratification. In his inaugural address Lincoln endorsed the proposed amendment.

The text was as follows:

No amendment shall be made to the Constitution which will authorize or give to Congress the power to abolish or interfere, within any State, with the domestic institutions thereof, including that of persons held to labor or service by the laws of said State.

Had it been ratified by the required number of states prior to 1865, it would have made institutionalized slavery immune to the constitutional amendment procedures and to interference by Congress.

The first secession state conventions from the Deep South sent representatives to meet at the Montgomery Convention in Montgomery, Alabama, on February 4, 1861. There the fundamental documents of government were promulgated, a provisional government was established, and a representative Congress met for the Confederate States of America.

The new 'provisional' Confederate President Jefferson Davis issued a call for 100,000 men from the various states' militias to defend the newly formed Confederacy. All Federal property was seized, along with gold bullion and coining dies at the U.S. mints in Charlotte, North Carolina; Dahlonega, Georgia; and New Orleans.
The Confederate capital was moved from Montgomery to Richmond, Virginia, in May 1861. On February 22, 1862, Davis was inaugurated as president with a term of six years.

The newly inaugurated Confederate administration pursued a policy of national territorial integrity, continuing earlier state efforts in 1860 and early 1861 to remove U.S. government presence from within their boundaries. These efforts included taking possession of U.S. courts, custom houses, post offices, and most notably, arsenals and forts. But after the Confederate attack and capture of Fort Sumter in April 1861, Lincoln called up 75,000 of the states' militia to muster under his command. The stated purpose was to re-occupy U.S. properties throughout the South, as the U.S. Congress had not authorized their abandonment. The resistance at Fort Sumter signaled his change of policy from that of the Buchanan Administration. Lincoln's response ignited a firestorm of emotion. The people of both North and South demanded war, and young men rushed to their colors in the hundreds of thousands. Four more states (Virginia, North Carolina, Tennessee, and Arkansas) refused Lincoln's call for troops and declared secession, while Kentucky maintained an uneasy "neutrality".

Secessionists argued that the United States Constitution was a contract among sovereign states that could be abandoned at any time without consultation and that each state had a right to secede. After intense debates and statewide votes, seven Deep South cotton states passed secession ordinances by February 1861 (before Abraham Lincoln took office as president), while secession efforts failed in the other eight slave states. Delegates from those seven formed the CSA in February 1861, selecting Jefferson Davis as the provisional president. Unionist talk of reunion failed and Davis began raising a 100,000 man army.

Initially, some secessionists may have hoped for a peaceful departure. Moderates in the Confederate Constitutional Convention included a provision against importation of slaves from Africa to appeal to the Upper South. Non-slave states might join, but the radicals secured a two-thirds hurdle for them.

Seven states declared their secession from the United States before Lincoln took office on March 4, 1861. After the Confederate attack on Fort Sumter April 12, 1861, and Lincoln's subsequent call for troops on April 15, four more states declared their secession:

Kentucky declared neutrality but after Confederate troops moved in, the state government asked for Union troops to drive them out. The splinter Confederate state government relocated to accompany western Confederate armies and never controlled the state population. By the end of the war, 90,000 Kentuckians had fought on the side of the Union, compared to 35,000 for the Confederate States.

In Missouri, a constitutional convention was approved and delegates elected by voters. The convention rejected secession 89–1 on March 19, 1861. The governor maneuvered to take control of the St. Louis Arsenal and restrict Federal movements. This led to confrontation, and in June Federal forces drove him and the General Assembly from Jefferson City. The executive committee of the constitutional convention called the members together in July. The convention declared the state offices vacant, and appointed a Unionist interim state government. The exiled governor called a rump session of the former General Assembly together in Neosho and, on October 31, 1861, passed an ordinance of secession. It is still a matter of debate as to whether a quorum existed for this vote. The Confederate state government was unable to control very much Missouri territory. It had its capital first at Neosho, then at Cassville, before being driven out of the state. For the remainder of the war, it operated as a government in exile at Marshall, Texas.

Neither Kentucky nor Missouri was declared in rebellion in Lincoln's Emancipation Proclamation. The Confederacy recognized the pro-Confederate claimants in both Kentucky (December 10, 1861) and Missouri (November 28, 1861) and laid claim to those states, granting them Congressional representation and adding two stars to the Confederate flag. Voting for the representatives was mostly done by Confederate soldiers from Kentucky and Missouri.

The order of secession resolutions and dates are:

1. South Carolina (December 20, 1860) 
2. Mississippi (January 9, 1861) 
3. Florida (January 10) 
4. Alabama (January 11) 
5. Georgia (January 19) 
6. Louisiana (January 26) 
7. Texas (February 1; referendum February 23) 
Ft. Sumter (April 12) and Lincoln's call up (April 15)
8. Virginia (April 17; referendum May 23, 1861) 
9. Arkansas (May 6) 
10. Tennessee (May 7; referendum June 8) 
11. North Carolina (May 20) 
In Virginia, the populous counties along the Ohio and Pennsylvania borders rejected the Confederacy. Unionists held a Convention in Wheeling in June 1861, establishing a "restored government" with a rump legislature, but sentiment in the region remained deeply divided. In the 50 counties that would make up the state of West Virginia, voters from 24 counties had voted for disunion in Virginia's May 23 referendum on the ordinance of secession. In the 1860 Presidential election "Constitutional Democrat" Breckenridge had outpolled "Constitutional Unionist" Bell in the 50 counties by 1,900 votes, 44% to 42%. Regardless of scholarly disputes over election procedures and results county by county, altogether they simultaneously supplied over 20,000 soldiers to each side of the conflict. Representatives for most of the counties were seated in both state legislatures at Wheeling and at Richmond for the duration of the war.

Attempts to secede from the Confederacy by some counties in East Tennessee were checked by martial law.
Although slave-holding Delaware and Maryland did not secede, citizens from those states exhibited divided loyalties. Regiments of Marylanders fought in Lee's Army of Northern Virginia. But overall, 24,000 men from Maryland joined the Confederate armed forces, compared to 63,000 who joined Union forces.

Delaware never produced a full regiment for the Confederacy, but neither did it emancipate slaves as did Missouri and West Virginia. District of Columbia citizens made no attempts to secede and through the war years, referendums sponsored by President Lincoln approved systems of compensated emancipation and slave confiscation from "disloyal citizens".

Citizens at Mesilla and Tucson in the southern part of New Mexico Territory formed a secession convention, which voted to join the Confederacy on March 16, 1861, and appointed Lewis Owings as the new territorial governor. They won the Battle of Mesilla and established a territorial government with Mesilla serving as its capital. The Confederacy proclaimed the Confederate Arizona Territory on February 14, 1862, north to the 34th parallel. Marcus H. MacWillie served in both Confederate Congresses as Arizona's delegate. In 1862 the Confederate New Mexico Campaign to take the northern half of the U.S. territory failed and the Confederate territorial government in exile relocated to San Antonio, Texas.

Confederate supporters in the trans-Mississippi west also claimed portions of United States Indian Territory after the United States evacuated the federal forts and installations. Over half of the American Indian troops participating in the Civil War from the Indian Territory supported the Confederacy; troops and one general were enlisted from each tribe. On July 12, 1861, the Confederate government signed a treaty with both the Choctaw and Chickasaw Indian nations. After several battles Union armies took control of the territory.

Indian Territory was never formally ceded into the Confederacy by American Indian councils, but like Missouri and Kentucky, the Five Civilized Nations received representation in the Confederate Congress and their citizens were integrated into regular Confederate Army units. After 1863 the tribal governments sent representatives to the Confederate Congress: Elias Cornelius Boudinot representing the Cherokee and Samuel Benton Callahan representing the Seminole and Creek people. The Cherokee Nation, aligning with the Confederacy, alleged northern violations of the Constitution, waging war against slavery commercial and political interests, abolishing slavery in the Indian Territory, and that the North intended to seize additional Indian lands.

Montgomery, Alabama served as the capital of the Confederate States of America from February 4 until May 29, 1861, in the Alabama State Capitol. Six states created the Confederate States of America there on February 8, 1861. The Texas delegation was seated at the time, so it is counted in the "original seven" states of the Confederacy; it had no roll call vote until after its referendum made secession "operative". Two sessions of the Provisional Congress were held in Montgomery, adjourning May 21. The Permanent Constitution was adopted there on March 12, 1861.

The permanent capital provided for in the Confederate Constitution called for a state cession of a ten-miles square (100 square mile) district to the central government. Atlanta, which had not yet supplanted Milledgeville, Georgia as its state capital, put in a bid noting its central location and rail connections, as did Opelika, Alabama, noting its strategically interior situation, rail connections and nearby deposits of coal and iron.

Richmond, Virginia was chosen for the interim capital at the Virginia State Capitol. The move was used by Vice President Stephens and others to encourage other border states to follow Virginia into the Confederacy. In the political moment it was a show of "defiance and strength". The war for southern independence was surely to be fought in Virginia, but it also had the largest Southern military-aged white population, with infrastructure, resources and supplies required to sustain a war. The Davis Administration's policy was that, "It must be held at all hazards."

The naming of Richmond as the new capital took place on May 30, 1861, and the last two sessions of the Provisional Congress were held in the new capital. The Permanent Confederate Congress and President were elected in the states and army camps on November 6, 1861. The First Congress met in four sessions in Richmond from February 18, 1862, to February 17, 1864. The Second Congress met there in two sessions, from May 2, 1864, to March 18, 1865.

As war dragged on, Richmond became crowded with training and transfers, logistics and hospitals. Prices rose dramatically despite government efforts at price regulation. A movement in Congress led by Henry S. Foote of Tennessee argued for moving the capital from Richmond. At the approach of Federal armies in mid-1862, the government's archives were readied for removal. As the Wilderness Campaign progressed, Congress authorized Davis to remove the executive department and call Congress to session elsewhere in 1864 and again in 1865. Shortly before the end of the war, the Confederate government evacuated Richmond, planning to relocate farther south. Little came of these plans before Lee's surrender at Appomattox Court House, Virginia on April 9, 1865. Davis and most of his cabinet fled to Danville, Virginia, which served as their headquarters for about a week.

Unionism was widespread in the Confederacy, especially in the mountain regions of Appalachia and the Ozarks. Unionists, led by Parson Brownlow and Senator Andrew Johnson, took control of eastern Tennessee in 1863. Unionists also attempted control over western Virginia but never effectively held more than half the counties that formed the new state of West Virginia. Union forces captured parts of coastal North Carolina, and at first were welcomed by local unionists. That changed as the occupiers became perceived as oppressive, callous, radical and favorable to the Freedmen. Occupiers engaged in pillaging, freeing of slaves, and eviction of those refusing to take or reneging on the loyalty oaths, as ex-Unionists began to support the Confederate cause.

Support for the Confederacy was perhaps weakest in Texas; Claude Elliott estimates that only a third of the population actively supported the Confederacy. Many unionists supported the Confederacy after the war began, but many others clung to their unionism throughout the war, especially in the northern counties, the German districts, and the Mexican areas. According to Ernest Wallace: "This account of a dissatisfied Unionist minority, although historically essential, must be kept in its proper perspective, for throughout the war the overwhelming majority of the people zealously supported the Confederacy..." Randolph B. Campbell states "In spite of terrible losses and hardships, most Texans continued throughout the war to support the Confederacy as they had supported secession". Dale Baum in his analysis of Texas politics in the era counters: "This idea of a Confederate Texas united politically against northern adversaries was shaped more by nostalgic fantasies than by wartime realities." He characterizes Texas Civil War history as "a morose story of intragovernmental rivalries coupled with wide-ranging disaffection that prevented effective implementation of state wartime policies."

In Texas local officials harassed unionists and engaged in large-scale massacres against unionists and Germans. In Cooke County 150 suspected unionists were arrested; 25 were lynched without trial and 40 more were hanged after a summary trial. Draft resistance was widespread especially among Texans of German or Mexican descent; many of the latter went to Mexico. Potential draftees went into hiding, Confederate officials hunted them down, and many were shot.

Civil liberties were of small concern in North and South. Lincoln and Davis both took a hard line against dissent. Neely explores how the Confederacy became a virtual police state with guards and patrols all about, and a domestic passport system whereby everyone needed official permission each time they wanted to travel. Over 4,000 suspected unionists were imprisoned without trial.

During the four years of its existence under trial by war, the Confederate States of America asserted its independence and appointed dozens of diplomatic agents abroad. None were ever officially recognized by a foreign government. The United States government regarded the southern states in rebellion and so refused any formal recognition of their status.

Even before Fort Sumter, U.S. Secretary of State William H. Seward issued formal instructions to the American minister to Britain, Charles Francis Adams:

Seward instructed Adams that if the British government seemed inclined to recognize the Confederacy, or even waver in that regard, it was to receive a sharp warning, with a strong hint of war: 

The United States government never declared war on those "kindred and countrymen" in the Confederacy, but conducted its military efforts beginning with a presidential proclamation issued April 15, 1861. It called for troops to recapture forts and suppress what Lincoln later called an "insurrection and rebellion".

Mid-war parleys between the two sides occurred without formal political recognition, though the laws of war predominantly governed military relationships on both sides of uniformed conflict.

On the part of the Confederacy, immediately following Fort Sumter the Confederate Congress proclaimed "... war exists between the Confederate States and the Government of the United States, and the States and Territories thereof ..." A state of war was not to formally exist between the Confederacy and those states and territories in the United States allowing slavery, although Confederate Rangers were compensated for destruction they could effect there throughout the war.
Concerning the international status and nationhood of the Confederate States of America, in 1869 the United States Supreme Court in ruled Texas' declaration of secession was legally null and void. Jefferson Davis, former President of the Confederacy, and Alexander H. Stephens, its former Vice-President, both wrote postwar arguments in favor of secession's legality and the international legitimacy of the Government of the Confederate States of America, most notably Davis' "The Rise and Fall of the Confederate Government".

Once the war with the United States began, the Confederacy pinned its hopes for survival on military intervention by Great Britain and France. The Confederates who had believed that "cotton is king" – that is, Britain had to support the Confederacy to obtain cotton – proved mistaken. The British had stocks to last over a year and had been developing alternative sources of cotton, most notably India and Egypt. They were not about to go to war with the U.S. to acquire more cotton at the risk of losing the large quantities of food imported from the North. The Confederate government sent repeated delegations to Europe but historians give them low marks for their poor diplomacy. James M. Mason went to London and John Slidell traveled to Paris. They were unofficially interviewed, but neither secured official recognition for the Confederacy.

In late 1861 the seizure of a British ship by the U.S. navy outraged Britain and led to a war scare in the Trent Affair. Recognition of the Confederacy seemed at hand, but Lincoln released the two detained Confederate diplomats, tensions cooled, and the Confederacy gained no advantage.

Throughout the early years of the war, British foreign secretary Lord John Russell, Emperor Napoleon III of France, and, to a lesser extent, British Prime Minister Lord Palmerston, showed interest in recognition of the Confederacy or at least mediation of the war. William Ewart Gladstone, the British finance minister whose family wealth was based on slavery, was the key advocate calling for intervention to help the Confederacy achieve independence. He failed to convince prime minister Palmerston. By September 1862 the Union victory at the Battle of Antietam, Lincoln's preliminary Emancipation Proclamation and abolitionist opposition in Britain put an end to these possibilities. The cost to Britain of a war with the U.S. would have been high: the immediate loss of American grain shipments, the end of exports to the U.S., and the seizure of billions of pounds invested in American securities. War would have meant higher taxes, another invasion of Canada, and full-scale worldwide attacks on the British merchant fleet. Outright recognition would have meant certain war with the United States; in mid-1862 fears of race war as had transpired in Haiti led to the British considering intervention for humanitarian reasons. Lincoln's Emancipation Proclamation did not lead to interracial violence let alone a bloodbath, but it did give the friends of the Union strong talking points in the arguments that raged across Britain.

John Slidell, emissary to France, did succeed in negotiating a loan of $15,000,000 from Erlanger and other French capitalists. The money was used to buy ironclad warships, as well as military supplies that came in by blockade runners. The British government did allow blockade runners to be built in Britain; they were owned and operated by British financiers and sailors; a few were owned and operated by the Confederacy. The British investors' goal was to get highly profitable cotton.

Several European nations maintained diplomats in place who had been appointed to the U.S., but no country appointed any diplomat to the Confederacy. Those nations recognized the Union and Confederate sides as belligerents. In 1863, the Confederacy expelled the European diplomatic missions for advising their resident subjects to refuse to serve in the Confederate army. Both Confederate and Union agents were allowed to work openly in British territories. Some state governments in northern Mexico negotiated local agreements to cover trade on the Texas border. Pope Pius IX wrote a letter to Jefferson Davis in which he addressed Davis as the "Honorable President of the Confederate States of America". The Confederacy appointed Ambrose Dudley Mann as special agent to the Holy See on September 24, 1863. But the Holy See never released a formal statement supporting or recognizing the Confederacy.

Nevertheless, the Confederacy was seen internationally as a serious attempt at nationhood, and European governments sent military observers, both official and unofficial, to assess whether there had been a de facto establishment of independence. These included Arthur Lyon Fremantle of the British Coldstream Guards, Fitzgerald Ross of the Austrian Hussars and Justus Scheibert of the Prussian Army. European travelers visited and wrote accounts for publication. Importantly in 1862, the Frenchman Charles Girard's "Seven months in the rebel states during the North American War" testified "this government ... is no longer a trial government ... but really a normal government, the expression of popular will".
Fremantle went on to write in his book ""Three Months in the Southern States"" that he had "not attempted to conceal any of the peculiarities or defects of the Southern people. Many persons will doubtless highly disapprove of some of their customs and habits in the wilder portion of the country; but I think no generous man, whatever may be his political opinions, can do otherwise than admire the courage, energy, and patriotism of the whole population, and the skill of its leaders, in this struggle against great odds. And I am also of opinion that many will agree with me in thinking that a people in which all ranks and both sexes display a unanimity and a heroism which can never have been surpassed in the history of the world, is destined, sooner or later, to become a great and independent nation."
French Emperor Napoleon III assured Confederate diplomat John Slidell that he would make "direct proposition" to Britain for joint recognition. The Emperor made the same assurance to Members of Parliament John A. Roebuck and John A. Lindsay. Roebuck in turn publicly prepared a bill to submit to Parliament June 30 supporting joint Anglo-French recognition of the Confederacy. "Southerners had a right to be optimistic, or at least hopeful, that their revolution would prevail, or at least endure". Following the dual reverses at Vicksburg and Gettysburg in July 1863, the Confederates "suffered a severe loss of confidence in themselves", and withdrew into an interior defensive position. There would be no help from the Europeans.

By December 1864, Davis considered sacrificing slavery in order to enlist recognition and aid from Paris and London; he secretly sent Duncan F. Kenner to Europe with a message that the war was fought solely for "the vindication of our rights to self-government and independence" and that "no sacrifice is too great, save that of honor." The message stated that if the French or British governments made their recognition conditional on anything at all, the Confederacy would consent to such terms. Davis's message could not explicitly acknowledge that slavery was on the bargaining table due to still-strong domestic support for slavery among the wealthy and politically influential. European leaders all saw that the Confederacy was on the verge of total defeat.

The great majority of young white men voluntarily joined Confederate national or state military units. Perman (2010) says historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:

Civil War historian E. Merton Coulter noted that for those who would secure its independence, "The Confederacy was unfortunate in its failure to work out a general strategy for the whole war". Aggressive strategy called for offensive force concentration. Defensive strategy sought dispersal to meet demands of locally minded governors. The controlling philosophy evolved into a combination "dispersal with a defensive concentration around Richmond". The Davis administration considered the war purely defensive, a "simple demand that the people of the United States would cease to war upon us". Historian James M. McPherson is a critic of Lee's Offensive Strategy: "Lee pursued a faulty military strategy that ensured Confederate defeat".

As the Confederate government lost control of territory in campaign after campaign, it was said that "the vast size of the Confederacy would make its conquest impossible". The enemy would be struck down by the same elements which so often debilitated or destroyed visitors and transplants in the South. Heat exhaustion, sunstroke, endemic diseases such as malaria and typhoid would match the destructive effectiveness of the Moscow winter on the invading armies of Napoleon.

Early in the war both sides believed that one great battle would decide the conflict; the Confederate won a great victory at the First Battle of Bull Run, also known as First Manassas (the name used by Confederate forces). It drove the Confederate people "insane with joy"; the public demanded a forward movement to capture Washington relocate the Confederate capital there, and admit Maryland to the Confederacy. A council of war by the victorious Confederate generals decided not to advance against larger numbers of fresh Federal troops in defensive positions. Davis did not countermand it. Following the Confederate incursion halted at the Battle of Antietam in October 1862, generals proposed concentrating forces from state commands to re-invade the north. Nothing came of it. Again in early 1863 at his incursion into Pennsylvania, Lee requested of Davis that Beauregard simultaneously attack Washington with troops taken from the Carolinas. But the troops there remained in place during the Gettysburg Campaign.

The eleven states of the Confederacy were outnumbered by the North about four to one in white men of military age. It was overmatched far more in military equipment, industrial facilities, railroads for transport, and wagons supplying the front.

Confederate military policy innovated to slow the invaders, but at heavy cost to the Southern infrastructure. The Confederates burned bridges, laid land mines in the roads, and made harbors inlets and inland waterways unusable with sunken mines (called "torpedos" at the time). Coulter reports:

The Confederacy relied on external sources for war materials. The first came from trade with the enemy. "Vast amounts of war supplies" came through Kentucky, and thereafter, western armies were "to a very considerable extent" provisioned with illicit trade via Federal agents and northern private traders. But that trade was interrupted in the first year of war by Admiral Porter's river gunboats as they gained dominance along navigable rivers north–south and east–west. Overseas blockade running then came to be of "outstanding importance". On April 17, President Davis called on privateer raiders, the "militia of the sea", to make war on U.S. seaborne commerce. Despite noteworthy effort, over the course of the war the Confederacy was found unable to match the Union in ships and seamanship, materials and marine construction.

Perhaps the greatest obstacle to success in the 19th century warfare of mass armies was the Confederacy's lack of manpower, and sufficient numbers of disciplined, equipped troops in the field at the point of contact with the enemy. During the winter of 1862–63, Lee observed that none of his famous victories had resulted in the destruction of the opposing army. He lacked reserve troops to exploit an advantage on the battlefield as Napoleon had done. Lee explained, "More than once have most promising opportunities been lost for want of men to take advantage of them, and victory itself had been made to put on the appearance of defeat, because our diminished and exhausted troops have been unable to renew a successful struggle against fresh numbers of the enemy."

The military armed forces of the Confederacy comprised three branches: Army, Navy and Marine Corps.

The Confederate military leadership included many veterans from the United States Army and United States Navy who had resigned their Federal commissions and had won appointment to senior positions in the Confederate armed forces. Many had served in the Mexican–American War (including Robert E. Lee and Jefferson Davis), but some such as Leonidas Polk (who graduated from West Point but did not serve in the Army) had little or no experience.

The Confederate officer corps consisted of men from both slave-owning and non-slave-owning families. The Confederacy appointed junior and field grade officers by election from the enlisted ranks. Although no Army service academy was established for the Confederacy, some colleges (such as The Citadel and Virginia Military Institute) maintained cadet corps that trained Confederate military leadership. A naval academy was established at Drewry's Bluff, Virginia in 1863, but no midshipmen graduated before the Confederacy's end.

The soldiers of the Confederate armed forces consisted mainly of white males aged between 16 and 28. The median year of birth was 1838, so half the soldiers were 23 or older by 1861. In early 1862, the Confederate Army was allowed to disintegrate for two months following expiration of short-term enlistments. A majority of those in uniform would not re-enlist following their one-year commitment, so on April 16, 1862, the Confederate Congress enacted the first mass conscription on the North American continent. (The U.S. Congress followed a year later on March 3, 1863, with the Enrollment Act.) Rather than a universal draft, the initial program was a selective service with physical, religious, professional and industrial exemptions. These were narrowed as the war progressed. Initially substitutes were permitted, but by December 1863 these were disallowed. In September 1862 the age limit was increased from 35 to 45 and by February 1864, all men under 18 and over 45 were conscripted to form a reserve for state defense inside state borders. By March 1864, the Superintendent of Conscription reported that all across the Confederacy, every officer in constituted authority, man and woman, "engaged in opposing the enrolling officer in the execution of his duties". Although challenged in the state courts, the Confederate State Supreme Courts routinely rejected legal challenges to conscription.

Many thousands of slaves served as laborers, cooks, and pioneers. Some freed blacks and men of color served in local state militia units of the Confederacy, primarily in Louisiana and South Carolina, but their officers deployed them for "local defense, not combat". Depleted by casualties and desertions, the military suffered chronic manpower shortages. In early 1865, the Confederate Congress, influenced by the public support by General Lee, approved the recruitment of black infantry units. Contrary to Lee's and Davis's recommendations, the Congress refused "to guarantee the freedom of black volunteers". No more than two hundred black combat troops were ever raised.

The immediate onset of war meant that it was fought by the "Provisional" or "Volunteer Army". State governors resisted concentrating a national effort. Several wanted a strong state army for self-defense. Others feared large "Provisional" armies answering only to Davis. When filling the Confederate government's call for 100,000 men, another 200,000 were turned away by accepting only those enlisted "for the duration" or twelve-month volunteers who brought their own arms or horses.

It was important to raise troops; it was just as important to provide capable officers to command them. With few exceptions the Confederacy secured excellent general officers. Efficiency in the lower officers was "greater than could have been reasonably expected". As with the Federals, political appointees could be indifferent. Otherwise, the officer corps was governor-appointed or elected by unit enlisted. Promotion to fill vacancies was made internally regardless of merit, even if better officers were immediately available.

Anticipating the need for more "duration" men, in January 1862 Congress provided for company level recruiters to return home for two months, but their efforts met little success on the heels of Confederate battlefield defeats in February. Congress allowed for Davis to require numbers of recruits from each governor to supply the volunteer shortfall. States responded by passing their own draft laws.

The veteran Confederate army of early 1862 was mostly twelve-month volunteers with terms about to expire. Enlisted reorganization elections disintegrated the army for two months. Officers pleaded with the ranks to re-enlist, but a majority did not. Those remaining elected majors and colonels whose performance led to officer review boards in October. The boards caused a "rapid and widespread" thinning out of 1700 incompetent officers. Troops thereafter would elect only second lieutenants.

In early 1862, the popular press suggested the Confederacy required a million men under arms. But veteran soldiers were not re-enlisting, and earlier secessionist volunteers did not reappear to serve in war. One Macon, Georgia, newspaper asked how two million brave fighting men of the South were about to be overcome by four million northerners who were said to be cowards.

The Confederacy passed the first American law of national conscription on April 16, 1862. The white males of the Confederate States from 18 to 35 were declared members of the Confederate army for three years, and all men then enlisted were extended to a three-year term. They would serve only in units and under officers of their state. Those under 18 and over 35 could substitute for conscripts, in September those from 35 to 45 became conscripts. The cry of "rich man's war and a poor man's fight" led Congress to abolish the substitute system altogether in December 1863. All principals benefiting earlier were made eligible for service. By February 1864, the age bracket was made 17 to 50, those under eighteen and over forty-five to be limited to in-state duty.

Confederate conscription was not universal; it was a selective service. The First Conscription Act of April 1862 exempted occupations related to transportation, communication, industry, ministers, teaching and physical fitness. The Second Conscription Act of October 1862 expanded exemptions in industry, agriculture and conscientious objection. Exemption fraud proliferated in medical examinations, army furloughs, churches, schools, apothecaries and newspapers.

Rich men's sons were appointed to the socially outcast "overseer" occupation, but the measure was received in the country with "universal odium". The legislative vehicle was the controversial Twenty Negro Law that specifically exempted one white overseer or owner for every plantation with at least 20 slaves. Backpedalling six months later, Congress provided overseers under 45 could be exempted only if they held the occupation before the first Conscription Act. The number of officials under state exemptions appointed by state Governor patronage expanded significantly. By law, substitutes could not be subject to conscription, but instead of adding to Confederate manpower, unit officers in the field reported that over-50 and under-17-year-old substitutes made up to 90% of the desertions.
The Conscription Act of February 1864 "radically changed the whole system" of selection. It abolished industrial exemptions, placing detail authority in President Davis. As the shame of conscription was greater than a felony conviction, the system brought in "about as many volunteers as it did conscripts." Many men in otherwise "bombproof" positions were enlisted in one way or another, nearly 160,000 additional volunteers and conscripts in uniform. Still there was shirking. To administer the draft, a Bureau of Conscription was set up to use state officers, as state Governors would allow. It had a checkered career of "contention, opposition and futility." Armies appointed alternative military "recruiters" to bring in the out-of-uniform 17–50-year-old conscripts and deserters. Nearly 3000 officers were tasked with the job. By late 1864, Lee was calling for more troops. "Our ranks are constantly diminishing by battle and disease, and few recruits are received; the consequences are inevitable." By March 1865 conscription was to be administered by generals of the state reserves calling out men over 45 and under 18 years old. All exemptions were abolished. These regiments were assigned to recruit conscripts ages 17–50, recover deserters, and repel enemy cavalry raids. The service retained men who had lost but one arm or a leg in home guards. April 1865 Lee surrendered an army of 50,000. Conscription had been a failure.

The survival of the Confederacy depended on a strong base of civilians and soldiers devoted to victory. The soldiers performed well, though increasing numbers deserted in the last year of fighting, and the Confederacy never succeeded in replacing casualties as the Union could. The civilians, although enthusiastic in 1861–62, seem to have lost faith in the future of the Confederacy by 1864, and instead looked to protect their homes and communities. As Rable explains, "This contraction of civic vision was more than a crabbed libertarianism; it represented an increasingly widespread disillusionment with the Confederate experiment."

The American Civil War broke out in April 1861 with a Confederate victory at the Battle of Fort Sumter in Charleston.

In January, President James Buchanan had attempted to resupply the garrison with the steamship, "Star of the West", but Confederate artillery drove it away. In March, President Lincoln notified South Carolina Governor Pickens that without Confederate resistance to the resupply there would be no military reinforcement without further notice, but Lincoln prepared to force resupply if it were not allowed. Confederate President Davis, in cabinet, decided to seize Fort Sumter before the relief fleet arrived, and on April 12, 1861, General Beauregard forced its surrender.

Following Sumter, Lincoln directed states to provide 75,000 troops for three months to recapture the Charleston Harbor forts and all other federal property. This emboldened secessionists in Virginia, Arkansas, Tennessee and North Carolina to secede rather than provide troops to march into neighboring Southern states. In May, Federal troops crossed into Confederate territory along the entire border from the Chesapeake Bay to New Mexico. The first battles were Confederate victories at Big Bethel (Bethel Church, Virginia), First Bull Run (First Manassas) in Virginia July and in August, Wilson's Creek (Oak Hills) in Missouri. At all three, Confederate forces could not follow up their victory due to inadequate supply and shortages of fresh troops to exploit their successes. Following each battle, Federals maintained a military presence and occupied Washington, DC; Fort Monroe, VA; and Springfield, MO. Both North and South began training up armies for major fighting the next year. Union General George B. McClellan's forces gained possession of much of northwestern Virginia in mid-1861, concentrating on towns and roads; the interior was too large to control and became the center of guerrilla activity. General Robert E. Lee was defeated at Cheat Mountain in September and no serious Confederate advance in western Virginia occurred until the next year.

Meanwhile, the Union Navy seized control of much of the Confederate coastline from Virginia to South Carolina. It took over plantations and the abandoned slaves. Federals there began a war-long policy of burning grain supplies up rivers into the interior wherever they could not occupy. The Union Navy began a blockade of the major southern ports and prepared an invasion of Louisiana to capture New Orleans in early 1862.

The victories of 1861 were followed by a series of defeats east and west in early 1862. To restore the Union by military force, the Federal strategy was to (1) secure the Mississippi River, (2) seize or close Confederate ports, and (3) march on Richmond. To secure independence, the Confederate intent was to (1) repel the invader on all fronts, costing him blood and treasure, and (2) carry the war into the North by two offensives in time to affect the mid-term elections.
Much of northwestern Virginia was under Federal control.
In February and March, most of Missouri and Kentucky were Union "occupied, consolidated, and used as staging areas for advances further South". Following the repulse of Confederate counter-attack at the Battle of Shiloh, Tennessee, permanent Federal occupation expanded west, south and east. Confederate forces repositioned south along the Mississippi River to Memphis, Tennessee, where at the naval Battle of Memphis, its River Defense Fleet was sunk. Confederates withdrew from northern Mississippi and northern Alabama. New Orleans was captured April 29 by a combined Army-Navy force under U.S. Admiral David Farragut, and the Confederacy lost control of the mouth of the Mississippi River. It had to concede extensive agricultural resources that had supported the Union's sea-supplied logistics base.

Although Confederates had suffered major reverses everywhere, as of the end of April the Confederacy still controlled territory holding 72% of its population. Federal forces disrupted Missouri and Arkansas; they had broken through in western Virginia, Kentucky, Tennessee and Louisiana. Along the Confederacy's shores, Union forces had closed ports and made garrisoned lodgments on every coastal Confederate state except Alabama and Texas. Although scholars sometimes assess the Union blockade as ineffectual under international law until the last few months of the war, from the first months it disrupted Confederate privateers, making it "almost impossible to bring their prizes into Confederate ports". British firms developed small fleets of blockade running companies, such as John Fraser and Company, and the Ordnance Department secured its own blockade runners for dedicated munitions cargoes.

During the Civil War fleets of armored warships were deployed for the first time in sustained blockades at sea. After some success against the Union blockade, in March the ironclad CSS "Virginia" was forced into port and burned by Confederates at their retreat. Despite several attempts mounted from their port cities, CSA naval forces were unable to break the Union blockade. Attempts were made by Commodore Josiah Tattnall's ironclads from Savannah in 1862 with the CSS "Atlanta". Secretary of the Navy Stephen Mallory placed his hopes in a European-built ironclad fleet, but they were never realized. On the other hand, four new English-built commerce raiders served the Confederacy, and several fast blockade runners were sold in Confederate ports. They were converted into commerce-raiding cruisers, and manned by their British crews.

In the east, Union forces could not close on Richmond. General McClellan landed his army on the Lower Peninsula of Virginia. Lee subsequently ended that threat from the east, then Union General John Pope attacked overland from the north only to be repulsed at Second Bull Run (Second Manassas). Lee's strike north was turned back at Antietam MD, then Union Major General Ambrose Burnside's offensive was disastrously ended at Fredericksburg VA in December. Both armies then turned to winter quarters to recruit and train for the coming spring.

In an attempt to seize the initiative, reprovision, protect farms in mid-growing season and influence U.S. Congressional elections, two major Confederate incursions into Union territory had been launched in August and September 1862. Both Braxton Bragg's invasion of Kentucky and Lee's invasion of Maryland were decisively repulsed, leaving Confederates in control of but 63% of its population. Civil War scholar Allan Nevins argues that 1862 was the strategic high-water mark of the Confederacy. The failures of the two invasions were attributed to the same irrecoverable shortcomings: lack of manpower at the front, lack of supplies including serviceable shoes, and exhaustion after long marches without adequate food. Also in September Confederate Gen. William W. Loring pushed Federal forces from Charleston, Virginia, and the Kanawha Valley in western Virginia, but lacking re-inforcements Loring abandoned his position and by November the region was back in Federal control.

The failed Middle Tennessee campaign was ended January 2, 1863, at the inconclusive Battle of Stones River, (Murfreesboro), both sides losing the largest percentage of casualties suffered during the war. It was followed by another strategic withdrawal by Confederate forces. The Confederacy won a significant victory April 1863, repulsing the Federal advance on Richmond at Chancellorsville, but the Union consolidated positions along the Virginia coast and the Chesapeake Bay.

Without an effective answer to Federal gunboats, river transport and supply, the Confederacy lost the Mississippi River following the capture of Vicksburg, Mississippi, and Port Hudson in July, ending Southern access to the trans-Mississippi West. July brought short-lived counters, Morgan's Raid into Ohio and the New York City draft riots. Robert E. Lee's strike into Pennsylvania was repulsed at Gettysburg, Pennsylvania despite Pickett's famous charge and other acts of valor. Southern newspapers assessed the campaign as "The Confederates did not gain a victory, neither did the enemy."

September and November left Confederates yielding Chattanooga, Tennessee, the gateway to the lower south. For the remainder of the war fighting was restricted inside the South, resulting in a slow but continuous loss of territory. In early 1864, the Confederacy still controlled 53% of its population, but it withdrew further to reestablish defensive positions. Union offensives continued with Sherman's March to the Sea to take Savannah and Grant's Wilderness Campaign to encircle Richmond and besiege Lee's army at Petersburg.

In April 1863, the C.S. Congress authorized a uniformed Volunteer Navy, many of whom were British. Wilmington and Charleston had more shipping while "blockaded" than before the beginning of hostilities. The Confederacy had altogether eighteen commerce destroying cruisers, which seriously disrupted Federal commerce at sea and increased shipping insurance rates 900 percent. Commodore Tattnall unsuccessfully attempted to break the Union blockade on the Savannah River in Georgia with an ironclad again in 1863. Beginning in April 1864 the ironclad CSS "Albemarle" engaged Union gunboats and sank or cleared them for six months on the Roanoke River North Carolina. The Federals closed Mobile Bay by sea-based amphibious assault in August, ending Gulf coast trade east of the Mississippi River. In December, the Battle of Nashville ended Confederate operations in the western theater.

Large numbers of families relocated to safer places, usually remote rural areas, bringing along household slaves if they had any. Mary Massey argues these elite exiles introduced an element of defeatism into the southern outlook.

The first three months of 1865 saw the Federal Carolinas Campaign, devastating a wide swath of the remaining Confederate heartland. The "breadbasket of the Confederacy" in the Great Valley of Virginia was occupied by Philip Sheridan. The Union Blockade captured Fort Fisher NC, and Sherman finally took Charleston SC by land attack.

The Confederacy controlled no ports, harbors or navigable rivers. Railroads were captured or had ceased operating. Its major food producing regions had been war-ravaged or occupied. Its administration survived in only three pockets of territory holding one-third its population. Its armies were defeated or disbanding. At the February 1865 Hampton Roads Conference with Lincoln, senior Confederate officials rejected his invitation to restore the Union with compensation for emancipated slaves. The three pockets of unoccupied Confederacy were southern Virginia-North Carolina, central Alabama-Florida, and Texas, the latter two areas less from any notion of resistance than from the disinterest of Federal forces to occupy them. The Davis policy was independence or nothing, while Lee's army was wracked by disease and desertion, barely holding the trenches defending Jefferson Davis' capital.

The Confederacy's last remaining blockade-running port, Wilmington, North Carolina, was lost. When the Union broke through Lee's lines at Petersburg, Richmond fell immediately. Lee surrendered the Army of Northern Virginia at Appomattox Court House, Virginia, on April 9, 1865. "The Surrender" marked the end of the Confederacy.
The CSS "Stonewall" sailed from Europe to break the Union blockade in March; on making Havana, Cuba it surrendered. Some high officials escaped to Europe, but President Davis was captured May 10; all remaining Confederate land forces surrendered by June 1865. The U.S. Army took control of the Confederate areas without post-surrender insurgency or guerrilla warfare against them, but peace was subsequently marred by a great deal of local violence, feuding and revenge killings. The last confederate military unit, the commerce raider CSS Shenandoah, surrendered on November 6, 1865 in Liverpool.

Historian Gary Gallagher concluded that the Confederacy capitulated in early 1865 because northern armies crushed "organized southern military resistance." The Confederacy's population, soldier and civilian, had suffered material hardship and social disruption. They had expended and extracted a profusion of blood and treasure until collapse; "the end had come". Jefferson Davis' assessment in 1890 determined, "With the capture of the capital, the dispersion of the civil authorities, the surrender of the armies in the field, and the arrest of the President, the Confederate States of America disappeared ... their history henceforth became a part of the history of the United States."

When the war ended over 14,000 Confederates petitioned President Johnson for a pardon; he was generous in giving them out. He issued a general amnesty to all Confederate participants in the "late Civil War" in 1868. Congress passed additional Amnesty Acts in May 1866 with restrictions on office holding, and the Amnesty Act in May 1872 lifting those restrictions. There was a great deal of discussion in 1865 about bringing treason trials, especially against Jefferson Davis. There was no consensus in President Johnson's cabinet and there were no treason trials against anyone. In the case of Davis there was a strong possibility of acquittal which would have been humiliating for the government.

Davis was indicted for treason but never tried; he was released from prison on bail in May 1867. The amnesty of December 25, 1868, by President Johnson eliminated any possibility of Jefferson Davis (or anyone else associated with the Confederacy) standing trial for treason.

Henry Wirz, the commandant of a notorious prisoner-of-war camp near Andersonville, Georgia was tried and convicted by a military court, and executed on November 10, 1865. The charges against him involved conspiracy and cruelty, not treason.

The U.S. government began a decade-long process known as Reconstruction which attempted to resolve the political and constitutional issues of the Civil War. The priorities were: to guarantee that Confederate nationalism and slavery were ended, to ratify and enforce the Thirteenth Amendment which outlawed slavery; the Fourteenth which guaranteed dual U.S. and state citizenship to all native-born residents, regardless of race; and the Fifteenth, which made it illegal to deny the right to vote because of race.

By 1877, the Compromise of 1877 ended Reconstruction in the former Confederate states. Federal troops were withdrawn from the South, where conservative white Southern Democrats had already regained political control of state governments, often through extreme violence and fraud to suppress black voting. Confederate veterans had been temporarily disenfranchised by Reconstruction policy. The prewar South had many rich areas; the war left the entire region economically devastated by military action, ruined infrastructure, and exhausted resources. Continuing to be dependent on an agricultural economy and resisting investment in infrastructure, the region remained dominated by the planter elite into the 20th century. After 1890 the Democratic-dominated legislatures worked to secure their control by passing new constitutions and amendments at the turn of the 20th century that disenfranchised most blacks and many poor whites. This exclusion of blacks from the political system, and great weakening of the Republican Party, was generally maintained until the passage of the Voting Rights Act of 1965. The Solid South of the early 20th century was built on white Democratic control of politics. The region did not achieve national levels of prosperity until long after World War II.

In "Texas v. White", the United States Supreme Court ruled – by a 5–3 majority – that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States of America. In this case, the court held that the Constitution did not permit a state to unilaterally secede from the United States. Further, that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the Constitution. This case settled the law that applied to all questions regarding state legislation during the war. Furthermore, it decided one of the "central constitutional questions" of the Civil War: The Union is perpetual and indestructible, as a matter of constitutional law. In declaring that no state could leave the Union, "except through revolution or through consent of the States", it was "explicitly repudiating the position of the Confederate states that the United States was a voluntary compact between sovereign states".

Historian Frank Lawrence Owsley argued that the Confederacy "died of states' rights." The central government was denied requisitioned soldiers and money by governors and state legislatures because they feared that Richmond would encroach on the rights of the states. Georgia's governor Joseph Brown warned of a secret conspiracy by Jefferson Davis to destroy states' rights and individual liberty. The first conscription act in North America authorizing Davis to draft soldiers was said to be the "essence of military despotism."
Vice President Alexander H. Stephens feared losing the very form of republican government. Allowing President Davis to threaten "arbitrary arrests" to draft hundreds of governor-appointed "bomb-proof" bureaucrats conferred "more power than the English Parliament had ever bestowed on the king. History proved the dangers of such unchecked authority." The abolishment of draft exemptions for newspaper editors was interpreted as an attempt by the Confederate government to muzzle presses, such as the Raleigh NC "Standard", to control elections and to suppress the peace meetings there. As Rable concludes, "For Stephens, the essence of patriotism, the heart of the Confederate cause, rested on an unyielding commitment to traditional rights" without considerations of military necessity, pragmatism or compromise.

In 1863 governor Pendleton Murrah of Texas determined that state troops were required for defense against Plains Indians and Union forces that might attack from Kansas. He refused to send his soldiers to the East. Governor Zebulon Vance of North Carolina showed intense opposition to conscription, limiting recruitment success. Vance's faith in states' rights drove him into repeated, stubborn opposition to the Davis administration.

Despite political differences within the Confederacy, no national political parties were formed because they were seen as illegitimate. "Anti-partyism became an article of political faith." Without a two-party system building alternative sets of national leaders, electoral protests tended to be narrowly state-based, "negative, carping and petty." The 1863 mid-term elections became mere expressions of futile and frustrated dissatisfaction. According to historian David M. Potter, this lack of a functioning two-party system caused "real and direct damage" to the Confederate war effort since it prevented the formulation of any effective alternatives to the conduct of the war by the Davis administration.

The enemies of President Davis proposed that the Confederacy "died of Davis." He was unfavorably compared to George Washington by critics such as Edward Alfred Pollard, editor of the most influential newspaper the "Richmond Examiner". Coulter summarizes, "The American Revolution had its Washington; the Southern Revolution had its Davis ... one succeeded and the other failed." Beyond the early honeymoon period, Davis was never popular. He unwittingly caused much internal dissension from early on. His ill health and temporary bouts of blindness disabled him for days at a time.

Coulter says Davis was heroic and his will was indomitable. But his "tenacity, determination, and will power" stirred up lasting opposition of enemies Davis could not shake. He failed to overcome "petty leaders of the states" who made the term "Confederacy" into a label for tyranny and oppression, denying the "Stars and Bars" from becoming a symbol of larger patriotic service and sacrifice. Instead of campaigning to develop nationalism and gain support for his administration, he rarely courted public opinion, assuming an aloofness, "almost like an Adams."

Escott argues that Davis was unable to mobilize Confederate nationalism in support of his government effectively, and especially failed to appeal to the small farmers who comprised the bulk of the population. In addition to the problems caused by states rights, Escott also emphasizes that the widespread opposition to any strong central government combined with the vast difference in wealth between the slave-owning class and the small farmers created insolvable dilemmas when the Confederate survival presupposed a strong central government backed by a united populace. The prewar claim that white solidarity was necessary to provide a unified Southern voice in Washington no longer held. Davis failed to build a network of supporters who would speak up when he came under criticism, and he repeatedly alienated governors and other state-based leaders by demanding centralized control of the war effort.

Davis was not an efficient administrator. He attended to too many details. He protected his friends after their failures were obvious. He spent too much time on military affairs versus his civic responsibilities. Coulter concludes he was not the ideal leader for the Southern Revolution, but he showed "fewer weaknesses than any other" contemporary character available for the role. Robert E. Lee's assessment of Davis as President was, "I knew of none that could have done as well."

The Southern leaders met in Montgomery, Alabama, to write their constitution. Much of the Confederate States Constitution replicated the United States Constitution verbatim, but it contained several explicit protections of the institution of slavery including provisions for the recognition and protection of slavery in any territory of the Confederacy. It maintained the existing ban on international slave-trading while protecting the existing internal trade of slaves among slaveholding states.

In certain areas, the Confederate Constitution gave greater powers to the states (or curtailed the powers of the central government more) than the U.S. Constitution of the time did, but in other areas, the states lost rights they had under the U.S. Constitution. Although the Confederate Constitution, like the U.S. Constitution, contained a commerce clause, the Confederate version prohibited the central government from using revenues collected in one state for funding internal improvements in another state. The Confederate Constitution's equivalent to the U.S. Constitution's general welfare clause prohibited protective tariffs (but allowed tariffs for providing domestic revenue), and spoke of "carry[ing] on the Government of the Confederate States" rather than providing for the "general welfare". State legislatures had the power to impeach officials of the Confederate government in some cases. On the other hand, the Confederate Constitution contained a Necessary and Proper Clause and a Supremacy Clause that essentially duplicated the respective clauses of the U.S. Constitution. The Confederate Constitution also incorporated each of the 12 amendments to the U.S. Constitution that had been ratified up to that point.

The Confederate Constitution did not specifically include a provision allowing states to secede; the Preamble spoke of each state "acting in its sovereign and independent character" but also of the formation of a "permanent federal government". During the debates on drafting the Confederate Constitution, one proposal would have allowed states to secede from the Confederacy. The proposal was tabled with only the South Carolina delegates voting in favor of considering the motion. The Confederate Constitution also explicitly denied States the power to bar slaveholders from other parts of the Confederacy from bringing their slaves into any state of the Confederacy or to interfere with the property rights of slave owners traveling between different parts of the Confederacy. In contrast with the language of the United States Constitution, the Confederate Constitution overtly asked God's blessing ("... invoking the favor and guidance of Almighty God ...").

The Montgomery Convention to establish the Confederacy and its executive met on February 4, 1861. Each state as a sovereignty had one vote, with the same delegation size as it held in the U.S. Congress, and generally 41 to 50 members attended. Offices were "provisional", limited to a term not to exceed one year. One name was placed in nomination for president, one for vice president. Both were elected unanimously, 6–0.

Jefferson Davis was elected provisional president. His U.S. Senate resignation speech greatly impressed with its clear rationale for secession and his pleading for a peaceful departure from the Union to independence. Although he had made it known that he wanted to be commander-in-chief of the Confederate armies, when elected, he assumed the office of Provisional President. Three candidates for provisional Vice President were under consideration the night before the February 9 election. All were from Georgia, and the various delegations meeting in different places determined two would not do, so Alexander H. Stephens was elected unanimously provisional Vice President, though with some privately held reservations. Stephens was inaugurated February 11, Davis February 18.

Davis and Stephens were elected President and Vice President, unopposed on November 6, 1861. They were inaugurated on February 22, 1862.

Historian E. M. Coulter observed, "No president of the U.S. ever had a more difficult task." Washington was inaugurated in peacetime. Lincoln inherited an established government of long standing. The creation of the Confederacy was accomplished by men who saw themselves as fundamentally conservative. Although they referred to their "Revolution", it was in their eyes more a counter-revolution against changes away from their understanding of U.S. founding documents. In Davis' inauguration speech, he explained the Confederacy was not a French-like revolution, but a transfer of rule. The Montgomery Convention had assumed all the laws of the United States until superseded by the Confederate Congress.

The Permanent Constitution provided for a President of the Confederate States of America, elected to serve a six-year term but without the possibility of re-election. Unlike the United States Constitution, the Confederate Constitution gave the president the ability to subject a bill to a line item veto, a power also held by some state governors.

The Confederate Congress could overturn either the general or the line item vetoes with the same two-thirds votes required in the U.S. Congress. In addition, appropriations not specifically requested by the executive branch required passage by a two-thirds vote in both houses of Congress. The only person to serve as president was Jefferson Davis, due to the Confederacy being defeated before the completion of his term.

The only two "formal, national, functioning, civilian administrative bodies" in the Civil War South were the Jefferson Davis administration and the Confederate Congresses. The Confederacy was begun by the Provisional Congress in Convention at Montgomery, Alabama on February 28, 1861. It had one vote per state in a unicameral assembly.

The Permanent Confederate Congress was elected and began its first session February 18, 1862. The Permanent Congress for the Confederacy followed the United States forms with a bicameral legislature. The Senate had two per state, twenty-six Senators. The House numbered 106 representatives apportioned by free and slave populations within each state. Two Congresses sat in six sessions until March 18, 1865.

The political influences of the civilian, soldier vote and appointed representatives reflected divisions of political geography of a diverse South. These in turn changed over time relative to Union occupation and disruption, the war impact on local economy, and the course of the war. Without political parties, key candidate identification related to adopting secession before or after Lincoln's call for volunteers to retake Federal property. Previous party affiliation played a part in voter selection, predominantly secessionist Democrat or unionist Whig.

The absence of political parties made individual roll call voting all the more important, as the Confederate "freedom of roll-call voting [was] unprecedented in American legislative history. Key issues throughout the life of the Confederacy related to (1) suspension of habeas corpus, (2) military concerns such as control of state militia, conscription and exemption, (3) economic and fiscal policy including impressment of slaves, goods and scorched earth, and (4) support of the Jefferson Davis administration in its foreign affairs and negotiating peace.

For the first year, the unicameral Provisional Confederate Congress functioned as the Confederacy's legislative branch.




The Confederate Constitution outlined a judicial branch of the government, but the ongoing war and resistance from states-rights advocates, particularly on the question of whether it would have appellate jurisdiction over the state courts, prevented the creation or seating of the "Supreme Court of the Confederate States;" the state courts generally continued to operate as they had done, simply recognizing the Confederate States as the national government.

Confederate district courts were authorized by Article III, Section 1, of the Confederate Constitution, and President Davis appointed judges within the individual states of the Confederate States of America. In many cases, the same US Federal District Judges were appointed as Confederate States District Judges. Confederate district courts began reopening in early 1861, handling many of the same type cases as had been done before. Prize cases, in which Union ships were captured by the Confederate Navy or raiders and sold through court proceedings, were heard until the blockade of southern ports made this impossible. After a Sequestration Act was passed by the Confederate Congress, the Confederate district courts heard many cases in which enemy aliens (typically Northern absentee landlords owning property in the South) had their property sequestered (seized) by Confederate Receivers.

When the matter came before the Confederate court, the property owner could not appear because he was unable to travel across the front lines between Union and Confederate forces. Thus, the District Attorney won the case by default, the property was typically sold, and the money used to further the Southern war effort. Eventually, because there was no Confederate Supreme Court, sharp attorneys like South Carolina's Edward McCrady began filing appeals. This prevented their clients' property from being sold until a supreme court could be constituted to hear the appeal, which never occurred. Where Federal troops gained control over parts of the Confederacy and re-established civilian government, US district courts sometimes resumed jurisdiction.

Supreme Court – not established.

District Courts – judges

When the Confederacy was formed and its seceding states broke from the Union, it was at once confronted with the arduous task of providing its citizens with a mail delivery system, and, in the midst of the American Civil War, the newly formed Confederacy created and established the Confederate Post Office. One of the first undertakings in establishing the Post Office was the appointment of John H. Reagan to the position of Postmaster General, by Jefferson Davis in 1861, making him the first Postmaster General of the Confederate Post Office as well as a member of Davis' presidential cabinet. Through Reagan's resourcefulness and remarkable industry, he had his department assembled, organized and in operation before the other Presidential cabinet members had their departments fully operational.

When the war began, the US Post Office still delivered mail from the secessionist states for a brief period of time. Mail that was postmarked after the date of a state's admission into the Confederacy through May 31, 1861, and bearing US postage was still delivered. After this time, private express companies still managed to carry some of the mail across enemy lines. Later, mail that crossed lines had to be sent by 'Flag of Truce' and was allowed to pass at only two specific points. Mail sent from the South to the North states was received, opened and inspected at Fortress Monroe on the Virginia coast before being passed on into the U.S. mail stream. Mail sent from the North to the South passed at City Point, also in Virginia, where it was also inspected before being sent on.

With the chaos of the war, a working postal system was more important than ever for the Confederacy. The Civil War had divided family members and friends and consequently letter writing increased dramatically across the entire divided nation, especially to and from the men who were away serving in an army. Mail delivery was also important for the Confederacy for a myriad of business and military reasons. Because of the Union blockade, basic supplies were always in demand and so getting mailed correspondence out of the country to suppliers was imperative to the successful operation of the Confederacy. Volumes of material have been written about the Blockade runners who evaded Union ships on blockade patrol, usually at night, and who moved cargo and mail in and out of the Confederate States throughout the course of the war. Of particular interest to students and historians of the American Civil War is "Prisoner of War mail" and "Blockade mail" as these items were often involved with a variety of military and other war time activities. The postal history of the Confederacy along with has helped historians document the various people, places and events that were involved in the American Civil War as it unfolded.
The Confederacy actively used the army to arrest people suspected of loyalty to the United States. Historian Mark Neely found 4,108 names of men arrested and estimated a much larger total. The Confederacy arrested pro-Union civilians in the South at about the same rate as the Union arrested pro-Confederate civilians in the North. Neely argues:

Across the South, widespread rumors alarmed the whites by predicting the slaves were planning some sort of insurrection. Patrols were stepped up. The slaves did become increasingly independent, and resistant to punishment, but historians agree there were no insurrections. In the invaded areas, insubordination was more the norm than loyalty to the old master; Bell Wiley says, "It was not disloyalty, but the lure of freedom." Many slaves became spies for the North, and large numbers ran away to federal lines.

Lincoln's Emancipation Proclamation, an executive order of the U.S. government on January 1, 1863, changed the legal status of 3 million slaves in designated areas of the Confederacy from "slave" to "free". The long-term effect was that the Confederacy could not preserve the institution of slavery, and lost the use of the core element of its plantation labor force. Slaves were legally freed by the Proclamation, and became free by escaping to federal lines, or by advances of federal troops. Many freed slaves served as volunteers in the federal army as teamsters, cooks, laundresses and laborers, and eventually as soldiers. Plantation owners, realizing that emancipation would destroy their economic system, sometimes moved their slaves as far as possible out of reach of the Union army. By "Juneteenth" (June 19, 1865, in Texas), the Union Army controlled all of the Confederacy and had liberated all its slaves. Their owners never received compensation.

Most whites were subsistence farmers who traded their surpluses locally. The plantations of the South, with white ownership and an enslaved labor force, produced substantial wealth from cash crops. It supplied two-thirds of the world's cotton, which was in high demand for textiles, along with tobacco, sugar, and naval stores (such as turpentine). These raw materials were exported to factories in Europe and the Northeast. Planters reinvested their profits in more slaves and fresh land, for cotton and tobacco depleted the soil. There was little manufacturing or mining; shipping was controlled by outsiders.

The plantations that enslaved over three million black people were the principal source of wealth. Most were concentrated in "black belt" plantation areas (because few white families in the poor regions owned slaves.) For decades there had been widespread fear of slave revolts. During the war extra men were assigned to "home guard" patrol duty and governors sought to keep militia units at home for protection. Historian William Barney reports, "no major slave revolts erupted during the Civil War." Nevertheless, slaves took the opportunity to enlarge their sphere of independence, and when union forces were nearby, many ran off to join them.

Slave labor was applied in industry in a limited way in the Upper South and in a few port cities. One reason for the regional lag in industrial development was top-heavy income distribution. Mass production requires mass markets, and slaves living in small cabins, using self-made tools and outfitted with one suit of work clothes each year of inferior fabric, did not generate consumer demand to sustain local manufactures of any description in the same way a mechanized family farm of free labor did in the North. The Southern economy was "pre-capitalist" in that slaves were put to work in the largest revenue-producing enterprises, not free labor market. That labor system as practiced in the American South encompassed paternalism, whether abusive or indulgent, and that meant labor management considerations apart from productivity.

Approximately 85% of both North and South white populations lived on family farms, both regions were predominantly agricultural, and mid-century industry in both was mostly domestic. But the Southern economy was pre-capitalist in its overwhelming reliance on the agriculture of cash crops to produce wealth, while the great majority of farmers fed themselves and supplied a small local market. Southern cities and industries grew faster than ever before, but the thrust of the rest of the country's exponential growth elsewhere was toward urban industrial development along transportation systems of canals and railroads. The South was following the dominant currents of the American economic mainstream, but at a "great distance" as it lagged in the all-weather modes of transportation that brought cheaper, speedier freight shipment and forged new, expanding inter-regional markets.

A third count of southern pre-capitalist economy relates to the cultural setting. The South and southerners did not adopt a work ethic, nor the habits of thrift that marked the rest of the country. It had access to the tools of capitalism, but it did not adopt its culture. The Southern Cause as a national economy in the Confederacy was grounded in "slavery and race, planters and patricians, plain folk and folk culture, cotton and plantations".

The Confederacy started its existence as an agrarian economy with exports, to a world market, of cotton, and, to a lesser extent, tobacco and sugarcane. Local food production included grains, hogs, cattle, and gardens. The cash came from exports but the Southern people spontaneously stopped exports in early 1861 to hasten the impact of "King Cotton". When the blockade was announced, commercial shipping practically ended (the ships could not get insurance), and only a trickle of supplies came via blockade runners. The cutoff of exports was an economic disaster for the South, rendering useless its most valuable properties, its plantations and their enslaved workers. Many planters kept growing cotton, which piled up everywhere, but most turned to food production. All across the region, the lack of repair and maintenance wasted away the physical assets.

The eleven states had produced $155 million in manufactured goods in 1860, chiefly from local grist-mills, and lumber, processed tobacco, cotton goods and naval stores such as turpentine. The main industrial areas were border cities such as Baltimore, Wheeling, Louisville and St. Louis, that were never under Confederate control. The government did set up munitions factories in the Deep South. Combined with captured munitions and those coming via blockade runners, the armies were kept minimally supplied with weapons. The soldiers suffered from reduced rations, lack of medicines, and the growing shortages of uniforms, shoes and boots. Shortages were much worse for civilians, and the prices of necessities steadily rose.

The Confederacy adopted a tariff or tax on imports of 15%, and imposed it on all imports from other countries, including the United States. The tariff mattered little; the Union blockade minimized commercial traffic through the Confederacy's ports, and very few people paid taxes on goods smuggled from the North. The Confederate government in its entire history collected only $3.5 million in tariff revenue. The lack of adequate financial resources led the Confederacy to finance the war through printing money, which led to high inflation. The Confederacy underwent an economic revolution by centralization and standardization, but it was too little too late as its economy was systematically strangled by blockade and raids.

In peacetime, the South's extensive and connected systems of navigable rivers and coastal access allowed for cheap and easy transportation of agricultural products. The railroad system in the South had developed as a supplement to the navigable rivers to enhance the all-weather shipment of cash crops to market. Railroads tied plantation areas to the nearest river or seaport and so made supply more dependable, lowered costs and increased profits. In the event of invasion, the vast geography of the Confederacy made logistics difficult for the Union. Wherever Union armies invaded, they assigned many of their soldiers to garrison captured areas and to protect rail lines.

At the onset of the Civil War the South had a rail network disjointed and plagued by changes in track gauge as well as lack of interchange. Locomotives and freight cars had fixed axles and could not use tracks of different gauges (widths). Railroads of different gauges leading to the same city required all freight to be off-loaded onto wagons for transport to the connecting railroad station, where it had to await freight cars and a locomotive before proceeding. Centers requiring off-loading included Vicksburg, New Orleans, Montgomery, Wilmington and Richmond. In addition, most rail lines led from coastal or river ports to inland cities, with few lateral railroads. Due to this design limitation, the relatively primitive railroads of the Confederacy were unable to overcome the Union naval blockade of the South's crucial intra-coastal and river routes.

The Confederacy had no plan to expand, protect or encourage its railroads. Southerners' refusal to export the cotton crop in 1861 left railroads bereft of their main source of income. Many lines had to lay off employees; many critical skilled technicians and engineers were permanently lost to military service. In the early years of the war the Confederate government had a hands-off approach to the railroads. Only in mid-1863 did the Confederate government initiate a national policy, and it was confined solely to aiding the war effort. Railroads came under the "de facto" control of the military. In contrast, the U.S. Congress had authorized military administration of Union-controlled railroad and telegraph systems in January 1862, imposed a standard gauge, and built railroads into the South using that gauge. Confederate armies successfully reoccupying territory could not be resupplied directly by rail as they advanced. The C.S. Congress formally authorized military administration of railroads in February 1865.

In the last year before the end of the war, the Confederate railroad system stood permanently on the verge of collapse. There was no new equipment and raids on both sides systematically destroyed key bridges, as well as locomotives and freight cars. Spare parts were cannibalized; feeder lines were torn up to get replacement rails for trunk lines, and rolling stock wore out through heavy use.

The Confederate army experienced a persistent shortage of horses and mules, and requisitioned them with dubious promissory notes given to local farmers and breeders. Union forces paid in real money and found ready sellers in the South. Both armies needed horses for cavalry and for artillery. Mules pulled the wagons. The supply was undermined by an unprecedented epidemic of glanders, a fatal disease that baffled veterinarians. After 1863 the invading Union forces had a policy of shooting all the local horses and mules they did not need – in order to keep them out of Confederate hands. The Confederate armies and farmers experienced a growing shortage of horses and mules, which hurt the Southern economy and the war effort. The South lost half of its 2.5 million horses and mules; many farmers ended the war with none left. Army horses were used up by hard work, malnourishment, disease and battle wounds; they had a life expectancy of about seven months.

Both the individual Confederate states and later the Confederate government printed Confederate States of America dollars as paper currency in various denominations, with a total face value of $1.5 billion. Much of it was signed by Treasurer Edward C. Elmore. Inflation became rampant as the paper money depreciated and eventually became worthless. The state governments and some localities printed their own paper money, adding to the runaway inflation. Many bills still exist, although in recent years counterfeit copies have proliferated.

The Confederate government initially wanted to finance its war mostly through tariffs on imports, export taxes, and voluntary donations of gold. After the spontaneous imposition of an embargo on cotton sales to Europe in 1861, these sources of revenue dried up and the Confederacy increasingly turned to issuing debt and printing money to pay for war expenses. The Confederate States politicians were worried about angering the general population with hard taxes. A tax increase might disillusion many Southerners, so the Confederacy resorted to printing more money. As a result, inflation increased and remained a problem for the southern states throughout the rest of the war. By April 1863, for example, the cost of flour in Richmond had risen to $100 a barrel and housewives were rioting.

The Confederate government took over the three national mints: the Charlotte Mint in North Carolina, the Dahlonega Mint in Georgia, and the New Orleans Mint in Louisiana. During 1861, the first two produced small amounts of gold coinage, the latter half dollars. Since the mints used the current dies on hand, these issues remain indistinguishable from those minted by the Union. In New Orleans the Confederacy used its own reverse design to strike four half dollars. US coinage was hoarded and did not have any general circulation. U.S. coinage was admitted as legal tender up to $10, as were British sovereigns, French Napoleons and Spanish and Mexican doubloons at a fixed rate of exchange. Confederate money was paper and postage stamps.

By mid-1861, the Union naval blockade virtually shut down the export of cotton and the import of manufactured goods. Food that formerly came overland was cut off.

Women had charge of making do. They cut back on purchases, brought out old spinning wheels and enlarged their gardens with flax and peas to provide clothing and food. They used ersatz substitutes when possible, but there was no real coffee and it was hard to develop a taste for the okra or chicory substitutes used. The households were severely hurt by inflation in the cost of everyday items like flour and the shortages of food, fodder for the animals, and medical supplies for the wounded.

State governments pleaded with planters to grow less cotton and more food. Most refused. When cotton prices soared in Europe, expectations were that Europe would soon intervene to break the blockade and make them rich. The myth of omnipotent "King Cotton" died hard. The Georgia legislature imposed cotton quotas, making it a crime to grow an excess. But food shortages only worsened, especially in the towns.

The overall decline in food supplies, made worse by the inadequate transportation system, led to serious shortages and high prices in urban areas. When bacon reached a dollar a pound in 1863, the poor women of Richmond, Atlanta and many other cities began to riot; they broke into shops and warehouses to seize food. The women expressed their anger at ineffective state relief efforts, speculators, and merchants. As wives and widows of soldiers they were hurt by the inadequate welfare system.

By the end of the war deterioration of the Southern infrastructure was widespread. The number of civilian deaths is unknown. Every Confederate state was affected, but most of the war was fought in Virginia and Tennessee, while Texas and Florida saw the least military action. Much of the damage was caused by direct military action, but most was caused by lack of repairs and upkeep, and by deliberately using up resources. Historians have recently estimated how much of the devastation was caused by military action. Paul Paskoff calculates that Union military operations were conducted in 56% of 645 counties in nine Confederate states (excluding Texas and Florida). These counties contained 63% of the 1860 white population and 64% of the slaves. By the time the fighting took place, undoubtedly some people had fled to safer areas, so the exact population exposed to war is unknown.

The eleven Confederate States in the 1860 United States Census had 297 towns and cities with 835,000 people; of these 162 with 681,000 people were at one point occupied by Union forces. Eleven were destroyed or severely damaged by war action, including Atlanta (with an 1860 population of 9,600), Charleston, Columbia, and Richmond (with prewar populations of 40,500, 8,100, and 37,900, respectively); the eleven contained 115,900 people in the 1860 census, or 14% of the urban South. Historians have not estimated what their actual population was when Union forces arrived. The number of people (as of 1860) who lived in the destroyed towns represented just over 1% of the Confederacy's 1860 population. In addition, 45 court houses were burned (out of 830). The South's agriculture was not highly mechanized. The value of farm implements and machinery in the 1860 Census was $81 million; by 1870, there was 40% less, worth just $48 million. Many old tools had broken through heavy use; new tools were rarely available; even repairs were difficult.

The economic losses affected everyone. Banks and insurance companies were mostly bankrupt. Confederate currency and bonds were worthless. The billions of dollars invested in slaves vanished. Most debts were also left behind. Most farms were intact but most had lost their horses, mules and cattle; fences and barns were in disrepair. Paskoff shows the loss of farm infrastructure was about the same whether or not fighting took place nearby. The loss of infrastructure and productive capacity meant that rural widows throughout the region faced not only the absence of able-bodied men, but a depleted stock of material resources that they could manage and operate themselves. During four years of warfare, disruption, and blockades, the South used up about half its capital stock. The North, by contrast, absorbed its material losses so effortlessly that it appeared richer at the end of the war than at the beginning.

The rebuilding took years and was hindered by the low price of cotton after the war. Outside investment was essential, especially in railroads. One historian has summarized the collapse of the transportation infrastructure needed for economic recovery:
One of the greatest calamities which confronted Southerners was the havoc wrought on the transportation system. Roads were impassable or nonexistent, and bridges were destroyed or washed away. The important river traffic was at a standstill: levees were broken, channels were blocked, the few steamboats which had not been captured or destroyed were in a state of disrepair, wharves had decayed or were missing, and trained personnel were dead or dispersed. Horses, mules, oxen, carriages, wagons, and carts had nearly all fallen prey at one time or another to the contending armies. The railroads were paralyzed, with most of the companies bankrupt. These lines had been the special target of the enemy. On one stretch of 114 miles in Alabama, every bridge and trestle was destroyed, cross-ties rotten, buildings burned, water-tanks gone, ditches filled up, and tracks grown up in weeds and bushes ... Communication centers like Columbia and Atlanta were in ruins; shops and foundries were wrecked or in disrepair. Even those areas bypassed by battle had been pirated for equipment needed on the battlefront, and the wear and tear of wartime usage without adequate repairs or replacements reduced all to a state of disintegration.

 
About 250,000 men never came home, some 30 percent of all white men aged 18 to 40, in 1860. Widows who were overwhelmed often abandoned the farm and merged into the households of relatives, or even became refugees living in camps with high rates of disease and death. In the Old South, being an "old maid" was something of an embarrassment to the woman and her family. Now it became almost a norm. Some women welcomed the freedom of not having to marry. Divorce, while never fully accepted, became more common. The concept of the "New Woman" emerged – she was self-sufficient and independent, and stood in sharp contrast to the "Southern Belle" of antebellum lore.

The first official flag of the Confederate States of America – called the "Stars and Bars" – originally had seven stars, representing the first seven states that initially formed the Confederacy. As more states joined, more stars were added, until the total was 13 (two stars were added for the divided states of Kentucky and Missouri). During the First Battle of Bull Run, (First Manassas) it sometimes proved difficult to distinguish the Stars and Bars from the Union flag. To rectify the situation, a separate "Battle Flag" was designed for use by troops in the field. Also known as the "Southern Cross", many variations sprang from the original square configuration. Although it was never officially adopted by the Confederate government, the popularity of the Southern Cross among both soldiers and the civilian population was a primary reason why it was made the main color feature when a new national flag was adopted in 1863. This new standard – known as the "Stainless Banner" – consisted of a lengthened white field area with a Battle Flag canton. This flag too had its problems when used in military operations as, on a windless day, it could easily be mistaken for a flag of truce or surrender. Thus, in 1865, a modified version of the Stainless Banner was adopted. This final national flag of the Confederacy kept the Battle Flag canton, but shortened the white field and added a vertical red bar to the fly end.

Because of its depiction in the 20th-century and popular media, many people consider the rectangular battle flag with the dark blue bars as being synonymous with "the Confederate Flag", but this flag was never adopted as a Confederate national flag. The "Confederate Flag" has a color scheme similar to the official Battle Flag, but is rectangular, not square. (Its design and shape matches the Naval Jack, but the blue bars are darker.) The "Confederate Flag" is a highly recognizable symbol of the South in the United States today, and continues to be a controversial icon.

The Confederate States of America claimed a total of of coastline, thus a large part of its territory lay on the seacoast with level and often sandy or marshy ground. Most of the interior portion consisted of arable farmland, though much was also hilly and mountainous, and the far western territories were deserts. The lower reaches of the Mississippi River bisected the country, with the western half often referred to as the Trans-Mississippi. The highest point (excluding Arizona and New Mexico) was Guadalupe Peak in Texas at .

Climate

Much of the area claimed by the Confederate States of America had a humid subtropical climate with mild winters and long, hot, humid summers. The climate and terrain varied from vast swamps (such as those in Florida and Louisiana) to semi-arid steppes and arid deserts west of longitude 100 degrees west. The subtropical climate made winters mild but allowed infectious diseases to flourish. Consequently, on both sides more soldiers died from disease than were killed in combat, a fact hardly atypical of pre–World War I conflicts.

The United States Census of 1860 gives a picture of the overall 1860 population of the areas that joined the Confederacy. Note that population-numbers exclude non-assimilated Indian tribes.

In 1860 the areas that later formed the eleven Confederate States (and including the future West Virginia) had 132,760 (1.46%) free blacks. Males made up 49.2% of the total population and females 50.8% (whites: 48.60% male, 51.40% female; slaves: 50.15% male, 49.85% female; free blacks: 47.43% male, 52.57% female).

The CSA was overwhelmingly rural land. Few towns had populations of more than 1,000 – the typical county seat had a population of fewer than 500 people. Cities were rare. Of the twenty largest U.S. cities in the 1860 census, only New Orleans lay in Confederate territory – and the Union captured New Orleans in 1862. Only 13 Confederate-controlled cities ranked among the top 100 U.S. cities in 1860, most of them ports whose economic activities vanished or suffered severely in the Union blockade. The population of Richmond swelled after it became the Confederate capital, reaching an estimated 128,000 in 1864. Other Southern cities in the Border slave-holding states such as Baltimore, Washington, D.C., Wheeling (W.Va., formerly Va.), Alexandria, Louisville, and St. Louis never came under the control of the Confederate government.

The cities of the Confederacy included most prominently in order of size of population:

"(See also Atlanta in the Civil War, Charleston, South Carolina, in the Civil War, Nashville in the Civil War, New Orleans in the Civil War, Wilmington, North Carolina, in the American Civil War, and Richmond in the Civil War)."

The CSA was overwhelmingly Protestant. Both free and enslaved populations identified with evangelical Protestantism. Freedom of religion and separation of church and state were fully ensured by Confederate laws. Church attendance was very high and chaplains played a major role in the Army.

Most large denominations experienced a North—South split in the prewar era on the issue of slavery. The creation of a new country necessitated independent structures. For example, the Presbyterian Church in the United States split, with much of the new leadership provided by Joseph Ruggles Wilson (father of President Woodrow Wilson). In 1861, he organized the meeting that formed General Assembly of the Southern Presbyterian Church and served as its chief executive for thirty-seven years. Baptists and Methodists together formed majorities of both the white and the slave population (see "Black church"). Both broke off from their Northern coreligionists over the slavery issue. Elites in the southeast favored the Protestant Episcopal Church in the Confederate States of America, which reluctantly split off the Episcopal Church (USA) in 1861. Other elites were Presbyterians belonging to the 1861 founded Presbyterian Church in the United States. Catholics included an Irish working class element in coastal cities and an old French element in southern Louisiana. Other insignificant and scattered religious populations included Lutherans, the Holiness movement, other Reformed, other Christian fundamentalists, the Stone-Campbell Restoration Movement, the Churches of Christ, the Latter-day Saints movement, Adventists, Muslims, Jews, Native American animists, deists and irreligious people.

The southern churches met the shortage of Army chaplains by sending missionaries. The Southern Baptists started in 1862 and had a total of 78 missionaries. Presbyterians were even more active with 112 missionaries and early 1865. Other missionaries were funded and supported by the Episcopalians, Methodists, and Lutherans. One result was wave after wave of revivals in the Army.

Military leaders of the Confederacy (with their state or country of birth and highest rank) included:


by John B. Boles and Evelyn Thomas Nolen









</doc>
<doc id="7025" url="https://en.wikipedia.org/wiki?curid=7025" title="Cranberry">
Cranberry

Cranberries are a group of evergreen dwarf shrubs or trailing vines in the subgenus Oxycoccus of the genus "Vaccinium". In Britain, cranberry may refer to the native species "Vaccinium oxycoccos", while in North America, cranberry may refer to "Vaccinium macrocarpon". "Vaccinium oxycoccos" is cultivated in central and northern Europe, while "Vaccinium macrocarpon" is cultivated throughout the northern United States, Canada and Chile. In some methods of classification, "Oxycoccus" is regarded as a genus in its own right. They can be found in acidic bogs throughout the cooler regions of the Northern Hemisphere.

Cranberries are low, creeping shrubs or vines up to long and in height; they have slender, wiry stems that are not thickly woody and have small evergreen leaves. The flowers are dark pink, with very distinct "reflexed" petals, leaving the style and stamens fully exposed and pointing forward. They are pollinated by bees. The fruit is a berry that is larger than the leaves of the plant; it is initially light green, turning red when ripe. It is edible, but with an acidic taste that usually overwhelms its sweetness.

Cranberries are a major commercial crop in certain American states and Canadian provinces (see cultivation and uses below). Most cranberries are processed into products such as juice, sauce, jam, and sweetened dried cranberries, with the remainder sold fresh to consumers. Cranberry sauce is a traditional accompaniment to turkey at Christmas dinner in the United Kingdom, and at Christmas and Thanksgiving dinners in the United States and Canada.

There are three to four species of cranberry, classified into two sections:

Cranberries are related to bilberries, blueberries, and huckleberries, all in "Vaccinium" subgenus "Vaccinium". These differ in having bell-shaped flowers, the petals not being reflexed, and woodier stems, forming taller shrubs.
Some plants of the completely unrelated genus "Viburnum" are sometimes called "highbush cranberries" (e.g. "Viburnum trilobum").

Cranberries are susceptible to false blossom, a harmful but controllable phytoplasma disease common in the eastern production areas of Massachusetts and New Jersey.

The name, "cranberry", derives from the German, "kraanbere" (English translation, "craneberry"), first named as "cranberry" in English by the missionary John Eliot in 1647. Around 1694, German and Dutch colonists in New England used the word, cranberry, to represent the expanding flower, stem, calyx, and petals resembling the neck, head, and bill of a crane. The traditional English name for the plant more common in Europe, "Vaccinium oxycoccos", , originated from plants with small red berries found growing in fen (marsh) lands of England.

In North America, the Narragansett people of the Algonquian nation in the regions of New England appeared to be using cranberries in pemmican for food and for dye. Calling the red berries, "sasemineash", the Narragansett people may have introduced cranberries to colonists in Massachusetts. American Revolutionary War veteran Henry Hall first cultivated cranberries in the Cape Cod town of Dennis around 1816. In the 1820s, Hall was shipping cranberries to New York City and Boston from which shipments were also sent to Europe. By 1900, were under cultivation in the New England region. In 2014, the total area of cranberries harvested in the United States was , with Massachusetts as the second largest producer after Wisconsin.

Cranberries are a major commercial crop in the U.S. states of Massachusetts, New Jersey, Oregon, Washington, and Wisconsin, as well as in the Canadian provinces of British Columbia, New Brunswick, Ontario, Nova Scotia, Prince Edward Island, Newfoundland and Quebec. British Columbia's Fraser River Valley region produces 17 million kg of cranberries annually from 1,150 hectares, about 95% of total Canadian production. In the United States, Wisconsin is the leading producer of cranberries, with over half of U.S. production. Massachusetts is the second largest U.S. producer. Small volume production occurs in southern Argentina, Chile and the Netherlands.

Historically, cranberry beds were constructed in wetlands. Today's cranberry beds are constructed in upland areas with a shallow water table. The topsoil is scraped off to form dykes around the bed perimeter. Clean sand is hauled in and spread to a depth of four to eight inches. The surface is laser leveled flat to provide even drainage. Beds are frequently drained with socked tile in addition to the perimeter ditch. In addition to making it possible to hold water, the dykes allow equipment to service the beds without driving on the vines. Irrigation equipment is installed in the bed to provide irrigation for vine growth and for spring and autumn frost protection.

A common misconception about cranberry production is that the beds remain flooded throughout the year. During the growing season cranberry beds are not flooded, but are irrigated regularly to maintain soil moisture. Beds are flooded in the autumn to facilitate harvest and again during the winter to protect against low temperatures. In cold climates like Wisconsin, New England, and eastern Canada, the winter flood typically freezes into ice, while in warmer climates the water remains liquid. When ice forms on the beds, trucks can be driven onto the ice to spread a thin layer of sand that helps to control pests and rejuvenate the vines. Sanding is done every three to five years.

Cranberry vines are propagated by moving vines from an established bed. The vines are spread on the surface of the sand of the new bed and pushed into the sand with a blunt disk. The vines are watered frequently during the first few weeks until roots form and new shoots grow. Beds are given frequent, light application of nitrogen fertilizer during the first year. The cost of establishment for new cranberry beds is estimated to be about US$70,000 per hectare (approx. $28,300 per acre).

Cranberries are harvested in the fall when the fruit takes on its distinctive deep red color. Berries that receive sun turn a deep red when fully ripe, while those that do not fully mature are a pale pink or white color. This is usually in September through the first part of November. To harvest cranberries, the beds are flooded with six to eight inches (15 to 20 centimeters) of water above the vines. A harvester is driven through the beds to remove the fruit from the vines. For the past 50 years, water reel type harvesters have been used. Harvested cranberries float in the water and can be corralled into a corner of the bed and conveyed or pumped from the bed. From the farm, cranberries are taken to receiving stations where they are cleaned, sorted, and stored prior to packaging or processing.

Although most cranberries are wet-picked as described above, 5–10% of the US crop is still dry-picked. This entails higher labor costs and lower yield, but dry-picked berries are less bruised and can be sold as fresh fruit instead of having to be immediately frozen or processed. Originally performed with two-handed comb scoops, dry picking is today accomplished by motorized, walk-behind harvesters which must be small enough to traverse beds without damaging the vines.

White cranberry juice is made from regular cranberries that have been harvested after the fruits are mature, but before they have attained their characteristic dark red color. Yields are lower on beds harvested early and the early flooding tends to damage vines, but not severely.

Cranberries for fresh market are stored in shallow bins or boxes with perforated or slatted bottoms, which deter decay by allowing air to circulate. Because harvest occurs in late autumn, cranberries for fresh market are frequently stored in thick walled barns without mechanical refrigeration. Temperatures are regulated by opening and closing vents in the barn as needed. Cranberries destined for processing are usually frozen in bulk containers shortly after arriving at a receiving station.

Raw cranberries have moderate levels of vitamin C, dietary fiber and the essential dietary mineral, manganese (each nutrient having more than 10% of the Daily Value per 100 g serving, as well as other essential micronutrients in minor amounts.

As fresh cranberries are hard, sour, and bitter, about 95% of cranberries are processed and used to make cranberry juice and sauce. They are also sold dried and sweetened.

Cranberry juice is usually sweetened or blended with other fruit juices to reduce its natural tartness. Many cocktails, including the Cosmopolitan, are made with cranberry juice. At one teaspoon of sugar per ounce, cranberry juice cocktail is more highly sweetened than even soda drinks that have been linked to obesity.

Usually cranberries as fruit are cooked into a compote or jelly, known as cranberry sauce. Such preparations are traditionally served with roast turkey, as a staple of English Christmas dinners, and Thanksgiving (both in Canada and in the United States). The berry is also used in baking (muffins, scones, cakes and breads). In baking it is often combined with orange or orange zest. Less commonly, cranberries are used to add tartness to savory dishes such as soups and stews.

Fresh cranberries can be frozen at home, and will keep up to nine months; they can be used directly in recipes without thawing.

A comprehensive review in 2012 of available research concluded there is no evidence that cranberry juice or cranberry extract as tablets or capsules are effective in preventing urinary tract infections (UTIs). The European Food Safety Authority reviewed the evidence for one brand of cranberry extract and concluded a cause and effect relationship has not been established between the consumption of the product and reducing the risk of UTIs.

One systematic review in 2017 showed that cranberry products significantly reduced the incidence of UTIs, indicating that cranberry products may be effective particularly for individuals with recurrent infections. When the quality of meta-analyses on the efficacy of cranberry products for preventing or treating UTIs is examined, large variation is seen, resulting from inconsistencies of clinical factors and study methods. Additional studies with better designs are warranted, particularly for women with recurrent infections and those who have developed antimicrobial resistance.

Raw cranberries, cranberry juice and cranberry extracts are a source of polyphenols - including proanthocyanidins, flavonols and quercetin. These compounds are being studied in vivo and in vitro for possible effects on the cardiovascular system, immune system and cancer. However, there is no confirmation from human studies that consuming cranberry polyphenols provides anti-cancer, immune, or cardiovascular benefits. Potential is limited by poor absorption and rapid excretion.

Cranberry juice contains a high molecular weight non-dializable material that is under research for its potential to affect formation of plaque by "Streptococcus mutans" pathogens that cause tooth decay. Cranberry juice components are also being studied for possible effects on kidney stone formation.

Problems may arise with the lack of validation for quantifying of A-type proanthocyanidins (PAC) extracted from cranberries. For instance, PAC extract quality and content can be performed using different methods including the European Pharmacopoeia method, liquid chromatography–mass spectrometry, or a modified 4-dimethylaminocinnamaldehyde colorimetric method. Variations in extract analysis can lead to difficulties in assessing the quality of PAC extracts from different cranberry starting material, such as by regional origin, ripeness at time of harvest and post-harvest processing. Assessments show that quality varies greatly from one commercial PAC extract product to another.

The anticoagulant effects of warfarin may be increased by consuming cranberry juice, resulting in adverse effects such as increased incidence of bleeding and bruising. Other safety concerns from consuming large quantities of cranberry juice or using cranberry supplements include potential for nausea, increasing stomach inflammation, sugar intake or kidney stone formation.

In 1550, James White Norwood made reference to Native Americans using cranberries. In James Rosier's book "The Land of Virginia" there is an account of Europeans coming ashore and being met with Native Americans bearing bark cups full of cranberries. In Plymouth, Massachusetts, there is a 1633 account of the husband of Mary Ring auctioning her cranberry-dyed petticoat for 16 shillings. In 1643, Roger Williams's book "A Key Into the Language of America" described cranberries, referring to them as "bearberries" because bears ate them. In 1648, preacher John Elliott was quoted in Thomas Shepard's book "Clear Sunshine of the Gospel" with an account of the difficulties the Pilgrims were having in using the Indians to harvest cranberries as they preferred to hunt and fish. In 1663, the Pilgrim cookbook appears with a recipe for cranberry sauce. In 1667, New Englanders sent to King Charles ten barrels of cranberries, three barrels of codfish and some Indian corn as a means of appeasement for his anger over their local coining of the Pine Tree shilling. In 1669, Captain Richard Cobb had a banquet in his house (to celebrate both his marriage to Mary Gorham and his election to the Convention of Assistance), serving wild turkey with sauce made from wild cranberries. In the 1672 book "New England Rarities Discovered" author John Josselyn described cranberries, writing:

"Sauce for the Pilgrims, cranberry or bearberry, is a small trayling ["sic"] plant that grows in salt marshes that are overgrown with moss. The berries are of a pale yellow color, afterwards red, as big as a cherry, some perfectly round, others oval, all of them hollow with sower ["sic"] astringent taste; they are ripe in August and September. They are excellent against the Scurvy. They are also good to allay the fervor of hoof diseases. The Indians and English use them mush, boyling ["sic"] them with sugar for sauce to eat with their meat; and it is a delicate sauce, especially with roasted mutton. Some make tarts with them as with gooseberries."

"The Compleat Cook's Guide", published in 1683, made reference to cranberry juice. In 1703, cranberries were served at the Harvard University commencement dinner. In 1787, James Madison wrote Thomas Jefferson in France for background information on constitutional government to use at the Constitutional Convention. Jefferson sent back a number of books on the subject and in return asked for a gift of apples, pecans and cranberries. William Aiton, a Scottish botanist, included an entry for the cranberry in volume II of his 1789 work "Hortus Kewensis". He notes that "Vaccinium macrocarpon" (American cranberry) was cultivated by James Gordon in 1760. In 1796, cranberries were served at the first celebration of the landing of the Pilgrims, and Amelia Simmons (an American orphan) wrote a book entitled "American Cookery" which contained a recipe for cranberry tarts. In 1816, Henry Hall first commercially grew cranberries in East Dennis, Massachusetts on Cape Cod. In 1843, Eli Howes planted his own crop of cranberries on Cape Cod, using the "Howes" variety. In 1847, Cyrus Cahoon planted a crop of "Early Black" variety near Pleasant Lake, Harwich, Massachusetts. In 1860, Edward Watson, a friend of Henry David Thoreau, wrote a poem called "The Cranberry Tart."

Cranberry sales in the United States have traditionally been associated with holidays of Thanksgiving and Christmas.

In the U.S., large-scale cranberry cultivation has been developed as opposed to other countries. American cranberry growers have a long history of cooperative marketing. As early as 1904, John Gaynor, a Wisconsin grower, and A.U. Chaney, a fruit broker from Des Moines, Iowa, organized Wisconsin growers into a cooperative called the Wisconsin Cranberry Sales Company to receive a uniform price from buyers. Growers in New Jersey and Massachusetts were also organized into cooperatives, creating the National Fruit Exchange that marketed fruit under the Eatmor brand. The success of cooperative marketing almost led to its failure. With consistent and high prices, area and production doubled between 1903 and 1917 and prices fell. In 1918, US$54,000 was spent on advertising, leading to US$1 million in increased sales.

With surplus cranberries and changing American households some enterprising growers began canning cranberries that were below-grade for fresh market. Competition between canners was fierce because profits were thin. The Ocean Spray cooperative was established in 1930 through a merger of three primary processing companies: Ocean Spray Preserving company, Makepeace Preserving Co, and Cranberry Products Co. The new company was called Cranberry Canners, Inc. and used the Ocean Spray label on their products. Since the new company represented over 90% of the market, it would have been illegal (cf. antitrust) had attorney John Quarles not found an exemption for agricultural cooperatives. Morris April Brothers were the producers of Eatmor brand cranberry sauce, in Tuckahoe, New Jersey; Morris April Brothers brought an action against Ocean Spray for violation of the Sherman Antitrust Act and won $200,000 in real damages plus triple damages, in 1958, just in time for the Great Cranberry Scare of 1959. , about 65% of the North American industry belongs to the Ocean Spray cooperative. (The percentage may be slightly higher in Canada than in the U.S.)

A turning point for the industry occurred on 9 November 1959, when the secretary of the United States Department of Health, Education, and Welfare Arthur S. Flemming announced that some of the 1959 crop was tainted with traces of the herbicide aminotriazole. The market for cranberries collapsed and growers lost millions of dollars. However, the scare taught the industry that they could not be completely dependent on the holiday market for their products: they had to find year-round markets for their fruit. They also had to be exceedingly careful about their use of pesticides.

After the aminotriazole scare, Ocean Spray reorganized and spent substantial sums on product development. New products such as cranberry/apple juice blends were introduced, followed by other juice blends.

A Federal Marketing Order that is authorized to synchronize supply and demand was approved in 1962. The order has been renewed and modified slightly in subsequent years, but it has allowed for more stable marketing. The market order has been invoked during six crop years: 1962 (12%), 1963 (5%), 1970 (10%), 1971 (12%), 2000 (15%), and 2001 (35%). Even though supply still exceeds demand, there is little will to invoke the Federal Marketing Order out of the realization that any pullback in supply by U.S. growers would easily be filled by Canadian production.

Prices and production increased steadily during the 1980s and 1990s. Prices peaked at about $65.00 per barrel (29 ¢/kg—a cranberry barrel equals 100 pounds or 45.4 kg.) in 1996 then fell to $18.00 per barrel (8.2 ¢/kg) in 2001. The cause for the precipitous drop was classic oversupply. Production had outpaced consumption leading to substantial inventory in freezers or as concentrate.

Cranberry handlers (processors) include Ocean Spray, Cliffstar Corporation, Northland Cranberries Inc.[Sun Northland LLC], Clement Pappas & Co., and Decas Cranberry Products as well as a number of small handlers and processors.

The Cranberry Marketing Committee is an organization that represents United States cranberry growers in four marketing order districts. The committee was established in 1962 as a Federal Marketing Order to ensure a stable, orderly supply of good quality product. The Cranberry Marketing Committee, based in Wareham, Massachusetts, represents more than 1,100 cranberry growers and 60 cranberry handlers across Massachusetts, Rhode Island, Connecticut, New Jersey, Wisconsin, Michigan, Minnesota, Oregon, Washington and Long Island in the state of New York. 
The authority for the actions taken by the Cranberry Marketing Committee is provided in Chapter IX, Title 7, Code of Federal Regulations which is called the Federal Cranberry Marketing Order. The Order is part of the Agricultural Marketing Agreement Act of 1937, identifying cranberries as a commodity good that can be regulated by Congress. The Federal Cranberry Marketing Order has been altered over the years to expand the Cranberry Marketing Committee's ability to develop projects in the United States and around the world. 
The Cranberry Marketing Committee currently runs promotional programs in the United States, China, India, Mexico, Pan-Europe, and South Korea.

As of 2016, the European Union was the largest importer of American cranberries, followed individually by Canada, China, Mexico, and South Korea. From 2013 to 2017, US cranberry exports to China grew exponentially, making China the second largest country importer, reaching $36 million in cranberry products.




</doc>
<doc id="7030" url="https://en.wikipedia.org/wiki?curid=7030" title="Code coverage">
Code coverage

In computer science, test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs. A program with high test coverage, measured as a percentage, has had more of its source code executed during testing, which suggests it has a lower chance of containing undetected software bugs compared to a program with low test coverage. Many different metrics can be used to calculate test coverage; some of the most basic are the percentage of program subroutines and the percentage of program statements called during execution of the test suite.

Test coverage was among the first methods invented for systematic software testing. The first published reference was by Miller and Maloney in "Communications of the ACM" in 1963.

To measure what percentage of code has been exercised by a test suite, one or more "coverage criteria" are used. Coverage criteria are usually defined as rules or requirements, which a test suite needs to satisfy.

There are a number of coverage criteria, the main ones being:

For example, consider the following C function:
Assume this function is a part of some bigger program and this program was run with some test suite. 

Condition coverage does not necessarily imply branch coverage. For example, consider the following fragment of code:

Condition coverage can be satisfied by two tests:
However, this set of tests does not satisfy branch coverage since neither case will meet the codice_5 condition.

Fault injection may be necessary to ensure that all conditions and branches of exception handling code have adequate coverage during testing.

A combination of function coverage and branch coverage is sometimes also called
decision coverage. This criterion requires that every point of entry and exit in the program has been invoked at least once, and every decision in the program has taken on all possible outcomes at least once. In this context the decision is a boolean expression composed of conditions and zero or more boolean operators. This definition is not the same as branch coverage, however, some do use the term "decision coverage" as a synonym for "branch coverage".

Condition/decision coverage requires that both decision and condition coverage be satisfied. However, for safety-critical applications (e.g., for avionics software) it is often required that modified condition/decision coverage (MC/DC) be satisfied. This criterion extends condition/decision criteria with requirements that each condition should affect the decision outcome independently. For example, consider the following code:

The condition/decision criteria will be satisfied by the following set of tests:
However, the above tests set will not satisfy modified condition/decision coverage, since in the first test, the value of 'b' and in the second test the value of 'c' would not influence the output. So, the following test set is needed to satisfy MC/DC:

This criterion requires that all combinations of conditions inside each decision are tested. For example, the code fragment from the previous section will require eight tests:

Parameter value coverage (PVC) requires that in a method taking parameters, all the common values for such parameters be considered. 
The idea is that all common possible values for a parameter are tested. For example, common values for a string are: 1) null, 2) empty, 3) whitespace (space, tabs, newline), 4) valid string, 5) invalid string, 6) single-byte string, 7) double-byte string. It may also be appropriate to use very long strings. Failure to test each possible parameter value may leave a bug. Testing only one of these could result in 100% code coverage as each line is covered, but as only one of seven options are tested, there is only 14.2% PVC.

There are further coverage criteria, which are used less often:

Safety-critical or dependable applications are often required to demonstrate 100% of some form of test coverage.
For example, the ECSS-E-ST-40C standard demands 100% statement and decision coverage for two out of four different criticality levels; for the other ones, target coverage values are up to negotiation between supplier and customer.
However, setting specific target values - and, in particular, 100% - has been criticized by practitioners for various reasons (cf.)
Martin Fowler writes: "I would be suspicious of anything like 100% - it would smell of someone writing tests to make the coverage numbers happy, but not thinking about what they are doing".

Some of the coverage criteria above are connected. For instance, path coverage implies decision, statement and entry/exit coverage. Decision coverage implies statement coverage, because every statement is part of a branch.

Full path coverage, of the type described above, is usually impractical or impossible. Any module with a succession of formula_1 decisions in it can have up to formula_2 paths within it; loop constructs can result in an infinite number of paths. Many paths may also be infeasible, in that there is no input to the program under test that can cause that particular path to be executed. However, a general-purpose algorithm for identifying infeasible paths has been proven to be impossible (such an algorithm could be used to solve the halting problem). Basis path testing is for instance a method of achieving complete branch coverage without achieving complete path coverage.

Methods for practical path coverage testing instead attempt to identify classes of code paths that differ only in the number of loop executions, and to achieve "basis path" coverage the tester must cover all the path classes.

The target software is built with special options or libraries and run under a controlled environment, to map every executed function to the function points in the source code. This allows testing parts of the target software that are rarely or never accessed under normal conditions, and helps reassure that the most important conditions (function points) have been tested. The resulting output is then analyzed to see what areas of code have not been exercised and the tests are updated to include these areas as necessary. Combined with other test coverage methods, the aim is to develop a rigorous, yet manageable, set of regression tests.

In implementing test coverage policies within a software development environment, one must consider the following:


Software authors can look at test coverage results to devise additional tests and input or configuration sets to increase the coverage over vital functions. Two common forms of test coverage are statement (or line) coverage and branch (or edge) coverage. Line coverage reports on the execution footprint of testing in terms of which lines of code were executed to complete the test. Edge coverage reports which branches or code decision points were executed to complete the test. They both report a coverage metric, measured as a percentage. The meaning of this depends on what form(s) of coverage have been used, as 67% branch coverage is more comprehensive than 67% statement coverage.

Generally, test coverage tools incur computation and logging in addition to the actual program thereby slowing down the application, so typically this analysis is not done in production. As one might expect, there are classes of software that cannot be feasibly subjected to these coverage tests, though a degree of coverage mapping can be approximated through analysis rather than direct testing.

There are also some sorts of defects which are affected by such tools. In particular, some race conditions or similar real time sensitive operations can be masked when run under test environments; and conversely, and reliably, some of these defects may become easier to find as a result of the additional overhead of the testing code.

Test coverage is one consideration in the safety certification of avionics equipment. The guidelines by which avionics gear is certified by the Federal Aviation Administration (FAA) is documented in DO-178B and the recently released DO-178C.

Test coverage is also a requirement in part 6 of the automotive safety standard ISO 26262 "Road Vehicles - Functional Safety".



</doc>
<doc id="7033" url="https://en.wikipedia.org/wiki?curid=7033" title="Caitlin Clarke">
Caitlin Clarke

Caitlin Clarke (May 3, 1952 – September 9, 2004) was an American theater and film actress best known for her role as Valerian in the 1981 fantasy film "Dragonslayer" and for her role as Charlotte Cardoza in the 1998–1999 Broadway musical "Titanic".

Clarke was born Katherine Anne Clarke in Pittsburgh, the oldest of five sisters, the youngest of whom is Victoria Clarke. Her family moved to Sewickley when she was ten.

Clarke received her B.A. in theater arts from Mount Holyoke College in 1974 and her M.F.A. from the Yale School of Drama in 1978. During her final year at Yale Clarke performed with the Yale Repertory Theater in such plays as Tales from the Vienna Woods.

The first few years of Clarke's professional career were largely theatrical, apart from her role in "Dragonslayer". After appearing in three Broadway plays in 1985, Clarke moved to Los Angeles for several years as a film and television actress. She appeared in the 1986 film "Crocodile Dundee" as Simone. She returned to theater in the early 1990s, and to Broadway as Charlotte Cardoza in "Titanic".

Clarke was diagnosed with ovarian cancer in 2000. She returned to Pittsburgh to teach theater at the University of Pittsburgh and at the Pittsburgh Musical Theater's Rauh Conservatory as well as to perform in Pittsburgh theatre until her death on September 9, 2004.





Series: "Northern Exposure", "The Equalizer", "Once A Hero", "Moonlighting", "Sex And The City", "Law & Order" ("Menace", "Juvenile", "Stiff").

Movies: "Mayflower Madam" (1986), "Love, Lies and Murder" (1991), "The Stepford Husbands" (1996).
The episode, " The Witness", Matlock 1990



</doc>
<doc id="7034" url="https://en.wikipedia.org/wiki?curid=7034" title="Cruiser">
Cruiser

A cruiser is a type of warship. The term has been in use for several hundred years, and has had different meanings throughout this period. During the Age of Sail, the term "cruising" referred to certain kinds of missions – independent scouting, commerce protection, or raiding – fulfilled by a frigate or sloop, which were the "cruising warships" of a fleet. Modern cruisers are generally the largest ships in a fleet after aircraft carriers, and can usually perform several roles.

In the middle of the 19th century, "cruiser" came to be a classification for the ships intended for cruising distant waters, commerce raiding, and scouting for the battle fleet. Cruisers came in a wide variety of sizes, from the medium-sized protected cruiser to large armored cruisers that were nearly as big (although not as powerful or as well-armored) as a pre-dreadnought battleship. With the advent of the dreadnought battleship before World War I, the armored cruiser evolved into a vessel of similar scale known as the battlecruiser. The very large battlecruisers of the World War I era that succeeded armored cruisers were now classified, along with dreadnought battleships, as capital ships.

By the early 20th century after World War I, the direct successors to protected cruisers could be placed on a consistent scale of warship size, smaller than a battleship but larger than a destroyer. In 1922, the Washington Naval Treaty placed a formal limit on these cruisers, which were defined as warships of up to 10,000 tons displacement carrying guns no larger than 8 inches in calibre; heavy cruisers had 8-inch guns, while those with guns of 6.1 inches or less were light cruisers, which shaped cruiser design until the end of World War II. Some variations on the Treaty cruiser design included the German "pocket battleships" which had heavier armament at the expense of speed compared to standard heavy cruisers, and the American , which was a scaled-up heavy cruiser design designated as a "cruiser-killer".

In the later 20th century, the obsolescence of the battleship left the cruiser as the largest and most powerful surface combatant after the aircraft carrier. The role of the cruiser varied according to ship and navy, often including air defense and shore bombardment. During the Cold War, the Soviet Navy's cruisers had heavy anti-ship missile armament designed to sink NATO carrier task forces via saturation attack. The U.S. Navy built guided-missile cruisers upon destroyer-style hulls (some called "destroyer leaders" or "frigates" prior to the 1975 reclassification) primarily designed to provide air defense while often adding anti-submarine capabilities, being larger and having longer-range surface-to-air missiles (SAMs) than early "Charles F. Adams" guided-missile destroyers tasked with the short-range air defense role. By the end of the Cold War, the line between cruisers and destroyers had blurred, with the cruiser using the hull of the destroyer but receiving the cruiser designation due to their enhanced mission and combat systems. Indeed, the newest U.S. Navy destroyers (for instance the and ) are more heavily armed than some of the cruisers that they succeeded.

Currently only two nations operate cruisers: the United States and Russia. ( was still in service with the Peruvian Navy until 2017, and was the last gun cruiser in service in any navy.)

The term "cruiser" or "cruizer" was first commonly used in the 17th century to refer to an independent warship. "Cruiser" meant the purpose or mission of a ship, rather than a category of vessel. However, the term was nonetheless used to mean a smaller, faster warship suitable for such a role. In the 17th century, the ship of the line was generally too large, inflexible, and expensive to be dispatched on long-range missions (for instance, to the Americas), and too strategically important to be put at risk of fouling and foundering by continual patrol duties.

The Dutch navy was noted for its cruisers in the 17th century, while the Royal Navy—and later French and Spanish navies—subsequently caught up in terms of their numbers and deployment. The British Cruiser and Convoy Acts were an attempt by mercantile interests in Parliament to focus the Navy on commerce defence and raiding with cruisers, rather than the more scarce and expensive ships of the line. During the 18th century the frigate became the preeminent type of cruiser. A frigate was a small, fast, long range, lightly armed (single gun-deck) ship used for scouting, carrying dispatches, and disrupting enemy trade. The other principal type of cruiser was the sloop, but many other miscellaneous types of ship were used as well.

During the 19th century, navies began to use steam power for their fleets. The 1840s saw the construction of experimental steam-powered frigates and sloops. By the middle of the 1850s, the British and U.S. Navies were both building steam frigates with very long hulls and a heavy gun armament, for instance or .

The 1860s saw the introduction of the ironclad. The first ironclads were frigates, in the sense of having one gun deck; however, they were also clearly the most powerful ships in the navy, and were principally to serve in the line of battle. In spite of their great speed, they would have been wasted in a cruising role.

The French constructed a number of smaller ironclads for overseas cruising duties, starting with the , commissioned 1865. These "station ironclads" were the beginning of the development of the armored cruisers, a type of ironclad specifically for the traditional cruiser missions of fast, independent raiding and patrol.

The first true armored cruiser was the Russian , completed in 1874, and followed by the British a few years later.

Until the 1890s armored cruisers were still built with masts for a full sailing rig, to enable them to operate far from friendly coaling stations.

Unarmored cruising warships, built out of wood, iron, steel or a combination of those materials, remained popular until towards the end of the 19th century. The ironclad's armor often meant that they were limited to short range under steam, and many ironclads were unsuited to long-range missions or for work in distant colonies. The unarmored cruiser - often a screw sloop or screw frigate - could continue in this role. Even though mid- or late-19th century cruisers typically carried up-to-date guns firing explosive shells, they were unable to face ironclads in combat. This was evidenced by the clash between , a modern British cruiser, and the Peruvian monitor "Huáscar". Even though the Peruvian vessel was obsolescent by the time of the encounter, it stood up well to roughly 50 hits from British shells.

In the 1880s, naval engineers began to use steel as a material for construction and armament. A steel cruiser could be lighter and faster than one built of iron or wood. The "Jeune Ecole" school of naval doctrine suggested that a fleet of fast unprotected steel cruisers were ideal for commerce raiding, while the torpedo boat would be able to destroy an enemy battleship fleet.

Steel also offered the cruiser a way of acquiring the protection needed to survive in combat. Steel armor was considerably stronger, for the same weight, than iron. By putting a relatively thin layer of steel armor above the vital parts of the ship, and by placing the coal bunkers where they might stop shellfire, a useful degree of protection could be achieved without slowing the ship too much. Protected cruisers generally had an armored deck with sloped sides, providing similar protection to a light armored belt at less weight and expense.

The first protected cruiser was the Chilean ship "Esmeralda", launched in 1883. Produced by a shipyard at Elswick, in Britain, owned by Armstrong, she inspired a group of protected cruisers produced in the same yard and known as the "Elswick cruisers". Her forecastle, poop deck and the wooden board deck had been removed, replaced with an armored deck.

"Esmeralda"s armament consisted of fore and aft 10-inch (25.4 cm) guns and 6-inch (15.2 cm) guns in the midships positions. It could reach a speed of , and was propelled by steam alone. It also had a displacement of less than 3,000 tons. During the two following decades, this cruiser type came to be the inspiration for combining heavy artillery, high speed and low displacement.

The torpedo cruiser (known in the Royal Navy as the torpedo gunboat) was a smaller unarmored cruiser, which emerged in the 1880s-1890s. These ships could reach speeds up to and were armed with medium to small calibre guns as well as torpedoes. These ships were tasked with guard and reconnaissance duties, to repeat signals and all other fleet duties for which smaller vessels were suited. These ships could also function as flagships of torpedo boat flotillas. After the 1900s, these ships were usually traded for faster ships with better sea going qualities.

Steel also affected the construction and role of armored cruisers. Steel meant that new designs of battleship, later known as pre-dreadnought battleships, would be able to combine firepower and armor with better endurance and speed than ever before. The armored cruisers of the 1890s greatly resembled the battleships of the day; they tended to carry slightly smaller main armament ( rather than 12-inch) and have somewhat thinner armor in exchange for a faster speed (perhaps rather than 18). Because of their similarity, the lines between battleships and armored cruisers became blurred.

Shortly after the turn of the 20th century there were difficult questions about the design of future cruisers. Modern armored cruisers, almost as powerful as battleships, were also fast enough to outrun older protected and unarmored cruisers. In the Royal Navy, Jackie Fisher cut back hugely on older vessels, including many cruisers of different sorts, calling them "a miser's hoard of useless junk" that any modern cruiser would sweep from the seas. The scout cruiser also appeared in this era; this was a small, fast, lightly armed and armored type designed primarily for reconnaissance. The Royal Navy and the Italian Navy were the primary developers of this type.

The growing size and power of the armored cruiser resulted in the battlecruiser, with an armament and size similar to the revolutionary new dreadnought battleship; the brainchild of British admiral Jackie Fisher. He believed that to ensure British naval dominance in its overseas colonial possessions, a fleet of large, fast, powerfully armed vessels which would be able to hunt down and mop up enemy cruisers and armored cruisers with overwhelming fire superiority was needed. This type of vessel came to be known as the "battlecruiser", and the first were commissioned into the Royal Navy in 1907. The British battlecruisers sacrificed protection for speed, and this became tragic with the loss of three of them at the Battle of Jutland. Germany and eventually Japan followed suit to build these vessels, replacing armored cruisers in most frontline roles. Battlecruisers were in many cases larger and more expensive than contemporary battleships, due to their much-larger propulsion plants.

At around the same time as the battlecruiser was developed, the distinction between the armored and the unarmored cruiser finally disappeared. By the British , the first of which was launched in 1909, it was possible for a small, fast cruiser to carry both belt and deck armor, particularly when turbine engines were adopted. These light armored cruisers began to occupy the traditional cruiser role once it became clear that the battlecruiser squadrons were required to operate with the battle fleet.

Some light cruisers were built specifically to act as the leaders of flotillas of destroyers.

These vessels were essentially large coastal patrol boats armed with multiple light guns. One such warship was "Grivița" of the Romanian Navy. She displaced 110 tons, measured 60 meters in length and was armed with four light guns. 

The auxiliary cruiser was a merchant ship hastily armed with small guns on the outbreak of war. Auxiliary cruisers were used to fill gaps in their long-range lines or provide escort for other cargo ships, although they generally proved to be useless in this role because of their low speed, feeble firepower and lack of armor. In both world wars the Germans also used small merchant ships armed with cruiser guns to surprise Allied merchant ships.

Some large liners were armed in the same way. In British service these were known as Armed Merchant Cruisers (AMC). The Germans and French used them in World War I as raiders because of their high speed (around 30 knots (56 km/h)), and they were used again as raiders early in World War II by the Germans and Japanese. In both the First World War and in the early part of the Second, they were used as convoy escorts by the British.

Cruisers were one of the workhorse types of warship during World War I.

Naval construction in the 1920s and 1930s was limited by international treaties designed to prevent the repetition of the Dreadnought arms race of the early 20th century. The Washington Naval Treaty of 1922 placed limits on the construction of ships with a standard displacement of more than 10,000 tons and an armament of guns larger than 8-inch (203 mm). A number of navies commissioned classes of cruisers at the top end of this limit, known as "treaty cruisers".

The London Naval Treaty in 1930 then formalised the distinction between these "heavy" cruisers and light cruisers: a "heavy" cruiser was one with guns of more than 6.1-inch (155 mm) calibre. The Second London Naval Treaty attempted to reduce the tonnage of new cruisers to 8,000 or less, but this had little effect; Japan and Germany were not signatories, and some navies had already begun to evade treaty limitations on warships. The first London treaty did touch off a period of the major powers building 6-inch or 6.1-inch gunned cruisers, nominally of 10,000 tons and with up to fifteen guns, the treaty limit. Thus, most light cruisers ordered after 1930 were the size of heavy cruisers but with more and smaller guns. The Imperial Japanese Navy began this new race with the , launched in 1934. After building smaller light cruisers with six or eight 6-inch guns launched 1931-35, the British Royal Navy followed with the 12-gun in 1936. To match foreign developments and potential treaty violations, in the 1930s the US developed a series of new guns firing "super-heavy" armor piercing ammunition; these included the 6-inch (152 mm)/47 caliber gun Mark 16 introduced with the 15-gun s in 1936, and the 8-inch (203 mm)/55 caliber gun Mark 12 introduced with in 1937.

The heavy cruiser was a type of cruiser designed for long range, high speed and an armament of naval guns around 203 mm (8 in) in calibre. The first heavy cruisers were built in 1915, although it only became a widespread classification following the London Naval Treaty in 1930. The heavy cruiser's immediate precursors were the light cruiser designs of the 1910s and 1920s; the US lightly armored 8-inch "treaty cruisers" of the 1920s (built under the Washington Naval Treaty) were originally classed as light cruisers until the London Treaty forced their redesignation.

Initially, all cruisers built under the Washington treaty had torpedo tubes, regardless of nationality. However, in 1930, results of war games caused the US Naval War College to conclude that only perhaps half of cruisers would use their torpedoes in action. In a surface engagement, long-range gunfire and destroyer torpedoes would decide the issue, and under air attack numerous cruisers would be lost before getting within torpedo range. Thus, beginning with launched in 1933, new cruisers were built without torpedoes, and torpedoes were removed from older heavy cruisers due to the perceived hazard of their being exploded by shell fire. The Japanese took exactly the opposite approach with cruiser torpedoes, and this proved crucial to their tactical victories in most of the numerous cruiser actions of 1942. Beginning with the launched in 1925, every Japanese heavy cruiser was armed with torpedoes, larger than any other cruisers'. By 1933 Japan had developed the Type 93 torpedo for these ships, eventually nicknamed "Long Lance" by the Allies. This type used compressed oxygen instead of compressed air, allowing it to achieve ranges and speeds unmatched by other torpedoes. It could achieve a range of at , compared with the US Mark 15 torpedo with at . The Mark 15 had a maximum range of at , still well below the "Long Lance". The Japanese were able to keep the Type 93's performance and oxygen power secret until the Allies recovered one in early 1943, thus the Allies faced a great threat they were not aware of in 1942. The Type 93 was also fitted to post-1930 Japanese light cruisers and the majority of their World War II destroyers.

Heavy cruisers continued in use until after World War II, with some converted to guided missile cruisers for air defense or strategic attack and some used for shore bombardment by the United States in the Korean War and the Vietnam War.

The German was a series of three "Panzerschiffe" ("armored ships"), a form of heavily armed cruiser, designed and built by the German Reichsmarine in nominal accordance with restrictions imposed by the Treaty of Versailles. All three ships were launched between 1931 and 1934, and served with Germany's Kriegsmarine during World War II. Within the Kriegsmarine, the Panzerschiffe had the propaganda value of capital ships: heavy cruisers with battleship guns, torpedoes, and scout aircraft. The similar Swedish "Panzerschiffe" were tactically used as centers of battlefleets and not as cruisers. They were deployed by Nazi Germany in support of the German interests in the Spanish Civil War. Panzerschiff "Admiral Graf Spee" represented Germany in the 1937 Cornation Fleet Review.

The British press referred to the vessels as pocket battleships, in reference to the heavy firepower contained in the relatively small vessels; they were considerably smaller than contemporary battleships, though at 28 knots were slower than battlecruisers. At up to 16,000 tons at full load, they were not treaty compliant 10,000 ton cruisers. And although their displacement and scale of armor protection were that of a heavy cruiser, their main armament was heavier than the guns of other nations' heavy cruisers, and the latter two members of the class also had tall conning towers resembling battleships. The Panzerschiffe were listed as Ersatz replacements for retiring Reichsmarine coastal defense battleships, which added to their propaganda status in the Kriegsmarine as Ersatz battleships; within the Royal Navy, only battlecruisers "HMS Hood", "Repulse" and "Renown" were capable of both outrunning and outgunning the Panzerschiffe. They were seen in the 1930s as a new and serious threat by both Britain and France. While the Kriegsmarine reclassified them as heavy cruisers in 1940, "Deutschland"-class ships continued to be called "pocket battleships" in the popular press.

The American represented the supersized cruiser design. Due to the German pocket battleships, the , and rumored Japanese "super cruisers", all of which carried guns larger than the standard heavy cruiser's 8-inch size dictated by naval treaty limitations, the "Alaska"s were intended to be "cruiser-killers". While superficially appearing similar to a battleship/battlecruiser and mounting three triple turrets of 12-inch guns, their actual protection scheme and design resembled a scaled-up heavy cruiser design. Their hull classification symbol of CB (cruiser, big) reflected this.

A precursor to the anti-aircraft cruiser was the Romanian British-built protected cruiser "Elisabeta". After the start of World War I, her four 120 mm main guns were landed and her four 75 mm (12-pounder) secondary guns were modified for anti-aircraft fire.

The development of the anti-aircraft cruiser began in 1935 when the Royal Navy re-armed and . Torpedo tubes and low-angle guns were removed from these World War I light cruisers and replaced by ten high-angle guns with appropriate fire-control equipment to provide larger warships with protection against high-altitude bombers.

A tactical shortcoming was recognised after completing six additional conversions of s. Having sacrificed anti-ship weapons for anti-aircraft armament, the converted anti-aircraft cruisers might need protection themselves against surface units. New construction was undertaken to create cruisers of similar speed and displacement with dual-purpose guns.

Dual-purpose guns offered good anti-aircraft protection with anti-surface capability for the traditional light cruiser role of defending capital ships from destroyers. The first purpose built anti-aircraft cruiser was the British , completed in 1940-42. The US Navy anti-aircraft cruisers (CLAA) were designed to match the capabilities of the Royal Navy. Both "Dido" and "Atlanta" initially carried torpedo tubes; the "Atlanta"s at least were originally designed as destroyer leaders, were originally designated CL, and did not receive the CLAA designation until 1949.

The quick-firing dual-purpose gun anti-aircraft cruiser concept was embraced in several designs completed too late to see combat including and completed in 1948 and 1949, two s completed in 1947, two s completed in 1953, and completed in 1955 and 1959, and , and completed between 1959 and 1961.

Most post–World War II cruisers were tasked with air defense roles. In the early 1950s, advances in aviation technology forced the move from anti-aircraft artillery to anti-aircraft missiles. Therefore, most cruisers of today are equipped with surface-to-air missiles as their main armament. The modern equivalent of the anti-aircraft cruiser is the guided missile cruiser (CAG/CLG/CG/CGN).

Cruisers participated in a number of surface engagements in the early part of World War II, along with escorting carrier and battleship groups throughout the war. In the later part of the war, Allied cruisers primarily provided anti-aircraft (AA) escort for carrier groups and performed shore bombardment. Japanese cruisers similarly escorted carrier and battleship groups in the later part of the war, notably in the disastrous Battle of the Philippine Sea and Battle of Leyte Gulf. In 1937-41 the Japanese, having withdrawn from all naval treaties, upgraded or completed the and es as heavy cruisers by replacing their triple turrets with twin turrets. Torpedo refits were also made to most heavy cruisers, resulting in up to sixteen tubes per ship, plus a set of reloads. In 1941 the 1920s light cruisers "Ōi" and "Kitakami" were converted to torpedo cruisers with four guns and forty torpedo tubes. In 1944 "Kitakami" was further converted to carry up to eight "Kaiten" suicide human torpedoes in place of ordinary torpedoes.

In December 1939, three British cruisers engaged the German "pocket battleship" "Admiral Graf Spee" (which was on a commerce raiding mission) in the Battle of the River Plate; the "Graf Spee" then took refuge in neutral Montevideo, Uruguay. By broadcasting messages indicating capital ships were in the area, the British caused the "Graf Spee"s captain to think he faced a hopeless situation while low on ammunition and order his ship scuttled. On 8 June 1940 the German capital ships "Scharnhorst" and "Gneisenau", classed as battleships but with large cruiser armament, sank the aircraft carrier HMS "Glorious" with gunfire. From October 1940 through March 1941 the German heavy cruiser (also known as "pocket battleship", see above) "Admiral Scheer" conducted a successful commerce-raiding voyage in the Atlantic and Indian Oceans.

On 27 May 1941, attempted to finish off the German battleship "Bismarck" with torpedoes, probably causing the Germans to scuttle the ship. "Bismarck" (accompanied by the heavy cruiser "Prinz Eugen") previously sank the battlecruiser HMS "Hood" and damaged the battleship with gunfire in the Battle of the Denmark Strait.

On 19 November 1941 sank in a mutually fatal engagement with the German raider "Kormoran" in the Indian Ocean near Western Australia.

Twenty-three British cruisers were lost to enemy action, mostly to air attack and submarines, in operations in the Atlantic, Mediterranean, and Indian Ocean. Sixteen of these losses were in the Mediterranean. The British included cruisers and anti-aircraft cruisers among convoy escorts in the Mediterranean and to northern Russia due to the threat of surface and air attack. Almost all cruisers in World War II were vulnerable to submarine attack due to a lack of anti-submarine sonar and weapons. Also, until 1943-44 the light anti-aircraft armament of most cruisers was weak.

In July 1942 an attempt to intercept Convoy PQ-17 with surface ships, including the heavy cruiser "Admiral Scheer", failed due to multiple German warships grounding, but air and submarine attacks sank 2/3 of the convoy's ships. In August 1942 "Admiral Scheer" conducted Operation Wunderland, a solo raid into northern Russia's Kara Sea. She bombarded Dikson Island but otherwise had little success.

On 31 December 1942 the Battle of the Barents Sea was fought, a rare action for a Murmansk run because it involved cruisers on both sides. Four British destroyers and five other vessels were escorting Convoy JW 51B from the UK to the Murmansk area. Another British force of two cruisers ( and ) and two destroyers were in the area. Two heavy cruisers (one the "pocket battleship" "Lützow"), accompanied by six destroyers, attempted to intercept the convoy near North Cape after it was spotted by a U-boat. Although the Germans sank a British destroyer and a minesweeper (also damaging another destroyer), they failed to damage any of the convoy's merchant ships. A German destroyer was lost and a heavy cruiser damaged. Both sides withdrew from the action for fear of the other side's torpedoes.

On 26 December 1943 the German capital ship "Scharnhorst" was sunk while attempting to intercept a convoy in the Battle of the North Cape. The British force that sank her was led by Vice Admiral Bruce Fraser in the battleship , accompanied by four cruisers and nine destroyers. One of the cruisers was the preserved .

"Scharnhorst"s sister "Gneisenau", damaged by a mine and a submerged wreck in the Channel Dash of 13 February 1942 and repaired, was further damaged by a British air attack on 27 February 1942. She began a conversion process to mount six guns instead of nine guns, but in early 1943 Hitler (angered by the recent failure at the Battle of the Barents Sea) ordered her disarmed and her armament used as coast defence weapons. One 28 cm triple turret survives near Trondheim, Norway.

The attack on Pearl Harbor on 7 December 1941 brought the United States into the war, but with eight battleships sunk or damaged by air attack. On 10 December 1941 HMS "Prince of Wales" and the battlecruiser HMS "Repulse" were sunk by land-based torpedo bombers northeast of Singapore. It was now clear that surface ships could not operate near enemy aircraft in daylight without air cover; most surface actions of 1942-43 were fought at night as a result. Generally, both sides avoided risking their battleships until the desperate Japanese attack at Leyte Gulf in 1944.

Six of the battleships from Pearl Harbor were eventually returned to service, but no US battleships engaged Japanese surface units at sea until the Naval Battle of Guadalcanal in November 1942, and not thereafter until the Battle of Surigao Strait in October 1944. was on hand for the initial landings at Guadalcanal on 7 August 1942, and escorted carriers in the Battle of the Eastern Solomons later that month. However, on 15 September she was torpedoed while escorting a carrier group and had to return to the US for repairs.

Generally, the Japanese held their capital ships out of all surface actions in the 1941-42 campaigns or they failed to close with the enemy; the Naval Battle of Guadalcanal in November 1942 was the sole exception. The four Kongō-class ships performed shore bombardment in Malaya, Singapore, and Guadalcanal and escorted the raid on Ceylon and other carrier forces in 1941-42. Japanese capital ships also participated ineffectively (due to not being engaged) in the Battle of Midway and the simultaneous Aleutian diversion; in both cases they were in battleship groups well to the rear of the carrier groups. Sources state that "Yamato" sat out the entire Guadalcanal Campaign due to lack of high-explosive bombardment shells, poor nautical charts of the area, and high fuel consumption. It is likely that the poor charts affected other battleships as well. Except for the Kongō class, most Japanese battleships spent the critical year of 1942, in which most of the war's surface actions occurred, in home waters or at the fortified base of Truk, far from any risk of attacking or being attacked.

In 1942 through mid-1943 US and other Allied cruisers were the heavy units on their side of the numerous surface engagements of the Dutch East Indies campaign, the Guadalcanal Campaign, and subsequent Solomon Islands fighting; they were usually opposed by strong Japanese cruiser-led forces equipped with Long Lance torpedoes. Destroyers also participated heavily on both sides of these battles and provided essentially all the torpedoes on the Allied side, with some battles in these campaigns fought entirely between destroyers.

Along with lack of knowledge of the capabilities of the Long Lance torpedo, the US Navy was hampered by a deficiency it was initially unaware of – the unreliability of the Mark 15 torpedo used by destroyers. This weapon shared the Mark 6 exploder and other problems with the more famously unreliable Mark 14 torpedo; the most common results of firing either of these torpedoes were a dud or a miss. The problems with these weapons were not solved until mid-1943, after almost all of the surface actions in the Solomon Islands had taken place. Another factor that shaped the early surface actions was the pre-war training of both sides. The US Navy concentrated on long-range 8-inch gunfire as their primary offensive weapon, leading to rigid battle line tactics, while the Japanese trained extensively for nighttime torpedo attacks. Since all post-1930 Japanese cruisers had 8-inch guns by 1941, almost all of the US Navy's cruisers in the South Pacific in 1942 were the 8-inch-gunned (203 mm) "treaty cruisers"; most of the 6-inch-gunned (152 mm) cruisers were deployed in the Atlantic.

Although their battleships were held out of surface action, Japanese cruiser-destroyer forces rapidly isolated and mopped up the Allied naval forces in the Dutch East Indies campaign of February–March 1942. In three separate actions, they sank five Allied cruisers (two Dutch and one each British, Australian, and American) with torpedoes and gunfire, against one Japanese cruiser damaged. With one other Allied cruiser withdrawn for repairs, the only remaining Allied cruiser in the area was the damaged . Despite their rapid success, the Japanese proceeded methodically, never leaving their air cover and rapidly establishing new air bases as they advanced.

After the key carrier battles of the Coral Sea and Battle of Midway|Midway in mid-1942, Japan had lost four of the six fleet carriers that launched the Pearl Harbor raid and was on the strategic defensive. On 7 August 1942 US Marines were landed on Guadalcanal and other nearby islands, beginning the Guadalcanal Campaign. This campaign proved to be a severe test for the Navy as well as the Marines. Along with two carrier battles, several major surface actions occurred, almost all at night between cruiser-destroyer forces.

On the night of 8–9 August 1942 the Japanese counterattacked near Guadalcanal in the Battle of Savo Island with a cruiser-destroyer force. In a controversial move, the US carrier task forces were withdrawn from the area on the 8th due to heavy fighter losses and low fuel. The Allied force included six heavy cruisers (two Australian), two light cruisers (one Australian), and eight US destroyers. Of the cruisers, only the Australian ships had torpedoes. The Japanese force included five heavy cruisers, two light cruisers, and one destroyer. Numerous circumstances combined to reduce Allied readiness for the battle. The results of the battle were three American heavy cruisers sunk by torpedoes and gunfire, one Australian heavy cruiser disabled by gunfire and scuttled, one heavy cruiser damaged, and two US destroyers damaged. The Japanese had three cruisers lightly damaged. This was the most lopsided outcome of the surface actions in the Solomon Islands. Along with their superior torpedoes, the opening Japanese gunfire was accurate and very damaging. Subsequent analysis showed that some of the damage was due to poor housekeeping practices by US forces. Stowage of boats and aircraft in midships hangars with full gas tanks contributed to fires, along with full and unprotected ready-service ammunition lockers for the open-mount secondary armament. These practices were soon corrected, and US cruisers with similar damage sank less often thereafter. It should be noted that Savo was the first surface action of the war for almost all the US ships and personnel; few US cruisers and destroyers were targeted or hit at Coral Sea or Midway.

On 24–25 August 1942 the Battle of the Eastern Solomons, a major carrier action, was fought. Part of the action was a Japanese attempt to reinforce Guadalcanal with men and equipment on troop transports. The Japanese troop convoy was attacked by Allied aircraft, resulting in the Japanese subsequently reinforcing Guadalcanal with troops on fast warships at night. These convoys were called the "Tokyo Express" by the Allies. Although the Tokyo Express often ran unopposed, most surface actions in the Solomons revolved around Tokyo Express missions. Also, US air operations had commenced from Henderson Field, the airfield on Guadalcanal. Fear of air power on both sides resulted in all surface actions in the Solomons being fought at night.

The Battle of Cape Esperance occurred on the night of 11–12 October 1942. A Tokyo Express mission was underway for Guadalcanal at the same time as a separate cruiser-destroyer bombardment group loaded with high explosive shells for bombarding Henderson Field. A US cruiser-destroyer force was deployed in advance of a convoy of US Army troops for Guadalcanal that was due on 13 October. The Tokyo Express convoy was two seaplane tenders and six destroyers; the bombardment group was three heavy cruisers and two destroyers, and the US force was two heavy cruisers, two light cruisers, and five destroyers. The US force engaged the Japanese bombardment force; the Tokyo Express convoy was able to unload on Guadalcanal and evade action. The bombardment force was sighted at close range () and the US force opened fire. The Japanese were surprised because their admiral was anticipating sighting the Tokyo Express force, and withheld fire while attempting to confirm the US ships' identity. One Japanese cruiser and one destroyer were sunk and one cruiser damaged, against one US destroyer sunk with one light cruiser and one destroyer damaged. The bombardment force failed to bring its torpedoes into action, and turned back. The next day US aircraft from Henderson Field attacked several of the Japanese ships, sinking two destroyers and damaging a third. The US victory resulted in overconfidence in some later battles, reflected in the initial after-action report claiming two Japanese heavy cruisers, one light cruiser, and three destroyers sunk by the gunfire of alone. The battle had little effect on the overall situation, as the next night two Kongō-class battleships bombarded and severely damaged Henderson Field unopposed, and the following night another Tokyo Express convoy delivered 4,500 troops to Guadalcanal. The US convoy delivered the Army troops as scheduled on the 13th.

The Battle of the Santa Cruz Islands took place 25–27 October 1942. It was a pivotal battle, as it left the US and Japanese with only two large carriers each in the South Pacific (another large Japanese carrier was damaged and under repair until May 1943). Due to the high carrier attrition rate with no replacements for months, for the most part both sides stopped risking their remaining carriers until late 1943, and each side sent in a pair of battleships instead. The next major carrier operations for the US were the carrier raid on Rabaul and support for the invasion of Tarawa, both in November 1943.

The Naval Battle of Guadalcanal occurred 12–15 November 1942 in two phases. A night surface action on 12–13 November was the first phase. The Japanese force consisted of two Kongō-class battleships with high explosive shells for bombarding Henderson Field, one small light cruiser, and 11 destroyers. Their plan was that the bombardment would neutralize Allied airpower and allow a force of 11 transport ships and 12 destroyers to reinforce Guadalcanal with a Japanese division the next day. However, US reconnaissance aircraft spotted the approaching Japanese on the 12th and the Americans made what preparations they could. The American force consisted of two heavy cruisers, one light cruiser, two anti-aircraft cruisers, and eight destroyers. The Americans were badly outgunned by the Japanese that night, and a lack of pre-battle orders by the US commander led to confusion. courageously closed with the battleship "Hiei", firing all torpedoes (though apparently none hit or detonated) and raking the battleship's bridge with gunfire, wounding the Japanese admiral and killing his chief of staff. The Americans initially lost four destroyers including "Laffey", with both heavy cruisers, most of the remaining destroyers, and both anti-aircraft cruisers damaged. The Japanese initially had one battleship and four destroyers damaged, but at this point they withdrew, possibly unaware that the US force was unable to further oppose them. At dawn US aircraft from Henderson Field, , and Espiritu Santo found the damaged battleship and two destroyers in the area. The battleship ("Hiei") was sunk by aircraft (or possibly scuttled), one destroyer was sunk by the damaged , and the other destroyer was attacked by aircraft but was able to withdraw. Both of the damaged US anti-aircraft cruisers were lost on the 13th, one () torpedoed by a Japanese submarine, and the other sank on the way to repairs. "Juneau"s loss was especially tragic; the submarine's presence prevented immediate rescue, over 100 survivors of a crew of nearly 700 were adrift for eight days, and all but ten died. Among the dead were the five Sullivan brothers.

The Japanese transport force was rescheduled for the 14th and a new cruiser-destroyer force (belatedly joined by the surviving battleship "Kirishima") was sent to bombard Henderson Field the night of the 13th. Only two cruisers actually bombarded the airfield, as "Kirishima" had not arrived yet and the remainder of the force was on guard for US warships. The bombardment caused little damage. The cruiser-destroyer force then withdrew, while the transport force continued towards Guadalcanal. Both forces were attacked by US aircraft on the 14th. The cruiser force lost one heavy cruiser sunk and one damaged. Although the transport force had fighter cover from the carrier "Jun'yō", six transports were sunk and one heavily damaged. All but four of the destroyers accompanying the transport force picked up survivors and withdrew. The remaining four transports and four destroyers approached Guadalcanal at night, but stopped to await the results of the night's action.

On the night of the 14th-15th a Japanese force of "Kirishima", two heavy and two light cruisers, and nine destroyers approached Guadalcanal. Two US battleships ( and ) were there to meet them, along with four destroyers. This was one of only two battleship-on-battleship encounters during the Pacific War; the other was the lopsided Battle of Surigao Strait in October 1944, part of the Battle of Leyte Gulf. The battleships had been escorting "Enterprise", but were detached due to the urgency of the situation. With nine 16-inch (406 mm) guns apiece against eight 14-inch (356 mm) guns on "Kirishima", the Americans had major gun and armor advantages. All four destroyers were sunk or severely damaged and withdrawn shortly after the Japanese attacked them with gunfire and torpedoes. Although her main battery remained in action for most of the battle, "South Dakota" spent much of the action dealing with major electrical failures that affected her radar, fire control, and radio systems. Although her armor was not penetrated, she was hit by 26 shells of various calibers and temporarily rendered, in a US admiral's words, "deaf, dumb, blind, and impotent". "Washington" went undetected by the Japanese for most of the battle, but withheld shooting to avoid "friendly fire" until "South Dakota" was illuminated by Japanese fire, then rapidly set "Kirishima" ablaze with a jammed rudder and other damage. "Washington", finally spotted by the Japanese, then headed for the Russell Islands to hopefully draw the Japanese away from Guadalcanal and "South Dakota", and was successful in evading several torpedo attacks. Unusually, only a few Japanese torpedoes scored hits in this engagement. "Kirishima" sank or was scuttled before the night was out, along with two Japanese destroyers. The remaining Japanese ships withdrew, except for the four transports, which beached themselves in the night and started unloading. However, dawn (and US aircraft, US artillery, and a US destroyer) found them still beached, and they were destroyed.

The Battle of Tassafaronga took place on the night of 30 November-1 December 1942. The US had four heavy cruisers, one light cruiser, and four destroyers. The Japanese had eight destroyers on a Tokyo Express run to deliver food and supplies in drums to Guadalcanal. The Americans achieved initial surprise, damaging one destroyer with gunfire which later sank, but the Japanese torpedo counterattack was devastating. One American heavy cruiser was sunk and three others heavily damaged, with the bows blown off of two of them. It was significant that these two were not lost to Long Lance hits as happened in previous battles; American battle readiness and damage control had improved. Despite defeating the Americans, the Japanese withdrew without delivering the crucial supplies to Guadalcanal. Another attempt on 3 December dropped 1,500 drums of supplies near Guadalcanal, but Allied strafing aircraft sank all but 300 before the Japanese Army could recover them. On 7 December PT boats interrupted a Tokyo Express run, and the following night sank a Japanese supply submarine. The next day the Japanese Navy proposed stopping all destroyer runs to Guadalcanal, but agreed to do just one more. This was on 11 December and was also intercepted by PT boats, which sank a destroyer; only 200 of 1,200 drums dropped off the island were recovered. The next day the Japanese Navy proposed abandoning Guadalcanal; this was approved by the Imperial General Headquarters on 31 December and the Japanese left the island in early February 1943.

After the Japanese abandoned Guadalcanal in February 1943, Allied operations in the Pacific shifted to the New Guinea campaign and isolating Rabaul. The Battle of Kula Gulf was fought on the night of 5–6 July. The US had three light cruisers and four destroyers; the Japanese had ten destroyers loaded with 2,600 troops destined for Vila to oppose a recent US landing on Rendova. Although the Japanese sank a cruiser, they lost two destroyers and were able to deliver only 850 troops. On the night of 12–13 July, the Battle of Kolombangara occurred. The Allies had three light cruisers (one New Zealand) and ten destroyers; the Japanese had one small light cruiser and five destroyers, a Tokyo Express run for Vila. All three Allied cruisers were heavily damaged, with the New Zealand cruiser put out of action for 25 months by a Long Lance hit. The Allies sank only the Japanese light cruiser, and the Japanese landed 1,200 troops at Vila. Despite their tactical victory, this battle caused the Japanese to use a different route in the future, where they were more vulnerable to destroyer and PT boat attacks.

The Battle of Empress Augusta Bay was fought on the night of 1–2 November 1943, immediately after US Marines invaded Bougainville in the Solomon Islands. A Japanese heavy cruiser was damaged by a nighttime air attack shortly before the battle; it is likely that Allied airborne radar had progressed far enough to allow night operations. The Americans had four of the new cruisers and eight destroyers. The Japanese had two heavy cruisers, two small light cruisers, and six destroyers. Both sides were plagued by collisions, shells that failed to explode, and mutual skill in dodging torpedoes. The Americans suffered significant damage to three destroyers and light damage to a cruiser, but no losses. The Japanese lost one light cruiser and a destroyer, with four other ships damaged. The Japanese withdrew; the Americans pursued them until dawn, then returned to the landing area to provide anti-aircraft cover.

After the Battle of the Santa Cruz Islands in October 1942, both sides were short of large aircraft carriers. The US suspended major carrier operations until sufficient carriers could be completed to destroy the entire Japanese fleet at once should it appear. The Central Pacific carrier raids and amphibious operations commenced in November 1943 with a carrier raid on Rabaul (preceded and followed by Fifth Air Force attacks) and the bloody but successful invasion of Tarawa. The air attacks on Rabaul crippled the Japanese cruiser force, with four heavy and two light cruisers damaged; they were withdrawn to Truk. The US had built up a force in the Central Pacific of six large, five light, and six escort carriers prior to commencing these operations.

From this point on, US cruisers primarily served as anti-aircraft escorts for carriers and in shore bombardment. The only major Japanese carrier operation after Guadalcanal was the disastrous (for Japan) Battle of the Philippine Sea in June 1944, nicknamed the "Marianas Turkey Shoot" by the US Navy.

The Imperial Japanese Navy's last major operation was the Battle of Leyte Gulf, an attempt to dislodge the American invasion of the Philippines in October 1944. The two actions at this battle in which cruisers played a significant role were the Battle off Samar and the Battle of Surigao Strait.

The Battle of Surigao Strait was fought on the night of 24–25 October, a few hours before the Battle off Samar. The Japanese had a small battleship group composed of "Fusō" and "Yamashiro", one heavy cruiser, and four destroyers. They were followed at a considerable distance by another small force of two heavy cruisers, a small light cruiser, and four destroyers. Their goal was to head north through Surigao Strait and attack the invasion fleet off Leyte. The Allied force, known as the 7th Fleet Support Force, guarding the strait was overwhelming. It included six battleships (all but one previously damaged in 1941 at Pearl Harbor), four heavy cruisers (one Australian), four light cruisers, and 28 destroyers, plus a force of 39 PT boats. The only advantage to the Japanese was that most of the battleships and cruisers were loaded mainly with high explosive shells, although a significant number of armor-piercing shells were also loaded. The lead Japanese force evaded the PT boats' torpedoes, but were hit hard by the destroyers' torpedoes, losing a battleship. Then they encountered the battleship and cruiser guns. Only one destroyer survived. The engagement is notable for being one of only two occasions in which battleships fired on battleships in the Pacific Theater, the other being the Naval Battle of Guadalcanal. Due to the starting arrangement of the opposing forces, the Allied force was in a "crossing the T" position, so this was the last battle in which this occurred, but it was not a planned maneuver. The following Japanese cruiser force had several problems, including a light cruiser damaged by a PT boat and two heavy cruisers colliding, one of which fell behind and was sunk by air attack the next day. An American veteran of Surigao Strait, , was transferred to Argentina in 1951 as the "General Belgrano", becoming most famous for being sunk by in the Falklands War on 2 May 1982. She was the first ship sunk by a nuclear submarine outside of accidents, and only the second ship sunk by a submarine since World War II.

At the Battle off Samar, a Japanese battleship group moving towards the invasion fleet off Leyte engaged a minuscule American force known as "Taffy 3" (formally Task Unit 77.4.3), composed of six escort carriers with about 28 aircraft each, three destroyers, and four destroyer escorts. The biggest guns in the American force were /38 caliber guns, while the Japanese had , , and guns. Aircraft from six additional escort carriers also participated for a total of around 330 US aircraft, a mix of F6F Hellcat fighters and TBF Avenger torpedo bombers. The Japanese had four battleships including the massive "Yamato", six heavy cruisers, two small light cruisers, and 11 destroyers. The Japanese force had earlier been driven off by air attack, losing "Yamato"s sister "Musashi". Admiral Halsey then decided to use his Third Fleet carrier force to attack the Japanese carrier group, located well to the north of Samar, which was actually a decoy group with few aircraft. The Japanese were desperately short of aircraft and pilots at this point in the war, and Leyte Gulf was the first battle in which "kamikaze" suicide attacks were used. Due to a tragedy of errors, Halsey took the American battleship force with him, leaving San Bernardino Strait guarded only by the small Seventh Fleet escort carrier force. The battle commenced at dawn on 25 October 1944, shortly after the Battle of Surigao Strait. In the engagement that followed, the Americans exhibited uncanny torpedo accuracy, blowing the bows off several Japanese heavy cruisers. The escort carriers' aircraft also performed very well, attacking with machine guns after their carriers ran out of bombs and torpedoes. The unexpected level of damage, and maneuvering to avoid the torpedoes and air attacks, disorganized the Japanese and caused them to think they faced at least part of the Third Fleet's main force. They had also learned of the defeat a few hours before at Surigao Strait, and did not hear that Halsey's force was busy destroying the decoy fleet. Convinced that the rest of the Third Fleet would arrive soon if it hadn't already, the Japanese withdrew, eventually losing three heavy cruisers sunk with three damaged to air and torpedo attacks. The Americans lost two escort carriers, two destroyers, and one destroyer escort sunk, with three escort carriers, one destroyer, and two destroyer escorts damaged, thus losing over one-third of their engaged force sunk with nearly all the remainder damaged.

The US built cruisers in quantity through the end of the war, notably 14 heavy cruisers and 27 light cruisers, along with eight anti-aircraft cruisers. The "Cleveland" class was the largest cruiser class ever built in number of ships completed, with nine additional "Cleveland"s completed as light aircraft carriers. The large number of cruisers built was probably due to the significant cruiser losses of 1942 in the Pacific theater (seven American and five other Allied) and the perceived need for several cruisers to escort each of the numerous s being built. Losing four heavy and two small light cruisers in 1942, the Japanese built only five light cruisers during the war; these were small ships with six guns each. Losing 20 cruisers in 1940-42, the British completed no heavy cruisers, thirteen light cruisers (Crown Colony and "Minotaur" classes), and sixteen anti-aircraft cruisers () during the war.

The rise of air power during World War II dramatically changed the nature of naval combat. Even the fastest cruisers could not maneuver quickly enough to evade aerial attack, and aircraft now had torpedoes, allowing moderate-range standoff capabilities. This change led to the end of independent operations by single ships or very small task groups, and for the second half of the 20th century naval operations were based on very large fleets believed able to fend off all but the largest air attacks, though this was not tested by any war in that period. The US Navy became centered around carrier groups, with cruisers and battleships primarily providing anti-aircraft defense and shore bombardment. Until the Harpoon missile entered service in the late 1970s, the US Navy was almost entirely dependent on carrier-based aircraft and submarines for conventionally attacking enemy warships. Lacking aircraft carriers, the Soviet Navy depended on anti-ship cruise missiles; in the 1950s these were primarily delivered from heavy land-based bombers. Soviet submarine-launched cruise missiles at the time were primarily for land attack; but by 1964 anti-ship missiles were deployed in quantity on cruisers, destroyers, and submarines.

The US Navy was aware of the potential missile threat as soon as World War II ended, and had considerable related experience due to Japanese "kamikaze" attacks in that war. The initial response was to upgrade the light AA armament of new cruisers from 40 mm and 20 mm weapons to twin 3 inch (76 mm)/50 caliber gun mounts. For the longer term, it was thought that gun systems would be inadequate to deal with the missile threat, and by the mid-1950s three naval SAM systems were developed: Talos (long range), Terrier (medium range), and Tartar (short range). Talos and Terrier were nuclear-capable and this allowed their use in anti-ship or shore bombardment roles in the event of nuclear war. Chief of Naval Operations Admiral Arleigh Burke is credited with speeding the development of these systems.

Terrier was initially deployed on two converted "Baltimore"-class cruisers (CAG), with conversions completed in 1955-56. Further conversions of six s (CLG) ( and classes), redesign of the "Farragut" class as guided missile "frigates" (DLG), and development of the DDGs resulted in the completion of numerous additional guided missile ships deploying all three systems in 1959-1962. Also completed during this period was the nuclear-powered , with two Terrier and one Talos launchers, plus an ASROC anti-submarine launcher the World War II conversions lacked. The converted World War II cruisers up to this point retained one or two main battery turrets for shore bombardment. However, in 1962-1964 three additional "Baltimore" and cruisers were more extensively converted as the . These had two Talos and two Tartar launchers plus ASROC and two 5-inch (127 mm) guns for self-defense, and were primarily built to get greater numbers of Talos launchers deployed. Of all these types, only the "Farragut" DLGs were selected as the design basis for further production, although their successors were significantly larger (5,670 tons standard versus 4,150 tons standard) due to a second Terrier launcher and greater endurance. An economical crew size compared with World War II conversions was probably a factor, as the "Leahy"s required a crew of only 377 versus 1,200 for the "Cleveland"-class conversions. Through 1980, the ten "Farragut"s were joined by four additional classes and two one-off ships for a total of 36 guided missile frigates, eight of them nuclear-powered (DLGN). In 1975 the "Farragut"s were reclassified as guided missile destroyers (DDG) due to their small size, and the remaining DLG/DLGN ships became guided missile cruisers (CG/CGN). The World War II conversions were gradually retired 1970-1980; in 1980 the Talos missile was withdrawn as a cost-saving measure and the "Albany"s were decommissioned. "Long Beach" had her Talos launcher removed in a refit shortly thereafter; the deck space was used for Harpoon missiles. Around this time the Terrier ships were upgraded with the RIM-67 Standard ER missile. The guided missile frigates and cruisers served in the Cold War and the Vietnam War; off Vietnam they performed shore bombardment and shot down enemy aircraft or, as Positive Identification Radar Advisory Zone (PIRAZ) ships, guided fighters to intercept enemy aircraft. By 1995 the former guided missile frigates were replaced by the "Ticonderoga"s and s.

The U.S. Navy's guided-missile cruisers were built upon destroyer-style hulls (some called "destroyer leaders" or "frigates" prior to the 1975 reclassification). As the U.S. Navy's strike role was centered around aircraft carriers, cruisers were primarily designed to provide air defense while often adding anti-submarine capabilities. These U.S. cruisers that were built in the 1960s and 1970s were larger, often nuclear-powered for extended endurance in escorting nuclear-powered fleet carriers, and carried longer-range surface-to-air missiles (SAMs) than early "Charles F. Adams" guided-missile destroyers that were tasked with the short-range air defense role. The U.S. cruiser was a major contrast to their contemporaries, Soviet "rocket cruisers" that were armed with large numbers of anti-ship cruise missiles (ASCMs) as part of the combat doctrine of saturation attack, though in the early 1980s the U.S. Navy retrofitted some of these existing cruisers to carry a small number of Harpoon anti-ship missiles and Tomahawk cruise missiles.

The line between U.S. Navy cruisers and destroyers blurred with the . While originally designed for anti-submarine warfare, a "Spruance" destroyer was comparable in size to existing U.S. cruisers, while having the advantage of an enclosed hangar (with space for up to two medium-lift helicopters) which was a considerable improvement over the basic aviation facilities of earlier cruisers. The "Spruance" hull design was used as the basis for two classes; the which had comparable anti-air capabilities to cruisers at the time, and then the DDG-47-class destroyers which were redesignated as the "Ticonderoga"-class guided missile cruisers to emphasize the additional capability provided by the ships' Aegis combat systems, and their flag facilities suitable for an admiral and his staff. In addition, 24 members of the "Spruance"-class were upgraded with the vertical launch system (VLS) for Tomahawk cruise missiles due to its modular hull design, along with the similarly VLS-equipped "Ticonderoga"-class, these ships had anti-surface strike capabilities beyond the 1960s-70s cruisers that received Tomahawk armored-box launchers as part of the New Threat Upgrade. Like the "Ticonderoga" ships with VLS, the and , despite being classified as destroyers, actually have much heavier anti-surface armament than previous U.S. ships classified as cruisers.

Prior to the introduction of the "Ticonderoga"s, the US Navy used odd naming conventions that left its fleet seemingly without many cruisers, although a number of their ships were cruisers in all but name. From the 1950s to the 1970s, US Navy cruisers were large vessels equipped with heavy offensive missiles (including the Regulus nuclear cruise missile) for wide-ranging combat against land-based and sea-based targets. All save one——were converted from World War II cruisers of the , and classes. "Long Beach" was also the last cruiser built with a World War II-era cruiser style hull (characterized by a long lean hull); later new-build cruisers were actually converted frigates (DLG/CG , , , , and the "California" and "Virginia" classes) or uprated destroyers (the DDG/CG "Ticonderoga" class was built on a "Spruance"-class destroyer hull).

Frigates under this scheme were almost as large as the cruisers and optimized for anti-aircraft warfare, although they were capable anti-surface warfare combatants as well. In the late 1960s, the US government perceived a "cruiser gap"—at the time, the US Navy possessed six ships designated as cruisers, compared to 19 for the Soviet Union, even though the USN had 21 ships designated as frigates with equal or superior capabilities to the Soviet cruisers at the time. Because of this, in 1975 the Navy performed a massive redesignation of its forces:

Also, a series of Patrol Frigates of the , originally designated PFG, were redesignated into the FFG line. The cruiser-destroyer-frigate realignment and the deletion of the Ocean Escort type brought the US Navy's ship designations into line with the rest of the world's, eliminating confusion with foreign navies. In 1980, the Navy's then-building DDG-47-class destroyers were redesignated as cruisers ("Ticonderoga" guided missile cruisers) to emphasize the additional capability provided by the ships' Aegis combat systems, and their flag facilities suitable for an admiral and his staff.

In the Soviet Navy, cruisers formed the basis of combat groups. In the immediate post-war era it built a fleet of gun-armed light cruisers, but replaced these beginning in the early 1960s with large ships called "rocket cruisers", carrying large numbers of anti-ship cruise missiles (ASCMs) and anti-aircraft missiles. The Soviet combat doctrine of saturation attack meant that their cruisers (as well as destroyers and even missile boats) mounted multiple missiles in large container/launch tube housings and carried far more ASCMs than their NATO counterparts, while NATO combatants instead used individually smaller and lighter missiles (while appearing under-armed when compared to Soviet ships).

In 1962-65 the four s entered service; these had launchers for eight long-range SS-N-3 Shaddock ASCMs with a full set of reloads; these had a range of up to with mid-course guidance. The four more modest s, with launchers for four SS-N-3 ASCMs and no reloads, entered service in 1967-69. In 1969-79 Soviet cruiser numbers more than tripled with ten s and seven s entering service. These had launchers for eight large-diameter missiles whose purpose was initially unclear to NATO. This was the SS-N-14 Silex, an over/under rocket-delivered heavyweight torpedo primarily for the anti-submarine role, but capable of anti-surface action with a range of up to . Soviet doctrine had shifted; powerful anti-submarine vessels (these were designated "Large Anti-Submarine Ships", but were listed as cruisers in most references) were needed to destroy NATO submarines to allow Soviet ballistic missile submarines to get within range of the United States in the event of nuclear war. By this time Long Range Aviation and the Soviet submarine force could deploy numerous ASCMs. Doctrine later shifted back to overwhelming carrier group defenses with ASCMs, with the "Slava" and "Kirov" classes.

The most recent Soviet/Russian rocket cruisers, the four s, were built in the 1970s and 1980s. Two of the "Kirov" class are in refit until 2020, and one was scheduled to leave refit in 2018, with the in active service. Russia also operates three s and one "Admiral Kuznetsov"-class carrier which is officially designated as a cruiser.

Currently, the "Kirov"-class heavy missile cruisers are used for command purposes, as "Pyotr Velikiy" is the flagship of the Northern Fleet. However, their air defense capabilities are still powerful, as shown by the array of Point defense missiles they carry, from 44 OSA-MA missiles to 196 9K311 Tor missiles. For longer range targets, the S-300 is used. For closer range targets, AK-630 or Kashtan CIWSs are used. Aside from that, "Kirov"s have 20 P-700 Granit missiles for anti-ship warfare. For target acquisition beyond the radar horizon, three helicopters can be used. Besides a vast array of armament, "Kirov"-class cruisers are also outfitted with many sensors and communications equipment, allowing them to lead the fleet.

The United States Navy has centered on the aircraft carrier since World War II. The "Ticonderoga"-class cruisers, built in the 1980s, were originally designed and designated as a class of destroyer, intended to provide a very powerful air-defense in these carrier-centered fleets.

Outside the US and Soviet navies, new cruisers were rare following World War II. Most navies use guided missile destroyers for fleet air defense, and destroyers and frigates for cruise missiles. The need to operate in task forces has led most navies to change to fleets designed around ships dedicated to a single role, anti-submarine or anti-aircraft typically, and the large "generalist" ship has disappeared from most forces. The United States Navy and the Russian Navy are the only remaining navies which operate cruisers. Italy used until 2003; France operated a single helicopter cruiser until May 2010, , for training purposes only.

In the years since the launch of in 1981 the class has received a number of upgrades that have dramatically improved its members' capabilities for anti-submarine and land attack (using the Tomahawk missile). Like their Soviet counterparts, the modern "Ticonderoga"s can also be used as the basis for an entire battle group. Their cruiser designation was almost certainly deserved when first built, as their sensors and combat management systems enable them to act as 'flagships' for a surface warship flotilla if no carrier is present, but newer ships rated as destroyers and also equipped with AEGIS approach them very closely in capability, and once more blur the line between the two classes.

From time to time, some navies have experimented with aircraft-carrying cruisers. One example is the Swedish . Another was the Japanese "Mogami", which was converted to carry a large floatplane group in 1942. Another variant is the "helicopter cruiser". The last example in service was the Soviet Navy's , whose last unit was converted to a pure aircraft carrier and sold to India as . The Russian Navy's is nominally designated as an aviation cruiser but otherwise resembles a standard medium aircraft carrier, albeit with a surface-to-surface missile battery. The Royal Navy's aircraft-carrying and the Italian Navy's aircraft-carrying vessels were originally designated 'through-deck cruisers', but have since been designated as small aircraft carriers. Similarly, the Japan Maritime Self-Defense Force's and "helicopter destroyers" are really more along the lines of helicopter cruisers in function and aircraft complement, but due to the Treaty of San Francisco, must be designated as destroyers.

One cruiser alternative studied in the late 1980s by the United States was variously entitled a Mission Essential Unit (MEU) or CG V/STOL. In a return to the thoughts of the independent operations cruiser-carriers of the 1930s and the Soviet "Kiev" class, the ship was to be fitted with a hangar, elevators, and a flight deck. The mission systems were Aegis, SQS-53 sonar, 12 SV-22 ASW aircraft and 200 VLS cells. The resulting ship would have had a waterline length of 700 feet, a waterline beam of 97 feet, and a displacement of about 25,000 tons. Other features included an integrated electric drive and advanced computer systems, both stand-alone and networked. It was part of the U.S. Navy's "Revolution At Sea" effort. The project was curtailed by the sudden end of the Cold War and its aftermath, otherwise the first of class would have been likely ordered in the early 1990s.

Few cruisers remain operational or still under construction in the world navies. Those that do are:


As of 2015, several decommissioned cruisers have been saved from scrapping and exist worldwide as museum ships. They are:







</doc>
<doc id="7037" url="https://en.wikipedia.org/wiki?curid=7037" title="Chlamydia infection">
Chlamydia infection

Chlamydia infection, often simply known as chlamydia, is a sexually transmitted infection caused by the bacterium "Chlamydia trachomatis". Most people who are infected have no symptoms. When symptoms do develop this can take a few weeks following infection to occur. Symptoms in women may include vaginal discharge or burning with urination. Symptoms in men may include discharge from the penis, burning with urination, or pain and swelling of one or both testicles. The infection can spread to the upper genital tract in women causing pelvic inflammatory disease which may result in future infertility or ectopic pregnancy. Repeated infections of the eyes that go without treatment can result in trachoma, a common cause of blindness in the developing world.
Chlamydia can be spread during vaginal, anal, or oral sex, and can be passed from an infected mother to her baby during childbirth. The eye infections may also be spread by personal contact, flies, and contaminated towels in areas with poor sanitation. "Chlamydia trachomatis" only occurs in humans. Diagnosis is often by screening which is recommended yearly in sexually active women under the age of twenty five, others at higher risk, and at the first prenatal visit. Testing can be done on the urine or a swab of the cervix, vagina, or urethra. Rectal or mouth swabs are required to diagnose infections in those areas.
Prevention is by not having sex, the use of condoms, or having sex with only one other person, who is not infected. Chlamydia can be cured by antibiotics with typically either azithromycin or doxycycline being used. Erythromycin or azithromycin is recommended in babies and during pregnancy. Sexual partners should also be treated and the infected people advised not to have sex for seven days and until symptom free. Gonorrhea, syphilis, and HIV should be tested for in those who have been infected. Following treatment people should be tested again after three months.
Chlamydia is one of the most common sexually transmitted infections worldwide affecting about 4.2% of women and 2.7% of men. In 2015 about 61 million new cases occurred globally. In the United States about 1.4 million cases were reported in 2014. Infections are most common among those between the ages of 15 and 25 and are more common in women than men. In 2015 infections resulted in about 200 deaths. The word "chlamydia" is from the Greek, χλαμύδα meaning "cloak".

Chlamydial infection of the cervix (neck of the womb) is a sexually transmitted infection which has no symptoms for 50–70% of women infected. The infection can be passed through vaginal, anal, or oral sex. Of those who have an asymptomatic infection that is not detected by their doctor, approximately half will develop pelvic inflammatory disease (PID), a generic term for infection of the uterus, fallopian tubes, and/or ovaries. PID can cause scarring inside the reproductive organs, which can later cause serious complications, including chronic pelvic pain, difficulty becoming pregnant, ectopic (tubal) pregnancy, and other dangerous complications of pregnancy.

Chlamydia is known as the "silent epidemic" as in women, it may not cause any symptoms in 70–80% of cases, and can linger for months or years before being discovered. Signs and symptoms may include abnormal vaginal bleeding or discharge, abdominal pain, painful sexual intercourse, fever, painful urination or the urge to urinate more often than usual (urinary urgency).

For sexually active women who are not pregnant, screening is recommended in those under 25 and others at risk of infection. Risk factors include a history of chlamydial or other sexually transmitted infection, new or multiple sexual partners, and inconsistent condom use. Guidelines recommend all women attending for emergency contraceptive are offered Chlamydia testing, with studies showing up to 9% of women aged <25 years had Chlamydia.

In men, those with a chlamydial infection show symptoms of infectious inflammation of the urethra in about 50% of cases. Symptoms that may occur include: a painful or burning sensation when urinating, an unusual discharge from the penis, testicular pain or swelling, or fever. If left untreated, chlamydia in men can spread to the testicles causing epididymitis, which in rare cases can lead to sterility if not treated. Chlamydia is also a potential cause of prostatic inflammation in men, although the exact relevance in prostatitis is difficult to ascertain due to possible contamination from urethritis.

Chlamydia conjunctivitis or trachoma was once the most important cause of blindness worldwide, but its role diminished from 15% of blindness cases by trachoma in 1995 to 3.6% in 2002. The infection can be spread from eye to eye by fingers, shared towels or cloths, coughing and sneezing and eye-seeking flies. Newborns can also develop chlamydia eye infection through childbirth (see below). Using the SAFE strategy (acronym for surgery for in-growing or in-turned lashes, antibiotics, facial cleanliness, and environmental improvements), the World Health Organization aims for the global elimination of trachoma by 2020 (GET 2020 initiative).

Chlamydia may also cause reactive arthritis—the triad of arthritis, conjunctivitis and urethral inflammation—especially in young men. About 15,000 men develop reactive arthritis due to chlamydia infection each year in the U.S., and about 5,000 are permanently affected by it. It can occur in both sexes, though is more common in men.

As many as half of all infants born to mothers with chlamydia will be born with the disease. Chlamydia can affect infants by causing spontaneous abortion; premature birth; conjunctivitis, which may lead to blindness; and pneumonia. Conjunctivitis due to chlamydia typically occurs one week after birth (compared with chemical causes (within hours) or gonorrhea (2–5 days)).

A different serovar of Chlamydia trachomatis is also the cause of lymphogranuloma venereum, an infection of the lymph nodes and lymphatics. It usually presents with genital ulceration and swollen lymph nodes in the groin, but it may also manifest as rectal inflammation, fever or swollen lymph nodes in other regions of the body.

Chlamydia can be transmitted during vaginal, anal, or oral sex or direct contact with infected tissue such as conjunctiva. Chlamydia can also be passed from an infected mother to her baby during vaginal childbirth.

"Chlamydiae" have the ability to establish long-term associations with host cells. When an infected host cell is starved for various nutrients such as amino acids (for example, tryptophan), iron, or vitamins, this has a negative consequence for "Chlamydiae" since the organism is dependent on the host cell for these nutrients. Long-term cohort studies indicate that approximately 50% of those infected clear within a year, 80% within two years, and 90% within three years.

The starved chlamydiae enter a persistent growth state wherein they stop cell division and become morphologically aberrant by increasing in size. Persistent organisms remain viable as they are capable of returning to a normal growth state once conditions in the host cell improve.

There is much debate as to whether persistence has "in vivo" relevance. Many believe that persistent chlamydiae are the cause of chronic chlamydial diseases. Some antibiotics such as β-lactams can also induce a persistent-like growth state, which can contribute to the chronicity of chlamydial diseases.

The diagnosis of genital chlamydial infections evolved rapidly from the 1990s through 2006. Nucleic acid amplification tests (NAAT), such as polymerase chain reaction (PCR), transcription mediated amplification (TMA), and the DNA strand displacement amplification (SDA) now are the mainstays. NAAT for chlamydia may be performed on swab specimens sampled from the cervix (women) or urethra (men), on self-collected vaginal swabs, or on voided urine. NAAT has been estimated to have a sensitivity of approximately 90% and a specificity of approximately 99%, regardless of sampling from a cervical swab or by urine specimen. In women seeking an STI clinic and a urine test is negative, a subsequent cervical swab has been estimated to be positive in approximately 2% of the time.

At present, the NAATs have regulatory approval only for testing urogenital specimens, although rapidly evolving research indicates that they may give reliable results on rectal specimens.

Because of improved test accuracy, ease of specimen management, convenience in specimen management, and ease of screening sexually active men and women, the NAATs have largely replaced culture, the historic gold standard for chlamydia diagnosis, and the non-amplified probe tests. The latter test is relatively insensitive, successfully detecting only 60–80% of infections in asymptomatic women, and often giving falsely positive results. Culture remains useful in selected circumstances and is currently the only assay approved for testing non-genital specimens. Other method also exist including: ligase chain reaction (LCR), direct fluorescent antibody resting, enzyme immunoassay, and cell culture.

For sexually active women who are not pregnant, screening is recommended in those under 25 and others at risk of infection. Risk factors include a history of chlamydial or other sexually transmitted infection, new or multiple sexual partners, and inconsistent condom use. For pregnant women, guidelines vary: screening women with age or other risk factors is recommended by the U.S. Preventive Services Task Force (USPSTF) (which recommends screening women under 25) and the American Academy of Family Physicians (which recommends screening women aged 25 or younger). The American College of Obstetricians and Gynecologists recommends screening all at risk, while the Centers for Disease Control and Prevention recommend universal screening of pregnant women. The USPSTF acknowledges that in some communities there may be other risk factors for infection, such as ethnicity. Evidence-based recommendations for screening initiation, intervals and termination are currently not possible. For men, the USPSTF concludes evidence is currently insufficient to determine if regular screening of men for chlamydia is beneficial. They recommend regular screening of men who are at increased risk for HIV or syphilis infection.

In the United Kingdom the National Health Service (NHS) aims to:


"C. trachomatis" infection can be effectively cured with antibiotics. Guidelines recommend azithromycin, doxycycline, erythromycin, levofloxacin or ofloxacin. Agents recommended during pregnancy include erythromycin or amoxicillin.

An option for treating sexual partners of those with chlamydia or gonorrhea include patient-delivered partner therapy (PDT or PDPT), which is the practice of treating the sex partners of index cases by providing prescriptions or medications to the patient to take to his/her partner without the health care provider first examining the partner.

Following treatment people should be tested again after three months to check for reinfection.

Globally, as of 2015, sexually transmitted chlamydia affects approximately 61 million people. It is more common in women (3.8%) than men (2.5%). In 2015 it resulted in about 200 deaths.

In the United States about 1.6 million cases were reported in 2016. The CDC estimates that if one includes unreported cases there are about 2.9 million each year. It affects around 2% of young people. Chlamydial infection is the most common bacterial sexually transmitted infection in the UK.

Chlamydia causes more than 250,000 cases of epididymitis in the U.S. each year. Chlamydia causes 250,000 to 500,000 cases of PID every year in the United States. Women infected with chlamydia are up to five times more likely to become infected with HIV, if exposed.



</doc>
<doc id="7038" url="https://en.wikipedia.org/wiki?curid=7038" title="Candidiasis">
Candidiasis

Candidiasis is a fungal infection due to any type of "Candida" (a type of yeast). When it affects the mouth, it is commonly called thrush. Signs and symptoms include white patches on the tongue or other areas of the mouth and throat. Other symptoms may include soreness and problems swallowing. When it affects the vagina, it is commonly called a yeast infection. Signs and symptoms include genital itching, burning, and sometimes a white "cottage cheese-like" discharge from the vagina. Less commonly the penis may be affected, resulting in itchiness. Very rarely, the infection may become invasive, spreading to other parts of the body. This may result in fevers along with other symptoms depending on the parts involved.
More than 20 types of "Candida" can cause infection with "Candida albicans" being the most common. Infections of the mouth are most common among children less than one month old, the elderly, and those with weak immune systems. Conditions that result in a weak immune system include HIV/AIDS, the medications used after organ transplantation, diabetes, and the use of corticosteroids. Other risks include dentures and following antibiotic therapy. Vaginal infections occur more commonly during pregnancy, in those with weak immune systems, and following antibiotic use. Risk factors for invasive candidiasis include being in an intensive care unit, following surgery, low birth weight infants, and those with weak immune systems.
Efforts to prevent infections of the mouth include the use of chlorhexidine mouth wash in those with poor immune function and washing out the mouth following the use of inhaled steroids. Little evidence supports probiotics for either prevention or treatment even among those with frequent vaginal infections. For infections of the mouth, treatment with topical clotrimazole or nystatin is usually effective. By mouth or intravenous fluconazole, itraconazole, or amphotericin B may be used if these do not work. A number of topical antifungal medications may be used for vaginal infections including clotrimazole. In those with widespread disease, an echinocandin such as caspofungin or micafungin is used. A number of weeks of intravenous amphotericin B may be used as an alternative. In certain groups at very high risk, antifungal medications may be used preventatively.
Infections of the mouth occur in about 6% of babies less than a month old. About 20% of those receiving chemotherapy for cancer and 20% of those with AIDS also develop the disease. About three-quarters of women have at least one yeast infection at some time during their lives. Widespread disease is rare except in those who have risk factors.

Signs and symptoms of candidiasis vary depending on the area affected. Most candidal infections result in minimal complications such as redness, itching, and discomfort, though complications may be severe or even fatal if left untreated in certain populations. In healthy (immunocompetent) persons, candidiasis is usually a localized infection of the skin, fingernails or toenails (onychomycosis), or mucosal membranes, including the oral cavity and pharynx (thrush), esophagus, and the genitalia (vagina, penis, etc.); less commonly in healthy individuals, the gastrointestinal tract, urinary tract, and respiratory tract are sites of candida infection.

In immunocompromised individuals, "Candida" infections in the esophagus occur more frequently than in healthy individuals and have a higher potential of becoming systemic, causing a much more serious condition, a fungemia called candidemia. Symptoms of esophageal candidiasis include difficulty swallowing, painful swallowing, abdominal pain, nausea, and vomiting.

Thrush is commonly seen in infants. It is not considered abnormal in infants unless it lasts longer than a few weeks.

Infection of the vagina or vulva may cause severe itching, burning, soreness, irritation, and a whitish or whitish-gray cottage cheese-like discharge. Symptoms of infection of the male genitalia (balanitis thrush) include red skin around the head of the penis, swelling, irritation, itchiness and soreness of the head of the penis, thick, lumpy discharge under the foreskin, unpleasant odour, difficulty retracting the foreskin (phimosis), and pain when passing urine or during sex.

Common symptoms of gastrointestinal candidiasis in healthy individuals are anal itching, belching, bloating, indigestion, nausea, diarrhea, gas, intestinal cramps, vomiting, and gastric ulcers. Perianal candidiasis can cause anal itching; the lesion can be erythematous, papular, or ulcerative in appearance, and it is not considered to be a sexually transmissible disease. Abnormal proliferation of the candida in the gut may lead to dysbiosis. While it is not yet clear, this alteration may be the source of symptoms generally described as the irritable bowel syndrome, and other gastrointestinal diseases.

"Candida" yeasts are generally present in healthy humans, frequently part of the human body's normal oral and intestinal flora, and particularly on the skin; however, their growth is normally limited by the human immune system and by competition of other microorganisms, such as bacteria occupying the same locations in the human body. 
"Candida" requires moisture for growth, notably on the skin. For example, wearing wet swimwear for long periods of time is believed to be a risk factor. In extreme cases, superficial infections of the skin or mucous membranes may enter into the bloodstream and cause systemic "Candida" infections.

Factors that increase the risk of candidiasis include HIV/AIDS, mononucleosis, cancer treatments, steroids, stress, antibiotic usage, diabetes, and nutrient deficiency. Hormone replacement therapy and infertility treatments may also be predisposing factors. Treatment with antibiotics can lead to eliminating the yeast's natural competitors for resources in the oral and intestinal flora; thereby increasing the severity of the condition. A weakened or undeveloped immune system or metabolic illnesses are significant predisposing factors of candidiasis. Almost 15% of people with weakened immune systems develop a systemic illness caused by "Candida" species. Diets high in simple carbohydrates have been found to affect rates of oral candidiases.

"C. albicans" was isolated from the vaginas of 19% of apparently healthy women, i.e., those who experienced few or no symptoms of infection. External use of detergents or douches or internal disturbances (hormonal or physiological) can perturb the normal vaginal flora, consisting of lactic acid bacteria, such as lactobacilli, and result in an overgrowth of "Candida" cells, causing symptoms of infection, such as local inflammation. Pregnancy and the use of oral contraceptives have been reported as risk factors. Diabetes mellitus and the use of antibiotics are also linked to increased rates of yeast infections.

In penile candidiasis, the causes include sexual intercourse with an infected individual, low immunity, antibiotics, and diabetes. Male genital yeast infections are less common, but a yeast infection on the penis caused from direct contact via sexual intercourse with an infected partner is not uncommon.

Symptoms of vaginal candidiasis are also present in the more common bacterial vaginosis; aerobic vaginitis is distinct and should be excluded in the differential diagnosis. In a 2002 study, only 33% of women who were self-treating for a yeast infection actually had such an infection, while most had either bacterial vaginosis or a mixed-type infection.

Diagnosis of a yeast infection is done either via microscopic examination or culturing. For identification by light microscopy, a scraping or swab of the affected area is placed on a microscope slide. A single drop of 10% potassium hydroxide (KOH) solution is then added to the specimen. The KOH dissolves the skin cells, but leaves the "Candida" cells intact, permitting visualization of pseudohyphae and budding yeast cells typical of many "Candida" species.

For the culturing method, a sterile swab is rubbed on the infected skin surface. The swab is then streaked on a culture medium. The culture is incubated at 37 °C (98.6 °F) for several days, to allow development of yeast or bacterial colonies. The characteristics (such as morphology and colour) of the colonies may allow initial diagnosis of the organism causing disease symptoms.

Respiratory, gastrointestinal, and esophageal candidiasis require an endoscopy to diagnose. For gastrointestinal candidiasis, it is necessary to obtain a 3–5 milliliter sample of fluid from the duodenum for fungal culture. The diagnosis of gastrointestinal candidiasis is based upon the culture containing in excess of 1,000 colony-forming units per milliliter.

Candidiasis may be divided into these types:

A diet that supports the immune system and is not high in simple carbohydrates contributes to a healthy balance of the oral and intestinal flora. While yeast infections are associated with diabetes, the level of blood sugar control may not affect the risk. Wearing cotton underwear may help to reduce the risk of developing skin and vaginal yeast infections, along with not wearing wet clothes for long periods of time.

Oral hygiene can help prevent oral candidiasis when people have a weakened immune system. For people undergoing cancer treatment, chlorhexidine mouthwash can prevent or reduce thrush. People who use inhaled corticosteroids can reduce the risk of developing oral candidiasis by rinsing the mouth with water or mouthwash after using the inhaler.

For women who experience recurrent yeast infections, there is limited evidence that oral or intravaginal probiotics help to prevent future infections. This includes either as pills or as yogurt.

Candidiasis is treated with antifungal medications; these include clotrimazole, nystatin, fluconazole, voriconazole, amphotericin B, and echinocandins. Intravenous fluconazole or an intravenous echinocandin such as caspofungin are commonly used to treat immunocompromised or critically ill individuals.

The 2016 revision of the clinical practice guideline for the management of candidiasis lists a large number of specific treatment regimens for "Candida" infections that involve different "Candida" species, forms of antifungal drug resistance, immune statuses, and infection localization and severity. Gastrointestinal candidiasis in immunocompetent individuals is treated with 100–200 mg fluconazole per day for 2–3 weeks.

Mouth and throat candidiasis are treated with antifungal medication. Oral candidiasis usually responds to topical treatments; otherwise, systemic antifungal medication may be needed for oral infections. Candidal skin infections in the skin folds (candidal intertrigo) typically respond well to topical antifungal treatments (e.g., nystatin or miconazole). Systemic treatment with antifungals by mouth is reserved for severe cases or if treatment with topical therapy is unsuccessful. Candida esophagitis may be treated orally or intravenously; for severe or azole-resistant esophageal candidiasis, treatment with amphotericin B may be necessary.

Vaginal yeast infections are typically treated with topical antifungal agents. A one-time dose of fluconazole is 90% effective in treating a vaginal yeast infection. For severe nonrecurring cases, several doses of fluconazole is recommended. Local treatment may include vaginal suppositories or medicated douches. Other types of yeast infections require different dosing. Gentian violet can be used for thrush in breastfeeding babies. "C. albicans" can develop resistance to fluconazole, this being more of an issue in those with HIV/AIDS who are often treated with multiple courses of fluconazole for recurrent oral infections.

For vaginal yeast infection in pregnancy, topical imidazole or triazole antifungals are considered the therapy of choice owing to available safety data. Systemic absorption of these topical formulations is minimal, posing little risk of transplacental transfer. In vaginal yeast infection in pregnancy, treatment with topical azole antifungals is recommended for 7 days instead of a shorter duration.

No benefit from probiotics has been found for active infections.

Systemic candidiasis occurs when Candida yeast enters the bloodstream and may spread (becoming disseminated candidiasis) to other organs, including the central nervous system, kidneys, liver, bones, muscles, joints, spleen, or eyes. Treatment typically consists of oral or intravenous antifungal medications. In candidal infections of the blood, intravenous fluconazole or an echinocandin such as caspofungin may be used. Amphotericin B is another option.

Among individuals being treated in intensive care units, the mortality rate is about 30–50% when systemic candidiasis develops.

Oral candidiasis is the most common fungal infection of the mouth, and it also represents the most common opportunistic oral infection in humans. In the Western Hemisphere, about 75% of females are affected at some time in their lives with a vaginal yeast infection.

Esophageal candidiasis is the most common esophageal infection in persons with AIDS and accounts for about 50% of all esophageal infections, often coexisting with other esophageal diseases. About two-thirds of people with AIDS and esophageal candidiasis also have oral candidiasis.

Candidal sepsis is rare. Candida is the fourth most common cause of bloodstream infections among hospital patients in the United States.

Descriptions of what sounds like oral thrush go back to the time of Hippocrates "circa" 460–370 BCE.

Vulvovaginal candidiasis was first described in 1849 by Wilkinson. In 1875, Haussmann demonstrated the causative organism in both vulvovaginal and oral candidiasis is the same.

With the advent of antibiotics following World War II, the rates of candidiasis increased. The rates then decreased in the 1950s following the development of nystatin.

The colloquial term "thrush" refers to the resemblance of the white flecks present in some forms of candidiasis ("e.g." pseudomembranous candidiasis) with the breast of the bird of the same name.<ref name="emedicine/medscape"></ref> The term candidosis is largely used in British English, and candidiasis in American English. "Candida" is also pronounced differently; in American English, the stress is on the "i", whereas in British English the stress is on the first syllable.

The genus "Candida" and species "C. albicans" were described by botanist Christine Marie Berkhout in her doctoral thesis at the University of Utrecht in 1923. Over the years, the classification of the genera and species has evolved. Obsolete names for this genus include "Mycotorula" and "Torulopsis". The species has also been known in the past as "Monilia albicans" and "Oidium albicans". The current classification is "nomen conservandum", which means the name is authorized for use by the International Botanical Congress (IBC).

The genus "Candida" includes about 150 different species; however, only a few are known to cause human infections. "C. albicans" is the most significant pathogenic species. Other species pathogenic in humans include "C. tropicalis", "C. glabrata", "C. krusei", "C. parapsilosis", "C. dubliniensis", and "C. lusitaniae".

The name "Candida" was proposed by Berkhout. It is from the Latin word "toga candida", referring to the white toga (robe) worn by candidates for the Senate of the ancient Roman republic. The specific epithet "albicans" also comes from Latin, "albicare" meaning "to whiten". These names refer to the generally white appearance of "Candida" species when cultured.

A 2005 publication noted that "a large pseudoscientific cult" has developed around the topic of "Candida", with claims up to one in three people are affected by yeast-related illness, particularly a condition called "Candidiasis hypersensitivity". Some practitioners of alternative medicine have promoted these purported conditions and sold dietary supplements as supposed cures; a number of them have been prosecuted. In 1990, alternative health vendor Nature's Way signed an FTC consent agreement not to misrepresent in advertising any self-diagnostic test concerning yeast conditions or to make any unsubstantiated representation concerning any food or supplement's ability to control yeast conditions, with a fine of $30,000 payable to the National Institutes of Health for research in genuine candidiasis.


</doc>

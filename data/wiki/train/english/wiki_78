<doc id="10243" url="https://en.wikipedia.org/wiki?curid=10243" title="Exxon Valdez oil spill">
Exxon Valdez oil spill

The "Exxon Valdez" oil spill occurred in Prince William Sound, Alaska, March 24, 1989, when "Exxon Valdez", an oil tanker owned by Exxon Shipping Company, bound for Long Beach, California, struck Prince William Sound's Bligh Reef at 12:04 am local time and spilled of crude oil over the next few days. It is considered to be one of the most devastating human-caused environmental disasters. The "Valdez" spill is the second largest in US waters, after the 2010 "Deepwater Horizon" oil spill, in terms of volume released. Prince William Sound's remote location, accessible only by helicopter, plane, or boat, made government and industry response efforts difficult and severely taxed existing response plans. The region is a habitat for salmon, sea otters, seals and seabirds. The oil, originally extracted at the Prudhoe Bay oil field, eventually covered of coastline, and of ocean.

According to official reports, the ship was carrying of oil, of which about were spilled into the Prince William Sound. An approximate figure of was a commonly accepted estimate of the spill's volume and has been used by the State of Alaska's "Exxon Valdez" Oil Spill Trustee Council, the National Oceanic and Atmospheric Administration and environmental groups such as Greenpeace and the Sierra Club.

Multiple factors have been identified as contributing to the incident:

Captain Joseph Hazelwood, who was widely reported to have been drinking heavily that night, was not at the controls when the ship struck the reef. However, as the senior officer, he was in command of the ship even though he was asleep in his bunk. In light of the other findings, investigative reporter Greg Palast stated in 2008, "Forget the drunken skipper fable. As to Captain Joe Hazelwood, he was below decks, sleeping off his bender. At the helm, the third mate never would have collided with Bligh Reef had he looked at his RAYCAS radar. But the radar was not turned on. In fact, the tanker's radar was left broken and disabled for more than a year before the disaster, and Exxon management knew it. It was just too expensive to fix and operate." Exxon blamed Captain Hazelwood for the grounding of the tanker.

Other factors, according to an MIT course entitled "Software System Safety" by Professor Nancy G. Leveson, included:
This disaster resulted in International Maritime Organization introducing comprehensive marine pollution prevention rules (MARPOL) through various conventions. The rules were ratified by member countries and, under International Ship Management rules, the ships are being operated with a common objective of "safer ships and cleaner oceans".

In 2009, "Exxon Valdez" Captain Joseph Hazelwood offered a "heartfelt apology" to the people of Alaska, suggesting he had been wrongly blamed for the disaster: "The true story is out there for anybody who wants to look at the facts, but that's not the sexy story and that's not the easy story," he said. Hazelwood said he felt Alaskans always gave him a fair shake.

Chemical dispersant, a surfactant and solvent mixture, was applied to the slick by a private company on March 24 with a helicopter. But the helicopter missed the target area. Scientific data on its toxicity were either thin or incomplete. In addition, public acceptance of a new, widespread chemical treatment was lacking. Landowners, fishing groups, and conservation organizations questioned the use of chemicals on hundreds of miles of shoreline when other alternatives may have been available."

According to a report by David Kirby for TakePart, the main component of the Corexit formulation used during cleanup, 2-butoxyethanol, was identified as "one of the agents that caused liver, kidney, lung, nervous system, and blood disorders among cleanup crews in Alaska following the 1989 "Exxon Valdez" spill.

Mechanical cleanup was started shortly afterwards using booms and skimmers, but the skimmers were not readily available during the first 24 hours following the spill, and thick oil and kelp tended to clog the equipment. Despite civilian insistence for a complete clean, only 10% of total oil was actually completely cleaned. Exxon was widely criticized for its slow response to cleaning up the disaster and John Devens, the mayor of Valdez, has said his community felt betrayed by Exxon's inadequate response to the crisis. More than 11,000 Alaska residents, along with some Exxon employees, worked throughout the region to try to restore the environment. 
Because Prince William Sound contained many rocky coves where the oil collected, the decision was made to displace it with high-pressure hot water. However, this also displaced and destroyed the microbial populations on the shoreline; many of these organisms (e.g. plankton) are the basis of the coastal marine food chain, and others (e.g. certain bacteria and fungi) are capable of facilitating the biodegradation of oil. At the time, both scientific advice and public pressure was to clean everything, but since then, a much greater understanding of natural and facilitated remediation processes has developed, due somewhat in part to the opportunity presented for study by the "Exxon Valdez" spill. Despite the extensive cleanup attempts, less than ten percent of the oil was recovered and a study conducted by NOAA determined that as of early 2007 more than of oil remain in the sandy soil of the contaminated shoreline, declining at a rate of less than 4% per year.
Both the long-term and short-term effects of the oil spill have been studied. Immediate effects included the deaths of 100,000 to as many as 250,000 seabirds, at least 2,800 sea otters, approximately 12 river otters, 300 harbor seals, 247 bald eagles, and 22 orcas, and an unknown number of salmon and herring.

In 2003, fourteen years after the spill, a team from the University of North Carolina found that the remaining oil was lasting far longer than anticipated, which in turn had resulted in more long-term loss of many species than had been expected. The researchers found that at only a few parts per billion, polycyclic aromatic hydrocarbons caused a long-term increase in mortality rates. They reported that "species as diverse as sea otters, harlequin ducks and killer whales suffered large, long-term losses and that oiled mussel beds and other tidal shoreline habitats will take an estimated 30 years to recover."

In 2006, a study done by the National Marine Fisheries Service in Juneau found that about of shoreline around Prince William Sound was still affected by the spill, with 101.6 tonnes of oil remaining in the area. Exxon Mobil denied any concerns over any remaining oil, stating that they anticipated a remaining fraction that they assert will not cause any long-term ecological impacts, according to the conclusions of the studies they had done: "We've done 350 peer-reviewed studies of Prince William Sound, and those studies conclude that Prince William Sound has recovered, it's healthy and it's thriving." However, in 2007 a NOAA study concluded that this contamination can produce chronic low-level exposure, discourage subsistence where the contamination is heavy, and decrease the "wilderness character" of the area.

The effects of the spill continued to be felt for many years afterwards. As of 2010 there were an estimated of Valdez crude oil still in Alaska's sand and soil, breaking down at a rate estimated at less than 4% per year.

On March 24, 2014, the twenty-fifth anniversary of the spill, NOAA scientists reported that some species seem to have recovered, with the sea otter the latest creature to return to pre-spill numbers. Scientists who have monitored the spill area for the last 25 years report that concern remains for one of two pods of local orca whales, with fears that one pod may eventually die out. Federal scientists estimate that between 16,000 and 21,000 US gallons (61 to 79 m) of oil remains on beaches in Prince William Sound and up to 450 miles (725 km) away. Some of the oil does not appear to have biodegraded at all. A USGS scientist who analyses the remaining oil along the coastline states that it remains among rocks and between tide marks. "The oil mixes with seawater and forms an emulsion...Left out, the surface crusts over but the inside still has the consistency of mayonnaise – or mousse." Alaska state senator Berta Gardner is urging Alaskan politicians to demand that the US government force ExxonMobil to pay the final $92 million (£57 million) still owed from the court settlement. The major part of the money would be spent to finish cleaning up oiled beaches and attempting to restore the crippled herring population.

In the case of "Exxon v. Baker", an Anchorage jury awarded $287 million for actual damages and $5 billion for punitive damages. To protect itself in case the judgment was affirmed, Exxon obtained a $4.8 billion credit line from J.P. Morgan & Co. J.P. Morgan created the first modern credit default swap in 1994, so that Morgan's would not have to hold as much money in reserve (8% of the loan under Basel I) against the risk of Exxon's default.

Meanwhile, Exxon appealed the ruling, and the 9th U.S. Circuit Court of Appeals ordered the original judge, Russel Holland, to reduce the punitive damages. On December 6, 2002, the judge announced that he had reduced the damages to $4 billion, which he concluded was justified by the facts of the case and was not grossly excessive. Exxon appealed again and the case returned to court to be considered in light of a recent Supreme Court ruling in a similar case, which caused Judge Holland to increase the punitive damages to $4.5 billion, plus interest.

After more appeals, and oral arguments heard by the 9th Circuit Court of Appeals on January 27, 2006, the damages award was cut to $2.5 billion on December 22, 2006. The court cited recent Supreme Court rulings relative to limits on punitive damages.

Exxon appealed again. On May 23, 2007, the 9th Circuit Court of Appeals denied ExxonMobil's request for a third hearing and let stand its ruling that Exxon owes $2.5 billion in punitive damages. Exxon then appealed to the Supreme Court, which agreed to hear the case. On February 27, 2008, the Supreme Court heard oral arguments for 90 minutes. Justice Samuel Alito, who at the time, owned between $100,000 and $250,000 in Exxon stock, recused himself from the case. In a decision issued June 25, 2008, Justice David Souter issued the judgment of the court, vacating the $2.5 billion award and remanding the case back to the lower court, finding that the damages were excessive with respect to maritime common law. Exxon's actions were deemed "worse than negligent but less than malicious." The punitive damages were further reduced to an amount of $507.5 million. The Court's ruling was that maritime punitive damages should not exceed the compensatory damages, supported by a peculiar precedent dating back from 1818. Senate Judiciary Committee Chairman Patrick J. Leahy has decried the ruling as "another in a line of cases where this Supreme Court has misconstrued congressional intent to benefit large corporations."

Exxon's official position was that punitive damages greater than $25 million were not justified because the spill resulted from an accident, and because Exxon spent an estimated $2 billion cleaning up the spill and a further $1 billion to settle related civil and criminal charges. Attorneys for the plaintiffs contended that Exxon bore responsibility for the accident because the company "put a drunk in charge of a tanker in Prince William Sound."

Exxon recovered a significant portion of clean-up and legal expenses through insurance claims associated with the grounding of the "Exxon Valdez". Also, in 1991, Exxon made a quiet, separate financial settlement of damages with a group of seafood producers known as the Seattle Seven for the disaster's effect on the Alaskan seafood industry. The agreement granted $63.75 million to the Seattle Seven, but stipulated that the seafood companies would have to repay almost all of any punitive damages awarded in other civil proceedings. The $5 billion in punitive damages was awarded later, and the Seattle Seven's share could have been as high as $750 million if the damages award had held. Other plaintiffs have objected to this secret arrangement, and when it came to light, Judge Holland ruled that Exxon should have told the jury at the start that an agreement had already been made, so the jury would know exactly how much Exxon would have to pay.

As of December 15, 2009, Exxon paid all owed $507.5 million punitive damages, including lawsuit costs, plus interest, which were further distributed to thousands of plaintiffs.

In October 1989, Exxon filed suit against the State of Alaska, charging that the government had interfered with Exxon's attempts to clean up the spill by refusing to approve the use of dispersant chemicals until the night of the 26th. The government disputed the claim, stating that there was a long-standing agreement to allow the use of dispersants to clean up spills, thus Exxon did not require permission to use them, and that in fact Exxon had not had enough dispersant on hand to effectively handle a spill of the size created by the "Valdez". Exxon filed claims in October 1990 against the Coast Guard, asking to be reimbursed for cleanup costs and damages awarded to plaintiffs in any lawsuits filed by the State of Alaska or the Federal Government against Exxon. The company claimed that the Coast Guard was "wholly or partially responsible" for the spill, because they had granted mariners' licenses to the crew of the Valdez, and because they had given the "Valdez" permission to leave regular shipping lanes to avoid ice. They also reiterated the claim that the Coast Guard had delayed cleanup by refusing to give permission to use chemical dispersants on the spill immediately.

The Oil Spill Recovery Institute was formed after United States Congress approved it to seek a solution. Collaborating with InnoCentive they found a partial solution for the flow of oil.

A report by the US National Response Team summarized the event and made a number of recommendations, such as changes to the work patterns of Exxon crew in order to address the causes of the accident.

In response to the spill, the United States Congress passed the Oil Pollution Act of 1990 (OPA). The legislation included a clause that prohibits any vessel that, after March 22, 1989, has caused an oil spill of more than in any marine area, from operating in Prince William Sound.

In April 1998, the company argued in a legal action against the Federal government that the ship should be allowed back into Alaskan waters. Exxon claimed OPA was effectively a bill of attainder, a regulation that was unfairly directed at Exxon alone. In 2002, the 9th Circuit Court of Appeals ruled against Exxon. As of 2002, OPA had prevented 18 ships from entering Prince William Sound.

OPA also set a schedule for the gradual phase in of a double hull design, providing an additional layer between the oil tanks and the ocean. While a double hull would likely not have prevented the "Valdez" disaster, a Coast Guard study estimated that it would have cut the amount of oil spilled by 60 percent.

The "Exxon Valdez" supertanker was towed to San Diego, arriving on July 10. Repairs began on July 30. Approximately of steel were removed and replaced. In June 1990 the tanker, renamed "S/R Mediterranean", left harbor after $30 million of repairs. It was still sailing as of January 2010, registered in Panama. The vessel was then owned by a Hong Kong company, who operated it under the name "Oriental Nicety". In August 2012, it was beached at Alang, India and dismantled.

In the aftermath of the spill, Alaska governor Steve Cowper issued an executive order requiring two tugboats to escort every loaded tanker from Valdez out through Prince William Sound to Hinchinbrook Entrance. As the plan evolved in the 1990s, one of the two routine tugboats was replaced with a Escort Response Vehicle (ERV). Tankers at Valdez are no longer single-hulled. Congress enacted legislation requiring all tankers to be double-hulled as of 2015.

In 1991, following the collapse of the local marine population (particularly clams, herring and seals) the Chugach Alaska Corporation, an Alaska Native Corporation, filed for Chapter 11 bankruptcy protection. It has since recovered.

According to several studies funded by the state of Alaska, the spill had both short-term and long-term economic effects. These included the loss of recreational sports, fisheries, reduced tourism, and an estimate of what economists call "existence value", which is the value to the public of a pristine Prince William Sound.

The economy of the city of Cordova, Alaska was adversely affected after the spill damaged stocks of salmon and herring in the area. The village of Chenega was transformed into an emergency base and media outlet.The local villagers had to cope with a tripling of their population from 80 to 250. When asked how they felt about the situation, a village councillor noted that they were too shocked and busy to be depressed; others emphasized the human costs of leaving children unattended while their parents worked to clean up. Many Native Americans were worried that too much time was spent on the fishery and not enough on the land that supports subsidence hunting.

In 2010, a CNN report alleged that many oil spill cleanup workers involved in the "Exxon Valdez" response had subsequently become sick. Anchorage lawyer Dennis Mestas found that this was true of 6,722 of 11,000 worker files he was able to inspect. Access to the records was controlled by Exxon. Exxon responded in a statement to CNN: 

After 20 years, there is no evidence suggesting that either cleanup workers or the residents of the communities affected by the Valdez spill have had any adverse health effects as a result of the spill or its cleanup.

In 1992, Exxon released a video titled "Scientists and the Alaska Oil Spill", to be distributed to schools. Dr. Michael Fry called it a piece of "corporate propaganda".

In December 1994, the Unabomber assassinated Burson-Marsteller executive Thomas Mosser, accusing him of having "helped Exxon clean up its public image after the "Exxon Valdez" incident". The PR company claimed not to have been contracted during the actual crisis.

Several weeks after the spill, "Saturday Night Live" aired a pointed sketch featuring Kevin Nealon, Phil Hartman, and Victoria Jackson as cleanup workers struggling to scrub the oil off of animals and rocks on a beach in Prince William Sound.

In the 1995 film "Waterworld", the "Exxon Valdez" is the flagship of the movie's villain, "The Deacon," the leader of a band of scavenging raiders. In the ship is a portrait of their patron saint, Joseph Hazelwood.

In the second Forrest Gump novel, Gump & Co. by Winston Groom, Gump commandeers the Exxon Valdez and accidently crashes it.




</doc>
<doc id="10244" url="https://en.wikipedia.org/wiki?curid=10244" title="Édouard de Pomiane">
Édouard de Pomiane

Édouard Alexandre de Pomiane, sometimes Édouard Pozerski (20 April 1875 in Paris – 26 January 1964 in Paris) was a French scientist, radio broadcaster and food writer.
His parents emigrated from Poland in 1863, changed their name from "Pozerski" to "de Pomiane", and became French citizens.

De Pomiane worked as a physician at the Institut Pasteur in Paris, where he gave Félix d'Herelle a place to work on bacteriophages.

His best known works that have been translated into English are "Cooking in Ten Minutes" and "Cooking with Pomiane". His writing was remarkable in its time for its directness (he frequently uses a strange second-person voice, telling you—the reader—what you are seeing and smelling as you follow a recipe) and for his general disdain for upper-class elaborate French cuisine. He travelled widely and quite a few of his recipes are from abroad. His recipes often take pains to demystify cooking by explaining the chemical processes at work.

"Vingt Plats Qui Donnent Goutte"IMG_7221.jpeg 1935 edition.



</doc>
<doc id="10245" url="https://en.wikipedia.org/wiki?curid=10245" title="Edward VI of England">
Edward VI of England

Edward VI (12 October 1537 – 6 July 1553) was King of England and Ireland from 28 January 1547 until his death. He was crowned on 20 February at the age of nine. Edward was the son of Henry VIII and Jane Seymour, and England's first monarch to be raised as a Protestant. During his reign, the realm was governed by a Regency Council because he never reached his majority. The Council was first led by his uncle Edward Seymour, 1st Duke of Somerset (1547–1549), and then by John Dudley, 1st Earl of Warwick (1550–1553), from 1551 Duke of Northumberland.
Edward's reign was marked by economic problems and social unrest that in 1549 erupted into riot and rebellion. An expensive war with Scotland, at first successful, ended with military withdrawal from Scotland and Boulogne-sur-Mer in exchange for peace. The transformation of the Church of England into a recognisably Protestant body also occurred under Edward, who took great interest in religious matters. Although his father, HenryVIII, had severed the link between the Church and Rome, HenryVIII had never permitted the renunciation of Catholic doctrine or ceremony. It was during Edward's reign that Protestantism was established for the first time in England with reforms that included the abolition of clerical celibacy and the Mass and the imposition of compulsory services in English.

In February 1553, at age 15, Edward fell ill. When his sickness was discovered to be terminal, he and his Council drew up a "Devise for the Succession", to prevent the country's return to Catholicism. Edward named his first cousin once removed, Lady Jane Grey, as his heir, excluding his half-sisters, Mary and Elizabeth. This decision was disputed following Edward's death, and Jane was deposed by Mary nine days after becoming queen. During her reign, Mary reversed Edward's Protestant reforms, which nonetheless became the basis of the Elizabethan Religious Settlement of 1559.

Edward was born on 12 October 1537 in his mother's room inside Hampton Court Palace, in Middlesex. He was the son of King Henry VIII by his third wife, Jane Seymour. Throughout the realm, the people greeted the birth of a male heir, "whom we hungered for so long", with joy and relief. "Te Deums" were sung in churches, bonfires lit, and "their was shott at the Tower that night above two thousand gonnes". Queen Jane, appearing to recover quickly from the birth, sent out personally signed letters announcing the birth of "a Prince, conceived in most lawful matrimony between my Lord the King's Majesty and us". Edward was christened on 15 October, with his half-sisters, the 21-year-old Lady Mary as godmother and the 4-year-old Lady Elizabeth carrying the chrisom; and the Garter King of Arms proclaimed him as Duke of Cornwall and Earl of Chester. The Queen, however, fell ill on 23 October from presumed postnatal complications, and died the following night. Henry VIII wrote to Francis I of France that "Divine Providence ... hath mingled my joy with bitterness of the death of her who brought me this happiness".

Edward was a healthy baby who suckled strongly from the outset. His father was delighted with him; in May 1538, Henry was observed "dallying with him in his arms ... and so holding him in a window to the sight and great comfort of the people". That September, the Lord Chancellor, Lord Audley, reported Edward's rapid growth and vigour; and other accounts describe him as a tall and merry child. The tradition that Edward VI was a sickly boy has been challenged by more recent historians. At the age of four, he fell ill with a life-threatening "quartan fever", but, despite occasional illnesses and poor eyesight, he enjoyed generally good health until the last six months of his life.

Edward was initially placed in the care of Margaret Bryan, "lady mistress" of the prince's household. She was succeeded by Blanche Herbert, Lady Troy. Until the age of six, Edward was brought up, as he put it later in his "Chronicle", "among the women". The formal royal household established around Edward was, at first, under Sir William Sidney, and later Sir Richard Page, stepfather of Edward Seymour's wife, Anne Stanhope. Henry demanded exacting standards of security and cleanliness in his son's household, stressing that Edward was "this whole realm's most precious jewel". Visitors described the prince, who was lavishly provided with toys and comforts, including his own troupe of minstrels, as a contented child.

From the age of six, Edward began his formal education under Richard Cox and John Cheke, concentrating, as he recalled himself, on "learning of tongues, of the scripture, of philosophy, and all liberal sciences". He received tuition from Elizabeth's tutor, Roger Ascham, and Jean Belmain, learning French, Spanish and Italian. In addition, he is known to have studied geometry and learned to play musical instruments, including the lute and the virginals. He collected globes and maps and, according to coinage historian C. E. Challis, developed a grasp of monetary affairs that indicated a high intelligence. Edward's religious education is assumed to have favoured the reforming agenda. His religious establishment was probably chosen by Archbishop Thomas Cranmer, a leading reformer. Both Cox and Cheke were "reformed" Catholics or Erasmians and later became Marian exiles. By 1549, Edward had written a treatise on the pope as Antichrist and was making informed notes on theological controversies. Many aspects of Edward's religion were essentially Catholic in his early years, including celebration of the mass and reverence for images and relics of the saints.

Both Edward's sisters were attentive to their brother and often visited him – on one occasion, Elizabeth gave him a shirt "of her own working". Edward "took special content" in Mary's company, though he disapproved of her taste for foreign dances; "I love you most", he wrote to her in 1546. In 1543, Henry invited his children to spend Christmas with him, signalling his reconciliation with his daughters, whom he had previously illegitimised and disinherited. The following spring, he restored them to their place in the succession with a Third Succession Act, which also provided for a regency council during Edward's minority. This unaccustomed family harmony may have owed much to the influence of Henry's new wife, Catherine Parr, of whom Edward soon became fond. He called her his "most dear mother" and in September 1546 wrote to her: "I received so many benefits from you that my mind can hardly grasp them."

Other children were brought to play with Edward, including the granddaughter of Edward's chamberlain, Sir William Sidney, who in adulthood recalled the prince as "a marvellous sweet child, of very mild and generous condition". Edward was educated with sons of nobles, "appointed to attend upon him" in what was a form of miniature court. Among these, Barnaby Fitzpatrick, son of an Irish peer, became a close and lasting friend. Edward was more devoted to his schoolwork than his classmates and seems to have outshone them, motivated to do his "duty" and compete with his sister Elizabeth's academic prowess. Edward's surroundings and possessions were regally splendid: his rooms were hung with costly Flemish tapestries, and his clothes, books, and cutlery were encrusted with precious jewels and gold. Like his father, Edward was fascinated by military arts, and many of his portraits show him wearing a gold dagger with a jewelled hilt, in imitation of Henry. Edward's "Chronicle" enthusiastically details English military campaigns against Scotland and France, and adventures such as John Dudley's near capture at Musselburgh in 1547.

On 1 July 1543, Henry VIII signed the Treaty of Greenwich with the Scots, sealing the peace with Edward's betrothal to the seven-month-old Mary, Queen of Scots. The Scots were in a weak bargaining position after their defeat at Solway Moss the previous November, and Henry, seeking to unite the two realms, stipulated that Mary be handed over to him to be brought up in England. When the Scots repudiated the treaty in December 1543 and renewed their alliance with France, Henry was enraged. In April 1544, he ordered Edward's uncle, Edward Seymour, Earl of Hertford, to invade Scotland and "put all to fire and sword, burn Edinburgh town, so razed and defaced when you have sacked and gotten what ye can of it, as there may remain forever a perpetual memory of the vengeance of God lightened upon [them] for their falsehood and disloyalty". Seymour responded with the most savage campaign ever launched by the English against the Scots. The war, which continued into Edward's reign, has become known as "The Rough Wooing".

The nine-year-old Edward wrote to his father and stepmother on 10 January 1547 from Hertford thanking them for his new year's gift of their portraits from life. By 28 January 1547, Henry VIII was dead. Those close to the throne, led by Edward Seymour and William Paget, agreed to delay the announcement of the king's death until arrangements had been made for a smooth succession. Seymour and Sir Anthony Browne, the Master of the Horse, rode to collect Edward from Hertford and brought him to Enfield, where Lady Elizabeth was living. He and Elizabeth were then told of the death of their father and heard a reading of the will.

The Lord Chancellor, Thomas Wriothesley, announced Henry's death to parliament on 31 January, and general proclamations of Edward's succession were ordered. The new king was taken to the Tower of London, where he was welcomed with "great shot of ordnance in all places there about, as well out of the Tower as out of the ships". The following day, the nobles of the realm made their obeisance to Edward at the Tower, and Seymour was announced as Protector. Henry VIII was buried at Windsor on 16 February, in the same tomb as Jane Seymour, as he had wished.

Edward VI was crowned at Westminster Abbey four days later on Sunday 20 February. The ceremonies were shortened, because of the "tedious length of the same which should weary and be hurtsome peradventure to the King's majesty, being yet of tender age", and also because the Reformation had rendered some of them inappropriate.

On the eve of the coronation, Edward progressed on horseback from the Tower to the Palace of Westminster through thronging crowds and pageants, many based on the pageants for a previous boy king, Henry VI. He laughed at a Spanish tightrope walker who "tumbled and played many pretty toys" outside St Paul's Cathedral.

At the coronation service, Cranmer affirmed the royal supremacy and called Edward a second Josiah, urging him to continue the reformation of the Church of England, "the tyranny of the Bishops of Rome banished from your subjects, and images removed". After the service, Edward presided at a banquet in Westminster Hall, where, he recalled in his "Chronicle", he dined with his crown on his head.

Henry VIII's will named sixteen executors, who were to act as Edward's Council until he reached the age of eighteen. These executors were supplemented by twelve men "of counsail" who would assist the executors when called on. The final state of Henry VIII's will has been the subject of controversy. Some historians suggest that those close to the king manipulated either him or the will itself to ensure a share-out of power to their benefit, both material and religious. In this reading, the composition of the Privy Chamber shifted towards the end of 1546 in favour of the reforming faction. In addition, two leading conservative Privy Councillors were removed from the centre of power.

Stephen Gardiner was refused access to Henry during his last months. Thomas Howard, 3rd Duke of Norfolk, found himself accused of treason; the day before the king's death his vast estates were seized, making them available for redistribution, and he spent the whole of Edward's reign in the Tower of London. Other historians have argued that Gardiner's exclusion was based on non-religious matters, that Norfolk was not noticeably conservative in religion, that conservatives remained on the Council, and that the radicalism of such men as Sir Anthony Denny, who controlled the dry stamp that replicated the king's signature, is debatable.

Whatever the case, Henry's death was followed by a lavish hand-out of lands and honours to the new power group. The will contained an "unfulfilled gifts" clause, added at the last minute, which allowed Henry's executors to freely distribute lands and honours to themselves and the court, particularly to Edward Seymour, 1st Earl of Hertford, the new king's uncle who became Lord Protector of the Realm, Governor of the King's Person, and Duke of Somerset.

In fact, Henry VIII's will did not provide for the appointment of a Protector. It entrusted the government of the realm during his son's minority to a Regency Council that would rule collectively, by majority decision, with "like and equal charge". Nevertheless, a few days after Henry's death, on 4 February, the executors chose to invest almost regal power in Edward Seymour, now Duke of Somerset. Thirteen out of the sixteen (the others being absent) agreed to his appointment as Protector, which they justified as their joint decision "by virtue of the authority" of Henry's will. Somerset may have done a deal with some of the executors, who almost all received hand-outs. He is known to have done so with William Paget, private secretary to Henry VIII, and to have secured the support of Sir Anthony Browne of the Privy Chamber.

Somerset's appointment was in keeping with historical precedent, and his eligibility for the role was reinforced by his military successes in Scotland and France. In March 1547, he secured letters patent from King Edward granting him the almost monarchical right to appoint members to the Privy Council himself and to consult them only when he wished. In the words of historian Geoffrey Elton, "from that moment his autocratic system was complete". He proceeded to rule largely by proclamation, calling on the Privy Council to do little more than rubber-stamp his decisions.

Somerset's takeover of power was smooth and efficient. The imperial ambassador, François van der Delft, reported that he "governs everything absolutely", with Paget operating as his secretary, though he predicted trouble from John Dudley, Viscount Lisle, who had recently been raised to Earl of Warwick in the share-out of honours. In fact, in the early weeks of his Protectorate, Somerset was challenged only by the Chancellor, Thomas Wriothesley, whom the Earldom of Southampton had evidently failed to buy off, and by his own brother. Wriothesley, a religious conservative, objected to Somerset's assumption of monarchical power over the Council. He then found himself abruptly dismissed from the chancellorship on charges of selling off some of his offices to delegates.

Somerset faced less manageable opposition from his younger brother Thomas Seymour, who has been described as a "worm in the bud". As King Edward's uncle, Thomas Seymour demanded the governorship of the king's person and a greater share of power. Somerset tried to buy his brother off with a barony, an appointment to the Lord Admiralship, and a seat on the Privy Council—but Thomas was bent on scheming for power. He began smuggling pocket money to King Edward, telling him that Somerset held the purse strings too tight, making him a "beggarly king". He also urged him to throw off the Protector within two years and "bear rule as other kings do"; but Edward, schooled to defer to the Council, failed to co-operate. In the Spring of 1547, using Edward's support to circumvent Somerset's opposition, Thomas Seymour secretly married Henry VIII's widow Catherine Parr, whose Protestant household included the 11-year-old Lady Jane Grey and the 13-year-old Lady Elizabeth.

In summer 1548, a pregnant Catherine Parr discovered Thomas Seymour embracing Lady Elizabeth. As a result, Elizabeth was removed from Catherine Parr's household and transferred to Sir Anthony Denny's. That September, Catherine Parr died shortly after childbirth, and Thomas Seymour promptly resumed his attentions to Elizabeth by letter, planning to marry her. Elizabeth was receptive, but, like Edward, unready to agree to anything unless permitted by the Council. In January 1549, the Council had Thomas Seymour arrested on various charges, including embezzlement at the Bristol mint. King Edward, whom Seymour was accused of planning to marry to Lady Jane Grey, himself testified about the pocket money. Lack of clear evidence for treason ruled out a trial, so Seymour was condemned instead by an Act of Attainder and beheaded on 20 March 1549.

Somerset's only undoubted skill was as a soldier, which he had proven on expeditions to Scotland and in the defence of Boulogne-sur-Mer in 1546. From the first, his main interest as Protector was the war against Scotland. After a crushing victory at the Battle of Pinkie Cleugh in September 1547, he set up a network of garrisons in Scotland, stretching as far north as Dundee. His initial successes, however, were followed by a loss of direction, as his aim of uniting the realms through conquest became increasingly unrealistic. The Scots allied with France, who sent reinforcements for the defence of Edinburgh in 1548. The Queen of Scots was moved to France, where she was betrothed to the Dauphin. The cost of maintaining the Protector's massive armies and his permanent garrisons in Scotland also placed an unsustainable burden on the royal finances. A French attack on Boulogne in August 1549 at last forced Somerset to begin a withdrawal from Scotland.

During 1548, England was subject to social unrest. After April 1549, a series of armed revolts broke out, fuelled by various religious and agrarian grievances. The two most serious rebellions, which required major military intervention to put down, were in Devon and Cornwall and in Norfolk. The first, sometimes called the Prayer Book Rebellion, arose from the imposition of Protestantism, and the second, led by a tradesman called Robert Kett, mainly from the encroachment of landlords on common grazing ground. A complex aspect of the social unrest was that the protesters believed they were acting legitimately against enclosing landlords with the Protector's support, convinced that the landlords were the lawbreakers.

The same justification for outbreaks of unrest was voiced throughout the country, not only in Norfolk and the west. The origin of the popular view of Somerset as sympathetic to the rebel cause lies partly in his series of sometimes liberal, often contradictory, proclamations, and partly in the uncoordinated activities of the commissions he sent out in 1548 and 1549 to investigate grievances about loss of tillage, encroachment of large sheep flocks on common land, and similar issues. Somerset's commissions were led by an evangelical M.P. called John Hales, whose socially liberal rhetoric linked the issue of enclosure with Reformation theology and the notion of a godly commonwealth. Local groups often assumed that the findings of these commissions entitled them to act against offending landlords themselves. King Edward wrote in his "Chronicle" that the 1549 risings began "because certain commissions were sent down to pluck down enclosures".

Whatever the popular view of Somerset, the disastrous events of 1549 were taken as evidence of a colossal failure of government, and the Council laid the responsibility at the Protector's door. In July 1549, Paget wrote to Somerset: "Every man of the council have misliked your proceedings ... would to God, that, at the first stir you had followed the matter hotly, and caused justice to be ministered in solemn fashion to the terror of others ...".

The sequence of events that led to Somerset's removal from power has often been called a "coup d'état". By 1 October 1549, Somerset had been alerted that his rule faced a serious threat. He issued a proclamation calling for assistance, took possession of the king's person, and withdrew for safety to the fortified Windsor Castle, where Edward wrote, "Me thinks I am in prison". Meanwhile, a united Council published details of Somerset's government mismanagement. They made clear that the Protector's power came from them, not from Henry VIII's will. On 11 October, the Council had Somerset arrested and brought the king to Richmond. Edward summarised the charges against Somerset in his "Chronicle": "ambition, vainglory, entering into rash wars in mine youth, negligent looking on Newhaven, enriching himself of my treasure, following his own opinion, and doing all by his own authority, etc." In February 1550, John Dudley, Earl of Warwick, emerged as the leader of the Council and, in effect, as Somerset's successor. Although Somerset was released from the Tower and restored to the Council, he was executed for felony in January 1552 after scheming to overthrow Dudley's regime. Edward noted his uncle's death in his "Chronicle": "the duke of Somerset had his head cut off upon Tower Hill between eight and nine o'clock in the morning".

Historians contrast the efficiency of Somerset's takeover of power, in which they detect the organising skills of allies such as Paget, the "master of practices", with the subsequent ineptitude of his rule. By autumn 1549, his costly wars had lost momentum, the crown faced financial ruin, and riots and rebellions had broken out around the country. Until recent decades, Somerset's reputation with historians was high, in view of his many proclamations that appeared to back the common people against a rapacious landowning class. More recently, however, he has often been portrayed as an arrogant and aloof ruler, lacking in political and administrative skills.

In contrast, Somerset's successor John Dudley, Earl of Warwick, made Duke of Northumberland in 1551, was once regarded by historians merely as a grasping schemer who cynically elevated and enriched himself at the expense of the crown. Since the 1970s, the administrative and economic achievements of his regime have been recognised, and he has been credited with restoring the authority of the royal Council and returning the government to an even keel after the disasters of Somerset's protectorate.

The Earl of Warwick's rival for leadership of the new regime was Thomas Wriothesley, 1st Earl of Southampton, whose conservative supporters had allied with Dudley's followers to create a unanimous Council, which they, and observers such as the Holy Roman Emperor Charles V's ambassador, expected to reverse Somerset's policy of religious reform. Warwick, on the other hand, pinned his hopes on the king's strong Protestantism and, claiming that Edward was old enough to rule in person, moved himself and his people closer to the king, taking control of the Privy Chamber. Paget, accepting a barony, joined Warwick when he realised that a conservative policy would not bring the emperor onto the English side over Boulogne. Southampton prepared a case for executing Somerset, aiming to discredit Warwick through Somerset's statements that he had done all with Warwick's co-operation. As a counter-move, Warwick convinced parliament to free Somerset, which it did on 14 January 1550. Warwick then had Southampton and his followers purged from the Council after winning the support of Council members in return for titles, and was made Lord President of the Council and great master of the king's household. Although not called a Protector, he was now clearly the head of the government.

As Edward was growing up, he was able to understand more and more government business. However, his actual involvement in decisions has long been a matter of debate, and during the 20th century, historians have presented the whole gamut of possibilities, "balanc[ing] an articulate puppet against a mature, precocious, and essentially adult king", in the words of Stephen Alford. A special "Counsel for the Estate" was created when Edward was fourteen. Edward chose the members himself. In the weekly meetings with this Council, Edward was "to hear the debating of things of most importance". A major point of contact with the king was the Privy Chamber, and there Edward worked closely with William Cecil and William Petre, the Principal Secretaries. The king's greatest influence was in matters of religion, where the Council followed the strongly Protestant policy that Edward favoured.

The Duke of Northumberland's mode of operation was very different from Somerset's. Careful to make sure he always commanded a majority of councillors, he encouraged a working council and used it to legitimatise his authority. Lacking Somerset's blood-relationship with the king, he added members to the Council from his own faction in order to control it. He also added members of his family to the royal household. He saw that to achieve personal dominance, he needed total procedural control of the Council. In the words of historian John Guy, "Like Somerset, he became quasi-king; the difference was that he managed the bureaucracy on the pretence that Edward had assumed full sovereignty, whereas Somerset had asserted the right to near-sovereignty as Protector".
Warwick's war policies were more pragmatic than Somerset's, and they have earned him criticism for weakness. In 1550, he signed a peace treaty with France that agreed to withdrawal from Boulogne and recalled all English garrisons from Scotland. In 1551, Edward was betrothed to Elisabeth of Valois, King Henry II's daughter. In practice, he realised that England could no longer support the cost of wars. At home, he took measures to police local unrest. To forestall future rebellions, he kept permanent representatives of the crown in the localities, including lords lieutenant, who commanded military forces and reported back to central government.

Working with William Paulet and Walter Mildmay, Warwick tackled the disastrous state of the kingdom's finances. However, his regime first succumbed to the temptations of a quick profit by further debasing the coinage. The economic disaster that resulted caused Warwick to hand the initiative to the expert Thomas Gresham. By 1552, confidence in the coinage was restored, prices fell, and trade at last improved. Though a full economic recovery was not achieved until Elizabeth's reign, its origins lay in the Duke of Northumberland's policies. The regime also cracked down on widespread embezzlement of government finances, and carried out a thorough review of revenue collection practices, which has been called "one of the more remarkable achievements of Tudor administration".

In the matter of religion, the regime of Northumberland followed the same policy as that of Somerset, supporting an increasingly vigorous programme of reform. Although Edward VI's practical influence on government was limited, his intense Protestantism made a reforming administration obligatory; his succession was managed by the reforming faction, who continued in power throughout his reign. The man Edward trusted most, Thomas Cranmer, Archbishop of Canterbury, introduced a series of religious reforms that revolutionised the English church from one that—while rejecting papal supremacy—remained essentially Catholic, to one that was institutionally Protestant. The confiscation of church property that had begun under Henry VIII resumed under Edward—notably with the dissolution of the chantries—to the great monetary advantage of the crown and the new owners of the seized property. Church reform was therefore as much a political as a religious policy under Edward VI. By the end of his reign, the church had been financially ruined, with much of the property of the bishops transferred into lay hands.

The religious convictions of both Somerset and Northumberland have proved elusive for historians, who are divided on the sincerity of their Protestantism. There is less doubt, however, about the religious fervour of King Edward, who was said to have read twelve chapters of scripture daily and enjoyed sermons, and was commemorated by John Foxe as a "godly imp". Edward was depicted during his life and afterwards as a new Josiah, the biblical king who destroyed the idols of Baal. He could be priggish in his anti-Catholicism and once asked Catherine Parr to persuade Lady Mary "to attend no longer to foreign dances and merriments which do not become a most Christian princess". Edward's biographer Jennifer Loach cautions, however, against accepting too readily the pious image of Edward handed down by the reformers, as in John Foxe's influential "Acts and Monuments", where a woodcut depicts the young king listening to a sermon by Hugh Latimer. In the early part of his life, Edward conformed to the prevailing Catholic practices, including attendance at mass: but he became convinced, under the influence of Cranmer and the reformers among his tutors and courtiers, that "true" religion should be imposed in England.

The English Reformation advanced under pressure from two directions: from the traditionalists on the one hand and the zealots on the other, who led incidents of iconoclasm (image-smashing) and complained that reform did not go far enough. Reformed doctrines were made official, such as justification by faith alone and communion for laity as well as clergy in both kinds, of bread and wine. The Ordinal of 1550 replaced the divine ordination of priests with a government-run appointment system, authorising ministers to preach the gospel and administer the sacraments rather than, as before, "to offer sacrifice and celebrate mass both for the living and the dead". Cranmer set himself the task of writing a uniform liturgy in English, detailing all weekly and daily services and religious festivals, to be made compulsory in the first Act of Uniformity of 1549. The "Book of Common Prayer" of 1549, intended as a compromise, was attacked by traditionalists for dispensing with many cherished rituals of the liturgy, such as the elevation of the bread and wine, while some reformers complained about the retention of too many "popish" elements, including vestiges of sacrificial rites at communion. The prayer book was also opposed by many senior Catholic clerics, including Stephen Gardiner, Bishop of Winchester, and Edmund Bonner, Bishop of London, who were both imprisoned in the Tower and, along with others, deprived of their sees.

After 1551, the Reformation advanced further, with the approval and encouragement of Edward, who began to exert more personal influence in his role as Supreme Head of the church. The new changes were also a response to criticism from such reformers as John Hooper, Bishop of Gloucester, and the Scot John Knox, who was employed as a minister in Newcastle upon Tyne under the Duke of Northumberland and whose preaching at court prompted the king to oppose kneeling at communion. Cranmer was also influenced by the views of the continental reformer Martin Bucer, who died in England in 1551, by Peter Martyr, who was teaching at Oxford, and by other foreign theologians. The progress of the Reformation was further speeded by the consecration of more reformers as bishops. In the winter of 1551–52, Cranmer rewrote the "Book of Common Prayer" in less ambiguous reformist terms, revised canon law, and prepared a doctrinal statement, the Forty-two Articles, to clarify the practice of the reformed religion, particularly in the divisive matter of the communion service. Cranmer's formulation of the reformed religion, finally divesting the communion service of any notion of the real presence of God in the bread and the wine, effectively abolished the mass. According to Elton, the publication of Cranmer's revised prayer book in 1552, supported by a second Act of Uniformity, "marked the arrival of the English Church at Protestantism". The prayer book of 1552 remains the foundation of the Church of England's services. However, Cranmer was unable to implement all these reforms once it became clear in spring 1553 that King Edward, upon whom the whole Reformation in England depended, was dying.

In February 1553, Edward VI became ill, and by June, after several improvements and relapses, he was in a hopeless condition. The king's death and the succession of his Catholic half-sister Mary would jeopardise the English Reformation, and Edward's Council and officers had many reasons to fear it. Edward himself opposed Mary's succession, not only on religious grounds but also on those of legitimacy and male inheritance, which also applied to Elizabeth. He composed a draft document, headed "My devise for the succession", in which he undertook to change the succession, most probably inspired by his father Henry VIII's precedent. He passed over the claims of his half-sisters and, at last, settled the Crown on his first cousin once removed, the 16-year-old Lady Jane Grey, who on 25 May 1553 had married Lord Guilford Dudley, a younger son of the Duke of Northumberland. In the document he writes:

In his document Edward provided, in case of "lack of issue of my body", for the succession of male heirs only, that is, Jane Grey's mother's male heirs, Jane's, or her sisters'. As his death approached and possibly persuaded by Northumberland, he altered the wording so that Jane and her sisters themselves should be able to succeed. Yet Edward conceded Jane's right only as an exception to male rule, demanded by reality, an example not to be followed if Jane or her sisters had only daughters. In the final document both Mary and Elizabeth were excluded because of bastardy; since both had been declared bastards under Henry VIII and never made legitimate again, this reason could be advanced for both sisters. The provisions to alter the succession directly contravened Henry VIII's Third Succession Act of 1543 and have been described as bizarre and illogical.

In early June, Edward personally supervised the drafting of a clean version of his devise by lawyers, to which he lent his signature "in six several places." Then, on 15 June he summoned high ranking judges to his sickbed, commanding them on their allegiance "with sharp words and angry countenance" to prepare his devise as letters patent and announced that he would have these passed in parliament. His next measure was to have leading councillors and lawyers sign a bond in his presence, in which they agreed faithfully to perform Edward's will after his death. A few months later, Chief Justice Edward Montagu recalled that when he and his colleagues had raised legal objections to the devise, Northumberland had threatened them "trembling for anger, and ... further said that he would fight in his shirt with any man in that quarrel". Montagu also overheard a group of lords standing behind him conclude "if they refused to do that, they were traitors". At last, on 21 June, the devise was signed by over a hundred notables, including councillors, peers, archbishops, bishops, and sheriffs; many of them later claimed that they had been bullied into doing so by Northumberland, although in the words of Edward's biographer Jennifer Loach, "few of them gave any clear indication of reluctance at the time".

It was now common knowledge that Edward was dying, and foreign diplomats suspected that some scheme to debar Mary was under way. France found the prospect of the emperor's cousin on the English throne disagreeable and engaged in secret talks with Northumberland, indicating support. The diplomats were certain that the overwhelming majority of the English people backed Mary, but nevertheless believed that Queen Jane would be successfully established.

For centuries, the attempt to alter the succession was mostly seen as a one-man-plot by the Duke of Northumberland. Since the 1970s, however, many historians have attributed the inception of the "devise" and the insistence on its implementation to the king's initiative. Diarmaid MacCulloch has made out Edward's "teenage dreams of founding an evangelical realm of Christ", while David Starkey has stated that "Edward had a couple of co-operators, but the driving will was his". Among other members of the Privy Chamber, Northumberland's intimate Sir John Gates has been suspected of suggesting to Edward to change his devise so that Lady Jane Grey herself—not just any sons of hers—could inherit the Crown. Whatever the degree of his contribution, Edward was convinced that his word was law and fully endorsed disinheriting his half-sisters: "barring Mary from the succession was a cause in which the young King believed."

Edward became ill during January 1553 with a fever and cough that gradually worsened. The imperial ambassador, Jean Scheyfve, reported that "he suffers a good deal when the fever is upon him, especially from a difficulty in drawing his breath, which is due to the compression of the organs on the right side". Edward felt well enough in early April to take the air in the park at Westminster and to move to Greenwich, but by the end of the month he had weakened again. By 7 May he was "much amended", and the royal doctors had no doubt of his recovery. A few days later the king was watching the ships on the Thames, sitting at his window. However, he relapsed, and on 11 June Scheyfve, who had an informant in the king's household, reported that "the matter he ejects from his mouth is sometimes coloured a greenish yellow and black, sometimes pink, like the colour of blood". Now his doctors believed he was suffering from "a suppurating tumour" of the lung and admitted that Edward's life was beyond recovery. Soon, his legs became so swollen that he had to lie on his back, and he lost the strength to resist the disease. To his tutor John Cheke he whispered, "I am glad to die".

Edward made his final appearance in public on 1 July, when he showed himself at his window in Greenwich Palace, horrifying those who saw him by his "thin and wasted" condition. During the next two days, large crowds arrived hoping to see the king again, but on 3 July, they were told that the weather was too chilly for him to appear. Edward died at the age of 15 at Greenwich Palace at 8pm on 6 July 1553. According to John Foxe's legendary account of his death, his last words were: "I am faint; Lord have mercy upon me, and take my spirit". He was buried in the Henry VII Lady Chapel at Westminster Abbey on 8 August 1553, with reformed rites performed by Thomas Cranmer. The procession was led by "a grett company of chylderyn in ther surples" and watched by Londoners "wepyng and lamenting"; the funeral chariot, draped in cloth of gold, was topped by an effigy of Edward, with crown, sceptre, and garter. Edward's burial place was unmarked until as late as 1966, when an inscribed stone was laid in the chapel floor by Christ's Hospital school to commemorate their founder. The inscription reads as follows: "In Memory Of King Edward VI Buried In This Chapel This Stone Was Placed Here By Christ's Hospital In Thanksgiving For Their Founder 7 October 1966".

The cause of Edward VI's death is not certain. As with many royal deaths in the 16th century, rumours of poisoning abounded, but no evidence has been found to support these. The Duke of Northumberland, whose unpopularity was underlined by the events that followed Edward's death, was widely believed to have ordered the imagined poisoning. Another theory held that Edward had been poisoned by Catholics seeking to bring Mary to the throne. The surgeon who opened Edward's chest after his death found that "the disease whereof his majesty died was the disease of the lungs". The Venetian ambassador reported that Edward had died of consumption—in other words, tuberculosis—a diagnosis accepted by many historians. Skidmore believes that Edward contracted tuberculosis after a bout of measles and smallpox in 1552 that suppressed his natural immunity to the disease. Loach suggests instead that his symptoms were typical of acute bronchopneumonia, leading to a "suppurating pulmonary infection" or lung abscess, septicaemia, and kidney failure.

Lady Mary was last seen by Edward in February, and was kept informed about the state of her brother's health by Northumberland and through her contacts with the imperial ambassadors. Aware of Edward's imminent death, she left Hunsdon House, near London, and sped to her estates around Kenninghall in Norfolk, where she could count on the support of her tenants. Northumberland sent ships to the Norfolk coast to prevent her escape or the arrival of reinforcements from the continent. He delayed the announcement of the king's death while he gathered his forces, and Jane Grey was taken to the Tower on 10 July. On the same day, she was proclaimed queen in the streets of London, to murmurings of discontent. The Privy Council received a message from Mary asserting her "right and title" to the throne and commanding that the Council proclaim her queen, as she had already proclaimed herself. The Council replied that Jane was queen by Edward's authority and that Mary, by contrast, was illegitimate and supported only by "a few lewd, base people".

Northumberland soon realised that he had miscalculated drastically, not least in failing to secure Mary's person before Edward's death. Although many of those who rallied to Mary were conservatives hoping for the defeat of Protestantism, her supporters also included many for whom her lawful claim to the throne overrode religious considerations. Northumberland was obliged to relinquish control of a nervous Council in London and launch an unplanned pursuit of Mary into East Anglia, from where news was arriving of her growing support, which included a number of nobles and gentlemen and "innumerable companies of the common people". On 14 July Northumberland marched out of London with three thousand men, reaching Cambridge the next day; meanwhile, Mary rallied her forces at Framlingham Castle in Suffolk, gathering an army of nearly twenty thousand by 19 July.

It now dawned on the Privy Council that it had made a terrible mistake. Led by the Earl of Arundel and the Earl of Pembroke, on 19 July the Council publicly proclaimed Mary as queen; Jane's nine-day reign came to an end. The proclamation triggered wild rejoicing throughout London. Stranded in Cambridge, Northumberland proclaimed Mary himself—as he had been commanded to do by a letter from the Council. William Paget and the Earl of Arundel rode to Framlingham to beg Mary's pardon, and Arundel arrested Northumberland on 24 July. Northumberland was beheaded on 22 August, shortly after renouncing Protestantism. His recantation dismayed his daughter-in-law, Jane, who followed him to the scaffold on 12 February 1554, after her father's involvement in Wyatt's rebellion.

Although Edward reigned for only six years and died at the age of 15, his reign made a lasting contribution to the English Reformation and the structure of the Church of England. The last decade of Henry VIII's reign had seen a partial stalling of the Reformation, a drifting back to more conservative values. By contrast, Edward's reign saw radical progress in the Reformation. In those six years, the Church transferred from an essentially Roman Catholic liturgy and structure to one that is usually identified as Protestant. In particular, the introduction of the Book of Common Prayer, the Ordinal of 1550, and Cranmer's Forty-two Articles formed the basis for English Church practices that continue to this day. Edward himself fully approved these changes, and though they were the work of reformers such as Thomas Cranmer, Hugh Latimer, and Nicholas Ridley, backed by Edward's determinedly evangelical Council, the fact of the king's religion was a catalyst in the acceleration of the Reformation during his reign.

Queen Mary's attempts to undo the reforming work of her brother's reign faced major obstacles. Despite her belief in the papal supremacy, she ruled constitutionally as the Supreme Head of the English Church, a contradiction under which she bridled. She found herself entirely unable to restore the vast number of ecclesiastical properties handed over or sold to private landowners. Although she burned a number of leading Protestant churchmen, many reformers either went into exile or remained subversively active in England during her reign, producing a torrent of reforming propaganda that she was unable to stem. Nevertheless, Protestantism was not yet "printed in the stomachs" of the English people, and had Mary lived longer, her Catholic reconstruction might have succeeded, leaving Edward's reign, rather than hers, as a historical aberration.

On Mary's death in 1558, the English Reformation resumed its course, and most of the reforms instituted during Edward's reign were reinstated in the Elizabethan Religious Settlement. Queen Elizabeth replaced Mary's councillors and bishops with ex-Edwardians, such as William Cecil, Northumberland's former secretary, and Richard Cox, Edward's old tutor, who preached an anti-Catholic sermon at the opening of parliament in 1559. Parliament passed an Act of Uniformity the following spring that restored, with modifications, Cranmer's prayer book of 1552; and the Thirty-nine Articles of 1563 were largely based on Cranmer's Forty-two Articles. The theological developments of Edward's reign provided a vital source of reference for Elizabeth's religious policies, though the internationalism of the Edwardian Reformation was never revived.







</doc>
<doc id="10248" url="https://en.wikipedia.org/wiki?curid=10248" title="Extrapyramidal">
Extrapyramidal

Extrapyramidal can refer to:


</doc>
<doc id="10251" url="https://en.wikipedia.org/wiki?curid=10251" title="Electronic delay storage automatic calculator">
Electronic delay storage automatic calculator

The electronic delay storage automatic calculator (EDSAC) was an early British computer. Inspired by John von Neumann's seminal "First Draft of a Report on the EDVAC", the machine was constructed by Maurice Wilkes and his team at the University of Cambridge Mathematical Laboratory in England. EDSAC was the second electronic digital stored-program computer to go into regular service.

Later the project was supported by J. Lyons & Co. Ltd., a British firm, who were rewarded with the first commercially applied computer, LEO I, based on the EDSAC design. Work on EDSAC started during 1947, and it ran its first programs on 6 May 1949, when it calculated a table of square numbers and a list of prime numbers. EDSAC 1 was finally shut down on 11 July 1958, having been superseded by EDSAC 2, which remained in use until 1965.

As soon as EDSAC was operational, it began serving the University's research needs. It used mercury delay lines for memory, and derated vacuum tubes for logic. Power consumption was 11 kW of electricity. Cycle time was 1.5 ms for all ordinary instructions, 6 ms for multiplication. Input was via five-hole punched tape and output was via a teleprinter.

Initially registers were limited to an accumulator and a multiplier register. In 1953, David Wheeler, returning from a stay at the University of Illinois, designed an index register as an extension to the original EDSAC hardware.

A magnetic tape drive was added in 1952 but never worked sufficiently well to be of real use.

Until 1952, the available main memory (instructions and data) was only 512 18-bit words, and there was no backing store. The delay lines (or "tanks") were arranged in two batteries providing 512 words each. The second battery came into operation in 1952.

The full 1024-word delay line store was not available until 1955 or early 1956, limiting programs to about 800 words until then.

John Lindley (diploma student 1958–1959) mentioned "the incredible difficulty we had ever to produce a single correct piece of paper tape with the crude and unreliable home-made punching, printing and verifying gear available in the late 50s".

The EDSAC's main memory consisted of 1024 locations, though only 512 locations were initially installed. Each contained 18 bits, but the topmost bit was always unavailable due to timing problems, so only 17 bits were used. An instruction consisted of a five-bit instruction code, one spare bit, a ten bit operand (usually a memory address), and a length bit to control whether the instruction used a 17-bit or a 35-bit operand (two consecutive words, little-endian). All instruction codes were by design represented by one mnemonic letter, so that the "Add" instruction, for example, used the EDSAC character code for the letter A.

Internally, the EDSAC used two's complement, binary numbers. Numbers were either 17 bits (one word) or 35 bits (two words) long. Unusually, the multiplier was designed to treat numbers as fixed-point fractions in the range −1 ≤ "x" < 1, i.e. the binary point was immediately to the right of the sign. The accumulator could hold 71 bits, including the sign, allowing two long (35-bit) numbers to be multiplied without losing any precision.

The instructions available were: 
There was no division instruction (but various division subroutines were supplied) and no way to directly load a number into the accumulator (a "sTore and zero accumulator" instruction followed by an "Add" instruction were necessary for this). There was no unconditional jump instruction, nor was there a procedure call instruction - it had not yet been invented.

The "initial orders" were hard-wired on a set of uniselector switches and loaded into the low words of memory at startup. By May 1949, the initial orders provided a primitive relocating assembler taking advantage of the mnemonic design described above, all in 31 words. This was the world's first assembler, and arguably the start of the global software industry. There is a simulation of EDSAC available and a full description of the initial orders and first programs.

The machine was used by other members of the University to solve real problems, and many early techniques were developed that are now included in operating systems.
Users prepared their programs by punching them (in assembler) onto a paper tape. They soon became good at being able to hold the paper tape up to the light and read back the codes. When a program was ready it was hung on a length of line strung up near the paper tape reader. The machine operators, who were present during the day, selected the next tape from the line and loaded it into EDSAC. This is of course well known today as job queues. If it printed something then the tape and the printout were returned to the user, otherwise they were informed at which memory location it had stopped. Debuggers were some time away, but a CRT screen could be set to display the contents of a particular piece of memory. This was used to see if a number was converging, for example. A loudspeaker was connected to the accumulator's sign bit; experienced users knew healthy and unhealthy sounds of programs, particularly programs 'hung' in a loop. After office hours certain "Authorised Users" were allowed to run the machine for themselves, which went on late into the night until a valve blew – which usually happened according to one such user.

The early programmers had to make use of techniques frowned upon today — especially altering the code. As there was no index register until much later, the only way of accessing an array was to alter which memory location a particular instruction was referencing.

David Wheeler, who earned the world's first Computer Science PhD working on the project, is credited with inventing the concept of a subroutine. Users wrote programs that called a routine by jumping to the start of the subroutine with the return address (i.e. the location-plus-one of the jump itself) in the accumulator (a Wheeler jump). By convention the subroutine expected this and the first thing it did was to modify its concluding jump instruction to that return address. Multiple and nested subroutines could be called so long as the user knew the length of each one in order to calculate the location to jump to; recursive calls were forbidden. The user then copied the code for the subroutine from a master tape onto their own tape following the end of their own program.

The subroutine concept led to the availability of a substantial subroutine library. By 1951, 87 subroutines in the following categories were available for general use: floating point arithmetic; arithmetic operations on complex numbers; checking; division; exponentiation; routines relating to functions; differential equations; special functions; power series; logarithms; miscellaneous; print and layout; quadrature; read (input); "n"th root; trigonometric functions; counting operations (simulating repeat until loops, while loops and for loops); vectors; and matrices.


EDSAC's successor, EDSAC 2, was commissioned in 1958.

In 1961, an EDSAC 2 version of Autocode, an ALGOL-like high-level programming language for scientists and engineers, was developed by David Hartley.

In the mid-1960s, a successor to the EDSAC 2 was planned, but the move was instead made to the Titan, a prototype Atlas 2 developed from the Atlas Computer of the University of Manchester, Ferranti, and Plessey.

On 13 January 2011, the Computer Conservation Society announced that it planned to build a working replica of EDSAC, at the National Museum of Computing (TNMoC) in Bletchley Park supervised by Andrew Herbert, who studied under Maurice Wilkes. The first parts of the recreation were switched on in November 2014. The ongoing project is open to visitors of the museum. As of November 2016, commissioning of the fully completed and operational state of the replica is estimated by the autumn of 2017.





</doc>
<doc id="10252" url="https://en.wikipedia.org/wiki?curid=10252" title="E. H. Shepard">
E. H. Shepard

Ernest Howard Shepard (10 December 1879 – 24 March 1976) was an English artist and book illustrator. He is known especially for illustrations of the anthropomorphic soft toy and animal characters in "The Wind in the Willows" by Kenneth Grahame and "Winnie-the-Pooh" by A. A. Milne.

Shepard was born in St John's Wood, London. Having shown some promise in drawing at St Paul's School, in 1897 he enrolled in the Heatherley School of Fine Art in Chelsea. After a productive year there, he attended the Royal Academy Schools, winning a Landseer scholarship in 1899 and a British Institute prize in 1900. There he met Florence Eleanor Chaplin, who he married in 1904. By 1906 Shepard had become a successful illustrator, having produced work for illustrated editions of Aesop's Fables, "David Copperfield", and "Tom Brown's Schooldays", while at the same time working as an illustrator on the staff of "Punch". The couple bought a house in London, but in 1905 moved to Shamley Green, near Guildford.

Shepard was a prolific painter, showing in a number of major exhibitions. He exhibited at the Royal Society of Artists, Birmingham—a traditional venue for generic painters—as well as in the more radical atmosphere of Glasgow's Institute of Fine Arts, where some of the most innovative artists were on show. He was twice an exhibitor at the prestigious Walker Art Gallery in Liverpool, one of the largest and most important provincial galleries in the country, and another at the Manchester Art Gallery, a Victorian institution now part of the public libraries. But at heart, Shepard was a Londoner, showing sixteen times at the Royal Academy on Piccadilly. His wife, who was also a painter, found a home in London's West End venue for her own modest output during a 25-year career.

Although in his mid-thirties when World War I broke out in 1914, Shepard received a commission as a second lieutenant in the Royal Garrison Artillery, an arm of the Royal Artillery. By 1916, Shepard started working for the Intelligence Department sketching the combat area within the view of his battery position. On 16 February 1917, he was made an acting captain whilst second-in-command of a siege battery, and briefly served as an acting major in late April and early May of that year, when he reverted to the acting rank of captain. He was promoted to substantive lieutenant on 1 July 1917. Whilst acting as Captain, he was awarded the Military Cross for his service at the Battle of Passchendaele. His citation read:

"For conspicuous gallantry and devotion to duty. As forward Observation Officer he continued to observe and send back valuable information, in spite of heavy shell and machine gun fire. His courage and coolness were conspicuous."

By war's end, he had achieved the rank of major.

Throughout the war he had been contributing to "Punch". He was hired as a regular staff cartoonist in 1921 and became lead cartoonist in 1945. He was removed from this post in 1953 by "Punch"'s new editor, Malcolm Muggeridge.

Shepard was recommended to A. A. Milne in 1923 by another "Punch" staffer, E. V. Lucas. Milne initially thought Shepard's style was not what he wanted, but used him to illustrate the book of poems "When We Were Very Young". Happy with the results, Milne then insisted Shepard illustrate "Winnie-the-Pooh". Realising his illustrator's contribution to the book's success, the writer arranged for Shepard to receive a share of his royalties. Milne also inscribed a copy of "Winnie-the-Pooh" with the following personal verse:
"When I am gone,<br>
"Let Shepard decorate my tomb,<br>
"And put (if there is room)<br>
"Two pictures on the stone:<br>
"Piglet from page a hundred and eleven,<br>
"And Pooh and Piglet walking (157) ...<br>
"And Peter, thinking that they are my own,<br>
"Will welcome me to Heaven."
Eventually Shepard came to resent "that silly old bear" as he felt that the Pooh illustrations overshadowed his other work.

Shepard modelled Pooh not on the toy owned by Milne's son Christopher Robin but on "Growler", a stuffed bear owned by his own son. (Growler no longer exists, having been given to his granddaughter Minnie Hunt and subsequently destroyed by a neighbour's dog.) His Pooh work is so famous that 300 of his preliminary sketches were exhibited at the Victoria and Albert Museum in 1969, when he was 90 years old.

A Shepard painting of Winnie the Pooh, believed to have been painted in the 1930s for a Bristol teashop, is the only known oil painting of the famous teddy bear. It was purchased at an auction for $243,000 in London late in 2000. The painting is displayed in the Pavilion Gallery at Assiniboine Park in Winnipeg, Manitoba, Canada.

Shepard wrote two autobiographies: "Drawn from Memory" (1957) and "Drawn From Life" (1961).

In 1972, Shepard gave his personal collection of papers and illustrations to the University of Surrey. These now form the E.H. Shepard Archive.

Shepard was made an Officer of the Order of the British Empire in the 1972 Queen's Birthday Honours.

Shepard lived at Melina Place in St John's Wood and from 1955 in Lodsworth, West Sussex. He and Florence had two children, Graham (born 1907) and Mary (born 1909), who both became illustrators. Lt. Graham Shepard died when his ship HMS "Polyanthus" was sunk by German submarine U-952 in September 1943. Mary married E.V. Knox, the editor of "Punch", and became known as the illustrator of the "Mary Poppins" series of children's books. Florence Shepard died in 1927. In November 1943 Shepard married Norah Carroll, a nurse at St Mary's Hospital, Paddington. They remained married until his death in 1976.







</doc>
<doc id="10253" url="https://en.wikipedia.org/wiki?curid=10253" title="Enterobacteriaceae">
Enterobacteriaceae

The Enterobacteriaceae are a large family of Gram-negative bacteria that includes, along with many harmless symbionts, many of the more familiar pathogens, such as "Salmonella", "Escherichia coli", "Yersinia pestis", "Klebsiella", and "Shigella". Other disease-causing bacteria in this family include "Proteus", "Enterobacter", "Serratia", and "Citrobacter". This family is the only representative in the order Enterobacteriales of the class Gammaproteobacteria in the phylum Proteobacteria. Phylogenetically, in the Enterobacteriales, several peptidoglycan-less insect endosymbionts form a sister clade to the Enterobacteriaceae, but as they are not validly described, this group is not officially a taxon; examples of these species are "Sodalis", "Buchnera", "Wigglesworthia", "Baumannia cicadellinicola", and "Blochmannia", but not former Rickettsias. Members of the Enterobacteriaceae can be trivially referred to as enterobacteria or "enteric bacteria", as several members live in the intestines of animals. In fact, the etymology of the family is enterobacterium with the suffix to designate a family (aceae)—not after the genus "Enterobacter" (which would be "Enterobacteraceae")—and the type genus is "Escherichia".

Members of the Enterobacteriaceae are bacilli (rod-shaped), and are typically 1–5 μm in length. They typically appear as medium to large-sized grey colonies on blood agar, although some can express pigments (such as Serratia marcescens). Like other proteobacteria, enterobactericeae have Gram-negative stains, and they are facultative anaerobes, fermenting sugars to produce lactic acid and various other end products. Most also reduce nitrate to nitrite, although exceptions exist (e.g. "Photorhabdus"). Unlike most similar bacteria, enterobacteriaceae generally lack cytochrome C oxidase, although there are exceptions (e.g. "Plesiomonas shigelloides"). Most have many flagella used to move about, but a few genera are nonmotile. They are not spore-forming. Catalase reactions vary among Enterobacteriaceae.

Many members of this family are a normal part of the gut flora found in the intestines of humans and other animals, while others are found in water or soil, or are parasites on a variety of different animals and plants. "Escherichia coli" is one of the most important model organisms, and its genetics and biochemistry have been closely studied.

Most members of Enterobacteriaceae have peritrichous, type I fimbriae involved in the adhesion of the bacterial cells to their hosts.
Some enterobacteria produce endotoxins. Endotoxins reside in the cell wall and are released when the cell dies and the cell wall disintegrates. Some members of the Enterobacteriaceae produce endotoxins that, when released into the bloodstream following cell lysis, cause a systemic inflammatory and vasodilatory response. The most severe form of this is known as endotoxic shock, which can be rapidly fatal.


To identify different genera of Enterobacteriaceae, a microbiologist may run a series of tests in the lab. These include:

In a clinical setting, three species make up 80 to 95% of all isolates identified. These are "Escherichia coli", "Klebsiella pneumoniae", and "Proteus mirabilis".

Several Enterobacteriaceae strains have been isolated which are resistant to antibiotics including carbapenems, which are often claimed as "the last line of antibiotic defense" against resistant organisms. For instance, some "Klebsiella pneumoniae" strains are carbapenem resistant.



</doc>
<doc id="10256" url="https://en.wikipedia.org/wiki?curid=10256" title="Eccentricity">
Eccentricity

Eccentricity or eccentric may refer to:







</doc>
<doc id="10257" url="https://en.wikipedia.org/wiki?curid=10257" title="Essendon Football Club">
Essendon Football Club

The Essendon Football Club is a professional Australian rules football club which plays in the Australian Football League (AFL), the sport's premier competition. Formed in 1871 as a junior club and playing as a senior club since 1878, Essendon is one of the oldest clubs in the AFL. It is historically associated with Essendon, a suburb in the north-west of Melbourne, Victoria. Since 2013, the club has been headquartered at The Hangar, Melbourne Airport, and plays its home games at either Docklands Stadium or the Melbourne Cricket Ground; throughout most of its history the club's home ground and headquarters was Windy Hill, Essendon. Dyson Heppell is the current team captain.

A founding member club of both the Victorian Football Association, in 1877, and the Victorian Football League (since renamed the AFL), in 1896, Essendon is one of Australia's best-known football clubs. The club claims to have at least one million supporters Australia wide. Essendon has won 16 VFL/AFL premierships which, along with Carlton, is the most of any club in the competition.

The club was founded by members of the Royal Agricultural Society, the Melbourne Hunt Club and the Victorian Woolbrokers. The Essendon Football Club is thought to have formed in 1872 at a meeting it the home of a well-known brewery family, the McCrackens, whose Ascot Vale property hosted a team of local junior players.

Robert McCracken, the owner of several city hotels, was the founder and first president of the Essendon club and his son, Alex, its first secretary. Alex would later become president of the newly formed VFL. Alex's cousin, Collier, who had already played with Melbourne, was the team's first captain.

The club played its first recorded match against the Carlton second twenty on 7 June 1873, with Essendon winning by one goal. Essendon played 13 matches in its first season, winning seven, with four draws and losing two. The club was one of the inaugural junior members of the Victorian Football Association (VFA) in 1877, and began competing as a senior club from the 1878 season. During its early years in the Association, Essendon played its home matches at Flemington Hill, but moved to the East Melbourne Cricket Ground in 1881.

In 1878, Essendon played in the first match on what would be considered by modern standards to be a full-sized field at Flemington Hill. In 1879 Essendon played Melbourne in one of the earliest night matches recorded when the ball was painted white. In 1883 the team played four matches in Adelaide.

In 1891 Essendon won their first VFA premiership, which they repeated in 1892, 1893 and 1894. One of the club's greatest players, Albert Thurgood played for the club during this period. Essendon was undefeated in the 1893 season.

At the end of the 1896 season Essendon along with seven other clubs formed the Victorian Football League. Essendon's first VFL game was in 1897 was against Geelong at Corio Oval in Geelong. Essendon won its first VFL premiership by winning the 1897 VFL finals series. Essendon again won the premiership in 1901, defeating Collingwood in the Grand Final. The club won successive premierships in 1911 and 1912 over Collingwood and South Melbourne respectively.

Having already re-located from its ground at Kent Street, Ascot Vale ("McCracken's Paddock") to Flemington Hill, the club was again forced to move in 1881; and, because the City of Essendon mayor of the day considered the Essendon Cricket Ground "to be suitable only for the gentleman's game of cricket", Essendon moved to East Melbourne.

The club became known by the nickname "the Same Old Essendon", from the title and hook of the principal song performed by a band of supporters which regularly occupied a section of the grandstand at the club's games. The nickname first appeared in print in the local "North Melbourne Advertiser" in 1889, and ended up gaining wide use, often as the diminutive "Same Olds".

This move away from Essendon, at a time when fans would walk to their local ground, didn't go down too well with many Essendon people; and, as a consequence, a new team and club was formed in 1900, unconnected with the first (although it played in the same colours), that was based at the Essendon Cricket Ground, and playing in the Victorian Football Association. It was known firstly as Essendon Town and, after 1905, as Essendon (although it was often called Essendon A, with the A standing for association).

After the 1921 season, the East Melbourne Cricket Ground was closed and demolished to expand the Flinders Street Railyard. Having played at the East Melbourne Cricket Ground from 1882 to 1921, and having won four VFA premierships (1891–1894) and four VFL premierships (1897, 1901, 1911 and 1912) whilst there, Essendon was looking for a new home, and was offered grounds at the current Royal Melbourne Showgrounds, at Victoria Park, at Arden St, North Melbourne, and the Essendon Cricket Ground. The Essendon City Council offered the (VFL) team the Essendon Cricket Ground, announcing that it would be prepared to spend over ₤12,000 on improvements, including a new grandstand, scoreboard and re-fencing of the oval.

The club's first preference was to move to North Melbourne – a move which the North Melbourne Football Club (then in the VFA) saw as a grand opportunity to get into the VFL. Most of Essendon's members and players were from the North Melbourne area, and sportswriters believed that Essendon would have been taken over by or rebranded as North Melbourne within only a few years of the move. However, the VFA, desperate for its own strategic reasons not to lose its use of the North Melbourne Cricket Ground, successfully appealed to the State Government to block Essendon's move to North Melbourne. With its preferred option off the table, the club returned to Essendon, and the Essendon VFA club disbanded, with most of its players moving to North Melbourne.

The old "Same Olds" nickname fell into disuse, and by 1922 the other nicknames "Sash Wearers" and "Essendonians" that had been variously used from time to time were also abandoned. The team became universally known as "the Dons" (from EssenDON); it was not until much later, during the War years of the early 1940s, that they became known as "The Bombers" — due to Windy Hill's proximity to the Essendon Aerodrome.

In the 1922 season, playing in Essendon for the first time in decades, Essendon reached the final four for the first time since 1912, finishing in third place. In the 1923 season the club topped the ladder with 13 wins from 16 games. After a 17-point second semi final loss to South Melbourne defeated Fitzroy (who had beaten South Melbourne) in the challenge final: Essendon 8.15 (63) to Fitzroy 6.10 (46). Amongst Essendon's best players were half forward flanker George "Tich" Shorten, centre half forward Justin McCarthy, centre half back Tom Fitzmaurice, rover Frank Maher and wingman Jack Garden.

This was one of Essendon's most famous sides, dubbed the "Mosquito Fleet", due to the number of small, very fast players in the side. Six players were 5'6" (167 cm) or smaller.

The 1924 season proved to be arguably the strangest year in Essendon's entire history. For the first time since 1897 there was no ultimate match — either "Challenge Final" or "Grand Final" — to determine the premiers; instead, the top four clubs after the home and away season played a round-robin to determine the premiers. Essendon, having previously defeated both Fitzroy (by 40 points) and South Melbourne (by 33 points), clinched the premiership by means of a 20-point loss to Richmond. With the Tigers having already lost a match to Fitzroy by a substantial margin, the Dons were declared premiers by virtue of their superior percentage, meaning that Essendon again managed to win successive premierships. But the poor crowds for the finals meant this was never attempted again, resulting in Essendon having the unique record of winning the only two premierships without a grand final.

Prominent contributors to Essendon's 1924 Premiership success included back pocket Clyde Donaldson, follower Norm Beckton, half back flanker Roy Laing, follower Charlie May and rover Charlie Hardy.

The 1924 season was not without controversy, with rumours of numerous players accepting bribes. Regardless of the accuracy of these allegations, the club's image was tarnished, and the side experienced its lowest period during the decade that followed, with poor results on the field and decreased support off it.

There was worse to follow, with various Essendon players publicly blaming each other for the poor performance against Richmond, and then, with dissension still rife in the ranks, the side plummeted to an humiliating 28-point loss to VFA premiers Footscray in a special charity match played a week later in front of 46,100 people, in aid of "Dame Nellie Melba's Limbless Soldiers' Appeal Fund", purportedly (but not officially) for the championship of Victoria.

While it is always difficult to assess the damage caused by events such as these, the club's fortunes dipped alarmingly, and persistently. Indeed, after finishing third in the 1926 season, it was to be 14 years before Essendon would even contest a finals series.

The 1933 season, was probably the start of the Essendon revival, seeing the debut of the player regarded as one of Essendon's greatest players Dick Reynolds. His impact was immediate. He won his first Brownlow Medal aged 19. His record of three Brownlow victories (1934, 1937, 1938), equalled Haydn Bunton, Sr (1931, 1932, 1935), and later equalled by Bob Skilton (1959, 1963, 1968), and Ian Stewart (1965, 1966, 1971).

Reynolds went on to arguably even greater achievements as a coach, a position to which he was first appointed, jointly with Harry Hunter, in 1939 (this was while Reynolds was still a player). A year later he took the reins on a solo basis and was rewarded with immediate success (at least in terms of expectations at the time which, after so long in the wilderness, were somewhat modest). He was regarded as having a sound tactical knowledge of the game and being an inspirational leader, as he led the side into the finals in 1940 for the first time since 1926, when the side finished 3rd. Melbourne, which defeated Essendon by just 5 points in the preliminary final, later went on to trounce Richmond by 39 points in the grand final.

1941 brought Essendon's first grand final appearance since 1923, but the side again lowered its colours to Melbourne. A year later war broke out and the competition was considerably weakened, with Geelong being forced to pull out of the competition due to travel restrictions as a result of petrol rationing. Attendances at games also declined dramatically, whilst some clubs had to move from their normal grounds due to them being used for military purposes. Many players were lost to football due to their military service. Nevertheless, Essendon went on to win the 1942 Premiership with Western Australian Wally Buttsworth in irrepressible form at centre half back. Finally, the long-awaited premiership was Essendon's after comprehensively outclassing Richmond in the grand final, 19.18 (132) to 11.13 (79). The match was played at Carlton in front of 49,000 spectators.

In any case, there could be no such reservations about Essendon's next premiership, which came just four years later. Prior to that Essendon lost a hard fought grand final to Richmond in 1943 by 5 points, finished 3rd in 1944, and dropped to 8th in 1945.

After World War II, Esssendon enjoyed great success. In the five years immediately after the war, Essendon won 3 premierships (1946, 1949, 1950) and were runners up twice (1947, 1948). In 1946, Essendon were clearly the VFL's supreme force, topping the ladder after the roster games and surviving a drawn second semi final against Collingwood to win through to the grand final a week later with a 10.16 (76) to 8.9 (57). Then, in the grand final against Melbourne, Essendon set a grand final record score of 22.18 (150) to Melbourne 13.9 (87), with 7 goal centre half forward Gordon Lane. Rover Bill Hutchinson, and defenders Wally Buttsworth, Cec Ruddell and Harold Lambert among the best players.

The 1947 Grand Final has to go down in the ledger as 'one of the ones that got away', Essendon losing to Carlton by a single point despite managing 30 scoring shots to 21. As if to prove that lightning does occasionally strike twice, the second of the 'ones that got away' came just a year later, the Dons finishing with a lamentable 7.27, to tie with Melbourne (who managed 10.9) in the 1948 grand final. A week later Essendon waved the premiership good-bye, as Melbourne raced to a 13.11 (89) to 7.8 (50) triumph. The club's Annual Report made an assessment that was at once restrained and, as was soon to emerge, tacitly and uncannily prophetic:

It is very apparent that no team is complete without a spearhead and your committee has high hopes of rectifying that fault this coming season.

The 1949 season heralded the arrival on the VFL scene of John Coleman, arguably the greatest player in Essendon's history, and, in the view of some, the finest player the game has known. In his first ever appearance for the Dons, against Hawthorn in Round 1 1949, he booted 12 of his side's 18 goals to create an opening round record which was to endure for forty five years. More importantly, however, he went on to maintain the same high level of performance throughout the season, kicking precisely 100 goals for the year to become the first player to top the ton since Richmond's Jack Titus in 1940.

The Coleman factor was just what Essendon needed to enable them to take that vital final step to premiership glory, but even so it was not until the business end of the season that this became clear. Essendon struggled to make the finals in 4th place, but once there they suddenly ignited to put in one of the most consistently devastating September performances in VFL history.
Collingwood succumbed first as the Dons powered their way to an 82-point first semi final victory, and a fortnight later it was the turn of the North Melbourne Football Club as Essendon won the preliminary final a good deal more comfortably than the ultimate margin of 17 points suggested. In the grand final, Essendon were pitted against Carlton and in a match that was a total travesty as a contest they overwhelmed the Blues to the tune of 73 points, 18.17 (125) to 6.16 (52). Best for the Dons included pacy aboriginal half back flanker Norm McDonald, ruckman Bob McLure, and rovers Bill Hutchinson and Ron McEwin. John Coleman also did well, registering 6 majors.

A year later Essendon were if anything even more dominant, defeating the North Melbourne Football Club in both the second semi final and the grand final to secure consecutive VFL premierships for the third time. Best afield in the grand final in what was officially his swansong as a player was captain-coach Dick Reynolds, who received sterling support from the likes of Norm McDonald, ruckman/back pocket Wally May, back pocket Les Gardiner, and big Bob McLure.

With 'King Richard' still holding court as coach in 1951, albeit now in a non-playing capacity, Essendon seemed on course for a third consecutive flag but a controversial four-week suspension dished out to John Coleman on the eve of the finals effectively put paid to their chances. Coleman was reported for retaliation after twice being struck by his Carlton opponent, Harry Caspar, and without him the Dons were rated a 4 goals poorer team. Nevertheless, they still managed to battle their way to a 6th successive grand final with wins over Footscray by 8 points in the first semi final and Collingwood by 2 points in the preliminary final.

The Dons sustained numerous injuries in the preliminary final and the selectors sprang a surprise on grand final day by naming the officially retired Dick Reynolds as 20th man. 'King Richard' was powerless to prevent the inevitable, although leading at half time, the Geelong kicked five goals to two points in the third quarter to set up victory by 11 points.

Essendon slumped to 8th in 1952 but John Coleman was in irrepressible form managing 103 goals for the year. Hugh Buggy noted in "The Argus": "It was the wettest season for twenty two years and Coleman showed that since the war he was without peer in the art of goal kicking."

Two seasons later Coleman's career was ended after he dislocated a knee during the Round 8 clash with the North Melbourne Football Club at Essendon. Aged just twenty five, he had kicked 537 goals in only 98 VFL games in what was generally a fairly low scoring period for the game. His meteoric rise and fall were clearly the stuff of legend, and few if any players, either before or since, have had such an immense impact over so brief a period.

According to Alf Brown, football writer for "The Herald":

Somewhat more colourful, R.S. Whittington suggested,

Without Coleman, Essendon's fortunes plummeted, and there were to be no further premierships in the 1950s. The nearest miss came in 1957 when the Bombers (as they were popularly known by this time) earned premiership favouritism after a superb 16 point second semi final defeat of Melbourne, only to lose by over 10 goals against the same side a fortnight later.

1959 saw another grand final loss to Melbourne, this time by 37 points, but the fact that the average age of the Essendon side was only 22 was seen as providing considerable cause for optimism. However, it was to take another three years, and a change of coach, before the team's obvious potential was translated into tangible success.

John Coleman started his coaching career at Essendon in 1961, thus ending the Dick Reynolds era at the club. In the same year Essendon finished the season mid table and supporters were not expecting too much for the following season. However, the club blitzed the opposition in this year, losing only two matches and finishing top of the table. Both losses were to the previous year's grand finalists. The finals posed no problems for the resurgent Dons, easily accounting for Carlton in the season's climax, winning the 1962 Premiership. This was a remarkable result for Coleman who in his second season of coaching pulled off the ultimate prize in Australian football. As so often is the case after a flag, the following two years were below standard. A further premiership in 1965 (won from 4th position on the ladder), was also unexpected due to periods of poor form during the season. The Bombers were a different club when the finals came around, but some of the credit for the improvement was given to the influence of Brian Sampson and Ted Fordham during the finals. Coleman's time as coach turned out to be much like his playing career: highly successful but cut short when he had to stand down due to health problems in 1967. Only six years later, on the eve of the 1973 season, he would be dead of a heart-attack at just 44 years of age.

Following Coleman's retirement, the club experienced tough times on and off the field. Finals appearances were rare for the side, which was often in contention for the wooden spoon. Essendon did manage to make the 1968 VFL Grand Final, but lost to Carlton by just three points and would not make it back to the big stage for a decade-and-a-half.

During the period from 1968 until 1980, five different coaches were tried, with none lasting longer than four years. Off the field the club went through troubled times as well. In 1970 five players went on strike before the season even began, demanding higher payments. Essendon did make the finals in 1972 and 1973 under the autocratic direction of Des Tuddenham (Collingwood) but they were beaten badly in successive elimination finals by St. Kilda and would not taste finals action again until the very end of the decade. The 70s Essendon sides were involved in many rough and tough encounters under Tuddenham, who himself came to logger heads with Ron Barassi at a quarter time huddle where both coaches exchanged heated words. Essendon had tough, but talented players with the likes of "Rotten Ronnie" Ron Andrews and experienced players such as Barry Davis, Ken Fletcher, Geoff Blethyn, Neville Fields and West Australian import Graham Moss. In May 1974, a controversial half time all-in-brawl with Richmond at Windy Hill and a 1975 encounter with Carlton were testimony of the era. Following the Carlton match, the 'Herald' described Windy Hill as "Boot Hill", because of the extent of the fights and the high number of reported players (eight in all – four from Carlton and four from Essendon). The peak of these incidents would occur in 1980 with new recruit Phil Carman making headlines for head-butting an umpire. The tribunal suspended him for sixteen weeks, and although most people thought this was a fair (or even lenient) sentence, he took his case to the supreme court, gathering even more unwanted publicity for the club. Despite this, the club had recruited many talented young players in the late 70s who would emerge as club greats. Three of those young players were Simon Madden, Tim Watson and Paul Van Der Haar. Terry Daniher and his brother Neale would come via a trade with South Melbourne, and Roger Merrett joined soon afterwards to form the nucleus of what would become the formidable Essendon sides of the 1980s. This raw but talented group of youngsters took Essendon to an elimination final in 1979 under Barry Davis but were again thrashed in an Elimination Final, this time at the hands of Fitzroy. Davis resigned at the end of the 1980 season after missing out on a finals appearance.

One of the few highlights for Essendon supporters during this time was when Graham Moss won the 1976 Brownlow Medal; he was the only Bomber to do so in a 40-year span from 1953–1993. Even that was bittersweet as he quit VFL football to move back to his native Western Australia, where Moss finished out his career as a player and coach at Claremont Football Club. In many ways, Moss' career reflects Essendon's mixed fortunes during the decade.

Former Richmond player Kevin Sheedy started as head coach in 1981.

Essendon reached the Grand Final in 1983, the first time since 1968. Hawthorn won by a then record 83 points.

In 1984, Essendon won the pre-season competition and completed the regular season on top of the ladder. The club played, and beat, Hawthorn in the 1984 VFL Grand Final to win their 13th premiership—their first since 1965. The teams met again in the 1985 Grand Final, which Essendon also won. At the start of 1986, Essendon were considered unbackable for three successive flags, but a succession of injuries to key players Paul Van der Haar (only fifteen games from 1986 to 1988), Tim Watson, Darren Williams, Roger Merrett and Simon Madden led the club to win only eight of its last eighteen games in 1986 and only nine games (plus a draw with Geelong) in 1987. In July 1987, the Bombers suffered a humiliation at the hands of Sydney, who fell two points short of scoring the then highest score in VFL history.

In 1988, Essendon made a rebound to sixth place with twelve wins, including a 140-point thrashing of Brisbane where they had a record sixteen individual goalkickers. In 1989, they rebounded further to second on the ladder with only five losses and thrashed Geelong in the Qualifying Final. However, after a fiery encounter with Hawthorn ended in a convincing defeat, the Bombers were no match for Geelong next week.

In 1990, Essendon were pace-setters almost from the start, but a disruption from the Qualifying Final draw between Collingwood and West Coast was a blow from which they never recovered. The Magpies comprehensively thrashed them in both the second semi final and the grand final.

Following the 1991 season, Essendon moved its home games from its traditional home ground at Windy Hill to the larger and newly renovated MCG. This move generated large increases in game attendance, membership and revenue for the club. The club's training and administrative base remained at Windy Hill until 2013.

Following the retirement of Tim Watson and Simon Madden in the early 1990s, the team was built on new players such as Gavin Wanganeen, Joe Misiti, Mark Mercuri, Michael Long, Dustin Fletcher (son of Ken) and James Hird, who was taken at #79 in the 1992 draft. This side became known as the "Baby Bombers", as the core of the side was made up of young players early in their careers.

The team won the 1993 Grand Final against Carlton and that same year, Gavin Wanganeen won the Brownlow Medal, the first awarded to an Essendon player since 1976. Three years later, James Hird was jointly awarded the medal with Michael Voss of Brisbane.

In 2000, Essendon won 20 consecutive matches before they lost to the Western Bulldogs in round 21. The team went on to win their 16th premiership, defeating , thereby completing the most dominant single season in AFL/VFL history. The defeat to the Bulldogs was the only defeat for Essendon throughout the entire calendar year (Essendon also won the 2000 pre-season competition).

Essendon was less successful after 2001. Lucrative contracts to a number of premiership players had caused serious pressure on the club's salary cap, forcing the club to trade several key players. Blake Caracella, Chris Heffernan, Justin Blumfield, Gary Moorcroft and Damien Hardwick had all departed by the end of 2002; in 2004, Mark Mercuri, Sean Wellman and Joe Misiti retired. The club remained competitive; however, they could progress no further than the second week of the finals each year for the years of 2002, 2003, and 2004. Sheedy signed a new three-year contract at the end of 2004.
In 2005, Essendon missed the finals for the first time since 1997; and in 2006, the club suffered its worst season under Sheedy, and its worst for more than 70 years, finishing second-last with only three wins (one of which was against defending premiers , in which Matthew Lloyd kicked eight goals) and one draw from twenty-two games. Matthew Lloyd replaced James Hird as captain at the start of the season, but after suffering a season-ending hamstring injury early in the season, David Hille was appointed captain for the remainder of the season. The club improved its on-field position in 2007, but again missed the finals.

Sheedy's contract was not renewed after 2007, ending his 27-year tenure as Essendon coach. Matthew Knights replaced Sheedy as coach, and coached the club for three seasons, reaching the finals once – an eighth-place finish in 2009 at the expense of reigning premiers . On 29 August 2010, shortly after the end of the 2010 home-and-away season, Knights was dismissed as coach.
On 28 September 2010, former captain James Hird was named as Essendon's new coach from 2011 on a four-year deal. Former dual premiership winning coach and Essendon triple-premiership winning player Mark Thompson later joined Hird on the coaching panel. In his first season, Essendon finished eighth. The club started strongly in 2012, sitting fourth with a 10-3 record at the halfway mark of the season; but the club won only one more match for the season, finishing eleventh to miss the finals.

In 2013 the club moved its training and administrative base to the True Value Solar Centre, a new facility in the suburb of Melbourne Airport which it had developed in conjunction with the Australian Paralympic Committee. Essendon holds a 37-year lease at the facility, and maintains a lease at Windy Hill to use the venue for home matches for its reserves team in the Victorian Football League, and for a social club and merchandise store on the site.

During 2013, the club was investigated by the AFL and the Australian Sports Anti-Doping Authority (ASADA) over its 2012 player supplements and sports science program, most specifically over allegations into illegal use of peptide supplements. An internal review found it to have "established a supplements program that was experimental, inappropriate and inadequately vetted and controlled", and on 27 August 2013 the club was found guilty of bringing the game into disrepute for this reason. Among its penalties, the club was fined A$2 million, stripped of early draft picks in the following two drafts, and forfeited its place in the 2013 finals series (having originally finished seventh on the ladder); Hird was suspended from coaching for twelve months. Several office-bearers also resigned their posts during the controversy, including chairman David Evans and CEO Ian Robson.

In the midst of the supplements saga, assistant coach Mark Thompson took over as coach for the 2014 season during Hird's suspension. He led the club back to the finals for a seventh-place finish but in a tense first elimination final against archrivals North Melbourne, the Bombers would lead by as much as 27 points at half time before a resurgent Kangaroos side came back and won the game by 12 points. After the 2014 season Mark Thompson left the club to make way for Hird's return to the senior coaching role.

In June 2014, thirty-four players were issued show-cause notices alleging the use of banned peptide Thymosin beta-4 during the program. The players faced the AFL Anti-Doping Tribunal over the 2014/15 offseason, and on 31 March 2015 the tribunal returned a not guilty verdict, determining that it was "not comfortably satisfied" that the players had been administered the peptide.

Hird returned as senior coach for the 2015 season, and after a strong start, the club's form severely declined after the announcement that WADA would appeal the decision of the AFL Anti-Doping Tribunal. The effect of the appeal on the team's morale was devastating and they would go on to win only six games for the year. Under extreme pressure, Hird resigned on 18 August 2015 following a disastrous 112-point loss to Adelaide. Former West Coast Eagles premiership coach John Worsfold was appointed as the new senior coach on a three-year contract.

On January 12, 2016, the Court of Arbitration for Sport overruled the AFL anti-doping tribunal's decision, deeming that 34 past and present players of the Essendon Football Club, took the banned substance Thymosin Beta-4. As a result, all 34 players, 12 of which were still at the club, were given two year suspensions. However, all suspensions were effectively less due to players having previously taken part in provisional suspensions undertaken during the 2014/2015 off season. 
As a result, Essendon contested the 2016 season with twelve of its regular senior players under suspension. In order for the club to remain competitive, the AFL granted Essendon the ability to upgrade all five of their rookie listed players and to sign an additional ten players to cover the loss of the suspended players for the season.

Due to this unprecedented situation, many in the football community predicted the club would go through the 2016 AFL season without a win; however, they were able to win three matches: against , and in rounds two, 21 and 23 respectively. The absence of its most experienced players also allowed the development of its young players, with Zach Merrett and Orazio Fantasia having breakout years, while Darcy Parish and Anthony McDonald-Tipungwuti, impressing in their debut seasons. Merrett acted as captain in the side's round 21 win over the Suns. The club eventually finished on the bottom of the ladder and thus claimed its first wooden spoon since 1933.

A new beginning (2017-onward)

Essendon made their final financial settlement related to the supplements saga in September 2017, just before finals started. They also improved vastly on their 2016 performance, finishing 7th in the home and away season and becoming the first team since in 2011 to go from wooden spooner to a finals appearance, but ultimately losing their only final to .

The 2017 season was also capped off by the retirement of much loved club legend and ex-captain Jobe Watson, midfielder Brent Stanton, and ex-Geelong star James Kelly, who later took up a development coach role at the club. 126 game midfielder Heath Hocking was delisted.

Essendon's first recorded jumpers were navy blue (The Footballers, edited by Thomas Power, 1875) although the club wore 'red and black caps and hose'. In 1877 The Footballers records the addition of 'a red sash over left shoulder'. This is the first time a red sash as part of the club jumper and by 1878 there are newspaper reports referring to Essendon players as 'the men in the sash'.

Given that blue and navy blue were the most popular colours at the time it is thought that Essendon adopted a red sash in 1877 to distinguish its players from others in similar coloured jumpers.

In 2007, the AFL Commission laid down the requirement that all clubs must produce an alternative jumper for use in matches where jumpers are considered to clash. From 2007–2011, the Essendon clash guernsey was the same design as its home guernsey, but with a substantially wider sash such that the guernsey was predominantly red rather than predominantly black. This was changed after 2011 when the AFL deemed that the wider sash did not provide a sufficient contrast.

From 2012 to 2016, Essendon's clash guernsey was predominantly grey, with a red sash fimbriated in black; the grey field contained, in small print, the names of all Essendon premiership players.

Before the 2016 season, Essendon's changed their clash guernsey to a predominately red one, featuring a red sash in black. Similar to the grey jumper, the names of Essendon premiership players were also printed outside the sash.

Following Adam Ramanauskas' personal battle with cancer, a "Clash for Cancer" match against was launched in 2006. This was a joint venture between Essendon and the Cancer Council of Victoria to raise funds for the organisation. Despite a formal request to the AFL being denied, players wore yellow armbands for the match which resulted in the club being fined $20,000. In 2007, the AFL agreed to allow yellow armbands to be incorporated into the left sleeve of the jumper. The 'Clash for Cancer' match against Melbourne has become an annual event, repeated in subsequent seasons, though in 2012, 2013, 2014 and 2016, (twice), the Sydney Swans and Brisbane Lions were the opponents in those respective seasons instead of Melbourne. In 2009, the jumpers were auctioned along with yellow boots worn by some players during the match.

The club's theme song, "See the Bombers Fly Up", was written c. 1959 by Kevin Andrews and is based on the tune of Johnnie Hamp's 1929 song "(Keep Your) Sunny Side Up" at an increased tempo. At the time, "(Keep Your) Sunny Side Up" was the theme song for the popular Melbourne-based TV show "Sunnyside Up". The official version of the song was recorded in 1972 by the Fable Singers and is still used today.

The song, as with all other AFL clubs, is played prior to every match and at the conclusion of matches when the team is victorious.

Songwriter Mike Brady, of "Up There Cazaly" fame, penned an updated version of the song in 1999 complete with a new verse arrangement, but it was not well received. However, this version is occasionally played at club functions.

The club's mascot is named "Skeeta Reynolds". Named after Dick Reynolds, he is a mosquito and was created in honour of the team's back-to-back premiership sides in the 1920s known as the "Mosquito Fleet". He was first named through a competition run in the Bomber magazine with "Skeeta" being the winning entry. This was later changed by the AFL to "Skeeta Reynolds". He appears as a red mosquito in an Essendon jumper and wears a red and black scarf.

Essendon has a four-way rivalry with , , and being the four biggest and most supported clubs in Victoria. Matches between the clubs are often close regardless of form and ladder positions. If out of the race themselves, all four have the desire to deny the others a finals spot or a premiership. Essendon also has a fierce rivalry with Hawthorn stemming from the 1980s.


Lindsay Tanner has served as chairman of the board since late 2015.

Essendon's board members are Paul Brasher, Paul Cousins, Chris Heffernan, Daryl Jackson, Catherine Lio, Paul Little, Simon Madden, Andrew Muir and Lindsay Tanner.

On 25 August 2008, Samsung was announced as major sponsor of the Essendon Football Club in a three-year deal touted as the biggest individual annual club sponsorship in AFL history. The deal included Samsung having naming rights on the front and back of the club jumper and signage. Although the amount was only confirmed by the club as a very significant lift from where 3 were, it was estimated to be worth around $7 million in total.

The club's apparel is currently produced by ISC. The club's apparel has also been made by Reebok (1996–1999), Fila (2000–2002), Puma (2003–2008), and Adidas (2009–2016).

"See Essendon Football Club honours"
"See W. S. Crichton Medal"


To celebrate the 125th anniversary of the club, as well as 100 years of the VFL/AFL, Essendon announced its "Team of the Century" in 1997.

In 2002, a club panel chose and ranked the 25 greatest players to have played for Essendon.


The Essendon reserves team first competed in the Victorian Football League's reserves competition when the competition was established in 1919. The team enjoyed success in the form of eight premierships between 1919 and 1999, including the last Victorian State Football League year in 1999. From 2000 until 2002, the club's reserves team competed in the new Victorian Football League competition.

At the end of 2002, the club dissolved its reserves team and established a reserves affiliation with the Bendigo Football Club in the VFL. The affiliation ran for ten years from 2003 until 2012, allowing reserves players from the Essendon list to play with Bendigo.

The club re-established its reserves team in 2013, seeking greater developmental autonomy. The reserves team has since competed in the VFL. The team plays its home games at Windy Hill. The team is made up of AFL senior listed players and VFL contracted players.

Essendon also has a women's team that competes in the women's VFL. In December 2017, the team announced the recruitment of two daughters of Essendon AFL legends: Stephanie Hird (daughter of James) and Michaela Long (daughter of Michael).

The same month, Essendon also entered E-Sports by acquiring Australian "League of Legends" team Abyss ESports. This made them the second AFL team to acquire an E-Sports division after Adelaide acquired Legacy ESports in May.




</doc>
<doc id="10258" url="https://en.wikipedia.org/wiki?curid=10258" title="Enid Blyton">
Enid Blyton

Enid Mary Blyton (11 August 1897 – 28 November 1968) was an English children's writer whose books have been among the world's best-sellers since the 1930s, selling more than 600 million copies. Blyton's books are still enormously popular, and have been translated into 90 languages; her first book, "Child Whispers", a 24-page collection of poems, was published in 1922. She wrote on a wide range of topics including education, natural history, fantasy, mystery, and biblical narratives and is best remembered today for her Noddy, Famous Five, and Secret Seven series.

Following the commercial success of her early novels such as "Adventures of the Wishing-Chair" (1937) and "The Enchanted Wood" (1939), Blyton went on to build a literary empire, sometimes producing fifty books a year in addition to her prolific magazine and newspaper contributions. Her writing was unplanned and sprang largely from her unconscious mind; she typed her stories as events unfolded before her. The sheer volume of her work and the speed with which it was produced led to rumours that Blyton employed an army of ghost writers, a charge she vigorously denied.

Blyton's work became increasingly controversial among literary critics, teachers and parents from the 1950s onwards, because of the alleged unchallenging nature of her writing and the themes of her books, particularly the Noddy series. Some libraries and schools banned her works, which the BBC had refused to broadcast from the 1930s until the 1950s because they were perceived to lack literary merit. Her books have been criticised as being elitist, sexist, racist, xenophobic and at odds with the more liberal environment emerging in post-war Britain, but they have continued to be best-sellers since her death in 1968.

Blyton felt she had a responsibility to provide her readers with a strong moral framework, so she encouraged them to support worthy causes. In particular, through the clubs she set up or supported, she encouraged and organised them to raise funds for animal and paediatric charities. The story of Blyton's life was dramatised in a BBC film entitled "Enid", featuring Helena Bonham Carter in the title role and first broadcast in the United Kingdom on BBC Four in 2009. There have also been several adaptations of her books for stage, screen and television.

Enid Blyton was born on 11 August 1897 in East Dulwich, South London, the eldest of three children, to Thomas Carey Blyton (1870–1920), a cutlery salesman, and his wife Theresa Mary ( Harrison; 1874–1950). Enid's younger brothers, Hanly (1899–1983) and Carey (1902–1976), were born after the family had moved to a semi-detached villa in Beckenham, then a village in Kent. A few months after her birth Enid almost died from whooping cough, but was nursed back to health by her father, whom she adored. Thomas Blyton ignited Enid's interest in nature; in her autobiography she wrote that he "loved flowers and birds and wild animals, and knew more about them than anyone I had ever met". He also passed on his interest in gardening, art, music, literature and the theatre, and the pair often went on nature walks, much to the disapproval of Enid's mother, who showed little interest in her daughter's pursuits. Enid was devastated when he left the family shortly after her thirteenth birthday to live with another woman. Enid and her mother did not have a good relationship, and she did not attend either of her parents' funerals.

From 1907 to 1915 Blyton attended St Christopher's School in Beckenham, where she enjoyed physical activities and became school tennis champion and captain of lacrosse. She was not so keen on all the academic subjects but excelled in writing, and in 1911 she entered Arthur Mee's children's poetry competition. Mee offered to print her verses, encouraging her to produce more. Blyton's mother considered her efforts at writing to be a "waste of time and money", but she was encouraged to persevere by Mabel Attenborough, the aunt of a school friend.
Blyton's father taught her to play the piano, which she mastered well enough for him to believe that she might follow in his sister's footsteps and become a professional musician. Blyton considered enrolling at the Guildhall School of Music, but decided she was better suited to becoming a writer. After finishing school in 1915 as head girl, she moved out of the family home to live with her friend Mary Attenborough, before going to stay with George and Emily Hunt at Seckford Hall near Woodbridge in Suffolk. Seckford Hall, with its allegedly haunted room and secret passageway provided inspiration for her later writing. At Woodbridge Congregational Church Blyton met Ida Hunt, who taught at Ipswich High School, and suggested that she train as a teacher. Blyton was introduced to the children at the nursery school, and recognising her natural affinity with them she enrolled in a National Froebel Union teacher training course at the school in September 1916. By this time she had almost ceased contact with her family.

Blyton's manuscripts had been rejected by publishers on many occasions, which only made her more determined to succeed: "it is partly the struggle that helps you so much, that gives you determination, character, self-reliance – all things that help in any profession or trade, and most certainly in writing". In March 1916 her first poems were published in "Nash's Magazine". She completed her teacher training course in December 1918, and the following month obtained a teaching appointment at Bickley Park School, a small independent establishment for boys in Bickley, Kent. Two months later Blyton received a teaching certificate with distinctions in zoology and principles of education, 1st class in botany, geography, practice and history of education, child hygiene and class teaching and 2nd class in literature and elementary mathematics. In 1920 she moved to Southernhay in Hook Road Surbiton as nursery governess to the four sons of architect Horace Thompson and his wife Gertrude, with whom Blyton spent four happy years. Owing to a shortage of schools in the area her charges were soon joined by the children of neighbours, and a small school developed at the house.

In 1920 Blyton relocated to Chessington, and began writing in her spare time. The following year she won the "Saturday Westminster Review" writing competition with her essay "On the Popular Fallacy that to the Pure All Things are Pure". Publications such as "The Londoner", "Home Weekly" and "The Bystander" began to show an interest in her short stories and poems.
Blyton's first book, "Child Whispers", a 24-page collection of poems, was published in 1922. It was illustrated by a schoolfriend, Phyllis Chase, who collaborated on several of her early works. Also in that year Blyton began writing in annuals for Cassell and George Newnes, and her first piece of writing, "Peronel and his Pot of Glue", was accepted for publication in "Teachers' World". Her success was boosted in 1923 when her poems were published alongside those of Rudyard Kipling, Walter de la Mare and G. K. Chesterton in a special issue of "Teachers' World". Blyton's educational texts were quite influential in the 1920s and '30s, her most sizeable being the three-volume "The Teacher's Treasury" (1926), the six-volume "Modern Teaching" (1928), the ten-volume "Pictorial Knowledge" (1930), and the four-volume "Modern Teaching in the Infant School" (1932).

In July 1923 Blyton published "Real Fairies", a collection of thirty-three poems written especially for the book with the exception of "Pretending", which had appeared earlier in "Punch" magazine. The following year she published "The Enid Blyton Book of Fairies", illustrated by Horace J. Knowles, and in 1926 the "Book of Brownies". Several books of plays appeared in 1927, including "A Book of Little Plays" and "The Play's the Thing" with the illustrator Alfred Bestall.

In the 1930s Blyton developed an interest in writing stories related to various myths, including those of ancient Greece and Rome; "The Knights of the Round Table", "Tales of Ancient Greece" and "Tales of Robin Hood" were published in 1930. In "Tales of Ancient Greece" Blyton retold sixteen well-known ancient Greek myths, but used the Latin rather than the Greek names of deities and invented conversations between the characters. "The Adventures of Odysseus", "Tales of the Ancient Greeks and Persians" and "Tales of the Romans" followed in 1934.

The first of twenty-eight books in Blyton's Old Thatch series, "The Talking Teapot and Other Tales", was published in 1934, the same year as the first book in her Brer Rabbit series, "Brer Rabbit Retold"; (note that Brer Rabbit originally featured in Uncle Remus stories by Joel Chandler Harris), her first serial story and first full-length book, "Adventures of the Wishing-Chair", followed in 1937. "The Enchanted Wood", the first book in the Faraway Tree series, published in 1939, is about a magic tree inspired by the Norse mythology that had fascinated Blyton as a child. According to Blyton's daughter Gillian the inspiration for the magic tree came from "thinking up a story one day and suddenly she was walking in the enchanted wood and found the tree. In her imagination she climbed up through the branches and met Moon-Face, Silky, the Saucepan Man and the rest of the characters. She had all she needed." As in the Wishing-Chair series, these fantasy books typically involve children being transported into a magical world in which they meet fairies, goblins, elves, pixies and other mythological creatures.

Blyton's first full-length adventure novel, "The Secret Island", was published in 1938, featuring the characters of Jack, Mike, Peggy and Nora. Described by "The Glasgow Herald" as a "Robinson Crusoe-style adventure on an island in an English lake", "The Secret Island" was a lifelong favourite of Gillian's and spawned the Secret series. The following year Blyton released her first book in the Circus series and her initial book in the Amelia Jane series, "Naughty Amelia Jane!" According to Gillian the main character was based on a large handmade doll given to her by her mother on her third birthday.

During the 1940s Blyton became a prolific author, her success enhanced by her "marketing, publicity and branding that was far ahead of its time". In 1940 Blyton published two books – "Three Boys and a Circus" and "Children of Kidillin" – under the pseudonym of Mary Pollock (middle name plus first married name), in addition to the eleven published under her own name that year. So popular were Pollock's books that one reviewer was prompted to observe that "Enid Blyton had better look to her laurels". But Blyton's readers were not so easily deceived and many complained about the subterfuge to her and her publisher, with the result that all six books published under the name of Mary Pollock – two in 1940 and four in 1943 – were reissued under Blyton's name. Later in 1940 Blyton published the first of her boarding school story books and the first novel in the Naughtiest Girl series, "The Naughtiest Girl in the School", which followed the exploits of the mischievous schoolgirl Elizabeth Allen at the fictional Whyteleafe School. The first of her six novels in the St. Clare's series, "The Twins at St. Clare's", appeared the following year, featuring the twin sisters Patricia and Isabel O'Sullivan.

In 1942 Blyton released the first book in the Mary Mouse series, "Mary Mouse and the Dolls' House", about a mouse exiled from her mousehole who becomes a maid at a dolls' house. Twenty-three books in the series were produced between 1942 and 1964; 10,000 copies were sold in 1942 alone. The same year, Blyton published the first novel in the Famous Five series, "Five on a Treasure Island", with illustrations by Eileen Soper. Its popularity resulted in twenty-one books between then and 1963, and the characters of Julian, Dick, Anne, George (Georgina) and Timmy the dog became household names in Britain. Matthew Grenby, author of "Children's Literature", states that the five were involved with "unmasking hardened villains and solving serious crimes", although the novels were "hardly 'hard-boiled' thrillers". Blyton based the character of Georgina, a tomboy she described as "short-haired, freckled, sturdy, and snub-nosed" and "bold and daring, hot-tempered and loyal", on herself.

Blyton had an interest in biblical narratives, and retold Old and New Testament stories. "The Land of Far-Beyond" (1942) is a Christian parable along the lines of John Bunyan's "The Pilgrim's Progress" (1698), with contemporary children as the main characters. In 1943 she published "The Children's Life of Christ", a collection of fifty-nine short stories related to the life of Jesus, with her own slant on popular biblical stories, from the Nativity and the Three Wise Men through to the trial, the crucifixion and the resurrection. "Tales from the Bible" was published the following year, followed by "The Boy with the Loaves and Fishes" in 1948.

The first book of Blyton's Five Find-Outers series, "The Mystery of the Burnt Cottage", was published in 1943, as was the second book in the Faraway series, "The Magic Faraway Tree", which in 2003 was voted 66th in the BBC's Big Read poll to find the UK's favourite book. Several of Blyton's works during this period have seaside themes; "John Jolly by the Sea" (1943), a picture book intended for younger readers, was published in a booklet format by Evans Brothers. Other books with a maritime theme include "The Secret of Cliff Castle" and "Smuggler Ben", both attributed to Mary Pollock in 1943; "The Island of Adventure", the first in the Adventure series of eight novels from 1944 onwards; and various novels of the Famous Five series such as "Five on a Treasure Island" (1942), "Five on Kirrin Island Again" (1947) and "Five Go Down to the Sea" (1953).

Capitalising on her success, with a loyal and ever-growing readership, Blyton produced a new edition of many of her series such as the Famous Five, the Five Find-Outers and St. Clare's every year in addition to many other novels, short stories and books. In 1946 Blyton launched the first in the Malory Towers series of six books based around the schoolgirl Darrell Rivers, "First Term at Malory Towers", which became extremely popular, particularly with girls.

The first book in Blyton's Barney Mysteries series, "The Rockingdown Mystery", was published in 1949, as was the first of her fifteen Secret Seven novels. The Secret Seven Society consists of Peter, his sister Janet, and their friends Colin, George, Jack, Pam and Barbara, who meet regularly in a shed in the garden to discuss peculiar events in their local community. Blyton rewrote the stories so they could be adapted into cartoons, which appeared in "Mickey Mouse Weekly" in 1951 with illustrations by George Brook. The French author Evelyne Lallemand continued the series in the 1970s, producing an additional twelve books, nine of which were translated into English by Anthea Bell between 1983 and 1987.
Blyton's Noddy, about a little wooden boy from Toyland, first appeared in the "Sunday Graphic" on 5 June 1949, and in November that year "Noddy Goes to Toyland", the first of at least two dozen books in the series, was published. The idea was conceived by one of Blyton's publishers, Sampson, Low, Marston and Company, who in 1949 arranged a meeting between Blyton and the Dutch illustrator Harmsen van der Beek. Despite having to communicate via an interpreter, he provided some initial sketches of how Toyland and its characters would be represented. Four days after the meeting Blyton sent the text of the first two Noddy books to her publisher, to be forwarded to van der Beek. The Noddy books became one of her most successful and best-known series, and were hugely popular in the 1950s. An extensive range of sub-series, spin-offs and strip books were produced throughout the decade, including "Noddy's Library", "Noddy's Garage of Books", "Noddy's Castle of Books", "Noddy's Toy Station of Books" and "Noddy's Shop of Books".

In 1950 Blyton established the company Darrell Waters Ltd to manage her affairs. By the early 1950s she had reached the peak of her output, often publishing more than fifty books a year, and she remained extremely prolific throughout much of the decade. By 1955 Blyton had written her fourteenth Famous Five novel, "Five Have Plenty of Fun", her fifteenth Mary Mouse book, "Mary Mouse in Nursery Rhyme Land", her eighth book in the Adventure series, "The River of Adventure", and her seventh Secret Seven novel, "Secret Seven Win Through". She completed the sixth and final book of the Malory Towers series, "Last Term at Malory Towers", in 1951.

Blyton published several further books featuring the character of Scamp the terrier, following on from "The Adventures of Scamp", a novel she had released in 1943 under the pseudonym of Mary Pollock. "Scamp Goes on Holiday" (1952) and "Scamp and Bimbo", "Scamp at School", "Scamp and Caroline" and "Scamp Goes to the Zoo" (1954) were illustrated by Pierre Probst. She introduced the character of Bom, a stylish toy drummer dressed in a bright red coat and helmet, alongside Noddy in "TV Comic" in July 1956. A book series began the same year with "Bom the Little Toy Drummer", featuring illustrations by R. Paul-Hoye, and followed with "Bom and His Magic Drumstick" (1957), "Bom Goes Adventuring" and "Bom Goes to Ho Ho Village" (1958), "Bom and the Clown" and "Bom and the Rainbow" (1959) and "Bom Goes to Magic Town" (1960). In 1958 she produced two annuals featuring the character, the first of which included twenty short stories, poems and picture strips.

Many of Blyton's series, including Noddy and The Famous Five, continued to be successful in the 1960s; by 1962, 26 million copies of Noddy had been sold. Blyton concluded several of her long-running series in 1963, publishing the last books of The Famous Five ("Five Are Together Again") and The Secret Seven ("Fun for the Secret Seven"); she also produced three more Brer Rabbit books with the illustrator Grace Lodge: "Brer Rabbit Again", "Brer Rabbit Book", and "Brer Rabbit's a Rascal". In 1962 many of her books were among the first to be published by Armada Books in paperback, making them more affordable to children.

After 1963 Blyton's output was generally confined to short stories and books intended for very young readers, such as "Learn to Count with Noddy" and "Learn to Tell Time with Noddy" in 1965, and "Stories for Bedtime" and the Sunshine Picture Story Book collection in 1966. Her declining health and a falling off in readership among older children have been put forward as the principal reasons for this change in trend. Blyton published her last book in the Noddy series, "Noddy and the Aeroplane", in February 1964. In May the following year she published "Mixed Bag", a song book with music written by her nephew Carey, and in August she released her last full-length books, "The Man Who Stopped to Help" and "The Boy Who Came Back".

Blyton cemented her reputation as a children's writer when in 1926 she took over the editing of "Sunny Stories", a magazine that typically included the re-telling of legends, myths, stories and other articles for children. That same year she was given her own column in "Teachers' World", entitled "From my Window". Three years later she began contributing a weekly page in the magazine, in which she published letters from her fox terrier dog Bobs. They proved to be so popular that in 1933 they were published in book form as "Letters from Bobs", and sold ten thousand copies in the first week. Her most popular feature was "Round the Year with Enid Blyton", which consisted of forty-eight articles covering aspects of natural history such as weather, pond life, how to plant a school garden and how to make a bird table. Among Blyton's other nature projects was her monthly "Country Letter" feature that appeared in "The Nature Lover" magazine in 1935.

"Sunny Stories" was renamed "Enid Blyton's Sunny Stories" in January 1937, and served as a vehicle for the serialisation of Blyton's books. Her first Naughty Amelia Jane story, about an anti-heroine based on a doll owned by her daughter Gillian, was published in the magazine. Blyton stopped contributing in 1952, and it closed down the following year, shortly before the appearance of the new fortnightly "Enid Blyton Magazine" written entirely by Blyton. The first edition appeared on 18 March 1953, and the magazine ran until September 1959.

Noddy made his first appearance in the "Sunday Graphic" in 1949, the same year as Blyton's first daily Noddy strip for the London "Evening Standard". It was illustrated by van der Beek until his death in 1953.

Blyton worked in a wide range of fictional genres, from fairy tales to animal, nature, detective, mystery, and circus stories, but she often "blurred the boundaries" in her books, and encompassed a range of genres even in her short stories. In a 1958 article published in "The Author", she wrote that there were a "dozen or more different types of stories for children", and she had tried them all, but her favourites were those with a family at their centre.

In a letter to the psychologist Peter McKellar, Blyton describes her writing technique:
In another letter to McKellar she describes how in just five days she wrote the 60,000-word book "The River of Adventure", the eighth in her Adventure Series, by listening to what she referred to as her "under-mind", which she contrasted with her "upper conscious mind". Blyton was unwilling to conduct any research or planning before beginning work on a new book, which coupled with the lack of variety in her life according to Druce almost inevitably presented the danger that she might unconsciously, and clearly did, plagiarise the books she had read, including her own. Gillian has recalled that her mother "never knew where her stories came from", but that she used to talk about them "coming from her 'mind's eye", as did William Wordsworth and Charles Dickens. Blyton had "thought it was made up of every experience she'd ever had, everything she's seen or heard or read, much of which had long disappeared from her conscious memory" but never knew the direction her stories would take. Blyton further explained in her biography that "If I tried to think out or invent the whole book, I could not do it. For one thing, it would bore me and for another, it would lack the 'verve' and the extraordinary touches and surprising ideas that flood out from my imagination."

Blyton's daily routine varied little over the years. She usually began writing soon after breakfast, with her portable typewriter on her knee and her favourite red Moroccan shawl nearby; she believed that the colour red acted as a "mental stimulus" for her. Stopping only for a short lunch break she continued writing until five o'clock, by which time she would usually have produced 6,000–10,000 words.

A 2000 article in "The Malay Mail" considers Blyton's children to have "lived in a world shaped by the realities of post-war austerity", enjoying freedom without the political correctness of today, which serves modern readers of Blyton's novels with a form of escapism. Brandon Robshaw of "The Independent" refers to the Blyton universe as "crammed with colour and character", "self-contained and internally consistent", noting that Blyton exemplifies a strong mistrust of adults and figures of authority in her works, creating a world in which children govern. Gillian noted that in her mother's adventure, detective and school stories for older children, "the hook is the strong storyline with plenty of cliffhangers, a trick she acquired from her years of writing serialised stories for children's magazines. There is always a strong moral framework in which bravery and loyalty are (eventually) rewarded". Blyton herself wrote that "my love of children is the whole foundation of all my work".

Victor Watson, Assistant Director of Research at Homerton College, Cambridge, believes that Blyton's works reveal an "essential longing and potential associated with childhood", and notes how the opening pages of "The Mountain of Adventure" present a "deeply appealing ideal of childhood". He argues that Blyton's work differs from that of many other authors in its approach, describing the narrative of The Famous Five series for instance as "like a powerful spotlight, it seeks to illuminate, to explain, to demystify. It takes its readers on a roller-coaster story in which the darkness is always banished; everything puzzling, arbitrary, evocative is either dismissed or explained". Watson further notes how Blyton often used minimalist visual descriptions and introduced a few careless phrases such as "gleamed enchantingly" to appeal to her young readers.

From the mid-1950s rumours began to circulate that Blyton had not written all the books attributed to her, a charge she found particularly distressing. She published an appeal in her magazine asking children to let her know if they heard such stories and, after one mother informed her that she had attended a parents' meeting at her daughter's school during which a young librarian had repeated the allegation, Blyton decided in 1955 to begin legal proceedings. The librarian was eventually forced to make a public apology in open court early the following year, but the rumours that Blyton operated "a 'company' of ghost writers" persisted, as some found it difficult to believe that one woman working alone could produce such a volume of work.

Blyton felt a responsibility to provide her readers with a positive moral framework, and she encouraged them to support worthy causes. Her view, expressed in a 1957 article, was that children should help animals and other children rather than adults:
Blyton and the members of the children's clubs she promoted via her magazines raised a great deal of money for various charities; according to Blyton, membership of her clubs meant "working for others, for no reward". The largest of the clubs she was involved with was the Busy Bees, the junior section of the People's Dispensary for Sick Animals, which Blyton had actively supported since 1933. The club had been set up by Maria Dickin in 1934, and after Blyton publicised its existence in the "Enid Blyton Magazine" it attracted 100,000 members in three years. Such was Blyton's popularity among children that after she became Queen Bee in 1952 more than 20,000 additional members were recruited in her first year in office. The Enid Blyton Magazine Club was formed in 1953. Its primary objective was to raise funds to help those children with cerebral palsy who attended a centre in Cheyne Walk, in Chelsea, London, by furnishing an on-site hostel among other things.

The Famous Five series gathered such a following that readers asked Blyton if they might form a fan club. She agreed, on condition that it serve a useful purpose, and suggested that it could raise funds for the Shaftesbury Society Babies' Home in Beaconsfield, on whose committee she had served since 1948. The club was established in 1952, and provided funds for equipping a Famous Five Ward at the home, a paddling pool, sun room, summer house, playground, birthday and Christmas celebrations, and visits to the pantomime. By the late 1950s Blyton's clubs had a membership of 500,000, and raised £35,000 in the six years of the "Enid Blyton Magazine"'s run.

By 1974 the Famous Five Club had a membership of 220,000, and was growing at the rate of 6,000 new members a year. The Beaconsfield home it was set up to support closed in 1967, but the club continued to raise funds for other paediatric charities, including an Enid Blyton bed at Great Ormond Street Hospital and a mini-bus for disabled children at Stoke Mandeville Hospital.

Blyton capitalised upon her commercial success as an author by negotiating agreements with jigsaw puzzle and games manufacturers from the late 1940s onwards; by the early 1960s some 146 different companies were involved in merchandising Noddy alone. In 1948 Bestime released four jigsaw puzzles featuring her characters, and the first Enid Blyton board game appeared, "Journey Through Fairyland", created by BGL. The first card game, Faraway Tree, appeared from Pepys in 1950. In 1954 Bestime released the first four jigsaw puzzles of the Secret Seven, and the following year a Secret Seven card game appeared.

Bestime released the Little Noddy Car Game in 1953 and the Little Noddy Leap Frog Game in 1955, and in 1956 American manufacturer Parker Brothers released Little Noddy's Taxi Game, a board game which features Noddy driving about town, picking up various characters. Bestime released its Plywood Noddy Jigsaws series in 1957 and a Noddy jigsaw series featuring cards appeared from 1963, with illustrations by Robert Lee. Arrow Games became the chief producer of Noddy jigsaws in the late 1970s and early 1980s. Whitman manufactured four new Secret Seven jigsaw puzzles in 1975, and produced four new Malory Towers ones two years later. In 1979 the company released a Famous Five adventure board game, Famous Five Kirrin Island Treasure. Stephen Thraves wrote eight Famous Five adventure game books, published by Hodder & Stoughton in the 1980s. The first adventure game book of the series, "The Wreckers' Tower Game", was published in October 1984.

On 28 August 1924 Blyton married Major Hugh Alexander Pollock, DSO (1888–1971) at Bromley Register Office, without inviting her family. They married shortly after he divorced from his first wife, with whom he had two sons. Pollock was editor of the book department in the publishing firm of George Newnes, which became her regular publisher. It was he who requested that Blyton write a book about animals, "The Zoo Book", which was completed in the month before they married. They initially lived in a flat in Chelsea before moving to Elfin Cottage in Beckenham in 1926, and then to Old Thatch in Bourne End (called Peterswood in her books) in 1929. Blyton's first daughter Gillian, was born on 15 July 1931, and after a miscarriage in 1934, she gave birth to a second daughter, Imogen, on 27 October 1935.

In 1938 Blyton and her family moved to a house in Beaconsfield, which was named Green Hedges by Blyton's readers following a competition in her magazine. By the mid-1930s, Pollock – possibly due to the trauma he had suffered during the First World War being revived through his meetings as a publisher with Winston Churchill – withdrew increasingly from public life and became a secret alcoholic. With the outbreak of the Second World War, he became involved in the Home Guard. Pollock met again Ida Crowe, a 19 years younger writer whom he had met years ago, and he offered her to join him as secretary in his posting to a Home Guard training centre at Denbies, a Gothic mansion in Surrey belonging to Lord Ashcombe, and they entered into a romantic relationship. Blyton's marriage to Pollock became troubled for years, and according to Crowe's memoir, Blyton began a series of affairs, including a lesbian relationship with one of the children's nannies. In 1941 Blyton met Kenneth Fraser Darrell Waters, a London surgeon with whom she began a serious affair. Pollock discovered the liaison, and threatened to initiate divorce proceedings against Blyton. Fearing that exposure of her adultery would ruin her public image, it was ultimately agreed that Blyton would instead file for divorce against Pollock. According to Crowe's memoir, Blyton promised that if he admitted to infidelity she would allow him parental access to their daughters; but after the divorce he was forbidden to contact them, and Blyton ensured he was subsequently unable to find work in publishing. Pollock, having married Crowe on 26 October 1943, eventually resumed his heavy drinking and was forced to petition for bankruptcy in 1950.

Blyton and Darrell Waters married at the City of Westminster Register Office on 20 October 1943. She changed the surname of her daughters to Darrell Waters and publicly embraced her new role as a happily married and devoted doctor's wife. After discovering she was pregnant in the spring of 1945, Blyton miscarried five months later, following a fall from a ladder. The baby would have been Darrell Waters's first child and it would also have been the son for which both of them longed.

Her love of tennis included playing naked, with nude tennis "a common practice in those days among the more louche members of the middle classes".

Blyton's health began to deteriorate in 1957, when during a round of golf she started to complain of feeling faint and breathless, and by 1960 she was displaying signs of dementia. Her agent George Greenfield recalled that it was "unthinkable" for the "most famous and successful of children's authors with her enormous energy and computer-like memory" to be losing her mind and suffering from what is now known as Alzheimer's disease in her mid-sixties. Blyton's situation was worsened by her husband's declining health throughout the 1960s; he suffered from severe arthritis in his neck and hips, deafness, and became increasingly ill-tempered and erratic until his death on 15 September 1967.

The story of Blyton's life was dramatised in a BBC film entitled "Enid", which aired in the United Kingdom on BBC Four on 16 November 2009. Helena Bonham Carter, who played the title role, described Blyton as "a complete workaholic, an achievement junkie and an extremely canny businesswoman" who "knew how to brand herself, right down to the famous signature".

During the months following her husband's death Blyton became increasingly ill, and moved into a nursing home three months before her death. She died at the Greenways Nursing Home, Hampstead, North London, on 28 November 1968, aged 71. A memorial service was held at St James's Church, Piccadilly, and she was cremated at Golders Green Crematorium, where her ashes remain. Blyton's home, Green Hedges, was auctioned on 26 May 1971 and demolished in 1973; the site is now occupied by houses and a street named Blyton Close. An English Heritage blue plaque commemorates Blyton at Hook Road in Chessington, where she lived from 1920 to 1924. In 2014 a plaque recording her time as a Beaconsfield resident from 1938 until her death in 1968 was unveiled in the town hall gardens, next to small iron figures of Noddy and Big Ears.

Since her death and the publication of her daughter Imogen's 1989 autobiography, "A Childhood at Green Hedges", Blyton has emerged as an emotionally immature, unstable and often malicious figure. Imogen considered her mother to be "arrogant, insecure, pretentious, very skilled at putting difficult or unpleasant things out of her mind, and without a trace of maternal instinct. As a child, I viewed her as a rather strict authority. As an adult I pitied her." Blyton's eldest daughter Gillian remembered her rather differently however, as "a fair and loving mother, and a fascinating companion".

The Enid Blyton Trust for Children was established in 1982 with Imogen as its first chairman, and in 1985 it established the National Library for the Handicapped Child. "Enid Blyton's Adventure Magazine" began publication in September 1985, and on 14 October 1992 the BBC began publishing "Noddy Magazine" and released the Noddy CD-Rom in October 1996.

The first Enid Blyton Day was held at Rickmansworth on 6 March 1993, and in October 1996 the Enid Blyton award, The Enid, was given to those who have made outstanding contributions towards children. The Enid Blyton Society was formed in early 1995, to provide "a focal point for collectors and enthusiasts of Enid Blyton" through its thrice-annual "Enid Blyton Society Journal", its annual Enid Blyton Day, and its website. On 16 December 1996 Channel 4 broadcast a documentary about Blyton, "Secret Lives". To celebrate her centenary in 1997 exhibitions were put on at the London Toy & Model Museum (now closed), Hereford and Worcester County Museum and Bromley Library, and on 9 September the Royal Mail issued centenary stamps.

The London-based entertainment and retail company Trocadero plc purchased Blyton's Darrell Waters Ltd in 1995 for £14.6 million and established a subsidiary, Enid Blyton Ltd, to handle all intellectual properties, character brands and media in Blyton's works. The group changed its name to Chorion in 1998, but after financial difficulties in 2012 sold its assets. Hachette UK acquired from Chorion world rights in the Blyton estate in March 2013, including The Famous Five series but excluding the rights to Noddy, which had been sold to DreamWorks Classics (formerly Classic Media, now a subsidiary of DreamWorks Animation) in 2012.

Blyton's granddaughter, Sophie Smallwood, wrote a new Noddy book to celebrate the character's 60th birthday, 46 years after the last book was published; "Noddy and the Farmyard Muddle" (2009) was illustrated by Robert Tyndall. In February 2011, the manuscript of a previously unknown Blyton novel, "Mr Tumpy's Caravan", was discovered by the archivist at Seven Stories, National Centre for Children's Books in a collection of papers belonging to Blyton's daughter Gillian, purchased by Seven Stories in 2010 following her death. It was initially thought to belong to a comic strip collection of the same name published in 1949, but it appears to be unrelated and is believed to be something written in the 1930s, which had been rejected by a publisher.

In a 1982 survey of 10,000 eleven-year-old children Blyton was voted their most popular writer. She is the world's fourth most-translated author, behind Agatha Christie, Jules Verne and William Shakespeare with her books being translated into 90 languages. From 2000 to 2010, Blyton was listed as a Top Ten author, selling almost 8 million copies (worth £31.2 million) in the UK alone. In 2003 "The Magic Faraway Tree" was voted 66 in the BBC's Big Read. In the 2008 Costa Book Awards, Blyton was voted Britain's best-loved author. Her books continue to be very popular among children in Commonwealth nations such as India, Pakistan, Sri Lanka, Singapore, Malta, New Zealand, and Australia, and around the world. They have also seen a surge of popularity in China, where they are "big with every generation". In March 2004 Chorion and the Chinese publisher Foreign Language Teaching and Research Press negotiated an agreement over the Noddy franchise, which included bringing the character to an animated series on television, with a potential audience of a further 95 million children under the age of five. Chorion spent around £10 million digitising Noddy, and as of 2002 had made television agreements with at least 11 countries worldwide.

Novelists influenced by Blyton include the crime writer Denise Danks, whose fictional detective Georgina Powers is based on George from the Famous Five. Peter Hunt's "A Step off the Path" (1985) is also influenced by the Famous Five, and the St. Clare's and Malory Towers series provided the inspiration for Jacqueline Wilson's "Double Act" (1996) and Adèle Geras's Egerton Hall trilogy (1990–92) respectively.

Blyton's range of plots and settings has been described as limited and continually recycled. Responding to claims that her moral views were "dependably predictable", Blyton commented that "most of you could write down perfectly correctly all the things that I believe in and stand for – you have found them in my books, and a writer's books are always a faithful reflection of himself". Many of her books were critically assessed by teachers and librarians, deemed unfit for children to read, and removed from syllabuses and public libraries.

From the 1930s to the 1950s the BBC operated a "de facto" ban on dramatising Blyton's books for radio, considering her to be a "second-rater" whose work was without literary merit. The children's literary critic Margery Fisher likened Blyton's books to "slow poison", and Jean E. Sutcliffe of the BBC's schools broadcast department wrote of Blyton's ability to churn out "mediocre material", noting that "her capacity to do so amounts to genius ... anyone else would have died of boredom long ago". Michael Rosen, Children's Laureate from 2007 until 2009, wrote that "I find myself flinching at occasional bursts of snobbery and the assumed level of privilege of the children and families in the books." The children's author Anne Fine presented an overview of the concerns about Blyton's work and responses to them on BBC Radio 4 in November 2008, in which she noted the "drip, drip, drip of disapproval" associated with the books. Blyton's response to her critics was that she was uninterested in the views of anyone over the age of 12, claiming that half the attacks on her work were motivated by jealousy and the rest came from "stupid people who don't know what they're talking about because they've never read any of my books".

Although Blyton's works have been banned from more public libraries than those of any other author, there is no evidence that the popularity of her books ever suffered, and by 1990 she was still described as being very widely read. Although some criticised her in the 1950s for the volume of work she produced, Blyton astutely capitalised on being considered a more "savoury" English alternative to what was seen by contemporaries as an invasion by American culture in the form of Disney and comics.

Some librarians felt that Blyton's restricted use of language, a conscious product of her teaching background, was prejudicial to an appreciation of more literary qualities. In a scathing article published in "Encounter" in 1958, the journalist Colin Welch remarked that it was "hard to see how a diet of Miss Blyton could help with the 11-plus or even with the Cambridge English Tripos", but reserved his harshest criticism for Blyton's Noddy, describing him as an "unnaturally priggish ... sanctimonious ... witless, spiritless, snivelling, sneaking doll."

The author and educational psychologist Nicholas Tucker notes that it was common to see Blyton cited as people's favourite or least favourite author according to their age, and argues that her books create an "encapsulated world for young readers that simply dissolves with age, leaving behind only memories of excitement and strong identification". Fred Inglis considers Blyton's books to be technically easy to read, but to also be "emotionally and cognitively easy". He mentions that the psychologist Michael Woods believed that Blyton was different from many other older authors writing for children in that she seemed untroubled by presenting them with a world that differed from reality. Woods surmised that Blyton "was a child, she thought as a child, and wrote as a child ... the basic feeling is essentially pre-adolescent ... Enid Blyton has no moral dilemmas ... Inevitably Enid Blyton was labelled by rumour a child-hater. If true, such a fact should come as no surprise to us, for as a child herself all other children can be nothing but rivals for her." Inglis argues though that Blyton was clearly devoted to children and put an enormous amount of energy into her work, with a powerful belief in "representing the crude moral diagrams and garish fantasies of a readership". Blyton's daughter Imogen has stated that she "loved a relationship with children through her books", but real children were an intrusion, and there was no room for intruders in the world that Blyton occupied through her writing.

Accusations of racism in Blyton's books were first made by Lena Jeger in a "Guardian" article published in 1966, in which she was critical of Blyton's "The Little Black Doll", published a few months earlier. Sambo, the black doll of the title, is hated by his owner and the other toys owing to his "ugly black face", and runs away. A shower of rain washes his face clean, after which he is welcomed back home with his now pink face. Jamaica Kincaid also considers the Noddy books to be "deeply racist" because of the blonde children and the black golliwogs. In Blyton's 1944 novel "The Island of Adventure", a black servant named Jo-Jo is very intelligent, but is particularly cruel to the children.

Accusations of xenophobia were also made. As George Greenfield observed, "Enid was very much part of that between-the-wars middle class which believed that foreigners were untrustworthy or funny or sometimes both". The publisher Macmillan conducted an internal assessment of Blyton's "The Mystery That Never Was", submitted to them at the height of her fame in 1960. The review was carried out by the author and books editor Phyllis Hartnoll, in whose view "There is a faint but unattractive touch of old-fashioned xenophobia in the author's attitude to the thieves; they are 'foreign' ... and this seems to be regarded as sufficient to explain their criminality." Macmillan rejected the manuscript, but it was published by William Collins in 1961, and then again in 1965 and 1983.

Blyton's depictions of boys and girls are considered by many critics to be sexist. In a "Guardian" article published in 2005 Lucy Mangan proposed that The Famous Five series depicts a power struggle between Julian, Dick and George (Georgina), in which the female characters either act like boys or are talked down to, as when Dick lectures George: "it's really time you gave up thinking you're as good as a boy".

To address criticisms levelled at Blyton's work some later editions have been altered to reflect more liberal attitudes towards issues such as race, gender and the treatment of children; modern reprints of the Noddy series substitute teddy bears or goblins for golliwogs, for instance. The golliwogs who steal Noddy's car and dump him naked in the Dark Wood in "Here Comes Noddy Again" are replaced by goblins in the 1986 revision, who strip Noddy only of his shoes and hat and return at the end of the story to apologise.

"The Faraway Tree"'s Dame Slap, who made regular use of corporal punishment, was changed to Dame Snap who no longer did so, and the names of Dick and Fanny in the same series were changed to Rick and Frannie. Characters in the Malory Towers and St. Clare's series are no longer spanked or threatened with a spanking, but are instead scolded. References to George's short hair making her look like a boy were removed in revisions to "Five on a Hike Together", reflecting the idea that girls need not have long hair to be considered feminine or normal. Anne of "The Famous Five" stating that boys cannot wear pretty dresses or like girl's dolls was removed.

In 2010 Hodder, the publisher of the Famous Five series, announced its intention to update the language used in the books, of which it sold more than half a million copies a year. The changes, which Hodder described as "subtle", mainly affect the dialogue rather than the narrative. For instance, "school tunic" becomes "uniform", "mother and father" and "mother and daddy" (this one used by young female characters and deemed sexist) becomes "mum and dad", "bathing" is replaced by "swimming", and "jersey" by "jumper". Some commentators see the changes as necessary to encourage modern readers, whereas others regard them as unnecessary and patronising. In 2016 Hodder's parent company Hachette announced that they would abandon the revisions as, based on feedback, they had not been a success.

In 1954 Blyton adapted Noddy for the stage, producing the "Noddy in Toyland" pantomime in just two or three weeks. The production was staged at the 2660-seat Stoll Theatre in Kingsway, London at Christmas. Its popularity resulted in the show running during the Christmas season for five or six years. Blyton was delighted with its reception by children in the audience, and attended the theatre three or four times a week. TV adaptations of Noddy since 1954 include one in the 1970s narrated by Richard Briers. In 1955 a stage play based on the Famous Five was produced, and in January 1997 the King's Head Theatre embarked on a six-month tour of the UK with "The Famous Five Musical", to commemorate Blyton's centenary. On 21 November 1998 "The Secret Seven Save the World" was first performed at the Sherman Theatre in Cardiff.

There have also been several film and television adaptations of the Famous Five: by the Children's Film Foundation in 1957 and 1964, Southern Television in 1978–79, and Zenith Productions in 1995–97. The series was also adapted for the German film "Fünf Freunde", directed by Mike Marzuk and released in 2011.

The Comic Strip, a group of British comedians, produced two extreme parodies of the Famous Five for Channel 4 television: "Five Go Mad in Dorset", broadcast in 1982, and "Five Go Mad on Mescalin", broadcast the following year. A third in the series, "Five Go to Rehab", was broadcast on Sky in 2012.

Blyton's "The Faraway Tree" series of books has also been adapted to television and film. On 27 October 1997 the BBC began broadcasting an animated series called "The Enchanted Lands", based on the series. It was announced in October 2014 that a deal had been signed with publishers Hachette for "The Faraway Tree" series to be adapted into a live-action film by director Sam Mendes’ production company. Marlene Johnson, head of children’s books at Hachette, said: "Enid Blyton was a passionate advocate of children’s storytelling, and The Magic Faraway Tree is a fantastic example of her creative imagination."

Seven Stories, the National Centre for Children's Books in Newcastle upon Tyne, holds the largest public collection of Blyton's papers and typescripts. The Seven Stories collection contains a significant number of Blyton's typescripts, including the previously unpublished novel, "Mr Tumpy's Caravan", as well as personal papers and diaries. The purchase of the material in 2010 was made possible by special funding from the Heritage Lottery Fund, the MLA/V&A Purchase Grant Fund, and two private donations.






</doc>
<doc id="10259" url="https://en.wikipedia.org/wiki?curid=10259" title="Epipaleolithic">
Epipaleolithic

In archaeology, the Epipaleolithic or Epi-paleolithic is a term for a period intervening between the Upper Paleolithic and Neolithic in the Stone Age. This position is also occupied by the Mesolithic and the two are sometimes confused, or used as synonyms. More often they are used for different areas: Epipaleolithic for the Levant (Middle East near the Mediterranean coast), and sometimes parts of Europe other than North and Western Europe, where Mesolithic is more often used. 

The Epipaleolithic has been defined as the "final Upper Palaeolithic industries occurring at the end of the final glaciation which appear to merge technologically into the Mesolithic" (not meaning that the Epipaleolithic is suceeded by the Mesolithic). The period is generally dated from  BP to 10,000 BP in the Levant, but later in Europe. If used as a synonym or equivalent for Mesolithic in Europe, it might end at about  BP or even later.

In the Levant the period may be subdivided into Early, Middle and Late Epipaleolithic, the last also being the Natufian. The preceding final Upper Paleolithic period is the Kebaran or "Upper Paleolithic Stage VI".

Epipalaeolithic hunter-gatherers, generally nomadic, made relatively advanced tools from small flint or obsidian blades, known as microliths, that were hafted in wooden implements. There are settlements with "flimsy structures", probably not permanently occupied except at some rich sites, but used and returned to seasonally.

In describing the period before the start of the Neolithic, Epipaleolithic is typically used for cultures in regions that were far from the glaciers of the Ice Age, so that the retreat of the glaciers made a less dramatic change to conditions. This was the case in the Levant. Conversely, the term Mesolithic is most likely to be used for Western Europe where climatic change and the extinction of the Megafauna had a great impact of the paleolithic populations at the end of the Ice Age, creating post-glacial cultures such as the Azilian, Sauveterrian, Tardenoisian, Maglemosian. In the past, French archaeologists had a general tendency to prefer the term "Epipaleolithic" to "Mesolithic", even for Western Europe. 
"Epipaleolithic" stresses the continuity with the Upper Paleolithic. Alfonso Moure says in this respect:
The Pre-Pottery Neolithic A (PPNA) period follows the Epipaleolithic in the Levant and West Asia. The term Protoneolithic is now usually included in this, although it was previously sometimes counted as Epipaleolithic, and used for some late Natufian sites which showed some Neolithic features.

The Epipaleolithic is best understood when discussing the southern Levant, as the period is well documented due to good preservation at the sites, at least of animal remains. The most prevalent animal food sources in the Levant during this period were: deer, gazelle, and ibex of various species, and smaller animals including birds, lizard, fox, tortoise, and hare. Less common were aurochs, wild equids, wild boar, wild cattle, and hartebeest. At Neve David near Haifa, 15 mammal species were found, and two reptile species. Despite then being very close to the coast, the rather small number of seashells found (7 genera) and the piercing of many, suggests these may have been collected as ornaments rather than food. 

However the period seems to be marked by an increase in plant foods and a decrease in meat eating. Over 40 plant species have been found by analysing one site in the Jordan Valley, and some grains were processed and baked. Stones with evidence of grinding have been found. These were most likely the main food sources throughout the Pre-Pottery Neolithic A, which introduced the widespread agricultural growing of crops. 



</doc>
<doc id="10263" url="https://en.wikipedia.org/wiki?curid=10263" title="Executive (government)">
Executive (government)

The executive is the organ exercising authority in and holding responsibility for the governance of a state. The executive executes and enforces law.

In political systems based on the principle of separation of powers, authority is distributed among several branches (executive, legislative, judicial) — an attempt to prevent the concentration of power in the hands of a small group of people. In such a system, the executive does not pass laws (the role of the legislature) or interpret them (the role of the judiciary). Instead, the executive enforces the law as written by the legislature and interpreted by the judiciary. The executive can be the source of certain types of law, such as a decree or executive order. Executive bureaucracies are commonly the source of regulations.

In the Westminster political system, the principle of separation of powers is not as entrenched. Members of the executive, called ministers, are also members of the legislature, and hence play an important part in both the writing and enforcing of law.

In this context, the executive consists of a leader(s) of an office or multiple offices. Specifically, the top leadership roles of the executive branch may include:

In a presidential system, the leader of the executive is both the "head of state and head of government". In a parliamentary system, a cabinet minister responsible to the legislature is the head of government, while the head of state is usually a largely ceremonial monarch or president.



</doc>
<doc id="10264" url="https://en.wikipedia.org/wiki?curid=10264" title="Enrico Fermi">
Enrico Fermi

Enrico Fermi (; ; 29 September 1901 – 28 November 1954) was an Italian-American physicist and the creator of the world's first nuclear reactor, the Chicago Pile-1. He has been called the "architect of the nuclear age" and the "architect of the atomic bomb". He was one of the very few physicists in history to excel both theoretically and experimentally. Fermi held several patents related to the use of nuclear power, and was awarded the 1938 Nobel Prize in Physics for his work on induced radioactivity by neutron bombardment and the discovery of transuranic elements. He made significant contributions to the development of quantum theory, nuclear and particle physics, and statistical mechanics.

Fermi's first major contribution was to statistical mechanics. After Wolfgang Pauli announced his exclusion principle in 1925, Fermi followed with a paper in which he applied the principle to an ideal gas, employing a statistical formulation now known as Fermi–Dirac statistics. Today, particles that obey the exclusion principle are called "fermions". Later Pauli postulated the existence of an uncharged invisible particle emitted along with an electron during beta decay, to satisfy the law of conservation of energy. Fermi took up this idea, developing a model that incorporated the postulated particle, which he named the "neutrino". His theory, later referred to as Fermi's interaction and still later as weak interaction, described one of the four fundamental forces of nature. Through experiments inducing radioactivity with recently discovered neutrons, Fermi discovered that slow neutrons were more easily captured than fast ones, and developed the Fermi age equation to describe this. After bombarding thorium and uranium with slow neutrons, he concluded that he had created new elements; although he was awarded the Nobel Prize for this discovery, the new elements were subsequently revealed to be fission products.

Fermi left Italy in 1938 to escape new Italian Racial Laws that affected his Jewish wife Laura Capon. He emigrated to the United States where he worked on the Manhattan Project during World War II. Fermi led the team that designed and built Chicago Pile-1, which went critical on 2 December 1942, demonstrating the first artificial self-sustaining nuclear chain reaction. He was on hand when the X-10 Graphite Reactor at Oak Ridge, Tennessee, went critical in 1943, and when the B Reactor at the Hanford Site did so the next year. At Los Alamos he headed F Division, part of which worked on Edward Teller's thermonuclear "Super" bomb. He was present at the Trinity test on 16 July 1945, where he used his Fermi method to estimate the bomb's yield.

After the war, Fermi served under J. Robert Oppenheimer on the General Advisory Committee, which advised the Atomic Energy Commission on nuclear matters and policy. Following the detonation of the first Soviet fission bomb in August 1949, he strongly opposed the development of a hydrogen bomb on both moral and technical grounds. He was among the scientists who testified on Oppenheimer's behalf at the 1954 hearing that resulted in the denial of the latter's security clearance. Fermi did important work in particle physics, especially related to pions and muons, and he speculated that cosmic rays arose through material being accelerated by magnetic fields in interstellar space. Many awards, concepts, and institutions are named after Fermi, including the Enrico Fermi Award, the Enrico Fermi Institute, the Fermi National Accelerator Laboratory, the Fermi Gamma-ray Space Telescope, the Enrico Fermi Nuclear Generating Station, and the synthetic element fermium, making him one of 16 scientists who have elements named after them.

Enrico Fermi was born in Rome, Italy, on 29 September 1901. He was the third child of Alberto Fermi, a division head ("") in the Ministry of Railways, and Ida de Gattis, an elementary school teacher. His only sister, Maria, was two years older than he, and his brother Giulio was a year older. After the two boys were sent to a rural community to be wet nursed, Enrico rejoined his family in Rome when he was two and a half. Although he was baptised a Roman Catholic in accordance with his grandparents' wishes, his family was not particularly religious; Enrico was an agnostic throughout his adult life. As a young boy he shared the same interests as his brother Giulio, building electric motors and playing with electrical and mechanical toys. Giulio died during the administration of an anesthetic for an operation on a throat abscess in 1915. Maria died in an airplane crash near Milano in 1959.

One of Fermi's first sources for his study of physics was a book he found at the local market at "Campo de' Fiori" in Rome. Published in 1840, the 900-page "Elementorum physicae mathematicae", was written in Latin by Jesuit Father Andrea Caraffa, a professor at the Collegio Romano. It covered mathematics, classical mechanics, astronomy, optics, and acoustics, insofar as these disciplines were understood when the book was written. Fermi befriended another scientifically inclined student, Enrico Persico, and together the two worked on scientific projects such as building gyroscopes and trying to accurately measure the acceleration of Earth's gravity. Fermi's interest in physics was further encouraged by his father's colleague Adolfo Amidei, who gave him several books on physics and mathematics, which he read and assimilated quickly.

Fermi graduated from high school in July 1918 and, at Amidei's urging, applied to the "Scuola Normale Superiore" in Pisa. Having lost one son, his parents were reluctant to let him move away from home for four years while attending it, but in the end they acquiesced. The school provided free lodging for students, but candidates had to take a difficult entrance exam that included an essay. The given theme was "Specific characteristics of Sounds". The 17-year-old Fermi chose to derive and solve the partial differential equation for a vibrating rod, applying Fourier analysis in the solution. The examiner, Professor Giulio Pittarelli from the Sapienza University of Rome, interviewed Fermi and praised him, saying that he would become an outstanding physicist in the future. Fermi achieved first place in the classification of the entrance exam.

During his years at the "Scuola Normale Superiore", Fermi teamed up with a fellow student named Franco Rasetti with whom he would indulge in light-hearted pranks and who would later become Fermi's close friend and collaborator. In Pisa, Fermi was advised by the director of the physics laboratory, Luigi Puccianti, who acknowledged that there was little that he could teach Fermi, and frequently asked Fermi to teach him something instead. Fermi's knowledge of quantum physics reached such a high level that Puccianti asked him to organize seminars on the topic. During this time Fermi learned tensor calculus, a mathematical technique invented by Gregorio Ricci and Tullio Levi-Civita that was needed to demonstrate the principles of general relativity. Fermi initially chose mathematics as his major, but soon switched to physics. He remained largely self-taught, studying general relativity, quantum mechanics, and atomic physics.

In September 1920, Fermi was admitted to the Physics department. Since there were only three students in the department—Fermi, Rasetti, and Nello Carrara—Puccianti let them freely use the laboratory for whatever purposes they chose. Fermi decided that they should research X-ray crystallography, and the three worked to produce a Laue photograph—an X-ray photograph of a crystal. During 1921, his third year at the university, Fermi published his first scientific works in the Italian journal "Nuovo Cimento". The first was entitled "On the dynamics of a rigid system of electrical charges in translational motion" ('). A sign of things to come was that the mass was expressed as a tensor—a mathematical construct commonly used to describe something moving and changing in three-dimensional space. In classical mechanics, mass is a scalar quantity, but in relativity it changes with velocity. The second paper was "On the electrostatics of a uniform gravitational field of electromagnetic charges and on the weight of electromagnetic charges" ('). Using general relativity, Fermi showed that a charge has a weight equal to U/c, where U was the electrostatic energy of the system, and c is the speed of light.

The first paper seemed to point out a contradiction between the electrodynamic theory and the relativistic one concerning the calculation of the electromagnetic masses, as the former predicted a value of 4/3 U/c. Fermi addressed this the next year in a paper "Concerning a contradiction between electrodynamic and the relativistic theory of electromagnetic mass" in which he showed that the apparent contradiction was a consequence of relativity. This paper was sufficiently well-regarded that it was translated into German and published in the German scientific journal "Physikalische Zeitschrift" in 1922. That year, Fermi submitted his article "On the phenomena occurring near a world line" ("") to the Italian journal "I Rendiconti dell'Accademia dei Lincei". In this article he examined the Principle of Equivalence, and introduced the so-called "Fermi coordinates". He proved that on a world line close to the time line, space behaves as if it were a Euclidean space.
Fermi submitted his thesis, "A theorem on probability and some of its applications" (""), to the "Scuola Normale Superiore" in July 1922, and received his laurea at the unusually young age of 20. The thesis was on X-ray diffraction images. Theoretical physics was not yet considered a discipline in Italy, and the only thesis that would have been accepted was one on experimental physics. For this reason, Italian physicists were slow in embracing the new ideas like relativity coming from Germany. Since Fermi was quite at home in the lab doing experimental work, this did not pose insurmountable problems for him.

While writing the appendix for the Italian edition of the book "Fundamentals of Einstein Relativity" by August Kopff in 1923, Fermi was the first to point out that hidden inside the famous Einstein equation () was an enormous amount of nuclear potential energy to be exploited. "It does not seem possible, at least in the near future", he wrote, "to find a way to release these dreadful amounts of energy—which is all to the good because the first effect of an explosion of such a dreadful amount of energy would be to smash into smithereens the physicist who had the misfortune to find a way to do it."

In 1924 Fermi was initiated into Freemasonry in the Masonic Lodge "Adriano Lemmi" of the Grand Orient of Italy.

Fermi decided to travel abroad, and spent a semester studying under Max Born at the University of Göttingen, where he met Werner Heisenberg and Pascual Jordan. Fermi then studied in Leiden with Paul Ehrenfest from September to December 1924 on a fellowship from the Rockefeller Foundation obtained through the intercession of the mathematician Vito Volterra. Here Fermi met Hendrik Lorentz and Albert Einstein, and became good friends with Samuel Goudsmit and Jan Tinbergen. From January 1925 to late 1926, Fermi taught mathematical physics and theoretical mechanics at the University of Florence, where he teamed up with Rasetti to conduct a series of experiments on the effects of magnetic fields on mercury vapour. He also participated in seminars at the Sapienza University of Rome, giving lectures on quantum mechanics and solid state physics. While giving lectures on the new quantum mechanics based on the remarkable accuracy of predictions of the Schrödinger equation, the Italian physicist would often say, "It has no business to fit so well!"

After Wolfgang Pauli announced his exclusion principle in 1925, Fermi responded with a paper "On the quantisation of the perfect monoatomic gas" ("), in which he applied the exclusion principle to an ideal gas. The paper was especially notable for Fermi's statistical formulation, which describes the distribution of particles in systems of many identical particles that obey the exclusion principle. This was independently developed soon after by the British physicist Paul Dirac, who also showed how it was related to the Bose–Einstein statistics. Accordingly, it is now known as Fermi–Dirac statistics. Following Dirac, particles that obey the exclusion principle are today called "fermions", while those that do not are called "bosons".

Professorships in Italy were granted by competition (") for a vacant chair, the applicants being rated on their publications by a committee of professors. Fermi applied for a chair of mathematical physics at the University of Cagliari on Sardinia, but was narrowly passed over in favour of Giovanni Giorgi. In 1926, at the age of 24, he applied for a professorship at the Sapienza University of Rome. This was a new chair, one of the first three in theoretical physics in Italy, that had been created by the Minister of Education at the urging of Professor Orso Mario Corbino, who was the University's professor of experimental physics, the Director of the Institute of Physics, and a member of Benito Mussolini's cabinet. Corbino, who also chaired the selection committee, hoped that the new chair would raise the standard and reputation of physics in Italy. The committee chose Fermi ahead of Enrico Persico and Aldo Pontremoli, and Corbino helped Fermi recruit his team, which was soon joined by notable students such as Edoardo Amaldi, Bruno Pontecorvo, Ettore Majorana and Emilio Segrè, and by Franco Rasetti, whom Fermi had appointed as his assistant. They were soon nicknamed the "Via Panisperna boys" after the street where the Institute of Physics was located.

Fermi married Laura Capon, a science student at the University, on 19 July 1928. They had two children: Nella, born in January 1931, and Giulio, born in February 1936. On 18 March 1929, Fermi was appointed a member of the Royal Academy of Italy by Mussolini, and on 27 April he joined the Fascist Party. He later opposed Fascism when the 1938 racial laws were promulgated by Mussolini in order to bring Italian Fascism ideologically closer to German National Socialism. These laws threatened Laura, who was Jewish, and put many of Fermi's research assistants out of work.

During their time in Rome, Fermi and his group made important contributions to many practical and theoretical aspects of physics. In 1928, he published his "Introduction to Atomic Physics" (""), which provided Italian university students with an up-to-date and accessible text. Fermi also conducted public lectures and wrote popular articles for scientists and teachers in order to spread knowledge of the new physics as widely as possible. Part of his teaching method was to gather his colleagues and graduate students together at the end of the day and go over a problem, often from his own research. A sign of success was that foreign students now began to come to Italy. The most notable of these was the German physicist Hans Bethe, who came to Rome as a Rockefeller Foundation fellow, and collaborated with Fermi on a 1932 paper "On the Interaction between Two Electrons" ().

At this time, physicists were puzzled by beta decay, in which an electron was emitted from the atomic nucleus. To satisfy the law of conservation of energy, Pauli postulated the existence of an invisible particle with no charge and little or no mass that was also emitted at the same time. Fermi took up this idea, which he developed in a tentative paper in 1933, and then a longer paper the next year that incorporated the postulated particle, which Fermi called a "neutrino". His theory, later referred to as Fermi's interaction, and still later as the theory of the weak interaction, described one of the four fundamental forces of nature. The neutrino was detected after his death, and his interaction theory showed why it was so difficult to detect. When he submitted his paper to the British journal "Nature", that journal's editor turned it down because it contained speculations which were "too remote from physical reality to be of interest to readers". Thus Fermi saw the theory published in Italian and German before it was published in English.

In the introduction to the 1968 English translation, physicist Fred L. Wilson noted that:

In January 1934, Irène Joliot-Curie and Frédéric Joliot announced that they had bombarded elements with alpha particles and induced radioactivity in them. By March, Fermi's assistant Gian-Carlo Wick had provided a theoretical explanation using Fermi's theory of beta decay. Fermi decided to switch to experimental physics, using the neutron, which James Chadwick had discovered in 1932. In March 1934, Fermi wanted to see if he could induce radioactivity with Rasetti's polonium-beryllium neutron source. Neutrons had no electric charge, and so would not be deflected by the positively charged nucleus. This meant that they needed much less energy to penetrate the nucleus than charged particles, and so would not require a particle accelerator, which the Via Panisperna boys did not have.
Fermi had the idea to resort to replacing the polonium-beryllium neutron source with a radon-beryllium one, which he created by filling a glass bulb with beryllium powder, evacuating the air, and then adding 50 mCi of radon gas, supplied by Giulio Cesare Trabacchi. This created a much stronger neutron source, the effectiveness of which declined with the 3.8-day half-life of radon. He knew that this source would also emit gamma rays, but, on the basis of his theory, he believed that this would not affect the results of the experiment. He started by bombarding platinum, an element with a high atomic number that was readily available, without success. He turned to aluminium, which emitted an alpha particle and produced sodium, which then decayed into magnesium by beta particle emission. He tried lead, without success, and then fluorine in the form of calcium fluoride, which emitted an alpha particle and produced nitrogen, decaying into oxygen by beta particle emission. In all, he induced radioactivity in 22 different elements. Fermi rapidly reported the discovery of neutron-induced radioactivity in the Italian journal "La Ricerca Scientifica" on 25 March 1934.

The natural radioactivity of thorium and uranium made it hard to determine what was happening when these elements were bombarded with neutrons but, after correctly eliminating the presence of elements lighter than uranium but heavier than lead, Fermi concluded that they had created new elements, which he called hesperium and ausonium. The chemist Ida Noddack criticised this work, suggesting that some of the experiments could have produced lighter elements than lead rather than new, heavier elements. Her suggestion was not taken seriously at the time because her team had not carried out any experiments with uranium, and its claim to have discovered masurium (technetium) was disputed. At that time, fission was thought to be improbable if not impossible on theoretical grounds. While physicists expected elements with higher atomic numbers to form from neutron bombardment of lighter elements, nobody expected neutrons to have enough energy to split a heavier atom into two light element fragments in the manner that Noddack suggested.
The Via Panisperna boys also noticed some unexplained effects. The experiment seemed to work better on a wooden table than a marble table top. Fermi remembered that Joliot-Curie and Chadwick had noted that paraffin wax was effective at slowing neutrons, so he decided to try that. When neutrons were passed through paraffin wax, they induced a hundred times as much radioactivity in silver compared with when it was bombarded without the paraffin. Fermi guessed that this was due to the hydrogen atoms in the paraffin. Those in wood similarly explained the difference between the wooden and the marble table tops. This was confirmed by repeating the effect with water. He concluded that collisions with hydrogen atoms slowed the neutrons. The lower the atomic number of the nucleus it collides with, the more energy a neutron loses per collision, and therefore the fewer collisions that are required to slow a neutron down by a given amount. Fermi realised that this induced more radioactivity because slow neutrons were more easily captured than fast ones. He developed a diffusion equation to describe this, which became known as the Fermi age equation.

In 1938 Fermi received the Nobel Prize in Physics at the age of 37 for his "demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons". After Fermi received the prize in Stockholm, he did not return home to Italy, but rather continued to New York City with his family in December 1938, where they applied for permanent residency. The decision to move to America and become U.S. citizens was due primarily to the racial laws in Italy.

Fermi arrived in New York City on 2 January 1939. He was immediately offered posts at five universities, and accepted a post at Columbia University, where he had already given summer lectures in 1936. He received the news that in December 1938, the German chemists Otto Hahn and Fritz Strassmann had detected the element barium after bombarding uranium with neutrons, which Lise Meitner and her nephew Otto Frisch correctly interpreted as the result of nuclear fission. Frisch confirmed this experimentally on 13 January 1939. The news of Meitner and Frisch's interpretation of Hahn and Strassmann's discovery crossed the Atlantic with Niels Bohr, who was to lecture at Princeton University. Isidor Isaac Rabi and Willis Lamb, two Columbia University physicists working at Princeton, found out about it and carried it back to Columbia. Rabi said he told Enrico Fermi, but Fermi later gave the credit to Lamb:
Noddack was proven right after all. Fermi had dismissed the possibility of fission on the basis of his calculations, but he had not taken into account the binding energy that would appear when a nuclide with an odd number of neutrons absorbed an extra neutron. For Fermi, the news came as a profound embarrassment, as the transuranic elements that he had partly been awarded the Nobel Prize for discovering had not been transuranic elements at all, but fission products. He added a footnote to this effect to his Nobel Prize acceptance speech.
The scientists at Columbia decided that they should try to detect the energy released in the nuclear fission of uranium when bombarded by neutrons. On 25 January 1939, in the basement of Pupin Hall at Columbia, an experimental team including Fermi conducted the first nuclear fission experiment in the United States. The other members of the team were Herbert L. Anderson, Eugene T. Booth, John R. Dunning, G. Norris Glasoe, and Francis G. Slack. The next day, the Fifth Washington Conference on Theoretical Physics began in Washington, D.C. under the joint auspices of George Washington University and the Carnegie Institution of Washington. There, the news on nuclear fission was spread even further, fostering many more experimental demonstrations.

French scientists Hans von Halban, Lew Kowarski, and Frédéric Joliot-Curie had demonstrated that uranium bombarded by neutrons emitted more neutrons than it absorbed, suggesting the possibility of a chain reaction. Fermi and Anderson did so too a few weeks later. Leó Szilárd obtained of uranium oxide from Canadian radium producer Eldorado Gold Mines Limited, allowing Fermi and Anderson to conduct experiments with fission on a much larger scale. Fermi and Szilárd collaborated on a design of a device to achieve a self-sustaining nuclear reaction—a nuclear reactor. Owing to the rate of absorption of neutrons by the hydrogen in water, it was unlikely that a self-sustaining reaction could be achieved with natural uranium and water as a neutron moderator. Fermi suggested, based on his work with neutrons, that the reaction could be achieved with uranium oxide blocks and graphite as a moderator instead of water. This would reduce the neutron capture rate, and in theory make a self-sustaining chain reaction possible. Szilárd came up with a workable design: a pile of uranium oxide blocks interspersed with graphite bricks. Szilárd, Anderson, and Fermi published a paper on "Neutron Production in Uranium". But their work habits and personalities were different, and Fermi had trouble working with Szilárd.

Fermi was among the first to warn military leaders about the potential impact of nuclear energy, giving a lecture on the subject at the Navy Department on 18 March 1939. The response fell short of what he had hoped for, although the Navy agreed to provide $1,500 towards further research at Columbia. Later that year, Szilárd, Eugene Wigner, and Edward Teller sent the famous letter signed by Einstein to U.S. President Roosevelt, warning that Nazi Germany was likely to build an atomic bomb. In response, Roosevelt formed the Advisory Committee on Uranium to investigate the matter.
The Advisory Committee on Uranium provided money for Fermi to buy graphite, and he built a pile of graphite bricks on the seventh floor of the Pupin Hall laboratory. By August 1941, he had six tons of uranium oxide and thirty tons of graphite, which he used to build a still larger pile in Schermerhorn Hall at Columbia.

The S-1 Section of the Office of Scientific Research and Development, as the Advisory Committee on Uranium was now known, met on 18 December 1941, with the U.S. now engaged in World War II, making its work urgent. Most of the effort sponsored by the Committee had been directed at producing enriched uranium, but Committee member Arthur Compton determined that a feasible alternative was plutonium, which could be mass-produced in nuclear reactors by the end of 1944. He decided to concentrate the plutonium work at the University of Chicago. Fermi reluctantly moved, and his team became part of the new Metallurgical Laboratory there.

The possible results of a self-sustaining nuclear reaction were unknown, so it seemed inadvisable to build the first nuclear reactor on the U. of C. campus in the middle of the city. Compton found a location in the Argonne Woods Forest Preserve, about from Chicago. Stone & Webster was contracted to develop the site, but the work was halted by an industrial dispute. Fermi then persuaded Compton that he could build the reactor in the squash court under the stands of the U of C's Stagg Field. Construction of the pile began on 6 November 1942, and Chicago Pile-1 went critical on 2 December. The shape of the pile was intended to be roughly spherical, but as work proceeded Fermi calculated that criticality could be achieved without finishing the entire pile as planned.

This experiment was a landmark in the quest for energy, and it was typical of Fermi's approach. Every step was carefully planned, every calculation meticulously done. When the first self-sustained nuclear chain reaction was achieved, Compton made a coded phone call to James B. Conant, the chairman of the National Defense Research Committee.
To continue the research where it would not pose a public health hazard, the reactor was disassembled and moved to the Argonne Woods site. There Fermi directed experiments on nuclear reactions, revelling in the opportunities provided by the reactor's abundant production of free neutrons. The laboratory soon branched out from physics and engineering into using the reactor for biological and medical research. Initially, Argonne was run by Fermi as part of the University of Chicago, but it became a separate entity with Fermi as its director in May 1944.

When the air-cooled X-10 Graphite Reactor at Oak Ridge went critical on 4 November 1943, Fermi was on hand just in case something went wrong. The technicians woke him early so that he could see it happen. Getting X-10 operational was another milestone in the plutonium project. It provided data on reactor design, training for DuPont staff in reactor operation, and produced the first small quantities of reactor-bred plutonium. Fermi became an American citizen in July 1944, the earliest date the law allowed.

In September 1944, Fermi inserted the first uranium fuel slug into the B Reactor at the Hanford Site, the production reactor designed to breed plutonium in large quantities. Like X-10, it had been designed by Fermi's team at the Metallurgical Laboratory, and built by DuPont, but it was much larger, and was water-cooled. Over the next few days, 838 tubes were loaded, and the reactor went critical. Shortly after midnight on 27 September, the operators began to withdraw the control rods to initiate production. At first all appeared to be well, but around 03:00, the power level started to drop and by 06:30 the reactor had shut down completely. The Army and DuPont turned to Fermi's team for answers. The cooling water was investigated to see if there was a leak or contamination. The next day the reactor suddenly started up again, only to shut down once more a few hours later. The problem was traced to neutron poisoning from xenon-135, a fission product with a half-life of 9.2 hours. Fortunately, DuPont had deviated from the Metallurgical Laboratory's original design in which the reactor had 1,500 tubes arranged in a circle, and had added 504 tubes to fill in the corners. The scientists had originally considered this over-engineering a waste of time and money, but Fermi realized that if all 2,004 tubes were loaded, the reactor could reach the required power level and efficiently produce plutonium.
In mid-1944, Robert Oppenheimer persuaded Fermi to join his Project Y at Los Alamos, New Mexico. Arriving in September, Fermi was appointed an associate director of the laboratory, with broad responsibility for nuclear and theoretical physics, and was placed in charge of F Division, which was named after him. F Division had four branches: F-1 Super and General Theory under Teller, which investigated the "Super" (thermonuclear) bomb; F-2 Water Boiler under L. D. P. King, which looked after the "water boiler" aqueous homogeneous research reactor; F-3 Super Experimentation under Egon Bretscher; and F-4 Fission Studies under Anderson. Fermi observed the Trinity test on 16 July 1945, and conducted an experiment to estimate the bomb's yield by dropping strips of paper into the blast wave. He paced off the distance they were blown by the explosion, and calculated the yield as ten kilotons of TNT; the actual yield was about 18.6 kilotons.

Along with Oppenheimer, Compton, and Ernest Lawrence, Fermi was part of the scientific panel that advised the Interim Committee on target selection. The panel agreed with the committee that atomic bombs would be used without warning against an industrial target. Like others at the Los Alamos Laboratory, Fermi found out about the atomic bombings of Hiroshima and Nagasaki from the public address system in the technical area. Fermi did not believe that atomic bombs would deter nations from starting wars, nor did he think that the time was ripe for world government. He therefore did not join the Association of Los Alamos Scientists.

Fermi became the Charles H. Swift Distinguished Professor of Physics at the University of Chicago on 1 July 1945, although he did not depart the Los Alamos Laboratory with his family until 31 December 1945. He was elected a member of the U.S. National Academy of Sciences in 1945. The Metallurgical Laboratory became the Argonne National Laboratory on 1 July 1946, the first of the national laboratories established by the Manhattan Project. The short distance between Chicago and Argonne allowed Fermi to work at both places. At Argonne he continued experimental physics, investigating neutron scattering with Leona Marshall. He also discussed theoretical physics with Maria Mayer, helping her develop insights into spin–orbit coupling that would lead to her receiving the Nobel Prize.

The Manhattan Project was replaced by the Atomic Energy Commission (AEC) on 1 January 1947. Fermi served on the AEC General Advisory Committee, an influential scientific committee chaired by Robert Oppenheimer. He also liked to spend a few weeks of each year at the Los Alamos National Laboratory, where he collaborated with Nicholas Metropolis, and with John von Neumann on Rayleigh–Taylor instability, the science of what occurs at the border between two fluids of different densities.
Following the detonation of the first Soviet fission bomb in August 1949, Fermi, along with Isidor Rabi, wrote a strongly worded report for the committee, opposing the development of a hydrogen bomb on moral and technical grounds. Nonetheless, Fermi continued to participate in work on the hydrogen bomb at Los Alamos as a consultant. Along with Stanislaw Ulam, he calculated that not only would the amount of tritium needed for Teller's model of a thermonuclear weapon be prohibitive, but a fusion reaction could still not be assured to propagate even with this large quantity of tritium. Fermi was among the scientists who testified on Oppenheimer's behalf at the Oppenheimer security hearing in 1954 that resulted in denial of Oppenheimer's security clearance.

In his later years, Fermi continued teaching at the University of Chicago. His PhD students in the post-war period included Owen Chamberlain, Geoffrey Chew, Jerome Friedman, Marvin Goldberger, Tsung-Dao Lee, Arthur Rosenfeld and Sam Treiman. Jack Steinberger was a graduate student. Fermi conducted important research in particle physics, especially related to pions and muons. He made the first predictions of pion-nucleon resonance, relying on statistical methods, since he reasoned that exact answers were not required when the theory was wrong anyway. In a paper co-authored with Chen Ning Yang, he speculated that pions might actually be composite particles. The idea was elaborated by Shoichi Sakata. It has since been supplanted by the quark model, in which the pion is made up of quarks, which completed Fermi's model, and vindicated his approach.

Fermi wrote a paper "On the Origin of Cosmic Radiation" in which he proposed that cosmic rays arose through material being accelerated by magnetic fields in interstellar space, which led to a difference of opinion with Teller. Fermi examined the issues surrounding magnetic fields in the arms of a spiral galaxy. He mused about what is now referred to as the "Fermi paradox": the contradiction between the presumed probability of the existence of extraterrestrial life and the fact that contact has not been made.
Toward the end of his life, Fermi questioned his faith in society at large to make wise choices about nuclear technology. He said:

Fermi underwent an exploratory operation in Billings Memorial Hospital on 9 October 1954, after which he returned home. 50 days later, Fermi died at age 53 of stomach cancer in his home in Chicago, and was interred at Oak Woods Cemetery.

Fermi received numerous awards in recognition of his achievements, including the Matteucci Medal in 1926, the Nobel Prize for Physics in 1938, the Hughes Medal in 1942, the Franklin Medal in 1947, and the Rumford Prize in 1953. He was awarded the Medal for Merit in 1946 for his contribution to the Manhattan Project. Fermi was elected a Foreign Member of the Royal Society (FRS) in 1950. The Basilica of Santa Croce, Florence, known as the "Temple of Italian Glories" for its many graves of artists, scientists and prominent figures in Italian history, has a plaque commemorating Fermi. In 1999, "Time" named Fermi on its list of the top 100 persons of the twentieth century. Fermi was widely regarded as an unusual case of a 20th-century physicist who excelled both theoretically and experimentally. The historian of physics, C. P. Snow, wrote that "if Fermi had been born a few years earlier, one could well imagine him discovering Rutherford's atomic nucleus, and then developing Bohr's theory of the hydrogen atom. If this sounds like hyperbole, anything about Fermi is likely to sound like hyperbole".

Fermi was known as an inspiring teacher, and was noted for his attention to detail, simplicity, and careful preparation of his lectures. Later, his lecture notes were transcribed into books. His papers and notebooks are today in the University of Chicago. Victor Weisskopf noted how Fermi "always managed to find the simplest and most direct approach, with the minimum of complication and sophistication." Fermi's ability and success stemmed as much from his appraisal of the art of the possible, as from his innate skill and intelligence. He disliked complicated theories, and while he had great mathematical ability, he would never use it when the job could be done much more simply. He was famous for getting quick and accurate answers to problems that would stump other people. Later on, his method of getting approximate and quick answers through back-of-the-envelope calculations became informally known as the "Fermi method", and is widely taught.

Fermi was fond of pointing out that Alessandro Volta, working in his laboratory, could have had no idea where the study of electricity would lead. Fermi is generally remembered for his work on nuclear power and nuclear weapons, especially the creation of the first nuclear reactor, and the development of the first atomic and hydrogen bombs. His scientific work has stood the test of time. This includes his theory of beta decay, his work with non-linear systems, his discovery of the effects of slow neutrons, his study of pion-nucleon collisions, and his Fermi–Dirac statistics. His speculation that a pion was not a fundamental particle pointed the way towards the study of quarks and leptons.

Many things bear Fermi's name. These include the Fermilab particle accelerator and physics lab in Batavia, Illinois, which was renamed in his honor in 1974, and the Fermi Gamma-ray Space Telescope, which was named after him in 2008, in recognition of his work on cosmic rays. Three nuclear reactor installations have been named after him: the Fermi 1 and Fermi 2 nuclear power plants in Newport, Michigan, the Enrico Fermi Nuclear Power Plant at Trino Vercellese in Italy, and the RA-1 Enrico Fermi research reactor in Argentina. A synthetic element isolated from the debris of the 1952 Ivy Mike nuclear test was named fermium, in honor of Fermi's contributions to the scientific community. This makes him one of 16 scientists who have elements named after them.

Since 1956, the United States Atomic Energy Commission has named its highest honor, the Fermi Award, after him. Recipients of the award include well-known scientists like Otto Hahn, Robert Oppenheimer, Edward Teller and Hans Bethe.


For a full list of his papers, see pages 75–78 in ref.




</doc>
<doc id="10267" url="https://en.wikipedia.org/wiki?curid=10267" title="Entente">
Entente

Entente, meaning a diplomatic "understanding", may refer to a number of agreements:





</doc>
<doc id="10268" url="https://en.wikipedia.org/wiki?curid=10268" title="Editor war">
Editor war

Editor war is the common name for the rivalry between users of the Emacs and vi (usually Vim) text editors. The rivalry has become a lasting part of hacker culture and the free software community.

The Emacs vs vi debate was one of the original "holy wars" conducted on Usenet groups, with many flame wars fought between those insisting that their editor of choice is the of editing perfection, and insulting the other, since at least 1985. Related battles have been fought over operating systems, programming languages, version control systems, and even source code indent style. Notably, unlike other wars (e.g., UNIX vs ITS vs VMS, C vs Pascal vs Fortran), the editor war has yet to be resolved with a clear winner, and the hacker community remains split roughly 50/50.

The most important differences between vi and Emacs are presented in the following table:


Frequently, at some point in the discussion, someone will point out that ed is the "standard text editor".

The Church of Emacs, formed by Emacs and the GNU Project's creator Richard Stallman, is a parody religion. While it refers to vi as the "editor of the beast" (vi-vi-vi being 6-6-6 in Roman numerals), it does not oppose the use of vi; rather, it calls proprietary software anathema. ("Using a free version of vi is not a sin but a penance.") The Church of Emacs has its own newsgroup, alt.religion.emacs, that has posts purporting to support this belief system.

Stallman has referred to himself as St IGNU−cius, a saint in the Church of Emacs.

Supporters of vi have created an opposing Cult of vi, argued by the more hardline Emacs users to be an attempt to "ape their betters".

Regarding vi's modal nature (a point of extreme frustration for new users) some Emacs users joke that vi has two modes – "beep repeatedly" and "break everything". vi users enjoy joking that Emacs's key-sequences induce carpal tunnel syndrome, or mentioning one of many satirical expansions of the acronym EMACS, such as "Escape Meta Alt Control Shift" (a jab at Emacs's reliance on modifier keys). or "Eight Megabytes And Constantly Swapping" (in a time when that was a great amount of memory) or "EMACS Makes Any Computer Slow" (a recursive acronym like those Stallman uses) or "Eventually Munches All Computer Storage", in reference to Emacs's high system resource requirements. GNU EMACS has been expanded to "Generally Not Used, Except by Middle-Aged Computer Scientists" referencing its most ardent fans, and its declining usage among younger programmers compared to more graphically-oriented editors such as TextMate or Sublime Text. The Emacs distribution includes the full list.

As a poke at Emacs' creeping featurism, vi advocates have been known to describe Emacs as "a great operating system, lacking only a decent editor". Emacs advocates have been known to respond that the editor is actually very good, but the operating system could use improvement (referring to Emacs' famous lack of concurrency).

A game among UNIX users, either to test the depth of an Emacs user's understanding of the editor or to poke fun at the complexity of Emacs, involved predicting what would happen if a user held down a modifier key (such as Control or Alt) and typed their own name. A similar "game" was reportedly played among users of the old TECO editor, in which lay the roots of Emacs.

Due to the unintuitive character sequence to exit vi (":q!"), hackers joke that there is a proposed method of creating a pseudorandom character sequence by having a user unfamiliar with vi seated in front of an open editor and asking them to exit the program.

In the past, many small editors modeled after or derived from vi flourished. This was due to the importance of conserving memory with the comparatively minuscule amount available at the time. As computers have become more powerful, many vi clones, Vim in particular, have grown in size and code complexity. These vi variants of today, as with the old lightweight Emacs variants, tend to have many of the perceived benefits and drawbacks of the opposing side. For example, Vim without any extensions requires about ten times the disk space required by vi, and recent versions of Vim can have more extensions and run slower than Emacs. In "The Art of Unix Programming", Eric S. Raymond called Vim's supposed light weight when compared with Emacs "a shared myth". Moreover, with the large amounts of RAM in modern computers, both Emacs and vi are lightweight compared to large integrated development environments such as Eclipse, which tend to draw derision from Emacs and vi users alike.

Tim O'Reilly said, in 1999, that O'Reilly Media's tutorial on vi sells twice as many copies as that on Emacs (but noted that Emacs came with a free manual). Many programmers use either Emacs and vi or their various offshoots, including Linus Torvalds who uses MicroEMACS. Also in 1999, vi creator Bill Joy said that vi was "written for a world that doesn't exist anymore" and stated that Emacs was written on much more capable machines with faster displays so they could have "funny commands with the screen shimmering and all that, and meanwhile, I'm sitting at home in sort of World War II surplus housing at Berkeley with a modem and a terminal that can just barely get the cursor off the bottom line".

In addition to Emacs and vi workalikes, pico and its free and open source clone nano and other text editors such as ne often have their own third-party advocates in the editor wars, though not to the extent of Emacs and vi.

As of 2014, both Emacs and vi can lay claim to being among the longest-lived application programs of all time, as well as being the two most commonly used text editors on Linux and Unix. Many operating systems, especially Linux and BSD derivatives, bundle multiple text editors with the operating system to cater to user demand. For example, a default installation of macOS contains Emacs, ed, nano, TextEdit, and Vim.




</doc>
<doc id="10270" url="https://en.wikipedia.org/wiki?curid=10270" title="Eastern Orthodox Church organization">
Eastern Orthodox Church organization

This article covers the organization of the Eastern Orthodox Churches rather than the doctrines, traditions, practices, or other aspects of Eastern Orthodoxy. Like the Catholic Church, the Eastern Orthodox Church claims to be the One, Holy, Catholic and Apostolic Church.

The term Western Orthodoxy is sometimes used to denominate what is technically a vicariate within the Antiochian Orthodox and the Russian Orthodox Churches and thus a part of the Eastern Orthodox Church as that term is defined here. The term "Western Orthodox Church" is disfavored by members of that vicariate.

In the 5th century, Oriental Orthodoxy separated from Chalcedonian Christianity (and is therefore separate from both the Eastern Orthodox and Catholic Church), well before the 11th century Great Schism. It should not be confused with Eastern Orthodoxy.

The Orthodox Church is a communion comprising the fifteen separate autocephalous hierarchical churches that recognize each other as "canonical" Orthodox Christian churches. Each constituent church is self-governing; its highest-ranking bishop (a patriarch, a metropolitan or an archbishop) reports to no higher earthly authority. Each regional church is composed of constituent eparchies (or dioceses) ruled by bishops. Some autocephalous churches have given an eparchy or group of eparchies varying degrees of autonomy (self-government). Such autonomous churches maintain varying levels of dependence on their mother church, usually defined in a Tomos or other document of autonomy. In many cases, autonomous churches are almost completely self-governing, with the mother church retaining only the right to appoint the highest-ranking bishop (an archbishop or metropolitan) of the autonomous church.

Normal governance is enacted through a synod of bishops within each church. In case of issues that go beyond the scope of a single church, multiple self-governing churches send representatives to a wider synod, sometimes wide enough to be called an Orthodox "ecumenical council". Such councils are deemed to have authority superior to that of any autocephalous church or its ranking bishop.

The Orthodox Church is decentralised, having no central authority, earthly head or a single Bishop in a leadership role. Thus, the Orthodox Church uses a synodical system canonically, which is significantly different from the hierarchically organised Catholic Church that follows the doctrine of papal supremacy. References to the Ecumenical Patriarch of Constantinople as a leader are an erroneous interpretation of his title ("first among equals"). His title is of honor rather than authority and in fact the Ecumenical Patriarch has no real authority over Churches other than the Constantinopolitan. His unique role often sees the Ecumenical Patriarch referred to as the "spiritual leader" of the Orthodox Church in some sources, though this is not an official title of the patriarch nor is it usually used in scholarly sources on the patriarchate.

The autocephalous churches are in full communion with each other, so any priest of any of those churches may lawfully minister to any member of any of them, and no member of any is excluded from any form of worship in any of the others, including reception of the Eucharist.

In the early Middle Ages, the One Holy Catholic and Apostolic Church was ruled by five patriarchs: the bishops of Rome, Constantinople, Alexandria, Antioch, and Jerusalem; these were collectively referred to as the Pentarchy. Each patriarch had jurisdiction over bishops in a specified geographic region. This continued until 927, when the autonomous Bulgarian Archbishopric became the first newly promoted patriarchate to join the original five.

The patriarch of Rome was "first in place of honor" among the five patriarchs. Disagreement about the limits of his authority was one of the causes of the Great Schism, conventionally dated to the year 1054, which split the church into the Catholic Church in the West, headed by the Bishop of Rome, and the Orthodox Church, led by the four eastern patriarchs. After the schism this honorary primacy shifted to the Patriarch of Constantinople, who had previously been accorded the second-place rank at the First Council of Constantinople.

Ranked in order of seniority, with the year of independence (autocephaly) given in parentheses, where applicable.




The four ancient patriarchates are the most senior, followed by the five junior patriarchates. Autocephalous archbishoprics follow the patriarchates in seniority, with the Church of Cyprus being the only ancient one (AD 431). In the diptychs of the Russian Orthodox Church and some of its daughter churches (e.g., the Orthodox Church in America), the ranking of the five junior patriarchal churches is different. Following the Russian Church in rank is Georgian, followed by Serbian, Romanian, and then Bulgarian Church. The ranking of the archbishoprics is the same.






<nowiki>*</nowiki>"Autonomy not universally recognised."


These are churches that have separated from the mainstream communion over issues of Ecumenism and Calendar reform since the 1920s. Due to what these churches perceive as being errors of modernism and ecumenism in mainstream Orthodoxy, they refrain from concelebration of the Divine Liturgy with the mainstream Orthodox, while maintaining that they remain fully within the canonical boundaries of the Church: i.e., professing Orthodox belief, retaining legitimate apostolic succession, and existing in communities with historical continuity. With the exception of the Orthodox Church of Greece (Holy Synod in Resistance), they will commune the faithful from all the canonical jurisdictions and are recognized by and in communion with the Russian Orthodox Church Outside Russia.

Due in part to the re-establishment of official ties between the Russian Orthodox Church Outside Russia and the Moscow Patriarchate, the "Orthodox Church of Greece (Holy Synod in Resistance)" has broken ecclesial communion with ROCOR, but the converse has not happened. Where the Old Calendar Romanian and Bulgarian churches stand on the matter is as yet unclear.

The Churches in resistance are:


These Churches do not practice Communion with any other Orthodox jurisdictions nor do they tend to recognize each other. Yet, like the "Churches in Resistance" above, they consider themselves to be within the canonical boundaries of the Church: i.e., professing Orthodox belief, retaining what they believe to be legitimate apostolic succession, and existing in communities with historical continuity. Nevertheless, their relationship with all other Orthodox Churches remains unclear, as Orthodox Churches normally recognize and are recognized by others.


The following Churches recognize all other mainstream Orthodox Churches, but are not recognized by any of them due to various disputes:


The following Churches use the term "Orthodox" in their name and carries belief or the traditions of Eastern Orthodox church, but blend beliefs and traditions from other denominations outside of Eastern Orthodoxy:





</doc>
<doc id="10271" url="https://en.wikipedia.org/wiki?curid=10271" title="EDT">
EDT

EDT may refer to:




</doc>
<doc id="10272" url="https://en.wikipedia.org/wiki?curid=10272" title="Electric guitar">
Electric guitar

An electric guitar is a guitar that uses one or more pickups to convert the vibration of its strings into electrical signals. The vibration occurs when a guitarist strums, plucks, fingerpicks, or taps the strings. The pickup used to sense the vibration generally uses electromagnetic induction to do so, though other technologies exist. In any case, the signal generated by an electric guitar is too weak to drive a loudspeaker, so it is sent to a guitar amplifier before being sent to the speaker, which converts it into audible sound. 

Since the output of an electric guitar is an electric signal, it can be electronically altered by to change the timbre of the sound. Often, the signal is modified using effects such as reverb and distortion and "overdrive", the latter effect is considered a key element of electric blues guitar music and rock guitar playing.

Invented in 1931, the amplified electric guitar was adopted by jazz guitarists, who wanted to play single-note guitar solos in large big band ensembles. Early proponents of the electric guitar on record included Les Paul, Lonnie Johnson, Sister Rosetta Tharpe, T-Bone Walker, and Charlie Christian. During the 1950s and 1960s, the electric guitar became the most important instrument in pop music. It has evolved into an instrument that is capable of a multitude of sounds and styles in genres ranging from pop and rock to country music, blues and jazz. It served as a major component in the development of electric blues, rock and roll, rock music, heavy metal music and many other genres of music.

Electric guitar design and construction vary greatly in the shape of the body and the configuration of the neck, bridge, and pickups. Guitars may have a fixed bridge or a spring-loaded hinged bridge that lets players "bend" the pitch of notes or chords up or down or perform vibrato effects. The sound of a guitar can be modified by new playing techniques such as string bending, tapping, hammering on, using audio feedback, or slide guitar playing. There are several types of electric guitar, including the solid-body guitar, various types of hollow-body guitars, the six-string guitar (the most common type, usually tuned E, A, D, G, B, E, from lowest to highest strings), the seven-string guitar, which typically adds a low B string below the low E, and the twelve-string electric guitar, which has six pairs of strings.

Popular music and rock groups often use the electric guitar in two roles: as a rhythm guitar, which plays the chord sequence or progression and riffs and sets the beat (as part of a rhythm section), and as a lead guitar, which is used to perform instrumental melody lines, melodic instrumental fill passages, and solos. In a small group, such as a power trio, one guitarist switches between both roles. In larger rock and metal bands, there is often a rhythm guitarist and a lead guitarist.
Many experiments at electrically amplifying the vibrations of a string instrument were made dating back to the early part of the 20th century. Patents from the 1910s show telephone transmitters were adapted and placed inside violins and banjos to amplify the sound. Hobbyists in the 1920s used carbon button microphones attached to the bridge; however, these detected vibration from the bridge on top of the instrument, resulting in a weak signal. With numerous people experimenting with electrical instruments in the 1920s and early 1930s, there are many claimants to have been the first to invent an electric guitar.
Electric guitars were originally designed by acoustic guitar makers and instrument manufacturers. Some of the earliest electric guitars adapted hollow-bodied acoustic instruments and used tungsten pickups. The first electrically amplified guitar was designed in 1931 by George Beauchamp, the general manager of the National Guitar Corporation, with Paul Barth, who was vice president. The maple body prototype for the one-piece cast aluminium "frying pan" was built by Harry Watson, factory superintendent of the National Guitar Corporation. Commercial production began in late summer of 1932 by the Ro-Pat-In Corporation (Electro-Patent-Instrument Company), in Los Angeles, a partnership of Beauchamp, Adolph Rickenbacker (originally Rickenbacher), and Paul Barth. In 1934, the company was renamed the Rickenbacker Electro Stringed Instrument Company. In that year Beauchamp applied for a United States patent for an "Electrical Stringed Musical Instrument" and the patent was issued in 1937.

By early-mid 1935, Electro String Instrument Corporation had achieved mainstream success with the "A-22" "Frying Pan" steel guitar, and set out to capture a new audience through its release of the "Electro-Spanish Model B" and the "Electro-Spanish Ken Roberts", which was the first full 25" scale electric guitar produced. The Electro-Spanish Ken Roberts provided players a full 25" scale, with 17 frets free of the fretboard. It is estimated that fewer than 50 Electro-Spanish Ken Roberts were constructed between 1933 and 1937; fewer than 10 are known to survive today.

The need for the amplified guitar became apparent during the big band era as orchestras increased in size, particularly when acoustic guitars had to compete with large, loud brass sections. The first electric guitars used in jazz were hollow archtop acoustic guitar bodies with electromagnetic transducers. Early electric guitar manufacturers include Rickenbacker in 1932; Dobro in 1933; National, AudioVox and Volu-tone in 1934; Vega, Epiphone (Electrophone and Electar), and Gibson in 1935 and many others by 1936.
Gibson's first production electric guitar, marketed in 1936, was the ES-150 model ("ES" for "Electric Spanish", and "150" reflecting the $150 price of the instrument, along with matching amplifier). The ES-150 guitar featured a single-coil, hexagonally shaped "bar" pickup, which was designed by Walt Fuller. It became known as the "Charlie Christian" pickup (named for the great jazz guitarist who was among the first to perform with the ES-150 guitar). The ES-150 achieved some popularity but suffered from unequal loudness across the six strings.
Early proponents of the electric guitar on record include Alvino Rey (Phil Spitalney Orchestra), Les Paul (Fred Waring Orchestra), Danny Stewart (Andy Iona Orchestra), George Barnes (under many aliases), Lonnie Johnson, Floyd Smith, Big Bill Broonzy, T-Bone Walker, George Van Eps, Charlie Christian (Benny Goodman Orchestra), Tampa Red, Memphis Minnie, and Arthur Crudup.

A functionally solid-body electric guitar was designed and built in 1940 by Les Paul from an Epiphone acoustic archtop. His "log guitar" (so called because it consisted of a simple 4x4 wood post with a neck attached to it and homemade pickups and hardware, with two detachable Epiphone hollow-body halves attached to the sides for appearance only) shares nothing in design or hardware with the solid-body Gibson Les Paul introduced in 1952. However, the feedback associated with hollow-bodied electric guitars was understood long before Paul's "log" was created in 1940; Gage Brewer's Ro-Pat-In of 1932 had a top so heavily reinforced that it essentially functioned as a solid-body instrument. In 1945, Richard D. Bourgerie made an electric guitar pickup and amplifier for professional guitar player George Barnes. Bourgerie worked through World War II at Howard Radio Company, making electronic equipment for the American military. Barnes showed the result to Les Paul, who then arranged for Bourgerie to have one made for him.

Electric guitar design and construction vary greatly in the shape of the body and the configuration of the neck, bridge, and pickups. However, some features are present on most guitars. The photo below shows the different parts of an electric guitar. The headstock (1) contains the metal machine heads (1.1), which use a worm gear for tuning. The nut (1.4)—a thin fret-like strip of metal, plastic, graphite or bone—supports the strings at the headstock end of the instrument. The frets (2.3) are thin metal strips that stop the string at the correct pitch when the player pushes a string against the fingerboard. The truss rod (1.2) is a metal rod (usually adjustable) that counters the tension of the strings to keep the neck straight. Position markers (2.2) provide the player with a reference to the playing position on the fingerboard.

The neck and fretboard (2.1) extend from the body. At the neck joint (2.4), the neck is either glued or bolted to the body. The body (3) is typically made of wood with a hard, polymerized finish. Strings vibrating in the magnetic field of the pickups (3.1, 3.2) produce an electric current in the pickup winding that passes through the tone and volume controls (3.8) to the output jack. Some guitars have piezo pickups, in addition to or instead of magnetic pickups.

Some guitars have a fixed bridge (3.4). Others have a spring-loaded hinged bridge called a "vibrato bar", "tremolo bar", or "whammy bar", which lets players bend notes or chords up or down in pitch or perform a vibrato embellishment. A plastic pickguard on some guitars protects the body from scratches or covers the control cavity, which holds most of the wiring.
The degree to which the choice of woods and other materials in the solid-guitar body (3) affects the sonic character of the amplified signal is disputed. Many believe it is highly significant, while others think the difference between woods is subtle. In acoustic and archtop guitars, wood choices more clearly affect tone.

Woods typically used in solid-body electric guitars include alder (brighter, but well rounded), swamp ash (similar to alder, but with more pronounced highs and lows), mahogany (dark, bassy, warm), poplar (similar to alder), and basswood (very neutral). Maple, a very bright tonewood, is also a popular body wood, but is very heavy. For this reason it is often placed as a "cap" on a guitar made primarily of another wood. Cheaper guitars are often made of cheaper woods, such as plywood, pine or agathis—not true hardwoods—which can affect durability and tone. Though most guitars are made of wood, any material may be used. Materials such as plastic, metal, and even cardboard have been used in some instruments.

The guitar output jack typically provides a monaural signal. Many guitars with active electronics use a jack with an extra contact normally used for stereo. These guitars use the extra contact to break the ground connection to the on-board battery to preserve battery life when the guitar is unplugged. These guitars require a mono plug to close the internal switch and connect the battery to ground. Standard guitar cables use a high-impedance 1/4-inch (6.35-mm) mono plug. These have a tip and sleeve configuration referred to as a TS phone connector. The voltage is usually around 1 to 9 millivolts.

A few guitars feature stereo output, such as Rickenbacker guitars equipped with "Rick-O-Sound". There are a variety of ways the "stereo" effect may be implemented. Commonly, but not exclusively, stereo guitars route the neck and bridge pickups to separate output buses on the guitar. A stereo cable then routes each pickup to its own signal chain or amplifier. For these applications, the most popular connector is a high-impedance 1/4-inch plug with a tip, ring and sleeve configuration, also known as a TRS phone connector. Some studio instruments, notably certain Gibson Les Paul models, incorporate a low-impedance three-pin XLR connector for balanced audio. Many exotic arrangements and connectors exist that support features such as midi and hexaphonic pickups.

The bridge and tailpiece, while serving separate purposes, work closely together to affect playing style and tone. There are four basic types of bridge and tailpiece systems on electric guitars. Within these four types are many variants.

A hard-tail guitar bridge anchors the strings at or directly behind the bridge and is fastened securely to the top of the instrument. These are common on carved-top guitars, such as the Gibson Les Paul and the Paul Reed Smith models, and on slab-body guitars, such as the Music Man Albert Lee and Fender guitars that are not equipped with a vibrato arm.

A "floating" or "trapeze" tailpiece (similar to a violin's) fastens to the body at the base of the guitar. These appear on Rickenbackers, Gretsches, Epiphones, a wide variety of archtop guitars, particularly Jazz guitars, and the 1952 Gibson Les Paul.

Pictured is a "tremolo arm" or "vibrato tailpiece" style bridge and tailpiece system, often called a "whammy bar" or "trem". It uses a lever ("vibrato arm") attached to the bridge that can temporarily slacken or tighten the strings to alter the pitch. A player can use this to create a vibrato or a portamento effect. Early vibrato systems were often unreliable and made the guitar go out of tune easily. They also had a limited pitch range. Later Fender designs were better, but Fender held the patent on these, so other companies used older designs for many years. 

With expiration of the Fender patent on the Stratocaster-style vibrato, various improvements on this type of internal, multi-spring vibrato system are now available. Floyd Rose introduced one of the first improvements on the vibrato system in many years when, in the late 1970s, he experimented with "locking" nuts and bridges that prevent the guitar from losing tuning, even under heavy vibrato bar use.

The fourth type of system employs string-through body anchoring. The strings pass over the bridge saddles, then through holes through the top of the guitar body to the back. The strings are typically anchored in place at the back of the guitar by metal ferrules. Many believe this design improves a guitar's sustain and timbre. 
A few examples of string-through body guitars are the Fender Telecaster Thinline, the Fender Telecaster Deluxe, the B.C. Rich IT Warlock and Mockingbird, and the Schecter Omen 6 and 7 series.

Compared to an acoustic guitar, which has a hollow body, electric guitars make much less audible sound when their strings are plucked, so electric guitars are normally plugged into a guitar amplifier and speaker. When an electric guitar is played, string movement produces a signal by generating (i.e., inducing) a small electric current in the magnetic pickups, which are magnets wound with coils of very fine wire. 
The signal passes through the tone and volume circuits to the output jack, and through a cable to an amplifier. The current induced is proportional to such factors as string density and the amount of movement over the pickups. 
Because in most cases it is desirable to isolate coil-wound pickups from the unintended sound of internal vibration of loose coil windings, a guitar's magnetic pickups are normally embedded or "potted" in wax, lacquer, or epoxy to prevent the pickup from producing a microphonic effect. Because of their natural inductive qualities, all magnetic pickups tend to pick up ambient, usually unwanted electromagnetic interference or EMI. The resulting hum is particularly strong with single-coil pickups, and it is aggravated by the fact that many vintage guitars are insufficiently shielded against electromagnetic interference. The most common source is 50- or 60-Hz hum from power transmission systems (house wiring, etc.). Since nearly all amplifiers and audio equipment associated with electric guitars must be plugged in, it is a continuing technical challenge to reduce or eliminate unwanted hum.

Double-coil or "humbucker" pickups were invented as a way to reduce or counter the unwanted ambient hum sounds (known as 60-cycle hum). Humbuckers have two coils of opposite magnetic and electric polarity to produce a differential signal. Electromagnetic noise that hits both coils equally tries to drive the pickup signal toward positive on one coil and toward negative on the other, which cancels out the noise. The two coils are wired in phase, so their signal adds together. This high combined inductance of the two coils leads to the richer, "fatter" tone associated with humbucking pickups.

Piezoelectric pickups use a "sandwich" of quartz crystal or other piezoelectric material, typically placed beneath the string saddles or nut. These devices respond to pressure changes from all vibration at these specific points.

Optical pickups are a type of pickup that sense string and body vibrations using infrared LED light. These pickups are not sensitive to EMI.

Some "hybrid" electric guitars are equipped with additional microphone, piezoelectric, optical, or other types of transducers to approximate an acoustic instrument tone and broaden the sonic palette of the instrument.

Electric guitar necks vary in composition and shape. The primary metric of guitar necks is the "scale length", which is the vibrating length of the strings from nut to bridge. A typical Fender guitar uses a 25.5-inch scale length, while Gibson uses a 24.75-inch scale length in their "Les Paul". While the scale length of the Les Paul is often described as 24.75 inches, it has varied through the years by as much as a half inch.

Frets are positioned proportionally to scale length—the shorter the scale length, the closer the fret spacing. Opinions vary regarding the effect of scale length on tone and feel. Popular opinion holds that longer scale length contributes to greater amplitude. Reports of playing feel are greatly complicated by the many factors involved in this perception. String gauge and design, neck construction and relief, guitar setup, playing style and other factors contribute to the subjective impression of playability or feel.

Necks are described as "bolt-on", "set-in", or "neck-through", depending on how they attach to the body. Set-in necks are glued to the body in the factory. They are said to have a warmer tone and greater sustain. This is the traditional type of joint. Leo Fender pioneered bolt-on necks on electric guitars to facilitate easy adjustment and replacement. Neck-through instruments extend the neck the length of the instrument, so that it forms the center of the body, and are known for long sustain and for being particularly sturdy. While a set-in neck can be carefully unglued by a skilled luthier, and a bolt-on neck can simply be unscrewed, a neck-through design is difficult or even impossible to repair, depending on the damage. Historically, the bolt-on style has been more popular for ease of installation and adjustment. Since bolt-on necks can be easily removed, there is an after-market in replacement bolt-on necks from companies such as Warmoth and Mighty Mite. Some instruments—notably most Gibson models—continue to use set-in glued necks. Neck-through bodies are somewhat more common in bass guitars.

Materials for necks are selected for dimensional stability and rigidity, and some allege that they influence tone. Hardwoods are preferred, with maple, mahogany, and ash topping the list. The neck and fingerboard can be made from different materials; for example, a guitar may have a maple neck with a rosewood or ebony fingerboard. In the 1970s, designers began to use exotic man-made materials such as aircraft-grade aluminum, carbon fiber, and ebonol. Makers known for these unusual materials include John Veleno, Travis Bean, Geoff Gould, and Alembic.

Aside from possible engineering advantages, some feel that in relation to the rising cost of rare tonewoods, man-made materials may be economically preferable and more ecologically sensitive. However, wood remains popular in production instruments, though sometimes in conjunction with new materials. Vigier guitars, for example, use a wooden neck reinforced by embedding a light, carbon fiber rod in place of the usual heavier steel bar or adjustable steel truss rod. After-market necks made entirely from carbon fiber fit existing bolt-on instruments. Few, if any, extensive formal investigations have been widely published that confirm or refute claims over the effects of different woods or materials on electric guitar sound.

Several neck shapes appear on guitars, including shapes known as C necks, U necks, and V necks. These refer to the cross-sectional shape of the neck (especially near the nut). Several sizes of fret wire are available, with traditional players often preferring thin frets, and metal shredders liking thick frets. Thin frets are considered better for playing chords, while thick frets allow lead guitarists to bend notes with less effort.

An electric guitar with a folding neck called the "Foldaxe" was designed and built for Chet Atkins by Roger C. Field. Steinberger guitars developed a line of exotic, carbon fiber instruments without headstocks, with tuning done on the bridge instead.

Fingerboards vary as much as necks. The fingerboard surface usually has a cross-sectional radius that is optimized to accommodate finger movement for different playing techniques. Fingerboard radius typically ranges from nearly flat (a very large radius) to radically arched (a small radius). The vintage Fender Telecaster, for example, has a typical small radius of approximately 7.25 inches. Some manufacturers have experimented with fret profile and material, fret layout, number of frets, and modifications of the fingerboard surface for various reasons. Some innovations were intended to improve playability by ergonomic means, such as Warmoth Guitars' compound radius fingerboard. Scalloped fingerboards added enhanced microtonality during fast legato runs. Fanned frets intend to provide each string with an optimal playing tension and enhanced musicality. Some guitars have no frets—and others, like the Gittler guitar, have no neck in the traditional sense.

While an acoustic guitar's sound depends largely on the vibration of the guitar's body and the air inside it, the sound of an electric guitar depends largely on the signal from the pickups. The signal can be "shaped" on its path to the amplifier via a range of effect devices or circuits that modify the tone and characteristics of the signal. Amplifiers and speakers also add coloration to the final sound.

Electric guitars usually have one to four magnetic pickups. Identical pickups produce different tones depending on location between the neck and bridge. Bridge pickups produce a bright or trebly timbre, and neck pickups are warmer or more bassy. The type of pickup also affects tone. Dual-coil pickups sound warm, thick, perhaps even muddy; single-coil pickups sound clear, bright, perhaps even biting. Guitars don't require a uniform pickup type: a common mixture is the "fat Strat" arrangement of one dual-coil at the bridge position and single coils in the middle and neck positions, known as HSS (humbucker/single/single). Some guitars have a piezoelectric pickup in addition to electromagnetic pickups. Piezo pickups produce a more acoustic sound. The piezo runs through a built-in equalizer (EQ) to improve similitude and control tone. A blend knob controls the mix between electromagnetic and piezoelectric sounds.

Where there is more than one pickup, a pickup selector switch is usually present to select or combine the outputs of two or more pickups, so that two-pickup guitars have three-way switches, and three-pickup guitars have five-way switches (a Gibson Les Paul three-pickup Black Beauty has a three-position toggle switch that configures bridge, bridge and middle [switch in middle position] and neck pickups). Further circuitry sometimes combines pickups in different ways. For instance, phase switching places one pickup out of phase with the other(s), leading to a "honky", "nasal", or "funky" sound. Individual pickups can also have their timbre altered by switches, typically coil tap switches that effectively short-circuit some of a dual-coil pickup's windings to produce a tone similar to a single-coil pickup (usually done with push-pull volume knobs).

The final stages of on-board sound-shaping circuitry are the volume control (potentiometer) and tone control (a low-pass filter which "rolls off" the treble frequencies). Where there are individual volume controls for different pickups, and where pickup signals can be combined, they would affect the timbre of the final sound by adjusting the balance between pickups from a straight 50:50.

The strings fitted to the guitar also have an influence on tone. Rock musicians often prefer the lightest gauge of roundwound string, which is easier to bend, while jazz musicians go for heavier, flatwound strings, which have a rich, dark sound. Steel, nickel, and cobalt are common string materials, and each gives a slightly different tone color. Recent guitar designs may incorporate much more complex circuitry than described above; see
Digital and synthesizer guitars, below.

The solid-body electric guitar does not produce enough sound for an audience to hear it in a performance setting unless it's electronically amplified—plugged into an amplifier, mixing console, or PA. 

Guitar amplifier design uses a different approach than sound reinforcement system power amplifiers and home "hi-fi" stereo systems. Audio amplifiers generally are intended to accurately reproduce the source signal without adding unwanted tonal coloration (i.e., they have a flat frequency response) or unwanted distortion. In contrast, most guitar amplifiers provide tonal coloration and overdrive or distortion of various types. A common tonal coloration sought by guitarists is rolling off some of the high frequencies. Along with a guitarist's playing style and choice of electric guitar and pickups, the choice of guitar amp model is a key part of a guitarist's unique tone. Many top guitarists are associated with a specific brand of guitar amp. As well, electric guitarists in blues, rock and many related sub-genres often intentionally choose amplifiers or effects units with controls that distort or alter the sound (to a greater or lesser degree).

In the 1950s and 1960s, some guitarists began exploring a wider range of tonal effects by distorting the sound of the instrument. To do this, they used overdrive — increasing the gain of the preamplifier beyond the level where the signal could be reproduced with little distortion, resulting in a "fuzzy" sound. This effect is called "clipping" by sound engineers, because when viewed with an oscilloscope, the wave forms of a distorted signal appear to have had their peaks "clipped off", in the process introducing additional tones (often approximating the harmonics characteristic of a square wave of that basic frequency). This was not actually a new development in the musical instrument or its supporting gear, but rather a shift of aesthetics, such sounds not having been thought desirable previously. Some distortion modes with an electric guitar increase the sustain of single notes and chords, which changes the sound of the instrument. In particular, distortion made it more feasible to perform guitar solos that used long, sustained notes.

After distortion became popular amongst rock music groups, guitar amplifier manufacturers included various provisions for it as part of amplifier design, making amps easier to overdrive, and providing separate "dirty" and "clean" channels so that distortion could easily be switched on and off. The distortion characteristics of vacuum tube amplifiers are particularly sought-after in blues and many rock music genres, and various attempts have been made to emulate them without the disadvantages (e.g., fragility, low power, expense) of actual tubes. Distortion, especially in tube based amplifiers, can come from several sources: power supply sag as more power is demanded than the supply can provide at a steady voltage, deliberate gain over drive of active elements, or alterations in the feedback provisions for various circuit stages.

Guitar amplifiers have long included at least a few effect units, often tone controls for bass and treble, an integrated tremolo system (sometimes incorrectly labeled (and marketed) as vibrato), or a mechanical spring reverb unit. In the 2010s, guitar amps often have onboard distortion effects. Some 2010-era amps provide multiple effects, such as chorus, flanger, phaser and octave down effects. The use of offboard effects such as stompbox pedals is made possible by either plugging the guitar into the external effect pedal and then plugging the effect pedal into the amp, or by using one or more effects loops, an arrangement that lets the player switch effects (electrically or mechanically) in or out of the signal path. In the signal chain, the effects loop is typically between the preamplifier stage and the power amplifier stages (though reverb units generally precede the effects loop an amplifier has both). This lets the guitarist add modulation effects to the signal after it passed through the preamplifier—which can be desirable, particularly with time-based effects such as delay. By the 2010s, guitar amplifiers usually included a distortion effect. Effects circuitry (whether internal to an amplifier or not) can be taken as far as amp modeling, by which is meant alteration of the electrical and audible behavior in such a way as to make an amp sound as though it were another (or one of several) amplifiers. When done well, a solid state amplifier can sound like a tube amplifier (even one with power supply sag), reducing the need to manage more than one amp. Some modeling systems even attempt to emulate the sound of different speakers/cabinets. Nearly all amp and speaker cabinet modeling is done digitally, using computer techniques (e.g., Digital Signal Processing or DSP circuitry and software). There is disagreement about whether this approach is musically satisfactory, and also whether this or that unit is more or less successful than another.

In the 1960s, the tonal palette of the electric guitar was further modified by introducing effect units in its signal path, before the guitar amp, of which one of the earliest units was the fuzz pedal. Effects units come in several formats, the most common of which are the stompbox "pedal" and the rackmount unit. A stomp box (or pedal) is a small metal or plastic box containing the circuitry, which is placed on the floor in front of the musician and connected in line with the patch cord connected to the instrument. The box is typically controlled by one or more foot-pedal on-off switches and it typically contains only one or two effects. Pedals are smaller than rackmount effects and usually less expensive. "Guitar pedalboards" are used by musicians who use multiple stomp-boxes; these may be a DIY project made with plywood or a commercial stock or custom-made pedalboard.

A rackmount effects unit may contain an electronic circuit nearly identical to a stompbox-based effect, but it is mounted in a standard 19" equipment rack, which is usually mounted in a road case that is designed to protect the equipment during transport. More recently, as signal-processing technology continuously becomes more feature-dense, rack-mount effects units frequently contain several types of effects. They are typically controlled by knobs or switches on the front panel, and often by a MIDI digital control interface.

Typical effects include:

In the 1970s, as effects pedals proliferated, their sounds were combined with tube amp distortion at lower, more controlled volumes by using power attenuators, such as Tom Scholz's Power Soak, as well as re-amplified dummy loads, such as Eddie Van Halen's use of dummy-load power resistor, post-power-tube effects, and a final solid-state amp driving the guitar speakers.

Recent amplifiers may include digital technology similar to modern effects pedals, including the ability to model or emulate a variety of classic amps.

A multi-effects device (also called a "multi-FX" device) is a single electronics effects pedal or rack-mount device that contains many electronic effects. In the late 1990s and throughout the 2000s, multi-FX manufacturers such as Zoom and Korg produced devices that were increasingly feature-laden. Multi-FX devices combine several effects together, and most devices allow users to use preset combinations of effects, including distortion, chorus, reverb, compression, and so on. This allows musicians to have quick on-stage access to different effects combinations. Some multi-FX pedals contain modelled versions of well-known effects pedals or amplifiers.

Multi-effects devices have garnered a large share of the effects device market, because they offer the user such a large variety of effects in a single package. A low-priced multi-effects pedal may provide 20 or more effects for the price of a regular single-effect pedal. More expensive multi-effect pedals may include 40 or more effects, amplifier modelling, and the ability to combine effects or modelled amp sounds in different combinations, as if the user was using multiple guitar amps. More expensive multi-effects pedals may also include more input and output jacks (e.g., an auxiliary input or a "dry" output), MIDI inputs and outputs, and an expression pedal, which can control volume or modify effect parameters (e.g., the rate of the simulated rotary speaker effect).

By the 1980s and 1990s, software effects became capable of replicating the analog effects used in the past. These new digital effects attempt to model the sound produced by analog effects and tube amps, with varying degrees of quality. There are many free guitar effects computer programs that can be downloaded from the Internet. Now, computers with sound cards can be used as digital guitar effects processors. Although digital and software effects offer many advantages, many guitarists still use analog effects.

In 2002, Gibson announced the first digital guitar, which performs analog-to-digital conversion internally. The resulting digital signal is delivered over a standard Ethernet cable, eliminating cable-induced line noise. The guitar also provides independent signal processing for each individual string. In 2003, modelling amplifier maker Line 6 introduced the Variax guitar. It differs in some fundamental ways from conventional solid-body electrics. It has on-board electronics capable of modelling the sound of a variety of unique guitars and some other stringed instruments. At one time, some models featured piezoelectric pickups instead of the conventional electromagnetic pickups.

The sound of a guitar can not only be adapted by electronic sound effects but is also heavily affected by various new techniques developed or becoming possible in combination with electric amplification. This is called extended technique.

Extended techniques include:



Other techniques, such as axial finger vibrato, pull-offs, hammer-ons, palm muting, harmonics and altered tunings, are also used on the classical and acoustic guitar. Shred guitar is a genre involving a number of extended techniques.

Unlike acoustic guitars, solid-body electric guitars have no vibrating soundboard to amplify string vibration. Instead, solid-body instruments depend on electric pickups and an amplifier (or amp) and speaker. The solid body ensures that the amplified sound reproduces the string vibration alone, thus avoiding the wolf tones and unwanted feedback associated with amplified acoustic guitars. These guitars are generally made of hardwood covered with a hard polymer finish, often polyester or lacquer. In large production facilities, the wood is stored for three to six months in a wood-drying kiln before being cut to shape. Premium custom-built guitars are frequently made with much older, hand-selected wood.

One of the first solid-body guitars was invented by Les Paul. Gibson did not present their Gibson Les Paul guitar prototypes to the public, as they did not believe the solid-body style would catch on. Another early solid-body Spanish style guitar, resembling what would become Gibson's Les Paul guitar a decade later, was developed in 1941 by O.W. Appleton, of Nogales, Arizona. Appleton made contact with both Gibson and Fender but was unable to sell the idea behind his "App" guitar to either company. In 1946, Merle Travis commissioned steel guitar builder Paul Bigsby to build him a solid-body Spanish-style electric. Bigsby delivered the guitar in 1948. The first mass-produced solid-body guitar was Fender Esquire and Fender Broadcaster (later to become the Fender Telecaster), first made in 1948, five years after Les Paul made his prototype. The Gibson Les Paul appeared soon after to compete with the Broadcaster. Another notable solid-body design is the Fender Stratocaster, which was introduced in 1954 and became extremely popular among musicians in the 1960s and 1970s for its wide tonal capabilities and more comfortable ergonomics than other models.

The history of Electric Guitars is summarized by Guitar World magazine, and the earliest electric guitar on their top 10 list is the Ro-Pat-In Electro A-25 “Frying Pan” (1932) described as 'The first-fully functioning solid-body electric guitar to be manufactured and sold'. The most recent electric guitar on this list is the Ibanez Jem (1987) which featured '24 frets', 'an impossibly thin neck' and was 'designed to be the ultimate shredder machine'. Numerous other important electric guitars are on the list including Gibson ES-150 (1936), Fender Telecaster (1951), Gibson Les Paul (1952), Gretsch 6128 Duo Jet (1953), Fender Stratocaster (1954), Rickenbacker 360/12 (1964), Van Halen Frankenstein (1975), Paul Reed Smith Custom (1985) many of these guitars were 'successors' to earlier designs . Electric Guitar designs eventually became culturally important and visually iconic, with various model companies selling miniature model versions of particularly famous electric guitars, for example the Gibson SG used by Angus Young from the group AC/DC.

Some solid-bodied guitars, such as the Gibson Les Paul Supreme, the PRS Singlecut, and the Fender Telecaster Thinline, among others, are built with hollows in the body. These hollows are designed specifically not to interfere with the critical bridge and string anchor point on the solid body. In the case of Gibson and PRS, these are called chambered bodies. The motivation for this may be to reduce weight, to achieve a semi-acoustic tone (see below) or both.

Semi-acoustic guitars have a hollow body (similar in depth to a solid-body guitar) and electronic pickups mounted on the body. They work in a similar way to solid-body electric guitars except that, because the hollow body also vibrates, the pickups convert a combination of string and body vibration into an electrical signal. Whereas chambered guitars are made, like solid-body guitars, from a single block of wood, semi-acoustic and full-hollowbody guitars bodies are made from thin sheets of wood. They do not provide enough acoustic volume for live performance, but they can be used unplugged for quiet practice. Semi-acoustics are noted for being able to provide a sweet, plaintive, or funky tone. They are used in many genres, including blues, funk, sixties pop, and indie rock. They generally have cello-style F-shaped sound holes. These can be blocked off to prevent feedback, as in B. B. King's famous Lucille. Feedback can also be reduced by making them with a solid block in the middle of the soundbox.

Full hollow-body guitars have large, deep bodies made of glued-together sheets, or "plates", of wood. They can often be played at the same volume as an acoustic guitar and therefore can be used unplugged at intimate gigs. They qualify as electric guitars inasmuch as they have fitted pickups. Historically, archtop guitars
with retrofitted pickups were among the very earliest electric guitars. The instrument originated during the Jazz Age, in the 1920s and 1930s, and are still considered the classic jazz guitar (nicknamed "jazzbox"). Like semi-acoustic guitars, they often have f-shaped sound holes.

Having humbucker pickups (sometimes just a neck pickup) and usually strung heavlly, jazzboxes are noted for their warm, rich tone. A variation with single-coil pickups, and sometimes with a Bigsby tremolo, has long been popular in country and rockabilly; it has a distinctly more twangy, biting tone than the classic jazzbox. The term "archtop" refers to a method of construction subtly different from the typical acoustic (or "folk" or "western" or "steel-string" guitar): the top is formed from a moderately thick (1 inch or 2–3 cm) piece of wood, which is then carved into a thin (0.1 in, or 2–3 mm) domed shape, whereas conventional acoustic guitars have a thin, flat top.

Some steel-string acoustic guitars are fitted with pickups purely as an alternative to using a separate microphone. They may also be fitted with a piezoelectric pickup under the bridge, attached to the bridge mounting plate, or with a low-mass microphone (usually a condenser mic) inside the body of the guitar that converts the vibrations in the body into electronic signals. Combinations of these types of pickups may be used, with an integral mixer/preamp/graphic equalizer. Such instruments are called electric acoustic guitars. They are regarded as acoustic guitars rather than electric guitars, because the pickups do not produce a signal directly from the vibration of the strings, but rather from the vibration of the guitar top or body.

Electric acoustic guitars should not be confused with semi-acoustic guitars, which have pickups of the type found on solid-body electric guitars, or solid-body hybrid guitars with piezoelectric pickups.

The one-string guitar is also known as the Unitar. Although rare, the one-string guitar is sometimes heard, particularly in Delta blues, where improvised folk instruments were popular in the 1930s and 1940s. Eddie "One String" Jones had some regional success. Mississippi blues musician Lonnie Pitchford played a similar, homemade instrument. In a more contemporary style, Little Willie Joe, the inventor of the Unitar, had a rhythm and blues instrumental hit in the 1950s with "Twitchy", recorded with the Rene Hall Orchestra.

The four-string guitar is better known as the tenor guitar. One of its best-known players was Tiny Grimes, who played on 52nd Street with the beboppers and played a major role in the Prestige Blues Swingers. Multi-instrumentalist Warren Ellis (musician) of Dirty Three and Nick Cave and the Bad Seeds is a contemporary player who includes a tenor guitar in his repertoire.

The four-string guitar is normally tuned CGDA, but some players, such as Tiny Grimes, tune to DGBE to preserve familiar 6-string guitar chord fingerings. The tenor guitar can also be tuned like a soprano, concert, or tenor ukulele, using versions of GCEA tuning.

Most seven-string guitars add a low B string below the low E. Both electric and classical guitars exist designed for this tuning. A high A string above the high E instead of the low B string is sometimes used. Another less common seven-string arrangement is a second G string situated beside the standard G string and tuned an octave higher, in the same manner as a twelve-stringed guitar (see below). Jazz guitarists using a seven-string include George Van Eps, Lenny Breau, Bucky Pizzarelli and his son John Pizzarelli.

Seven-string electric guitars were popularized among rock players in the 1980s by Steve Vai. Along with the Japanese guitar company Ibanez, Vai created the Universe series seven-string guitars in the 1980s, with a double locking tremolo system for a seven-string guitar. These models were based on Vai's six-string signature series, the Ibanez Jem. Seven-string guitars experienced a resurgence in popularity in the 2000s, championed by Deftones, Limp Bizkit, Slayer, KoRn, Fear Factory, Strapping Young Lad, Nevermore, Muse and other hard rock and metal bands. Metal musicians often prefer the seven-string guitar for its extended lower range. The seven-string guitar has also played an essential role in progressive metal rock and is commonly used in bands such as Dream Theater and Pain of Salvation and by experimental guitarists such as Ben Levin.

Eight-string electric guitars are rare but not unused. One is played by Charlie Hunter, which was manufactured by Novax Guitars. The largest manufacturer of eight- to 14-string instruments is Warr Guitars. Their models are used by Trey Gunn (ex King Crimson), who has his own "signature" line from the company. Similarly, Mårten Hagström and Fredrik Thordendal of Meshuggah used 8-string guitars made by Nevborn Guitars and now guitars by Ibanez. Munky of the nu metal band KoRn is also known to use seven-string Ibanez guitars, and it is rumored that he is planning to release a K8 eight-string guitar similar to his K7 seven-string guitar. Another Ibanez player is Tosin Abasi, lead guitarist of the progressive metal band Animals as Leaders, who uses an Ibanez RG2228 to mix bright chords with very heavy low riffs on the seventh and eighth strings. Stephen Carpenter of Deftones also switched from a seven-string to an eight-string in 2008 and released his signature STEF B-8 with ESP Guitars. In 2008, Ibanez released the Ibanez RG2228-GK, which is the first mass-produced eight-string guitar. Jethro Tull's first album uses a nine-string guitar. Bill Kelliher, guitarist for the heavy metal group Mastodon, worked with First Act on a custom mass-produced nine-string guitar.

B.C. Rich manufactured a ten-string six-course electric guitar, the Bich, whose radical shape positioned the machine heads for the four secondary strings onto the body, avoiding the head-heaviness of many electric twelve-string guitars. However, many players bought it for the body shape or electrics and simply removed the extra strings. The company recognized this and released six-string models of the Bich, a shape now generally incorporated into their standard Warlock.

Twelve-string electric guitars feature six pairs of strings, usually with each pair tuned to the same note. The extra E, A, D, and G strings add a note one octave above, and the extra B and E strings are in unison. The pairs of strings are played together as one, so the technique and tuning are the same as a conventional guitar, but they create a much fuller tone, with the additional strings adding a natural chorus effect. They are used almost solely to play harmony and rhythm parts, rather than for guitar solos. They are relatively common in folk rock music. Lead Belly is the folk artist most identified with the twelve-string guitar, usually acoustic with a pickup.

George Harrison of the Beatles and Roger McGuinn of the Byrds brought the electric twelve-string to notability in rock and roll. During the Beatles' first trip to the United States, in February 1964, Harrison received a new 360/12 model guitar from the Rickenbacker company, a twelve-string electric made to look onstage like a six-string. He began using the 360 in the studio on Lennon's "You Can't Do That" and other songs. McGuinn began using electric twelve-string guitars to create the jangly, ringing sound of the Byrds. Both Jimmy Page, the guitarist with Led Zeppelin, and Leo Kottke, a solo artist, are well known as twelve-string guitar players.

The third-bridge guitar is an electric prepared guitar with an additional, third bridge. This can be a normal guitar with, for instance, a screwdriver placed under the strings, or it can be a custom-made instrument. Lee Ranaldo of Sonic Youth plays with a third bridge.

Double-neck (or, less commonly, "twin-neck") guitars enable guitarists to play both guitar and bass guitar or, more commonly, both a six-string and a twelve-string. In the mid-1960s, one of the first players to use this type of guitar was Paul Revere & the Raiders' guitarist Drake Levin. Another early user was John McLaughlin. The double-neck guitar was popularized by Jimmy Page, who used a custom-made, cherry-finished Gibson EDS-1275 to perform "Stairway to Heaven", "The Song Remains the Same" and "The Rain Song", although for the recording of "Stairway to Heaven" he used a Fender Telecaster and a Fender XII electric twelve-string. Mike Rutherford of Genesis and Mike + the Mechanics is also famous for his use of a double-neck guitar during live shows. Don Felder of the Eagles used the Gibson EDS-1275 during the Hotel California tour. Muse guitarist and vocalist Matthew Bellamy uses a silver Manson double-neck on his band's Resistance Tour. Rush guitarist Alex Lifeson is also known for using double-neck guitars in the live performance of several songs. In performances of the song "Xanadu" during the band's 2015 R40 anniversary tour, Lifeson played a white Gibson EDS-1275 double-neck guitar with six-string and twelve-string necks, while bassist Geddy Lee performed with a double-neck Rickenbacker guitar with four-string bass and twelve-string guitar necks.

Popular music typically uses the electric guitar in two roles: as a rhythm guitar to provide the basic chord progression and rhythm, and a "lead guitar" that plays melody lines, melodic instrumental fill passages, and solos. In some bands with two guitarists, both may play in tandem, and trade off rhythm and lead roles. In bands with a single guitarist, the guitarist may switch between these roles, playing chords to accompany the singer's lyrics, and a solo.

In the most commercially available and consumed pop and rock genres, electric guitars tend to dominate their acoustic cousins in both the recording studio and live venues, especially in the "harder" genres such as heavy metal and hard rock. However the acoustic guitar remains a popular choice in country, western and especially bluegrass music, and it is widely used in folk music. Even metal and hard rock guitarists play acoustic guitars for some ballads and for MTV unplugged acoustic performances.

Jazz guitar playing styles include rhythm guitar-style "comping" (accompanying) with jazz chord voicings (and in some cases, walking basslines) and "blowing" (improvising solos) over jazz chord progressions with jazz-style phrasing and ornaments. The accompanying style for electric guitar in most jazz styles differs from the way chordal instruments accompany in many popular styles of music. In rock and pop, the rhythm guitarist typically performs chords in dense and regular fashion to define a tune's rhythm. Simpler music tends to use chord voicings focused on the first, third, and fifth notes of the chord. In contrast, more complex music styles of pop might intermingle periodic chords and delicate voicings into pauses in the melody or solo. Complex guitar chord voicings are often have no root, especially in chords that have more than six notes. Such chords typically emphasize the third and seventh notes of the chord. These chords also often include the 9th, 11th and 13th notes of the chord, which are called "extensions", or "color notes".

When guitarists who play jazz and other more complex styles improvise, they use scales, modes, and arpeggios associated with the chord progression. The must learn how to use scales (whole tone scale, chromatic scale, etc.) to solo over chord progressions. Soloists try to imbue melodic phrasing with the sense of natural breathing and legato phrasing used by players of other instruments. Jazz guitarists are influenced by trumpet, saxophone, and other horn players. Celtic fingerstyle players are influenced pipes and fiddles.

Jazz guitarists typically play hollow-body instruments, but also use solid-body guitars. Hollow-body instruments were the first guitars used in jazz in the 1930s and 1940s. During the 1970s jazz fusion era, many jazz guitarists switched to the solid body guitars that dominated the rock world, using powerful guitar amps for volume.

Until the 1950s, the acoustic, nylon-stringed classical guitar was the only type of guitar favored by classical, or art music composers. In the 1950s a few contemporary classical composers began to use the electric guitar in their compositions. Examples of such works include Luciano Berio's "Nones" (1954) Karlheinz Stockhausen's "Gruppen" (1955–57); Donald Erb's "String Trio" (1966), Morton Feldman's "The Possibility of a New Work for Electric Guitar" (1966); George Crumb's "Songs, Drones, and Refrains of Death" (1968); Hans Werner Henze's "Versuch über Schweine" (1968); Francis Thorne's "Sonar Plexus" (1968) and "Liebesrock" (1968–69), Michael Tippett's "The Knot Garden" (1965–70); Leonard Bernstein's "MASS" (1971) and "Slava!" (1977); Louis Andriessen's "De Staat" (1972–76); Helmut Lachenmann's "Fassade, für grosses Orchester" (1973, rev. 1987), Valery Gavrilin "Anyuta" (1982), Steve Reich's "Electric Counterpoint" (1987), Arvo Pärt's "Miserere" (1989/92), György Kurtág's "Grabstein für Stephan" (1989), and countless works composed for the quintet of Ástor Piazzolla.
Alfred Schnittke also used electric guitar in several works, like the "Requiem", "Concerto Grosso N°2" and "Symphony N°1".

In the 1970s, 1980s and 1990s, a growing number of composers (many of them composer-performers who had grown up playing the instrument in rock bands) began writing contemporary classical music for the electric guitar. These include Frank Zappa, Shawn Lane, Steven Mackey, Nick Didkovsky, Scott Johnson, Lois V Vierk, Tim Brady, Tristan Murail, and Randall Woolf.

Yngwie Malmsteen released his Concerto Suite for Electric Guitar and Orchestra in 1998, and Steve Vai released a double-live CD entitled "Sound Theories", of his work with the Netherlands Metropole Orchestra in June 2007. The American composers Rhys Chatham and Glenn Branca have written "symphonic" works for large ensembles of electric guitars, in some cases numbering up to 100 players, and the instrument is a core member of the Bang on a Can All-Stars (played by Mark Stewart). Still, like many electric and electronic instruments, the electric guitar remains primarily associated with rock and jazz music, rather than with classical compositions and performances. R. Prasanna plays a style of Indian classical music (Carnatic music) on the electric guitar.

In the 21st century, European avant garde composers like Richard Barrett, Fausto Romitelli, Peter Ablinger, Bernhard Lang, Claude Ledoux and Karlheinz Essl have used the electric guitar (together with extended playing techniques) in solo pieces or ensemble works. Probably the most ambitious and perhaps significant work to date is "Ingwe" (2003–2009) by Georges Lentz (written for Australian guitarist Zane Banks), a 60-minute work for solo electric guitar, exploring that composer's existential struggles and taking the instrument into realms previously unknown in a concert music setting.

In Vietnam, electric guitars are often used as an instrument in cải lương music (traditional southern Vietnamese folk opera), sometimes as a substitute for certain traditional stringed instruments like the Đàn nguyệt (two-stringed lute) when they are not available. Electric guitars used in cải lương are played in finger vibrato (string bending), with no amplifiers or sound effects.




</doc>
<doc id="10273" url="https://en.wikipedia.org/wiki?curid=10273" title="Embryo drawing">
Embryo drawing

Embryo drawing is the illustration of embryos in their developmental sequence. In plants and animals, an embryo develops from a zygote, the single cell that results when an egg and sperm fuse during fertilization. In animals, the zygote divides repeatedly to form a ball of cells, which then forms a set of tissue layers that migrate and fold to form an early embryo. Images of embryos provide a means of comparing embryos of different ages, and species. To this day, embryo drawings are made in undergraduate developmental biology lessons.

Comparing different embryonic stages of different animals is a tool that can be used to infer relationships between species, and thus biological evolution. This has been a source of quite some controversy, both now and in the past. Ernst Haeckel pioneered in this field. By comparing different embryonic stages of different vertebrate species, he formulated the recapitulation theory. This theory states that an animal's embryonic development follows exactly the same sequence as the sequence of its evolutionary ancestors. Haeckel's work and the ensuing controversy linked the fields of developmental biology and comparative anatomy into comparative embryology. From a more modern perspective, Haeckel's drawings were the beginnings of the field of evolutionary developmental biology (evo-devo).

The study of comparative embryology aims to prove or disprove that vertebrate embryos of different classes (e.g. mammals vs. fish) follow a similar developmental path due to their common ancestry. Such developing vertebrates have similar genes, which determine the basic body plan. However, further development allows for the distinguishing of distinct characteristics as adults.

In current biology, fundamental research in developmental biology and evolutionary developmental biology is no longer driven by morphological comparisons between embryos, but more by molecular biology. This is partly because Haeckel's drawings were very inaccurate.

The exactness of Ernst Haeckel's drawings of embryos has caused much controversy among Intelligent Design proponents recently and Haeckel's intellectual opponents in the past. Although the early embryos of different species exhibit similarities, Haeckel apparently exaggerated these similarities in support of his Recapitulation theory, sometimes known as the Biogenetic Law or "Ontogeny recapitulates phylogeny". Furthermore, Haeckel even proposed theoretical life-forms to accommodate certain stages in embryogenesis. A recent review concluded that the "biogenetic law is supported by several recent studies - if applied to single characters only".

Critics in the late 19th and early 20th centuries, Karl von Baer and Wilhelm His, did not believe that living embryos reproduce the evolutionary process and produced embryo drawings of their own which emphasized the differences in early embryological development. Late 20th and early 21st century critic Stephen Jay Gould have objected to the continued use of Haeckel’s embryo drawings in textbooks.

On the other hand, Michael K. Richardson, Professor of Evolutionary Developmental Zoology, Leiden University, while recognizing that some criticisms of the drawings are legitimate (indeed, it was he and his co-workers who began the modern criticisms in 1998), has supported the drawings as teaching aids, and has said that "on a fundamental level, Haeckel was correct"

Haeckel’s illustrations show vertebrate embryos at different stages of development, which exhibit embryonic resemblance as support for evolution, recapitulation as evidence of the Biogenetic Law, and phenotypic divergence as evidence of von Baer’s laws. The series of twenty-four embryos from the early editions of Haeckel’s "Anthropogenie" remain the most famous. The different species are arranged in columns, and the different stages in rows. Similarities can be seen along the first two rows; the appearance of specialized characters in each species can be seen in the columns and a diagonal interpretation leads one to Haeckel’s idea of recapitulation.

Haeckel’s embryo drawings are primarily intended to express his idiosyncratic theory of embryonic development, the Biogenetic Law, which in turn assumes (but is not crucial to) the evolutionary concept of common descent. His postulation of embryonic development coincides with his understanding of evolution as a developmental process. In and around 1800, embryology fused with comparative anatomy as the primary foundation of morphology. Ernst Haeckel, along with Karl von Baer and Wilhelm His, are primarily influential in forming the preliminary foundations of ‘phylogenetic embryology’ based on principles of evolution. Haeckel’s ‘Biogenetic Law’ portrays the parallel relationship between an embryo’s development and phylogenetic history. The term, ‘recapitulation,’ has come to embody Haeckel’s Biogenetic Law, for embryonic development is a recapitulation of evolution. Haeckel proposes that all classes of vertebrates pass through an evolutionarily conserved “phylotypic” stage of development, a period of reduced phenotypic diversity among higher embryos. Only in later development do particular differences appear. Haeckel portrays a concrete demonstration of his Biogenetic Law through his ‘Gastrea’ theory, in which he argues that the early cup-shaped gastrula stage of development is a universal feature of multi-celled animals. An ancestral form existed, known as the gastrea, which was a common ancestor to the corresponding gastrula.

Haeckel argues that certain features in embryonic development are conserved and palingenetic, while others are caenogenetic. Caenogenesis represents “the blurring of ancestral resemblances in development,” which are said to be the result of certain adaptations to embryonic life due to environmental changes. In his drawings, Haeckel cites the notochord, pharyngeal arches and clefts, pronephros and neural tube as palingenetic features. However, the yolk sac, extra-embryonic membranes, egg membranes and endocardial tube are considered caenogenetic features. The addition of terminal adult stages and the telescoping, or driving back, of such stages to descendant’s embryonic stages are likewise representative of Haeckelian embryonic development. In addressing his embryo drawings to a general audience, Haeckel does not cite any sources, which gives his opponents the freedom to make assumptions regarding the originality of his work.

Haeckel was not the only one to create a series of drawings representing embryonic development. Karl E. von Baer and Haeckel both struggled to model one of the most complex problems facing embryologists at the time: the arrangement of general and special characters during development in different species of animals. In relation to developmental timing, von Baer's scheme of development differs from Haeckel's scheme. Von Baer's scheme of development need not be tied to developmental stages defined by particular characters, where recapitulation involves heterochrony. Heterochrony represents a gradual alteration in the original phylogenetic sequence due to embryonic adaptation.
As well, von Baer early noted that embryos of different species could not be easily distinguished from one another as in adults.

Von Baer’s laws governing embryonic development are specific rejections of recapitulation. As a response to Haeckel’s theory of recapitulation, von Baer enunciates his most notorious laws of development. Von Baer’s laws state that general features of animals appear earlier in the embryo than special features, where less general features stem from the most general, each embryo of a species departs more and more from a predetermined passage through the stages of other animals, and there is never a complete morphological similarity between an embryo and a lower adult. Von Baer’s embryo drawings display that individual development proceeds from general features of the developing embryo in early stages through differentiation into special features specific to the species, establishing that linear evolution could not occur. Embryological development, in von Baer’s mind, is a process of differentiation, "a movement from the more homogeneous and universal to the more heterogeneous and individual."

Von Baer argues that embryos will resemble each other before attaining characteristics differentiating them as part of a specific family, genus or species, but embryos are not the same as the final forms of lower organisms.

Wilhelm His was one of Haeckel’s most authoritative and primary opponents advocating physiological embryology. His "Anatomie menschlicher Embryonen" (Anatomy of human embryos) employs a series of his most important drawings chronicling developing embryos from the end of the second week through the end of the second month of pregnancy. His, in opposition to Haeckel, seeks to take human embryos out of the hands of Darwinist proponents. In 1878, His begins to engage in serious study of the anatomy of human embryos for his drawings. During the 19th century, embryologists often obtained early human embryos from abortions and miscarriages, postmortems of pregnant women and collections in anatomical museums. In order to construct his series of drawings, His collected specimens which he manipulated into a form that he could operate with.

In His’ "Normentafel", he displays specific individual embryos rather than ideal types. His does not produce norms from aborted specimens, but rather visualizes the embryos in order to make them comparable and specifically subjects his embryo specimens to criticism and comparison with other cases. Ultimately, His’ critical work in embryonic development comes with his production of a series of embryo drawings of increasing length and degree of development. His’ depiction of embryological development strongly differs from Haeckel’s depiction, for His argues that the phylogenetic explanation of ontogenetic events is unnecessary. His argues that all ontogenetic events are the “mechanical” result of differential cell growth. His’ embryology is not explained in terms of ancestral history.

The debate between Haeckel and His ultimately becomes fueled by the description of an embryo that Wilhelm Krause propels directly into the ongoing feud between Haeckel and His. Haeckel speculates that the allantois is formed in a similar way in both humans and other mammals. His, on the other hand, accuses Haeckel of altering and playing with the facts. Although Haeckel is proven right about the allantois, the utilization of Krause’s embryo as justification turns out to be problematic, for the embryo is that of a bird rather than a human. The underlying debate between Haeckel and His derives from differing viewpoints regarding the similarity or dissimilarity of vertebrate embryos. In response to Haeckel’s evolutionary claim that all vertebrates are essentially identical in the first month of embryonic life as proof of common descent, His responds by insisting that a more skilled observer would recognize even sooner that early embryos can be distinguished. His also counteracts Haeckel’s sequence of drawings in the "Anthropogenie" with what he refers to as “exact” drawings, highlighting specific differences. Ultimately, His goes so far as to accuse Haeckel of “faking” his embryo illustrations to make the vertebrate embryos appear more similar than in reality. His also accuses Haeckel of creating early human embryos that he conjured in his imagination rather than obtained through empirical observation. His completes his denunciation of Haeckel by pronouncing that Haeckel had “‘relinquished the right to count as an equal in the company of serious researchers.’”

Haeckel encountered numerous oppositions to his artistic depictions of embryonic development during the late nineteenth and early twentieth centuries. Haeckel’s opponents believe that he de-emphasizes the differences between early embryonic stages in order to make the similarities between embryos of different species more pronounced.

The first suggestion of fakery against Haeckel was made in late 1868 by Ludwig Rutimeyer in the "Archiv für Anthropogenie". Rutimeyer was a professor of zoology and comparative anatomy at the University of Basel, who rejected natural selection as simply mechanistic and proposed an anti-materialist view of nature. Rutimeyer claimed that Haeckel “had taken to kinds of liberty with established truth.” Rutimeyer claimed that Haeckel presented the same image three consecutive times as the embryo of the dog, the chicken, and the turtle. Although Rutimeyer did not denounce Haeckel’s embryo drawings as fraud, he argued that such drawings are manipulations of public and scientific thought.

Theodor Bischoff (1807–1882), was a strong opponent of Darwinism. As a pioneer in mammalian embryology, he was one of Haeckel’s strongest critics. Although Bischoff’s 1840 surveys depict how similar the early embryos of man are to other vertebrates, he later demanded that such hasty generalization was inconsistent with his recent findings regarding the dissimilarity between hamster embryos and those of rabbits and dogs. Nevertheless, Bischoff’s main argument was in reference to Haeckel’s drawings of human embryos, for Haeckel is later accused of miscopying the dog embryo from him. Throughout Haeckel’s time, criticism of his embryo drawings was often due in part to his critics' belief in his representations of embryological development as “crude schemata.” In this way, Haeckel specifically selected relevant features to portray in his drawings. Haeckel’s opponents found his methods problematic because such simplification eliminates certain structures that differentiate between higher and lower vertebrates. In 1877, Rudolf Virchow (1821–1902), once an inspiration to Haeckel at Würzburg, proclaimed that Haeckel’s embryo drawings represent mere hypotheses.

Michael Richardson and his colleagues in a July 1997 issue of "Anatomy and Embryology", demonstrated that Haeckel fudged his drawings in order to exaggerate the similarity of the phylotypic stage.
In a March 2000 issue of "Natural History", Stephen Jay Gould argued that Haeckel "exaggerated the similarities by idealizations and omissions." As well, Gould argued that Haeckel’s drawings are simply inaccurate and falsified. On the other hand, one of those who criticized Haeckel's drawings, Michael Richardson, has argued that "Haeckel's much-criticized drawings are important as phylogenetic hypotheses, teaching aids, and evidence for evolution".
But even Richardson admitted in "Science" Magazine in 1997 that his team's investigation of Haeckel's drawings were showing them to be "one of the most famous fakes in biology."

Some version of Haeckel’s drawings can be found in many modern biology textbooks in discussions of the history of embryology, with clarification that these are no longer considered valid .

Although Charles Darwin accepted Haeckel's support for natural selection, he was tentative in using Haeckel's ideas in his writings; with regard to embryology, Darwin relied far more on von Baer's work. Haeckel's work was published in 1866 and 1874, years after Darwin's "The Origin of Species" (1859).

Despite the numerous oppositions, Haeckel has influenced many disciplines in science in his drive to integrate such disciplines of taxonomy and embryology into the Darwinian framework and to investigate phylogenetic reconstruction through his Biogenetic Law. As well, Haeckel served as a mentor to many important scientists, including Anton Dohrn, Richard and Oscar Hertwig, Wilhelm Roux, and Hans Driesch.

One of Haeckel's earliest proponents was Carl Gegenbaur at the University of Jena (1865–1873), during which both men were absorbing the impact of Darwin's theory. The two quickly sought to integrate their knowledge into an evolutionary program. In determining the relationships between "phylogenetic linkages" and "evolutionary laws of form," both Gegenbaur and Haeckel relied on a method of comparison. As Gegenbaur argued, the task of comparative anatomy lies in explaining the form and organization of the animal body in order to provide evidence for the continuity and evolution of a series of organs in the body. Haeckel then provided a means of pursuing this aim with his biogenetic law, in which he proposed to compare an individual's various stages of development with its ancestral line. Although Haeckel stressed comparative embryology and Gegenbaur promoted the comparison of adult structures, both believed that the two methods could work in conjunction to produce the goal of evolutionary morphology.

The philologist and anthropologist, Friedrich Müller, used Haeckel's concepts as a source for his ethnological research, involving the systematic comparison of the folklore, beliefs and practices of different societies. Müller's work relies specifically on theoretical assumptions that are very similar to Haeckel's and reflects the German practice to maintain strong connections between empirical research and the philosophical framework of science. Language is particularly important, for it establishes a bridge between natural science and philosophy. For Haeckel, language specifically represented the concept that all phenomena of human development relate to the laws of biology. Although Müller did not specifically have an influence in advocating Haeckel's embryo drawings, both shared a common understanding of development from lower to higher forms, for Müller specifically saw humans as the last link in an endless chain of evolutionary development.

Modern acceptance of Haeckel's Biogenetic Law, despite current rejection of Haeckelian views, finds support in the certain degree of parallelism between ontogeny and phylogeny. A. M. Khazen, on the one hand, states that "ontogeny is obliged to repeat the main stages of phylogeny." A. S. Rautian, on the other hand, argues that the reproduction of ancestral patterns of development is a key aspect of certain biological systems. Dr. Rolf Siewing acknowledges the similarity of embryos in different species, along with the laws of von Baer, but does not believe that one should compare embryos with adult stages of development. According to M. S. Fischer, reconsideration of the Biogenetic Law is possible as a result of two fundamental innovations in biology since Haeckel's time: cladistics and developmental genetics.

In defense of Haeckel's embryo drawings, the principal argument is that of "schematisation." Haeckel's drawings were not intended to be technical and scientific depictions, but rather schematic drawings and reconstructions for a specifically lay audience. Therefore, as R. Gursch argues, Haeckel's embryo drawings should be regarded as "reconstructions." Although his drawings are open to criticism, his drawings should not be considered falsifications of any sort. Although modern defense of Haeckel's embryo drawings still considers the inaccuracy of his drawings, charges of fraud are considered unreasonable. As Erland Nordenskiöld argues, charges of fraud against Haeckel are unnecessary. R. Bender ultimately goes so far as to reject His's claims regarding the fabrication of certain stages of development in Haeckel's drawings, arguing that Haeckel's embryo drawings are faithful representations of real stages of embryonic development in comparison to published embryos.

Haeckel's embryo drawings, as comparative plates, were at first only copied into biology textbooks, rather than texts on the study of embryology. Even though Haeckel's program in comparative embryology virtually collapsed after the First World War, his embryo drawings have often been reproduced and redrawn with increased precision and accuracy in works that have kept the study of comparative embryology alive. Nevertheless, neither His-inspired human embryology nor developmental biology are concerned with the comparison of vertebrate embryos. Although Stephen Jay Gould's 1977 book "Ontogeny and Phylogeny" helps to reassess Haeckelian embryology, it does not address the controversy over Haeckel's embryo drawings. Nevertheless, new interest in evolution in and around 1977 inspired developmental biologists to look more closely at Haeckel's illustrations.




</doc>
<doc id="10274" url="https://en.wikipedia.org/wiki?curid=10274" title="Enthalpy">
Enthalpy

Enthalpy is a property of a thermodynamic system. The enthalpy of a system is equal to the system's internal energy plus the product of its pressure and volume.

The unit of measurement for enthalpy in the International System of Units (SI) is the joule. Other historical conventional units still in use include the British thermal unit (BTU) and the calorie.

Enthalpy comprises a system's internal energy, which is the energy required to create the system, plus the amount of work required to make room for it by displacing its environment and establishing its volume and pressure.

Enthalpy is defined as a state function that depends only on the prevailing equilibrium state identified by the system's internal energy, pressure, and volume. It is an extensive quantity.

Enthalpy is the preferred expression of system energy changes in many chemical, biological, and physical measurements at constant pressure, because it simplifies the description of energy transfer. At constant pressure, the enthalpy change equals the energy transferred from the environment through heating or work other than expansion work.

The total enthalpy, "H", of a system cannot be measured directly. The same situation exists in classical mechanics: only a change or difference in energy carries physical meaning. Enthalpy itself is a thermodynamic potential, so in order to measure the enthalpy of a system, we must refer to a defined reference point; therefore what we measure is the change in enthalpy, Δ"H". The Δ"H" is a positive change in endothermic reactions, and negative in heat-releasing exothermic processes.

For processes under constant pressure, Δ"H" is equal to the change in the internal energy of the system, plus the pressure-volume work that the system has done on its surroundings. This means that the change in enthalpy under such conditions is the heat absorbed (or released) by the material through a chemical reaction or by external heat transfer. Enthalpies for chemical substances at constant pressure assume standard state: most commonly 1 bar pressure. Standard state does not, strictly speaking, specify a temperature (see standard state), but expressions for enthalpy generally reference the standard heat of formation at 25 °C.

Enthalpy of ideal gases and incompressible solids and liquids does not depend on pressure, unlike entropy and Gibbs energy. Real materials at common temperatures and pressures usually closely approximate this behavior, which greatly simplifies enthalpy calculation and use in practical designs and analyses.

The word "enthalpy" stems from the Ancient Greek verb "enthalpein" (), which means "to warm in". It combines the Classical Greek prefix "en-", meaning "to put into", and the verb "thalpein", meaning "to heat". The word "enthalpy" is often incorrectly attributed to Benoît Paul Émile Clapeyron and Rudolf Clausius through the 1850 publication of their Clausius–Clapeyron relation. This misconception was popularized by the 1927 publication of "The Mollier Steam Tables and Diagrams". However, neither the concept, the word, nor the symbol for enthalpy existed until well after Clapeyron's death.

The earliest writings which contained the concept of enthalpy did not appear until 1875,
when Josiah Willard Gibbs introduced "a heat function for constant pressure". However, Gibbs did not use the word "enthalpy" in his writings.

The actual word first appears in the scientific literature in a 1909 publication by J. P. Dalton. According to that publication, Heike Kamerlingh Onnes actually coined the word.

Over the years, scientists used many different symbols to denote enthalpy. In 1922 Alfred W. Porter proposed the symbol ""H"" as a standard,
thus finalizing the terminology still in use today.

In the past, enthalpy was sometimes called "heat content". The fact that the change in enthalpy ΔH equals the heat absorbed in processes at constant pressure is the reason for this name. However this equality is not true in general (when the pressure varies), so that the term "heat content" is considered misleading and is now deprecated.

The enthalpy of a thermodynamic system is defined as
where
Enthalpy is an extensive property. This means that, for homogeneous systems, the enthalpy is proportional to the size of the system. It is convenient to introduce the specific enthalpy "h" = , where "m" is the mass of the system, or the molar enthalpy "H" = , where "n" is the number of moles ("h" and "H" are intensive properties). For inhomogeneous systems the enthalpy is the sum of the enthalpies of the composing subsystems:

where the label "k" refers to the various subsystems. In case of continuously varying "p", "T" or composition, the summation becomes an integral:

where "ρ" is the density.

The enthalpy of homogeneous systems can be viewed as function "H"("S","p") of the entropy "S" and the pressure "p", and a differential relation for it can be derived as follows. We start from the first law of thermodynamics for closed systems for an infinitesimal process:

Here, "δQ" is a small amount of heat added to the system, and "δW" a small amount of work performed by the system. In a homogeneous system only reversible processes can take place, so the second law of thermodynamics gives , with "T" the absolute temperature of the system. Furthermore, if only "pV" work is done, . As a result,

Adding "d"("pV") to both sides of this expression gives

or

So

The above expression of "dH" in terms of entropy and pressure may be unfamiliar to some readers. However, there are expressions in terms of more familiar variables such as temperature and pressure:

Here "C" is the heat capacity at constant pressure and "α" is the coefficient of (cubic) thermal expansion:

With this expression one can, in principle, determine the enthalpy if "C" and "V" are known as functions of "p" and "T".

Note that for an ideal gas, "αT" = 1, so that

In a more general form, the first law describes the internal energy with additional terms involving the chemical potential and the number of particles of various types. The differential statement for "dH" then becomes

where "μ" is the chemical potential per particle for an "i"-type particle, and "N" is the number of such particles. The last term can also be written as (with "dn" the number of moles of component "i" added to the system and, in this case, "μ" the molar chemical potential) or as (with "dm" the mass of component "i" added to the system and, in this case, "μ" the specific chemical potential).

The "U" term can be interpreted as the energy required to create the system, and the "pV" term as the work that would be required to "make room" for the system if the pressure of the environment remained constant. When a system, for example, "n" moles of a gas of volume "V" at pressure "p" and temperature "T", is created or brought to its present state from absolute zero, energy must be supplied equal to its internal energy "U" plus "pV", where "pV" is the work done in pushing against the ambient (atmospheric) pressure.

In basic physics and statistical mechanics it may be more interesting to study the internal properties of the system and therefore the internal energy is used. In basic chemistry, experiments are often conducted at constant atmospheric pressure, and the pressure-volume work represents an energy exchange with the atmosphere that cannot be accessed or controlled, so that Δ"H" is the expression chosen for the heat of reaction.

For a heat engine a change in its internal energy is the difference between the heat input and the pressure-volume work done by the working substance while a change in its enthalpy is the difference between the heat input and the work done by the engine:
where the work W done by the engine is: 

In order to discuss the relation between the enthalpy increase and heat supply, we return to the first law for closed systems: . We apply it to the special case with a uniform pressure at the surface. In this case the work term can be split into two contributions, the so-called "pV" work, given by (where here "p" is the pressure at the surface, "dV" is the increase of the volume of the system) and all other types of work "δW′", such as by a shaft or by electromagnetic interaction. So we write . In this case the first law reads:

or

From this relation we see that the increase in enthalpy of a system is equal to the added heat:

provided that the system is under constant pressure ("dp" = 0) and that the only work done by the system is expansion work ("δW"' = 0).

In thermodynamics, one can calculate enthalpy by determining the requirements for creating a system from "nothingness"; the mechanical work required, "pV", differs based upon the conditions that obtain during the creation of the thermodynamic system.

Energy must be supplied to remove particles from the surroundings to make space for the creation of the system, assuming that the pressure "p" remains constant; this is the "pV" term. The supplied energy must also provide the change in internal energy, "U", which includes activation energies, ionization energies, mixing energies, vaporization energies, chemical bond energies, and so forth. Together, these constitute the change in the enthalpy "U" + "pV". For systems at constant pressure, with no external work done other than the "pV" work, the change in enthalpy is the heat received by the system.

For a simple system, with a constant number of particles, the difference in enthalpy is the maximum amount of thermal energy derivable from a thermodynamic process in which the pressure is held constant.

The total enthalpy of a system cannot be measured directly, the "enthalpy change" of a system is measured instead. Enthalpy change is defined by the following equation:

where

For an exothermic reaction at constant pressure, the system's change in enthalpy equals the energy released in the reaction, including the energy retained in the system and lost through expansion against its surroundings. In a similar manner, for an endothermic reaction, the system's change in enthalpy is equal to the energy "absorbed" in the reaction, including the energy "lost by" the system and "gained" from compression from its surroundings. If Δ"H" is positive, the reaction is endothermic, that is heat is absorbed by the system due to the products of the reaction having a greater enthalpy than the reactants. On the other hand, if Δ"H" is negative, the reaction is exothermic, that is the overall decrease in enthalpy is achieved by the generation of heat.

From the definition of enthalpy as formula_19, the enthalpy change at constant pressure formula_20 However for most chemical reactions, the work term formula_21 is much smaller than the internal energy change formula_22 which is approximately equal to formula_23 As an example, for the combustion of carbon monoxide 2 CO(g) + O(g) → 2 CO(g), formula_24 = –566.0 kJ and formula_22 = –563.5 kJ. Since the differences are so small, reaction enthalpies are often loosely described as reaction energies and analyzed in terms of bond energies.

The specific enthalpy of a uniform system is defined as "h" = where "m" is the mass of the system. The SI unit for specific enthalpy is joule per kilogram. It can be expressed in other specific quantities by , where "u" is the specific internal energy, "p" is the pressure, and "v" is specific volume, which is equal to , where "ρ" is the density.

An enthalpy change describes the change in enthalpy observed in the constituents of a thermodynamic system when undergoing a transformation or chemical reaction. It is the difference between the enthalpy after the process has completed, i.e. the enthalpy of the products, and the initial enthalpy of the system, i.e. the reactants. These processes are reversible and the enthalpy for the reverse process is the negative value of the forward change.

A common standard enthalpy change is the enthalpy of formation, which has been determined for a large number of substances. Enthalpy changes are routinely measured and compiled in chemical and physical reference works, such as the CRC Handbook of Chemistry and Physics. The following is a selection of enthalpy changes commonly recognized in thermodynamics.

When used in these recognized terms the qualifier "change" is usually dropped and the property is simply termed "enthalpy of 'process"'. Since these properties are often used as reference values it is very common to quote them for a standardized set of environmental parameters, or standard conditions, including: 
For such standardized values the name of the enthalpy is commonly prefixed with the term "standard", e.g. "standard enthalpy of formation".

Chemical properties:

Physical properties:

In thermodynamic open systems, matter may flow in and out of the system boundaries. The first law of thermodynamics for open systems states: The increase in the internal energy of a system is equal to the amount of energy added to the system by matter flowing in and by heating, minus the amount lost by matter flowing out and in the form of work done by the system:

where "U" is the average internal energy entering the system, and "U" is the average internal energy leaving the system.

The region of space enclosed by the boundaries of the open system is usually called a control volume, and it may or may not correspond to physical walls. If we choose the shape of the control volume such that all flow in or out occurs perpendicular to its surface, then the flow of matter into the system performs work as if it were a piston of fluid pushing mass into the system, and the system performs work on the flow of matter out as if it were driving a piston of fluid. There are then two types of work performed: "flow work" described above, which is performed on the fluid (this is also often called "pV work"), and "shaft work", which may be performed on some mechanical device.

These two types of work are expressed in the equation

Substitution into the equation above for the control volume (cv) yields:

The definition of enthalpy, "H", permits us to use this thermodynamic potential to account for both internal energy and "pV" work in fluids for open systems:

If we allow also the system boundary to move (e.g. due to moving pistons), we get a rather general form of the first law for open systems. In terms of time derivatives it reads:

with sums over the various places "k" where heat is supplied, matter flows into the system, and boundaries are moving. The "Ḣ" terms represent enthalpy flows, which can be written as

with "ṁ" the mass flow and "ṅ the molar flow at position "k" respectively. The term represents the rate of change of the system volume at position "k" that results in "pV" power done by the system. The parameter "P" represents all other forms of power done by the system such as shaft power, but it can also be e.g. electric power produced by an electrical power plant.

Note that the previous expression holds true only if the kinetic energy flow rate is conserved between system inlet and outlet. Otherwise, it has to be included in the enthalpy balance. During steady-state operation of a device ("see turbine, pump, and engine"), the average may be set equal to zero. This yields a useful expression for the average power generation for these devices in the absence of chemical reactions:

where the angle brackets denote time averages. The technical importance of the enthalpy is directly related to its presence in the first law for open systems, as formulated above.

Nowadays the enthalpy values of important substances can be obtained using commercial software. Practically all relevant material properties can be obtained either in tabular or in graphical form. There are many types of diagrams, such as "h"–"T" diagrams, which give the specific enthalpy as function of temperature for various pressures, and "h"–"p" diagrams, which give "h" as function of "p" for various "T". One of the most common diagrams is the temperature–specific entropy diagram ("T"–"s"-diagram). It gives the melting curve and saturated liquid and vapor values together with isobars and isenthalps. These diagrams are powerful tools in the hands of the thermal engineer.

The points a through h in the figure play a role in the discussion in this section.

One of the simple applications of the concept of enthalpy is the so-called throttling process, also known as Joule-Thomson expansion. It concerns a steady adiabatic flow of a fluid through a flow resistance (valve, porous plug, or any other type of flow resistance) as shown in the figure. This process is very important, since it is at the heart of domestic refrigerators, where it is responsible for the temperature drop between ambient temperature and the interior of the refrigerator. It is also the final stage in many types of liquefiers.

For a steady state flow regime, the enthalpy of the system (dotted rectangle) has to be constant. Hence

Since the mass flow is constant, the specific enthalpies at the two sides of the flow resistance are the same:

that is, the enthalpy per unit mass does not change during the throttling. The consequences of this relation can be demonstrated using the "T"–"s" diagram above. Point c is at 200 bar and room temperature (300 K). A Joule–Thomson expansion from 200 bar to 1 bar follows a curve of constant enthalpy of roughly 425 kJ/kg (not shown in the diagram) lying between the 400 and 450 kJ/kg isenthalps and ends in point d, which is at a temperature of about 270 K. Hence the expansion from 200 bar to 1 bar cools nitrogen from 300 K to 270 K. In the valve, there is a lot of friction, and a lot of entropy is produced, but still the final temperature is below the starting value!

Point e is chosen so that it is on the saturated liquid line with . It corresponds roughly with and . Throttling from this point to a pressure of 1 bar ends in the two-phase region (point f). This means that a mixture of gas and liquid leaves the throttling valve. Since the enthalpy is an extensive parameter, the enthalpy in f ("h") is equal to the enthalpy in g ("h") multiplied by the liquid fraction in f ("x") plus the enthalpy in h ("h") multiplied by the gas fraction in f . So

With numbers: , so "x" = 0.64. This means that the mass fraction of the liquid in the liquid–gas mixture that leaves the throttling valve is 64%.

A power "P" is applied e.g. as electrical power. If the compression is adiabatic, the gas temperature goes up. In the reversible case it would be at constant entropy, which corresponds with a vertical line in the "T"–"s" diagram. For example, compressing nitrogen from 1 bar (point a) to 2 bar (point b) would result in a temperature increase from 300 K to 380 K. In order to let the compressed gas exit at ambient temperature "T", heat exchange, e.g. by cooling water, is necessary. In the ideal case the compression is isothermal. The average heat flow to the surroundings is "Q̇". Since the system is in the steady state the first law gives

The minimal power needed for the compression is realized if the compression is reversible. In that case the second law of thermodynamics for open systems gives

Eliminating "Q̇" gives for the minimal power

For example, compressing 1 kg of nitrogen from 1 bar to 200 bar costs at least . With the data, obtained with the "T"–"s" diagram, we find a value of 476 kJ/kg.

The relation for the power can be further simplified by writing it as

With , this results in the final relation





</doc>
<doc id="10275" url="https://en.wikipedia.org/wiki?curid=10275" title="Erdoğan Atalay">
Erdoğan Atalay

Erdoğan Atalay (born September 22, 1966 in Hanover, Germany) is a Turkish-German actor. 

In 2017 he married his girlfriend and manager Katja Ohneck.

Having already made his first appearance as a minor actor in "Aladdin and the miracle lamp" at the National Theatre of Hanover before studying acting at the Hochschule für Musik und Theater Hamburg. Afterwards he took on first parts in several German television series such as "Music Groschenweise", "Employment for Lohbeck", "Double Employment" and "The Guard". In 1996 Action Concept engaged him for the role he has played successfully up to now: Semir Gerkhan in the German television series "Alarm für Cobra 11 – Die Autobahnpolizei".



</doc>
<doc id="10277" url="https://en.wikipedia.org/wiki?curid=10277" title="Ennio Morricone">
Ennio Morricone

Ennio Morricone, (; born 10 November 1928) is an Italian composer, orchestrator, conductor, and former trumpet player. He composes a wide range of music styles, making him one of the most versatile, experimental and influential composers of all time, working in any medium. Since 1946 Morricone has composed over 500 scores for cinema and television, as well as over 100 classical works. His filmography includes over 70 award-winning films, including all Sergio Leone films since "A Fistful of Dollars" (including "For a Few Dollars More", "The Good, the Bad and the Ugly", "Once Upon a Time in the West" and "Once Upon a Time in America"), all Giuseppe Tornatore films (since "Cinema Paradiso"), "The Battle of Algiers", the "Animal Trilogy", "1900", "", "Days of Heaven", several major films in French cinema, in particular the comedy trilogy "La Cage aux Folles I", "II", "" and "Le Professionnel", "The Thing", "The Mission", "The Untouchables", "Mission to Mars", "Bugsy", "Disclosure", "In the Line of Fire", "Bulworth", "Ripley's Game" and "The Hateful Eight".

After playing the trumpet in jazz bands in the 1940s, he became a studio arranger for RCA Victor and in 1955 started ghost writing for film and theatre. Throughout his career, he has composed music for artists such as Paul Anka, Mina, Milva, Zucchero and Andrea Bocelli. From 1960 to 1975, Morricone gained international fame for composing music for westerns. His score to 1966's "The Good, the Bad and the Ugly" is considered one of the most influential soundtracks in history and was inducted into the Grammy Hall of Fame. With an estimated 10 million copies sold, "Once Upon a Time in the West" is one of the best-selling scores worldwide. He also scored seven westerns for Sergio Corbucci, Duccio Tessari's "Ringo" duology and Sergio Sollima's "The Big Gundown" and "Face to Face". Morricone worked extensively for other film genres with directors such as Bernardo Bertolucci, Mauro Bolognini, Giuliano Montaldo, Roland Joffé, Roman Polanski and Henri Verneuil. His acclaimed soundtrack for "The Mission" (1986) was certified gold in the United States. The album "Yo-Yo Ma Plays Ennio Morricone" stayed 105 weeks on the Billboard Top Classical Albums.

Morricone's best-known compositions include "The Ecstasy of Gold", "Se Telefonando", "Man with a Harmonica", "Here's to You", the UK No. 2 single "Chi Mai", "Gabriel's Oboe" and "E Più Ti Penso". He functioned during the period 1966–1980 as a main member of Il Gruppo, one of the first experimental composers collectives. In 1969, he co-founded Forum Music Village, a prestigious recording studio. From the 1970s, Morricone excelled in Hollywood, composing for prolific American directors such as Don Siegel, Mike Nichols, Brian De Palma, Barry Levinson, Oliver Stone, Warren Beatty and Quentin Tarantino. In 1977, he composed the official theme for the 1978 FIFA World Cup. He continued to compose music for European productions, such as "Marco Polo", "La piovra", "Nostromo", "Fateless", "" and "En mai, fais ce qu'il te plait". Morricone's music has been reused in television series, including "The Simpsons" and "The Sopranos", and in many films, including "Inglourious Basterds" and "Django Unchained".

As of 2013, Ennio Morricone has sold over 70 million records worldwide. In 1971, he received a "Targa d'Oro" for the worldwide sales of 22 million. In 2007, he received the Academy Honorary Award "for his magnificent and multifaceted contributions to the art of film music." He has been nominated for a further six Oscars. In 2016, Morricone received his first Academy Award for his score to Quentin Tarantino's film "The Hateful Eight" (2015), at the time becoming the oldest person ever to win a competitive Oscar. His other achievements include three Grammy Awards, three Golden Globes, six BAFTAs, ten David di Donatello, eleven Nastro d'Argento, two European Film Awards, the Golden Lion Honorary Award and the Polar Music Prize in 2010.

Morricone was born in Rome, the son of Libera Ridolfi and Mario Morricone, a musician. His family came from Arpino, near Frosinone. Morricone, who had four siblings, Adriana, Aldo, Maria and Franca, lived in Trastevere, in the centre of Rome, with his parents. Mario was a trumpet player who worked professionally in different light-music orchestras, while Libera set up a small textile business.

His first teacher was his father Mario Morricone, who taught him how to read music and also to play several instruments. Compelled to take up the trumpet, he entered the National Academy of St Cecilia, to take trumpet lessons under the guidance of Umberto Semproni.
Morricone formally entered the conservatory in 1940 at age 12, enrolling in a four-year harmony program. He completed it within six months. He studied the trumpet, composition, and choral music, under direction of Goffredo Petrassi, who influenced him; Morricone has since dedicated his concert pieces to Petrassi. In 1941, Morricone was chosen among the students of the National Academy of St Cecilia to be a part of the Orchestra of the Opera directed by Carlo Zecchi on the occasion of a tour of the Veneto region. In 1946, he received his Diploma in Trumpet. After he graduated, he continued to work in classical composition and arrangement.

Although the composer had received the "Diploma in Instrumentation for Band Arrangement" (fanfare) with a mark of 9/10 in 1952, his studies concluded at the Conservatory of Santa Cecilia in 1954 and obtained a final 9.5/10 in his Diploma in Composition, under the composer Goffredo Petrassi.

Morricone wrote his first compositions when he was six years old and was encouraged to develop his natural talents. In 1946, he composed "Il Mattino" ("The Morning") for voice and piano on a text by Fukuko, first in a group of seven "youth" Lieder.

In the following years, he continued to write music for the theatre as well as classical music for voice and piano, such as "Imitazione", based on a text by Italian poet Giacomo Leopardi, "Intimità", based on a text by Olinto Dini, "Distacco I" and "Distacco II" with words by R. Gnoli, "Oboe Sommerso" for baritone and five instruments with words by poet Salvatore Quasimodo and "Verrà la Morte", for contralto and piano, based on a text by novelist Cesare Pavese.

In 1953, Morricone was asked by Gorni Kramer and Lelio Luttazzi to write an arrangement for some medleys in an American style for a series of evening radio shows. The composer continued with the composition of other 'serious' classical pieces, thus demonstrating the flexibility and eclecticism which has always been an integral part of his character. Many orchestral and chamber compositions date, in fact, from the period between 1954 and 1959: "Musica per archi e pianoforte" (1954), "Invenzione, Canone e Ricercare per piano"; "Sestetto per flauto, oboe, fagotto, violino, viola e violoncello" (1955), "Dodici Variazione per oboe, violoncello e piano"; "Trio per clarinetto, corno e violoncello"; "Variazione su un tema di Frescobaldi" (1956); "Quattro pezzi per chitarra" (1957); "Distanze per violino, violoncello e piano"; "Musica per undici violini, Tre Studi per flauto, clarinetto e fagotto" (1958); and the "Concerto per orchestra" (1957), dedicated to his teacher Goffredo Petrassi.

Morricone soon gained popularity by writing his first background music for radio dramas and quickly moved into film.

Composing for radio, television and pop artists

Morricone's career as an arranger started in 1950, by arranging the piece "Mamma Bianca" (Narciso Parigi). In occasion of the "Anno Santo" (Holy Year), he arranged a long group of popular songs of devotion for radio broadcasting.

In 1956, Morricone started to support his family by playing in a jazz band and arranging pop songs for the Italian broadcasting service RAI. He was hired by RAI in 1958, but quit his job on his first day at work when he was told that broadcasting of music composed by employees was forbidden by a company rule. Subsequently, Morricone became a top studio arranger at RCA Victor, working with Renato Rascel, Rita Pavone, and Mario Lanza.

Throughout his career Morricone has composed songs for several national and international jazz and pop artists. In 1962 Morricone worked with American jazz singer Helen Merrill as an arranger on an EP "Helen Merrill sings Italian Songs" on the RCA Italiana label. Gianni Morandi ("Go Kart Twist", 1962), Alberto Lionello ("La donna che vale", 1959), Edoardo Vianello ("Ornella", 1960; "Cicciona cha-cha", 1960; "Faccio finta di dormire", 1961; "T'ho conosciuta", 1963; ), Nora Orlandi ("Arianna", 1960), Jimmy Fontana ("Twist no. 9"; "Nicole", 1962), Rita Pavone ("Come te non-ce nessuno" and "Pel di carota" from 1962, arranged by Luis Bacalov), Catherine Spaak ("Penso a te"; "Questi vent'anni miei", 1964), Luigi Tenco ("Quello che conta"; "Tra tanta gente"; 1962), Gino Paoli ("Nel corso" from 1963, written by Morricone with Paoli), Renato Rascel ("Scirocco", 1964), Paul Anka ("Ogni Volta"), Amii Stewart, Rosy Armen ("L'Amore Gira"), Milva ("Ridevi", "Metti Una Sera A Cena"), Françoise Hardy ("Je changerais d'avis", 1966), Mireille Mathieu ("Mon ami de toujours"; "Pas vu, pas pris", 1971; "J'oublie la pluie et le soleil", 1974) and Demis Roussos ("I Like The World", 1970).

In 1963, the composer co-wrote (with Roby Ferrante) the music for the composition "Ogni volta" ("Every Time"), a song that was performed by Paul Anka for the first time during the Festival di San Remo in 1964. This song was arranged and conducted by Morricone and sold over three million copies worldwide, including one million copies in Italy alone.

Another particular success was his composition, "Se telefonando." Performed by Mina, it was a standout track of "Studio Uno 66", the fifth-biggest-selling album of the year 1966 in Italy. Morricone's sophisticated arrangement of "Se telefonando" was a combination of melodic trumpet lines, Hal Blaine–style drumming, a string set, a '60s Europop female choir, and intensive subsonic-sounding trombones. The Italian Hitparade No. 7 song had eight transitions of tonality building tension throughout the chorus. During the following decades, the song was covered by several performers in Italy and abroad most notably by Françoise Hardy and Iva Zanicchi (1966), Delta V (2005), Vanessa and the O's (2007), and Neil Hannon (2008). "Françoise Hardy – Mon amie la rose" site in the reader's poll conducted by the la Repubblica newspaper to celebrate Mina's 70th anniversary in 2010, 30,000 voters picked the track as the best song ever recorded by Mina.

In 1987, Morricone co-wrote 'It Couldn't Happen Here' with the Pet Shop Boys. Other notable compositions for international artists include: "La metà di me" and "Immagina" (1988) by Ruggero Raimondi, "Libera l'amore" (1989) performed by Zucchero, "Love Affair" (1994) by k.d. lang, "Ha fatto un sogno" (1997) by Antonello Venditti, "Di Più" (1997) by Tiziana Tosca Donati, "Come un fiume tu" (1998), "Un Canto" (1998) and "Conradian" (2006) by Andrea Bocelli, "Ricordare" (1998) and "Salmo" (2000) by Angelo Branduardi and "My heart and I" (2001) by Sting.

After graduating in 1954, Morricone started writing and arranging music as a ghost writer for films credited to other already well-known composers, while also arranging for many light music orchestras of the RAI television network, working most notably with Armando Trovajoli, Alessandro Cicognini and Carlo Savina. He occasionally adopted Anglicized pseudonyms, such as Dan Savio and Leo Nichols.

In 1959, Morricone was the conductor (and uncredited co-composer) for Mario Nascimbene's score to "Morte Di Un Amico" (Death of a Friend), an Italian drama directed by Franco Rossi. In the same year, he composed music for the theatre show "Il Lieto Fine" by Luciano Salce.

The 1960s began on a positive note: 1961 marked in fact his real film debut with Luciano Salce's "Il Federale (The Fascist)". In an interview with American composer Fred Karlin, Morricone discussed his beginnings, stating, "My first films were light comedies or costume movies that required simple musical scores that were easily created, a genre that I never completely abandoned even when I went on to much more important films with major directors".

"Il Federale" marked the beginning of a long-run collaboration with Luciano Salce. In 1962 Morricone composed the jazz-influenced score for Salce's comedy "La voglia matta (Crazy Desire)". That year Morricone arranged also Italian singer Edoardo Vianello's summer hit "Pinne, Fucile e Occhiali", a cha-cha song, peppered with added water effects, unusual instrumental sounds and unexpected stops and starts.

Morricone wrote more works in the climate of the Italian avant-garde. A few of these compositions have been made available on CD, such as "Ut", his trumpet concerto dedicated to the soloist Mauro Maur, one of his favorite musicians; some have yet to be premiered.

From 1964 up to their eventual disbandment in 1980, he was part of Gruppo di Improvvisazione di Nuova Consonanza (G.I.N.C.), a group of composers who performed and recorded avant-garde free improvisations. The Rome-based avant-garde ensemble was dedicated to the development of improvisation and new music methods. The ensemble functioned as a laboratory of sorts, working with anti-musical systems and sound techniques in an attempt to redefine the new music ensemble and explore "New Consonance."

Known as "The Group" or "Il Gruppo," they released seven albums across the Deutsche Grammophon, RCA and Cramps labels: "Gruppo di Improvvisazione Nuova Consonanza" (1966), "The Private Sea of Dreams" (1967), "Improvisationen" (1968), "The Feed-back" (1970), "Improvvisazioni a Formazioni Variate" (1973), "Nuova Consonanza" (1975) and "Musica su Schemi" (1976). Perhaps the most famous of these is their album entitled "The Feed-back", which combines free jazz and avant-garde classical music with funk; the album is frequently sampled by hip hop DJs and is considered to be one of the most collectable records in existence, often fetching over $1,000 at auction.

Morricone played a key role in The Group and was among the core members in its revolving line-up; in addition to serving as their trumpet player, he directed them on many occasions and they can be heard on a large number of his scores from the 1970s.

Held in high regard in avant-garde music circles, they are considered to be the first experimental composers collective, their only peers being the British improvisation collective AMM. Their influence can be heard in free improvising ensembles from the European movements including Evan Parker Electro-Acoustic Ensemble, the Swiss electronic free improvisation group Voice Crack, John Zorn and in the techniques of modern classical music and avant-garde jazz groups. The ensemble's groundbreaking work informed their work in composition. The ensemble also performed in varying capacities with Morricone, contributing to some of his '60s and '70s Italian soundtracks, including "A Quiet Place in the Country" (1969) and "Cold Eyes of Fear" (1971).

His earliest scores were Italian light comedy and costume pictures, where Morricone learned to write simple, memorable themes. During the sixties and seventies he composed the scores for comedies such as "Diciottenni al sole" (1962), "Il Successo" (1963), Lina Wertmüller's "I basilischi" (1963), "Slalom" (1965), "Menage all'italiana" (1965), "How I Learned to Love Women" (1966), "L'harem" (1967), "A Fine Pair" (1968), "L'Alibi" (1969), "Questa specie d'amore" (1972), "Forza "G"" (1972) and "Fiorina la vacca" (1972).

His best-known scores for comedies includes "La Cage aux Folles" (1978) and "La Cage aux Folles II" (1980), both directed by Édouard Molinaro, "Il ladrone" (1980), Georges Lautner's "" (1985), Pedro Almodóvar's "Tie Me Up! Tie Me Down!" (1990) and Warren Beatty's "Bulworth" (1998). Morricone has never ceased to arrange and write music for comedies. In 2007, he composed a lighthearted score for the Italian romantic comedy "Tutte le Donne della mia Vita" by Simona Izzo, the director who co-wrote the Morricone-scored religious mini-series "Il Papa Buono".

Though his first films were undistinguished, Morricone's arrangement of an American folk song intrigued director and former schoolmate Sergio Leone. Before being associated with Leone, Morricone had already composed some music for less-known western movies such as "Duello nel Texas" (aka "Gunfight at Red Sands") (1963). In 1962, Morricone met American folksinger Peter Tevis, who is credited with singing the lyrics of Morricone's songs such as "A Gringo Like Me" (from "Gunfight at Red Sands") and "Lonesome Billy" (from "Bullets Don't Argue").

The turning point in Morricone's career took place in 1964, the year in which his third child, Andrea Morricone, who would also become a film composer, was born. Film director Sergio Leone hired Morricone, and together they created a distinctive score to accompany Leone's different version of the Western, "A Fistful of Dollars" (1964).

The "Dollars" trilogy
Because budget strictures limited Morricone's access to a full orchestra, he used gunshots, cracking whips, whistle, voices, jew's harp, trumpets, and the new Fender electric guitar, instead of orchestral arrangements of Western standards à la John Ford. Morricone used his special effects to punctuate and comically tweak the action—cluing in the audience to the taciturn man's ironic stance. Though sonically bizarre for a movie score, Morricone's music was viscerally true to Leone's vision.

As memorable as Leone's close-ups, harsh violence, and black comedy, Morricone's work helped to expand the musical possibilities of film scoring. Morricone was initially billed on the film as Dan Savio. "A Fistful of Dollars" came out in Italy in 1964 and was released in America three years later, greatly popularizing the so-called Spaghetti Western genre. For the American release, Sergio Leone and Ennio Morricone decided to adopt American-sounding names, so they called themselves respectively Bob Robertson and Dan Savio. Over the film's theatrical release, it grossed more than any other Italian film up to that point. The film debuted in the United States in January 1967, where it grossed for the year. It eventually grossed $14.5 million in its American release, against its budget of 200–250,000.

With the score of "A Fistful of Dollars", Morricone began his 20-year collaboration with his childhood friend Alessandro Alessandroni and his Cantori Moderni. Alessandroni provided the whistling and the twanging guitar on the film scores, while his Cantori Moderni were a flexible troupe of modern singers. Morricone specifically exploited the solo soprano of the group, Edda Dell'Orso, at the height of her powers "an extraordinary voice at my disposal".

The composer subsequently scored Leone's other two "Dollars Trilogy" (or "Man With No Name Trilogy") spaghetti westerns: "For a Few Dollars More" (1965) and "The Good, the Bad and the Ugly" (1966). All three films starred the American actor Clint Eastwood as "The Man With No Name" and depicted Leone's own intense vision of the mythical West. Some of the music was written before the film, which was unusual. Leone's films were made like that because he wanted the music to be an important part of it; he kept the scenes longer because he did not want the music to end. According to Morricone this explains why the films are so slow.

Despite the small film budgets, the "Dollars Trilogy" was a box-office success. The available budget for "The Good, the Bad and The Ugly" was about 1.2 million, but it became the most successful film of the "Dollars Trilogy", grossing 25.1 million in the United States and over 2,3 billion lire (1,2 million EUR) in Italy alone. Morricone's score became a major success and sold over three million copies worldwide. On 14 August 1968 the original score was certified by the RIAA with a golden record for the sale of 500,000 copies in the United States only.

The main theme of "For a Few Dollars More" ("Per Qualche Dollaro in Più") was covered by Hugo Montenegro ("For a Few Dollars More"), Babe Ruth ("Theme From a Few Dollars More"), Golden Palominos ("For A Few Dollars More"), Material ("For a Few Dollars More"), and Matti Heinivaho ("Arosusi"). More recently, a Techno-Industrial cover was done by Komor Kommando ("Hasta Luego"). A remix was done by Terranova, "For a Few Dollars More (Terranova Remix)".

Hugo Montenegro's version of the main theme of "The Good, the Bad and the Ugly" sold over one million copies worldwide. Montenegro's album with the same name included a selection of Morricone's compositions from the "Dollars Trilogy". In the United States, the album was certified gold by the RIAA on 9 September 1969. The main theme was later sampled by artists such as Cameo ("Word Up!"), Bomb the Bass and LL Cool J.

"The Ecstasy of Gold" became one of Morricone's best-known compositions. The opening scene of Jeff Tremaine's "Jackass Number Two" (2006), in which the cast is chased through a suburban neighborhood by bulls, is accompanied by this piece. While punk rock band the Ramones used "The Ecstasy of Gold" as closing theme during their live performances, Metallica uses "The Ecstasy of Gold" as the introductory music for its concerts since 1983 This composition is also included on Metallica's live symphonic album "S&M" as well as the live album "". An instrumental metal cover by Metallica (with minimal vocals by lead singer James Hetfield) appeared on the 2007 Morricone tribute album "We All Love Ennio Morricone". This metal version was nominated for a Grammy Award in the category of Best Rock Instrumental Performance. In 2009, the Grammy Award-winning hip-hop artist Coolio extensively sampled the theme for his song "Change".

Subsequent to the success of the "Dollars trilogy", Morricone composed also the scores for "Once Upon a Time in the West" (1968) and Leone's last credited western film "A Fistful of Dynamite" (1971), as well as the scores for "My Name Is Nobody" (1973) and "A Genius, Two Partners and a Dupe" (1975), produced by Sergio Leone.

Morricone's score for "Once Upon a Time in the West" is one of the best-selling original instrumental scores in the world today, with up to 10 million copies sold, including one million copies in France and over 800,000 copies in the Netherlands. One of the main themes from the score, "A Man with Harmonica" (L'uomo Dell'armonica), became worldwide known and sold over 1,260,000 copies in France alone. This theme was later sampled in popular songs such as Beats International's "Dub Be Good to Me" (1990) and The Orb's ambient single "Little Fluffy Clouds" (1990). Film composer Hans Zimmer sampled "A Man with Harmonica" in 2007 as part of his composition "Parlay" (from the soundtrack "").

The collaboration with Leone is considered one of the exemplary collaborations between a director and a composer. Morricone's last score for Leone was for his last film, the gangster drama "Once Upon a Time in America" (1984). Leone died on 30 April 1989 of a heart attack at the age of 60. Before his death in 1989, Leone was part-way through planning a film on the Siege of Leningrad, set during World War II. By 1989, Leone had been able to acquire 100 million in financing from independent backers for the war epic. He had convinced Morricone to compose the film score. The project was canceled when Leone died two days before he was to officially sign on for the film. In early 2003, Italian filmmaker Giuseppe Tornatore announced he would direct a film called "Leningrad". The film has yet to go into production and Morricone has been cagey thus far as to details on account of Tornatore's superstitious nature.

Two years after the start of his collaboration with Sergio Leone, Morricone also started to score music for another Spaghetti Western director, Sergio Corbucci. The composer wrote music for Corbucci's "Navajo Joe" (1966), "The Hellbenders" (1967), "The Mercenary/The Professional Gun" (1968), "The Great Silence" (1968), "Compañeros" (1970), "Sonny and Jed" (1972) and "What Am I Doing in the Middle of the Revolution?" (1972).

In addition, Morricone composed music for the western films by Sergio Sollima, "The Big Gundown" (with Lee Van Cleef, 1966), "Face to Face" (1967) and "Run, Man, Run" (1968), as well as the 1970 crime thriller "Violent City" (with Charles Bronson) and the poliziottesco film "Revolver" (1973).

Other relevant scores for less popular Spaghetti Westerns include "Duello nel Texas" (1963), "" (1964), "A Pistol for Ringo" (1965), "The Return of Ringo" (1965), "Seven Guns for the MacGregors" (1966), "The Hills Run Red" (1966), Giulio Petroni's "Death Rides a Horse" (1967) and "Tepepa" (1968), "A Bullet for the General" (1967), "Guns for San Sebastian" (with Charles Bronson and Anthony Quinn, 1968), "A Sky Full of Stars for a Roof" (1968), "The Five Man Army" (1969), Don Siegel's "Two Mules for Sister Sara" (1970), "Life Is Tough, Eh Providence?" (1972) and "Buddy Goes West" (1981).

With Leone's films, Ennio Morricone's name had been put firmly on the map. Most of Morricone's film scores of the 1960s were composed outside the Spaghetti Western genre, while still using Alessandroni's team. Their music included the themes for "Il Malamondo" (1964), "Slalom" (1965) and "Listen, Let's Make Love" (1967). In 1968, Morricone reduced his work outside the movie business and wrote scores for 20 films in the same year. The scores included psychedelic accompaniment for Mario Bava's superhero romp "" (1968).

His talent and creativity were such that many other directors were soon keen to collaborate with him, and in the next few years Morricone scored a lot of films by politically committed directors: collaborating with Marco Bellocchio ("Fists in the Pocket", 1965), Gillo Pontecorvo ("The Battle of Algiers" (1966) and "Queimada!" (1969) with Marlon Brando), Roberto Faenza (H2S, 1968), Giuliano Montaldo ("Sacco e Vanzetti", 1971), Giuseppe Patroni Griffi ("'Tis Pity She's a Whore", 1971), Mauro Bolognini ("Drama of the Rich", 1974), Umberto Lenzi ("Almost Human", 1974), Pier Paolo Pasolini ("Salò, or the 120 Days of Sodom", 1975), Bernardo Bertolucci ("Novecento", 1976) and Tinto Brass ("The Key", 1983).

In 1970, Morricone wrote the score for "Violent City". That same year, he received his first Nastro d'Argento for the music in "Metti, una sera a cena" (Giuseppe Patroni Griffi, 1969) and his second only a year later for "Sacco e Vanzetti" (Giuliano Montaldo, 1971), in which he had made a memorable collaboration with the legendary American folk singer and activist Joan Baez. His soundtrack for "Sacco e Vanzetti" contains another well-known composition by Morricone, the folk song "Here's to You", sung by Joan Baez. For the writing of the lyrics, Baez was inspired by a letter from Bartolomeo Vanzetti: ""Father, yes, I am a prisoner / Fear not to relay my crime"". The song became a hit in several countries, selling over 790,000 copies in France only. The song was later included in movies such as "The Life Aquatic with Steve Zissou" and in the video game "" as the closing theme as well as .

In the same year, Morricone composed the score for the less-known drama "Maddalena" (1971) by the Polish film director Jerzy Kawalerowicz which included its composition 'Chi Mai'. The theme appeared on the million-selling score for Georges Lautner's "Le Professionnel" (1981), as well as the TV series, "An Englishman's Castle" (1978) and "The Life and Times of David Lloyd George" (1981). Because of its appearance on the latter, "Chi Mai" reached number 2 on the UK Singles Chart in 1981. The single was certified by the BPI with a golden record on 1 May 1981 and sold over 900,000 copies in France alone. "Chi Mai" is also the name of the online community about Morricone, which offers a repository of information and a free online magazine called "Maestro", containing reviews, articles, discoveries and free comments.

In the beginning of the 1970s, Morricone achieved success with other singles, including "A Fistful of Dynamite" (1971) and "God With Us" (1974), having sold respectively 477,000 and 378,000 copies in France only.

Between 1967 and 1993 the composer had a long-term collaboration with director Mauro Bolognini. Morricone wrote more than 15 film scores for Bolognini, including "Le streghe" (1966), "L'assoluto naturale" (1969), "Un bellissimo novembre" (1969), "Metello" (1970), "Chronicle of a Homicide" (1972), "Libera, My Love" (1973), "Per le antiche scale" (1975), "La Dame aux camelias" (1980), "Mosca addio" (1987), "Gli indifferenti" (1988) and "Husband and Lovers" (1992).

Ennio Morricone's eclecticism and knack for creating highly poignant, melodic and emotional music found great scope also in horror movies, such as the baroque thrillers of Dario Argento, from "The Bird with the Crystal Plumage" (1969), "The Cat o' Nine Tails" (1970) and "Four Flies on Grey Velvet" (1971) to "The Stendhal Syndrome" (1996) and "The Phantom of the Opera" (1998). His other horror scores include "Nightmare Castle" (1965), "A Quiet Place in the Country" (1968), "The Antichrist" (1974), "Autopsy" (1975) and "Night Train Murders" (1975).

In addition, Morricone's music has also been featured in many popular and cult Italian giallo films, such as "Senza sapere niente di lei" (1969), "Forbidden Photos of a Lady Above Suspicion" (1970), "A Lizard in a Woman's Skin" (1971), "Cold Eyes of Fear" (1971), "The Fifth Cord" (1971), "Short Night of Glass Dolls" (1971), "My Dear Killer" (1972), "What Have You Done to Solange?" (1972), "Black Belly of the Tarantula" (1972), "Who Saw Her Die?" (1972) and "Spasmo" (1974).

In 1977 Morricone scored Alberto De Martino's apocalyptic horror film "Holocaust 2000", starring Kirk Douglas. In 1982 he composed the score for John Carpenter's science fiction horror movie "The Thing". Morricone's main theme for the film was reflected in Marco Beltrami's film's score of the prequel of the 1982 film, which was released in 2011.

The "Dollars Trilogy" was not released in the United States until 1967 when United Artists, who had already enjoyed success distributing the British-produced James Bond films in the United States, decided to release Sergio Leone's Spaghetti Westerns. The American release gave Morricone an exposure in America and his film music became quite popular in the United States.

One of Morricone's first contributions to an American director concerned his music for the religious epic film "" by John Huston. According to Sergio Miceli's book "Morricone, la musica, il cinema", Morricone wrote about 15 or 16 minutes of music, which were recorded for a screen test and conducted by Franco Ferrara. At first Morricone's teacher Goffredo Petrassi had been engaged to write the score for the great big budget epic, but Huston preferred another composer. RCA Records then proposed Morricone who was under contract with them, but a conflict between the film's producer Dino De Laurentiis and RCA occurred. The producer wanted to have the exclusive rights for the soundtrack, while RCA still had the monopoly on Morricone at that time and did not want to release the composer. Subsequently, Morricone's work was rejected because he did not get the ok by RCA to work for Dino De Laurentiis alone. The composer reused the parts of his unused score for "The Bible: In the Beginning" in such films as "The Return of Ringo" (1965) by Duccio Tessari and Alberto Negrin's "The Secret of the Sahara" (1987).

Morricone never left Rome to compose his music and never learned to speak English. But given that the composer has always worked in a wide field of composition genres, from absolute music, which he has always produced, to applied music, working as orchestrator as well as conductor in the recording field, and then as a composer for theatre, radio and cinema, the impression arises that he never really cared that much about his standing in the eyes of Hollywood.

In 1970, Morricone composed the music for Don Siegel's "Two Mules for Sister Sara", an American-Mexican western film starring Shirley MacLaine and Clint Eastwood. The same year the composer also delivered the title theme "The Men from Shiloh" for the American Western television series The Virginian<ref name="https://www.youtube.com/watch?v=8xna6GjwSJ0&feature=related"></ref> and the score for Phil Karlson's war film "Hornets' Nest", starring Rock Hudson, and scored "Bluebeard", starring Richard Burton, two years later.

In 1974 Morricone wrote music for some unknown episodes of the science-fiction television series "", directed by Lee H. Katzin, and the following year he scored the George Kennedy revenge thriller "The "Human" Factor (1975 film)", which was the final film of director Edward Dmytryk. Two years later he composed the score for the sequel to William Friedkin's 1973 film "The Exorcist", directed by John Boorman: "". The horror film was a major disappointment at the box office. The film grossed 30,749,142 in the United States, turning a profit but still disappointing in comparison to the original film's gross. The same year he scored the Dino De Laurentiis produced adventure film "Orca", starring Richard Harris, which was also only a minor hit but later developed a cult following.

In 1978, the composer worked with Terrence Malick for "Days of Heaven", starring Richard Gere. During the lengthy editing process of the romantic drama, which won an Academy Award for Best Cinematography with an additional three nominations for the score, Terrence Malick and Billy Weber made use of a temporary score dominated by Morricone's music for the Bernardo Bertolucci film "Novecento". Malick also chose the ethereal Aquarium music from Camille Saint-Saëns ("The Carnival of the Animals") to frame the film. When Malick decided he wanted Morricone to score his movie, the director sent a version of it to Italy with the Novecento temp track in place. Morricone agreed to the assignment and Malick flew to Italy because the composer did not fly, so would not travel to the United States. Malick took the movie over to Morricone in Italy and Morricone was writing for "Days of Heaven" the whole time. Afterwards they scored the music in Italy. In "Days of Heaven", Morricone's elegiac music coexists with pre-existing selections.

Despite the fact that Morricone had produced some of the most popular and widely imitated film music ever written throughout the 1960s and '70s, "Days of Heaven" earned him his first Oscar nomination for Best Original Score, with his score up against Jerry Goldsmith's "The Boys from Brazil", Dave Grusin's "Heaven Can Wait", Giorgio Moroder's "Midnight Express" (the eventual winner) and John Williams's "Superman: The Movie" at the Oscar ceremonies in 1979.

In 1979, Morricone provided the music for the thriller "Bloodline", directed by Terence Young, best known for directing the James Bond films "Dr. No" (1962), "From Russia with Love" (1963), and "Thunderball" (1965). Subsequently, the composer was asked to score Michael Ritchie's "The Island" (1980, starring Michael Caine), Gordon Willis's thriller "Windows" (1980), Andrew Bergman's comedy "So Fine" (1981) starring Ryan O'Neal, Matt Cimber's film "Butterfly" (1982), starring Pia Zadora, Samuel Fuller's controversial drama film "White Dog" (1982) and "Thieves After Dark" (1984), Jerry London's critically acclaimed TV movie "The Scarlet and the Black" (1983), starring Gregory Peck, and Richard Fleischers box office bomb "Red Sonja" (1985), starring Arnold Schwarzenegger and Brigitte Nielsen.

Morricone's most fruitful and often long-term collaborations have been with Hollywood-related directors such as Brian De Palma, Barry Levinson, Warren Beatty, Oliver Stone and especially Roland Joffé, for whom Morricone wrote one of his best-known scores, the highly evocative soundtrack for "The Mission" (1986).

Association with Roland Joffé
"The Mission", directed by Joffé, was about a piece of history considerably more distant, as Spanish Jesuit missionaries see their work undone as a tribe of Paraguayan natives fall within a territorial dispute between the Spanish and Portuguese. Morricone's score is considered as an example of an absolute pinnacle of what music can do for a film, and what a soundtrack album can do to enrich the listener's life. At one point the score was one of the world's best-selling film scores, selling over 3 million copies worldwide.

Morricone finally received a second Oscar nomination for "The Mission". Morricone's original score lost out to Herbie Hancock's coolly arranged jazz on Bertrand Tavernier's "Round Midnight". It was considered as a surprising win and a controversial one, given that much of the music in the film was pre-existing. Morricone stated the following during a 2001 interview with "The Guardian": "I definitely felt that I should have won for The Mission. Especially when you consider that the Oscar-winner that year was Round Midnight, which was not an original score. It had a very good arrangement by Herbie Hancock, but it used existing pieces. So there could be no comparison with The Mission. There was a theft!" His score for "The Mission" was ranked at number 1 in a poll of the all-time greatest film scores. The top 10 list was compiled by 40 film composers such as Michael Giacchino and Carter Burwell. The score is ranked 23rd on the AFI's list of 25 greatest film scores of all time.

The composer wrote also the music for three other movies by Joffé: "Fat Man and Little Boy" (1989, starring Paul Newman), "City of Joy" (1992, starring Patrick Swayze) and the opening film for the 2000 Cannes Film Festival, "Vatel", starring Gérard Depardieu, Uma Thurman and Tim Roth.

On three occasions, Brian De Palma worked with Morricone: "The Untouchables" (1987), the 1989 war drama "Casualties of War" and the science fiction film "Mission to Mars" (2000). De Palma's "The Untouchables", starring rising star Kevin Costner as Eliot Ness, Robert De Niro as Al Capone and the Oscar-winning Sean Connery, was released in 1987. Morricone's score for "The Untouchables" resulted in his third nomination for Academy Award for Best Original Score.

In a 2001 interview with "The Guardian", Morricone stated that he had good experiences with De Palma: "De Palma is delicious! He respects music, he respects composers. For The Untouchables, everything I proposed to him was fine, but then he wanted a piece that I didn't like at all, and of course we didn't have an agreement on that. It was something I didn't want to write – a triumphal piece for the police. I think I wrote nine different pieces for this in total and I said, 'Please don't choose the seventh!' because it was the worst. And guess what he chose? The seventh one. But it really suits the movie."

Another American director, Barry Levinson, commissioned the composer on two occasions. First, for the crime-drama "Bugsy", starring Warren Beatty, which received ten Oscar nominations, winning two for Best Art Direction-Set Decoration (Dennis Gassner, Nancy Haigh) and Best Costume Design.

The highest-grossing American movie for which the composer wrote a complete score was for Levinson's "Disclosure" in 1994, starring Michael Douglas and Demi Moore.

"He doesn't have a piano in his studio, I always thought that with composers, you sit at the piano, and you try to find the melody. There's no such thing with Morricone. He hears a melody, and he writes it down. He hears the orchestration completely done", said Barry Levinson in an interview.

During his career in Hollywood, Morricone was approached for numerous other projects, including the Gregory Nava drama "A Time of Destiny" (1988), "Frantic" by Polish-French director Roman Polanski (1988, starring Harrison Ford), Franco Zeffirelli's 1990 drama film "Hamlet" (starring Mel Gibson and Glenn Close), the neo-noir crime film "State of Grace" by Phil Joanou (1990, starring Sean Penn and Ed Harris), "Rampage" (1992) by William Friedkin, and the romantic drama "Love Affair" (1994) by Warren Beatty.

None of the aforementioned films were box office successes, but fortunately Morricone was also commissioned for more successful motion pictures such as "In the Line of Fire" (1993) by Wolfgang Petersen, starring Clint Eastwood and John Malkovich, the horror film "Wolf" (1994, Mike Nichols), which featured Jack Nicholson and Michelle Pfeiffer in the lead roles, and "Bulworth" by Warren Beatty.

In 1997, Morricone composed the music for "Lolita" (by Adrian Lyne) and Oliver Stone's "U Turn", starring Sean Penn and Jennifer Lopez. A year later, Ennio Morricone wrote a complete score for the 1998 drama "What Dreams May Come", but Vincent Ward found the music too emotional and replaced Morricone with Michael Kamen.

One of his last complete scores for an American-related project includes the 2002 thriller "Ripley's Game", starring John Malkovich, by Liliana Cavani.

Besides the 500 original film scores that have been composed by Morricone for movies and television series in a career of over six decades, his music is in addition frequently reused in more than 150 other film projects. Morricone's compositions appeared in the German TV series "Derrick" (1989), the live-action comedy film "Inspector Gadget", "Ally McBeal" (2001), "The Simpsons" (2002), "The Sopranos" (2001–2002) and more recently in "Dancing with the Stars" (2010).

Quentin Tarantino borrowed Morricone's music for several of his films. The Main Title of "Death Rides a Horse" (1967) can be heard in "", while "" contains music originally from "For a Few Dollars More", "The Good, the Bad and the Ugly", "The Mercenary" and "Navajo Joe". The themes "Paranoia Prima" and "Unexpected Violence" ("Violenza inattesa"), originally from respectively "The Cat o' Nine Tails" and "The Bird with the Crystal Plumage", were used in "Death Proof" (2007) by Tarantino.

In 2010, Tarantino originally wanted Morricone to compose the film score for "Inglourious Basterds". Morricone was unable to, because the film's sped-up production schedule conflicted with his scoring of Giuseppe Tornatore's "Baarìa". However, Tarantino did use eight tracks composed by Morricone in the film, with four of them included on the soundtrack. The tracks came originally from Morricone's scores for "The Big Gundown" (1966), "Revolver" (1973) and "Allonsanfàn" (1974).

In 2012, Morricone composed the song "Ancora Qui" with lyrics by Italian singer Elisa for Tarantino's "Django Unchained", a track that appeared together with three existing music tracks composed by Morricone on the soundtrack. "Ancora Qui" was one of the contenders for an Academy Award nomination in the Best Original Song category, but eventually the song was not nominated. On 4 January 2013 Morricone presented Tarantino with a Life Achievement Award at a special ceremony being cast as a continuation of the International Rome Film Festival. In 2014, Morricone claimed that he would "never work" with Tarantino again, but later agreed to write an original film score for Tarantino's "The Hateful Eight", which won an Academy Award in 2016 in the Best Original Score category. His nomination for this film marked him at that time as the second oldest nominee in Academy history, behind Gloria Stuart. Morricone's win marked his first competitive Oscar, and at the age of 87 he became the oldest person at the time to win a competitive Oscar.

In 2014, Morricone's song "Giù La Testa" was featured in Florian Habicht's feature film "Pulp: a Film about Life, Death & Supermarkets", an unconventional rockumentary about British group Pulp which premiered at SXSW that year.

In 1988 Morricone started an ongoing and very successful collaboration with Italian director Giuseppe Tornatore. His first score for Tornatore was for the drama film "Cinema Paradiso". The international version of the film won the Special Jury Prize at the 1989 Cannes Film Festival and the 1989 Best Foreign Language Film Oscar. In 2002, the director's cut 173-minute version was released (known in the U.S. as Cinema Paradiso: The New Version). Morricone received a BAFTA award and a David di Donatello for his score.

After the success of "Cinema Paradiso", the composer wrote the music for all subsequent films by Tornatore: the drama film "Everybody's Fine" (Stanno Tutti Bene, 1990), "A Pure Formality" (1994) starring Gérard Depardieu and Roman Polanski, "The Star Maker" (1995), "The Legend of 1900" (1998) starring Tim Roth, the 2000 romantic drama "Malèna" (which featured Monica Bellucci) and the psychological thriller mystery film "La sconosciuta" (2006).

More recently, Morricone composed the scores for "Baarìa - La porta del vento" (2009), "The Best Offer" (2013) starring Geoffrey Rush, Jim Sturgess and Donald Sutherland and the romantic drama "The Correspondence" (2015) starring Jeremy Irons and Olga Kurylenko.

The composer won several music awards for his scores to Tornatore's movies. So, Morricone received a fifth Academy Award nomination and a Golden Globe nomination for "Malèna". For "Legend of 1900", he won a Golden Globe Award for Best Original Score.

Morricone has worked for television, from a single title piece to variety shows and documentaries to TV series, including "Moses the Lawgiver" (1974), "The Life and Times of David Lloyd George" (1981), "Marco Polo" (1982) (which won two Primetime Emmys), "The Secret of the Sahara" (1987), I Promessi Sposi and "Nostromo" (1996).

He wrote the score for the Mafia television series "La piovra" seasons 2 to 10 from 1985 to 2001, including the themes "Droga e sangue" ("Drugs and Blood"), "La Morale", and "L'Immorale". Morricone worked as the conductor of seasons 3 to 5 of the series. He also worked as the music supervisor for the television project "La bibbia" ("The Bible").

In the late 1990s, he collaborated with his son Andrea on the "Ultimo" crime dramas, resulting in "Ultimo" (1998), "Ultimo 2 – La sfida" (1999), "Ultimo 3 – L'infiltrato" (2004) and "Ultimo 4 – L'occhio del falco" (2013).

In the 2000s, Morricone continued to compose music for successful television series such as "Il Cuore nel Pozzo" (2005), "" (2005), "La provinciale" (2006), "Giovanni Falcone" (2007), "Pane e libertà" (2009) and "Come Un Delfino 1–2" (2011–2013).

With an estimated 13 million viewers, "" became an incredible success. Morricone wrote additional music for the sequel, "" (2006), which portrayed Karol's life as Pope from his papal inauguration to his death. Both scores were originally released respectively in 2005 and 2006. One year later, a double disc album with both scores is released.

In 2003, Morricone scored another epic, for Japanese television, called "Musashi" and was the Taiga drama about Miyamoto Musashi, Japan's legendary warrior. A part of his "applied music" is now applied to Italian television films.

Morricone provided the string arrangements on Morrissey's "Dear God Please Help Me" from the album "Ringleader of the Tormentors" in 2006.

Since 2004, Morricone wrote music for almost exclusively Italian television movies and mini-series, especially for directors such as Giuseppe Tornatore, Alberto Negrin, Giuliano Montaldo, and Franza Di Rosa.

In 2008, the composer recorded music for a Lancia commercial, featuring Richard Gere and directed by Harald Zwart (known for directing "The Pink Panther 2").

Tarantino originally wanted Morricone to compose the soundtrack for his film, "Inglourious Basterds". However, Morricone refused because of the sped-up production schedule of the film. Tarantino did use several Morricone tracks from previous films in the soundtrack. Morricone instead wrote the music for "Baaria - La porta del vento" by Tornatore. It was the second time Morricone's turned down the director, he also turned down an offer to write some music for "Pulp Fiction" in 1994.

In spring and summer 2010, Morricone worked with Hayley Westenra for a collaboration on her album "Paradiso". The album features new songs written by Morricone, as well as some of his best-known film compositions of the last 50 years. Hayley recorded the album with Morricone's orchestra in Rome during the summer of 2010.

Since 1995, he composed the music for several advertising campaigns of Dolce & Gabbana. The commercials were directed by Giuseppe Tornatore.

In 2013, Morricone collaborated with Italian singer-songwriter Laura Pausini on a new version of her hit single "La solitudine" for her 20 years anniversary greatest hits album "20 – The Greatest Hits".

Morricone composed the music for "The Best Offer" (2013) by Giuseppe Tornatore.

In 2014, Ennio Morricone became an honorary chairman of the First International Open Competition in author's music video "Mediamusic." The final of the competition was scheduled on 1 March 2015 in Moscow.

He wrote the score for Christian Carion's "En mai, fais ce qu'il te plait" (2015) and the most recent movie by Tornatore: "The Correspondence" (2016), featuring Jeremy Irons and Olga Kurylenko.

In July 2015, Quentin Tarantino announced after the screening of footage of his movie "The Hateful Eight" at the San Diego Comic-Con International that Morricone would score the film, the first Western that Morricone has scored since 1981. The score was critically acclaimed and won several awards including the Golden Globe Award for Best Original Score and the Academy Award for Best Original Score.

Before receiving his diplomas in trumpet, composition and instrumentation from the conservatory, Morricone was already active as a trumpet player, often performing in an orchestra that specialized in music written for films. After completing his education at Saint Cecilia, the composer honed his orchestration skills as an arranger for Italian radio and television. In order to support himself, he moved to RCA in the early sixties and entered the front ranks of the Italian recording industry. Since 1964, Morricone was also a founding member of the Rome-based avant-garde ensemble Gruppo di Improvvisazione di Nuova Consonanza. During the existence of the group (until 1978), Morricone performed several times with the group as trumpet player.

To ready his music for live performance, he joined smaller pieces of music together into longer suites. Rather than single pieces, which would require the audience to applaud every few minutes, Morricone thought the best idea was to create a series of suites lasting from 15 to 20 minutes, which form a sort of symphony in various movements – alternating successful pieces with personal favorites. In concert, Morricone normally has 180 to 200 musicians and vocalists under his baton, performing multiple genre-crossing collections of music. Rock, symphonic and ethnic instruments share the stage.

On 20 September 1984 Morricone conducted the Orchestre national des Pays de la Loire at "Cinésymphonie '84" ("Première nuit de la musique de film/First night of film music") in the French concert hall Salle Pleyel in Paris. He performed some of his best-known compositions such as "Metti, una sera a cena", "Novecento" and "The Good, the Bad and the Ugly". Michel Legrand and Georges Delerue performed on the same evening.

On 15 October 1987 Morricone gave a concert in front of 12,000 people in the Sportpaleis in Antwerp, Belgium, with the Dutch Metropole Orchestra and the Italian operatic soprano Alide Maria Salvetta. A live-album with a recording of this concert was released in the same year.

On 9 June 2000 Morricone went to the Flanders International Film Festival Ghent to conduct his music together with the National Orchestra of Belgium. During the concert's first part, the screening of "The Life and Death of King Richard III" (1912) was accompanied with live music by Morricone. It was the very first time that the score was performed live in Europe. The second part of the evening consisted of an anthology of the composer's work. The event took place on the eve of Euro 2000, the European Football Championship in Belgium and the Netherlands.

Morricone performed over 250 concerts as of 2001. Since 2001, the composer has been on a world tour, the latter part sponsored by Giorgio Armani, with the Orchestra Roma Sinfonietta, touring London (Barbican 2001; 75th birthday "Concerto", Royal Albert Hall 2003), Paris, Verona, and Tokyo. Morricone performed his classic film scores at the Munich Philharmonie in 2005 and Hammersmith Apollo Theatre in London, UK, on 1 & 2 December 2006.
He made his North American concert debut on 3 February 2007 at Radio City Music Hall in New York City. The previous evening, Morricone had already presented at the United Nations a concert comprising some of his film themes, as well as the cantata "Voci dal silenzio" to welcome the new Secretary-General Ban Ki-Moon. A "Los Angeles Times" review bemoaned the poor acoustics and opined of Morricone: "His stick technique is adequate, but his charisma as a conductor is zero." Morricone, though, has said: "Conducting has never been important to me. If the audience comes for my gestures, they had better stay outside."

On 12 December 2007 Morricone conducted the Orchestra Roma Sinfonietta at the Wiener Stadthalle in Vienna, presenting a selection of his own works.
Together with the Roma Sinfonietta and the Belfast Philharmonic Choir, Morricone performed at the Opening Concerts of the Belfast Festival at Queen's, in the Waterfront Hall on 17 and 18 October 2008.
Morricone and Orchestra Roma Sinfonietta also held a concert at the Belgrade Arena (Belgrade, Serbia) on 14 February 2009.

On 10 April 2010 Morricone conducted a concert at the Royal Albert Hall in London with the Orchestra Roma Sinfonietta and (as in all of his previous London concerts) the Crouch End Festival Chorus. On 11 September he conducted a concert in Verona.

On 26 February 2012 Morricone made his Australian debut when he conducted the Western Australian Youth Orchestra together with a 100 voice chorus (made up primarily of WASO chorus members) at the Burswood Theatre (part of Crown Perth (formerly known as Burswood Entertainment Complex)) in Perth. On 2 March 2012 he conducted the Adelaide Symphony Orchestra at Elder Park, Adelaide as part of the Adelaide Festival of Arts.

On 22 December 2012 Morricone conducted the 85-piece Belgian orchestra "Orkest der Lage Landen" and a 100-piece choir during a two-hour concert in the Sportpaleis in Antwerp.

In November 2013 Morricone began a world tour to coincide with the 50th anniversary of his film music career and performed in locations such as the Crocus City Hall in Moscow, Santiago, Chile, Berlin, Germany (O2 World), Budapest, Hungary, and Vienna (Stadhalle). Back in June 2014, Morricone had to cancel a U.S tour in New York (Barclays Center) and Los Angeles (Nokia Theatre LA Live) due to a back procedure on 20 February. Morricone postponed the rest of his world tour.

In November 2014 Morricone stated that he will resume his European tour starting from February 2015.

In the late 1960s, Morricone and three other Italian composers (Piero Piccioni, Armando Trovajoli and Luis Bacalov) founded Forum Music Village (Rome), previously called Ortophonic recording studio. The recording studio has some peculiarities, one of them is the ability to record a church organ directly to the studio.

Morricone has been using the studio to create his scores for the past 40 years. The studio has hosted many directors who have worked alongside him, including Brian De Palma, Oliver Stone and Barry Levinson.

The Academy Award-winning scores of "" by Luis Bacalov and "Life Is Beautiful" by Nicola Piovani were recorded in Studio A of Forum Music Village.

Notable artists who have recorded at Forum Music Village are Quincy Jones, Jon and Vangelis, Plácido Domingo, Andrea Bocelli, Red Hot Chili Peppers, Will.i.am, Yo-Yo Ma, Morrissey, Bruno Nicolai, Alessandro Alessandroni, Goblin, Pino Donaggio, Nicola Piovani, Danger Mouse, Daniele Luppi and Cher.

On 13 October 1956 he married Maria Travia, whom he had met in 1950. Travia has written lyrics to complement her husband's pieces. Her works include the Latin texts for "The Mission". They have three sons and a daughter, in order of birth: Marco (1957), Alessandra (1961), the conductor and film composer Andrea (Andrew) (1964), and Giovanni Morricone (1966), a filmmaker, who lives in New York City.

Morricone has lived in Italy his entire life and has never desired to live in Hollywood. Morricone is also not fluent in English and will take interviews only in his native language.

Ennio Morricone has influenced many artists from other styles and genres, including Danger Mouse, Dire Straits, Muse, Metallica, Radiohead and Hans Zimmer.


Ennio Morricone has sold well over 70 million records worldwide during his career that spanned over seven decades, including 6.5 million albums and singles in France, over three million in the United States and more than two million albums in Korea. In 1971, the composer received his first golden record (disco d'oro) for the sale of 1,000,000 records in Italy and a "Targa d'Oro" for the worldwide sales of 22 million.

Ennio Morricone has been involved with at least 19 different movies grossing over 20 million at the box office

" * " = US-only figures. Other successful movies with Morricone's work are "La Luna" (1979), "Kill Bill: Volumes 1 & 2" (2003, 2004), "Inglourious Basterds" (2009) and "Django Unchained" (2012) though the tracks used are sampled from older pictures.

Selected long-time collaborations with directors

Ennio Morricone received his first Academy Award nomination in 1979 for the score to "Days of Heaven" (Terrence Malick, 1978).

In 1984, the U.S. distributor of Sergio Leone's "Once Upon a Time in America" reportedly failed to file the proper paperwork so that Morricone's score, regarded as one of his best, would be eligible for consideration for an Academy Award.

Two years later, Morricone received his second Oscar nomination for "The Mission". He also received Oscar nominations for his scores to "The Untouchables" (1987), "Bugsy" (1991), "Malèna" (2000), and "The Hateful Eight" (2016). On 28 February 2016, Morricone won his first Academy Award for his score to "The Hateful Eight."

Morricone and Alex North are the only composers to receive the Academy Honorary Award since the award's introduction in 1928. Ennio Morricone received the Academy Honorary Award on 25 February 2007, presented by Clint Eastwood, "for his magnificent and multifaceted contributions to the art of film music." With the statuette came a standing ovation. In conjunction with the honor, Morricone released a tribute album, "We All Love Ennio Morricone", that featured as its centerpiece Celine Dion's rendition of "I Knew I Loved You" (based on "Deborah's Theme" from "Once Upon a Time in America"), which she performed at the ceremony. Behind-the-scenes studio production and recording footage of "I Knew I Loved You" can be viewed in the debut episode of the QuincyJones.com Podcast. The lyric, as with Morricone's "Love Affair", had been written by Alan and Marilyn Bergman. Morricone's acceptance speech was in his native Italian tongue and was interpreted by Clint Eastwood, who stood to his left. Eastwood and Morricone had in fact met two days earlier for the first time in 40 years at a reception.

AFI

In 2005 four film scores by Ennio Morricone were nominated by the American Film Institute for an honoured place in the AFI's Top 25 of Best American Film Scores of All Time. His score for "The Mission" was ranked 23rd in the Top 25 list.

Golden Globes
Italian Golden Globes

Grammy Awards

Morricone was nominated seven times for a Grammy Award. In 2009 The Recording Academy inducted his score for The Good, the Bad and the Ugly (1966) into the Grammy Hall of Fame.

Nastro d'Argento (Silver Ribbon)

ASCAP Awards

BAFTA Awards

César Awards

David Award

European Film Awards

LAFCA






</doc>
<doc id="10278" url="https://en.wikipedia.org/wiki?curid=10278" title="List of explosives used during World War II">
List of explosives used during World War II

Almost all the common explosives listed here were mixtures of several common components:


This is only a partial list; there were many others. Many of these compositions are now obsolete and only encountered in legacy munitions and unexploded ordnance.

Two nuclear explosives, containing mixtures of uranium and plutonium, respectively, were also used at the bombings of Hiroshima and Nagasaki



</doc>
<doc id="10281" url="https://en.wikipedia.org/wiki?curid=10281" title="Emin Boztepe">
Emin Boztepe

Emin Boztepe (born 17 July 1962) is an American martial artist of Turkish origin who held German nationality prior to naturalization. He first came to prominence for his fight in 1986 with noted Wing Chun practitioner William Cheung, and he continued to gain attention in the 1990s with a public challenge of the Gracie family. He was a notable member of Leung Ting's Wing Tsun organisation until 2002, when he formed his own organisation.

Boztepe was born in Eskişehir, Turkey, the second of six children. At the age of four, his family, originally from Bağlıca, Emirdağ, moved to Germany. According to Boztepe, his early days in Germany were difficult due to his Turkish heritage—he was a constant target for racial insults and, more often than not, verbal abuse would escalate into physical abuse. Martial arts became something of a necessity and his father urged him to begin training. Boztepe recalled, "Germany was not the healthiest place for a young Turk in those days of growing racism and neo-Nazi movements." He claimed to have been in over 300 street fights, many of them a result of this period, though he also claimed to have not started any of these fights.

In 1976, at the age of 14, Boztepe began studying martial arts, including judo, Shotokan karate, wrestling, Muay Thai, and traditional boxing. During this period, he also fought as an amateur boxer in 16 matches.

In 1980, Boztepe was attracted to Wing Chun when he saw a demonstration by Keith R. Kernspecht who was teaching in Kiel, Germany. He said, "Wing tsun was, really, love at first sight. And it fit me. For whatever reason, I was a natural at it." Boztepe also began training in Latosa-Escrima, which Kernspecht's German Wing Tsun Organization had decided to make part of the family in 1982.

His fight with Traditional Wing Chun Kung Fu master William Cheung in 1986, at a seminar in Germany, caused controversy in the wider Wing Chun community.

After some financial problems occurred with his master, Kernspecht, Boztepe headed to his master's master, Leung Ting, and took special lessons from him until a financial dispute, which caused him to leave the Wing Tsun organisation. Boztepe formed his own organisation, called Emin Boztepe Martial Arts System (EBMAS), afterwards.

Boztepe was romantically linked with English actress Jacqueline Bisset from 1997 until 2005. He is known to have dated Zeynep Urgancı, the ex-wife of Turkish businessman Mehmet Urgancı after Bisset. He is currently engaged to Roma Gonzalesáenz a Spanish professional photographer, graphic designer, creative director, classic ballet and TV dancer and bass player.



</doc>
<doc id="10283" url="https://en.wikipedia.org/wiki?curid=10283" title="Erlang (unit)">
Erlang (unit)

The erlang (symbol E) is a dimensionless unit that is used in telephony as a measure of offered load or carried load on service-providing elements such as telephone circuits or telephone switching equipment. A single cord circuit has the capacity to be used for 60 minutes in one hour. Full utilization of that capacity, 60 minutes of traffic, constitutes 1 erlang. 

Carried traffic in erlangs is the average number of concurrent calls measured over a given period (often one hour), while offered traffic is the traffic that would be carried if all call-attempts succeeded. How much offered traffic is carried in practice will depend on what happens to unanswered calls when all servers are busy.

The CCITT named the international unit of telephone traffic the erlang in 1946 in honor of Agner Krarup Erlang. In Erlang's analysis of efficient telephone line usage he derived the formulae for two important cases, Erlang-B and Erlang-C, which became foundational results in teletraffic engineering and queueing theory. His results, which are still used today, relate quality of service to the number of available servers. Both formulae take offered load as one of their main inputs (in erlangs), which is often expressed as call arrival rate times average call length. 

A distinguishing assumption behind the Erlang B formula is that there is no queue, so that if all service elements are already in use then a newly arriving call will be blocked and subsequently lost. The formula gives the probability of this occurring. In contrast, the Erlang C formula provides for the possibility of an unlimited queue and it gives the probability that a new call will need to wait in the queue due to all servers being in use. Erlang's formulae apply quite widely, but they may fail when congestion is especially high causing unsuccessful traffic to repeatedly retry. One way of accounting for retries when no queue is available is the Extended Erlang B method.

When used to represent carried traffic, a value (which can be a non-integer such as 43.5) followed by “erlangs” represents the average number of concurrent calls carried by the circuits (or other service-providing elements), where that average is calculated over some reasonable period of time. The period over which the average is calculated is often one hour, but shorter periods (e.g., 15 minutes) may be used where it is known that there are short spurts of demand and a traffic measurement is desired that does not mask these spurts.
One erlang of carried traffic refers to a single resource being in continuous use, or two channels each being in use fifty percent of the time, and so on. For example, if an office has two telephone operators who are both busy all the time, that would represent two erlangs (2 E) of traffic; or a radio channel that is occupied continuously during the period of interest (eg. one hour) is said to have a load of 1 erlang.

When used to describe offered traffic, a value followed by “erlangs” represents the average number of concurrent calls that would have been carried if there were an unlimited number of circuits (that is, if the call-attempts that were made when all circuits were in use had not been rejected). The relationship between offered traffic and carried traffic depends on the design of the system and user behavior. Three common models are (a) callers whose call-attempts are rejected go away and never come back, (b) callers whose call-attempts are rejected try again within a fairly short space of time, and (c) the system allows users to wait in queue until a circuit becomes available.

A third measurement of traffic is instantaneous traffic, expressed as a certain number of erlangs, meaning the exact number of calls taking place at a point in time. In this case the number is an integer. Traffic-level-recording devices, such as moving-pen recorders, plot instantaneous traffic.

The concepts and mathematics introduced by Agner Krarup Erlang have broad applicability beyond telephony. They apply wherever users arrive more or less at random to receive exclusive service from any one of a group of service-providing elements without prior reservation, for example, where the service-providing elements are ticket-sales windows, toilets on an airplane, or motel rooms. (Erlang’s models do not apply where the service-providing elements are shared between several concurrent users or different amounts of service are consumed by different users, for instance, on circuits carrying data traffic.)

The goal of Erlang’s traffic theory is to determine exactly how many service-providing elements should be provided in order to satisfy users, without wasteful over-provisioning. To do this, a target is set for the grade of service (GoS) or quality of service (QoS). For example, in a system where there is no queuing, the GoS may be that no more than 1 call in 100 is blocked (i.e., rejected) due to all circuits being in use (a GoS of 0.01), which becomes the target probability of call blocking, "P", when using the Erlang B formula.

There are several resulting formulae, including Erlang B, Erlang C and the related Engset formula, based on different models of user behavior and system operation. These may each be derived by means of a special case of continuous-time Markov processes known as a birth-death process. The more recent Extended Erlang B method provides a further traffic solution that draws on Erlang's results.

Offered traffic (in erlangs) is related to the call arrival rate, λ, and the average call-holding time (the average time of a phone call), "h", by:

provided that "h" and λ are expressed using the same units of time (seconds and calls per second, or minutes and calls per minute).

The practical measurement of traffic is typically based on continuous observations over several days or weeks, during which the instantaneous traffic is recorded at regular, short intervals (such as every few seconds). These measurements are then used to calculate a single result, most commonly the busy hour traffic (in erlangs). This is the average number of concurrent calls during a given one-hour period of the day, where that period is selected to give the highest result. (This result is called the time-consistent busy hour traffic). An alternative is to calculate a busy hour traffic value separately for each day (which may correspond to slightly different times each day) and take the average of these values. This generally gives a slightly higher value than the time-consistent busy hour value.

Where the existing busy-hour carried traffic, "E", is measured on an already-overloaded system, with a significant level of blocking, it is necessary to take account of the blocked calls in estimating the busy-hour offered traffic "E" (which is the traffic value to be used in the Erlang formulae). The offered traffic can be estimated by "E" = "E"/(1 − "P"). For this purpose, where the system includes a means of counting blocked calls and successful calls, "P" can be estimated directly from the proportion of calls that are blocked. Failing that, "P" can be estimated by using "E" in place of "E" in the Erlang formula and the resulting estimate of "P" can then be used in "E" = "E"/(1 − "P") to provide a first estimate of "E".

Another method of estimating "E" in an overloaded system is to measure the busy-hour call arrival rate, "λ" (counting successful calls and blocked calls), and the average call-holding time (for successful calls), "h", and then estimate "E" using the formula "E" = "λh".

For a situation where the traffic to be handled is completely new traffic, the only choice is to try to model expected user behavior. For example, one could estimate active user population, "N", expected level of use, "U" (number of calls/transactions per user per day), busy-hour concentration factor, "C" (proportion of daily activity that will fall in the busy hour), and average holding time/service time, "h" (expressed in minutes). A projection of busy-hour offered traffic would then be "E" = (NUC/60)"h" erlangs. (The division by 60 translates the busy-hour call/transaction arrival rate into a per-minute value, to match the units in which "h" is expressed.)

The Erlang B formula (or Erlang-B with a hyphen), also known as the Erlang loss formula, is a formula for the blocking probability that describes the probability of call losses for a group of identical parallel resources (telephone lines, circuits, traffic channels, or equivalent), sometimes referred to as an M/M/c/c queue. It is, for example, used to dimension a telephone network's links. The formula was derived by Agner Krarup Erlang and is not limited to telephone networks, since it describes a probability in a queuing system (albeit a special case with a number of servers but no queueing space for incoming calls to wait for a free server). Hence, the formula is also used in certain inventory systems with lost sales.

The formula applies under the condition that an unsuccessful call, because the line is busy, is not queued or retried, but instead really vanishes forever. It is assumed that call attempts arrive following a Poisson process, so call arrival instants are independent. Further, it is assumed that the message lengths (holding times) are exponentially distributed (Markovian system), although the formula turns out to apply under general holding time distributions.

The Erlang B formula assumes an infinite population of sources (such as telephone subscribers), which jointly offer traffic to "N" servers (such as telephone lines). The rate expressing the frequency at which new calls arrive, λ, (birth rate, traffic intensity, etc.) is constant, and does "not" depend on the number of active sources. The total number of sources is assumed to be infinite. 
The Erlang B formula calculates the blocking probability of a buffer-less loss system, where a request that is not served immediately is aborted, causing that no requests become queued. Blocking occurs when a new request arrives at a time where all available servers are currently busy. The formula also assumes that blocked traffic is cleared and does not return.

The formula provides the GoS (grade of service) which is the probability "P" that a new call arriving to the resources group is rejected because all resources (servers, lines, circuits) are busy: "B"("E", "m") where "E" is the total offered traffic in erlang, offered to "m" identical parallel resources (servers, communication channels, traffic lanes). 

where:

Note: The "erlang" is a dimensionless load unit calculated as the mean arrival rate, λ, multiplied by the mean call holding time, "h". 
See Little's law to prove that the erlang unit has to be dimensionless for Little's Law to be dimensionally sane.

This may be expressed recursively as follows, in a form that is used to simplify the calculation of tables of the Erlang B formula:

Typically, instead of "B"("E", "m") the inverse 1/"B"("E", "m") is calculated in numerical computation in order to ensure numerical stability:

The Erlang B formula is decreasing and convex in "m".
It requires that call arrivals can be modeled by a Poisson process, which not always is a good match, but it is valid for any statistical distribution of call holding times with finite mean. 
It applies to traffic transmission systems that do not buffer traffic. 
More modern examples compared to POTS where Erlang B is still applicable, are optical burst switching (OBS) and several current approaches to optical packet switching (OPS). 
Erlang B was developed as a trunk sizing tool for telephone networks with holding times in the minutes range, but being a mathematical equation it applies on any time-scale.

Extended Erlang B differs from the classic Erlang-B assumptions by allowing for a proportion of blocked callers to try again, causing an increase in offered traffic from the initial baseline level. It is an iterative calculation rather than a formula and adds an extra parameter, the recall factor formula_8, which defines the recall attempts.

The steps in the process are as follows. It starts at iteration formula_9 with a known initial baseline level of traffic formula_10, which is successively adjusted to calculate a sequence of new offered traffic values formula_11, each of which accounts for the recalls arising from the previously calculated offered traffic formula_12.

1. Calculate the probability of a caller being blocked on their first attempt
as above for Erlang B.

2. Calculate the probable number of blocked calls

3. Calculate the number of recalls, formula_15 assuming a fixed Recall Factor, formula_8,

4. Calculate the new offered traffic
where formula_10 is the initial (baseline) level of traffic.
5. Return to step 1, substituting formula_11 for formula_12, and iterate until a stable value of formula_22 is obtained.

Once a satisfactory value of formula_22 has been found, the blocking probability formula_3 and the recall factor can be used to calculate the probability that all of a caller's attempts are lost, not just their first call but also any subsequent retries.

The Erlang C formula expresses the probability that an arriving customer will need to queue (as opposed to immediately being served). Just as the Erlang B formula, Erlang C assumes an infinite population of sources, which jointly offer traffic of formula_22 erlangs to formula_26 servers. However, if all the servers are busy when a request arrives from a source, the request is queued. An unlimited number of requests may be held in the queue in this way simultaneously. This formula calculates the probability of queuing offered traffic, assuming that blocked calls stay in the system until they can be handled. This formula is used to determine the number of agents or customer service representatives needed to staff a call centre, for a specified desired probability of queuing. However, the Erlang C formula assumes that callers never hang up while in queue, which makes the formula predict that more agents should be used than are really needed to maintain a desired service level.)

where:

It is assumed that the call arrivals can be modeled by a Poisson process and that call holding times are described by an exponential distribution.

When Erlang developed the Erlang-B and Erlang-C traffic equations, they were developed on a set of assumptions. These assumptions are accurate under most conditions; however in the event of extremely high traffic congestion, Erlang's equations fail to accurately predict the correct number of circuits required because of re-entrant traffic. This is termed a high-loss system, where congestion breeds further congestion at peak times. In such cases, it is first necessary for many additional circuits to be made available so that the high loss can be alleviated. Once this action has been taken, congestion will return to reasonable levels and Erlang's equations can then be used to determine how exactly many circuits are really required.

An example of an instance which would cause such a High Loss System to develop would be if a TV-based competition were to announce a particular telephone number to call at a specific time. In this case a large number of people would simultaneously phone the number provided. If the service provider had not catered for this sudden unanticipated peak demand, extreme traffic congestion will develop and Erlang's equations cannot be used.





</doc>
<doc id="10285" url="https://en.wikipedia.org/wiki?curid=10285" title="Eligible receiver">
Eligible receiver

In American football and Canadian football, not all players on offense are entitled to receive a forward pass. Only an eligible pass receiver may legally catch a forward pass, and only an eligible receiver may advance beyond the neutral zone if a forward pass crosses the neutral zone. If the pass is received by a non-eligible receiver, it is "illegal touching" (five yards and loss of down). If an ineligible receiver is beyond the neutral zone when a forward pass crossing the neutral zone is thrown, a foul of "ineligible receiver downfield" (five yards, but no loss of down) is called. Each league has slightly different rules regarding who is considered an eligible receiver.

The NCAA rulebook defines eligible receivers for college football in Rule 7, Section 3, Article 3. The determining factors are the player's position on the field at the snap and their jersey number. Specifically, any players on offense wearing numbers between 50 and 79 are always ineligible. All defensive players are eligible receivers and offensive players who are not wearing an ineligible number are eligible receivers if they meet one of the following three criteria:

Players may only wear eligible numbers at an ineligible position when it is obvious that a punt or field goal is to be attempted.

If a player is to change between eligible and ineligible positions, they must physically change jersey numbers to reflect the position.

A receiver loses his eligibility by leaving the field of play unless he was forced out by a defensive player and immediately attempts to get back inbounds (Rule 7-3-4). All players on the field become eligible as soon as the ball is touched by a defensive player or an official during play (Rule 7-3-5).

In both American and Canadian professional football, every player on the defensive team is considered eligible. The offensive team must have at least seven players lined up on the line of scrimmage. Of the players on the line of scrimmage, only the two players on the ends of the line of scrimmage are eligible receivers. The remaining players are in the backfield (four in American football, five in Canadian football), including the quarterback. These backfield players are also eligible receivers. In the National Football League, a quarterback who takes his stance behind center as a T-formation quarterback is not eligible unless, before the ball is snapped, he legally moves to a position at least one yard behind the line of scrimmage or on the end of the line, and is stationary in that position for at least one second before the snap, but is nonetheless not counted toward the seven men required on the line of scrimmage.

If, for example, eight men line up on the line of scrimmage, the team loses an eligible receiver. This can often happen when a flanker or slot receiver, who is supposed to line up behind the line of scrimmage, instead lines up on the line of scrimmage between the offensive line and a split end. In most cases where a pass is caught by an ineligible receiver, it is usually because the quarterback was under pressure and threw it to an offensive lineman out of desperation.

In many leagues eligible receivers must wear certain uniform numbers, so that the officials can more easily distinguish between eligible and ineligible receivers. In the NFL running backs must wear numbers 20 to 49, tight ends must wear numbers 80 to 89 (or 40 to 49 if the numbers 80 to 89 have been exhausted), and wide receivers must wear numbers 10 to 19 or 80 to 89. In the CFL ineligible receivers must wear numbers 50 to 69; all other numbers (including 0 and 00) may be worn by eligible receivers. A player who is not wearing a number that corresponds to an eligible receiver is ineligible even if he lines up in an eligible position. However, a player who reports to the referee that he intends to be eligible in the following play is allowed to line up and act as an eligible receiver. An example of this was a 1985 NFL game in which William Perry, wearing number 72 and normally a defensive lineman, was made an eligible receiver on an offensive play, and successfully caught a touchdown pass attempt. A more recent example, and more commonly used, has been former New England Patriots linebacker Mike Vrabel lining up as a tight end in goal line situations.

Before the snap of the ball, in the American game, backfield players may only move parallel to the line of scrimmage, only one back may be in motion at any given time, and if forward motion has occurred, the back must be still for a full second before the snap. The receiver may be in motion laterally or away from the line of scrimmage at the snap. A breach of this rule results in a penalty for illegal procedure (five yards). However, in the Canadian game, eligible receivers may move in any direction before the snap, any number may be in motion at any one time, and there is no need to be motionless before the snap.

The rules on eligible receivers only apply to forward passes. Any player may legally catch a backwards or lateral pass. 

In the American game, once the play has started, eligible receivers can become ineligible depending on how the play develops. Any eligible receiver that goes out of bounds is no longer an eligible receiver and cannot receive a forward pass, unless that player re-establishes by taking three steps in bounds. Also, if a pass is touched by any defensive player or eligible offensive receiver (tipped by a defensive lineman, slips through a receiver's hands, etc.), every offensive player immediately becomes eligible. In the CFL all players become eligible receivers if a pass is touched by a member of the defensive team.

In high school football, the rules of eligibility are roughly the same as in the college game. However, as of February 1999, at least five players must wear numbers between 50 and 79 on first, second, or third down, which by rule would make them ineligible receivers. This was because of a change in the definition of a scrimmage-kick formation made by the National Federation of State High School Associations (NFHS). The change was intended to close a loophole in the rules which allowed teams to run an A-11 offense, in which a team could legally be exempted from eligibility numbering restrictions if the player receiving the snap was at least seven yards behind the line of scrimmage.



</doc>
<doc id="10286" url="https://en.wikipedia.org/wiki?curid=10286" title="Enver Hoxha">
Enver Hoxha

Enver Halil Hoxha (; 16 October 190811 April 1985) was an Albanian communist politician who served as the head of state of Albania from 1944 until his death in 1985, as the First Secretary of the Party of Labour of Albania. He was chairman of the Democratic Front of Albania and commander-in-chief of the armed forces from 1944 until his death. He served as the 22nd Prime Minister of Albania from 1944 to 1954 and at various times served as foreign minister and defence minister as well.

Born in Gjirokastër in 1908, Hoxha became a teacher in grammar school in 1936. Following Italy's invasion of Albania, he entered into the Party of Labour of Albania at its creation in 1941. Hoxha was elected First Secretary in March 1943 at the age of 34. The Yugoslav Partisans assisted the Albanians. Less than two years after the liberation of the country, the monarchy was abolished, King Zog was deposed and Hoxha rose to power as the head of state of Albania.

During his 40-year-rule, he focused on rebuilding the country, which was left in ruins after World War II, building Albania's first railway line, eliminating adult illiteracy and leading Albania towards becoming agriculturally self-sufficient. However, detractors criticize him for a series of political repressions which included the establishment and use of forced labor camps, extrajudicial killings and executions that targeted and eliminated anti-communists, a large amount of which was carried out by the Sigurimi secret police.

Hoxha's government was characterized by his proclaimed firm adherence to anti-revisionist Marxism–Leninism from the mid-1970s onwards. After his break with Maoism in the 1976–1978 period, numerous Maoist parties around the world declared themselves Hoxhaist. The International Conference of Marxist–Leninist Parties and Organizations (Unity & Struggle) is the best known association of these parties today.

Hoxha was born in Gjirokastër, a city in southern Albania (then under the Ottoman Empire), son of Halil Hoxha, a Muslim Tosk cloth merchant who travelled widely across Europe and the United States, and Gjylihan (Gjylo) Hoxha née "Çuçi".

The Hoxha family was attached to the Bektashi Order; in 1916 his father brought him to seek the blessing of Baba Selim of the Zall Teqe.

After the maktab, he followed his studies in the city senior high school "Liria". He started his studies at the Gjirokastër Lyceum at 1923. After the lyceum was closed, thanks to the intervention of Eqrem Libohova Hoxa was awarded a state scholarship for the continuation of his studies in Korçë, at the French language Albanian National Lyceum until 1930.
In 1930, Hoxha went to study at the University of Montpellier in France on a state scholarship for the faculty of natural science. He never took an exam staying four years in Montpellier, against the Albanian laws of the time. He never returned the scholarship. He later went to Paris, where he presented himself to anti-Zogist immigrants as the brother-in-law of Bahri Omari.

In the years 1935 to 1936 he was employed as a secretary at the Albanian consulate in Brussels. After returning to Albania, he worked as a contract teacher in the Gymnasium of Tirana. Hoxha taught morals in the Korça Liceum from 1937 to 1939 and also served as the caretaker of the school library.

On 7 April 1939, Albania was invaded by Fascist Italy. The Italians established a puppet government, the Albanian Kingdom (1939–43), under Shefqet Vërlaci. Hoxha was indifferent regarding the invasion. 
In the summer of 1939 he went to Italy on vacation. At the end of 1939 he was transferred to the Gjirokastra Gymnasium, but he soon returned to Tirana. He was helped by his best friend, Esat Dishnica, who introduced Hoxa to Dishnica's cousin Ibrahim Biçakçiu. Hoxha started to sleep in Biçakçiu's tobacco factory "Flora", and after a while Dishnica opened a shop with the same name, where Hoxha started to work. He was a sympathizer of Korça's Communist Group.

On 8 November 1941, the Communist Party of Albania (later renamed the Party of Labour of Albania in 1948) was founded. Hoxha was chosen from the "Korca group" as a Muslim representative by the two Yugoslav envoys as one of the seven members of the provisional Central Committee. The First Consultative Meeting of Activists of the Communist Party of Albania was held in Tirana from April 8 to 11, 1942, with Hoxha himself delivering the main report on 8 April 1942.

In July 1942, Hoxha wrote "Call to the Albanian Peasantry", issued in the name of the Communist Party of Albania. The call sought to enlist support in Albania for the war against the fascists. The peasants were encouraged to hoard their grain and refuse to pay taxes or livestock levies brought by the government. After the September 1942 Conference at Pezë, the National Liberation Movement was founded with the purpose of uniting the anti-fascist Albanians, regardless of ideology or class.

By March 1943, the first National Conference of the Communist Party elected Hoxha formally as First Secretary. During World War II, the Soviet Union's role in Albania was negligible. On 10 July 1943, the Albanian partisans were organised in regular units of companies, battalions and brigades and named the Albanian National Liberation Army. The organization received military support from the British intelligence service, SOE. The General Headquarters was created, with Spiro Moisiu as the commander and Hoxha as political commissar. The Yugoslav Partisans had a much more practical role, helping to plan attacks and exchanging supplies, but communication between them and the Albanians was limited and letters would often arrive late, sometimes well after a plan had been agreed upon by the National Liberation Army without consultation from the Yugoslav partisans.

Within Albania, repeated attempts were made during the war to remedy the communications difficulties which faced partisan groups. In August 1943, a secret meeting, the Mukje Conference, was held between the anti-communist Balli Kombëtar (National Front) and the Communist Party of Albania. The result of this was an agreement to:


To encourage the Balli Kombëtar to sign, the Greater Albania sections that included Kosovo (part of Yugoslavia) and Chamëria were made part of the Agreement.

A problem developed when the Yugoslav Communists disagreed with the goal of a Greater Albania and asked the Communists in Albania to withdraw their agreement. According to Hoxha, Josip Broz Tito had agreed that "Kosovo was Albanian" but that Serbian opposition made transfer an unwise option. After the Albanian Communists repudiated the Greater Albania agreement, the Balli Kombëtar condemned the Communists, who in turn accused the Balli Kombëtar of siding with the Italians. The Balli Kombëtar, however, lacked support from the people. After judging the Communists as an immediate threat, the Balli Kombëtar sided with Nazi Germany, fatally damaging its image among those fighting the Fascists. The Communists quickly added to their ranks many of those disillusioned with the Balli Kombëtar and took centre stage in the fight for liberation.
The Permet National Congress held during that time called for a "new democratic Albania for the people". Although the monarchy was not formally abolished, King Zog was barred from returning to the country, which further increased the Communists' control. The Anti-Fascist Committee for National Liberation was founded, chaired by Hoxha. On 22 October 1944, the Committee became the Democratic Government of Albania after a meeting in Berat and Hoxha was chosen as interim Prime Minister. Tribunals were set up to try alleged war criminals who were designated "enemies of the people" and were presided over by Koçi Xoxe.

After liberation on 29 November 1944, several Albanian partisan divisions crossed the border into German-occupied Yugoslavia, where they fought alongside Tito's partisans and the Soviet Red Army in a joint campaign which succeeded in driving out the last pockets of German resistance. Marshal Tito, during a Yugoslavian conference in later years, thanked Hoxha for the assistance that the Albanian partisans had given during the War for National Liberation ("Lufta Nacionalçlirimtare"). The Democratic Front, dominated by the Albanian Communist Party, succeeded the National Liberation Front in August 1945 and the first post-war election was the held on 2 December. The Front was the only legal political organisation allowed to stand in the elections, and the government reported that 93% of Albanians voted for it.

On 11 January 1946, Zog was officially deposed and Albania was proclaimed the People's Republic of Albania (renamed the People's Socialist Republic of Albania in 1976). As First Secretary, Hoxha was "de facto" head of state and the most powerful man in the country.

Albanians celebrate their independence day on 28 November (which is the date on which they declared their independence from the Ottoman Empire in 1912), while in the former People's Socialist Republic of Albania the national day was 29 November, the day the country was liberated from the Italians. Both days are currently national holidays.

Hoxha declared himself a Marxist–Leninist and strongly admired Soviet leader Joseph Stalin. During the period of 1945–1950, the government adopted policies which were intended to consolidate power. The Agrarian Reform Law was passed in August 1945. It confiscated land from beys and large landowners, giving it without compensation to peasants. 52% of all land was owned by large landowners before the law was passed; this declined to 16% after the law's passage. Illiteracy, which was 90–95% in rural areas in 1939 went down to 30% by 1950 and by 1985 it was equal to that of a Western country.

The State University of Tirana was established in 1957, which was the first of its kind in Albania. The Medieval Gjakmarrja (blood feud) was banned. Malaria, the most widespread disease, was successfully fought through advances in health care, the use of DDT, and through the draining of swamplands. From 1965 to 1985, no cases of malaria were reported, whereas previously Albania had the greatest number of infected patients in Europe. No cases of syphilis had been recorded for 30 years.

By 1949, the United States and British intelligence organisations were working with King Zog and the mountain men of his personal guard. They recruited Albanian refugees and émigrés from Egypt, Italy and Greece, trained them in Cyprus, Malta and the Federal Republic of Germany (West Germany), and infiltrated them into Albania. Guerrilla units entered Albania in 1950 and 1952, but they were killed or captured by Albanian security forces. Kim Philby, a Soviet double agent working as a liaison officer between the British intelligence service and the United States Central Intelligence Agency, had leaked details of the infiltration plan to Moscow, and the security breach claimed the lives of about 300 infiltrators.

At this point, relations with Yugoslavia had begun to change. The roots of the change began on 20 October 1944 at the Second Plenary Session of the Communist Party of Albania. The Session considered the problems that the post-independence Albanian government would face. However, the Yugoslav delegation led by Velimir Stoinić accused the party of "sectarianism and opportunism" and blamed Hoxha for these errors. He also stressed the view that the Yugoslav Communist partisans spearheaded the Albanian partisan movement.

Anti-Yugoslav members of the Albanian Communist Party had begun to think that this was a plot by Tito who intended to destabilize the Party. Koçi Xoxe, Sejfulla Malëshova and others who supported Yugoslavia were looked upon with deep suspicion. Tito's position on Albania was that it was too weak to stand on its own and that it would do better as a part of Yugoslavia. Hoxha alleged that Tito had made it his goal to get Albania into Yugoslavia, firstly by creating the Treaty of Friendship, Co-operation and Mutual Aid in 1946. In time, Albania began to feel that the treaty was heavily slanted towards Yugoslav interests, much like the Italian agreements with Albania under Zog that made the nation dependent upon Italy.

The first issue was that the Albanian lek became revalued in terms of the Yugoslav dinar as a customs union was formed and Albania's economic plan was decided more by Yugoslavia. Albanian economists H. Banja and V. Toçi stated that the relationship between Albania and Yugoslavia during this period was exploitative and that it constituted attempts by Yugoslavia to make the Albanian economy an "appendage" to the Yugoslav economy. Hoxha then began to accuse Yugoslavia of misconduct:

Joseph Stalin advised Hoxha that Yugoslavia was attempting to annex Albania: "We did not know that the Yugoslavs, under the pretext of 'defending' your country against an attack from the Greek fascists, wanted to bring units of their army into the PRA [People's Republic of Albania]. They tried to do this in a very secretive manner. In reality, their aim in this direction was utterly hostile, for they intended to overturn the situation in Albania." By June 1947, the Central Committee of Yugoslavia began publicly condemning Hoxha, accusing him of talking an individualistic and anti-Marxist line. When Albania responded by making agreements with the Soviet Union to purchase a supply of agricultural machinery, Yugoslavia said that Albania could not enter into any agreements with other countries without Yugoslav approval.

Koçi Xoxe tried to stop Hoxha from improving relations with Bulgaria, reasoning that Albania would be more stable with one trading partner rather than with many. Nako Spiru, an anti-Yugoslav member of the Party, condemned Xoxe and vice versa. With no one coming to Spiru's defense, he viewed the situation as hopeless and feared that Yugoslav domination of his nation was imminent, which caused him to commit suicide in November.

At the Eighth Plenum of the Central Committee of the Party which lasted from 26 February to 8 March 1948, Xoxe was implicated in a plot to isolate Hoxha and consolidate his own power. He accused Hoxha of being responsible for the decline in relations with Yugoslavia, and stated that a Soviet military mission should be expelled in favor of a Yugoslav counterpart. Hoxha managed to remain firm and his support had not declined. When Yugoslavia publicly broke with the Soviet Union, Hoxha's support base grew stronger. Then, on 1 July 1948, Tirana called on all Yugoslav technical advisors to leave the country and unilaterally declared all treaties and agreements between the two countries null and void. Xoxe was expelled from the party and on 13 June 1949 he was executed by hanging.

After the break with Yugoslavia, Hoxha aligned himself with the Soviet Union, for which he had a great admiration. From 1948 to 1960, $200 million in Soviet aid was given to Albania for technical and infrastructural expansion. Albania was admitted to the Comecon on 22 February 1949 and remained important both as a way to pressure Yugoslavia and to serve as a pro-Soviet force in the Adriatic Sea. A submarine base was built on the island of Sazan near Vlorë, posing a possible threat to the United States Sixth Fleet. Relations remained close until the death of Stalin on 5 March 1953. His death was met with 14 days of national mourning in Albania—more than in the Soviet Union. Hoxha assembled the entire population in the capital's largest square featuring a statue of Stalin, requested that they kneel, and made them take a two-thousand word oath of "eternal fidelity" and "gratitude" to their "beloved father" and "great liberator" to whom the people owed "everything".

Under Nikita Khrushchev, Stalin's successor, aid was reduced and Albania was encouraged to adopt Khrushchev's specialization policy. Under this policy, Albania would develop its agricultural output in order to supply the Soviet Union and other Warsaw Pact nations while these nations would be developing specific resource outputs of their own, which would in theory strengthen the Warsaw Pact by greatly reducing the lack of certain resources that many of the nations faced. However, this also meant that Albanian industrial development, which was stressed heavily by Hoxha, would have to be significantly reduced.
From 16 May to 17 June 1955, Nikolai Bulganin and Anastas Mikoyan visited Yugoslavia and Khrushchev renounced the expulsion of Yugoslavia from the Communist bloc. Khrushchev also began making references to Palmiro Togliatti's polycentrism theory. Hoxha had not been consulted on this and was offended. Yugoslavia began asking for Hoxha to rehabilitate the image of Koçi Xoxe, which Hoxha steadfastly rejected. In 1956 at the Twentieth Party Congress of the Soviet Communist Party, Khrushchev condemned the cult of personality that had been built up around Joseph Stalin and also accused him of many grave mistakes. Khrushchev then announced the theory of peaceful coexistence, which angered Hoxha greatly. The Institute of Marxist–Leninist Studies, led by Hoxha's wife Nexhmije, quoted Vladimir Lenin: "The fundamental principle of the foreign policy of a socialist country and of a Communist party is proletarian internationalism; not peaceful coexistence." Hoxha now took a more active stand against perceived revisionism.

Unity within the Albanian Party of Labour began to decline as well, with a special delegate meeting held in Tirana in April 1956, composed of 450 delegates and having unexpected results. The delegates "criticized the conditions in the party, the negative attitude toward the masses, the absence of party and socialist democracy, the economic policy of the leadership, etc." while also calling for discussions on the cult of personality and the Twentieth Party Congress.

In 1956, Hoxha called for a resolution which would uphold the current leadership of the Party. The resolution was accepted, and all of the delegates who had spoken out were expelled from the party and imprisoned. Hoxha stated that this was yet another of many attempts to overthrow the leadership of Albania which had been organized by Yugoslavia. This incident further consolidated Hoxha's power, effectively making Khrushchev-esque reforms nearly impossible. In the same year, Hoxha traveled to the People's Republic of China, which was then enduring the Sino-Soviet split, and met Mao Zedong. Relations with China improved, as evidenced by Chinese aid to Albania being 4.2% in 1955 before the visit, and rising to 21.6% in 1957.

In an effort to keep Albania in the Soviet sphere, increased aid was given but the Albanian leadership continued to move closer towards China. Relations with the Soviet Union remained at the same level until 1960, when Khrushchev met Sofoklis Venizelos, a left-wing Greek politician. Khrushchev sympathized with the concept of an autonomous Greek North Epirus and he hoped to use Greek claims to keep the Albanian leadership in line with Soviet interests. Hoxha reacted by only sending Hysni Kapo, a member of the Albanian Political Bureau, to the Third Congress of the Romanian Communist Party in Bucharest, an event that heads of state were normally expected to attend. As relations between the two countries continued to deteriorate in the course of the meeting, Khrushchev said:

Relations with the Soviet Union began to decline rapidly. A hardline policy was adopted and the Soviets reduced aid shipments, specifically grain, at a time when Albania needed them due to the possibility of a flood-induced famine. In July 1960, a plot to overthrow the Albanian government was discovered. It was to be organized by Soviet-trained Rear Admiral Teme Sejko. After this, two pro-Soviet members of the Party, Liri Belishova and Koço Tashko, were expelled, with a humorous incident involving Tashko pronouncing "" (Russian for "full stop").

In August, the Party's Central Committee sent a letter of protest to the Central Committee of the Communist Party of the Soviet Union, stating its displeasure at having an anti-Albanian Soviet Ambassador in Tirana. The Fourth Congress of the Party, held from 13 to 20 February 1961, was the last meeting that the Soviet Union or other Eastern European nations attended in Albania. During the congress, the Soviet Union was condemned while China was praised. Mehmet Shehu stated that while many members of the Party were accused of tyranny, this was a baseless charge and unlike the Soviet Union, Albania was led by genuine Marxists.

The Soviet Union retaliated by threatening "dire consequences" if the condemnations were not retracted. Days later, Khrushchev and Antonín Novotný, President of Czechoslovakia (which was Albania's largest source of aid besides the Soviets), threatened to cut off economic aid. In March, Albania was not invited to attend the meeting of the Warsaw Pact nations (Albania had been one of its founding members in 1955) and in April all Soviet technicians were withdrawn from the nation. In May nearly all Soviet troops on the Orikum naval base were withdrawn, leaving the Albanians with 4 submarines and other military equipment.

On 7 November 1961, Hoxha made a speech in which he called Khrushchev a "revisionist, an anti-Marxist and a defeatist". Hoxha portrayed Stalin as the last Communist leader of the Soviet Union and he began to stress Albania's independence. By 11 November, the USSR and every other Warsaw Pact nation broke relations with Albania. Albania was unofficially excluded (by not being invited) from both the Warsaw Pact and Comecon. The Soviet Union also attempted to claim control of the Vlorë port due to a lease agreement; the Albanian Party then passed a law prohibiting any other nation from owning an Albanian port through lease or otherwise. The Soviet–Albanian split was now complete.

As Hoxha's leadership continued, he took on an increasingly theoretical stance. He wrote criticisms based both on current events at the time and on theory; most notably his condemnations of Maoism post-1978. A major achievement under Hoxha was the advancement of women's rights. Albania had been one of the most, if not the most, patriarchal countries in Europe. The "Code of Lekë", which regulated the status of women, states, "A woman is known as a sack, made to endure as long as she lives in her husband's house." Women were not allowed to inherit anything from their parents, and discrimination was even made in the case of the murder of a pregnant woman:
Women were forbidden from obtaining a divorce, and the wife's parents were obliged to return a runaway daughter to her husband or else suffer shame which could even result in a generations-long blood feud. During World War II, the Albanian Communists encouraged women to join the partisans and following the war women were encouraged to take up menial jobs, as the education necessary for higher level work was out of most women's reach. In 1938, 4% worked in various sectors of the economy. In 1970, this number had risen to 38%, and in 1982 to 46%.

During the Cultural and Ideological Revolution (discussed below), women were encouraged to take up "all" jobs, including government posts, which resulted in 40.7% of the People's Councils and 30.4% of the People's Assembly being made up of women, including two women in the Central Committee by 1985. In 1978, 15.1 times as many females attended eight-year schools as had done so in 1938 and 175.7 times as many females attended secondary schools. By 1978, 101.9 times as many women attended higher schools as in 1957. Hoxha said of women's rights in 1967:The entire party and country should hurl into the fire and break the neck of anyone who dared trample underfoot the sacred edict of the party on the defense of women's rights.In 1969, direct taxation was abolished and during this period the quality of schooling and health care continued to improve. An electrification campaign was begun in 1960 and the entire nation was expected to have electricity by 1985. Instead, it achieved this on 25 October 1970, making it the first nation with complete electrification in the world. During the Cultural & Ideological Revolution of 1967–1968 the military changed from traditional Communist army tactics and began to adhere to the Maoist strategy known as people's war, which included the abolition of military ranks, which were not fully restored until 1991. Mehmet Shehu said of the country's health service in 1979:'

Hoxha's legacy also included a complex of 750,000 one-man concrete bunkers across a country of 3 million inhabitants, to act as look-outs and gun emplacements along with chemical weapons. The bunkers were built strong and mobile, with the intention that they could be easily placed by a crane or a helicopter in a previously dug hole. The types of bunkers vary from machine gun pillboxes, beach bunkers, to underground naval facilities, and even Air Force Mountain and underground bunkers.

Hoxha's internal policies were true to Stalin's paradigm which he admired, and the personality cult developed in the 1970s organized around him by the Party also bore a striking resemblance to that of Stalin. At times it even reached an intensity similar to the personality cult surrounding Kim Il-sung (which Hoxha condemned) with Hoxha being portrayed as a genius commenting on virtually all facets of life from culture to economics to military matters. Each schoolbook required one or more quotations from him on the subjects being studied. The Party honored him with titles such as Supreme Comrade, Sole Force and Great Teacher.

Hoxha's governance was also distinguished by his encouragement of a high birthrate policy. For instance, a woman who bore an above-average number of children would be given the government award of "Heroine Mother" (in Albanian: "Nënë Heroinë") along with cash rewards. Abortion was essentially restricted (to encourage high birth rates), except if the birth posed a danger to the mother's life, though it was not completely banned; the process being decided by district medical commissions. As a result, the population of Albania tripled from 1 million in 1944 to around 3 million in 1985.

In Albania's Third Five Year Plan, China promised a loan of $125 million to build twenty-five chemical, electrical and metallurgical plants called for under the Plan. However, the nation experienced a difficult transition period, because Chinese technicians were of a lower quality than Soviet ones and the great distance between the two nations, plus the poor relations which Albania had with its neighbors, further complicated matters. Unlike Yugoslavia or the USSR, China had less economic influence on Albania during Hoxha's leadership. The previous fifteen years (1946–1961) had at least 50% of the economy under foreign commerce.

By the time the 1976 Constitution which prohibited foreign debt was promulgated, aid and investments, Albania had basically become self-sufficient although it was lacking in modern technology. Ideologically, Hoxha found Mao's initial views to be in line with Marxism-Leninism. Mao condemned Nikita Khrushchev's alleged revisionism and he was also critical of Yugoslavia. Aid given from China was interest-free and it did not have to be repaid until Albania could afford to do so.

China never intervened in what Albania's economic output should be, and Chinese technicians worked for the same wages as Albanian workers, unlike Soviet technicians who sometimes made more than three times the pay of Hoxha. Albanian newspapers were reprinted in Chinese newspapers and read on Chinese radio. Finally, Albania led the movement to give the People's Republic of China a seat on the UN Security Council, an effort which would prove successful in 1971 when it replaced the Republic of China's seat.

During this period, Albania became the second largest producer of chromium in the world, which was considered an important export for Albania. Strategically, the Adriatic Sea was also attractive to China, and the Chinese leadership had hoped to gain more allies in Eastern Europe with Albania's help, although this effort failed. Zhou Enlai visited Albania in January 1964. On 9 January, "The 1964 Sino-Albanian Joint Statement" was signed in Tirana. The statement said of relations between socialist countries:
Like Albania, China defended the "purity" of Marxism by attacking both US imperialism and "Soviet and Yugoslav revisionism", both equally as part of a "dual adversary" theory. Yugoslavia was viewed as both a "special detachment of U.S. imperialism" and a "saboteur against world revolution." These views however began to change in China, which was one of the major issues which Albania had with the alliance. Also unlike Yugoslavia and the Soviet Union, the Sino-Albanian alliance lacked "... an organizational structure for regular consultations and policy coordination, and it was also characterized by an informal relationship which was conducted on an "ad hoc" basis." Mao made a speech on 3 November 1966 in which he claimed that Albania was the only Marxist-Leninist state in Europe and in the same speech, he also stated that "an attack on Albania will have to reckon with great People's China. If the U.S. imperialists, the modern Soviet revisionists or any of their lackeys dare to touch Albania in the slightest, nothing lies ahead for them but a complete, shameful and memorable defeat." Likewise, Hoxha stated that "You may rest assured, comrades, that come what may in the world at large, our two parties and our two peoples will certainly remain together. They will fight together and they will win together."

China entered into a four-year period of relative diplomatic isolation following the Cultural Revolution and at this point relations between China and Albania reached their zenith. On 20 August 1968, the Soviet invasion of Czechoslovakia was condemned by Albania, as was the Brezhnev doctrine. Albania then officially withdrew from the Warsaw Pact on 5 September. Relations with China began to deteriorate on 15 July 1971, when United States President Richard Nixon agreed to visit China to meet with Zhou Enlai. Hoxha felt betrayed and the government was in a state of shock. On 6 August a letter was sent from the Central Committee of the Albanian Party of Labour to the Central Committee of the Communist Party of China, calling Nixon a "frenzied anti-Communist". The letter stated:

The result was a 1971 message from the Chinese leadership stating that Albania could not depend on an indefinite flow of further Chinese aid and in 1972, Albania was advised to "curb its expectations about further Chinese contributions to its economic development". By 1973, Hoxha wrote in his diary "Reflections on China" that the Chinese leaders:

In response, trade with COMECON (although trade with the Soviet Union was still blocked) and Yugoslavia grew. Trade with Third World nations was $0.5 million in 1973, but $8.3 million in 1974. Trade rose from 0.1% to 1.6%. Following Mao's death on 9 September 1976, Hoxha (who attended Mao's funeral in Beijing) remained optimistic about Sino-Albanian relations, but in August 1977, Hua Guofeng, the new leader of China, stated that Mao's Three Worlds Theory would become official foreign policy. Hoxha viewed this as a way for China to justify having the U.S. as the "secondary enemy" while viewing the Soviet Union as the main one, thus allowing China to trade with the U.S. "the Chinese plan of the 'third world' is a major diabolical plan, with the aim that China should become another superpower, precisely by placing itself at the head of the 'third world' and the 'non-aligned world'." From 30 August to 7 September 1977, Tito visited Beijing and was welcomed by the Chinese leadership. At this point, the Albanian Party of Labour had declared that China was now a revisionist state akin to the Soviet Union and Yugoslavia, and that Albania was the only Marxist–Leninist state on earth. Hoxha stated:

On 13 July 1978, China announced that it was cutting off all aid to Albania. For the first time in modern history, Albania did not have either an ally or a major trading partner.

Certain clauses in the 1976 constitution effectively circumscribed the exercise of political liberties which the government interpreted as contrary to the established order. In addition, the government denied the population access to information other than that disseminated by the government-controlled media. Internally, the Sigurimi followed the methods of the NKVD, MGB, KGB, and the East German Stasi. At one point, every third Albanian had either been incarcerated in labour camps or interrogated by the Sigurimi.

The government imprisoned thousands in forced-labour camps or executed them for crimes such as treachery or for disrupting the proletarian dictatorship. Travel abroad was forbidden after 1968 to all but those who were on official business. Western European culture was looked upon with deep suspicion, resulting in arrests and bans on unauthorised foreign material. Art was required to reflect the styles of socialist realism. Beards were banned as unhygienic in order to curb the influence of Islam (many Imams and Babas had beards) and the Eastern Orthodox faith.

The justice system regularly degenerated into show trials. An American human rights group described the proceedings of one trial: "... [The defendant] was not permitted to question the witnesses and that, although he was permitted to state his objections to certain aspects of the case, his objections were dismissed by the prosecutor who said, 'Sit down and be quiet. We know better than you.'" In order to lessen the threat of political dissidents and other exiles, relatives of the accused were often arrested, ostracised, and accused of being "enemies of the people". At least 5,000 people—possibly as many as 25,000—were executed by the government.

Torture was used to obtain confessions:One émigré, for example, testified to being bound by his hands and legs for one and a half months, and to being beaten with a belt, fists or boots for periods of two to three hours every two or three days. Another was detained in a cell one meter by eight meters large in the local police station and kept in solitary confinement for a five-day period punctuated by two beating sessions until he signed a confession; he was taken to "Sigurimi" headquarters, where he was again tortured and questioned, despite his prior confession, until his three-day trial. Still another witness was confined underground for more than a year in a three-meter square cell. During this time he was interrogated at irregular intervals and subjected to various forms of physical and psychological torture. He was chained to a chair, beaten, and subjected to electric shocks. He was shown a bullet that was supposedly meant for him and told that car engines starting within his earshot were driving victims to their executions, the next of which would be his.During Hoxha's rule, "[t]here were six institutions for political prisoners and fourteen labour camps where political prisoners and common criminals worked together. It has been estimated that there were approximately 32,000 people imprisoned in Albania in 1985."

Article 47 of the Albanian Criminal Code stated that to "escape outside the state, as well as refusal to return to the Fatherland by a person who has been sent to serve or has been permitted temporarily to go outside the state" was an act of treason, a crime punishable by a minimum sentence of ten years and a maximum sentence of death. The Albanian government went to great lengths in order to prevent people from defecting by fleeing the country:An electrically-wired metal fence stands 600 meters to one kilometer from the actual border. Anyone touching the fence not only risks electrocution, but also sets off alarm bells and lights which alert guards stationed at approximately one-kilometer intervals along the fence. Two meters of soil on either side of the fence are cleared in order to check for footprints of escapees and infiltrators. The area between the fence and the actual border is seeded with booby traps such as coils of wire, noise makers consisting of thin pieces of metal strips on top of two wooden slats with stones in a tin container which rattle if stepped on, and flares that are triggered by contact, thus illuminating would-be escapees during the night.

Albania, the only predominantly Muslim country in Europe at that time, largely owing to Turkish influence in the region, had not, like the Ottoman Empire, identified religion with ethnicity. In the Ottoman Empire, Muslims were viewed as Turks, Orthodox Christians were viewed as Greeks, and Roman Catholics were viewed as Latins. Hoxha believed this was a serious issue, feeling that it both fueled Greek separatists in southern Albania and that it also divided the nation in general. The Agrarian Reform Law of 1945 confiscated much of the church's property in the country. Catholics were the earliest religious community to be targeted, since the Vatican was seen as being an agent of Fascism and anti-Communism. In 1946 the Jesuit Order was banned and the Franciscans were banned in 1947. "Decree No. 743" (On religion) sought a national church and forbade religious leaders to associate with foreign powers.

The Party focused on atheist education in schools. This tactic was effective, primarily due to the high birthrate policy encouraged after the war. During what the religious consider "holy periods," such as Lent and Ramadan, many foods which are scorned by them (dairy products, meat, etc.) were distributed in schools and factories, and those who refused to eat those foods were denounced.

Starting on 6 February 1967, the Party began to promote secularism over Abrahamic religions. Hoxha, who had declared a "Cultural and Ideological Revolution" after being partly inspired by China's Cultural Revolution, encouraged communist students and workers to use more forceful tactics to discourage religious practices, although violence was initially condemned.

According to Hoxha, the surge in anti-theist activity began with the youth. The result of this "spontaneous, unprovoked movement" was the closing of all 2,169 churches and mosques in Albania. State atheism became official policy, and Albania was declared the world's first atheist state. Town and city names which echoed Abrahamic religious themes were abandoned for neutral secular ones, as well as personal names. During this period religiously based names were also made illegal. The "Dictionary of People's Names", published in 1982, contained 3,000 approved, secular names. In 1992, Monsignor Dias, the Papal Nuncio for Albania appointed by Pope John Paul II, said that of the three hundred Catholic priests present in Albania prior to the Communists coming to power, only thirty were still active. Promotion of religion and all clerics were outlawed as reactionaries. Those religious figures who refused to embrace the principles of Marxist-Leninism were either arrested or carried on their activities from in hiding.

Enver Hoxha had declared during the anti-religious campaign that "the only religion of Albania is Albanianism", a quotation from the poem "O moj Shqiperi" ("O Albania") by the 19th-century Albanian writer Pashko Vasa.

Muzafer Korkuti, one of the dominant figures in post-war Albanian archaeology and now Director of the institute of Archaeology in Tirana, said this in an interview on 10 July 2002:

Efforts were focused on an Illyrian-Albanian continuity issue.
An Illyrian origin of the Albanians (without denying "Pelasgian" roots) continued to play a significant role in Albanian nationalism, resulting in a revival of given names supposedly of "Illyrian" origin, at the expense of given names associated with Christianity. At first, Albanian nationalist writers opted for the Pelasgians as the forefathers of the Albanians, but as this form of nationalism flourished in Albania under Enver Hoxha, the Pelasgians became a secondary element to the Illyrian theory of Albanian origins, which could claim some support in scholarship.

The Illyrian descent theory soon became one of the pillars of Albanian nationalism, especially because it could provide some evidence of continuity of an Albanian presence both in Kosovo and in southern Albania, i.e., areas that were subject to ethnic conflicts between Albanians, Serbs and Greeks. Under the government of Enver Hoxha, an autochthonous ethnogenesis was promoted and physical anthropologists tried to demonstrate that Albanians were different from any other Indo-European populations, a theory now disproved.
They claimed that the Illyrians were the most ancient people in the Balkans and greatly extended the age of the Illyrian language.

Hoxha and his government were hostile to "Western" (American and British-led) popular culture as it manifested in the mass media, along with the consumerism and social liberalism associated with it. In a speech on the Fourth Plenum of the Central Committee of the PLA (PLA-CC) on 26 June 1973, Hoxha declared a definitive break from any such Western bourgeois influence and what he described as its "degenerated bourgeois culture". In a speech in which he also criticised the "spread of certain vulgar, alien tastes in music and art", which ran "contrary to socialist ethics and the positive traditions of our people", including "degenerate importations such as long hair, extravagant dress, screaming jungle music, coarse language, shameless behaviour and so on", Hoxha declared; 

A new Constitution was decided upon by the Seventh Congress of the Albanian Party of Labour on 1–7 November 1976. According to Hoxha, "The old Constitution was the Constitution of the building of the foundations of socialism, whereas the new Constitution will be the Constitution of the complete construction of a socialist society."

Self-reliance was now stressed more than ever. Citizens were encouraged to train in the use of weapons, and this activity was also taught in schools. This was to encourage the creation of quick partisans.

Borrowing and foreign investment were banned under Article 26 of the Constitution, which read: "The granting of concessions to, and the creation of foreign economic and financial companies and other institutions or ones formed jointly with bourgeois and revisionist capitalist monopolies and states as well as obtaining credits from them are prohibited in the People's Socialist Republic of Albania." Hoxha said of borrowing money and allowing investment from other countries:
During this period Albania was the most isolated and poorest country in Europe and socially backwards by European standards. It had the lowest standard of living in Europe. However, a result of economic self-sufficiency, Albania had a minimal foreign debt. In 1983, Albania imported goods worth $280 million but exported goods worth $290 million, producing a trade surplus of $10 million.

In 1981, Hoxha ordered the execution of several party and government officials in a new purge. Prime Minister Mehmet Shehu, the second-most powerful man in Albania and Hoxha's closest comrade-in-arms for 40 years, was reported to have committed suicide in December 1981. He was subsequently condemned as a "traitor" to Albania, and was also accused of operating in the service of multiple intelligence agencies. It is generally believed that he was either killed or shot himself during a power struggle or over differing foreign policy matters with Hoxha. Hoxha also wrote a large assortment of books during this period, resulting in over 65 volumes of collected works, condensed into six volumes of selected works.
Hoxha suffered a heart attack in 1973 from which he never fully recovered. In increasingly precarious health from the late 1970s onward, he turned most state functions over to Ramiz Alia. In his final days he was confined to a wheelchair and suffering from diabetes, which had developed in 1948, and cerebral ischemia, from which he had suffered since 1983. On 9 April 1985, he was struck by a massive ventricular fibrillation. All efforts to reverse it failed, and he died in the early morning of 11 April 1985, aged 76. He was succeeded by Ramiz Alia.

Hoxha's death left Albania with a legacy of isolation and fear of the outside world. Despite some economic progress made by Hoxha, the country was in economic stagnation; Albania had been the poorest European country throughout much of the Cold War period. Following the transition to capitalism in 1992, Hoxha's legacy diminished, so that by the early 21st century very little of it was still in place in Albania.

The surname "Hoxha" is the Albanian variant of Hodja (from ), a title given to his ancestors due to their efforts to teach Albanians about Islam. In addition, among the population he was widely known by his nickname of "Dulla", a short form for the Muslim name Abdullah stemming from his Muslim roots.

Hoxha's parents were Halil and Gjylihan (Gjylo) Hoxha, and Hoxha had three sisters named Fahrije, Haxhire and Sanije. Hysen Hoxha () was Enver Hoxha's uncle and was a militant who campaigned vigorously for the independence of Albania, which occurred when Enver was four years old. His grandfather Beqir was involved in the Gjirokastër section of the League of Prizren.

Hoxha's son Sokol Hoxha was the CEO of the Albanian Post and Telecommunication service and is married to Liliana Hoxha. The later democratic president of Albania Sali Berisha was often seen socializing with Sokol Hoxha and other close relatives of leading communist figures in Albania.

Hoxha's daughter, Pranvera, is an architect. Along with her husband, Klement Kolaneci, she designed the Enver Hoxha Museum in Tirana, a white-tiled pyramid. Some sources have referred to the edifice, said to be the most expensive ever constructed in Albanian history, as the "Enver Hoxha Mausoleum," though this was not an official appellation. The museum opened in 1988, three years after her father's death, and in 1991 was transformed into a conference centre and exhibition venue renamed Pyramid of Tirana.

Banda Mustafaj was a group of four Albanian emigres, led by Xhevdet Mustafa, who wanted to assassinate Enver Hoxha in 1982. The gang was connected to counter-revolutionary elements such as the Albanian mafia and members of the royal House of Zogu. The plan failed and two of its members were killed and another one was arrested. It marked the only real effort to kill Hoxha.




</doc>
<doc id="10287" url="https://en.wikipedia.org/wiki?curid=10287" title="Hirohito">
Hirohito

Hirohito (; 29 April 1901 – 7 January 1989) was the 124th Emperor of Japan according to the traditional order of succession, reigning from 25 December 1926, until his death. He was succeeded by his eldest son, Akihito. In Japan, he is now referred to primarily by his posthumous name, Emperor Shōwa. The word "Shōwa" is the name of the era that corresponded with the Emperor's reign, and was made the Emperor's own name upon his death. The name Hirohito means "abundant benevolence".

At the start of his reign, Japan was already one of the great powers—the ninth-largest economy in the world, the third-largest naval power, and one of the four permanent members of the council of the League of Nations. He was the head of state under the Constitution of the Empire of Japan during Japan's imperial expansion, militarization, and involvement in World War II. After Japan's surrender he was not prosecuted for war crimes as many other leading government figures were, and his degree of involvement in wartime decisions remains controversial. During the post-war period, he became the symbol of the new state under the post-war constitution and Japan's recovery, and by the end of his reign, Japan had emerged as the world's second largest economy.

Born in Tokyo's Aoyama Palace (during the reign of his grandfather, Emperor Meiji), Hirohito was the first son of Crown Prince Yoshihito (the future Emperor Taishō) and Crown Princess Sadako (the future Empress Teimei). He was the grandson of Emperor Meiji and Yanagihara Naruko. His childhood title was Prince Michi. On the 70th day after his birth, Hirohito was removed from the court and placed in the care of the family of Count Kawamura Sumiyoshi, a former vice-admiral, who was to rear him as if he were his own grandchild. At the age of 3, Hirohito and his brother Chichibu were returned to court when Kawamura died – first to the imperial mansion in Numazu, Shizuoka, then back to the Aoyama Palace. In 1908, he began elementary studies at the Gakushūin (Peers School).

When his grandfather, Emperor Meiji, died on 30 July 1912, Hirohito's father, Yoshihito, assumed the throne and Hirohito became the heir apparent. At the same time, he was formally commissioned in both the army and navy as a second lieutenant and ensign, respectively, and was also decorated with the Grand Cordon of the Order of the Chrysanthemum. In 1914, he was promoted to the ranks of lieutenant in the army and sub-lieutenant in the navy, then to captain and lieutenant in 1916. He was formally proclaimed Crown Prince and heir apparent on 2 November 1916; but an investiture ceremony was not strictly necessary to confirm this status as heir to the throne.

Hirohito attended Gakushūin Peers' School from 1908 to 1914 and then a special institute for the crown prince (Tōgū-gogakumonsho) from 1914 to 1921.
In 1920, Hirohito was promoted to the rank of Major in the army and Lieutenant Commander in the navy. In 1921, Hirohito took a six-month tour of Western Europe, including the United Kingdom, France, Italy, the Netherlands, and Belgium.

After his return to Japan, Hirohito became Regent of Japan (Sesshō) on 29 November 1921, in place of his ailing father who was affected by a mental illness. In 1923, he was promoted to the rank of Lieutenant-Colonel in the army and Commander in the navy, and to army Colonel and Navy Captain in 1925.

During Hirohito's regency, a number of important events occurred:

In the Four-Power Treaty on Insular Possessions signed on 13 December 1921, Japan, the United States, Britain, and France agreed to recognize the status quo in the Pacific, and Japan and Britain agreed to terminate formally the Anglo-Japanese Alliance. The Washington Naval Treaty was signed on 6 February 1922. Japan withdrew troops from the Siberian Intervention on 28 August 1922. The Great Kantō earthquake devastated Tokyo on 1 September 1923. On 27 December 1923, Daisuke Namba attempted to assassinate Hirohito in the Toranomon Incident but his attempt failed. During interrogation, he claimed to be a communist and was executed but some have suggested that he was in contact with the Nagacho faction in the Army.

Prince Hirohito married his distant cousin Princess Nagako Kuni (the future Empress Kōjun), the eldest daughter of Prince Kuniyoshi Kuni, on 26 January 1924. They had two sons and five daughters. (see Issue)

The daughters who lived to adulthood left the imperial family as a result of the American reforms of the Japanese imperial household in October 1947 (in the case of Princess Shigeko) or under the terms of the Imperial Household Law at the moment of their subsequent marriages (in the cases of Princesses Kazuko, Atsuko, and Takako).

On 25 December 1926, Hirohito assumed the throne upon his father, Yoshihito's, death. The Crown Prince was said to have received the succession ("senso"). The Taishō era's end and the Shōwa era's beginning (Enlightened Peace) were proclaimed. The deceased Emperor was posthumously renamed Emperor Taishō within days. Following Japanese custom, the new Emperor was never referred to by his given name, but rather was referred to simply as "His Majesty the Emperor", which may be shortened to "His Majesty". In writing, the Emperor was also referred to formally as "The Reigning Emperor".

In November 1928, the Emperor's ascension was confirmed in ceremonies ("sokui") which are conventionally identified as "enthronement" and "coronation" ("Shōwa no tairei-shiki"); but this formal event would have been more accurately described as a public confirmation that his Imperial Majesty possesses the Japanese Imperial Regalia, also called the Three Sacred Treasures, which have been handed down through the centuries.

The first part of Hirohito's reign took place against a background of financial crisis and increasing military power within the government, through both legal and extralegal means. The Imperial Japanese Army and Imperial Japanese Navy held veto power over the formation of cabinets since 1900, and between 1921 and 1944 there were no fewer than 64 incidents of political violence.

Hirohito narrowly missed assassination by a hand grenade thrown by a Korean independence activist, Lee Bong-chang, in Tokyo on 9 January 1932, in the Sakuradamon Incident.

Another notable case was the assassination of moderate Prime Minister Inukai Tsuyoshi in 1932, which marked the end of civilian control of the military. This was followed by an attempted military coup in February 1936, the February 26 incident, mounted by junior Army officers of the "Kōdōha" faction who had the sympathy of many high-ranking officers including Prince Chichibu (Yasuhito), one of the Emperor's brothers. This revolt was occasioned by a loss of political support by the militarist faction in Diet elections. The coup resulted in the murders of a number of high government and Army officials.

When Chief Aide-de-camp Shigeru Honjō informed him of the revolt, the Emperor immediately ordered that it be put down and referred to the officers as "rebels" ("bōto"). Shortly thereafter, he ordered Army Minister Yoshiyuki Kawashima to suppress the rebellion within the hour, and he asked reports from Honjō every thirty minutes. The next day, when told by Honjō that little progress was being made by the high command in quashing the rebels, the Emperor told him "I Myself, will lead the Konoe Division and subdue them." The rebellion was suppressed following his orders on 29 February.

Starting from the Mukden Incident in 1931, Japan occupied various Chinese territories and established various puppet governments. Such "aggression was recommended to Hirohito" by his chiefs of staff and prime minister Fumimaro Konoe, and Hirohito never personally objected to any invasions of China. His main concern seems to have been the possibility of an attack by the Soviet Union in the north. His questions to his chief of staff, Prince Kan'in, and minister of the army, Hajime Sugiyama, were mostly about the time it could take to crush Chinese resistance.

According to Akira Fujiwara, Hirohito endorsed the policy of qualifying the invasion of China as an "incident" instead of a "war"; therefore, he did not issue any notice to observe international law in this conflict (unlike what his predecessors did in previous conflicts officially recognized by Japan as wars), and the Deputy Minister of the Japanese Army instructed the Chief of staff of Japanese China Garrison Army on August 5 to not use the term "prisoners of war" for Chinese captives. This instruction led to the removal of the constraints of international law on the treatment of Chinese prisoners. And the works of Yoshiaki Yoshimi and Seiya Matsuno show that the Emperor authorized, by specific orders (rinsanmei), the use of chemical weapons against the Chinese. During the invasion of Wuhan, from August to October 1938, the Emperor authorized the use of toxic gas on 375 separate occasions, despite the resolution adopted by the League of Nations on May 14 condemning Japanese use of toxic gas.

On September 27, 1940, ostensibly under Hirohito's leadership, Japan was a contracting partner of the Tripartite Pact with Nazi Germany and Fascist Italy forming the Axis Powers. Before that, in July 1939, the Emperor quarrelled with his brother, Prince Chichibu, who was visiting him three times a week to support the treaty, and reprimanded the army minister Seishirō Itagaki. But after the success of the Wehrmacht in Europe, the Emperor consented to the alliance.

On September 4, 1941, the Japanese Cabinet met to consider war plans prepared by Imperial General Headquarters, and decided that:

The objectives to be obtained were clearly defined: a free hand to continue with the conquest of China and Southeast Asia, no increase in US or British military forces in the region, and cooperation by the West "in the acquisition of goods needed by our Empire".

On September 5, Prime Minister Konoe informally submitted a draft of the decision to the Emperor, just one day in advance of the Imperial Conference at which it would be formally implemented. On this evening, the Emperor had a meeting with the chief of staff of the army, Sugiyama, chief of staff of the navy, Osami Nagano, and Prime Minister Konoe. The Emperor questioned Sugiyama about the chances of success of an open war with the Occident. As Sugiyama answered positively, the Emperor scolded him:

Chief of Naval General Staff Admiral Nagano, a former Navy Minister and vastly experienced, later told a trusted colleague, "I have never seen the Emperor reprimand us in such a manner, his face turning red and raising his voice."

According to the traditional view, Hirohito was deeply concerned by the decision to place "war preparations first and diplomatic negotiations second", and he announced his intention to break with tradition. At the Imperial Conference on the following day, the Emperor directly questioned the chiefs of the Army and Navy general staffs, which was quite an unprecedented action.

Nevertheless, all speakers at the Imperial Conference were united in favor of war rather than diplomacy. Baron Yoshimichi Hara, President of the Imperial Council and the Emperor's representative, then questioned them closely, producing replies to the effect that war would be considered only as a last resort from some, and silence from others.

At this point, the Emperor astonished all present by addressing the conference personally, and in breaking the tradition of Imperial silence left his advisors "struck with awe." (Prime Minister Konoe's description of the event.) Hirohito stressed the need for peaceful resolution of international problems, expressed regret at his ministers' failure to respond to Baron Hara's probings, and recited a poem written by his grandfather, Emperor Meiji which, he said, he had read "over and over again":

Recovering from their shock, the ministers hastened to express their profound wish to explore all possible peaceful avenues. The Emperor's presentation was in line with his practical role as leader of the State Shinto religion.

At this time, Army Imperial Headquarters was continually communicating with the Imperial household in detail about the military situation. On October 8, Sugiyama signed a 47-page report to the Emperor (sōjōan) outlining in minute detail plans for the advance into Southeast Asia. During the third week of October, Sugiyama gave the Emperor a 51-page document, "Materials in Reply to the Throne", about the operational outlook for the war.

As war preparations continued, Prime Minister Fumimaro Konoe found himself more and more isolated and gave his resignation on October 16. He justified himself to his chief cabinet secretary, Kenji Tomita, by stating:

The army and the navy recommended the candidacy of Prince Naruhiko Higashikuni, one of the Emperor's uncles. According to the Shōwa "Monologue", written after the war, the Emperor then said that if the war were to begin while a member of the imperial house was prime minister, the imperial house would have to carry the responsibility and he was opposed to this.

Instead, the Emperor chose the hard-line General Hideki Tōjō, who was known for his devotion to the imperial institution, and asked him to make a policy review of what had been sanctioned by the Imperial Conferences. On November 2, Tōjō, Sugiyama and Nagano reported to the Emperor that the review of eleven points had been in vain. Emperor Hirohito gave his consent to the war and then asked: "Are you going to provide justification for the war?" The decision for war against the United States was presented for approval to Hirohito by General Tōjō, Naval Minister Admiral Shigetarō Shimada, and Japanese Foreign Minister Shigenori Tōgō.

On November 3, Nagano explained in detail the plan of the attack on Pearl Harbor to the Emperor. On November 5, Emperor Hirohito approved in imperial conference the operations plan for a war against the Occident and had many meetings with the military and Tōjō until the end of the month. On November 25 Henry L. Stimson, United States Secretary of War noted in his diary that he had discussed with US President Franklin D. Roosevelt the severe likelihood that Japan was about to launch a surprise attack, and that the question had been "how we should maneuver them [the Japanese] into the position of firing the first shot without allowing too much danger to ourselves".

On the following day, November 26, 1941, US Secretary of State Cordell Hull presented the Japanese ambassador with the Hull note, which as one of its conditions demanded the complete withdrawal of all Japanese troops from French Indochina and China. Japanese Prime Minister Tojo Hideki said to his cabinet, "this is an ultimatum". On December 1, an Imperial Conference sanctioned the "War against the United States, United Kingdom and the Kingdom of the Netherlands". On December 8 (December 7 in Hawaii) 1941, in simultaneous attacks, Japanese forces struck at the Hong Kong Garrison, the US Fleet in Pearl Harbor and in the Philippines and began the invasion of Malaya.

With the nation fully committed to the war, the Emperor took a keen interest in military progress and sought to boost morale. According to Akira Yamada and Akira Fujiwara, the Emperor made major interventions in some military operations. For example, he pressed Sugiyama four times, on January 13 and 21 and February 9 and 26, to increase troop strength and launch an attack on Bataan. On February 9, March 19 and May 29, the Emperor ordered the Army Chief of staff to examine the possibilities for an attack on Chungking, which led to Operation Gogo.

As the tide of war began to turn against Japan (around late 1942 and early 1943), some people argue that the flow of information to the palace gradually began to bear less and less relation to reality, while others suggest that the Emperor worked closely with Prime Minister Hideki Tōjō, continued to be well and accurately briefed by the military, and knew Japan's military position precisely right up to the point of surrender. The chief of staff of the General Affairs section of the Prime Minister's office, Shuichi Inada, remarked to Tōjō's private secretary, Sadao Akamatsu:

In the first six months of war, all the major engagements had been victories. As the tide turned in the summer of 1942 with the battle of Midway and the landing of the American forces on Guadalcanal and Tulagi in August, the Emperor recognized the potential danger and pushed the navy and the army for greater efforts. In September 1942, Emperor Hirohito signed the Imperial Rescript condemning to death American Fliers: Lieutenants Dean E. Hallmark and William G. Farrow and Corporal Harold A. Spatz and commuting to life sentences: Lieutenants Robert J. Meder, Chase Nielsen, Robert L. Hite and George Barr and Corporal Jacob DeShazer. When informed in August 1943 by Sugiyama that the American advance through the Solomon Islands could not be stopped, the Emperor asked his chief of staff to consider other places to attack: "When and where are you ever going to put up a good fight? And when are you ever going to fight a decisive battle?" On August 24, the Emperor reprimanded Nagano and on September 11, he ordered Sugiyama to work with the Navy to implement better military preparation and give adequate supply to soldiers fighting in Rabaul.

Throughout the following years from 1943 to 1945, the sequence of drawn and then decisively lost naval and land engagements was reported to the public as a series of great victories. Only gradually did it become apparent to the Japanese people that the situation was very grim due to growing shortages of food, medicine, and fuel as U.S submarines began wiping out Japanese shipping. Starting in mid 1944, U.S. air raids on the cities of Japan made a mockery of the unending tales of victory. Later that year, with the downfall of Hideki Tōjō's government, two other prime ministers were appointed to continue the war effort, Kuniaki Koiso and Kantarō Suzuki—each with the formal approval of the Emperor. Both were unsuccessful and Japan was nearing defeat.

In early 1945, in the wake of the losses in Battle of Leyte, Emperor Hirohito began a series of individual meetings with senior government officials to consider the progress of the war. All but ex-Prime Minister Fumimaro Konoe advised continuing the war. Konoe feared a communist revolution even more than defeat in war and urged a negotiated surrender. In February 1945, during the first private audience with the Emperor which he had been allowed in three years, Konoe advised Hirohito to begin negotiations to end the war. According to Grand Chamberlain Hisanori Fujita, the Emperor, still looking for a "tennozan" (a great victory) in order to provide a stronger bargaining position, firmly rejected Konoe's recommendation.

With each passing week a great victory became less likely. In April the Soviet Union issued notice that it would not renew its neutrality agreement. Japan's ally Germany surrendered in early May 1945. In June, the cabinet reassessed the war strategy, only to decide more firmly than ever on a fight to the last man. This strategy was officially affirmed at a brief Imperial Council meeting, at which, as was normal, the Emperor did not speak.

The following day, Lord Keeper of the Privy Seal Kōichi Kido prepared a draft document which summarized the hopeless military situation and proposed a negotiated settlement. Extremists in Japan were also calling for a death-before-dishonor mass suicide, modeled on the "47 Ronin" incident. By mid-June 1945, the cabinet had agreed to approach the Soviet Union to act as a mediator for a negotiated surrender, but not before Japan's bargaining position had been improved by repulse of the anticipated Allied invasion of mainland Japan.

On June 22, the Emperor met with his ministers, saying "I desire that concrete plans to end the war, unhampered by existing policy, be speedily studied and that efforts be made to implement them." The attempt to negotiate a peace via the Soviet Union came to nothing. There was always the threat that extremists would carry out a coup or foment other violence. On July 26, 1945, the Allies issued the Potsdam Declaration demanding unconditional surrender. The Japanese government council, the Big Six, considered that option and recommended to the Emperor that it be accepted only if one to four conditions were agreed upon, including a guarantee of the Emperor's continued position in Japanese society. The Emperor decided not to surrender.

On August 9, 1945, following the atomic bombings of Hiroshima and Nagasaki and the Soviet declaration of war, Emperor Hirohito told Kōichi Kido: "the Soviet Union has declared war and today began hostilities against us." On August 10, the cabinet drafted an "Imperial Rescript ending the War" following the Emperor's indications that the declaration did not compromise any demand which prejudiced the prerogatives of His Majesty as a Sovereign Ruler.

On August 12, 1945, the Emperor informed the imperial family of his decision to surrender. One of his uncles, Prince Yasuhiko Asaka, asked whether the war would be continued if the "kokutai" (national polity) could not be preserved. The Emperor simply replied "of course." On August 14, the Suzuki government notified the Allies that it had accepted the Potsdam Declaration.

On August 15, a recording of the Emperor's surrender speech (""Gyokuon-hōsō"", literally ""Jewel Voice Broadcast"") was broadcast over the radio (the first time the Emperor was heard on the radio by the Japanese people) signifying the unconditional surrender of Japan's military forces. During the historic broadcast the Emperor stated: "Moreover, the enemy has begun to employ a new and most cruel bomb, the power of which to do damage is, indeed, incalculable, taking the toll of many innocent lives. Should we continue to fight, not only would it result in an ultimate collapse and obliteration of the Japanese nation, but also it would lead to the total extinction of human civilization." The speech also noted that "the war situation has developed not necessarily to Japan's advantage" and ordered the Japanese to "endure the unendurable". The speech, using formal, archaic Japanese, was not readily understood by many commoners. According to historian Richard Storry in "A History of Modern Japan", the Emperor typically used "a form of language familiar only to the well-educated" and to the more traditional samurai families.

A faction of the army opposed to the surrender attempted a coup d'état on the evening of 14 August. They seized the Imperial Palace (the Kyūjō incident), but the physical recording of the emperor's speech was hidden and preserved overnight. The coup was crushed by the next morning, and the speech was broadcast.

In his first ever press conference given in Tokyo in 1975, when he was asked what he thought of the bombing of Hiroshima, the Emperor answered: "It's very regrettable that nuclear bombs were dropped and I feel sorry for the citizens of Hiroshima but it couldn't be helped because that happened in wartime" (shikata ga nai).

Some historians believe Emperor Hirohito was directly responsible for the atrocities committed by the imperial forces in the Second Sino-Japanese War and in World War II. They feel that he, and some members of the imperial family such as his brother Prince Chichibu, his cousins Prince Takeda and Prince Fushimi, and his uncles Prince Kan'in, Prince Asaka, and Prince Higashikuni, should have been tried for war crimes.

The debate over Hirohito's responsibility for war crimes concerns how much real control the Emperor had over the Japanese military during the two wars. Officially, the imperial constitution, adopted under Emperor Meiji, gave full power to the Emperor. Article 4 prescribed that, "The Emperor is the head of the Empire, combining in Himself the rights of sovereignty, and exercises them, according to the provisions of the present Constitution", while, according to article 6, "The Emperor gives sanction to laws and orders them to be promulgated and executed", and article 11, "The Emperor has the supreme command of the Army and the Navy." The Emperor was thus the leader of the Imperial General Headquarters.

Poison gas weapons, such as phosgene, were produced by Unit 731 and authorized by specific orders given by Hirohito himself, transmitted by the chief of staff of the army. For example, Hirohito authorised the use of toxic gas 375 times during the battle of Wuhan from August to October 1938.

Historians such as Herbert Bix, Akira Fujiwara, Peter Wetzler, and Akira Yamada assert that the post-war view focusing on imperial conferences misses the importance of numerous "behind the chrysanthemum curtain" meetings where the real decisions were made between the Emperor, his chiefs of staff, and the cabinet. Historians such as Fujiwara and Wetzler, based on the primary sources and the monumental work of Shirō Hara, have produced evidence suggesting that the Emperor worked through intermediaries to exercise a great deal of control over the military and was neither bellicose nor a pacifist, but an opportunist who governed in a pluralistic decision-making process. American historian Herbert P. Bix argues that Emperor Hirohito might have been the prime mover of most of the events of the two wars.

The view promoted by both the Japanese Imperial Palace and the American occupation forces immediately after World War II portrayed Emperor Hirohito as a powerless figurehead behaving strictly according to protocol, while remaining at a distance from the decision-making processes. This view was endorsed by Prime Minister Noboru Takeshita in a speech on the day of Hirohito's death, in which Takeshita asserted that the war "had broken out against [Hirohito's] wishes". Takeshita's statement provoked outrage in nations in East Asia and Commonwealth nations such as the United Kingdom, Canada, Australia, and New Zealand. According to historian Fujiwara "the thesis that the Emperor, as an organ of responsibility, could not reverse cabinet decision, is a myth fabricated after the war".

In Japan, debate over the Emperor's responsibility was taboo while he was still alive. After his death, however, debate began to surface over the extent of his involvement and thus his culpability.

In the years immediately after Hirohito's death, the debate in Japan was fierce. Susan Chira reported, "Scholars who have spoken out against the late Emperor have received threatening phone calls from Japan's extremist right wing." One example of actual violence occurred in 1990 when the mayor of Nagasaki, Hitoshi Motoshima, was shot and critically wounded by a member of the ultranationalist group, Seikijuku. A year before, in 1989, Motoshima had broken what was characterized as "one of [Japan's] most sensitive taboos" by asserting that Emperor Hirohito bore responsibility for World War II. Motoshima managed to recover from the attack.

Kentarō Awaya argues that post-war Japanese public opinion supporting protection of the Emperor was influenced by U.S. propaganda promoting the view that the Emperor together with the Japanese people had been fooled by the military.

As the Emperor chose his uncle Prince Higashikuni as prime minister to assist the occupation, there were attempts by numerous leaders to have him put on trial for alleged war crimes. Many members of the imperial family, such as Princes Chichibu, Takamatsu and Higashikuni, pressured the Emperor to abdicate so that one of the Princes could serve as regent until Crown Prince Akihito came of age. On February 27, 1946, the Emperor's youngest brother, Prince Mikasa (Takahito), even stood up in the privy council and indirectly urged the Emperor to step down and accept responsibility for Japan's defeat. According to Minister of Welfare Ashida's diary, "Everyone seemed to ponder Mikasa's words. Never have I seen His Majesty's face so pale."

U.S. General Douglas MacArthur insisted that Emperor Hirohito retain the throne. MacArthur saw the Emperor as a symbol of the continuity and cohesion of the Japanese people. Some historians criticize the decision to exonerate the Emperor and all members of the imperial family who were implicated in the war, such as Prince Chichibu, Prince Asaka, Prince Higashikuni and Prince Hiroyasu Fushimi, from criminal prosecutions.

Before the war crime trials actually convened, the SCAP, the IPS, and Japanese officials worked behind the scenes not only to prevent the Imperial family from being indicted, but also to influence the testimony of the defendants to ensure that no one implicated the Emperor. High officials in court circles and the Japanese government collaborated with Allied GHQ in compiling lists of prospective war criminals, while the individuals arrested as "Class A" suspects and incarcerated solemnly vowed to protect their sovereign against any possible taint of war responsibility. Thus, "months before the Tokyo tribunal commenced, MacArthur's highest subordinates were working to attribute ultimate responsibility for Pearl Harbor to Hideki Tōjō" by allowing "the major criminal suspects to coordinate their stories so that the Emperor would be spared from indictment." According to John W. Dower, "This successful campaign to absolve the Emperor of war responsibility knew no bounds. Hirohito was not merely presented as being innocent of any formal acts that might make him culpable to indictment as a war criminal, he was turned into an almost saintly figure who did not even bear moral responsibility for the war." According to Bix, "MacArthur's truly extraordinary measures to save Hirohito from trial as a war criminal had a lasting and profoundly distorting impact on Japanese understanding of the lost war."

Hirohito was not put on trial, but he was forced to explicitly reject the quasi-official claim that the Emperor of Japan was an "arahitogami", i.e., an incarnate divinity. This was motivated by the fact that, according to the Japanese constitution of 1889, the Emperor had a divine power over his country, which was derived from the Shinto belief that the Japanese Imperial Family was the offspring of the sun goddess Amaterasu. Hirohito was however persistent in the idea that the Emperor of Japan should be considered a descendant of the gods. In December 1945, he told his vice-grand-chamberlain Michio Kinoshita: "It is permissible to say that the idea that the Japanese are descendants of the gods is a false conception; but it is absolutely impermissible to call chimerical the idea that the Emperor is a descendant of the gods." In any case, the "renunciation of divinity" was noted more by foreigners than by Japanese, and seems to have been intended for the consumption of the former. The theory of a constitutional monarchy had already had some proponents in Japan. In 1935, when Tatsukichi Minobe advocated the theory that sovereignty resides in the state, of which the Emperor is just an organ (the "tennō kikan setsu"), it caused a furor. He was forced to resign from the House of Peers and his post at the Tokyo Imperial University, his books were banned and an attempt was made on his life. Not until 1946 was the tremendous step made to alter the Emperor's title from "imperial sovereign" to "constitutional monarch".

Although the Emperor had supposedly repudiated claims to divinity, his public position was deliberately left vague, partly because General MacArthur thought him probable to be a useful partner to get the Japanese to accept the occupation, and partly due to behind-the-scenes maneuverings by Shigeru Yoshida to thwart attempts to cast him as a European-style monarch.

Nevertheless, Hirohito's status as a limited constitutional monarch status was formalized with the enactment of the 1947 Constitution–officially, an amendment to the Meiji Constitution. It defined the Emperor as "the symbol of the state and the unity of the people," and stripped him of even nominal power in government matters. His role was limited to matters of state as delineated in the Constitution, and in most cases his actions in that realm were carried out in accordance with the binding instructions of the Cabinet.

For the rest of his life, Hirohito was an active figure in Japanese life, and performed many of the duties commonly associated with a constitutional head of state. He and his family maintained a strong public presence, often holding public walkabouts, and making public appearances on special events and ceremonies. He also played an important role in rebuilding Japan's diplomatic image, traveling abroad to meet with many foreign leaders, including Queen Elizabeth II (1971) and President Gerald Ford (1975).
His status and image became strongly positive in the United States.

The Emperor was deeply interested in and well-informed about marine biology, and the Imperial Palace contained a laboratory from which the Emperor published several papers in the field under his personal name "Hirohito". His contributions included the description of several dozen species of Hydrozoa new to science.

Emperor Hirohito maintained an official boycott of the Yasukuni Shrine after it was revealed to him that Class-A war criminals had secretly been enshrined after its post-war rededication. This boycott lasted from 1978 until his death. This boycott has been maintained by his son Akihito.

On July 20, 2006, "Nihon Keizai Shimbun" published a front-page article about the discovery of a memorandum detailing the reason that the Emperor stopped visiting Yasukuni. The memorandum, kept by former chief of Imperial Household Agency Tomohiko Tomita, confirms for the first time that the enshrinement of 14 Class-A war criminals in Yasukuni was the reason for the boycott. Tomita recorded in detail the contents of his conversations with the Emperor in his diaries and notebooks. According to the memorandum, in 1988, the Emperor expressed his strong displeasure at the decision made by Yasukuni Shrine to include Class-A war criminals in the list of war dead honored there by saying, "At some point, Class-A criminals became enshrined, including Matsuoka and Shiratori. I heard Tsukuba acted cautiously." Tsukuba is believed to refer to Fujimaro Tsukuba, the former chief Yasukuni priest at the time, who decided not to enshrine the war criminals despite having received in 1966 the list of war dead compiled by the government. "What's on the mind of Matsudaira's son, who is the current head priest?" "Matsudaira had a strong wish for peace, but the child didn't know the parent's heart. That's why I have not visited the shrine since. This is my heart." Matsudaira is believed to refer to Yoshitami Matsudaira, who was the grand steward of the Imperial Household immediately after the end of World War II. His son, Nagayoshi, succeeded Fujimaro Tsukuba as the chief priest of Yasukuni and decided to enshrine the war criminals in 1978. Nagayoshi Matsudaira died in 2006, which some commentators have speculated is the reason for release of the memo.

On September 22, 1987, the Emperor underwent surgery on his pancreas after having digestive problems for several months. The doctors discovered that he had duodenal cancer. The Emperor appeared to be making a full recovery for several months after the surgery. About a year later, however, on September 19, 1988, he collapsed in his palace, and his health worsened over the next several months as he suffered from continuous internal bleeding. On January 7, 1989, at 7:55 AM, the grand steward of Japan's Imperial Household Agency, Shoichi Fujimori, officially announced the death of Emperor Hirohito, and revealed details about his cancer for the first time. Hirohito was survived by his wife, his five surviving children, ten grandchildren and one great-grandchild.

The Emperor was succeeded by his son, the current Emperor Akihito, whose enthronement ceremony was held on November 12, 1990.

The Emperor's death ended the Shōwa era. On the same day a new era began: the Heisei era, effective at midnight the following day. From January 7, until January 31, the Emperor's formal appellation was "Departed Emperor". His definitive posthumous name, Shōwa Tennō, was determined on January 13 and formally released on January 31 by Toshiki Kaifu, the prime minister.

On February 24, Emperor Hirohito's state funeral was held, and unlike that of his predecessor, it was formal but not conducted in a strictly Shinto manner. A large number of world leaders attended the funeral. Emperor Hirohito is buried in the Musashi Imperial Graveyard in Hachiōji, alongside Emperor Taishō, his father.










</doc>
<doc id="10289" url="https://en.wikipedia.org/wiki?curid=10289" title="Emsworth">
Emsworth

Emsworth is a small town in Hampshire on the south coast of England, near the border of West Sussex. It lies at the north end of an arm of Chichester Harbour, a large and shallow inlet from the English Channel.

Emsworth has a population of approximately 10,000. The town has a basin for small yachts and fishing boats, which fills at high tide and can be emptied through a sluice at low tide.

Emsworth began as a Saxon village. At first it was linked to the settlement of Warblington nearby. People from Emsworth worshipped at St Peter's Chapel or in the church at Warblington. Emsworth was not mentioned in the Domesday Book of 1086, as it was included with Warblington.

Emsworth's name came from Anglo Saxon "Æmeles worþ" = "a man called Æmele's enclosure".

Emsworth grew to be larger and more important: in 1239 Emsworth was granted the right to hold a market, and there was also an annual fair In 1332 Emsworth ("Empnesworth") was one of Hampshire's four Customs Ports.

During the 18th and 19th centuries, Emsworth was still a port. Emsworth was known for shipbuilding, boat building and rope making. Grain from the area was ground into flour by tidal mills and transported by ship to places such as London and Portsmouth. Timber from the area was also exported in the 18th and 19th centuries. The River Ems, which is named after the town (not, as often believed, the town being named after the river), flows into the Slipper millpond. The mill itself is now used as offices.
In the 19th century Emsworth had as many as 30 pubs and beer houses; today, only nine remain. Emsworth's once famous oyster industry went into decline in the early years of the 20th century. Recently, Emsworth's last remaining oyster boat, "The Terror," was restored and is now sailing again.

At the beginning of the 19th century, Emsworth had a population of less than 1,200 but it was still considered a large village for the time. By the end of the 18th century, it became fashionable for wealthy people to spend the summer by the sea. In 1805 a bathing house was built where people could have a bath in seawater.

The parish Church of St James was built in 1840. Queen Victoria visited Emsworth in 1842, resulting in Queen Street and Victoria Road being named after her. In 1847 the London, Brighton and South Coast Railway (now the West Coastway line) came to Emsworth, with a railway station built to serve the town.

By 1901 the population of Emsworth was about 2,000. It grew rapidly during the 20th century to about 5,000 by the middle of the century. In 1906 construction began on the post office, with local cricketer George Wilder laying an inscribed brick. The renamed Emsworth Recreation Ground dates from 1909 and is the current home of Emsworth Cricket Club, which was founded in 1811. Cricket in Emsworth has been played at the same ground, Cold Harbour Lawn, since 1761.

In 1902 the Emsworth oyster industry went into rapid decline. This was after many of the guests at mayoral banquets in Southampton and Winchester became seriously ill and four died after consuming oysters. The infection was due to oysters sourced from Emsworth, as the oyster beds had been contaminated with raw sewage. Fishing oysters at Emsworth was subsequently halted until new sewers were dug, though the industry never completely recovered.
During the Second World War, nearby Thorney Island was used as a Royal Air Force station, playing a role in defence in the Battle of Britain. The north of Emsworth at this time was used for growing flowers and further north was woodland (today Hollybank Woods). In the run up to D-Day, the Canadian Army used these woods as one of their pre-invasion assembly points for men and material. Today the foundations of their barracks can still be seen. In the 1960s large parts of this area were developed with a mix of bungalow and terraced housing.

For a few years (2001 to 2007), Emsworth held a food festival. It was the largest event of its type in the UK, with more than 50,000 visitors in 2007. The festival was cancelled due to numerous complaints of disruption to residents and businesses in the proximity.

The harbour is now used almost exclusively for recreational sailing. The town has two sailing clubs, Emsworth Sailing Club (established in 1919) and Emsworth Slipper Sailing Club (in 1921). Both clubs organise a programme of racing and social events during the sailing season. In April 2014 Emsworth Sailing Club received national media coverage, after a car was driven into the clubhouse, causing a loud explosion and requiring 30 firefighters to extinguish the blaze.

Emsworth is twinned with Saint-Aubin-sur-Mer in Normandy, France

The town is part of the Havant constituency, which since the 1983 election has been a Conservative seat. The current Member of Parliament is Alan Mak MP. The town is represented at Havant Borough Council by Councillors Colin Mackey, Rivka Cresswell and Lulu Bowerman. The local County Councillor is Ray Bolton. The town has branches of the Conservative Party, Liberal Democrats, the Labour Party and United Kingdom Independence Party.

Emsworth railway station is on the West Coastway Line. It has services that run to Portsmouth, Southampton, Brighton and London Victoria.

Stagecoach South operate the number 700 bus which operates between Brighton and Southsea. Local bus services are provided by Emsworth & District, which operate services to Havant and Chichester.





</doc>
<doc id="10290" url="https://en.wikipedia.org/wiki?curid=10290" title="Emulsion">
Emulsion

An emulsion is a mixture of two or more liquids that are normally immiscible (unmixable or unblendable). Emulsions are part of a more general class of two-phase systems of matter called colloids. Although the terms "colloid" and "emulsion" are sometimes used interchangeably, "emulsion" should be used when both phases, dispersed and continuous, are liquids. In an emulsion, one liquid (the dispersed phase) is dispersed in the other (the continuous phase). Examples of emulsions include vinaigrettes, homogenized milk, mayonnaise, and some cutting fluids for metal working. Graphene and its modified forms are also a good example of recent unconventional surfactants helping in stabilizing emulsion systems.

The word "emulsion" comes from the Latin mulgeo, mulgere "to milk", as milk is an emulsion of fat and water, along with other components.

Two liquids can form different types of emulsions. As an example, oil and water can form, first, an oil-in-water emulsion, wherein the oil is the dispersed phase, and water is the dispersion medium. (Lipoproteins, as implemented by all complex living organisms, are one example of this.) Second, they can form a water-in-oil emulsion, wherein water is the dispersed phase and oil is the external phase. Multiple emulsions are also possible, including a "water-in-oil-in-water" emulsion and an "oil-in-water-in-oil" emulsion.

Emulsions, being liquids, do not exhibit a static internal structure. The droplets dispersed in the liquid matrix (called the “dispersion medium”) are usually assumed to be statistically distributed.

The term "emulsion" is also used to refer to the photo-sensitive side of photographic film. Such a photographic emulsion consists of silver halide colloidal particles dispersed in a gelatin matrix. Nuclear emulsions are similar to photographic emulsions, except that they are used in particle physics to detect high-energy elementary particles.

Emulsions contain both a dispersed and a continuous phase, with the boundary between the phases called the "interface". Emulsions tend to have a cloudy appearance because the many phase interfaces scatter light as it passes through the emulsion. Emulsions appear white when all light is scattered equally. If the emulsion is dilute enough, higher-frequency (low-wavelength) light will be scattered more, and the emulsion will appear bluer – this is called the "Tyndall effect". If the emulsion is concentrated enough, the color will be distorted toward comparatively longer wavelengths, and will appear more yellow. This phenomenon is easily observable when comparing skimmed milk, which contains little fat, to cream, which contains a much higher concentration of milk fat. One example would be a mixture of water and oil.

Two special classes of emulsions – microemulsions and nanoemulsions, with droplet sizes below 100 nm – appear translucent. This property is due to the fact that light waves are scattered by the droplets only if their sizes exceed about one-quarter of the wavelength of the incident light. Since the visible spectrum of light is composed of wavelengths between 390 and 750 nanometers (nm), if the droplet sizes in the emulsion are below about 100 nm, the light can penetrate through the emulsion without being scattered. Due to their similarity in appearance, translucent nanoemulsions and microemulsions are frequently confused. Unlike translucent nanoemulsions, which require specialized equipment to be produced, microemulsions are spontaneously formed by “solubilizing” oil molecules with a mixture of surfactants, co-surfactants, and co-solvents. The required surfactant concentration in a microemulsion is, however, several times higher than that in a translucent nanoemulsion, and significantly exceeds the concentration of the dispersed phase. Because of many undesirable side-effects caused by surfactants, their presence is disadvantageous or prohibitive in many applications. In addition, the stability of a microemulsion is often easily compromised by dilution, by heating, or by changing pH levels.

Common emulsions are inherently unstable and, thus, do not tend to form spontaneously. Energy input – through shaking, stirring, homogenizing, or exposure to power ultrasound – is needed to form an emulsion. Over time, emulsions tend to revert to the stable state of the phases comprising the emulsion. An example of this is seen in the separation of the oil and vinegar components of vinaigrette, an unstable emulsion that will quickly separate unless shaken almost continuously. There are important exceptions to this rule – microemulsions are thermodynamically stable, while translucent nanoemulsions are kinetically stable.

Whether an emulsion of oil and water turns into a "water-in-oil" emulsion or an "oil-in-water" emulsion depends on the volume fraction of both phases and the type of emulsifier (surfactant) (see "Emulsifier", below) present. In general, the Bancroft rule applies. Emulsifiers and emulsifying particles tend to promote dispersion of the phase in which they do not dissolve very well. For example, proteins dissolve better in water than in oil, and so tend to form oil-in-water emulsions (that is, they promote the dispersion of oil droplets throughout a continuous phase of water).

The geometric structure of an emulsion mixture of two lyophobic liquids with a large concentration of the secondary component is fractal: Emulsion particles unavoidably form dynamic inhomogeneous structures on small length scale. The geometry of these structures is fractal. The size of elementary irregularities is governed by a universal function which depends on the volume content of the components. The fractal dimension of these irregularities is 2.5.

Emulsion stability refers to the ability of an emulsion to resist change in its properties over time. There are four types of instability in emulsions: flocculation, creaming, coalescence, and Ostwald ripening. Flocculation occurs when there is an attractive force between the droplets, so they form flocs, like bunches of grapes. Coalescence occurs when droplets bump into each other and combine to form a larger droplet, so the average droplet size increases over time. Emulsions can also undergo creaming, where the droplets rise to the top of the emulsion under the influence of buoyancy, or under the influence of the centripetal force induced when a centrifuge is used.

An appropriate "surface active agent" (or "surfactant") can increase the kinetic stability of an emulsion so that the size of the droplets does not change significantly with time. It is then said to be stable.

The stability of emulsions can be characterized using techniques such as light scattering, focused beam reflectance measurement, centrifugation, and rheology. Each method has advantages and disadvantages.

The kinetic process of destabilization can be rather long – up to several months, or even years for some products. Often the formulator must accelerate this process in order to test products in a reasonable time during product design. Thermal methods are the most commonly used – these consist of increasing the emulsion temperature to accelerate destabilization (if below critical temperatures for phase inversion or chemical degradation). Temperature affects not only the viscosity but also the inter-facial tension in the case of non-ionic surfactants or, on a broader scope, interactions of forces inside the system. Storing an emulsion at high temperatures enables the simulation of realistic conditions for a product (e.g., a tube of sunscreen emulsion in a car in the summer heat), but also to accelerate destabilization processes up to 200 times.

Mechanical methods of acceleration, including vibration, centrifugation, and agitation, can also be used.

These methods are almost always empirical, without a sound scientific basis.

An emulsifier (also known as an "emulgent") is a substance that stabilizes an emulsion by increasing its kinetic stability. One class of emulsifiers is known as "surface active agents", or surfactants. Emulsifiers are compounds that typically have a polar or hydrophilic (i.e. water-soluble) part and a non-polar (i.e. hydrophobic or lipophilic) part. Because of this, emulsifiers tend to have more or less solubility either in water or in oil. Emulsifiers that are more soluble in water (and conversely, less soluble in oil) will generally form oil-in-water emulsions, while emulsifiers that are more soluble in oil will form water-in-oil emulsions.

Examples of food emulsifiers are:

Detergents are another class of surfactant, and will interact physically with both oil and water, thus stabilizing the interface between the oil and water droplets in suspension. This principle is exploited in soap, to remove grease for the purpose of cleaning. Many different emulsifiers are used in pharmacy to prepare emulsions such as creams and lotions. Common examples include emulsifying wax, cetearyl alcohol, polysorbate 20, and ceteareth 20. 

Sometimes the inner phase itself can act as an emulsifier, and the result is a nanoemulsion, where the inner state disperses into "nano-size" droplets within the outer phase. A well-known example of this phenomenon, the "Ouzo effect", happens when water is poured into a strong alcoholic anise-based beverage, such as ouzo, pastis, absinthe, arak, or raki. The anisolic compounds, which are soluble in ethanol, then form nano-size droplets and emulsify within the water. The resulting color of the drink is opaque and milky white.

"See also: explained, in the Simple English Wikipedia"

A number of different chemical and physical processes and mechanisms can be involved in the process of emulsification:


Oil-in-water emulsions are common in food products:

Water-in-oil emulsions are less common in food, but still exist:

Other foods can be turned into products similar to emulsions, for example meat emulsion is a suspension of meat in liguid that is similar to true emulsions. 

In pharmaceutics, hairstyling, personal hygiene, and cosmetics, emulsions are frequently used. These are usually oil and water emulsions but dispersed, and which is continuous depends in many cases on the pharmaceutical formulation. These emulsions may be called creams, ointments, liniments (balms), pastes, films, or liquids, depending mostly on their oil-to-water ratios, other additives, and their intended route of administration. The first 5 are topical dosage forms, and may be used on the surface of the skin, transdermally, ophthalmically, rectally, or vaginally. A highly liquid emulsion may also be used orally, or may be injected in some cases. Popular medications occurring in emulsion form include cod liver oil, Polysporin, cortisol cream, Canesten, and Fleet.

Microemulsions are used to deliver vaccines and kill microbes. Typical emulsions used in these techniques are nanoemulsions of soybean oil, with particles that are 400–600 nm in diameter. The process is not chemical, as with other types of antimicrobial treatments, but mechanical. The smaller the droplet the greater the surface tension and thus the greater the force required to merge with other lipids. The oil is emulsified with detergents using a high-shear mixer to stabilize the emulsion so, when they encounter the lipids in the cell membrane or envelope of bacteria or viruses, they force the lipids to merge with themselves. On a mass scale, in effect this disintegrates the membrane and kills the pathogen. The soybean oil emulsion does not harm normal human cells, or the cells of most other higher organisms, with the exceptions of sperm cells and blood cells, which are vulnerable to nanoemulsions due to the peculiarities of their membrane structures. For this reason, these nanoemulsions are not currently used intravenously (IV). The most effective application of this type of nanoemulsion is for the disinfection of surfaces. Some types of nanoemulsions have been shown to effectively destroy HIV-1 and tuberculosis pathogens on non-porous surfaces.

Emulsifying agents are effective at extinguishing fires on small, thin-layer spills of flammable liquids (class B fires). Such agents encapsulate the fuel in a fuel-water emulsion, thereby trapping the flammable vapors in the water phase. This emulsion is achieved by applying an aqueous surfactant solution to the fuel through a high-pressure nozzle. Emulsifiers are not effective at extinguishing large fires involving bulk/deep liquid fuels, because the amount of emulsifier agent needed for extinguishment is a function of the volume of the fuel, whereas other agents such as aqueous film-forming foam need cover only the surface of the fuel to achieve vapor mitigation.

Emulsions are used to manufacture polymer dispersions – polymer production in an emulsion 'phase' has a number of process advantages, including prevention of coagulation of product. Products produced by such polymerisations may be used as the emulsions – products including primary components for glues and paints. Synthetic latexes (rubbers) are also produced by this process.



</doc>
<doc id="10292" url="https://en.wikipedia.org/wiki?curid=10292" title="Louis Mountbatten, 1st Earl Mountbatten of Burma">
Louis Mountbatten, 1st Earl Mountbatten of Burma

Admiral of the Fleet Louis Francis Albert Victor Nicholas Mountbatten, 1st Earl Mountbatten of Burma, (born Prince Louis of Battenberg; 25 June 1900 – 27 August 1979) was a British naval officer and statesman, an uncle of Prince Philip, Duke of Edinburgh, and second cousin once removed of Queen Elizabeth II. During the Second World War, he was Supreme Allied Commander, South East Asia Command (1943–46). He was the last Viceroy of India (1947) and the first Governor-General of independent India (1947–48).

From 1954 until 1959 he was First Sea Lord, a position that had been held by his father, Prince Louis of Battenberg, some forty years earlier. Thereafter he served as Chief of the Defence Staff until 1965, making him the longest serving professional head of the British Armed Forces to date. During this period Mountbatten also served as Chairman of the NATO Military Committee for a year.

In 1979, Mountbatten, his grandson Nicholas, and two others were killed by the Provisional Irish Republican Army (IRA), which had placed a bomb in his fishing boat, "Shadow V", in Mullaghmore, County Sligo, Ireland.

From the time of his birth at Frogmore House in the Home Park, Windsor, Berkshire until 1917, when he and several other relations of King George V dropped their German styles and titles, Mountbatten was known as His Serene Highness Prince Louis of Battenberg. He was the youngest child and the second son of Prince Louis of Battenberg and his wife Princess Victoria of Hesse and by Rhine. His maternal grandparents were Louis IV, Grand Duke of Hesse, and Princess Alice of the United Kingdom, who was a daughter of Queen Victoria and Prince Albert of Saxe-Coburg and Gotha. His paternal grandparents were Prince Alexander of Hesse and by Rhine and Julia, Princess of Battenberg.

His paternal grandparents' marriage was morganatic because his grandmother was not of royal lineage; as a result, he and his father were styled "Serene Highness" rather than "Grand Ducal Highness", were not eligible to be titled Princes of Hesse and were given the less exalted Battenberg title. His siblings were Princess Alice of Battenberg (mother of Prince Philip, Duke of Edinburgh), Queen Louise of Sweden, and George Mountbatten, 2nd Marquess of Milford Haven.

Young Mountbatten's nickname among family and friends was "Dickie", although "Richard" was not among his given names. This was because his great-grandmother, Queen Victoria, had suggested the nickname of "Nicky", but to avoid confusion with the many Nickys of the Russian Imperial Family ("Nicky" was particularly used to refer to Nicholas II, the last Tsar), "Nicky" was changed to "Dickie".

He was baptised in the large drawing room of Frogmore House on 17 July 1900 by the Dean of Windsor, Philip Eliot. His godparents were Queen Victoria, Nicholas II of Russia (represented by his father) and Prince Francis Joseph of Battenberg (represented by Lord Edward Clinton).

Mountbatten was educated at home for the first 10 years of his life: he was then sent to Lockers Park School in Hertfordshire and on to the Royal Naval College, Osborne in May 1913. In childhood he visited the Imperial Court of Russia at St Petersburg and became intimate with the doomed Russian Imperial Family, harbouring romantic feelings towards his maternal first cousin Grand Duchess Maria Nikolaevna, whose photograph he kept at his bedside for the rest of his life.

Mountbatten was posted as midshipman to the battlecruiser in July 1916 and, after seeing action in August 1916, transferred to the battleship during the closing phases of the First World War. In June 1917, when the royal family stopped using their German names and titles and adopted the more British-sounding "Windsor", Prince Louis of Battenberg became Louis Mountbatten, and was created Marquess of Milford Haven. His second son acquired the courtesy title "Lord Louis Mountbatten" and was known as "Lord Louis" until he was created a peer in 1946. He paid a visit of ten days to the Western Front, in July 1918.

He was appointed executive officer (second-in-command) of the small warship HMS P.31 on 13 October 1918 and was promoted sub-lieutenant on 15 January 1919. HMS P.31 took part in the Peace River Pageant on 4 April 1919. Mountbatten attended Christ's College, Cambridge for two terms, starting in October 1919, where he studied English literature (including John Milton and Lord Byron) in a programme that was specially designed for ex-servicemen. He was elected for a term to the Standing Committee of the Cambridge Union Society, and was suspected of sympathy for the Labour Party, then emerging as a potential party of government for the first time.

He was posted to the battlecruiser in March 1920 and accompanied Edward, Prince of Wales, on a royal tour of Australia in her. He was promoted lieutenant on 15 April 1920. HMS "Renown" returned to Portsmouth on 11 October 1920. Early in 1921 Royal Navy personnel were used for civil defence duties as serious industrial unrest seemed imminent. Mountbatten had to command a platoon of stokers, many of whom had never handled a rifle before, in northern England. He transferred to the battlecruiser in March 1921 and accompanied the Prince of Wales on a Royal tour of India and Japan. Edward and Mountbatten formed a close friendship during the trip. Mountbatten survived the deep defence cuts known as the Geddes Axe. Fifty-two percent of the officers of his year had had to leave the Royal Navy by the end of 1923; although he was highly regarded by his superiors, it was rumoured that wealthy and well-connected officers were more likely to be retained. He was posted to the battleship in the Mediterranean Fleet in January 1923.

Pursuing his interests in technological development and gadgetry, Mountbatten joined the Portsmouth Signals School in August 1924 and then went on briefly to study electronics at the Royal Naval College, Greenwich. Mountbatten became a Member of the Institution of Electrical Engineers (IEE), now the Institution of Engineering and Technology (IET), which annually awards the Mountbatten Medal for an outstanding contribution, or contributions over a period, to the promotion of electronics or information technology and their application. He was posted to the battleship in the Reserve Fleet in 1926 and became Assistant Fleet Wireless and Signals Officer of the Mediterranean Fleet under the command of Admiral Sir Roger Keyes in January 1927. Promoted lieutenant-commander on 15 April 1928, he returned to the Signals School in July 1929 as Senior Wireless Instructor. He was appointed Fleet Wireless Officer to the Mediterranean Fleet in August 1931, and having been promoted commander on 31 December 1932, was posted to the battleship .

In 1934, Mountbatten was appointed to his first command – the destroyer . His ship was a new destroyer, which he was to sail to Singapore and exchange for an older ship, . He successfully brought "Wishart" back to port in Malta and then attended the funeral of King George V in January 1936. Mountbatten was appointed a Personal Naval Aide-de-Camp to King Edward VIII on 23 June 1936, and, having joined the Naval Air Division of the Admiralty in July 1936, he attended the coronation of King George VI and Queen Elizabeth in May 1937. He was promoted Captain on 30 June 1937 and was then given command of the destroyer in June 1939.

In July 1939, Mountbatten was granted a patent (UK Number 508,956) for a system for maintaining a warship in a fixed position relative to another ship.

When war broke out in September 1939, Mountbatten became commander of the 5th Destroyer Flotilla aboard HMS "Kelly", which became famous for its exploits. In late 1939 he brought the Duke of Windsor back from exile in France and in early May 1940, Mountbatten led a British convoy in through the fog to evacuate the Allied forces participating in the Namsos Campaign during the Norwegian Campaign.

On the night of 9/10 May 1940, "Kelly" was torpedoed amidships by a German E-boat "S 31" off the Dutch coast, and Mountbatten thereafter commanded the 5th Destroyer Flotilla from the destroyer . He rejoined "Kelly" in December 1940, by which time the torpedo damage had been repaired.

"Kelly" was sunk by German dive bombers on 23 May 1941 during the Battle of Crete; the incident serving as the basis for Noël Coward's film "In Which We Serve". Coward was a personal friend of Mountbatten and copied some of his speeches into the film. Mountbatten was mentioned in despatches on 9 August 1940 and 21 March 1941 and awarded the Distinguished Service Order in January 1941.

In August 1941, Mountbatten was appointed captain of the aircraft carrier which lay in Norfolk, Virginia, for repairs following action at Malta in the Mediterranean in January. During this period of relative inactivity, he paid a flying visit to Pearl Harbor, three months before the Japanese attack on the US naval base there. Mountbatten, appalled at the base's lack of preparedness, drawing on Japan's history of launching wars with surprise attacks as well as the successful British surprise attack at the Battle of Taranto which had effectively knocked Italy's fleet out of the war, and the sheer effectiveness of aircraft against warships, accurately predicted that the US entry into the war would begin with a Japanese surprise attack on Pearl Harbor.

Mountbatten was a favourite of Winston Churchill. On 27 October 1941 Mountbatten replaced Roger Keyes as Chief of Combined Operations and promoted commodore.

His duties in this role included inventing new technical aids to assist with opposed landings. Noteworthy technical achievements of Mountbatten and his staff include the construction of "PLUTO", an underwater oil pipeline from the English coast to Normandy, an artificial harbour constructed of concrete caissons and sunken ships, and the development of amphibious tank-landing ships. Another project that Mountbatten proposed to Churchill was Project Habakkuk. It was to be a massive and impregnable 600-metre aircraft carrier made from reinforced ice ("Pykrete"): Habakkuk was never carried out due to its enormous cost.

As commander of Combined Operations, Mountbatten and his staff planned the highly successful Bruneval raid, which gained important information and also captured part of a German Würzburg radar installation and one of the machine's technicians on 27 February 1942. It was Mountbatten who recognized that surprise and speed were essential to ensure the radar was captured, and saw that an airborne assault was the only viable method. He was in large part responsible for the planning and organisation of The Raid at St. Nazaire in mid-1942, an operation which put out of action one of the most heavily defended docks in Nazi-occupied France until well after war's end, the ramifications of which contributed to allied supremacy in the Battle of the Atlantic. After these two successes came the Dieppe Raid of 19 August 1942. He was central in the planning and promotion of the raid on the port of Dieppe. The raid was a marked failure, with casualties of almost 60%, the great majority of them Canadians. Following the Dieppe raid Mountbatten became a controversial figure in Canada, with the Royal Canadian Legion distancing itself from him during his visits there during his later career. His relations with Canadian veterans, who blamed him for the losses, "remained frosty" after the war.

Mountbatten claimed that the lessons learned from the Dieppe Raid were necessary for planning the Normandy invasion on D-Day nearly two years later. However, military historians such as former Royal Marine Julian Thompson have written that these lessons should not have needed a debacle such as Dieppe to be recognised. Nevertheless, as a direct result of the failings of the Dieppe raid, the British made several innovations, most notably Hobart's Funnies – specialized armoured vehicles which, in the course of the Normandy Landings, undoubtedly saved many lives on those three beachheads upon which Commonwealth soldiers were landing (Gold Beach, Juno Beach, and Sword Beach).

In August 1943, Churchill appointed Mountbatten the Supreme Allied Commander South East Asia Command (SEAC) with promotion to acting full admiral. His less practical ideas were sidelined by an experienced planning staff led by Lieutenant-Colonel James Allason, though some, such as a proposal to launch an amphibious assault near Rangoon, got as far as Churchill before being quashed.

British interpreter Hugh Lunghi recounted an embarrassing episode which occurred during the Potsdam Conference, when Mountbatten, desiring to receive an invitation to visit the Soviet Union, repeatedly attempted to impress Josef Stalin with his former connections to the Russian imperial family. The attempt fell predictably flat, with Stalin dryly inquiring whether "it was some time ago that he had been there." Says Lunghi, "The meeting was embarrassing because Stalin was so unimpressed. He offered no invitation. Mountbatten left with his tail between his legs."

During his time as Supreme Allied Commander of the Southeast Asia Theatre, his command oversaw the recapture of Burma from the Japanese by General William Slim. A personal high point was the receipt of the Japanese surrender in Singapore when British troops returned to the island to receive the formal surrender of Japanese forces in the region led by General Itagaki Seishiro on 12 September 1945, codenamed Operation Tiderace. South East Asia Command was disbanded in May 1946 and Mountbatten returned home with the substantive rank of rear-admiral.

Following the war, Mountbatten was known to have largely shunned the Japanese for the rest of his life out of respect for his men killed during the war, and as per his will, Japan was not invited to send diplomatic representatives to his funeral in 1979, though he did meet Emperor Hirohito during a state visit to Britain in 1971, reportedly at the urging of the Queen.

His experience in the region and in particular his perceived Labour sympathies at that time led to Clement Attlee appointing him Viceroy of India on 20 February 1947 charged with overseeing the transition of British India to independence no later than 30 June 1948. Mountbatten's instructions emphasised a united India as a result of the transference of power but authorised him to adapt to a changing situation in order to get Britain out promptly with minimal reputational damage. Soon after he arrived, Mountbatten concluded that the situation was too volatile for even that short a wait. Although his advisers favoured a gradual transfer of independence, Mountbatten decided the only way forward was a quick and orderly transfer of independence before 1947 was out. In his view, any longer would mean civil war. The Viceroy also hurried so he could return to his senior technical Navy courses.
Mountbatten was fond of Congress leader Jawaharlal Nehru and his liberal outlook for the country. He felt differently about the Muslim leader Muhammed Ali Jinnah, but was aware of his power, stating "If it could be said that any single man held the future of India in the palm of his hand in 1947, that man was Mohammad Ali Jinnah." During his meeting with Jinnah on 5 April 1947, Mountbatten tried to persuade Jinnah of a united India, citing the difficult task of dividing the mixed states of Punjab and Bengal, but the Muslim leader was unyielding in his goal of establishing a separate Muslim state called Pakistan.

Given the British government's recommendations to grant independence quickly, Mountbatten concluded that a united India was an unachievable goal and resigned himself to a plan for partition, creating the independent nations of India and Pakistan. Mountbatten set a date for the transfer of power from the British to the Indians, arguing that a fixed timeline would convince Indians of his and the British government's sincerity in working towards a swift and efficient independence, excluding all possibilities of stalling the process.

Among the Indian leaders, Mahatma Gandhi emphatically insisted on maintaining a united India and for a while successfully rallied people to this goal. During his meeting with Mountbatten, Gandhi asked Mountbatten to invite Jinnah to form a new Central government, but Mountbatten never uttered a word of Gandhi's ideas to Jinnah. And when Mountbatten's timeline offered the prospect of attaining independence soon, sentiments took a different turn. Given Mountbatten's determination, Nehru and Patel's inability to deal with the Muslim League and lastly Jinnah's obstinacy, all Indian party leaders (except Gandhi) acquiesced to Jinnah's plan to divide India, which in turn eased Mountbatten's task. Mountbatten also developed a strong relationship with the Indian princes, who ruled those portions of India not directly under British rule. His intervention was decisive in persuading the vast majority of them to see advantages in opting to join the Indian Union. On one hand, the integration of the princely states can be viewed as one of the positive aspects of his legacy. But on the other, the refusal of Hyderabad, Jammu and Kashmir, and Junagadh to join one of the dominions led to future tension between Pakistan and India.

Mountbatten brought forward the date of the partition from June 1948 to 15 August 1947. The uncertainty of the borders caused Muslims and Hindus to move into the direction where they felt they would get the majority. Hindus and Muslims were thoroughly terrified, and the Muslim movement from the East was balanced by the similar movement of Hindus from the West. A boundary committee chaired by Sir Cyril Radcliffe was charged with drawing boundaries for the new nations. With a mandate to leave as many Hindus and Sikhs in India and as many Muslims in Pakistan as possible, Radcliffe came up with a map that split the two countries along the Punjab and Bengal borders. This left 14 million people on the "wrong" side of the border, and very many of them fled to "safety" on the other side when the new lines were announced.
When India and Pakistan attained independence at midnight on the night of 14–15 August 1947, Mountbatten remained in New Delhi for 10 months, serving as India's first governor general until June 1948. On Mountbatten's advice, India took the issue of Kashmir to the newly formed United Nations in January 1948. The issue of Kashmir would become a lasting thorn in his legacy, one that is not resolved to this day. Accounts differ on the future Mountbatten desired for Kashmir. Pakistani accounts suggest that Mountbatten favored the accession of Kashmir to India citing his close relationship to Nehru. Mountbatten's own account says that he simply wanted the maharaja Hari Singh to make up his mind. The viceroy made several attempts to mediate between the Congress leaders, Muhammad Ali Jinnah and Hari Singh on issues relating to the accession of Kashmir though he was largely unsuccessful in resolving the conflict. After the tribal invasion of Kashmir, it was on his suggestion that India moved to secure the accession of Kashmir from Hari Singh before sending in military forces for his defence.

Notwithstanding the self-promotion of his own part in Indian independence – notably in the television series "The Life and Times of Admiral of the Fleet Lord Mountbatten of Burma", produced by his son-in-law Lord Brabourne, and "Freedom at Midnight" by Dominique Lapierre and Larry Collins (of which he was the main quoted source) – his record is seen as very mixed; one common view is that he hastened the independence process unduly and recklessly, foreseeing vast disruption and loss of life and not wanting this to occur on the British watch, but thereby actually helping it to occur, especially in Punjab and Bengal. John Kenneth Galbraith, the Canadian-American Harvard University economist, who advised governments of India during the 1950s, an intimate of Nehru who served as the American ambassador from 1961 to 1963, was a particularly harsh critic of Mountbatten in this regard.

The creation of Pakistan was never emotionally accepted by many British leaders, among them being Mountbatten. Mountbatten clearly expressed his lack of support and faith in the Muslim League's idea of Pakistan. Jinnah refused Mountbatten's offer to serve as Governor-General of Pakistan. When Mountbatten was asked by Collins and Lapierre if he would have sabotaged Pakistan had he known that Jinnah was dying of tuberculosis, he replied, "Most probably."

After India, Mountbatten served as commander of the 1st Cruiser Squadron in the Mediterranean Fleet and, having been granted the substantive rank of vice-admiral on 22 June 1949, he became Second-in-Command of the Mediterranean Fleet in April 1950. He became Fourth Sea Lord at the Admiralty in June 1950. He then returned to the Mediterranean to serve as Commander-in-Chief, Mediterranean Fleet and NATO Commander Allied Forces Mediterranean from June 1952. He was promoted to the substantive rank of full admiral on 27 February 1953. In March 1953, he was appointed Personal Aide-de-Camp to the Queen.

Mountbatten served his final posting at the Admiralty as First Sea Lord and Chief of the Naval Staff from April 1955 to July 1959, the position which his father had held some forty years prior. This was the first time in Royal Naval history that a father and son had both attained such high rank. He was promoted to Admiral of the Fleet on 22 October 1956.

While serving as First Sea Lord, his primary concerns dealt with devising plans on how the Royal Navy would keep shipping lanes open if Britain fell victim to a nuclear attack. Today, this seems of minor importance but at the time few people comprehended the potentially limitless destruction nuclear weapons possess and the ongoing dangers posed by the fallout. Military commanders did not understand the physics involved in a nuclear explosion. This became evident when Mountbatten had to be reassured that the fission reactions from the Bikini Atoll tests would not spread through the oceans and blow up the planet. As Mountbatten became more familiar with this new form of weaponry, he increasingly grew opposed to its use in combat yet at the same time he realised the potential nuclear energy had, especially with regards to submarines. Mountbatten expressed his feelings towards the use of nuclear weapons in combat in his article "A Military Commander Surveys The Nuclear Arms Race", which was published shortly after his death in "International Security" in the winter of 1979–80.
After leaving the Admiralty, Lord Mountbatten took the position of Chief of the Defence Staff. He served in this post for six years during which he was able to consolidate the three service departments of the military branch into a single Ministry of Defence.
Mountbatten was appointed Colonel of the Life Guards, Gold Stick in Waiting and Life Colonel Commandant of the Royal Marines in 1965. He was Governor of the Isle of Wight from 20 July 1965 and then the first Lord Lieutenant of the Isle of Wight from 1 April 1974.

Mountbatten was elected a Fellow of the Royal Society and had received an Honorary Doctorate from Heriot-Watt University in 1968.

In 1969, Mountbatten tried unsuccessfully to persuade his cousin, the Spanish pretender Infante Juan, Count of Barcelona, to ease the eventual accession of his son, Juan Carlos, to the Spanish throne by signing a declaration of abdication while in exile. The next year Mountbatten attended an official White House dinner during which he took the opportunity to have a 20-minute conversation with Richard Nixon and Secretary of State William P. Rogers, about which he later wrote, "I was able to talk to the President a bit about both Tino [Constantine II of Greece] and Juanito [Juan Carlos of Spain] to try and put over their respective points of view about Greece and Spain, and how I felt the US could help them." In January 1971, Nixon hosted Juan Carlos and his wife Sofia (sister of the exiled King Constantine) during a visit to Washington and later that year the "Washington Post" published an article alleging that Nixon's administration was seeking to get Franco to retire in favour of the young Bourbon prince.

From 1967 until 1978, Mountbatten was president of the United World Colleges Organisation, then represented by a single college: that of Atlantic College in South Wales. Mountbatten supported the United World Colleges and encouraged heads of state, politicians and personalities throughout the world to share his interest. Under Mountbatten's presidency and personal involvement, the United World College of South East Asia was established in Singapore in 1971, followed by the United World College of the Pacific (now known as the Lester B Pearson United World College of the Pacific) in Victoria, British Columbia, in 1974. In 1978, Mountbatten passed the presidency of the college to his great-nephew, the Prince of Wales.

Peter Wright, in his book "Spycatcher", claimed that in May 1968 Mountbatten attended a private meeting with press baron Cecil King, and the Government's Chief Scientific Adviser, Solly Zuckerman. Wright alleged that "up to thirty" MI5 officers had joined a secret campaign to undermine the crisis-stricken Labour government of Harold Wilson and that King was an MI5 agent. In the meeting, King allegedly urged Mountbatten to become the leader of a government of national salvation. Solly Zuckerman pointed out that it was "rank treachery", and the idea came to nothing because of Mountbatten's reluctance to act.

In 2006, the BBC documentary "The Plot Against Harold Wilson" alleged that there had been another plot involving Mountbatten to oust Wilson during his second term in office (1974–76). The period was characterised by high inflation, increasing unemployment and widespread industrial unrest. The alleged plot revolved around right-wing former military figures who were supposedly building private armies to counter the perceived threat from trade unions and the Soviet Union. They believed that the Labour Party, which was (and ) partly funded by affiliated trade unions, was unable and unwilling to counter these developments and that Wilson was either a Soviet agent or at the very least a Communist sympathiser – claims Wilson strongly denied. The documentary alleged that a coup was planned to overthrow Wilson and replace him with Mountbatten using the private armies and sympathisers in the military and MI5.

The first official history of MI5, "The Defence of the Realm" (2009), tacitly confirmed that there was a plot against Wilson and that MI5 did have a file on him. Yet it also made clear that the plot was in no way official and that any activity centred on a small group of discontented officers. This much had already been confirmed by former cabinet secretary Lord Hunt, who concluded in a secret inquiry conducted in 1996 that "there is absolutely no doubt at all that a few, a very few, malcontents in MI5...a lot of them like Peter Wright who were right-wing, malicious and had serious personal grudges – gave vent to these and spread damaging malicious stories about that Labour government."

Mountbatten was married on 18 July 1922 to Edwina Cynthia Annette Ashley, daughter of Wilfred William Ashley, later 1st Baron Mount Temple, himself a grandson of the 7th Earl of Shaftesbury. She was the favourite granddaughter of the Edwardian magnate Sir Ernest Cassel and the principal heir to his fortune. There followed a honeymoon tour of European royal courts and America which included a visit to Niagara Falls (because "all honeymooners went there").

Mountbatten admitted "Edwina and I spent all our married lives getting into other people's beds." He maintained an affair for several years with Yola Letellier, the wife of Henri Letellier, publisher of "Le Journal" and mayor of Deauville (1925–28). Yola Letellier's life story was the inspiration for Colette's novel "Gigi". Edwina and Jawaharlal Nehru became intimate friends after Indian Independence. During the summers, she would frequent the prime minister's house so she could lounge about on his veranda during the hot Delhi days. Personal correspondence between the two reveals a satisfying yet frustrating relationship. Edwina states in one of her letters. "Nothing that we did or felt would ever be allowed to come between you and your work or me and mine – because that would spoil everything."

Lord and Lady Mountbatten had two daughters: Patricia Knatchbull, 2nd Countess Mountbatten of Burma (born 14 February 1924, died 13 June 2017), sometime lady-in-waiting to Queen Elizabeth II, and Lady Pamela Hicks (born 19 April 1929), who accompanied them to India in 1947–48 and was also sometime lady-in-waiting to the Queen.

Since Mountbatten had no sons, when he was created Viscount Mountbatten of Burma, of Romsey in the County of Southampton on 27 August 1946 and then Earl Mountbatten of Burma and Baron Romsey, in the County of Southampton on 28 October 1947, the Letters Patent were drafted such that in the event he left no sons or issue in the male line, the titles could pass to his daughters, in order of seniority of birth, and to their male heirs respectively.

Like many members of the royal family, Mountbatten was an aficionado of polo. He received U.S. patent 1,993,334 in 1931 for a polo stick. Mountbatten introduced the sport to the Royal Navy in the 1920s, and wrote a book on the subject.
He also served as Commodore of Emsworth Sailing Club in Hampshire from 1931.

Mountbatten was a strong influence in the upbringing of his grand-nephew, Charles, Prince of Wales, and later as a mentor – "Honorary Grandfather" and "Honorary Grandson", they fondly called each other according to the Jonathan Dimbleby biography of the Prince – though according to both the Ziegler biography of Mountbatten and the Dimbleby biography of the Prince, the results may have been mixed. He from time to time strongly upbraided the Prince for showing tendencies towards the idle pleasure-seeking dilettantism of his predecessor as Prince of Wales, King Edward VIII, whom Mountbatten had known well in their youth. Yet he also encouraged the Prince to enjoy the bachelor life while he could and then to marry a young and inexperienced girl so as to ensure a stable married life.

Mountbatten's qualification for offering advice to this particular heir to the throne was unique; it was he who had arranged the visit of King George VI and Queen Elizabeth to Dartmouth Royal Naval College on 22 July 1939, taking care to include the young Princesses Elizabeth and Margaret in the invitation, but assigning his nephew, Cadet Prince Philip of Greece, to keep them amused while their parents toured the facility. This was the first recorded meeting of Charles's future parents. But a few months later, Mountbatten's efforts nearly came to naught when he received a letter from his sister Alice in Athens informing him that Philip was visiting her and had agreed to permanently repatriate to Greece. Within days, Philip received a command from his cousin and sovereign, King George II of Greece, to resume his naval career in Britain which, though given without explanation, the young prince obeyed.

In 1974, Mountbatten began corresponding with Charles about a potential marriage to his granddaughter, Hon. Amanda Knatchbull. It was about this time he also recommended that the 25-year-old prince get on with "sowing some wild oats".
Charles dutifully wrote to Amanda's mother (who was also his godmother), Lady Brabourne, about his interest. Her answer was supportive, but advised him that she thought her daughter still rather young to be courted.

In February 1975, Charles visited New Delhi to play polo and was shown around Rashtrapati Bhavan, the former Viceroy's House, by Mountbatten.

Four years later Mountbatten secured an invitation for himself and Amanda to accompany Charles on his planned 1980 tour of India. Their fathers promptly objected. Prince Philip thought that the Indian public's reception would more likely reflect response to the uncle than to the nephew. Lord Brabourne counselled that the intense scrutiny of the press would be more likely to drive Mountbatten's godson and granddaughter apart than together.

Charles was rescheduled to tour India alone, but Mountbatten did not live to the planned date of departure. When Charles finally did propose marriage to Amanda later in 1979, the circumstances were changed, and she refused him.

In 1969 Mountbatten participated in a 12-part autobiographical television series "Lord Mountbatten: A Man for the Century", also known as "The Life and Times of Lord Mountbatten", produced by Associated-Rediffusion and scripted by historian John Terraine. The episodes were:

1. The King's Ships Were at Sea (1900–1917)<br>
2. The Kings Depart (1917–1922)<br>
3. Azure Main (1922–1936)<br>
4. The Stormy Winds (1936–1941)<br>

5. United We Conquer (1941–1943)<br>
6. The Imperial Enemy<br>
7. The March to Victory<br>
8. The Meaning of Victory (1945–1947)<br>

9. The Last Viceroy<br>
10. Fresh Fields (1947–1955)<br>
11. Full Circle (1955–1965)<br>
12. A Man of This Century (1900–1968)
On 27 April 1977, shortly before his 77th birthday, Mountbatten became the first member of the Royal Family to appear on the TV guest show "This Is Your Life".

Mountbatten usually holidayed at his summer home, Classiebawn Castle, in Mullaghmore, a small seaside village in County Sligo, Ireland. The village was only from the border with Northern Ireland and near an area known to be used as a cross-border refuge by IRA members. In 1978, the IRA had allegedly attempted to shoot Mountbatten as he was aboard his boat, but "choppy seas had prevented the sniper lining up his target".

On 27 August 1979, Mountbatten went lobster-potting and tuna fishing in his wooden boat, "Shadow V", which had been moored in the harbour at Mullaghmore. IRA member Thomas McMahon had slipped onto the unguarded boat that night and attached a radio-controlled bomb weighing . When Mountbatten was aboard, just a few hundred yards from the shore, the bomb was detonated. The boat was destroyed by the force of the blast, and Mountbatten's legs were almost blown off. Mountbatten, then aged 79, was pulled alive from the water by nearby fishermen, but died from his injuries before being brought to shore. Also aboard the boat were his elder daughter Patricia (Lady Brabourne), her husband John (Lord Brabourne), their twin sons Nicholas and Timothy Knatchbull, John's mother Doreen, (dowager) Lady Brabourne, and Paul Maxwell, a young crew member from County Fermanagh. Nicholas (aged 14) and Paul (aged 15) were killed by the blast and the others were seriously injured. Doreen, Lady Brabourne (aged 83) died from her injuries the following day.

The IRA issued a statement afterward, saying:

The IRA claim responsibility for the execution of Lord Louis Mountbatten. This operation is one of the discriminate ways we can bring to the attention of the English people the continuing occupation of our country. [...] The death of Mountbatten and the tributes paid to him will be seen in sharp contrast to the apathy of the British government and the English people to the deaths of over three hundred British soldiers, and the deaths of Irish men, women and children at the hands of their forces.

Six weeks later, Sinn Féin vice-president Gerry Adams said of Mountbatten's death:

The IRA gave clear reasons for the execution. I think it is unfortunate that anyone has to be killed, but the furor created by Mountbatten's death showed up the hypocritical attitude of the media establishment. As a member of the House of Lords, Mountbatten was an emotional figure in both British and Irish politics. What the IRA did to him is what Mountbatten had been doing all his life to other people; and with his war record I don't think he could have objected to dying in what was clearly a war situation. He knew the danger involved in coming to this country. In my opinion, the IRA achieved its objective: people started paying attention to what was happening in Ireland.

In May 2015, during a meeting with Prince Charles, Adams did not apologize. He later said in an interview, "I stand over what I said then. I'm not one of those people that engages in revisionism. Thankfully the war is over".

On the day of the bombing, the IRA also ambushed and killed eighteen British soldiers in Northern Ireland, sixteen of them from the Parachute Regiment, in what became known as the Warrenpoint ambush. It was the deadliest attack on the British Army during the Troubles.

On 5 September 1979 Mountbatten received a ceremonial funeral at Westminster Abbey, which was attended by the Queen, the Royal Family and members of the European royal houses. Watched by thousands of people, the funeral procession, which started at Wellington Barracks, included representatives of all three British Armed Services, and military contingents from Burma, India, the United States, France and Canada. His coffin was drawn on a gun carriage by 118 Royal Navy ratings. During the televised service, the Prince of Wales read the lesson from Psalm 107. In an address, the Archbishop of Canterbury, Donald Coggan, highlighted his various achievements and his "lifelong devotion to the Royal Navy". After the public ceremonies, which he had planned himself, Mountbatten was buried in Romsey Abbey. As part of the funeral arrangements, his body had been embalmed by Desmond Henley.

Thomas McMahon, who had been arrested two hours before the bomb detonated at a Garda checkpoint between Longford and Granard on suspicion of driving a stolen vehicle, was tried for the assassinations in Ireland, and convicted on 23 November 1979 by forensic evidence supplied by James O'Donovan that showed flecks of paint from the boat and traces of nitroglycerine on his clothes. He was released in 1998 under the terms of the Good Friday Agreement.

On hearing of Mountbatten's death the then Master of the Queen's Music, Malcolm Williamson, was moved to write the "Lament in Memory of Lord Mountbatten of Burma" for violin and string orchestra. The 11-minute work was given its first performance on 5 May 1980 by the Scottish Baroque Ensemble, conducted by Leonard Friedman.

Mountbatten took pride in enhancing intercultural understanding and in 1984, with his elder daughter as the patron, the Mountbatten Institute was developed to allow young adults the opportunity to enhance their intercultural appreciation and experience by spending time abroad.

The city of Ottawa, Ontario, erected Mountbatten Avenue in his memory. The avenue runs from Blossom Drive to Fairbanks Avenue.




</doc>
<doc id="10293" url="https://en.wikipedia.org/wiki?curid=10293" title="Elbridge Gerry">
Elbridge Gerry

Elbridge Gerry (; July 17, 1744 (O.S. July 6, 1744) – November 23, 1814) was an American statesman and diplomat. As a Democratic-Republican he served as the fifth Vice President of the United States from March 1813 until his death in November 1814. He is known best for being the namesake of gerrymandering, a process by which electoral districts are drawn with the aim of aiding the party in power, although its initial "g" has recently softened to from the hard of his name.

Born into a wealthy merchant family, Gerry vocally opposed British colonial policy in the 1760s, and was active in the early stages of organizing the resistance in the American Revolutionary War. Elected to the Second Continental Congress, Gerry signed both the Declaration of Independence and the Articles of Confederation. He was one of three men who attended the Constitutional Convention in 1787 who refused to sign the United States Constitution because it did not then include a Bill of Rights. After its ratification he was elected to the inaugural United States Congress, where he was actively involved in drafting and passage of the Bill of Rights as an advocate of individual and state liberties.

Gerry was at first opposed to the idea of political parties, and cultivated enduring friendships on both sides of the political divide between Federalists and Democratic-Republicans. He was a member of a diplomatic delegation to France that was treated poorly in the XYZ Affair, in which Federalists held him responsible for a breakdown in negotiations. Gerry thereafter became a Democratic-Republican, running unsuccessfully for Governor of Massachusetts several times before winning the office in 1810. During his second term, the legislature approved new state senate districts that led to the coining of the word "gerrymander"; he lost the next election, although the state senate remained Democratic-Republican. Chosen by Madison as his vice presidential candidate in 1812, Gerry was elected, but died a year and a half into his term. He is the only signer of the Declaration of Independence who is buried in Washington, D.C.

Elbridge Gerry was born on July 17, 1744, in Marblehead, Massachusetts. His father, Thomas Gerry, was a merchant operating ships out of Marblehead, and his mother, Elizabeth (Greenleaf) Gerry, was the daughter of a successful Boston merchant. Gerry's first name came from John Elbridge, one of his mother's ancestors. Gerry's parents had eleven children in all, although only five survived to adulthood. Of these, Elbridge was the third. He was first educated by private tutors, and entered Harvard College shortly before turning fourteen. After receiving a B.A. in 1762 and an M.A. in 1765, he entered his father's merchant business. By the 1770s the Gerrys numbered among the wealthiest Massachusetts merchants, with trading connections in Spain, the West Indies, and along the North American coast. Gerry's father, who had emigrated from England in 1730, was active in local politics and had a leading role in the local militia.

Gerry was from an early time a vocal opponent of Parliamentary efforts to tax the colonies after the French and Indian War ended in 1763. In 1770 he sat on a Marblehead committee that sought to enforce importation bans on taxed British goods. He frequently communicated with other Massachusetts opponents of British policy, including Samuel Adams, John Adams, Mercy Otis Warren, and others.

In May 1772 he won election to the Great and General Court of the Province of Massachusetts Bay (its legislative assembly). There he worked closely with Samuel Adams to advance colonial opposition to Parliamentary colonial policies. He was responsible for establishing Marblehead's committee of correspondence, one of the first to be set up after that of Boston. However, an incident of mob action prompted him to resign from the committee the next year. Gerry and other prominent Marbleheaders had established a hospital for performing smallpox inoculations on Cat Island; because the means of transmission of the disease were not known at the time, fears amongst the local population led to protests which escalated into violence that wrecked the facilities and threatened the proprietors' other properties.

Gerry reentered politics after the Boston Port Act closed that city's port in 1774, and Marblehead became a port to which relief supplies from other colonies could be delivered. As one of the town's leading merchants and Patriots, Gerry played a major role in ensuring the storage and delivery of supplies from Marblehead to Boston, interrupting those activities only to care for his dying father. He was elected as a representative to the First Continental Congress in September 1774, but refused, still grieving the loss of his father.

Gerry was elected to the provincial assembly, which reconstituted itself as the Massachusetts Provincial Congress after Governor Thomas Gage dissolved the body in October 1774. He was assigned to its committee of safety, responsible for assuring that the province's limited supplies of weapons and gunpowder remained out of British Army hands. His actions were partly responsible for the storage of weapons and ammunition in Concord; these stores were the target of the British raiding expedition that sparked the start of the American Revolutionary War with the Battles of Lexington and Concord in April 1775. (Gerry was staying at an inn at Menotomy, now Arlington, when the British marched through on the night of April 18.) During the Siege of Boston that followed, Gerry continued to take a leading role in supplying the nascent Continental Army, something he would continue to do as the war progressed. He leveraged business contacts in France and Spain to acquire not just munitions, but supplies of all types, and was involved in the transfer of financial subsidies from Spain to Congress. He sent ships to ports all along the American coast, and dabbled in financing privateering operations.
Unlike some merchants, there is no evidence that Gerry profiteered from this activity (he spoke out against it, and in favor of price controls), although his war-related merchant activities notably increased the family's wealth. His gains were tempered to some extent by the precipitous decline in the value of paper currencies, which he held in large quantities and speculated in.

Gerry served in the Second Continental Congress from February 1776 to 1780, when matters of the ongoing war occupied the body's attention. He was influential in convincing a number of delegates to support passage of the United States Declaration of Independence in the debates held during the summer of 1776; John Adams wrote of him, "If every Man here was a Gerry, the Liberties of America would be safe against the Gates of Earth and Hell." He was implicated as a member of the so-called "Conway Cabal", a group of Congressmen and military officers who were dissatisfied with the performance of General George Washington during the 1777 military campaign. However, Gerry took Pennsylvania leader Thomas Mifflin, one of Washington's critics, to task early in the episode, and specifically denied knowledge of any sort of conspiracy against Washington in February 1778.

Gerry's political philosophy was one of limited central government, and he regularly advocated for the maintenance of civilian control of the military. He held these positions fairly consistently throughout his political career (wavering principally on the need for stronger central government in the wake of the 1786–87 Shays's Rebellion) and was well known for his personal integrity. In later years he was against the idea of political parties, remaining somewhat distant from the developing Federalist and Democratic-Republican parties until later in his career. It was not until 1800 that he would formally associate with the Democratic-Republicans in opposition to what he saw as attempts by the Federalists to centralize too much power in the national government. In 1780 he resigned from the Continental Congress over the issue, and refused offers from the state legislature to return to the Congress. He also refused appointment to the state senate, claiming he would be more effective in the state's lower chamber, and also refused appointment as a county judge, comparing the offer by Governor John Hancock to those made by royally appointed governors to benefit their political allies. He was elected a Fellow of the American Academy of Arts and Sciences in 1781.

Gerry was convinced to rejoin the Confederation Congress in 1783, when the state legislature agreed to support his call for needed reforms. He served in that body until September 1785, during which time it met in New York City. The following year he married Ann Thompson, the daughter of a wealthy New York merchant who was twenty years his junior; his best man was his good friend James Monroe. The couple had ten children between 1787 and 1801, straining Ann's health.

The war made Gerry sufficiently wealthy that when it ended he sold off his merchant interests, and began investing in land. In 1787 he purchased the Cambridge, Massachusetts estate of the last royal lieutenant governor of Massachusetts, Thomas Oliver, which had been confiscated by the state. This property, known as Elmwood, became the family home for the rest of Gerry's life. He continued to own property in Marblehead, and bought a number of properties in other Massachusetts communities. He also owned shares in the Ohio Company, prompting some political opponents to characterize him as an owner of vast tracts of western lands.

Gerry played a major role in the U.S. Constitutional Convention, held in Philadelphia during the summer of 1787. In its deliberations he consistently advocated for a strong delineation between state and federal government powers, with state legislatures shaping the membership of federal government positions. Gerry's opposition to popular election of representatives was rooted in part by the events of Shays's Rebellion, a populist uprising in western Massachusetts in the year preceding the convention. Despite this position, he also sought to maintain individual liberties by providing checks on government power that might abuse or limit those freedoms. He supported the idea that the Senate composition should not be determined by population; the view that it should instead be composed of equal numbers of members for each state prevailed in the Connecticut Compromise. The compromise was adopted on a narrow vote in which the Massachusetts delegation was divided, Gerry and Caleb Strong voting in favor. Gerry further proposed that senators of a state, rather than casting a single vote on behalf of the state, instead vote as individuals. Gerry was also vocal in opposing the Three-Fifths Compromise, which counted slaves as 3/5 of a person for the purposes of apportionment in the House of Representatives and gave southern states a decided advantage.

Because of his fear of demagoguery and belief the people of the United States could be easily misled, Gerry also advocated indirect elections. Although he was unsuccessful in obtaining them for the lower house of Congress, Gerry did obtain such indirect elections for the U.S. Senate, whose members were to be elected by the state legislatures. Gerry also advanced numerous proposals for indirect elections of the President of the United States, most of them involving limiting the right to vote to the state governors and electors.

Gerry was also unhappy about the lack of expression of any sort of individual liberties in the proposed constitution, and generally opposed proposals that strengthened the central government. He was one of only three delegates who voted against the proposed constitution in the convention (the others were George Mason and Edmund Randolph), citing a concern about the convention's lack of authority to enact such major changes to the nation's system of government, and to the constitution's lack of "federal features".

During the ratification debates that took place in the states following the convention, Gerry continued his opposition, publishing a widely circulated letter documenting his objections to the proposed constitution. In this document he cited the lack of a Bill of Rights as his primary objection, but also expressed qualified approval of the constitution, indicating that he would accept it with some amendment. Strong pro-Constitution forces attacked him in the press, comparing him unfavorably to the Shaysites. Henry Jackson was particularly vicious: "[Gerry has] done more injury to this country by that infamous Letter than he will be able to make atonement in his whole life", and Oliver Ellsworth, a convention delegate from Connecticut, charged him with deliberately courting the Shays faction. One consequence of the furor over his letter was that he was not selected as a delegate to the Massachusetts ratifying convention, although he was later invited to attend by the convention's leadership. The convention leadership was dominated by Federalists, and Gerry was not given any formal opportunity to speak; he left the convention after a shouting match with convention chair Francis Dana. The state ratified the constitution by a vote of 187 to 168. The debate had the result of estranging Gerry from a number of previously friendly politicians, including chairman Dana and Rufus King.

Anti-Federalist forces nominated Gerry for governor in 1788, but he was predictably defeated by the popular incumbent John Hancock. Following ratification, Gerry recanted his opposition to the Constitution, noting that a number of state ratifying conventions had called for amendments that he supported. He was nominated by friends (over his own opposition to the idea) for a seat in the inaugural House of Representatives, where he then served two terms.
In June 1789 Gerry proposed that Congress consider all of the proposed constitutional amendments that various state ratifying conventions had called for (notably those of Rhode Island and North Carolina, which had at the time still not ratified the constitution). In the debate that followed, he led opposition to some of the proposals, arguing that they did not go far enough in ensuring individual liberties. He successfully lobbied for inclusion of freedom of assembly in the First Amendment, and was a leading architect of the Fourth Amendment protections against search and seizure. He sought unsuccessfully to insert the word "expressly" into the Tenth Amendment, which might have more significantly limited the federal government's power. He was successful in efforts to severely limit the federal government's ability to control state militias. In tandem, with this protection, he had once argued against the idea of the federal government controlling a large standing army, comparing it – most memorably and mischievously – to a standing penis: "An excellent assurance of domestic tranquility, but a dangerous temptation to foreign adventure."

Gerry vigorously supported Alexander Hamilton's reports on public credit, including the assumption at full value of state debts, and supported Hamilton's new Bank of the United States, positions consistent with earlier calls he had made for economic centralization. Although he speculated in depreciated Continental bills of credit (the IOUs at issue), there is no evidence he participated in large-scale speculation that attended the debate when it took place in 1790, and he became a major investor in the new bank. He used the floor of the House to speak out against aristocratic and monarchical tendencies he saw as threats to republican ideals, and generally opposed laws and their provisions that he perceived as limiting individual and state liberties. He opposed any attempt to give officers of the executive significant powers, specifically opposing establishment of the Treasury Department because its head might gain more power than the President. He opposed measures that strengthened the Presidency (such as the ability to fire cabinet officers), seeking instead to give the legislature more power over appointments.

Gerry did not stand for re-election in 1792, returning home to raise his children and care for his sickly wife. He agreed to serve as a presidential elector for John Adams in the 1796 election. During Adams' term in office, Gerry maintained good relations with both Adams and Vice President Thomas Jefferson, hoping that the divided executive might lead to less friction. His hopes were not realized: the split between Federalists (Adams) and Democratic-Republicans (Jefferson) widened.

President Adams appointed Gerry to be a member of a special diplomatic commission sent to Republican France in 1797. Tensions had risen between the two nations after the 1796 ratification of the Jay Treaty, made between the US and Great Britain. It was seen by French leaders as signs of an Anglo-American alliance, and France had consequently stepped up seizures of American ships. Adams chose Gerry, over his cabinet's opposition (on political grounds that Gerry was insufficiently Federalist), because of their long-standing relationship; Adams described Gerry as one of the "two most impartial men in America" (Adams himself being the other).

Gerry joined co-commissioners Charles Cotesworth Pinckney and John Marshall in France in October 1797 and met briefly with Foreign Minister Talleyrand. Some days after that meeting, the delegation was approached by three French agents (at first identified as "X", "Y", and "Z" in published papers, leading the controversy to be called the "XYZ Affair") who demanded substantial bribes from the commissioners before negotiations could continue. The commissioners refused, and sought unsuccessfully to engage Talleyrand in formal negotiations. Believing Gerry to be the most approachable of the commissioners, Talleyrand successively froze first Pinckney and then Marshall out of the informal negotiations, and they left France in April 1798. Gerry, who sought to leave with them, stayed behind because Talleyrand threatened war if he left. Gerry refused to make any significant negotiations afterward and left Paris in August. By then dispatches describing the commission's reception had been published in the United States, raising calls for war. The undeclared naval Quasi-War (1798–1800) followed. Federalists, notably Secretary of State Timothy Pickering, accused Gerry of supporting the French and abetting the breakdown of the talks, while Adams and Republicans such as Thomas Jefferson supported him. The negative press damaged Gerry's reputation, and he was burned in effigy by protestors in front of his home. He was only later vindicated, when his correspondence with Talleyrand was published. In response to the Federalist attacks on him, and because of his perception that the Federalist-led military buildup threatened republican values, Gerry formally joined the Democratic-Republican Party in early 1800, standing for election as Governor of Massachusetts.

For four years Gerry unsuccessfully sought the governorship of Massachusetts. His opponent in these races, Caleb Strong, was a popular moderate Federalist, whose party dominated the state's politics despite a national shift toward the Republicans. In 1803 Republicans in the state were divided, and Gerry only had regional support of the party. He decided not to run in 1804, returning to semi-retirement and to deal with a personal financial crisis. His brother Samuel Russell had mismanaged his own business affairs, and Gerry had propped him up by guaranteeing a loan that was due. The matter ultimately ruined Gerry's finances for his remaining years.

Republican James Sullivan won the governor's seat from Strong in 1807, but his successor was unable to hold the seat in the 1809 election, which went to Federalist Christopher Gore. Gerry stood for election again in 1810 against Gore, and won a narrow victory. Republicans cast Gore as an ostentatious British-loving Tory who wanted to restore the monarchy (his parents had remained Loyal during the Revolution), and Gerry as a patriotic American, while Federalists described Gerry as a "French partizan" and Gore as an honest man devoted to ridding the government of foreign influence. A temporary lessening in the threat of war with Britain aided Gerry. The two battled again in 1811, with Gerry once again victorious in a highly acrimonious campaign.
Gerry's first year as governor was less controversial than his second, because the Federalists controlled the state senate. He preached moderation in the political discourse, noting that it was important that the nation present a unified front in its dealings with foreign powers. In his second term, with full Republican control of the legislature, he became notably more partisan, purging much of the state government of Federalist appointees. The legislature also enacted "reforms" of the court system that resulted in an increase in the number of judicial appointments, which Gerry filled with Republican partisans. Infighting within the party and a shortage of qualified candidates, however, played against Gerry, and the Federalists scored points by complaining vocally about the partisan nature of the reforms.

Other legislation passed during Gerry's second year included a bill broadening the membership of Harvard's Board of Overseers to diversify its religious membership, and another that liberalized religious taxes. The Harvard bill had significant political slant because the recent split between orthodox Congregationalists and Unitarians also divided the state to some extent along party lines, and Federalist Unitarians had recently gained control over the Harvard board.

In 1812 the state adopted new constitutionally mandated electoral district boundaries. The Republican-controlled legislature had created district boundaries designed to enhance their party's control over state and national offices, leading to some oddly shaped legislative districts. Although Gerry was unhappy about the highly partisan districting (according to his son-in-law, he thought it "highly disagreeable"), he signed the legislation. The shape of one of the state senate districts in Essex County was compared to a salamander by a local Federalist newspaper in a political cartoon, calling it a "Gerry-mander". Ever since, the creation of such districts has been called gerrymandering. Gerry also engaged in partisan investigations of potential libel against him by elements of the Federalist press, further damaging his popularity with moderates. The redistricting controversy, along with the libel investigation and the impending War of 1812, contributed to Gerry's defeat in 1812 (once again at the hands of Caleb Strong, whom the Federalists had brought out of retirement). The gerrymandering of the state senate was a notable success in the 1812 election: the body was thoroughly dominated by Republicans, even though the house and the governor's seat went to Federalists by substantial margins.

Gerry's financial difficulties prompted him to ask President James Madison for a federal position after his loss in the 1812 election (which was held early in the year). He was chosen by the Democratic-Republican party congress to be Madison's vice presidential running mate in the 1812 presidential election, although the nomination was first offered to John Langdon. He was viewed as a relatively safe choice who would attract Northern votes but not pose a threat to James Monroe, who was thought likely to succeed Madison. Madison easily won reelection, and Gerry took the oath of office at Elmwood in March 1813. At that time the office of vice president was largely a sinecure; Gerry's duties included advancing the administration's agenda in Congress and dispensing patronage positions in New England. Gerry's actions in support of the War of 1812 had a partisan edge: he expressed concerns over a possible Federalist seizure of Fort Adams (as Boston's Fort Independence was then known) as a prelude to Anglo-Federalist cooperation, and sought the arrest of printers of Federalist newspapers.
On November 23, 1814, he fell seriously ill while visiting Joseph Nourse of the Treasury Department, and died not long after returning to his home in the Seven Buildings. He is buried in the Congressional Cemetery in Washington, D.C., with a memorial by John Frazee. He is the only signer of the Declaration buried in the nation's capital. The estate he left his wife and children was rich in land and poor in cash; he had managed to repay his brother's debts with his pay as vice president. Aged 68 at the start of his Vice Presidency, he would be the oldest person to become VP until Charles Curtis in 1929.

Gerry is generally remembered for the use of his name in the word "gerrymander", for his refusal to sign the United States Constitution, and for his role in the XYZ Affair. His path through the politics of the age has been difficult to characterize; early biographers, including his son-in-law James T. Austin and Samuel Eliot Morison, struggled to explain his apparent changes in position. Biographer George Athan Billias posits that Gerry was a consistent advocate and practitioner of republicanism as it was originally envisioned, and that his role in the Constitutional Convention had a significant impact on the document it eventually produced.

Gerry had ten children, of which seven survived into adulthood: Gerry's son, James Thompson Gerry, commanded the USS "Albany", a United States Navy war sloop that went down with all hands in 1854.

Gerry's grandson Elbridge Thomas Gerry became a distinguished lawyer and philanthropist in New York. His great-grandson, Peter G. Gerry (1879–1957), was a member of the U.S. House of Representatives and a United States Senator from Rhode Island.
Gerry is depicted in two of John Trumbull's paintings, the "Declaration of Independence" and "General George Washington Resigning His Commission". Both are on view in the rotunda of the United States Capitol.

The upstate New York town of Elbridge is believed to have been named in his honor, as is the western New York town of Gerry, in Chautauqua County. The town of Phillipston, Massachusetts was originally incorporated in 1786 under the name Gerry in his honor, but was changed to its present name after the town submitted a petition in 1812, citing Democratic-Republican support for the War of 1812.

Gerry's Landing Road in Cambridge, Massachusetts is located near the Eliot Bridge not far from Elmwood. During the 19th century, the area was known as Gerry's Landing (formerly known as Sir Richard's Landing), and was used by a Gerry relative for a short time as a landing and storehouse. The supposed house of his birth, the Elbridge Gerry House (it is uncertain whether he was born in the house currently standing on the site or an earlier structure) stands in Marblehead, and that town's Elbridge Gerry School is named in his honor.

Notes
Sources




</doc>
<doc id="10294" url="https://en.wikipedia.org/wiki?curid=10294" title="Encryption">
Encryption

In cryptography, encryption is the process of encoding a message or information in such a way that only authorized parties can access it and those who are not authorized cannot. Encryption does not itself prevent interference, but denies the intelligible content to a would-be interceptor. In an encryption scheme, the intended information or message, referred to as plaintext, is encrypted using an encryption algorithm – a cipher – generating ciphertext that can be read only if decrypted. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is in principle possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users.

In symmetric-key schemes, the encryption and decryption keys are the same. Communicating parties must have the same key in order to achieve secure communication.

In public-key encryption schemes, the encryption key is published for anyone to use and encrypt messages. However, only the receiving party has access to the decryption key that enables messages to be read. Public-key encryption was first described in a secret document in 1973; before then all encryption schemes were symmetric-key (also called private-key).

A publicly available public key encryption application called Pretty Good Privacy (PGP) was written in 1991 by Phil Zimmermann, and distributed free of charge with source code; it was purchased by Symantec in 2010 and is regularly updated.

Encryption has long been used by militaries and governments to facilitate secret communication. It is now commonly used in protecting information within many kinds of civilian systems. For example, the Computer Security Institute reported that in 2007, 71% of companies surveyed utilized encryption for some of their data in transit, and 53% utilized encryption for some of their data in storage. Encryption can be used to protect data "at rest", such as information stored on computers and storage devices (e.g. USB flash drives). In recent years, there have been numerous reports of confidential data, such as customers' personal records, being exposed through loss or theft of laptops or backup drives. Encrypting such files at rest helps protect them if physical security measures fail. Digital rights management systems, which prevent unauthorized use or reproduction of copyrighted material and protect software against reverse engineering (see also copy protection), is another somewhat different example of using encryption on data at rest.

In response to encryption of data at rest, cyber-adversaries have developed new types of attacks. These more recent threats to encryption of data at rest include cryptographic attacks, stolen ciphertext attacks, attacks on encryption keys, insider attacks, data corruption or integrity attacks, data destruction attacks, and ransomware attacks. Data fragmentation and active defense data protection technologies attempt to counter some of these attacks, by distributing, moving, or mutating ciphertext so it is more difficult to identify, steal, corrupt, or destroy.

Encryption is also used to protect data in transit, for example data being transferred via networks (e.g. the Internet, e-commerce), mobile telephones, wireless microphones, wireless intercom systems, Bluetooth devices and bank automatic teller machines. There have been numerous reports of data in transit being intercepted in recent years. Data should also be encrypted when transmitted across networks in order to protect against eavesdropping of network traffic by unauthorized users.

Encryption, by itself, can protect the confidentiality of messages, but other techniques are still needed to protect the integrity and authenticity of a message; for example, verification of a message authentication code (MAC) or a digital signature. Standards for cryptographic software and hardware to perform encryption are widely available, but successfully using encryption to ensure security may be a challenging problem. A single error in system design or execution can allow successful attacks. Sometimes an adversary can obtain unencrypted information without directly undoing the encryption. See, e.g., traffic analysis, TEMPEST, or Trojan horse.

Digital signature and encryption must be applied to the ciphertext when it is created (typically on the same device used to compose the message) to avoid tampering; otherwise any node between the sender and the encryption agent could potentially tamper with it. Encrypting at the time of creation is only secure if the encryption device itself has not been tampered with.

Conventional methods for deleting data permanently from a storage device involve overwriting its whole content with zeros, ones or other patterns – a process which can take a significant amount of time, depending on the capacity and the type of the medium. Cryptography offers a way of making the erasure almost instantaneous. This method is called crypto-shredding. An example implementation of this method can be found on iOS devices, where the cryptographic key is kept in a dedicated 'Effaceable Storage'. Because the key is stored on the same device, this setup on its own does not offer full confidentiality protection in case an unauthorised person gains physical access to the device.





</doc>
<doc id="10296" url="https://en.wikipedia.org/wiki?curid=10296" title="EPR paradox">
EPR paradox

The Einstein–Podolsky–Rosen paradox or the EPR paradox
of 1935 is a thought experiment in quantum mechanics with which Albert Einstein and his colleagues Boris Podolsky and Nathan Rosen (EPR) claimed to demonstrate that the wave function does not provide a complete description of physical reality, and hence that the Copenhagen interpretation is unsatisfactory; resolutions of the paradox have important implications for the interpretation of quantum mechanics.

While EPR felt that the paradox showed that quantum theory was incomplete and should be extended with hidden variables,
the usual modern resolution is to say that due to the common preparation of the two particles (for example the creation of an electron-positron pair from a photon) the property we want to measure has a well defined meaning only when analyzed for the whole system while the same property for the parts individually remains undefined. Therefore, if similar measurements are being performed on the two entangled subsystems, there will always be a correlation between the outcomes resulting in a well defined global outcome i.e. for both subsystems together. However, the outcomes for each subsystem separately at each repetition of the experiment will not be well defined or predictable. This correlation does not imply any action of the measurement of one particle on the measurement of the other; therefore it does not imply any form of action at a distance. This modern resolution eliminates the need for hidden variables, action at a distance or other structures introduced over time in order to explain the phenomenon.

A preference for the latter resolution is supported by experiments suggested by Bell's theorem of 1964, which exclude some classes of hidden variable theory.

According to quantum mechanics, under some conditions, a pair of quantum systems may be described by a single wave function, which encodes the probabilities of the outcomes of experiments that may be performed on the two systems, whether jointly or individually. At the time the EPR article discussed below was written, it was known from experiments that the outcome of an experiment sometimes cannot be uniquely predicted. An example of such indeterminacy can be seen when a beam of light is incident on a half-silvered mirror. One half of the beam will reflect, and the other will pass. If the intensity of the beam is reduced until only one photon is in transit at any time, whether that photon will reflect or transmit cannot be predicted quantum mechanically.

The routine explanation of this effect was, at that time, provided by Heisenberg's uncertainty principle. Physical quantities come in pairs called conjugate quantities. Examples of such conjugate pairs are (position, momentum), (time, energy), and (angular position, angular momentum). When one quantity was measured, and became determined, the conjugated quantity became indeterminate. Heisenberg explained this uncertainty as due to the quantization of the disturbance from measurement.

The EPR paper, written in 1935, was intended to illustrate that this explanation is inadequate. It considered two entangled particles, referred to as A and B, and pointed out that measuring a quantity of a particle A will cause the conjugated quantity of particle B to become undetermined, even if there was no contact, no classical disturbance. The basic idea was that the quantum states of two particles in a system cannot always be decomposed from the joint state of the two, as is the case for the Bell state, formula_1

Heisenberg's principle was an attempt to provide a classical explanation of a quantum effect sometimes called non-locality. According to EPR there were two possible explanations. Either there was some interaction between the particles (even though they were separated) or the information about the outcome of all possible measurements was already present in both particles.

The EPR authors preferred the second explanation according to which that information was encoded in some 'hidden parameters'. The first explanation of an effect propagating instantly across a distance is in conflict with the theory of relativity. They then concluded that quantum mechanics was incomplete since its formalism does not permit hidden parameters.

Violations of the conclusions of Bell's theorem are generally understood to have demonstrated that the hypotheses of Bell's theorem, also assumed by Einstein, Podolsky and Rosen, do not apply in our world. Most physicists who have examined the issue concur that experiments, such as those of Alain Aspect and his group, have confirmed that physical probabilities, as predicted by quantum theory, do exhibit the phenomena of Bell-inequality violations that are considered to invalidate EPR's preferred "local hidden-variables" type of explanation for the correlations to which EPR first drew attention.

The article that first brought forth these matters, "Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?" was published in 1935. The paper prompted a response by Bohr, which he published in the same journal, in the same year, using the same title. There followed a debate between Bohr and Einstein about the fundamental nature of reality. Einstein had been skeptical of the Heisenberg uncertainty principle and the role of chance in quantum theory. But the crux of this debate was not about chance, but something even deeper: Is there one objective physical reality, which every observer sees from his own vantage? (Einstein's view) Or does the observer co-create physical reality by the questions he poses with experiments? (Bohr's view)

Einstein struggled to the end of his life for a theory that could better comply with his idea of causality, protesting against the view that there exists no objective physical reality other than that which is revealed through measurement interpreted in terms of quantum mechanical formalism. However, since Einstein's death, experiments analogous to the one described in the EPR paper have been carried out, starting in 1976 by French scientists Lamehi-Rachti and Mittig at the Saclay Nuclear Research Centre. These experiments appear to show that the local realism idea is false, vindicating Bohr.

Since the early twentieth century, quantum theory has proved to be successful in describing accurately the physical reality of the mesoscopic and microscopic world, in multiple reproducible physics experiments.

Quantum mechanics was developed with the aim of describing atoms and explaining the observed spectral lines in a measurement apparatus. Although disputed especially in the early twentieth century, it has yet to be seriously challenged. Philosophical interpretations of quantum phenomena, however, are another matter: the question of how to "interpret" the mathematical formulation of quantum mechanics has given rise to a variety of different answers from people of different philosophical persuasions (see Interpretations of quantum mechanics).

Quantum theory and quantum mechanics do not provide single measurement outcomes in a deterministic way. According to the understanding of quantum mechanics known as the Copenhagen interpretation, measurement causes an instantaneous collapse of the wave function describing the quantum system into an eigenstate of the observable that was measured. Einstein characterized this imagined collapse in the 1927 Solvay Conference. He presented a thought experiment in which electrons are introduced through a small hole in a sphere whose inner surface serves as a detection screen. The electrons will contact the spherical detection screen in a widely dispersed manner. Those electrons, however, are all individually described by wave fronts that expand in all directions from the point of entry. A wave as it is understood in everyday life would paint a large area of the detection screen, but the electrons would be found to impact the screen at single points and would eventually form a pattern in keeping with the probabilities described by their identical wave functions. Einstein asks what makes each electron's wave front "collapse" at its respective location. Why do the electrons appear as single bright scintillations rather than as dim washes of energy across the surface? Why does any single electron appear at one point rather than some alternative point? The behavior of the electrons gives the impression of some signal having been sent to all possible points of contact that would have nullified all but one of them, or, in other words, would have preferentially selected a single point to the exclusion of all others.

Einstein was the most prominent opponent of the Copenhagen interpretation. In his view, quantum mechanics was incomplete. Commenting on this, other writers (such as John von Neumann and David Bohm) hypothesized that consequently there would have to be 'hidden' variables responsible for random measurement results, something which was not expressly claimed in the original paper.

The 1935 EPR paper condensed the philosophical discussion into a physical argument. The authors claim that given a specific experiment, in which the outcome of a measurement is known before the measurement takes place, there must exist something in the real world, an "element of reality", that determines the measurement outcome. They postulate that these elements of reality are local, in the sense that each belongs to a certain point in spacetime. Each element may only be influenced by events which are located in the backward light cone of its point in spacetime (i.e., the past). These claims are founded on assumptions about nature that constitute what is now known as local realism.

Though the EPR paper has often been taken as an exact expression of Einstein's views, it was primarily authored by Podolsky, based on discussions at the Institute for Advanced Study with Einstein and Rosen. Einstein later expressed to Erwin Schrödinger that, "it did not come out as well as I had originally wanted; rather, the essential thing was, so to speak, smothered by the formalism." In 1936, Einstein presented an individual account of his local realist ideas.

The original EPR paradox challenges the prediction of quantum mechanics that it is impossible to know both the position and the momentum of a quantum particle. This challenge can be extended to other pairs of physical properties.

The original paper purports to describe what must happen to "two systems I and II, which we permit to interact ...", and, after some time, "we suppose that there is no longer any interaction between the two parts." As explained by Manjit Kumar (2009), the EPR description involves "two particles, A and B, [which] interact briefly and then move off in opposite directions." According to Heisenberg's uncertainty principle, it is impossible to measure both the momentum and the position of particle B exactly. However, it is possible to measure the exact position of particle A. By calculation, therefore, with the exact position of particle A known, the exact position of particle B can be known. Alternatively, the exact momentum of particle A can be measured, so the exact momentum of particle B can be worked out. Kumar writes: "EPR argued that they had proved that ... [particle] B can have simultaneously exact values of position and momentum. ... Particle B has a position that is real and a momentum that is real." EPR appeared to have contrived a means to establish the exact values of "either" the momentum "or" the position of B due to measurements made on particle A, without the slightest possibility of particle B being physically disturbed.

EPR tried to set up a paradox to question the range of true application of Quantum Mechanics: Quantum theory predicts that both values cannot be known for a particle, and yet the EPR thought experiment purports to show that they must all have determinate values. The EPR paper says: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete."

The EPR paper ends by saying: While we have thus shown that the wave function does not provide a complete description of the physical reality, we left open the question of whether or not such a description exists. We believe, however, that such a theory is possible.

We have a source that emits electron–positron pairs, with the electron sent to destination "A", where there is an observer named Alice, and the positron sent to destination "B", where there is an observer named Bob. According to quantum mechanics, we can arrange our source so that each emitted pair occupies a quantum state called a spin singlet. The particles are thus said to be entangled. This can be viewed as a quantum superposition of two states, which we call state I and state II. In state I, the electron has spin pointing upward along the "z"-axis ("+z") and the positron has spin pointing downward along the "z"-axis (−"z"). In state II, the electron has spin −"z" and the positron has spin +"z". Because it is in a superposition of states it is impossible without measuring to know the definite state of spin of either particle in the spin singlet.

Alice now measures the spin along the "z"-axis. She can obtain one of two possible outcomes: +"z" or −"z". Suppose she gets +"z". According to the Copenhagen interpretation of quantum mechanics, the quantum state of the system collapses into state I. The quantum state determines the probable outcomes of any measurement performed on the system. In this case, if Bob subsequently measures spin along the "z"-axis, there is 100% probability that he will obtain −"z". Similarly, if Alice gets −"z", Bob will get +"z".

There is, of course, nothing special about choosing the "z"-axis: according to quantum mechanics the spin singlet state may equally well be expressed as a superposition of spin states pointing in the "x" direction. Suppose that Alice and Bob had decided to measure spin along the "x"-axis. We'll call these states Ia and IIa. In state Ia, Alice's electron has spin +"x" and Bob's positron has spin −"x". In state IIa, Alice's electron has spin −"x" and Bob's positron has spin +"x". Therefore, if Alice measures +"x", the system 'collapses' into state Ia, and Bob will get −"x". If Alice measures −"x", the system collapses into state IIa, and Bob will get +"x".

Whatever axis their spins are measured along, they are always found to be opposite. This can only be explained if the particles are linked in some way. Either they were created with a definite (opposite) spin about every axis—a "hidden variable" argument—or they are linked so that one electron "feels" which axis the other is having its spin measured along, and becomes its opposite about that one axis—an "entanglement" argument. Moreover, if the two particles have their spins measured about "different" axes, once the electron's spin has been measured about the "x"-axis (and the positron's spin about the "x"-axis deduced), the positron's spin about the "z"-axis will no longer be certain, as if (a) it knows that the measurement has taken place, or (b) it has a definite spin already, about a "second" axis—a hidden variable. However, it turns out that the predictions of Quantum Mechanics, which have been confirmed by experiment, cannot be explained by any local hidden variable theory. This is demonstrated in Bell's theorem.

In quantum mechanics, the "x"-spin and "z"-spin are "incompatible observables", meaning the Heisenberg uncertainty principle applies to alternating measurements of them: a quantum state cannot possess a definite value for both of these variables. Suppose Alice measures the "z"-spin and obtains "+z", so that the quantum state collapses into state I. Now, instead of measuring the "z"-spin as well, Bob measures the "x"-spin. According to quantum mechanics, when the system is in state I, Bob's "x"-spin measurement will have a 50% probability of producing +"x" and a 50% probability of -"x". It is impossible to predict which outcome will appear until Bob actually "performs" the measurement.

Here is the crux of the matter:

You might imagine that, when Bob measures the "x"-spin of his positron, he would get an answer with absolute certainty, since prior to this he hasn't disturbed his particle at all. But it turns out that Bob's positron has a 50% probability of producing +"x" and a 50% probability of −"x", meaning the outcome is not certain. It's as if Bob's positron "knows" that Alice has measured the "z"-spin of her electron, and hence his positron's own "z"-spin must also be set, but its "x"-spin remains uncertain.

Put another way, how does Bob's positron know which way to point if Alice decides (based on information unavailable to Bob) to measure x (i.e., to be the opposite of Alice's electron's spin about the "x"-axis) and "also" how to point if Alice measures z, since it is only supposed to know one thing at a time? The Copenhagen interpretation rules that say the wave function "collapses" at the time of measurement, so there must be action at a distance (entanglement) or the positron must know more than it's supposed to know (hidden variables). 

Here is the paradox summed up:

It is one thing to say that physical measurement of the first particle's momentum affects uncertainty in its "own" position, but to say that measuring the first particle's momentum affects the uncertainty in the position of the "other" is another thing altogether. Einstein, Podolsky and Rosen asked how can the second particle "know" to have precisely defined momentum but uncertain position? Since this implies that one particle is communicating with the other instantaneously across space, i.e., faster than light, this is the "paradox".

Incidentally, Bell used spin as his example, but many types of physical quantities—referred to as "observables" in quantum mechanics—can be used. The EPR paper used momentum for the observable. Experimental realisations of the EPR scenario often use photon polarization, because polarized photons are easy to prepare and measure.

The principle of locality states that physical processes occurring at one place should have no immediate effect on the elements of reality at another location. At first sight, this appears to be a reasonable assumption to make, as it seems to be a consequence of special relativity, which states that information can never be transmitted faster than the speed of light without violating causality. It is generally believed that any theory which violates causality would also be internally inconsistent, and thus useless.

It turns out that the usual rules for combining quantum mechanical and classical descriptions violate the principle of locality without violating causality. Causality is preserved because there is no way for Alice to transmit messages (i.e., information) to Bob by manipulating her measurement axis. Whichever axis she uses, she has a 50% probability of obtaining "+" and 50% probability of obtaining "−", completely at random; according to quantum mechanics, it is fundamentally impossible for her to influence what result she gets. Furthermore, Bob is only able to perform his measurement "once": there is a fundamental property of quantum mechanics, known as the "no cloning theorem", which makes it impossible for him to make a million copies of the electron he receives, perform a spin measurement on each, and look at the statistical distribution of the results. Therefore, in the one measurement he is allowed to make, there is a 50% probability of getting "+" and 50% of getting "−", regardless of whether or not his axis is aligned with Alice's.

However, the principle of locality appeals powerfully to physical intuition, and Einstein, Podolsky and Rosen were unwilling to abandon it. Einstein derided the quantum mechanical predictions as "spooky action at a distance". The conclusion they drew was that quantum mechanics is not a complete theory.

In recent years, however, doubt has been cast on EPR's conclusion due to developments in understanding locality and especially quantum decoherence. The word locality has several different meanings in physics. For example, in quantum field theory "locality" means that quantum fields at different points of space do not interact with one another. However, quantum field theories that are "local" in this sense "appear" to violate the principle of locality as defined by EPR, but they nevertheless do not violate locality in a more general sense. Wavefunction collapse can be viewed as an epiphenomenon of quantum decoherence, which in turn is nothing more than an effect of the underlying local time evolution of the wavefunction of a system and "all" of its environment. Since the "underlying" behaviour doesn't violate local causality, it follows that neither does the additional effect of wavefunction collapse, whether real "or" apparent. Therefore, as outlined in the example above, neither the EPR experiment nor any quantum experiment demonstrates that faster-than-light signaling is possible.

There are several ways to resolve the EPR paradox. The one suggested by EPR is that quantum mechanics, despite its success in a wide variety of experimental scenarios, is actually an incomplete theory. In other words, there is some yet undiscovered theory of nature to which quantum mechanics acts as a kind of statistical approximation (albeit an exceedingly successful one). Unlike quantum mechanics, the more complete theory contains variables corresponding to all the "elements of reality". There must be some unknown mechanism acting on these variables to give rise to the observed effects of "non-commuting quantum observables", i.e. the Heisenberg uncertainty principle. Such a theory is called a hidden variable theory.

To illustrate this idea, we can formulate a very simple hidden variable theory for the above thought experiment. One supposes that the quantum spin-singlet states emitted by the source are actually approximate descriptions for "true" physical states possessing definite values for the "z"-spin and "x"-spin. In these "true" states, the positron going to Bob always has spin values opposite to the electron going to Alice, but the values are otherwise completely random. For example, the first pair emitted by the source might be "(+"z", −"x") to Alice and (−"z", +"x") to Bob", the next pair "(−"z", −"x") to Alice and (+"z", +"x") to Bob", and so forth. Therefore, if Bob's measurement axis is aligned with Alice's, he will necessarily get the opposite of whatever Alice gets; otherwise, he will get "+" and "−" with equal probability.

Assuming we restrict our measurements to the "z"- and "x"-axes, such a hidden variable theory is experimentally indistinguishable from quantum mechanics. In reality, there may be an infinite number of axes along which Alice and Bob can perform their measurements, so there would have to be an infinite number of independent hidden variables. However, this is not a serious problem; we have formulated a very simplistic hidden variable theory, and a more sophisticated theory might be able to patch it up. It turns out that there is a much more serious challenge to the idea of hidden variables.

In 1964, John Bell showed that the predictions of quantum mechanics in the EPR thought experiment are significantly different from the predictions of a particular class of hidden variable theories (the "local" hidden variable theories). Roughly speaking, quantum mechanics has a much stronger statistical correlation with measurement results performed on different axes than do these hidden variable theories. These differences, expressed using inequality relations known as "Bell's inequalities", are in principle experimentally detectable. After the publication of Bell's paper, a variety of experiments to test Bell's inequalities were devised. These generally relied on measurement of photon polarization. All experiments conducted to date have found behavior in line with the predictions of standard quantum mechanics theory.

Later work by Henry Stapp showed that a key property of local hidden variable theories which lead to Bell's inequalities was counter-factual definiteness. Building on Stapp's observations, P.H. Eberhard showed that any local counter-factual model results in Bell's inequality even without the assumption of there being hidden variables unknown to physics upon which the relevant observables depend. Arthur Fine subsequently showed that any theory satisfying the inequalities can be modeled by a local hidden variable theory. (Although Eberhard referred to his result as "Bell's theorem without hidden variables", Fine used a more general definition of "hidden variables" that includes the possibility of the observables being elementary.) Fine went on to show that any stochastic factorizable model leads to Bell's inequality. Itamar Pitowsky showed that Bell's inequality was a special case of an inequality discovered by George Boole which provides a consistency check on whether data can be represented by variables on a single classical probability space. He interpreted this to be an indication that the locality assumption prevented the data from being represented as events on such a space.

As Eberhard's proof made use of both locality and counter-factual definiteness it was assumed that an interpretation could reject either one of these to escape Bell's inequality. Violation of locality is difficult to reconcile with special relativity, and is thought to be incompatible with the principle of causality, nevertheless there was renewed interest in the Bohm interpretation of quantum mechanics which keeps counter-factual definiteness while introducing a conjectured non-local mechanism in the form of the 'quantum potential' that is defined as one of the terms of the Schrödinger equation. Mainstream physics preferred to keep locality and reject counter-factual definiteness. Fine's work showed that, taking locality as a given, there exist scenarios in which two statistical variables are correlated in a manner inconsistent with counter-factual definiteness, and that such scenarios are no more mysterious than any other, despite the fact that the inconsistency with counter-factual definiteness may seem 'counter-intuitive'.

Further insights resulted from the work of Lawrence J. Landau. Landau showed that if it is assumed that there is a single classical probability space underlying all the observables under consideration in the EPR experiment, Bell's inequality will result. Thus the fundamental issue is that Quantum mechanical probabilities cannot be modeled using classical (Kolmogorovian) probability regardless of whether Quantum Mechanics is considered a complete description of reality or not. Regarding Landau's proof Ray Streater notes that it shows that Bohmian mechanics is inconsistent with Quantum mechanics and succumbs to Bell's inequality despite claims to the contrary by its proponents. Streater notes that Landau's proof only requires the assumption of a single classical probability space (a condition still satisfied by Bohm's theory) and the fact that Bohmian mechanics additionally postulates the existence of a non-local mechanism, cannot prevent Bell's inequality from applying to it.

Similar observations have been made by Karl Hess, Walter, Philipp, Hans de Raedt and Kristel Michielsen, who note that in Bell's proof, Bell's assumption of a space of hidden variables behaving as a classical probability space is sufficient to produce a contradiction with the predications of Quantum mechanics via a consistency theorem of N. N. Vorob'ev, a statistician who had built on the same work of Boole used by Pitowsky. The additional assumption of locality used by Bell is redundant and indeed Fine's work had included a derivation of Bell's inequality that did not require the assumption of locality .

However, Bell's theorem does not apply to all possible philosophically realist theories. It is a common misconception that quantum mechanics is inconsistent with all notions of philosophical realism. Realist interpretations of quantum mechanics are possible, although as discussed above, such interpretations must reject counter-factual definiteness. Examples of such realist interpretations are the consistent histories interpretation and the transactional interpretation (first proposed by John G. Cramer in 1986). Griffiths notes that it is not "local realism" that is ruled out by quantum mechanics but "classical realism". Some workers in the field have also attempted to formulate hidden variable theories that exploit loopholes in actual experiments, such as the assumptions made in interpreting experimental data, although no theory has been proposed that can reproduce all the results of quantum mechanics.

Alternatives are still possible. A recent review article based on the Wheeler–Feynman time-symmetric theory rewrites the entire theory in terms of retarded Liénard–Wiechert potentials only, which becomes manifestly causal, and, establishes a conservation law for total generalized momenta held instantaneously for any closed system. The outcome results in correlation between particles from a "handshake principle" based on a variational principle applied to a system as a whole, an idea with a slightly non-local feature but the theory is nonetheless in agreement with the essential results of quantum electrodynamics and relativistic quantum chemistry.

There are also individual EPR-like experiments that have no local hidden variables explanation. Examples have been suggested by David Bohm and by Lucien Hardy.

The Bohm interpretation of quantum mechanics hypothesizes that the state of the universe evolves smoothly through time with no collapsing of quantum wavefunctions. One problem for the Copenhagen interpretation is to precisely define wavefunction collapse. Einstein maintained that quantum mechanics is physically incomplete and logically unsatisfactory. In "The Meaning of Relativity", Einstein wrote, "One can give good reasons why reality cannot at all be represented by a continuous field. From the quantum phenomena it appears to follow with certainty that a finite system of finite energy can be completely described by a finite set of numbers (quantum numbers). This does not seem to be in accordance with a continuum theory and must lead to an attempt to find a purely algebraic theory for the representation of reality. But nobody knows how to find the basis for such a theory."
If time, space, and energy are secondary features derived from a substrate below the Planck scale, then Einstein's hypothetical algebraic system might resolve the EPR paradox (although Bell's theorem would still be valid). If physical reality is totally finite, then the Copenhagen interpretation might be an approximation to an information processing system below the Planck scale.

According to the present view of the situation, quantum mechanics flatly contradicts Einstein's philosophical postulate that any acceptable physical theory must fulfill "local realism".

In the EPR paper (1935), the authors realised that quantum mechanics was inconsistent with their assumptions, but Einstein nevertheless thought that quantum mechanics might simply be augmented by hidden variables (i.e., variables which were, at that point, still obscure to him), without any other change, to achieve an acceptable theory. He pursued these ideas for over twenty years until the end of his life, in 1955.

In contrast, John Bell, in his 1964 paper, showed that quantum mechanics and the class of hidden variable theories Einstein favored would lead to different experimental results: different by a factor of for certain correlations. So the issue of "acceptability", up to that time mainly concerning theory, finally became experimentally decidable.

There are many Bell test experiments, e.g., those of Alain Aspect and others. They support the predictions of quantum mechanics rather than the class of hidden variable theories supported by Einstein.

Most physicists today believe that quantum mechanics is correct, and that the EPR paradox is a "paradox" only because classical intuitions do not correspond to physical reality. How EPR is interpreted regarding locality depends on the interpretation of quantum mechanics one uses. In the Copenhagen interpretation, it is usually understood that instantaneous wave function collapse does occur. However, the view that there is no "causal" instantaneous effect has also been proposed within the Copenhagen interpretation: in this alternate view, measurement affects our ability to define (and measure) quantities in the physical system, not the system itself. In the many-worlds interpretation, locality is strictly preserved, since the effects of operations such as measurement affect only the state of the particle that is measured. However, the results of the measurement are not unique—every possible result is obtained.

The EPR paradox has deepened our understanding of quantum mechanics by exposing the fundamentally non-classical characteristics of the measurement process. Before the publication of the EPR paper, a measurement was often visualized as a physical disturbance that had to be inflicted "directly" upon the measured subsystem. For instance, when measuring the position of an electron, one imagines shining a light on it, thus disturbing the electron and producing the quantum mechanical uncertainties in its position. Such pat and convenient but unhelpful explanations of quantum mechanics remain commonplace today, but they fail to explain (among other things) the EPR paradox, which shows that a "measurement" can be performed on a particle without disturbing it directly, by performing a measurement on a distant entangled particle. In fact, Yakir Aharonov and his collaborators have developed a whole theory of so-called Weak measurement.

Technologies relying on quantum entanglement are now being developed. In quantum cryptography, entangled particles are used to transmit signals that cannot be eavesdropped upon without leaving a trace. In quantum computation, entangled quantum states are used to perform computations in parallel, which may allow certain calculations to be performed much more quickly than they ever could be with classical computers.

The above discussion can be expressed mathematically using the quantum mechanical formulation of spin. The spin degree of freedom for an electron is associated with a two-dimensional complex vector space "V", with each quantum state corresponding to a vector in that space. The operators corresponding to the spin along the "x", "y", and "z" direction, denoted "S", "S", and "S" respectively, can be represented using the Pauli matrices:

where formula_3 is the reduced Planck constant (or the Planck constant divided by 2π).

The eigenstates of "S" are represented as

and the eigenstates of "S" are represented as

The vector space of the electron-positron pair is formula_6, the tensor product of the electron's and positron's vector spaces. The spin singlet state is

where the two terms on the right hand side are what we have referred to as state I and state II above.

From the above equations, it can be shown that the spin singlet can also be written as

where the terms on the right hand side are what we have referred to as state Ia and state IIa.

To illustrate how this leads to the violation of local realism, we need to show that after Alice's measurement of "S" (or "S"), Bob's value of "S" (or "S") is uniquely determined, and therefore corresponds to an "element of physical reality". This follows from the principles of measurement in quantum mechanics. When "S" is measured, the system state ψ collapses into an eigenvector of "S". If the measurement result is "+z", this means that immediately after measurement the system state undergoes an orthogonal projection of ψ onto the
space of states of the form

For the spin singlet, the new state is

Similarly, if Alice's measurement result is −"z", the system undergoes an orthogonal projection onto

which means that the new state is

This implies that the measurement for "S" for Bob's positron is now determined. It will be −"z" in the first case or +"z" in the second case.

It remains only to show that "S" and "S" cannot simultaneously possess definite values in quantum mechanics. One may show in a straightforward manner that no possible vector can be an eigenvector of both matrices. More generally, one may use the fact that the operators do not commute,

along with the Heisenberg uncertainty relation





</doc>
<doc id="10301" url="https://en.wikipedia.org/wiki?curid=10301" title="Encapsulation">
Encapsulation

Encapsulation may refer to:






</doc>
<doc id="10302" url="https://en.wikipedia.org/wiki?curid=10302" title="Ethnologue">
Ethnologue

Ethnologue: Languages of the World is an annual reference publication in print and online that provides statistics and other information on the living languages of the world. It was first issued in 1951, and is now published annually by SIL International, a U.S.-based, worldwide, Christian non-profit organization. SIL's main purpose is to study, develop and document languages to promote literacy and for religious purposes. 

As of 2017, "Ethnologue" contains web-based information on about 7,099 languages in its 20th edition, including the number of speakers, location, dialects, linguistic affiliations, autonym, availability of the Bible in each language and dialect described, a cursory description of revitalization efforts where reported, and an estimate of language viability using the Expanded Graded Intergenerational Disruption Scale (EGIDS).

"Ethnologue" has been published by SIL International (formerly known as the Summer Institute of Linguistics), a Christian linguistic service organization with an international office in Dallas, Texas. 
The organization studies numerous minority languages to facilitate language development, and to work with speakers of such language communities in translating portions of the Bible into their language.
The determination of what characteristics define a single language depends upon sociolinguistic evaluation by various scholars; as the preface to "Ethnologue" states, "Not all scholars share the same set of criteria for what constitutes a 'language' and what features define a 'dialect'." "Ethnologue" follows general linguistic criteria, which are based primarily on mutual intelligibility. Shared language intelligibility features are complex, and usually include etymological and grammatical evidence that is agreed upon by experts.

In addition to choosing a primary name for a language, "Ethnologue" provides listings of the name(s) for the language and any dialects that are used by its speakers, government, foreigners and neighbors. Also included are any names that have been commonly referenced historically, regardless of whether a name is considered official, politically correct or offensive; this allows more complete historic research to be done. These lists of names are not necessarily complete.

In 1984, "Ethnologue" released a three-letter coding system, called an SIL code, to identify each language that it described. This set of codes significantly exceeded the scope of other standards, e.g. ISO 639-1 and ISO 639-2. The 14th edition, published in 2000, included 7,148 language codes.

In 2002, "Ethnologue" was asked to work with the International Organization for Standardization (ISO) to integrate its codes into a draft international standard. The 15th edition of "Ethnologue" was the first edition to use this standard, called ISO 639-3. This standard is now administered separately from Ethnologue (though still by SIL according to rules established by ISO, and since then "Ethnologue" relies on the standard to determine what is listed as a language). In only one case, "Ethnologue" and the ISO standards treat languages slightly differently. ISO 639-3 considers Akan to be a macrolanguage consisting of two distinct languages, Twi and Fante, whereas "Ethnologue" considers Twi and Fante to be dialects of a single language (Akan), since they are mutually intelligible. This anomaly resulted because the ISO 639-2 standard has separate codes for Twi and Fante, which have separate literary traditions, and all 639-2 codes for individual languages are automatically part of 639–3, even though 639-3 would not normally assign them separate codes.

In 2014, with the 17th edition, "Ethnologue" introduced a numerical code for language status using a framework called EGIDS (Expanded Graded Intergenerational Disruption Scale), an elaboration of Fishman's GIDS (Graded Intergenerational Disruption Scale). It ranks a language from 0 for an international language to 10 for an extinct language, i.e. a language with which no-one retains a sense of ethnic identity.

In December 2015, "Ethnologue" launched a metered paywall; users in high-income countries who want to refer to more than seven pages of data per month must buy a paid subscription.

As of 2017, "Ethnologue"'s 20th edition described 237 language families including 86 language isolates and six typological categories, namely sign languages, creoles, pidgins, mixed languages, constructed languages, and as yet unclassified languages.

In 1986, William Bright, then editor of the journal "Language", wrote of "Ethnologue" that it "is indispensable for any reference shelf on the languages of the world". In 2008 in the same journal, Lyle Campbell and Verónica Grondona said: ""Ethnologue"...has become the standard reference, and its usefulness is hard to overestimate."

In 2015, Harald Hammarström, an editor of "Glottolog", criticized the publication for frequently lacking citations and failing to articulate clear principles of language classification and identification. However, he concluded that, on balance, ""Ethnologue" is an impressively comprehensive catalogue of world languages, and it is far superior to anything else produced prior to 2009."

Starting with the 17th edition, "Ethnologue" has been published every year.




</doc>
<doc id="10303" url="https://en.wikipedia.org/wiki?curid=10303" title="Evaporation">
Evaporation

Evaporation is a type of vaporization, that occurs on the surface of a liquid as it changes into the gaseous phase. The surrounding gas must not be saturated with the evaporating substance. When the molecules of the liquid collide, they transfer energy to each other based on how they collide. When a molecule near the surface absorbs enough energy to overcome the vapor pressure, it will "escape" and enter the surrounding air as a gas. When evaporation occurs, the energy removed from the vaporized liquid will reduce the temperature of the liquid, resulting in evaporative cooling.

On average, only a fraction of the molecules in a liquid have enough heat energy to escape from the liquid. The evaporation will continue until an equilibrium is reached when the evaporation of the liquid is the equal to its condensation. In an enclosed environment, a liquid will evaporate until the surrounding air is saturated.

Evaporation is an essential part of the water cycle. The sun (solar energy) drives evaporation of water from oceans, lakes, moisture in the soil, and other sources of water. In hydrology, evaporation and transpiration (which involves evaporation within plant stomata) are collectively termed evapotranspiration. Evaporation of water occurs when the surface of the liquid is exposed, allowing molecules to escape and form water vapor; this vapor can then rise up and form clouds. With sufficient energy, the liquid will turn into vapor.

For molecules of a liquid to evaporate, they must be located near the surface, they have to be moving in the proper direction, and have sufficient kinetic energy to overcome liquid-phase intermolecular forces. When only a small proportion of the molecules meet these criteria, the rate of evaporation is low. Since the kinetic energy of a molecule is proportional to its temperature, evaporation proceeds more quickly at higher temperatures. As the faster-moving molecules escape, the remaining molecules have lower average kinetic energy, and the temperature of the liquid decreases. This phenomenon is also called evaporative cooling. This is why evaporating sweat cools the human body.
Evaporation also tends to proceed more quickly with higher flow rates between the gaseous and liquid phase and in liquids with higher vapor pressure. For example, laundry on a clothes line will dry (by evaporation) more rapidly on a windy day than on a still day. Three key parts to evaporation are heat, atmospheric pressure (determines the percent humidity), and air movement.

On a molecular level, there is no strict boundary between the liquid state and the vapor state. Instead, there is a Knudsen layer, where the phase is undetermined. Because this layer is only a few molecules thick, at a macroscopic scale a clear phase transition interface cannot be seen.

Liquids that do not evaporate visibly at a given temperature in a given gas (e.g., cooking oil at room temperature) have molecules that do not tend to transfer energy to each other in a pattern sufficient to frequently give a molecule the heat energy necessary to turn into vapor. However, these liquids "are" evaporating. It is just that the process is much slower and thus significantly less visible.

If evaporation takes place in an enclosed area, the escaping molecules accumulate as a vapor above the liquid. Many of the molecules return to the liquid, with returning molecules becoming more frequent as the density and pressure of the vapor increases. When the process of escape and return reaches an equilibrium, the vapor is said to be "saturated", and no further change in either vapor pressure and density or liquid temperature will occur. For a system consisting of vapor and liquid of a pure substance, this equilibrium state is directly related to the vapor pressure of the substance, as given by the Clausius–Clapeyron relation:

where "P", "P" are the vapor pressures at temperatures "T", "T" respectively, Δ"H" is the enthalpy of vaporization, and "R" is the universal gas constant. The rate of evaporation in an open system is related to the vapor pressure found in a closed system. If a liquid is heated, when the vapor pressure reaches the ambient pressure the liquid will boil.

The ability for a molecule of a liquid to evaporate is based largely on the amount of kinetic energy an individual particle may possess. Even at lower temperatures, individual molecules of a liquid can evaporate if they have more than the minimum amount of kinetic energy required for vaporization.

Note: Air used here is a common example; however, the vapor phase can be other gases.


In the US, the National Weather Service measures the actual rate of evaporation from a standardized "pan" open water surface outdoors, at various locations nationwide. Others do likewise around the world. The US data is collected and compiled into an annual evaporation map. The measurements range from under 30 to over per year.

Evaporation is an endothermic process, in that heat is absorbed during evaporation.


Fuel droplets vaporize as they receive heat by mixing with the hot gases in the combustion chamber. Heat (energy) can also be received by radiation from any hot refractory wall of the combustion chamber.

Internal combustion engines rely upon the vaporization of the fuel in the cylinders to form a fuel/air mixture in order to burn well.
The chemically correct air/fuel mixture for total burning of gasoline has been determined to be 15 parts air to one part gasoline or 15/1 by weight. Changing this to a volume ratio yields 8000 parts air to one part gasoline or 8,000/1 by volume.

Thin films may be deposited by evaporating a substance and condensing it onto a substrate, or by dissolving the substance in a solvent, spreading the resulting solution thinly over a substrate, and evaporating the solvent. The Hertz–Knudsen equation is often used to estimate the rate of evaporation in these instances.



</doc>
<doc id="10304" url="https://en.wikipedia.org/wiki?curid=10304" title="Esbat">
Esbat

An esbat is a coven meeting at a time other than one of the Sabbats within Wicca and other Wiccan-influenced forms of contemporary Paganism. Janet and Stewart Farrar describe esbats as an opportunity for a "love feast, healing work, psychic training and all."

The term "esbat" is derived from Old French "s'esbattre" (Modern French "ébat"), meaning "to frolic and amuse oneself", "diversion". It was a borrowing by 20th century anthropologist Margaret Murray's use of French witch trial sources on supposed Witches' Sabbaths in her attempts to "reconstruct" a "Witch Cult in Western Europe".

An esbat is commonly understood to be a ritual observance on the night of a full moon. However, the late high priestess Doreen Valiente distinguished between "full moon Esbat[s]" and other esbatic occasions.

The term "esbat" in this sense was described by Margaret Murray.


</doc>
<doc id="10307" url="https://en.wikipedia.org/wiki?curid=10307" title="Equal temperament">
Equal temperament

An equal temperament is a musical temperament, or a system of tuning, in which the frequency interval between every pair of adjacent notes has the same ratio. In other words, there are equal ratios of the frequencies of any adjacent pair, and, since pitch is perceived roughly as the logarithm of frequency, equal perceived "distance" from every note to its nearest neighbor.

In equal temperament tunings, the generating interval is often found by dividing some larger desired interval, often the octave (ratio 2:1), into a number of smaller equal steps (equal frequency ratios between successive notes). In classical music and Western music in general, the most common tuning system for the past few hundred years has been and remains twelve-tone equal temperament (also known as 12 equal temperament, 12-TET, or 12-ET), which divides the octave into 12 parts, all of which are equal on a logarithmic scale, with a ratio equal to the 12th root of 2 ( ≈ 1.05946). That resulting smallest interval, the width of an octave, is called a semitone or half step. In modern times, 12TET is usually tuned relative to a standard pitch of 440 Hz, called A440, meaning one note is tuned to A440, and all other notes are some multiple of semitones away from that in either direction. (The standard pitch has not always been 440 Hz. It has varied and generally risen over the past few hundred years.)

Other equal temperaments exist. They divide the octave differently. For example, some music has been written in 19-TET and 31-TET. Arabic music uses 24-TET as a notational convention. In Western countries, when people use the term "equal temperament" without qualification, they usually mean 12-TET. To avoid ambiguity between equal temperaments that divide the octave and ones that divide some other interval (or that use an arbitrary generator without first dividing a larger interval), the term equal division of the octave, or EDO is preferred for the former. According to this naming system, "12-TET" is called "12-EDO", "31-TET" is called "31-EDO", and so on.

An example of an equal temperament that finds its smallest interval by dividing an interval other than the octave into equal parts is the equal-tempered version of the Bohlen–Pierce scale, which divides the just interval of an octave and a fifth (ratio 3:1), called a "tritave" or a "pseudo-octave" in that system, into 13 equal parts.

Unfretted string ensembles, which can adjust the tuning of all notes except for open strings, and vocal groups, who have no mechanical tuning limitations, sometimes use a tuning much closer to just intonation for acoustic reasons. Other instruments, such as some wind, keyboard, and fretted instruments, often only approximate equal temperament, where technical limitations prevent exact tunings. Some wind instruments that can easily and spontaneously bend their tone, most notably trombones, use tuning similar to string ensembles and vocal groups.

The two figures frequently credited with the achievement of exact calculation of equal temperament are Zhu Zaiyu (also romanized as Chu-Tsaiyu. Chinese: ) in 1584 and Simon Stevin in 1585. According to Fritz A. Kuttner, a critic of the theory, it is known that "Chu-Tsaiyu presented a highly precise, simple and ingenious method for arithmetic calculation of equal temperament mono-chords in 1584" and that "Simon Stevin offered a mathematical definition of equal temperament plus a somewhat less precise computation of the corresponding numerical values in 1585 or later." The developments occurred independently.

Kenneth Robinson attributes the invention of equal temperament to Zhu Zaiyu and provides textual quotations as evidence. Zhu Zaiyu is quoted as saying that, in a text dating from 1584, "I have founded a new system. I establish one foot as the number from which the others are to be extracted, and using proportions I extract them. Altogether one has to find the exact figures for the pitch-pipers in twelve operations." Kuttner disagrees and remarks that his claim "cannot be considered correct without major qualifications." Kuttner proposes that neither Zhu Zaiyu or Simon Stevin achieved equal temperament, and that neither of the two should be treated as inventors.

The origin of the Chinese pentatonic scale is traditionally ascribed to the mythical Ling Lun. Allegedly his writings discussed the equal division of the scale in the 27th century BC. However, evidence of the origins of writing in this period (the early Longshan) in China is limited to rudimentary inscriptions on oracle bones and pottery.

A complete set of bronze chime bells, among many musical instruments found in the tomb of the Marquis Yi of Zeng (early Warring States, c. 5th century BCE in the Chinese Bronze Age), covers 5 full 7 note octaves in the key of C Major, including 12 note semi-tones in the middle of the range.

An approximation for equal temperament was described by He Chengtian, a mathematician of Southern and Northern Dynasties around 400 AD. He came out with the earliest recorded approximate numerical sequence in relation to equal temperament in history: 900 849 802 758 715 677 638 601 570 536 509.5 479 450.
Historically, there was a seven-equal temperament or hepta-equal temperament practice in Chinese tradition.

Zhu Zaiyu (), a prince of the Ming court, spent thirty years on research based on the equal temperament idea originally postulated by his father. He described his new pitch theory in his "Fusion of Music and Calendar" published in 1580. This was followed by the publication of a detailed account of the new theory of the equal temperament with a precise numerical specification for 12-TET in his 5,000-page work "Complete Compendium of Music and Pitch" ("Yuelü quan shu" ) in 1584.
An extended account is also given by Joseph Needham.
Zhu obtained his result mathematically by dividing the length of string and pipe successively by ≈ 1.059463, and for pipe length by , such that after twelve divisions (an octave) the length was divided by a factor of 2:

Similarly, after 84 divisions (7 octaves) the length was divided by a factor of 128:

According to Gene Cho, Zhu Zaiyu was the first person to solve the equal temperament problem mathematically. Matteo Ricci, a Jesuit in China recorded this work in his personal journal and very likely brought it back to the West. In 1620, Zhu's work was referenced by a European mathematician. Murray Barbour said, "The first known appearance in print of the correct figures for equal temperament was in China, where Prince Tsaiyü's brilliant solution remains an enigma." The 19th-century German physicist Hermann von Helmholtz wrote in "On the Sensations of Tone" that a Chinese prince (see below) introduced a scale of seven notes, and that the division of the octave into twelve semitones was discovered in China.
Zhu Zaiyu illustrated his equal temperament theory by construction of a set of 36 bamboo tuning pipes ranging in 3 octaves, with instructions of the type of bamboo, color of paint, and detailed specification on their length and inner and outer diameters. He also constructed a 12-string tuning instrument, with a set of tuning pitch pipes hidden inside its bottom cavity. In 1890, Victor-Charles Mahillon, curator of the Conservatoire museum in Brussels, duplicated a set of pitch pipes according to Zhu Zaiyu's specification. He said that the Chinese theory of tones knew more about the length of pitch pipes than its Western counterpart, and that the set of pipes duplicated according to the Zaiyu data proved the accuracy of this theory.

One of the earliest discussions of equal temperament occurs in the writing of Aristoxenus in the 4th century BC.

Vincenzo Galilei (father of Galileo Galilei) was one of the first practical advocates of twelve-tone equal temperament. He composed a set of dance suites on each of the 12 notes of the chromatic scale in all the "transposition keys", and published also, in his 1584 "Fronimo", 24 + 1 ricercars. He used the 18:17 ratio for fretting the lute (although some adjustment was necessary for pure octaves).

Galilei's countryman and fellow lutenist Giacomo Gorzanis had written music based on equal temperament by 1567. Gorzanis was not the only lutenist to explore all modes or keys: Francesco Spinacino wrote a "Recercare de tutti li Toni" (Ricercar in all the Tones) as early as 1507.
In the 17th century lutenist-composer John Wilson wrote a set of 30 preludes including 24 in all the major/minor keys.

Henricus Grammateus drew a close approximation to equal temperament in 1518. The first tuning rules in equal temperament were given by Giovani Maria Lanfranco in his "Scintille de musica". Zarlino in his polemic with Galilei initially opposed equal temperament but eventually conceded to it in relation to the lute in his "Sopplimenti musicali" in 1588.

The first mention of equal temperament related to the twelfth root of two in the West appeared in Simon Stevin's manuscript "Van De Spiegheling der singconst" (ca. 1605), published posthumously nearly three centuries later in 1884. However, due to insufficient accuracy of his calculation, many of the chord length numbers he obtained were off by one or two units from the correct values. As a result, the frequency ratios of Simon Stevin's chords has no unified ratio, but one ratio per tone, which is claimed by Gene Cho as incorrect.

The following were Simon Stevin's chord length from "Van de Spiegheling der singconst":

A generation later, French mathematician Marin Mersenne presented several equal tempered
chord lengths obtained by Jean Beaugrand, Ismael Bouillaud, and Jean Galle.

In 1630 Johann Faulhaber published a 100 cent monochord table, which contained several errors due to his use of logarithmic tables. He did not explain how he obtained his results.

From 1450 to about 1800, plucked instrument players (lutenists and guitarists) generally favored equal temperament, and the Brossard lute Manuscript compiled in the last quarter of the 17th century contains a series of 18 preludes attributed to Bocquet written in all keys, including the last prelude, entitled "Prelude sur tous les tons", which enharmonically modulates through all keys. Angelo Michele Bartolotti published a series of passacaglias in all keys, with connecting enharmonically modulating passages. Among the 17th-century keyboard composers Girolamo Frescobaldi advocated equal temperament. Some theorists, such as Giuseppe Tartini, were opposed to the adoption of equal temperament; they felt that degrading the purity of each chord degraded the aesthetic appeal of music, although Andreas Werckmeister emphatically advocated equal temperament in his 1707 treatise published posthumously.

J. S. Bach wrote "The Well-Tempered Clavier" to demonstrate the musical possibilities of well temperament, where in some keys the consonances are even more degraded than in equal temperament. It is possible that when composers and theoreticians of earlier times wrote of the moods and "colors" of the keys, they each described the subtly different dissonances made available within a particular tuning method. However, it is difficult to determine with any exactness the actual tunings used in different places at different times by any composer. (Correspondingly, there is a great deal of variety in the particular opinions of composers about the moods and colors of particular keys.)

Twelve tone equal temperament took hold for a variety of reasons. It was a convenient fit for the existing keyboard design, and permitted total harmonic freedom at the expense of just a little impurity in every interval. This allowed greater expression through enharmonic modulation, which became extremely important in the 18th century in music of such composers as Francesco Geminiani, Wilhelm Friedemann Bach, Carl Philipp Emmanuel Bach and Johann Gottfried Müthel.

The progress of equal temperament from the mid-18th century on is described with detail in quite a few modern scholarly publications: it was already the temperament of choice during the Classical era (second half of the 18th century), and it became standard during the Early Romantic era (first decade of the 19th century), except for organs that switched to it more gradually, completing only in the second decade of the 19th century. (In England, some cathedral organists and choirmasters held out against it even after that date; Samuel Sebastian Wesley, for instance, opposed it all along. He died in 1876.)

A precise equal temperament is possible using the 17th-century Sabbatini method of splitting the octave first into three tempered major thirds. This was also proposed by several writers during the Classical era. Tuning without beat rates but employing several checks, achieving virtually modern accuracy, was already done in the first decades of the 19th century. Using beat rates, first proposed in 1749, became common after their diffusion by Helmholtz and Ellis in the second half of the 19th century. The ultimate precision was available with 2-decimal tables published by White in 1917.

It is in the environment of equal temperament that the new styles of symmetrical tonality and polytonality, atonal music such as that written with the twelve tone technique or serialism, and jazz (at least its piano component) developed and flourished.

In an equal temperament, the distance between two adjacent steps of the scale is the same interval. Because the perceived identity of an interval depends on its ratio, this scale in even steps is a geometric sequence of multiplications. (An arithmetic sequence of intervals would not sound evenly spaced, and would not permit transposition to different keys.) Specifically, the smallest interval in an equal-tempered scale is the ratio:

where the ratio "r" divides the ratio "p" (typically the octave, which is 2:1) into "n" equal parts. ("See Twelve-tone equal temperament below.")

Scales are often measured in cents, which divide the octave into 1200 equal intervals (each called a cent). This logarithmic scale makes comparison of different tuning systems easier than comparing ratios, and has considerable use in Ethnomusicology. The basic step in cents for any equal temperament can be found by taking the width of "p" above in cents (usually the octave, which is 1200 cents wide), called below "w", and dividing it into "n" parts:

In musical analysis, material belonging to an equal temperament is often given an integer notation, meaning a single integer is used to represent each pitch. This simplifies and generalizes discussion of pitch material within the temperament in the same way that taking the logarithm of a multiplication reduces it to addition. Furthermore, by applying the modular arithmetic where the modulus is the number of divisions of the octave (usually 12), these integers can be reduced to pitch classes, which removes the distinction (or acknowledges the similarity) between pitches of the same name, e.g. "c" is 0 regardless of octave register. The MIDI encoding standard uses integer note designations.

In twelve-tone equal temperament, which divides the octave into 12 equal parts, the width of a semitone, i.e. the frequency ratio of the interval between two adjacent notes, is the twelfth root of two:

This interval is divided into 100 cents.

To find the frequency, "P", of a note in 12-TET, the following definition may be used:

In this formula "P" refers to the pitch, or frequency (usually in hertz), you are trying to find. "P" refers to the frequency of a reference pitch (usually 440Hz). "n" and "a" refer to numbers assigned to the desired pitch and the reference pitch, respectively. These two numbers are from a list of consecutive integers assigned to consecutive semitones. For example, A (the reference pitch) is the 49th key from the left end of a piano (tuned to 440 Hz), and C (middle C) is the 40th key. These numbers can be used to find the frequency of C:

The intervals of 12-TET closely approximate some intervals in just intonation. The fifths and fourths are almost indistinguishably close to just, while thirds and sixths are further away.

In the following table the sizes of various just intervals are compared against their equal-tempered counterparts, given as a ratio as well as cents.

Violins, violas and cellos are tuned in perfect fifths (G – D – A – E, for violins, and C – G – D – A, for violas and cellos), which suggests that their semi-tone ratio is slightly higher than in the conventional twelve-tone equal temperament. Because a perfect fifth is in 3:2 relation with its base tone, and this interval is covered in 7 steps, each tone is in the ratio of to the next (100.28 cents), which provides for a perfect fifth with ratio of 3:2 but a slightly widened octave with a ratio of ≈ 517:258 or ≈ 2.00388:1 rather than the usual 2:1 ratio, because twelve perfect fifths do not equal seven octaves. During actual play, however, the violinist chooses pitches by ear, and only the four unstopped pitches of the strings are guaranteed to exhibit this 3:2 ratio.

Five and seven tone equal temperament (5-TET and 7-TET ), with 240 and 171 cent steps respectively, are fairly common.

A Thai xylophone measured by Morton (1974) "varied only plus or minus 5 cents," from 7-TET. According to Morton, "Thai instruments of fixed pitch are tuned to an equidistant system of seven pitches per octave ... As in Western traditional music, however, all pitches of the tuning system are not used in one mode (often referred to as 'scale'); in the Thai system five of the seven are used in principal pitches in any mode, thus establishing a pattern of nonequidistant intervals for the mode." 

Indonesian gamelans are tuned to 5-TET according to Kunst (1949), but according to Hood (1966) and McPhee (1966) their tuning varies widely, and according to Tenzer (2000) they contain stretched octaves. It is now well-accepted that of the two primary tuning systems in gamelan music, slendro and pelog, only slendro somewhat resembles five-tone equal temperament while pelog is highly unequal; however, Surjodiningrat et al. (1972) has analyzed pelog as a seven-note subset of nine-tone equal temperament (133 cent steps ).

A South American Indian scale from a preinstrumental culture measured by Boiles (1969) featured 175 cent seven-tone equal temperament, which stretches the octave slightly as with instrumental gamelan music.

5-TET and 7-TET mark the endpoints of the syntonic temperament's valid tuning range, as shown in Figure 1.

24-EDO, the quarter tone scale (or 24-TET), was a popular microtonal tuning In the 20th century probably because it represented a convenient access point for composers conditioned on standard Western 12-EDO pitch and notation practices who were also interested in microtonality. Because 24-EDO contains all of the pitches of 12-EDO plus new pitches halfway between each adjacent pair of 12-EDO pitches, they could employ the additional colors without losing any tactics available in 12-tone harmony. The fact that 24 is a multiple of 12 also made 24-EDO easy to achieve instrumentally by employing two traditional 12-EDO instruments purposely tuned a quarter-tone apart, such as two pianos, which also allowed each performer (or one performer playing a different piano with each hand) to read familiar 12-tone notation. Various composers including Charles Ives experimented with music for quarter-tone pianos.

27-EDO is the smallest EDO that uniquely represents all intervals involving the first eight harmonics. It tempers out the septimal comma but not the syntonic comma.

29-TET is the lowest number of equal divisions of the octave that produces a better perfect fifth than 12-TET. Its major third is roughly as inaccurate as 12-TET; however, it is tuned 14 cents flat rather than 14 cents sharp. It tunes the 7th, 11th, and 13th harmonics flat as well, by roughly the same amount. This means intervals such as 7:5, 11:7, 13:11, etc., are all matched extremely well in 29-TET.

31 tone equal temperament was advocated by Christiaan Huygens and Adriaan Fokker. 31-TET has a slightly less accurate fifth than 12-TET, but provides near-just major thirds, and provides decent matches for harmonics up to at least 13, of which the seventh harmonic is particularly accurate.

34 EDO gives slightly less total combined errors of approximation to the 5-limit just ratios 3:2, 5:4, 6:5, and their inversions than 31 EDO does, although the approximation of 5:4 is worse. 34 EDO doesn't approximate ratios involving prime 7 well. It contains a 600-cent tritone, since it is an even-numbered EDO.

41-TET is the second lowest number of equal divisions that produces a better perfect fifth than 12-TET. Its major third is more accurate than 12-ET and 29-ET, about 6 cents flat.

53-TET is better at approximating the traditional just consonances than 12, 19 or 31-TET, but has had only occasional use. Its extremely good perfect fifths make it interchangeable with an extended Pythagorean tuning, but it also accommodates schismatic temperament, and is sometimes used in Turkish music theory. It does not, however, fit the requirements of meantone temperaments, which put good thirds within easy reach via the cycle of fifths. In 53-TET the very consonant thirds would be reached instead by strange enharmonic relationships like C–F, as it is an example of schismatic temperament. A consequence of this is that chord progressions like I–vi–ii–V–I won't land you back where you started in 53-TET, but rather one 53-tone step flat (unless the motion by I–vi wasn't by the 5-limit minor third).

72-TET approximates many just intonation intervals well, even into the 7-limit and 11-limit, such as 7:4, 9:7, 11:5, 11:6 and 11:7. 72-TET has been taught, written and performed in practice by Joe Maneri and his students (whose atonal inclinations interestingly typically avoid any reference to just intonation whatsoever). It can be considered an extension of 12 EDO because 72 is a multiple of 12. 72 EDO has a smallest interval that is six times smaller than the smallest interval of 12 EDO and therefore contains six copies of 12 EDO starting on different pitches. It also contains three copies of 24 EDO and two copies of 36 EDO, which are themselves multiples of 12 EDO.

Other equal divisions of the octave that have found occasional use include, 15-TET, 17-TET, 19-TET and 22-TET.

2, 5, 12, 41, 53, 306, 665 and 15601 are denominators of first convergents of log(3), so 2, 5, 12, 41, 53, 306, 665 and 15601 twelfths (and fifths), being in correspondent equal temperaments equal to an integer number of octaves, are better approximation of 2, 5, 12, 41, 53, 306, 665 and 15601 just twelfths/fifths than for any equal temperaments with less tones.

1, 2, 3, 5, 7, 12, 29, 41, 53, 200... is the sequence of divisions of octave that provide better and better approximations of the perfect fifth. Related sequences contain divisions approximating other just intervals. It is noteworthy that many elements of this sequences are sums of previous elements.

This application: calculates the frequencies, approximate cents, and MIDI pitch bend values for any systems of equal division of the octave. Note that 'rounded' and 'floored' produce the same MIDI pitch bend value.

The equal-tempered version of the Bohlen–Pierce scale consists of the ratio 3:1, 1902 cents, conventionally a perfect fifth plus an octave (that is, a perfect twelfth), called in this theory a tritave (), and split into thirteen equal parts. This provides a very close match to justly tuned ratios consisting only of odd numbers. Each step is 146.3 cents (), or .

Wendy Carlos created three unusual equal temperaments after a thorough study of the properties of possible temperaments having a step size between 30 and 120 cents. These were called "alpha", "beta", and "gamma". They can be considered as equal divisions of the perfect fifth. Each of them provides a very good approximation of several just intervals. Their step sizes:
Alpha and Beta may be heard on the title track of her 1986 album "Beauty in the Beast".

In this section, "semitone" and "whole tone" may not have their usual 12-EDO meanings, as it discusses how they may be tempered in different ways from their just versions to produce desired relationships. Let the number of steps in a semitone be "s", and the number of steps in a tone be "t".

There is exactly one family of equal temperaments that fixes the semitone to any proper fraction of a whole tone, while keeping the notes in the right order (meaning that, for example, C, D, E, F, and F are in ascending order if they preserve their usual relationships to C). That is, fixing "q" to a proper fraction in the relationship "qs" = "t" also defines a unique family of one equal temperament and its multiples that fulfill this relationship.

For example, where "k" is an integer, 12"k"-EDO sets "q" = , and 19"k"-EDO sets "q" = . The smallest multiples in these families (e.g. 12 and 19 above) has the additional property of having no notes outside the circle of fifths. (This is not true in general; in 24-EDO, the half-sharps and half-flats are not in the circle of fifths generated starting from C.) The extreme cases are 5"k"-EDO, where "q" = 0 and the semitone becomes a unison, and 7"k"-EDO, where "q" = 1 and the semitone and tone are the same interval.

Once one knows how many steps a semitone and a tone are in this equal temperament, one can find the number of steps it has in the octave. An equal temperament fulfilling the above properties (including having no notes outside the circle of fifths) divides the octave into 7"t" − 2"s" steps, and the perfect fifth into 4"t" − "s" steps. If there are notes outside the circle of fifths, one must then multiply these results by "n", which is the number of nonoverlapping circles of fifths required to generate all the notes (e.g. two in 24-EDO, six in 72-EDO). (One must take the small semitone for this purpose: 19-EDO has two semitones, one being tone and the other being .)

The smallest of these families is 12"k"-EDO, and in particular 12-EDO is the smallest equal temperament that has the above properties. Additionally, it also makes the semitone exactly half a whole tone, the simplest possible relationship. These are some of the reasons why 12-EDO has become the most commonly used equal temperament. (Another reason is that 12-EDO is the smallest equal temperament to closely approximate 5-limit harmony, the next-smallest being 19-EDO.)

Each choice of fraction "q" for the relationship results in exactly one equal temperament family, but the converse is not true: 47-EDO has two different semitones, where one is tone and the other is , which are not complements of each other like in 19-EDO ( and ). Taking each semitone results in a different choice of perfect fifth.

The diatonic tuning in twelve equal can be generalized to any regular diatonic tuning dividing the octave as a sequence of steps TTSTTTS (or a rotation of it) with all the T's and all the S's the same size and the S's smaller than the T's. In twelve equal the S is the semitone and is exactly half the size of the tone T. When the S's reduce to zero the result is TTTTT or a five tone equal temperament, As the semitones get larger, eventually the steps are all the same size, and the result is in seven tone equal temperament. These two end points are not included as regular diatonic tunings.

The notes in a regular diatonic tuning are connected together by a cycle of seven tempered fifths. The twelve tone system similarly generalizes to a sequence CDCDDCDCDCDD (or a rotation of it) of chromatic and diatonic semitones connected together in a cycle of twelve fifths. In this case, seven equal is obtained in the limit as the size of C tends to zero and five equal is the limit as D tends to zero while twelve equal is of course the case C = D.

Some of the intermediate sizes of tones and semitones can also be generated in equal temperament systems. For instance if the diatonic semitone is double the size of the chromatic semitone, i.e. D = 2*C the result is nineteen equal with one step for the chromatic semitone, two steps for the diatonic semitone and three steps for the tone and the total number of steps 5*T + 2*S = 15 + 4 = 19 steps. The resulting twelve tone system closely approximates to the historically important 1/3 comma meantone.

If the chromatic semitone is two thirds of the size of the diatonic semitone, i.e. C = (2/3)*D, the result is thirty one equal, with two steps for the chromatic semitone, three steps for the diatonic semitone, and five steps for the tone where 5*T + 2*S = 25 + 6 = 31 steps. The resulting twelve tone system closely approximates to the historically important 1/4 comma meantone.






</doc>
<doc id="10310" url="https://en.wikipedia.org/wiki?curid=10310" title="Edward Gibbon">
Edward Gibbon

Edward Gibbon FRS (; 8 May 173716 January 1794) was an English historian, writer and Member of Parliament. His most important work, "The History of the Decline and Fall of the Roman Empire", was published in six volumes between 1776 and 1788 and is known for the quality and irony of its prose, its use of primary sources, and its open criticism of organised religion.

Edward Gibbon was born in 1737, the son of Edward and Judith Gibbon at Lime Grove, in the town of Putney, Surrey. He had six siblings: five brothers and one sister, all of whom died in infancy. His grandfather, also named Edward, had lost all of his assets as a result of the South Sea Bubble stock market collapse in 1720, but eventually regained much of his wealth, so that Gibbon's father was able to inherit a substantial estate. One of his grandparents, Catherine Acton, descended from Sir Walter Acton, 2nd Baronet.

As a youth, Gibbon's health was under constant threat. He described himself as "a puny child, neglected by my Mother, starved by my nurse". At age nine, he was sent to Dr. Woddeson's school at Kingston upon Thames (now Kingston Grammar School), shortly after which his mother died. He then took up residence in the Westminster School boarding house, owned by his adored "Aunt Kitty", Catherine Porten. Soon after she died in 1786, he remembered her as rescuing him from his mother's disdain, and imparting "the first rudiments of knowledge, the first exercise of reason, and a taste for books which is still the pleasure and glory of my life". By 1751, Gibbon's reading was already extensive and certainly pointed toward his future pursuits: Laurence Echard's "Roman History" (1713), William Howel(l)'s "An Institution of General History" (1680–85), and several of the 65 volumes of the acclaimed "Universal History from the Earliest Account of Time" (1747–1768).

Following a stay at Bath in 1752 to improve his health, at the age of 15 Gibbon was sent by his father to Magdalen College, Oxford, where he was enrolled as a gentleman-commoner. He was ill-suited, however, to the college atmosphere and later rued his 14 months there as the "most idle and unprofitable" of his life. Because he himself says so in his autobiography, it used to be thought that his penchant for "theological controversy" (his aunt's influence) fully bloomed when he came under the spell of the deist or rationalist theologian Conyers Middleton (1683–1750), the author of "Free Inquiry into the Miraculous Powers" (1749). In that tract, Middleton denied the validity of such powers; Gibbon promptly objected, or so the argument used to run. The product of that disagreement, with some assistance from the work of Catholic Bishop Jacques-Bénigne Bossuet (1627–1704), and that of the Elizabethan Jesuit Robert Parsons (1546–1610), yielded the most memorable event of his time at Oxford: his conversion to Roman Catholicism on 8 June 1753. He was further "corrupted" by the 'free thinking' deism of the playwright/poet couple David and Lucy Mallet; and finally Gibbon's father, already "in despair," had had enough. David Womersley has shown, however, that Gibbon's claim to having been converted by a reading of Middleton is very unlikely, and was introduced only into the final draft of the "Memoirs" in 1792–93. Bowersock suggests that Gibbon fabricated the Middleton story retrospectively in his anxiety about the impact of the French Revolution and Edmund Burke's claim that it was provoked by the French "philosophes", so influential on Gibbon.

Within weeks of his conversion, the adolescent was removed from Oxford and sent to live under the care and tutelage of Daniel Pavillard, Reformed pastor of Lausanne, Switzerland. It was here that he made one of his life's two great friendships, that of Jacques Georges Deyverdun (the French-language translator of Goethe's "The Sorrows of Young Werther"), and that of John Baker Holroyd (later Lord Sheffield). Just a year and a half later, after his father threatened to disinherit him, on Christmas Day, 1754, he reconverted to Protestantism. "The various articles of the Romish creed," he wrote, "disappeared like a dream". He remained in Lausanne for five intellectually productive years, a period that greatly enriched Gibbon's already immense aptitude for scholarship and erudition: he read Latin literature; travelled throughout Switzerland studying its cantons' constitutions; and studied the works of Hugo Grotius, Samuel von Pufendorf, John Locke, Pierre Bayle, and Blaise Pascal.

He also met the one romance in his life: the daughter of the pastor of Crassy, a young woman named Suzanne Curchod, who was later to become the wife of Louis XVI's finance minister Jacques Necker, and the mother of Madame de Staël. The two developed a warm affinity; Gibbon proceeded to propose marriage, but ultimately wedlock was out of the question, blocked both by his father's staunch disapproval and Curchod's equally staunch reluctance to leave Switzerland. Gibbon returned to England in August 1758 to face his father. There could be no refusal of the elder's wishes. Gibbon put it this way: "I sighed as a lover, I obeyed as a son." He proceeded to cut off all contact with Curchod, even as she vowed to wait for him. Their final emotional break apparently came at Ferney, France in early 1764, though they did see each other at least one more time a year later.

Upon his return to England, Gibbon published his first book, "Essai sur l'Étude de la Littérature" in 1761, which produced an initial taste of celebrity and distinguished him, in Paris at least, as a man of letters. From 1759 to 1770, Gibbon served on active duty and in reserve with the South Hampshire militia, his deactivation in December 1762 coinciding with the militia's dispersal at the end of the Seven Years' War. The following year he embarked on the Grand Tour, which included a visit to Rome. In his autobiography Gibbon vividly records his rapture when he finally neared "the great object of [my] pilgrimage":

...at the distance of twenty-five years I can neither forget nor express the strong emotions which agitated my mind as I first approached and entered the "eternal City". After a sleepless night, I trod, with a lofty step the ruins of the Forum; each memorable spot where Romulus "stood", or Tully spoke, or Caesar fell, was at once present to my eye; and several days of intoxication were lost or enjoyed before I could descend to a cool and minute investigation.

And it was here that Gibbon first conceived the idea of composing a history of the city, later extended to the entire empire, a moment known to history as the "Capitoline vision":

It was at Rome, on the fifteenth of October 1764, as I sat musing amidst the ruins of the Capitol, while the barefooted fryars were singing Vespers in the temple of Jupiter, that the idea of writing the decline and fall of the City first started to my mind.

Womersley ("Oxford Dictionary of National Biography", p. 12) notes the existence of "good reasons" to doubt the statement's accuracy. Elaborating, Pocock ("Classical History," ¶ #2) refers to it as a likely "creation of memory" or a "literary invention", given that Gibbon, in his autobiography, claimed that his journal dated the reminiscence to 15 October, when in fact the journal gives no date.

In June 1765, Gibbon returned to his father's house, and remained there until the latter's death in 1770. These years were considered by Gibbon as the worst five of his life, but he tried to remain busy by making early attempts towards writing full histories. His first historical narrative known as the "History of Switzerland", which represented Gibbon's love for Switzerland, was never published nor finished. Even under the guidance of Deyverdun (a German translator for Gibbons), Gibbon became too critical of himself, and completely abandoned the project, only writing 60 pages of text. However, after Gibbon's death, his writings on Switzerland's history were discovered and published by Lord Sheffield in 1815. Soon after abandoning his "History of Switzerland", Gibbon made another attempt towards completing a full history.

His second work, "Memoires Litteraires de la Grande Bretagne", was a two-volume set which described the literary and social conditions of England at the time, such as Lord Lyttelton's history of Henry II and Nathaniel Lardner's "The Credibility of the Gospel History". Gibbon's "Memoires Litteraires" failed to gain any notoriety, and was considered a flop by fellow historians and literary scholars.

After tending to his father's estate—which was by no means in good condition—there remained quite enough for Gibbon to settle fashionably in London at 7 Bentinck Street, free of financial concern. By February 1773, he was writing in earnest, but not without the occasional self-imposed distraction. He took to London society quite easily, joined the better social clubs, including Dr. Johnson's Literary Club, and looked in from time to time on his friend Holroyd in Sussex. He succeeded Oliver Goldsmith at the Royal Academy as 'professor in ancient history' (honorary but prestigious). In late 1774, he was initiated a freemason of the Premier Grand Lodge of England.

He was also, perhaps least productively in that same year, 1774, returned to the House of Commons for Liskeard, Cornwall through the intervention of his relative and patron, Edward Eliot. He became the archetypal back-bencher, benignly "mute" and "indifferent," his support of the Whig ministry invariably automatic. Gibbon's indolence in that position, perhaps fully intentional, subtracted little from the progress of his writing. Gibbon lost the Liskeard seat in 1780 when Eliot joined the opposition, taking with him "the Electors of Leskeard [who] are commonly of the same opinion as Mr. El[l]iot." (Murray, p. 322.) The following year, owing to the good grace of Prime Minister Lord North, he was again returned to Parliament, this time for Lymington on a by-election.

After several rewrites, with Gibbon "often tempted to throw away the labours of seven years," the first volume of what was to become his life's major achievement, "The History of the Decline and Fall of the Roman Empire", was published on 17 February 1776. Through 1777, the reading public eagerly consumed three editions, for which Gibbon was rewarded handsomely: two-thirds of the profits, amounting to approximately £1,000. Biographer Leslie Stephen wrote that thereafter, "His fame was as rapid as it has been lasting." And as regards this first volume, "Some warm praise from David Hume overpaid the labour of ten years."

Volumes II and III appeared on 1 March 1781, eventually rising "to a level with the previous volume in general esteem." Volume IV was finished in June 1784; the final two were completed during a second Lausanne sojourn (September 1783 to August 1787) where Gibbon reunited with his friend Deyverdun in leisurely comfort. By early 1787, he was "straining for the goal" and with great relief the project was finished in June. Gibbon later wrote:

Volumes IV, V, and VI finally reached the press in May 1788, their publication having been delayed since March so it could coincide with a dinner party celebrating Gibbon's 51st birthday (the 8th). Mounting a bandwagon of praise for the later volumes were such contemporary luminaries as Adam Smith, William Robertson, Adam Ferguson, Lord Camden, and Horace Walpole. Smith remarked that Gibbon's triumph had positioned him "at the very head of [Europe's] literary tribe."

In November of that year, he was elected a Fellow of the Royal Society, the main proposer being his good friend Lord Sheffield.

The years following Gibbon's completion of "The History" were filled largely with sorrow and increasing physical discomfort. He had returned to London in late 1787 to oversee the publication process alongside Lord Sheffield. With that accomplished, in 1789 it was back to Lausanne only to learn of and be "deeply affected" by the death of Deyverdun, who had willed Gibbon his home, La Grotte. He resided there with little commotion, took in the local society, received a visit from Sheffield in 1791, and "shared the common abhorrence" of the French Revolution.

In a letter to Lord Sheffield on 5 February 1791, Gibbon praised Burke's "Reflections on the Revolution in France":

In 1793, word came of Lady Sheffield's death; Gibbon immediately left Lausanne and set sail to comfort a grieving but composed Sheffield. His health began to fail critically in December, and at the turn of the new year, he was on his last legs.

Gibbon is believed to have suffered from an extreme case of scrotal swelling, probably a hydrocele testis, a condition which causes the scrotum to swell with fluid in a compartment overlying either testicle. In an age when close-fitting clothes were fashionable, his condition led to a chronic and disfiguring inflammation that left Gibbon a lonely figure. As his condition worsened, he underwent numerous procedures to alleviate the condition, but with no enduring success. In early January, the last of a series of three operations caused an unremitting peritonitis to set in and spread, from which he died.

The "English giant of the Enlightenment" finally succumbed at 12:45 pm, 16 January 1794 at age 56. He was buried in the Sheffield Mausoleum attached to the north transept of the Church of St Mary and St Andrew, Fletching, East Sussex, having died in Fletching while staying with his great friend, Lord Sheffield. Gibbon's estate was valued at approx. £26,000. He left most of his property to cousins. As stipulated in his will, Sheffield oversaw the sale of his library at auction to William Beckford for £950.

Gibbon's work has been criticised for its scathing view of Christianity as laid down in chapters XV and XVI, a situation which resulted in the banning of the book in several countries. Gibbon's alleged crime was disrespecting, and none too lightly, the character of sacred Christian doctrine, by "treat[ing] the Christian church as a phenomenon of general history, not a special case admitting supernatural explanations and disallowing criticism of its adherents". More specifically, the chapters excoriated the church for "supplanting in an unnecessarily destructive way the great culture that preceded it" and for "the outrage of [practicing] religious intolerance and warfare".
Gibbon, though assumed to be entirely anti-religion, was actually supportive to some extent, insofar as it did not obscure his true endeavour – a history that was not influenced and swayed by official church doctrine. Although the most famous two chapters are heavily ironical and cutting about religion, it is not utterly condemned, and its purported truth and rightness are upheld however thinly.

Gibbon, in letters to Holroyd and others, expected some type of church-inspired backlash, but the utter harshness of the ensuing torrents far exceeded anything he or his friends could possibly have anticipated. Contemporary detractors such as Joseph Priestley and Richard Watson stoked the nascent fire, but the most severe of these attacks was an "acrimonious" piece by the young cleric, Henry Edwards Davis.
Gibbon subsequently published his "Vindication" in 1779, in which he categorically denied Davis' "criminal accusations", branding him a purveyor of "servile plagiarism." Davis followed Gibbon's "Vindication" with yet another reply (1779).

Gibbon's apparent antagonism to Christian doctrine spilled over into the Jewish faith, leading to charges of anti-Semitism. For example, he wrote:

From the reign of Nero to that of Antoninus Pius, the Jews discovered a fierce impatience of the dominion of Rome, which repeatedly broke out in the most furious massacres and insurrections. Humanity is shocked at the recital of the horrid cruelties which they committed in the cities of Egypt, of Cyprus, and of Cyrene, where they dwelt in treacherous friendship with the unsuspecting natives; and we are tempted to applaud the severe retaliation which was exercised by the arms of legions against a race of fanatics, whose dire and credulous superstition seemed to render them the implacable enemies not only of the Roman government, but also of humankind.

Edward Gibbon's central thesis, that Rome fell due to embracing Christianity, was never accepted by mainstream scholars.

Gibbon is considered to be a son of the Enlightenment and this is reflected in his famous verdict on the history of the Middle Ages: "I have described the triumph of barbarism and religion." However, politically, he aligned himself with the conservative Edmund Burke's rejection of the democratic movements of the time as well as with Burke's dismissal of the "rights of man."

Gibbon's work has been praised for its style, his piquant epigrams and its effective irony. Winston Churchill memorably noted, "I set out upon...Gibbon's "Decline and Fall of the Roman Empire" [and] was immediately dominated both by the story and the style. ...I devoured Gibbon. I rode triumphantly through it from end to end and enjoyed it all." Churchill modelled much of his own literary style on Gibbon's. Like Gibbon, he dedicated himself to producing a "vivid historical narrative, ranging widely over period and place and enriched by analysis and reflection."

Unusually for the 18th century, Gibbon was never content with secondhand accounts when the primary sources were accessible (though most of these were drawn from well-known printed editions). "I have always endeavoured," he says, "to draw from the fountain-head; that my curiosity, as well as a sense of duty, has always urged me to study the originals; and that, if they have sometimes eluded my search, I have carefully marked the secondary evidence, on whose faith a passage or a fact were reduced to depend." In this insistence upon the importance of primary sources, Gibbon is considered by many to be one of the first modern historians:

In accuracy, thoroughness, lucidity, and comprehensive grasp of a vast subject, the 'History' is unsurpassable. It is the one English history which may be regarded as definitive...Whatever its shortcomings the book is artistically imposing as well as historically unimpeachable as a vast panorama of a great period.

The subject of Gibbon's writing, as well as his ideas and style, have influenced other writers. Besides his influence on Churchill, Gibbon was also a model for Isaac Asimov in his writing of "The Foundation Trilogy", which he said involved "a little bit of cribbin' from the works of Edward Gibbon".

Evelyn Waugh admired Gibbon's style, but not his secular viewpoint. In Waugh's 1950 novel "Helena", the early Christian author Lactantius worried about the possibility of "'a false historian, with the mind of Cicero or Tacitus and the soul of an animal,' and he nodded towards the gibbon who fretted his golden chain and chattered for fruit."

J. C. Stobart, author of "The Grandeur that was Rome" (1911), wrote of Gibbon that "The mere notion of empire continuing to decline and fall for five centuries is ridiculous...this is one of the cases which prove that History is made not so much by heroes or natural forces as by historians."


Most of this article, including quotations unless otherwise noted, has been adapted from Stephen's entry on Edward Gibbon in the "Dictionary of National Biography".






</doc>
<doc id="10312" url="https://en.wikipedia.org/wiki?curid=10312" title="East Pakistan">
East Pakistan

East Pakistan was the eastern provincial wing of Pakistan between 1955 and 1971, covering the territory of the modern country Bangladesh. Its land borders were with India and Burma, with a coastline on the Bay of Bengal.
East Pakistan was renamed from East Bengal by the One Unit scheme of Prime Minister Mohammad Ali of Bogra. The Constitution of Pakistan of 1956 replaced the British monarchy with an Islamic republic. Bengali politician H. S. Suhrawardy served as the Prime Minister of Pakistan between 1956 and 1957. A Bengali bureaucrat Iskandar Mirza became the first President of Pakistan. The 1958 Pakistani coup d'état brought general Ayub Khan to power. Khan replaced Mirza as president and launched a crackdown against pro-democracy leaders. Khan enacted the Constitution of Pakistan of 1962 which ended universal suffrage. By 1966, Sheikh Mujibur Rahman emerged as the preeminent opposition leader in Pakistan and launched the six point movement for autonomy and democracy. The 1969 uprising in East Pakistan contributed to Ayub Khan's overthrow. Another general, Yahya Khan, usurped the presidency and enacted martial law. In 1970, a major tropical cyclone struck East Pakistan, causing major casualties. In the same year, Yahya Khan organized Pakistan's first federal general election. The Awami League emerged as the single largest party, followed by the Pakistan Peoples Party. The military junta stalled in accepting the results, leading to civil disobedience, the Bangladesh Liberation War and the 1971 Bangladesh genocide. East Pakistan seceded with the help of India.

The East Pakistan Provincial Assembly was the legislative body of the territory.

Due to the strategic importance of East Pakistan, the Pakistani union was a member of the Southeast Asia Treaty Organization. The economy of East Pakistan grew at an average of 2.6% between 1960 and 1965. The federal government invested more funds and foreign aid in West Pakistan, even though East Pakistan generated a major share of exports. However, President Ayub Khan did implement significant industrialization in East Pakistan. The Kaptai Dam was built in 1965. The Eastern Refinery was established in Chittagong. Dacca was declared as the "second capital" of Pakistan and planned as the home of the national parliament. The government recruited American architect Louis Kahn to design the national assembly complex in Dacca.

In 1955, Prime Minister Mohammad Ali Bogra implemented the One Unit scheme which merged the four western provinces into a single unit called West Pakistan while East Bengal was renamed as East Pakistan.
Pakistan ended its dominion status and adopted a republican constitution in 1956, which proclaimed an Islamic republic. The populist leader H. S. Suhrawardy of East Pakistan was appointed as the Prime Minister of Pakistan. As soon as he became the prime minister, Suhrawardy initiated a legal work reviving the joint electorate system. There was a strong opposition and resentment to the joint electorate system in West Pakistan. The Muslim League had taken the cause to the public and began calling for implementation of separate electorate system. In contrast to West Pakistan, the joint electorate was highly popular in East Pakistan. The tug of war with the Muslim League to establish the appropriate electorate caused problems for his government.

The constitutionally obliged National Finance Commission Program (NFC Program) was immediately suspended by Prime Minister Suhrawardy despite the reserves of the four provinces of the West Pakistan in 1956. Suhrawardy advocated for the USSR-based Five-Year Plans to centralize the national economy. In this view, the East Pakistan's economy was quickly centralized and all major economic planning shifted to West Pakistan.

Efforts leading to centralizing the economy was met with great resistance in West Pakistan when the elite monopolist and the business community angrily refused to oblige to his policies. The business community in Karachi began its political struggle to undermine any attempts of financial distribution of the US$10 million ICA aid to the better part of the East Pakistan and to set up a consolidated national shipping corporation. In the financial cities of West Pakistan, such as Karachi, Lahore, Quetta, and Peshawar, there were series of major labour strikes against the economic policies of Suhrawardy supported by the elite business community and the private sector.

Furthermore, in order to divert attention from the controversial One Unit Program, Prime Minister Suhrawardy tried to end the crises by calling a small group of investors to set up small business in the country. Despite many initiatives and holding off the NFC Award Program, Suhrawardy's political position and image deteriorated in the four provinces in West Pakistan. Many nationalist leaders and activists of the Muslim League were dismayed with the suspension of the constitutionally obliged NFC Program. His critics and Muslim League leaders observed that with the suspension of NFC Award Program, Suhrawardy tried to give more financial allocations, aids, grants, and opportunity to East-Pakistan than West Pakistan, including West Pakistan's four provinces. During the last days of his Prime ministerial years, Suhrawardy tried to remove the economic disparity between the Eastern and Western wings of the country but to no avail. He also tried unsuccessfully to alleviate the food shortage in the country.

Suhrawardy strengthened relations with the United States by reinforcing Pakistani membership in the Central Treaty Organization and Southeast Asia Treaty Organization. Suhrawardy also promoted relations with the People’s Republic of China. 
His contribution in formulating the 1956 constitution of Pakistan was substantial as he played a vital role in incorporating provisions for civil liberties and universal adult franchise in line with his adherence to parliamentary form of liberal democracy.

In 1958, President Iskandar Mirza enacted martial law as part of a military coup by the Pakistan Army's chief Ayub Khan. Roughly after two weeks, President Mirza's relations with Pakistan Armed Forces deteriorated leading Army Commander General Ayub Khan relieving the president from his presidency and forcefully exiling President Mirza to the United Kingdom. General Ayub Khan justified his actions after appearing on national radio declaring that: "the armed forces and the people demanded a clean break with the past...". Until 1962, the martial law continued while Field Marshal Ayub Khan purged a number of politicians and civil servants from the government and replaced them with military officers. Ayub called his regime a "revolution to clean up the mess of black marketing and corruption.". Khan replaced Mirza as president and became the country’s strongman for eleven years. Martial law continued until 1962 when the government of Field Marshal Ayub Khan commissioned a constitutional bench under Chief Justice of Pakistan Muhammad Shahabuddin, composed of ten senior justices, each five from East Pakistan and five from West Pakistan. On 6 May 1961, the commission sent its draft to President Ayub Khan. He thoroughly examined the draft while consulting with his cabinet.

In January 1962, the cabinet finally approved the text of the new constitution, promulgated by President Ayub Khan on 1 March 1962, which came into effect on 8 June 1962. Under the 1962 constitution, Pakistan became a presidential republic. Universal suffrage was abolished in favor of a system dubbed 'Basic Democracy'. Under the system, an electoral college would be responsible for electing the president and national assembly. The 1962 constitution created a gubernatorial system in West and East Pakistan. Each provinces ran their own separate provincial gubernatorial governments. The constitution defined a division of powers between the central government and the provinces. Fatima Jinnah received strong support in East Pakistan during her failed bid to unseat Ayub Khan in the 1965 presidential election.

Dacca was declared as the "second capital" of Pakistan in 1962. It was designated as the legislative capital and Louis Kahn was tasked with designing a national assembly complex. Dacca's population increased in the 1960s. Seven natural gas fields were tapped in the province. The petroleum industry developed as the Eastern Refinery was established in the port city of Chittagong.

In 1966, Awami League leader Sheikh Mujibur Rahman announced the six point movement in Lahore. The movement demanded greater provincial autonomy and the restoration of democracy in Pakistan. Rahman was indicted for treason during the Agartala Conspiracy Case after launching the six point movement. He was released in the 1969 uprising in East Pakistan, which ousted Ayub Khan from the presidency. Below includes the historical six points:-

Ayub Khan was replaced by general Yahya Khan who became the Chief Martial Law Administrator. Khan organized the Pakistani general election, 1970. The 1970 Bhola cyclone was one of the deadliest natural disasters of the 20th century. The cyclone claimed half a million lives. The disastrous effects of the cyclone caused huge resentment against the federal government. After a decade of military rule, East Pakistan was a hotbed of Bengali nationalism. There were open calls for self-determination.
When the federal general election was held, the Awami League emerged as the single largest party in the Pakistani parliament. The League won 167 out of 169 seats in East Pakistan, thereby crossing the half way mark of 150 in the 300-seat National Assembly of Pakistan. In theory, this gave the League the right to form a government under the Westminster tradition. But the League failed to win a single seat in West Pakistan, where the Pakistan Peoples Party emerged as the single largest party with 81 seats. The military junta stalled the transfer of power and conducted prolonged negotiations with the League. A civil disobedience movement erupted across East Pakistan demanding the convening of parliament. Rahman announced a struggle for independence from Pakistan during a speech on 7 March 1971. Between 7–26 March, East Pakistan was virtually under the popular control of the Awami League. On Pakistan's Republic Day on 23 March 1971, the first flag of Bangladesh was hoisted in many East Pakistani households. The Pakistan Army launched a crackdown on 26 March, including Operation Searchlight and the 1971 Dhaka University massacre. This led to the Bangladeshi Declaration of Independence.
As the Bangladesh Liberation War and the 1971 Bangladesh genocide continued for nine months, East Pakistani military units like the East Bengal Regiment and the East Pakistan Rifles defected to form the Bangladesh Forces. The Provisional Government of Bangladesh allied with neighboring India which intervened in the final two weeks of the war and secured the surrender of Pakistan.

With Ayub Khan ousted from office in 1969, Commander of the Pakistani Army, General Yahya Khan became the country's second ruling chief martial law administrator. Both Bhutto and Mujib strongly disliked General Khan, but patiently endured him and his government as he had promised to hold an election in 1970. During this time, strong nationalistic sentiments in East Pakistan were perceived by the Pakistani Armed Forces and the central military government. Therefore, Khan and his military government wanted to divert the nationalistic threats and violence against non-East Pakistanis. The Eastern Military High Command was under constant pressure from the Awami League, and requested an active duty officer to control the command under such extreme pressure. The high flag rank officers, junior officers and many high command officers from the Pakistan's Armed Forces were highly cautious about their appointment in East-Pakistan, and the assignment of governing East Pakistan and appointment of an officer was considered highly difficult for the Pakistan High Military Command.
East Pakistan's Armed Forces, under the military administrations of Major-General Muzaffaruddin and Lieutenant-General Sahabzada Yaqub Khan, used an excessive amount of show of military force to curb the uprising in the province. With such action, the situation became highly critical and civil control over the province slipped away from the government. On 24 March, dissatisfied with the performance of his generals, Yahya Khan removed General Muzaffaruddin and General Yaqub Khan from office on 1 September 1969. The appointment of a military administrator was considered quite difficult and challenging with the crisis continually deteriorating. Vice-Admiral Syed Mohammad Ahsan, Chief of Naval Staff of Pakistan Navy, had previously served as political and military adviser of East Pakistan to former President Ayub Khan. Having such a strong background in administration, and being an expert on East Pakistan affairs, General Yahya Khan appointed Vice-Admiral Syed Mohammad Ahsan as Martial Law Administrator, with absolute authority in his command. He was relieved as Chief of Naval Staff, and received extension from the government. On 1 September Admiral Ahsan assumed the command of the Eastern Military High Command, and became a unified commander of Pakistan Armed Forces in East Pakistan. Under his command, the Pakistani Armed Forces were removed from the cities and deployed along the border. The rate of violence in East Pakistan dropped, nearly coming to an end. Civil rule improved and stabilised in East Pakistan under Martial Law Administrator Admiral Ahsan's era.

The tense relations between East and West Pakistan reached a climax in 1970 when the Awami League, the largest East Pakistani political party, led by Sheikh Mujibur Rahman, (Mujib), won a landslide victory in the national elections in East Pakistan. The party won 160 of the 162 seats allotted to East Pakistan, and thus a majority of the 300 seats in the Parliament. This gave the Awami League the constitutional right to form a government without forming a coalition with any other party. Khan invited Mujib to Rawalpindi to take the charge of the office, and negotiations took place between the military government and the Awami Party. Bhutto was shocked with the results, and threatened his fellow Peoples Party members if they attended the inaugural session at the National Assembly, famously saying he would "break the legs" of any member of his party who dared enter and attend the session. However, fearing East Pakistani separatism, Bhutto demanded Mujib to form a coalition government. After a secret meeting held in Larkana, Mujib agreed to give Bhutto the office of presidency with Mujib as prime minister. General Yahya Khan and his military government were kept unaware of these developments and under pressure from his own military government, refused to allow Rahman to become the Prime Minister of Pakistan. This increased agitation for greater autonomy in East Pakistan. The Military Police arrested Mujib and Bhutto and placed them in Adiala Jail in Rawalpindi. The news spread like a fire in both East and West Pakistan, and the struggle for independence began in East Pakistan.

The senior high command officers in Pakistan Armed Forces, and Zulfikar Ali Bhutto, began to pressure General Yahya Khan to take armed action against Mujib and his party. Bhutto later distanced himself from Yahya Khan after he was arrested by Military Police along with Mujib. Soon after the arrests, a high level meeting was chaired by Yahya Khan. During the meeting, high commanders of Pakistan Armed Forces unanimously recommended an armed and violent military action. East Pakistan's Martial Law Administrator Admiral Ahsan, unified commander of Eastern Military High Command (EMHC), and Air Marshal Mitty Masud, Commander of Eastern Air Force Command (EAFC), were the only officers to object to the plans. When it became obvious that a military action in East Pakistan was inevitable, Admiral Ahsan resigned from his position as martial law administrator in protest, and immediately flew back to Karachi, West Pakistan. Disheartened and isolated, Admiral Ahsan took early retirement from the Navy and quietly settled in Karachi. Once Operation Searchlight and Operation Barisal commenced, Air Marshal Masud flew to West Pakistan, and unlike Admiral Ahsan, tried to stop the violence in East Pakistan. When he failed in his attempts to meet General Yahya Khan, Masud too resigned from his position as Commander of Eastern Air Command, and took retirement from Air Force.

Lieutenant-General Sahabzada Yaqub Khan was sent into East Pakistan in emergency, following a major blow of the resignation of Vice Admiral Ahsan. General Yaqub temporarily assumed the control of the province, as he was made the unified commander of Pakistan Armed Forces. General Yaqub mobilised the entire major forces in East Pakistan, and were re-deployed in East Pakistan.

Sheikh Mujibur Rahman made a declaration of independence at Dacca on 26 March 1971. All major Awami League leaders including elected leaders of National Assembly and Provincial Assembly fled to neighbouring India and an exile government was formed headed by Mujibur Rahman. While he was in Pakistan Prison, Syed Nazrul Islam was the acting president with Tazuddin Ahmed as the prime minister. The exile government took oath on 17 April 1971 at Mujib Nagar, within East Pakistan territory of Kustia district and formally formed the government. Colonel MOG Osmani was appointed the Commander in Chief of Liberation Forces and whole East Pakistan was divided into eleven sectors headed by eleven sector commanders. All sector commanders were Bengali officers who had defected from the Pakistan Army. This started the Bangladesh Liberation War in which the freedom fighters, joined in December 1971 by 400,000 Indian soldiers, faced the Pakistani Armed Forces of 365,000 plus Paramilitary and collaborationist forces. An additional approximately 25,000 ill-equipped civilian volunteers and police forces also sided with the Pakistan Armed Forces. Bloody guerrilla warfare ensued in East Pakistan.

The Pakistan Armed Forces were unable to counter such threats. Poorly trained and inexperienced in guerrilla tactics, Pakistan Armed Forces and their assets were defeated by the Bangladesh Liberation Forces. On April 1971, Lieutenant-General Tikka Khan succeeded General Yaqub Khan as Commander of unified forces. General Tikka Khan led the massive violent and massacre campaigns in the region. He is held responsible for killing hundreds of thousands of Bengali people in East Pakistan, mostly civilians and unarmed peoples. For his role, General Tikka Khan gained the title as "Butcher of Bengal". General Khan faced an international reaction against Pakistan, and therefore, General Tikka was removed as Commander of Eastern front. He installed a civilian administration under Abdul Motaleb Malik on 31 August 1971, which proved to be ineffective. However, during the meeting, with no high officers willing to assume the command of East Pakistan, Lieutenant-General Amir Abdullah Khan Niazi volunteered for the command of East Pakistan. Inexperienced and the large magnitude of this assignment, the government sent Vice-Admiral Mohammad Shariff as second-in-command of General Niazi. Admiral Shariff served as the deputy unified commander of Pakistan Armed Forces in East Pakistan. However, General Niazi proved to be a failure and ineffective ruler. Therefore, General Niazi and Air Marshal Enamul Haque, Commander of Eastern Air Force Command (EAFC), failed to launch any operation in East Pakistan against Indian or its allies. Except Admiral Shariff who continued to press pressure on Indian Navy until the end of the conflict. Admiral Shariff's effective plans made it nearly impossible for Indian Navy to land its naval forces on the shores of East Pakistan. The Indian Navy was unable to land forces in East Pakistan and the Pakistan Navy was still offering resistance. The Indian Army, entered East Pakistan from all three directions of the province. The Indian Navy then decided to wait near the Bay of Bengal until the Army reached the shore.

The Indian Air Force dismantled the capability of Pakistan Air Force in East Pakistan. Air Marshal Enamul Haque, Commander of Eastern Air Force Command (EAFC), failed to offer any serious resistance to the actions of the Indian Air Force. For most part of the war, the IAF enjoyed complete dominance in the skies over East Pakistan.

On 16 December 1971, the Pakistan Armed Forces surrendered to the joint liberation forces of Mukti Bahini and the Indian army, headed by Lieutenant-General Jagjit Singh Arora, the General Officer Commanding-in-Chief (GOC-in-C) of the Eastern Command of the Indian Army. Lieutenant General AAK Niazi, the last unified commander of Pakistan Armed Forces' Eastern Military High Command, signed the Instrument of Surrender at about 4:31 pm. Over 93,000 personnel, including Lt. General Niazi and Admiral Shariff, were taken as prisoners of war.

On 16 December 1971, East Pakistan was liberated from Pakistan as the newly independent state of Bangladesh. The Eastern Military High Command, civilian institutions and paramilitary forces were disbanded.

In contrast to the desert and rugged mountainous terrain of West Pakistan, East Pakistan featured the world's largest delta, 700 rivers and tropical hilly jungles.
East Pakistan inherited districts from British Bengal. In 1960, Lower Tippera was renamed as Comilla. In 1969, new districts were created with Tangail separated from Mymensingh and Patuakhali from Barisal. East Pakistan's districts are listed in the following.
East Pakistan's divisions are listed in the following.

At the time of the Partition of British India, East Bengal had a plantation economy. The Chittagong Tea Auction was established in 1949 as the region was home to the world's largest tea plantations. The East Pakistan Stock Exchange Association was established in 1954. Many wealthy Muslim immigrants from India, Burma and former British colonies settled in East Pakistan. The Ispahani family, Africawala brothers and the Adamjee family were pioneers of industrialization in the region. Many of modern Bangladesh's leading companies were born in the East Pakistan period.
An airline founded in British Bengal, Orient Airways, launched the vital air link between East and West Pakistan with DC-3 aircraft on the Dacca-Calcutta-Delhi-Karachi route. Orient Airways later evolved into Pakistan International Airlines, whose first chairman was the East Pakistan-based industrialist Mirza Ahmad Ispahani.
By the 1950s, East Bengal surpassed West Bengal in having the largest jute industries in the world. The Adamjee Jute Mills was the largest jute processing plant in history and its location in Narayanganj was nicknamed the "Dundee of the East". The Adamjees were descendants of Sir Haji Adamjee Dawood, who made his fortune in British Burma.
Natural gas was discovered in the northeastern part of East Pakistan in 1955 by the Burmah Oil Company. Industrial use of natural gas began in 1959. The Shell Oil Company and Pakistan Petroleum tapped 7 gas fields in the 1960s. The industrial seaport city of Chittagong hosted the headquarters of Burmah Eastern and Pakistan National Oil. Iran, an erstwhile leading oil producer, assisted in establishing the Eastern Refinery in Chittagong.
The Comilla Model of the Pakistan Academy for Rural Development (present-day Bangladesh Academy for Rural Development) was conceived by Akhtar Hameed Khan and replicated in many developing countries.
In 1965, Pakistan implemented the Kaptai Dam hydroelectric project in the southeastern part of East Pakistan with American assistance. It was the sole hydroelectric dam in East Pakistan. The project was controversial for displacing over 40,000 indigenous people from the area.
The centrally located metropolis Dacca witnessed significant urban growth.
Although East Pakistan had a larger population, West Pakistan dominated the divided country politically and received more money from the common budget. According to the World Bank, there was much economic discrimination against East Pakistan, including higher government spending on West Pakistan, financial transfers from East to West and the use of the East's foreign exchange surpluses to finance the West's imports.
The discrimination occurred despite fact that East Pakistan generated a major share of Pakistan's exports.

The annual rate of growth of the gross domestic product per capita was 4.4% in the West Pakistan versus 2.6% in East Pakistan from 1960 to 1965. Bengali politicians pushed for more autonomy, arguing that much of Pakistan's export earnings were generated in East Pakistan from the exportation of Bengali jute and tea. As late as 1960, approximately 70% of Pakistan's export earnings originated in East Pakistan, although this percentage declined as international demand for jute dwindled. By the mid-1960s, East Pakistan was accounting for less than 60% of the nation's export earnings, and by the time Bangladesh gained its independence in 1971, this percentage had dipped below 50%. In 1966, Mujib demanded that separate foreign exchange accounts be kept and that separate trade offices be opened overseas. By the mid-1960s, West Pakistan was benefiting from Ayub's "Decade of Progress" with its successful green revolution in wheat and from the expansion of markets for West Pakistani textiles, while East Pakistan's standard of living remained at an abysmally low level. Bengalis were also upset that West Pakistan, the seat of the national government, received more foreign aid.
Economists in East Pakistan argued of a "Two Economies Theory" within Pakistan itself, which was founded on the Two Nation Theory with India. The so-called Two Economies Theory suggested that East and West Pakistan had different economic features which should not be regulated by a federal government in Islamabad.

East Pakistan was home to 55% of Pakistan's population. The largest ethnic group of the province were Bengalis, who in turn were the largest ethnic group in Pakistan. Bengali Muslims formed the predominant majority, followed by Bengali Hindus, Bengali Buddhists and Bengali Christians. East Pakistan also had many tribal groups, including the Chakmas, Marmas, Tangchangyas, Garos, Manipuris, Tripuris, Santhals and Bawms. They largely followed the religions of Buddhism, Christianity and Hinduism. East Pakistan was home to immigrant Muslims from across the Indian subcontinent, including West Bengal, Bihar, Gujarat, the Northwest Frontier Province, Assam, Orissa, the Punjab and Kerala. A small Armenian and Jewish minority resided in East Pakistan.
The Asiatic Society of Pakistan was founded in Old Dacca by Ahmad Hasan Dani in 1948. The Varendra Research Museum in Rajshahi was an important center of research on the Indus Valley Civilization. The Bangla Academy was established in 1954.
Among East Pakistan's newspapers, "The Daily Ittefaq" was the leading Bengali language title; while "Holiday" was a leading English title.
At the time of partition, East Bengal had 80 cinemas. The first movie produced in East Pakistan was The Face and the Mask in 1955. Pakistan Television established its second studio in Dacca after Lahore in 1965. Runa Laila was Pakistan's first pop star and became popular in India as well. Shabnam was a leading actress from East Pakistan. Feroza Begum was a leading exponent of Bengali classical Nazrul geeti. Jasimuddin and Abbasuddin Ahmed promoted Bengali folk music. Munier Chowdhury, Syed Mujtaba Ali, Nurul Momen, Sufia Kamal and Shamsur Rahman were among the leading literary figures in East Pakistan. Several East Pakistanis were awarded the Sitara-e-Imtiaz and the Pride of Performance.
Bengalis were hugely under-represented in Pakistan's bureaucracy and military. In the federal government, only 15% of offices were occupied by East Pakistanis. Only 10% of the military were from East Pakistan. Cultural discrimination also prevailed, causing the eastern wing to forge a distinct political identity. There was a bias against Bengali culture in state media, such as a ban on broadcasts of the works of Nobel laureate Rabindranath Tagore.

Since its unification with Pakistan, the East Pakistan Army had consisted of only one infantry brigade made up of two battalions, the 1st East Bengal Regiment and the 1/14 or 3/8 Punjab Regiment in 1948. These two battalions boasted only five rifle companies between them (an infantry battalion normally had 5 companies). This weak brigade was under the command of Brigadier-General Ayub Khan (local rank Major-General – GOC of 14th Army Division), together with the East Pakistan Rifles, which was tasked with defending East Pakistan during the Indo-Pakistani War of 1947. The PAF, Marines, and the Navy had little presence in the region. Only one PAF combatant squadron, No. 14 Squadron "Tail Choppers", was active in East Pakistan. This combatant squadron was commanded by Air Force Major Parvaiz Mehdi Qureshi, who later became a four-star general. The East Pakistan military personnel were trained in combat diving, demolitions, and guerrilla/anti-guerrilla tactics by the advisers from the Special Service Group (Navy) who were also charged with intelligence data collection and management cycle.

The East Pakistan Navy had only one active-duty combatant destroyer, the PNS "Sylhet"; one submarine "Ghazi" (which was repeatedly deployed in West); four gunboats, inadequate to function in deep water. The joint special operations were managed and undertaken by the Naval Special Service Group (SSG(N)) who were assisted by the army, air force and marines unit. The entire service, the Marines were deployed in East Pakistan, initially tasked with conducting exercises and combat operations in riverine areas and at near shoreline. The small directorate of Naval Intelligence (while the headquarters and personnel, facilities, and directions were coordinated by West) had vital role in directing special and reconnaissance missions, and intelligence gathering, also was charged with taking reasonable actions to slow down the Indian threat. The armed forces of East Pakistan also consisted the paramilitary organisation, the "Razakars" from the intelligence unit of the ISI's Covert Action Division (CAD). All of these armed forces were commanded by the unified command structure, the Eastern Military High Command, led by an officer of three-star rank equivalent.

The trauma was extremely severe in Pakistan when the news of secession of East Pakistan as Bangladesh arrived – a psychological setback, complete and humiliating defeat that shattered the prestige of Pakistan Armed Forces. The governor and martial law administrator Lieutenant-General Amir Abdullah Khan Niazi was defamed, his image was maligned and he was stripped of his honors. The people of Pakistan could not come to terms with the magnitude of defeat, and spontaneous demonstrations and mass protests erupted on the streets of major cities in (West) Pakistan. General Yahya Khan surrendered powers to Nurul Amin of Pakistan Muslim League, the first and last Vice-President and Prime minister of Pakistan.

Prime Minister Amin invited then-President Zulfikar Ali Bhutto and the Pakistan Peoples Party to take control of Pakistan. In a color ceremony where, Bhutto gave a daring speech to the nation on national television. At the ceremony, Bhutto waved his fist in the air and pledged to his nation to never again allow the surrender of his country like what happened with East Pakistan. He launched and orchestrated the large-scale atomic bomb project in 1972. In memorial of East Pakistan, the East-Pakistan diaspora in Pakistan established the East-Pakistan colony in Karachi, Sindh. In accordance, the East-Pakistani diaspora also composed patriotic tributes to Pakistan after the war; songs such as "Sohni Dharti" (lit. Beautiful land) and "Jeevay, Jeevay Pakistan" (lit. long-live, long-live Pakistan), were composed by Bengali singer Shahnaz Rahmatullah in the 1970s and 1980s.

To Western observers, the loss of East Pakistan was a blessing— but it has never been seen that way in Pakistan. In the book ""Scoop! Inside Stories from the Partition to the Present"", Indian politician Kuldip Nayar opined, "Losing East Pakistan and Bhutto's releasing of Mujib did not mean anything to Pakistan's policy – as if there was no liberation war." Bhutto's policy, and even today, the policy of Pakistan is that "she will continue to fight for the honor and integrity of Pakistan. East Pakistan is an inseparable and inseverable part of Pakistan".




</doc>
<doc id="10313" url="https://en.wikipedia.org/wiki?curid=10313" title="E. O. Wilson">
E. O. Wilson

Edward Osborne Wilson (born June 10, 1929), usually cited as E. O. Wilson, is an American biologist, researcher, theorist, naturalist and author. His biological specialty is myrmecology, the study of ants, on which he has been called the world's leading expert.

Wilson has been called "the father of sociobiology" and "the father of biodiversity", his environmental advocacy, and his secular-humanist and deist ideas pertaining to religious and ethical matters. Among his greatest contributions to ecological theory is the theory of island biogeography, which he developed in collaboration with the mathematical ecologist Robert MacArthur, which was the foundation of the development of conservation area design, as well as the unified neutral theory of biodiversity of Stephen Hubbell.

Wilson was born in Birmingham, Alabama. According to his autobiography "Naturalist", he grew up mostly around Washington, D.C. and in the countryside around Mobile, Alabama. From an early age, he was interested in natural history. His parents, Edward and Inez Wilson, divorced when he was seven. The young naturalist grew up in several cities and towns, moving around with his father and his stepmother.

In the same year that his parents divorced, Wilson blinded himself in one eye in a fishing accident. He suffered for hours, but he continued fishing. He did not complain because he was anxious to stay outdoors. He did not seek medical treatment. Several months later, his right pupil clouded over with a cataract. He was admitted to Pensacola Hospital to have the lens removed. Wilson writes, in his autobiography, that the "surgery was a terrifying [19th] century ordeal". Wilson was left with full sight in his left eye, with a vision of 20/10. The 20/10 vision prompted him to focus on "little things": "I noticed butterflies and ants more than other kids did, and took an interest in them automatically."

Although he had lost his stereoscopic vision, he could still see fine print and the hairs on the bodies of small insects. His reduced ability to observe mammals and birds led him to concentrate on insects.

At nine, Wilson undertook his first expeditions at the Rock Creek Park in Washington, DC. He began to collect insects and he gained a passion for butterflies. He would capture them using nets made with brooms, coat hangers, and cheesecloth bags. Going on these expeditions led to Wilson's fascination with ants. He describes in his autobiography how one day he pulled the bark of a rotting tree away and discovered citronella ants underneath. The worker ants he found were "short, fat, brilliant yellow, and emitted a strong lemony odor". Wilson said the event left a "vivid and lasting impression on [him]". He also earned the Eagle Scout award and served as Nature Director of his Boy Scout summer camp. At the age of 18, intent on becoming an entomologist, he began by collecting flies, but the shortage of insect pins caused by World War II caused him to switch to ants, which could be stored in vials. With the encouragement of Marion R. Smith, a myrmecologist from the National Museum of Natural History in Washington, Wilson began a survey of all the ants of Alabama. This study led him to report the first colony of fire ants in the US, near the port of Mobile.

Concerned that he might not be able to afford to go to a university, Wilson tried to enlist in the United States Army. He planned to earn U.S. government financial support for his education, but failed the Army medical examination due to his impaired eyesight. Wilson was able to afford to enroll in the University of Alabama after all, earning his B.S. and M.S. degrees in biology there in 1950. In 1952 he transferred to Harvard University.
Appointed to the Harvard Society of Fellows, he could travel on overseas expeditions, collecting ant species of Cuba and Mexico and travel the South Pacific, including Australia, New Guinea, Fiji, New Caledonia and Sri Lanka. In 1955, he received his Ph.D. and married Irene Kelley.

From 1956 until 1996 Wilson was part of the faculty of Harvard. 
He began as an ant taxonomist and worked on understanding their evolution, how they developed into new species by escaping environmental disadvantages and moving into new habitats. He developed a theory of the "taxon cycle".

He collaborated with mathematician William Bossert, and discovered the chemical nature of ant communication, via pheromones. In the 1960s he collaborated with mathematician and ecologist Robert MacArthur. Together, they tested the theory of species equilibrium on a tiny island in the Florida Keys. He eradicated all insect species and observed the re-population by new species. A book "The Theory of Island Biogeography" about this experiment became a standard ecology text.

In 1971, he published the book "The Insect Societies" about the biology of social insects like ants, bees, wasps and termites. In 1973, Wilson was appointed 'Curator of Insects' at the Museum of Comparative Zoology. In 1975, he published the book "" applying his theories of insect behavior to vertebrates, and in the last chapter, humans. He speculated that evolved and inherited tendencies were responsible for hierarchical social organisation among humans. In 1978 he published "On Human Nature", which dealt with the role of biology in the evolution of human culture and won a Pulitzer Prize for General Nonfiction.

In 1981 after collaborating with Charles Lumsden, he published "Genes, Mind and Culture", a theory of gene-culture coevolution. In 1990 he published "The Ants", co-written with Bert Hölldobler, his second Pulitzer Prize for General Nonfiction.

In the 1990s, he published "The Diversity of Life" (1992) an autobiography, "Naturalist" (1994), and "Consilience: The Unity of Knowledge" (1998) about the unity of the natural and social sciences.

In 1996, Wilson officially retired from Harvard University, where he continues to hold the positions of Professor Emeritus and Honorary Curator in Entomology. He founded the E.O. Wilson Biodiversity Foundation, which finances the PEN/E. O. Wilson Literary Science Writing Award and is an "independent foundation" at the Nicholas School of the Environment, Duke University. Wilson became a special lecturer at Duke University as part of the agreement.

Wilson has published 14 books during the new millennium: "The Future of Life", 2002, 
"Pheidole in the New World: A Dominant, Hyperdiverse Ant Genus," 2003, 
"From So Simple a Beginning: Darwin's Four Great Books," 2005, 
"The Creation: An Appeal to Save Life on Earth," September 2006, 
"Nature Revealed: Selected Writings" 1949–2006, 
"The Superorganism: The Beauty, Elegance, and Strangeness of Insect Societies," 2009,
"" April 2010, 
"Kingdom of Ants: Jose Celestino Mutis and the Dawn of Natural History in the New World", 2010,
"The Leafcutter Ants: Civilization by Instinct," 2011, 
"The Social Conquest of Earth", 2012,
"Half-Earth: Our Planet's Fight for Life", 2016.

He published 3 books in 2014 alone: "Letters to a Young Scientist", 
"A Window on Eternity: A Biologist's Walk Through Gorongosa National Park," and
"The Meaning of Human Existence".

He and his wife Irene reside in Lexington, Massachusetts. His daughter, Catherine, and her husband Jonathan, reside in nearby Stow, Massachusetts.

Wilson used sociobiology and evolutionary principles to explain the behavior of social insects and then to understand the social behavior of other animals, including humans, thus established sociobiology as a new scientific field. He argued that all animal behavior, including that of humans, is the product of heredity, environmental stimuli, and past experiences, and that free will is an illusion. He has referred to the biological basis of behaviour as the "genetic leash". The sociobiological view is that all animal social behavior is governed by epigenetic rules worked out by the laws of evolution. This theory and research proved to be seminal, controversial, and influential.

Wilson has argued that the unit of selection is a gene, the basic element of heredity. The "target" of selection is normally the individual who carries an ensemble of genes of certain kinds. With regard to the use of kin selection in explaining the behavior of eusocial insects, the "new view that I'm proposing is that it was group selection all along, an idea first roughly formulated by Darwin."

Sociobiological research was at the time particularly controversial with regard to its application to humans. The theory established a scientific argument for rejecting the common doctrine of tabula rasa, which holds that human beings are born without any innate mental content and that culture functions to increase human knowledge and aid in survival and success. In the final chapter of the book "Sociobiology" Wilson argues that the human mind is shaped as much by genetic inheritance as it is by culture if not more. There are, Wilson suggests in the chapter, limits on just how much influence social and environmental factors can have in altering human behavior.

"Sociobiology" was initially met with substantial criticism. Several of Wilson's colleagues at Harvard, such as Richard Lewontin and Stephen Jay Gould, were strongly opposed to his ideas regarding sociobiology. Gould, Lewontin, and others from the Sociobiology Study Group from the Boston area wrote "Against 'Sociobiology'" in an open letter criticizing Wilson's "deterministic view of human society and human action". Although attributed to members of the Sociobiology Study Group, it seems that Lewontin was the main author. In a 2011 interview, Wilson said, "I believe Gould was a charlatan. I believe that he was ... seeking reputation and credibility as a scientist and writer, and he did it consistently by distorting what other scientists were saying and devising arguments based upon that distortion."

Marshall Sahlins's 1976 work "The Use and Abuse of Biology" was a direct criticism of Wilson's theories.

There was also political opposition. Sociobiology re-ignited the nature and nurture debate. Wilson was accused of racism, misogyny, and sympathy to eugenics. In one incident in November 1978, his lecture was attacked by the International Committee Against Racism, a front group of the Marxist Progressive Labor Party, where one member poured a pitcher of water on Wilson's head and chanted "Wilson, you're all wet" at an AAAS conference. Wilson later spoke of the incident as a source of pride: "I believe...I was the only scientist in modern times to be physically attacked for an idea."

Objections from evangelical Christians included those of Paul E. Rothrock in 1987: "... sociobiology has the potential of becoming a religion of scientific materialism."
Philosopher Mary Midgley encountered "Sociobiology" in the process of writing "Beast and Man" (1996) and significantly rewrote the book to offer a critique of Wilson's views. Midgley praised the book for the study of animal behavior, clarity, scholarship, and encyclopedic scope, but extensively critiqued Wilson for conceptual confusion, scientism, and anthropomorphism of genetics.

The book and its reception were mentioned in Jonathan Haidt’s book The Righteous Mind.

Wilson wrote in his 1978 book "On Human Nature", "The evolutionary epic is probably the best myth we will ever have." Wilson's use of the word "myth" provides people with meaningful placement in time celebrating shared heritage. Wilson's fame prompted use of the morphed phrase epic of evolution. The book won the Pulitzer Prize in 2011.

Wilson, along with Bert Hölldobler, carried out a systematic study of ants and ant behavior, culminating in the 1990 encyclopedic work "The Ants". Because much self-sacrificing behavior on the part of individual ants can be explained on the basis of their genetic interests in the survival of the sisters, with whom they share 75% of their genes (though the actual case is some species' queens mate with multiple males and therefore some workers in a colony would only be 25% related), Wilson argued for a sociobiological explanation for all social behavior on the model of the behavior of the social insects.

Wilson has said in reference to ants "Karl Marx was right, socialism works, it is just that he had the wrong species". He meant that while ants and other eusocial species appear to live in communist-like societies, they only do so because they are forced to do so from their basic biology, as they lack reproductive independence: worker ants, being sterile, need their ant-queen in order to survive as a colony and a species, and individual ants cannot reproduce without a queen and are thus forced to live in centralised societies. Humans, however, do possess reproductive independence so they can give birth to offspring without the need of a "queen", and in fact humans enjoy their maximum level of Darwinian fitness only when they look after themselves and their offspring, while finding innovative ways to use the societies they live in for their own benefit.

In his 1998 book "Consilience: The Unity of Knowledge", Wilson discussed methods that have been used to unite the sciences, and might be able to unite the sciences with the humanities. Wilson used the term "consilience" to describe the synthesis of knowledge from different specialized fields of human endeavor. He defined human nature as a collection of epigenetic rules, the genetic patterns of mental development. He argued that culture and rituals are products, not parts, of human nature. He said art is not part of human nature, but our appreciation of art is. He suggested that concepts such as art appreciation, fear of snakes, or the incest taboo (Westermarck effect) could be studied by scientific methods of the natural sciences and be part of interdisciplinary research.

The book was mentioned in Jonathan Haidt’s book The Righteous Mind.

Wilson coined the phrase "scientific humanism" as "the only worldview compatible with science's growing knowledge of the real world and the laws of nature". Wilson argued that it is best suited to improve the human condition. In 2003, he was one of the signers of the "Humanist Manifesto".

In a "New Scientist" interview published on 21 January 2015, Wilson said that "Religion 'is dragging us down' and must be eliminated 'for the sake of human progress, and "So I would say that for the sake of human progress, the best thing we could possibly do would be to diminish, to the point of eliminating, religious faiths."
On the question of God, Wilson has described his position as "provisional deism" and explicitly denied the label of "atheist", preferring "agnostic". He has explained his faith as a trajectory away from traditional beliefs: "I drifted away from the church, not definitively agnostic or atheistic, just Baptist & Christian no more." Wilson argues that the belief in God and rituals of religion are products of evolution. He argues that they should not be rejected or dismissed, but further investigated by science to better understand their significance to human nature. In his book "The Creation", Wilson suggests that scientists ought to "offer the hand of friendship" to religious leaders and build an alliance with them, stating that "Science and religion are two of the most potent forces on Earth and they should come together to save the creation."

Wilson made an appeal to the religious community on the lecture circuit at Midland College, Texas, for example, and that "the appeal received a 'massive reply'", that a covenant had been written and that a "partnership will work to a substantial degree as time goes on".

Wilson has said that, if he could start his life over he would work in microbial ecology, when discussing the reinvigoration of his original fields of study since the 1960s. He studied the mass extinctions of the 20th century and their relationship to modern society, and in 1998 argued for an ecological approach at the Capitol:

Wilson has been part of the international conservation movement, as a consultant to Columbia University's Earth Institute, as a director of the American Museum of Natural History, Conservation International, The Nature Conservancy and the World Wildlife Fund.

Understanding the scale of the extinction crisis has led him to advocate for forest protection, including the "Act to Save America's Forests", first introduced in 1998, until 2008, but never passed. The Forests Now Declaration calls for new markets-based mechanisms to protect tropical forests. In 2014, Wilson called for setting aside 50% of the earth's surface for other species to thrive in as the only possible strategy to solve the extinction crisis.

Wilson's scientific and conservation honors include:






</doc>
<doc id="10315" url="https://en.wikipedia.org/wiki?curid=10315" title="Edwin Howard Armstrong">
Edwin Howard Armstrong

Edwin Howard Armstrong (December 18, 1890 – January 31, 1954) was an American electrical engineer and inventor, best known for developing FM (frequency modulation) radio and the superheterodyne receiver system. He held 42 patents and received numerous awards, including the first Medal of Honor awarded by the Institute of Radio Engineers (now IEEE), the French Legion of Honor, the 1941 Franklin Medal and the 1942 Edison Medal. He was inducted into the National Inventors Hall of Fame and included in the International Telecommunication Union's roster of great inventors.

Armstrong was born in the Chelsea district of New York City, the oldest of John and Emily (Smith) Armstrong's three children. His father began working at a young age at the American branch of the Oxford University Press, which published bibles and standard classical works, eventually advancing to the position of vice president. His parents first met at the North Presbyterian Church, located at 31st Street and Ninth Avenue. His mother's family had strong ties to Chelsea, and an active role in church functions. When the church moved north, the Smiths and Armstrongs followed, and in 1895 the Armstrong family moved from their brownstone row house at 347 West 29th Street to a similar house at 26 West 97th Street in the Upper West Side. The family was comfortably middle class.

At the age of eight, Armstrong contracted Sydenham's chorea (then known as St. Vitus' Dance), an infrequent but serious neurological disorder precipitated by rheumatic fever. For the rest of his life, Armstrong was afflicted with a physical tic exacerbated by excitement or stress. Due to this illness, he withdrew from public school and was home-tutored for two years. To improve his health, the Armstrong family moved to a house overlooking the Hudson River, at 1032 Warburton Avenue in Yonkers. The Smith family subsequently moved next door. Armstrong's tic and the time missed from school led him to become socially withdrawn.

From an early age, Armstrong showed an interest in electrical and mechanical devices, particularly trains. He loved heights and constructed a makeshift backyard antenna tower that included a bosun's chair for hoisting himself up and down its length, to the concern of neighbors. Much of his early research was conducted in the attic of his parent's house.

In 1909, Armstrong enrolled at Columbia University in New York City, where he became a member of the Epsilon Chapter of the Theta Xi engineering fraternity, and studied under Professor Michael Pupin at the Hartley Laboratories, a separate research unit at Columbia. Another of his instructors, Professor John H. Morecroft, later remembered Armstrong as being intensely focused on the topics that interested him, but somewhat indifferent to the rest of his studies. He was known for challenging conventional wisdom and being quick to question the opinions of both professors and peers. In one case, he recounted how he tricked an instructor he disliked into receiving a severe electrical shock. He also stressed the practical over the theoretical, stating that progress was more likely the product of experimentation and work based on physical reasoning than on mathematical calculation and formulae (known as part of "mathematical physics").

Armstrong graduated from Columbia in 1913, earning an electrical engineering degree.

During World War I, Armstrong served in the Signal Corps as a captain and later a major. 

In 1934, he filled the vacancy left by John H. Morecroft's death, receiving an appointment as a Professor of Electrical Engineering at Columbia, a position he held the remainder of his life.

Following college graduation, he received a $600 one-year appointment as a laboratory assistant at Columbia, after which he nominally worked as a research assistant, for a salary of $1 a year, under Professor Pupin. Unlike most engineers, Armstrong never became a corporate employee. He set up a self-financed independent research and development laboratory at Columbia, and owned his patents outright.

Armstrong began working on his first major invention while still an undergraduate at Columbia. In late 1906, Lee de Forest had invented the three-element (triode) "grid Audion" vacuum-tube. How vacuum tubes worked was not understood at the time. De Forest's initial Audions did not have a high vacuum and developed a blue glow at modest plate voltages; De Forest improved the vacuum for Federal Telegraph. By 1912, how vacuum tubes worked was understood, and the advantages of high vacuum tubes were appreciated.

While growing up Armstrong had experimented with the early, temperamental, "gassy" Audions. Spurred by the later discoveries, he developed a keen interest in gaining a detailed scientific understanding of how vacuum-tubes worked. In conjunction with Professor Morecroft he used an oscillograph to conduct comprehensive studies. His breakthrough discovery was determining that employing positive feedback (also known as "regeneration") produced amplification hundreds of times greater than previously attained, with the amplified signals now strong enough so that receivers could use loudspeakers instead of headphones. Further investigation revealed that when the feedback was increased beyond a certain level a vacuum-tube would go into oscillation, thus could also be used as a continuous-wave radio transmitter.

Beginning in 1913 Armstrong prepared a series of comprehensive demonstrations and papers that carefully documented his research, and in late 1913 applied for patent protection covering the regenerative circuit. On October 6, 1914, U.S. patent 1,113,149 was issued for his discovery. Although Lee de Forest initially discounted Armstrong's findings, beginning in 1915 de Forest filed a series of competing patent applications that largely copied Armstrong's claims, now stating that he had discovered regeneration first, based on August 6, 1912 notebook entry, while working for the Federal Telegraph company, prior to the January 31, 1913 date recognized for Armstrong. The result was an interference hearing at the patent office to determine priority. De Forest was not the only other inventor involved — the four competing claimants included Armstrong, de Forest, General Electric's Langmuir, and Alexander Meissner, who was a German national, which led to his application being seized by the Office of Alien Property Custodian during World War I.

Following the end of World War I Armstrong enlisted representation by the law firm of Pennie, Davis, Martin and Edmonds. In order to finance his legal expenses he began issuing non-transferable licenses for use of the regenerative patents to a select group of small radio equipment firms, and by November 1920 seventeen companies had been licensed. These licensees paid 5% royalties on their sales which were restricted to only "amateurs and experimenters". Meanwhile, Armstrong reviewed his options for selling the commercial rights to his work. Although the obvious candidate was the Radio Corporation of America (RCA), on October 5, 1920 the Westinghouse Electric & Manufacturing Company took out an option for $335,000 for the commercial rights for both the regenerative and superheterodyne patents, with an additional $200,000 to be paid if Armstrong prevailed in the regenerative patent dispute. Westinghouse exercised this option on November 4, 1920.

Legal proceedings related to the regeneration patent became separated into two groups of court cases. An initial court action was triggered in 1919 when Armstrong sued de Forest's company in district court, alleging infringement of patent 1,113,149. This court ruled in Armstrong's favor on May 17, 1921. But a second line of court cases, the result of the patent office interference hearing, would have a different outcome. The interference board had also sided with Armstrong, but he was unwilling to settle with de Forest for less than what he considered full compensation. Thus pressured, de Forest decided to continue his legal defense, and appealed the interference board decision to the District of Columbia district court. On May 8, 1924, that court ruled that it was de Forest who should be considered regeneration's inventor. Armstrong (along with much of the engineering community) was shocked by this course of events, and his side appealed this unexpected decision. But although the legal proceeding twice went before the U.S. Supreme Court, in 1928 and 1934, he was unsuccessful in overturning the decision.

In response to the second Supreme Court decision upholding de Forest as the inventor of regeneration, Armstrong attempted to return his 1917 IRE Medal of Honor, which had been awarded "in recognition of his work and publications dealing with the action of the oscillating and non-oscillating audion". However, the organization's board refused to let him, and issued a statement that it "strongly affirms the original award".

The United States entered into World War I in April 1917, and later that year Armstrong was commissioned as a Captain in the U.S. Army Signal Corps, and assigned to a laboratory in Paris, France to help develop radio communication for the Allied war effort. He returned to the United States in the fall of 1919, after being promoted to the rank of Major. (During both world wars, Armstrong gave the U.S. military free use of his patents.)

During this period Armstrong's most significant accomplishment was the development of a "supersonic heterodyne" — soon shortened to "superheterodyne" — radio receiver circuit. This circuit made radio receivers more sensitive and selective and is still extensively used today. The key feature of the superheterodyne approach is the mixing of the incoming radio signal with a locally generated, different frequency signal within a radio set. This circuit is typically referred to as the mixer. The end result is a fixed, unchanging intermediate frequency, or I.F. signal which is more easily amplified and detected by subsequent circuit stages that follow the mixer. In 1919, Armstrong filed an application for a U.S. patent of the superheterodyne circuit which was issued the next year. This patent was subsequently sold to Westinghouse. The patent would be challenged, however, triggering yet another patent office interference hearing. Armstrong ultimately lost this patent battle; although the outcome was less controversial than that involving the regeneration proceedings.

The challenger was Lucien Lévy of France who had also worked developing Allied radio communication during World War I. He had been awarded French patents in 1917 and 1918 that covered some of the same basic ideas used in Armstrong's superheterodyne receiver. AT&T, which was interested in radio development at this time, primarily for point-to-point extensions of its wired telephone exchanges, purchased the U.S. rights to Lévy's patent and contested Armstrong's grant. The subsequent court reviews continued until 1928, when the District of Columbia Court of Appeals disallowed all nine claims of Armstrong's patent, assigning priority for seven of the claims to Lévy, and one each to Ernst Alexanderson of General Electric and Burton W. Kendall of Bell Laboratories.

Although most early radio receivers used regeneration Armstrong approached RCA's David Sarnoff, whom he had known since giving a demonstration of his regeneration receiver in 1913, about the corporation offering superheterodynes as a superior offering to the general public. (The ongoing patent dispute was not a hindrance, because extensive cross-licensing agreements signed in 1920 and 1921 between RCA, Westinghouse and AT&T meant that Armstrong could freely use the Lévy patent.) Superheterodyne sets were initially thought to be prohibitively complicated and expensive as the initial designs required multiple tuning knobs and used nine vacuum-tubes. However, in conjunction with RCA engineers, Armstrong developed a simpler, less costly design. RCA introduced its superheterodyne Radiola sets in the U.S. market in early 1924, and they were an immediate success, dramatically increasing the corporation's profits. These sets were considered so valuable that RCA would not license the superheterodyne to other U.S. manufacturing companies until 1930.

The regeneration legal battle had one serendipitous outcome for Armstrong. While he was preparing apparatus to counteract a claim made by a patent attorney, he "accidentally ran into the phenomenon of super-regeneration", where, by rapidly "quenching" the vacuum-tube oscillations, he was able to achieve even greater levels of amplification. A year later, in 1922, Armstrong sold his super-regeneration patent to RCA for $200,000 plus 60,000 shares of corporation stock, which was later increased to 80,000 shares in payment for consulting services. This made Armstrong RCA's largest shareholder, and he noted that "The sale of that invention was to net me more than the sale of the regenerative circuit and the superheterodyne combined". RCA envisioned selling a line of super-regenerative receivers until superheterodyne sets could be perfected for general sales, but it turned out the circuit was not selective enough to make it practical for broadcast receivers.

"Static" interference — extraneous noises caused by sources such as thunderstorms and electrical equipment — bedeviled early radio communication using amplitude modulation (AM) and perplexed numerous inventors attempting to eliminate it. Many ideas for static elimination were investigated, with little success. In the mid-1920s, Armstrong began researching whether he could come up with a solution. He initially, and unsuccessfully, attempted to resolve the problem by modifying the characteristics of existing AM transmissions.

One approach considered as a potential solution had been the use of frequency modulation (FM) transmissions, where, in order to encode audio, instead of varying (technically known as "modulating") the amplitude (strength) of a radio signal, as was done for AM transmissions, the frequency was varied. However, in 1922 John Renshaw Carson of AT&T, inventor of Single-sideband modulation (SSB), had published a "Proceedings of the IRE" paper which included a detailed mathematical analysis which showed that FM transmissions did not provide any improvement over AM. Although the Carson bandwidth rule for FM is still important today, this review turned out to be incomplete, because it analyzed only what is now known as "narrow-band" FM.

In early 1928 Armstrong began researching the capabilities of frequency modulation. Although there were few others involved in FM research at this time, he did have knowledge of a project being conducted by RCA engineers, who were investigating whether FM shortwave transmissions were less susceptible to fading than AM. In 1931 these engineers constructed a successful FM shortwave link transmitting the Schmeling-Stribling fight broadcast from California to Hawaii, and noted at the time that the signals seemed to be less affected by static, but the project made little further progress.

Working in secret in the basement laboratory of Columbia's Philosophy Hall, Armstrong slowly developed what eventually resulted in wide-band FM, in the process discovering significant advantages over the earlier "narrow-band" FM transmissions. He was granted five U.S. patents covering the basic features of new system on December 26, 1933. Initially, the primary claim was that his FM system was effective at filtering out the noise produced in receivers by vacuum tubes.

Armstrong had a standing agreement to give RCA the right of first refusal to his patents. In 1934 he made a presentation of his new system to RCA president Sarnoff. Sarnoff was somewhat taken aback by its complexity, as he had hoped it would be possible to eliminate static merely by adding a simple device to existing receivers. From May 1934 until October 1935 Armstrong conducted field tests of his FM technology from an RCA laboratory located on the 85th floor of the Empire State Building in New York City. An antenna attached to the building's spire transmitted signals for distances up to . These tests helped demonstrate FM's static-reduction and high-fidelity capabilities. However RCA, which was heavily invested in perfecting television broadcasting, chose not to invest in FM, and instructed Armstrong to remove his equipment.

Denied the marketing and financial clout that RCA would have brought, Armstrong decided to finance his own development and form ties with smaller members of the radio industry, including Zenith and General Electric, to promote his invention. Armstrong thought that FM had the potential to replace AM stations within 5 years, which he promoted as a boost for the radio manufacturing industry, then suffering from the effects of the Great Depression, since making existing AM radio transmitters and receivers obsolete would necessitate that stations buy replacement transmitters and listeners purchase FM-capable receivers. In 1936 he published a landmark paper in the "Proceedings of the IRE" that documented the superior capabilities of using wide-band FM. (This paper would be reprinted in the August 1984 issue of "Proceedings of the IEEE".) A year later, a paper by Murray G. Crosby (inventor of Crosby system for FM Stereo) in the same journal provided further analysis of the wide-band FM characteristics, and introduced the concept of "threshold", demonstrating that there is a superior signal to noise ratio when the signal is stronger than a certain level.

In June 1936, Armstrong gave a formal presentation of his new system at the U.S. Federal Communications Commission (FCC) headquarters in Washington, D.C. For comparison, he played a jazz record using a conventional AM radio, then switched to an FM transmission. A United Press correspondent was present, and recounted in a wire service report that: "if the audience of 500 engineers had shut their eyes they would have believed the jazz band was in the same room. There were no extraneous sounds." Moreover, "Several engineers said after the demonstration that they consider Dr. Armstrong's invention one of the most important radio developments since the first earphone crystal sets were introduced." Armstrong was quoted as saying he could "visualize a time not far distant when the use of ultra-high frequency wave bands will play the leading role in all broadcasting", although the article noted that "A switchover to the ultra-high frequency system would mean the junking of present broadcasting equipment and present receivers in homes, eventually causing the expenditure of billions of dollars."

In the late 1930s, as technical advances made it possible to transmit on higher frequencies, the FCC investigated options for increasing the number of broadcasting stations, in addition to ideas for better audio quality, known as "high-fidelity". In 1937 it introduced what became known as the Apex band, consisting of 75 broadcasting frequencies from 41.02 to 43.98 MHz. As on the standard broadcast band these were AM stations, but with higher quality audio — in one example, a frequency response from 20 Hz to 17,000 Hz +/- 1 dB — because station separations were 40 kHz instead of the 10 kHz spacings used on the original AM band. Armstrong worked to convince the FCC that a band of FM broadcasting stations would be a superior approach. That year he financed the construction of the first FM radio station, W2XMN (later KE2XCC) at Alpine, New Jersey. FCC engineers had believed that transmissions using high frequencies would travel little farther than line-of-sight distances, limited by the horizon. However, when operating with 40 kilowatts on 42.8 MHz, the station could be clearly heard away, matching the daytime coverage of a full power 50-kilowatt AM station.

FCC studies comparing the Apex station transmissions with Armstrong's FM system concluded that his approach was superior. In early 1940, the FCC held hearings on whether to establish a commercial FM service. Following this review, the FCC announced the establishment of an FM band effective January 1, 1941, consisting of forty 200 kHz-wide channels on a band from 42-50 MHz, with the first five channels reserved for educational stations. Existing Apex stations were notified that they would not be allowed to operate after January 1, 1941 unless they converted to FM.

Although there was interest in the new FM band by station owners, construction restrictions that went into place during World War II limited the growth of the new service. Following the end of World War II, the FCC moved to standardize its frequency allocations. One area of concern was the effects of tropospheric and Sporadic E propagation, which at times reflected station signals over great distances, causing mutual interference. A particularly controversial proposal, spearheaded by RCA, was that the FM band needed to be shifted to higher frequencies in order to avoid this potential problem. This reassignment was fiercely opposed as unneeded by Armstrong, but in the end he lost. The FCC made its decision final on June 27, 1945. It allocated one hundred FM channels from 88–108 MHz, and assigned the former FM band to 'non government fixed and mobile' (42–44 MHz), and television channel 1 (44–50 MHz), curiously now sidestepping the interference concerns. A period of allowing existing FM stations to broadcast on both low and high bands ended at midnight on January 8, 1949, at which time any low band transmitters had to be shut down, officially making obsolete 395,000 receivers that had already been purchased by the public for the original band. Although converters allowing low band FM sets to receive high band were manufactured, they ultimately proved to be complicated to install, and often as (or more) expensive than buying a new high band set outright.

Armstrong felt the FM band reassignment had been inspired primarily by a desire to cause a disruption that would limit FM's ability to challenge the existing radio industry, including RCA's AM radio properties that included the NBC radio network, plus the other major networks including CBS, ABC and Mutual. The change was also thought to have been favored by AT&T, as the elimination of FM relaying stations would require radio stations to lease wired links from that company. Particularly galling was the FCC assignment of TV channel 1 to the 44-50 MHz segment of the old FM band. Channel 1 was later deleted, since periodic radio propagation would make local TV signals unviewable. 

Although the FM band shift was an economic setback, there was still reason for optimism, and a book published in 1946 by Charles A. Siepmann heralded FM stations as "Radio's Second Chance". In late 1945, Armstrong contracted with John Orr Young, founding member of the public relations firm Young & Rubicam, to conduct a national campaign promoting FM broadcasting, especially by educational institutions. Article placements promoting both Armstrong personally and FM were made with general circulation publications including "The Nation", "Fortune", the "New York Times", "Atlantic Monthly", and "The Saturday Evening Post".

In 1940, RCA offered Armstrong $1,000,000 for a non-exclusive, royalty-free license to use his FM patents. But he refused this offer, primarily because he felt this would be unfair to the other licensed companies, which had to pay 2% royalties on their sales. Over time this impasse with RCA would come to dominate Armstrong's life. RCA countered by conducting its own FM research, eventually developing what it claimed was a non-infringing FM system. The corporation also encouraged other companies to stop paying royalties to Armstrong. Outraged by this turn of events, in 1948 Armstrong filed suit against RCA and the National Broadcasting Company, accusing them of patent infringement and that they had "deliberately set out to oppose and impair the value" of his invention, for which he requested treble damages. Although he was confident that this suit would be successful and result in a major monetary award, the protracted legal maneuvering that followed eventually began to impair his finances, especially after his primary patents expired in late 1950.

Bitter and overtaxed by years of litigation and mounting financial problems, Armstrong lashed out at his wife one day with a fireplace poker, striking her on the arm. She left their apartment to stay with her sister, Marjorie Tuttle, in Granby, Connecticut.

Sometime during the night of January 31–February 1, 1954, with his wife in Connecticut and three servants having left for the day, Armstrong removed the air conditioner from a window in his twelve-room apartment on the thirteenth-floor of River House in Manhattan, New York City, and jumped to his death. His body—fully clothed, with a hat, overcoat and gloves—was found in the morning on a third-floor balcony by a River House employee. The "New York Times" described the contents of his two-page suicide note to his wife: "he was heartbroken at being unable to see her once again, and expressing deep regret at having hurt her, the dearest thing in his life." The note concluded, "God keep you and Lord have mercy on my Soul." David Sarnoff disclaimed any responsibility, telling Carl Dreher directly that "I did not kill Armstrong." After his death, a friend of Armstrong estimated that 90 percent of his time was spent on litigation against RCA. U.S. Senator Joseph McCarthy (R-Wisconsin) reported that Armstrong had recently met with one of his investigators, and had been "mortally afraid" that secret radar discoveries by him and other scientists "were being fed to the Communists as fast as they could be developed". Armstrong was buried in Locust Grove Cemetery, Merrimac, Massachusetts.

Following her husband's death, Marion Armstrong took charge of pursuing his estate's legal cases. In late December 1954, it was announced that through arbitration an out-of-court settlement of "approximately $1,000,000" had been made with RCA. Dana Raymond of Cravath, Swaine & Moore in New York served as counsel in that litigation. Marion Armstrong was able to formally establish Armstrong as the inventor of FM following protracted court proceedings over five of his basic FM patents, with a series of successful suits, which lasted until 1967, against other companies that were found guilty of infringement.

It wasn't until the 1960s that FM stations in the United States started to challenge the popularity of the AM band, helped by the development of FM stereo by General Electric. Armstrong's FM system was also used for communications between NASA and the Apollo program astronauts. (He is of no known relation to Apollo astronaut Neil Armstrong.)

Armstrong has been called "the most prolific and influential inventor in radio history". The superheterodyne process is still extensively used by radio equipment. Eighty years after its invention, FM technology has started to be supplemented, and in some cases replaced, by more efficient digital technologies. The introduction of digital television eliminated the FM audio channel that had been used by analog television, HD Radio has added digital sub-channels to FM band stations, and, in Europe and Pacific Asia, Digital Audio Broadcasting bands have been created that will, in some cases, eliminate existing FM stations altogether. However, FM broadcasting is still used internationally, and remains the dominant system employed for audio broadcasting services.

In 1923, combining his love for high places with courtship rituals, Armstrong climbed the WJZ (now WABC) antenna located atop a twenty-story building in New York City, where he reportedly did a handstand, and when a witness asked him what motivated him to "do these damnfool things", Armstrong replied "I do it because the spirit moves me." Armstrong had arranged to have photographs taken, which he had delivered to David Sarnoff's secretary, Marion MacInnis. Armstrong and MacInnis married later that year. Armstrong bought a Hispano-Suiza motor car before the wedding, which he kept until his death, and which he drove to Palm Beach, Florida for their honeymoon. A publicity photograph was made of him presenting Marion with the world's first portable superheterodyne radio as a wedding gift.

He was an avid tennis player until an injury in 1940, and drank an Old Fashioned with dinner. Politically, he was described by one of his associates as "a revolutionist only in technology — in politics he was one of the most conservative of men."

In 1955, Marion Armstrong founded the Armstrong Memorial Research Foundation, and participated in its work until her death in 1979 at the age of 81. She was survived by two nephews and a niece.

In 1917, Armstrong was the first recipient of the IRE's (now IEEE) Medal of Honor. 

For his wartime work on radio, the French government gave him the Legion of Honor in 1919. He was awarded the 1941 Franklin Medal, and in 1942 received the AIEEs Edison Medal "for distinguished contributions to the art of electric communication, notably the regenerative circuit, the superheterodyne, and frequency modulation." The ITU added him to its roster of great inventors of electricity in 1955. 

He later received two honorary doctorates, from Columbia in 1929, and Muhlenberg College in 1941.

In 1980, he was inducted into the National Inventors Hall of Fame, and appeared on a U.S. postage stamp in 1983. The Consumer Electronics Hall of Fame inducted him in 2000, "in recognition of his contributions and pioneering spirit that have laid the foundation for consumer electronics." Columbia University established the Edwin Howard Armstrong Professorship in the School of Engineering and Applied Science in his memory.

Philosophy Hall, the Columbia building where Armstrong developed FM, was declared a National Historic Landmark. Armstrong's boyhood home in Yonkers, New York was recognized by the National Historic Landmark program and the National Register of Historic Places, although this was withdrawn when the house was demolished.

Armstrong Hall at Columbia was named in his honor. The hall, located at the northeast corner of Broadway and 112th Street, was originally an apartment house but was converted to research space after being purchased by the university. It is currently home to the Goddard Institute for Space Studies, a research institute dedicated to atmospheric and climate science that is jointly operated by Columbia and the National Aeronautics and Space Administration. A storefront in a corner of the building houses Tom's Restaurant, a longtime neighborhood fixture that inspired Susanne Vega's song "Tom's Diner" and was used for establishing shots for the fictional "Monk's diner" in the "Seinfeld" television series.

A second Armstrong Hall, also named for the inventor, is located at the United States Army Communications and Electronics Life Cycle Management Command (CECOM-LCMC) Headquarters at Aberdeen Proving Ground, Maryland.

E. H. Armstrong patents:

U.S. Patent and Tradmark Office Database Search






</doc>
<doc id="10322" url="https://en.wikipedia.org/wiki?curid=10322" title="EverQuest">
EverQuest

EverQuest is a 3D fantasy-themed massively multiplayer online role-playing game (MMORPG) developed and published by Sony Online Entertainment, and released on March 16, 1999. It was the second commercially viable MMORPG to be released, after "Tibia", and the first commercially successful MMORPG to employ a three-dimensional game engine.

"EverQuest" has had a wide influence on subsequent releases within the market, and holds an important position in the history of massively multiplayer online games. The game surpassed early subscription expectations and grew for many years after its release. It has received awards, including 1999 GameSpot Game of the Year and a 2007 Technology & Engineering Emmy Award.

"EverQuest" began as a concept by John Smedley in 1996. The original design is credited to Brad McQuaid, Steve Clover, and Bill Trost. It was developed by Sony's 989 Studios and its early-1999 spin-off Verant Interactive, and published by Sony Online Entertainment (SOE). 
Since its acquisition of Verant in late 1999, EverQuest was developed by Sony Online Entertainment.<ref name="Sony/Verant"></ref>

The design and concept of "EverQuest" is heavily indebted to text-based MUDs, in particular DikuMUD, and as such "EverQuest" is considered a 3D evolution of the text MUD genre like some of the MMOs that preceded it, such as "Meridian 59" and "The Realm Online". John Smedley, Brad McQuaid, Steve Clover and Bill Trost, who jointly are credited with creating the world of "EverQuest", have repeatedly pointed to their shared experiences playing MUDs such as "Sojourn" and "TorilMUD" as the inspiration for the game. Famed book cover illustrator Keith Parkinson created the box covers for earlier installments of EverQuest.

Development of "EverQuest" began in 1996 when Sony Interactive Studios America (SISA) executive John Smedley secured funding for a 3D game like text-based MUDs following the successful launch of "Meridian 59" the previous year. To implement the design, Smedley hired programmers Brad McQuaid and Steve Clover, who had come to Smedley's attention through their work on the single player RPG "Warwizard". McQuaid soon rose through the ranks to become executive producer for the "EverQuest" franchise and emerged during development of "EverQuest" as a popular figure among the fan community through his in-game avatar, Aradune. Other key members of the development team included Bill Trost, who created the history, lore and major characters of Norrath (including "EverQuest" protagonist Firiona Vie), Geoffrey "GZ" Zatkin, who implemented the spell system, and artist Milo D. Cooper, who did the original character modeling in the game.

The start of beta testing was announced by Brad McQuaid in November 1997.

"EverQuest" launched with modest expectations from Sony on 16 March 1999 under its Verant Interactive brand and quickly became successful. By the end of the year, it had surpassed competitor "Ultima Online" in number of subscriptions. Numbers continued rising rapidly until mid-2001 when growth slowed. Sony's last reported subscription numbers were given as more than 430,000 players on 14 January 2004.

"EverQuest" initially launched with volunteer "Guides" who would act as basic customer service/support via 'petitions'. Issues could be forwarded to the Game Master assigned to the server or resolved by the volunteer. Other guides would serve in administrative functions within the program or assisting the Quest Troupe with dynamic and persistent live events throughout the individual servers. Volunteers were compensated with free subscription and expansions to the game. In 2003 the program changed for the volunteer guides taking them away from the customer service focus and placing them into their current roles as roving 'persistent characters' role-playing with the players.

In anticipation of PlayStation's launch, Sony Interactive Studios America made the decision to focus primarily on console titles under the banner 989 Studios, while spinning off its sole computer title, "EverQuest", which was ready to launch, to a new computer game division named Redeye (renamed Verant Interactive). Executives initially had very low expectations for "EverQuest", but in 2000, following the surprising continued success and unparalleled profits of "EverQuest", Sony reorganized Verant Interactive into Sony Online Entertainment (SOE) with Smedley retaining control of the company.

Many of the original "EverQuest" team, including Brad McQuaid and Steve Clover left SOE by 2002.

While the exact statistics on "EverQuest" subscriptions are not public, computer games analyst Bruce Woodcock estimates, based on public sources such as press statements, that the game had 200,000 subscriptions in March 2000, one year after initial release, with an increase to more than 450,000 subscriptions by July 2003. However, the same analysis points at a sharp decline after mid-2005, back to 200,000 in May 2006.

The first four expansions were released in traditional physical boxes at roughly one-year intervals. These were highly ambitious and offered huge new landmasses, new playable races and new classes. The expansion "" (2001) gave a significant facelift to player character models, bringing the by-then dated 1999 graphics up to modern standards. However, non-player characters which do not correspond to any playable race-gender-class combination (such as vendors) were not updated, leading to the coexistence of 1999-era and 2001-era graphics in many locations. The expansion "" (2002) introduced The Plane of Knowledge, a hub zone from which players could quickly teleport to many other destinations. This made the pre-existing roads and ships largely redundant, and long-distance overland travel is now virtually unheard of.

"EverQuest" made a push to enter the European market in 2002 with the "New Dawn" promotional campaign, which not only established local servers in Germany, France and Great Britain but also offered localized versions of the game in German and French to accommodate players who prefer those languages to English. In the following year the game also moved beyond the PC market with a Mac OS X version.

In 2003 experiments began with digital distribution of expansions, starting with the "Legacy of Ykesha". From this point on expansions would be less ambitious in scope than the original four, but on the other hand the production rate increased to two expansions a year instead of one.

This year the franchise also ventured into the console market with "EverQuest Online Adventures", released for Sony's internet-capable PlayStation 2. It was the second MMORPG for this console, after "Final Fantasy XI". Story-wise it was a prequel, with the events taking place 500 years before the original "EverQuest". Other spin-off projects were the PC strategy game "Lords of EverQuest" (2003) and the co-op "Champions of Norrath" (2004) for the PlayStation 2.

After these side projects, the first proper sequel was released in late 2004, titled simply "EverQuest II" . The game is set 500 years "after" the original, as opposed to "EverQuest Online Adventures" which took place 500 years "before". "EverQuest II" would face severe competition from Blizzard's "World of Warcraft", which was released at virtually the same time and quickly grew to dominate the MMORPG genre.

Since the release of "World of Warcraft" and other modern MMORPGs, there have been a number of signs that the EverQuest population is shrinking. The national "New Dawn" servers were discontinued in 2005 and merged into a general (English-language) European server.

The 2006 expansion The Serpent's Spine introduced the "adventure-friendly" city of Crescent Reach in which all races and classes are able (and encouraged) to start. Crescent Reach is supposed to provide a more pedagogic starting environment than the original 1999 cities, where players were given almost no guidance on what to do. The common starting city also concentrates the dwindling number of new players in a single location, making grouping easier. expansion introduced computer controlled companions called "mercenaries" that can join groups in place of human players; a response to the increasing difficulty of finding other players of appropriate level for group activities. As of "Seeds" the production rate also returned to one expansion a year instead of two.

In March 2012 EverQuest departed from the traditional monthly subscription business model by introducing three tiers of commitment: a completely free-to-play Bronze Level, a one-time fee Silver Level, and a subscription Gold Level. The same month saw the closure of EverQuest Online Adventures. Just a few months earlier EverQuest II had gone free-to-play and SOE flagship Star Wars Galaxies also closed.

In June of the same year SOE removed the ability to buy game subscription time with Station Cash without any warning to players. SOE apologized for this abrupt change in policy and reinstated the option for an additional week, after which it was removed permanently.

November 2013 saw the closure of the sole Mac OS server Al'Kabor.

In February 2015 Sony sold its online entertainment division to private equity group Columbus Nova, with Sony Online Entertainment subsequently renamed Daybreak Game Company (DBG). An initial period of uncertainty followed, with all projects such as expansions and sequels put on hold and staff laid off. The situation stabilized around the game's 16th anniversary celebrations, and a new expansion was released in November 2015.

As a result of the layoffs, customer service was severely curtailed. The expansions DBG released consisted of about half the content that was released prior while the Everquest franchise was owned by Sony. Each released expansion by DBG had numerous serious bugs, where either the events were not tuned correctly, players were allowed to do what they should not be allowed to do, or so buggy that the game was unplayable.

The third iteration in the series, with the working title "EverQuest Next", was under development, with in-game screenshots, concept art, and information revealed at the SOE Fan Faire in August 2010. However, this early version of the game has since been scrapped and development cancelled.

Many of the elements in "EverQuest" have been drawn from text-based MUD (Multi-User Dungeon) games, particularly DikuMUDs, which in turn were inspired by traditional role-playing games such as "Dungeons & Dragons". In "EverQuest", players create a character (also known as an avatar, or colloquially as a "char" or "toon") by selecting one of 12 races in the game, which were humans , high-elves, wood-elves, half-elves, dark-elves, erudites, barbarians, dwarves, halflings, gnomes, ogres, and trolls. In the first expansion, lizard-people (Iksar) were introduced. Cat-people (Vah Shir), frog-people (Froglok), and dragon-people (Drakkin) were all introduced in later expansions. At creation, players select each character's adventuring occupation (such as a wizard, ranger, or cleric — called a "class" — see below for particulars), a patron deity, and starting city. Customization of the character facial appearance is available at creation (hair, hair color, face style, facial hair, facial hair color, eye color, etc.).

Players move their character throughout the medieval fantasy world of Norrath, often fighting monsters and enemies for treasure and experience points, and optionally mastering trade skills. As they progress, players advance in level, gaining power, prestige, spells, and abilities through valorous deeds such as entering overrun castles and keeps, defeating worthy opponents found within, and looting their remains. Experience and prestigious equipment can also be obtained by completing quests given out by non-player characters found throughout the land.

"EverQuest" allows players to interact with other people through role-play, joining player guilds, and dueling other players (in restricted situations – "EverQuest" only allows player versus player (PVP) combat on the PvP-specific server, specified arena zones and through agreed upon dueling).

The game-world of "EverQuest" consists of over five hundred zones.

Multiple instances of the world exist on various servers. In the past, game server populations were visible during log-in, and showed peaks of more than 3000 players per server. The design of "EverQuest", like other massively multiplayer online role-playing games, makes it highly amenable to cooperative play, with each player having a specific role within a given group.

The fourteen classes of the original 1999 version of "EverQuest" were later expanded to include the Beastlord and Berserker classes with the "" (2001) and "" (2004) expansions, respectively.

The classes can be grouped into five general roles that share similar characteristics, as described below.

Members of this group have a high number of hitpoints for their level, and can equip heavy armor. They have the ability to taunt enemies into focusing on them, either directly or through the use of aggravating spells and abilities. This is to keep their more lightly-armored companions alive and well, who may otherwise provoke the wrath of one or more deadly creatures.


The following classes are able to deal high corporal damage to opponents. Within the game, these classes are often referred to as 'DPS', which stands for Damage Per Second. There isn't a definitive top DPS class, as damage dealt will depend on numerous factors which vary from one encounter to another (such as the enemy's armor, its positioning, and its magic resistance). Another complication is that while Wizards can readily deal tremendous damage to enemies, their ability to do so is limited by their remaining mana pool, as well as how fast they are able to regenerate mana. That said, Berserkers, Rogues, and Wizards are three classes most commonly cited as the highest overall damage dealers.

These melee damage dealers have a medium number of hit points per level, but cannot wear the heaviest armors and are less likely than a tank class to be able to survive direct attacks for a sustained period of time.

Caster classes have the lowest hit points per level and can only wear the lightest of armors. Casters draw their power from an internal pool of "mana", which takes some time to regenerate and thus demands judicious and efficient use of spells.

These classes share the ability to prevent enemies from attacking the party, as well as improving mana regeneration for themselves, teammates, and in the Enchanter's case, anyone they come across.


Priest classes have medium level of hit points per level and have access to healing and "buff" spells.

There are several deities in "EverQuest" who each have a certain area of responsibility and play a role in the backstory of the game setting. A wide array of armor and weapons are also deity-tied, making it possible for only those who worship that deity to wear/equip them. Additionally, deities determine, to some extent, where characters may and may not go without being attacked on sight.

The "EverQuest" universe is divided into more than five hundred zones. These zones represent a wide variety of geographical features, including plains, oceans, cities, deserts, and other planes of existence. One of the most popular zones in the game is the Plane of Knowledge, one of the few zones in which all races and classes can coexist harmoniously without interference. The Plane of Knowledge is also home to portals to many other zones, including portals to other planes and to the outskirts of nearly every starting city.

There have been twenty-four expansions to the original game since release. Expansions are purchased separately and provide additional content to the game (for example: raising the maximum character level; adding new races, classes, zones, continents, quests, equipment, game features). When you purchase the latest expansion you receive all previous expansions you may not have previously purchased. Additionally, the game is updated through downloaded patches. The "EverQuest" expansions are as follows:

The game runs on multiple game servers, each with a unique name for identification. These names were originally the deities of the world of Norrath. In technical terms, each game server is actually a cluster of server machines. Once a character is created, it can be played only on that server unless the character is transferred to a new server by the customer service staff, generally for a fee. Each server often has a unique community and people often include the server name when identifying their character outside of the game.

SOE devoted one server (Al'Kabor) to an OS X version of the game. The game was never developed beyond the Planes of Power expansion. In January 2012, SOE announced plans to shut down the server, but based on the passionate response of the player base, rescinded the decision and changed Al'Kabor to a free-to-play subscription model. At about the same time, SOE revised the Macintosh client software to run natively on Intel processors. Players running on older, PowerPC-based systems lost access to the game at that point. Finally in November 2013, SOE closed Al'Kabor.

Two SOE servers were set up to better support players in and around Europe: Antonius Bayle and Kane Bayle. Kane Bayle was merged into Antonius Bayle.

With the advent of the "New Dawn" promotion, three additional servers were set up and maintained by Ubisoft: Venril Sathir (British), Sebilis (French) and Kael Drakkal (German). The downside of the servers was that while it was possible to transfer to them, it was impossible to transfer off.

The servers were subsequently acquired by SOE and all three were merged into Antonius Bayle server.

Reviews of "Everquest" were mostly positive upon release in 1999, earning an 85 out of 100 score from aggregate review website Metacritic. Comparing it to other online role-playing titles at the time, critics routinely called it "the best game in its class," the "most immersive and most addictive online RPG to date." Dan Amrich of "GamePro" magazine declared that "the bar for online gaming has not so much been raised as obliterated," and that the game's developers had "created the first true online killer app." The reviewer would find fault with its repetitive gameplay in the early levels and lack of sufficient documentation to help new players, urging them to turn to fansites for help instead. Greg Kasavin of GameSpot similarly felt that the game's combat was "uninteresting" but did note that, unlike earlier games in the genre, "EverQuest" offered the opportunity to play on servers that wouldn't allow players to fight each other unless they chose to, and that it heavily promoted cooperation. Ultimately, the reviewer would declare that "the combat may be a little boring, the manual may be horrible, the quest system half-baked, and the game not without its small share of miscellaneous bugs. But all you need is to find a like-minded adventurer or two, and all of a sudden "EverQuest" stands to become one of the most memorable gaming experiences you've ever had." Baldric of Game Revolution likewise stated that the game was more co-operative than "Ultima Online", but that there was less interaction with the environment, calling it more "player oriented" instead of "'world' oriented."

Despite server issues during the initial launch of the game, reviewers felt that the game played well even on lower-end network cards, with Tal Blevins of IGN remarking that it "rarely suffered from major lag issues, even on a 28.8k modem." The reviewer did feel that the game suffered from a lack of player customization aside from different face types, meaning all like races looked mostly the same, but the game's visual quality on the whole was "excellent" with "particularly impressive" spell, lighting, and particle effects. "Computer Games Magazine" would also commend the title's three-dimensional graphics and environments, remarking that "With its 3D graphics, first-person perspective, and elegantly simple combat system, "EverQuest" has finally given us the first step towards a true virtual world. Internet gaming will never be the same."

"Everquest" was named GameSpot's 1999 Game of the Year in its Best & Worst of 1999 awards, remarking that "Following EverQuest's release in March, the whole gaming industry effectively ground to a halt [...] At least one prominent game developer blamed "EverQuest" for product delays, and for several weeks GameSpot's editors were spending more time exploring Norrath than they were doing their jobs." GameSpot UK would also rank the title 14th on its list of the 100 Best Computer Games of the Millennium the following year, calling it "a technological tour de force" and "the first online RPG to bring the production values of single-player games to the online masses." The Academy of Interactive Arts and Sciences named "EverQuest" their Online Game of the Year for 1999, and was included in "Time Magazine"s Best of 1999 in the "Tech" category. "Entertainment Weekly" would include the game in their Top Ten Hall of Fame Video Games of the '90s, calling its virtual world "the nearest you could get to being on a "Star Trek" holodeck." In 2007, Sony Online Entertainment received a Technology & Engineering Emmy Award for "EverQuest" under the category of "Development of Massively Multiplayer Online Graphical Role Playing Games".

The editors of "Computer Gaming World" nominated "EverQuest" for their 1999 "Role-Playing Game of the Year" award, which ultimately went to "".

The sale of in-game objects for real currency is a controversial and lucrative industry with topics concerning issues practices of hacking/stealing accounts for profit. Critics often cite how it affects the virtual economy inside the game. In 2001, the sales of in-game items for real life currency was banned on eBay.

A practice in the real-world trade economy is of companies creating characters, powerleveling them to make them powerful, and then reselling the characters for large sums of money or in-game items of other games.

Sony discourages the payment of real-world money for online goods, except on certain "Station Exchange" servers in "EverQuest II", launched in July 2005. The program facilitates buying in-game items for real money from fellow players for a nominal fee. At this point this system only applies to select "EverQuest II" servers; none of the pre-"Station Exchange" "EverQuest II" or "EverQuest" servers are affected.

In 2012, Sony added an in-game item called a "Krono", which adds 30 days of game membership throughout EverQuest and EverQuest II. The item can be initially bought starting at $17.99 USD. Up to 25 "Kronos" can be bought for $424.99 USD. Krono can be resold via player trading, which has allowed Krono to be frequently used in the real-world trade economy.

On October 2000, Verant banned a player by the name of Mystere, allegedly for creating controversial fan fiction, causing outrage among some "EverQuest" players and sparking a debate about players' rights and the line between roleplaying and intellectual property infringement. The case was used by several academics in discussing such rights in the digital age.

Some argue the game has addictive qualities. Many players refer to it as "EverCrack" (a comparison to crack cocaine). There was one well-publicized suicide of an "EverQuest" user named Shawn Woolley, that inspired his mother, Liz, to found Online Gamers Anonymous.

Massively multiplayer online role-playing games (MMORPGs) are described by some players as "chat rooms with a graphical interface". The sociological aspects of "EverQuest" (and other MMORPGs) are explored in a series of online studies on a site known as "the HUB". The studies make use of data gathered from player surveys and discuss topics like virtual relationships, player personalities, gender issues, and more.

In May 2004, Woody Hearn of GU Comics called for all "EverQuest" gamers to boycott the "Omens of War" expansion in an effort to force SOE to address existing issues with the game rather than release another "quick-fire" expansion. The call to boycott was rescinded after SOE held a summit to address player concerns, improve (internal and external) communication, and correct specific issues within the game.

On 17 January 2008, the Judge of the 17th Federal Court of Minas Gerais State forbade the sales of the game in that Brazilian territory. The reason was that "the game leads the players to a loss of moral virtue and takes them into "heavy" psychological conflicts because of the game quests".

Since "EverQuest"s release, Sony Online Entertainment has added several "EverQuest"-related games. These include:


A line of novels have been published in the world of "EverQuest", including:


</doc>
<doc id="10326" url="https://en.wikipedia.org/wiki?curid=10326" title="Human evolution">
Human evolution

Human evolution is the evolutionary process that led to the emergence of anatomically modern humans, beginning with the evolutionary history of primates – in particular genus "Homo" – and leading to the emergence of "Homo sapiens" as a distinct species of the hominid family, the great apes. This process involved the gradual development of traits such as human bipedalism and language.

The study of human evolution involves many scientific disciplines, including physical anthropology, primatology, archaeology, paleontology, neurobiology, ethology, linguistics, evolutionary psychology, embryology and genetics. Genetic studies show that primates diverged from other mammals about , in the Late Cretaceous period, and the earliest fossils appear in the Paleocene, around .

Within the Hominoidea (apes) superfamily, the Hominidae family diverged from the Hylobatidae (gibbon) family some 15–20 million years ago; African great apes (subfamily Homininae) diverged from orangutans (Ponginae) about ; the Hominini tribe (humans, "Australopithecines" and other extinct biped genera, and chimpanzee) parted from the Gorillini tribe (gorillas) between and ; and, in turn, the subtribes Hominina (humans and biped ancestors) and Panina (chimps) separated about to .

Human evolution from its first separation from the last common ancestor of humans and chimpanzees is characterized by a number of morphological, developmental, physiological, and behavioral changes.
The most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate. Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in "H. erectus".

Bipedalism is the basic adaptation of the hominid and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominids. The earliest hominin, of presumably primitive bipedalism, is considered to be either "Sahelanthropus" or "Orrorin", both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorilla and chimpanzee, diverged from the hominin line over a period covering the same time, so either of "Sahelanthropus" or "Orrorin" may be our last shared ancestor. "Ardipithecus", a full biped, arose somewhat later. 

The early bipeds eventually evolved into the australopithecines and still later into the genus "Homo". There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion, enabled long distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna and woodland environment created as a result of the East African Rift Valley uplift versus the previous closed forest habitat. A new study provides support for the hypothesis that walking on two legs, or bipedalism, evolved because it used less energy than quadrupedal knuckle-walking. However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal. This change in gait saw a lengthening of the legs proportionately when compared to the length of the arms, which were shortened through the removal of the need for brachiation. Another change is the shape of the big toe. Recent studies suggest that Australopithecines still lived part of the time in trees as a result of maintaining a grasping big toe. This was progressively lost in Habilines.

Anatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull. The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.

The most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking; bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, permitting the passage of newborns due to the increase in cranial size but this is limited to the upper portion, since further increase can hinder normal bipedal movement.

The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit. The smaller birth canal became a limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age. The increased brain growth after birth and the increased dependency of children on mothers had a big effect upon the female reproductive cycle, and the more frequent appearance of alloparenting in humans when compared with other hominids. Delayed human sexual maturity also led to the evolution of menopause with one explanation providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more children of their own.

The human species eventually developed a much larger brain than that of other primates—typically in modern humans, nearly three times the size of a chimpanzee or gorilla brain. After a period of stasis with "Australopithecus anamensis" and "Ardipithecus", species which had smaller brains as a result of their bipedal locomotion, the pattern of encephalization started with "Homo habilis", whose brain was slightly larger than that of chimpanzees. This evolution continued in "Homo erectus" with , and reached a maximum in Neanderthals with , larger even than modern "Homo sapiens". This brain increase manifested during postnatal brain growth, far exceeding that of other apes (heterochrony). It also allowed for extended periods of social learning and language acquisition in juvenile humans, beginning as much as 2 million years ago.

Furthermore, the changes in the structure of human brains may be even more significant than the increase in size.

The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Its function has traditionally been associated with balance and fine motor control, but more recently with speech and cognition. The great apes, including hominids, had a more pronounced cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and learning complex muscular actions, the cerebellum may have underpinned human technological adaptations, including the preconditions of speech.

The immediate survival advantage of encephalization is difficult to discern, as the major brain changes from "Homo erectus" to "Homo heidelbergensis" were not accompanied by major changes in technology. It has been suggested that the changes were mainly social, including increased empathic abilities and increases in size of social groups

The reduced degree of sexual dimorphism in humans is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans and bonobos are the only apes in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling during estrus).

Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females. These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.

The ulnar opposition – the contact between the thumb and the tip of the little finger of the same hand – is unique to anatomically modern humans. In other primates the thumb is short and unable to touch the little finger. The ulnar opposition facilitates the precision grip and power grip of the human hand, underlying all the skilled manipulations.

A number of other changes have also characterized the evolution of humans, among them an increased importance on vision rather than smell; a smaller gut; loss of body hair; evolution of sweat glands; a change in the shape of the dental arcade from being u-shaped to being parabolic; development of a chin (found in "Homo sapiens" alone); development of styloid processes; and the development of a descended larynx.

The word "homo", the name of the biological genus to which humans belong, is Latin for "human". It was chosen originally by Carl Linnaeus in his classification system. The word "human" is from the Latin "humanus", the adjectival form of "homo". The Latin "homo" derives from the Indo-European root *"dhghem", or "earth". Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.

The possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's "On the Origin of Species", in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that "Light will be thrown on the origin of man and his history."

The first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and apes, and did so particularly in his 1863 book "Evidence as to Man's Place in Nature". However, many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans when he published "The Descent of Man" in 1871.

A major problem in the 19th century was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of "On the Origin of Species", and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were human remains of a creature suffering some kind of illness. Despite the 1891 discovery by Eugène Dubois of what is now called "Homo erectus" at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate. In 1925, Raymond Dart described "Australopithecus africanus". The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocast of the brain.

Although the brain was small (410 cm), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.

During the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. The driving force of these searches was the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave—all successful and world-renowned fossil hunters and paleoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and "Homo" species, and even "Homo erectus".

These finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of paleoanthropology after "Lucy", the most complete fossil member of the species "Australopithecus afarensis", was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect. Lucy was classified as a new species, "Australopithecus afarensis", which is thought to be more closely related to the genus "Homo" as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range; "see" terms "hominid" and "hominin". (The specimen was nicknamed "Lucy" after the Beatles' song "Lucy in the Sky with Diamonds", which was played loudly and repeatedly in the camp during the excavations.) The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including "Ardipithecus ramidus" and "Ardipithecus kadabba".

In 2013, fossil skeletons of "Homo naledi", an extinct species of hominin assigned (provisionally) to the genus "Homo", were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg. , fossils of at least fifteen individuals, amounting to 1550 specimens, have been excavated from the cave. The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to "Australopithecus", and a cranial morphology (skull shape) similar to early "Homo" species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils have not yet been dated.

The genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas). The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.

In their seminal 1967 paper in "Science", Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago, at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably "Lucy", and reinterpretation of older fossil materials, notably "Ramapithecus", showed the younger estimates to be correct and validated the albumin method.

Progress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins. Application of the molecular clock principle revolutionized the study of molecular evolution.

On the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimps noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimps to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimp populations in 8 locations suggests that chimps reproduce at age 26.5 years, on average; which suggests the human divergence from chimps occurred between 7 and 13 million years ago. And these data suggest that "Ardipithecus" (4.5 Ma), "Orrorin" (6 Ma) and "Sahelanthropus" (7 Ma) all may be on the hominid lineage, and even that the separation may have occurred outside the East African Rift region.

Furthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between "proto-human" and "proto-chimps" nonetheless occurred regularly enough to change certain genes in the new gene pool:
The research suggests:

In the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered "Australopithecus anamensis". The find was overshadowed by Tim D. White's 1995 discovery of "Ardipithecus ramidus", which pushed back the fossil record to .

In 2000, Martin Pickford and Brigitte Senut discovered, in the Tugen Hills of Kenya, a 6-million-year-old bipedal hominin which they named "Orrorin tugenensis". And in 2001, a team led by Michel Brunet discovered the skull of "Sahelanthropus tchadensis" which was dated as , and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin ( Hominidae; terms "hominids" and hominins).

Anthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the "Homo" genus. Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that genus "Homo" have migrated out of Africa at least three and possibly four times (e.g. "Homo erectus", "Homo heidelbergensis" and two or three times for "Homo sapiens"). Recent evidence suggests these dispersals are closely related to fluctuating periods of climate change.

Recent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artefacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus Homo at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, tools found at a Chinese cave strengthen the case that humans used tools as far back as 2.48 million years ago. This suggests that the Asian "Chopper" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe.

Up until the genetic evidence became available there were two dominant models for the dispersal of modern humans. The multiregional hypothesis proposed that "Homo" genus contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple million years. This model was proposed in 1988 by Milford H. Wolpoff. In contrast the "out of Africa" model proposed that modern "H. sapiens" speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in nearly complete replacement of other "Homo" species. This model has been developed by Chris B. Stringer and Peter Andrews.

Sequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage, and strengthened the Out of Africa theory and weakened the views of Multiregional Evolutionism. Aligned in genetic tree differences were interpreted as supportive of a recent single origin. Analyses have shown a greater diversity of DNA patterns throughout Africa, consistent with the idea that Africa is the ancestral home of mitochondrial Eve and Y-chromosomal Adam, and that modern human dispersal out of Africa has only occurred over the last 55,000 years.

"Out of Africa" has thus gained much support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. "Out of Africa" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.

A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 "ancestral population clusters". The research also located a possible origin of modern human migration in south-western Africa, near the coastal border of Namibia and Angola. The fossil evidence was insufficient for archaeologist Richard Leakey to resolve the debate about exactly where in Africa modern humans first appeared. Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin. All the evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans, both in Africa and later, throughout Eurasia has recently been suggested by a number of studies.

Recent sequencing of Neanderthal and Denisovan genomes shows that some admixture with these populations has occurred. Modern humans outside Africa have 2–4% Neanderthal alleles in their genome, and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the "out of Africa" model, except in its strictest interpretation, although they make the situation more complex. After recovery from a genetic bottleneck that could possibly be due to the Toba supervolcano catastrophe, a fairly small group left Africa and later briefly interbred on three separate occasions with Neanderthals, probably in the middle-east, on the Eurasian steppe or even in North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in south-east Asia, before populating Melanesia. HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations. The Denisovan EPAS1 gene has also been found in Tibetan populations.

There are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory, which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa crossing the Bab el Mandib to Yemen at a lower sea level around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant. This group seems to have been dependent upon marine resources for their survival.

Stephen Oppenheimer has proposed a second wave of humans may have later dispersed through the Persian Gulf oases, and the Zagros mountains into the Middle East. Alternatively it may have come across the Sinai Peninsula into Asia, from shortly after 50,000 yrs BP, resulting in the bulk of the human populations of Eurasia. It has been suggested that this second group possibly possessed a more sophisticated "big game hunting" tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum. The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA L3 lineages, which support a single migration out of Africa that gave rise to all non-African populations.

Stephen Oppenheimer, on the basis of the early date of Badoshan Iranian Aurignacian, suggests that this second dispersal, may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.

The evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.

The closest living relatives of humans are bonobos and chimpanzees (both genus "Pan") and gorillas (genus "Gorilla"). With the sequencing of both the human and chimpanzee genome, as of 2012 estimates of the similarity between their DNA sequences range between 95% and 99%. By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated.

The gibbons (family Hylobatidae) and then orangutans (genus "Pongo") were the first groups to split from the line leading to the hominins, including humans—followed by gorillas, and, ultimately, by the chimpanzees (genus "Pan"). The splitting date between hominin and chimpanzee lineages is placed by some between , that is, during the Late Miocene. Speciation, however, appears to have been unusually drawn-out. Initial divergence occurred sometime between , but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at .

Genetic evidence has also been employed to resolve the question of whether there was any gene flow between early modern humans and Neanderthals, and to enhance our understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.

Each time a certain mutation (single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.

Human evolutionary genetics studies how one human genome differs from the other, the evolutionary past that gave rise to it, and its current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.

There is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages. The earliest fossils that have been proposed as members of the hominin lineage are "Sahelanthropus tchadensis" dating from , "Orrorin tugenensis" dating from , and "Ardipithecus kadabba" dating to . Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.

The question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around and diverged into robust (also called "Paranthropus") and gracile branches, one of which (possibly "A. garhi") probably went on to become ancestors of the genus "Homo". The australopithecine species that is best represented in the fossil record is "Australopithecus afarensis" with more than one hundred fossil individuals represented, found from Northern Ethiopia (such as the famous "Lucy"), to Kenya, and South Africa. Fossils of robust australopithecines such as "Au. robustus" (or alternatively "Paranthropus robustus") and "Au./P. boisei" are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.

The earliest member of the genus "Homo" is "Homo habilis" which evolved around . "Homo habilis" is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider "Homo rudolfensis", a larger bodied group of fossils with similar morphology to the original "H. habilis" fossils, to be a separate species while others consider them to be part of "H. habilis"—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.

During the next million years, a process of encephalization began and, by the arrival (about ) of "Homo erectus" in the fossil record, cranial capacity had doubled. "Homo erectus" were the first of the hominins to emigrate from Africa, and, from , this species spread through Africa, Asia, and Europe. One population of "H. erectus", also sometimes classified as a separate species "Homo ergaster", remained in Africa and evolved into "Homo sapiens". It is believed that these species, "H. erectus" and "H. ergaster", were the first to use fire and complex tools.

The earliest transitional fossils between "H. ergaster/erectus" and archaic "H. sapiens" are from Africa, such as "Homo rhodesiensis", but seemingly transitional forms were also found at Dmanisi, Georgia. These descendants of African "H. erectus" spread through Eurasia from ca. 500,000 years ago evolving into "H. antecessor", "H. heidelbergensis" and "H. neanderthalensis". The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 200,000 years ago such as the Omo remains of Ethiopia; later fossils from Es Skhul cave in Israel and Southern Europe begin around 90,000 years ago ().

As modern humans spread out from Africa, they encountered other hominins such as "Homo neanderthalensis" and the so-called Denisovans, who may have evolved from populations of "Homo erectus" that had left Africa around . The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.

This migration out of Africa is estimated to have begun about 85,000 years BP dated by a finger bone unearthed in Saudi Arabia’s Nefud Desert and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.

Evolutionary history of the primates can be traced back 65 million years. One of the oldest known primate-like mammal species, the "Plesiadapis", came from North America; another, "Archicebus", came from China. Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.

David R. Begun concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to "Dryopithecus", migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or "bush babies" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids.

The earliest known catarrhine is "Kamoyapithecus" from uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago. Its ancestry is thought to be species related to "Aegyptopithecus", "Propliopithecus", and "Parapithecus" from the Faiyum, at around 35 million years ago. In 2010, "Saadanius" was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 million years ago, helping to fill an 11-million-year gap in the fossil record.

In the Early Miocene, about 22 million years ago, the many kinds of arboreally adapted primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to "Victoriapithecus", the earliest Old World monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are "Proconsul", "Rangwapithecus", "Dendropithecus", "Limnopithecus", "Nacholapithecus", "Equatorius", "Nyanzapithecus", "Afropithecus", "Heliopithecus", and "Kenyapithecus", all from East Africa.

The presence of other generalized non-cercopithecids of Middle Miocene from sites far distant—"Otavipithecus" from cave deposits in Namibia, and "Pierolapithecus" and "Dryopithecus" from France, Spain and Austria—is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, "Oreopithecus", is from coal beds in Italy that have been dated to 9 million years ago.

Molecular evidence indicates that the lineage of gibbons (family Hylobatidae) diverged from the line of great apes some 18–12 million years ago, and that of orangutans (subfamily Ponginae) diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown South East Asian hominoid population, but fossil proto-orangutans may be represented by "Sivapithecus" from India and "Griphopithecus" from Turkey, dated to around 10 million years ago.

Species close to the last common ancestor of gorillas, chimpanzees and humans may be represented by "Nakalipithecus" fossils found in Kenya and "Ouranopithecus" found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus "Pan") split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation—rain forest soils tend to be acidic and dissolve bone—and sampling bias probably contribute to this problem.

Other hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are "Sahelanthropus tchadensis" (7 Ma) and "Orrorin tugenensis" (6 Ma), followed by "Ardipithecus" (5.5–4.4 Ma), with species "Ar. kadabba" and "Ar. ramidus".

It has been argued in a study of the life history of "Ar. ramidus" that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape. This study demonstrated affinities between the skull morphology of "Ar. ramidus" and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos ("Pan paniscus") the less aggressive species of chimpanzee, may have evolved via the process of self-domestication. Consequently, arguing against the so-called "chimpanzee referential model" the authors suggest it is no longer tenable to use common chimpanzee ("Pan troglodytes") social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in "Ar. ramidus" and the implications this has for the evolution of hominin social psychology, they wrote:

The authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans.

The "Australopithecus" genus evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including "Australopithecus anamensis", "Au. afarensis", "Au. sediba", and "Au. africanus". There is still some debate among academics whether certain African hominid species of this time, such as "Au. robustus" and "Au. boisei", constitute members of the same genus; if so, they would be considered to be "Au. robust australopiths" whilst the others would be considered "Au. gracile australopiths". However, if these species do indeed constitute their own genus, then they may be given their own name, the "Paranthropus".

A new proposed species "Australopithecus deyiremeda" is claimed to have been discovered living at the same time period of "Au. afarensis". There is debate if Au. deyiremeda is a new species or is "Au. afarensis." Australopithecus prometheus, otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the Australopithecus genus as old as the Afarensis. Given the opposable big toe found on Little Foot, it seems that he was a good climber, and it is thought given the night predators of the region, he probably, like gorillas and chimpanzees, built a nesting platform at night, in the trees.

The earliest documented representative of the genus "Homo" is "Homo habilis", which evolved around , and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of "Homo erectus" and "Homo ergaster" in the fossil record, cranial capacity had doubled to 850 cm. (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that "Homo erectus" and "Homo ergaster" were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between .

According to the recent African origin of modern humans theory, modern humans evolved in Africa possibly from "Homo heidelbergensis", "Homo rhodesiensis" or "Homo antecessor" and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of "Homo erectus", Denisova hominins, "Homo floresiensis" and "Homo neanderthalensis". Archaic "Homo sapiens", the forerunner of anatomically modern humans, evolved in the Middle Paleolithic between 400,000 and 250,000 years ago. Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited inter-breeding between these species.<ref name="10.1126/science.1209202"></ref> The transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago according to some anthropologists although others point to evidence that suggests that a gradual change in behavior took place over a longer time span.

"Homo sapiens" is the only extant species of its genus, "Homo". While some (extinct) "Homo" species might have been ancestors of "Homo sapiens", many, perhaps most, were likely "cousins", having speciated away from the ancestral hominin line. There is yet no consensus as to which of these groups should be considered a separate species and which should be a subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the "Homo" genus. The Sahara pump theory (describing an occasionally passable "wet" Sahara desert) provides one possible explanation of the early variation in the genus "Homo".

Based on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices of various "Homo" species and to study the role of diet in physical and behavioral evolution within "Homo".

Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatran island in Indonesia some 70,000 years ago caused global consequences, killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today.

"Homo habilis" lived from about 2.8 to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines. "Homo habilis" had smaller molars and larger brains than the australopithecines, and made tools from stone and perhaps animal bones. One of the first known hominins, it was nicknamed 'handy man' by discoverer Louis Leakey due to its association with stone tools. Some scientists have proposed moving this species out of "Homo" and into "Australopithecus" due to the morphology of its skeleton being more adapted to living on trees rather than to moving on two legs like "Homo sapiens".

In May 2010, a new species, "Homo gautengensis", was discovered in South Africa.

These are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to "Homo habilis" is not yet clear.


The first fossils of "Homo erectus" were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material "Pithecanthropus erectus" based on its morphology, which he considered to be intermediate between that of humans and apes. "Homo erectus" lived from about 1.8 Ma to about 70,000 years ago—which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby "Homo floresiensis" survived it. The early phase of "Homo erectus", from 1.8 to 1.25 Ma, is considered by some to be a separate species, "Homo ergaster", or as "Homo erectus ergaster", a subspecies of "Homo erectus".

In Africa in the Early Pleistocene, 1.5–1 Ma, some populations of "Homo habilis" are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, "Homo erectus"—in Africa. The evolution of locking knees and the movement of the foramen magnum are thought to be likely drivers of the larger population changes. This species also may have used fire to cook meat. suggests that the fact that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, "and swelled our brains to their current, horrendously fuel-inefficient size", suggest that control of fire and releasing increased nutritional value through cooking was the key adaptation that separated Homo from tree-sleeping Australopitheicines.

A famous example of "Homo erectus" is Peking Man; others were found in Asia (notably in Indonesia), Africa, and Europe. Many paleoanthropologists now use the term "Homo ergaster" for the non-Asian forms of this group, and reserve "Homo erectus" only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from "H. ergaster".

These are proposed as species that may be intermediate between "H. erectus" and "H. heidelbergensis".


"H. heidelbergensis" ("Heidelberg Man") lived from about 800,000 to about 300,000 years ago. Also proposed as "Homo sapiens heidelbergensis" or "Homo sapiens paleohungaricus".


"Homo neanderthalensis", alternatively designated as "Homo sapiens neanderthalensis", lived in Europe and Asia from 400,000 to about 28,000 years ago.
There are a number of clear anatomical differences between anatomically modern humans (AMH) and Neanderthal populations. Many of these relate to the superior adaptation to cold environments possessed by the Neanderthal populations. Their surface to volume ratio is an extreme version of that found amongst Inuit populations, indicating that they were less inclined to lose body heat than were AMH. From brain Endocasts, Neanderthals also had significantly larger brains. This would seem to indicate that the intellectual superiority of AMH populations may be questionable. More recent research by Eiluned Pearce, Chris Stringer, R. I. M. Dunbar, however, have shown important differences in Brain architecture. For example, in both the orbital chamber size and in the size of the occipital lobe, the larger size suggests that the Neanderthal had a better visual acuity than modern humans. This would give a superior vision in the inferior light conditions found in Glacial Europe. It also seems that the higher body mass of Neanderthals had a correspondingly larger brain mass required for body care and control. The Neanderthal populations seem to have been physically superior to AMH populations. These differences may have been sufficient to give Neanderthal populations an environmental superiority to AMH populations from 75,000 to 45,000 years BP. With these differences, Neanderthal brains show a smaller area was available for social functioning. Plotting group size possible from endocrainial volume, suggests that AMH populations (minus occipital lobe size), had a Dunbars number of 144 possible relationships. Neanderthal populations seem to have been limited to about 120 individuals. This would show up in a larger number of possible mates for AMH humans, with increased risks of inbreeding amongst Neanderthal populations. It also suggests that humans had larger trade catchment areas than Neanderthals (confirmed in the distribution of stone tools). With larger populations, social and technological innovations were easier to fix in human populations, which may have all contributed to the fact that modern Homo sapiens replaced the Neanderthal populations by 28,000 BP.
Earlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between "H. neanderthalensis" and "H. sapiens", and that the two were separate species that shared a common ancestor about 660,000 years ago. However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans "circa" 45,000 to 80,000 years ago (at the approximate time that modern humans migrated out from Africa, but before they dispersed into Europe, Asia and elsewhere). The genetic sequencing of a human from Romania dated 40,000 years ago showed that 11% of their genome was Neanderthal. This would indicate that this individual had a Neanderthal great grandparent, 4 generations previously. It seems that this individual has left no living descendants.

Nearly all modern non-African humans have 1% to 4% of their DNA derived from Neanderthal DNA, and this finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although the interpretation of these studies has been questioned. Neanderthals and "Homo sapiens" could have co-existed in Europe for as long as 10,000 years, during which human populations exploded vastly outnumbering Neanderthals, possibly outcompeting them by sheer numerical strength.

In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of Denisovans. Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.

While the divergence point of the mtDNA was unexpectedly deep in time, the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans. Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years, and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought. Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.

Alleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside of Africa. HLA haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians, indicating strong positive selection for these introgressed alleles. Corinne Simoneti at Vanderbilt University, in Nashville and her team have found from medical records of 28,000 people of European descent that the presence of Neanderthal DNA segments may be associated with a likelihood to suffer depression more frequently.

The flow of genes from Neanderthal populations to modern human was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology in Leipzig, Germany, has in 2016 reported that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show similarity to the modern human gene pool, more so than to European Neanderthal populations. The evidence suggests that the Neanderthal populations interbred with modern humans possibly 100,000 years ago, probably somewhere in the Near East.

Studies of a Neanderthal child at Gibraltar show from brain development and teeth eruption that Neanderthal children may have matured more rapidly than is the case for Homo sapiens.

"H. floresiensis", which lived from approximately 190,000 to 50,000 years before present, has been nicknamed "hobbit" for its small size, possibly a result of insular dwarfism. "H. floresiensis" is intriguing both for its size and its age, being an example of a recent species of the genus "Homo" that exhibits derived traits not shared with modern humans. In other words, "H. floresiensis" shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm (considered small for a chimpanzee and less than a third of the "H. sapiens" average of 1400 cm). 

However, there is an ongoing debate over whether "H. floresiensis" is indeed a separate species. Some scientists hold that "H. floresiensis" was a modern "H. sapiens" with pathological dwarfism. This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on "H. floresiensis" as a separate species is that it was found with tools only associated with "H. sapiens".

The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual. 

"H. sapiens" (the adjective "sapiens" is Latin for "wise" or "intelligent") emerged around 300,000 years ago, likely derived from "Homo heidelbergensis". Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from "H. erectus" to "H. sapiens". The direct evidence suggests there was a migration of "H. erectus" out of Africa, then a further speciation of "H. sapiens" from "H. erectus" in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed "H. erectus". This migration and origin theory is usually referred to as the "recent single-origin hypothesis" or "out of Africa" theory. 
"H. sapiens" interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans. 

The Toba catastrophe theory, which postulates a population bottleneck for "H. sapiens" about 70,000 years ago, was controversial from its first proposal in the 1990s and by the 2010s had very little support.

Distinctive human genetic variability has arisen as the result of the founder effect, by archaic admixture and by recent evolutionary pressures.

The use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain. Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption. Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.

Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts. There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.

It should be noted that many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are the Oldowan stone tools from Ethiopia, 2.5–2.6 million years old. A "Homo" fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the "Homo" species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence. The third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.

Bernard Wood noted that "Paranthropus" co-existed with the early "Homo" species in the area of the "Oldowan Industrial Complex" over roughly the same span of time. Although there is no direct evidence which identifies "Paranthropus" as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early "Homo" species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, "Homo" was always present, but "Paranthropus" was not.

In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the "Homo" and "Paranthropus" species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.

Stone tools are first attested around 2.6 Million years ago, when "H. habilis" in Eastern Africa used so-called pebble tools, choppers made out of round pebbles that had been split by simple strikes. This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000-10,000 years ago.

Archaeologists working in the Great Rift Valley in Kenya claim to have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.

The period from 700,000–300,000 years ago is also known as the Acheulean, when "H. ergaster" (or "erectus") made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later "retouched" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers ("racloirs"), needles, and flattened needles were made. Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). In this period they also started to make tools out of bone.

Until about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase ("H. habilis", "H. ergaster", "H. neanderthalensis") started at a higher level than the previous one, but after each phase started, further development was slow. Currently paleoanthropologists are debating whether these "Homo" species possessed some or many of the cultural and behavioral traits associated with modern humans such as language, complex symbolic thinking, technological creativity etc. It seems that they were culturally conservative maintaining simple technologies and foraging patterns over very long periods.

Around 50,000 BP, modern human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by most as a Eurasian "Great Leap Forward", or as the "Upper Palaeolithic Revolution", due to the sudden appearance of distinctive signs of modern behavior and big game hunting in the archaeological record. Some other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African "Homo sapiens" since 200,000 years ago. Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a sea journey of up to 160 km 60,000 years ago, which may diminish the evidence of the Upper Paleolithic Revolution.

Modern humans started burying their dead, using animal hides to make clothing, hunting with more sophisticated techniques (such as using trapping pits or driving animals off cliffs), and engaging in cave painting. As human culture advanced, different populations of humans introduced novelty to existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of variation among different populations of humans, something that had not been seen in human cultures prior to 50,000 BP. Typically, "H. neanderthalensis" populations do not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal innovations produced as a result of exposure to the Homo sapiens Aurignacian technologies.

Among concrete examples of modern human behavior, anthropologists include specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (for example, burials with grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks. Debate continues as to whether a "revolution" led to modern humans ("the big bang of human consciousness"), or whether the evolution was more "gradual".

Evolution has continued in anatomically modern human populations, which are affected by both natural selection and genetic drift. Although selection pressure on some traits, such as resistance to smallpox, has decreased in modern human life, humans are still undergoing natural selection for many other traits. Some of these are due to specific environmental pressures, while others are related to lifestyle changes since the development of agriculture (10,000 years ago), urban civilization (5,000), and industrialization (250 years ago). It has been argued that human evolution has accelerated since the development of agriculture 10,000 years ago and civilization some 5,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations.

Particularly conspicuous is variation in superficial characteristics, such as Afro-textured hair, or the recent evolution of light skin and blond hair in some populations, which are attributed to differences in climate. Particularly strong selective pressures have resulted in high-altitude adaptation in humans, with different ones in different isolated populations. Studies of the genetic basis show that some developed very recently, with Tibetans evolving over 3,000 years to have high proportions of an allele of EPAS1 that is adaptive to high altitudes.

Other evolution is related to endemic diseases: the presence of malaria selected for sickle cell trait (the heterozygote form of sickle cell gene), while the absence of malaria and the health effects of sickle-cell anemia select against this trait. For example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons.

Recent human evolution related to agriculture includes genetic resistance to infectious disease that has appeared in human populations by crossing the species barrier from domesticated animals, as well as changes in metabolism due to changes in diet, such as lactase persistence.

In contemporary times, since industrialization, some trends have been observed: for instance, menopause is evolving to occur later. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.<ref name="doi10.1073/pnas.0906199106"></ref>

This list is in chronological order across the table by genus. Some species/subspecies names are well-established, and some are less established – especially in genus "Homo". Please see articles for more information.




</doc>
<doc id="10328" url="https://en.wikipedia.org/wiki?curid=10328" title="Evliya Çelebi">
Evliya Çelebi

Mehmed Zilli (25 March 1611 – 1682), known as Evliya Çelebi (), was an Ottoman explorer who travelled through the territory of the Ottoman Empire and neighboring lands over a period of forty years, recording his commentary in a travelogue called the "Seyahatname" ("Book of Travel"). The name Çelebi is an honorific title meaning gentleman (see pre-1934 Turkish naming conventions).

Evliya Çelebi was born in Constantinople (now Istanbul) in 1611 to a wealthy family from Kütahya. Both his parents were attached to the Ottoman court, his father, Derviş Mehmed Zilli, as a jeweller, and his mother as an Abkhazian relation of the grand vizier Melek Ahmed Pasha. In his book, Evliya Çelebi traces his paternal genealogy back to Khoja Akhmet Yassawi, an early Sufi mystic. Evliya Çelebi received a court education from the Imperial "ulama" (scholars). He may have joined the Gulshani Sufi order, as he shows an intimate knowledge of their "khanqah" in Cairo, and a graffito exists in which he referred to himself as "Evliya-yı Gülşenî" ("Evliya of the Gülşenî").

A devout Muslim opposed to fanaticism, Evliya could recite the Quran from memory and joked freely about Islam. Though employed as clergy and entertainer to the Ottoman grandees, Evliya refused employment that would keep him from travelling. His journal writing began in Constantinople, taking notes on buildings, markets, customs and culture, and in 1640 it was extended with accounts of his travels beyond the confines of the city. The collected notes of his travels form a ten-volume work called the "Seyahatname" ("Travelogue").

He fought the House of Habsburg in Principality of Transylvania.

Evliya Çelebi died in 1684, it is unclear whether he was in Constantinople or Cairo at the time.

Evliya Çelebi visited the town of Mostar, then in Ottoman Bosnia and Herzegovina. He wrote that the name "Mostar" means "bridge-keeper", in reference to the town's celebrated bridge, 28 meters long and 20 meters high. Çelebi wrote that it "is like a rainbow arch soaring up to the skies, extending from one cliff to the other. ...I, a poor and miserable slave of Allah, have passed through 16 countries, but I have never seen such a high bridge. It is thrown from rock to rock as high as the sky."

Çelebi claimed to have encountered Native Americans as a guest in Rotterdam during his visit of 1663. He wrote: "[they] cursed those priests, saying, 'Our world used to be peaceful, but it has been filled by greedy people, who make war every year and shorten our lives.'"

While visiting Vienna in 1665–66, Çelebi noted some similarities between words in German and Persian, an early observation of the relationship between what would later be known as two Indo-European languages.

Çelebi visited Crete and in book II describes the fall of Chania to the Sultan; in book VIII he recounts the Candia campaign.

Of oil merchants in Baku Çelebi wrote: "By Allah's decree oil bubbles up out of the ground, but in the manner of hot springs, pools of water are formed with oil congealed on the surface like cream. Merchants wade into these pools and collect the oil in ladles and fill goatskins with it, these oil merchants then sell them in different regions. Revenues from this oil trade are delivered annually directly to the Safavid Shah."

Evliya Çelebi remarked on the impact of Cossack raids from Azak upon the territories of the Crimean Khanate, destroying trade routes and severely depopulating the regions. By the time of Çelebi's arrival, many of the towns visited were affected by the Cossacks, and the only place he reported as safe was the Ottoman fortress at Arabat.

Çelebi wrote of the slave trade in the Crimea:
In 1667 Çelebi expressed his marvel at the Parthenon's sculptures and described the building as "like some impregnable fortress not made by human agency." He composed a poetic supplication that the Parthenon, as "a work less of human hands than of Heaven itself, should remain standing for all time."

In contrast to many European and some Jewish travelogues of Syria and Palestine in the 17th century, Çelebi wrote one of the few detailed travelogues from an Islamic point of view. Çelebi visited Palestine twice, once in 1649 and once in 1670–1. An English translation of the first part, with some passages from the second, was published in 1935–1940 by the self-taught Palestinian scholar Stephan Hanna Stephan who worked for the Palestine Department of Antiquities.

Although many of the descriptions the "Seyâhatnâme" were written in an exaggerated manner or were plainly inventive fiction or third-source misinterpretation, his notes remain a useful guide to the culture and lifestyles of the 17th century Ottoman Empire. The first volume deals exclusively with Constantinople, the final volume with Egypt.

Currently there is no English translation of the entire "Seyahatname", although there are translations of various parts. The longest single English translation was published in 1834 by Joseph von Hammer-Purgstall, an Austrian orientalist: it may be found under the name "Evliya Efendi." Von Hammer's work covers the first two volumes (Constantinople and Anatolia) but its language is antiquated. Other translations include Erich Prokosch's nearly complete translation into German of the tenth volume, the 2004 introductory work entitled "The World of Evliya Çelebi: An Ottoman Mentality" written by University of Chicago professor Robert Dankoff, and Dankoff and Sooyong Kim's 2010 translation of select excerpts of the ten volumes, "An Ottoman Traveller: Selections from the Book of Travels of Evliya Çelebi".

Evliya is noted for having collected specimens of the languages in each region he traveled in. There are some 30 Turkic dialects and languages cataloged in the "Seyâhatnâme". Çelebi notes the similarities between several words from the German and Persian, though he denies any common Indo-European heritage. The "Seyâhatnâme" also contains the first transcriptions of many languages of the Caucasus and Tsakonian, and the only extant specimens of written Ubykh outside the linguistic literature.

In the 10 volumes of his "Seyahatname", he describes the following journeys:


"İstanbul Kanatlarımın Altında" (Istanbul Under My Wings, 1996) is a film about the lives of legendary aviator brothers Hezârfen Ahmed Çelebi and Lagâri Hasan Çelebi, and the Ottoman society in the early 17th century, during the reign of Murad IV, as witnessed and narrated by Evliya Çelebi.

Çelebi appears in Orhan Pamuk's novel "The White Castle", and is featured in "The Adventures of Captain Bathory" (Dobrodružstvá kapitána Báthoryho) novels by Slovak writer Juraj Červenák.

"Evliya Çelebi ve Ölümsüzlük Suyu" (Evliya Çelebi and the Water of Life, 2014, dir. Serkan Zelzele), a children's adaptation of Çelebi's adventures, is the first full-length Turkish animated film.

United Nations Educational, Scientific and Cultural Organization, UNESCO included the 400th anniversary of Evliya Celebi's birth in its timetable for the celebration of anniversaries.









</doc>
<doc id="10331" url="https://en.wikipedia.org/wiki?curid=10331" title="Ancient Egyptian religion">
Ancient Egyptian religion

Ancient Egyptian religion was a complex system of polytheistic beliefs and rituals which were an integral part of ancient Egyptian society. It centered on the Egyptians' interaction with many deities who were believed to be present in, and in control of, the forces of nature. Rituals such as prayers and offerings were efforts to provide for the gods and gain their favor. Formal religious practice centered on the pharaoh, the ruler of Egypt, who was believed to possess a divine power by virtue of their position. He acted as the intermediary between their people and the gods and was obligated to sustain the gods through rituals and offerings so that they could maintain order in the universe. The state dedicated enormous resources to Egyptian rituals and to the construction of the temples.

Individuals could interact with the gods for their own purposes, appealing for their help through prayer or compelling them to act through magic. These practices were distinct from, but closely linked with, the formal rituals and institutions. The popular religious tradition grew more prominent in the course of Egyptian history as the status of the Pharaoh declined. Another important aspect was the belief in the afterlife and funerary practices. The Egyptians made great efforts to ensure the survival of their souls after death, providing tombs, grave goods, and offerings to preserve the bodies and spirits of the deceased.

The religion had its roots in Egypt's prehistory and lasted for more than 3,000 years. The details of religious belief changed over time as the importance of particular gods rose and declined, and their intricate relationships shifted. At various times, certain gods became preeminent over the others, including the sun god Ra, the creator god Amun, and the mother goddess Isis. For a brief period, in the theology promulgated by the Pharaoh Akhenaten, a single god, the Aten, replaced the traditional pantheon. Ancient Egyptian religion and mythology left behind many writings and monuments, along with significant influences on ancient and modern cultures.

The beliefs and rituals now referred to as "ancient Egyptian religion" were integral within every aspect of Egyptian culture. Their language possessed no single term corresponding to the modern European concept of religion. Ancient Egyptian religion was not a monolithic institution, but consisted of a vast and varying set of beliefs and practices, linked by their common focus on the interaction between the world of humans and the world of the divine. The characteristics of the gods who populated the divine realm were inextricably linked to the Egyptians' understanding of the properties of the world in which they lived.

The Egyptians believed that the phenomena of nature were divine forces in and of themselves. These deified forces included the elements, animal characteristics, or abstract forces. The Egyptians believed in a pantheon of gods, which were involved in all aspects of nature and human society. Their religious practices were efforts to sustain and placate these phenomena and turn them to human advantage. This polytheistic system was very complex, as some deities were believed to exist in many different manifestations, and some had multiple mythological roles. Conversely, many natural forces, such as the sun, were associated with multiple deities. The diverse pantheon ranged from gods with vital roles in the universe to minor deities or "demons" with very limited or localized functions. It could include gods adopted from foreign cultures, and sometimes humans: deceased Pharaohs were believed to be divine, and occasionally, distinguished commoners such as Imhotep also became deified.

The depictions of the gods in art were not meant as literal representations of how the gods might appear if they were visible, as the gods' true natures were believed to be mysterious. Instead, these depictions gave recognizable forms to the abstract deities by using symbolic imagery to indicate each god's role in nature. Thus, for example, the funerary god Anubis was portrayed as a jackal, a creature whose scavenging habits threatened the preservation of the body, in an effort to counter this threat and employ it for protection. His black skin was symbolic of the color of mummified flesh and the fertile black soil that Egyptians saw as a symbol of resurrection. This iconography was not fixed, and many of the gods could be depicted in more than one form.

Many gods were associated with particular regions in Egypt where their cults were most important. However, these associations changed over time, and they did not mean that the god associated with a place had originated there. For instance, the god Monthu was the original patron of the city of Thebes. Over the course of the Middle Kingdom, however, he was displaced in that role by Amun, who may have arisen elsewhere. The national popularity and importance of individual gods fluctuated in a similar way.

The Egyptian gods had complex interrelationships, which partly reflected the interaction of the forces they represented. The Egyptians often grouped gods together to reflect these relationships. Some groups of deities were of indeterminate size, and were linked by their similar functions. These often consisted of minor deities with little individual identity. Other combinations linked independent deities based on the symbolic meaning of numbers in Egyptian mythology; for instance, pairs of deities usually represent the duality of opposite phenomena. One of the more common combinations was a family triad consisting of a father, mother, and child, who were worshipped together. Some groups had wide-ranging importance. One such group, the Ennead, assembled nine deities into a theological system that was involved in the mythological areas of creation, kingship, and the afterlife.

The relationships between deities could also be expressed in the process of syncretism, in which two or more different gods were linked to form a composite deity. This process was a recognition of the presence of one god "in" another when the second god took on a role belonging to the first. These links between deities were fluid, and did not represent the permanent merging of two gods into one; therefore, some gods could develop multiple syncretic connections. Sometimes, syncretism combined deities with very similar characteristics. At other times it joined gods with very different natures, as when Amun, the god of hidden power, was linked with Ra, the god of the sun. The resulting god, Amun-Ra, thus united the power that lay behind all things with the greatest and most visible force in nature.

Many deities could be given epithets that seem to indicate that they were greater than any other god, suggesting some kind of unity beyond the multitude of natural forces. In particular, this is true of a few gods who, at various times in history, rose to supreme importance in Egyptian religion. These included the royal patron Horus, the sun god Ra, and the mother goddess Isis. During the New Kingdom (c. 1550–1070 BC), Amun held this position. The theology of the period described in particular detail Amun's presence in and rule over all things, so that he, more than any other deity, embodied the all-encompassing power of the divine.

Because of theological statements like this, many past Egyptologists, such as , believed that beneath the polytheistic traditions of Egyptian religion there was an increasing belief in a unity of the divine, moving toward monotheism. Instances in Egyptian literature where "god" is mentioned without reference to any specific deity would seem to give this view added weight. However, in 1971 Erik Hornung pointed out that the traits of an apparently supreme being could be attributed to many different gods, even in periods when other gods were preeminent, and further argued that references to an unspecified "god" are meant to refer flexibly to any deity. He therefore argued that, while some individuals may have henotheistically chosen one god to worship, Egyptian religion as a whole had no notion of a divine being beyond the immediate multitude of deities. Yet the debate did not end there; Jan Assmann and James P. Allen have since asserted that the Egyptians did to some degree recognize a single divine force. In Allen's view, the notion of an underlying unity of the divine coexisted inclusively with the polytheistic tradition. It is possible that only the Egyptian theologians fully recognized this underlying unity, but it is also possible that ordinary Egyptians identified the single divine force with a single god in particular situations.

During the New Kingdom the pharaoh Akhenaten abolished the official worship of other gods in favor of the sun-disk Aten. This is often seen as the first instance of true monotheism in history, although the details of Atenist theology are still unclear and the suggestion that it was monotheistic is disputed. The exclusion of all but one god from worship was a radical departure from Egyptian tradition and some see Akhenaten as a practitioner of monolatry rather than monotheism, as he did not actively deny the existence of other gods; he simply refrained from worshipping any but the Aten. Under Akhenaten's successors Egypt reverted to its traditional religion, and Akhenaten himself came to be reviled as a heretic.

The Egyptian conception of the universe centered on "Ma'at", a word that encompasses several concepts in English, including "truth", "justice", and "order." It was the fixed, eternal order of the universe, both in the cosmos and in human society. It had existed since the creation of the world, and without it the world would lose its cohesion. In Egyptian belief, Ma'at was constantly under threat from the forces of disorder, so all of society was required to maintain it. On the human level this meant that all members of society should cooperate and coexist; on the cosmic level it meant that all of the forces of nature—the gods—should continue to function in balance. This latter goal was central to Egyptian religion. The Egyptians sought to maintain Ma'at in the cosmos by sustaining the gods through offerings and by performing rituals which staved off disorder and perpetuated the cycles of nature.

The most important part of the Egyptian view of the cosmos was the conception of time, which was greatly concerned with the maintenance of Ma'at. Throughout the linear passage of time, a cyclical pattern recurred, in which Ma'at was renewed by periodic events which echoed the original creation. Among these events were the annual Nile flood and the succession from one king to another, but the most important was the daily journey of the sun god Ra.

When thinking of the shape of the cosmos, the Egyptians saw the earth as a flat expanse of land, personified by the god Geb, over which arched the sky goddess Nut. The two were separated by Shu, the god of air. Beneath the earth lay a parallel underworld and undersky, and beyond the skies lay the infinite expanse of Nu, the chaos that had existed before creation. The Egyptians also believed in a place called the Duat, a mysterious region associated with death and rebirth, that may have lain in the underworld or in the sky. Each day, Ra traveled over the earth across the underside of the sky, and at night he passed through the Duat to be reborn at dawn.

In Egyptian belief, this cosmos was inhabited by three types of sentient beings. One was the gods; another was the spirits of deceased humans, who existed in the divine realm and possessed many of the gods' abilities. Living humans were the third category, and the most important among them was the pharaoh, who bridged the human and divine realms.

Egyptologists have long debated the degree to which the Pharaoh was considered a god. It seems most likely that the Egyptians viewed royal authority itself as a divine force. Therefore, although the Egyptians recognized that the Pharaoh was human and subject to human weakness, they simultaneously viewed him as a god, because the divine power of kingship was incarnated in him. He therefore acted as intermediary between Egypt's people and the gods. He was key to upholding Ma'at, both by maintaining justice and harmony in human society and by sustaining the gods with temples and offerings. For these reasons, he oversaw all state religious activity. However, the Pharaoh’s real-life influence and prestige could differ from that depicted in official writings and depictions, and beginning in the late New Kingdom his religious importance declined drastically.

The king was also associated with many specific deities. He was identified directly with Horus, who represented kingship itself, and he was seen as the son of Ra, who ruled and regulated nature as the Pharaoh ruled and regulated society. By the New Kingdom he was also associated with Amun, the supreme force in the cosmos. Upon his death, the king became fully deified. In this state, he was directly identified with Ra, and was also associated with Osiris, god of death and rebirth and the mythological father of Horus. Many mortuary temples were dedicated to the worship of deceased pharaohs as gods.

The Egyptians had elaborate beliefs about death and the afterlife. They believed that humans possessed a "ka", or life-force, which left the body at the point of death. In life, the "ka" received its sustenance from food and drink, so it was believed that, to endure after death, the "ka" must continue to receive offerings of food, whose spiritual essence it could still consume. Each person also had a "ba", the set of spiritual characteristics unique to each individual. Unlike the "ka", the "ba" remained attached to the body after death. Egyptian funeral rituals were intended to release the "ba" from the body so that it could move freely, and to rejoin it with the "ka" so that it could live on as an "akh". However, it was also important that the body of the deceased be preserved, as the Egyptians believed that the "ba" returned to its body each night to receive new life, before emerging in the morning as an "akh".

In early times the deceased pharaoh was believed to ascend to the sky and dwell among the stars. Over the course of the Old Kingdom (c. 2686–2181 BC), however, he came to be more closely associated with the daily rebirth of the sun god Ra and with the underworld ruler Osiris as those deities grew more important.

In the fully developed afterlife beliefs of the New Kingdom, the soul had to avoid a variety of supernatural dangers in the Duat, before undergoing a final judgment known as the "Weighing of the Heart". In this judgment, the gods compared the actions of the deceased while alive (symbolized by the heart) to Ma'at, to determine whether he or she had behaved in accordance with Ma'at. If the deceased was judged worthy, his or her "ka" and "ba" were united into an "akh". Several beliefs coexisted about the "akh"<nowiki>'</nowiki>s destination. Often the dead were said to dwell in the realm of Osiris, a lush and pleasant land in the underworld. The solar vision of the afterlife, in which the deceased soul traveled with Ra on his daily journey, was still primarily associated with royalty, but could extend to other people as well. Over the course of the Middle and New Kingdoms, the notion that the "akh" could also travel in the world of the living, and to some degree magically affect events there, became increasingly prevalent.

While the Egyptians had no unified religious scripture, they produced many religious writings of various types. Together the disparate texts provide a very extensive, but still incomplete, understanding of Egyptian religious practices and beliefs.

Egyptian myths were metaphorical stories intended to illustrate and explain the gods' actions and roles in nature. The details of the events they recounted could change to convey different symbolic perspectives on the mysterious divine events they described, so many myths exist in different and conflicting versions. Mythical narratives were rarely written in full, and more often texts only contain episodes from or allusions to a larger myth. Knowledge of Egyptian mythology, therefore, is derived mostly from hymns that detail the roles of specific deities, from ritual and magical texts which describe actions related to mythic events, and from funerary texts which mention the roles of many deities in the afterlife. Some information is also provided by allusions in secular texts. Finally, Greeks and Romans such as Plutarch recorded some of the extant myths late in Egyptian history.

Among the significant Egyptian myths were the creation myths. According to these stories, the world emerged as a dry space in the primordial ocean of chaos. Because the sun is essential to life on earth, the first rising of Ra marked the moment of this emergence. Different forms of the myth describe the process of creation in various ways: a transformation of the primordial god Atum into the elements that form the world, as the creative speech of the intellectual god Ptah, and as an act of the hidden power of Amun. Regardless of these variations, the act of creation represented the initial establishment of maat and the pattern for the subsequent cycles of time.

The most important of all Egyptian myths was the myth of Osiris and Isis. It tells of the divine ruler Osiris, who was murdered by his jealous brother Set, a god often associated with chaos. Osiris' sister and wife Isis resurrected him so that he could conceive an heir, Horus. Osiris then entered the underworld and became the ruler of the dead. Once grown, Horus fought and defeated Set to become king himself. Set's association with chaos, and the identification of Osiris and Horus as the rightful rulers, provided a rationale for Pharaonic succession and portrayed the Pharaohs as the upholders of order. At the same time, Osiris' death and rebirth were related to the Egyptian agricultural cycle, in which crops grew in the wake of the Nile inundation, and provided a template for the resurrection of human souls after death.

Another important mythic motif was the journey of Ra through the Duat each night. In the course of this journey, Ra met with Osiris, who again acted as an agent of regeneration, so that his life was renewed. He also fought each night with Apep, a serpentine god representing chaos. The defeat of Apep and the meeting with Osiris ensured the rising of the sun the next morning, an event that represented rebirth and the victory of order over chaos.

The procedures for religious rituals were frequently written on papyri, which were used as instructions for those performing the ritual. These ritual texts were kept mainly in the temple libraries. Temples themselves are also inscribed with such texts, often accompanied by illustrations. Unlike the ritual papyri, these inscriptions were not intended as instructions, but were meant to symbolically perpetuate the rituals even if, in reality, people ceased to perform them. Magical texts likewise describe rituals, although these rituals were part of the spells used for specific goals in everyday life. Despite their mundane purpose, many of these texts also originated in temple libraries and later became disseminated among the general populace.

The Egyptians produced numerous prayers and hymns, written in the form of poetry. Hymns and prayers follow a similar structure and are distinguished mainly by the purposes they serve. Hymns were written to praise particular deities. Like ritual texts, they were written on papyri and on temple walls, and they were probably recited as part of the rituals they accompany in temple inscriptions. Most are structured according to a set literary formula, designed to expound on the nature, aspects, and mythological functions of a given deity. They tend to speak more explicitly about fundamental theology than other Egyptian religious writings, and became particularly important in the New Kingdom, a period of particularly active theological discourse. Prayers follow the same general pattern as hymns, but address the relevant god in a more personal way, asking for blessings, help, or forgiveness for wrongdoing. Such prayers are rare before the New Kingdom, indicating that in earlier periods such direct personal interaction with a deity was not believed possible, or at least was less likely to be expressed in writing. They are known mainly from inscriptions on statues and stelae left in sacred sites as votive offerings.

Among the most significant and extensively preserved Egyptian writings are funerary texts designed to ensure that deceased souls reached a pleasant afterlife. The earliest of these are the Pyramid Texts. They are a loose collection of hundreds of spells inscribed on the walls of royal pyramids during the Old Kingdom, intended to magically provide pharaohs with the means to join the company of the gods in the afterlife. The spells appear in differing arrangements and combinations, and few of them appear in all of the pyramids.

At the end of the Old Kingdom a new body of funerary spells, which included material from the Pyramid Texts, began appearing in tombs, inscribed primarily on coffins. This collection of writings is known as the Coffin Texts, and was not reserved for royalty, but appeared in the tombs of non-royal officials. In the New Kingdom, several new funerary texts emerged, of which the best-known is the Book of the Dead. Unlike the earlier books, it often contains extensive illustrations, or vignettes. The book was copied on papyrus and sold to commoners to be placed in their tombs.

The Coffin Texts included sections with detailed descriptions of the underworld and instructions on how to overcome its hazards. In the New Kingdom, this material gave rise to several "books of the netherworld", including the Book of Gates, the Book of Caverns, and the Amduat. Unlike the loose collections of spells, these netherworld books are structured depictions of Ra's passage through the Duat, and by analogy, the journey of the deceased person's soul through the realm of the dead. They were originally restricted to pharaonic tombs, but in the Third Intermediate Period they came to be used more widely.

As Egypt became more modernized, its archaic practices were substituted with new and efficient scientific techniques. Some of these scientific advancements were related to the development of mummification. By enhancing their advanced practice of mummification, the Egyptians were able to reach a new level of excellency concerning afterlife.

Temples existed from the beginning of Egyptian history, and at the height of the civilization they were present in most of its towns. They included both mortuary temples to serve the spirits of deceased pharaohs and temples dedicated to patron gods, although the distinction was blurred because divinity and kingship were so closely intertwined. The temples were not primarily intended as places for worship by the general populace, and the common people had a complex set of religious practices of their own. Instead, the state-run temples served as houses for the gods, in which physical images which served as their intermediaries were cared for and provided with offerings. This service was believed to be necessary to sustain the gods, so that they could in turn maintain the universe itself. Thus, temples were central to Egyptian society, and vast resources were devoted to their upkeep, including both donations from the monarchy and large estates of their own. Pharaohs often expanded them as part of their obligation to honor the gods, so that many temples grew to enormous size. However, not all gods had temples dedicated to them, as many gods who were important in official theology received only minimal worship, and many household gods were the focus of popular veneration rather than temple ritual.

The earliest Egyptian temples were small, impermanent structures, but through the Old and Middle Kingdoms their designs grew more elaborate, and they were increasingly built out of stone. In the New Kingdom, a basic temple layout emerged, which had evolved from common elements in Old and Middle Kingdom temples. With variations, this plan was used for most of the temples built from then on, and most of those that survive today adhere to it. In this standard plan, the temple was built along a central processional way that led through a series of courts and halls to the sanctuary, which held a statue of the temple's god. Access to this most sacred part of the temple was restricted to the pharaoh and the highest-ranking priests. The journey from the temple entrance to the sanctuary was seen as a journey from the human world to the divine realm, a point emphasized by the complex mythological symbolism present in temple architecture. Well beyond the temple building proper was the outermost wall. In the space between the two lay many subsidiary buildings, including workshops and storage areas to supply the temple's needs, and the library where the temple's sacred writings and mundane records were kept, and which also served as a center of learning on a multitude of subjects.

Theoretically it was the duty of the pharaoh to carry out temple rituals, as he was Egypt's official representative to the gods. In reality, ritual duties were almost always carried out by priests. During the Old and Middle Kingdoms, there was no separate class of priests; instead, many government officials served in this capacity for several months out of the year before returning to their secular duties. Only in the New Kingdom did professional priesthood become widespread, although most lower-ranking priests were still part-time. All were still employed by the state, and the pharaoh had final say in their appointments. However, as the wealth of the temples grew, the influence of their priesthoods increased, until it rivaled that of the pharaoh. In the political fragmentation of the Third Intermediate Period (c. 1070–664 BC), the high priests of Amun at Karnak even became the effective rulers of Upper Egypt. The temple staff also included many people other than priests, such as musicians and chanters in temple ceremonies. Outside the temple were artisans and other laborers who helped supply the temple's needs, as well as farmers who worked on temple estates. All were paid with portions of the temple's income. Large temples were therefore very important centers of economic activity, sometimes employing thousands of people.

State religious practice included both temple rituals involved in the cult of a deity, and ceremonies related to divine kingship. Among the latter were coronation ceremonies and the sed festival, a ritual renewal of the pharaoh's strength that took place periodically during his reign. There were numerous temple rituals, including rites that took place across the country and rites limited to single temples or to the temples of a single god. Some were performed daily, while others took place annually or on rarer occasions. The most common temple ritual was the morning offering ceremony, performed daily in temples across Egypt. In it, a high-ranking priest, or occasionally the pharaoh, washed, anointed, and elaborately dressed the god's statue before presenting it with offerings. Afterward, when the god had consumed the spiritual essence of the offerings, the items themselves were taken to be distributed among the priests.

The less frequent temple rituals, or festivals, were still numerous, with dozens occurring every year. These festivals often entailed actions beyond simple offerings to the gods, such as reenactments of particular myths or the symbolic destruction of the forces of disorder. Most of these events were probably celebrated only by the priests and took place only inside the temple. However, the most important temple festivals, like the Opet Festival celebrated at Karnak, usually involved a procession carrying the god's image out of the sanctuary in a model barque to visit other significant sites, such as the temple of a related deity. Commoners gathered to watch the procession and sometimes received portions of the unusually large offerings given to the gods on these occasions.

At many sacred sites, the Egyptians worshipped individual animals which they believed to be manifestations of particular deities. These animals were selected based on specific sacred markings which were believed to indicate their fitness for the role. Some of these cult animals retained their positions for the rest of their lives, as with the Apis bull worshipped in Memphis as a manifestation of Ptah. Other animals were selected for much shorter periods. These cults grew more popular in later times, and many temples began raising stocks of such animals from which to choose a new divine manifestation. A separate practice developed in the Twenty-sixth Dynasty, when people began mummifying any member of a particular animal species as an offering to the god whom the species represented. Millions of mummified cats, birds, and other creatures were buried at temples honoring Egyptian deities. Worshippers paid the priests of a particular deity to obtain and mummify an animal associated with that deity, and the mummy was placed in a cemetery near the god's cult center.

The Egyptians used oracles to ask the gods for knowledge or guidance. Egyptian oracles are known mainly from the New Kingdom and afterward, though they probably appeared much earlier. People of all classes, including the king, asked questions of oracles, and, especially in the late New Kingdom their answers could be used to settle legal disputes or inform royal decisions. The most common means of consulting an oracle was to pose a question to the divine image while it was being carried in a festival procession, and interpret an answer from the barque's movements. Other methods included interpreting the behavior of cult animals, drawing lots, or consulting statues through which a priest apparently spoke. The means of discerning the god's will gave great influence to the priests who spoke and interpreted the god's message.

While the state cults were meant to preserve the stability of the Egyptian world, lay individuals had their own religious practices that related more directly to daily life. This popular religion left less evidence than the official cults, and because this evidence was mostly produced by the wealthiest portion of the Egyptian population, it is uncertain to what degree it reflects the practices of the populace as a whole.

Popular religious practice included ceremonies marking important transitions in life. These included birth, because of the danger involved in the process, and naming, because the name was held to be a crucial part of a person's identity. The most important of these ceremonies were those surrounding death (see "Funerary practices" below), because they ensured the soul's survival beyond it. Other religious practices sought to discern the gods' will or seek their knowledge. These included the interpretation of dreams, which could be seen as messages from the divine realm, and the consultation of oracles. People also sought to affect the gods' behavior to their own benefit through magical rituals (see "Magic" below).

Individual Egyptians also prayed to gods and gave them private offerings. Evidence of this type of personal piety is sparse before the New Kingdom. This is probably due to cultural restrictions on depiction of nonroyal religious activity, which relaxed during the Middle and New Kingdoms. Personal piety became still more prominent in the late New Kingdom, when it was believed that the gods intervened directly in individual lives, punishing wrongdoers and saving the pious from disaster. Official temples were important venues for private prayer and offering, even though their central activities were closed to laypeople. Egyptians frequently donated goods to be offered to the temple deity and objects inscribed with prayers to be placed in temple courts. Often they prayed in person before temple statues or in shrines set aside for their use. Yet in addition to temples, the populace also used separate local chapels, smaller but more accessible than the formal temples. These chapels were very numerous, and probably staffed by members of the community. Households, too, often had their own small shrines for offering to gods or deceased relatives.

The deities invoked in these situations differed somewhat from those at the center of state cults. Many of the important popular deities, such as the fertility goddess Taweret and the household protector Bes, had no temples of their own. However, many other gods, including Amun and Osiris, were very important in both popular and official religion. Some individuals might be particularly devoted to a single god. Often they favored deities affiliated with their own region, or with their role in life. The god Ptah, for instance, was particularly important in his cult center of Memphis, but as the patron of craftsmen he received the nationwide veneration of many in that occupation.

The word "magic" is used to translate the Egyptian term "heka", which meant, as James P. Allen puts it, "the ability to make things happen by indirect means". Heka was believed to be a natural phenomenon, the force which was used to create the universe and which the gods employed to work their will. Humans could also use it, however, and magical practices were closely intertwined with religion. In fact, even the regular rituals performed in temples were counted as magic. Individuals also frequently employed magical techniques for personal purposes. Although these ends could be harmful to other people, no form of magic was considered inimical in itself. Instead, magic was seen primarily as a way for humans to prevent or overcome negative events.
Magic was closely associated with the priesthood. Because temple libraries contained numerous magical texts, great magical knowledge was ascribed to the lector priests who studied these texts. These priests often worked outside their temples, hiring out their magical services to laymen. Other professions also commonly employed magic as part of their work, including doctors, scorpion-charmers, and makers of magical amulets. It is also likely that the peasantry used simple magic for their own purposes, but because this magical knowledge would have been passed down orally, there is limited evidence of it.

Language was closely linked with heka, to such a degree that Thoth, the god of writing, was sometimes said to be the inventor of heka. Therefore, magic frequently involved written or spoken incantations, although these were usually accompanied by ritual actions. Often these rituals invoked the power of an appropriate deity to perform the desired action, using the power of heka to compel it to act. Sometimes this entailed casting the practitioner or subject of a ritual in the role of a character in mythology, thus inducing the god to act toward that person as it had in the myth. Rituals also employed sympathetic magic, using objects believed to have a magically significant resemblance to the subject of the rite. The Egyptians also commonly used objects believed to be imbued with heka of their own, such as the magically protective amulets worn in great numbers by ordinary Egyptians.

Because it was considered necessary for the survival of the soul, preservation of the body was a central part of Egyptian funerary practices. Originally the Egyptians buried their dead in the desert, where the arid conditions mummified the body naturally. In the Early Dynastic Period, however, they began using tombs for greater protection, and the body was insulated from the desiccating effect of the sand and was subject to natural decay. Thus the Egyptians developed their elaborate embalming practices, in which the corpse was artificially desiccated and wrapped to be placed in its coffin. The quality of the process varied according to cost, however, and those who could not afford it were still buried in desert graves.
Once the mummification process was complete, the mummy was carried from the deceased person's house to the tomb in a funeral procession that included his or her friends and relatives, along with a variety of priests. Before the burial, these priests performed several rituals, including the Opening of the mouth ceremony intended to restore the dead person's senses and give him or her the ability to receive offerings. Then the mummy was buried and the tomb sealed. Afterward, relatives or hired priests gave food offerings to the deceased in a nearby mortuary chapel at regular intervals. Over time, families inevitably neglected offerings to long-dead relatives, so most mortuary cults only lasted one or two generations. However, while the cult lasted, the living sometimes wrote letters asking deceased relatives for help, in the belief that the dead could affect the world of the living as the gods did.

The first Egyptian tombs were mastabas, rectangular brick structures where kings and nobles were entombed. Each of them contained a subterranean burial chamber and a separate, above ground chapel for mortuary rituals. In the Old Kingdom the mastaba developed into the pyramid, which symbolized the primeval mound of Egyptian myth. Pyramids were reserved for royalty, and were accompanied by large mortuary temples sitting at their base. Middle Kingdom pharaohs continued to build pyramids, but the popularity of mastabas waned. Increasingly, commoners with sufficient means were buried in rock-cut tombs with separate mortuary chapels nearby, an approach which was less vulnerable to tomb robbery. By the beginning of the New Kingdom even the pharaohs were buried in such tombs, and they continued to be used until the decline of the religion itself.

Tombs could contain a great variety of other items, including statues of the deceased to serve as substitutes for the body in case it was damaged. Because it was believed that the deceased would have to do work in the afterlife, just as in life, burials often included small models of humans to do work in place of the deceased. The tombs of wealthier individuals could also contain furniture, clothing, and other everyday objects intended for use in the afterlife, along with amulets and other items intended to provide magical protection against the hazards of the spirit world. Further protection was provided by funerary texts included in the burial. The tomb walls also bore artwork, including images of the deceased eating food which were believed to allow him or her to magically receive sustenance even after the mortuary offerings had ceased.

The beginnings of Egyptian religion extend into prehistory, though evidence for them comes only from the sparse and ambiguous archaeological record. Careful burials during the Predynastic period imply that the people of this time believed in some form of an afterlife. At the same time, animals were ritually buried, a practice which may reflect the development of zoomorphic deities like those found in the later religion. The evidence is less clear for gods in human form, and this type of deity may have emerged more slowly than those in animal shape. Each region of Egypt originally had its own patron deity, but it is likely that as these small communities conquered or absorbed each other, the god of the defeated area was either incorporated into the other god's mythology or entirely subsumed by it. This resulted in a complex pantheon in which some deities remained only locally important while others developed more universal significance. As the time changed and the shifting of the empires changed like the middle kingdom, new kingdom, and old kingdom, usually the religion followed stayed within the border of that territory.

The Early Dynastic Period began with the unification of Egypt around 3000 BC. This event transformed Egyptian religion, as some deities rose to national importance and the cult of the divine pharaoh became the central focus of religious activity. Horus was identified with the king, and his cult center in the Upper Egyptian city of Nekhen was among the most important religious sites of the period. Another important center was Abydos, where the early rulers built large funerary complexes.

During the Old Kingdom, the priesthoods of the major deities attempted to organize the complicated national pantheon into groups linked by their mythology and worshipped in a single cult center, such as the Ennead of Heliopolis which linked important deities such as Atum, Ra, Osiris, and Set in a single creation myth. Meanwhile, pyramids, accompanied by large mortuary temple complexes, replaced mastabas as the tombs of pharaohs. In contrast with the great size of the pyramid complexes, temples to gods remained comparatively small, suggesting that official religion in this period emphasized the cult of the divine king more than the direct worship of deities. The funerary rituals and architecture of this time greatly influenced the more elaborate temples and rituals used in worshipping the gods in later periods.
Early in the Old Kingdom, Ra grew in influence, and his cult center at Heliopolis became the nation's most important religious site. By the Fifth Dynasty, Ra was the most prominent god in Egypt, and had developed the close links with kingship and the afterlife that he retained for the rest of Egyptian history. Around the same time, Osiris became an important afterlife deity. "The Pyramid Texts," first written at this time, reflect the prominence of the solar and Osirian concepts of the afterlife, although they also contain remnants of much older traditions. The texts are an extremely important source for understanding early Egyptian theology.

In the 22nd century BC, the Old Kingdom collapsed into the disorder of the First Intermediate Period, with important consequences for Egyptian religion. Old Kingdom officials had already begun to adopt the funerary rites originally reserved for royalty, but now, less rigid barriers between social classes meant that these practices and the accompanying beliefs gradually extended to all Egyptians, a process called the "democratization of the afterlife". The Osirian view of the afterlife had the greatest appeal to commoners, and thus Osiris became one of the most important gods.

Eventually rulers from Thebes reunified the Egyptian nation in the Middle Kingdom (c. 2055–1650 BC). These Theban pharaohs initially promoted their patron god Monthu to national importance, but during the Middle Kingdom, he was eclipsed by the rising popularity of Amun. In this new Egyptian state, personal piety grew more important and was expressed more freely in writing, a trend which continued in the New Kingdom.

The Middle Kingdom crumbled in the Second Intermediate Period (c. 1650–1550 BC), but the country was again reunited by Theban rulers, who became the first pharaohs of the New Kingdom. Under the new regime, Amun became the supreme state god. He was syncretized with Ra, the long-established patron of kingship, and his temple at Karnak in Thebes became Egypt's most important religious center. Amun's elevation was partly due to the great importance of Thebes, but it was also due to the increasingly professional priesthood. Their sophisticated theological discussion produced detailed descriptions of Amun's universal power.

Increased contact with outside peoples in this period led to the adoption of many Near Eastern deities into the pantheon. At the same time, the subjugated Nubians absorbed Egyptian religious beliefs, and in particular, adopted Amun as their own.
The New Kingdom religious order was disrupted when Akhenaten acceded, and replaced Amun with the Aten as the state god. Eventually he eliminated the official worship of most other gods, and moved Egypt's capital to a new city at Amarna. This part of Egyptian history, the Amarna period, is named after this. In doing so, Akhenaten claimed unprecedented status: only he could worship the Aten, and the populace directed their worship toward him. The Atenist system lacked well-developed mythology and afterlife beliefs, and the Aten seemed distant and impersonal, so the new order did not appeal to ordinary Egyptians. Thus, many probably continued to worship the traditional gods in private. Nevertheless, the withdrawal of state support for the other deities severely disrupted Egyptian society. Akhenaten's successors restored the traditional religious system, and eventually they dismantled all Atenist monuments.

Before the Amarna period, popular religion had trended toward more personal relationships between worshippers and their gods. Akhenaten's changes had reversed this trend, but once the traditional religion was restored, there was a backlash. The populace began to believe that the gods were much more directly involved in daily life. Amun, the supreme god, was increasingly seen as the final arbiter of human destiny, the true ruler of Egypt. The pharaoh was correspondingly more human and less divine. The importance of oracles as a means of decision-making grew, as did the wealth and influence of the oracles' interpreters, the priesthood. These trends undermined the traditional structure of society and contributed to the breakdown of the New Kingdom.

In the 1st millennium BC, Egypt was significantly weaker than in earlier times, and in several periods foreigners seized the country and assumed the position of pharaoh. The importance of the pharaoh continued to decline, and the emphasis on popular piety continued to increase. Animal cults, a characteristically Egyptian form of worship, became increasingly popular in this period, possibly as a response to the uncertainty and foreign influence of the time. Isis grew more popular as a goddess of protection, magic, and personal salvation, and became the most important goddess in Egypt.
In the 4th century BC, Egypt became a Hellenistic kingdom under the Ptolemaic dynasty (305–30 BC), which assumed the pharaonic role, maintaining the traditional religion and building or rebuilding many temples. The kingdom's Greek ruling class identified the Egyptian deities with their own. From this cross-cultural syncretism emerged Serapis, a god who combined Osiris and Apis with characteristics of Greek deities, and who became very popular among the Greek population. Nevertheless, for the most part the two belief systems remained separate, and the Egyptian deities remained Egyptian.

Ptolemaic-era beliefs changed little after Egypt became a province of the Roman Empire in 30 BC, with the Ptolemaic kings replaced by distant emperors. The cult of Isis appealed even to Greeks and Romans outside Egypt, and in Hellenized form it spread across the empire. In Egypt itself, as the empire weakened, official temples fell into decay, and without their centralizing influence religious practice became fragmented and localized. Meanwhile, Christianity spread across Egypt, and in the third and fourth centuries AD, edicts by Christian emperors and iconoclasm by local Christians eroded traditional beliefs. While it persisted among the populace for some time, Egyptian religion slowly faded away.

Egyptian religion produced the temples and tombs which are ancient Egypt's most enduring monuments, but it also influenced other cultures. In pharaonic times many of its symbols, such as the sphinx and winged solar disk, were adopted by other cultures across the Mediterranean and Near East, as were some of its deities, such as Bes. Some of these connections are difficult to trace. The Greek concept of Elysium may have derived from the Egyptian vision of the afterlife. In late antiquity, the Christian conception of Hell was most likely influenced by some of the imagery of the Duat. Biblical accounts of Jesus and Mary may have been influenced by that of Isis and Osiris. Egyptian beliefs also influenced or gave rise to several esoteric belief systems developed by Greeks and Romans, who considered Egypt as a source of mystic wisdom. Hermeticism, for instance, derived from the tradition of secret magical knowledge associated with Thoth.

Traces of ancient beliefs remained in Egyptian folk traditions into modern times, but its influence on modern societies greatly increased with the French Campaign in Egypt and Syria in 1798 and their seeing the monuments and images. As a result of it, Westerners began to study Egyptian beliefs firsthand, and Egyptian religious motifs were adopted into Western art. Egyptian religion has since had a significant influence in popular culture. Due to continued interest in Egyptian belief, in the late 20th century, several new religious groups have formed based on different reconstructions of ancient Egyptian religion.





</doc>
<doc id="10332" url="https://en.wikipedia.org/wiki?curid=10332" title="Educational psychology">
Educational psychology

Educational psychology is the branch of psychology concerned with the scientific study of human learning. The study of learning processes, from both cognitive and behavioral perspectives, allows researchers to understand individual differences in intelligence, cognitive development, affect, motivation, self-regulation, and self-concept, as well as their role in learning. The field of educational psychology relies heavily on quantitative methods, including testing and measurement, to enhance educational activities related to instructional design, classroom management, and assessment, which serve to facilitate learning processes in various educational settings across the lifespan.

Educational psychology can in part be understood through its relationship with other disciplines. It is informed primarily by psychology, bearing a relationship to that discipline analogous to the relationship between medicine and biology. It is also informed by neuroscience. Educational psychology in turn informs a wide range of specialities within educational studies, including instructional design, educational technology, curriculum development, organizational learning, special education, classroom management, and student motivation. Educational psychology both draws from and contributes to cognitive science and the learning sciences. In universities, departments of educational psychology are usually housed within faculties of education, possibly accounting for the lack of representation of educational psychology content in introductory psychology textbooks.

The field of educational psychology involves the study of memory, conceptual processes, and individual differences (via cognitive psychology) in conceptualizing new strategies for learning processes in humans. Educational psychology has been built upon theories of operant conditioning, functionalism, structuralism, constructivism, humanistic psychology, Gestalt psychology, and information processing.

Educational psychology has seen rapid growth and development as a profession in the last twenty years. School psychology began with the concept of intelligence testing leading to provisions for special education students, who could not follow the regular classroom curriculum in the early part of the 20th century. However, "school psychology" itself has built a fairly new profession based upon the practices and theories of several psychologists among many different fields. Educational psychologists are working side by side with psychiatrists, social workers, teachers, speech and language therapists, and counselors in attempt to understand the questions being raised when combining behavioral, cognitive, and social psychology in the classroom setting.

Educational psychology is a fairly new and growing field of study. Though it can date back as early as the days of Plato and Aristotle, it was not identified as a specific practice. It was unknown that everyday teaching and learning in which individuals had to think about individual differences, assessment, development, the nature of a subject being taught, problem solving, and transfer of learning was the beginning to the field of educational psychology. These topics are important to education and as a result it is important to understanding human cognition, learning, and social perception.

Educational psychology dates back to the time of Aristotle and Plato. Plato and Aristotle researched individual differences in the field of education, training of the body and the cultivation of psycho-motor skills, the formation of good character, the possibilities and limits of moral education. Some other educational topics they spoke about were the effects of music, poetry, and the other arts on the development of individual, role of teacher, and the relations between teacher and student. Plato saw knowledge as an innate ability, which evolves through experience and understanding of the world. Such a statement has evolved into a continuing argument of nature vs. nurture in understanding conditioning and learning today. Aristotle observed the phenomenon of "association." His four laws of association included succession, contiguity, similarity, and contrast. His studies examined recall and facilitated learning processes.

John Locke followed by taking issue with Plato's theory of innate learning processes. In place of this theory, he introduced a new theory of learning based on the term "tabula rasa," which means "blank slate." Locke explained that learning took place primarily through experience, and we were all born without knowledge. This doctrine is known as "empiricism," the view that knowledge is primarily built on learning and experience.

In the late 1600s, John Locke advanced the hypothesis that people learn primarily from external forces. He believed that the mind was like a blank tablet (tabula rasa), and that successions of simple impressions give rise to complex ideas through association and reflection. Locke is credited with establishing "empiricism" as a criterion for testing the validity of knowledge, thus providing a conceptual framework for later development of experimental methodology in the natural and social sciences.

Philosophers of education such as Juan Vives, Johann Pestalozzi, Friedrich Fröbel, and Johann Herbart had examined, classified and judged the methods of education centuries before the beginnings of psychology in the late 1800s.

Juan Vives (1493–1540) proposed induction as the method of study and believed in the direct observation and investigation of the study of nature. His studies focus of humanistic learning, which opposed scholasticism and was influenced by a variety of sources including philosophy, psychology, politics, religion, and history. He was one of the first to emphasize that the location of the school is important to learning. He suggested that the school should be located away from disturbing noises; the air quality should be good and there should be plenty of food for the students and teachers. Vives emphasized the importance of understanding individual differences of the students and suggested practice as an important tool for learning.

Vives introduced his educational ideas in his writing, "De anima et vita" in 1538. In this publication, Vives explores moral philosophy as a setting for his educational ideals; with this, he explains that the different parts of the soul (similar to that of Aristotle's ideas) are each responsible for different operations, which function distinctively. The first book covers the different "souls": "The Vegatative Soul;" this is the soul of nutrition, growth, and reproduction, "The Sensitive Soul," which involves the five external senses; "The Cogitative soul," which includes internal senses and cognitive facilities. The second book involves functions of the rational soul: mind, will, and memory. Lastly, the third book explains the analysis of emotions.

Johann Pestalozzi (1746–1827), a Swiss educational reformer, emphasized the child rather than the content of the school. Pestalozzi fostered an educational reform backed by the idea that early education was crucial for children, and could be manageable for mothers. Eventually, this experience with early education would lead to a "wholesome person characterized by morality." Pestalozzi has been acknowledged for opening institutions for education, writing books for mother's teaching home education, and elementary books for students, mostly focusing on the kindergarten level. In his later years, he published teaching manuals and methods of teaching.

During the time of The Enlightenment, Pestalozzi's ideals introduced "educationalisation." This created the bridge between social issues and education by introducing the idea of social issues to be solved through education. Horlacher describes the most prominent example of this during The Enlightenment to be "improving agricultural production methods."

Johann Herbart (1776–1841) is considered the father of educational psychology. He believed that learning was influenced by interest in the subject and the teacher. He thought that teachers should consider the students' existing mental sets—what they already know—when presenting new information or material. Herbart came up with what are now known as the formal steps. The 5 steps that teachers should use are:


The period of 1890–1920 is considered the golden era of educational psychology where aspirations of the new discipline rested on the application of the scientific methods of observation and experimentation to educational problems. From 1840 to 1920 37 million people immigrated to the United States. This created an expansion of elementary schools and secondary schools. The increase in immigration also provided educational psychologists the opportunity to use intelligence testing to screen immigrants at Ellis Island. Darwinism influenced the beliefs of the prominent educational psychologists. Even in the earliest years of the discipline, educational psychologists recognized the limitations of this new approach. The pioneering American psychologist William James commented that: 

James is the father of psychology in America but he also made contributions to educational psychology. In his famous series of lectures "Talks to Teachers on Psychology", published in 1899 and now regarded as the first educational psychology textbook, James defines education as "the organization of acquired habits of conduct and tendencies to behavior". He states that teachers should "train the pupil to behavior" so that he fits into the social and physical world. Teachers should also realize the importance of habit and instinct. They should present information that is clear and interesting and relate this new information and material to things the student already knows about. He also addresses important issues such as attention, memory, and association of ideas.

Alfred Binet published "Mental Fatigue" in 1898, in which he attempted to apply the experimental method to educational psychology. In this experimental method he advocated for two types of experiments, experiments done in the lab and experiments done in the classroom. In 1904 he was appointed the Minister of Public Education. This is when he began to look for a way to distinguish children with developmental disabilities. Binet strongly supported special education programs because he believed that "abnormality" could be cured. The Binet-Simon test was the first intelligence test and was the first to distinguish between "normal children" and those with developmental disabilities. Binet believed that it was important to study individual differences between age groups and children of the same age. He also believed that it was important for teachers to take into account individual students strengths and also the needs of the classroom as a whole when teaching and creating a good learning environment. He also believed that it was important to train teachers in observation so that they would be able to see individual differences among children and adjust the curriculum to the students. Binet also emphasized that practice of material was important. In 1916 Lewis Terman revised the Binet-Simon so that the average score was always 100. The test became known as the Stanford-Binet and was one of the most widely used tests of intelligence. Terman, unlike Binet, was interested in using intelligence test to identify gifted children who had high intelligence. In his longitudinal study of gifted children, who became known as the Termites, Terman found that gifted children become gifted adults.

Edward Thorndike (1874–1949) supported the scientific movement in education. He based teaching practices on empirical evidence and measurement. Thorndike developed the theory of instrumental conditioning or the law of effect. The law of effect states that associations are strengthened when it is followed by something pleasing and associations are weakened when followed by something not pleasing. He also found that learning is done a little at a time or in increments, learning is an automatic process and all the principles of learning apply to all mammals. Thorndike's research with Robert Woodworth on the theory of transfer found that learning one subject will only influence your ability to learn another subject if the subjects are similar. This discovery led to less emphasis on learning the classics because they found that studying the classics does not contribute to overall general intelligence. Thorndike was one of the first to say that individual differences in cognitive tasks were due to how many stimulus response patterns a person had rather than a general intellectual ability. He contributed word dictionaries that were scientifically based to determine the words and definitions used. The dictionaries were the first to take into consideration the users maturity level. He also integrated pictures and easier pronunciation guide into each of the definitions. Thorndike contributed arithmetic books based on learning theory. He made all the problems more realistic and relevant to what was being studied, not just to improve the general intelligence. He developed tests that were standardized to measure performance in school related subjects. His biggest contribution to testing was the CAVD intelligence test which used a multidimensional approach to intelligence and the first to use a ratio scale. His later work was on programmed instruction, mastery learning and computer-based learning:

John Dewey (1859–1952) had a major influence on the development of progressive education in the United States. He believed that the classroom should prepare children to be good citizens and facilitate creative intelligence. He pushed for the creation of practical classes that could be applied outside of a school setting. He also thought that education should be student-oriented, not subject-oriented. For Dewey, education was a social experience that helped bring together generations of people. He stated that students learn by doing. He believed in an active mind that was able to be educated through observation, problem solving and enquiry. In his 1910 book "How We Think", he emphasizes that material should be provided in a way that is stimulating and interesting to the student since it encourages original thought and problem solving. He also stated that material should be relative to the student's own experience.

Jean Piaget (1896–1980) developed the theory of cognitive development. The theory stated that intelligence developed in four different stages. The stages are the sensorimotor stage from birth to 2 years old, the preoperational state from 2 years old to 7 years old, the concrete operational stage from 7 years old to 10 years old, and formal operational stage from 11 years old and up. He also believed that learning was constrained to the child's cognitive development. Piaget influenced educational psychology because he was the first to believe that cognitive development was important and something that should be paid attention to in education. Most of the research on Piagetian theory was carried out by American educational psychologists.

The number of people receiving a high school and college education increased dramatically from 1920 to 1960. Because very few jobs were available to teens coming out of eighth grade, there was an increase in high school attendance in the 1930s. The progressive movement in the United State took off at this time and led to the idea of progressive education. John Flanagan, an educational psychologist, developed tests for combat trainees and instructions in combat training. In 1954 the work of Kenneth Clark and his wife on the effects of segregation on black and white children was influential in the Supreme Court case Brown v. Board of Education. From the 1960s to present day, educational psychology has switched from a behaviorist perspective to a more cognitive based perspective because of the influence and development of cognitive psychology at this time.

Jerome Bruner is notable for integrating Piaget's cognitive approaches into educational psychology. He advocated for discovery learning where teachers create a problem solving environment that allows the student to question, explore and experiment. In his book "The Process of Education" Bruner stated that the structure of the material and the cognitive abilities of the person are important in learning. He emphasized the importance of the subject matter. He also believed that how the subject was structured was important for the student's understanding of the subject and it is the goal of the teacher to structure the subject in a way that was easy for the student to understand. In the early 1960s Bruner went to Africa to teach math and science to school children, which influenced his view as schooling as a cultural institution. Bruner was also influential in the development of MACOS, Man a Course of Study, which was an educational program that combined anthropology and science. The program explored human evolution and social behavior. He also helped with the development of the head start program. He was interested in the influence of culture on education and looked at the impact of poverty on educational development.

Benjamin Bloom (1913–1999) spent over 50 years at the University of Chicago, where he worked in the department of education. He believed that all students can learn. He developed taxonomy of educational objectives. The objectives were divided into three domains: cognitive, affective, and psychomotor. The cognitive domain deals with how we think. It is divided into categories that are on a continuum from easiest to more complex. The categories are knowledge or recall, comprehension application, analysis, synthesis and evaluation. The affective domain deals with emotions and has 5 categories. The categories are receiving phenomenon, responding to that phenomenon, valuing, organization, and internalizing values. The psychomotor domain deals with the development of motor skills, movement and coordination and has 7 categories, that also goes from simplest to complex. The 7 categories of the psychomotor domain are perception, set, guided response, mechanism, complex overt response, adaptation, and origination. The taxonomy provided broad educational objectives that could be used to help expand the curriculum to match the ideas in the taxonomy. The taxonomy is considered to have a greater influence internationally than in the United States. Internationally, the taxonomy is used in every aspect of education from training of the teachers to the development of testing material. Bloom believed in communicating clear learning goals and promoting an active student. He thought that teachers should provide feedback to the students on their strengths and weaknesses. Bloom also did research on college students and their problem solving processes. He found that they differ in understanding the basis of the problem and the ideas in the problem. He also found that students differ in process of problem solving in their approach and attitude toward the problem.

Nathaniel Gage (1917 -2008) is an important figure in educational psychology as his research focused on improving teaching and understanding the processes involved in teaching. He edited the book "Handbook of Research on Teaching" (1963), which helped develop early research in teaching and educational psychology. Gage founded the Stanford Center for Research and Development in Teaching, which contributed research on teaching as well as influencing the education of important educational psychologists.

Applied behavior analysis, a research-based science utilizing behavioral principles of operant conditioning, is effective in a range of educational settings. For example, teachers can alter student behavior by systematically rewarding students who follow classroom rules with praise, stars, or tokens exchangeable for sundry items. Despite the demonstrated efficacy of awards in changing behavior, their use in education has been criticized by proponents of self-determination theory, who claim that praise and other rewards undermine intrinsic motivation. There is evidence that tangible rewards decrease intrinsic motivation in specific situations, such as when the student already has a high level of intrinsic motivation to perform the goal behavior. But the results showing detrimental effects are counterbalanced by evidence that, in other situations, such as when rewards are given for attaining a gradually increasing standard of performance, rewards enhance intrinsic motivation. Many effective therapies have been based on the principles of applied behavior analysis, including pivotal response therapy which is used to treat autism spectrum disorders.

Among current educational psychologists, the cognitive perspective is more widely held than the behavioral perspective, perhaps because it admits causally related mental constructs such as traits, beliefs, memories, motivations and emotions. Cognitive theories claim that memory structures determine how information is perceived, processed, stored, retrieved and forgotten. Among the memory structures theorized by cognitive psychologists are separate but linked visual and verbal systems described by Allan Paivio's dual coding theory. Educational psychologists have used dual coding theory and cognitive load theory to explain how people learn from multimedia presentations.
The spaced learning effect, a cognitive phenomenon strongly supported by psychological research, has broad applicability within education. For example, students have been found to perform better on a test of knowledge about a text passage when a second reading of the passage is delayed rather than immediate (see figure). Educational psychology research has confirmed the applicability to education of other findings from cognitive psychology, such as the benefits of using mnemonics for immediate and delayed retention of information.

Problem solving, according to prominent cognitive psychologists, is fundamental to learning. It resides as an important research topic in educational psychology. A student is thought to interpret a problem by assigning it to a schema retrieved from long-term memory. A problem students run into while reading is called "activation." This is when the student's representations of the text are present during working memory. This causes the student to read through the material without absorbing the information and being able to retain it. When working memory is absent from the readers representations of the working memory they experience something called "deactivation." When deactivation occurs, the student has an understanding of the material and is able to retain information. If deactivation occurs during the first reading, the reader does not need to undergo deactivation in the second reading. The reader will only need to reread to get a "gist" of the text to spark their memory. When the problem is assigned to the wrong schema, the student's attention is subsequently directed away from features of the problem that are inconsistent with the assigned schema. The critical step of finding a mapping between the problem and a pre-existing schema is often cited as supporting the centrality of analogical thinking to problem solving.

Each person has an individual profile of characteristics, abilities and challenges that result from predisposition, learning and development. These manifest as individual differences in intelligence, creativity, cognitive style, motivation and the capacity to process information, communicate, and relate to others. The most prevalent disabilities found among school age children are attention deficit hyperactivity disorder (ADHD), learning disability, dyslexia, and speech disorder. Less common disabilities include intellectual disability, hearing impairment, cerebral palsy, epilepsy, and blindness.

Although theories of intelligence have been discussed by philosophers since Plato, intelligence testing is an invention of educational psychology, and is coincident with the development of that discipline. Continuing debates about the nature of intelligence revolve on whether intelligence can be characterized by a single factor known as general intelligence, multiple factors (e.g., Gardner's theory of multiple intelligences), or whether it can be measured at all. In practice, standardized instruments such as the Stanford-Binet IQ test and the WISC are widely used in economically developed countries to identify children in need of individualized educational treatment. Children classified as gifted are often provided with accelerated or enriched programs. Children with identified deficits may be provided with enhanced education in specific skills such as phonological awareness. In addition to basic abilities, the individual's personality traits are also important, with people higher in conscientiousness and hope attaining superior academic achievements, even after controlling for intelligence and past performance.

Developmental psychology, and especially the psychology of cognitive development, opens a special perspective for educational psychology. This is so because education and the psychology of cognitive development converge on a number of crucial assumptions. First, the psychology of cognitive development defines human cognitive competence at successive phases of development. Education aims to help students acquire knowledge and develop skills which are compatible with their understanding and problem-solving capabilities at different ages. Thus, knowing the students' level on a developmental sequence provides information on the kind and level of knowledge they can assimilate, which, in turn, can be used as a frame for organizing the subject matter to be taught at different school grades. This is the reason why Piaget's theory of cognitive development was so influential for education, especially mathematics and science education. In the same direction, the neo-Piagetian theories of cognitive development suggest that in addition to the concerns above, sequencing of concepts and skills in teaching must take account of the processing and working memory capacities that characterize successive age levels.
Second, the psychology of cognitive development involves understanding how cognitive change takes place and recognizing the factors and processes which enable cognitive competence to develop. Education also capitalizes on cognitive change, because the construction of knowledge presupposes effective teaching methods that would move the student from a lower to a higher level of understanding. Mechanisms such as reflection on actual or mental actions vis-à-vis alternative solutions to problems, tagging new concepts or solutions to symbols that help one recall and mentally manipulate them are just a few examples of how mechanisms of cognitive development may be used to facilitate learning.

Finally, the psychology of cognitive development is concerned with individual differences in the organization of cognitive processes and abilities, in their rate of change, and in their mechanisms of change. The principles underlying intra- and inter-individual differences could be educationally useful, because knowing how students differ in regard to the various dimensions of cognitive development, such as processing and representational capacity, self-understanding and self-regulation, and the various domains of understanding, such as mathematical, scientific, or verbal abilities, would enable the teacher to cater for the needs of the different students so that no one is left behind.

Constructivism is a category of learning theory in which emphasis is placed on the agency and prior "knowing" and experience of the learner, and often on the social and cultural determinants of the learning process. Educational psychologists distinguish individual (or psychological) constructivism, identified with Piaget's theory of cognitive development, from social constructivism. A dominant influence on the latter type is Lev Vygotsky's work on sociocultural learning, describing how interactions with adults, more capable peers, and cognitive tools are internalized to form mental constructs. Elaborating on Vygotsky's theory, Jerome Bruner and other educational psychologists developed the important concept of instructional scaffolding, in which the social or information environment offers supports for learning that are gradually withdrawn as they become internalized.

To understand the characteristics of learners in childhood, adolescence, adulthood, and old age, educational psychology develops and applies theories of human development. Often represented as stages through which people pass as they mature, developmental theories describe changes in mental abilities (cognition), social roles, moral reasoning, and beliefs about the nature of knowledge.

For example, educational psychologists have conducted research on the instructional applicability of Jean Piaget's theory of development, according to which children mature through four stages of cognitive capability. Piaget hypothesized that children are not capable of abstract logical thought until they are older than about 11 years, and therefore younger children need to be taught using concrete objects and examples. Researchers have found that transitions, such as from concrete to abstract logical thought, do not occur at the same time in all domains. A child may be able to think abstractly about mathematics, but remain limited to concrete thought when reasoning about human relationships. Perhaps Piaget's most enduring contribution is his insight that people actively construct their understanding through a self-regulatory process.

Piaget proposed a developmental theory of moral reasoning in which children progress from a naïve understanding of morality based on behavior and outcomes to a more advanced understanding based on intentions. Piaget's views of moral development were elaborated by Kohlberg into a stage theory of moral development. There is evidence that the moral reasoning described in stage theories is not sufficient to account for moral behavior. For example, other factors such as modeling (as described by the social cognitive theory of morality) are required to explain bullying.

Rudolf Steiner's model of child development interrelates physical, emotional, cognitive, and moral development in developmental stages similar to those later described by Piaget.

Developmental theories are sometimes presented not as shifts between qualitatively different stages, but as gradual increments on separate dimensions. Development of epistemological beliefs (beliefs about knowledge) have been described in terms of gradual changes in people's belief in: certainty and permanence of knowledge, fixedness of ability, and credibility of authorities such as teachers and experts. People develop more sophisticated beliefs about knowledge as they gain in education and maturity.

Motivation is an internal state that activates, guides and sustains behavior. Motivation can have several impacting effects on how students learn and how they behave towards subject matter: 

Educational psychology research on motivation is concerned with the volition or will that students bring to a task, their level of interest and intrinsic motivation, the personally held goals that guide their behavior, and their belief about the causes of their success or failure. As intrinsic motivation deals with activities that act as their own rewards, extrinsic motivation deals with motivations that are brought on by consequences or punishments. A form of attribution theory developed by Bernard Weiner describes how students' beliefs about the causes of academic success or failure affect their emotions and motivations. For example, when students attribute failure to lack of ability, and ability is perceived as uncontrollable, they experience the emotions of shame and embarrassment and consequently decrease effort and show poorer performance. In contrast, when students attribute failure to lack of effort, and effort is perceived as controllable, they experience the emotion of guilt and consequently increase effort and show improved performance.

The self-determination theory (SDT) was developed by psychologists Edward Deci and Richard Ryan. SDT focuses on the importance of intrinsic and extrinsic motivation in driving human behavior and posits inherent growth and development tendencies. It emphasizes the degree to which an individual's behavior is self-motivated and self-determined. When applied to the realm of education, the self-determination theory is concerned primarily with promoting in students an interest in learning, a value of education, and a confidence in their own capacities and attributes.

Motivational theories also explain how learners' goals affect the way they engage with academic tasks. Those who have "mastery goals" strive to increase their ability and knowledge. Those who have "performance approach goals" strive for high grades and seek opportunities to demonstrate their abilities. Those who have "performance avoidance" goals are driven by fear of failure and avoid situations where their abilities are exposed. Research has found that mastery goals are associated with many positive outcomes such as persistence in the face of failure, preference for challenging tasks, creativity and intrinsic motivation. Performance avoidance goals are associated with negative outcomes such as poor concentration while studying, disorganized studying, less self-regulation, shallow information processing and test anxiety. Performance approach goals are associated with positive outcomes, and some negative outcomes such as an unwillingness to seek help and shallow information processing.

Locus of control is a salient factor in the successful academic performance of students. During the 1970s and '80s, Cassandra B. Whyte did significant educational research studying locus of control as related to the academic achievement of students pursuing higher education coursework. Much of her educational research and publications focused upon the theories of Julian B. Rotter in regard to the importance of internal control and successful academic performance. Whyte reported that individuals who perceive and believe that their hard work may lead to more successful academic outcomes, instead of depending on luck or fate, persist and achieve academically at a higher level. Therefore, it is important to provide education and counseling in this regard.

Instructional design, the systematic design of materials, activities and interactive environments for learning, is broadly informed by educational psychology theories and research. For example, in defining learning goals or objectives, instructional designers often use a taxonomy of educational objectives created by Benjamin Bloom and colleagues. Bloom also researched mastery learning, an instructional strategy in which learners only advance to a new learning objective after they have mastered its prerequisite objectives. Bloom discovered that a combination of mastery learning with one-to-one tutoring is highly effective, producing learning outcomes far exceeding those normally achieved in classroom instruction. Gagné, another psychologist, had earlier developed an influential method of task analysis in which a terminal learning goal is expanded into a hierarchy of learning objectives connected by prerequisite relationships.
The following list of technological resources incorporate computer-aided instruction and intelligence for educational psychologists and their students: 

Technology is essential to the field of educational psychology, not only for the psychologist themselves as far as testing, organization, and resources, but also for students. Educational Psychologists whom reside in the K- 12 setting focus the majority of their time with Special Education students. It has been found that students with disabilities learning through technology such as IPad applications and videos are more engaged and motivated to learn in the classroom setting. Liu et al. explain that learning-based technology allows for students to be more focused, and learning is more efficient with learning technologies. The authors explain that learning technology also allows for students with social- emotional disabilities to participate in distance learning.

Research on classroom management and pedagogy is conducted to guide teaching practice and form a foundation for teacher education programs. The goals of classroom management are to create an environment conducive to learning and to develop students' self-management skills. More specifically, classroom management strives to create positive teacher–student and peer relationships, manage student groups to sustain on-task behavior, and use counseling and other psychological methods to aid students who present persistent psychosocial problems.

Introductory educational psychology is a commonly required area of study in most North American teacher education programs. When taught in that context, its content varies, but it typically emphasizes learning theories (especially cognitively oriented ones), issues about motivation, assessment of students' learning, and classroom management. A developing gives more detail about the educational psychology topics that are typically presented in preservice teacher education.

In order to become an educational psychologist, students can complete an undergraduate degree in their choice. They then must go to graduate school to study education psychology, counseling psychology, and/ or school counseling. Most students today are also receiving their doctorate degrees in order to hold the "psychologist" title. Educational psychologists work in a variety of settings. Some work in university settings where they carry out research on the cognitive and social processes of human development, learning and education. Educational psychologists may also work as consultants in designing and creating educational materials, classroom programs and online courses.Educational psychologists who work in k–12 school settings (closely related are school psychologists in the US and Canada) are trained at the master's and doctoral levels. In addition to conducting assessments, school psychologists provide services such as academic and behavioral intervention, counseling, teacher consultation, and crisis intervention. However, school psychologists are generally more individual-oriented towards students.

Many colleges and high schools are starting to teach students how to teach students in the classroom. In colleges educational psychology is starting to be a general education requirement.

Employment for psychologists in the United States is expected to grow faster than most occupations through the year 2014, with anticipated growth of 18–26%. One in four psychologists are employed in educational settings. In the United States, the median salary for psychologists in primary and secondary schools is US$58,360 as of May 2004. Colleges offer and allow someone to obtain an PHD in educational Psychology.

In recent decades the participation of women as professional researchers in North American educational psychology has risen dramatically.

Educational psychology, as much as any other field of psychology heavily relies on a balance of pure observation and quantitative methods in psychology. The study of education generally combines the studies of history, sociology, and ethics with theoretical approaches. Smeyers and Depaepe explain that historically, the study of education and child rearing have been associated with the interests of policymakers and practitioners within the educational field, however, the recent shift to sociology and psychology has opened the door for new findings in education as a social science. Now being its own academic discipline, educational psychology has proven to be helpful for social science researchers.

Quantitative research is the backing to most observable phenomena in psychology. This involves observing, creating, and understanding a distribution of data based upon the studies subject matter. Researchers use particular variables to interpret their data distributions from their research and employ statistics as a way of creating data tables and analyzing their data. Psychology has moved from the "common sense" reputations initially posed by Thomas Reid to the methodology approach comparing independent and dependent variables through natural observation, experiments, or combinations of the two. Though results are still, with statistical methods, objectively true based upon significance variables or p- values.





</doc>
<doc id="10333" url="https://en.wikipedia.org/wiki?curid=10333" title="EFTPOS">
EFTPOS

Electronic funds transfer at point of sale (EFTPOS ) is an electronic payment system involving electronic funds transfers based on the use of payment cards, such as debit or credit cards, at payment terminals located at points of sale. EFTPOS technology originated in the United States in 1981 and was adopted by other countries. In Australia and New Zealand, it is also the brand name of a specific system used for such payments; these systems are mainly country specific and do not interconnect.

Debit and credit cards are embossed plastic cards complying with ISO/IEC 7810 ID-1 standard. The cards have an embossed bank card number conforming with the ISO/IEC 7812 numbering standard.

EFTPOS technology originated in the United States in 1981 and was rolled out in 1982. Initially, a number of nationwide systems were set up, such as "Interlink", which were limited to participating correspondent banking relationships, not being linked to each other. Consumers and merchants were slow to accept it, and there was minimal marketing. As a result, growth and market penetration of EFTPOS was minimal in the US up to the turn of the century.

In a short time, other countries adopted the EFTPOS technology, but these systems too were limited to the national borders. Each country adopted various interbank co-operative models. In New Zealand, Bank of New Zealand started issuing EFTPOS debit cards in 1985 with the first merchant terminals being installed in petrol stations. In Australia, in 1984 Westpac was the first major Australian bank to implement an EFTPOS system, at BP petrol stations. The other major banks implemented EFTPOS systems during 1984, initially with petrol stations. The banks' existing debit and credit cards (but only allowed to access debit accounts) were used in the EFTPOS systems. In 1985, the State Bank of Victoria developed the capacity to host connect individual ATMS and helped create the ATM (Financial) Network. Banks started to link their EFTPOS systems to provide access for all customers across all EFTPOS devices. Cards issued by all banks could then be used at all EFTPOS terminals nationally, but debit cards issued in other countries could not. Prior to 1986, the Australian banks organized a widespread uniform credit card, called Bankcard, which had been in existence since 1974. There was a dispute between the banks whether Bankcard (or credit cards in general) should be permitted into the proposed EFTPOS system. At that time several banks were actively promoting MasterCard and Visa credit cards. Store cards and proprietary cards were shut out of the new system.

In 1996, mobile EFTPOS arrived, with hotels in Kuala Lumpur installing systems in 1997 and the first example of a pizza delivery in Singapore accepting Visa card via cellular payment in 1998 (a collaboration between Signet, Visa, Citi Bank, and Dynamic Data Systems) beginning the rollout of mobile systems in Asia. By 2004 Cellular based Eftpos infrastructure had really taken off, and by 2010 Cellular Eftpos had become the standard for the Global market.

Since 2002 the use of EFTPOS has grown significantly, and it has become the standard payment method, displacing the use of cash. Subsequently, networks facilitating the process of money transfer and payment settlement between the consumer and the merchant grew from a small number of nationwide systems to the majority of payment processing transactions. For EFTPOS, USA based systems allow the use of debit cards or credit cards.

In recent years, MasterCard and Visa have introduced a debit card which is widely accepted internationally. International transactions are generally in the local currency, requiring a currency exchange by the card company to the currency of the primary account. Other charges may also apply.

In Australia, debit and credit cards are the most common non-cash payment methods at “points of sale” (POS) or via ATMs. Not all merchants provide eftpos facilities, but those who wish to accept eftpos payments must enter an agreement with one of the seven merchant service providers, which rent an eftpos terminal to the merchant. The eftpos system in Australia is managed by Eftpos Payments Australia Ltd, which also sets the EFTPOS interchange fee. For credit cards to be accepted by a merchant a separate agreement must be entered into with each credit card company, each of which has its own flexible merchant fee rate.

The clearing arrangements for eftpos are regulated by Australian Payments Clearing Association (APCA). The system for ATM and eftpos interchanges is called Issuers and Acquirers Community (formerly Consumer Electronic Clearing System (CECS)) also called CS3. CECS required authorisations from the Australian Competition and Consumer Commission (ACCC), which was obtained in 2001 and reaffirmed in 2009. ATM and EFTPOS clearances are the made under individual bilateral arrangements between the institutions involved.

Australian financial institutions provide their customers a plastic card, which can be used as a debit card or as an ATM card, and sometimes as a credit card. The card merely provides the means by which a customer's linked bank or other accounts can be accessed using an EFTPOS terminal or ATM. These cards can also be used on some vending machines and other automatic payment mechanisms, such as ticket vending machines. Australian debit cards cannot be used for online and telephone banking transactions, unless they are also a credit card.

Each Australian bank has given a different name to its debit cards, such as:

Some banks offer alternative debit card facilities to their customers using the Visa or MasterCard clearance system. For example, St George Bank offers a Visa Debit Card, as does the National Australia Bank. The main difference with regular debit cards is that these cards can be used outside Australia where the respective credit card is accepted.

Those merchants that enter the EFTPOS payment system must accept debit cards issued by any Australian bank, and some also accept various credit cards and other cards. Some merchants set minimum transaction amounts for EFTPOS transactions, which can be different for debit and credit card transactions. Some merchant impose a surcharge on the use of EFTPOS. These can vary between merchants and on the type of card being used, and generally are not imposed on debit card transactions, and widely not on MasterCard and a Visa credit card transactions.

A feature of a debit card is that an EFTPOS transaction will only be accepted if there is an available credit balance in the bank cheque or savings account linked to the card.

Australian debit cards normally cannot be used outside Australia. They can only be used outside Australia if they carry the MasterCard/Maestro/Cirrus or Visa/Plus or other similar logos, in which case the non-Australian transaction will be processed through those transaction systems. Similarly, non-Australian debit and credit cards can only be used at Australian EFTPOS terminals or ATMs if they have these logos or the MasterCard or Visa logos. Diners Club and/or American Express cards will be accepted only if the merchant has an agreement with those card companies, or increasingly if the merchant has modern alternative payment options available for those cards, such as through Paypal. The Discover Card is accepted in Australia as a Diners Club card .

In addition, credit card companies issue prepaid cards which act like generic gift cards, which are anonymous and not linked to any bank accounts. These cards are accepted by merchants who accept credit cards and are processed through the EFTPOS terminal in the same way as credit cards.

A number of merchants permit customers using a debit card to withdraw cash as part of the EFTPOS transaction. In Australia, this facility (known as debit card cashback in many other countries) is known as "cash out". For the merchant, cash out is a way of reducing their net cash takings, saving on banking of cash. There is no additional cost to the merchant in providing cash out because banks charge a merchant a debit card transaction fee per EFTPOS transaction, and not on the transaction value. Cash out is a facility provided by the merchant, and not the bank, so the merchant can limit or vary how much cash can be withdrawn at a time, or suspend the facility at any time. When available, cash out is convenient for the customer, who can bypass having to visit a bank branch or ATM. Cash out is also cheaper for the customer, since only one bank transaction is involved. For people in some remote areas, cash out may be the only way they can withdraw cash from their personal accounts. However, most merchants who provide the facility set a relatively low limit on cash out, generally $50, and some also charge for the service. Some merchants in Australia only allow cash out with the purchase of goods; other merchants allow cash out whether or not customers buy any goods. Cash out is not available in association with credit card sales because on credit card transactions the merchant is charged a percentage commission based on the transaction value, and also because cash withdrawals are treated differently from purchase transactions by the credit card company. (However, though inconsistent with a merchant's agreement with each credit card company, the merchant may treat a cash withdrawal as part of an ordinary credit card sale.)

EFTPOS transactions involving a debit, credit or prepaid card are primarily authenticated via the entry of a personal identification number (PIN) at the point of sale. Historically, these transactions were authenticated by the merchant using the cardholder's signature, as signed on their receipt. However, merchants had become increasingly lax in enforcing this verification, resulting in an increase in fraud. Australian banks have since deployed chip and PIN technology using the global EMV card standard; as of 1 August 2014, Australian merchants no longer accept signatures on transactions by domestic customers at point of sale terminals.

As a further security measure, if a user enters an incorrect PIN three times, the card may be locked out of EFTPOS and require reactivation over the phone or at a bank branch. In the case of an ATM, the card will not be returned, and the cardholder will need to visit the branch to retrieve the card, or request a new card to be issued.

All debit cards now have a magnetic stripe on which is encoded the card's service codes, consisting of three-digit values. These codes are used to convey instructions to merchant terminals on how a card should be processed. The first digit indicates if a card can be used internationally or is valid for domestic use only. It is also used to signal if the card is chip-enabled. The second digit indicates if the transaction must be sent online for authorization always or if transactions that are below floor limit can take place without authorization. The third digit is used to indicate the preferred card verification method (e.g. PIN) and the environment where the card can be used (e.g. at point of sale only). Merchant terminals are required to recognize and act on service codes or send all transactions for online authorization.

In the late 2000s, MasterCard and Visa introduced contactless smart debit cards under the brand names MasterCard PayPass and Visa payWave. These payments are made using either electronic payment networks separate from the regular EFTPOS payment networks, or newer EFTPOS with tap sensors. And is an alternative to the previous swipe or chip systems. These networks are operated by MasterCard and Visa, and not by the banks as is the EFTPOS network, through EFTPOS Payments Australia Limited (ePAL).

These cards are based on EMV technology and contain a RFID chip and antenna loop embedded in the plastic of the card. To pay using this system, a customer passes the card within 4 cm of a reader at a merchant checkout. Using this method, for transactions under $100, the customer does not need to authenticate his or her identity by PIN entry or signature, as on a regular EFTPOS machine. For transactions over $100, PIN verification is required.

The facility is only available for cards branded with the MasterCard PayPass or Visa payWave logos, indicating that they have the system-permitted embedded chip. ANZ has launched ATM solution based on Visa payWave in 2015, where the consumer tap the card on a reader at the ATM and insert a PIN to finalize the cash withdrawals, and not all merchants offer the facility. Bank debit cards and other credit cards do not currently offer a contactless payment facility. ePAL is developing a contactless payment system for debit cards based on EMV technology as well as an extension of debit cards for use for on-line transactions, and a mobile payment system. Using contactless debit cards on tap-and-go terminals routes the transaction through the more expensive credit card system instead of the EFTPOS route, adding to the cost to the merchant, and ultimately the consumer.

The name and logo for EFTPOS in Australia were originally owned by the National Australia Bank and were trade marks from 1986 until 1991. The ownership was for convenience and all the banks used the name and logo (commonly called "fat-E") on their cards and advertising.

In 1991 Dialup Eftpos was conceived by Key Corp (John Wood) and deployment of Dialup commenced in 1993. Until 1993, communications, connections and transactions between banks, ATM Banks and Eftpos devices where conducted via lease lines (a specific power assisted communication line that detects any attempt to tamper with it) but in 1993 Mobile wireless Eftpos was conceived Dynamic Data Systems (H. Daniel Elbaum). In 1995 Dynamic Data Systems and the banking industry worked together to implement, certify and introduce protocols and standards for cellular networks, and by1998 the use of mobile Eftpos began to appear in Australia.

In 2006 Commonwealth Bank and MasterCard ran a six-month trial of the contactless smart card system PayPass in Sydney and Wollongong, 
supplementing the traditional EFTPOS swipe or chip system. The system was rolled out across Australia in 2009; other systems being rolled out are Westpac Bank's MasterCard PayPass and Visa payWave branded cards.

In April 2009, a company, “EFTPOS Payments Australia Ltd” (ePal) was formed to manage and promote the EFTPOS system in Australia. ePal regulation commenced in January 2011. The initial members of EFTPOS Payments Australia Ltd were:


In Australia, store cards have been excluded from participation in the EFTPOS and ATM systems. Consequently, several larger store accounts have entered into co-branding arrangements with credit card networks for the store-based accounts to be widely accepted. This was the case with Coles (previously, Coles-Myer) which co-branded with MasterCard, and David Jones which co-branded with American Express. Woolworths organized its credit card called Everyday Rewards (now Woolworths Money) which initially was partnered with credit provider HSBC Bank, but changed on 26 October 2014 to Macquarie Bank.

As of December 2010, there were over 707,000 EFTPOS terminals in Australia and over 28,000 ATMs. Of the terminals, over 60,000 offered cash withdrawals. In 2010, 183 million transactions, worth A$12 billion, were made using Australian EFTPOS terminals per month.

In 2011, these figures increased to 750,000 terminals, with 325,000 individual businesses, processing over 2 billion transactions with combined value of approximately $131 billion for the year.

The EFT network in Australia is made up of seven proprietary networks in which peers have interchange agreements, making an effective single network. A merchant who wishes to accept EFTPOS payments must enter an agreement with one of the seven merchant service providers, which rent the terminal to the merchant. All the merchant's EFTPOS transactions are processed through one of these gateways. Some of these peers are:

Other organisations may have peering agreements with the one or more of the central peers.

The network uses the AS 2805 protocol, which is closely related to ISO 8583.

EFTPOS is highly popular in New Zealand. The system is operated by two providers, Paymark Limited (formerly Electronic Transaction Services Limited) which processes 75% of all electronic transactions in New Zealand, and EFTPOS New Zealand. Although the term eftpos is popularly used to describe the system, EFTPOS is a trademark of EFTPOS New Zealand the smaller of the two providers. Both providers run an interconnected financial network that allows the processing of not only of debit cards at point of sale terminals but also credit cards and charge cards.

The Bank of New Zealand introduced EFTPOS to New Zealand in 1985 through a pilot scheme with petrol stations.

In 1989 the system was officially launched and two providers owned by the major banks now run the system. The largest of the two providers, Paymark Limited (formerly Electronic Transaction Services Limited) is owned equally by ASB Bank, Westpac, Bank of New Zealand and ANZ Bank New Zealand (formerly ANZ National Bank). The second is operated by EFTPOS New Zealand which is fully owned by VeriFone Systems, following its sale by ANZ New Zealand in December 2012.

1995 was the first deployment of cellular Eftpos in NZ, by Dynamic Data Systems.

During July 2006 the five billionth EFTPOS payment was processed, and at the start of 2012 the 10 billionth transaction was processed.

EFTPOS is highly popular in New Zealand, and being used for about 60% of all retail transactions. In 2009, there were 200 EFTPOS transactions per person.

Paymark process over 900 million transactions (worth over NZ$48 billion) yearly. More than 75,000 merchants and over 110,000 EFTPOS terminals are connected to Paymark.




</doc>
<doc id="10334" url="https://en.wikipedia.org/wiki?curid=10334" title="Epistle to the Laodiceans">
Epistle to the Laodiceans

The Epistle to the Laodiceans is a lost letter of Paul the Apostle, the original existence of which is inferred from an instruction to the church in Colossae to send their letter to the church in Laodicea, and likewise obtain a copy of the letter "from Laodicea" (, "ek laodikeas").
Several ancient texts purporting to be the missing "Epistle to the Laodiceans" have been known to have existed, most of which are now lost. These were generally considered, both at the time and by modern scholarship, to be attempts to supply a forged copy of a lost document. The exception is a Latin "Epistola ad Laodicenses" ("Epistle to the Laodiceans"), which is actually a short compilation of verses from other Pauline epistles, principally Philippians, and on which scholarly opinion is divided as to whether it is the lost Marcionite forgery or alternatively an orthodox replacement of the Marcionite text. In either case it is generally considered a "clumsy forgery" and an attempt to seek to fill the "gap" suggested by Colossians 4:16.

Some ancient sources, such as Hippolytus, and some modern scholars consider that the epistle "from Laodicea" was never a lost epistle, but simply Paul re-using one of his other letters (the most common candidate is the contemporary Epistle to the Ephesians), just as he asks for the copying and forwarding of the Letter to Colossians to Laodicea.

Paul, the earliest known Christian author, wrote several letters (or epistles) in Greek to various churches. Paul apparently dictated all his epistles through a secretary (or amanuensis), but wrote the final few paragraphs of each letter by his own hand. Many survived and are included in the New Testament, but others are known to have been lost. The Epistle to the Colossians states "After this letter has been read to you, see that it is also read in the church of the Laodiceans and that you in turn read the letter from Laodicea." The last words can be interpreted as "letter written to the Laodiceans", but also "letter written from Laodicea". The New American Standard Bible (NASB) translates this verse in the latter manner, and translations in other languages such as the Dutch Statenvertaling translate it likewise: "When this letter is read among you, have it also read in the church of the Laodiceans; and you, for your part read my letter (that is coming) from Laodicea." Those who read here "letter written to the Laodiceans" presume that, at the time that the Epistle to the Colossians was written, Paul also had written an epistle to the Laodicean Church.

Some scholars have suggested that this refers to the canonical Epistle to the Ephesians, contending that it was a circular letter (an "encyclical") to be read to many churches in the Laodicean area. Others dispute this view.

According to the Muratorian fragment, Marcion's canon contained an epistle called the Epistle to the Laodiceans which is commonly thought to be a forgery written to conform to his own point of view. This is not at all clear, however, since none of the text survives. It is not known what this letter might have contained. Some scholars suggest it may have been the Vulgate epistle described below, while others believe it must have been more explicitly Marcionist in its outlook. Others believe it to be the Epistle to the Ephesians.

For centuries some Western Latin Bibles used to contain a small Epistle from Paul to the Laodiceans. The oldest known Bible copy of this epistle is in a Fulda manuscript written for Victor of Capua in 546. It is mentioned by various writers from the fourth century onwards, notably by Pope Gregory the Great, to whose influence may ultimately be due the frequent occurrence of it in Bibles written in England; for it is more common in English Bibles than in others. John Wycliffe included Paul's letter to the Laodiceans in his Bible translation from the Latin to English. However this epistle is not without controversy because there is no evidence of a Greek text.

The text was almost unanimously considered pseudepigraphal when the Christian Biblical canon was decided upon, and does not appear in any Greek copies of the Bible at all, nor is it known in Syriac or other versions. Jerome, who wrote the Latin Vulgate translation, wrote in the 4th century, "it is rejected by everyone". However, it evidently gained a certain degree of respect. It appeared in over 100 surviving early Latin copies of the Bible. According to "Biblia Sacra iuxta vulgatum versionem", there are Latin Vulgate manuscripts containing this epistle dating between the 6th and 12th century, including Latin manuscripts F (Codex Fuldensis), M, Q, B, D (Ardmachanus), C, and Lambda.

The apocryphal epistle is generally considered a transparent attempt to supply this supposed lost sacred document. Some scholars suggest that it was created to offset the popularity of the Marcionite epistle.

Wilhelm Schneemelcher's standard work, "New Testament Apocrypha" (Chapter 14 "Apostolic Pseudepigrapha") includes a section on the Latin Epistle to the Laodiceans and a translation of the Latin text.


</doc>
<doc id="10335" url="https://en.wikipedia.org/wiki?curid=10335" title="Extermination camp">
Extermination camp

Nazi Germany built extermination camps (also called death camps or killing centers) between 1939 and 1945 during World War II and The Holocaust, to systematically kill millions of Jews, Slavs, Communists, and others whom the Nazis considered "Untermenschen" ("subhumans"). The victims were primarily killed by gassing either in permanent installations constructed for this specific purpose or by means of gas vans. Some Nazi camps such as Auschwitz and Majdanek served a dual-purpose: extermination by poison gas but also through extreme work under starvation conditions.

The idea of mass extermination with the use of stationary facilities to which the victims were taken by train, was a result of earlier Nazi experimentation with the chemically manufactured poison gas during the secretive Action T4 euthanasia programme against hospital patients with mental and physical disabilities. The technology was adapted, expanded, and applied in wartime to unsuspecting victims of many ethnic and national groups; the Jews however were the primary targets, accounting for over 90 percent of the extermination camp death toll. This genocide of the Jewish people of Europe was the Third Reich's "Final Solution to the Jewish question". It is now collectively known as the Holocaust.

Extermination camps were also set up by the fascist Ustaše regime of the Independent State of Croatia, a puppet state of Germany, carrying out genocide between 1941 and 1945 against Serbs, Jews, Roma and its Croat and Bosniak Muslim political opponents.

After the invasion of Poland in September 1939, the secret Action T4 euthanasia programme – the systematic murder of German, Austrian, and Polish hospital patients with mental or physical disabilities – was initiated by the "SS" in order to eliminate "life unworthy of life" (), a Nazi designation for people who had no right to life. In 1941, the experience gained in the secretive killing of these hospital patients led to the creation of extermination camps for the implementation of the Final Solution. By then, the Jews were already confined to new ghettos and interned in Nazi concentration camps along with other targeted groups, including Roma, and the Soviet POWs. The Nazi "Endlösung der Judenfrage" (The Final Solution of the Jewish Question), based on the systematic killing of Europe's Jews by gassing, began during Operation Reinhard, after the onset of the Nazi-Soviet war of June 1941. The adoption of the gassing technology by Nazi Germany was preceded by a wave of hands-on killings carried out by the SS "Einsatzgruppen", who followed the Wehrmacht army during Operation Barbarossa on the Eastern Front.

The camps designed specifically for the mass gassings of Jews were established in the months following the Wannsee Conference chaired by Reinhard Heydrich in January 1942 in which the principle was made clear that the Jews of Europe were to be exterminated. Responsibility for the logistics were to be executed by the programme administrator, Adolf Eichmann.

On 13 October 1941, the SS and Police Leader Odilo Globocnik stationing in Lublin received an oral order from "Reichsführer-SS" Heinrich Himmler – anticipating the fall of Moscow – to start immediate construction work on the killing centre at Bełżec in the General Government territory of occupied Poland. Notably, the order preceded the Wannsee Conference by three months,<ref name="M/MPwB"></ref> but the gassings at Kulmhof north of Łódź using gas vans began already in December, under "Sturmbannführer" Herbert Lange. The camp at Bełżec was operational by March 1942, with leadership brought in from Germany under the guise of Organisation Todt (OT). By mid-1942, two more death camps had been built on Polish lands for Operation Reinhard: Sobibór (ready in May 1942) under the command of "Hauptsturmführer" Franz Stangl, and Treblinka (operational by July 1942) under "Obersturmführer" Irmfried Eberl from T4, the only doctor to have served in such a capacity. Auschwitz concentration camp was fitted with brand new gassing bunkers in March 1942. Majdanek had them built in September.

The Nazis distinguished between extermination and concentration camps, although the terms "extermination camp" ("Vernichtungslager") and "death camp" ("Todeslager") were interchangeable, each referring to camps whose primary function was genocide. "Todeslagers" were designed specifically for the systematic killing of people delivered en masse by the Holocaust trains. The executioners did not expect the prisoners to survive more than a few hours beyond arrival at Belzec, Sobibór, and Treblinka. The Reinhard extermination camps were under Globocnik's direct command; each of them was run by 20 to 35 men from the "SS-Totenkopfverbände" branch of the "Schutzstaffel", augmented by about one hundred Trawnikis – auxiliaries mostly from Soviet Ukraine, and up to one thousand "Sonderkommando" slave labourers each. The Jewish men, women and children were delivered from the ghettos for "special treatment" in an atmosphere of terror by uniformed police battalions from both, Orpo and Schupo.

Death camps differed from concentration camps located in Germany proper, such as Bergen-Belsen, Oranienburg, Ravensbrück, and Sachsenhausen, which were prison camps set up prior to World War II for people defined as 'undesirable'. From March 1936, all Nazi concentration camps were managed by the "SS-Totenkopfverbände" (the Skull Units, SS-TV), who operated extermination camps from 1941 as well. An SS anatomist, Dr. Johann Kremer, after witnessing the gassing of victims at Birkenau, wrote in his diary on 2 September 1942: "Dante's Inferno seems to me almost a comedy compared to this. They don't call Auschwitz the camp of annihilation for nothing!" The distinction was evident during the Nuremberg trials, when Dieter Wisliceny (a deputy to Adolf Eichmann) was asked to name the "extermination" camps, and he identified Auschwitz and Majdanek as such. Then, when asked "How do you classify the camps Mauthausen, Dachau, and Buchenwald?" he replied, "They were normal concentration camps, from the point of view of the department of Eichmann."

Irrespective of round-ups for extermination camps, the Nazis abducted millions of foreigners for slave labour in other types of camps, which provided perfect cover for the extermination programme. Prisoners represented about a quarter of the total workforce of the Reich, with mortality rates exceeding 75 percent due to starvation, disease, exhaustion, executions and physical brutality.

In the early years of World War II, the Jews were primarily sent to forced labour camps and ghettoised, but from 1942 onward they were deported to the extermination camps under the guise of "resettlement". For political and logistical reasons, the most infamous Nazi German killing factories were built in occupied Poland, where most of the intended victims lived; Poland had the greatest Jewish population in Nazi controlled Europe. On top of that, the new death camps outside the prewar borders of the Third Reich proper could be kept secret from the German civil populace.

During the initial phase of the Final Solution, gas vans producing poisonous exhaust fumes were developed in the occupied Soviet Union (USSR) and at the Chełmno extermination camp in occupied Poland, before being used elsewhere. The killing method was based on experience gained by the SS during the secretive "Aktion T4" programme of involuntary euthanasia. There were two types of death chambers operating during the Holocaust.

Unlike at Auschwitz, where the cyanide-based Zyklon-B was used to exterminate trainloads of prisoners under the guise of "relocation", the camps at Treblinka, Bełżec, and Sobibór, built during Operation Reinhard (October 1941 – November 1943), used lethal exhaust fumes produced by large internal combustion engines. The three killing centres of "Einsatz Reinhard" were constructed predominantly for the extermination of Poland's Jews trapped in the Nazi ghettos. At first, the victim's bodies were buried with the use of crawler excavators, but they were later exhumed and incinerated in open-air pyres to hide the evidence of genocide.

Whereas the Auschwitz II (Auschwitz–Birkenau) and Majdanek camps were parts of a labor camp complex, the Operation Reinhard camps and the Chełmno camp were built exclusively for the quick extermination of entire communities of people (primarily Jews) within hours of their arrival. All were constructed near branch lines that linked to the Polish railway system. They had almost identical design, including staff members transferring between locations. Selected able-bodied prisoners delivered to death camps were not immediately killed, but pressed into labor units called "Sonderkommandos" to help with the extermination process by removing corpses from the gas chambers and burning them. The extermination camps were physically small (only several hundred metres long and wide) and equipped with minimal housing and support installations, not meant for the railway transports. The Nazis deceived the victims upon their arrival, telling them that they were at a temporary transit stop, and soon would continue to German "Arbeitslagers" (work camps) farther east.

At the camps of Operation Reinhard, including Bełżec, Sobibór, and Treblinka, trainloads of prisoners were destined for immediate death in gas chambers designed exclusively for that purpose. The mass killing facilities were developed at about the same time inside the Auschwitz II-Birkenau subcamp of a forced labour complex, and at the Majdanek concentration camp. In most other camps prisoners were selected for slave labor first; they were kept alive on starvation rations and made available to work as required. Auschwitz, Majdanek, and Jasenovac were retrofitted with Zyklon-B gas chambers and crematoria buildings as the time went on, remaining operational until war's end in 1945. The Maly Trostenets extermination camp in the USSR initially operated as a prison camp. It became an extermination camp later in the war with victims undergoing mass shootings. This was supplemented with gassings in a van by exhaust fumes from October 1943.

The Sajmište concentration camp operated by the Nazis in Yugoslavia had a gas van stationed for use from March to June 1942. Once the industrial killings were completed, the van was returned to Berlin. After a refit the van was then sent to Maly Trostinets for use at the camp there. The Janowska concentration camp near Lwow (now Lviv) in occupied eastern Poland implemented a selection process. Some prisoners were assigned to work before death. Others were either transported to Belzec or victims of mass shootings on two slopes in the Piaski sand-hills behind the camp. The Warsaw concentration camp was a camp complex of the German concentration camps, possibly including an extermination camp located in German-occupied Warsaw. The various details regarding the camp are very controversial and remain subject of historical research and public debate.

With the support of Nazi Germany and Fascist Italy, the Independent State of Croatia (NDH) was established on 10 April 1941, and adopted parallel racial and political doctrines. Death camps were established by the fascist Ustaše government for contributing to the Nazi "final solution" to the "Jewish problem", the killing of Roma people, and the elimination of political opponents, but most significantly to achieve the destruction of the Serbian population of the NDH. The degree of cruelty with which the Serb population was persecuted by Ustaše men shocked even the Germans.

The Jadovno concentration camp was located in a secluded area about from the town of Gospić. It held thousands of Serbs and Jews over a period of 122 days from May to August 1941. Prisoners were usually but not exclusively killed by being pushed into deep ravines located near the camp.

The Jasenovac concentration camp complex of five sub-camps replaced Jadovno. Many inmates arriving at Jasenovac were scheduled for systematic extermination. An important criterion for selection was the duration of a prisoner's anticipated detention. Strong men who were capable of labour and sentenced to less than three years of incarceration were allowed to live. All inmates with indeterminate sentences or sentences of three years or more were immediately scheduled for execution, regardless of their level of fitness. Some of the mass executions were mechanical according to Nazi methodology. Others were performed manually with tools such as mallets and agricultural knives and these tools were often used to throw victims off the end of a ramp into the River Sava.

Heinrich Himmler visited the outskirts of Minsk in 1941 to witness a mass shooting. He was told by the commanding officer there that the shootings were proving psychologically damaging to those being asked to pull the triggers. Thus Himmler knew another method of mass killing was required. After the war, the diary of the Auschwitz Commandant, Rudolf Höss, revealed that psychologically "unable to endure wading through blood any longer", many "Einsatzkommandos" – the killers – either went mad or killed themselves.

The Nazis had first used gassing with carbon monoxide cylinders to kill 70,000 disabled people in Germany in what they called a 'euthanasia programme' to disguise that mass murder was taking place. Despite the lethal effects of carbon monoxide, this was seen as unsuitable for use in the East due to the cost of transporting the carbon monoxide in cylinders.

Each extermination camp operated differently, yet each had designs for quick and efficient industrialized killing. While Höss was away on an official journey in late August 1941 his deputy, Karl Fritzsch, tested out an idea. At Auschwitz clothes infested with lice were treated with crystallised prussic acid. The crystals were made to order by the IG Farben chemicals company for which the brand name was Zyklon-B. Once released from their container, Zyklon-B crystals in the air released a lethal cyanide gas. Fritzch tried out the effect of Zyklon B on Soviet POWs, who were locked up in cells in the basement of the bunker for this experiment. Höss on his return was briefed and impressed with the results and this became the camp strategy for extermination as it was also to be at Majdanek. Besides gassing, the camp guards continued killing prisoners via mass shooting, starvation, torture, etc.

SS "Obersturmführer" Kurt Gerstein, of the Institute for Hygiene of the Waffen-SS, told a Swedish diplomat during the war of life in a death camp. He recounted that, on 19 August 1942, he arrived at Belzec extermination camp (which was equipped with carbon monoxide gas chambers) and was shown the unloading of 45 train cars filled with 6,700 Jews, many already dead. The rest were marched naked to the gas chambers, where:

Auschwitz Camp Commandant Rudolf Höss reported that the first time Zyklon B pellets were used on the Jews, many suspected they were to be killed – despite having been deceived into believing they were to be deloused and then returned to the camp. As a result, the Nazis identified and isolated "difficult individuals" who might alert the prisoners, and removed them from the mass – lest they incite revolt among the deceived majority of prisoners en route to the gas chambers. The "difficult" prisoners were led to a site out of view to be killed off discreetly.

A prisoner "Sonderkommando" (Special Detachment) effected in the processes of extermination; they encouraged the Jews to undress without a hint of what was about to happen. They accompanied them into the gas chambers outfitted to appear as shower rooms (with nonworking water nozzles, and tile walls); and remained with the victims until just before the chamber door closed. To psychologically maintain the "calming effect" of the delousing deception, an SS man stood at the door until the end. The "Sonderkommando" talked to the victims about life in the camp to pacify the suspicious ones, and hurried them inside; to that effect, they also assisted the aged and the very young in undressing.

To further persuade the prisoners that nothing harmful was happening, the "Sonderkommando" deceived them with small talk about friends or relations who had arrived in earlier transports. Many young mothers hid their infants beneath their piled clothes fearing that the delousing "disinfectant" might harm them. Camp Commandant Höss reported that the "men of the Special Detachment were particularly on the look-out for this", and encouraged the women to take their children into the "shower room". Likewise, the "Sonderkommando" comforted older children who might cry "because of the strangeness of being undressed in this fashion".

Yet, not every prisoner was deceived by such psychological tactics; Commandant Höss spoke of Jews "who either guessed, or knew, what awaited them, nevertheless ... [they] found the courage to joke with the children, to encourage them, despite the mortal terror visible in their own eyes". Some women would suddenly "give the most terrible shrieks while undressing, or tear their hair, or scream like maniacs"; the "Sonderkommando" immediately took them away for execution by shooting. In such circumstances, others, meaning to save themselves at the gas chamber's threshold, betrayed the identities and "revealed the addresses of those members of their race still in hiding".

Once the door of the filled gas chamber was sealed, pellets of Zyklon B were dropped through special holes in the roof. Regulations required that the Camp Commandant supervise the preparations, the gassing (through a peephole), and the aftermath looting of the corpses. Commandant Höss reported that the gassed victims "showed no signs of convulsion"; the Auschwitz camp physicians attributed that to the "paralyzing effect on the lungs" of the Zyklon-B gas, which killed "before" the victim began suffering convulsions.
As a matter of political training, some high-ranked Nazi Party leaders and SS officers were sent to Auschwitz–Birkenau to witness the gassings; Höss reported that "all were deeply impressed by what they saw ... [yet some] ... who had previously spoken most loudly, about the necessity for this extermination, fell silent once they had actually seen the 'final solution of the Jewish problem'." As the Auschwitz Camp Commandant Rudolf Höss justified the extermination by explaining the need for "the iron determination with which we must carry out Hitler's orders"; yet saw that even "[Adolf] Eichmann, who certainly [was] tough enough, had no wish to change places with me."

After the gassings, the "Sonderkommando" removed the corpses from the gas chambers, then extracted any gold teeth. Initially, the victims were buried in mass graves, but were later cremated during "Sonderaktion 1005" in all camps of Operation Reinhard.

The "Sonderkommando" were responsible for burning the corpses in the pits, stoking the fires, draining surplus body fat and turning over the "mountain of burning corpses ... so that the draught might fan the flames" wrote Commandant Höss in his memoir while in the Polish custody. He was impressed by the diligence of prisoners from the so-called Special Detachment who carried out their duties despite their being well aware that they, too, would meet exactly the same fate in the end. At the Lazaret killing station they held the sick so they would never see the gun while being shot. They did it "in such a matter-of-course manner that they might, themselves, have been the exterminators" wrote Höss. He further said that the men ate and smoked "even when engaged in the grisly job of burning corpses which had been lying for some time in mass graves." They occasionally encountered the corpse of a relative, or saw them entering the gas chambers. According to Höss they were obviously shaken by this but "it never led to any incident." He mentioned the case of a "Sonderkommando" who found the body of his wife, yet continued to drag corpses along "as though nothing had happened."

At Auschwitz, the corpses were incinerated in crematoria and the ashes either buried, scattered, or dumped in the river. At Sobibór, Treblinka, Belzec, and Chełmno, the corpses were incinerated on pyres. The efficiency of industrialised killing at Auschwitz-Birkenau led to the construction of three buildings with crematoria designed by specialists from Topf und Söhne. They handled the body disposal around the clock, day and night, and yet the speed of gassing required that some corpses burn in an open air pit also.<ref name="B/G-199"></ref>

The United States Holocaust Memorial Museum (USHMM) in Washington, DC, presently estimates that the Ustaša regime in Croatia murdered between 77,000 and 99,000 people at their own Jasenovac concentration camp between 1941 and 1945. The Jasenovac Memorial Site quotes a similar figure of between 80,000 and 100,000 victims. The television documentary, "Nazi Collaborators" on Dinko Sakic stated that over 300,000 people were killed at Jasenovac. The mechanical means of mass killing at Jasenovac initially included the use of gas vans and later Zyklon B in stationary gas chambers. The Jasenovac guards were also reported to have cremated living inmates in the crematorium. A notable difference with the Ustaše camps as compared to the German "SS" camps was the widespread use of manual methods in the mass killings. These involved instruments such as mallets and agricultural knives which were often used in a manner where victims were thrown off the end of a ramp into the Sava River while they were still alive .

The estimates for the Jadovno concentration camp generally offer a range of 10,000 – 72,000 deaths at the camp over a period of 122 days (May to August 1941). Most commonly Jadovno victims were bound together in a line and the first few victims were murdered with rifle butts or other objects. Afterwards, an entire row of inmates were pushed into the ravine. Hand grenades were hurled inside in order to finish off the victims. Dogs would also be thrown in to feed on the wounded and the dead. Inmates were also killed by machine gunfire, as well as with knives and blunt objects.

The estimated total number of people executed in the Nazi extermination camps in the table below is over three million:
The Nazis attempted to either partly or completely dismantle the extermination camps in order to hide any evidence that people had been murdered. This was an attempt to conceal not only the extermination process but also the buried remains. As a result of the secretive Sonderaktion 1005, camps were dismantled by commandos of condemned prisoners, records destroyed, and mass graves were dug up. Some extermination camps that remained uncleared of evidence were liberated by Soviet troops, who had different standards of documentation and openness than the Western allies.

The notable exception to this is Majdanek. Majdanek was ordered to share a fate similar to that of the other camps by the Nazi leadership in its attempt to cover up the murderous events. Majdanek though was captured nearly intact. This was due to the rapid advance of the Soviet Red Army during Operation Bagration preventing the SS from destroying most of its infrastructure. Commandant Anton Thernes failed in his task of removing incriminating evidence of war crimes.

In the post-war period the government of the People's Republic of Poland created monuments at the extermination camp sites. These early monuments mentioned no ethnic, religious, or national particulars of the Nazi victims. The extermination camps sites have been accessible to everyone in recent decades. They are popular destinations for visitors from all over the world, especially the most infamous Nazi death camp, Auschwitz near the town of Oświęcim. In the early 1990s, the Jewish Holocaust organisations debated with the Polish Catholic groups about "What religious symbols of martyrdom are appropriate as memorials in a Nazi death camp such as Auschwitz?" The Jews opposed the placement of Christian memorials such as the Auschwitz cross near Auschwitz I where mostly Poles were killed. The Jewish victims of the Holocaust were mostly killed at Auschwitz II Birkenau.

The March of the Living is organized in Poland annually since 1988. Marchers come from countries as diverse as Estonia, New Zealand, Panama, and Turkey.

Holocaust deniers or negationists are people and organizations who assert that the Holocaust did not occur, or that it did not occur in the historically recognized manner and extent.

Extermination camp research is difficult because of extensive attempts by the SS and Nazi regime to conceal the existence of the extermination camps. The existence of the extermination camps is firmly established by testimonies of camp survivors and Final Solution perpetrators, material evidence (the remaining camps, etc.), Nazi photographs and films of the killings, and camp administration records.

Holocaust deniers often start by pointing out legitimate public misconceptions about the extermination camps. For example, widely published images in America were mostly of typhoid victims and Soviet POWs at the Buchenwald and Dachau concentration camps – the first to be liberated by American troops and the most available imagery in America. In early news reports and for years afterwards these images were often used by the news media somewhat inaccurately in conjunction with descriptions of extermination camps and Jewish suffering. Holocaust deniers, after pointing out such common errors, put it forward as evidence that extermination camps did not exist and the limited evidence about them is mostly a hoax arising out of a deliberate Jewish conspiracy.

Holocaust denial has been thoroughly discredited by scholars and is a criminal offence in 16 countries: Austria, Belgium, Czech Republic, France, Germany, Hungary, Israel, Liechtenstein, Lithuania, Luxembourg, Netherlands, Poland, Portugal, Romania, Russia, Slovakia, and Switzerland.





</doc>
<doc id="10336" url="https://en.wikipedia.org/wiki?curid=10336" title="Enterprise">
Enterprise

Enterprise (or the archaic spelling Enterprize) may refer to:
















</doc>

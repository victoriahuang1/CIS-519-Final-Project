<doc id="9995" url="https://en.wikipedia.org/wiki?curid=9995" title="EastEnders">
EastEnders

EastEnders is a British soap opera created by Julia Smith and Tony Holland which has been broadcast on BBC One since 1985. Set in Albert Square in the East End of London in the fictional Borough of Walford, the programme follows the stories of local residents and their families as they go about their daily lives. Initially there were two 30-minute episodes per week but since 2001 episodes have been broadcast every weekday apart from Wednesdays.

Within eight months of the show's launch, it reached the number-one spot in BARB's TV ratings and has consistently remained among the top-rated TV programmes in Britain. In 2013, the average audience share for an episode was around 30 per cent. Today, "EastEnders" remains a significant programme in terms of the BBC's success and audience share, and also in the history of British television drama, tackling many dilemmas that are considered to be controversial and taboo issues in British culture and social life previously unseen on United Kingdom mainstream television.

, "EastEnders" has won nine BAFTA Awards and the Inside Soap Award for Best Soap for 14 years running (from 1997 to 2012), as well as twelve National Television Awards for Most Popular Serial Drama and 11 awards for Best Soap at the British Soap Awards. It has also won 13 TV Quick and TV Choice Awards for Best Soap, six TRIC Awards for Soap of The Year, four Royal Television Society Awards for Best Continuing Drama and has been inducted into the Rose d'Or Hall of Fame.

In March 1983, under two years before "EastEnders" first episode was broadcast, the show was a vague idea in the mind of a handful of BBC executives, who decided that what BBC1 needed was a popular bi-weekly drama series that would attract the kind of mass audiences that ITV was getting with "Coronation Street". The first people to whom David Reid, then head of series and serials, turned were Julia Smith and Tony Holland, a well established producer/script editor team who had first worked together on "Z-Cars". The outline that Reid presented was vague: two episodes a week, 52 weeks a year. After the concept was put to them on 14 March 1983, Smith and Holland then went about putting their ideas down on paper; they decided it would be set in the East End of London. Granada Television gave Smith unrestricted access to the "Coronation Street" production for a month so that she could get a sense how a continuing drama was produced.

There was anxiety at first that the viewing public would not accept a new soap set in the south of England, though research commissioned by lead figures in the BBC revealed that southerners would accept a northern soap, northerners would accept a southern soap and those from the Midlands, as Julia Smith herself pointed out, did not mind where it was set as long as it was somewhere else. This was the beginning of a close and continuing association between "EastEnders" and audience research, which, though commonplace today, was something of a revolution in practice.

The show's creators were both Londoners, but when they researched Victorian squares, they found massive changes in areas they thought they knew well. However, delving further into the East End of London, they found exactly what they had been searching for: a real East End spirit—an inward looking quality, a distrust of strangers and authority figures, a sense of territory and community that the creators summed up as "Hurt one of us and you hurt us all".

When developing "EastEnders", both Smith and Holland looked at influential models like "Coronation Street", but they found that it offered a rather outdated and nostalgic view of working-class life. Only after "EastEnders" began, and featured the characters of Tony Carpenter and Kelvin Carpenter, did "Coronation Street" start to feature black characters, for example. They came to the conclusion that "Coronation Street" had grown old with its audience, and that "EastEnders" would have to attract a younger, more socially extensive audience, ensuring that it had the longevity to retain it for many years thereafter. They also looked at "Brookside" but found there was a lack of central meeting points for the characters, making it difficult for the writers to intertwine different storylines, so "EastEnders" was set in Albert Square.

A previous UK soap set in an East End market was ATV's "Market in Honey Lane" between 1967 and 1969. However this show, which graduated from one showing a week to two in three separate series (the latter series being shown in different time slots across the ITV network) was very different in style and approach to "EastEnders". The British Film Institute described "Market In Honey Lane" thus: "It was not an earth-shaking programme, and certainly not pioneering in any revolutionary ideas in technique and production, but simply proposed itself to the casual viewer as a mildly pleasant affair." "EastEnders", while also featuring an East End street market, would be very different in its approach and impact.

The target launch date was originally January 1985. Smith and Holland had eleven months in which to write, cast and shoot the whole thing. However, in February 1984, they did not even have a title or a place to film. Both Smith and Holland were unhappy about the January 1985 launch date, favouring November or even September 1984 when seasonal audiences would be higher, but the BBC stayed firm, and Smith and Holland had to concede that, with the massive task of getting the Elstree Studios operational, January was the most realistic date. However, this was later to be changed to February.

The project had a number of working titles—"Square Dance", "Round the Square", "Round the Houses", "London Pride" and "East 8". It was the latter that stuck (E8 is the postcode for Hackney) in the early months of creative process. However, the show was renamed after many casting agents mistakenly thought the show was to be called "Estate", and the fictional postcode E20 was created, instead of using E8. Julia Smith came up with the name "Eastenders" after she and Holland had spent months telephoning theatrical agents and asking "Do you have any real East Enders on your books?" However, Smith thought ""Eastenders"" "looked ugly written down" and was "hard to say", so decided to capitalise the second 'e'.

After they decided on the filming location of BBC Elstree Centre in Hertfordshire, Smith and Holland set about creating the 23 characters needed, in just 14 days. They took a holiday in Playa de los Pocillos, Lanzarote, and started to create the characters. Holland created the Beale and Fowler family, drawing on his own background. His mother, Ethel Holland, was one of four sisters raised in Walthamstow. Her eldest sister, Lou, had married a man named Albert Beale and had two children, named Peter and Pauline. These family members were the basis for Lou Beale, Pete Beale and Pauline Fowler. Holland also created Pauline's unemployed husband Arthur Fowler, their children Mark Fowler and Michelle Fowler, Pete's wife Kathy Beale and their son Ian Beale. Smith used her personal memories of East End residents she met when researching Victorian squares. Ethel Skinner was based on an old woman she met in a pub, with ill-fitting false teeth, and a "face to rival a neon sign", holding a Yorkshire Terrier in one hand and a pint of Guinness in the other. Other characters created included Jewish doctor Harold Legg, the Anglo-Cypriot Osman family, Ali Osman, Sue Osman and baby Hassan Osman, black father and son, Tony Carpenter and Kelvin Carpenter, single mother Mary Smith and Bangladeshi couple Saeed Jeffery and Naima Jeffery. Jack, Pearl and Tracey Watts were created to bring "flash, trash, and melodrama" to the Square (they were later renamed Den Watts, Angie Watts and Sharon Watts). The characters of Andy O'Brien and Debbie Wilkins were created to show a modern couple with outwardly mobile pretensions, and Lofty Holloway to show an outsider; someone who did not fit in with other residents. It was decided that he would be a former soldier, as Holland's personal experiences of ex-soldiers were that they had trouble fitting into society after being in the army. When they compared the characters they had created, Smith and Holland realised they had created a cross-section of East End residents. The Beale and Fowler family represented the old families of the East End, who had always been there. The Osmans, Jefferys and Carpenters represented the more modern diverse ethnic community of the East End. Debbie, Andy and Mary represented more modern-day individuals.

Once they had decided on their 23 characters, they returned to London for a meeting with the BBC. Everyone agreed that "EastEnders" would be tough, violent on occasion, funny and sharp—set in Margaret Thatcher's Britain—and it would start with a bang (namely the death of Reg Cox). They decided that none of their existing characters were wicked enough to have killed Reg, so a 24th character, Nick Cotton was added to the line-up. He was a racist thug, who often tried to lead other young characters astray. When all the characters had been created, Smith and Holland set about casting the actors for the show.

Through the next few months, the set was growing rapidly at Elstree, and a composer and designer had been commissioned to create the title sequence. Simon May wrote the theme music and Alan Jeapes created the visuals. The visual images were taken from an aircraft flying over the East End of London at 1000 feet. Approximately 800 photographs were taken and pieced together to create one big image. The credits were later updated when the Millennium Dome was built.

The launch was delayed until February 1985 due to a delay in the chat show "Wogan", that was to be a part of the major revamp in BBC1's schedules. Smith was uneasy about the late start as "EastEnders" no longer had the winter months to build up a loyal following before the summer ratings lull. The press were invited to Elstree to meet the cast and see the lot, and stories immediately started circulating about the show, about a rivalry with ITV (who were launching their own market-based soap, "Albion Market") and about the private lives of the cast. Anticipation and rumour grew in equal measure until the first transmission at 7p.m. on 19 February 1985. Both Holland and Smith could not watch; they both instead returned to the place where it all began, Albertine's Wine Bar on Wood Lane. The next day, viewing figures were confirmed at 17 million. The reviews were largely favourable, although, after three weeks on air, BBC1's early evening share had returned to the pre-"EastEnders" figure of seven million, though "EastEnders" then climbed to highs of up to 23 million later on in the year. Following the launch, both group discussions and telephone surveys were conducted to test audience reaction to early episodes. Detailed reactions were taken after six months and since then regular monitoring was conducted.

Press coverage of "EastEnders", which was already intense, went into overdrive once the show was broadcast. With public interest so high, the media began investigating the private lives of the show's popular stars. Within days, the scandalous headline the producers had all dreaded appeared—"EASTENDERS STAR IS A KILLER". This referred to Leslie Grantham, and his prison sentence for the murder of a taxi driver in an attempted robbery nearly 20 years earlier. This shocking tell-all style set the tone for relations between Albert Square and the press for the next 20 years.

The show's first episode attracted some 17 million viewers, and it continued to attract high viewing figures from then on. By Christmas 1985, the tabloids couldn't get enough of the show. 'Exclusives' about "EastEnders" storylines and the actors on the show became a staple of tabloid buyers daily reading.

Writer Colin Brake suggested that 1989 was a year of big change for "EastEnders", both behind the cameras and in front of them. Original production designer, Keith Harris, left the show, and Holland and Smith both decided that the time had come to move on too; their final contribution coinciding with the exit of one of "EastEnders" most successful characters, Den Watts (Leslie Grantham). Producer Mike Gibbon was given the task of running the show and he enlisted the most experienced writers to take over the storylining of the programme, including Charlie Humphreys, Jane Hollowood and Tony McHale.

According to Brake, the departure of two of the soap's most popular characters, Den and Angie Watts (Anita Dobson), left a void in the programme, which needed to be filled. In addition, several other long-running characters left the show that year including Sue and Ali Osman (Sandy Ratcliff and Nejdet Salih) and their family; Donna Ludlow (Matilda Ziegler); Carmel Jackson (Judith Jacob) and Colin Russell (Michael Cashman). Brake indicated that the production team decided that 1989 was to be a year of change in Walford, commenting, "it was almost as if Walford itself was making a fresh start".

By the end of 1989 "EastEnders" had acquired a new executive producer, Michael Ferguson, who had previously been a successful producer on ITV's "The Bill". Brake suggested that Ferguson was responsible for bringing in a new sense of vitality and creating a programme that was more in touch with the real world than it had been over the previous year.

A new era began in 1990 with the introduction of Phil Mitchell (Steve McFadden) and Grant Mitchell (Ross Kemp)—the Mitchell brothers—successful characters who would go on to dominate the soap thereafter. As the new production team cleared the way for new characters and a new direction, all of the characters introduced under Gibbon were axed from the show at the start of the year. Ferguson introduced other characters and was responsible for storylines including HIV, Alzheimer's disease and murder. After a successful revamp of the soap, Ferguson decided to leave "EastEnders" in July 1991. Furguson was succeeded by both Leonard Lewis and Helen Greaves who initially shared the role as Executive Producer for "EastEnders". Lewis and Greaves formulated a new regime for "EastEnders", giving the writers of the serial more authority in storyline progression, with the script department providing "guidance rather than prescriptive episode storylines". By the end of 1992, Greaves left and Lewis became executive and series producer. He left "EastEnders" in 1994 after the BBC controllers demanded an extra episode a week, taking its weekly airtime from 60 to 90 minutes. Lewis felt that producing an hour of "reasonable quality drama" a week was the maximum that any broadcasting system could generate without loss of integrity. Having set up the transition to the new schedule, the first trio of episodes—dubbed The Vic siege—marked Lewis's departure from the programme. Barbara Emile then became the Executive Producer of "EastEnders", remaining with "EastEnders" until early 1995. She was succeeded by Corinne Hollingworth.

Hollingworth's contributions to the soap were awarded in 1997 when "EastEnders" won the BAFTA for Best Drama Series. Hollingworth shared the award with the next Executive Producer, Jane Harris. Harris was responsible for the critically panned Ireland episodes and Cindy Beale's attempted assassination of Ian Beale, which brought in an audience of 23 million in 1996, roughly four million more than "Coronation Street". In 1998 Matthew Robinson was appointed as the Executive Producer of "EastEnders". During his reign, "EastEnders" won the BAFTA for "Best Soap" in consecutive years 1999 and 2000 and many other awards. Robinson also earned tabloid soubriquet "Axeman of Albert Square" after sacking a large number of characters in one hit, and several more thereafter. In their place, Robinson introduced new long-running characters including Melanie Healy, Jamie Mitchell, Lisa Shaw, Steve Owen and Billy Mitchell.

John Yorke became the Executive Producer of "EastEnders" in 2000. Yorke was given the task of introducing the soap's fourth weekly episode. He axed the majority of the Di Marco family and helped introduce popular characters such as the Slater family. As what Mal Young described as "two of "EastEnders" most successful years", Yorke was responsible for highly rated storlines such as "Who Shot Phil?", Ethel Skinner's death, Jim Branning and Dot Cotton's marriage, Trevor Morgan's domestic abuse of his wife Little Mo Morgan, and Kat Slater's revelation to her daughter Zoe Slater that she was her mother.

In 2002, Louise Berridge succeeded Yorke as the Executive Producer. During her time at "EastEnders", Berridge introduced popular characters such as Alfie Moon, Dennis Rickman, Chrissie Watts, Jane Beale, Stacey Slater and the critically panned Indian Ferreira family.

Berridge was responsible for some ratings success stories, such as Alfie and Kat Slater's relationship, Janine Butcher getting her comeuppance, Trevor Morgan and Jamie Mitchell's death storylines and the return of one of the greatest soap icons, Den Watts, who had been presumed dead for 14 years. His return in late 2003 was watched by over 16 million viewers, putting "EastEnders" back at number one in the rating war with the "Coronation Street". However, other storylines, such as one about a kidney transplant involving the Ferreiras, were not well received, and although Den Watts's return proved to be a ratings success, the British press branded the plot unrealistic and felt that it questioned the show's credibility. A severe press backlash followed after Den's actor, Leslie Grantham, was outed in an internet sex scandal, which coincided with a swift decline in viewer ratings. The scandal led to Grantham's departure from the soap, but the occasion was used to mark the 20th anniversary of "EastEnders", with an episode showing Den's murder at the Queen Vic pub.

On 21 September 2004, Berridge quit as executive producer of "EastEnders" following continued criticism of the show. Kathleen Hutchison was swiftly appointed as the Executive Producer of "EastEnders", and was tasked with quickly turning the fortunes of the soap. During her time at the soap Hutchison axed multiple characters, and reportedly ordered the rewriting of numerous scripts. Newspapers reported on employee dissatisfaction with Hutchison's tenure at "EastEnders". In January 2005, Hutchison left the soap and John Yorke (who by this time, was the BBC Controller of Continuing Drama Series) took total control of the show himself and became acting Executive Producer for a short period, before appointing Kate Harwood to the role. Harwood stayed at "EastEnders" for 20 months before being promoted by the BBC. On Friday 11 November 2005, "EastEnders" was the first British drama to feature a two-minute silence. This episode later went on to win the British Soap Award for 'Best Single Episode'. In October 2006, Diederick Santer took over as Executive Producer of "EastEnders". He introduced several characters to the show, including ethnic minority and homosexual characters to make the show 'feel more 21st Century'. Santer also reintroduced past and popular characters to the programme.

On 2 March 2007, BBC signed a deal with Google to put videos on YouTube. A behind the scenes video of "EastEnders", hosted by Matt Di Angelo, who played Deano Wicks on the show, was put on the site the same day, and was followed by another on 6 March 2007. In April 2007, "EastEnders" became available to view on mobile phones, via 3G technology, for 3, Vodafone and Orange customers. On 21 April 2007, the BBC launched a new advertising campaign using the slogan "There's more to "EastEnders"". The first television advert showed Dot Branning with a refugee baby, Tomas, whom she took in under the pretence of being her grandson. The second and third featured Stacey Slater and Dawn Swann, respectively. There have also been adverts in magazines and on radio.

In 2009, producers introduced a limit on the number of speaking parts in each episode due to budget cuts, with an average of 16 characters per episode. The decision was criticised by Martin McGrath of Equity, who said: "Trying to produce quality TV on the cheap is doomed to fail." The BBC responded by saying they had been working that way for some time and it had not affected the quality of the show.

From 4 February 2010, CGI was used in the show for the first time, with the addition of computer-generated trains.

"EastEnders" celebrated its 25th anniversary on 19 February 2010. Santer came up with several plans to mark the occasion, including the show's first episode to be broadcast live, the second wedding between Ricky Butcher and Bianca Jackson and the return of Bianca's relatives, mother Carol Jackson, and siblings Robbie Jackson, Sonia Fowler and Billie Jackson. He told entertainment website Digital Spy, "It's really important that the feel of the week is active and exciting and not too reflective. There'll be those moments for some of our longer-serving characters that briefly reflect on themselves and how they've changed. The characters don't know that it's the 25th anniversary of anything, so it'd be absurd to contrive too many situations in which they're reflective on the past. The main engine of that week is great stories that'll get people talking." The live episode featured the death of Bradley Branning and the conclusion of the "Who Killed Archie?" storyline, when Stacey Branning revealed she was the murderer. Viewing figures peaked at 16.6 million, which was the highest viewed episode in seven years. Other events to mark the anniversary were a spin-off DVD, "EastEnders: Last Tango in Walford", and an Internet spin-off, "".
Santer officially left "EastEnders" in March 2010, and was replaced by Bryan Kirkwood. Kirkwood's first signing was the reintroduction of characters Alfie Moon (Shane Richie) and Kat Moon (Jessie Wallace), and his first new character was Vanessa Gold, played by Zöe Lucker. In April and May 2010, Kirkwood axed eight characters from the show, Barbara Windsor left her role of Peggy Mitchell, which left a hole in the show, which Kirkwood decided to fill by bringing back Kat and Alfie, which he said would "herald the new era of "EastEnders"." "EastEnders" started broadcasting in high definition on 25 December 2010. Old sets had to be rebuilt, so The Queen Victoria set was burnt down in a storyline (and in reality) to facilitate this.

In November 2011, a storyline showed character Billy Mitchell, played by Perry Fenwick, selected to be a torch bearer for the 2012 Summer Olympics. In reality, Fenwick carried the torch through the setting of Albert Square, with live footage shown in the episode on 23 July 2012. This was the second live broadcast of "EastEnders". In 2012, Kirkwood chose to leave his role as executive producer and was replaced by Lorraine Newman. The show lost many of its significant characters during this period. Newman stepped down as executive producer after 16 months in the job in 2013 after the soap was criticised for its boring storylines and its lowest-ever figures pointing at around 4.8 million. Dominic Treadwell-Collins was appointed as the new executive producer on 19 August 2013 and was credited on 9 December. He axed multiple characters from the show and introduced the extended Carter family. He also introduced a long-running storyline, "Who Killed Lucy Beale?", which peaked during the show's 30th anniversary in 2015 with a week of live episodes. Treadwell-Collins announced his departure from "EastEnders" on 18 February 2016.

Sean O'Connor, former "EastEnders" series story producer and then-editor on radio soap opera "The Archers", was announced to be taking over the role. Treadwell-Collins left on 6 May and O'Connor's first credited episode was broadcast on 11 July Although O'Connor's first credited episode aired in July, his own creative work was not seen onscreen until late September. Additionally, Oliver Kent was brought in as the Head of Continuing Drama Series for BBC Scripted Studios, meaning that Kent would oversee "EastEnders" along with O'Connor. O'Connor's approach to the show was to have a firmer focus on realism, which he said was being "true to "EastEnders" DNA and [finding] a way of capturing what it would be like if Julia Smith and Tony Holland were making the show now." He said that ""EastEnders" has always had a distinctly different tone from the other soaps but over time we've diluted our unique selling point. I think we need to be ourselves and go back to the origins of the show and what made it successful in the first place. It should be entertaining but it should also be informative—that's part of our unique BBC compact with the audience. It shouldn't just be a distraction from your own life, it should be an exploration of the life shared by the audience and the characters." O'Connor planned to stay with "EastEnders" until the end of 2017, but announced his departure on 23 June 2017 with immediate effect, saying he wanted to concentrate on a career in film. John Yorke returned as a temporary creative director. Kent said, "John Yorke is a Walford legend and I am thrilled that he will be joining us for a short period to oversee the show and to help us build on Sean's legacy while we recruit a long-term successor." Yorke initially returned for three months but his contract was later extended to twelve months.

The central focus of "EastEnders" is the fictional Victorian square Albert Square in the fictional London Borough of Walford. In the show's narrative, Albert Square is a 19th-century street, named after Prince Albert (1819–61), the husband of Queen Victoria (1819–1901, reigned 1837–1901). Thus, central to Albert Square is The Queen Victoria Public House (also known as The Queen Vic or The Vic). The show's producers based the square's design on Fassett Square in Dalston. There is also a market close to Fassett Square at Ridley Road. The postcode for the area, E8, was one of the working titles for the series. The name "Walford" is both a street in Dalston where Tony Holland lived and a blend of Walthamstow and Stratford—the areas of Greater London where the creators were born. Other parts of the Square and set interiors are based on other locations. The bridge is based upon one near BBC Television Centre which carries the Hammersmith & City tube line over Wood Lane W12, the Queen Vic on the former College Park Hotel pub in Willesden at the end of Scrubs Lane at the junction with Harrow Road NW10 just a couple of miles from BBC Television Centre, and the interior to the Fowlers' is based on a house in Manor Road, Colchester, close to where the supervising art director lived. The fictional local newspaper, the "Walford Gazette", in which local news events such as the arrests or murders of characters appear, mirrors the real "Hackney Gazette".

Walford East is a fictional tube station for Walford, and a tube map that was first seen on air in 1996 showed Walford East between Bow Road and West Ham, in the actual location of Bromley-by-Bow on the District and Hammersmith & City lines.

Walford has the postal district of E20. The postcode district was selected as if it were part of the actual E postcode area which covers much of east London although the next unused postcode district in the area was, and still is (), E19. The "E" stands for "Eastern". In 1917 the postal districts in London were assigned alphabetically according to the name of the main sorting office for each district. If Walford had been assigned in this scheme it would have been given E17, which is the postcode district for Walthamstow. In March 2011, Royal Mail allocated the E20 postal district to the 2012 Olympic Park. The postal district in "EastEnders" was entirely fictional up to that point, as London East postal districts stopped at E18 at that time. The show's creators opted for E20 instead of E19 as it was thought to sound better. In September 2011, the postal code for Albert Square was revealed in an episode as E20 6PQ.

"EastEnders" is built around the idea of relationships and strong families, with each character having a place in the community. This theme encompasses the whole Square, making the entire community a family of sorts, prey to upsets and conflict, but pulling together in times of trouble. Co-creator Tony Holland was from a large East End family, and such families have typified "EastEnders". The first central family was the combination of the Fowler family, consisting of Pauline Fowler, her husband Arthur Fowler, and teenage children Mark Fowler and Michelle Fowler and the Beale family, consisting of Pete Beale (Pauline's twin brother), his wife Kathy Beale and their teenage son Ian Beale. Pauline and Pete's mother was the domineering Lou Beale, who lived with Pauline and her family. Holland drew on the names of his own family for the characters.

The Watts and Mitchell families have been central to many notable "EastEnders" storylines, the show having been dominated by the Watts in the 1980s, with the 1990s focusing on the Mitchells. The early 2000s saw a shift in attention towards the newly introduced female Slater clan, before a renewal of emphasis upon the restored Watts family beginning in 2003. Since 2006, "EastEnders" has largely been dominated by the Mitchell and Branning families, though the early 2010s also saw a renewed focus on the Moon family, and from 2013 onwards, on the Carters. The Beales are the show's longest running family, having been in "EastEnders" since it began in 1985. Key people involved in the production of "EastEnders" have stressed how important the idea of strong families is to the programme. Peggy Mitchell, in particular, is notorious for her ceaseless repetition of such statements as "You're a Mitchell!" and "It's all about family!" Pauline Fowler is also known for her insistence on family and mentioning her brother and husband to instill loyalty from family members. Her mother Lou Beale is renowned for her family meetings and traditional approach to family. More recently, Derek Branning regularly expresses the importance of a strong family unit. As the eldest sibling, he is constantly asserting his position as head of his family and reminding everyone to pull together in times of trouble. Additionally, Derek commonly refers to himself, Max Branning and Jack Branning as "the Branning brothers."

"EastEnders" has an emphasis on strong family matriarchs, with examples including Pauline Fowler and Peggy Mitchell, helping to attract a female audience. John Yorke, then the BBC's head of drama production, put this down to Tony Holland's "gay sensibility, which showed a love for strong women". The matriarchal role is one that has been seen in various reincarnations since the programme's inception, often depicted as the centre of the family unit. The original matriarch was Lou Beale, though later examples include Mo Harris, Pat Butcher, Zainab Masood and Cora Cross. These characters are seen as being loud and interfering but most importantly, responsible for the well-being of the family and usually stressing the importance of family, reflecting on the past.

The show often includes strong, brassy, long-suffering women who exhibit diva-like behaviour and stoically battle through an array of tragedy and misfortune. Such characters include Angie Watts, Kathy Beale, Sharon Watts, Pat Butcher, Denise Fox and Tanya Branning. Conversely there are female characters who handle tragedy less well, depicted as eternal victims and endless sufferers, who include Sue Osman, Little Mo Mitchell, Laura Beale, Lisa Fowler, Ronnie Mitchell and Linda Carter. The 'tart with a heart' is another recurring character, often popular with viewers. Often their promiscuity masks a hidden vulnerability and a desire to be loved. Such characters have included Pat Butcher (though in her latter years, this changed), Tiffany Mitchell, Kat Slater, Stacey Slater, Dawn Swann, Roxy Mitchell and Whitney Dean.

A gender balance in the show is maintained via the inclusion of various "macho" male personalities such as Mick Carter, Phil Mitchell, Grant Mitchell, Jack Branning and Max Branning, "bad boys" such as Den Watts, Michael Moon and Vincent Hubbard, and "heartthrobs" such as Simon Wicks, Jamie Mitchell, Dennis Rickman and Joey Branning. Another recurring male character type is the smartly dressed businessman, often involved in gang culture and crime and seen as a local authority figure. Examples include Steve Owen, Jack Dalton, Andy Hunter, Johnny Allen and Derek Branning. Following criticism aimed at the show's over-emphasis on 'gangsters' in 2005, such characters have been significantly reduced. Another recurring male character seen in "EastEnders" is the 'loser' or 'soft touch', males often comically under the thumb of their female counterparts, which have included Arthur Fowler, Ricky Butcher, Lofty Holloway and Billy Mitchell. Other recurring character types that have appeared throughout the serial are "cheeky-chappies" Pete Beale, Alfie Moon, Garry Hobbs and Kush Kazemi, "lost girls" such as Mary Smith, Donna Ludlow and Mandy Salter, delinquents such as Stacey Slater, Jay Brown and Lola Pearce, "villains" such as Nick Cotton, Trevor Morgan, May Wright, Yusef Khan, Archie Mitchell and Dean Wicks, "bitches" such as Cindy Beale, Janine Butcher, Lucy Beale, Abi Branning and Babe Smith and cockney "wide boys" or "wheeler dealers" such as Frank Butcher, Alfie Moon, Kevin Wicks, Darren Miller and Fatboy.

Over the years "EastEnders" has typically featured a number of elderly residents, who are used to show vulnerability, nostalgia, stalwart-like attributes and are sometimes used for comedic purposes. The original elderly residents included Lou Beale, Ethel Skinner and Dot Cotton. Over the years they have been joined by the likes of Mo Butcher, Jules Tavernier, Marge Green, Nellie Ellis, Jim Branning, Charlie Slater, Mo Harris, Patrick Trueman, Cora Cross, Les Coker, Rose Cotton, Pam Coker, Stan Carter, Babe Smith, Claudette Hubbard, Sylvie Carter, Ted Murray and Joyce Murray. Focus on elderly characters has decreased since the show's inception. The programme has more recently included a higher number of teenagers and successful young adults in a bid to capture the younger television audience. This has spurred criticism, most notably from the actress Anna Wing, who played Lou Beale in the show. She commented, "I don't want to be disloyal, but I think you need a few mature people in a soap because they give it backbone and body... if all the main people are young it gets a bit thin and inexperienced. It gets too lightweight."

"EastEnders" has been known to feature a 'comedy double-act', originally demonstrated with the characters of Dot and Ethel, whose friendship was one of the serial's most enduring. Other examples include Paul Priestly and Trevor Short, Huw Edwards and Lenny Wallace, Shirley Carter and Heather Trott, Garry Hobbs and Minty Peterson, Denise Fox and Zainab Masood, Poppy Meadow and Jodie Gold and Peggy Mitchell and Pat Butcher. In 1989 especially, characters were brought in who were deliberately conceived as comic or light-hearted. Such characters included Julie Cooper—a brassy maneater; Marge Green—a batty older lady played by veteran comedy actress Pat Coombs; Trevor Short (Phil McDermott)—the "village idiot"; his friend, northern heartbreaker Paul Priestly (Mark Thrippleton); wheeler-dealer Vince Johnson (Hepburn Graham) and Laurie Bates (Gary Powell), who became Pete Beale's (Peter Dean) sparring partner. The majority of "EastEnders" characters are working-class. Middle-class characters do occasionally become regulars, but have been less successful and rarely become long-term characters. In the main, middle-class characters exist as villains, such as James Wilmott-Brown, May Wright, Stella Crawford and Yusef Khan, or are used to promote positive liberal influences, such as Colin Russell or Rachel Kominski.

"EastEnders" has always featured a culturally diverse cast which has included black, Asian, Turkish, Polish and Latvian characters. "The expansion of minority representation signals a move away from the traditional soap opera format, providing more opportunities for audience identification with the characters and hence a wider appeal". Despite this, the programme has been criticised by the Commission for Racial Equality, who argued in 2002 that "EastEnders" was not giving a realistic representation of the East End's "ethnic make-up". They suggested that the average proportion of visible minority faces on "EastEnders" was substantially lower than the actual ethnic minority population in East London boroughs, and it therefore reflected the East End in the 1960s, not the East End of the 2000s. Furthermore, it was suggested that an element of "tokenism" and stereotyping surrounded many of these minority characters. The programme has since attempted to address these issues. A sari shop was opened and various characters of different ethnicities were introduced throughout 2006 and 2007, including the Fox family, the Masoods, and various background artists. This was part of producer Diederick Santer's plan to "diversify", to make "EastEnders" "feel more 21st century". "EastEnders" has had varying success with ethnic minority characters. Possibly the least successful were the Indian Ferreira family, who were not well received by critics or viewers and were dismissed as unrealistic by the Asian community in the UK.

"EastEnders" has been praised for its portrayal of characters with disabilities, including Adam Best (spina bifida), Noah Chambers (deaf), Jean Slater and her daughter Stacey (bipolar disorder), Janet Mitchell (Down syndrome) and Jim Branning (stroke). The show also features a large number of gay, lesbian and bisexual characters (see list of soap operas with LGBT characters), including Colin Russell, Barry Clark, Simon Raymond, Tony Hills, Sonia Fowler, Naomi Julien, Tina Carter, Tosh Mackintosh, Ben Mitchell and Paul Coker. Kyle Slater, a transgender character, was introduced in 2015.

"EastEnders" has a high cast turnover and characters are regularly changed to facilitate storylines or refresh the format. The show has also become known for the return of characters after they have left the show. Sharon Rickman returned in August 2012 for her third stint on the show. Den Watts returned 14 years after he was believed to have died, a feat repeated by Kathy Beale in 2015. Speaking extras, including Tracey the barmaid (who has been in the show since the first episode in 1985), have made appearances throughout the show's duration, without being the focus of any major storylines. The character of Nick Cotton gained a reputation for making constant exits and returns since the programme's first year, until the character's death in 2015.

, Adam Woodyatt, Gillian Taylforth and Letitia Dean are the only members of the original cast remaining in the show, in their roles of Ian Beale, Kathy Beale and Sharon Watts respectively. Original character Michelle Fowler also appears in the show although recast. Ian Beale is the only character to have appeared continuously from the first episode without officially leaving, and is the longest-serving character in "EastEnders". Dot Cotton is the longest-serving female character in the show having served since 1985, whilst Pat Butcher is the longest-serving former character, appearing from 1986 until 2012.

"EastEnders" programme makers took the decision that the show was to be about "everyday life" in the inner city "today" and regarded it as a "slice of life". Creator/producer Julia Smith declared that "We don't make life, we reflect it". She also said, "We decided to go for a realistic, fairly outspoken type of drama which could encompass stories about homosexuality, rape, unemployment, racial prejudice, etc., in a believable context. Above all, we wanted realism". In 2011, the head of BBC drama, John Yorke, said that the real East End had changed significantly since "EastEnders" started, and the show no longer truly reflected real life, but that it had an "emotional truthfulness" and was partly "true to the original vision" and partly "adapt[ing] to a changing world", adding that "If it was a show where every house cost a fortune and everyone drove a Lexus, it wouldn't be "EastEnders". You have to show shades of that change, but certain things are immutable, I would argue, like The Vic and the market."

In the 1980s, "EastEnders" featured "gritty" storylines involving drugs and crime, representing the issues faced by working-class Thatcherite Britain. Storylines included the cot death of 14-month-old Hassan Osman, Nick Cotton's homophobia, racism and murder of Reg Cox, Arthur Fowler's unemployment reflecting the recession of the 1980s, the rape of Kathy Beale in 1988 by James Willmott-Brown and Michelle Fowler's teenage pregnancy. The show also dealt with prostitution, mixed-race relationships, shoplifting, sexism, divorce, domestic violence and mugging. In 1989, the programme came under criticism in the British media for being too depressing, and according to writer Colin Brake, the programme makers were determined to change this. In 1989, there was a deliberate attempt to increase the lighter, more comic aspects of life in Albert Square. This led to the introduction of some characters who were deliberately conceived as comic or light-hearted. Brake suggested that humour was an important element in "EastEnders" storylines during 1989, with a greater amount of slapstick and light comedy than before. He classed 1989's changes as a brave experiment, and suggested that while some found this period of "EastEnders" entertaining, many other viewers felt that the comedy stretched the programme's credibility. Although the programme still covered many issues in 1989, such as domestic violence, drugs, rape and racism, Brake reflected that the new emphasis on a more balanced mix between "light and heavy storylines" gave the illusion that the show had lost a "certain edge".

As the show progressed into the 1990s, "EastEnders" still featured hard-hitting issues such as Mark Fowler discovering he was HIV positive in 1991, the death of his wife Gill from an AIDS-related illness in 1992, murder, adoption, abortion, Peggy Mitchell's battle with breast cancer, and Phil Mitchell's alcoholism and violence towards wife Kathy. Mental health issues were confronted in 1996 when 16-year-old Joe Wicks developed schizophrenia following the off-screen death of his sister in a car crash. The long-running storyline of Mark Fowler's HIV was so successful in raising awareness that in 1999, a survey by the National Aids Trust found teenagers got most of their information about HIV from the soap, though one campaigner noted that in some ways the storyline was not reflective of what was happening at the time as the condition was more common among the gay community. Still, heterosexual Mark struggled with various issues connected to his HIV status, including public fears of contamination, a marriage breakdown connected to his inability to have children and the side effects of combination therapies.

In the early 2000s, "EastEnders" covered the issue of euthanasia (Ethel Skinner's death in a pact with her friend Dot Cotton), the unveiling of Kat Slater's abuse by her uncle Harry as a child (which led to the birth of her daughter Zoe, who had been brought up to believe that Kat was her sister), the domestic abuse of Little Mo Morgan by husband Trevor (which involved rape and culminated in Trevor's death after he tried to kill Little Mo in a fire), Sonia Jackson giving birth at the age of 15 and then putting her baby up for adoption, and Janine Butcher's prostitution, agoraphobia and drug addiction. The soap also tackled the issue of mental illness and carers of people who have mental conditions, illustrated with mother and daughter Jean and Stacey Slater; Jean suffers from bipolar disorder, and teenage daughter Stacey was her carer (this storyline won a Mental Health Media Award in September 2006). Stacey went on to struggle with the disorder herself. The issue of illiteracy was highlighted by the characters of middle-aged Keith and his young son Darren. "EastEnders" has also covered the issue of Down syndrome, as Billy and Honey Mitchell's baby, Janet Mitchell, was born with the condition in 2006. "EastEnders" covered child abuse with its storyline involving Phil Mitchell's 11-year-old son Ben and lawyer girlfriend Stella Crawford, and child grooming involving the characters Tony King and Whitney Dean.

Aside from this, soap opera staples of youthful romance, jealousy, domestic rivalry, gossip and extramarital affairs are regularly featured, with high-profile storylines occurring several times a year. Whodunits also feature regularly, including the "Who Shot Phil?" storyline in 2001 that attracted over 19 million viewers and was one of the biggest successes in British soap television, the "Who Killed Archie?" story, which was revealed in a special live episode of the show that drew a peak of 17 million viewers, and "Who Killed Lucy Beale?".

The exterior set for the fictional Albert Square is located in the permanent backlot of the BBC Elstree Centre, Borehamwood, Hertfordshire, at , and is outdoors and open to the weather. It was initially built in 1984 with a specification that it should last for at least fifteen years at a cost of £750,000. The "EastEnders" lot was designed by Keith Harris, who was a senior designer within the production team together with supervising art directors Peter Findley and Gina Parr. The main buildings on the square consisted originally of hollow shells, constructed from marine plywood facades mounted onto steel frames. The lower walls, pavements, etc., were constructed of real brick and tarmac. The set had to be made to look as if it had been standing for years. This was done by a number of means, including chipping the pavements, using chemicals to crack the top layer of the paint work, using varnish to create damp patches underneath the railway bridge, and making garden walls in such a way they appeared to sag. The final touches were added in summer 1984, these included a telephone box, telegraph pole that was provided by British Telecom, lampposts that were provided by Hertsmere Borough Council and a number of vehicles parked on the square. On each set all the appliances are fully functional such as gas cookers, the laundry washing machines and The Queen Victoria beer pumps.

The walls were intentionally built crooked to give them an aged appearance. The drains around the set are real so rainwater can naturally flow from the streets. The square was built in two phases with only three sides being built, plus Bridge Street, to begin with in 1984, in time to be used for the show's first episode. Then in 1986, Harris added an extension to the set, building the fourth side of Albert Square, and in 1987, Turpin Road was added, which included buildings such as The Dagmar.

In 1993, George Street was added, and soon after Walford East Underground station was built, to create further locations when "EastEnders" went from two to three episodes per week. The set was constructed by the BBC in-house construction department under construction manager Mike Hagan. The initial build took six months to complete. Most of the buildings on Albert Square have no interior filming space, with a few exceptions, and most do not have rears or gardens. Most areas by the front (and sometimes back) doors are decorated and dressed to match the interior set to allow shots of doors being opened. The grocery shop was originally open fronted, it was turned into a closed front shop, with removable interior walls to allow for filming inside the shop when the set was expanded in 1987. Some interior shots are filmed in the actual buildings, and the café also has some interior decoration so some limited filming can take place by the door. The newer exterior sets including fish and chip shop, video shop and beauty salon had some interior filming space to create a greater sense of realism. As the show is filmed up to six weeks in advance, the trees need to have extra leaves stuck on them during the spring to make them look like they would in summer.

In February 2008, it was reported that the set would transfer to Pinewood Studios in Buckinghamshire, where a new set would be built as the set was looking "shabby", with its flaws showing up on high-definition television broadcasts. However, by April 2010 a follow-up report confirmed that Albert Square would remain at Elstree Studios for at least another four years, taking the set through its 25th anniversary. The set was consequently rebuilt for high definition on the same site, using mostly real brick with some areas using a new improved plastic brick. Throughout rebuilding filming would still take place, and so scaffolding was often seen on screen during the process, with some storylines written to accommodate the rebuilding, such as the Queen Vic fire.

In 2014, then executive producer Dominic Treadwell-Collins said that he wanted Albert Square to look like a real-life east London neighbourhood so that the soap would "better reflect the more fashionable areas of east London beloved of young professionals" giving a flavour of the "creeping gentrification" of east London. He added, "It should feel more like London. It's been frozen in aspic for too long." The BBC announced that they would rebuild the "EastEnders" set, to secure the long-term future of the show, with completion expected to be in 2018. The set will provide a modern, upgraded exterior filming resource for "EastEnders", and will copy the appearance of the existing buildings. However, it will be 20 per cent bigger, in order to enable greater editorial ambition and improve working conditions for staff. A temporary set will be created on site to enable filming to continue while the permanent structure is rebuilt. As of May 2016, the rebuild has been delayed until 2020 and will cost in excess of £15 million, although the main part of the set is scheduled to be able to start filming in May 2019.

The majority of "EastEnders" episodes are filmed at the BBC Elstree Centre in Borehamwood, Hertfordshire. When the number of episodes was increased to four per week, more studio space was needed, so "Top of the Pops" was moved from its studio at Elstree to BBC Television Centre in April 2001. Episodes are produced in "quartets" of four episodes, each of which starts filming on a Tuesday and takes nine days to record. Each day, between 25 and 30 scenes are recorded. During the filming week, actors can film for as many as eight to twelve episodes. Exterior scenes are filmed on a specially constructed film lot, and interior scenes take place in four studios. The episodes are usually filmed about six to eight weeks in advance of broadcast. During the winter period, filming can take place up to twelve weeks in advance, due to less daylight for outdoor filming sessions. This time difference has been known to cause problems when filming outdoor scenes. On 8 February 2007, heavy snow fell on the set and filming had to be cancelled as the scenes due to be filmed on the day were to be transmitted in April. "EastEnders" is normally recorded using four cameras. When a quartet is completed, it is edited by the director, videotape editor and script supervisor. The producer then reviews the edits and decides if anything needs to be re-edited, which the director will do. A week later, sound is added to the episodes and they are technically reviewed, and are ready for transmission if they are deemed of acceptable quality.

Although episodes are predominantly recorded weeks before they are broadcast, occasionally, "EastEnders" includes current events in their episodes. In 1987, "EastEnders" covered the general election. Using a plan devised by co-creators Smith and Holland, five minutes of material was cut from four of the pre-recorded episodes preceding the election. These were replaced by specially recorded election material, including representatives from each major party, and a scene recorded on the day after the election reflecting the result, which was broadcast the following Tuesday. The result of the 2010 general election was referenced in 7 May 2010 episode. During the 2006 FIFA World Cup, actors filmed short scenes following the tournament's events that were edited into the programme in the following episode. Last-minute scenes have also been recorded to reference the fiftieth anniversary of the end of the Second World War in 1995, the two-minute silence on Remembrance Day 2005 (2005 also being the year for the sixtieth anniversary of the end of the Second World War and the 200th anniversary of the Battle of Trafalgar), Barack Obama's election victory in 2008, the death of Michael Jackson in 2009, the 2010 Comprehensive Spending Review, Andy Murray winning the Men's Singles at the 2013 Wimbledon Championships, the wedding of Prince William and Kate Middleton, the birth of Prince George of Cambridge. Scotland voting no against independence in 2014, and the 100th anniversary of the beginning of the Great War.

"EastEnders" is often filmed on location, away from the studios in Borehamwood. Sometimes an entire quartet is filmed on location, which has a practical function and are the result of "EastEnders" making a "double bank", when an extra week's worth of episodes are recorded at the same time as the regular schedule, enabling the production of the programme to stop for a two-week break at Christmas. These episodes often air in late June or early July and again in late October or early November. The first time this happened was in December 1985 when Pauline (Wendy Richard) and Arthur Fowler (Bill Treacher) travelled to the Southend-on-Sea to find their son Mark, who had run away from home. In 1986, "EastEnders" filmed overseas for the first time, in Venice, and this was also the first time it was not filmed on videotape, as a union rule at the time prevented producers taking a video crew abroad and a film crew had to be used instead. In 2011, it was reported that eight per cent of the series is filmed on location.

If scenes during a normal week are to be filmed on location, this is done during the normal recording week. Off-set locations that have been used for filming include Clacton (1989), Devon (September 1990), Hertfordshire (used for scenes set in Gretna Green in July 1991), Portsmouth (November 1991), Milan (1997), Ireland (1997), Amsterdam (December 1999), Brighton (2001) and Portugal (2003). In 2003, filming took place at Loch Fyne Hotel and Leisure Club in Inveraray, The Arkinglass Estate in Cairndow and Grims Dyke Hotel, Harrow Weald, north London, for a week of episodes set in Scotland. 9 April 2007 episode featured scenes filmed at St Giles Church and The Blacksmiths Arms public house in Wormshill, the Ringlestone Inn, two miles away and Court Lodge Farm in Stansted, Kent. and the Port of Dover, Kent. .

Other locations have included the court house, a disused office block, Evershed House, and St Peter's Church, all in St Albans, an abandoned mental facility in Worthing, Carnaby Street in London, and a wedding dress shop in Muswell Hill, north London. A week of episodes in 2011 saw filming take place on a beach in Thorpe Bay and a pier in Southend-on-Sea—during which a stuntman was injured when a gust of wind threw him off balance and he fell onto rocks— with other scenes filmed on the Essex coast. In 2012, filming took place in Keynsham, Somerset. In January 2013, on-location filming at Grahame Park in Colindale, north London, was interrupted by at least seven youths who threw a firework at the set and threatened to cut members of the crew. In October 2013, scenes were filmed on a road near London Southend Airport in Essex.

The two-handers (when only two actors appear in an episode) were originally done for speed; while a two-hander is being filmed, the rest of the cast can be making another episode.

"EastEnders" has featured seven live broadcasts. For its 25th anniversary in February 2010, a live episode was broadcast in which Stacey Slater (Lacey Turner) was revealed as Archie Mitchell's (Larry Lamb) killer. Turner was told only 30 minutes before the live episode and to maintain suspense, she whispers this revelation to former lover and current father-in-law, Max Branning, in the very final moments of the live show. Many other cast members only found out at the same time as the public, when the episode was broadcast. On 23 July 2012, a segment of that evening's episode was screened live as Billy Mitchell (Perry Fenwick) carried the Olympic Flame around Walford in preparation for the 2012 Summer Olympics. In February 2015, for the soap's 30th anniversary, five episodes in a week featured live inserts throughout them. Episodes airing on Tuesday 17, Wednesday 18 and Thursday 19 (which featured an hour long episode and a second episode) all featured at least one live insert. The show revealed that the killer of Lucy Beale (Hetti Bywater) was her younger brother, Bobby (Eliot Carrington), during the second episode on Thursday, after a ten month mystery regarding who killed her. In a flashback episode which revisited the night of the murder, Bobby was revealed to have killed his sister. The aftermath episode, which aired on Friday 20, was completely live and explained in detail Lucy's death. Carrington was told he was Lucy's killer on Monday 16, while Laurie Brett (who plays Bobby's adoptive mother, Jane) was informed in November, due to the character playing a huge role in the cover-up of Lucy's murder. Bywater only discovered Bobby was responsible for Lucy's death on the morning of Thursday, 19 November, several hours before they filmed the scenes revealing Bobby as Lucy's killer.

Each episode should run for 27 minutes and 15 seconds, however, if any episode runs over or under then it is the job of post-production to cut or add scenes where appropriate. As noted in the 1994 behind-the-scenes book, "EastEnders: The First 10 Years", after filming, tapes were sent to the videotape editor, who then edited the scenes together into an episode. The videotape editor used the director's notes so they knew which scenes the director wanted to appear in a particular episode. The producer might have asked for further changes to be made. The episode was then copied onto D3 video. The final process was to add the audio which included background noise such as a train or a jukebox music and to check it met the BBC's technical standard for broadcasting.

Since 2010, "EastEnders" no longer uses tapes in the recording or editing process. After footage is recorded, the material is sent digitally to the post production team. The editors then assemble all the scenes recorded for the director to view and note any changes that are needed. The sound team also have the capability to access the edited episode, enabling them to dub the sound and create the final version.

According to the book "How to Study Television", in 1995 "EastEnders" cost the BBC £40,000 per episode on average. A 2012 agreement between the BBC, the Writers' Guild of Great Britain and the Personal Managers' Association set out the pay rate for "EastEnders" scripts as £137.70 per minute of transmission time (£4,131 for 30 minutes), which is 85 per cent of the rate for scripts for other BBC television series. The writers would be paid 75 per cent of that fee for any repeats of the episode. In 2011, it was reported that actors receive a per-episode fee of between £400 and £1,200, and are guaranteed a certain number of episodes per year, perhaps as few as 30 or as many as 100, therefore annual salaries could range from £12,000 to £200,000 depending on the popularity of a character. Some actors' salaries were leaked in 2006, revealing that Natalie Cassidy (Sonia Fowler) was paid £150,000, Cliff Parisi (Minty Peterson) received £220,000, Barbara Windsor (Peggy Mitchell) and Steve McFadden (Phil Mitchell) each received £360,000 and Wendy Richard (Pauline Fowler) had a salary of £370,000. In 2017, it was revealed that Danny Dyer (Mick Carter) and Adam Woodyatt (Ian Beale) were the highest-paid actors in "EastEnders", earning between £200,000 and £249,999, followed by Laurie Brett (Jane Beale), Letitia Dean (Sharon Watts), Tameka Empson (Kim Fox), Linda Henry (Shirley Carter), Scott Maslen (Jack Branning), Diane Parish (Denise Fox), Gillian Taylforth (Kathy Beale) and Lacey Turner (Stacey Slater), earning between £150,000 and £199,999.

A 2011 report from the National Audit Office (NAO) showed that "EastEnders" had an annual budget of £29.9 million. Of that, £2.9 million was spent on scripts and £6.9 million went towards paying actors, extras and chaperones for child actors. According to the NAO, BBC executives approved £500,000 of additional funding for the 25th anniversary live episode (19 February 2010). With a total cost of £696,000, the difference was covered from the 2009–2010 series budget for "EastEnders". When repeats and omnibus editions are shown, the BBC pays additional fees to cast and scriptwriters and incurs additional editing costs, which in the period 2009–2010, amounted to £5.5million. According to a Radio Times article for 212 episodes it works out at £141,000 per episode or 3.5p per viewer hour.

In 2014, two new studios were built and they were equipped with low-energy lighting which has saved approximately 90,000 kwh per year. A carbon literacy course was run with Heads of Departments of "EastEnders" attending and as a result, representatives from each department agreed to meet quarterly to share new sustainability ideas. The paper usage was reduced by 50 per cent across script distribution and other weekly documents and 20 per cent across all other paper usage. The production team now use recycled paper and recycled stationery.

Also changes to working online has also saved transportation cost of distribution 2,500 DVDs per year. Sets, costumes, paste pots and paint are all recycled by the design department. Cars used by the studio are low emission vehicles and the production team take more efficient energy efficient generators out on location. Caterers no longer use polystyrene cups and recycling on location must be provided.

As a result of "EastEnders" sustainability, it was awarded albert+, an award that recognises the production's commitment to becoming a more eco-friendly television production. The albert+ logo was first shown at the end of the "EastEnders" titles for episode 5281 on 9 May 2016.

Since 1985, "EastEnders" has remained at the centre of BBC One's primetime schedule. Since 2001, it has been broadcast at 7:30pm on Tuesday and Thursday, and 8pm on Monday and Friday. "EastEnders" was originally broadcast twice weekly at 7:00pm on Tuesdays and Thursdays from 19 February 1985. However, in September 1985 the two episodes were moved to 7:30pm as Michael Grade did not want the soap running in direct competition with "Emmerdale Farm", and this remained the same until 7 April 1994. The BBC had originally planned to take advantage of the 'summer break' that "Emmerdale Farm" usually took to capitalise on ratings, but ITV1 added extra episodes and repeats so that "Emmerdale Farm" was not taken off the air over the summer. Realising the futility of the situation, Grade decided to move the show to the later 7:30pm slot, but to avoid tabloid speculation that it was a "panic move" on the BBC's behalf, they had to "dress up the presentation of that move in such a way as to protect the show" giving "all kinds of reasons" for the move.

"EastEnders" output then increased to three times a week on Mondays, Tuesday and Thursdays from 11 April 1994 until 2 August 2001.
From 10 August 2001, "EastEnders" then added its fourth episode (shown on Fridays). This caused some controversy as it clashed with "Coronation Street", which at the time was moved to 8pm to make way for an hour-long episode of rural soap "Emmerdale" at 7pm. The move immediately provoked an angry response from ITV insiders, who argued that the BBC's last-minute move—only revealed at 3.30pm on the day—broke an unwritten scheduling rule that the two flagship soaps would not be put directly against each other. In this first head-to-head battle, "EastEnders" claimed victory over its rival.

In early 2003, viewers could watch episodes of "EastEnders" on digital channel BBC Three before they were broadcast on BBC One. This was to coincide with the relaunch of the channel and helped BBC Three break the one million viewers mark for the first time with 1.03 million who watched to see Mark Fowler's departure. According to the "EastEnders" website, there are on average 208 episodes outputted each year.

"EastEnders" was regularly repeated at 10pm on BBC Choice from the channel's launch in 1998, a practice continued by BBC Three for many years until mid-2012 with the repeat moving to 10:30pm. From 25 December 2010 to 29 April 2011 the show was repeated on BBC HD in a Simulcast with BBC Three. In 2015, the BBC Three repeat moved back to 10pm. In February 2016, the repeat moved to W, the rebranded Watch, after BBC Three became an online-only channel. Episodes of "EastEnders" are available on-demand through BBC iPlayer for thirty days after their original screening.

The omnibus edition, a compilation of the week's episodes in a continuous sequence, originally aired on BBC One on Sunday afternoons, until 1 April 2012 when it was changed to a late Friday night or early Saturday morning slot, commencing 6 April 2012, though the exact time differed. It reverted to a weekend daytime slot as from January 2013 on BBC Two. In 2014, the omnibus moved back to around midnight on Friday nights, and in April 2015, the omnibus was axed, following detailed audience research and the introduction of 30-day catch up on BBC iPlayer and the planning of BBC One +1. The last omnibus on the BBC was shown on 24 April 2015. W took over the same-day repeat of "EastEnders" the first being 16 February 2016, they also returned the weekend omnibus 20 February 2016. On 10 April 2018, it was announced that W had ceased to air the repeats at the conclusion of their contract with the show.

From 20 February to 26 May 1995, as part of the programme's 10th Anniversary celebrations, episodes from 1985 were repeated each morning at 10am, starting from episode one. Four specially selected episodes from 1985 and 1986 were also repeated on BBC1 on Friday evenings at 8pm under the banner "The Unforgettable EastEnders". These included The wedding of Michelle Fowler and Lofty Holloway, The revelation of the father of Michelle's baby, a two-hander between Dot Cotton and Ethel Skinner and the 1986 Christmas episode featuring Den Watts presenting Angie Watts with divorce papers.

"EastEnders" reruns began on UKTV Gold when the channel launched in 1992. The series ran from the first episode and followed its original broadcast order until August 1996 when the channel looped back to the first episode. In October 2008, UKTV Gold ceased showing "EastEnders". The last episode shown was from January 2006. Watch launched in October 2008 and "EastEnders" reruns from 5 January 2009 to 22 October 2009, finishing with episodes originally broadcast in December 2006.

On 1 December 2012, the BBC uploaded the first 54 episodes of "EastEnders" to YouTube, and on 23 July 2013 they uploaded a further 14 episodes bringing the total to 68. None of these episodes was available as of 7 October 2017.

"EastEnders" is broadcast around the world in many English-speaking countries. New Zealand became the first to broadcast "EastEnders" overseas, the first episode being shown on 27 September 1985. This was followed by the Netherlands on 8 December 1986, Australia on 5 January 1987, Norway on 27 April, and Barcelona on 30 June with a Catalan dub. On 9 July 1987, it was announced that the show would be aired in the United States on PBS. BBC Worldwide licensed 200 hours of "EastEnders" to for broadcast in Serbia on RTS (dubbed into Serbian); it began airing the first episode in December 1997. The series was broadcast in the United States until BBC America ceased broadcasts of the serial in 2003, amidst fan protests. In June 2004, the Dish Network satellite television provider picked up "EastEnders", broadcasting episodes starting at the point where BBC America had ceased broadcasting them, offering the serial as a pay-per-view item. Episodes air two months behind the UK schedule. Episodes from prior years are still shown on various PBS stations in the US. Since 7 March 2017, "EastEnders" has been available in the United States on demand, 24 hours after it has aired in the United Kingdom via BritBox, a joint venture between BBC and ITV.

The series was screened in Australia by ABC TV from 1987 until 1991. It is aired in Australia on Satellite & Streaming services on BBC UKTV, from Mondays to Thursdays 7:50pm–8:30pm with two advert breaks of five minutes each. Episodes are shown about one week after their UK broadcast. In New Zealand, it was shown by TVNZ on TVNZ 1 for several years, and then on Prime each weekday afternoon. It is shown by BBC UKTV Mondays to Thursdays at 8pm. Episodes are about two weeks behind the UK.

"EastEnders" is shown on BBC Entertainment (formerly BBC Prime) in Europe and in Africa, where it is approximately six episodes behind the UK. It was also shown on BBC Prime in Asia, but when the channel was replaced by BBC Entertainment, it ceased showing the series. In Canada, "EastEnders" was shown on BBC Canada until 2010, at which point it was picked up by VisionTV. In Sweden, Denmark, Finland and Norway, the show is now cancelled but was for decades shown on BBC Nordic channels with local subtitles.

In Ireland, "EastEnders" was shown on TV3 from September 1998 until March 2001, when it moved over to RTÉ One, after RTÉ lost the rights to air rival soap "Coronation Street" to TV3. The series is simulcast with BBC One, which is widely available in the Republic, but carries advertising since its 1998 debut on Irish TV. Additionally episodes of "EastEnders" are available on-demand through RTÉ Player for seven days after their original screening.

HM Forces and their families stationed overseas can watch "EastEnders" on BBC One, carried by the British Forces Broadcasting Service, which is also available to civilians in the Falkland Islands and Tristan da Cunha. It was previously shown on BFBS1.

In 1991 the BBC sold the programme's format rights to a Dutch production company IDTV, the programme was renamed "Het Oude Noorden" (Translation: Old North). The Dutch version was re-written from already existing "EastEnders" scripts. The schedule remained the same as "EastEnders" twice weekly episodes, however some notable changes included the programme is now set in Rotterdam rather than London, characters are given Dutch names Den and Angie became Ger and Ankie and The Queen Victoria pub is renamed "Cade Faas".

According to Barbara Jurgen who re-wrote the scripts for a Dutch audience he said "The power of the show is undeniable. The Scripts are full of hard, sharp drama, plus great one-liners which will translate well to Holland." The Dutch version began broadcasting on VARA 13 March 1993 and ran for 20 episodes but was cancelled after twenty episodes.

On 26 December 1988, the first "EastEnders" "bubbles" was shown, titled "CivvyStreet". Since then, "Return of Nick Cotton" (2000), "Ricky & Bianca" (2002), "Dot's Story" (2003), "Perfectly Frank" (2003) and "Pat and Mo" (2004) have all been broadcast, each episode looking into lives of various characters and revealing part of their backstories or lives since leaving "EastEnders". In 1993, the two-part story "Dimensions in Time", a charity cross-over with "Doctor Who", was shown.

In 1998, "EastEnders Revealed" was launched on BBC Choice (now BBC Three). The show takes a look behind the scenes of the "EastEnders" and investigates particular places, characters or families within "EastEnders". An episode of "EastEnders Revealed" that was commissioned for BBC Three attracted 611,000 viewers. As part of the BBC's digital push, "EastEnders Xtra" was introduced in 2005. The show was presented by Angellica Bell and was available to digital viewers at 8:30pm on Monday nights. It was also shown after the Sunday omnibus. The series went behind the scenes of the show and spoke to some of the cast members. A new breed of behind-the-scenes programmes have been broadcast on BBC Three since 1 December 2006. These are all documentaries related to current storylines in "EastEnders", in a similar format to "EastEnders Revealed", though not using the "EastEnders Revealed" name.

In October 2009, a 12-part Internet spin-off series entitled "" was announced. The series was conceived by executive producer Diederick Santer "as a way of nurturing new, young talent, both on- and off-screen, and exploring the stories of the soaps' anonymous bystanders." "E20" features a group of sixth-form characters and targets the ""Hollyoaks" demographic". It was written by a team of young writers and was shown three times a week on the "EastEnders" website from 8 January 2010. A second ten-part series started in September 2010, with twice-weekly episodes available online and an omnibus on BBC Three. A third series of 15 episodes started in September 2011.

"EastEnders" and rival soap opera "Coronation Street" took part in a crossover episode for Children in Need on 19 November 2010 called "East Street". On 4 April 2015, "EastEnders" confirmed plans for a BBC One series featuring Kat and Alfie Moon. The six-part drama, "", was created by executive producer Dominic Treadwell-Collins and his team. In the spin-off, the Moons visit Ireland where they "search for answers to some very big questions."

Until its closure, BBC Store released 553 "EastEnders" episodes from various years, including the special episode "CivvyStreet", available to buy as digital downloads.

An example of "EastEnders" popularity is that after episodes, electricity use in the United Kingdom rises significantly as viewers who have waited for the show to end begin boiling water for tea, a phenomenon known as TV pickup. Over five minutes, power demand rises by three GW, the equivalent of 1.5 to 1.75 million teakettles. National Grid personnel watch the show to know when closing credits begin so they can prepare for the surge, asking for additional power from France if necessary.

"EastEnders" is the BBC's most consistent programme in terms of ratings. It has proved highly popular and Appreciation Indexes reflected this, rising from 55–60 at the launch to 85–95 later on, a figure which was nearly ten points higher than the average for a British soap opera. Research suggested that people found the characters true to life, the plots believable and, importantly in the face of criticism of the content, people watched as a family and regarded it as viewing for all the family. Based on market research by BBC commissioning in 2003, "EastEnders" is most watched by 60- to 74-year-olds, closely followed by 45- to 59-year-olds. An average "EastEnders" episode attracts a total audience share between 35 and 40 per cent. The same-day repeat showing on BBC Three attracted an average of 500,000 viewers, whilst the Sunday omnibus generally attracted 3 million. "EastEnders" is one of the more popular programmes on British television and while the show's ratings have fallen since its initial surge in popularity and the advent of multichannel digital television, the programme continues to be successful for the BBC. "EastEnders" two main rivals are ITV soaps "Coronation Street" and "Emmerdale".
The launch show in 1985 attracted 17.35 million viewers. 25 July 1985 was the first time the show's viewership rose to first position in the weekly top ten shows for BBC One. The highest rated episode of "EastEnders" is the Christmas Day 1986 episode, which attracted a combined 30.15 million viewers who tuned into either the original transmission or the omnibus to see Den Watts hand over divorce papers to his wife Angie. This remains the highest rated episode of a soap in British television history.

In 2001, "EastEnders" clashed with "Coronation Street" for the first time. "EastEnders" won the battle with 8.4 million viewers (41% share) whilst "Coronation Street" lagged behind with 7.3 million viewers (34 per cent share). On 21 September 2004, Louise Berridge, the then executive producer, quit following criticism of the show. The following day the show received its lowest ever ratings at that time (6.2 million) when ITV scheduled an hour-long episode of "Emmerdale" against it. "Emmerdale" was watched by 8.1 million people. The poor ratings motivated the press into reporting viewers were bored with implausible and ill-thought-out storylines. Under new producers, "EastEnders" and "Emmerdale" continued to clash at times, and "Emmerdale" tended to come out on top, giving "EastEnders" lower than average ratings. In 2006, "EastEnders" regularly attracted between 8 and 12 million viewers in official ratings. "EastEnders" received its second lowest ratings on 17 May 2007, when 4.0 million viewers tuned in. This was also the lowest ever audience share, with just 19.6 per cent. This was attributed to a conflicting one hour special episode of "Emmerdale" on ITV1. However, ratings for the 10pm "EastEnders" repeat on BBC Three reached an all-time high of 1.4 million. However, there have been times when "EastEnders" had higher ratings than "Emmerdale" despite the two going head-to-head.

The ratings increased in 2010, thanks to the "Who Killed Archie?" storyline and second wedding of Ricky Butcher and Bianca Jackson, and the show's first live episode on 19 February 2010. The live-episode averaged 15.6 million viewers, peaking at 16.6 million in the final five minutes of broadcast. In January 2010, the average audience was higher than that of "Coronation Street" for the first time in three years. During the 30th anniversary week in which there were live elements and the climax of the Who Killed Lucy Beale? storyline, 10.84 million viewers tuned in for the 30th anniversary episode itself in an hour long special on 19 February 2015 (peaking with 11.9 million). Later on in the same evening, a special flashback episode averaged 10.3 million viewers, and peaked with 11.2 million. The following day, the anniversary week was rounded off with another fully live episode (the second after 2010) with 9.97 million viewers watching the aftermath of the reveal, the Beale family finding out the truth of Lucy's killer and deciding to keep it a secret.

"EastEnders" is the most complained about programme on the BBC. It has received both praise and criticism for most of its storylines, which have dealt with difficult themes, such as violence, rape, murder and child abuse.

Mary Whitehouse, social critic, argued at the time that "EastEnders" represented a violation of "family viewing time" and that it undermined the watershed policy. She regarded "EastEnders" as a fundamental assault on the family and morality itself. She made reference to representation of family life and emphasis on psychological and emotional violence within the show. She was also critical of language such as "bleeding", "bloody hell", "bastard" and "for Christ's sake". However, Whitehouse also praised the programme, describing Michelle Fowler's decision not to have an abortion as a "very positive storyline". She also felt that "EastEnders" had been cleaned up as a result of her protests, though she later commented that "EastEnders" had returned to its old ways. Her criticisms were widely reported in the tabloid press as ammunition in its existing hostility towards the BBC. The stars of "Coronation Street" in particular aligned themselves with Mary Whitehouse, gaining headlines such as "STREETS AHEAD! RIVALS LASH SEEDY EASTENDERS" and "CLEAN UP SOAP! Street Star Bill Lashes 'Steamy' EastEnders".

"EastEnders" has been criticised for being too violent, most notably during a domestic violence storyline between Little Mo Morgan and her husband Trevor Morgan. As "EastEnders" is shown pre-watershed, there were worries that some scenes in this storyline were too graphic for its audience. Complaints against a scene in which Little Mo's face was pushed in gravy on Christmas Day were upheld by the Broadcasting Standards Council. However, a helpline after this episode attracted over 2000 calls. Erin Pizzey, who became internationally famous for having started one of the first women's refuges, said that "EastEnders" had done more to raise the issue of violence against women in one story than she had done in 25 years. The character of Phil Mitchell (played by Steve McFadden since early 1990) has been criticised on several occasions for glorifying violence and proving a bad role model to children. On one occasion following a scene in an episode broadcast in October 2002, where Phil brutally beat his godson, Jamie Mitchell (Jack Ryder), 31 complaints came from viewers who watched the scenes.

In 2003, cast member Shaun Williamson, who was in the final months of his role of Barry Evans, said that the programme had become much grittier over the past ten to fifteen years, and found it "frightening" that parents let their young children watch.

In 2005, the BBC was accused of anti-religious bias by a House of Lords committee, who cited "EastEnders" as an example. Dr. Indarjit Singh, editor of the Sikh Messenger and patron of the World Congress of Faiths, said: ""EastEnders" Dot Cotton is an example. She quotes endlessly from the Bible and it ridicules religion to some extent." In July 2010, complaints were received following the storyline of Christian minister Lucas Johnson committing a number of murders that he believed was his duty to God, claiming that the storyline was offensive to Christians.

In 2008, "EastEnders", along with "Coronation Street", was criticised by Martin McGuinness, then Northern Ireland's deputy first minister, for "the level of concentration around the pub" and the "antics portrayed in the [...] Queen Vic".

In 1997 several episodes were shot and set in Ireland, resulting in criticisms for portraying the Irish in a negatively stereotypical way. Ted Barrington, the Irish Ambassador to the UK at the time, described the portrayal of Ireland as an "unrepresentative caricature", stating he was worried by the negative stereotypes and the images of drunkenness, backwardness and isolation. Jana Bennett, the BBC's then director of production, later apologised for the episodes, stating on BBC1's news bulletin: "It is clear that a significant number of viewers have been upset by the recent episodes of "EastEnders", and we are very sorry, because the production team and programme makers did not mean to cause any offence." A year later BBC chairman Christopher Bland admitted that as result of the Irish-set EastEnders episodes, the station failed in its pledge to represent all groups accurately and avoid reinforcing prejudice.

In 2008, the show was criticised for stereotyping their Asian and Black characters, by having a black single mum, Denise Wicks, and an Asian shopkeeper, Zainab Masood. There has been criticism that the programme does not authentically portray the ethnic diversity of the population of East London, with the programme being 'twice as white' as the real East End.

Some storylines have provoked high levels of viewer complaints. In August 2006, a scene involving Carly Wicks (Kellie Shirley) and Jake Moon (Joel Beckett) having sex on the floor of Scarlet nightclub, and another scene involving Owen Turner violently attacking Denise Fox, prompted 129 and 128 complaints, respectively. Carly and Jake's sex scenes were later removed from the Sunday omnibus edition. The showdown of Rob, Dawn and May's storyline where May stated to Dawn she could give her an elective caesarean (Dawn being handcuffed to the bed) prompted 200 complaints. The 2007 domestic abuse storyline involving Ben Mitchell and Stella Crawford attracted sixty complaints from viewers, who found scenes where Ben was attacked by bullies as Stella looked on "upsetting".

In March 2008, scenes showing Tanya Branning (Jo Joyner) and boyfriend, Sean Slater (Rob Kazinsky), burying Tanya's husband Max (Jake Wood) alive, attracted many complaints. The UK communications regulator Ofcom later found that the episodes depicting the storyline were in breach of the 2005 Broadcasting Code. They contravened the rules regarding protection of children by appropriate scheduling, appropriate depiction of violence before the 9 p.m. watershed and appropriate depiction of potentially offensive content. In September 2008, "EastEnders" began a grooming and paedophilia storyline involving characters Tony King (Chris Coghill), Whitney Dean (Shona McGarty), Bianca Jackson (Patsy Palmer), Lauren Branning (Madeline Duggan) and Peter Beale (Thomas Law). The storyline attracted over 200 complaints .

In December 2010, Ronnie swapped her newborn baby, who died in cot, with Kat Moon's living baby. Around 3,400 complaints were received, with viewers branding the storyline "insensitive", "irresponsible" and "desperate". Roz Laws from the "Sunday Mercury" called the plot "shocking and ridiculous" and asked "are we really supposed to believe that Kat won't recognise that the baby looks different?" The Foundation for the Study of Infant Deaths (FSID) praised the storyline, and its director Joyce Epstein explained, "We are very grateful to "EastEnders" for their accurate depiction of the devastating effect that the sudden death of an infant can have on a family. We hope that this story will help raise the public's awareness of cot death, which claims 300 babies' lives each year." By 7 January, that storyline had generated the most complaints in show history: the BBC received about 8,500 complaints, and media regulator Ofcom received 374. Despite the controversy however, "EastEnders" pulled in rating highs of 9–10 million throughout the duration of the storyline.

In October 2014, the BBC defended a storyline, after receiving 278 complaints about 6 October 2014 episode where pub landlady Linda Carter was raped. On 17 November 2014 it was announced that Ofcom will investigate over the storyline. On 5 January 2015, the investigation was cleared by Ofcom. A spokesman of Ofcom said: "After carefully investigating complaints about this scene, Ofcom found the BBC took appropriate steps to limit offence to viewers. This included a warning before the episode and implying the assault, rather than depicting it. Ofcom also took into account the programme's role in presenting sometimes challenging or distressing social issues."

In 2010, "EastEnders" came under criticism from the police for the way that they were portrayed during the "Who Killed Archie?" storyline. During the storyline, DCI Jill Marsden and DC Wayne Hughes talk to locals about the case and Hughes accepts a bribe. The police claimed that such scenes were "damaging" to their reputation and added that the character DC Deanne Cunningham was "irritatingly inaccurate". In response to the criticism, "EastEnders" apologised for offending real life detectives and confirmed that they use a police consultant for such storylines.

In October 2012, a storyline involving Lola Pearce, forced to hand over her baby Lexi Pearce, was criticised by the charity The Who Cares? Trust, who called the storyline an "unhelpful portrayal" and said it had already received calls from members of the public who were "distressed about the "EastEnders" scene where a social worker snatches a baby from its mother's arms". The scenes were also condemned by the British Association of Social Workers (BASW), calling the BBC "too lazy and arrogant" to correctly portray the child protection process, and saying that the baby was taken "without sufficient grounds to do so". Bridget Robb, acting chief of the BASW, said the storyline provoked "real anger among a profession well used to a less than accurate public and media perception of their jobs .. "EastEnders" shabby portrayal of an entire profession has made a tough job even tougher."

Since its premiere in 1985, "EastEnders" has had a large impact on British popular culture. It has frequently been referred to in many different media, including songs and television programmes.

Many books have been written about "EastEnders". Notably, from 1985 to 1988, author and television writer Hugh Miller wrote 17 novels, detailing the lives of many of the show's original characters before 1985, when events on screen took place.

Kate Lock also wrote four novels centred on more recent characters; Steve Owen, Grant Mitchell, Bianca Jackson and Tiffany Mitchell. Lock also wrote a character guide entitled "Who's Who in EastEnders" () in 2000, examining main characters from the first 15 years of the show.

Show creators Julia Smith and Tony Holland also wrote a book about the show in 1987, entitled "EastEnders: The Inside Story" (), telling the story of how the show made it to screen. Two special anniversary books have been written about the show; "EastEnders: The First 10 Years: A Celebration" () by Colin Brake in 1995 and "EastEnders: 20 Years in Albert Square" () by Rupert Smith in 2005.





</doc>
<doc id="9996" url="https://en.wikipedia.org/wiki?curid=9996" title="Embroidery">
Embroidery

Embroidery is the craft of decorating fabric or other materials using a needle to apply thread or yarn.

Embroidery may also incorporate other materials such as pearls, beads, quills, and sequins. In modern days, embroidery is usually seen on caps, hats, coats, blankets, dress shirts, denim, dresses, stockings, and golf shirts. Embroidery is available with a wide variety of thread or yarn color.

Some of the basic techniques or stitches of the earliest embroidery are chain stitch, buttonhole or blanket stitch, running stitch, satin stitch, cross stitch. Those stitches remain the fundamental techniques of hand embroidery today.

The process used to tailor, patch, mend and reinforce cloth fostered the development of sewing techniques, and the decorative possibilities of sewing led to the art of embroidery. Indeed, the remarkable stability of basic embroidery stitches has been noted:

The art of embroidery has been found worldwide and several early examples have been found. Works in China have been dated to the Warring States period (5th–3rd century BC). In a garment from Migration period Sweden, roughly 300–700 AD, the edges of bands of trimming are reinforced with running stitch, back stitch, stem stitch, tailor's buttonhole stitch, and whip-stitching, but it is uncertain whether this work simply reinforced the seams or should be interpreted as decorative embroidery.

Ancient Greek mythology has credited the goddess Athena with passing down the art of embroidery along with weaving, leading to the famed competition between herself and the mortal Arachne.

Depending on time, location and materials available, embroidery could be the domain of a few experts or a widespread, popular technique. This flexibility led to a variety of works, from the royal to the mundane.

Elaborately embroidered clothing, religious objects, and household items often were seen as a mark of wealth and status, as in the case of Opus Anglicanum, a technique used by professional workshops and guilds in medieval England. In 18th-century England and its colonies, samplers employing fine silks were produced by the daughters of wealthy families. Embroidery was a skill marking a girl's path into womanhood as well as conveying rank and social standing.

Conversely, embroidery is also a folk art, using materials that were accessible to nonprofessionals. Examples include Hardanger from Norway, Merezhka from Ukraine, Mountmellick embroidery from Ireland, Nakshi kantha from Bangladesh and West Bengal, and Brazilian embroidery. Many techniques had a practical use such as Sashiko from Japan, which was used as a way to reinforce clothing.

Embroidery was an important art in the Medieval Islamic world. The 17th-century Turkish traveler Evliya Çelebi called it the "craft of the two hands". Because embroidery was a sign of high social status in Muslim societies, it became widely popular. In cities such as Damascus, Cairo and Istanbul, embroidery was visible on handkerchiefs, uniforms, flags, calligraphy, shoes, robes, tunics, horse trappings, slippers, sheaths, pouches, covers, and even on leather belts. Craftsmen embroidered items with gold and silver thread. Embroidery cottage industries, some employing over 800 people, grew to supply these items.

In the 16th century, in the reign of the Mughal Emperor Akbar, his chronicler Abu al-Fazl ibn Mubarak wrote in the famous Ain-i-Akbari:
"His majesty (Akbar) pays much attention to various stuffs; hence Irani, Ottoman, and Mongolian articles of wear are in much abundance especially textiles embroidered in the patterns of "Nakshi", "Saadi", "Chikhan", "Ari", "Zardozi", "Wastli", "Gota" and "Kohra". The imperial workshops in the towns of Lahore, Agra, Fatehpur and Ahmedabad turn out many masterpieces of workmanship in fabrics, and the figures and patterns, knots and variety of fashions which now prevail astonish even the most experienced travelers. Taste for fine material has since become general, and the drapery of embroidered fabrics used at feasts surpasses every description."

The development of machine embroidery and its mass production came about in stages in the Industrial Revolution. The earliest machine embroidery used a combination of machine looms and teams of women embroidering the textiles by hand. This was done in France by the mid-1800s. The manufacture of machine-made embroideries in St. Gallen in eastern Switzerland flourished in the latter half of the 19th century.

Embroidery can be classified according to what degree the design takes into account the nature of the base material and by the relationship of stitch placement to the fabric. The main categories are free or surface embroidery, counted embroidery, and needlepoint or canvas work.

In free or surface embroidery, designs are applied without regard to the weave of the underlying fabric. Examples include crewel and traditional Chinese and Japanese embroidery.

Counted-thread embroidery patterns are created by making stitches over a predetermined number of threads in the foundation fabric. Counted-thread embroidery is more easily worked on an even-weave foundation fabric such as embroidery canvas, aida cloth, or specially woven cotton and linen fabrics . Examples include cross-stitch and some forms of blackwork embroidery.

While similar to counted thread in regards to technique, in canvas work or needlepoint, threads are stitched through a fabric mesh to create a dense pattern that completely covers the foundation fabric. Examples of canvas work include bargello and Berlin wool work.

Embroidery can also be classified by the similarity of appearance. In drawn thread work and cutwork, the foundation fabric is deformed or cut away to create holes that are then embellished with embroidery, often with thread in the same color as the foundation fabric. When created with white thread on white linen or cotton, this work is collectively referred to as whitework. However, whitework can either be counted or free. Hardanger embroidery is a counted embroidery and the designs are often geometric. Conversely, styles such as Broderie anglaise are similar to free embroidery, with floral or abstract designs that are not dependent on the weave of the fabric.

The fabrics and yarns used in traditional embroidery vary from place to place. Wool, linen, and silk have been in use for thousands of years for both fabric and yarn. Today, embroidery thread is manufactured in cotton, rayon, and novelty yarns as well as in traditional wool, linen, and silk. Ribbon embroidery uses narrow ribbon in silk or silk/organza blend ribbon, most commonly to create floral motifs.

Surface embroidery techniques such as chain stitch and couching or laid-work are the most economical of expensive yarns; couching is generally used for goldwork. Canvas work techniques, in which large amounts of yarn are buried on the back of the work, use more materials but provide a sturdier and more substantial finished textile.

In both canvas work and surface embroidery an embroidery hoop or frame can be used to stretch the material and ensure even stitching tension that prevents pattern distortion. Modern canvas work tends to follow symmetrical counted stitching patterns with designs emerging from the repetition of one or just a few similar stitches in a variety of hues. In contrast, many forms of surface embroidery make use of a wide range of stitching patterns in a single piece of work.

Contemporary embroidery is stitched with a computerized embroidery machine using patterns digitized with embroidery software. In machine embroidery, different types of "fills" add texture and design to the finished work. Machine embroidery is used to add logos and monograms to business shirts or jackets, gifts, and team apparel as well as to decorate household linens, draperies, and decorator fabrics that mimic the elaborate hand embroidery of the past.

There has also been a development in free hand machine embroidery, new machines have been designed that allow for the user to create free-motion embroidery which has its place in textile arts, quilting, dressmaking, home furnishings and more.

City and Guilds qualification in Embroidery allows embroiderers to become recognized for their skill. This qualification also gives them the credibility to teach. For example, the notable textiles artist, Kathleen Laurel Sage, began her teaching career by getting the City and Guilds Embroidery 1 and 2 qualifications. She has now gone on to write a book on the subject.




</doc>
<doc id="9997" url="https://en.wikipedia.org/wiki?curid=9997" title="Edward Mitchell Bannister">
Edward Mitchell Bannister

Edward Mitchell Bannister (ca. 1828 – January 9, 1901) was a Black Canadian-American Tonalist painter. Like other Tonalists, his style and predominantly pastoral subject matter were drawn from his admiration for Millet and the French Barbizon School.

Bannister was born in St. Andrews, New Brunswick and moved to New England in the late 1840s, where he remained for the rest of his life. While Bannister was well known in the artistic community of his adopted home of Providence, Rhode Island and admired within the wider East Coast art world (he won a bronze medal for his large oil "Under the Oaks" at the 1876 Philadelphia Centennial), he was largely forgotten for almost a century for a complexity of reasons, principally connected with racial prejudice. Bannister began his official career as an artist when an article in the 1867 "New York Herald" belittled both him and his work, stating ““- "the negro has an appreciation for art while being manifestly unable to produce it".” Prior to working as a painter, Bannister worked as barber and tinted photos. In the late 1850s he attended lectures given by D. William Rimmer, a sculptor noted for his accuracy in rendering the human figure. Over the course of his career, Bannister was inspired by the Barbizon school-inspired paintings of William Morris Hunt who had studied in Europe and held numerous public exhibitions in Boston around the 1860s.

With the ascendency of the civil rights movement in the 1970s, his work was again celebrated and collected. In 1978, Rhode Island College dedicated its Art Gallery in Bannister's name with the exhibition: "Four From Providence ~ Alston, Bannister, Jennings & Prophet". This event was attended and commented on by numerous notable political figures of the time, and supported by the Rhode Island Committee for Humanities and the Rhode Island Historical Society. Events like this, across the entire cultural landscape, have ensured that his artwork and life will not be again forgotten. Although committed to freedom and equal rights for Afro-Americans, he chose not to inject those issues into his work, adopting instead a spiritual philosophy and individually expressive style which represented harmony and liberty on a more universal plane.

Although primarily known for his idealised landscapes and seascapes, Bannister also executed portraits, biblical and mythological scenes, and genre scenes. An intellectual autodidact, his tastes in literature were typical of an educated Victorian painter, including Spenser, Virgil, Ruskin and Tennyson, from whose works much of his iconography can be traced. His work was stunning, oftentimes reflecting the composition, mood and influences of French Barbizon painters Jean Baptise mille Corot"," J"ean" Francois Millete, and Charles-Francois Daubigny"." He had an affinity for Native American thought which was reflected in the spirituality of his work. Progressively his understanding for color / color mixing improved and the quality of work increased, really digging into naturalistic territory.

Bannister died of a heart attack in 1901 while attending a prayer meeting at his church, Elmwood Avenue Free Baptist Church. He is buried in the North Burial Ground in Providence.

The historian Anne Louise Avery is currently compiling the first Catalogue Raisonné and major biography of Bannister's work. See the International Foundation for Art Research for further details.

E. M. Bannister Gallery at Rhode Island College is named after Bannister.

As his career matured, Banister accumulated many honors, several from the Massachusetts Charitable Mechanics Association. His two biggest support systems were his mother, who was the catalyst from the very beginning for his passion for the arts, and his wife, who also was an activist. Both strong supporters of abolition, wife Christina lobbied for equal pay for black soldiers during the Civil War and also organized the soldiers’ relief fair in 1864. In 1880 a group of professional artists, amateurs, and art collectors founded the Providence Art Club to stimulate the appreciation of art in the community, one of those people was Edward Mitchell Bannister. Bannister was the only major African American artist of the late nineteenth century who developed his talents without the benefit of European exposure,

The house at 93 Benevolent Street in Providence, where Bannister lived between 1884 and 1899, was owned by Brown University until 2016. Brown renovated the property and restored it to its original appearance, and it was sold to Professor Jeff Huang as part of the Brown to Brown Home Ownership Program.





</doc>
<doc id="10000" url="https://en.wikipedia.org/wiki?curid=10000" title="Eiffel">
Eiffel

Eiffel may refer to:






</doc>
<doc id="10002" url="https://en.wikipedia.org/wiki?curid=10002" title="Emil Kraepelin">
Emil Kraepelin

Emil Kraepelin (; 15 February 1856 – 7 October 1926) was a German psychiatrist. H. J. Eysenck's "Encyclopedia of Psychology" identifies him as the founder of modern scientific psychiatry, psychopharmacology and psychiatric genetics.

Kraepelin believed the chief origin of psychiatric disease to be biological and genetic malfunction. His theories dominated psychiatry at the start of the 20th century and, despite the later psychodynamic influence of Sigmund Freud and his disciples, enjoyed a revival at century's end. While he proclaimed his own high clinical standards of gathering information "by means of expert analysis of individual cases", he also drew on reported observations of officials not trained in psychiatry. His textbooks do not contain detailed case histories of individuals but mosaic-like compilations of typical statements and behaviors from patients with a specific diagnosis. He has been described as "a scientific manager" and "a political operator", who developed "a large-scale, clinically oriented, epidemiological research programme".

Kraepelin, whose father, Karl Wilhelm, was a former opera singer, music teacher, and later successful story teller, was born in 1856 in Neustrelitz, in the Duchy of Mecklenburg-Strelitz in Germany. He was first introduced to biology by his brother Karl, 10 years older and, later, the director of the Zoological Museum of Hamburg.

Kraepelin began his medical studies in 1874 at the University of Leipzig and completed them at the University of Würzburg (1877–78). At Leipzig, he studied neuropathology under Paul Flechsig and experimental psychology with Wilhelm Wundt. Kraepelin would be a disciple of Wundt and had a lifelong interest in experimental psychology based on his theories. While there, Kraepelin wrote a prize-winning essay, "The Influence of Acute Illness in the Causation of Mental Disorders". At Würzburg he completed his "Rigorosum" (roughly equivalent to an MBBS viva-voce examination) in March 1878, his "Staatsexamen" (licensing examination) in July 1878, and his "Approbation" (his license to practice medicine; roughly equivalent to an MBBS) on 9 August 1878. From August 1878 to 1882, he worked with Bernhard von Gudden at the University of Munich. Returning to the University of Leipzig in February 1882, he worked in Wilhelm Heinrich Erb's neurology clinic and in Wundt's psychopharmacology laboratory. He completed his "Habilitation" thesis at Leipzig; it was entitled "The Place of Psychology in Psychiatry". On 3 December 1883 he completed his "" (habilitation recognition procedure) at Munich.

Kraepelin's major work, "Compendium der Psychiatrie: Zum Gebrauche für Studirende und Aertze" ("Compendium of Psychiatry: For the Use of Students and Physicians"), was first published in 1883 and was expanded in subsequent multivolume editions to "Ein Lehrbuch der Psychiatrie" ("A Textbook: Foundations of Psychiatry and Neuroscience"). In it, he argued that psychiatry was a branch of medical science and should be investigated by observation and experimentation like the other natural sciences. He called for research into the physical causes of mental illness, and started to establish the foundations of the modern classification system for mental disorders. Kraepelin proposed that by studying case histories and identifying specific disorders, the progression of mental illness could be predicted, after taking into account individual differences in personality and patient age at the onset of disease.

In 1884 he became senior physician in the Prussian provincial town of Leubus, Silesia Province, and the following year he was appointed director of the Treatment and Nursing Institute in Dresden. On 1 July 1886, at the age of 30, Kraepelin was named Professor of Psychiatry at the University of Dorpat (today the University of Tartu) in what is today Estonia (see Burgmair et al., vol. IV). Four years later, on 5 December 1890, he became department head at the University of Heidelberg, where he remained until 1904. While at Dorpat he became the director of the 80-bed University Clinic. There he began to study and record many clinical histories in detail and "was led to consider the importance of the course of the illness with regard to the classification of mental disorders".

In 1903 Kraepelin moved to Munich to become Professor of Clinical Psychiatry at the University of Munich.

He was elected a member of the Royal Swedish Academy of Sciences in 1908.

In 1912 at the request of the German Society of Psychiatry, he began plans to establish a centre for research. Following a large donation from the Jewish German-American banker James Loeb, who had at one time been a patient, and promises of support from "patrons of science", the German Institute for Psychiatric Research was founded in 1917 in Munich. Initially housed in existing hospital buildings, it was maintained by further donations from Loeb and his relatives. In 1924 it came under the auspices of the Kaiser Wilhelm Society for the Advancement of Science. The German American Rockefeller family's Rockefeller Foundation made a large donation enabling the development of a new dedicated building for the institute, along Kraepelin's guidelines, which was officially opened in 1928.

Kraepelin spoke out against the barbarous treatment that was prevalent in the psychiatric asylums of the time, and crusaded against alcohol, capital punishment and the imprisonment rather than treatment of the insane. He rejected psychoanalytical theories that posited innate or early sexuality as the cause of mental illness, and he rejected philosophical speculation as unscientific. He focused on collecting clinical data and was particularly interested in neuropathology (e.g., diseased tissue).

In the later period of his career, as a convinced champion of social Darwinism, he actively promoted a policy and research agenda in racial hygiene and eugenics.

Kraepelin retired from teaching at the age of 66, spending his remaining years establishing the Institute. The ninth and final edition of his "Textbook" was published in 1927, shortly after his death. It comprised four volumes and was ten times larger than the first edition of 1883.

In the last years of his life, Kraepelin was preoccupied with Buddhist teachings and was planning to visit Buddhist shrines at the time of his death, according to his daughter, Antonie Schmidt-Kraepelin.

Kraepelin announced that he had found a new way of looking at mental illness, referring to the traditional view as "symptomatic" and to his view as "clinical". This turned out to be his paradigm-setting synthesis of the hundreds of mental disorders classified by the 19th century, grouping diseases together based on classification of syndrome—common "patterns" of symptoms over time—rather than by simple similarity of major symptoms in the manner of his predecessors. Kraepelin described his work in the 5th edition of his textbook as a "decisive step from a symptomatic to a clinical view of insanity. . . . The importance of external clinical signs has . . . been subordinated to consideration of the conditions of origin, the course, and the terminus which result from individual disorders. Thus, all purely symptomatic categories have disappeared from the nosology".

Kraepelin is specifically credited with the classification of what was previously considered to be a unitary concept of psychosis, into two distinct forms (known as the Kraepelinian dichotomy):


Drawing on his long-term research, and using the criteria of course, outcome and prognosis, he developed the concept of dementia praecox, which he defined as the "sub-acute development of a peculiar simple condition of mental weakness occurring at a youthful age". When he first introduced this concept as a diagnostic entity in the fourth German edition of his "Lehrbuch der Psychiatrie" in 1893, it was placed among the degenerative disorders alongside, but separate from, catatonia and dementia paranoides. At that time, the concept corresponded by and large with Ewald Hecker's hebephrenia. In the sixth edition of the "Lehrbuch" in 1899 all three of these clinical types are treated as different expressions of one disease, dementia praecox.

One of the cardinal principles of his method was the recognition that any given symptom may appear in virtually any one of these disorders; e.g., there is almost no single symptom occurring in dementia praecox which cannot sometimes be found in manic depression. What distinguishes each disease symptomatically (as opposed to the underlying pathology) is not any particular (pathognomonic) symptom or symptoms, but a specific pattern of symptoms. In the absence of a direct physiological or genetic test or marker for each disease, it is only possible to distinguish them by their specific pattern of symptoms. Thus, Kraepelin's system is a method for pattern recognition, not grouping by common symptoms.

Kraepelin also demonstrated specific patterns in the genetics of these disorders and specific and characteristic patterns in their course and outcome. Generally speaking, there tend to be more schizophrenics among the relatives of schizophrenic patients than in the general population, while manic depression is more frequent in the relatives of manic depressives. Though, of course, this does not demonstrate genetic linkage, as this might be a socio-environmental factor as well.

He also reported a pattern to the course and outcome of these conditions. Kraepelin believed that schizophrenia had a deteriorating course in which mental function continuously (although perhaps erratically) declines, while manic-depressive patients experienced a course of illness which was intermittent, where patients were relatively symptom-free during the intervals which separate acute episodes. This led Kraepelin to name what we now know as schizophrenia, dementia praecox (the dementia part signifying the irreversible mental decline). It later became clear that dementia praecox did not necessarily lead to mental decline and was thus renamed schizophrenia by Eugen Bleuler to correct Kraepelin's misnomer.

In addition, as Kraepelin accepted in 1920, "It is becoming increasingly obvious that we cannot satisfactorily distinguish these two diseases"; however, he maintained that "On the one hand we find those patients with irreversible dementia and severe cortical lesions. On the other are those patients whose personality remains intact". Nevertheless, overlap between the diagnoses and neurological abnormalities (when found) have continued, and in fact a diagnostic category of schizoaffective disorder would be brought in to cover the intermediate cases.

Kraepelin devoted very few pages to his speculations about the etiology of his two major insanities, dementia praecox and manic-depressive insanity. However, from 1896 to his death in 1926 he held to the speculation that these insanities (particularly dementia praecox) would one day probably be found to be caused by a gradual systemic or "whole body" disease process, probably metabolic, which affected many of the organs and nerves in the body but affected the brain in a final, decisive cascade.

In the first through sixth edition of Kraepelin's influential psychiatry textbook, there was a section on moral insanity, which meant then a disorder of the emotions or moral sense without apparent delusions or hallucinations, and which Kraepelin defined as "lack or weakness of those sentiments which counter the ruthless satisfaction of egotism". He attributed this mainly to degeneration. This has been described as a psychiatric redefinition of Cesare Lombroso's theories of the "born criminal", conceptualised as a "moral defect", though Kraepelin stressed it was not yet possible to recognise them by physical characteristics.

In fact from 1904 Kraepelin changed the section heading to "The born criminal", moving it from under "Congenital feeble-mindedness" to a new chapter on "Psychopathic personalities". They were treated under a theory of degeneration. Four types were distinguished: born criminals (inborn delinquents), pathological liars, querulous persons, and Triebmenschen (persons driven by a basic compulsion, including vagabonds, spendthrifts, and dipsomaniacs). The concept of "psychopathic inferiorities" had been recently popularised in Germany by Julius Ludwig August Koch, who proposed congenital and acquired types. Kraepelin had no evidence or explanation suggesting a congenital cause, and his assumption therefore appears to have been simple "biologism". Others, such as Gustav Aschaffenburg, argued for a varying combination of causes. Kraepelin's assumption of a moral defect rather than a positive drive towards crime has also been questioned, as it implies that the moral sense is somehow inborn and unvarying, yet it was known to vary by time and place, and Kraepelin never considered that the moral sense might just be different. Kurt Schneider criticized Kraepelin's nosology for appearing to be a list of behaviors that he considered undesirable, rather than medical conditions, though Schneider's alternative version has also been criticised on the same basis. Nevertheless, many essentials of these diagnostic systems were introduced into the diagnostic systems, and remarkable similarities remain in the DSM-IV and ICD-10. The issues would today mainly be considered under the category of personality disorders, or in terms of Kraepelin's focus on psychopathy.

Kraepelin had referred to psychopathic conditions (or "states") in his 1896 edition, including compulsive insanity, impulsive insanity, homosexuality, and mood disturbances. From 1904, however, he instead termed those "original disease conditions, and introduced the new alternative category of psychopathic personalities. In the eighth edition from 1909 that category would include, in addition to a separate "dissocial" type, the excitable, the unstable, the Triebmenschen driven persons, eccentrics, the liars and swindlers, and the quarrelsome. It has been described as remarkable that Kraepelin now considered mood disturbances to be not part of the same category, but only attenuated (more mild) phases of manic depressive illness; this corresponds to current classification schemes.

Kraepelin postulated that there is a specific brain or other biological pathology underlying each of the major psychiatric disorders. As a colleague of Alois Alzheimer, he was a co-discoverer of Alzheimer's disease, and his laboratory discovered its pathological basis. Kraepelin was confident that it would someday be possible to identify the pathological basis of each of the major psychiatric disorders.

Upon moving to become Professor of Clinical Psychiatry at the University of Munich in 1903, Kraepelin increasingly wrote on social policy issues. He was a strong and influential proponent of eugenics and racial hygiene. His publications included a focus on alcoholism, crime, degeneration and hysteria. Kraepelin was convinced that such institutions as the education system and the welfare state, because of their trend to break the processes of natural selection, undermined the Germans’ biological "struggle for survival". He was concerned to preserve and enhance the German people, the Volk, in the sense of nation or race. He appears to have held Lamarckian concepts of evolution, such that cultural deterioration could be inherited. He was a strong ally and promoter of the work of fellow psychiatrist (and pupil and later successor as director of the clinic) Ernst Rudin to clarify the mechanisms of genetic inheritance as to make a so-called "empirical genetic prognosis".

Martin Brune has pointed out that Kraepelin and Rudin also appear to have been ardent advocates of a self-domestication theory, a version of social Darwinism which held that modern culture was not allowing people to be weeded out, resulting in more mental disorder and deterioration of the gene pool. Kraepelin saw a number of "symptoms" of this, such as "weakening of viability and resistance, decreasing fertility, proletarianisation, and moral damage due to "penning up people" ["Zusammenpferchung"]. He also wrote that "the number of idiots, epileptics, psychopaths, criminals, prostitutes, and tramps who descend from alcoholic and syphilitic parents, and who transfer their inferiority to their offspring, is incalculable". He felt that "the well-known example of the Jews, with their strong disposition towards nervous and mental disorders, teaches us that their extraordinarily advanced domestication may eventually imprint clear marks on the race". Brune states that Kraepelin's nosological system was "to a great deal, built on the degeneration paradigm".

Kraepelin's great contribution in classifying schizophrenia and manic depression remains relatively unknown to the general public, and his work, which had neither the literary quality nor paradigmatic power of Freud's, is little read outside scholarly circles. Kraepelin's contributions were also to a large extent marginalized throughout a good part of the 20th century during the success of Freudian etiological theories. However, his views now dominate many quarters of psychiatric research and academic psychiatry. His fundamental theories on the diagnosis of psychiatric disorders form the basis of the major diagnostic systems in use today, especially the American Psychiatric Association's DSM-IV and the World Health Organization's ICD system, based on the Research Diagnostic Criteria and earlier Feighner Criteria developed by espoused "neo-Kraepelinians", though Robert Spitzer and others in the DSM committees were keen not to include assumptions about causation as Kraepelin had.

Kraepelin has been described as a "scientific manager" and political operator, who developed a large-scale, clinically oriented, epidemiological research programme. In this role he took in clinical information from a wide range of sources and networks. Despite proclaiming high clinical standards for himself to gather information "by means of expert analysis of individual cases", he would also draw on the reported observations of officials not trained in psychiatry. The various editions of his textbooks do not contain detailed case histories of individuals, however, but mosaiclike compilations of typical statements and behaviors from patients with a specific diagnosis. In broader terms, he has been described as a bourgeois or reactionary citizen.

Kraepelin wrote in a "knapp und klar" (concise and clear) style that made his books useful tools for physicians. Abridged and clumsy English translations of the sixth and seventh editions of his textbook in 1902 and 1907 (respectively) by Allan Ross Diefendorf (1871–1943), an assistant physician at the Connecticut Hospital for the Insane at Middletown, inadequately conveyed the literary quality of his writings that made them so valuable to practitioners.

In the Heidelberg and early Munich years he edited "Psychologische Arbeiten", a journal on experimental psychology. One of his own famous contributions to this journal also appeared in the form of a monograph (105 pp.) entitled "Über Sprachstörungen im Traume" ("On Language Disturbances in Dreams"). Kraepelin, on the basis on the dream-psychosis analogy, studied for more than 20 years language disorder in dreams in order to study indirectly schizophasia. The dreams Kraepelin collected are mainly his own. They lack extensive comment by the dreamer. In order to study them the full range of biographical knowledge available today on Kraepelin is necessary (see, e.g., Burgmair et al., I-VII).





For biographies of Kraepelin see:

For English translations of Kraepelin's work see:


</doc>
<doc id="10003" url="https://en.wikipedia.org/wiki?curid=10003" title="Evoluon">
Evoluon

The Evoluon is a conference centre and former science museum erected by the electronics and electrical company Philips at Eindhoven in the Netherlands in 1966. Since its construction, it has become a landmark and a symbol for the city.

The building is unique due to its very futuristic design, resembling a landed flying saucer. It was designed by architects Leo de Bever and Louis Christiaan Kalff, while the exhibition it housed was conceived by James Gardner. De Bever and Kalff only got two demands for the design of the building, it had to be "spectacular" and it had to be possible to hold exhibitions in the building.

Its concrete dome is in diameter and is held in place by of reinforcing steel bars.

In the 1960s and 1970s the Evoluon attracted large visitor numbers, since its interactive exhibitions were a new and unique concept in the Netherlands at that time. But when competing science museums opened in other cities, the number of visitors began to decline. After several years of losing money, the original museum closed down in 1989 and the Evoluon was converted into a conference centre, opening in 1998.

In the UK the Evoluon is chiefly remembered from Bert Haanstra's wordless short film entitled simply "Evoluon", commissioned by Philips to publicise the museum, and shown as a trade test colour film on BBC television from 1968 to 1972.

In October 2013 the Evoluon was used to stage four 3D-concerts by the German electronic band Kraftwerk, each before an audience of 1,200 spectators. Key band member Ralf Hütter handpicked the venue for its retro-futuristic look. Bespoke 3D-visuals of the saucer section of the building descending from space were used in the live rendition of their track "Spacelab".



</doc>
<doc id="10004" url="https://en.wikipedia.org/wiki?curid=10004" title="Educational essentialism">
Educational essentialism

Educational essentialism is an educational philosophy whose adherents believe that children should learn the traditional basic subjects thoroughly. In this philosophical school of thought, the aim is to instill students with the "essentials" of academic knowledge, enacting a back-to-basics approach. Essentialism ensures that the accumulated wisdom of our civilization as taught in the traditional academic disciplines is passed on from teacher to student. Such disciplines might include Reading, Writing, Literature, Foreign Languages, History, Mathematics, Science, Art, and Music. Moreover, this traditional approach is meant to train the mind, promote reasoning, and ensure a common culture.

Essentialism is a relatively conservative stance to education that strives to teach students the knowledge of a society and civilization through a core curriculum. This core curriculum involves such areas that include the study of the surrounding environment, basic natural laws, and the disciplines that promote a happier, more educated living. Other non-traditional areas are also integrated as well in moderation to balance the education. Essentialists' goals are to instill students with the "essentials" of academic knowledge, patriotism, and character development through traditional (or back-to-basic) approaches. This is to promote reasoning, train the mind, and ensure a common culture for all citizens.

Essentialism is the most typically enacted philosophy in American classrooms today. Traces of this can be found in the organized learning centered on teachers and textbooks, in addition to the regular assignments and evaluations.

The role of the teacher as the leader of the classroom is a very important tenet of Educational essentialism. The teacher is the center of the classroom, so they should be rigid and disciplinary. Establishing order in the classroom is crucial for student learning; effective teaching cannot take place in a loud and disorganized environment. It is the teacher's responsibility to keep order in the classroom. The teacher must interpret essentials of the learning process, take the leadership position and set the tone of the classroom. These needs require an educator who is academically well-qualified with an appreciation for learning and development. The teacher must control the students with distributions of rewards and penalties.

The Essentialist movement first began in the United States in the year 1938. In Atlantic City, New Jersey, a group met for the first time called "The Essentialist's Committee for the Advancement of Education." Their emphasis was to reform the educational system to a rationality-based system.

The term essentialist first appeared in the book "An Introduction to the Philosophy of Education" which was written by Michael John Demiashkevich. In his book, Demiashkevich labels some specific educators (including William C. Bagley) as “essentialists." Demiashkevich compared the essentialists to the different viewpoints of the Progressive Education Association. He described how the Progressives preached a “hedonistic doctrine of change” whereas the essentialists stressed the moral responsibility of man for his actions and looked toward permanent principles of behavior (Demiashkevich likened the arguments to those between the Socratics and the Sophists in Greek philosophy). In 1938 Bagley and other educators met together where Bagley gave a speech detailing the main points of the essentialism movement and attacking the public education in the United States. One point that Bagley noted was that students in the U.S. were not getting an education on the same levels as students in Europe who were the same age.

A recent branch has emerged within the essentialist school of thought called "neoessentialism." Emerging in the eighties as a response to the essentialist ideals of the thirties as well as to the criticism of the fifties and the advocates for education in the seventies, neoessentialism was created to try to appease the problems facing the United States at the time. The most notable change within this school of thought is that it called for the creation of a new discipline, computer science.

William Bagley (1874–1946) was an important historical essentialist. William C. Bagley completed his undergraduate degree at Michigan Agricultural College in 1895. It wasn’t until after finishing his undergraduate studies that he truly wanted to be a teacher. Bagley did his Graduate studies at the University of Chicago and at Cornell University. He acquired his Ph.D. in 1900, after which he took his first school job as a Principal in a St. Louis, Missouri Elementary School. Bagley’s devotion increased during his work at Montana State Normal School in Dillon, Montana. It was here where he decided to dedicate his time to the education of teachers and where he published "The Educative Process", launching his name across the nation. Throughout his career Bagley argued against the conservative position that teachers were not in need of special training for their work. He believed that liberal arts material was important in teacher education. Bagley also believed the dominant theories of education of the time were weak and lacking.

In April 1938, he published the "Essentialist's Platform", in which he outlined three major points of essentialism. He described the right of students to a well-educated and culturally knowledgeable teacher. Secondly, he discussed the importance of teaching the ideals of community to each group of students. Lastly, Bagley wrote of the importance of accuracy, thoroughness and effort on the part of the student in the classroom.

Another important essentialist is E. D. Hirsch (1928-). Hirsch was Founder and Chairman of the Core Knowledge Foundation and authored several books concerning fact-based approaches to education. Now retired, he spent many years teaching at the University of Virginia while also being an advocate for the "back to basics" movement. In his most popular book, "Cultural Literacy — What Every American Needs To Know", he offers lists, quotations, and information regarding what he believes is essential knowledge.

See also Arthur Bestor.

The Core Knowledge Schools were founded on the philosophy of essentialist E.D. Hirsch. Although it is difficult to maintain a pure and strict essentialist-only curriculum, these schools have the central aim of establishing a common knowledge base for all citizens. To do so, they follow a nationwide, content-specific, and teacher-centered curriculum. The Core Knowledge curriculum also allows for local variance above and beyond the core curriculum. Central curricular aims are academic excellence and the learning of knowledge, and teachers who are masters of their knowledge areas serve this aim.

Because Essentialism is largely teacher-centered, the role of the student is often called into question. Presumably, in an essentialist classroom, the teacher is the one designing the curriculum for the students based upon the core disciplines. Moreover, he or she is enacting the curriculum and setting the standards which the students must meet. The teacher's evaluative role may undermine students' interest in study. As a result, the students begin to take on more of a passive role in their education as they are forced to meet and learn such standards and information.

Furthermore, there is also speculation that an essentialist education helps in promoting the cultural lag. This philosophy of education is very traditional in the mindset of passing on the knowledge of the culture via the academic disciplines. Thus, students are forced to think in the mindset of the larger culture, and individual creativity and subversive investigation are often not emphasized, or even outright discouraged.



</doc>
<doc id="10005" url="https://en.wikipedia.org/wiki?curid=10005" title="Progressive education">
Progressive education

Progressive education is a pedagogical movement that began in the late nineteenth century; it has persisted in various forms to the present. The term "progressive" was engaged to distinguish this education from the traditional Euro-American curricula of the 19th century, which was rooted in classical preparation for the university and strongly differentiated by social class. By contrast, progressive education finds its roots in present experience. Most progressive education programs have these qualities in common: 


Progressive education can be traced back to the works of John Locke and Jean-Jacques Rousseau, both of whom are known as forerunners of ideas that would be developed by theorists such as Dewey. Locke believed that "truth and knowledge… arise out of observation and experience rather than manipulation of accepted or given ideas". He further discussed the need for children to have concrete experiences in order to learn. Rousseau deepened this line of thinking in Emile, or On Education, where he argued that subordination of students to teachers and memorization of facts would not lead to an education.

In Germany, Johann Bernhard Basedow (1724–1790) established the Philanthropinum at Dessau in 1774. He developed new teaching methods based on conversation and play with the child, and a program of physical development. Such was his success that he wrote a treatise on his methods, "On the best and hitherto unknown method of teaching children of noblemen".

Christian Gotthilf Salzmann (1744–1811) was the founder of the Schnepfenthal institution, a school dedicated to new modes of education (derived heavily from the ideas of Jean-Jacques Rousseau). He wrote "Elements of Morality, for the Use of Children", one of the first books translated into English by Mary Wollstonecraft.

Johann Heinrich Pestalozzi (1746–1827) was a Swiss pedagogue and educational reformer who exemplified Romanticism in his approach. He founded several educational institutions both in German- and French-speaking regions of Switzerland and wrote many works explaining his revolutionary modern principles of education. His motto was "Learning by head, hand and heart". His research and theories closely resemble those outlined by Rousseau in Emile. He is further considered by many to be the "father of modern educational science" His psychological theories pertain to education as they focus on the development of object teaching, that is, he felt that individuals best learned through experiences and through a direct manipulation and experience of objects. He further speculated that children learn through their own internal motivation rather than through compulsion. (See Intrinsic vs. Extrinsic motivation). A teacher's task will be to help guide their students as individuals through their learning and allow it to unfold naturally.

Friedrich Wilhelm August Fröbel (1782–1852) was a student of Pestalozzi who laid the foundation for modern education based on the recognition that children have unique needs and capabilities. He believed in "self-activity" and play as essential factors in child education. The teacher's role was not to indoctrinate but to encourage self-expression through play, both individually and in group activities. He created the concept of kindergarten.

Johann Friedrich Herbart (1776–1841) emphasized the connection between individual development and the resulting societal contribution. The five key ideas which composed his concept of individual maturation were Inner Freedom, Perfection, Benevolence, Justice, and Equity or Recompense. According to Herbart, abilities were not innate but could be instilled, so a thorough education could provide the framework for moral and intellectual development. In order to develop a child to lead to a consciousness of social responsibility, Herbart advocated that teachers utilize a methodology with five formal steps: "Using this structure a teacher prepared a topic of interest to the children, presented that topic, and questioned them inductively, so that they reached new knowledge based on what they had already known, looked back, and deductively summed up the lesson's achievements, then related them to moral precepts for daily living".

John Melchior Bosco (1815–1888) was concerned about the education of street children who had left their villages to find work in the rapidly industrialized city of Turin, Italy. Exploited as cheap labor or imprisoned for unruly behavior, Bosco saw the need of creating a space where they would feel at home. He called it an 'Oratory' where they could play, learn, share friendships, express themselves, develop their creative talents and pick up skills for gainful self-employment. With those who had found work, he set up a mutual-fund society (an early version of the Grameen Bank) to teach them the benefits of saving and self-reliance. The principles underlying his educational method that won over the hearts and minds of thousands of youth who flocked to his oratory were: 'be reasonable', 'be kind', 'believe' and 'be generous in service'. Today his method of education is practiced in nearly 3000 institutions set up around the world by the members of the Salesian Society he founded in 1873.

While studying for his doctorate in Göttingen in 1882–1883, Cecil Reddie was greatly impressed by the progressive educational theories being applied there. Reddie founded Abbotsholme School in Derbyshire, England in 1889. Its curriculum enacted the ideas of progressive education. Reddie rejected rote learning, classical languages and corporal punishment. He combined studies in modern languages and the sciences and arts with a program of physical exercise, manual labour, recreation, crafts and arts. Abbotsholme was imitated throughout Europe and was particularly influential in Germany. He often engaged foreign teachers, who learned its practices, before returning home to start their own schools. Hermann Lietz an Abbotsholme teacher founded five schools (Landerziehungsheime für Jungen) on Abbotsholme's principles. Other people he influenced included Kurt Hahn, Adolphe Ferrière and Edmond Demolins. His ideas also reached Japan, where it turned into "Taisho-era Free Education Movement" (Taisho Jiyu Kyoiku Undo)

In the United States the "Progressive Education Movement", starting in the 1880s and lasting for sixty years, helped boost American public schools from a budding idea to the regular norm. John Dewey, a principal figure in this movement from the 1880s to 1904, set the tone for educational philosophy as well as concrete school reforms. His thinking had been influenced by the ideas of Fröbel and Herbart. His reactions to the prevailing theories and practices in education, corrections made to these philosophies, and recommendations to teachers and administrators to embrace "the new education", provide a vital account of the history of the development of educational thinking in the late nineteenth and early twentieth centuries. Dewey placed so-called pragmatism above moral absolutes and helped give rise to situational ethics. Beginning in 1897 John Dewey published a summary of his theory on progressive education in School Journal. His theoretical standpoints are divided into five sections outlined below.

Education according to Dewey is the "participation of the individual in the social consciousness of the race" (Dewey, 1897, para. 1). As such, education should take into account that the student is a social being. The process begins at birth with the child unconsciously gaining knowledge and gradually developing their knowledge to share and partake in society.

The educational process has two sides, the psychological and the sociological, with the psychological forming the basis. (Dewey, 1897). A child's own instincts will help develop the material that is presented to them. These instincts also form the basis of their knowledge with everything building upon it. This forms the basis of Dewey's assumption that one cannot learn without motivation.

Instruction must focus on the child as a whole for you can never be sure as to where society may end or where that student will be needed or will take them.

"Education fails because it neglects this fundamental principle of the school as a form of community life. It conceives the school as a place where certain information is to be given, where certain lessons are to be learned, or where certain habits are to be formed" (Dewey, 1897, para. 17) Dewey felt that as education is a social construct, it is, therefore, a part of society and should reflect the community.

"Education is the process of living and is not meant to be the preparation of future living", (Dewey, 1897), so the school must represent the present life. As such, parts of the student's home life (such as moral and ethical education) should take part in the schooling process. The teacher is a part of this, not as an authoritative figure, but as a member of the community who is there to assist the student.

According to Dewey, the curriculum in the schools should reflect that of society. The center of the school curriculum should reflect the development of humans in society. The study of the core subjects (language, science, history) should be coupled with the study of cooking, sewing, and manual training. Furthermore, he feels that "progress is not in the succession of studies but in the development of new attitudes towards, and new interests in, experience" (Dewey, 1897, para. 38)

The method is focused on the child's powers and interests. If the child is thrown into a passive role as a student, absorbing information, the result is a waste of the child's education. (Dewey, 1897). Information presented to the student will be transformed into new forms, images, and symbols by the student so that they fit with their development and interests. The development of this is natural. To repress this process and attempt to "substitute the adult for the child" (Dewey, 1897, para. 52) would weaken the intellectual curiosity of the child.

For Dewey, education, which regulates "the process of coming to share in the social consciousness," is the "only sure" method of ensuring social progress and reform (Dewey, 1897, para. 60). In this respect, Dewey foreshadows Social Reconstructionism, whereby schools are a means to reconstruct society. As schools become a means for social reconstruction, they must be given the proper equipment to perform this task and guide their students.

The American teacher Helen Parkhurst (1886–1973) developed the Dalton Plan at the beginning of the twentieth century with the goal of reforming the then current pedagogy and classroom management. She wanted to break the teacher-centered lockstep teaching. During her first experiment, which she implemented in a small elementary school as a young teacher in 1904, she noticed that when students are given freedom for self-direction and self-pacing and to help one another, their motivation increases considerably and they learn more. In a later experiment in 1911 and 1912, Parkhurst re-organized the education in a large school for nine- to fourteen-year-olds. Instead of each grade, each subject was appointed its own teacher and its own classroom. The subject teachers made assignments: they converted the subject matter for each grade into learning assignments. In this way, learning became the students’ own work; they could carry out their work independently, work at their own pace and plan their work themselves. The classroom turned into a laboratory, a place where students are working, furnished and equipped as work spaces, tailored to meet the requirements of specific subjects. Useful and attractive learning materials, instruments and reference books were put within the students’ reach. The benches were replaced by large tables to facilitate co-operation and group instruction. This second experiment formed the basis for the next experiments, those in Dalton and New York, from 1919 onwards. The only addition was the use of graphs, charts enabling students to keep track of their own progress in each subject.

In the nineteen-twenties and nineteen-thirties, Dalton education spread throughout the world. There is no certainty regarding the exact numbers of Dalton schools, but there was Dalton education in America, Australia, England, Germany, the Netherlands, the Soviet Union, India, China and Japan. Particularly in the Netherlands, China and Japan, Dalton education has remained in existence. In recent years there has been a revival of international interest, particularly in England, Germany, the Czech Republic and Slovakia.

Rudolf Steiner (1869–1925) first described the principles of what was to become Waldorf education in 1907. He established a series of schools based on these principles beginning in 1919. The focus of the education is on creating a developmentally appropriate curriculum that holistically integrates practical, artistic, social, and academic experiences. There are more than a thousand schools and many more early childhood centers worldwide; it has also become a popular form of homeschooling.

Maria Montessori (1870–1952) began to develop her philosophy and methods in 1897. She based her work on her observations of children and experimentation with the environment, materials, and lessons available to them. She frequently referred to her work as "scientific pedagogy", arguing for the need to go beyond observation and measurement of students, to developing new methods to transform them. Although Montessori education spread to the United States in 1911 there were conflicts with the American educational establishment and was opposed by William Heard Kilpatrick. However Montessori education returned to the United States in 1960 and has since spread to thousands of schools there.

In July 1906, Ernest Thompson Seton sent Robert Baden-Powell a copy of his book "The Birchbark Roll of the Woodcraft Indians". Seton was a British-born Canadian-American living in the United States. They shared ideas about youth training programs. In 1907 Baden-Powell wrote a draft called "Boy Patrols". In the same year, to test his ideas, he gathered 21 boys of mixed social backgrounds and held a week-long camp in August on Brownsea Island in England. His organizational method, now known as the Patrol System and a key part of Scouting training, allowed the boys to organize themselves into small groups with an elected patrol leader. Baden Powell then wrote "Scouting for Boys" (London, 1908). The Brownsea camp and the publication of "Scouting for Boys" are generally regarded as the start of the Scout movement which spread throughout the world. Baden-Powell and his sister Agnes Baden-Powell introduced the Girl Guides in 1910.

Traditional education uses extrinsic motivation, such as grades and prizes. Progressive education is more likely to use intrinsic motivation, basing activities on the interests of the child. Praise may be discouraged as a motivator.

21st century skills are a series of higher-order skills, abilities, and learning dispositions that have been identified as being required for success in the rapidly changing, digital society and workplaces. Many of these skills are also defining qualities of "progressive education" as well as being associated with deeper learning, which is based on mastering skills such as analytic reasoning, complex problem solving, and teamwork. These skills differ from traditional academic skills in that they are not primarily content knowledge-based.

Edmond Demolins was inspired by Abbotsholme and Bedales to found the École des Roches in Verneuil-sur-Avre in 1899. Paul Robin implemented progressive principles at the orphanage at Cempuis from 1880 to 1894. This was the first French mixed school, and a scandal at that time. Sebastien Faure in 1904 created a libertarian school 'La Ruche' (the Hive).

Hermann Lietz founded three Landerziehungsheime (country boarding schools) in 1904 based on Reddie's model for boys of different ages. Lietz eventually succeeded in establishing five more Landerziehungsheime. Edith and Paul Geheeb founded Odenwaldschule in Heppenheim in the Odenwald in 1910 using their concept of progressive education, which integrated the work of the head and hand.

Janusz Korczak was one notable follower and developer of Pestalozzi's ideas. He wrote
"The names of Pestalozzi, Froebel and Spencer shine with no less brilliance than the names of the greatest inventors of the twentieth century. For they discovered more than the unknown forces of nature; they discovered the unknown half of humanity: children." His Orphan's Home in Warsaw became a model institution and exerted influence on the educational process in other orphanages of the same type.

In Spain, the Escuela Moderna was founded in 1901 by Francisco Ferrer, a Catalan educator and anarchist. He had been influenced by Cecil Reddie. The Modern Schools, also called 'Ferrer Schools', that were founded in the United States, were based on Escuela Moderna. As in Spain the schools were intended to educate the working-classes from a secular, class-conscious perspective. The Modern Schools imparted day-time academic classes for children, and night-time continuing-education lectures for adults.

In Sweden, an early proponent of progressive education was Alva Myrdal, who with her husband Gunnar co-wrote "Kris i befolkningsfrågan" (1934), a most influential program for the social-democratic hegemony (1932–1976) popularly known as "Folkhemmet". School reforms went through government reports in the 1940s and trials in the 1950s, resulting in the introduction in 1962 of public comprehensive schools ("grundskola") instead of the previously separated parallel schools for theoretical and non-theoretical education.

The ideas from Reddie's Abbotsholme spread to schools such as Bedales School (1893), King Alfred School, London (1898) and St Christopher School, Letchworth (1915), as well as all the Friends' schools, Steiner Waldorf schools and those belonging to the Round Square Conference. The King Alfred School was radical for its time in that it provided a secular education and that boys and girls were educated together. Alexander Sutherland Neill believed children should achieve self-determination and should be encouraged to think critically rather than blindly obeying. He implemented his ideas with the founding of Summerhill School in 1921. Neill believed that children learn better when they are not compelled to attend lessons. The school was also managed democratically, with regular meetings to determine school rules. Pupils had equal voting rights with school staff.

Fröbel's student Margarethe Schurz founded the first kindergarten in the United States at Watertown, Wisconsin in 1856, and she also inspired Elizabeth Peabody, who went on to found the first English-speaking kindergarten in the United States – the language at Schurz's kindergarten had been German, to serve an immigrant community – in Boston in 1860. This paved the way for the concept's spread in the USA. The German émigré Adolph Douai had also founded a kindergarten in Boston in 1859, but was obliged to close it after only a year. By 1866, however, he was founding others in New York City.

William Heard Kilpatrick (1871–1965) was a pupil of Dewey and one of the most effective practitioners of the concept as well as the more adept at proliferating the progressive education movement and spreading word of the works of Dewey. He is especially well known for his "project method of teaching". This developed the progressive education notion that students were to be engaged and taught so that their knowledge may be directed to society for a socially useful need. Like Dewey he also felt that students should be actively engaged in their learning rather than actively disengaged with the simple reading and regurgitation of material.

The most famous early practitioner of progressive education was Francis Parker; its best-known spokesperson was the philosopher John Dewey. In 1875 Francis Parker became superintendent of schools in Quincy, Massachusetts after spending two years in Germany studying emerging educational trends on the continent. Parker was opposed to rote learning, believing that there was no value in knowledge without understanding. He argued instead schools should encourage and respect the child's creativity. Parker's Quincy System called for child-centered and experience-based learning. He replaced the traditional curriculum with integrated learning units based on core themes related to the knowledge of different disciplines. He replaced traditional readers, spellers and grammar books with children's own writing, literature, and teacher prepared materials. In 1883 Parker left Massachusetts to become Principal of the Cook County Normal School in Chicago, a school that also served to train teachers in Parker's methods. In 1894 Parker's Talks on Pedagogics, which drew heavily on the thinking of Fröbel, Pestalozzi and Herbart, became one of the first American writings on education to gain international fame.

That same year, philosopher John Dewey moved from the University of Michigan to the newly established University of Chicago where he became chair of the department of philosophy, psychology and education. He and his wife enrolled their children in Parker's school before founding their own school two years later.

Whereas Parker started with practice and then moved to theory, Dewey began with hypotheses and then devised methods and curricula to test them. By the time Dewey moved to Chicago at the age of thirty-five, he had already published two books on psychology and applied psychology. He had become dissatisfied with philosophy as pure speculation and was seeking ways to make philosophy directly relevant to practical issues. Moving away from an early interest in Hegel, Dewey proceeded to reject all forms of dualism and dichotomy in favor of a philosophy of experience as a series of unified wholes in which everything can be ultimately related.

In 1896, John Dewey opened what he called the laboratory school to test his theories and their sociological implications. With Dewey as the director and his wife as principal, the University of Chicago Laboratory school, was dedicated "to discover in administration, selection of subject-matter, methods of learning, teaching, and discipline, how a school could become a cooperative community while developing in individuals their own capacities and satisfy their own needs." (Cremin, 136) For Dewey the two key goals of developing a cooperative community and developing individuals' own capacities were not at odds; they were necessary to each other. This unity of purpose lies at the heart of the progressive education philosophy. In 1912, Dewey sent out students of his philosophy to found The Park School of Buffalo and The Park School of Baltimore to put it into practice. These schools operate to this day within a similar progressive approach.

At Columbia, Dewey worked with other educators such as Charles Eliot and Abraham Flexner to help bring progressivism into the mainstream of American education. In 1917 Columbia established the Lincoln School of Teachers College "as a laboratory for the working out of an elementary and secondary curriculum which shall eliminate obsolete material and endeavor to work up in usable form material adapted to the needs of modern living." (Cremin, 282) Based on Flexner's demand that the modern curriculum "include nothing for which an affirmative case can not be made out" (Cremin, 281) the new school organized its activities around four fundamental fields: science, industry, aesthetics and civics. The Lincoln School built its curriculum around "units of work" that reorganized traditional subject matter into forms embracing the development of children and the changing needs of adult life. The first and second grades carried on a study of community life in which they actually built a city. A third grade project growing out of the day-to-day life of the nearby Hudson River became one of the most celebrated units of the school, a unit on boats, which under the guidance of its legendary teacher Miss Curtis, became an entrée into history, geography, reading, writing, arithmetic, science, art and literature. Each of the units was broadly enough conceived so that different children could concentrate on different aspects depending on their own interests and needs. Each of the units called for widely diverse student activities, and each sought to deal in depth with some critical aspect of contemporary civilization. Finally each unit engaged children working together cooperatively and also provided opportunities for individual research and exploration.

In 1924, Agnes de Lima, the lead writer on education for "The New Republic" and "The Nation", published a collection of her articles on progressive education as a book, titled "Our Enemy the Child".

In 1918 The National Education Association, representing superintendents and administrators in smaller districts across the country, issued its report "Cardinal Principles of Secondary Education." It emphasized the education of students in terms of health, a command of fundamental processes, worthy home membership, vocation, citizenship, worthy use of leisure, and ethical character. They Emphasized life adjustment and reflected the social efficiency model of progressive education.

From 1919 to 1955 the Progressive Education Association founded by Stanwood Cobb and others worked to promote a more student-centered approach to education. During the Great Depression the organization conducted the Eight-Year Study, evaluating the effects of progressive programs. More than 1500 students over four years were compared to an equal number of carefully matched students at conventional schools. When they reached college, the experimental students were found to equal or surpass traditionally educated students on all outcomes: grades, extracurricular participation, dropout rates, intellectual curiosity, and resourcefulness. Moreover, the study found that the more the school departed from the traditional college preparatory program, the better was the record of the graduates. (Kohn, Schools, 232)

By mid-century many public school programs had also adopted elements of progressive curriculum. At mid-century Dewey believed that progressive education had "not really penetrated and permeated the foundations of the educational institution."(Kohn, Schools, 6,7) As the influence of progressive pedagogy grew broader and more diffuse, practitioners began to vary their application of progressive principles. As varying interpretations and practices made evaluation of progressive reforms more difficult to assess, critics began to propose alternative approaches.

The seeds of the debate over progressive education can be seen in the differences of Parker and Dewey. These have to do with how much and by whom curriculum should be worked out from grade to grade, how much the child's emerging interests should determine classroom activities, the importance of child-centered vs. societal–centered learning, the relationship of community building to individual growth, and especially the relationship between emotion, thought and experience.

In 1955 the publication of Rudolf Flesch's "Why Johnny Can't Read" leveled criticism of reading programs at the progressive emphasis on reading in context. The conservative McCarthy era raised questions about the liberal ideas at the roots of the progressive reforms. The launching of Sputnik in 1957 at the height of the cold war gave rise to a number of intellectually competitive approaches to disciplinary knowledge, such as BSCS biology PSSC physics, led by university professors such as Jerome Bruner and Jerrold Zacharias.

Interestingly, some of the cold war reforms incorporated elements of progressivism. For example, the work of Zacharias and Bruner was based in the developmental psychology of Jean Piaget and incorporated many of Dewey's ideas of experiential education. Bruner's analysis of developmental psychology became the core of a pedagogical movement known as constructivism, which argues that the child is an active participant in making meaning and must be engaged in the progress of education for learning to be effective. This psychological approach has deep connections to the work of both Parker and Dewey and led to a resurgence of their ideas in second half of the century.

In 1965, President Johnson inaugurated the Great Society and the Elementary and Secondary Education Act suffused public school programs with funds for sweeping education reforms. At the same time the influx of federal funding also gave rise to demands for accountability and the behavioral objectives approach of Robert F. Mager and others foreshadowed the No Child Left Behind Act passed in 2002. Against these critics eloquent spokespersons stepped forward in defense of the progressive tradition. The Open Classroom movement, led by Herb Kohl and George Dennison, recalled many of Parker's child centered reforms.

The late 1960s and early 1970s saw a rise and decline in the number of progressive schools. There were several reasons for the decline:


Progressive education has been viewed as an alternative to the test-oriented instruction legislated by the No Child Left Behind educational funding act. Alfie Kohn has been an outspoken critic of the No Child Left Behind Act and a passionate defender of the progressive tradition.

Taxpayer revolts, leading to cuts in funding for public education in many states, have led to the founding of an unprecedented number of independent schools, many of which have progressive philosophies. The charter school movement has also spawned an increase in progressive programs. Most recently, public outcry against No Child Left Behind testing and teaching to the test has brought progressive education again into the limelight. Despite the variations that still exist among the progressive programs throughout the country, most progressive schools today are vitalized by these common practices:


Rabindranath Tagore (1861–1941) was one of the most effective practitioners of the concept of progressive education. He expanded Santiniketan, which is a small town near Bolpur in the Birbhum district of West Bengal, India, approximately 160 km north of Kolkata. He de-emphasized textbook learning in favor of varied learning resources from nature. The emphasis here was on self-motivation rather than on discipline, and on fostering intellectual curiosity rather than competitive excellence. There were courses on a great variety of cultures, and study programs devoted to China, Japan, and the Middle East. He was of the view that education should be a "joyous exercise of our inventive and constructive energies that help us to build up character."

Seikatsu tsuzurikata is a grassroots movement in Japan that has many parallels to the progressive education movement, but it developed completely independently, beginning in
the late 1920s. The Japanese progressive educational movement was one of the stepping stones to the modernization of Japan and it has resonated down to the present.





</doc>
<doc id="10006" url="https://en.wikipedia.org/wiki?curid=10006" title="Electronic musical instrument">
Electronic musical instrument

An electronic musical instrument is a musical instrument that produces sound using electronic circuitry. Such an instrument sounds by outputting an electrical, electronic or digital audio signal that ultimately is plugged into a power amplifier which drives a loudspeaker, creating the sound heard by the performer and listener.

An electronic instrument might include a user interface for controlling its sound, often by adjusting the pitch, frequency, or duration of each note. A common user interface is the musical keyboard, which functions similarly to the keyboard on an acoustic piano, except that with an electronic keyboard, the keyboard itself does not make any sound. An electronic keyboard sends a signal to a synth module, computer or other electronic or digital sound generator, which then creates a sound. However, it is increasingly common to separate user interface and sound-generating functions into a music controller (input device) and a music synthesizer, respectively, with the two devices communicating through a musical performance description language such as MIDI or Open Sound Control.

All electronic musical instruments can be viewed as a subset of audio signal processing applications. Simple electronic musical instruments are sometimes called sound effects; the border between sound effects and actual musical instruments is often unclear.

In the 2010s, electronic musical instruments are now widely used in most styles of music. In popular music styles such as electronic dance music, almost all of the instrument sounds used in recordings are electronic instruments (e.g., bass synth, synthesizer, drum machine). Development of new electronic musical instruments, controllers, and synthesizers continues to be a highly active and interdisciplinary field of research. Specialized conferences, notably the International Conference on New Interfaces for Musical Expression, have organized to report cutting-edge work, as well as to provide a showcase for artists who perform or create music with new electronic music instruments, controllers, and synthesizers.

In the 18th-century, musicians and composers adapted a number of acoustic instruments to exploit the novelty of electricity. Thus, in the broadest sense, the first electrified musical instrument was the Denis d'or keyboard, dating from 1753, followed shortly by the clavecin électrique by the Frenchman Jean-Baptiste de Laborde in 1761. The Denis d'or consisted of a keyboard instrument of over 700 strings, electrified temporarily to enhance sonic qualities. The clavecin électrique was a keyboard instrument with plectra (picks) activated electrically. However, neither instrument used electricity as a sound-source.

The first electric synthesizer was invented in 1876 by Elisha Gray. The "Musical Telegraph" was a chance by-product of his telephone technology when Gray accidentally discovered that he could control sound from a self-vibrating electromagnetic circuit and so invented a basic oscillator. The Musical Telegraph used steel reeds oscillated by electromagnets and transmitted over a telephone line. Gray also built a simple loudspeaker device into later models, which consisted of a diaphragm vibrating in a magnetic field.

A significant invention, which later had a profound effect on electronic music, was the audion in 1906. This was the first thermionic valve, or vacuum tube and which led to the generation and amplification of electrical signals, radio broadcasting, and electronic computation, among other things. Other early synthesizers included the Telharmonium (1897), the Theremin (1919), Jörg Mager's Spharophon (1924) and Partiturophone, Taubmann's similar Electronde (1933), Maurice Martenot's ondes Martenot ("Martenot waves", 1928), Trautwein's Trautonium (1930). The Mellertion (1933) used a non-standard scale, Bertrand's Dynaphone could produce octaves and perfect fifths, while the Emicon was an American, keyboard-controlled instrument constructed in 1930 and the German Hellertion combined four instruments to produce chords. Three Russian instruments also appeared, Oubouhof's Croix Sonore (1934), Ivor Darreg's microtonal 'Electronic Keyboard Oboe' (1937) and the ANS synthesizer, constructed by the Russian scientist Evgeny Murzin from 1937 to 1958. Only two models of this latter were built and the only surviving example is currently stored at the Lomonosov University in Moscow. It has been used in many Russian movies—like "Solaris"—to produce unusual, "cosmic" sounds.

Hugh Le Caine, John Hanert, Raymond Scott, composer Percy Grainger (with Burnett Cross), and others built a variety of automated electronic-music controllers during the late 1940s and 1950s. In 1959 Daphne Oram produced a novel method of synthesis, her "Oramics" technique, driven by drawings on a 35 mm film strip; it was used for a number of years at the BBC Radiophonic Workshop. This workshop was also responsible for the theme to the TV series Doctor Who, a piece, largely created by Delia Derbyshire, that more than any other ensured the popularity of electronic music in the UK.

In 1897 Thaddeus Cahill patented an instrument called the Telharmonium (or Teleharmonium, also known as the Dynamaphone). Using tonewheels to generate musical sounds as electrical signals by additive synthesis, it was capable of producing any combination of notes and overtones, at any dynamic level. This technology was later used to design the Hammond organ. Between 1901 and 1910 Cahill had three progressively larger and more complex versions made, the first weighing seven tons, the last in excess of 200 tons. Portability was managed only by rail and with the use of thirty boxcars. By 1912, public interest had waned, and Cahill's enterprise was bankrupt.

Another development, which aroused the interest of many composers, occurred in 1919-1920. In Leningrad, Leon Theremin (actually Lev Termen) built and demonstrated his Etherophone, which was later renamed the Theremin. This led to the first compositions for electronic instruments, as opposed to noisemakers and re-purposed machines. The Theremin was notable for being the first musical instrument played without touching it. In 1929, Joseph Schillinger composed "First Airphonic Suite for Theremin and Orchestra", premièred with the Cleveland Orchestra with Leon Theremin as soloist. The next year Henry Cowell commissioned Theremin to create the first electronic rhythm machine, called the Rhythmicon. Cowell wrote some compositions for it, and he and Schillinger premiered it in 1932.

The 1920s have been called the apex of the Mechanical Age and the dawning of the Electrical Age. In 1922, in Paris, Darius Milhaud began experiments with "vocal transformation by phonograph speed change." These continued until 1927. This decade brought a wealth of early electronic instruments—along with the Theremin, there is the presentation of the Ondes Martenot, which was designed to reproduce the microtonal sounds found in Hindu music, and the Trautonium.
Maurice Martenot invented the Ondes Martenot in 1928, and soon demonstrated it in Paris. Composers using the instrument ultimately include Boulez, Honegger, Jolivet, Koechlin, Messiaen, Milhaud, Tremblay, and Varèse. Radiohead guitarist and multi-instrumentalist Jonny Greenwood also uses it in his compositions and a plethora of Radiohead songs. In 1937, Messiaen wrote "Fête des belles eaux" for 6 ondes Martenot, and wrote solo parts for it in "Trois petites Liturgies de la Présence Divine" (1943–44) and the "Turangalîla-Symphonie" (1946–48/90).

The Trautonium was invented in 1928. It was based on the subharmonic scale, and the resulting sounds were often used to emulate bell or gong sounds, as in the 1950s Bayreuth productions of "Parsifal". In 1942, Richard Strauss used it for the bell- and gong-part in the Dresden première of his "Japanese Festival Music". This new class of instruments, microtonal by nature, was only adopted slowly by composers at first, but by the early 1930s there was a burst of new works incorporating these and other electronic instruments.

In 1929 Laurens Hammond established his company for the manufacture of electronic instruments. He went on to produce the Hammond organ, which was based on the principles of the Telharmonium, along with other developments including early reverberation units. The Hammond organ is an electromechanical instrument, as it used both mechanical elements and electronic parts. A Hammond organ used spinning metal tonewheels to produce different sounds. A magnetic pickup similar in design to the pickups in an electric guitar is used to transmit the pitches in the tonewheels to an amplifier and speaker enclosure. While the Hammond organ was designed to be a lower-cost alternative to a pipe organ for church music, musicians soon discovered that the Hammond was an excellent instrument for blues and jazz; indeed, an entire genre of music developed built around this instrument, known as the organ trio (typically Hammond organ, drums, and a third instrument, either saxophone or guitar).

The first commercially manufactured synthesizer was the Novachord, built by the Hammond Organ Company from 1938 to 1942, which offered 72-note polyphony using 12 oscillators driving monostable-based divide-down circuits, basic envelope control and resonant low-pass filters. The instrument featured 163 vacuum tubes and weighed 500 pounds. The instrument's use of envelope control is significant, since this is perhaps the most significant distinction between the modern synthesizer and other electronic instruments.

The most commonly used electronic instruments are synthesizers, so-called because they artificially generate sound using a variety of techniques. All early circuit-based synthesis involved the use of analogue circuitry, particularly voltage controlled amplifiers, oscillators and filters. An important technological development was the invention of the Clavivox synthesizer in 1956 by Raymond Scott with subassembly by Robert Moog. French composer and engineer Edgard Varèse created a variety of compositions using electronic horns, whistles, and tape. Most notably, he wrote "Poème électronique" for the Phillips pavilion at the Brussels World Fair in 1958.

RCA produced experimental devices to synthesize voice and music in the 1950s. The Mark II Music Synthesizer, housed at the Columbia-Princeton Electronic Music Center in New York City. Designed by Herbert Belar and Harry Olson at RCA, with contributions from Vladimir Ussachevsky and Peter Mauzey, it was installed at Columbia University in 1957. Consisting of a room-sized array of interconnected sound synthesis components, it was only capable of producing music by programming, using a paper tape sequencer punched with holes to control pitch sources and filters, similar to a mechanical player piano but capable of generating a wide variety of sounds. The vacuum tube system had to be patched to create timbres.
In the 1960s synthesizers were still usually confined to studios due to their size. They were usually modular in design, their stand-alone signal sources and processors connected with patch cords or by other means and controlled by a common controlling device. Harald Bode, Don Buchla, Hugh Le Caine, Raymond Scott and Paul Ketoff were among the first to build such instruments, in the late 1950s and early 1960s. Buchla later produced a commercial modular synthesizer, the Buchla Music Easel. Robert Moog, who had been a student of Peter Mauzey and one of the RCA Mark II engineers, created a synthesizer that could reasonably be used by musicians, designing the circuits while he was at Columbia-Princeton. The Moog synthesizer was first displayed at the Audio Engineering Society convention in 1964. It required experience to set up sounds but was smaller and more intuitive than what had come before, less like a machine and more like a musical instrument. Moog established standards for control interfacing, using a logarithmic 1-volt-per-octave for pitch control and a separate triggering signal. This standardization allowed synthesizers from different manufacturers to operate simultaneously. Pitch control was usually performed either with an organ-style keyboard or a music sequencer producing a timed series of control voltages. During the late 1960s hundreds of popular recordings used Moog synthesizers. Other early commercial synthesizer manufacturers included ARP, who also started with modular synthesizers before producing all-in-one instruments, and British firm EMS.

In 1970, Moog designed the Minimoog, a non-modular synthesizer with a built-in keyboard. The analogue circuits were interconnected with switches in a simplified arrangement called "normalization." Though less flexible than a modular design, normalization made the instrument more portable and easier to use. The Minimoog sold 12,000 units. further standardized the design of subsequent synthesizers with its integrated keyboard, pitch and modulation wheels and VCO->VCF->VCA signal flow. It has become celebrated for its "fat" sound—and its tuning problems. Miniaturized solid-state components allowed synthesizers to become self-contained, portable instruments that soon appeared in live performance and quickly became widely used in popular music and electronic art music.

Many early analog synthesizers were monophonic, producing only one tone at a time. Popular monophonic synthesizers include the Moog Minimoog. A few, such as the Moog Sonic Six, ARP Odyssey and EML 101, could produce two different pitches at a time when two keys were pressed. Polyphony (multiple simultaneous tones, which enables chords) was only obtainable with electronic organ designs at first. Popular electronic keyboards combining organ circuits with synthesizer processing included the ARP Omni and Moog's Polymoog and Opus 3.

By 1976 affordable polyphonic synthesizers began to appear, notably the Yamaha CS-50, CS-60 and CS-80, the Sequential Circuits Prophet-5 and the Oberheim Four-Voice. These remained complex, heavy and relatively costly. The recording of settings in digital memory allowed storage and recall of sounds. The first practical polyphonic synth, and the first to use a microprocessor as a controller, was the Sequential Circuits Prophet-5 introduced in late 1977. For the first time, musicians had a practical polyphonic synthesizer that could save all knob settings in computer memory and recall them at the touch of a button. The Prophet-5's design paradigm became a new standard, slowly pushing out more complex and recondite modular designs.
In 1935, another significant development was made in Germany. Allgemeine Elektrizitäts Gesellschaft (AEG) demonstrated the first commercially produced magnetic tape recorder, called the "Magnetophon". Audio tape, which had the advantage of being fairly light as well as having good audio fidelity, ultimately replaced the bulkier wire recorders.

The term "electronic music" (which first came into use during the 1930s) came to include the tape recorder as an essential element: "electronically produced sounds recorded on tape and arranged by the composer to form a musical composition" It was also indispensable to Musique concrète.

Tape also gave rise to the first, analogue, sample-playback keyboards, the Chamberlin and its more famous successor the Mellotron, an electro-mechanical, polyphonic keyboard originally developed and built in Birmingham, England in the early 1960s.

During the 1940s–1960s, Raymond Scott, an American composer of electronic music, invented various kind of music sequencers for his electric compositions. Step sequencers played rigid patterns of notes using a grid of (usually) 16 buttons, or steps, each step being 1/16 of a measure. These patterns of notes were then chained together to form longer compositions. Software sequencers were continuously utilized since the 1950s in the context of computer music, including computer-"played" music (software sequencer), computer-"composed" music (music synthesis), and computer "sound generation" (sound synthesis).

The first digital synthesizers were academic experiments in sound synthesis using digital computers. FM synthesis was developed for this purpose; as a way of generating complex sounds digitally with the smallest number of computational operations per sound sample. In 1983 Yamaha introduced the first stand-alone digital synthesizer, the DX-7. It used frequency modulation synthesis (FM synthesis), first developed by John Chowning at Stanford University during the late sixties. Chowning exclusively licensed his FM synthesis patent to Yamaha in 1975. Yamaha subsequently released their first FM synthesizers, the GS-1 and GS-2, which were costly and heavy. There followed a pair of smaller, preset versions, the CE20 and CE25 Combo Ensembles, targeted primarily at the home organ market and featuring four-octave keyboards. Yamaha's third generation of digital synthesizers was a commercial success; it consisted of the DX7 and DX9 (1983). Both models were compact, reasonably priced, and dependent on custom digital integrated circuits to produce FM tonalities. The DX7 was the first mass market all-digital synthesizer. It became indispensable to many music artists of the 1980s, and demand soon exceeded supply. The DX7 sold over 200,000 units within three years.

The DX series was not easy to program but offered a detailed, percussive sound that led to the demise of the electro-mechanical Rhodes piano, which was heavier and larger than a DX synth. Following the success of FM synthesis Yamaha signed a contract with Stanford University in 1989 to develop digital waveguide synthesis, leading to the first commercial physical modeling synthesizer, Yamaha's VL-1, in 1994. The DX-7 was affordable enough for amateurs and young bands to buy, unlike the costly synthesizers of previous generations, which were mainly used by top professionals.

The Fairlight CMI (Computer Musical Instrument), the first polyphonic digital sampler, was the harbinger of sample-based synthesizers. Designed in 1978 by Peter Vogel and Kim Ryrie and based on a dual microprocessor computer designed by Tony Furse in Sydney, Australia, the Fairlight CMI gave musicians the ability to modify volume, attack, decay, and use special effects like vibrato. Sample waveforms could be displayed on-screen and modified using a light pen. The Synclavier from New England Digital was a similar system. Jon Appleton (with Jones and Alonso) invented the Dartmouth Digital Synthesizer, later to become the New England Digital Corp's Synclavier. The Kurzweil K250, first produced in 1983, was also a successful polyphonic digital music synthesizer, noted for its ability to reproduce several instruments synchronously and having a velocity-sensitive keyboard.

An important new development was the advent of computers for the purpose of composing music, as opposed to manipulating or creating sounds. Iannis Xenakis began what is called "musique stochastique," or "stochastic music", which is a method of composing that employs mathematical probability systems. Different probability algorithms were used to create a piece under a set of parameters. Xenakis used graph paper and a ruler to aid in calculating the velocity trajectories of glissandi for his orchestral composition "Metastasis" (1953–54), but later turned to the use of computers to compose pieces like "ST/4" for string quartet and "ST/48" for orchestra (both 1962).

The impact of computers continued in 1956. Lejaren Hiller and Leonard Issacson composed "Illiac Suite" for string quartet, the first complete work of computer-assisted composition using algorithmic composition.

In 1957, Max Mathews at Bell Lab wrote MUSIC-N series, a first computer program family for generating digital audio waveforms through direct synthesis. Then Barry Vercoe wrote MUSIC 11 based on MUSIC IV-BF, a next-generation music synthesis program (later evolving into csound, which is still widely used).

In mid 80s, Miller Puckette at IRCAM developed graphic signal-processing software for 4X called Max (after Max Mathews), and later ported it to Macintosh (with Dave Zicarelli extending it for Opcode ) for real-time MIDI control, bringing algorithmic composition availability to most composers with modest computer programming background.

In 1980, a group of musicians and music merchants met to standardize an interface by which new instruments could communicate control instructions with other instruments and the prevalent microcomputer. This standard was dubbed MIDI (Musical Instrument Digital Interface). A paper was authored by Dave Smith of Sequential Circuits and proposed to the Audio Engineering Society in 1981. Then, in August 1983, the MIDI Specification 1.0 was finalized.

The advent of MIDI technology allows a single keystroke, control wheel motion, pedal movement, or command from a microcomputer to activate every device in the studio remotely and in synchrony, with each device responding according to conditions predetermined by the composer.

MIDI instruments and software made powerful control of sophisticated instruments easily affordable by many studios and individuals. Acoustic sounds became reintegrated into studios via sampling and sampled-ROM-based instruments.

The increasing power and decreasing cost of sound-generating electronics (and especially of the personal computer), combined with the standardization of the MIDI and Open Sound Control musical performance description languages, has facilitated the separation of musical instruments into music controllers and music synthesizers.

By far the most common musical controller is the musical keyboard. Other controllers include the radiodrum, Akai's EWI and Yamah's WX wind controllers, the guitar-like SynthAxe, the BodySynth, the Buchla Thunder, the Continuum Fingerboard, the Roland Octapad, various isomorphic keyboards including the Thummer, and Kaossilator Pro, and kits like I-CubeX.

The Reactable is a round translucent table with a backlit interactive display. By placing and manipulating blocks called "tangibles" on the table surface, while interacting with the visual display via finger gestures, a virtual modular synthesizer is operated, creating music or sound effects.

AudioCubes are autonomous wireless cubes powered by an internal computer system and rechargeable battery. They have internal RGB lighting, and are capable of detecting each other's location, orientation and distance. The cubes can also detect distances to the user's hands and fingers. Through interaction with the cubes, a variety of music and sound software can be operated. AudioCubes have applications in sound design, music production, DJing and live performance.

The Kaossilator and Kaossilator Pro are compact instruments where the position of a finger on the touch pad controls two note-characteristics; usually the pitch is changed with a left-right motion and the tonal property, filter or other parameter changes with an up-down motion. The touch pad can be set to different musical scales and keys. The instrument can record a repeating loop of adjustable length, set to any tempo, and new loops of sound can be layered on top of existing ones. This lends itself to electronic dance-music but is more limited for controlled sequences of notes, as the pad on a regular Kaossilator is featureless.

The Eigenharp is a large instrument resembling a bassoon, which can be interacted with through big buttons, a drum sequencer and a mouthpiece. The sound processing is done on a separate computer.

The XTH Sense is a wearable instrument that uses muscle sounds from the human body (known as mechanomyogram) to make music and sound effects. As a performer moves, the body produces muscle sounds that are captured by a chip microphone worn on arm or legs. The muscle sounds are then live sampled using a dedicated software program and a library of modular audio effects. The performer controls the live sampling parameters by weighing force, speed and articulation of the movement.

The AlphaSphere is a spherical instrument that consists of 48 tactile pads that respond to pressure as well as touch. Custom software allows the pads to be indefinitely programmed individually or by groups in terms of function, note, and pressure parameter among many other settings. The primary concept of the AlphaSphere is to increase the level of expression available to electronic musicians, by allowing for the playing style of a musical instrument.

Chiptune, chipmusic, or chip music is music written in sound formats where many of the sound textures are synthesized or sequenced in real time by a computer or video game console sound chip, sometimes including sample-based synthesis and low bit sample playback. Many chip music devices featured synthesizers in tandem with low rate sample playback.
During the late 1970s and early 1980s, DIY (Do it yourself) designs were published in hobby electronics magazines (notably the Formant modular synth, a DIY clone of the Moog system, published by Elektor) and kits were supplied by companies such as Paia in the US, and Maplin Electronics in the UK.

In 1966, Reed Ghazala discovered and began to teach math "circuit bending"—the application of the creative short circuit, a process of chance short-circuiting, creating experimental electronic instruments, exploring sonic elements mainly of timbre and with less regard to pitch or rhythm, and influenced by John Cage’s aleatoric music concept.

Much of this manipulation of circuits directly, especially to the point of destruction, was pioneered by Louis and Bebe Barron in the early 1950s, such as their work with John Cage on the "Williams Mix" and especially in the soundtrack to Forbidden Planet.

Modern circuit bending is the creative customization of the circuits within electronic devices such as low voltage, battery-powered guitar effects, children's toys and small digital synthesizers to create new musical or visual instruments and sound generators. Emphasizing spontaneity and randomness, the techniques of circuit bending have been commonly associated with noise music, though many more conventional contemporary musicians and musical groups have been known to experiment with "bent" instruments. Circuit bending usually involves dismantling the machine and adding components such as switches and potentiometers that alter the circuit. With the revived interest for analogue synthesizer circuit bending became a cheap solution for many experimental musicians to create their own individual analogue sound generators. Nowadays many schematics can be found to build noise generators such as the Atari Punk Console or the Dub Siren as well as simple modifications for children toys such as the famous Speak & Spells that are often modified by circuit benders.

The modular synthesizer is a type of synthesizer consisting of separate interchangeable modules. These are also available as kits for hobbyist DIY constructors. Many hobbyist designers also make available bare PCB boards and front panels for sale to other hobbyists.

According to a forum post in December 2010, Sixense Entertainment is working on musical control with the Sixense TrueMotion motion controller. Immersive virtual musical instruments, or immersive virtual instruments for music and sound aim to represent musical events and sound parameters in a virtual reality so that they can be perceived not only through auditory feedback but also visually in 3D and possibly through tactile as well as haptic feedback, allowing the development of novel interaction metaphors beyond manipulation such as prehension.




</doc>
<doc id="10008" url="https://en.wikipedia.org/wiki?curid=10008" title="Electrode">
Electrode

An electrode is an electrical conductor used to make contact with a nonmetallic part of a circuit (e.g. a semiconductor, an electrolyte, a vacuum or air). The word was coined by William Whewell at the request of the scientist Michael Faraday from the Greek words "elektron", meaning amber (from which the word electricity is derived), and "hodos", a way.

The electrophore, invented by Johan Wilcke, was an early version of an electrode used to study static electricity.

An electrode in an electrochemical cell is referred to as either an "anode" or a "cathode" (words that were coined by William Whewell at Faraday's request). The anode is now defined as the electrode at which electrons leave the cell and oxidation occurs (indicated by a minus symbol, "−"), and the cathode as the electrode at which electrons enter the cell and reduction occurs (indicated by a plus symbol, "+"). Each electrode may become either the anode or the cathode depending on the direction of current through the cell. A bipolar electrode is an electrode that functions as the anode of one cell and the cathode of another cell.

A primary cell is a special type of electrochemical cell in which the reaction cannot be reversed, and the identities of the anode and cathode are therefore fixed. The anode is always the negative electrode. The cell can be discharged but not recharged.

A secondary cell, for example a rechargeable battery, is a cell in which the chemical reactions are reversible. When the cell is being charged, the anode becomes the positive (+) and the cathode the negative (−) electrode. This is also the case in an electrolytic cell. When the cell is being discharged, it behaves like a primary cell, with the anode as the negative and the cathode as the positive electrode.

In a vacuum tube or a semiconductor having polarity (diodes, electrolytic capacitors) the anode is the positive (+) electrode and the cathode the negative (−). The electrons enter the device through the cathode and exit the device through the anode. Many devices have other electrodes to control operation, e.g., base, gate, control grid.

In a three-electrode cell, a counter electrode, also called an auxiliary electrode, is used only to make a connection to the electrolyte so that a current can be applied to the working electrode. The counter electrode is usually made of an inert material, such as a noble metal or graphite, to keep it from dissolving.

In arc welding, an electrode is used to conduct current through a workpiece to fuse two pieces together. Depending upon the process, the electrode is either consumable, in the case of gas metal arc welding or shielded metal arc welding, or non-consumable, such as in gas tungsten arc welding. For a direct current system, the weld rod or stick may be a cathode for a filling type weld or an anode for other welding processes. For an alternating current arc welder, the welding electrode would not be considered an anode or cathode.

For electrical systems which use alternating current, the electrodes are the connections from the circuitry to the object to be acted upon by the electric current but are not designated anode or cathode because the direction of flow of the electrons changes periodically, usually many times per second.

Electrodes are used to provide current through nonmetal objects to alter them in numerous ways and to measure conductivity for numerous purposes. Examples include:

Chemically modified electrodes are electrodes that have their surfaces chemically modified to change the electrode's physical, chemical, electrochemical, optical, electrical, and transportive properties. These electrodes are used for advanced purposes in research and investigation.


</doc>
<doc id="10011" url="https://en.wikipedia.org/wiki?curid=10011" title="Epistolary novel">
Epistolary novel

An epistolary novel is a novel written as a series of documents. The usual form is letters, although diary entries, newspaper clippings and other documents are sometimes used. Recently, electronic "documents" such as recordings and radio, blogs, and e-mails have also come into use. The word "epistolary" is derived from Latin from the Greek word ἐπιστολή "epistolē", meaning a letter (see epistle).

The epistolary form can add greater realism to a story, because it mimics the workings of real life. It is thus able to demonstrate differing points of view without recourse to the device of an omniscient narrator.

There are two theories on the genesis of the epistolary novel. The first claims that the genre originated from novels with inserted letters, in which the portion containing the third person narrative in between the letters was gradually reduced. The other theory claims that the epistolary novel arose from miscellanies of letters and poetry: some of the letters were tied together into a (mostly amorous) plot. Both claims have some validity. The first truly epistolary novel, the Spanish "Prison of Love" ("Cárcel de amor") (c.1485) by Diego de San Pedro, belongs to a tradition of novels in which a large number of inserted letters already dominated the narrative. Other well-known examples of early epistolary novels are closely related to the tradition of letter-books and miscellanies of letters. Within the successive editions of Edmé Boursault's "Letters of Respect, Gratitude and Love" ("Lettres de respect, d'obligation et d'amour") (1669), a group of letters written to a girl named Babet were expanded and became more and more distinct from the other letters, until it formed a small epistolary novel entitled "Letters to Babet" ("Lettres à Babet"). The immensely famous "Letters of a Portuguese Nun" ("Lettres portugaises") (1669) generally attributed to Gabriel-Joseph de La Vergne, comte de Guilleragues, though a small minority still regard Marianna Alcoforado as the author, is claimed to be intended to be part of a miscellany of Guilleragues prose and poetry.
The founder of the epistolary novel in English is said by many to be James Howell (1594–1666) with "Familiar Letters" (1645–50), who writes of prison, foreign adventure, and the love of women.

The first novel to expose the complex play that the genre allows was Aphra Behn's "Love-Letters Between a Nobleman and His Sister", which appeared in three volumes in 1684, 1685, and 1687. The novel shows the genre's results of changing perspectives: individual points were presented by the individual characters, and the central voice of the author and moral evaluation disappeared (at least in the first volume; her further volumes introduced a narrator). Behn furthermore explored a realm of intrigue with letters that fall into the wrong hands, faked letters, letters withheld by protagonists, and even more complex interaction.

The epistolary novel as a genre became popular in the 18th century in the works of such authors as Samuel Richardson, with his immensely successful novels "Pamela" (1740) and "Clarissa" (1749). In France, there was "Lettres persanes" (1721) by Montesquieu, followed by "Julie, ou la nouvelle Héloïse" (1761) by Jean-Jacques Rousseau, and Laclos' "Les Liaisons dangereuses" (1782), which used the epistolary form to great dramatic effect, because the sequence of events was not always related directly or explicitly. In Germany, there was Johann Wolfgang von Goethe's "Die Leiden des jungen Werthers" (1774) ("The Sorrows of Young Werther") and Friedrich Hölderlin's "Hyperion". The first North American novel, "The History of Emily Montague" (1769) by Frances Brooke was written in epistolary form.

Starting in the 18th century, the epistolary form was subject to much ridicule, resulting in a number of savage burlesques. The most notable example of these was Henry Fielding's "Shamela" (1741), written as a parody of "Pamela". In it, the female narrator can be found wielding a pen and scribbling her diary entries under the most dramatic and unlikely of circumstances. Oliver Goldsmith used the form to satirical effect in "The Citizen of the World", subtitled "Letters from a Chinese Philosopher Residing in London to his Friends in the East" (1760–61). So did the diarist Fanny Burney in a successful comic first novel, "Evelina" (1788).

The epistolary novel slowly fell out of use in the late 18th century. Although Jane Austen tried her hand at the epistolary in juvenile writings and her novella "Lady Susan" (1794), she abandoned this structure for her later work. It is thought that her lost novel "First Impressions", which was redrafted to become "Pride and Prejudice", may have been epistolary: "Pride and Prejudice" contains an unusual number of letters quoted in full and some play a critical role in the plot.

The epistolary form nonetheless saw continued use, surviving in exceptions or in fragments in nineteenth-century novels. In Honoré de Balzac's novel "Letters of Two Brides", two women who became friends during their education at a convent correspond over a 17-year period, exchanging letters describing their lives. Mary Shelley employs the epistolary form in her novel "Frankenstein" (1818). Shelley uses the letters as one of a variety of framing devices, as the story is presented through the letters of a sea captain and scientific explorer attempting to reach the north pole who encounters Victor Frankenstein and records the dying man's narrative and confessions. Published in 1848, Anne Brontë's novel "The Tenant of Wildfell Hall" is framed as a retrospective letter from one of the main heroes to his friend and brother-in-law with the diary of the eponymous tenant inside it. In the late 19th century, Bram Stoker released one of the most widely recognized and successful novels in the epistolary form to date, "Dracula". Printed in 1897, the novel is compiled entirely of letters, diary entries, newspaper clippings, telegrams, doctor's notes, ship's logs, and the like, which Stoker adroitly employs to balance believability and dramatic tension.

There are three types of epistolary novels: monologic (giving the letters of only one character, like "Letters of a Portuguese Nun" and "The Sorrows of Young Werther"), dialogic (giving the letters of two characters, like Mme Marie Jeanne Riccoboni's "Letters of Fanni Butlerd" (1757), and polylogic (with three or more letter-writing characters, such as in Bram Stoker's "Dracula"). In addition, a crucial element in polylogic epistolary novels like "Clarissa", and "Dangerous Liaisons" is the dramatic device of 'discrepant awareness': the simultaneous but separate correspondences of the heroines and the villains creating dramatic tension.

An important strategic device in the epistolary novel for creating the impression of authenticity of the letters is the fictional editor.

Epistolary novels have made several memorable appearances in more recent literature:






</doc>
<doc id="10013" url="https://en.wikipedia.org/wiki?curid=10013" title="Evidence-based medicine">
Evidence-based medicine

Evidence-based medicine (EBM) is an approach to medical practice intended to optimize decision-making by emphasizing the use of evidence from well-designed and well-conducted research. Although all medicine based on science has some degree of empirical support, EBM goes further, classifying evidence by its epistemologic strength and requiring that only the strongest types (coming from meta-analyses, systematic reviews, and randomized controlled trials) can yield strong recommendations; weaker types (such as from case-control studies) can yield only weak recommendations. The term was originally used to describe an approach to teaching the practice of medicine and improving decisions by individual physicians about individual patients. Use of the term rapidly expanded to include a previously described approach that emphasized the use of evidence in the design of guidelines and policies that apply to groups of patients and populations ("evidence-based practice policies"). It has subsequently spread to describe an approach to decision-making that is used at virtually every level of health care as well as other fields (evidence-based practice).

Whether applied to medical education, decisions about individuals, guidelines and policies applied to populations, or administration of health services in general, evidence-based medicine advocates that to the greatest extent possible, decisions and policies should be based on evidence, not just the beliefs of practitioners, experts, or administrators. It thus tries to assure that a clinician's opinion, which may be limited by knowledge gaps or biases, is supplemented with all available knowledge from the scientific literature so that best practice can be determined and applied. It promotes the use of formal, explicit methods to analyze evidence and makes it available to decision makers. It promotes programs to teach the methods to medical students, practitioners, and policy makers.

In its broadest form, evidence-based medicine is the application of the scientific method into healthcare decision-making. Medicine has a long tradition of both basic and clinical research that dates back at least to Avicenna and more recently to protestant reformation exegesis of the 17th and 18th centuries. An early critique of statistical methods in medicine was published in 1835.

However, until recently, the process by which research results were incorporated in medical decisions was highly subjective. Called "clinical judgment" and "the art of medicine", the traditional approach to making decisions about individual patients depended on having each individual physician determine what research evidence, if any, to consider, and how to merge that evidence with personal beliefs and other factors. In the case of decisions which applied to groups of patients or populations, the guidelines and policies would usually be developed by committees of experts, but there was no formal process for determining the extent to which research evidence should be considered or how it should be merged with the beliefs of the committee members. There was an implicit assumption that decision makers and policy makers would incorporate evidence in their thinking appropriately, based on their education, experience, and ongoing study of the applicable literature.

Beginning in the late 1960s, several flaws became apparent in the traditional approach to medical decision-making. Alvan Feinstein's publication of "Clinical Judgment" in 1967 focused attention on the role of clinical reasoning and identified biases that can affect it. In 1972, Archie Cochrane published "Effectiveness and Efficiency", which described the lack of controlled trials supporting many practices that had previously been assumed to be effective. In 1973, John Wennberg began to document wide variations in how physicians practiced. Through the 1980s, David M. Eddy described errors in clinical reasoning and gaps in evidence. In the mid 1980s, Alvin Feinstein, David Sackett and others published textbooks on clinical epidemiology, which translated epidemiological methods to physician decision making. Toward the end of the 1980s, a group at RAND showed that large proportions of procedures performed by physicians were considered inappropriate even by the standards of their own experts. These areas of research increased awareness of the weaknesses in medical decision making at the level of both individual patients and populations, and paved the way for the introduction of evidence-based methods.

The term "evidence-based medicine", as it is currently used, has two main tributaries. Chronologically, the first is the insistence on explicit evaluation of evidence of effectiveness when issuing clinical practice guidelines and other population-level policies. The second is the introduction of epidemiological methods into medical education and individual patient-level decision-making.

The term "evidence-based" was first used by David M. Eddy in the course of his work on population-level policies such as clinical practice guidelines and insurance coverage of new technologies. He first began to use the term "evidence-based" in 1987 in workshops and a manual commissioned by the Council of Medical Specialty Societies to teach formal methods for designing clinical practice guidelines. The manual was widely available in unpublished form in the late 1980s and eventually published by the American College of Medicine. Eddy first published the term "evidence-based" in March, 1990 in an article in the "Journal of the American Medical Association" that laid out the principles of evidence-based guidelines and population-level policies, which Eddy described as "explicitly describing the available evidence that pertains to a policy and tying the policy to evidence. Consciously anchoring a policy, not to current practices or the beliefs of experts, but to experimental evidence. The policy must be consistent with and supported by evidence. The pertinent evidence must be identified, described, and analyzed. The policymakers must determine whether the policy is justified by the evidence. A rationale must be written." He discussed "evidence-based" policies in several other papers published in "JAMA" in the spring of 1990. Those papers were part of a series of 28 published in "JAMA" between 1990 and 1997 on formal methods for designing population-level guidelines and policies.

The term "evidence-based medicine" was introduced slightly later, in the context of medical education. This branch of evidence-based medicine has its roots in clinical epidemiology. In the autumn of 1990, Gordon Guyatt used it in an unpublished description of a program at McMaster University for prospective or new medical students. Guyatt and others first published the term two years later (1992) to describe a new approach to teaching the practice of medicine.

In 1996, David Sackett and colleagues clarified the definition of this tributary of evidence-based medicine as "the conscientious, explicit and judicious use of current best evidence in making decisions about the care of individual patients. ... [It] means integrating individual clinical expertise with the best available external clinical evidence from systematic research." This branch of evidence-based medicine aims to make individual decision making more structured and objective by better reflecting the evidence from research. Population-based data are applied to the care of an individual patient, while respecting the fact that practitioners have clinical expertise reflected in effective and efficient diagnosis and thoughtful identification and compassionate use of individual patients' predicaments, rights, and preferences.

This tributary of evidence-based medicine had its foundations in clinical epidemiology, a discipline that teaches health care workers how to apply clinical and epidemiological research studies to their practices. Between 1993 and 2000, the Evidence-based Medicine Working Group at McMaster University published the methods to a broad physician audience in a series of 25 "Users’ Guides to the Medical Literature" in "JAMA". In 1995 Rosenberg and Donald defined individual level evidence-based medicine as "the process of finding, appraising, and using contemporaneous research findings as the basis for medical decisions." In 2010, Greenhalgh used a definition that emphasized quantitative methods: "the use of mathematical estimates of the risk of benefit and harm, derived from high-quality research on population samples, to inform clinical decision-making in the diagnosis, investigation or management of individual patients." Many other definitions have been offered for individual level evidence-based medicine, but the one by Sackett and colleagues is the most commonly cited.

The two original definitions highlight important differences in how evidence-based medicine is applied to populations versus individuals. When designing guidelines applied to large groups of people in settings where there is relatively little opportunity for modification by individual physicians, evidence-based policymaking stresses that there should be good evidence to document a test´s or treatment´s effectiveness. In the setting of individual decision-making, practitioners can be given greater latitude in how they interpret research and combine it with their clinical judgment. in 2005 Eddy offered an umbrella definition for the two branches of EBM: "Evidence-based medicine is a set of principles and methods intended to ensure that to the greatest extent possible, medical decisions, guidelines, and other types of policies are based on and consistent with good evidence of effectiveness and benefit."

Both branches of evidence-based medicine spread rapidly. On the evidence-based guidelines and policies side, explicit insistence on evidence of effectiveness was introduced by the American Cancer Society in 1980. The U.S. Preventive Services Task Force (USPSTF) began issuing guidelines for preventive interventions based on evidence-based principles in 1984. In 1985, the Blue Cross Blue Shield Association applied strict evidence-based criteria for covering new technologies. Beginning in 1987, specialty societies such as the American College of Physicians, and voluntary health organizations such as the American Heart Association, wrote many evidence-based guidelines. In 1991, Kaiser Permanente, a managed care organization in the US, began an evidence-based guidelines program. In 1991, Richard Smith wrote an editorial in the "British Medical Journal" and introduced the ideas of evidence-based policies in the UK. In 1993, the Cochrane Collaboration created a network of 13 countries to produce of systematic reviews and guidelines. In 1997, the US Agency for Healthcare Research and Quality (then known as the Agency for Health Care Policy and Research, or AHCPR) established Evidence-based Practice Centers (EPCs) to produce evidence reports and technology assessments to support the development of guidelines. In the same year, a National Guideline Clearinghouse that followed the principles of evidence-based policies was created by AHRQ, the AMA, and the American Association of Health Plans (now America's Health Insurance Plans). In 1999, the National Institute for Clinical Excellence (NICE) was created in the UK. A central idea of this branch of evidence-based medicine is that evidence should be classified according to the rigor of its experimental design, and the strength of a recommendation should depend on the strength of the evidence.

On the medical education side, programs to teach evidence-based medicine have been created in medical schools in Canada, the US, the UK, Australia, and other countries. A 2009 study of UK programs found the more than half of UK medical schools offered some training in evidence-based medicine, although there was considerable variation in the methods and content, and EBM teaching was restricted by lack of curriculum time, trained tutors and teaching materials. Many programs have been developed to help individual physicians gain better access to evidence. For example, UpToDate was created in the early 1990s. The Cochrane Collaboration began publishing evidence reviews in 1993. BMJ Publishing Group launched a 6-monthly periodical in 1995 called Clinical Evidence that provided brief summaries of the current state of evidence about important clinical questions for clinicians. Since then many other programs have been developed to make evidence more accessible to practitioners.

The term evidence-based medicine is now applied to both the programs that are designing evidence-based guidelines and the programs that teach evidence-based medicine to practitioners. By 2000, "evidence-based medicine" had become an umbrella term for the emphasis on evidence in both population-level and individual-level decisions. In subsequent years, use of the term "evidence-based" had extended to other levels of the health care system. An example is "evidence-based health services", which seek to increase the competence of health service decision makers and the practice of evidence-based medicine at the organizational or institutional level. The concept has also spread outside of healthcare; for example, in his 1996 inaugural speech as President of the Royal Statistical Society, Adrian Smith proposed that "evidence-based policy" should be established for education, prisons and policing policy and all areas of government work.

The multiple tributaries of evidence-based medicine share an emphasis on the importance of incorporating evidence from formal research in medical policies and decisions. However they differ on the extent to which they require good evidence of effectiveness before promulgating a guideline or payment policy, and they differ on the extent to which it is feasible to incorporate individual-level information in decisions. Thus, evidence-based guidelines and policies may not readily 'hybridise' with experience-based practices orientated towards ethical clinical judgement, and can lead to contradictions, contest, and unintended crises. The most effective 'knowledge leaders' (managers and clinical leaders) use a broad range of management knowledge in their decision making, rather than just formal evidence. Evidence-based guidelines may provide the basis for governmentality in health care and consequently play a central role in the distant governance of contemporary health care systems.

The steps for designing explicit, evidence-based guidelines were described in the late 1980s: Formulate the question (population, intervention, comparison intervention, outcomes, time horizon, setting); search the literature to identify studies that inform the question; interpret each study to determine precisely what it says about the question; if several studies address the question, synthesize their results (meta-analysis); summarize the evidence in "evidence tables"; compare the benefits, harms and costs in a "balance sheet"; draw a conclusion about the preferred practice; write the guideline; write the rationale for the guideline; have others review each of the previous steps; implement the guideline.

For the purposes of medical education and individual-level decision making, five steps of EBM in practice were described in 1992 and the experience of delegates attending the 2003 Conference of Evidence-Based Health Care Teachers and Developers was summarized into five steps and published in 2005. This five step process can broadly be categorized as:


Systematic reviews of published research studies is a major part of the evaluation of particular treatments. The Cochrane Collaboration is one of the best-known programs that conducts systematic reviews. Like other collections of systematic reviews, it requires authors to provide a detailed and repeatable plan of their literature search and evaluations of the evidence. Once all the best evidence is assessed, treatment is categorized as (1) likely to be beneficial, (2) likely to be harmful, or (3) evidence did not support either benefit or harm.

A 2007 analysis of 1,016 systematic reviews from all 50 Cochrane Collaboration Review Groups found that 44% of the reviews concluded that the intervention was likely to be beneficial, 7% concluded that the intervention was likely to be harmful, and 49% concluded that evidence did not support either benefit or harm. 96% recommended further research. A 2001 review of 160 Cochrane systematic reviews (excluding complementary treatments) in the 1998 database revealed that, according to two readers, 41% concluded positive or possibly positive effect, 20% concluded evidence of no effect, 8% concluded net harmful effects, and 21% of the reviews concluded insufficient evidence. A review of 145 alternative medicine Cochrane reviews using the 2004 database revealed that 38.4% concluded positive effect or possibly positive (12.4%) effect, 4.8% concluded no effect, 0.7% concluded harmful effect, and 56.6% concluded insufficient evidence. In 2017, a study assessed the role of systematic reviews produced by Cochrane Collaboration to inform US private payers' policies making; it showed that though medical policy documents of major US private were informed by Cochrane systematic review; there was still scope to encourage the further usage.

Evidence quality can be assessed based on the source type (from meta-analyses and systematic reviews of triple-blind randomized clinical trials with concealment of allocation and no attrition at the top end, down to conventional wisdom at the bottom), as well as other factors including statistical validity, clinical relevance, currency, and peer-review acceptance. Evidence-based medicine categorizes different types of clinical evidence and rates or grades them according to the strength of their freedom from the various biases that beset medical research. For example, the strongest evidence for therapeutic interventions is provided by systematic review of randomized, triple-blind, placebo-controlled trials with allocation concealment and complete follow-up involving a homogeneous patient population and medical condition. In contrast, patient testimonials, case reports, and even expert opinion (however, some critics have argued that expert opinion "does not belong in the rankings of the quality of empirical evidence because it does not represent a form of empirical evidence" and continue that "expert opinion would seem to be a separate, complex type of knowledge that would not fit into hierarchies otherwise limited to empirical evidence alone"). have little value as proof because of the placebo effect, the biases inherent in observation and reporting of cases, difficulties in ascertaining who is an expert and more.

Several organizations have developed grading systems for assessing the quality of evidence. For example, in 1989 the U.S. Preventive Services Task Force (USPSTF) put forth the following:


Another example is the Oxford (UK) CEBM Levels of Evidence. First released in September 2000, the Oxford CEBM Levels of Evidence provides 'levels' of evidence for claims about prognosis, diagnosis, treatment benefits, treatment harms, and screening, which most grading schemes do not address. The original CEBM Levels was Evidence-Based On Call to make the process of finding evidence feasible and its results explicit. In 2011, an international team redesigned the Oxford CEBM Levels to make it more understandable and to take into account recent developments in evidence ranking schemes. The Oxford CEBM Levels of Evidence have been used by patients, clinicians and also to develop clinical guidelines including recommendations for the optimal use of phototherapy and topical therapy in psoriasis and guidelines for the use of the BCLC staging system for diagnosing and monitoring hepatocellular carcinoma in Canada.

In 2000, a system was developed by the GRADE (short for Grading of Recommendations Assessment, Development and Evaluation) working group and takes into account more dimensions than just the quality of medical research. It requires users of GRADE who are performing an assessment of the quality of evidence, usually as part of a systematic review, to consider the impact of different factors on their confidence in the results. Authors of GRADE tables grade the quality of evidence into four levels, on the basis of their confidence in the observed effect (a numerical value) being close to what the true effect is. The confidence value is based on judgements assigned in five different domains in a structured manner. The GRADE working group defines 'quality of evidence' and 'strength of recommendations' based on the quality as two different concepts which are commonly confused with each other.

Systematic reviews may include randomized controlled trials that have low risk of bias, or, observational studies that have high risk of bias. In the case of randomized controlled trials, the quality of evidence is high, but can be downgraded in five different domains.


In the case of observational studies per GRADE, the quality of evidence starts of lower and may be upgraded in three domains in addition to being subject to downgrading.


Meaning of the levels of quality of evidence as per GRADE:

In guidelines and other publications, recommendation for a clinical service is classified by the balance of risk versus benefit and the level of evidence on which this information is based. The U.S. Preventive Services Task Force uses:


GRADE guideline panelists may make strong or weak recommendations on the basis of further criteria. Some of the important criteria are the balance between desirable and undesirable effects (not considering cost), the quality of the evidence, values and preferences and costs (resource utilization).

Despite the differences between systems, the purposes are the same: to guide users of clinical research information on which studies are likely to be most valid. However, the individual studies still require careful critical appraisal.

Evidence-based medicine attempts to express clinical benefits of tests and treatments using mathematical methods. Tools used by practitioners of evidence-based medicine include:


Evidence-based medicine attempts to objectively evaluate the quality of clinical research by critically assessing techniques reported by researchers in their publications.


Although evidence-based medicine is regarded as the gold standard of clinical practice, there are a number of limitations and criticisms of its use. Two widely cited categorization schemes for the various published critiques of EBM include the three-fold division of Straus and McAlister ("limitations universal to the practice of medicine, limitations unique to evidence-based medicine and misperceptions of evidence-based-medicine") and the five-point categorization of Cohen, Stavri and Hersh (EBM is a poor philosophic basis for medicine, defines evidence too narrowly, is not evidence-based, is limited in usefulness when applied to individual patients, or reduces the autonomy of the doctor/patient relationship).

In no particular order, some published objections include:


One of the ongoing challenges with evidence-based medicine is that some healthcare providers do not follow the evidence. This happens partly because the current balance of evidence for and against treatments shifts constantly, and it is impossible to learn about every change. Even when the evidence is unequivocally against a treatment, it usually takes ten years for other treatments to be adopted. In other cases, significant change can require a generation of physicians to retire or die, and be replaced by physicians who were trained with more recent evidence.

Another major cause of physicians and other healthcare providers treating patients in ways unsupported by the evidence is that these healthcare providers are subject to the same cognitive biases as all other humans. They may reject the evidence because they have a vivid memory of a rare but shocking outcome (the availability heuristic), such as a patient dying after refusing treatment. They may overtreat to "do something" or to address a patient's emotional needs. They may worry about malpractice charges based on a discrepancy between what the patient expects and what the evidence recommends. They may also overtreat or provide ineffective treatments because the treatment feels biologically plausible.

The Berlin questionnaire and the Fresno Test are validated instruments for assessing the effectiveness of education in evidence-based medicine. These questionnaires have been used in diverse settings.

A Campbell systematic review that included 24 trials examined the effectiveness of e-learning in improving evidence-based health care knowledge and practice. It was found that e-learning, compared to no learning, improves evidence-based health care knowledge and skills but not attitudes and behaviour. There is no difference in outcomes when comparing e-learning to face-to-face learning. Combining e-learning with face-to-face learning (blended learning) has a positive impact on evidence-based knowledge, skills, attitude and behaviour. Related to e-learning, medical school students have engaged with editing Wikipedia to increase their EBM skills.



</doc>
<doc id="10016" url="https://en.wikipedia.org/wiki?curid=10016" title="End zone">
End zone

The end zone is the scoring area on the field, according to gridiron-based codes of football. It is the area between the end line and goal line bounded by the sidelines. There are two end zones, each being on an opposite side of the field. It is bordered on all sides by a white line indicating its beginning and end points, with orange, square pylons placed at each of the four corners as a visual aid (however, prior to around the early 1970s, flags were used instead to denote the end zone). Canadian rule books use the terms "goal area" and "dead line" instead of "end zone" and "end line" respectively, but the latter terms are the more common in colloquial Canadian English. Unlike sports like association football and ice hockey which require the ball/puck to pass completely over the goal line to count as a score, both Canadian and American football merely need the nose of the ball to break the vertical plane of the outer edge of the goal line.

A similar concept exists in both rugby football codes, where it is known as the "in-goal area". The difference between rugby and gridiron-based codes is that in rugby, the ball must be touched to the ground in the in-goal area to count as a try (the rugby equivalent of a touchdown), whereas in the gridiron-based games, simply possessing the ball while it is in the end zone is sufficient to count it as a touchdown.

Ultimate frisbee also uses an end zone scoring area. Scores in this sport are counted when a pass is received in the end zone.

The end zones were invented as a result of the creation of the forward pass. Prior to this, the goal line and end line were the same, and players scored a touchdown by leaving the field of play through that line. Goal posts were placed on the goal line, and any kicks that did not result in field goals but left the field through the end lines were simply recorded as touchbacks (or, in the Canadian game, singles; it was during the pre-end zone era that Hugh Gall set the record for most singles in a game, with eight).

In the earliest days of the forward pass, the pass had to be caught in-bounds and could not be thrown across the goal line (as the receiver would be out of bounds). This also made it difficult to pass the ball when very close to one's own goal line, since merely dropping back to pass or kick would result in a safety (rules of the forward pass at the time required the passer to be five yards behind the line of scrimmage, which would make throwing the forward pass when the ball was snapped from behind one's own five-yard line illegal in itself).

Thus, in 1912, the end zone was introduced in American football. In an era when professional football was still in its early years and college football dominated the game, the resulting enlargement of the field was constrained by fact that many college teams were already playing in well-developed stadiums, complete with stands and other structures at the ends of the fields, thereby making any substantial enlargement of the field unfeasible at many schools. Eventually, a compromise was reached: 12 yards of end zone were added to each end of the field, but in return, the playing field was shortened from 110 yards to 100, resulting in the physical size of the field being only slightly longer than before. Goal posts were originally kept on the goal lines, but after they began to interfere with play, they moved back to the end lines in 1927, where they have remained in college football ever since. The National Football League moved the goal posts up to the goal line again in 1933, then back again to the end line in 1974.

As with many other aspects of gridiron football, Canadian football adopted the forward pass and end zones much later than American football. The forward pass and end zones were adopted in 1929. In Canada, college football never reached a level of prominence comparable to U.S. college football, and professional football was still in its infancy in the 1920s. As a result, Canadian football was still being played in rudimentary facilities in the late 1920s. A further consideration was that the Canadian Rugby Union (the governing body of Canadian football at the time) wanted to reduce the prominence of single points (then called "rouges") in the game. Therefore, the CRU simply appended 25-yard end zones to the ends of the existing 110-yard field, creating a much larger field of play. Since moving the goal posts back 25 yards would have made the scoring of field goals excessively difficult, and since the CRU did not want to reduce the prominence of field goals, the goal posts were left on the goal line where they remain today. However, the rules governing the scoring of singles were changed: teams were required to either kick the ball out of bounds through the end zone or force the opposition to down a kicked ball in their own end zone in order to be awarded a point. By 1986, at which point CFL stadiums were becoming bigger and comparable in development to their American counterparts in an effort to stay financially competitive, the CFL reduced the depth of the end zone to 20 yards.

A team scores a touchdown by entering its opponent's end zone while carrying the ball or catching the ball while being within the end zone. If the ball is carried by a player, it is considered a score when any part of the ball is directly above or beyond any part of the goal line between the pylons. In addition, a two-point conversion may be scored after a touchdown by similar means.

In Ultimate Frisbee, a goal is scored by completing a pass into the end zone.

The end zone in American football is 10 yards long by yards (160 feet) wide. Each corner is marked with a pylon.

A full-sized end zone in Canadian football is 20 yards long by 65 yards wide. Prior to the 1980s, the Canadian end zone was 25 yards long. The first stadium to use the 20 yard long end zone was B.C. Place in Vancouver, which was completed in 1983. The floor of B.C. Place was (and is) too short to accommodate a field 160 yards in length. The shorter end zone proved popular enough that the CFL adopted it league-wide in 1986. At BMO Field, home to the Toronto Argonauts, the end zones are only 18 yards. 

In Canadian football stadiums that also feature a running track, it is usually necessary to truncate the back corners of the end zones, since a rectangular field 150 yards long and 65 yards wide will not fit completely inside an oval-shaped running track. Such truncations are marked as straight diagonal lines, resulting in an end zone with six corners and six pylons. As of 2016, Montreal's Percival Molson Stadium is the only CFL stadium that has the rounded-off style end zones.

During the CFL's American expansion in the mid-1990s, several stadiums, by necessity, used 15-yard end zones (some even shorter than 15).

Ultimate Frisbee uses an end zone 40 yards wide and 20 yards deep (37 m × 18 m).

The location and dimensions of a goal post differ from league to league, but it is usually within the boundaries of the end zone. In earlier football games (both professional and collegiate), the goal post began at the goal line, and was usually an H-shaped bar. Nowadays, for player safety reasons, almost all goal posts in the professional and collegiate levels of American football are T-shaped, and reside just outside the rear of both end zones.

The goal posts in Canadian football still reside on the goal line instead of the back of the end zones, partly because the number of field goal attempts would dramatically decrease if the posts were moved 20 yards back in that sport, and also because the larger end zone and wider field makes the resulting interference in play by the goal post a less serious problem.

At the high school level, it is not uncommon to see multi-purpose goal posts that include football goal posts at the top and a soccer net at the bottom; these are usually seen at smaller schools and in multi-purpose stadiums where facilities are used for multiple sports. When these or H-shaped goal posts are used in football, the lower portions of the posts are covered with several inches of heavy foam padding to protect the safety of the players.

Most professional and collegiate teams have their logo, team name, or both painted on the surface of the end zone, with team colors filling the background. Many championship and bowl games at college and professional level are commemorated by the names of the opposing teams each being painted in one of the opposite end zones. In some leagues, along with bowl games, local, national, or bowl game sponsors may also have their logos placed in the end zone. In the CFL, fully painted end zones are nonexistent, though some feature club logos or sponsors. Additionally, the Canadian end zone, being a live-ball part of the field, often features yardage dashes, not unlike the field of play itself.

In many places, particularly in smaller high schools and colleges, end zones are undecorated, or have plain white diagonal stripes spaced several yards apart, in lieu of colors and decorations. One notable use of this design in higher levels is with the Pittsburgh Steelers, who kept their diagonal-line end zone decoration at Heinz Field after positive fan reaction.

One of the quirks of the American Football League was its use of unusual patterns such as argyle in its end zones, a tradition revived in 2009 by the Denver Broncos, itself a former AFL team. The XFL standardized its playing fields so that all eight of its teams had uniform fields with the XFL logo in each end zone and no team identification.



</doc>
<doc id="10017" url="https://en.wikipedia.org/wiki?curid=10017" title="Ettore Ximenes">
Ettore Ximenes

Ettore Ximenes (April 11, 1855, Palermo – December 20, 1926, Rome) was an Italian sculptor.

Son of Antonio Ximenes and Giulia Tolentino, a sicilian noble woman, Ettore Ximenes initially embarked on literary studies but then took up sculpture and attended the courses at the Palermo Academy of Fine Arts. After 1872, he continued training at the Naples Academy under Domenico Morelli and Stanislao Lista. He also established a close relationship with Vincenzo Gemito.

He returned to Palermo in 1874 and won a competition for a four-year grant, which enabled him to study and open a studio for sculpture in Florence. In 1873 at Vienna, he exhibited "Work without Genius". In 1877 at Naples, he exhibited a life-size statue titled "The Equilibrium" about a gymnast walking on a sphere. He would make copies of this work in small marble and bronze statuettes.

He exhibited a stucco "Christ and the Adultress" and "Il cuore del re (Heart of the King)", the latter depicting an oft-repeated story of King Vittorio Emanuele during one of his frequent hunts, encountering and offering charity to a peasant child. At the 1878 Paris World Exposition he displayed: "The Brawl" and "il Marmiton". In Paris, he met with Auguste Rodin and Jean-Baptiste Carpeaux.

In 1878, he also completed a life-size stucco of "il Ciceruacchio", a statue of the Italian patriot Angelo Brunetti and his thirteen-year-old son, depicting them at the moment of their execution in 1849 by Austrian troops. The Cicervacchio statue, with its tinge of revolutionary zeal, did not find commissions for completing the work in marble.

He then completed a nude statue of "Nanà" based on the novel by Émile Zola; the statue was exhibited at the 1879 Salon di Paris. The next year at the Paris Salon, he displayed "La Pesca meravigliosa", where a fisherman rescues a bathing maiden. Returning to Italy, he displayed the bust del minister Giuseppe Zanardelli. At the Mostra of Rome, he displayed "The assassination of Julius Caeser"; and at the Exposition of Venice, "Ragazzi messi in fila". Ximenes' realism gave way to Symbolist and Neo-Renaissance elements. In addition to sculpture, he also produced illustrations for the works of Edmondo De Amicis published by the Treves publishing house.

Ximenes was involved in many of the major official monumental projects in Italy from the 1880s on and devoted his energies as from 1911 primarily to commissions for important public works in São Paulo, Kiev, New York and Buenos Aires.







</doc>
<doc id="10018" url="https://en.wikipedia.org/wiki?curid=10018" title="Edsger W. Dijkstra">
Edsger W. Dijkstra

Edsger Wybe Dijkstra (; 11 May 1930 – 6 August 2002) was a Dutch systems scientist, programmer, software engineer, science essayist, and early pioneer in computing science. He held the Schlumberger Centennial Chair in Computer Sciences at the University of Texas at Austin from 1984 until his retirement in 1999. A theoretical physicist by training, Dijkstra worked as a programmer at the Mathematisch Centrum (Amsterdam) from 1952 to 1962. He was a professor of mathematics at the Eindhoven University of Technology (1962–1984) and a research fellow at the Burroughs Corporation (1973–1984).

One of the most influential members of computing science's founding generation, Dijkstra helped shape the new discipline from both an engineering and a theoretical perspective. His fundamental contributions cover diverse areas of computing science, including compiler construction, operating systems, distributed systems, sequential and concurrent programming, programming paradigm and methodology, programming language research, program design, program development, program verification, software engineering principles, graph algorithms, and philosophical foundations of computer programming and computer science. Many of his papers are the source of new research areas. Several concepts and problems that are now standard in computer science were first identified by Dijkstra or bear names coined by him.

Computer programming in the 1950s to 1960s was not recognized as an academic discipline. In the late 1960s, computer programming was in a state of crisis. He was one of a small group of academics and industrial programmers who advocated a new programming style to improve the quality of programs. Dijkstra, who had a background in mathematics and physics, was one of the driving forces behind the acceptance of computer programming as a scientific discipline. He coined the phrase "structured programming" and during the 1970s this became the new programming orthodoxy. His ideas about structured programming helped lay the foundations for the birth and development of the professional discipline of software engineering, enabling programmers to organize and manage increasingly complex software projects. As Bertrand Meyer (2009) noted, "The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming."

The academic study of concurrent computing started in the 1960s, with Dijkstra (1965) credited with being the first paper in this field, identifying and solving the mutual exclusion problem. He was also one of the early pioneers of the research on principles of distributed computing. His foundational work on concurrency, semaphores, mutual exclusion, deadlock (deadly embrace), finding shortest paths in graphs, fault-tolerance, self-stabilization, among many other contributions comprises many of the pillars upon which the field of distributed computing is built. Shortly before his death in 2002, he received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor. As the prize, sponsored jointly by the ACM Symposium on Principles of Distributed Computing (PODC) and the EATCS International Symposium on Distributed Computing (DISC), recognizes that "No other individual has had a larger influence on research in principles of distributed computing".

Edsger W. Dijkstra was born in Rotterdam. His father was a chemist who was president of the Dutch Chemical Society; he taught chemistry at a secondary school and was later its superintendent. His mother was a mathematician, but never had a formal job.

Dijkstra had considered a career in law and had hoped to represent the Netherlands in the United Nations. However, after graduating from school in 1948, at his parents' suggestion he studied mathematics and physics and then theoretical physics at the University of Leiden.

In the early 1950s, electronic computers were a novelty. Dijkstra stumbled on his career quite by accident, and through his supervisor, Professor A. Haantjes, he met Adriaan van Wijngaarden, the director of the Computation Department at the Mathematical Center in Amsterdam, who offered Dijkstra a job; he officially became the Netherlands' first "programmer" in March 1952.

For some time Dijkstra remained committed to physics, working on it in Leiden three days out of each week. With increasing exposure to computing, however, his focus began to shift. As he recalled:
When Dijkstra married Maria (Ria) C. Debets in 1957, he was required as a part of the marriage rites to state his profession. He stated that he was a programmer, which was unacceptable to the authorities, there being no such profession at that time in The Netherlands.
In 1959 he received his PhD from the University of Amsterdam for a thesis entitled 'Communication with an Automatic Computer', devoted to a description of the assembly language designed for the first commercial computer developed in the Netherlands, the X1. His thesis supervisor was van Wijngaarden.

From 1952 until 1962 Dijkstra worked at the Mathematisch Centrum in Amsterdam, where he worked closely with Bram Jan Loopstra and Carel S. Scholten, who had been hired to build a computer. Their mode of interaction was disciplined: They would first decide upon the interface between the hardware and the software, by writing a programming manual. Then the hardware designers would have to be faithful to their part of the contract, while Dijkstra, the programmer, would write software for the nonexistent machine. Two of the lessons he learned from this experience were the importance of clear documentation, and that program debugging can be largely avoided through careful design.
Dijkstra formulated and solved the shortest path problem for a demonstration at the official inauguration of the ARMAC computer in 1956, but—because of the absence of journals dedicated to automatic computing—did not publish the result until 1959.

At the Mathematical Center, Dijkstra and his colleague developed a compiler for the programming language ALGOL 60; it had a profound influence on his later thinking on programming as a scientific activity. He and Zonneveld had completed the implementation of the first ALGOL 60 compiler by August 1960, more than a year before a compiler was produced by another group.

In 1962 Dijkstra moved to Eindhoven, and later to Nuenen, in the south of the Netherlands, where he became a professor in the Mathematics Department at the Eindhoven University of Technology. The university did not have a separate computer science department and the culture of the mathematics department did not particularly suit him. Dijkstra tried to build a group of computer scientists who could collaborate on solving problems. This was an unusual model of research for the Mathematics Department. In the late 1960s he built the THE operating system (named for the university, then known as Technische Hogeschool Eindhoven), which has influenced the designs of subsequent operating systems through its use of software based paged virtual memory.

Dijkstra joined Burroughs Corporation, a company known at that time for the production of computers based on an innovative hardware architecture, as its Research Fellow in August 1973. His duties consisted of visiting some of the company's research centers a few times a year and carrying on his own research, which he did in the smallest Burroughs research facility, namely, his study on the second floor of his house in Nuenen. In fact, Dijkstra was the only research fellow of Burroughs Corporation and worked for it from home, occasionally travelling to its branches in the United States. As a result, he reduced his appointment at the university to one day a week. That day, Tuesday, soon became known as the day of the famous 'Tuesday Afternoon Club', a seminar during which he discussed with his colleagues scientific articles, looking at all aspects – notation, organisation, presentation, language, content, etc. Shortly after he moved in 1984 to the University of Texas at Austin (USA), a new 'branch' of the Tuesday Afternoon Club emerged in Austin.

The Burroughs years saw him at his most prolific in output of research articles. He wrote nearly 500 documents in the EWD series (described below), most of them technical reports, for private circulation within a select group.

Dijkstra accepted the Schlumberger Centennial Chair in the Computer Science Department at the University of Texas at Austin in 1984.

Dijkstra worked in Austin until his retirement in November 1999. To mark the occasion and to celebrate his forty-plus years of seminal contributions to computing science, the Department of Computer Sciences organized a symposium, which took place on his 70th birthday in May 2000.

Dijkstra and his wife returned from Austin to his original house in Nuenen (Netherlands) where he found that he had only months to live. He said that he wanted to retire in Austin, Texas, but to die in the Netherlands. Dijkstra died on 6 August 2002 after a long struggle with cancer. He and his wife Maria (Ria) Debets were survived by their three children: Marcus, Femke and the computer scientist Rutger M. Dijkstra.

As an early theoretical pioneer in many research areas of computing science, Dijkstra helped shape the new discipline from both an engineering and an academic perspective. Many of his papers are the source of new research areas. Many concepts that are now standard in computer science were first identified by Dijkstra and/or bear names coined by him. Several important problems were also first formulated and solved by him. A 1994 survey of over a thousand professors of computer science was conducted to obtain a list of 38 most influential scholarly papers in the field, and Dijkstra is the author of five papers.

During his forty-plus years as a computing scientist, which included positions in both academia and industry, Dijkstra made numerous seminal contributions to many areas of computing science, including compiler construction, operating systems, concurrent programming (concurrent computing), distributed programming (distributed computing), programming paradigm and methodology, programming language research, program design, program development, program verification, software engineering principles, algorithm design, and philosophical foundations of computer programming and computer science. In addition, Dijkstra was intensely interested in teaching computer science, and in the relationships between academic computing science and the software industry.

Dijkstra's algorithmic work (especially graph algorithms, concurrent algorithms, and distributed algorithms) plays an important role in many areas of computing science. According to Leslie Lamport (2002), Dijkstra "started the field of concurrent and distributed algorithms with his 1965 CACM paper "Solution of a Problem in Concurrent Programming Control", in which he first stated and solved the mutual exclusion problem." As Lamport explains, "that paper is probably why PODC exists (...). It remains to this day the most influential paper in the field. That it did not win a PODC Influential Paper Award reflects an artificial separation between concurrent and distributed algorithms–a separation that has never existed in Dijkstra's work."

In 1959 Dijkstra published in a 3-page article 'A note on two problems in connexion with graphs' the algorithm to find the shortest path in a graph between any two given nodes, now called Dijkstra's algorithm. Its impact over the next 40 years is summarised from the article of Mikkel Thorup, 'Undirected Single Source Shortest Paths with Positive Integer Weights in Linear Time' (1999): "Since 1959, all theoretical developments in SSSP [Single-Source Shortest Paths] for general directed and undirected graphs have been based on Dijkstra's algorithm." Dijkstra's algorithm is used in SPF, Shortest Path First, which is used in the routing protocols OSPF and IS-IS. Various modifications to Dijkstra's algorithm have been proposed by many authors using heuristics to reduce the run time of shortest path search. One of the most used heuristic algorithms is the A* search algorithm (first described by Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute in 1968), the main goal is to reduce the run time by reducing the search space. Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate capabilities of a new computer called ARMAC. His objective was to choose both a problem as well as an answer (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm in about 20 minutes without aid of paper and pen and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (so that 6 bits would suffice to represent the city in the algorithm). As he recalled, in an interview published in 2001:
A year later, he came across another problem from hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim's minimal spanning tree algorithm. The Prim's algorithm was originally developed in 1930 by Czech mathematician Vojtěch Jarník and later independently rediscovered and republished by Robert C. Prim in 1957 and Dijkstra in 1959. Therefore, it is also sometimes called the DJP algorithm.

In 1961 Dijkstra first described the shunting-yard algorithm, a method for parsing mathematical expressions specified in infix notation, in the Mathematisch Centrum report. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was named the "shunting yard" algorithm because its operation resembles that of a railroad shunting yard. The shunting-yard algorithm is commonly used to implement operator-precedence parsers.

In 1962 or 1963 Dijkstra proposed the semaphore mechanism for mutual exclusion algorithm for n processes (a generalization of Dekker's algorithm), which was probably the first published concurrent algorithm and which introduced a new area of algorithmic research. He also identified the deadlock problem and proposed the banker's algorithm that prevents deadlock.

In 1974 Dijkstra presented three self-stabilizing algorithms for mutual exclusion on a ring. Dijkstra's work is considered to be the first to introduce and demonstrate the self-stabilization concept.

In the mid-1970s Dijkstra (together with other authors) introduced two useful abstractions (mutator and collector) to the study of garbage collection. The mutator abstracts the process that performs the computation, including allocation of a new storage cell. The collector is the process that automatically reclaims garbage. Furthermore, this paper gives a formalization of "tri-color marking" that is basic to incremental garbage collection.

In the early 1980s Dijkstra and Carel S. Scholten proposed the Dijkstra–Scholten algorithm for detecting termination in distributed systems.

In 1981 Dijkstra developed smoothsort, a comparison-based sorting algorithm and a variation of heapsort.

Dijkstra was known to be a fan of ALGOL 60, and worked on the team that implemented the first compiler for that language. He was closely involved in the ALGOL 60 development, realisation and popularisation. As discussed by Peter Naur in the article 'The European side of the last phase of the development of ALGOL 60', in the "Proceedings of the First ACM SIGPLAN Conference on History of Programming Languages", January 1978, Dijkstra took part in the period 1958–1959 in a number of meetings that culminated in the publication of the report defining the ALGOL 60 language. Dijkstra's name does not appear in the list of 13 authors of the final report. Apparently, he eventually left the committee because he could not agree with the majority opinions. Still, while at the Mathematisch Centrum (Amsterdam), he wrote jointly with Jaap Zonneveld the first ALGOL 60 compiler. Dijkstra and Zonneveld, who collaborated on the compiler, agreed not to shave until the project was completed; while Zonneveld shaved shortly thereafter, Dijkstra kept his beard for the rest of his life.

ALGOL was the result of a collaboration of American and European committees. ALGOL 60 (short for ALGOrithmic Language 1960) is a member of the ALGOL family of computer programming languages. It followed on from ALGOL 58 and inspired many languages that followed it. It gave rise to many other programming languages, including BCPL, B, Pascal, Simula and C. Algol 60 was a sophisticatedly designed computer language and it provided a large number of hitherto unknown implementation challenges. As Bjarne Stroustrup notes, "one problem with Algol60 was that no one knew how to implement it." A major new challenge in Algol 60 implementation was the run-time allocation and management of data. In 1960 Dijkstra and Zonneveld showed how recursive procedures could be executed using a run-time stack of activation records, and how to efficiently access identifiers from statically enclosing scopes using the so-called 'display'. The ALGOL 60 compiler was one of the first to support recursion employing a novel method to do so. Dijkstra's short book "Primer of Algol 60 Programming", originally published in 1962, was the standard reference for the language for several years.

Computer programming in the 1950s to 1960s was not recognized as an academic discipline and unlike mature sciences there were no theoretical concepts or coding systems. Programming as a professional activity was poorly understood in those years.

In the late 1960s computer programming was in state of crisis. Software crisis is a term used in the early days of computing science for the difficulty of writing useful and efficient computer programs in the required time. The software crisis was due to the rapid increases in computer power and the complexity of the problems that could be tackled. With the increase in the complexity of the software, many software problems arose because existing methods were neither sufficient nor up to the mark. The term "software crisis" was coined by some attendees at the first NATO Software Engineering Conference in 1968 at Garmisch, Germany. His 1972 ACM Turing Award Lecture makes reference to this same problem: "The major cause of the software crisis is that the machines have become several orders of magnitude more powerful! To put it quite bluntly: as long as there were no machines, programming was no problem at all; when we had a few weak computers, programming became a mild problem, and now we have gigantic computers, programming has become an equally gigantic problem."

While Dijkstra had programmed extensively in machine code in the 1950s, he came to the conclusion that in high-level languages frequent use of the GOTO statement was usually symptomatic of poor structure. In 1968 he wrote a private paper "A Case against the GO TO Statement", which was then published as a letter in CACM. Editor Niklaus Wirth gave this letter the heading "Go To Statement Considered Harmful", which introduced the phrase "considered harmful" into computing.

Dijkstra argued that the programming statement GOTO, found in many high-level programming languages, is a major source of errors, and should therefore be eliminated. This letter caused a huge debate in the programming community. Some went to the length of equating good programming with the elimination of GO TO. Dijkstra refused to mention the debate, or even the GO TO statement, in his article "Notes on Structured Programming". The debate has long since died down; programming languages provide alternatives to the GO TO, few programmers today use it liberally, and most never use it at all.

Dijkstra's thesis was that departures from linear control flow were clearer if allowed only in disciplined higher-level structures such as the if-then-else statement and the while loop. This methodology was developed into structured programming movement, the title of his 1972 book, coauthored with C.A.R. Hoare and Ole-Johan Dahl. Considered by many as the first significant movement in history of computer programming, structured programming became the new programming orthodoxy during the 1970s. Bertrand Meyer remarked that, "The revolution in views of programming started by Dijkstra's iconoclasm led to a movement known as structured programming, which advocated a systematic, rational approach to program construction. Structured programming is the basis for all that has been done since in programming methodology, including object-oriented programming."

Structured programming is often regarded as "goto-less programming". But as Bertrand Meyer notes, "As the first book on the topic ["Structured Programming" by Dijkstra, Dahl, and Hoare] shows, structured programming is about much more than control structures and the goto. Its principal message is that programming should be considered a scientific discipline based on mathematical rigor." , structured programming – especially in the 1970s and 1980s – significantly influenced the birth of many modern programming languages such as Pascal, C, Modula-2, and Ada. The Fortran 77 version which incorporates the concepts of structured programming, was released in 1978. The C++ language was a considerably extended and enhanced version of the popular C (see also: list of C-based programming languages). Since C++ was developed from a more traditional , it is a 'hybrid language', rather than a pure object-oriented programming language.

In his article "Structured Programming: Retrospect and Prospect" (1986), Harlan Mills writes, "Edsger W. Dijkstra's 1969 "Structured Programming" article precipitated a decade of intense focus on programming techniques that has fundamentally altered human expectations and achievements in software development. Before this decade of intense focus, programming was regarded as a private, puzzle-solving activity of writing computer instructions to work as a program. After this decade, programming could be regarded as a public, mathematics-based activity of restructuring specifications into programs. Before, the challenge was in getting programs to run at all, and then in getting them further debugged to do the right things. After, programs could be expected to both run and do the right things with little or no debugging. Before, it was common wisdom that no sizable program could be error-free. After, many sizable programs have run a year or more with no errors detected. These expectations and achievements are not universal because of the inertia of industrial practices. But they are well-enough established to herald fundamental change in software development."

The book "Concise Encyclopedia of Computer Science" (2004), edited by Edwin D. Reilly, notes that "the major contributions of structured programming have been twofold—the elevation of programming technique to something less of an art and more of a science, and the demonstration that carefully structured programs can be creative works of sufficient literary merit to deserve being read by humans and not just by computer."

Dijkstra's ideas about programming methodology (especially the structured programming movement) helped lay the foundations for the birth and development of the professional discipline of software engineering (in particular the software design and development), enabling programmers to organize and manage increasingly complex software projects. In the late 1960s Dijkstra discussed the concept of program families. And in the mid 1970s David Parnas and others clarified the idea and showed how to apply it in software engineering principles.

The rise of the structured programming movement led to many other "structured" approaches applied to software design. The techniques of structured analysis and structured design are outgrowths of structured programming concepts and techniques, and of the early ideas about modular design. Principles of modularity were strengthened by Larry Constantine's concepts of coupling (to be minimized between modules) and cohesion (to be maximized within modules), by David Parnas's techniques of information hiding, and by abstract data types. A number of tools and methods employing structured concepts were developed, such as Structured Design, Jackson's Structured Programming, Ross' Structured Analysis and Design Technique (SADT), Yourdon's Structured Method, Structured Systems Analysis and Design Method (SSADM), and James Martin's Information Engineering. The field of software metrics is often considered as a direct influence of the structured programming movement on software engineering in the 1970s.

Separation of concerns (SoC), one of the basic principles in software engineering, is a design principle for separating a computer program into distinct sections, such that each section addresses a separate concern. The term "separation of concerns" was coined by Dijkstra in his 1974 paper "On the role of scientific thought".

In the 1960s Dijkstra and his colleagues in Eindhoven designed and implemented THE (standing for 'Technische Hogeschool Eindhoven') operating system, which was organised into clearly identified layers. His 1968 article on this subject provided the foundation for subsequent designs of the operating systems. The IEEE Computer Society's David Alan Grier writes, "We generally trace the idea of building computer systems in layers back to a 1967 paper that the Dutch computer scientist Edsger Dijkstra gave to a joint IEEE Computer Society/ACM conference. Prior to this paper, engineers had struggled with the problem of how to organize software. If you look at early examples of programs, and you can find many in the electronic library of the Computer Society, you will find that most code of that era is complicated, difficult to read, hard to modify, and challenging to reuse. In his 1967 paper, Dijkstra described how software could be constructed in layers and gave an example of a simple operating system that used five layers. He admitted that this system might not be a realistic test of his ideas but he argued that the "larger the project, the more essential the structuring!" The idea of using layers to control complexity has become a mainstay of software architecture. We see it in many forms and apply it to many problems. We see it in the hierarchy of classes in object-oriented programming and in the structure of Service-Oriented Architecture (SOA). SOA is a relatively recent application of layering in computer science. It was articulated in 2007 as a means of controlling complexity in business systems, especially distributed systems that make substantial use of the Internet. Like Dijkstra's plan for system development, its layering system is called the SOA Solution Stack or S3. The S3's nine layers are: 1) operational systems, 2) service components, 3) services, 4) business processes, 5) consumer actions, 6) system integration, 7) quality control and assurance, 8) information architecture, and 9) system governance and policies."

Dijkstra organized the design of the system in layers in order to reduce the overall complexity of the software. Though the term 'architecture' had not yet been used to describe software design, this was certainly considered the first glimpse of software architecture. It introduced a number of design principles which have become part of the working vocabulary of every professional programmer: levels of abstraction, programming in layers, the semaphore, and cooperating sequential processes. His original paper on the THE operating system was reprinted in the 25th Anniversary issue of Communications of the ACM, in January 1983. By way of introduction, the Editor-in-Chief says, "This project initiated a long line of research in multilevel systems architecture — a line that continues to the present day because hierarchical modularity is a powerful approach to organizing large systems."

In a one-page paper from 1965 Dijkstra introduced the 'mutual exclusion problem' for n processes and discussed a solution to it. It was probably the first published concurrent algorithm. The notion, standard by now, of a 'critical section' was also introduced in this paper. Per Brinch Hansen, a pioneer in the field of concurrent computing, considers Dijkstra's "Cooperating Sequential Processes" (1965) to be the first classic paper in concurrent programming. As Brinch Hansen notes, 'Dijkstra lays the conceptual foundation for abstract concurrent programming' with that paper.
In 1968 Dijkstra published his seminal paper 'Cooperating sequential processes', a 70-page essay that originated the field of concurrent programming. He discussed in it the notion of mutual exclusion (mutex) and the criteria a satisfactory solution should satisfy. He also redressed the historical perspective left out of his 1965 paper by including the first known correct solution to the mutual exclusion problem, for two processes, due to Theodorus Dekker. Dijkstra subsequently generalized Dekker's solution to n processes. Further, he proposed the first synchronisation mechanism for concurrent processes, the semaphore with its two operations, P and V. He also identified the 'deadlock problem' (called there 'the problem of the deadly embrace') and proposed an elegant 'Banker's algorithm' that prevents deadlock. The deadlock detection and prevention became perennial research problems in the field of concurrent programming.
The dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them. It was originally formulated in 1965 by Dijkstra as a student exam exercise, presented in terms of computers competing for access to tape drive peripherals. Soon after, Tony Hoare gave the problem its present formulation. The sleeping barber problem is also attributed to Dijkstra.

In his book "Concurrent Programming: Algorithms, Principles, and Foundations" Michel Raynal writes, "Since the early work of E.W. Dijkstra (1965), who introduced the mutual exclusion problem, the concept of a process, the semaphore object, the notion of a weakest precondition, and guarded commands (among many other contributions), synchronization is no longer a catalog of tricks but a domain of computing science with its own concepts, mechanisms, and techniques whose results can be applied in many domains. This means that process synchronization has to be a major topic of any computer science curriculum."

John W. McCormick et al. (2011) notes, "The notion of the concurrent program as a means for writing parallel programs without regard for the underlying hardware was first introduced by Edsger Dijkstra (1968). Moti Ben-Ari (1982) elegantly summed up Dijkstra's idea in three sentences: ‘Concurrent programming is the name given to programming notation and techniques for expressing potential parallelism and solving the resulting synchronization and communication problems. Implementation of parallelism is a topic in computer systems (hardware and software) that is essentially independent of concurrent programming. Concurrent programming is important because it provides an abstract setting in which to study parallelism without getting bogged down in the implementation details.’"

Dijkstra was one of the very early pioneers of the research on principles of distributed computing. As the citation for the Dijkstra Prize recognizes, "no other individual has had a larger influence on research in principles of distributed computing." Some of his papers are even considered to be those that established the field. Dijkstra's 1965 paper, "Solution of a Problem in Concurrent Programming Control" was the first to present the correct solution to the mutual exclusion problem. Leslie Lamport writes that this work "is probably why PODC exists" and it "started the field of concurrent and distributed algorithms".

In particular, his paper "Self-stabilizing Systems in Spite of Distributed Control" (1974) started the sub-field of self-stabilization. It is also considered as the first scientific examination of fault-tolerant systems. Dijkstra's paper was not widely noticed until Leslie Lamport's invited talk at the ACM Symposium on Principles of Distributed Computing (PODC) in 1983. In his report on Dijkstra's work on self-stabilizing distributed systems, Lamport regard it to be 'a milestone in work on fault tolerance' and 'a very fertile field for research'.

From the 1970s, Dijkstra's chief interest was formal verification. In 1976 Dijkstra published a seminal book, "A Discipline of Programming", which put forward his method of systematic development of programs together with their correctness proofs. In his exposition he used his 'Guarded Command Language'. The language, with its reliance on non-determinism, the adopted weakest precondition semantics and the proposed development method has had a considerable impact on the field to this day. The refinement calculus, originally proposed by Ralph-Johan Back and developed by Carroll Morgan, is an extension of Dijkstra's weakest precondition calculus, where program statements are modeled as predicate transformers.

In 1984, to add further support to this approach to programming, he published jointly with Wim Feijen an introductory textbook for first-year students of computer science. The book, first published in Dutch, was entitled "Een methode van programmeren". The English edition appeared in 1988 as "A Method of Programming".

Many of his opinions on computer science and programming have become widespread. For example, the programming phrase "two or more, use a for" (a rule of thumb when to use a loop) is sometimes attributed to him.

He was the first to make the claim that programming is so inherently complex that, in order to manage it successfully, programmers need to harness every trick and abstraction possible.

Dijkstra was one of the most famous opponents of the engineering view of computing science. Like Peter Naur and Kristen Nygaard, Dijkstra disliked the very term 'computer science'. Computer science, as Dijkstra pointed out, deserves a better name. He suggests it can be called 'computing science'. Instead of the computer, or computing technology, Dijkstra wanted to emphasize the abstract mechanisms that computing science uses to master complexity. When expressing the abstract nature of computing science, he wrote,

In "The Humble Programmer" (1972), Dijkstra wrote: "We must not forget that it is not our [computing scientists'] business to make programs, it is our business to design classes of computations that will display a desired behaviour."

Dijkstra also opposed the inclusion of software engineering under the umbrella of academic computer science. He wrote that, "As economics is known as "The Miserable Science", software engineering should be known as "The Doomed Discipline", doomed because it cannot even approach its goal since its goal is self-contradictory." And "software engineering has accepted as its charter "How to program if you cannot."."

In the world of computing science, Dijkstra is well known as a "character". In the preface of his book "A Discipline of Programming" (1976) he stated the following: "For the absence of a bibliography I offer neither explanation nor apology." In fact, most of his articles and books have no references at all. This approach to references was deplored by some researchers. But Dijkstra chose this way of working to preserve his self-reliance.

As a university professor for much of his life, Dijkstra saw teaching not just as a required activity but as a serious research endeavor. His approach to teaching was unconventional. His lecturing style has been described as idiosyncratic. When lecturing, the long pauses between sentences have often been attributed to the fact that English is not Dijkstra's first language. However the pauses also served as a way for him to think on his feet and he was regarded as a quick and deep thinker while engaged in the act of lecturing. His courses for students in Austin had little to do with computer science but they dealt with the presentation of mathematical proofs. At the beginning of each semester he would take a photo of each of the students, in order to memorize their names. He never followed a textbook, with the possible exception of his own while it was under preparation. When lecturing, he would write proofs in chalk on a blackboard rather than using overhead foils. He invited the students to suggest ideas, which he then explored, or refused to explore because they violated some of his tenets. He assigned challenging homework problems, and would study his students' solutions thoroughly. He conducted his final examinations orally, over a whole week. Each student was examined in Dijkstra's office or home, and an exam lasted several hours.

He was also highly original in his way of assessing people's capacity for a job. When Vladimir Lifschitz came to Austin in 1990 for a job interview, Dijkstra gave him a puzzle. Vladimir solved it and has been working in Austin since then.

Despite having invented much of the technology of software, Dijkstra eschewed the use of computers in his own work for many decades. Even after he succumbed to his UT colleagues' encouragement and acquired a Macintosh computer, he used it only for e-mail and for browsing the World Wide Web. Dijkstra never wrote his articles using a computer. He preferred to rely on his typewriter and later on his Montblanc pen. Dijkstra's favorite writing instrument was the Montblanc Meisterstück fountain pen. He repeatedly tried other pens, but none ever displaced the Montblanc.

He had no use for word processors, believing that one should be able to write a letter or article without rough drafts, rewriting, or any significant editing. He would work it all out in his head before putting pen to paper, and once mentioned that when he was a physics student he would solve his homework problems in his head while walking the streets of Leiden.
Most of Dijkstra's publications were written by him alone. He never had a secretary and took care of all his correspondence alone. When colleagues prepared a Festschrift for his sixtieth birthday, published by Springer-Verlag, he took the trouble to thank each of the 61 contributors separately, in a hand-written letter.

Throughout Dijkstra's career, his work was characterized by elegance and economy. A prolific writer (especially as an essayist), Dijkstra authored more than 1,300 papers, many written by hand in his precise script. They were essays and parables; fairy tales and warnings; comprehensive explanation and pedagogical pretext. Most were about mathematics and computer science; others were trip reports that are more revealing about their author than about the people and places visited. It was his habit to copy each paper and circulate it to a small group of colleagues who would copy and forward the papers to another limited group of scientists. His love affair with simplicity came at an early age and under his mother's guidance. He once said he had asked his mother whether trigonometry was a difficult topic. She replied that he must learn all the formulas and that furthermore if he required more than five lines to prove something, he was on the wrong track.

Dijkstra was famous for his wit, eloquence, and way with words, such as in his remark, "The question of whether Machines Can Think (…) is about as relevant as the question of whether Submarines Can Swim."; his advice to a promising researcher, who asked how to select a topic for research, "Do only what only you can do". Dijkstra was also known for his vocal criticism. As an outspoken and critical visionary, he strongly opposed the teaching of BASIC.

In many of his more humorous essays, Dijkstra described a fictional company of which he served as chairman. The company was called Mathematics, Inc., a company that he imagined having commercialized the production of mathematical theorems in the same way that software companies had commercialized the production of computer programs. He invented a number of activities and challenges of Mathematics Inc. and documented them in several papers in the EWD series. The imaginary company had produced a proof of the Riemann Hypothesis but then had great difficulties collecting royalties from mathematicians who had proved results assuming the Riemann Hypothesis. The proof itself was a trade secret. Many of the company's proofs were rushed out the door and then much of the company's effort had to be spent on maintenance. A more successful effort was the Standard Proof for Pythagoras' Theorem, that replaced the more than 100 incompatible existing proofs. Dijkstra described Mathematics Inc. as "the most exciting and most miserable business ever conceived". EWD 443 (1974) describes his fictional company as having over 75 percent of the world's market share.

Dijkstra was well known for his habit of carefully composing manuscripts with his fountain pen. The manuscripts are called EWDs, since Dijkstra numbered them with "EWD", his initials, as a prefix. According to Dijkstra himself, the EWDs started when he moved from the Mathematical Centre in Amsterdam to the Eindhoven University of Technology (then Technische Hogeschool Eindhoven). After going to Eindhoven, Dijkstra experienced a writer's block for more than a year. Dijkstra distributed photocopies of a new EWD among his colleagues. Many recipients photocopied and forwarded their copies, so the EWDs spread throughout the international computer science community. The topics were computer science and mathematics, and included trip reports, letters, and speeches. These short articles span a period of 40 years. Almost all EWDs appearing after 1972 were hand-written. They are rarely longer than 15 pages and are consecutively numbered. The last one, No. 1318, is from 14 April 2002. Within computer science they are known as the EWD reports, or, simply the EWDs. More than 1300 EWDs have been scanned, with a growing number transcribed to facilitate search, and are available online at the Dijkstra archive of the University of Texas.

Dijkstra's self-confidence went together with a remarkably modest lifestyle, to the point of being spartan. His and his wife's house in Nuenen was simple, small and unassuming. He did not own a TV, a VCR or a mobile telephone, and did not go to the movies. In contrast, he played the piano well and, while in Austin, liked to go to concerts. An enthusiastic listener of classical music, Dijkstra's favorite composer was Mozart.

Dijkstra died on 6 August 2002. According to officials at the University of Texas, the cause of death was cancer.

In 1972 the Association for Computing Machinery (ACM) acknowledged Dijkstra's seminal contributions to the field by awarding him the distinguished Turing Award. The citation for the award reads:
The introduction given at the awards ceremony is a tribute to Dijkstra:
In the words of Sir Tony Hoare, FRS, delivered by him at Dijkstra's funeral:
In March 2003, the following email was sent to the distributed computing community:
Former ACM President Peter J. Denning wrote about Dijkstra:
Among Dijkstra's awards and honors are:

The Distinguished Fellowship of the British Computer Society (BCS) is awarded under bylaw 7 of the BCS's Royal Charter. The award was first approved in 1969 and the first election was made in 1971 to Dijkstra.

On the occasion of Dijkstra's 60th birthday in 1990, The Department of Computer Science (UTCS) at the University of Texas at Austin organized a two-day seminar in his honor. Speakers came from all over the United States and Europe, and a group of computer scientists contributed research articles which were edited into a book.

In 2002, the C&C Foundation of Japan recognized Dijkstra "for his pioneering contributions to the establishment of the scientific basis for computer software through creative research in basic software theory, algorithm theory, structured programming, and semaphores." Dijkstra was alive to receive notice of the award, but it was accepted by his family in an award ceremony after his death.

Shortly before his death in 2002, Dijkstra received the ACM PODC Influential-Paper Award in distributed computing for his work on self-stabilization of program computation. This annual award was renamed the Dijkstra Prize (Edsger W. Dijkstra Prize in Distributed Computing) the following year, in his honor.

The Dijkstra Award for Outstanding Academic Achievement in Computer Science (Loyola University Chicago, Department of Computer Science) is named for Edsger W. Dijkstra. Beginning in 2005, this award recognizes the top academic performance by a graduating computer science major. Selection is based on GPA in all major courses and election by department faculty.

The Department of Computer Science (UTCS) at the University of Texas at Austin hosted the inaugural Edsger W. Dijkstra Memorial Lecture on 12 October 2010. Tony Hoare, Emeritus Professor at Oxford and Principal Researcher at Microsoft Research, was the speaker for the event. This lecture series was made possible by a generous grant from Schlumberger to honor the memory of Dijkstra.


Books:
Selected articles:



</doc>
<doc id="10021" url="https://en.wikipedia.org/wiki?curid=10021" title="Educational perennialism">
Educational perennialism

Educational perennialism is a normative educational philosophy. Perennialists believe that one should teach the things that are of everlasting pertinence to all people everywhere, and that the emphasis should be on principles, not facts. Since people are human, one should teach first about humans, rather than machines or techniques and liberal rather than vocational topics.

Although perennialism may appear similar to essentialism, perennialism focuses first on personal development, while essentialism focuses first on essential skills. Essentialist curricula thus tend to be much more vocational and fact-based, and far less liberal and principle-based. Both philosophies are typically considered to be teacher-centered, as opposed to student-centered philosophies of education such as progressivism. However, since the teachers associated with perennialism are in a sense the authors of the Western masterpieces themselves, these teachers may be open to student criticism through the associated Socratic method, which, if carried out as true dialogue, is a balance between students, including the teacher promoting the discussion.

The word perennial in secular perennialism suggests something that lasts an indefinitely long time, recurs again and again, or is self-renewing. As promoted primarily by Robert Hutchins and Mortimer Adler, a universal curriculum based upon the common and essential nature of all human beings is recommended. This form of perennialism comprises the humanist and scientific traditions. Hutchins and Adler implemented these ideas with great success at the University of Chicago, where they still strongly influence the curriculum in the form of the undergraduate Common Core. Other notable figures in the movement include Stringfellow Barr and Scott Buchanan (who together initiated the Great Books program at St. John's College in Annapolis, Maryland), Mark Van Doren, Alexander Meiklejohn, and Sir Richard Livingstone, an English classicist with an American following.

Secular perennialists espouse the idea that education should focus on the historical development of a continually developing common oriented base of human knowledge and art, the timeless value of classic thought on central human issues by landmark thinkers, and revolutionary ideas critical to historical paradigm shifts or changes in world view. A program of studies which is highly general, nonspecialized, and nonvocational is advocated. They firmly believe that exposure of all citizens to the development of thought by those most responsible for the evolution of the occidental oriented tradition is integral to the survival of the freedoms, human rights and responsibilities inherent to a true Democracy.

Adler states: 
... our political democracy depends upon the reconstitution of our schools. Our schools are not turning out young people prepared for the high office and the duties of citizenship in a democratic republic. Our political institutions cannot thrive, they may not even survive, if we do not produce a greater number of thinking citizens, from whom some statesmen of the type we had in the 18th century might eventually emerge. We are, indeed, a nation at risk, and nothing but radical reform of our schools can save us from impending disaster... Whatever the price... the price we will pay for not doing it will be much greater.
Hutchins writes in the same vein: 
The business of saying ... that people are not capable of achieving a good education is too strongly reminiscent of the opposition of every extension of democracy. This opposition has always rested on the allegation that the people were incapable of exercising the power they demanded. Always the historic statement has been verified: you cannot expect the slave to show the virtues of the free man unless you first set him free. When the slave has been set free, he has, in the passage of time, become indistinguishable from those who have always been free ... There appears to be an innate human tendency to underestimate the capacity of those who do not belong to "our" group. Those who do not share our background cannot have our ability. Foreigners, people who are in a different economic status, and the young seem invariably to be regarded as intellectually backward ...
As with the essentialists, perennialists are educationally conservative in the requirement of a curriculum focused upon fundamental subject areas, but stress that the overall aim should be exposure to history's finest thinkers as models for discovery. The student should be taught such basic subjects as English, languages, history, mathematics, natural science, philosophy, and fine arts. Adler states: "The three R's, which always signified the formal disciplines, are the essence of liberal or general education."

Secular perennialists agree with progressivists that memorization of vast amounts of factual information and a focus on second-hand information in textbooks and lectures does not develop rational thought. They advocate learning through the development of meaningful conceptual thinking and judgement by means of a directed reading list of the profound, aesthetic, and meaningful great books of the Western canon. These books, secular perennialists argue, are written by the world's finest thinkers, and cumulatively comprise the "Great Conversation" of mankind with regard to the central human questions. Their basic argument for the use of original works (abridged translations being acceptable as well) is that these are the products of "genius". Hutchins remarks:

Great books are great teachers; they are showing us every day what ordinary people are capable of. These books come out of ignorant, inquiring humanity. They are usually the first announcements for success in learning. Most of them were written for, and addressed to, ordinary people.

It is important to note that the Great Conversation is not static, which is the impression that one might obtain from some descriptions of perennialism, a confusion with religious perennialism, or even the term perennialism itself. The Great Conversation and the set of related great books changes as the representative thought of man changes or progresses, and is therefore representative of an evolution of thought, but is not based upon the whim or fancy of the latest cultural fads. Hutchins makes this point very clear:
In the course of history... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation. ...the West needs to recapture and reemphasize and bring to bear upon its present problems the wisdom that lies in the works of its greatest thinkers and in the discussion that they have carried on.
Perennialism was a solution proposed in response to what was considered by many to be a failing educational system. Again Hutchins writes:
The products of American high schools are illiterate; and a degree from a famous college or university is no guarantee that the graduate is in any better case. One of the most remarkable features of American society is that the difference between the "uneducated" and the "educated" is so slight.
In this regard John Dewey and Hutchins were in agreement. Hutchins's book "The Higher Learning in America" deplored the "plight of higher learning" that had turned away from cultivation of the intellect and toward anti-intellectual practicality due in part, to a lust for money. In a highly negative review of the book, Dewey wrote a series of articles in "The Social Frontier" which began by applauding Hutchins' attack on "the aimlessness of our present educational scheme.

Perennialists believe that reading is to be supplemented with mutual investigations (between the teacher and the student) and minimally-directed discussions through the Socratic method in order to develop a historically oriented understanding of concepts. They argue that accurate, independent reasoning distinguishes the developed or educated mind and they thus stress the development of this faculty. A skilled teacher would keep discussions on topic and correct errors in reasoning, but it would be the class, not the teacher, who would reach the conclusions. While not directing or leading the class to a conclusion, the teacher may work to accurately formulate problems within the scope of the texts being studied.

While the standard argument for utilizing a modern text supports distillation of information into a form relevant to modern society, perennialists argue that many of the historical debates and the development of ideas presented by the great books are relevant to any society, at any time, and thus that the suitability of the great books for instructional use is unaffected by their age.

Perennialists freely acknowledge that any particular selection of great books will disagree on many topics; however, they see this as an advantage, rather than a detriment. They believe that the student must learn to recognize such disagreements, which often reflect current debates. The student becomes responsible for thinking about the disagreements and reaching a reasoned, defensible conclusion. This is a major goal of the Socratic discussions. They do not advocate teaching a settled scholarly interpretation of the books, which would cheat the student of the opportunity to learn rational criticism and to know his own mind.

Perennialism was originally religious in nature, developed first by Thomas Aquinas in the thirteenth century in his work "De Magistro" ("The Teacher").

In the nineteenth century, John Henry Newman presented a defense of religious perennialism in "The Idea of a University". Discourse 5 of that work, "Knowledge Its Own End", is a recent statement of a Christian educational perennialism.

There are several epistemological options, which affect the pedagogical options. The possibilities may be surveyed by considering four extreme positions, as indicated in the following table:




</doc>
<doc id="10024" url="https://en.wikipedia.org/wiki?curid=10024" title="MDMA">
MDMA

3,4-Methylenedioxymethamphetamine (MDMA), commonly known as ecstasy (E), is a psychoactive drug used primarily as a recreational drug. The desired recreational effects include increased empathy, euphoria, and heightened sensations. When taken by mouth, effects begin after 30–45 minutes and last 3–6 hours. , MDMA has no approved medical uses.
Adverse effects of MDMA use include addiction, memory problems, paranoia, difficulty sleeping, teeth grinding, blurred vision, sweating, and a rapid heartbeat. Use may also lead to depression and fatigue. Deaths have been reported due to increased body temperature and dehydration. MDMA increases the release and slows the reuptake of the neurotransmitters serotonin, dopamine, and norepinephrine in parts of the brain. It has stimulant and psychedelic effects. The initial increase is followed by a short-term decrease in the neurotransmitters. MDMA belongs to the substituted methylenedioxyphenethylamine and substituted amphetamine classes of drugs.
MDMA was first made in 1912. It was used to improve psychotherapy beginning in the 1970s and became popular as a street drug in the 1980s. MDMA is commonly associated with dance parties, raves, and electronic dance music. It is often sold mixed with other substances such as ephedrine, amphetamine, and methamphetamine. In 2014, between 9 and 29 million people between the ages of 15 and 64 used ecstasy (0.2% to 0.6% of the world population). This was broadly similar to the percentage of people who use cocaine, amphetamines, and opioids, but fewer than for cannabis. In the United States, about 900,000 people used ecstasy in 2010.
MDMA is generally illegal in most countries. Limited exceptions are sometimes made for research. Researchers are investigating whether MDMA may assist in treating severe, treatment-resistant posttraumatic stress disorder (PTSD). In November 2016, phase 3 clinical trials for PTSD were approved by the United States Food and Drug Administration (FDA) to assess effectiveness and safety. In 2017 the FDA granted MDMA a breakthrough therapy designation for PTSD meaning if studies show promise a review for potential medical use could occur more quickly.
In general, MDMA users report feeling the onset of subjective effects within 30–60 minutes of MDMA consumption and reaching the peak effect at 75–120 minutes, which then plateaus for about 3.5 hours. The desired short-term psychoactive effects of MDMA have been reported to include:

The experience elicited by MDMA depends on the dose, setting, and user. The variability of the induced altered state by MDMA is lower compared to other psychedelics. For example, MDMA used at parties is associated with high motor activity, reduced sense of self-identity as well as poor awareness of the background surroundings. Use of MDMA individually or in a small groups in a quiet environment and when concentrating, is associated with increased lucidity, capability of concentration, sensitivity of aesthetic aspects of the background and emotions, as well as greater capability of communication with others. In psychotherapeutic settings MDMA effects have been described by infantile ideas, alternating phases of mood, sometimes memories and moods connected with childhood experiences.

Sometimes MDMA is labelled as an “empathogenic” drug, because of its empathy-producing effects. Results of different studies show its effects of powerful empathy with others. When testing the MDMA for medium and high dosage ranges it showed increase on hedonic as well as arousal continuum. The effect of MDMA increasing sociability is consistent, however effects on empathy have been more mixed.

MDMA is often considered the drug of choice within the rave culture and is also used at clubs, festivals and house parties. In the rave environment, the sensory effects from the music and lighting are often highly synergistic with the drug. The psychedelic amphetamine quality of MDMA offers multiple reasons for its appeal to users in the rave setting. Some users enjoy the feeling of mass communion from the inhibition-reducing effects of the drug, while others use it as party fuel because of the drug's stimulatory effects. MDMA is used less frequently than other stimulants, typically less than once per week.

MDMA is sometimes taken in conjunction with other psychoactive drugs such as LSD, psilocybin mushrooms, and ketamine, an act called "candy-flipping". 

, MDMA has no accepted medical indications. Before it was widely banned, it saw limited use in therapy.

A small number of therapists continue to use MDMA in therapy despite its illegal status.

Small doses of MDMA are used as an entheogen to enhance prayer or meditation by some religious practitioners. MDMA has been used as an adjunct to New Age spiritual practices.

MDMA has become widely known as ecstasy (shortened "E", "X", or "XTC"), usually referring to its tablet form, although this term may also include the presence of possible adulterants or dilutants. The UK term "mandy" and the US term "molly" colloquially refer to MDMA in a crystalline powder form that is thought to be free of adulterants. MDMA is also sold in the form of the hydrochloride salt, either as loose crystals or in gelcaps.

In part due to the global supply shortage of sassafras oil, substances that are sold as molly frequently contain no MDMA and instead contain methylone, ethylone, MDPV, mephedrone, or any other of the group of compounds commonly known as bath salts. Powdered MDMA ranges from pure MDMA to crushed tablets with 30–40% purity. MDMA tablets typically have low purity due to bulking agents that are added to dilute the drug and increase profits (e.g., lactose) and binding agents. Tablets sold as ecstasy sometimes contain 3,4-methylenedioxyamphetamine (MDA), 3,4-methylenedioxyethylamphetamine (MDEA), other amphetamine derivatives, caffeine, opiates, or painkillers. Some tablets contain little or no MDMA. The proportion of seized ecstasy tablets with MDMA-like impurities has varied annually and by country. The average content of MDMA in a preparation is 70 to 120 mg with the purity having increased since the 1990s.

MDMA is usually consumed by mouth. It is also sometimes snorted.

Acute adverse effects are usually the result of high or multiple doses, although single dose toxicity can occur in susceptible individuals. The most serious short-term physical health risks of MDMA are hyperthermia and dehydration. Cases of life-threatening or fatal hyponatremia (excessively low sodium concentration in the blood) have developed in MDMA users attempting to prevent dehydration by consuming excessive amounts of water without replenishing electrolytes.

The immediate adverse effects of MDMA use can include:

The adverse effects that last up to a week following cessation of moderate MDMA use include:




, the long-term effects of MDMA on human brain structure and function have not been fully determined. However, there is consistent evidence of structural and functional deficits in MDMA users with a high lifetime exposure. In contrast, there is no evidence of structural or functional changes in MDMA users with only a moderate (<50 doses used and <100 tablets consumed) lifetime exposure. MDMA use at high doses has been shown to produce brain lesions, a form of brain damage, in the serotonergic neural pathways of humans and animals. It is unclear if typical MDMA users may develop neurotoxic brain lesions. Long-term exposure to MDMA in humans has been shown to produce marked neurodegeneration in striatal, hippocampal, prefrontal, and occipital serotonergic axon terminals. Neurotoxic damage to serotonergic axon terminals has been shown to persist for more than two years. Elevations in brain temperature from MDMA use are positively correlated with MDMA-induced neurotoxicity. Adverse neuroplastic changes to brain microvasculature and white matter also occur in humans using low doses of MDMA. Reduced gray matter density in certain brain structures has also been noted in human MDMA users. Global reductions in gray matter volume, thinning of the parietal and orbitofrontal cortices, and decreased hippocampal activity have been observed in long term users. The effects established so far for recreational use of ecstasy lie in the range of moderate to large effects for SERT reduction.

Impairments in multiple aspects of cognition, including attention, learning, memory, visual processing, and sleep have been found in regular MDMA users. The magnitude of these impairments is correlated with lifetime MDMA usage and are partially reversible with abstinence. Several forms of memory are impaired by chronic ecstasy use; however, the effect sizes for memory impairments in ecstasy users are generally small overall. MDMA use is also associated with increased impulsivity and depression.

Serotonin depletion following MDMA use can cause depression in subsequent days. In some cases depressive symptoms persist for longer. Some studies indicate repeated recreational users of ecstasy have increased rates of depression and anxiety, even after quitting the drug. Depression is one of the main factors for cessation of use.

At high doses, MDMA induces a neuroimmune response which, through several mechanisms, increases the permeability of the blood-brain barrier, thereby making the brain more susceptible to environmental toxins and pathogens. In addition, MDMA has immunosuppressive effects in the peripheral nervous system and pro-inflammatory effects in the central nervous system.

MDMA is a moderately teratogenic drug (i.e., it is toxic to the fetus). In utero exposure to MDMA is associated with a neuro- and cardiotoxicity and impaired motor functioning. Motor delays may be temporary during infancy or long-term. The severity of these developmental delays increases with heavier MDMA use.

Approximately 60% of MDMA users experience withdrawal symptoms when they stop taking MDMA. Some of these symptoms include fatigue, loss of appetite, depression, and trouble concentrating. Tolerance to some of the desired and adverse effects of MDMA is expected to occur with consistent MDMA use. A 2007 analysis estimated MDMA to have a psychological dependence and physical dependence potential roughly three fourths and four fifths that of cannabis.
MDMA has been shown to induce ΔFosB in the nucleus accumbens. Because MDMA releases dopamine in the striatum, the mechanisms by which it induces ΔFosB in the nucleus accumbens are analogous to other dopaminergic psychostimulants. Therefore, chronic use of MDMA at high doses can result in altered brain structure and drug addiction, which occur as a consequence of ΔFosB overexpression in the nucleus accumbens. MDMA is less addictive than other stimulants such as methamphetamine and cocaine. Compared with amphetamine, MDMA and its metabolite MDA are less reinforcing.
One study found approximately 15% of chronic MDMA users met the DSM-IV diagnostic criteria for substance dependence. However, there is little evidence for a specific diagnosable MDMA dependence syndrome because MDMA is typically used relatively infrequently.
There are currently no medications to treat MDMA addiction.

A 2007 UK study ranked MDMA 18th in harmfulness out of 20 recreational drugs. Rankings for each drug were based on the risk for acute physical harm, the propensity for physical and psychological dependency on the drug, and the negative familial and societal impacts of the drug. The authors did not evaluate or rate the negative impact of ecstasy on the cognitive health of ecstasy users (e.g. impaired memory and concentration).

MDMA overdose symptoms vary widely due to the involvement of multiple organ systems. Some of the more overt overdose symptoms are listed in the table below. The number of instances of fatal MDMA intoxication is low relative to its usage rates. In most fatalities MDMA was not the only drug involved. Acute toxicity is mainly caused by serotonin syndrome and sympathomimetic effects. MDMA's toxicity in overdose may be exacerbated by caffeine, with which it is frequently cut (mixed with to increase volume). A scheme for management of acute MDMA toxicity has been published focusing on treatment of hyperthermia, hyponatraemia, seratonin syndrome, and multiple organ failure.

A number of drug interactions can occur between MDMA and other drugs, including serotonergic drugs. MDMA also interacts with drugs which inhibit CYP450 enzymes, like ritonavir (Norvir), particularly CYP2D6 inhibitors. Concurrent use of MDMA high dosages with another serotonergic drug can result in a life-threatening condition called serotonin syndrome. Severe overdose resulting in death has also been reported in people who took MDMA in combination with certain monoamine oxidase inhibitors, such as phenelzine (Nardil), tranylcypromine (Parnate), or moclobemide (Aurorix, Manerix).

MDMA acts primarily as a presynaptic releasing agent of serotonin, norepinephrine, and dopamine, which arises from its activity at trace amine-associated receptor 1 (TAAR1) and vesicular monoamine transporter 2 (VMAT2). MDMA is a monoamine transporter substrate (i.e., a substrate for DAT, NET, and SERT), so it enters monoamine neurons via these neuronal membrane transport proteins; by acting as a monoamine transporter substrate, MDMA produces competitive reuptake inhibition at the neuronal membrane transporters (i.e., it competes with endogenous monoamines for reuptake). MDMA inhibits both vesicular monoamine transporters (VMATs), the second of which (VMAT2) is highly expressed within monoamine neurons at vesicular membranes. Once inside a monoamine neuron, MDMA acts as a VMAT2 inhibitor and a TAAR1 agonist.

Inhibition of VMAT2 by MDMA results in increased concentrations of the associated neurotransmitter (serotonin, norepinephrine, or dopamine) in the cytosol of a monoamine neuron. Activation of TAAR1 by MDMA triggers protein kinase A and protein kinase C signaling events which then phosphorylates the associated monoamine transporters – DAT, NET, or SERT – of the neuron. In turn, these phosphorylated monoamine transporters either reverse transport direction – i.e., move neurotransmitters from the cytosol to the synaptic cleft – or withdraw into the neuron, respectively producing neurotransmitter efflux and noncompetitive reuptake inhibition at the neuronal membrane transporters. MDMA has ten times more affinity for uptake at serotonin transporters compared to dopamine and norepinephrine transporters and consequently has mainly serotonergic effects.

In summary, MDMA enters monoamine neurons by acting as a monoamine transporter substrate. MDMA activity at VMAT2 moves neurotransmitters out from synaptic vesicles and into the cytosol; MDMA activity at TAAR1 moves neurotransmitters out of the cytosol and into the synaptic cleft.

MDMA also has weak agonist activity at postsynaptic serotonin receptors 5-HT and 5-HT receptors, and its more efficacious metabolite MDA likely augments this action. Cortisol, prolactin, and oxytocin quantities in serum are increased by MDMA.

MDMA is a ligand at both sigma receptor subtypes, though its efficacies at the receptors have not yet been elucidated.

MDMA reaches maximal concentrations in the blood stream between 1.5 and 3 hours after ingestion. It is then slowly metabolized and excreted, with levels of MDMA and its metabolites decreasing to half their peak concentration over the next several hours. The duration of action of MDMA is usually four to six hours, after which serotonin levels in the brain are depleted. Serotonin levels typically return to normal within 24–48 hours.

Metabolites of MDMA that have been identified in humans include 3,4-methylenedioxyamphetamine (MDA), 4-hydroxy-3-methoxymethamphetamine (HMMA), 4-hydroxy-3-methoxyamphetamine (HMA), 3,4-dihydroxyamphetamine (DHA) (also called alpha-methyldopamine (α-Me-DA)), 3,4-methylenedioxyphenylacetone (MDP2P), and 3,4-Methylenedioxy-N-hydroxyamphetamine (MDOH). The contributions of these metabolites to the psychoactive and toxic effects of MDMA are an area of active research. 80% of MDMA is metabolised in the liver, and about 20% is excreted unchanged in the urine.

MDMA is known to be metabolized by two main metabolic pathways: (1) "O"-demethylenation followed by catechol-"O"-methyltransferase (COMT)-catalyzed methylation and/or glucuronide/sulfate conjugation; and (2) "N"-dealkylation, deamination, and oxidation to the corresponding benzoic acid derivatives conjugated with glycine. The metabolism may be primarily by cytochrome P450 (CYP450) enzymes CYP2D6 and CYP3A4 and COMT. Complex, nonlinear pharmacokinetics arise via autoinhibition of CYP2D6 and CYP2D8, resulting in zeroth order kinetics at higher doses. It is thought that this can result in sustained and higher concentrations of MDMA if the user takes consecutive doses of the drug.

MDMA and metabolites are primarily excreted as conjugates, such as sulfates and glucuronides. MDMA is a chiral compound and has been almost exclusively administered as a racemate. However, the two enantiomers have been shown to exhibit different kinetics. The disposition of MDMA may also be stereoselective, with the S-enantiomer having a shorter elimination half-life and greater excretion than the R-enantiomer. Evidence suggests that the area under the blood plasma concentration versus time curve (AUC) was two to four times higher for the ("R")-enantiomer than the ("S")-enantiomer after a 40 mg oral dose in human volunteers. Likewise, the plasma half-life of ("R")-MDMA was significantly longer than that of the ("S")-enantiomer (5.8 ± 2.2 hours vs 3.6 ± 0.9 hours). However, because MDMA excretion and metabolism have nonlinear kinetics, the half-lives would be higher at more typical doses (100 mg is sometimes considered a typical dose).

MDMA is in the substituted methylenedioxyphenethylamine and substituted amphetamine classes of chemicals. As a free base, MDMA is a colorless oil insoluble in water. The most common salt of MDMA is the hydrochloride salt; pure MDMA hydrochloride is water-soluble and appears as a white or off-white powder or crystal.

There are numerous methods available to synthesize MDMA via different intermediates. The original MDMA synthesis described in Merck's patent involves brominating safrole to 1-(3,4-methylenedioxyphenyl)-2-bromopropane and then reacting this adduct with methylamine. Most illicit MDMA is synthesized using MDP2P (3,4-methylenedioxyphenyl-2-propanone) as a precursor. MDP2P in turn is generally synthesized from piperonal, safrole or isosafrole. One method is to isomerize safrole to isosafrole in the presence of a strong base, and then oxidize isosafrole to MDP2P. Another method uses the Wacker process to oxidize safrole directly to the MDP2P intermediate with a palladium catalyst. Once the MDP2P intermediate has been prepared, a reductive amination leads to racemic MDMA (an equal parts mixture of ("R")-MDMA and ("S")-MDMA). Relatively small quantities of essential oil are required to make large amounts of MDMA. The essential oil of "Ocotea cymbarum", for example, typically contains between 80 and 94% safrole. This allows 500 ml of the oil to produce between 150 and 340 grams of MDMA.

MDMA and MDA may be quantitated in blood, plasma or urine to monitor for use, confirm a diagnosis of poisoning or assist in the forensic investigation of a traffic or other criminal violation or a sudden death. Some drug abuse screening programs rely on hair, saliva, or sweat as specimens. Most commercial amphetamine immunoassay screening tests cross-react significantly with MDMA or its major metabolites, but chromatographic techniques can easily distinguish and separately measure each of these substances. The concentrations of MDA in the blood or urine of a person who has taken only MDMA are, in general, less than 10% those of the parent drug.

MDMA was first synthesized in 1912 by Merck chemist Anton Köllisch. At the time, Merck was interested in developing substances that stopped abnormal bleeding. Merck wanted to avoid an existing patent held by Bayer for one such compound: hydrastinine. Köllisch developed a preparation of a hydrastinine analogue, methylhydrastinine, at the request of fellow lab members, Walther Beckh and Otto Wolfes. MDMA (called methylsafrylamin, safrylmethylamin or N-Methyl-a-Methylhomopiperonylamin in Merck laboratory reports) was an intermediate compound in the synthesis of methylhydrastinine. Merck was not interested in MDMA itself at the time. On 24 December 1912, Merck filed two patent applications that described the synthesis and some chemical properties of MDMA and its subsequent conversion to methylhydrastinine.

Merck records indicate its researchers returned to the compound sporadically. A 1920 Merck patent describes a chemical modification to MDMA. In 1927, Max Oberlin studied the pharmacology of MDMA while searching for substances with effects similar to adrenaline or ephedrine, the latter being structurally similar to MDMA. Compared to ephedrine, Oberlin observed that it had similar effects on vascular smooth muscle tissue, stronger effects at the uterus, and no "local effect at the eye". MDMA was also found to have effects on blood sugar levels comparable to high doses of ephedrine. Oberlin concluded that the effects of MDMA were not limited to the sympathetic nervous system. Research was stopped "particularly due to a strong price increase of safrylmethylamine", which was still used as an intermediate in methylhydrastinine synthesis. Albert van Schoor performed simple toxicological tests with the drug in 1952, most likely while researching new stimulants or circulatory medications. After pharmacological studies, research on MDMA was not continued. In 1959, Wolfgang Fruhstorfer synthesized MDMA for pharmacological testing while researching stimulants. It is unclear if Fruhstorfer investigated the effects of MDMA in humans.

Outside of Merck, other researchers began to investigate MDMA. In 1953 and 1954, the United States Army commissioned a study of toxicity and behavioral effects in animals injected with mescaline and several analogues, including MDMA. Conducted at the University of Michigan in Ann Arbor, these investigations were declassified in October 1969 and published in 1973. A 1960 Polish paper by Biniecki and Krajewski describing the synthesis of MDMA as an intermediate was the first published scientific paper on the substance.

MDMA may have been in non-medical use in the western United States in 1968. An August 1970 report at a meeting of crime laboratory chemists indicates MDMA was being used recreationally in the Chicago area by 1970. MDMA likely emerged as a substitute for its analog methylenedioxyamphetamine (MDA), a drug at the time popular among users of psychedelics which was made a Schedule 1 substance in the United States in 1970.

American chemist and psychopharmacologist Alexander Shulgin reported he synthesized MDMA in 1965 while researching methylenedioxy compounds at Dow Chemical Company, but did not test the psychoactivity of the compound at this time. Around 1970, Shulgin sent instructions for N-methylated MDA (MDMA) synthesis to the founder of a Los Angeles chemical company who had requested them. This individual later provided these instructions to a client in the Midwest. Shulgin may have suspected he played a role in the emergence of MDMA in Chicago.

Shulgin first heard of the psychoactive effects of N-methylated MDA around 1975 from a young student who reported "amphetamine-like content". Around 30 May 1976, Shulgin again heard about the effects of N-methylated MDA, this time from a graduate student in a medicinal chemistry group he advised at San Francisco State University who directed him to the University of Michigan study. She and two close friends had consumed 100 mg of MDMA and reported positive emotional experiences. Following the self-trials of a colleague at the University of San Francisco, Shulgin synthesized MDMA and tried it himself in September and October 1976. Shulgin first reported on MDMA in a presentation at a conference in Bethesda, Maryland in December 1976. In 1978, he and David E. Nichols published a report on the drug's psychoactive effect in humans. They described MDMA as inducing "an easily controlled altered state of consciousness with emotional and sensual overtones" comparable "to marijuana, to psilocybin devoid of the hallucinatory component, or to low levels of MDA".

While not finding his own experiences with MDMA particularly powerful, Shulgin was impressed with the drug's disinhibiting effects and thought it could be useful in therapy. Believing MDMA allowed users to strip away habits and perceive the world clearly, Shulgin called the drug "window". Shulgin occasionally used MDMA for relaxation, referring to it as "my low-calorie martini", and gave the drug to friends, researchers, and others who he thought could benefit from it. One such person was Leo Zeff, a psychotherapist who had been known to use psychedelic substances in his practice. When he tried the drug in 1977, Zeff was impressed with the effects of MDMA and came out of his semi-retirement to promote its use in therapy. Over the following years, Zeff traveled around the United States and occasionally to Europe, eventually training an estimated four thousand psychotherapists in the therapeutic use of MDMA. Zeff named the drug "Adam", believing it put users in a state of primordial innocence.

Psychotherapists who used MDMA believed the drug eliminated the typical fear response and increased communication. Sessions were usually held in the home of the patient or the therapist. The role therapist was minimized in favor of patient self-discovery accompanied by MDMA induced feelings of empathy. Depression, substance abuse, relationship problems, premenstrual syndrome, and autism were among several psychiatric disorders MDMA assisted therapy was reported to treat. According to psychiatrist George Greer, therapists who used MDMA in their practice were impressed by the results. Anecdotally, MDMA was said to greatly accelerate therapy.

In the late 1970s and early 1980s, "Adam" spread through personal networks of psychotherapists, psychiatrists, users of psychedelics, and yuppies. Hoping MDMA could avoid criminalization like LSD and mescaline, psychotherapists and experimenters attempted to limit the spread of MDMA and information about it while conducting informal research. Early MDMA distributors were deterred from large scale operations by the threat of possible legislation. Between the 1970s and the mid-1980s, this network of MDMA users consumed an estimated 500,000 doses.

A small recreational market for MDMA developed by the late 1970s, consuming perhaps 10,000 doses in 1976. By the early 1980s MDMA was being used in Boston and New York City nightclubs such as Studio 54 and Paradise Garage. Into the early 1980s, as the recreational market slowly expanded, production of MDMA was dominated by a small group of therapeutically minded Boston chemists. Having commenced production in 1976, this "Boston Group" did not keep up with growing demand and shortages frequently occurred.

Perceiving a business opportunity, Michael Clegg, the Southwest distributor for the Boston Group, started his own "Texas Group" backed financially by Texas friends. In 1981, Clegg had coined "Ecstasy" as a slang term for MDMA to increase its marketability. Starting in 1983, the Texas Group mass-produced MDMA in a Texas lab or imported it from California and marketed tablets using pyramid sales structures and toll-free numbers. MDMA could be purchased via credit card and taxes were paid on sales. Under the brand name "Sassyfras", MDMA tablets were sold in brown bottles. The Texas Group advertised "Ecstasy parties" at bars and discos, describing MDMA as a "fun drug" and "good to dance to". MDMA was openly distributed in Austin and Dallas-Fort Worth area bars and nightclubs, becoming popular with yuppies, college students, and gays.

Recreational use also increased after several cocaine dealers switched to distributing MDMA following experiences with the drug. A California laboratory that analyzed confidentially submitted drug samples first detected MDMA in 1975. Over the following years the number of MDMA samples increased, eventually exceeding the number of MDA samples in the early 1980s. By the mid-1980s, MDMA use had spread to colleges around the United States.

In an early media report on MDMA published in 1982, a Drug Enforcement Administration (DEA) spokesman stated the agency would ban the drug if enough evidence for abuse could be found. By mid-1984, MDMA use was becoming more noticed. Bill Mandel reported on "Adam" in a 10 June San Francisco Chronicle article, but misidentified the drug as methyloxymethylenedioxyamphetamine (MMDA). In the next month, the World Health Organization identified MDMA as the only substance out of twenty phenethylamines to be seized a significant number of times.

After a year of planning and data collection, MDMA was proposed for scheduling by the DEA on 27 July 1984 with a request for comments and objections. The DEA was surprised when a number of psychiatrists, psychotherapists, and researchers objected to the proposed scheduling and requested a hearing. In a Newsweek article published the next year, a DEA pharmacologist stated that the agency had been unaware of its use among psychiatrists. An initial hearing was held on 1 February 1985 at the DEA offices in Washington, D.C. with administrative law judge Francis L. Young presiding. It was decided there to hold three more hearings that year: Los Angeles on 10 June, Kansas City, Missouri on 10–11 July, and Washington, D.C. on 8–11 October.

Sensational media attention was given to the proposed criminalization and the reaction of MDMA proponents, effectively advertising the drug. In response to the proposed scheduling, the Texas Group increased production from 1985 estimates of 30,000 tablets a month to as many as 8,000 per day, potentially making two million ecstasy tablets in the months before MDMA was made illegal. By some estimates the Texas Group distributed 500,000 tablets per month in Dallas alone. According to one participant in an ethnographic study, the Texas Group produced more MDMA in eighteen months than all other distribution networks combined across their entire histories. By May 1985, MDMA use was widespread in California, Texas, southern Florida, and the northeastern United States. According to the DEA there was evidence of use in twenty-eight states and Canada. Urged by Senator Lloyd Bentsen, the DEA announced an emergency Schedule I classification of MDMA on 31 May 1985. The agency cited increased distribution in Texas, escalating street use, and new evidence of MDA (an analog of MDMA) neurotoxicity as reasons for the emergency measure. The ban took effect one month later on 1 July 1985 in the midst of Nancy Reagan's "Just Say No" campaign.

As a result of several expert witnesses testifying that MDMA had an accepted medical usage, the administrative law judge presiding over the hearings recommended that MDMA be classified as a Schedule III substance. Despite this, DEA administrator John C. Lawn overruled and classified the drug as Schedule I. Later Harvard psychiatrist Lester Grinspoon sued the DEA, claiming that the DEA had ignored the medical uses of MDMA, and the federal court sided with Grinspoon, calling Lawn's argument "strained" and "unpersuasive", and vacated MDMA's Schedule I status. Despite this, less than a month later Lawn reviewed the evidence and reclassified MDMA as Schedule I again, claiming that the expert testimony of several psychiatrists claiming over 200 cases where MDMA had been used in a therapeutic context with positive results could be dismissed because they weren't published in medical journals. No double blind studies had yet been conducted as to the efficacy of MDMA for therapy.

While engaged in scheduling debates in the United States, the DEA also pushed for international scheduling. In 1985 the World Health Organization's Expert Committee on Drug Dependence recommended that MDMA be placed in Schedule I of the 1971 United Nations Convention on Psychotropic Substances. The committee made this recommendation on the basis of the pharmacological similarity of MDMA to previously scheduled drugs, reports of illicit trafficking in Canada, drug seizures in the United States, and lack of well-defined therapeutic use. While intrigued by reports of psychotherapeutic uses for the drug, the committee viewed the studies as lacking appropriate methodological design and encouraged further research. Committee chairman Paul Grof dissented, believing international control was not warranted at the time and a recommendation should await further therapeutic data. The Commission on Narcotic Drugs added MDMA to Schedule I of the convention on 11 February 1986.

The use of MDMA in Texas clubs declined rapidly after criminalization, although by 1991 the drug remained popular among young middle-class whites and in nightclubs. In 1985, MDMA use became associated with Acid House on the Spanish island of Ibiza. Thereafter in the late 1980s, the drug spread alongside rave culture to the UK and then to other European and American cities. Illicit MDMA use became increasingly widespread among young adults in universities and later, in high schools. Since the mid-1990s, MDMA has become the most widely used amphetamine-type drug by college students and teenagers. MDMA became one of the four most widely used illicit drugs in the US, along with cocaine, heroin, and cannabis.
According to some estimates as of 2004, only marijuana attracts more first time users in the US.

After MDMA was criminalized, most medical use stopped, although some therapists continued to prescribe the drug illegally. Later, Charles Grob initiated an ascending-dose safety study in healthy volunteers. Subsequent legally-approved MDMA studies in humans have taken place in the US. in Detroit (Wayne State University), Chicago (University of Chicago), San Francisco (UCSF and California Pacific Medical Center), Baltimore (NIDA–NIH Intramural Program), and South Carolina, as well as in Switzerland (University Hospital of Psychiatry, Zürich), the Netherlands (Maastricht University), and Spain (Universitat Autònoma de Barcelona).

"Molly", short for 'molecule', was recognized as a slang term for crystalline or powder MDMA in the 2000s.

In 2010, the BBC reported that use of MDMA had decreased in the UK in previous years. This may be due to increased seizures during use and decreased production of the precursor chemicals used to manufacture MDMA. Unwitting substitution with other drugs, such as mephedrone and methamphetamine, as well as legal alternatives to MDMA, such as BZP, MDPV, and methylone, are also thought to have contributed to its decrease in popularity.
MDMA is legally controlled in most of the world under the UN Convention on Psychotropic Substances and other international agreements, although exceptions exist for research and limited medical use. In general, the unlicensed use, sale or manufacture of MDMA are all criminal offences.

In Australia, MDMA was declared an illegal substance in 1986 because of its harmful effects and potential for abuse. It is classed as a Schedule 9 Prohibited Substance in the country, meaning it is available for scientific research purposes only. Any other type of sale, use or manufacture is strictly prohibited by law. Permits for research uses on humans must be approved by a recognized ethics committee on human research.

In Western Australia under the Misuse of Drugs Act 1981 4.0g of MDMA is the amount required determining a court of trial, 2.0g is considered a presumption with intent to sell or supply and 28.0g is considered trafficking under Australian law.

In the United Kingdom, MDMA was made illegal in 1977 by a modification order to the existing Misuse of Drugs Act 1971. Although MDMA was not named explicitly in this legislation, the order extended the definition of Class A drugs to include various ring-substituted phenethylamines. The drug is therefore illegal to sell, buy, or possess without a licence in the UK. Penalties include a maximum of seven years and/or unlimited fine for possession; life and/or unlimited fine for production or trafficking.

Some researchers such as David Nutt have criticized the current scheduling of MDMA, what he determined to be a relatively harmless drug. An editorial he wrote in the Journal of Psychopharmacology, where he compared the risk of harm for horse riding (1 adverse event in 350) to that of ecstasy (1 in 10,000) resulted in his dismissal as well as the resignation of his colleagues from the ACMD.

In the United States, MDMA is currently placed in Schedule I of the Controlled Substances Act. In a 2011 federal court hearing the American Civil Liberties Union successfully argued that the sentencing guideline for MDMA/ecstasy is based on outdated science, leading to excessive prison sentences. Other courts have upheld the sentencing guidelines. The United States District Court for the Eastern District of Tennessee explained its ruling by noting that "an individual federal district court judge simply cannot marshal resources akin to those available to the Commission for tackling the manifold issues involved with determining a proper drug equivalency."

In the Netherlands, the Expert Committee on the List (Expertcommissie Lijstensystematiek Opiumwet) issued a report in June 2011 which discussed the evidence for harm and the legal status of MDMA, arguing in favor of maintaining it on List I.

In Canada, MDMA is listed as a Schedule 1 as it is an analogue of amphetamine. The CDSA was updated as a result of the Safe Streets Act changing amphetamines from Schedule III to Schedule I in March 2012.

In 2014, 3.5% of 18 to 25 year-olds had used MDMA in the United States. In Europe, an estimated 37% of regular club-goers aged 14 to 35 used MDMA in the past year according to the 2015 European Drug report. The highest one-year prevalence of MDMA use in Germany in 2012 was 1.7% among people aged 25 to 29 compared with a population average of 0.4%. Among adolescent users in the United States between 1999 and 2008, girls were more likely to use MDMA than boys.

In 2008 the European Monitoring Centre for Drugs and Drug Addiction noted that although there were some reports of tablets being sold for as little as €1, most countries in Europe then reported typical retail prices in the range of €3 to €9 per tablet, typically containing 25–65 mg of MDMA. By 2014 the EMCDDA reported that the range was more usually between €5 and €10 per tablet, typically containing 57–102 mg of MDMA, although MDMA in powder form was becoming more common.

The United Nations Office on Drugs and Crime stated in its 2014 World Drug Report that US ecstasy retail prices range from US$1 to $70 per pill, or from $15,000 to $32,000 per kilogram. A new research area named Drug Intelligence aims to automatically monitor distribution networks based on image processing and machine learning techniques, in which an Ecstasy pill picture is analyzed to detect correlations among different production batches. These novel techniques allow police scientists to facilitate the monitoring of illicit distribution networks.

, most of the MDMA in the United States is produced in British Columbia, Canada and imported by Canada-based Asian transnational criminal organizations. The market for MDMA in the United States is relatively small compared to methamphetamine, cocaine, and heroin. In the United States, about 0.9 million people used ecstasy in 2010.

MDMA is particularly expensive in Australia, costing A$15–A$30 per tablet. In terms of purity data for Australian MDMA, the average is around 34%, ranging from less than 1% to about 85%. The majority of tablets contain 70–85 mg of MDMA. Most MDMA enters Australia from the Netherlands, the UK, Asia, and the US.

A number of ecstasy manufacturers brand their pills with a logo, often being the logo of an unrelated corporation. Some pills depict logos of products or shows popular with children, such as Shaun the Sheep.

The Multidisciplinary Association for Psychedelic Studies (MAPS) is funding pilot studies and clinical trials investigating the use of MDMA in psychotherapy to treat posttraumatic stress disorder (PTSD), social anxiety in autistic adults, and anxiety in terminal illness. In November 2016, the United States Food and Drug Administration (FDA) approved large-scale phase 3 clinical trials involving the use of MDMA for the treatment of PTSD in individuals who do not respond to traditional prescription drugs or psychotherapy. MDMA has also been proposed as an adjunct to substance abuse treatment. In 2017, doctors in the UK began the first clinical study of MDMA in alcohol addiction.

The potential for MDMA to be used as a rapid-acting antidepressant has been studied in clinical trials, but as of 2017 the evidence on efficacy and safety were insufficient to reach a conclusion. A 2014 review of the safety and efficacy of MDMA as a treatment for various disorders, particularly PTSD, indicated that MDMA has therapeutic efficacy in some patients; however, it emphasized that issues regarding the control-ability of MDMA-induced experiences and neurochemical recovery must be addressed. The author noted that oxytocin and -cycloserine are potentially safer co-drugs in PTSD treatment, albeit with limited evidence of efficacy. This review and a second corroborating review by a different author both concluded that, because of MDMA's demonstrated potential to cause lasting harm in humans (e.g., serotonergic neurotoxicity and persistent memory impairment), "considerably more research must be performed" on its efficacy in PTSD treatment to determine if the potential treatment benefits outweigh its potential to harm to a patient.



</doc>
<doc id="10025" url="https://en.wikipedia.org/wiki?curid=10025" title="Flag of Europe">
Flag of Europe

The European Flag is an official symbol of two separate organisations—the Council of Europe (CoE) and the European Union (EU). It consists of a circle of twelve five-pointed yellow (or) stars on a blue (azure) field. 

The flag was designed in 1955, and officially launched later that year by the Council of Europe as a symbol for the whole of Europe. The Council of Europe urged it to be adopted by other European organisations, and in 1985 the European Communities (EC) adopted it.

The EU inherited the flag's use when it was formed in 1993, being the successor organisation to the EC. It has been in wide official use by the EU since the 1990s, but it has never been given official status in any of the EU's treaties. Its adoption as an official symbol of the EU was planned as part of the proposed European Constitution, which failed to be ratified in 2005. Alternatively, it is sometimes called the "Flag of the European Union" when representing the EU. 

Since its adoption by the European Union, it has become broadly associated with the supranational organisation, due to its high profile and heavy usage of the emblem. It has also been used by pro-EU protestors in the colour revolutions of the 2000s, e.g., in Belarus (2004) or Moldova.
There are also a number of derivative designs used as logos or flags of other European organisations, and in the flags of the Republic of Kosovo (2008) and of Bosnia and Herzegovina (1998).

The flag is rectangular with 2:3 proportions: its fly (width) is one and a half times the length of its hoist (height). Twelve yellow stars are centred in a circle (the radius of which is a third of the length of the hoist) upon a blue background. All the stars are upright (one point straight up), have five points and are spaced equally according to the hour positions on the face of a clock. The diameter of each star is equal to one-ninth of the height of the hoist.

The graphical specifications given by the EU describe the design as: "On an azure field a circle of twelve golden mullets, their points not touching." The Council of Europe gives the flag a symbolic description in the following terms:

The base colour of the flag is a dark blue (reflex blue, a mix of cyan and magenta), while the golden stars are portrayed in yellow. The colours are regulated according to the Pantone colouring system (see table for specifications).

A large number of designs were proposed for the flag before the current flag was agreed. The rejected proposals are preserved in the Council of Europe Archives in a page dedicated to the history of the flag. One of these consists of a design of white stars on a light blue field, as a gesture to the peace and internationalism of the United Nations. An official website makes a reference to blue and gold being the original colours of Richard von Coudenhove-Kalergi, who proposed a Paneuropean Union in 1923, and was an active proponent of the early Community.

The number of stars on the flag is fixed at 12, and is not related to the number of member states of the EU (although the EU did have 12 member states at the time of Maastricht Treaty). This is because it originally was the flag of the Council of Europe. In 1953, the Council of Europe had 15 members; it was proposed that the future flag should have one star for each member, and would not change based on future members. West Germany objected to this as one of the members was the disputed area of Saarland, and to have its own star would imply sovereignty for the region. Twelve was eventually adopted as a number with no political connotations and as a symbol of unity. While 12 is the correct number of stars, sometimes flags or emblems can be found that incorrectly show 15 (as of the rejected proposal) or 25 (as suggested by some after the expansion of the EU to 25 member states in 2004). However, the flag also remains that of the Council of Europe, which now has 47 member states.

The search for a symbol began in 1950 when a committee was set up in order to look into the question of a European flag. There were numerous proposals but a clear theme for stars and circles emerged. Richard von Coudenhove-Kalergi proposed that they adopt the flag of his International Paneuropean Union, which was a blue field, with a red cross inside an orange circle at the centre, which he had himself recently adopted for the European Parliamentary Union. Due to the cross symbolism, this was rejected by Turkey (a member of the Council of Europe since 1949). Kalergi then suggested adding a crescent to the cross design, to overcome the Muslim objections. Another organisation's flag was the European Movement, which had a large green E on a white background. A further design was one based on the Olympic rings: eight silver rings on a blue background. It was rejected due to the rings' similarity with "dial", "chain" and "zeros". One proposal had a large yellow star on a blue background, but it was rejected due to its similarity with the so-called Burnet flag and the flag of the Belgian Congo.

The Consultative Assembly narrowed their choice to two designs. One was by Salvador de Madariaga, the founder of the College of Europe, who suggested a constellation of stars on a blue background (positioned according to capital cities, with a large star for Strasbourg, the seat of the Council). He had circulated his flag round many European capitals and the concept had found favour. The second was a variant by Arsène Heitz, who worked for the Council's postal service and had submitted dozens of designs; the design of his that was accepted by the Assembly was similar to Salvador de Madariaga's, but rather than a constellation, the stars were arranged in a circle. In 1987, Heitz claimed that his inspiration had been the crown of twelve stars of the Woman of the Apocalypse, often found in Marian iconography (see below).

The Consultative Assembly favoured Heitz's design. However, the flag the Assembly chose had fifteen stars, reflecting the number of states of the Council of Europe. The Consultative Assembly chose this flag and recommended the Council of Europe to adopt it. The Committee of Ministers (the Council's main decision making body) agreed with the Assembly that the flag should be a circle of stars, but the number was a source of contention. The number twelve was chosen, and Paul M. G. Lévy drew up the exact design of the new flag as it is today. The Parliamentary Assembly of the Council of Europe approved it on 25 October 1955. Adopted on 8 December 1955, the flag was unveiled at the Château de la Muette in Paris on 13 December 1955.

Following Expo 58 in Brussels, the flag caught on and the Council of Europe lobbied for other European organisations to adopt the flag as a sign of European unity. 
The European Parliament took the initiative in seeking a flag to be adopted by the European Communities. Shortly after the first direct elections in 1979 a draft resolution was put forward on the issue. The resolution proposed that the Communities' flag should be that of the Council of Europe and it was adopted by the Parliament on 11 April 1983.

The June 1984 European Council (the Communities' leaders) summit in Fontainebleau stressed the importance of promoting a European image and identity to citizens and the world. The following year, meeting in Milan, the 28–29 June European Council approved a proposal from the Committee on a People’s Europe (Adonnino Committee) in favour of the flag and adopted it. Following the permission of the Council of Europe, the Communities began to use it from 1986, with it being raised outside the Berlaymont building (the seat of the European Commission) for the first time on 29 May 1986. 

Prior to development of political institutions, flags representing Europe were limited to unification movements. The most popular were the European Movement's large green 'E' on a white background, and the "Pan European flag" (see "Creation" below). With the development of institutions, aside from the Council of Europe, came other emblems and flags. None were intended to represent wider Europe and have since been replaced by the current flag of Europe.

The first major organisation to adopt one was the European Coal and Steel Community (ECSC), which merged into the European Communities. The ECSC was created in 1952 and the flag of the ECSC was unveiled in 1958 Expo in Brussels.

The flag had two stripes, blue at the top, black at the bottom with six gold (silver after 1973) stars, three on each stripe. Blue was for steel, black for coal and the stars were the six member-states. The stars increased with the members until 1986 when they were fixed at twelve. When the ECSC treaty expired in 2002, the flag was lowered outside the European Commission in Brussels and replaced with the European flag.
The European Parliament also used its own flag from 1973, but never formally adopted it. It fell out of use with the adoption of the twelve star flag by the Parliament in 1983. The flag followed the yellow and blue colour scheme however instead of twelve stars there were the letters EP and PE (initials of the European Parliament in the six community languages at the time) surrounded by a wreath.

In 2002, Dutch architect Rem Koolhaas and his architecture firm Office for Metropolitan Architecture (OMA) designed a new flag in response to Commission President Romano Prodi's request to find ways of rebranding the Union in a way that represents Europe's "diversity and unity". The proposed new design was dubbed the "barcode", as it displays the colours of every European flag (of the then 15 members) as vertical stripes, approximately in geographical order from west to east. As well as the barcode comparison, it had been compared unfavourably to wallpaper, a TV test card, and deckchair fabric. Unlike the current flag, it would change to reflect the member states.

It was never officially adopted by the EU or any organisation; however, it was used as the logo of the Austrian EU Presidency in 2006. It had been updated with the colours of the 10 members who had joined since the proposal, and was designed by Koolhaas's firm. Its described aim is "to portray Europe as the common effort of different nations, with each retaining its own unique cultural identity". There were initially some complaints, as the stripes of the flag of Estonia were displayed incorrectly.

The European Union, which was established by the Maastricht Treaty in 1992 to replace the European Communities and encompass its functions, has retained use of the flag.

A Framework Agreement establishing the legal basis for cooperation between the European Space Agency and the European Union came into force in May 2004; already in April 2004, the European flag was flown on behalf of the European Space Agency, by astronaut André Kuipers while on board the International Space Station. Following the 2004 Summer Olympics, President Romano Prodi pointed out that the combined medal total of the European Union was far greater than that of any other country and called for EU athletes to fly the European flag at the following games alongside their own as a sign of solidarity. Use of the flag has also been reported as representing the European team at the Ryder Cup golf competition in the early 2000s, although most European participants preferred to use their own national flags.

The official status of the flag as a symbol of the European Union was to be formalised as part of the Constitution of the European Union. However, as the proposed constitution failed ratification, the mention of all state-like emblems, including the flag, were removed from the replacement Treaty of Lisbon of 2007.
Instead, a separate declaration by sixteen Member States was included in the final act of the Treaty of Lisbon stating that the flag, the anthem, the motto and the currency and Europe Day "will for them continue as symbols to express the sense of community of the people in the European Union and their allegiance to it."

In reaction to the removal of the flag from the treaty, the European Parliament, which had supported the inclusion of such symbols, backed a proposal to use these symbols "more often" on behalf of the Parliament itself; Jo Leinen, MEP for Germany, suggested that the Parliament should "take the "avant-garde"" in their use. 

In September 2008, the Parliament's Committee on Constitutional Affairs proposed a formal change in the institution's rules of procedure to make "better use of the symbols". Specifically, the flag would be present in all meeting rooms (not just the hemicycle) and at all official events. The proposal was passed on 8 October 2008 by 503 votes to 96 (15 abstentions).

The flag was used as a banner for "pro-Europeanism" outside the Union, for example in several of the "colour revolutions" during the 2000s. In Belarus, it was used on protest marches alongside the banned former national flag and flags of opposition movements during the protests of 2004–2006. The flag was used widely in a 2007 European March in Minsk as protesters rallied in support of democracy and accession to the EU. 

In Georgia, the flag was on most government buildings since the coming to power of Mikhail Saakashvili (2007), who used it during his inauguration, stating: "[the European] flag is Georgia’s flag as well, as far as it embodies our civilisation, our culture, the essence of our history and perspective, and our vision for the future of Georgia." 

It was used in 2008 by pro-western Serbian voters ahead of an election.

The flag became a symbol of European integration of Ukraine in the 2010s, particularly after Euromaidan. Ukraine is not a part of the EU but is a member of the Council of Europe. The flag is used by the Cabinet of Ukraine, Prime Minister of Ukraine, and MFA UA during official meetings. It was flown during the 2013 Euromaidan protests in Ukraine.

The Council of Europe, and in a web page archived in 2002 expressed its satisfaction with the "growing awareness of the European flag and emblem among European citizens", stating that with the adoption of the flag by the European Union, both "[t]he European Commission and the Council of Europe are responsible for ensuring that all uses of this symbol respect the dignity of the European flag and emblem".
According to the EU web portal, the flag should be taken to symbolise "both the European Union and, more broadly, the identity and unity of Europe". All EU institutions, bodies and agencies have their own logo or emblem, albeit often inspired by the flag's design and colours. As part of the EU's usage, the flag appears on the euro banknotes. Euro coins also display the twelve stars of the flag on both the national and common sides and the flag is sometimes used as an indication of the currency or the eurozone (a collective name for those countries that use the Euro). The flag appears also on many driver's licenses and vehicle registration plates issued in the Union.

It is mandatory for the flag to be used in every official speech made by the President of the European Council and it is often used at official meetings between the leaders of an EU state and a non-EU state (the national flag and European flag appearing together). While normally the national flag takes precedence over the European flag in the national context, meetings between EU leaders sometimes differ. For example, the Italian flag code as of 2008 expressly replaces the Italian flag with the European flag in precedence when dignitaries from other EU countries visit – for example the EU flag would be in the middle of a group of three flags rather than the Italian flag.
The flag is usually flown by the government of the country holding the rotating presidency Council of Ministers.

The design of the European flag was displayed on the Eiffel Tower in Paris to celebrate the French Presidency of the Council of the EU in the second half of 2008.
In 2009, the Czech President Václav Klaus, a eurosceptic, refused to fly the flag from his castle. In response, Greenpeace projected an image of the flag onto the castle and attempted to fly the flag from the building themselves.

Some members also have their own rules regarding the use of the flag alongside their national flag on domestic occasions, for example the obligatory use alongside national flags outside police stations or local government buildings. As an example according to the Italian laws it is mandatory for most public offices and buildings to hoist the European Flag alongside the Italian national Flag (Law 22/2000 and Presidential Decree 121/2000). Outside official use, the flag may not be used for aims incompatible with European values.

In national usage, national protocol usually demands the national flag takes precedence over the European flag (which is usually displayed to the right of the national flag from the observer's perspective). On occasions where the European flag is flown alongside all national flags (for example, at a European Council meeting), the national flags are placed in alphabetical order (according to their name in the main language of that state) with the European flag either at the head, or the far-right, of the order of flags.

Extraordinary flying of the flag is common on the EU's flag day, known as Europe Day, which is celebrated annually on 9 May. On Europe Day 2008, the flag was flown for the first time above the German Reichstag.

In addition to the flags use by the government and people, the flag is also used in EU military operations; however, it is not used as a civil ensign. In 2003, a member of the European Parliament tabled a proposal in a temporary committee of the European Parliament that national civil ensigns be defaced with the European flag. This proposal was rejected by Parliament in 2004, and hence the European flag is not used as a European civil ensign.

Despite not having a civil ensign, the EU's Fishery Inspection teams display a blue and yellow pennant. The pennant is flown by inspection vessels in EU waters. The flag is triangular and quartered blue and yellow and was adopted according to "EEC Regulation #1382/87" on 20 May 1978. There are no other variants or alternative flags used by the EU (in contrast to countries which have presidential, naval and military variants).

The design of the European flag has been used in a variation, such as that of the Council of Europe mentioned above, and also to a greater extent such as the flag of the Western European Union (WEU; now defunct), which uses the same colours and the stars but has a number of stars based on membership and in a semicircle rather than a circle. It is also defaced with the initials of the former Western European Union in two languages.

The flag of Bosnia and Herzegovina does not have such a strong connection as the WEU flag, but was partly inspired by the European involvement in, and aspirations of, Bosnia and Herzegovina. It uses the same blue and yellow colours and the stars, although of a different number and colour, are a direct reference to those of the European flag.

Likewise, the Republic of Kosovo uses blue, yellow and stars in its flag, which has been mocked as "a none too subtle nod to the flag of the European Union, which is about to become Kosovo's new best friend as it takes over protector status from the United Nations". 
The flag of the Brussels-Capital Region consists of a yellow iris with a white outline upon a blue background. Its colours are based on the colours of the Flag of Europe, because Brussels is considered the unofficial capital of the EU.

The national flag of Cape Verde also shows similarity to the flag of the European Union. The flag is made of a circular formation of ten yellow stars on a dark blue background and a band of white and red. The stars represent the main islands of the nation (a chain of islands off the coast of West Africa). The blue represents the ocean and the sky. The band of white and red represents the road toward the construction of the nation, and the colours stand for peace (white) and effort (red). The flag was adopted on 22 September 1992.

Other labels take reference to the European flag such as the EU organic food label that uses the twelve stars but reorders them into the shape of a leaf on a green background. The original logo of the European Broadcasting Union used the twelve stars on a blue background adding ray beams to connect the countries.

In 1987, following the adoption of the flag by the EEC, Arsène Heitz (1908–89), one of the designers who had submitted proposals for the flag's design, suggested a religious inspiration for it. He claimed that the circle of stars was based on the iconographic tradition of showing the Blessed Virgin Mary as the Woman of the Apocalypse, wearing a "crown of twelve stars".
The French satirical magazine ' reacted to Heitz's statement with an article entitled ' ("Europe Raped by the Blessed Virgin") in the 20 December 1989 edition.
Heitz also made a connection to the date of the flag's adoption, 8 December 1955, coinciding with the Catholic Feast of the Immaculate Conception of the Blessed Virgin Mary.

Paul M. G. Lévy, then Director of Information at the Council of Europe responsible for designing the flag, in a 1989 statement maintained that he had not been aware of any religious connotations.

In an interview given 26 February 1998, Lévy denied not only awareness of the "Marian" connection, but also denied that the final design of a circle of twelve stars was Heitz's. To the question "Who really designed the flag?" Lévy replied:

Carlo Curti Gialdino (2005) has reconstructed the design process to the effect that Heitz's proposal contained varying numbers of stars, from which the version with twelve stars was chosen by the Committee of Ministers meeting at Deputy level in January 1955 as one out of two remaining candidate designs.

Lévy's 1998 interview apparently gave rise to a new variant of the "Marian" anecdote. An article published in "" in August 1998 alleged that it was Lévy himself who was inspired to introduce a "Marian" element as he walked past a statue of the Blessed Virgin Mary.

An article posted in ' in February 2000 further connected the donation of a stained glass window for Strasbourg Cathedral by the Council of Europe on 21 October 1956. This window, a work by Parisian master Max Ingrand, shows a blessing Madonna underneath a circle of 12 stars on dark blue ground. The overall design of the Madonna is inspired by the banner of the cathedral's ', and the twelve stars are found on the statue venerated by this congregation inside the cathedral (twelve is also the number of members of the congregation's council). The Regional Office for Cultural Affairs describe this stained glass window called "Le vitrail de l'Europe de Max Ingand" (The Glass Window of Europe of Max Ingand). 



</doc>
<doc id="10026" url="https://en.wikipedia.org/wiki?curid=10026" title="Anthem of Europe">
Anthem of Europe

"Anthem of Europe" is the anthem of the Council of Europe and the European Union. It is based on "Ode to Joy" from the final movement of Beethoven's 9th Symphony composed in 1823, and is played on official occasions by both organisations.

Friedrich Schiller wrote the poem "An die Freude" ("To Joy") in 1785 as a "celebration of the brotherhood of man". In later life, the poet was contemptuous of this popularity and dismissed the poem as typical of "the bad taste of the age" in which it had been written. After Schiller's death, the poem provided the words for the choral movement of Ludwig van Beethoven's 9th Symphony.

In 1971 the Parliamentary Assembly of the Council of Europe decided to propose adopting the prelude to the "Ode to Joy" from Beethoven's 9th Symphony as the European anthem, taking up a suggestion made by Richard von Coudenhove-Kalergi in 1955. Beethoven was generally seen as the natural choice for a European anthem. The Committee of Ministers of the Council of Europe officially announced the European Anthem on 19 January 1972 at Strasbourg: the prelude to "Ode to Joy", 4th movement of Ludwig van Beethoven's 9th symphony. In 1974 the same piece of music was adopted as the national anthem of the unrecognized state of Rhodesia.

Conductor Herbert von Karajan was asked to write three instrumental arrangements – for solo piano, for wind instruments and for symphony orchestra and he conducted the performance used to make the official recording. He wrote his decisions on the score, notably those concerning the tempo. Karajan decided on minim (half note) = 80 whereas Beethoven had written crotchet (quarter note) = 120.

The anthem was launched via a major information campaign on Europe Day in 1972. In 1985, it was adopted by EU heads of state and government as the official anthem of the then European Community – since 1993 the European Union. It is not intended to replace the national anthems of the member states but rather to celebrate the values they all share and their unity in diversity. It expresses the ideals of a united Europe: freedom, peace, and solidarity.

It was to have been included in the European Constitution along with the other European symbols; however, the treaty failed ratification and was replaced by the Treaty of Lisbon, which does not include any symbols. A declaration was attached to the treaty, in which sixteen member states formally recognised the proposed symbols. In response, the European Parliament decided that it would make greater use of the anthem, for example at official occasions. In October 2008, the Parliament changed its rules of procedure to have the anthem played at the opening of Parliament after elections and at formal sittings.

"Ode to Joy" is the anthem of the Council of Europe and the European Union, promoted as a symbol for the whole of Europe as are the other European symbols. It is used on occasions such as Europe Day and formal events such as the signing of treaties. The European Parliament seeks to make greater use of the music, then-Parliament President Hans-Gert Pöttering stated he was moved when the anthem was played for him on his visit to Israel and ought to be used in Europe more often.

In 2008 it was used by Kosovo as its national anthem until it adopted its own, and it was played at its declaration of independence, as a nod to the EU's role in its independence from Serbia.

At the 2007 signing ceremony for the Treaty of Lisbon, the plenipotentiaries of the European Union's twenty-seven member states stood in attendance while the "Ode to Joy" was played and a choir of 26 Portuguese children sang the original German lyrics.

The German public radio station Deutschlandfunk has broadcast the anthem together with the Deutschlandlied shortly before midnight since New Year's Eve 2006. The two anthems were specially recorded by the Berlin Radio Symphony Orchestra in versions characterized by "modesty and intensity".

In 1992 the anthem was used by CIS national football team at the 1992 UEFA European Football Championship.

On 4 October 2010 the anthem was used when a European team beat a team representing the United States of America to win the Ryder Cup golf tournament. The European Ryder Cup captain Colin Montgomerie decided to break with tradition and play the European anthem by itself instead of the individual anthems from participating European nations. It was similarly employed at the 2014 Ryder Cup prizegiving ceremony on 28 September, after Europe had beaten America under its captain, Paul McGinley.

"Ode to Joy" is used as the theme song to the 2016 UEFA Euro qualifying and the European qualifying of the 2018 FIFA World Cup football competition at the introduction of every match.

"Ode to Joy", automatically orchestrated in seven different styles, has been used on 18 June 2015 during the ceremony celebrating the 5000th ERC grantee as anthem of the European Research Council to represent achievements of European research.

In 2017, Members of the Parliament of the United Kingdom from the Scottish National Party first whistled and then sang "Ode to Joy" at the House of Commons to protest against the Brexit referendum.

Due to the large number of languages used in the European Union, the anthem is purely instrumental, and the German lyrics that Friedrich Schiller wrote and on which Beethoven based the melody have no official status. Despite this, the German lyrics are often sung by choirs or ordinary people when the anthem is played: for example, at the 2004 enlargement on the German-Polish border, the crowd watching the ceremony playing the music sang along with the German lyrics.

Aside from this, several translations of the poem used by Beethoven as well as original works have attempted to provide lyrics to the anthem in various languages. Versions of the anthem including lyrics have been sung outside official EU occasions.

In France, several adaptations of Beethoven's composition were known long before the onset of European Union. A version by the librettist Maurice Bouchor (1855–1929) entitled "Hymn to Universal Humanity" ("") adding several verses to a preceding version of Jean Ruault, was published. This version and another by Maurice Bouchor, published with Julien Thiersot under the title "Hymn for future times" ("Hymne des temps futurs") in a music book which was widespread among basic schools, is performed unofficially by school choirs during European events. Another version by the Catholic writer Joseph Folliet (1903–1972) is also known.

In 2004, Austrian Professor Peter Roland of the Europa Academy in Vienna presented new, Latin lyrics to European Commission President Romano Prodi, although it has yet to be made official.




</doc>
<doc id="10029" url="https://en.wikipedia.org/wiki?curid=10029" title="Timeline of the evolutionary history of life">
Timeline of the evolutionary history of life

This timeline of the evolutionary history of life represents the current scientific theory outlining the major events during the development of life on planet Earth. In biology, evolution is any change across successive generations in the heritable characteristics of biological populations. Evolutionary processes give rise to diversity at every level of biological organization, from kingdoms to species, and individual organisms and molecules, such as DNA and proteins. The similarities between all present day organisms indicate the presence of a common ancestor from which all known species, living and extinct, have diverged through the process of evolution. More than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described.

While the dates given in this article are estimates based on scientific evidence, there has been controversy between more traditional views of increased biodiversity through a cone of diversity with the passing of time and the view that the basic pattern on Earth has been one of annihilation and diversification and that in certain past times, such as the Cambrian explosion, there was great diversity.

Species go extinct constantly as environments change, organisms compete for environmental niches, and genetic mutation leads to the rise of new species from older ones. Occasionally biodiversity on the planet takes a hit in the form of a mass extinction in which the extinction rate is much higher than usual. A large extinction event often represents an accumulation of smaller extinction events that take place in a relatively brief period of time.

The first known mass extinction in earth's history was the Great Oxygenation Event 2.4 billion years ago. The event led to the loss of most of the planet's obligate anaerobes. The five largest extinction events in earth's history since are these:


Smaller extinction events have occurred in the periods between these larger catastrophes, with some standing at the delineation points of the periods and epochs recognized by scientists in geologic time. The Holocene extinction event is currently under way.

Factors in mass extinctions include continental drift, changes in atmospheric and marine chemistry, volcanism and other aspects of mountain formation, changes in glaciation, changes in sea level, and impact events.

In this timeline, Ma (for "megaannum") means "million years ago," ka (for "kiloannum") means "thousand years ago," and ya means "years ago."

4000 Ma and earlier.

4000 Ma – 2500 Ma

2500 Ma – 542 Ma. Contains the Palaeoproterozoic, Mesoproterozoic and Neoproterozoic eras.

542 Ma – present

The Phanerozoic Eon, literally the "period of well-displayed life," marks the appearance in the fossil record of abundant, shell-forming and/or trace-making organisms. It is subdivided into three eras, the Paleozoic, Mesozoic and Cenozoic, which are divided by major mass extinctions.

542 Ma – 251.0 Ma and contains the Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian periods.

From 251.4 Ma to 66 Ma and containing the Triassic, Jurassic and Cretaceous periods.

66 Ma – present




</doc>
<doc id="10030" url="https://en.wikipedia.org/wiki?curid=10030" title="Edmund Burke">
Edmund Burke

Edmund Burke (; 12 January <nowiki>[</nowiki>NS<nowiki>]</nowiki> 17309 July 1797) was an Irish statesman born in Dublin, as well as an author, orator, political theorist and philosopher, who after moving to London in 1750 served as a member of parliament (MP) between 1766 and 1794 in the House of Commons with the Whig Party.

Burke was a proponent of underpinning virtues with manners in society and of the importance of religion in moral life. These views were expressed in his "A Vindication of Natural Society". Burke criticized British treatment of the American colonies, including through its taxation policies. He also supported the rights of the colonists to resist metropolitan authority, though he opposed the attempt to achieve independence. Burke is remembered for his support for Catholic emancipation, the impeachment of Warren Hastings from the East India Company and for his staunch opposition to the French Revolution. In his "Reflections on the Revolution in France", Burke claimed that the revolution was destroying the fabric of good society, traditional institutions of state and society and condemned the persecution of the Catholic Church that resulted from it. This led to his becoming the leading figure within the conservative faction of the Whig Party, which he dubbed the "Old Whigs", as opposed to the pro-French Revolution "New Whigs", led by Charles James Fox.

In the nineteenth century, Burke was praised by both conservatives and liberals. Subsequently, in the twentieth century he became widely regarded as the philosophical founder of modern conservatism.

Burke was born in Dublin, Ireland. His mother Mary "née" Nagle (c. 1702–1770) was a Roman Catholic who hailed from a declasse County Cork family (and a cousin of Nano Nagle), whereas his father, a successful solicitor, Richard (died 1761), was a member of the Church of Ireland; it remains unclear whether this is the same Richard Burke who converted from Catholicism. The Burke dynasty descends from an Anglo-Norman knight surnamed de Burgh (Latinised as "de Burgo") who arrived in Ireland in 1185 following Henry II of England's 1171 invasion of Ireland and is among the chief "Gall" families that assimilated into Gaelic society, becoming "more Irish than the Irish themselves".

Burke adhered to his father's faith and remained a practising Anglican throughout his life, unlike his sister Juliana who was brought up as and remained a Roman Catholic. Later, his political enemies repeatedly accused him of having been educated at the Jesuit College of St. Omer, near Calais, France, and of harbouring secret Catholic sympathies at a time when membership of the Catholic Church would disqualify him from public office ("see" Penal Laws in Ireland). As Burke told Frances Crewe:

Mr. Burke's Enemies often endeavoured to convince the World that he had been bred up in the Catholic Faith, & that his Family were of it, & that he himself had been educated at St. Omer—but this was false, as his father was a regular practitioner of the Law at Dublin, which he could not be unless of the Established Church: & it so happened that though Mr. B—was twice at Paris, he never happened to go through the Town of St. Omer.

After being elected to the House of Commons, Burke was required to take the oath of Allegiance and abjuration, the oath of supremacy, and declare against transubstantiation. Although never denying his Irishness, Burke often described himself as "an Englishman". According to the historian J. C. D. Clark, this was in an age "before 'Celtic nationalism' sought to make Irishness and Englishness incompatible".

As a child he sometimes spent time away from the unhealthy air of Dublin with his mother's family in the Blackwater Valley in County Cork. He received his early education at a Quaker school in Ballitore, County Kildare, some from Dublin; and possibly, like his cousin Nano Nagle at a Hedge school. He remained in correspondence with his schoolmate from there, Mary Leadbeater, the daughter of the school's owner, throughout his life.

In 1744, Burke started at Trinity College Dublin, a Protestant establishment, which up until 1793, did not permit Catholics to take degrees.
In 1747, he set up a debating society, "Edmund Burke's Club", which, in 1770, merged with TCD's Historical Club to form the College Historical Society; it is the oldest undergraduate society in the world. The minutes of the meetings of Burke's Club remain in the collection of the Historical Society. Burke graduated from Trinity in 1748. Burke's father wanted him to read Law, and with this in mind he went to London in 1750, where he entered the Middle Temple, before soon giving up legal study to travel in Continental Europe. After eschewing the Law, he pursued a livelihood through writing.

The late Lord Bolingbroke's "Letters on the Study and Use of History" was published in 1752 and his collected works appeared in 1754. This provoked Burke into writing his first published work, "A Vindication of Natural Society: A View of the Miseries and Evils Arising to Mankind", appearing in Spring 1756. Burke imitated Bolingbroke's style and ideas in a "reductio ad absurdum" of his arguments for atheistic rationalism, in order to demonstrate their absurdity.
Burke claimed that Bolingbroke's arguments against revealed religion could apply to all social and civil institutions as well. Lord Chesterfield and Bishop Warburton (and others) initially thought that the work was genuinely by Bolingbroke rather than a satire. All the reviews of the work were positive, with critics especially appreciative of Burke's quality of writing. Some reviewers failed to notice the ironic nature of the book, which led to Burke stating in the preface to the second edition (1757) that it was a satire.

Richard Hurd believed that Burke's imitation was near-perfect and that this defeated his purpose: an ironist "should take care by a constant exaggeration to make the "ridicule" shine through the Imitation. Whereas this "Vindication" is everywhere enforc'd, not only in the language, and on the principles of L. Bol., but with so apparent, or rather so real an earnestness, that half his purpose is sacrificed to the other". A minority of scholars have taken the position that, in fact, Burke did write the "Vindication" in earnest, later disowning it only for political reasons.

In 1757, Burke published a treatise on aesthetics, "A Philosophical Enquiry into the Origin of Our Ideas of the Sublime and Beautiful", which attracted the attention of prominent Continental thinkers such as Denis Diderot and Immanuel Kant. It was his only purely philosophical work, and when asked by Sir Joshua Reynolds and French Laurence to expand it thirty years later, Burke replied that he was no longer fit for abstract speculation (Burke had written it before he was nineteen years of age).

On 25 February 1757, Burke signed a contract with Robert Dodsley to write a "history of England from the time of Julius Caesar to the end of the reign of Queen Anne", its length being eighty quarto sheets (640 pages), nearly 400,000 words. It was to be submitted for publication by Christmas 1758. Burke completed the work to the year 1216 and stopped; it was not published until after Burke's death, being included in an 1812 collection of his works, entitled "An Essay Towards an Abridgement of the English History". G. M. Young did not value Burke's history and claimed that it was "demonstrably a translation from the French". Lord Acton, on commenting on the story that Burke stopped his history because David Hume published his, said "it is ever to be regretted that the reverse did not occur".

During the year following that contract, with Dodsley, Burke founded the influential "Annual Register", a publication in which various authors evaluated the international political events of the previous year. The extent to which Burke contributed to the "Annual Register" is unclear: in his biography of Burke, Robert Murray quotes the "Register" as evidence of Burke's opinions, yet Philip Magnus in his biography does not cite it directly as a reference. Burke remained the chief editor of the publication until at least 1789 and there is no evidence that any other writer contributed to it before 1766.

On 12 March 1757, Burke married Jane Mary Nugent (1734–1812), daughter of Dr Christopher Nugent, a Catholic physician who had provided him with medical treatment at Bath. Their son Richard was born on 9 February 1758; an elder son, Christopher, died in infancy. Burke also helped raise a ward, Edmund Nagle ("later" Admiral Sir Edmund Nagle), the son of a maternal cousin orphaned in 1763.

At about this same time, Burke was introduced to William Gerard Hamilton (known as "Single-speech Hamilton"). When Hamilton was appointed Chief Secretary for Ireland, Burke accompanied him to Dublin as his private secretary, a position he held for three years. In 1765 Burke became private secretary to the liberal Whig statesman, Charles, Marquess of Rockingham, then Prime Minister of Great Britain, who remained Burke's close friend and associate until his untimely death in 1782. Rockingham also introduced Burke as a Freemason.

In December 1765, Burke entered the House of Commons of the British Parliament as Member for Wendover, a pocket borough in the gift of Lord Fermanagh, later 2nd Earl Verney and a close political ally of Rockingham. Having delivered his maiden speech, William Pitt the Elder said Burke had "spoken in such a manner as to stop the mouths of all Europe" and that the Commons should congratulate itself on acquiring such a Member.

The first great subject Burke addressed was the controversy with the American colonies, which soon developed into war and ultimate separation; in reply to the 1769 Grenvillite pamphlet, "The Present State of the Nation", he published his own pamphlet on, "Observations on a Late State of the Nation". Surveying the finances of France, Burke predicts "some extraordinary convulsion in that whole system".

During the same year, with mostly borrowed money, Burke purchased "Gregories", a estate near Beaconsfield. Although the estate included saleable assets such as art works by Titian, "Gregories" proved a heavy financial burden in the following decades and Burke was never able to repay its purchase price in full. His speeches and writings, having made him famous, led to the suggestion that he was the author of the "Letters of Junius".

At about this time, Burke joined the circle of leading intellectuals and artists in London of whom Samuel Johnson was the central luminary. This circle also included David Garrick, Oliver Goldsmith, and Joshua Reynolds. Edward Gibbon described Burke as 'the most eloquent and rational madman that I ever knew'. Although Johnson admired Burke's brilliance, he found him a dishonest politician.

Burke took a leading role in the debate regarding the constitutional limits to the executive authority of the king. He argued strongly against unrestrained royal power and for the role of political parties in maintaining a principled opposition capable of preventing abuses, either by the monarch, or by specific factions within the government. His most important publication in this regard was his "Thoughts on the Cause of the Present Discontents" of 23 April 1770. Burke identified the "discontents" as stemming from the "secret influence" of a neo-Tory group he labelled as, the "king's friends", whose system "comprehending the exterior and interior administrations, is commonly called, in the technical language of the Court, "Double Cabinet"". Britain needed a party with "an unshaken adherence to principle, and attachment to connexion, against every allurement of interest". Party divisions "whether operating for good or evil, are things inseparable from free government".
During 1771, Burke wrote a Bill that, if passed, would have given juries the right to determine what was libel. Burke spoke in favour of the Bill but it was opposed by some, including Charles James Fox thus not becoming law. Fox, when introducing his own Bill in 1791 in Opposition, repeated almost verbatim the text of Burke's Bill without acknowledgement. Burke also was prominent in securing the right to publish debates held in Parliament.

Speaking in a parliamentary debate on the prohibition on the export of grain on 16 November 1770, Burke argued in favour of a free market in corn: "There are no such things as a high, & a low price that is encouraging, & discouraging; there is nothing but a natural price, which grain brings at an universal market." In 1772 Burke was instrumental in the passing of the Repeal of Certain Laws Act 1772, which repealed various old laws against dealers and forestallers in corn.

In the "Annual Register" for 1772 (published in July 1773), Burke condemned the Partition of Poland. He saw it as "the first very great breach in the modern political system of Europe" and as upsetting the balance of power in Europe.

In 1774, Burke was elected Member for Bristol, at the time "England's second city" and a large constituency with a genuine electoral contest.

In May 1778, Burke supported a parliamentary motion revising restrictions on Irish trade. His constituents, citizens of the great trading city of Bristol, however urged Burke to oppose free trade with Ireland. Burke resisted their protestations and said: "If, from this conduct, I shall forfeit their suffrages at an ensuing election, it will stand on record an example to future representatives of the Commons of England, that one man at least had dared to resist the desires of his constituents when his judgment assured him they were wrong".

Burke published, "Two Letters to Gentlemen of Bristol on the Bills relative to the Trade of Ireland", in which he espoused "some of the chief principles of commerce; such as the advantage of free intercourse between all parts of the same kingdom...the evils attending restriction and monopoly...and that the gain of others is not necessarily our loss, but on the contrary an advantage by causing a greater demand for such wares as we have for sale".

Burke also supported the attempts of Sir George Savile to repeal some of the penal laws against Catholics. Burke also called capital punishment "the Butchery which we call justice" in 1776 and in 1780 he condemned the use of the pillory for two men convicted for attempting to practice sodomy.

This support for unpopular causes, notably free trade with Ireland and Catholic Emancipation, led to Burke losing his seat in 1780. For the remainder of his parliamentary career, Burke represented Malton, another pocket borough under the Marquess of Rockingham's patronage.

Burke expressed his support for the grievances of the American Colonies under the government of King George III and his appointed representatives. On 19 April 1774 Burke made a speech, "On American Taxation" (published in January 1775), on a motion to repeal the tea duty:

Again and again, revert to your old principles—seek peace and ensue it; leave America, if she has taxable matter in her, to tax herself. I am not here going into the distinctions of rights, nor attempting to mark their boundaries. I do not enter into these metaphysical distinctions; I hate the very sound of them. Leave the Americans as they anciently stood, and these distinctions, born of our unhappy contest, will die along with it... Be content to bind America by laws of trade; you have always done it... Do not burthen them with taxes... But if intemperately, unwisely, fatally, you sophisticate and poison the very source of government by urging subtle deductions, and consequences odious to those you govern, from the unlimited and illimitable nature of supreme sovereignty, you will teach them by these means to call that sovereignty itself in question... If that sovereignty and their freedom cannot be reconciled, which will they take? They will cast your sovereignty in your face. No body of men will be argued into slavery.

On 22 March 1775, in the House of Commons, Burke delivered a speech (published during May 1775) on reconciliation with America. Burke appealed for peace as preferable to civil war and reminded the House of America's growing population, its industry, and its wealth. He warned against the notion that the Americans would back down in the face of force, since most Americans were of British descent:

... the people of the colonies are descendants of Englishmen... They are therefore not only devoted to liberty, but to liberty according to English ideas and on English principles. The people are Protestants... a persuasion not only favourable to liberty, but built upon it... My hold of the colonies is in the close affection which grows from common names, from kindred blood, from similar privileges, and equal protection. These are ties which, though light as air, are as strong as links of iron. Let the colonies always keep the idea of their civil rights associated with your government—they will cling and grapple to you, and no force under heaven will be of power to tear them from their allegiance. But let it be once understood that your government may be one thing and their privileges another, that these two things may exist without any mutual relation—the cement is gone, the cohesion is loosened, and everything hastens to decay and dissolution. As long as you have the wisdom to keep the sovereign authority of this country as the sanctuary of liberty, the sacred temple consecrated to our common faith, wherever the chosen race and sons of England worship freedom, they will turn their faces towards you. The more they multiply, the more friends you will have; the more ardently they love liberty, the more perfect will be their obedience. Slavery they can have anywhere. It is a weed that grows in every soil. They may have it from Spain, they may have it from Prussia. But, until you become lost to all feeling of your true interest and your natural dignity, freedom they can have from none but you.

Burke prized peace with America above all else, pleading with the House of Commons to remember that the interest by way of money received from the American colonies was far more attractive than any sense of putting the colonists in their place:

The proposition is peace. Not peace through the medium of war, not peace to be hunted through the labyrinth of intricate and endless negotiations, not peace to arise out of universal discord... it is simple peace, sought in its natural course and in its ordinary haunts. It is peace sought in the spirit of peace, and laid in principles purely pacific.

Burke was not merely presenting a peace agreement to Parliament; rather, he stepped forward with four reasons against using force, carefully reasoned. He laid out his objections in an orderly manner, focusing on one before moving to the next. His first concern was that the use of force would have to be temporary, and that the uprisings and objections to British governance in America would not be. Second, Burke worried about the uncertainty surrounding whether Britain would win a conflict in America. "An armament", Burke said, "is not a victory". Third, Burke brought up the issue of impairment; it would do the British Government no good to engage in a scorched earth war and have the object they desired (America) become damaged or even useless. The American colonists could always retreat into the mountains, but the land they left behind would most likely be unusable, whether by accident or design. The fourth and final reason to avoid the use of force was experience; the British had never attempted to rein in an unruly colony by force, and they did not know if it could be done, let alone accomplished thousands of miles away from home. Not only were all of these concerns reasonable, but some turned out to be prophetic—the American colonists did not surrender, even when things looked extremely bleak, and the British were ultimately unsuccessful in their attempts to win a war fought on American soil.

It was not temporary force, uncertainty, impairment, or even experience that Burke cited as the number one reason for avoiding war with the American colonies, however; it was the character of the American people themselves: "In this character of Americans, a love of freedom is the predominating feature which marks and distinguishes the whole... this fierce spirit of liberty is stronger in the English colonies, probably, than in any other people of the earth... [the] men [are] acute, inquisitive, dextrous, prompt in attack, ready in defense, full of resources..." Burke concludes with another plea for peace, and a prayer that Britain might avoid actions which, in Burke's words, "may bring on the destruction of this Empire".

Burke proposed six resolutions to settle the American conflict peacefully:

The effect of these resolutions, had they been passed, can never be known. Unfortunately, Burke delivered this speech just less than a month before the explosive conflict at Concord and Lexington, and as these resolutions were not enacted, little was done that would help to dissuade conflict.

Among the reasons this speech was so greatly admired was its passage on Lord Bathurst (1684–1775); Burke describes an angel in 1704 prophesying to Bathurst the future greatness of England and also of America: "Young man, There is America—which at this day serves little more than to amuse you with stories of savage men, and uncouth manners; yet shall, before you taste of death, shew itself equal to the whole of that commerce which now attracts the envy of the world". Samuel Johnson was so irritated at hearing it continually praised, that he made a parody of it, where the devil appears to a young Whig and predicts that in short time, Whiggism will poison even the paradise of America!

The administration of Lord North (1770–1782) tried to defeat the colonist rebellion by military force. British and American forces clashed in 1775 and, in 1776, came the American Declaration of Independence. Burke was appalled by celebrations in Britain of the defeat of the Americans at New York and Pennsylvania. He claimed the English national character was being changed by this authoritarianism. Burke wrote: "As to the good people of England, they seem to partake every day more and more of the Character of that administration which they have been induced to tolerate. I am satisfied, that within a few years there has been a great Change in the National Character. We seem no longer that eager, inquisitive, jealous, fiery people, which we have been formerly".

In Burke's view, the British Government was fighting "the American English" ("our English Brethren in the Colonies"), with a Germanic king employing "the hireling sword of German boors and vassals" to destroy the English liberties of the colonists. On American independence, Burke wrote: "I do not know how to wish success to those whose Victory is to separate from us a large and noble part of our Empire. Still less do I wish success to injustice, oppression and absurdity".

During the Gordon Riots in 1780, Burke became a target of hostility and his home was placed under armed guard by the military.

The fall of North led to Rockingham being recalled to power in March 1782. Burke was appointed Paymaster of the Forces and a Privy Counsellor, but without a seat in Cabinet. Rockingham's unexpected death in July 1782 and replacement with Shelburne as Prime Minister, put an end to his administration after only a few months, however, Burke did manage to introduce two Acts.

The Paymaster General Act 1782 ended the post as a lucrative sinecure. Previously, Paymasters had been able to draw on money from HM Treasury at their discretion. Now they were required to put the money they had requested to withdraw from the Treasury into the Bank of England, from where it was to be withdrawn for specific purposes. The Treasury would receive monthly statements of the Paymaster's balance at the Bank. This act was repealed by Shelburne's administration, but the act that replaced it repeated verbatim almost the whole text of the Burke Act.

The Civil List and Secret Service Money Act 1782 was a watered down version of Burke's original intentions as outlined in his famous "Speech on Economical Reform" of 11 February 1780. He managed, however, to abolish 134 offices in the royal household and civil administration. The third Secretary of State and the Board of Trade were abolished and pensions were limited and regulated. The Act was anticipated to save £72,368 a year.

In February 1783, Burke resumed the post of Paymaster of the Forces when Shelburne's government fell and was replaced by a coalition headed by North that included Charles James Fox. That coalition fell in 1783, and was succeeded by the long Tory administration of William Pitt the Younger, which lasted until 1801. Accordingly, having supported Fox and North, Burke was in opposition for the remainder of his political life.

In 1774, Burke's "Speech to the Electors at Bristol at the Conclusion of the Poll" was noted for its defence of the principles of representative government against the notion that elected officials should merely be delegates:

... it ought to be the happiness and glory of a representative to live in the strictest union, the closest correspondence, and the most unreserved communication with his constituents. Their wishes ought to have great weight with him; their opinion, high respect; their business, unremitted attention. It is his duty to sacrifice his repose, his pleasures, his satisfactions, to theirs; and above all, ever, and in all cases, to prefer their interest to his own. But his unbiased opinion, his mature judgment, his enlightened conscience, he ought not to sacrifice to you, to any man, or to any set of men living. These he does not derive from your pleasure; no, nor from the law and the constitution. They are a trust from Providence, for the abuse of which he is deeply answerable. Your representative owes you, not his industry only, but his judgment; and he betrays, instead of serving you, if he sacrifices it to your opinion.

Political scientist Hanna Pitkin points out that Burke linked the interest of the district with the proper behaviour of its elected official, explaining, "Burke conceives of broad, relatively fixed interest, few in number and clearly defined, of which any group or locality has just one. These interests are largely economic or associated with particular localities whose livelihood they characterize, in his over-all prosperity they involve."

Burke was a leading sceptic with respect to democracy. While admitting that theoretically, in some cases it might be desirable, he insisted a democratic government in Britain in his day would not only be inept, but also oppressive. He opposed democracy for three basic reasons. First, government required a degree of intelligence and breadth of knowledge of the sort that occurred rarely among the common people. Second, he thought that if they had the vote, common people had dangerous and angry passions that could be aroused easily by demagogues; he feared that the authoritarian impulses that could be empowered by these passions would undermine cherished traditions and established religion, leading to violence and confiscation of property. Third, Burke warned that democracy would create a tyranny over unpopular minorities, who needed the protection of the upper classes.

For years Burke pursued impeachment efforts against Warren Hastings, formerly Governor-General of Bengal, that resulted in the trial during 1786. His interaction with the British dominion of India began well before Hastings' impeachment trial. For two decades prior to the impeachment, Parliament had dealt with the Indian issue. This trial was the pinnacle of years of unrest and deliberation. In 1781 Burke was first able to delve into the issues surrounding the East India Company when he was appointed Chairman of the Commons Select Committee on East Indian Affairs—from that point until the end of the trial; India was Burke's primary concern. This committee was charged "to investigate alleged injustices in Bengal, the war with Hyder Ali, and other Indian difficulties". While Burke and the committee focused their attention on these matters, a second 'secret' committee was formed to assess the same issues. Both committee reports were written by Burke. Among other purposes, the reports conveyed to the Indian princes that Britain would not wage war on them, along with demanding that the East India Company should recall Hastings. This was Burke's first call for substantive change regarding imperial practices. When addressing the whole House of Commons regarding the committee report, Burke described the Indian issue as one that "began 'in commerce' but 'ended in empire.'"

On 28 February 1785, Burke delivered a now-famous speech, "The Nabob of Arcot's Debts", wherein he condemned the damage to India by the East India Company. In the province of the Carnatic the Indians had constructed a system of reservoirs to make the soil fertile in a naturally dry region, and centred their society on the husbandry of water:

These are the monuments of real kings, who were the fathers of their people; testators to a posterity which they embraced as their own. These are the grand sepulchres built by ambition; but by the ambition of an insatiable benevolence, which, not contented with reigning in the dispensation of happiness during the contracted term of human life, had strained, with all the reachings and graspings of a vivacious mind, to extend the dominion of their bounty beyond the limits of nature, and to perpetuate themselves through generations of generations, the guardians, the protectors, the nourishers of mankind.

Burke held that the advent of British dominion, and in particular the conduct of the East India Company, had destroyed much that was good in these traditions and that, as a consequence of this, and the lack of new customs to replace them, the Indians were suffering. He set about establishing a set of British expectations, whose moral foundation would, in his opinion, warrant the empire.

On 4 April 1786, Burke presented the Commons with the "Article of Charge of High Crimes and Misdemeanors" against Hastings. The impeachment in Westminster Hall, which did not begin until 14 February 1788, would be the "first major public discursive event of its kind in England", bringing the morality and duty of imperialism to the forefront of public perception. Burke already was known for his eloquent rhetorical skills and his involvement in the trial only enhanced its popularity and significance. Burke's indictment, fuelled by emotional indignation, branded Hastings a 'captain-general of iniquity'; who never dined without 'creating a famine'; whose heart was 'gangrened to the core', and who resembled both a 'spider of Hell' and a 'ravenous vulture devouring the carcasses of the dead'. The House of Commons eventually impeached Hastings, but subsequently, the House of Lords acquitted him of all charges.

Initially, Burke did not condemn the French Revolution. In a letter of 9 August 1789, he wrote: "England gazing with astonishment at a French struggle for Liberty and not knowing whether to blame or to applaud! The thing indeed, though I thought I saw something like it in progress for several years, has still something in it paradoxical and Mysterious. The spirit it is impossible not to admire; but the old Parisian ferocity has broken out in a shocking manner". The events of 5–6 October 1789, when a crowd of Parisian women marched on Versailles to compel King Louis XVI to return to Paris, turned Burke against it. In a letter to his son, Richard Burke, dated 10 October he said: "This day I heard from Laurence who has sent me papers confirming the portentous state of France—where the Elements which compose Human Society seem all to be dissolved, and a world of Monsters to be produced in the place of it—where Mirabeau presides as the Grand Anarch; and the late Grand Monarch makes a figure as ridiculous as pitiable". On 4 November Charles-Jean-François Depont wrote to Burke, requesting that he endorse the Revolution. Burke replied that any critical language of it by him should be taken "as no more than the expression of doubt" but he added: "You may have subverted Monarchy, but not recover'd freedom". In the same month he described France as "a country undone". Burke's first public condemnation of the Revolution occurred on the debate in Parliament on the army estimates on 9 February 1790, provoked by praise of the Revolution by Pitt and Fox:

Since the House had been prorogued in the summer much work was done in France. The French had shewn themselves the ablest architects of ruin that had hitherto existed in the world. In that very short space of time they had completely pulled down to the ground, their monarchy; their church; their nobility; their law; their revenue; their army; their navy; their commerce; their arts; and their manufactures... [there was a danger of] an imitation of the excesses of an irrational, unprincipled, proscribing, confiscating, plundering, ferocious, bloody and tyrannical democracy... [in religion] the danger of their example is no longer from intolerance, but from Atheism; a foul, unnatural vice, foe to all the dignity and consolation of mankind; which seems in France, for a long time, to have been embodied into a faction, accredited, and almost avowed.

In January 1790, Burke read Dr. Richard Price's sermon of 4 November 1789 entitled, "A Discourse on the Love of Our Country", to the Revolution Society. That society had been founded to commemorate the Glorious Revolution of 1688. In this sermon Price espoused the philosophy of universal "Rights of Men". Price argued that love of our country "does not imply any conviction of the superior value of it to other countries, or any particular preference of its laws and constitution of government". Instead, Price asserted that Englishmen should see themselves "more as citizens of the world than as members of any particular community".

A debate between Price and Burke ensued that was "the classic moment at which two fundamentally different conceptions of national identity were presented to the English public". Price claimed that the principles of the Glorious Revolution included "the right to choose our own governors, to cashier them for misconduct, and to frame a government for ourselves".

Immediately after reading Price's sermon, Burke wrote a draft of what eventually became "Reflections on the Revolution in France". On 13 February 1790, a notice in the press said that shortly, Burke would publish a pamphlet on the Revolution and its British supporters, however he spent the year revising and expanding it. On 1 November he finally published the "Reflections" and it was an immediate best-seller. Priced at five shillings, it was more expensive than most political pamphlets, but by the end of 1790, it had gone through ten printings and sold approximately 17,500 copies. A French translation appeared on 29 November and on 30 November the translator, Pierre-Gaëton Dupont, wrote to Burke saying 2,500 copies had already been sold. The French translation ran to ten printings by June 1791.

What the Glorious Revolution had meant was as important to Burke and his contemporaries as it had been for the last one hundred years in British politics. In the "Reflections", Burke argued against Price's interpretation of the Glorious Revolution and instead, gave a classic Whig defence of it. Burke argued against the idea of abstract, metaphysical rights of humans and instead advocated national tradition:

Burke said "We fear God, we look up with awe to kings; with affection to parliaments; with duty to magistrates; with reverence to priests; and with respect to nobility. Why? Because when such ideas are brought before our minds, it is "natural" to be so affected". Burke defended this prejudice on the grounds that it is "the general bank and capital of nations, and of ages" and superior to individual reason, which is small in comparison. "Prejudice", Burke claimed, "is of ready application in the emergency; it previously engages the mind in a steady course of wisdom and virtue, and does not leave the man hesitating in the moment of decision, skeptical, puzzled, and unresolved. Prejudice renders a man's virtue his habit". Burke criticised social contract theory by claiming that society is indeed, a contract, but "a partnership not only between those who are living, but between those who are living, those who are dead, and those who are to be born".

The most famous passage in Burke's "Reflections" was his description of the events of 5–6 October 1789 and the part of Marie-Antoinette in them. Burke's account differs little from modern historians who have used primary sources. His use of flowery language to describe it, however, provoked both praise and criticism. Philip Francis wrote to Burke saying that what he wrote of Marie-Antoinette was "pure foppery". Edward Gibbon, however, reacted differently: "I adore his chivalry". Burke was informed by an Englishman who had talked with the Duchesse de Biron, that when Marie-Antoinette was reading the passage, she burst into tears and took considerable time to finish reading it. Price had rejoiced that the French king had been "led in triumph" during the October Days, but to Burke this symbolised the opposing revolutionary sentiment of the Jacobins and the natural sentiments of those who shared his own view with horror—that the ungallant assault on Marie-Antoinette—was a cowardly attack on a defenceless woman.

Louis XVI translated the "Reflections" "from end to end" into French. Fellow Whig MPs Richard Sheridan and Charles James Fox, disagreed with Burke and split with him. Fox thought the "Reflections" to be "in very bad taste" and "favouring Tory principles". Other Whigs such as the Duke of Portland and Earl Fitzwilliam privately agreed with Burke, but did not wish for a public breach with their Whig colleagues. Burke wrote on 29 November 1790: "I have received from the Duke of Portland, Lord Fitzwilliam, the Duke of Devonshire, Lord John Cavendish, Montagu (Frederick Montagu MP), and a long et cetera of the old Stamina of the Whiggs a most full approbation of the principles of that work and a kind indulgence to the execution". The Duke of Portland said in 1791 that when anyone criticised the "Reflections" to him, he informed them that he had recommended the book to his sons as containing the true Whig creed.

In the opinion of Paul Langford, Burke crossed something of a Rubicon when he attended a levee on 3 February 1791 to meet the king, later described by Jane Burke:

On his coming to Town for the Winter, as he generally does, he went to the Levee with the Duke of "Portland", who went with Lord William to kiss hands on his going into the Guards—while Lord William was kissing hands, The King was talking to The Duke, but his Eyes were fixed on [Burke] who was standing in the Crowd, and when He said His say to The Duke, without waiting for [Burke]'s coming up in his turn, The King went up to him, and, after the usual questions of how long have you been in Town and the weather, He said you have been very much employed of late, and very much confined. [Burke] said, no, Sir, not more than usual—You have and very well employed too, but there are none so deaf as those that w'ont hear, and none so blind as those that w'ont see—[Burke] made a low bow, Sir, I certainly now understand you, but was afraid my vanity or presumption might have led me to imagine what Your Majesty has said referred to what I have done—You cannot be vain—You have been of "use to us all", it is a general opinion, is it not so Lord Stair? who was standing near. It is said Lord Stair;—Your Majesty's adopting it, Sir, will make the opinion general, said [Burke]—I know it is the general opinion, and I know that there is no Man who calls himself a Gentleman that must not think himself obliged to you, for you have supported the cause of the Gentlemen—You know the tone at Court is a whisper, but The King said all this loud, so as to be heard by every one at Court.

Burke's "Reflections" sparked a pamphlet war. Mary Wollstonecraft was one of the first into print, publishing "A Vindication of the Rights of Men" a few weeks after Burke. Thomas Paine followed with the "Rights of Man" in 1791. James Mackintosh, who wrote "Vindiciae Gallicae", was the first to see the "Reflections" as "the manifesto of a Counter Revolution". Mackintosh later agreed with Burke's views, remarking in December 1796 after meeting him, that Burke was "minutely and accurately informed, to a wonderful exactness, with respect to every fact relating to the French Revolution". Mackintosh later said: "Burke was one of the first thinkers as well as one of the greatest orators of his time. He is without parallel in any age, excepting perhaps Lord Bacon and Cicero; and his works contain an ampler store of political and moral wisdom than can be found in any other writer whatever".

In November 1790, François-Louis-Thibault de Menonville, a member of the National Assembly of France, wrote to Burke, praising "Reflections" and requesting more "very refreshing mental food" that he could publish. This Burke did in April 1791 when he published "A Letter to a Member of the National Assembly". Burke called for external forces to reverse the revolution and included an attack on the late French philosopher Jean-Jacques Rousseau, as being the subject of a personality cult that had developed in revolutionary France. Although Burke conceded that Rousseau sometimes showed "a considerable insight into human nature" he mostly was critical. Although he did not meet Rousseau on his visit to Britain in 1766–67 Burke was a friend of David Hume, with whom Rousseau had stayed. Burke said Rousseau "entertained no principle either to influence of his heart, or to guide his understanding—but "vanity""—which he "was possessed to a degree little short of madness". He also cited Rousseau's "Confessions" as evidence that Rousseau had a life of "obscure and vulgar vices" that was not "chequered, or spotted here and there, with virtues, or even distinguished by a single good action". Burke contrasted Rousseau's theory of universal benevolence and his having sent his children to a foundling hospital: "a lover of his kind, but a hater of his kindred".

These events and the disagreements that arose from them within the Whig Party, led to its break-up and to the rupture of Burke's friendship with Fox. In debate in Parliament on Britain's relations with Russia, Fox praised the principles of the revolution, although Burke was not able to reply at this time as he was "overpowered by continued cries of question from his own side of the House". When Parliament was debating the Quebec Bill for a constitution for Canada, Fox praised the revolution and criticised some of Burke's arguments, such as hereditary power. On 6 May 1791, during another debate in Parliament on the Quebec Bill, Burke used the opportunity to answer Fox, and to condemn the new French Constitution and "the horrible consequences flowing from the French idea of the Rights of Man". Burke asserted that those ideas were the antithesis of both the British and the American constitutions. Burke was interrupted, and Fox intervened, saying that Burke should be allowed to carry on with his speech. A vote of censure was moved against Burke, however, for noticing the affairs of France, which was moved by Lord Sheffield and seconded by Fox. Pitt made a speech praising Burke, and Fox made a speech—both rebuking and complimenting Burke. He questioned the sincerity of Burke, who seemed to have forgotten the lessons he had learned from him, quoting from Burke's own speeches of fourteen and fifteen years before.

Burke's response was:

It certainly was indiscreet at any period, but especially at his time of life, to parade enemies, or give his friends occasion to desert him; yet if his firm and steady adherence to the British constitution placed him in such a dilemma, he would risk all, and, as public duty and public experience taught him, with his last words exclaim, "Fly from the French Constitution".

At this point, Fox whispered that there was "no loss of friendship". "I regret to say there is", Burke replied, "I have indeed made a great sacrifice; I have done my duty though I have lost my friend. There is something in the detested French constitution that envenoms every thing it touches". This provoked a reply from Fox, yet he was unable to give his speech for some time since he was overcome with tears and emotion, he appealed to Burke to remember their inalienable friendship, but also repeated his criticisms of Burke and uttered "unusually bitter sarcasms". This only aggravated the rupture between the two men. Burke demonstrated his separation from the party on 5 June 1791 by writing to Fitzwilliam, declining money from him.

Burke was dismayed that some Whigs, instead of reaffirming the principles of the Whig Party he laid out in the "Reflections", had rejected them in favour of "French principles" and that they criticised Burke for abandoning Whig principles. Burke wanted to demonstrate his fidelity to Whig principles and feared that acquiescence to Fox and his followers would allow the Whig Party to become a vehicle for Jacobinism.

Burke knew that many members of the Whig Party did not share Fox's views and he wanted to provoke them into condemning the French Revolution. Burke wrote that he wanted to represent the whole Whig party "as tolerating, and by a toleration, countenancing those proceedings" so that he could "stimulate them to a public declaration of what every one of their acquaintance privately knows to be...their sentiments". Therefore, on 3 August 1791 Burke published his "Appeal from the New to the Old Whigs", in which he renewed his criticism of the radical revolutionary programmes inspired by the French Revolution and attacked the Whigs who supported them, as holding principles contrary to those traditionally held by the Whig party.

Burke owned two copies of what has been called "that practical compendium of Whig political theory", "The Tryal of Dr. Henry Sacheverell" (1710). Burke wrote of the trial: "It rarely happens to a party to have the opportunity of a clear, authentic, recorded, declaration of their political tenets upon the subject of a great constitutional event like that of the [Glorious] Revolution". Writing in the third person, Burke asserted in his "Appeal":

... that the foundations laid down by the Commons, on the trial of Doctor Sacheverel, for justifying the revolution of 1688, are the very same laid down in Mr. Burke's Reflections; that is to say,—a breach of the "original contract", implied and expressed in the constitution of this country, as a scheme of government fundamentally and inviolably fixed in King, Lords and Commons.—That the fundamental subversion of this antient constitution, by one of its parts, having been attempted, and in effect accomplished, justified the Revolution. That it was justified "only" upon the "necessity" of the case; as the "only" means left for the recovery of that "antient" constitution, formed by the "original contract" of the British state; as well as for the future preservation of the "same" government. These are the points to be proved.

Burke then provided quotations from Paine's "Rights of Man" to demonstrate what the New Whigs believed. Burke's belief that Foxite principles corresponded to Paine's was genuine. Finally, Burke denied that a majority of "the people" had, or ought to have, the final say in politics and alter society at their pleasure. People had rights, but also duties, and these duties were not voluntary. Also, the people could not overthrow morality derived from God.

Although Whig grandees such as Portland and Fitzwilliam privately agreed with Burke's "Appeal", they wished he had used more moderate language. Fitzwilliam saw the "Appeal" as containing "the doctrines I have sworn by, long and long since". Francis Basset, a backbench Whig MP, wrote to Burke: "...though for reasons which I will not now detail I did not then deliver my sentiments, I most perfectly differ from Mr. Fox & from the great Body of opposition on the French Revolution". Burke sent a copy of the "Appeal" to the king and the king requested a friend to communicate to Burke that he had read it "with great Satisfaction". Burke wrote of its reception: "Not one word from one of our party. They are secretly galled. They agree with me to a title; but they dare not speak out for fear of hurting Fox. ... They leave me to myself; they see that I can do myself justice". Charles Burney viewed it as "a most admirable book—the best & most useful on political subjects that I have ever seen" but believed the differences in the Whig Party between Burke and Fox should not be aired publicly.

Eventually, most of the Whigs sided with Burke and gave their support to Pitt's "conservative" government, which, in response to France's declaration of war against Britain, declared war on France's Revolutionary Government in 1793.

In December 1791, Burke sent Government ministers his "Thoughts on French Affairs" where he put forward three main points: no counter-revolution in France would come about by purely domestic causes; the longer the Revolutionary Government exists the stronger it becomes; and the Revolutionary Government's interest and aim is to disturb all of the other governments of Europe.

Burke, as a Whig, did not wish to see an absolute monarchy again in France after the extirpation of Jacobinism. Writing to an "émigré" in 1791, Burke expressed his views against a restoration of the "ancien régime":

When such a complete convulsion has shaken the State, and hardly left any thing whatsoever, either in civil arrangements, or in the Characters and disposition of men's minds, exactly where it was, whatever shall be settled although in the former persons and upon old forms, will be in some measure a new thing and will labour under something of the weakness as well as other inconveniences of a Change. My poor opinion is that you mean to establish what you call 'L'ancien Régime,' If any one means that system of Court Intrigue miscalled a Government as it stood, at Versailles before the present confusions as the thing to be established, that I believe will be found absolutely impossible; and if you consider the Nature, as well of persons, as of affairs, I flatter myself you must be of my opinion. That was tho' not so violent a State of Anarchy as well as the present. If it were even possible to lay things down exactly as they stood, before the series of experimental politicks began, I am quite sure that they could not long continue in that situation. In one Sense of L'Ancien Régime I am clear that nothing else can reasonably be done.

Burke delivered a speech on the debate of the Aliens Bill on 28 December 1792. He supported the Bill as it would exclude "murderous atheists, who would pull down Church and state; religion and God; morality and happiness". The peroration included a reference to a French order for 3,000 daggers. Burke revealed a dagger he had concealed in his coat and threw it to the floor: "This is what you are to gain by an alliance with France". Burke picked up the dagger and continued:

When they smile, I see blood trickling down their faces; I see their insidious purposes; I see that the object of all their cajoling is—blood! I now warn my countrymen to beware of these execrable philosophers, whose only object it is to destroy every thing that is good here, and to establish immorality and murder by precept and example—'Hic niger est hunc tu Romane caveto' ['Such a man is evil; beware of him, Roman'. Horace, "Satires" I. 4. 85.].

Burke supported the war against revolutionary France, seeing Britain as fighting on the side of the royalists and "émigres" in a civil war, rather than fighting against the whole nation of France. Burke also supported the royalist uprising in La Vendée, describing it on 4 November 1793 in a letter to William Windham, as "the sole affair I have much heart in". Burke wrote to Henry Dundas on 7 October urging him to send reinforcements there, as he viewed it as the only theatre in the war that might lead to a march on Paris. Dundas did not follow Burke's advice, however.

Burke believed the Government was not taking the uprising seriously enough, a view reinforced by a letter he had received from the Prince Charles of France ("S.A.R. le comte d'Artois"), dated 23 October, requesting that he intercede on behalf of the royalists to the Government. Burke was forced to reply on 6 November: "I am not in His Majesty's Service; or at all consulted in his Affairs". Burke published his "Remarks on the Policy of the Allies with Respect to France", begun in October, where he said: "I am sure every thing has shewn us that in this war with France, one Frenchman is worth twenty foreigners. La Vendée is a proof of this".

On 20 June 1794, Burke received a vote of thanks from the Commons for his services in the Hastings Trial and he immediately resigned his seat, being replaced by his son Richard. A tragic blow fell upon Burke with the loss of Richard in August 1794, to whom he was tenderly attached, and in whom he saw signs of promise, which were not patent to others and which, in fact, appear to have been non-existent (though this view may have rather reflected the fact that Richard Burke had worked successfully in the early battle for Catholic emancipation). King George III, whose favour he had gained by his attitude on the French Revolution, wished to create him Earl of Beaconsfield, but the death of his son deprived the opportunity of such an honour and all its attractions, so the only award he would accept was a pension of £2,500. Even this modest reward was attacked by the Duke of Bedford and the Earl of Lauderdale, to whom Burke replied in his "Letter to a Noble Lord" (1796): "It cannot at this time be too often repeated; line upon line; precept upon precept; until it comes into the currency of a proverb, "To innovate is not to reform"". He argued that he was rewarded on merit, but the Duke of Bedford received his rewards from inheritance alone, his ancestor being the original pensioner: "Mine was from a mild and benevolent sovereign; his from Henry the Eighth". Burke also hinted at what would happen to such people if their revolutionary ideas were implemented, and included a description of the British constitution:

But as to "our" country and "our" race, as long as the well compacted structure of our church and state, the sanctuary, the holy of holies of that ancient law, defended by reverence, defended by power, a fortress at once and a temple, shall stand inviolate on the brow of the British Sion—as long as the British Monarchy, not more limited than fenced by the orders of the State, shall, like the proud Keep of Windsor, rising in the majesty of proportion, and girt with the double belt of its kindred and coeval towers, as long as this awful structure shall oversee and guard the subjected land—so long as the mounds and dykes of the low, fat, Bedford level will have nothing to fear from all the pickaxes of all the levellers of France.

Burke's last publications were the "Letters on a Regicide Peace" (October 1796), called forth by negotiations for peace with France by the Pitt government. Burke regarded this as appeasement, injurious to national dignity and honour. In his "Second Letter", Burke wrote of the French Revolutionary Government: "Individuality is left out of their scheme of government. The State is all in all. Everything is referred to the production of force; afterwards, everything is trusted to the use of it. It is military in its principle, in its maxims, in its spirit, and in all its movements. The State has dominion and conquest for its sole objects—dominion over minds by proselytism, over bodies by arms".

This is held to be the first explanation of the modern concept of totalitarian state. Burke regarded the war with France as ideological, against an "armed doctrine". He wished that France would not be partitioned due to the effect this would have on the balance of power in Europe, and that the war was not against France, but against the revolutionaries governing her. Burke said: "It is not France extending a foreign empire over other nations: it is a sect aiming at universal empire, and beginning with the conquest of France".

In November 1795, there was a debate in Parliament on the high price of corn and Burke wrote a memorandum to Pitt on the subject. In December Samuel Whitbread MP introduced a bill giving magistrates the power to fix minimum wages and Fox said he would vote for it. This debate probably led Burke to editing his memorandum, as there appeared a notice that Burke would soon publish a letter on the subject to the Secretary of the Board of Agriculture, Arthur Young; but he failed to complete it. These fragments were inserted into the memorandum after his death and published posthumously in 1800 as, "Thoughts and Details on Scarcity". In it, Burke expounded "some of the doctrines of political economists bearing upon agriculture as a trade". Burke criticised policies such as maximum prices and state regulation of wages, and set out what the limits of government should be:

That the State ought to confine itself to what regards the State, or the creatures of the State, namely, the exterior establishment of its religion; its magistracy; its revenue; its military force by sea and land; the corporations that owe their existence to its fiat; in a word, to every thing that is "truly and properly" public, to the public peace, to the public safety, to the public order, to the public prosperity.

The economist Adam Smith remarked that Burke was "the only man I ever knew who thinks on economic subjects exactly as I do, without any previous communications having passed between us".

Writing to a friend in May 1795, Burke surveyed the causes of discontent: "I think I can hardly overrate the malignity of the principles of Protestant ascendency, as they affect Ireland; or of Indianism [i.e. corporate tyranny, as practiced by the British East Indies Company], as they affect these countries, and as they affect Asia; or of Jacobinism, as they affect all Europe, and the state of human society itself. The last is the greatest evil". By March 1796, however Burke had changed his mind: "Our Government and our Laws are beset by two different Enemies, which are sapping its foundations, Indianism, and Jacobinism. In some Cases they act separately, in some they act in conjunction: But of this I am sure; that the first is the worst by far, and the hardest to deal with; and for this amongst other reasons, that it weakens discredits, and ruins that force, which ought to be employed with the greatest Credit and Energy against the other; and that it furnishes Jacobinism with its strongest arms against all "formal" Government".

For more than a year prior to his death, Burke knew that his 'stomach' was "irrecoverably ruind". After hearing that Burke was nearing death, Fox wrote to Mrs. Burke enquiring after him. Fox received the reply the next day:

Mrs. Burke presents her compliments to Mr. Fox, and thanks him for his obliging inquiries. Mrs. Burke communicated his letter to Mr. Burke, and by his desire has to inform Mr. Fox that it has cost Mr. Burke the most heart-felt pain to obey the stern voice of his duty in rending asunder a long friendship, but that he deemed this sacrifice necessary; that his principles continue the same; and that in whatever of life may yet remain to him, he conceives that he must live for others and not for himself. Mr. Burke is convinced that the principles which he has endeavoured to maintain are necessary to the welfare and dignity of his country, and that these principles can be enforced only by the general persuasion of his sincerity.

Burke died in Beaconsfield, Buckinghamshire, on 9 July 1797 and was buried there alongside his son and brother. His wife survived him by nearly fifteen years.

Burke is regarded by most political historians in the English-speaking world as the father of modern British conservatism. Burke was utilitarian and empirical in his arguments, while Joseph de Maistre, a fellow conservative from the Continent, was more a providentialist and sociological, and deployed a more confrontational tone in his arguments.

Burke believed that property was essential to human life. Because of his conviction that people desire to be ruled and controlled, the division of property formed the basis for social structure, helping develop control within a property-based hierarchy. He viewed the social changes brought on by property as the natural order of events, which should be taking place as the human race progressed. With the division of property and the class system, he also believed that it kept the monarch in check to the needs of the classes beneath the monarch. Since property largely aligned or defined divisions of social class, class too, was seen as natural—part of a social agreement that the setting of persons into different classes, is the mutual benefit of all subjects. Concern for property is not Burke's only influence. As Christopher Hitchens summarises, "If modern conservatism can be held to derive from Burke, it is not just because he appealed to property owners in behalf of stability but also because he appealed to an everyday interest in the preservation of the ancestral and the immemorial."

Burke's support for Irish Catholics and Indians often led him to be criticised by Tories. His opposition to British imperialism in Ireland and India and his opposition to French imperialism and radicalism in Europe, made it difficult for Whig or Tory to accept Burke wholly as their own.

In the nineteenth century Burke was praised by both liberals and conservatives. Burke's friend Philip Francis wrote that Burke "was a man who truly & prophetically foresaw all the consequences which would rise from the adoption of the French principles" but because Burke wrote with so much passion, people were doubtful of his arguments. William Windham spoke from the same bench in the House of Commons as Burke had, when he had separated from Fox, and an observer said Windham spoke "like the ghost of Burke" when he made a speech against peace with France in 1801. William Hazlitt, a political opponent of Burke, regarded him as amongst his three favourite writers (the others being Junius and Rousseau), and made it "a test of the sense and candour of any one belonging to the opposite party, whether he allowed Burke to be a great man". William Wordsworth was originally a supporter of the French Revolution and attacked Burke in 'A Letter to the Bishop of Llandaff' (1793), but by the early nineteenth century he had changed his mind and came to admire Burke. In his "Two Addresses to the Freeholders of Westmorland" Wordsworth called Burke "the most sagacious Politician of his age" whose predictions "time has verified". He later revised his poem "The Prelude" to include praise of Burke ("Genius of Burke! forgive the pen seduced/By specious wonders") and portrayed him as an old oak. Samuel Taylor Coleridge came to have a similar conversion: he had criticised Burke in "The Watchman", but in his "Friend" (1809–10) Coleridge defended Burke from charges of inconsistency. Later, in his "Biographia Literaria" (1817) Coleridge hails Burke as a prophet and praises Burke for referring "habitually to "principles". He was a "scientific" statesman; and therefore a "seer"". Henry Brougham wrote of Burke: "... all his predictions, save one momentary expression, had been more than fulfilled: anarchy and bloodshed had borne sway in France; conquest and convulsion had desolated Europe...the providence of mortals is not often able to penetrate so far as this into futurity". George Canning believed that Burke's "Reflections" "has been justified by the course of subsequent events; and almost every prophecy has been strictly fulfilled". In 1823 Canning wrote that he took Burke's "last works and words [as] the manual of my politics". The Conservative Prime Minister Benjamin Disraeli "was deeply penetrated with the spirit and sentiment of Burke's later writings".

The 19th-century Liberal Prime Minister William Ewart Gladstone considered Burke "a magazine of wisdom on Ireland and America" and in his diary recorded: "Made many extracts from Burke—"sometimes almost divine"". The Radical MP and anti-Corn Law activist Richard Cobden often praised Burke's "Thoughts and Details on Scarcity". The Liberal historian Lord Acton considered Burke one of the three greatest Liberals, along with William Gladstone and Thomas Babington Macaulay. Lord Macaulay recorded in his diary: "I have now finished reading again most of Burke's works. Admirable! The greatest man since Milton". The Gladstonian Liberal MP John Morley published two books on Burke (including a biography) and was influenced by Burke, including his views on prejudice. The Cobdenite Radical Francis Hirst thought Burke deserved "a place among English libertarians, even though of all lovers of liberty and of all reformers he was the most conservative, the least abstract, always anxious to preserve and renovate rather than to innovate. In politics he resembled the modern architect who would restore an old house instead of pulling it down to construct a new one on the site". Burke's "Reflections on the Revolution in France" was controversial at the time of its publication, but after his death, it was to become his best known and most influential work, and a manifesto for Conservative thinking.

Two contrasting assessments of Burke also were offered long after his death by Karl Marx and Winston Churchill. In "Das Kapital", Marx wrote:
The sycophant—who in the pay of the English oligarchy played the romantic "laudator temporis acti" against the French Revolution just as, in the pay of the North American colonies at the beginning of the American troubles, he had played the liberal against the English oligarchy—was an out-and-out vulgar bourgeois. "The laws of commerce are the laws of Nature, and therefore the laws of God." (E. Burke, l.c., pp. 31, 32) No wonder that, true to the laws of God and Nature, he always sold himself in the best market.
Winston Churchill, in "Consistency in Politics", wrote:
On the one hand [Burke] is revealed as a foremost apostle of Liberty, on the other as the redoubtable champion of Authority. But a charge of political inconsistency applied to this life appears a mean and petty thing. History easily discerns the reasons and forces which actuated him, and the immense changes in the problems he was facing which evoked from the same profound mind and sincere spirit these entirely contrary manifestations. His soul revolted against tyranny, whether it appeared in the aspect of a domineering Monarch and a corrupt Court and Parliamentary system, or whether, mouthing the watch-words of a non-existent liberty, it towered up against him in the dictation of a brutal mob and wicked sect. No one can read the Burke of Liberty and the Burke of Authority without feeling that here was the same man pursuing the same ends, seeking the same ideals of society and Government, and defending them from assaults, now from one extreme, now from the other.
The historian Piers Brendon asserts that Burke laid the moral foundations for the British Empire, epitomised in the trial of Warren Hastings, that was ultimately to be its undoing: when Burke stated that "The British Empire must be governed on a plan of freedom, for it will be governed by no other", this was "...an ideological bacillus that would prove fatal. This was Edmund Burke's paternalistic doctrine that colonial government was a trust. It was to be so exercised for the benefit of subject people that they would eventually attain their birthright—freedom". As a consequence of this opinion, Burke objected to the opium trade, which he called a "smuggling adventure" and condemned "the great Disgrace of the British character in India".

A Royal Society of Arts blue plaque commemorates Burke at 37 Gerrard Street now in London's Chinatown.

One of Burke's largest and most developed critics was Leo Strauss, who was thoroughly analyzed by Steven Lenzner. Strauss in his book "Natural Right and History" makes a series of points in which he somewhat harshly evaluates Burke's writings.

One of the topics that he first addresses is the fact that Burke creates a definitive separation between happiness and virtue, and explains that "Burke, therefore, seeks the foundation of government 'in a conformity to our duties' and not in 'imaginary rights of man"
Strauss views Burke as believing that government should focus solely on the duties that a man should have in society as opposed to trying to address any additional needs or desires. Government is simply a practicality to Burke, and not necessarily meant to function as a tool to help individuals live their best lives. Strauss also argues that in a sense, Burke's theory could be seen as opposing opposes the very idea of forming such philosophies. Burke expresses the view that theory cannot adequately predict future occurrences, and thus, men need to have instincts that can't be practiced or derived from ideology.

This leads to an overarching criticism that Strauss holds regarding Burke, which is his rejection of the use of logic. Burke dismisses a widely held view amongst theorists that reason should be the primary tool in the forming of a constitution or contract. Burke instead believes that constitutions should be made based on 'natural processes' as opposed to rational planning for the future. Strauss points out, however, that criticising rationality actually works against Burke's original stance of returning to traditional ways because some amount human reason is inherent, and therefore is in part grounded in tradition. In regards to this formation of legitimate social order, Strauss does not necessarily support Burke's opinion— that order cannot be established by individual wise people, but exclusively by a culmination of individuals with historical knowledge of past functions to use as a foundation. Strauss notes that Burke would oppose more newly formed republics due to this thought, although Lenzner adds the fact that he did seem to believe that America's constitution could be justified given the specific circumstances. Britain's constitution, on the other hand, was much too radical as it relied too heavily on enlightened reasoning as opposed to traditional methods and values.

Burke's religious writing comprises published works and commentary on the subject of religion. Burke's religious thought was grounded in the belief that religion is the foundation of civil society. He sharply criticised deism and atheism, and emphasised Christianity as a vehicle of social progress. Born in Ireland to a Catholic mother and a Protestant father, Burke vigorously defended the Anglican Church, but also demonstrated sensitivity to Catholic concerns. He linked the conservation of a state (established) religion with the preservation of citizens' constitutional liberties and highlighted Christianity's benefit not only to the believer's soul, but also to political arrangements.

The statement that "The only thing necessary for the triumph of evil is for good men to do nothing" is often attributed to Burke despite the debated origin of this quote. In 1770, however, it is known that in "Thoughts on the Cause of the Present Discontents", Burke wrote that:
John Stuart Mill later made a similar statement in an inaugural address delivered before the University of St. Andrews during 1867:
Burke is sometimes credited with George Santayana's quote: "Those who don't know history are doomed to repeat it", but scholars have not found any reliable evidence indicating that Burke actually spoke (or wrote) those words.







</doc>
<doc id="10033" url="https://en.wikipedia.org/wiki?curid=10033" title="Early music">
Early music

Early music is music, especially Western art music, composed prior to the Classical era. The term generally comprises Medieval music (500–1400) and Renaissance music (1400–1600), but can also include Baroque music (1600–1760), and, according to some authorities such as Kennedy (who excludes Baroque), Ancient music (before 500 AD). According to the UK's National Centre for Early Music, the term "early music" refers to both a repertory (European music written between 1250 and 1750 embracing Medieval, Renaissance and the Baroque) – and a historically informed approach to the performance of that music. However, today this term has come to include "any music for which a historically appropriate style of performance must be reconstructed on the basis of surviving scores, treatises, instruments and other contemporary evidence."

According to Margaret Bent, "Renaissance notation is under-prescriptive by our standards; when translated into modern form it acquires a prescriptive weight that overspecifies and distorts its original openness. Accidentals … may or may not have been notated, but what modern notation requires would then have been perfectly apparent without notation to a singer versed in counterpoint".





</doc>
<doc id="10035" url="https://en.wikipedia.org/wiki?curid=10035" title="Elfenland">
Elfenland

Elfenland is a German-style board game designed by Alan R. Moon and published by Amigo Spiele in German and Rio Grande Games in English in 1998.

It is originally based on his earlier game "Elfenroads" (published by White Wind), but since "Elfenroads" took about four hours for a game, the play was simplified to reduce the time closer to an hour, making it appeal more as a family game.

It won the Spiel des Jahres award in 1998 and won the third place Deutscher Spiele Preis award in 1998.

The game is played by 2–6 players, with 4–5 making for the best game. 
Each player tries to reach as many cities as possible and then return to his "home city." 
Home cities are drawn at the beginning of the game from a pack of city cards and they remain hidden throughout the game.
The game is thus reminiscent of the traveling salesman problem.

Players move using transportation cards. 
Elves can travel on a wide variety of vehicles including troll wagons, elf cycles, rafts, giant pigs, unicorns, dragons and magic clouds. 
Different types of transportation will travel better over different terrain, and some methods of transport cannot cross certain terrains at all. 
There is only one problem: you cannot travel over a route (except water) unless there is a tile on that road, and only the type of transport shown on the tile can be used to move along that road. Before anyone can move, tiles are drawn and laid out across the board. This part is the one that calls for the most strategy, as players try to line up their tiles to set up a nice route for themselves and a difficult one for their opponents at the same time.

As well as normal tiles, each player receives one trouble tile for use during the game.
These hinder other players by forcing them to use an extra transportation card at that point. 
Also, any player can simply use any three cards to pass over any route that has a tile already there, allowing the type of transport shown on the tile to be ignored.

The game has subtle strategies to make others navigate through the cities. 
When a player puts a transportation type you don't want in your path then you have to find a way around it. 
All of the aspects of the game make for a very exciting race to visit the most cities while never quite being sure who is winning until the last round.

There was an expansion for "Elfenland" published, called "Elfengold". 
Note that this is different from the original "Elfengold" published by White Wind. 
The expansion, however, is hard to find.


</doc>
<doc id="10037" url="https://en.wikipedia.org/wiki?curid=10037" title="Euroscepticism">
Euroscepticism

Euroscepticism (also known as EU-scepticism) means criticism of the European Union (EU) and European integration. It can also mean opposition to and total rejection of the EU (anti-EU-ism).

The main sources of Euroscepticism have been notions that integration weakens national sovereignty and the nation state; that there is a democratic deficit in the European Union; that the EU is too bureaucratic; that it encourages high levels of migration; or perceptions that it is a neoliberal organisation which benefits the business elite at the expense of the working class. Euroscepticism is found in political parties across the political spectrum, both left-wing and right-wing. Recently, the rise in populist right-wing parties in Europe is strongly linked to a rise in Euroscepticism on the continent.

Eurobarometer surveys of EU citizens show that trust in the EU and its institutions has declined strongly since a peak in 2007. Since then it has been consistently below 50%. A 2009 survey showed that support for EU membership was lowest in Latvia, the United Kingdom (UK) and Hungary. By 2016, the countries viewing the EU most unfavourably were Greece, France, Spain and the UK. A referendum on continued EU membership was held in the UK in 2016, which resulted in a 51.9% vote in favour of leaving the EU. Since 2015, trust in the EU has risen slightly in most EU countries as a consequence of falling unemployment rates and accelerating economic growth. Euroscepticism should not be confused with anti-Europeanism as the former is internal while the latter is external and the latter refers to rejection of European culture and Europeanisation and sentiments, opinions and discrimination against European ethnic groups. The opposite of Euroscepticism is known as pro-Europeanism (or European Unionism).

While having some overlaps, Euroscepticism and anti-Europeanism are different. Anti-Europeanism has always had a strong influence in American culture and American exceptionalism, which sometimes sees Europe on the decline or as a rising rival power, or both. Some aspects of Euroscepticism in the United Kingdom have been mirrored by U.S. authors.

There can be considered to be several different types of Eurosceptic thought, which differ in the extent to which adherents reject integration between member states of the European Union (EU) and in their reasons for doing so. Aleks Szczerbiak and Paul Taggart described two of these as hard and soft Euroscepticism.

According to Taggart and Szczerbiak, hard Euroscepticism (also called anti-EU-ism) is "a principled opposition to the EU and European integration and therefore can be seen in parties who think that their countries should withdraw from membership, or whose policies towards the EU are tantamount to being opposed to the whole project of European integration as it is currently conceived."

The Europe of Freedom and Direct Democracy group in the European Parliament, typified by such parties as the United Kingdom Independence Party (UKIP), displays hard Euroscepticism. In western European EU member countries, hard Euroscepticism is currently a characteristic of many anti-establishment parties.

Some hard Eurosceptics prefer to call themselves 'Eurorealists' rather than 'sceptics', and regard their position as pragmatic rather than "in principle". Additionally, Tony Benn, a left-wing Labour Party MP who fought against European integration in 1975 by opposing membership of the European Communities in that year's referendum on the issue, emphasised his opposition to xenophobia and his support of democracy, saying: "My view about the European Union has always been not that I am hostile to foreigners, but that I am in favour of democracy [...] I think they're building an empire there, they want us to be a part of their empire and I don't want that."

The Czech president Václav Klaus rejected the term "Euroscepticism" for its purported negative undertones, saying (at a meeting in April 2012) that the expressions for a Eurosceptic and their opponent should be "a Euro-realist" and someone who is "Euro-naïve", respectively.

François Asselineau of the French Popular Republican Union has criticised the use of the term 'sceptic' to describe hard Eurosceptics, and would rather advocate the use of the term "Euro opponent". However, he believes the use of the term 'sceptic' for soft Eurosceptics to be correct, since other Eurosceptic parties in France are "merely criticising" the EU without taking into account the fact that the Treaty on the functioning of the European Union can only be modified with a unanimous agreement of all the EU member states, something he considers impossible to achieve.

Soft Euroscepticism is support for the existence of, and membership of, a form of European Union, but with opposition to specific EU policies; or, in Taggart's and Szczerbiak's words, "where there is NOT a principled objection to European integration or EU membership but where concerns on one (or a number) of policy areas lead to the expression of qualified opposition to the EU, or where there is a sense that 'national interest' is currently at odds with the EU's trajectory." The European Conservatives and Reformists group, typified by centre-right parties such as the British Conservative Party or Czech Civic Democratic Party, along with the European United Left–Nordic Green Left which is an alliance of the left-wing parties in the European Parliament, display soft Euroscepticism.

Some have claimed that there is no clear line between the presumed 'hard' and 'soft' Euroscepticism. Kopecky and Mudde have said that if the demarcation line is the number of and which policies a party opposes, then the question arises of how many must a party oppose and which ones should a party oppose that makes them 'hard' Eurosceptic instead of 'soft'.

Some scholars consider the gradual difference in terminology between 'hard' and 'soft' Euroscepticism inadequate to accommodate the large differences in terms of political agenda. Therefore, "hard Euroscepticism" has also been referred to as "Europhobia" as opposed to mere "Euroscepticism". Other alternative names for 'hard' and 'soft' Euroscepticism include, respectively, "withdrawalist" and "reformist" Euroscepticism.

A survey in , conducted by TNS Opinion and Social on behalf of the European Commission, showed that, across the EU as a whole, those with a positive image of the EU are down from a high of 52% in 2007 to 37% in autumn 2015; this compares with 23% with a negative image of the EU, and 38% with a neutral image.
About 43% of Europeans thought things were "going in the wrong direction” in the EU, compared with 23% who thought things were going "in the right direction" (11% "don't know").
About 32% of EU citizens tend to trust the EU as an institution, and about 55% do not tend to trust it (13% "don't know"). 
Distrust of the EU was highest in Greece (81%), Cyprus (72%), Austria (65%), France (65%) and Germany, the United Kingdom (UK) and the Czech Republic (all 63%).
Overall, more respondents distrusted their own government (66%) than the EU (55%).
Distrust of national government was highest in Greece (82%), Slovenia (80%), Portugal (79%), Cyprus (76%) and France (76%).

A study analysed voting records of the Fifth European Parliament and ranked groups, concluding: "Towards the top of the figure are the more pro-European parties (PES, EPP-ED, and ALDE), whereas towards the bottom of the figure are the more anti-European parties (EUL/NGL, G/EFA, UEN and EDD)."

In 2004, 37 Members of the European Parliament (MEPs) from the UK, Poland, Denmark and Sweden founded a new European Parliament group called "Independence and Democracy" from the old Europe of Democracies and Diversities (EDD) group.

The main goal of the ID group was to reject the proposed Treaty establishing a constitution for Europe. Some delegations within the group, notably that from UKIP, also advocated the complete withdrawal of their country from the EU, while others only wished to limit further European integration.

The elections of 2009 saw a significant fall in support in some areas for Eurosceptic parties, with all such MEPs from Poland, Denmark and Sweden losing their seats. However, in the UK, the Eurosceptic UKIP achieved second place in the election, finishing ahead of the governing Labour Party, and the British National Party (BNP) won its first ever two MEPs. Although new members joined the ID group from Greece and the Netherlands, it was unclear whether the group would reform in the new parliament.

The ID group did reform, as the Europe of Freedom and Democracy (EFD) and is represented by 32 MEPs from nine countries.

The elections of 2014 saw a big anti-establishment vote in favour of eurosceptic parties, which took around a quarter of the seats available. Those that won their national elections included: UKIP in the UK (the first time since 1906 that a party other than Labour or the Conservatives had won a national vote), the National Front in France, the People's Party in Denmark and Syriza in Greece. Second places were taken by Sinn Féin in Ireland and the Five Star Movement in Italy. Herman Van Rompuy, the President of the European Council, agreed following the election to re-evaluate the economic area's agenda and to launch consultations on future policy areas with the 28 member states.

As of 2013, six parties together held all 183 National Council seats, and all bar one of the 62 Federal Council seats and 19 European Parliament seats. The Sozialdemokratische Partei Österreichs (SPÖ - social democrats), which holds 56/183 NC, 24/62 FC, and 5/19 EP seats, is pro-European integration, as is the Österreichische Volkspartei (ÖVP - conservative/Christian), which holds 51/183 NC, 28/62 FC, and 6/19 EP seats, and Die Grünen – Die Grüne Alternative (green), which holds 20/183 NC, 3/62 FC, and 2/19 EP seats.

The Freiheitliche Partei Österreichs (FPÖ), established in 1956, is a right-wing populist party that mainly attracts support from young people and workers. In 1989, it changed its stance over the EU to Euroscepticism. It opposed Austria joining the EU in 1994, and opposed the introduction of the euro in 1998. The party would like to leave the EU if it threatens to develop into a country, or if Turkey joins. The FPÖ received 20–27% of the national vote in the 1990s, and more recently received 17.5% in 2008. It currently has 34/183 National Council seats, 4/62 Federal Council seats, and 2/19 European Parliament seats.

The Bündnis Zukunft Österreich (BZÖ), established in 2005, is a socially conservative party that has always held Eurosceptic elements. In 2011 the party openly supported leaving the eurozone, and in 2012 it announced that it supported a full withdrawal from the European Union. The party has also called upon a referendum on the Lisbon Treaty. In polls it currently receives around 10%–15%, although in one state it did receive 45% of the vote in 2009. It currently has 13/183 National Council seats, 0/62 Federal Council seats, and 1/19 European Parliament seats.

Team Stronach, established in 2012, has campaigned to reform the European Union, as well as to replace the euro with an Austrian Euro. In 2012, it regularly received 8–10% support in national polls. Politicians from many different parties (including the Social Democratic Party and the BZÖ) as well as previous independents switched their allegiances to the new party upon creation. In two local elections in March 2013, it won 11% of the vote in Carinthia, and 10% of the vote in Lower Austria. It currently has 6/183 National Council seats, 1/62 Federal Council seats, and 0/19 European Parliament seats.

Ewald Stadler, a former member of FPÖ (and later of BZÖ) was very Eurosceptic, but in 2011 became member of the European Parliament due to the Lisbon Treaty. Before Stadler accepted the seat, this led to heavy critics by Jörg Leichtfried (SPÖ) "Stadler wants to just rescue his political career" because Stadler before mentioned he would never accept a seat as MEP if this was only due to the Lisbon Treaty. On 23 December 2013 he founded a conservative and Eurosceptic party called The Reform Conservatives.

In the 2014 European Parliament election, the FPÖ increased its vote to 19.72% (up 7.01%), gaining 2 new MEPs, making a total of 4; the party came third, behind the ÖVP and the SPÖ. EU-STOP (the electoral alliance of the EU Withdrawal Party and the Neutral Free Austria Federation) polled 2.76%, gaining no seats, and the Reform Conservatives 1.18%, with Team Stronach putting up no candidates.

The main Eurosceptic party in Belgium is Vlaams Belang.

In the 2014 European Parliament election, Belgium's Vlaams Belang lost over half of its previous vote share, polling 4.26% (down 5.59%) and losing 1 of its 2 MEPs.

Parties with mainly Eurosceptic views are Union of Communists in Bulgaria, NFSB, Attack, and VMRO – BND (also to some degree Bulgaria Without Censorship, which is in a coalition with VMRO – BND, both members of the Eurosceptic European Conservatives and Reformists).
Bulgaria's Minister of Finance, Simeon Djankov, stated in 2011 that ERM II membership to enter the Euro zone would be postponed until after the Eurozone crisis had stabilised.

In the 2014 European Parliament election Bulgaria remained overwhelmingly pro-EU, with the Eurosceptic Attack party receiving 2.96% of the vote, down 9%, with the splinter group National Front for the Salvation of Bulgaria taking 3.05%; neither party secured any MEPs.

Followers of Eurosceptic Attack tore down and trampled the European flag on 3 March 2016 at a meeting of the party in the Bulgarian capital Sofia, dedicated to the commemoration of the 138th anniversary of the liberation of Bulgaria from the Ottoman Empire.

Parties with Eurosceptic views are mainly small right-wing parties like Croatian Party of Rights, Croatian Party of Rights dr. Ante Starčević, Croatian Pure Party of Rights, Autochthonous Croatian Party of Rights, Croatian Christian Democratic Party and Only Croatia – Movement for Croatia.

The only major parliamentary party that is vocally eurosceptic is the Human Shield, whose candidates came fourth in the 2016 parliamentary election, winning 8 of 151 available seats. Their position is generally considered to waver between hard and soft Euroscepticism; it requests thorough reform of the EU so that all member states would be perfectly equal.

In May 2010, the Czech president Václav Klaus claimed that they "needn't hurry to enter the Eurozone".

Petr Mach, an economist, a close associate of president Václav Klaus and a member of the Civic Democratic Party between 1997 and 2007, founded the Free Citizens Party in 2009. The party aims to mainly attract dissatisfied Civic Democratic Party voters. At the time of the Lisbon Treaty ratification, they were actively campaigning against it, supported by the president Vaclav Klaus, who demanded opt-outs such as were granted to the United Kingdom and Poland, unlike the governing Civic Democratic Party, who endorsed it in the Chamber of Deputies. After the treaty has been ratified, Mach's party is in favour of withdrawing from the European Union completely. In the 2014 European Parliament election, the Free Citizens Party won one mandate and allied with UKIP in the Europe of Freedom and Direct Democracy (EFD).

The 2017 Czech legislative election brought into Parliament three Eurosceptic parties. The soft Eurosceptic Civic Democratic Party (ODS) is the second largest, the new hard Eurosceptic Freedom and Direct Democracy (SPD) is the fourth largest and the Communist Party of Bohemia and Moravia (KSČM) that is largely regarded as a Eurosceptic party is the fifth largest party in the Czech parliament.

Parties with mainly Eurosceptic views in Cyprus are New Internationalist Left, the Progressive Party of Working People, Committee for a Radical Left Rally and ELAM.

The People's Movement against the EU only takes part in European Parliament elections and has one member in the European Parliament. The soft Eurosceptic June Movement, originally a split-off from the People's Movement against the EU, existed from 1992 to 2009.

In the Danish Parliament, the Unity List has withdrawal from the EU as a policy. The Danish People's Party also advocates withdrawal, but has claimed to support some EU structures such as the internal market, and supported the EU-positive Liberal-Conservative coalition between 2001 and 2011.

The Socialist People's Party, minorities within the Social Liberal Party and Social Democratic Party, and some smaller parties were against accession to the European Union in 1972. Still in 1986, these parties advocated a no vote in the Single European Act referendum. Later, the Social Liberal Party changed to a strongly EU-positive party, and EU opposition within the Social Democratic Party faded. The Socialist People's Party were against the Amsterdam Treaty in 1998 and Denmark's joining the euro in 2000, but has become increasingly EU-positive, for example when MEP Margrete Auken left the European United Left–Nordic Green Left and joined The Greens–European Free Alliance in 2004.

In the 2014 European Parliament election, the Danish People's Party came first by a large margin with 26.6% of the vote, gaining 2 extra seats for a total of 4 MEPs. The People's Movement against the EU polled 8.1%, retaining its single MEP.

The Independence Party and Centre Party were against accession to the EU, but only the Independence Party still wants Estonia to withdraw from the EU. The Conservative People's Party (EKRE) also has some Eurosceptic policies.

The largest Eurosceptic party in Finland is the Finns Party. In the European Parliament election, 2014, the Finns Party increased their vote share by 3.1% to 12.9%, adding a second MEP.

In Eurobarometer 77 (fieldwork in Spring 2012), 41% of Finns trusted the European Union (EU-27 average: 31%), 51% trusted The European Parliament (EU-27average: 40%), and 74% were in favour of the euro currency (EU-27 average: 52%).

In France there are multiple parties that are Eurosceptic to different degrees, varying from advocating less EU intervention in national affairs, to advocating outright withdrawal from the EU and the Eurozone. These parties belong to all sides of the political spectrum, so the reasons for their Euroscepticism may differ. In the past many French people appeared to be uninterested in such matters, with only 40% of the French electorate voting in the 2009 European Parliament elections.

Right-wing Eurosceptic parties include the Gaullist "Debout la République", and "Mouvement pour la France", which was part of Libertas, a pan-European Eurosceptic party. In the 2009 European Parliament elections, Debout la République received 1.77% of the national vote, and Libertas 4.8%. In a similar way to some moderate parties, the French right and far-right in general are naturally opposed to the EU, as they criticise France's loss of political and economic sovereignty to a supranational entity. Some of these hard Eurosceptic parties include the Popular Republican Union and the Front National (FN). Front National and Popular Republican Union both seek France's withdrawal from the EU and the euro, although Popular Republican Union also seeks France's withdrawal from NATO. The FN received 33.9% of votes in the French presidential election, 2017, making it the largest Eurosceptic party in France.

Eurosceptic parties on the left in France tend to criticise what they see as the neoliberal agenda of the EU, as well as the elements of its structure which are undemocatic and seen as top-down. These parties include the "Parti de Gauche" and the French Communist Party, which formed the "Front de Gauche" for the 2009 European Parliament elections and received 6.3% of the votes. The leader of the Left Front defends a complete reform of the Monetary Union, rather than the withdrawal of France from the Eurozone. Some of the major far-left Eurosceptic parties in France include the New Anticapitalist Party which received 4.8% and Lutte Ouvrière which received 1.2%. The Citizen and Republican Movement, a left-wing Eurosceptic and souverainist party, have not participated in any elections for the European Parliament.

The party "Chasse, Pêche, Nature & Traditions", is an agrarianist Eurosceptic party that claims to be neither left nor right.

In the European Parliament election, 2014, the National Front won the elections with 24.85% of the vote, a swing of 18.55%, winning 24 seats, up from 3 previously. The former French President François Hollande had called for the EU to be reformed and for a scaling back of its power.

The Alternative for Germany (AfD) is Germany's largest Eurosceptic party. It has been elected into the German Parliament with 94 seats in September 2017.
Initially the AfD was a soft Eurosceptic party, that considered itself pro-Europe and pro-EU, but opposed the euro, which it believed had undermined European integration.

In the European Parliament election, 2014, the Alternative for Germany came 5th with 7% of the vote, winning 7 seats and is a member of the Eurosceptic European Conservatives and Reformists. The Alternative for Germany went on to take seats in three state legislatures in the Autumn of 2014.

The party became purely Eurosceptic in 2015, when a split occurred in the party, leading to Frauke Petry's leadership and a more hard line approach to the European Union.

In July 2015 a split from AfD created a new soft Eurosceptic party called Alliance for Progress and Renewal.

Golden Dawn, Communist Party of Greece (KKE), ANEL, Course of Freedom, Popular Unity, and LAOS are the main Eurosceptic parties in Greece. According to the London School of Economics, Greece is the 2nd most Eurosceptic country in the European Union, with 50% (only behind UK) of the Greeks thinking that their country has not benefited at all from the EU. Meanwhile, 33% of the Greeks views Greek membership in EU as a good thing, marginally ahead of UK. 81% of the Greeks say that the EU is going in the wrong direction. These figures represent a major increase in Euroscepticism in Greece since 2009. 

In June 2012, the Eurosceptic parties in Greece that were represented in the parliament before the Election in January 2015 (ANEL, Golden Dawn, KKE) got 45.8% of the votes and 40.3% of the seats in the parliament. In the legislative election of January 2015 the pro-European (left and right-wing) parties (ND, PASOK, Potami, KIDISO, EK and Prasinoi-DIMAR) got 43.28% of the votes. The Eurosceptic parties got 54.64%. The Eurosceptic left (KKE, ANTARSYA-MARS and KKE (M–L)/M–L KKE) got 42.58% of the votes and the Eurosceptic right (Golden Dawn, ANEL and LAOS) got 12.06% of the votes, with Syriza ahead with 36.34%. The Eurosceptic parties got 194 seats in the new parliament and the pro-EU parties got 106 seats. 

According to the polls conducted in June and July 2015 (12 polls), the Eurosceptic left would get on average 48.03% (excluding extraparliamentary parties as ANTARSYA-MARS and KKE (m–l)/ML-KKE), the parliamentary pro-EU parties (Potami, New Democracy and PASOK) would get 33.82%, the extra-parliamentary (not represented in the Hellenic Parliament) pro-EU parties (KIDISO and EK) would get 4.44% and the Eurosceptic right would get 10.2% (excluding extraparliamentary parties, such as LAOS, not displayed on recent opinion polls). The soft Eurosceptic parties would get 42.31%, the hard Eurosceptic parties (including KKE, ANEL and Golden Dawn) would get 15.85%, and the pro-EU parties (including extra-parliamentary parties displayed on opinion polls) would get 38.27% of the votes.

In the European Parliament election, 2014, Syriza won the election with 26.58% of the vote (a swing of 21.88%) taking 6 seats (up 5), with Golden Dawn coming 3rd taking 3 seats, the Communist Party taking 2 seats and the Independent Greeks gaining their first ever seat. Syriza's leader Tsipras said he's not anti-European and does not want to leave the euro. According to "The Economist", Tsipras is willing to negotiate with Greece's European partners, and it is believed a Syriza victory could encourage radical leftist parties across Europe. Alexis Tsipras vowed to reverse many of the austerity measures adopted by Greece since a series of bailouts began in 2010, at odds with the Eurogroup's positions.
The current government coalition in Greece is composed by Syriza and ANEL (right-wing hard Eurosceptic party, led by Panos Kammenos, who is the current Minister of Defence).

Viktor Orbán is the soft Eurosceptic Prime Minister of Hungary for the national-conservative Fidesz Party. A hardline Eurosceptic party in Hungary is Jobbik, a radical, xenophobic and far-right party.

In Hungary 39% of the population have a positive image of the EU, 20% have a negative image, and 40% neutral (1% "Don't know").

In the 2014 Hungarian parliamentary election, Fidesz got 44.54% of the votes, Jobbik got 20.54% of the votes and the communist Hungarian Workers' Party got 0.58% of the votes. Thus, Eurosceptic parties in Hungary obtained 65.66% of the votes, one of the highest figures in Europe.

The green-liberal Politics Can Be Different classifies as a soft or reformist Eurosceptic party given its self-professed "euro-critical" stance. During the European parliamentary campaign of 2014 party Co-President András Schiffer described LMP as having a pronounced pro-integration position on environmental, wage and labour policy however, as supporting member state autonomy on the self-determination of local communities concerning land resources. So as to combat the differentiated integration of the multi-speed Europe which discriminates against Eastern and Southern member states, LMP would like to initiate an eco-social market economy within the union.

Euroscepticism is a minority view in Ireland, with opinion polls in 2016 indicating around 80% support for membership of the European Union (EU).

The Irish people initially voted against ratifying the Nice and Lisbon Treaties. However following renegotiations, second referendums on both were passed with approximately 2:1 majorities in both cases. Some commentators and smaller political groups questioned the validity of the Irish Government's decision to call second referendums.

The left-wing Irish republican party Sinn Féin expresses soft Eurosceptic positions on the current structure of the European Union and the direction in which it is moving. The party expresses, "support for Europe-wide measures that promote and enhance human rights, equality and the all-Ireland agenda", but has a "principled opposition" to a European superstate. However, in its manifesto for the 2015 UK general election, Sinn Féin pledged that the party would campaign for the UK to stay within the EU. In the last European Parliament election in 2014, Sinn Féin won 3 seats coming second with 19.5% of the vote up 8.3%.

The Trotskyist organisation, the Socialist Party, supports Ireland leaving the EU and supported the Brexit result. It argues that the European Union is institutionally capitalist and neoliberal. The Socialist Party campaigned against the Lisbon and Nice Treaties and favours the foundation of an alternative Socialist European Union.

The Five Star Movement (M5S), an anti-establishment movement founded by the former comedian Beppe Grillo, is often considered a Eurosceptic party. The M5S gained 25.5% of vote in the 2013 general election, becoming the largest anti-establishment and Eurosceptic party in Europe. The party also advocates a non-binding referendum on the withdrawal of Italy from the Eurozone (but not from the European Union) and the return to the lira. The M5S's popular support is evenly distributed all across Italy, but in 2013 the party was particularly strong in Sicily, Liguria and Marche, where it gained more than 30% of the vote.
Another Eurosceptic party is Lega Nord, a regionalist movement led by Matteo Salvini favouring Italy's exit from the Eurozone and the re-introduction of the lira. When in government, LN however approved the Treaty of Lisbon. The party won 6.2% of the vote in the 2014 European Parliament elections, but two of its leading members are presidents of Lombardy and Veneto (where LN gained 40.9% of the vote in 2015).

In the European Parliament election, 2014 the Five Star Movement came second, gaining 17 seats and 21.2% of the vote in contesting EP seats for the first time. Lega Nord took 5 seats and The Other Europe with Tsipras gained 3 seats.

Other minor eurosceptic organizations include right-wing political parties (e.g., Brothers of Italy , Tricolour Flame, New Force, National Front, CasaPound, National Movement for Sovereignty, the No Euro Movement), left-wing political parties (e.g., the Communist Party of Marco Rizzo, the Italian Communist Party) and other political movements (e.g., the Sovereignist Front, MMT Italy). In addition, the European Union is criticized (especially for the austerity and the creation of the euro) by some left-wing thinkers, like the trade unionist Giorgio Cremaschi and the journalist Paolo Barnard, and some academics, such as the economists Alberto Bagnai and Vladimiro Giacchè, the philosopher Diego Fusaro and the mathematician Marino Badiale.

According to 22 opinion polls conducted in July 2017, the pro-EU parties that were polled (Democratic Party, "Forza Italia", Democrats and Progressives and Popular Alternative) would get, on average, 46.64% of the votes, while the Eurosceptic parties (Five Star Movement, "Lega Nord", Us with Salvini, Italian Left and Brothers of Italy) would get 49.02% of the votes. According to the Standard Eurobarometer 87 conducted by the European Commission in the spring of 2017, 48% of Italians tend not to trust the European Union compared to 36% of Italians who trust it.

The National Alliance (For Fatherland and Freedom/LNNK/All for Latvia!), Union of Greens and Farmers and For Latvia from the Heart are parties that are described by some political commentators as bearing soft Eurosceptic views. A small hard Eurosceptic party exists, but it has failed to gain any administrative seats throughout history of its existence.

The Order and Justice party has mainly Eurosceptic views.

The Alternative Democratic Reform Party is a soft Eurosceptic party. It is a member of the Alliance of European Conservatives and Reformists.

Parties with mainly Eurosceptic views were Labour Party and "Libertas Malta".

The Labour Party was not in favour of Malta entering the European Union. However, it was in favour of a partnership with the EU. After a long battle, the Nationalist Party led by Eddie Fenech Adami won the referendum and the following election, making Malta one of the states to enter the European Union on 1 May 2004. The party is now pro-European.

The Libertas Party is inactive, as of 2016.

Historically, the Netherlands have been a very pro-European country, being one of the six founding members of the European Coal and Steel Community in 1952, and campaigning with much effort to include the United Kingdom into the Community in the 1970s and others after that. It has become slightly more Eurosceptic in the 2000s, rejecting the European Constitution in 2005 and complaining about the relatively high financial investment into the Union or the democratic deficit amongst other issues.

Despite these concerns, in 2014 the majority of the Dutch electorate continued to support parties that favour ongoing European integration: the Social Democrats, the Christian Democrats, the Liberals, but most of all the (Liberal) Democrats.

In 2016, a substantial majority in a low-turnout referendum rejected the ratification of an EU trade and association treaty with Ukraine.

Parties with mainly Eurosceptic views are Liberty, Congress of the New Right, National Movement (together with Real Politics Union) and Law and Justice, the current ruling party in Poland.

The late president of Poland Lech Kaczyński resisted the signature of the Treaty of Lisbon, namely in what concerned to the Charter of Fundamental Rights of the European Union. Subsequently, Poland got an opt-out from this charter. As Polish President, Kaczyński also slammed the Polish government's intentions to join the eurozone.

In 2015, it was reported that Euroscepticism was growing in Poland, which was thought to be due to the "economic crisis, concern over perceived interference from Brussels and migration". Polish president Andrzej Duda indicated that he wished for Poland to step back from further EU integration. He suggested the country would "hold a referendum on joining the euro, resist further integration and fight the EU’s green policies".

The main Eurosceptic parties in Portugal are National Renovator Party (PNR), Portuguese Communist Party (PCP), and Left Bloc (BE). Opinion polling in Portugal in 2015 indicated that 48 per cent tended not to trust the EU, while 79 per cent tended not to trust the Portuguese government (then lead by Portugal Ahead). Eurosceptic political parties hold a combined total of 34 seats out of 230 in Portugal's parliament (BE 19, PCP 15, PNR 0) and a combined total of 4 out of Portugal's 21 seats in the European parliament (PCP 3, BE 1, PNR 0).

In the last European Parliament election, 2014, the Portuguese Communist Party won three seats and the Left Bloc won one seat.

Several parties espousing Eurosceptic views exist on the right, such as the New Republic the Greater Romania Party and Noua Dreaptă, but as of August 2016 none of these parties are represented in European Parliament. Euroscepticism is relatively unpopular in Romania; all mainstream political parties are pro-European and a 2015 survey found 65% of Romanians had a positive view of the country's EU membership.

Parties with primarily hard Eurosceptic views represented in the National Council are People's Party - Our Slovakia and We Are Family. Prominent Slovak Eurosceptic politicians include Richard Sulík, Boris Kollár and Marian Kotleba. Soft Eurosceptic views are represented in Freedom and Solidarity, Slovak National Party, Ordinary People and Independent Personalities and New Majority.

Parties with mainly Eurosceptic views are Slovenian National Party and United Left.

Candidatura d'Unitat Popular, a left-wing to far-left political party with about 1,300 members, advocates independence for Catalonia outside of the European Union.

Spain was one of the few countries to vote Yes for the European Constitution in a referendum in February 2005, though by a lower margin in Catalonia and the Basque Country.
However, trust in the EU later declined. , according to a Eurobarometer public opinion survey, 61 per cent of the Spanish people did not trust the EU, compared to 25% that trust it (14% "don't know").

The Left Party of Sweden is against accession to the European Union and wants Sweden to leave the European Union.

The right-wing populist party Sweden Democrats are also strongly against the Union and favour withdrawal from the EEA.

The June List, a Eurosceptic list consisting of members from both the political right and left won three seats in the 2004 Elections to the European Parliament and sat in the EU-critical IND/DEM group in the European Parliament. The movement favours a withdrawal from the EU.

Around 80% of the Riksdag members represent parties that officially supports the Sweden membership.

In the European Parliament election, 2014, the Sweden Democrats gained 2 seats with 9.67% of the vote, up 6.4%, and the Left Party took one seat with 6.3% of the vote.

Euroscepticism in the United Kingdom has been a significant element in British politics since the inception of the European Economic Community (EEC), the predecessor to the EU. The European Union strongly divides the British public, political parties, media and civil society.

The UK Independence Party has backed the idea of the UK's unilaterally leaving the European Union (Brexit) since its inception. During the 23 June 2016 referendum on the issue, the Conservatives had no official position on the issue; although its leader David Cameron was in favour of remaining in the EU, the party was divided on the issue. The Labour Party officially supported remaining in the EU, although party leader Jeremy Corbyn did suggest early on in the campaign that he would consider withdrawal. The Liberal Democrats were the most adamantly pro-EU party, and since the referendum, pro-Europeanism has been their main policy.

The referendum resulted in an overall vote to leave the EU, as opposed to remaining an EU member, by 51.9% to 48.1%, on a turnout of 72.2%. The vote was split between the constituent countries of the United Kingdom, with a majority in England and Wales voting to leave, and a majority in Scotland and Northern Ireland, as well as Gibraltar (a British Overseas Territory), voting to remain. As a result of the referendum, the government notified the EU of its intention to withdraw on 29 March 2017 by invoking Article 50 of the Lisbon Treaty.

The three main Eurosceptic parties in Iceland are the Independence Party, Left-Green Movement and the Progressive Party. The Independence Party and the Progressive Party won the parliamentary election in April 2013 and they have halted the current negotiations with the European Union regarding Icelandic membership and tabled a parliamentary resolution on 21 February 2014 to withdraw the application completely.

The two main Eurosceptic parties in Moldova are the left-wing Party of Socialists of the Republic of Moldova, which officially declared its main purpose to be the integration of Moldova in the Eurasian Economic Union and the Party of Communists of the Republic of Moldova, even if nowadays its leader speech became more soft on the issue of Euroscepticism. As of November 2014 both parties are represented in Moldovan Parliament, with 45 MPs out of a total of 101 MPs.

All parliamentary parties in Montenegro officially support the country's bid for accession to the European Union. The only party that rejected the European integration and instead publicly advocates a tighter political and economic integration with Russia was the non-parliamentary far-right party Serb List.

Norway has rejected EU membership in two referendums, 1972 and 1994. The Centre Party, Christian Democratic Party, Socialist Left Party and Liberal Party were against EU membership in both referendums. The Centre Party, Socialist Left Party, Capitalist Party, Christians and Red Party are also against Norway's current membership of the European Economic Area.

Parties with mainly Eurosceptic views are the Communist Party of the Russian Federation, United Russia and Liberal Democratic Party of Russia.

Following the 2014 Crimean crisis, the European Union issued sanctions on the Russian Federation "in response to the illegal annexation of Crimea and deliberate destabilisation of a neighbouring sovereign country". In response to this, Alexey Borodavkin – Russia's permanent representative with the UN – said "The EU is committing a direct violation of human rights by its actions against Russia. The unilateral sanctions introduced against us are not only illegitimate according to international law, they also undermine Russian citizens' freedom of travel, freedom of development, freedom of work and others". In the same year, Russian president Vladimir Putin said: "What are the so-called European values? Maintaining the coup, the armed seizure of power and the suppression of dissent with the help of the armed forces?"

A referendum was held in the landlocked microstate on 20 October 2013 in which the citizens were asked whether the country should submit an application to join the European Union. The proposal was rejected because of a low turnout, even if 50.3% of voters approved it. The "Yes" campaign was supported by the main left-wing parties (Socialist Party, United Left) and the Union for the Republic whereas the Sammarinese Christian Democratic Party suggested voting with a blank ballot, the Popular Alliance declared itself neutral, and We Sammarinese and the RETE movement supported the "No" campaign. The Citizens' Rights Directive, which defines the right of free movement for the European citizens, may have been an important reason for those voting no.

Parties with mainly Eurosceptic views are Serbian Radical Party, Democratic Party of Serbia, Dveri and Serbian People's Party of Nenad Popović.

Switzerland has long been known for its neutrality in international politics. Swiss voters rejected EEA membership in 1992, and EU membership in 2001. Despite the passing of several referendums calling for closer relations between Switzerland and the European Union such as the adoption of bilateral treaties and the joining of the Schengen Area, a second referendum of the joining of the EEA or the EU is not expected, and the general public remains opposed to joining.

In February 2014, the Swiss voters narrowly approved a referendum limiting the freedom of movement of EU citizens to Switzerland.

Eurosceptic political parties include the Swiss People's Party, which is the largest political party in Switzerland, with 29.4% of the popular vote as of the 2015 federal election. Smaller Eurosceptic parties include, but are not limited to, the Federal Democratic Union, the Ticino League, and the Geneva Citizens' Movement, all of which are considered right-wing parties.

In addition, the Campaign for an Independent and Neutral Switzerland is a political organisation in Switzerland that is strongly opposed to the European Union.

Regionally, the German-speaking majority of Switzerland is the most Eurosceptic, while the French-speaking Switzerland tends to be more pro-EU. However, in the 2001 referendum, the majority of French-speakers voted against EU membership. According to a 2016 survey conducted by M.I.S Trend and published in "L'Hebdo", 69 percent of the Swiss population supports systematic border controls, and 53 percent want restrictions on the EU accord of the free movements of peoples and 14 percent want it completely abolished. However, 54% of the Swiss population said that if necessary, they would ultimately keep the freedom of movement of people's accord.

The two main Eurosceptic parties are the far-right ultranationalist, Nationalist Movement Party (MHP), which secured 16.29% of votes, and 40 seats in the Parliament at the last election, and the Felicity Party (Saadet Partisi), a far-right Sunni Islamist party, which has no seats in the Parliament, as it only secured 0.68% of the votes in the last election, far below the 10% threshold necessary to be represented in the Parliament.

Many left-wing nationalist and far-left parties hold no seats at parliament but they control many activist and student movements in Turkey. The Patriotic Party (formerly called Workers' Party) consider the European Union as a frontrunner of global imperialism.

Parties with mainly Eurosceptic views are Party of Regions, Communist Party of Ukraine and Right Sector.

The far-right Ukrainian group Right Sector opposes joining the European Union. It regards the EU as an "oppressor" of European nations.




</doc>
<doc id="10042" url="https://en.wikipedia.org/wiki?curid=10042" title="EAN">
EAN

EAN may refer to:



Ean-name, full caucasian

</doc>
<doc id="10043" url="https://en.wikipedia.org/wiki?curid=10043" title="Estimator">
Estimator

In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished.

There are point and interval estimators. The point estimators yield single-valued results, although this includes the possibility of single vector-valued results and results that can be expressed as a single function. This is in contrast to an interval estimator, where the result would be a range of plausible values (or vectors or functions).

Estimation theory is concerned with the properties of estimators; that is, with defining properties that can be used to compare different estimators (different rules for creating estimates) for the same quantity, based on the same data. Such properties can be used to determine the best rules to use under given circumstances. However, in robust statistics, statistical theory goes on to consider the balance between having good properties, if tightly defined assumptions hold, and having less good properties that hold under wider conditions.

An "estimator" or "point estimate" is a statistic (that is, a function of the data) that is used to infer the value of an unknown parameter in a statistical model. The parameter being estimated is sometimes called the "estimand". It can be either finite-dimensional (in parametric and semi-parametric models), or infinite-dimensional (semi-parametric and non-parametric models). If the parameter is denoted formula_1 then the estimator is traditionally written by adding a circumflex over the symbol: formula_2. Being a function of the data, the estimator is itself a random variable; a particular realization of this random variable is called the "estimate". Sometimes the words "estimator" and "estimate" are used interchangeably.

The definition places virtually no restrictions on which functions of the data can be called the "estimators". The attractiveness of different estimators can be judged by looking at their properties, such as unbiasedness, mean square error, consistency, asymptotic distribution, etc.. The construction and comparison of estimators are the subjects of the estimation theory. In the context of decision theory, an estimator is a type of decision rule, and its performance may be evaluated through the use of loss functions.

When the word "estimator" is used without a qualifier, it usually refers to point estimation. The estimate in this case is a single point in the parameter space. There also exists an other type of estimator: interval estimators, where the estimates are subsets of the parameter space.

The problem of density estimation arises in two applications. Firstly, in estimating the probability density functions of random variables and secondly in estimating the spectral density function of a time series. In these problems the estimates are functions that can be thought of as point estimates in an infinite dimensional space, and there are corresponding interval estimation problems.

Suppose there is a fixed "parameter" formula_1 that needs to be estimated. Then an "estimator" is a function that maps the sample space to a set of "sample estimates". An estimator of formula_1 is usually denoted by the symbol formula_2. It is often convenient to express the theory using the algebra of random variables: thus if "X" is used to denote a random variable corresponding to the observed data, the estimator (itself treated as a random variable) is symbolised as a function of that random variable, formula_6. The estimate for a particular observed data value formula_7 (i.e. for formula_8) is then formula_9, which is a fixed value. Often an abbreviated notation is used in which formula_2 is interpreted directly as a random variable, but this can cause confusion.

The following definitions and attributes are relevant.

For a given sample formula_11, the "error" of the estimator formula_2 is defined as
where formula_14 is the parameter being estimated. Note that the error, "e", depends not only on the estimator (the estimation formula or procedure), but also on the sample.
The "mean squared error" of formula_2 is defined as the expected value (probability-weighted average, over all samples) of the squared errors; that is,
It is used to indicate how far, on average, the collection of estimates are from the single parameter being estimated. Consider the following analogy. Suppose the parameter is the bull's-eye of a target, the estimator is the process of shooting arrows at the target, and the individual arrows are estimates (samples). Then high MSE means the average distance of the arrows from the bull's-eye is high, and low MSE means the average distance from the bull's-eye is low. The arrows may or may not be clustered. For example, even if all arrows hit the same point, yet grossly miss the target, the MSE is still relatively large. Note, however, that if the MSE is relatively low, then the arrows are likely more highly clustered (than highly dispersed).
For a given sample formula_11, the "sampling deviation" of the estimator formula_2 is defined as
where formula_20 is the expected value of the estimator. Note that the sampling deviation, "d", depends not only on the estimator, but also on the sample.

The "variance" of formula_2 is simply the expected value of the squared sampling deviations; that is, formula_22. It is used to indicate how far, on average, the collection of estimates are from the "expected value" of the estimates. Note the difference between MSE and variance. If the parameter is the bull's-eye of a target, and the arrows are estimates, then a relatively high variance means the arrows are dispersed, and a relatively low variance means the arrows are clustered. Some things to note: even if the variance is low, the cluster of arrows may still be far off-target, and even if the variance is high, the diffuse collection of arrows may still be unbiased. Finally, note that even if all arrows grossly miss the target, if they nevertheless all hit the same point, the variance is zero.

The "bias" of formula_2 is defined as formula_24. It is the distance between the average of the collection of estimates, and the single parameter being estimated. Note that the bias of formula_2 is a function of the true value of formula_26 so saying that the bias of formula_2 is formula_28 means that for every formula_26 the bias of formula_2 is formula_28. 

The bias also is the expected value of the error, since formula_32. If the parameter is the bull's-eye of a target, and the arrows are estimates, then a relatively high absolute value for the bias means the average position of the arrows is off-target, and a relatively low absolute bias means the average position of the arrows is on target. They may be dispersed, or may be clustered. The relationship between bias and variance is analogous to the relationship between accuracy and precision.

The estimator formula_2 is an "unbiased estimator" of formula_1 if and only if formula_35. Note that bias is a property of the estimator, not of the estimate. Often, people refer to a "biased estimate" or an "unbiased estimate," but they really are talking about an "estimate from a biased estimator," or an "estimate from an unbiased estimator." Also, people often confuse the "error" of a single estimate with the "bias" of an estimator. Just because the error for one estimate is large, does not mean the estimator is biased. In fact, even if all estimates have astronomical absolute values for their errors, if the expected value of the error is zero, the estimator is unbiased. Also, just because an estimator is biased, does not preclude the error of an estimate from being zero (we may have gotten lucky). The ideal situation, of course, is to have an unbiased estimator with low variance, and also try to limit the number of samples where the error is extreme (that is, have few outliers). Yet unbiasedness is not essential. Often, if just a little bias is permitted, then an estimator can be found with lower MSE and/or fewer outlier sample estimates.

An alternative to the version of "unbiased" above, is "median-unbiased", where the median of the distribution of estimates agrees with the true value; thus, in the long run half the estimates will be too low and half too high. While this applies immediately only to scalar-valued estimators, it can be extended to any measure of central tendency of a distribution: see median-unbiased estimators.


A consistent sequence of estimators is a sequence of estimators that converge in probability to the quantity being estimated as the index (usually the sample size) grows without bound. In other words, increasing the sample size increases the probability of the estimator being close to the population parameter.

Mathematically, a sequence of estimators } is a consistent estimator for parameter "θ" if and only if, for all , no matter how small, we have

The consistency defined above may be called weak consistency. The sequence is "strongly consistent", if it converges almost surely to the true value.

An estimator that converges to a "multiple" of a parameter can be made into a consistent estimator by multiplying the estimator by a scale factor, namely the true value divided by the asymptotic value of the estimator. This occurs frequently in estimation of scale parameters by measures of statistical dispersion.

An asymptotically normal estimator is a consistent estimator whose distribution around the true parameter "θ" approaches a normal distribution with standard deviation shrinking in proportion to formula_38 as the sample size "n" grows. Using formula_39 to denote convergence in distribution, "t" is asymptotically normal if
for some "V".

In this formulation "V/n" can be called the "asymptotic variance" of the estimator. However, some authors also call "V" the "asymptotic variance".
Note that convergence will not necessarily have occurred for any finite "n", therefore this value is only an approximation to the true variance of the estimator, while in the limit the asymptotic variance (V/n) is simply zero. Stated a little more accurately, the distribution of the estimator "t" converges weakly to a dirac delta function centered at formula_26.

The central limit theorem implies asymptotic normality of the sample mean formula_42 as an estimator of the true mean.
More generally, maximum likelihood estimators are asymptotically normal under fairly weak regularity conditions — see the asymptotics section of the maximum likelihood article. However, not all estimators are asymptotically normal; the simplest examples are found when the true value of a parameter lies on the boundary of the allowable parameter region.

Two naturally desirable properties of estimators are for them to be unbiased and have minimal mean squared error (MSE). These cannot in general both be satisfied simultaneously: a biased estimator may have lower mean squared error (MSE) than any unbiased estimator; see estimator bias.

Among unbiased estimators, there often exists one with the lowest variance, called the minimum variance unbiased estimator (MVUE). In some cases an unbiased efficient estimator exists, which, in addition to having the lowest variance among unbiased estimators, satisfies the Cramér–Rao bound, which is an absolute lower bound on variance for statistics of a variable.

Concerning such "best unbiased estimators", see also Cramér–Rao bound, Gauss–Markov theorem, Lehmann–Scheffé theorem, Rao–Blackwell theorem.

See: Robust estimator, Robust statistics




</doc>
<doc id="10045" url="https://en.wikipedia.org/wiki?curid=10045" title="Emerald">
Emerald

Emerald is a precious gemstone and a variety of the mineral beryl (BeAl(SiO)) colored green by trace amounts of chromium and sometimes vanadium. Beryl has a hardness of 7.5–8 on the Mohs scale. Most emeralds are highly included, so their toughness (resistance to breakage) is classified as generally poor. Emerald is a cyclosilicate.

The word "emerald" is derived (via and ), from Vulgar Latin: "esmaralda"/"esmaraldus", a variant of Latin "smaragdus", which originated in (smaragdos; "green gem").

Emeralds, like all colored gemstones, are graded using four basic parameters–the four "C"s of connoisseurship: "color", "clarity," "cut" and "carat weight". Normally, in the grading of colored gemstones, color is by far the most important criterion. However, in the grading of emeralds, clarity is considered a close second. A fine emerald must possess not only a pure verdant green hue as described below, but also a high degree of transparency to be considered a top gem.

In the 1960s, the American jewelry industry changed the definition of "emerald" to include the green vanadium-bearing beryl. As a result, "vanadium emeralds" purchased as emeralds in the United States are not recognized as such in the UK and Europe. In America, the distinction between traditional emeralds and the new vanadium kind is often reflected in the use of terms such as "Colombian emerald".

In gemology, color is divided into three components: "hue", "saturation", and "tone". Emeralds occur in hues ranging from yellow-green to blue-green, with the primary hue necessarily being green. Yellow and blue are the normal secondary hues found in emeralds. Only gems that are medium to dark in tone are considered emeralds; light-toned gems are known instead by the species name "green beryl". The finest emeralds are approximately 75% tone on a scale where 0% tone is colorless and 100% is opaque black. In addition, a fine emerald will be saturated and have a hue that is bright (vivid). Gray is the normal saturation modifier or mask found in emeralds; a grayish-green hue is a dull-green hue.

Emeralds tend to have numerous inclusions and surface breaking fissures. Unlike diamonds, where the loupe standard, i.e. 10× magnification, is used to grade clarity, emeralds are graded by eye. Thus, if an emerald has no visible inclusions to the eye (assuming normal visual acuity) it is considered flawless. Stones that lack surface breaking fissures are extremely rare and therefore almost all emeralds are treated ("oiled", see below) to enhance the apparent clarity. The inclusions and fissures within an emerald are sometime described as "jardin" (French for "garden"), because of their mossy appearance. Imperfections are unique for each emerald and can be used to identify a particular stone. Eye-clean stones of a vivid primary green hue (as described above), with no more than 15% of any secondary hue or combination (either blue or yellow) of a medium-dark tone, command the highest prices. The relative non-uniformity motivates the cutting of emeralds in cabochon form, rather than faceted shapes. Faceted emeralds are most commonly given an oval cut, or the signature emerald cut, a rectangular cut with facets around the top edge.

Most emeralds are oiled as part of the post-lapidary process, in order to fill in surface-reaching cracks so that clarity and stability are improved. Cedar oil, having a similar refractive index, is often used in this widely adopted practice. Other liquids, including synthetic oils and polymers with refractive indexes close to that of emeralds, such as "Opticon", are also used. These treatments are typically applied in a vacuum chamber under mild heat, to open the pores of the stone and allow the fracture-filling agent to be absorbed more effectively. The U.S. Federal Trade Commission requires the disclosure of this treatment when an oil treated emerald is sold. The use of oil is traditional and largely accepted by the gem trade, although oil treated emeralds are worth much less than un-treated emeralds of similar quality. Other treatments, for example the use of green-tinted oil, are not acceptable in the trade. Gems are graded on a four-step scale; "none", "minor", "moderate" and "highly" enhanced. These categories reflect levels of enhancement, not "clarity". A gem graded "none" on the enhancement scale may still exhibit visible inclusions. Laboratories apply these criteria differently. Some gemologists consider the mere presence of oil or polymers to constitute enhancement. Others may ignore traces of oil if the presence of the material does not improve the look of the gemstone.

Emeralds in antiquity were mined in Egypt at locations on Mount Smaragdus since 1500 BCE, and India, and Austria since at least the 14th century CE. The Egyptian mines were exploited on an industrial scale by the Roman and Byzantine Empires, and later by Islamic conquerors. Mining ceased with the discovery of the Columbian deposits, only ruins remain.

Colombia is by far the world's largest producer of emeralds, constituting 50–95% of the world production, with the number depending on the year, source and grade. Emerald production in Colombia has increased drastically in the last decade, increasing by 78% from 2000 to 2010. The three main emerald mining areas in Colombia are Muzo, Coscuez, and Chivor. Rare "trapiche" emeralds are found in Colombia, distinguished by ray-like spokes of dark impurities.

Zambia is the world's second biggest producer, with its Kafubu River area deposits (Kagem Mines) about southwest of Kitwe responsible for 20% of the world's production of gem-quality stones in 2004. In the first half of 2011, the Kagem Mines produced 3.74 tons of emeralds.

Emeralds are found all over the world in countries such as Afghanistan, Australia, Austria, Brazil, Bulgaria, Cambodia, Canada, China, Egypt, Ethiopia, France, Germany, India, Italy, Kazakhstan, Madagascar, Mozambique, Namibia, Nigeria, Norway, Pakistan, Russia, Somalia, South Africa, Spain, Switzerland, Tanzania, the United States, Zambia, and Zimbabwe. In the US, emeralds have been found in Connecticut, Montana, Nevada, North Carolina, and South Carolina. In Canada, in 1997 emeralds were discovered in the Yukon.

Since the onset of concerns regarding diamond origins, research has been conducted to determine if the mining location could be determined for an emerald already in circulation. Traditional research used qualitative guidelines such as an emerald’s color, style and quality of cutting, type of fracture filling, and the anthropological origins of the artifacts bearing the mineral to determine the emerald's mine location. More recent studies using energy dispersive X-ray spectroscopy methods have uncovered trace chemical element differences between emeralds; even emeralds mined within close proximity to one another. American gemologist David Cronin and his colleagues have extensively examined the chemical signatures of emeralds resulting from fluid dynamics and subtle precipitation mechanisms, and their research demonstrated the chemical homogeneity of emeralds from the same mining location and the statistical differences that exist between emeralds from different mining locations, including those between the three locations: Muzo, Coscuez, and Chivor, in Colombia, South America.

Both hydrothermal and "flux-growth" synthetics have been produced, and a method has been developed for producing an emerald overgrowth on colorless beryl. The first commercially successful emerald synthesis process was that of Carroll Chatham, likely involving a lithium vanadate flux process, as Chatham's emeralds do not have any water and contain traces of vanadate, molybdenum and vanadium. The other large producer of flux emeralds was Pierre Gilson Sr., whose products have been on the market since 1964. Gilson's emeralds are usually grown on natural colorless beryl seeds, which are coated on both sides. Growth occurs at the rate of 1 mm per month, a typical seven-month growth run produces emerald crystals 7 mm thick.

Hydrothermal synthetic emeralds have been attributed to IG Farben, Nacken, Tairus, and others, but the first satisfactory commercial product was that of Johann Lechleitner of Innsbruck, Austria, which appeared on the market in the 1960s. These stones were initially sold under the names "Emerita" and "Symeralds", and they were grown as a thin layer of emerald on top of natural colorless beryl stones. Later, from 1965 to 1970, the Linde Division of Union Carbide produced completely synthetic emeralds by hydrothermal synthesis. According to their patents (attributable to E.M. Flanigen), acidic conditions are essential to prevent the chromium (which is used as the colorant) from precipitating. Also, it is important that the silicon-containing nutrient be kept away from the other ingredients to prevent nucleation and confine growth to the seed crystals. Growth occurs by a diffusion-reaction process, assisted by convection. The largest producer of hydrothermal emeralds today is Tairus, in Russia, which has succeeded in synthesizing emeralds with chemical composition similar to emeralds in alkaline deposits in Colombia, and whose products are thus known as “Colombian created emeralds” or “Tairus created emeralds”. Luminescence in ultraviolet light is considered a supplementary test when making a natural versus synthetic determination, as many, but not all, natural emeralds are inert to ultraviolet light. Many synthetics are also UV inert.

Synthetic emeralds are often referred to as "created", as their chemical and gemological composition is the same as their natural counterparts. The U.S. Federal Trade Commission (FTC) has very strict regulations as to what can and what cannot be called a "synthetic" stone. The FTC says: "§ 23.23(c) It is unfair or deceptive to use the word "laboratory-grown," "laboratory-created," "[manufacturer name]-created," or "synthetic" with the name of any natural stone to describe any industry product unless such industry product has essentially the same optical, physical, and chemical properties as the stone named."

Emerald is regarded as the traditional birthstone for May as well as the traditional gemstone for the astrological signs of Cancer.

One of the quainter anecdotes about emeralds was told by the 16th-century historian Brantôme, who referred to the many impressive emeralds the Spanish under Cortez had brought back to Europe from Latin America. On one of Cortez's most notable emeralds he had the text engraved, "Inter Natos Mulierum non sur-rexit mayor" ("Among those born of woman there hath not arisen a greater," Matthew 11:11) which referred to John the Baptist. Brantôme considered engraving such a beautiful and simple product of nature sacrilegious and considered this act the cause for Cortez's loss of an extremely precious pearl (to which he dedicated a work, "A beautiful and incomparable pearl"), and even for the death of King Charles IX of France, who died soon afterward.

The chief deity of one of India's most famous temple, the Meenakshi Amman Temple in Madurai, is the goddess Meenakshi, whose idol is traditionally thought to be made of emerald.





</doc>
<doc id="10046" url="https://en.wikipedia.org/wiki?curid=10046" title="Erie Canal">
Erie Canal

The Erie Canal is a canal in New York, United States that is part of the east–west, cross-state route of the New York State Canal System (formerly known as the New York State Barge Canal). Originally, it ran from where Albany meets the Hudson River to where Buffalo meets Lake Erie. It was built to create a navigable water route from New York City and the Atlantic Ocean to the Great Lakes. When completed in 1825, it was the second longest canal in the world (after the Grand Canal in China) and greatly affected the development and economy of New York, New York City, and the United States.

The canal was first proposed in the 1780s, then re-proposed in 1807. A survey was authorized, funded, and executed in 1808. Proponents of the project gradually wore down opponents; its construction began in 1817. The canal has 34 numbered locks starting with Black Rock Lock and ending downstream with the Troy Federal Lock. Both are owned by the federal government. It has an elevation difference of about . It opened on October 26, 1825.

In a time when bulk goods were limited to pack animals (a maximum), and there were no railways, water was the most cost-effective way to ship bulk goods.

The canal was denigrated by its political opponents as "Clinton's Folly" or "Clinton's Big Ditch". It was the first transportation system between the Eastern Seaboard and the western interior of the United States that did not require portage.

It was faster than carts pulled by draft animals and cut transport costs by about 95%. The canal gave New York City's port an incomparable advantage over all other U.S. port cities and ushered in the state's 19th century political and cultural ascendancy. The canal fostered a population surge in western New York and opened regions farther west to settlement. It was enlarged between 1834 and 1862. The canal's peak year was 1855, when 33,000 commercial shipments took place. In 1918, the western part of the canal was enlarged to become part of the New York State Barge Canal, which also extended to the Hudson River running parallel to the eastern half of the Erie Canal.

In 2000, the United States Congress designated the Erie Canalway National Heritage Corridor to recognize the national significance of the canal system as the most successful and influential human-built waterway and one of the most important works of civil engineering and construction in North America. The canal has been mainly used by recreational watercraft since the retirement of the last large commercial ship, "Day Peckinpaugh", min 1994. The canal saw a recovery in commercial traffic in 2008.

From the first days of the expansion of the British colonies from the coast of North America into the heartland of the continent, a recurring problem was that of transportation between the coastal ports and the interior. This was not unique to the Americas, and the problem still exists in those parts of the world where muscle power provides a primary means of transportation within a region. An equally ancient solution was implemented in many cultures—things in the water weighed far less and took less effort to move since friction became negligible. Close to the seacoast, rivers often provided adequate waterways, but the Appalachian Mountains, inland, running over long as a barrier range with just five places where mule trains or wagon roads could be routed, presented a great challenge. Passengers and freight had to travel overland, a journey made more difficult by the rough condition of the roads. In 1800, it typically took 2-1/2 weeks to travel overland from New York to Cleveland, Ohio (); 4 weeks to Detroit ().

The principal exportable product of the Ohio Valley was grain, which was a high-volume, low-priced commodity, bolstered by supplies from the coast. Frequently it was not worth the cost of transporting it to far-away population centers. This was a factor leading to farmers in the west turning their grains into whiskey for easier transport and higher sales, and later the Whiskey Rebellion. In the 18th and early 19th centuries, it became clear to coastal residents that the city or state that succeeded in developing a cheap, reliable route to the West would enjoy economic success, and the port at the seaward end of such a route would see business increase greatly. In time, projects were devised in Virginia, Maryland, Pennsylvania, and relatively deep into the coastal states.

The successes of the Canal du Midi in France (1681), Bridgewater Canal in Britain (1769), and Eiderkanal in Denmark (1784) spurred on what was called in Britain "canal mania". The idea of a canal to tie the East Coast to the new western settlements was discussed as early as 1724: New York provincial official Cadwallader Colden made a passing reference (in a report on fur trading) to improving the natural waterways of western New York.

Gouverneur Morris and Elkanah Watson were early proponents of a canal along the Mohawk River. Their efforts led to the creation of the "Western and Northern Inland Lock Navigation Companies" in 1792, which took the first steps to improve navigation on the Mohawk and construct a canal between the Mohawk and Lake Ontario, but it was soon discovered that private financing was insufficient.
Christopher Colles (who was familiar with the Bridgewater Canal) surveyed the Mohawk Valley, and made a presentation to the New York state legislature in 1784, proposing a shorter canal from Lake Ontario. The proposal drew attention and some action but was never implemented.

Jesse Hawley had envisioned encouraging the growing of large quantities of grain on the western New York plains (then largely unsettled) for sale on the Eastern seaboard. However, he went bankrupt trying to ship grain to the coast. While in Canandaigua debtors' prison, Hawley began pressing for the construction of a canal along the -long Mohawk River valley with support from Joseph Ellicott (agent for the Holland Land Company in Batavia). Ellicott realized that a canal would add value to the land he was selling in the western part of the state. He later became the first canal commissioner.

The Mohawk River (a tributary of the Hudson) rises near Lake Ontario and runs in a glacial meltwater channel just north of the Catskill range of the Appalachian Mountains, separating them from the geologically distinct Adirondacks to the north. The Mohawk and Hudson valleys form the only cut across the Appalachians north of Alabama, allowing an almost complete water route from New York City in the south to Lake Ontario and Lake Erie in the west. Along its course and from these lakes, other Great Lakes, and to a lesser degree, related rivers, a large part of the continent's interior (and many settlements) would be made well connected to the Eastern seaboard.

The problem was that the land rises about from the Hudson to Lake Erie. Locks at the time could handle up to of lift, so even with the heftiest cuttings and viaducts, fifty locks would be required along the canal. Such a canal would be expensive to build even with modern technology; in 1800, the expense was barely imaginable. President Thomas Jefferson called it "a little short of madness" and rejected it; however, Hawley interested New York Governor DeWitt Clinton in the project. There was much opposition, and the project was ridiculed as "Clinton's folly" and "Clinton's ditch." In 1817, though, Clinton received approval from the legislature for $7 million for construction.

The original canal was long, from Albany on the Hudson to Buffalo on Lake Erie. The channel was cut wide and deep, with removed soil piled on the downhill side to form a walkway known as a towpath.

Its construction, through limestone and mountains, proved a daunting task. In 1823 construction reached the Niagara Escarpment, necessitating the building of five locks along a corridor to carry the canal over the escarpment. To move earth, animals pulled a "slip scraper" (similar to a bulldozer). The sides of the canal were lined with stone set in clay, and the bottom was also lined with clay. The stonework required hundreds of German masons, who later built many of New York's buildings. All labor on the canal depended upon human (and animal) power or the force of water. Engineering techniques developed during its construction included the building of aqueducts to redirect water; one aqueduct was long to span of river. As the canal progressed, the crews and engineers working on the project developed expertise and became a skilled labor force.
Canal boats up to in draft were pulled by horses and mules on the towpath. The canal had one towpath, generally on the north side. When canal boats met, the boat with the right of way remained on the towpath side of the canal. The other boat steered toward the berm (or heelpath) side of the canal. The driver (or "hoggee", pronounced HO-gee) of the privileged boat kept his towpath team by the canalside edge of the towpath, while the hoggee of the other boat moved to the outside of the towpath and stopped his team. His towline would be unhitched from the horses, go slack, fall into the water and sink to the bottom, while his boat decelerated on with its remaining momentum. The privileged boat's team would step over the other boat's towline, with their horses pulling the boat over the sunken towline without stopping. Once clear, the other boat's team would continue on its way.

Pulled by teams of horses, canal boats still moved slowly, but methodically, shrinking time and distance. Efficiently, the nonstop smooth method of transportation cut the travel time between Albany and Buffalo nearly in half, moving by day and by night. Venturing west, men and women boarded packets to visit relatives, or solely for a relaxing excursion. Emigrants took passage on freight boats, camping on deck, or on top of crates. Packet boats, serving passengers exclusively, reached speeds of up to five miles an hour, and ran at much more frequent intervals than the cramped, bumpy stages.

Packet boats, measuring up to seventy-eight feet in length and fourteen and a half feet across, made ingenious use of space, in order to accommodate up to forty passengers at night and up to three times as many in the daytime. The best examples, furnished with carpeted floors, stuffed chairs, and mahogany tables stocked with current newspapers and books, served as sitting rooms during the days. At mealtimes, crews transformed the cabin into dining rooms. Drawing a curtain across the width of the room divided the cabin into ladies' and gentlemen's sleeping quarters in the evening hours. Pull-down tiered beds folded from the walls, and additional cots could be hung from hooks in the ceiling. Some captains hired musicians and held dances. The canal had brought civilization into the wilderness.

The men who planned and oversaw construction were novices as surveyors and as engineers. There were no civil engineers in the United States. James Geddes and Benjamin Wright, who laid out the route, were judges whose experience in surveying was in settling boundary disputes. Geddes had only used a surveying instrument for a few hours before his work on the Canal. Canvass White was a 27-year-old amateur engineer who persuaded Clinton to let him go to Britain at his own expense to study the canal system there. Nathan Roberts was a mathematics teacher and land speculator. Yet these men "carried the Erie Canal up the Niagara escarpment at Lockport, maneuvered it onto a towering embankment to cross over Irondequoit Creek, spanned the Genesee River on an awesome aqueduct, and carved a route for it out of the solid rock between Little Falls and Schenectady—and all of those venturesome designs worked precisely as planned". (Bernstein, p. 381)

Construction began July 4, 1817, at Rome, New York. The first , from Rome to Utica, opened in 1819. At that rate, the canal would not be finished for 30 years. The main delays were caused by felling trees to clear a path through virgin forest and moving excavated soil, which took longer than expected, but the builders devised ways to solve these problems. To fell a tree, they threw rope over the top branches and winched it down. They pulled out the stumps with an innovative stump puller. A pair of huge wheels were mounted loose on an axle. A large wheel, barely smaller than the others, was fixed to the center of the axle. A chain was wrapped around the axle and hooked to the stump. A rope was wrapped around the center wheel and hooked to a team of oxen. The mechanical advantage (torque) obtained ripped the stumps out of the soil. Soil to be moved was shoveled into large wheelbarrows that were dumped into mule-pulled carts. Using a scraper and a plow, a three-man team with oxen, horses, and mules could build a mile in a year.

The remaining problem was finding labor; increased immigration helped fill the need. Many of the laborers working on the canal were Irish, who had recently come to the United States as a group of about 5,000 from Ireland, most of whom were Roman Catholic, a religion that raised much suspicion in early America due to its hierarchic structure, and many laborers on the canal suffered violent assault as the result of misjudgment and xenophobia.

Construction continued at an increased rate as new workers arrived. When the canal reached Montezuma Marsh (at the outlet of Cayuga Lake west of Syracuse), it was rumored over 1,000 workers died of "swamp fever" (malaria), and construction was temporarily stopped. However, recent research has revealed the death toll was likely much lower, as no contemporary reports mention significant worker mortality, and mass graves from the period have never been found in the area. Work continued on the downhill side towards the Hudson, and when the marsh froze in winter, the crews worked to complete the section across the swamps.

The middle section from Utica to Salina (Syracuse) was completed in 1820, and traffic on that section started up immediately. Expansion to the east and west proceeded, and the whole eastern section, from Brockport to Albany, opened on September 10, 1823 to great fanfare.

The Champlain Canal, a separate but interconnected north-south route from Watervliet on the Hudson to Lake Champlain, opened on the same date.

In 1824, before the canal was completed, a detailed "Pocket Guide for the Tourist and Traveler, Along the Line of the Canals, and the Interior Commerce of the State of New York", was published for the benefit of travelers and land speculators.

After Montezuma Marsh, the next difficulties were crossing Irondequoit Creek and the Genesee River near Rochester. The first ultimately required building the long "Great Embankment," which carried the canal at a height of above the level of the creek, which was carried through a culvert underneath. The river was crossed on a stone aqueduct long and wide, with 11 arches.

After the Genesee, the next obstacle was crossing the Niagara Escarpment, an wall of hard dolomitic limestone, to rise to the level of Lake Erie. The route followed the channel of a creek that had cut a ravine steeply down the escarpment, with two sets of five locks in a series, soon giving rise to the community of Lockport. The lift-locks had a total lift of , exiting into a deeply cut channel. The final leg had to be cut through another limestone layer, the Onondaga ridge. Much of that section was blasted with black powder, and the inexperience of the crews often led to accidents, and sometimes rocks falling on nearby homes.

Two villages competed to be the terminus: Black Rock, on the Niagara River, and Buffalo, at the eastern tip of Lake Erie. Buffalo expended great energy to widen and deepen Buffalo Creek to make it navigable and to create a harbor at its mouth. Buffalo won over Black Rock, and grew into a large city, eventually encompassing its former competitor.

The entire canal was officially completed on October 26, 1825. The event was marked by a statewide "Grand Celebration," culminating in successive cannon shots along the length of the canal and the Hudson, a 90-minute cannonade from Buffalo to New York City. A flotilla of boats, led by Governor Dewitt Clinton aboard "Seneca Chief", sailed from Buffalo to New York City in ten days. Clinton then ceremonially poured Lake Erie water into New York Harbor to mark the "Wedding of the Waters". On its return trip, "Seneca Chief" brought a keg of Atlantic Ocean water back to Buffalo to be poured into Lake Erie by Buffalo's Judge Samuel Wilkeson, who would later become mayor.

The Erie Canal was thus completed in eight years at a cost of $7,143,000 (). It was acclaimed as an engineering marvel that united the country and helped New York City become a financial capital.

The canal began on the west side of the Hudson River at Albany, and ran north to Watervliet, where the Champlain Canal branched off. At Cohoes, it climbed the escarpment on the west side of the Hudson River and then turned west along the south shore of the Mohawk River, crossing to the north side at Crescent and again to the south at Rexford. The canal continued west near the south shore of the Mohawk River all the way to Rome, where the Mohawk turns north.

At Rome, the canal continued west parallel to Wood Creek, which flows westward into Oneida Lake, and turned southwest and west cross-country to avoid the lake. From Canastota west, it ran roughly along the north (lower) edge of the Onondaga Escarpment, passing through Syracuse, Onondaga Lake, and Rochester. Before reaching Rochester, the canal uses a series of natural ridges to cross the deep valley of Irondequoit Creek. At Lockport the canal turned southwest to rise to the top of the Niagara Escarpment, using the ravine of Eighteen Mile Creek.

The canal continued south-southwest to Pendleton, where it turned west and southwest, mainly using the channel of Tonawanda Creek. From the Tonawanda south toward Buffalo, it ran just east of the Niagara River, where it reached its "Western Terminus" at Little Buffalo Creek (later it became the Commercial Slip), which discharged into the Buffalo River just above its confluence with Lake Erie. With Buffalo's re-excavation of the Commercial Slip, completed in 2008, the Canal's original terminus is now re-watered and again accessible by boats. With several miles of the Canal inland of this location still lying under 20th-century fill and urban construction, the effective western navigable terminus of the Erie Canal is found at Tonawanda.

The Erie made use of the favorable conditions of New York's unique topography, which provided that area with the only break in the Appalachians south of the Saint Lawrence River. The Hudson is tidal to Troy, and Albany is west of the Appalachians. It allowed for east–west navigation from the coast to the Great Lakes within US territory. The canal system thus gave New York a competitive advantage, helped New York City develop as an international trade center, and allowed Buffalo to grow from just 200 settlers in 1820 to more than 18,000 people by 1840. The port of New York became essentially the Atlantic home port for all of the Midwest—because of this vital connection and others to follow, such as the railroads, New York would become known as the "Empire State" or "the great Empire State".

Problems developed but were quickly solved. Leaks developed along the entire length of the canal, but these were sealed using cement that hardened underwater (hydraulic cement). Erosion on the clay bottom proved to be a problem and the speed was limited to .

The original design planned for an annual tonnage of 1.5 million tons (1.36 million metric tons), but this was exceeded immediately. An ambitious program to improve the canal began in 1834. During this massive series of construction projects, known as the First Enlargement, the canal was widened from and deepened from . Locks were widened and/or rebuilt in new locations, and many new navigable aqueducts were constructed. The canal was straightened and slightly re-routed in some stretches, resulting in the abandonment of short segments of the original 1825 canal. The First Enlargement was completed in 1862, with further minor enlargements in later decades.

Today, the reconfiguration of the canal created during the First Enlargement is commonly referred to as the "Improved Erie Canal" or the "Old Erie Canal", to distinguish it from the canal's modern-day course. Existing remains of the 1825 canal abandoned during the Enlargement are sometimes referred to today as "Clinton's Ditch" (which was also the popular nickname for the entire Erie Canal project during its original 1817–1825 construction).

Additional feeder canals soon extended the Erie Canal into a system. These included the Cayuga-Seneca Canal south to the Finger Lakes, the Oswego Canal from Three Rivers north to Lake Ontario at Oswego, and the Champlain Canal from Troy north to Lake Champlain. From 1833 to 1877, the short Crooked Lake Canal connected Keuka Lake and Seneca Lake. The Chemung Canal connected the south end of Seneca Lake to Elmira in 1833, and was an important route for Pennsylvania coal and timber into the canal system. The Chenango Canal in 1836 connected the Erie Canal at Utica to Binghamton and caused a business boom in the Chenango River valley. The Chenango and Chemung canals linked the Erie with the Susquehanna River system. The Black River Canal connected the Black River to the Erie Canal at Rome and remained in operation until the 1920s. The Genesee Valley Canal was run along the Genesee River to connect with the Allegheny River at Olean, but the Allegheny section, which would have connected to the Ohio and Mississippi rivers, was never built. The Genesee Valley Canal was later abandoned and became the route of the Genesee Valley Canal Railroad.

In 1903 the New York State legislature authorized construction of the New York State Barge Canal as the "Improvement of the Erie, the Oswego, the Champlain, and the Cayuga and Seneca Canals".
In 1905, construction of the Barge Canal began, which was completed in 1918, at a cost of $96.7 million.
Freight traffic reached a total of 5.2 million short tons (4.7 million metric tons) by 1951, before declining in the face of combined rail and truck competition.

As the canal brought travelers to New York City, it took business away from other ports such as Philadelphia and Baltimore. Those cities and their states started projects to compete with the Erie Canal. In Pennsylvania, the Main Line of Public Works was a combined canal and railroad running west from Philadelphia to Pittsburgh on the Ohio River, opened in 1834. In Maryland, the Baltimore and Ohio Railroad ran west to Wheeling, West Virginia, also on the Ohio River, and was completed in 1853.

Other competition was more direct. The Mohawk and Hudson Railroad opened in 1837, providing a bypass to the slowest part of the canal between Albany and Schenectady. Other railroads were soon chartered and built to continue the line west to Buffalo, and in 1842 a continuous line (which later became the New York Central Railroad and its Auburn Road in 1853) was open the whole way to Buffalo. As the railroad served the same general route as the canal, but provided for faster travel, passengers soon switched to it. However, as late as 1852, the canal carried thirteen times more freight tonnage than all the railroads in New York State combined; it continued to compete well with the railroads through 1902, when tolls were abolished.

The New York, West Shore and Buffalo Railway was completed in 1884, as a route running closely parallel to both the canal and the New York Central Railroad. However, it went bankrupt and was acquired the next year by the New York Central.

The Erie Canal greatly lowered the cost of shipping between the Midwest and the Northeast, bringing much lower food costs to Eastern cities and allowing the East to economically ship machinery and manufactured goods to the Midwest. The canal also made an immense contribution to the wealth and importance of New York City, Buffalo, and New York State. Its impact went much further, increasing trade throughout the nation by opening eastern and overseas markets to Midwestern farm products and by enabling migration to the West.

The Erie Canal was an immediate success. Tolls collected on freight had already exceeded the state's construction debt in its first year of official operation. By 1828, import duties collected at the New York Customs House supported federal government operations and provided funds for all the expenses in Washington except the interest on the national debt. Additionally, New York state's initial loan for the original canal had been paid by 1837. Although it had been envisioned as primarily a commercial channel for freight boats, passengers also traveled on the canal's packet boats. In 1825 more than forty thousand passengers took advantage of the convenience and beauty of canal travel. The canal's steady flow of tourists, businessmen and settlers lent it to uses never imagined by its initial sponsors. Evangelical preachers made their circuits of the upstate region and the canal served as the last leg of the underground railroad ferrying runaway slaves to Buffalo near the Canada–US border. Aspiring merchants found that tourists proved to double as reliable customers. Vendors moved from boat to boat peddling items such as books, watches and fruit while less scrupulous "confidence men" sold remedies for foot corns or passed off counterfeit bills. Tourists were carried along the "northern tour", which ultimately led to the popular honeymoon destination Niagara Falls, just north of Buffalo.

Consisting of a massive stone aqueduct which carried boats over incredible cascades, Little Falls was one of the most popular stops for American and foreign tourists—as depicted in Scene 4 of William Dunlap's play "A Trip to Niagara", where he depicts the general preference of tourists to travel by canal so that they could see a combination of artificial and natural sites. Canal travel was, for many, an opportunity to take in the sublime and commune with nature. The play also reflects the less enthusiastic view of some seeing movement on the canal as tedious.

New ethnic Irish communities formed in some towns along its route after completion, as Irish immigrants were a large portion of the construction labor force. Earth extracted from the canal was transported to the New York city area and used as landfill in New York and New Jersey. A plaque honoring the canal's construction is located in Battery Park in southern Manhattan.

Because so many immigrants traveled on the canal, many genealogists have sought copies of canal passenger lists. Apart from the years 1827–1829, canal boat operators were not required to record or report passenger names to the government, which, in this case, was the state of New York. Those passenger lists that were recorded survive today in the New York State Archives, and other sources of traveler information are sometimes available.

The Canal also helped bind the still-new nation closer to Britain and Europe. British repeal of the Corn Law resulted in a huge increase in exports of Midwestern wheat to Britain. Trade between the United States and Canada also increased as a result of the Corn Law and a reciprocity (free-trade) agreement signed in 1854; much of this trade flowed along the Erie.

Its success also prompted imitation: a rash of canal-building followed. Also, the many technical hurdles that had to be overcome made heroes of those whose innovations made the canal possible. This led to an increased public esteem for practical education. Chicago, among other Great Lakes cities, recognized the commercial importance of the canal to its economy, and two West Loop streets are named "Canal" and "Clinton" (for canal proponent DeWitt Clinton).

Concern that erosion caused by logging in the Adirondacks could silt up the canal contributed to the creation of another New York National Historic Landmark, the Adirondack Park, in 1885.
Many notable authors wrote about the canal, including Herman Melville, Frances Trollope, Nathaniel Hawthorne, Harriet Beecher Stowe, Mark Twain, Samuel Hopkins Adams and the Marquis de Lafayette, and many tales and songs were written about life on the canal. The popular song "Low Bridge" by Thomas S. Allen was written in 1905 to memorialize the canal's early heyday, when barges were pulled by mules rather than engines.

The New York State Legislature debated closing the locks of the Erie Canal on Sundays, when they convened in 1858. However, George Jeremiah and Dwight Bacheller, two of the bill's opponents, argued that the state had no right to stop canal traffic on the grounds that the Erie Canal and its tributaries had ceased to be wards of the state. The canal at its inception had been imagined as an extension of nature, an artificial river where there had been none. The canal succeeded by sharing more in common with lakes and seas than it had with public roads. Jeremiah and Bacheller argued, successfully, that just as it was unthinkable to halt oceangoing navigation on Sunday, it was so with the canal.

In 1918, the Canal was replaced by the larger New York State Barge Canal. This new canal replaced much of the original route, leaving many abandoned sections (most notably between Syracuse and Rome). New digging and flood control technologies allowed engineers to canalize rivers that the original canal had sought to avoid, such as the Mohawk, Seneca, and Clyde rivers, and Oneida Lake. In sections that did not consist of canalized rivers (particularly between Rochester and Buffalo), the original Erie Canal channel was enlarged to wide and deep. The expansion allowed barges up to to use the Canal. This expensive project was politically unpopular in parts of the state not served by the canal, and failed to save it from becoming obsolete for commercial shipping.

The new alignment began on the Hudson River at the border between Cohoes and Waterford, where it ran northwest with five locks (the so-called "Waterford Flight"), running into the Mohawk River east of Crescent. The Waterford Flight is claimed to be one of the steepest series of locks in the world.

While the old Canal ran next to the Mohawk all the way to Rome, the new canal ran through the river, which was straightened or widened where necessary. At Ilion, the new canal left the river for good, but continued to run on a new alignment parallel to both the river and the old canal to Rome. From Rome, the new route continued almost due west, merging with Fish Creek just east of its entry into Oneida Lake.

From Oneida Lake, the new canal ran west along the Oneida River, with cutoffs to shorten the route. At Three Rivers the Oneida River turns northwest, and was deepened for the Oswego Canal to Lake Ontario. The new Erie Canal turned south there along the Seneca River, which turns west near Syracuse and continues west to a point in the Montezuma Marsh (). There the Cayuga and Seneca Canal continued south with the Seneca River, and the new Erie Canal again ran parallel to the old canal along the bottom of the Niagara Escarpment, in some places running along the Clyde River, and in some places replacing the old canal. At Pittsford, southeast of Rochester, the canal turned west to run around the south side of Rochester, rather than through downtown. The canal crosses the Genesee River at the Genesee Valley Park (), then rejoins the old path near North Gates.

From there it was again roughly an upgrade to the original canal, running west to Lockport. This reach of 64.2 miles from Henrietta to Lockport is called "the 60‑mile level" since there are no locks and the water level rises only two feet over the entire segment. Diversions from and to adjacent natural streams along the way are used to maintain the canal's level. It runs southwest to Tonawanda, where the new alignment discharges into the Niagara River, which is navigable upstream to the New York Barge Canal's Black Rock Lock and thence to the Canal's original "Western Terminus" at Buffalo's Inner Harbor.

The growth of railroads and highways across the state, and the opening of the Saint Lawrence Seaway, caused commercial traffic on the canal to decline dramatically during the second half of the 20th century.

In 1992, the New York State Barge Canal was renamed the New York State Canal System (including the Erie, Cayuga-Seneca, Oswego, and Champlain canals) and placed under the newly created New York State Canal Corporation, a subsidiary of the New York State Thruway Authority. The canal system is operated using money generated by Thruway tolls.

Since the 1990s, the canal system has been used primarily by recreational traffic, although a small but growing amount of cargo traffic still uses it.

Today, the Erie Canalway National Heritage Corridor covers of navigable water from Lake Champlain to the Capital Region and west to Buffalo. The area has a population of 2.7 million: about 75% of Central and Western New York's population lives within of the Erie Canal.

The Erie Canal is open to small craft and some larger vessels from May through November each year. During winter, water is drained from parts of the canal for maintenance. The Champlain Canal, Lake Champlain, and the Chambly Canal, and Richelieu River in Canada form the Lakes to Locks Passage, making a tourist attraction of the former waterway linking eastern Canada to the Erie Canal. In 2006 recreational boating fees were eliminated to attract more visitors.

Travel on the canal's middle section (particularly in the Mohawk Valley) was severely hampered by flooding in late June and early July 2006. Flood damage to the canal and its facilities was estimated as at least $15 million.

There were some 42 commercial shipments on the canal in 2008, compared to 15 such shipments in 2007 and more than 33,000 shipments in 1855, the canal's peak year. The new growth in commercial traffic is due to the rising cost of diesel fuel. Canal barges can carry a short ton of cargo on one gallon of diesel fuel, while a gallon allows a train to haul the same amount of cargo and a truck . Canal barges can carry loads up to , and are used to transport objects that would be too large for road or rail shipment. Today, the system is served by several commercial towing companies. In 2012, the New York State Canal System (which consists of the Erie Canal and a few smaller canals) was used to ship 42,000 tons of cargo.

Aside from transportation, the canal's waters are still utilized for other purposes such as irrigation for farmland, hydroelectricity, research, industry, and even drinking by numerous businesses, farms, factories and communities alongside its banks. Users of the canal system have an estimated total economic impact of $6.2 billion annually.

Sections of the old Erie Canal not used after 1918 are owned by New York State, or have been ceded to or purchased by counties or municipalities. Many stretches of the old canal have been filled in to create roads such as Erie Boulevard in Syracuse and Schenectady, and Broad Street and the Rochester Subway in Rochester. A 36‑mile (58 km) stretch of the old canal from the town of DeWitt, New York, east of Syracuse, to just outside Rome, New York, is preserved as the Old Erie Canal State Historic Park. In 1960 the Schoharie Crossing State Historic Site, a section of the canal in Montgomery County, was one of the first sites recognized as a National Historic Landmark.

Some municipalities have preserved sections as town or county canal parks, or have plans to do so. Camillus Erie Canal Park preserves a stretch and has restored Nine Mile Creek Aqueduct, built in 1841 as part of the First Enlargement of the canal. In some communities, the old canal has refilled with overgrowth and debris. Proposals have been made to rehydrate the old canal through downtown Rochester or Syracuse as a tourist attraction. In Syracuse, the location of the old canal is represented by a reflecting pool in downtown's Clinton Square and the downtown hosts a canal barge and weigh lock structure, now dry. Buffalo's Commercial Slip is the restored and re-watered segment of the canal which formed its "Western Terminus".

The Erie Canal is a destination for tourists from all over the world, and has inspired guidebooks dedicated to exploration of the waterway. An Erie Canal Cruise company, based in Herkimer, operates from mid-May until mid-October with daily cruises. The cruise goes through the history of the canal and also takes passengers through Lock 18.

In 2004, the administration of New York Governor George Pataki was criticized when officials of New York State Canal Corporation attempted to sell private development rights to large stretches of the Old Erie Canal to a single developer for $30,000, far less than the land was worth on the open market. After an investigation by the "Syracuse Post-Standard" newspaper, the Pataki administration nullified the deal.
Records of the planning, design, construction, and administration of the Erie Canal are vast and can be found in the New York State Archives. Except for two years (1827–1829), the State of New York did not require canal boat operators to maintain or submit passenger lists.

Parks and museums related to the old Erie Canal include (listed from East to West):

The following list of locks is provided for the current canal, from east to west. There are a total of 36 (35 numbered) locks on the Erie Canal.

All locks on the New York State Canal System are single-chamber; the dimensions are long and wide with a minimum depth of water over the miter sills at the upstream gates upon lift. They can accommodate a vessel up to long and wide. Overall sidewall height will vary by lock, ranging between depending on the lift and navigable stages. Lock E17 at Little Falls has the tallest sidewall height at .

Distance is based on position markers from an interactive canal map provided online by the New York State Canal Corporation and may not exactly match specifications on signs posted along the canal. Mean surface elevations are comprised from a combination of older canal profiles and history books as well as specifications on signs posted along the canal. The margin of error should normally be within .

The Waterford Flight series of locks (comprising Locks E2 through E6) is one of the steepest in the world, lifting boats in less than .

"All surface elevations are approximate."

There is roughly a natural rise between locks E33 and E34 as well as a natural rise between Lock E35 and the Niagara River.

There is no Lock E1 or Lock E31 on the Erie Canal. The place of "Lock E1" on the passage from the lower Hudson River to Lake Erie is taken by the Troy Federal Lock, located just north of Troy, New York, and is not part of the Erie Canal System proper. It is operated by the United States Army Corps of Engineers. The Erie Canal officially begins at the confluence of the Hudson and Mohawk rivers at Waterford, New York.

Although the original alignment of the Erie Canal through Buffalo has been filled in, travel by water is still possible from Buffalo via the Black Rock Lock in the Niagara River to the canal's modern western terminus in Tonawanda, and eastward to Albany. The Black Rock Lock is operated by the United States Army Corps of Engineers.

Oneida Lake lies between locks E22 and E23, and has a mean surface elevation of . Lake Erie has a mean surface elevation of .




</doc>
<doc id="10048" url="https://en.wikipedia.org/wiki?curid=10048" title="Ethanol">
Ethanol

Ethanol, also called alcohol, ethyl alcohol, and drinking alcohol, is a chemical compound, a simple alcohol with the chemical formula . Its formula can be written also as −− or − (an ethyl group linked to a hydroxyl group), and is often abbreviated as EtOH. Ethanol is a volatile, flammable, colorless liquid with a slight characteristic odor. It is a psychoactive substance and is the principal type of alcohol found in alcoholic drinks.

Ethanol is naturally produced by the fermentation of sugars by yeasts or via petrochemical processes, and is most commonly consumed as a popular recreational drug. It also has medical applications as an antiseptic and disinfectant. The compound is widely used as a chemical solvent, either for scientific chemical testing or in synthesis of other organic compounds, and is a vital substance utilized across many different kinds of manufacturing industries. Ethanol is also used as a clean-burning fuel source.

"Ethanol" is the systematic name defined by the International Union of Pure and Applied Chemistry (IUPAC) for a compound consisting of alkyl group with two carbon atoms (prefix “eth-”), having a single bond between them (infix “-an-”), attached functional group −OH group (suffix “-ol”).

The “eth-” prefix and the qualifier “ethyl” in “ethyl alcohol” originally come from the name “ethyl” assigned in 1834 to the group − by Justus Liebig. He coined the word from the German name "Aether" of the compound −O− (commonly called “ether” in English, more specifically called “diethyl ether”). According to the Oxford English Dictionary, "Ethyl" is a contraction of the Ancient Greek αἰθήρ (', “upper air”) and the Greek word ὕλη (', “substance”).

The name "ethanol" was coined as a result of a resolution that was adopted at the International Conference on Chemical Nomenclature that was held in April 1892 in Geneva, Switzerland.

The term “alcohol” now refers to a wider class of substances in chemistry nomenclature, but in common parlance it remains the name of ethanol. The Oxford English Dictionary claims that it is a medieval loan from Arabic "al-kuḥl", a powdered ore of antimony used since antiquity as a cosmetic, and retained that meaning in Middle Latin. The use of “alcohol” for ethanol (in full, “alcohol of wine”) is modern, first recorded 1753, and by the later 17th century referred to “any sublimated substance; distilled spirit” use for “the spirit of wine” (shortened from a full expression "alcohol of wine"). The systematic use in chemistry dates to 1850.

Ethanol is used in medical wipes and most common antibacterial hand sanitizer gels as an antiseptic. Ethanol kills organisms by denaturing their proteins and dissolving their lipids and is effective against most bacteria and fungi, and many viruses. However, ethanol is ineffective against bacterial spores. 70% ethanol is the most effective concentration, particularly because of osmotic pressure. Absolute ethanol may inactivate microbes without destroying them because the alcohol is unable to fully permeate the microbe's membrane.

Ethanol may be administered as an antidote to methanol and ethylene glycol poisoning.

Ethanol, often in high concentrations, is used to dissolve many water-insoluble medications and related compounds. Liquid preparations of cough and cold remedies, pain medication, and mouth washes may be dissolved in 1 to 25% concentrations of ethanol and may need to be avoided in individuals with adverse reactions to ethanol such as alcohol-induced respiratory reactions. Ethanol is present mainly as an antimicrobial preservative in over 700 liquid preparations of medicine including acetaminophen, iron supplements, ranitidine, furosemide, mannitol, phenobarbital, trimethoprim/sulfamethoxazole and over-the-counter cough medicine.

Ethyl Alcohol is extensively metabolized by the liver, particularly via the enzyme CYP450. Ethyl Alcohol increases the secretion of acids in the stomach. The metabolite acetaldehyde is responsible for much of the short term, and long term effects of ethyl alcohol toxicity.

As a central nervous system depressant, ethanol is one of the most commonly consumed psychoactive drugs. It can lift mood, cause feelings of euphoria, decrease anxiety, and increase sociability and talkativeness. 

The largest single use of ethanol is as an engine fuel and fuel additive. Brazil in particular relies heavily upon the use of ethanol as an engine fuel, due in part to its role as the globe's leading producer of ethanol. Gasoline sold in Brazil contains at least 25% anhydrous ethanol. Hydrous ethanol (about 95% ethanol and 5% water) can be used as fuel in more than 90% of new gasoline fueled cars sold in the country. Brazilian ethanol is produced from sugar cane and noted for high carbon sequestration. The US and many other countries primarily use E10 (10% ethanol, sometimes known as gasohol) and E85 (85% ethanol) ethanol/gasoline mixtures.

Ethanol has been used as rocket fuel and is currently in lightweight rocket-powered racing aircraft.

Australian law limits the use of pure ethanol from sugarcane waste to 10% in automobiles. Older cars (and vintage cars designed to use a slower burning fuel) should have the engine valves upgraded or replaced.

According to an industry advocacy group, ethanol as a fuel reduces harmful tailpipe emissions of carbon monoxide, particulate matter, oxides of nitrogen, and other ozone-forming pollutants. Argonne National Laboratory analyzed greenhouse gas emissions of many different engine and fuel combinations, and found that biodiesel/petrodiesel blend (B20) showed a reduction of 8%, conventional E85 ethanol blend a reduction of 17% and cellulosic ethanol 64%, compared with pure gasoline.

Ethanol combustion in an internal combustion engine yields many of the products of incomplete combustion produced by gasoline and significantly larger amounts of formaldehyde and related species such as acetaldehyde. This leads to a significantly larger photochemical reactivity and more ground level ozone. These data have been assembled into The Clean Fuels Report comparison of fuel emissions and show that ethanol exhaust generates 2.14 times as much ozone as gasoline exhaust. When this is added into the custom "Localised Pollution Index (LPI)" of The Clean Fuels Report, the local pollution of ethanol (pollution that contributes to smog) is rated 1.7, where gasoline is 1.0 and higher numbers signify greater pollution. The California Air Resources Board formalized this issue in 2008 by recognizing control standards for formaldehydes as an emissions control group, much like the conventional NOx and Reactive Organic Gases (ROGs).

World production of ethanol in 2006 was , with 69% of the world supply coming from Brazil and the United States. More than 20% of Brazilian cars are able to use 100% ethanol as fuel, which includes ethanol-only engines and flex-fuel engines. Flex-fuel engines in Brazil are able to work with all ethanol, all gasoline or any mixture of both. In the US flex-fuel vehicles can run on 0% to 85% ethanol (15% gasoline) since higher ethanol blends are not yet allowed or efficient. Brazil supports this population of ethanol-burning automobiles with large national infrastructure that produces ethanol from domestically grown sugar cane. Sugar cane not only has a greater concentration of sucrose than corn (by about 30%), but is also much easier to extract. The bagasse generated by the process is not wasted, but is used in power plants to produce electricity.

In the United States, the ethanol fuel industry is based largely on corn. According to the Renewable Fuels Association, as of 30 October 2007, 131 grain ethanol bio-refineries in the United States have the capacity to produce of ethanol per year. An additional 72 construction projects underway (in the U.S.) can add of new capacity in the next 18 months. Over time, it is believed that a material portion of the ≈ per year market for gasoline will begin to be replaced with fuel ethanol.

Sweet sorghum is another potential source of ethanol, and is suitable for growing in dryland conditions. The International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) is investigating the possibility of growing sorgham as a source of fuel, food, and animal feed in arid parts of Asia and Africa. Sweet sorghum has one-third the water requirement of sugarcane over the same time period. It also requires about 22% less water than corn (also known as maize). The world’s first sweet sorghum ethanol distillery began commercial production in 2007 in Andhra Pradesh, India.

Ethanol's high miscibility with water makes it unsuitable for shipping through modern pipelines like liquid hydrocarbons. Mechanics have seen increased cases of damage to small engines (in particular, the carburetor) and attribute the damage to the increased water retention by ethanol in fuel.

Ethanol was commonly used as fuel in early bipropellant rocket (liquid propelled) vehicles, in conjunction with an oxidizer such as liquid oxygen. The German V-2 rocket of World War II, credited with beginning the space age, used ethanol, mixed with 25% of water to reduce the combustion chamber temperature. The V-2's design team helped develop U.S. rockets following World War II, including the ethanol-fueled Redstone rocket which launched the first U.S. satellite. Alcohols fell into general disuse as more efficient rocket fuels were developed.

Commercial fuel cells operate on reformed natural gas, hydrogen or methanol. Ethanol is an attractive alternative due to its wide availability, low cost, high purity and low toxicity. There are a wide range of fuel cell concepts that have been trialled including direct-ethanol fuel cells, auto-thermal reforming systems and thermally integrated systems. The majority of work is being conducted at a research level although there are a number of organizations at the beginning of commercialization of ethanol fuel cells.

Ethanol fireplaces can be used for home heating or for decoration.

Ethanol is an important industrial ingredient. It has widespread use as a precursor for other organic compounds such as ethyl halides, ethyl esters, diethyl ether, acetic acid, and ethyl amines.

Ethanol is miscible with water and is a good general purpose solvent. It is found in paints, tinctures, markers, and personal care products such as mouthwashes, perfumes and deodorants. However, polysaccharides precipitate from aqueous solution in the presence of alcohol, and ethanol precipitation is used for this reason in the purification of DNA and RNA.

Because of its low melting point (−114.14 °C) and low toxicity, ethanol is sometimes used in laboratories (with dry ice or other coolants) as a cooling bath to keep vessels at temperatures below the freezing point of water. For the same reason, it is also used as the active fluid in alcohol thermometers.

Ethanol is a 2-carbon alcohol. Its molecular formula is CHCHOH. An alternative notation is CH−CH−OH, which indicates that the carbon of a methyl group (CH−) is attached to the carbon of a methylene group (−CH–), which is attached to the oxygen of a hydroxyl group (−OH). It is a constitutional isomer of dimethyl ether. Ethanol is sometimes abbreviated as EtOH, using the common organic chemistry notation of representing the ethyl group (CH−) with Et.

Ethanol is a volatile, colorless liquid that has a slight odor. It burns with a smokeless blue flame that is not always visible in normal light. The physical properties of ethanol stem primarily from the presence of its hydroxyl group and the shortness of its carbon chain. Ethanol's hydroxyl group is able to participate in hydrogen bonding, rendering it more viscous and less volatile than less polar organic compounds of similar molecular weight, such as propane.

Ethanol is slightly more refractive than water, having a refractive index of 1.36242 (at λ=589.3 nm and ). The triple point for ethanol is at a pressure of .

Ethanol is a versatile solvent, miscible with water and with many organic solvents, including acetic acid, acetone, benzene, carbon tetrachloride, chloroform, diethyl ether, ethylene glycol, glycerol, nitromethane, pyridine, and toluene. It is also miscible with light aliphatic hydrocarbons, such as pentane and hexane, and with aliphatic chlorides such as trichloroethane and tetrachloroethylene.

Ethanol's miscibility with water contrasts with the immiscibility of longer-chain alcohols (five or more carbon atoms), whose water miscibility decreases sharply as the number of carbons increases. The miscibility of ethanol with alkanes is limited to alkanes up to undecane: mixtures with dodecane and higher alkanes show a miscibility gap below a certain temperature (about 13 °C for dodecane). The miscibility gap tends to get wider with higher alkanes and the temperature for complete miscibility increases.

Ethanol-water mixtures have less volume than the sum of their individual components at the given fractions. Mixing equal volumes of ethanol and water results in only 1.92 volumes of mixture. Mixing ethanol and water is exothermic, with up to 777 J/mol being released at 298 K.

Mixtures of ethanol and water form an azeotrope at about 89 mole-% ethanol and 11 mole-% water or a mixture of 95.6 percent ethanol by mass (or about 97% alcohol by volume) at normal pressure, which boils at 351K (78 °C). This azeotropic composition is strongly temperature- and pressure-dependent and vanishes at temperatures below 303 K.

Hydrogen bonding causes pure ethanol to be hygroscopic to the extent that it readily absorbs water from the air. The polar nature of the hydroxyl group causes ethanol to dissolve many ionic compounds, notably sodium and potassium hydroxides, magnesium chloride, calcium chloride, ammonium chloride, ammonium bromide, and sodium bromide. Sodium and potassium chlorides are slightly soluble in ethanol. Because the ethanol molecule also has a nonpolar end, it will also dissolve nonpolar substances, including most essential oils and numerous flavoring, coloring, and medicinal agents.

The addition of even a few percent of ethanol to water sharply reduces the surface tension of water. This property partially explains the "tears of wine" phenomenon. When wine is swirled in a glass, ethanol evaporates quickly from the thin film of wine on the wall of the glass. As the wine's ethanol content decreases, its surface tension increases and the thin film "beads up" and runs down the glass in channels rather than as a smooth sheet.

An ethanol-water solution that contains 40% alcohol by weight (about 56% by volume) will catch fire if heated to about and if an ignition source is applied to it. This is called its flash point. The flash point of pure ethanol is , less than average room temperature. 

Dishes using burning alcohol for culinary effects are called Flambé.

Ethanol is a byproduct of the metabolic process of yeast. As such, ethanol will be present in any yeast habitat. Ethanol can commonly be found in overripe fruit. Ethanol produced by symbiotic yeast can be found in bertam palm blossoms. Although some animal species such as the pentailed treeshrew exhibit ethanol-seeking behaviors, most show no interest or avoidance of food sources containing ethanol. Ethanol is also produced during the germination of many plants as a result of natural anerobiosis. Ethanol has been detected in outer space, forming an icy coating around dust grains in interstellar clouds.
Minute quantity amounts (average 196 ppb) of endogenous ethanol and acetaldehyde were found in the exhaled breath of healthy volunteers. Auto-brewery syndrome, also known as gut fermentation syndrome, is a rare medical condition in which intoxicating quantities of ethanol are produced through endogenous fermentation within the digestive system.

Ethanol is produced both as a petrochemical, through the hydration of ethylene and, via biological processes, by fermenting sugars with yeast. Which process is more economical depends on prevailing prices of petroleum and grain feed stocks. In the 1970s most industrial ethanol in the United States was made as a petrochemical, but in the 1980s the United States introduced subsidies for corn based ethanol and today it is almost all made from that source.

Ethanol for use as an industrial feedstock or solvent (sometimes referred to as synthetic ethanol) is made from petrochemical feed stocks, primarily by the acid-catalyzed hydration of ethylene:

The catalyst is most commonly phosphoric acid, adsorbed onto a porous support such as silica gel or diatomaceous earth. This catalyst was first used for large-scale ethanol production by the Shell Oil Company in 1947. The reaction is carried out in the presence of high pressure steam at where a 5:3 ethylene to steam ratio is maintained. In the U.S., this process was used on an industrial scale by Union Carbide Corporation and others, but now only LyondellBasell uses it commercially.

In an older process, first practiced on the industrial scale in 1930 by Union Carbide, but now almost entirely obsolete, ethylene was hydrated indirectly by reacting it with concentrated sulfuric acid to produce ethyl sulfate, which was hydrolyzed to yield ethanol and regenerate the sulfuric acid:

CO can also be used as the raw material.

CO can be converted using such organisms as Clostridium ljungdahlii, Clostridium autoethanogenum or Moorella sp. HUC22-1.

CO can be converted using electrochemical reactions at room temperature and pressure.

Lipids can also be used to make ethanol and can be found in such raw materials such as algae.

Ethanol in alcoholic beverages and fuel is produced by fermentation. Certain species of yeast (e.g., "Saccharomyces cerevisiae") metabolize sugar, producing ethanol and carbon dioxide. The chemical equations below summarize the conversion:

Fermentation is the process of culturing yeast under favorable thermal conditions to produce alcohol. This process is carried out at around . Toxicity of ethanol to yeast limits the ethanol concentration obtainable by brewing; higher concentrations, therefore, are obtained by fortification or distillation. The most ethanol-tolerant yeast strains can survive up to approximately 18% ethanol by volume.

To produce ethanol from starchy materials such as cereal grains, the starch must first be converted into sugars. In brewing beer, this has traditionally been accomplished by allowing the grain to germinate, or malt, which produces the enzyme amylase. When the malted grain is mashed, the amylase converts the remaining starches into sugars.

Sugars for ethanol fermentation can be obtained from cellulose. Deployment of this technology could turn a number of cellulose-containing agricultural by-products, such as corncobs, straw, and sawdust, into renewable energy resources. Other agricultural residues such as sugar cane bagasse and energy crops such as switchgrass may also be a sources of fermentable sugars.

Breweries and biofuel plants employ two methods for measuring ethanol concentration. Infrared ethanol sensors measure the vibrational frequency of dissolved ethanol using the CH band at 2900 cm. This method uses a relatively inexpensive solid state sensor that compares the CH band with a reference band to calculate the ethanol content. The calculation makes use of the Beer-Lambert law. Alternatively, by measuring the density of the starting material and the density of the product, using a hydrometer, the change in specific gravity during fermentation indicates the alcohol content. This inexpensive and indirect method has a long history in the beer brewing industry.

Ethylene hydration or brewing produces an ethanol–water mixture. For most industrial and fuel uses, the ethanol must be purified. Fractional distillation at atmospheric pressure can concentrate ethanol to 95.6% by weight (89.5 mole%). This mixture is an azeotrope with a boiling point of , and "cannot" be further purified by distillation. Addition of an entraining agent, such as benzene, cyclohexane, or heptane, allows a new ternary azeotrope comprising the ethanol, water, and the entraining agent to be formed. This lower-boiling ternary azeotrope is removed preferentially, leading to water-free ethanol.

At pressures less than atmospheric pressure, the composition of the ethanol-water azeotrope shifts to more ethanol-rich mixtures, and at pressures less than 70 torr (9.333 kPa), there is no azeotrope, and it is possible to distill absolute ethanol from an ethanol-water mixture. While vacuum distillation of ethanol is not presently economical, pressure-swing distillation is a topic of current research. In this technique, a reduced-pressure distillation first yields an ethanol-water mixture of more than 95.6% ethanol. Then, fractional distillation of this mixture at atmospheric pressure distills off the 95.6% azeotrope, leaving anhydrous ethanol at the bottom.

Apart from distillation, ethanol may be dried by addition of a desiccant, such as molecular sieves, cellulose, and cornmeal. The desiccants can be dried and reused. Molecular sieves can be used to selectively absorb the water from the 95.6% ethanol solution. Synthetic zeolite in pellet form can be used, as well as a variety of plant-derived absorbents, including cornmeal, straw, and sawdust. The zeolite bed can be regenerated essentially an unlimited number of times by drying it with a blast of hot carbon dioxide. Cornmeal and other plant-derived absorbents cannot readily be regenerated, but where ethanol is made from grain, they are often available at low cost. Absolute ethanol produced this way has no residual benzene, and can be used to fortify port and sherry in traditional winery operations.

Membranes can also be used to separate ethanol and water. Membrane-based separations are not subject to the limitations of the water-ethanol azeotrope because the separations are not based on vapor-liquid equilibria. Membranes are often used in the so-called hybrid membrane distillation process. This process uses a pre-concentration distillation column as first separating step. The further separation is then accomplished with a membrane operated either in vapor permeation or pervaporation mode. Vapor permeation uses a vapor membrane feed and pervaporation uses a liquid membrane feed.

A variety of other techniques have been discussed, including the following:


Pure ethanol and alcoholic beverages are heavily taxed as psychoactive drugs, but ethanol has many uses that do not involve its consumption. To relieve the tax burden on these uses, most jurisdictions waive the tax when an agent has been added to the ethanol to render it unfit to drink. These include bittering agents such as denatonium benzoate and toxins such as methanol, naphtha, and pyridine. Products of this kind are called "denatured alcohol."

Absolute or anhydrous alcohol refers to ethanol with a low water content. There are various grades with maximum water contents ranging from 1% to a few parts per million (ppm) levels. If azeotropic distillation is used to remove water, it will contain trace amounts of the material separation agent (e.g. benzene). Absolute alcohol is not intended for human consumption. Absolute ethanol is used as a solvent for laboratory and industrial applications, where water will react with other chemicals, and as fuel alcohol. Spectroscopic ethanol is an absolute ethanol with a low absorbance in ultraviolet and visible light, fit for use as a solvent in ultraviolet-visible spectroscopy.

Pure ethanol is classed as 200 proof in the U.S., equivalent to 175 degrees proof in the UK system.

Rectified spirit, an azeotropic composition of 96% ethanol containing 4% water, is used instead of anhydrous ethanol for various purposes. Wine spirits are about 94% ethanol (188 proof). The impurities are different from those in 95% (190 proof) laboratory ethanol.

Ethanol is classified as a primary alcohol, meaning that the carbon its hydroxyl group attaches to has at least two hydrogen atoms attached to it as well. Many ethanol reactions occur at its hydroxyl group.

In the presence of acid catalysts, ethanol reacts with carboxylic acids to produce ethyl esters and water:
This reaction, which is conducted on large scale industrially, requires the removal of the water from the reaction mixture as it is formed. Esters react in the presence of an acid or base to give back the alcohol and a salt. This reaction is known as saponification because it is used in the preparation of soap. Ethanol can also form esters with inorganic acids. Diethyl sulfate and triethyl phosphate are prepared by treating ethanol with sulfur trioxide and phosphorus pentoxide respectively. Diethyl sulfate is a useful ethylating agent in organic synthesis. Ethyl nitrite, prepared from the reaction of ethanol with sodium nitrite and sulfuric acid, was formerly used as a diuretic.

Strong acid desiccants cause the partial dehydration of ethanol to form diethyl ether and other byproducts. If the dehydration temperature exceeds around , full dehydration will occur and ethylene will be the main product.

Complete combustion of ethanol forms carbon dioxide and water:

Specific heat = 2.44 kJ/(kg·K)

Ethanol is a neutral molecule and the pH of a solution of ethanol in water is nearly 7.00. Ethanol can be quantitatively converted to its conjugate base, the ethoxide ion (CHCHO), by reaction with an alkali metal such as sodium:
or a very strong base such as sodium hydride:
The acidity of water and ethanol are nearly the same, as indicated by their pKa of 15.7 and 16 respectively. Thus, sodium ethoxide and sodium hydroxide exist in an equilibrium that is closely balanced:

Ethanol is not used industrially as a precursor to ethyl halides, but the reactions are illustrative. Ethanol reacts with hydrogen halides to produce ethyl halides such as ethyl chloride and ethyl bromide via an S2 reaction:
These reactions require a catalyst such as zinc chloride.
HBr requires refluxing with a sulfuric acid catalyst. Ethyl halides can, in principle, also be produced by treating ethanol with more specialized halogenating agents, such as thionyl chloride or phosphorus tribromide.

Upon treatment with halogens in the presence of base, ethanol gives the corresponding haloform (CHX, where X = Cl, Br, I). This conversion is called the haloform reaction. "
An intermediate in the reaction with chlorine is the aldehyde called chloral, which forms chloral hydrate upon reaction with water:

Ethanol can be oxidized to acetaldehyde and further oxidized to acetic acid, depending on the reagents and conditions. This oxidation is of no importance industrially, but in the human body, these oxidation reactions are catalyzed by the enzyme liver alcohol dehydrogenase. The oxidation product of ethanol, acetic acid, is a nutrient for humans, being a precursor to acetyl CoA, where the acetyl group can be spent as energy or used for biosynthesis.

Pure ethanol will irritate the skin and eyes. Nausea, vomiting, and intoxication are symptoms of ingestion. Long-term use by ingestion can result in serious liver damage. Atmospheric concentrations above one in a thousand are above the European Union occupational exposure limits.

The fermentation of sugar into ethanol is one of the earliest biotechnologies employed by humans. The intoxicating effects of ethanol consumption have been known since ancient times. Ethanol has been used by humans since prehistory as the intoxicating ingredient of alcoholic beverages. Dried residue on 9,000-year-old pottery found in China suggests that Neolithic people consumed alcoholic beverages.

The medieval Muslims used the distillation process extensively, and applied it to the distillation of alcohol. The Arab chemist Al-Kindi unambiguously described the distillation of wine in the 9th century. The process later spread from the Middle East to Italy. Production of alcohol from distilled wine was later recorded by the School of Salerno alchemists in the 12th century. Mention of absolute alcohol, in contrast with alcohol-water mixtures, was later made by Raymond Lull in the 14th century.

In China, archaeological evidence indicates that the true distillation of alcohol began during the 12th century Jin or Southern Song dynasties. A still has been found at an archaeological site in Qinglong, Hebei, dating to the 12th century. In India, the true distillation of alcohol was introduced from the Middle East, and was in wide use in the Delhi Sultanate by the 14th century.

In 1796, German-Russian chemist Johann Tobias Lowitz obtained pure ethanol by mixing partially purified ethanol (the alcohol-water azeotrope) with an excess of anhydrous alkali and then distilling the mixture over low heat. French chemist Antoine Lavoisier described ethanol as a compound of carbon, hydrogen, and oxygen, and in 1807 Nicolas-Théodore de Saussure determined ethanol's chemical formula. Fifty years later, Archibald Scott Couper published the structural formula of ethanol. It was one of the first structural formulas determined.

Ethanol was first prepared synthetically in 1825 by Michael Faraday. He found that sulfuric acid could absorb large volumes of coal gas. He gave the resulting solution to Henry Hennell, a British chemist, who found in 1826 that it contained "sulphovinic acid" (ethyl hydrogen sulfate). In 1828, Hennell and the French chemist Georges-Simon Serullas independently discovered that sulphovinic acid could be decomposed into ethanol. Thus, in 1825 Faraday had unwittingly discovered that ethanol could be produced from ethylene (a component of coal gas) by acid-catalyzed hydration, a process similar to current industrial ethanol synthesis.

Ethanol was used as lamp fuel in the United States as early as 1840, but a tax levied on industrial alcohol during the Civil War made this use uneconomical. The tax was repealed in 1906. Use as an automotive fuel dates back to 1908, with the Ford Model T able to run on petrol (gasoline) or ethanol. It fuels some spirit lamps.

Ethanol intended for industrial use is often produced from ethylene. Ethanol has widespread use as a solvent of substances intended for human contact or consumption, including scents, flavorings, colorings, and medicines. In chemistry, it is both a solvent and a feedstock for the synthesis of other products. It has a long history as a fuel for heat and light, and more recently as a fuel for internal combustion engines.




</doc>
<doc id="10049" url="https://en.wikipedia.org/wiki?curid=10049" title="Eric Clapton">
Eric Clapton

Eric Patrick Clapton, (born 1945), is an English rock and blues guitarist, singer, and songwriter. He is the only three-time inductee to the Rock and Roll Hall of Fame: once as a solo artist and separately as a member of the Yardbirds and of Cream. Clapton has been referred to as one of the most important and influential guitarists of all time. Clapton ranked second in "Rolling Stone" magazine's list of the "100 Greatest Guitarists of All Time" and fourth in Gibson's "Top 50 Guitarists of All Time". He was also named number five in "Time" magazine's list of "The 10 Best Electric Guitar Players" in 2009.

In the mid-1960s Clapton left the Yardbirds to play with John Mayall & the Bluesbreakers. Immediately after leaving Mayall, Clapton formed the power trio Cream with drummer Ginger Baker and bassist Jack Bruce, in which Clapton played sustained blues improvisations and "arty, blues-based psychedelic pop". After Cream broke up, he formed blues rock band Blind Faith with Baker, Steve Winwood, and Ric Grech. Clapton's solo career began in the 1970s, where his work bore the influence of the mellow style of J. J. Cale and the reggae of Bob Marley. His version of Marley's "I Shot the Sheriff" helped reggae reach a mass market. Two of his most popular recordings were "Layla", recorded with Derek and the Dominos; and Robert Johnson's "Crossroads", recorded with Cream. Following the death of his son Conor in 1991, Clapton's grief was expressed in the song "Tears in Heaven", which was featured on his "Unplugged" album.

Clapton has been the recipient of 18 Grammy Awards, and the Brit Award for Outstanding Contribution to Music. In 2004 he was awarded a CBE at Buckingham Palace for services to music. He has received four Ivor Novello Awards from the British Academy of Songwriters, Composers and Authors, including the Lifetime Achievement Award. In his solo career, Clapton has sold more than 130 million records worldwide. In 1998, Clapton, a recovering alcoholic and drug addict, founded the Crossroads Centre on Antigua, a medical facility for recovering substance abusers.

Clapton was born on 30 March 1945 in Ripley, Surrey, England, to 16-year-old Patricia Molly Clapton ( 1929 – March 1999) and Edward Walter Fryer ( 1920 – 1985), a 25-year-old soldier from Montreal, Quebec. Fryer shipped off to war prior to Clapton's birth and then returned to Canada. Clapton grew up believing that his grandmother, Rose, and her second husband, Jack Clapp, Patricia's stepfather, were his parents, and that his mother was actually his older sister. The similarity in surnames gave rise to the erroneous belief that Clapton's real surname is Clapp (Reginald Cecil Clapton was the name of Rose's first husband, Eric Clapton's maternal grandfather). Years later, his mother married another Canadian soldier and moved to Germany, leaving young Eric with his grandparents in Surrey.

Clapton received an acoustic Hoyer guitar, made in Germany, for his thirteenth birthday, but the inexpensive steel-stringed instrument was difficult to play and he briefly lost interest. Two years later Clapton picked it up again and started playing consistently. Clapton was influenced by the blues from an early age, and practised long hours to learn the chords of blues music by playing along to the records. He preserved his practice sessions using his portable Grundig reel-to-reel tape recorder, listening to them over and over until he felt he'd got it right.

In 1961, after leaving Hollyfield School in Surbiton, Clapton studied at the Kingston College of Art but was dismissed at the end of the academic year because his focus remained on music rather than art. His guitar playing was so advanced that, by the age of 16, he was getting noticed. Around this time, Clapton began busking around Kingston, Richmond, and the West End.
In 1962, Clapton started performing as a duo with fellow blues enthusiast David Brock in pubs around Surrey. When he was seventeen years old, Clapton joined his first band, an early British R&B group, the Roosters, whose other guitarist was Tom McGuinness. He stayed with this band from January until August 1963. In October of that year, Clapton did a seven-gig stint with Casey Jones & the Engineers.

In October 1963, Clapton joined the Yardbirds, a blues-influenced rock and roll band, and stayed with them until March 1965. Synthesising influences from Chicago blues and leading blues guitarists such as Buddy Guy, Freddie King, and B.B. King, Clapton forged a distinctive style and rapidly became one of the most talked-about guitarists in the British music scene. The band initially played Chess/Checker/Vee-Jay blues numbers and began to attract a large cult following when they took over the Rolling Stones' residency at the Crawdaddy Club in Richmond. They toured England with American bluesman Sonny Boy Williamson II; a joint LP album, recorded in December 1963, was issued in 1965.

Yardbirds' rhythm guitarist, Chris Dreja, recalled that whenever Clapton broke a guitar string during a concert, he would stay on stage and replace it. The English audiences would wait out the delay by doing what is called a "slow handclap". Clapton's nickname of "Slowhand" came from Giorgio Gomelsky, a pun on the slow handclapping that ensued when Clapton stopped playing while he replaced a string. In December 1964, Clapton made his first appearance at the Royal Albert Hall, London, with the Yardbirds. Since then, Clapton has performed at the Hall over 200 times, and has stated that performing at the venue is like "playing in my front room".

In March 1965, Clapton and the Yardbirds had their first major hit, "For Your Love", written by songwriter Graham Gouldman, who also wrote hit songs for Herman's Hermits and the Hollies (and would later achieve success of his own as a member of 10cc). In part because of its success, the Yardbirds elected to move toward a pop-oriented sound, much to the annoyance of Clapton, who was devoted to the blues and not commercial success. He left the Yardbirds on the day that "For Your Love" went public, a move that left the band without its lead guitarist and most accomplished member. Clapton suggested fellow guitarist Jimmy Page to be his replacement, but Page declined out of loyalty to Clapton, putting Jeff Beck forward.
Beck and Page played together in the Yardbirds for a while, but Beck, Page, and Clapton were never in the group together. They first appeared together on the 12-date benefit tour for Action for Research into multiple sclerosis in 1983 with the first date taking place on 23 September at the Royal Albert Hall.

Clapton joined John Mayall & the Bluesbreakers in April 1965, only to quit a few months later. In June, Clapton was invited to jam with Jimmy Page, recording a number of tracks that would be retroactively credited to The Immediate All-Stars. In the summer of 1965 he left for Greece with a band called the Glands, which included his old friend Ben Palmer on piano. In November 1965 he rejoined John Mayall. In March 1966, while still a member of the Bluesbreakers, Clapton briefly collaborated on a side project with Jack Bruce and Steve Winwood among others, recording only a few of tracks under the name Eric Clapton and the Powerhouse. During his second Bluesbreakers stint, Clapton gained a reputation as the best blues guitarist on the club circuit. Although Clapton gained world fame for his playing on the influential album, "Blues Breakers – John Mayall – With Eric Clapton", this album was not released until he had left the band for the last time in July 1966.

Having swapped his Fender Telecaster and Vox AC30 amplifier for a 1960 Gibson Les Paul Standard guitar and Marshall amplifier, Clapton's sound and playing inspired the famous slogan "Clapton is God", spray-painted by an unknown admirer on a wall in Islington in 1967.
The graffiti was captured in a now-famous photograph, in which a dog is urinating on the wall. Clapton is reported to have been embarrassed by the slogan, saying in his "The South Bank Show" profile in 1987, "I never accepted that I was the greatest guitar player in the world. I always "wanted" to be the greatest guitar player in the world, but that's an ideal, and I accept it as an ideal".

Clapton left the Bluesbreakers in July 1966 (replaced by Peter Green) and was invited by drummer Ginger Baker to play in his newly formed band Cream, one of the earliest supergroups, with Jack Bruce on bass (previously of the Bluesbreakers, the Graham Bond Organisation and Manfred Mann). Before the formation of Cream, Clapton was not well known in the United States; he left the Yardbirds before "For Your Love" hit the U.S. Top Ten, and had yet to perform there. During his time with Cream, Clapton began to develop as a singer, songwriter, and guitarist, though Bruce took most of the lead vocals and wrote the majority of the material with lyricist Pete Brown. Cream's first gig was an unofficial performance at the Twisted Wheel Club in Manchester on 1966 before their full debut two nights later at the National Jazz and Blues Festival in Windsor. Cream established its enduring legend with the high-volume blues jamming and extended solos of their live shows.
By early 1967, fans of the emerging blues-rock sound in Britain had begun to portray Clapton as Britain's top guitarist; however, he found himself rivalled by the emergence of Jimi Hendrix, an acid rock-infused guitarist who used wailing feedback and effects pedals to create new sounds for the instrument. Hendrix attended a performance of the newly formed Cream at the Central London Polytechnic on 1966, during which he sat in on a double-timed version of "Killing Floor". Top UK stars, including Clapton, Pete Townshend, members of the Rolling Stones, and the Beatles, avidly attended Hendrix's early club performances. Hendrix's arrival had an immediate and major effect on the next phase of Clapton's career.

Clapton first visited the United States while touring with Cream. In March 1967, Cream performed a nine-show stand at the RKO Theater in New York. They recorded "Disraeli Gears" in New York from 11 to 15 May 1967. Cream's repertoire varied from hard rock ("I Feel Free") to lengthy blues-based instrumental jams ("Spoonful"). "Disraeli Gears" featured Clapton's searing guitar lines, Bruce's soaring vocals and prominent, fluid bass playing, and Baker's powerful, polyrhythmic jazz-influenced drumming. Together, Cream's talents secured them as an influential power trio.

In 28 months, Cream had become a commercial success, selling millions of records and playing throughout the U.S. and Europe. They redefined the instrumentalist's role in rock and were one of the first blues-rock bands to emphasise musical virtuosity and lengthy jazz-style improvisation sessions. Their US hit singles include "Sunshine of Your Love" (, 1968), "White Room" (, 1968) and "Crossroads" (, 1969) – a live version of Robert Johnson's "Cross Road Blues". Though Cream was hailed as one of the greatest groups of its day, and the adulation of Clapton as a guitar legend reached new heights, the supergroup was short-lived. Drug and alcohol use escalated tension between the three members, and conflicts between Bruce and Baker eventually led to Cream's demise. A strongly critical "Rolling Stone" review of a concert of the group's second headlining U.S. tour was another significant factor in the trio's demise, and it affected Clapton profoundly.

Cream's farewell album, "Goodbye", featuring live performances recorded at The Forum, Los Angeles, 1968, was released shortly after Cream disbanded; it also featured the studio single "Badge", co-written by Clapton and George Harrison. Clapton met Harrison and became close friends with him after the Beatles shared a bill with the Clapton-era Yardbirds at the London Palladium. Clapton played the lead guitar solo on Harrison's "While My Guitar Gently Weeps" from the Beatles' "White Album" (1968). Harrison's debut solo album, "Wonderwall Music", in 1968, became the first of many Harrison solo records to include Clapton on guitar. Clapton would go largely uncredited for his contributions to Harrison's albums due to contractual restraints, and Harrison was credited as "L'Angelo Misterioso" for his contributions to the song "Badge" on "Goodbye". The pair would often play live together as each other's guest. A year after Harrison's death in 2001, Clapton was musical director for the Concert for George.

In January 1969, when the Beatles were recording/filming what became "Let It Be", tensions became so acute that Harrison quit the group for several days, prompting the others to consider replacing him with Clapton, an idea that particularly appealed to John Lennon. Michael Lindsay-Hogg, television director of the recording sessions for "Let It Be", states, "I was there when John mentioned Clapton — but that wasn't going to happen. Would Eric have become a Beatle? No. Paul didn't want to go there. He didn't want them to break up. Then George came back." Clapton was on good terms with all four of the Beatles; in December 1968 he had played with Lennon at "The Rolling Stones Rock and Roll Circus" as part of the one-off group the Dirty Mac.

Cream briefly reunited in 1993 to perform at the ceremony inducting them into the Rock and Roll Hall of Fame; a full reunion took place in May 2005, with Clapton, Bruce, and Baker playing four sold-out concerts at London's Royal Albert Hall, and three shows at New York's Madison Square Garden that October. Recordings from the London shows, "Royal Albert Hall London May 2-3-5-6, 2005", were released on CD, LP, and DVD in September/December 2005.

Clapton's next group, Blind Faith (1969), was composed of Cream drummer Ginger Baker, Steve Winwood of Traffic, and Ric Grech of Family, and yielded one LP and one arena-circuit tour. The supergroup debuted before 100,000 fans in London's Hyde Park on 1969. They performed several dates in Scandinavia and began a sold-out American tour in July before their only album was released. The LP "Blind Faith" consisted of just six songs, one of them the hit "Can't Find My Way Home". The album's jacket image of a topless pubescent girl was deemed controversial in the United States and was replaced by a photograph of the band. Blind Faith dissolved after less than seven months.

Clapton subsequently toured as a sideman for an act that had opened for Blind Faith, Delaney and Bonnie and Friends. He also played two dates as a member of the Plastic Ono Band that autumn, including a recorded performance at the Toronto Rock and Roll Revival in September 1969 released as the album "Live Peace in Toronto 1969". On 30 September 1969, Clapton played lead guitar on Lennon's second solo single, "Cold Turkey". On 15 December 1969 Clapton performed with Lennon, George Harrison, and others as the Plastic Ono Band at a fundraiser for UNICEF in London.

Delaney Bramlett encouraged Clapton in his singing and writing. During the summer of 1969, Clapton and Bramlett contributed to the "Music From Free Creek" "supersession" project. Clapton, appearing as "King Cool" for contractual reasons, played with Dr. John on three songs, joined by Bramlett on two tracks. Using the Bramletts' backing group and an all-star cast of session players (including Leon Russell and Stephen Stills), Clapton recorded his first solo album during two brief tour hiatuses, titled "Eric Clapton". Delaney Bramlett co-wrote six of the songs with Clapton, also producing the LP, and Bonnie Bramlett co-wrote "Let It Rain". The album yielded the unexpected US No. 18 hit, J. J. Cale's "After Midnight". Clapton also worked with much of Delaney and Bonnie's band to record George Harrison's "All Things Must Pass" in spring 1970. During this busy period, Clapton recorded with other artists including Dr. John, Leon Russell, Plastic Ono Band, Billy Preston, Ringo Starr and Dave Mason. Other notable recordings from this period include Clapton's guitar work on "Go Back Home" from Stephen Stills' self-titled first solo album.

With the intention of counteracting the "star" cult faction that had begun to form around him, Clapton assembled a new band composed of Delaney and Bonnie's former rhythm section, Bobby Whitlock as keyboardist and vocalist, Carl Radle as the bassist, and drummer Jim Gordon, with Clapton playing guitar. It was his intention to show that he need not fill a starring role, and functioned well as a member of an ensemble. During this period, Clapton was increasingly influenced by The Band and their album "Music from Big Pink", saying, "What I appreciated about the Band was that they were more concerned with songs and singing. They would have three- and four-part harmonies, and the guitar was put back into perspective as being accompaniment. That suited me well, because I had gotten so tired of the virtuosity—or "pseudo"-virtuosity—thing of long, boring guitar solos just because they were expected. The Band brought things back into perspective. The priority was the song."
The band was originally called "Eric Clapton and Friends". The name "Derek and the Dominos" was a fluke that occurred when the band's provisional name of "Del and the Dynamos" was misread as Derek and the Dominos. Clapton's biography states that Tony Ashton of Ashton, Gardner and Dyke told Clapton to call the band "Del and the Dominos", since "Del" was his nickname for Eric Clapton. Del and Eric were combined and the final name became "Derek and the Dominos".

Clapton's close friendship with George Harrison brought him into contact with Harrison's wife, Pattie Boyd, with whom he became deeply infatuated. When she spurned his advances, Clapton's unrequited affections prompted most of the material for the Dominos' album, "Layla and Other Assorted Love Songs" (1970). Heavily blues-influenced, the album features the twin lead guitars of Clapton and Duane Allman, with Allman's slide guitar as a key ingredient of the sound. Working at Criteria Studios in Miami with Atlantic Records producer Tom Dowd, who had worked with Clapton on Cream's "Disraeli Gears", the band recorded a double album.

The album features the hit love song "Layla", inspired by the classical poet of Persian literature, Nizami Ganjavi's "The Story of Layla and Majnun", a copy of which Ian Dallas had given to Clapton. The book moved Clapton profoundly, as it was the tale of a young man who fell hopelessly in love with a beautiful, unavailable woman and who went crazy because he could not marry her. The two parts of "Layla" were recorded in separate sessions: the opening guitar section was recorded first, and for the second section, laid down a few weeks later, drummer Jim Gordon played the piano part for the melody which he claimed to have written (though Bobby Whitlock stated that Rita Coolidge wrote it).

The "Layla" LP was actually recorded by a five-piece version of the group, thanks to the unforeseen inclusion of guitarist Duane Allman of the Allman Brothers Band. A few days into the Layla sessions, Dowd—who was also producing the Allmans—invited Clapton to an Allman Brothers outdoor concert in Miami. The two guitarists met first on stage, then played all night in the studio, and became friends. Duane first added his slide guitar to "Tell the Truth" and "Nobody Knows You When You're Down and Out". In four days, the five-piece Dominos recorded "Key to the Highway", "Have You Ever Loved a Woman" (a blues standard popularised by Freddie King and others), and "Why Does Love Got to be So Sad". In September, Duane briefly left the sessions for gigs with his own band, and the four-piece Dominos recorded "I Looked Away", "Bell Bottom Blues", and "Keep on Growing". Duane returned to record "I am Yours", "Anyday", and "It's Too Late". On 9 September, they recorded Hendrix's "Little Wing" and the title track. The following day, the final track, "It's Too Late", was recorded.
Tragedy dogged the group throughout its brief career. During the sessions, Clapton was devastated by news of the death of Jimi Hendrix; eight days previously the band had cut a cover of "Little Wing" as a tribute to Hendrix. On 1970, one day before Hendrix's death, Clapton had purchased a left-handed Fender Stratocaster that he had planned to give to Hendrix as a birthday gift. Adding to Clapton's woes, the "Layla" album received only lukewarm reviews upon release. The shaken group undertook a US tour without Allman, who had returned to the Allman Brothers Band. Despite Clapton's later admission that the tour took place amidst a veritable blizzard of drugs and alcohol, it resulted in the live double album "In Concert".

A second record was in the works when a clashing of egos took place and Clapton walked, thus disbanding the group. Allman was killed in a motorcycle accident on 1971. Clapton wrote later in his autobiography that he and Allman were inseparable during the sessions in Florida; he talked about Allman as the "musical brother I'd never had but wished I did". Although Radle would remain Clapton's bass player until the summer of 1979 (Radle died in May 1980 from the effects of alcohol and narcotics), it would be 2003 before Clapton and Whitlock appeared together again (Clapton guested on Whitlock's appearance on the "Later with Jools Holland" show). Another tragic footnote to the Dominos story was the fate of drummer Jim Gordon, who was an undiagnosed schizophrenic and years later murdered his mother during a psychotic episode. Gordon was confined to 16-years-to-life imprisonment, later being moved to a mental institution, where he remains today.

Clapton's career successes in the 1970s were in stark contrast with the struggles he coped with in his personal life, which was troubled by romantic longings and drug and alcohol addiction. He became infatuated with Pattie Boyd, who at the time was married to close friend George Harrison, and withdrew from recording and touring to isolation in his Surrey residence as the band broke up. There he nursed a heroin addiction, which resulted in a lengthy career hiatus interrupted only by the Concert for Bangladesh in August 1971 (where he passed out on stage, was revived, and managed to finish his performance). In January 1973, the Who's Pete Townshend organised a comeback concert for Clapton at London's Rainbow Theatre, aptly titled the "Rainbow Concert", to help Clapton kick his addiction. Clapton returned the favour by playing "The Preacher" in Ken Russell's film version of the Who's "Tommy" in 1975; his appearance in the film (performing "Eyesight to the Blind") is notable as he is clearly wearing a fake beard in some shots, the result of deciding to shave off his real beard after the initial takes in an attempt to force the director to remove his earlier scene from the movie and leave the set.
In 1974, Clapton started living with Pattie Boyd (they would not marry until 1979) and was no longer using heroin (although he gradually began to drink heavily). He assembled a low-key touring band that included Radle, Miami guitarist George Terry, keyboardist Dick Sims (who died in 2011), drummer Jamie Oldaker, and vocalists Yvonne Elliman and Marcy Levy (also known as Marcella Detroit). With this band Clapton recorded "461 Ocean Boulevard" (1974), an album with an emphasis on more compact songs and fewer guitar solos; the cover version of "I Shot the Sheriff" was Clapton's first number one hit and was important in bringing reggae and the music of Bob Marley to a wider audience. The 1975 album "There's One in Every Crowd" continued this trend. The album's original title, "The World's Greatest Guitar Player (There's One in Every Crowd)", was changed before pressing, as it was felt its ironic intention would be misunderstood. The band toured the world and subsequently released the 1975 live LP, "E. C. Was Here". Clapton continued to release albums and toured regularly. Highlights of the period include "No Reason to Cry" (a collaboration with Bob Dylan and The Band); "Slowhand", which featured "Wonderful Tonight" and a second J. J. Cale cover, "Cocaine". In 1976 he performed as one of a string of notable guests at the farewell performance of The Band, filmed in a Martin Scorsese documentary called "The Last Waltz".

In 1981 Clapton was invited by producer Martin Lewis to appear at the Amnesty International benefit "The Secret Policeman's Other Ball" in London. Clapton accepted the invitation and teamed up with Jeff Beck to perform a series of duets—reportedly their first ever billed stage collaboration. Three of the performances were released on the album of the show, and one of the songs was featured in the film. The performances at London's Drury Lane theatre heralded a return to form and prominence for Clapton in the new decade. Many factors had influenced Clapton's comeback, including his "deepening commitment to Christianity", to which he had converted prior to his heroin addiction.

After calling his manager and admitting he was an alcoholic, Clapton flew to Minneapolis–Saint Paul in January 1982 and checked in at Hazelden Treatment Center, located in Center City, Minnesota. On the flight over, Clapton indulged in a large number of drinks, for fear he would never be able to drink again. Clapton wrote in his autobiography:

After being discharged, it was recommended by doctors of Hazelden that Clapton not partake in any activities that would act as triggers for his alcoholism or stress. A few months after his discharge, Clapton began working on his next album, against doctors' orders. Working with Tom Dowd, he produced what he thought as his "most forced" album to date, "Money and Cigarettes". Clapton chose the name of the album "because that's all I saw myself having left" after his first rehabilitation from alcoholism.

In 1984 he performed on former Pink Floyd member Roger Waters' solo album, "The Pros and Cons of Hitch Hiking", and joined the supporting tour. Since then Waters and Clapton have had a close relationship. In 2005 they performed together for the Tsunami Relief Fund. In 2006 they performed at the Highclere Castle, in aid of the Countryside Alliance, playing two set pieces of "Wish You Were Here" and "Comfortably Numb". Clapton, now a seasoned charity performer, played at the Live Aid concert on 13 July 1985. When offered a slot close to peak viewing hours, he was apparently flattered. As Clapton recovered from his addictions, his album output continued in the 1980s, including two produced with Phil Collins, 1985's "Behind the Sun", which produced the hits "Forever Man" and "She's Waiting", and 1986's "August".
"August" was suffused with Collins's trademark drum and horn sound, and became Clapton's biggest seller in the UK to date, matching his highest chart position, number 3. The album's first track, the hit "It's in the Way That You Use It", was featured in the Tom Cruise – Paul Newman movie "The Color of Money." The horn-peppered "Run" echoed Collins' "Sussudio" and other work, while "Tearing Us Apart" (with Tina Turner) and "Miss You" continued Clapton's more angry sound. This rebound kicked off Clapton's two-year period of touring with Collins and their "August" collaborators, bassist Nathan East and keyboard player/songwriter Greg Phillinganes. While on tour for "August", two concert videos were recorded of the four-man band, "Eric Clapton Live from Montreux" and "Eric Clapton and Friends". Clapton later remade "After Midnight" as a single and a promotional track for the Michelob beer brand, which had also used earlier songs by Collins and Steve Winwood. Clapton won a British Academy Television Award for his collaboration with Michael Kamen on the score for the 1985 BBC Television thriller serial "Edge of Darkness". In 1989, Clapton released "Journeyman", an album which covered a wide range of styles including blues, jazz, soul and pop. Collaborators included George Harrison, Phil Collins, Daryl Hall, Chaka Khan, Mick Jones, David Sanborn and Robert Cray. At the 1987 Brit Awards in London, Clapton was awarded the prize for Outstanding Contribution to Music.

Clapton would also get together with the Bee Gees for charity. The supergroup called itself The Bunburys, and recorded a charity album with the proceeds going to the Bunbury Cricket Club in Cheshire, which plays exhibition cricket matches to raise money for nonprofit organisations in England. The Bunburys recorded three songs for "The Bunbury Tails": "We're the Bunburys", "Bunbury Afternoon", and "Fight (No Matter How Long)". The last song also appeared on "The 1988 Summer Olympics Album", and went to on the rock music chart. Clapton also played at the cricket club's 25th anniversary celebrations in 2011 which was held at London's Grosvenor House Hotel.

The 1990s brought a series of 32 concerts to the Royal Albert Hall, such as the 24 Nights series of concerts that took place around January through February 1990, and February to March 1991. On 1990, fellow blues guitarist Stevie Ray Vaughan, who was touring with Clapton, and three members of their road crew were killed in a helicopter crash between concerts. Then, on 1991, Clapton's four-year-old son, Conor, died after falling from the 53rd-floor window of his mother's friend's New York City apartment at 117 East 57th Street. Conor's funeral took place on 28 March at St Mary Magdalene's Church in Clapton's home village in Ripley, Surrey.

Clapton's grief was expressed in the song "Tears in Heaven", which was co-written by Will Jennings. At the 35th Annual Grammy Awards, Clapton received six Grammys for the single "Tears in Heaven" and his "Unplugged" album. "Unplugged" features Clapton performing live in front of a small audience on 16 January 1992 at Bray Film Studios in Windsor, Berkshire, England. The album reached number one on the "Billboard" 200, and is certified Diamond by the RIAA for selling over 10 million copies in the US. It reached number two in the UK Albums Chart and is certified four times platinum in the UK. On 9 September 1992, Clapton performed "Tears in Heaven" at the 1992 MTV Video Music Awards, and won the award for Best Male Video. In 1992 he received the Ivor Novello Award for Lifetime Achievement from the British Academy of Songwriters, Composers and Authors.

In October 1992 Clapton was among the dozens of artists performing at Bob Dylan's 30th Anniversary Concert Celebration. Recorded at Madison Square Garden in New York City, the live two-disk CD/DVD captured a show full of celebrities performing classic Dylan songs, with Clapton playing the lead on a nearly 7-minute version of Dylan's "Knockin' on Heaven's Door" as part of the finale. While "Unplugged" featured Clapton playing acoustic guitar, his 1994 album "From the Cradle" contained new versions of old blues standards, highlighted by his electric guitar playing. In 1995, Clapton for the first and only time featured on a UK No. 1 single, collaborating with Chrissie Hynde, Cher and Neneh Cherry on a solo to a cover of "Love Can Build a Bridge" released in aid of the British charity telethon Comic Relief.

Clapton's 1996 recording of the Wayne Kirkpatrick/Gordon Kennedy/Tommy Sims tune "Change the World" (featured in the soundtrack of the film "Phenomenon") won the Grammy Award for Song of the Year in 1997, the same year he recorded "Retail Therapy" (an album of electronic music with Simon Climie under the pseudonym TDF). On 15 September 1997, Clapton appeared at the "Music for Montserrat" concert at the Royal Albert Hall, London, performing "Layla" and "Same Old Blues" before finishing with "Hey Jude" alongside fellow English artists Paul McCartney, Elton John, Phil Collins, Mark Knopfler and Sting. That autumn, Clapton released the album "Pilgrim", the first record featuring new material for almost a decade.

In 1996 Clapton had a relationship with singer-songwriter Sheryl Crow. They remain friends, and Clapton appeared as a guest on Crow's Central Park Concert. The duo performed a Cream hit single, "White Room". Later, Clapton and Crow performed an alternate version of "Tulsa Time" with other guitar legends at the Crossroads Guitar Festival in June 2007 as well as Robert Johnson's blues classic "Crossroads" at London's Hyde Park in August 2008 with John Mayer and Robert Randolph.

In 1998 Clapton, then 53, met 22-year-old administrative assistant Melia McEnery in Columbus, Ohio, at a party given for him after a performance. He quietly dated her for a year, and went public with the relationship in 1999. They married on 2001 at St Mary Magdalene church in Clapton's birthplace, Ripley. They have three daughters, Julie Rose ( 2001), Ella May ( 2003), and Sophie Belle ( 2005).

At the 41st Annual Grammy Awards on 24 February 1999, Clapton received his third Grammy Award for Best Male Pop Vocal Performance, for his song "My Father's Eyes". In October 1999, the compilation album, "", was released, which contained a new song, "Blue Eyes Blue", that also appears in soundtrack for the film, "Runaway Bride". Clapton finished the twentieth century with collaborations with Carlos Santana and B.B. King.

Following the release of the 2001 record "Reptile", in June 2002, Clapton performed "Layla" and "While My Guitar Gently Weeps" at the Party at the Palace concert in the grounds of Buckingham Palace. On 29 November 2002, the Concert for George was held at the Royal Albert Hall, a tribute to George Harrison, who had died a year earlier of lung cancer. Clapton was a performer and the musical director. The concert featured Paul McCartney, Ringo Starr, Jeff Lynne, Tom Petty and the Heartbreakers, Ravi Shankar, Gary Brooker, Billy Preston, Joe Brown and Dhani Harrison. In 2004, Clapton released two albums of covers of songs by bluesman Robert Johnson, "Me and Mr. Johnson" and "Sessions for Robert J". Guitarist Doyle Bramhall II worked on the album with Clapton (after opening Clapton's 2001 tour with his band Smokestack) and would join him on his 2004 tour. In 2004, "Rolling Stone" ranked Clapton No. 53 on their list of the "100 Greatest Artists of All Time".
On 22 January 2005, Clapton performed in the Tsunami Relief Concert held at the Millennium Stadium in Cardiff, in aid of the victims of the 2004 Indian Ocean earthquake. In May 2005 Eric Clapton, Jack Bruce, and Ginger Baker reunited as Cream for a series of concerts at the Royal Albert Hall in London. Concert recordings were released on CD and DVD. Later, Cream performed in New York at Madison Square Garden. "Back Home", Clapton's first album of new original material in nearly five years, was released on Reprise Records on .

A collaboration with guitarist J. J. Cale, titled "The Road to Escondido", was released on 2006, featuring Derek Trucks and Billy Preston (Preston had also been a part of Clapton's 2004 touring band). The 14-track CD was produced and recorded by the duo in August 2005 in California. He invited Trucks to join his band for his 2006–2007 world tour. Bramhall remained in the band as well, giving Clapton three elite guitarists in his band and thus allowing him to revisit many Derek and the Dominos songs that he hadn't played in decades. Trucks became the third member of the Allman Brothers Band to tour supporting Clapton, the second being pianist/keyboardist Chuck Leavell, who appeared on the "MTV Unplugged" album and the "24 Nights" performances at the Royal Albert Hall, London in 1990 and 1991, as well as Clapton's 1992 US tour.

On 20 May 2006, Clapton performed with Queen drummer Roger Taylor and former Pink Floyd bassist/songwriter Roger Waters at Highclere Castle, Hampshire, in support of the Countryside Alliance which promotes issues relating to the British countryside. On 2006, Clapton made a guest appearance at the Bob Dylan concert in Columbus, Ohio, playing guitar on three songs in Jimmie Vaughan's opening act. The chemistry between Trucks and Clapton convinced him to invite the Derek Trucks Band to open for Clapton's set at his 2007 Crossroads Guitar Festival. Trucks remained on set afterward and performed with Clapton's band throughout his performances. The rights to Clapton's official memoirs, written by Christopher Simon Sykes and published in 2007, were sold at the 2005 Frankfurt Book Fair for .

On 26 February 2008, it was reported that North Korean officials had invited Clapton to play a concert in the communist state. Clapton's management received the invitation and passed it on to the singer, who agreed in principle and suggested it take place sometime in 2009. Kristen Foster, a spokesperson, said, "Eric Clapton receives numerous offers to play in countries around the world", and "[t]here is no agreement whatsoever for him to play in North Korea".
In 2007 Clapton learned more about his father, a Canadian soldier who left the UK after the war. Although Clapton's grandparents eventually told him the truth about his parentage, he only knew that his father's name was Edward Fryer. This was a source of disquiet for Clapton, as witnessed by his 1998 song "My Father's Eyes". A Montreal journalist named Michael Woloschuk researched Canadian Armed Forces service records and tracked down members of Fryer's family, and finally pieced together the story. He learned that Clapton's father was Edward Walter Fryer, born 1920, in Montreal and died in Newmarket, Ontario. Fryer was a musician (piano and saxophone) and a lifelong drifter who was married several times, had several children, and apparently never knew that he was the father of Eric Clapton. Clapton thanked Woloschuk in an encounter at Macdonald Cartier Airport, in Ottawa, Ontario, Canada.

In February 2008 Clapton performed with his long-time friend Steve Winwood at Madison Square Garden and guested on his recorded single, "Dirty City", on Winwood's album "Nine Lives". The two former Blind Faith bandmates met again for a series of 14 concerts throughout the United States in June 2009. Clapton's 2008 Summer Tour began on at the Ford Amphitheatre, Tampa, Florida, and then moved to Canada, Ireland, England, Norway, Iceland, Denmark, Poland, Germany, and Monaco. On 2008, he headlined Saturday night for Hard Rock Calling 2008 in London's Hyde Park (previously Hyde Park Calling) with support from Sheryl Crow and John Mayer. In September 2008 Clapton performed at a private charity fundraiser for The Countryside Alliance at Floridita in Soho, London, that included such guests as the London Mayor Boris Johnson.
In March 2009, the Allman Brothers Band (amongst many notable guests) celebrated their 40th year, dedicating their string of concerts to the late Duane Allman on their annual run at the Beacon Theatre. Eric Clapton was one of the performers, with drummer Butch Trucks remarking that the performance was not the typical Allman Brothers experience, given the number and musical styles of the guests who were invited to perform. Songs like "In Memory of Elizabeth Reed" were punctuated with others, including "The Weight", with Levon Helm; Johnny Winter sitting in on Hendrix's "Red House"; and "Layla". On 2009 Clapton appeared as a featured guest at the Royal Albert Hall, playing "Further on Up the Road" with Joe Bonamassa.

Clapton was scheduled to be one of the performers at the Rock and Roll Hall of Fame's 25th anniversary concert in Madison Square Garden on 2009, but cancelled due to gallstone surgery. Van Morrison (who also cancelled) said in an interview that he and Clapton were to do a "couple of songs", but that they would do something else together at "some other stage of the game".

Clapton performed a two-night show with Jeff Beck at London's O2 Arena on 2010. The two former Yardbirds extended their 2010 tour with stops at Madison Square Garden, the Air Canada Centre in Toronto, and the Bell Centre in Montreal. Clapton performed a series of concerts in 11 cities throughout the United States from to 2010, including Roger Daltrey as opening act. His third European tour with Steve Winwood began on and ended , including Tom Norris as opening act. He then began a short North American tour lasting from to , starting with his third Crossroads Guitar Festival on at Toyota Park in Bridgeview, Illinois. Clapton released a new studio album, "Clapton", on 2010 in the United Kingdom and 28 September 2010 in the United States. On 2010, Clapton performed as guest on the Prince's Trust rock gala held at the Royal Albert Hall, supported by the house band for the evening, which included Jools Holland, Midge Ure and Mark King.
On 24 June 2011, Clapton was in concert with Pino Daniele in Cava de' Tirreni stadium before performing a series of concerts in South America from 6 to 16 October 2011. He spent November and December 2011 touring Japan with Steve Winwood, playing 13 shows in various cities throughout the country. On 24 February 2012 Clapton, Keith Richards, Gary Clark Jr., Derek Trucks, Doyle Bramhall II, Kim Wilson and other artists performed together in the Howlin' For Hubert Tribute concert held at the Apollo Theater of New York honouring blues guitarist Hubert Sumlin who died at age 80 on 4 December 2011. On 29 November 2012, Clapton joined The Rolling Stones at London's O2 Arena during the band's second of five arena dates celebrating their 50th anniversary. On 12 December, Clapton performed at Madison Square Garden, broadcast live via television, radio, cinemas and the Internet across six continents.

In January 2013, Surfdog Records announced a signed deal with Clapton for the release of his forthcoming album "Old Sock" on 12 March. On 8 April 2013, Eric and Hard Rock International launched the limited-edition Eric Clapton Artist Spotlight merchandise programme benefiting Crossroads Centre Antigua. Clapton toured the US and Europe from 14 March to 19 June 2013 to celebrate 50 years as a professional musician. On 28 February 2013, Clapton announced his intention to stop touring in 2015 due to hassles with travel.

On 15 October 2013, Clapton's popular "Unplugged" album and concert DVD were re-released, titled "Unplugged: Expanded & Remastered." The album includes the original 14 tracks, remastered, as well as 6 additional tracks, including 2 versions of "My Father's Eyes". The DVD includes a restored version of the concert, as well as over 60 minutes of unseen footage from the rehearsal. On 13 and 14 November 2013, Clapton headlined the final two evenings of the "Baloise Sessions", an annual indoor music festival in Basel, Switzerland. On 20 November 2013, Warner Bros released Crossroads Guitar Festival 2013 in CD/DVD/Blu-ray. On 30 April 2014, Clapton announced the release of "" as an homage to J. J. Cale who died on 26 July 2013. This tribute album is named after the 1972 single "Call Me the Breeze" and features 16 Cale songs performed by Clapton, Mark Knopfler, John Mayer, Willie Nelson, Tom Petty and others.

On 21 June 2014, Clapton abruptly walked off stage during a concert at the Glasgow Hydro. Although he did return to perform one final song, thousands of fans were upset by the lack of explanation from Clapton or the venue and booed after the concert ended around 40 minutes before advertised to finish. Both Clapton and the venue apologised the next day, blaming 'technical difficulties' for making sound conditions 'unbearable' for Clapton on stage. A week later he confirmed his retirement plans attributing his decision to the road being "unbearable" in addition to "odd ailments" that may force him to put down his guitar permanently. In a 2016 interview with "Classic Rock" magazine, Clapton revealed that he had been diagnosed with peripheral neuropathy in 2013, a condition which refers to damage in one's peripheral nerves and often results in stabbing, burning or tingling pain in the arms and legs.

Clapton performed two shows at Madison Square Garden in New York on 1 and 3 May 2015 followed by a 7-night residency at London's Royal Albert Hall from 14 to 23 May 2015 to celebrate his 70th birthday on 30 March. The shows also mark 50 years since Clapton first played at the Royal Albert Hall – his debut was on 7 December 1964 when he performed as part of The Yardbirds for the BBC's "Top Beat Show". The concert film, "Slowhand at 70 – Live at the Royal Albert Hall", was released by Eagle Rock Entertainment on 13 November 2015 on DVD, CD, Blu-Ray and LP. The 2-night concerts in the US marked the 46th anniversary since Clapton, with Cream, opened the "new" Madison Square Garden on 2 November 1968. Clapton has performed more times at Madison Square Garden than any other US venue, a total of 45 times. On 20 May 2016, Clapton released his twenty-third studio album "I Still Do". On 30 September 2016 the live-album "Live in San Diego" was released.

Clapton cites Muddy Waters, Freddie King, B.B. King, Albert King, Buddy Guy, and Hubert Sumlin as guitar playing influences. Clapton stated blues musician Robert Johnson to be his single most important influence. In 2004 Clapton released CDs and DVDs entitled "Sessions for Robert Johnson", featuring covers of Robert Johnson songs using electric and acoustic guitars.

Clapton co-authored with others the book "Discovering Robert Johnson", in which Clapton said Johnson was:
Clapton also singled out Buddy Holly as an influence. "The Chirping Crickets" was the first album Clapton ever bought; he later saw Holly on "Sunday Night at the London Palladium". In his autobiography, Clapton recounted the first time he saw Holly and his Fender, saying, "I thought I'd died and gone to heaven ... it was like seeing an instrument from outer space and I said to myself: 'That's the future – that's what I want.

Clapton has been referred to as one of the most important and influential guitarists of all time. Clapton is the only three-time inductee to the Rock and Roll Hall of Fame: once as a solo artist, and separately as a member of the Yardbirds and Cream. He ranked second in "Rolling Stone" magazine's list of the "100 Greatest Guitarists of All Time" and fourth in "Gibson's" Top 50 Guitarists of All Time.

In 2011, "The Guardian" attributed the creation of the cult of the guitar hero to Clapton, ranking it number seven on their list of the 50 key events in rock music history;

In 2012, Clapton was among the British cultural icons selected by artist Sir Peter Blake to appear in a new version of his most famous artwork – the Beatles' "Sgt. Pepper's Lonely Hearts Club Band" album cover – to celebrate the British cultural figures of his life that he most admires to mark his 80th birthday.

Clapton's choice of electric guitars has been as notable as the man himself; like Hank Marvin, the Beatles and Jimi Hendrix, Clapton exerted a crucial and widespread influence in popularising particular models of electric guitar. With the Yardbirds, Clapton played a Fender Telecaster, a Fender Jazzmaster, a double-cutaway Gretsch 6120, and a 1964 Cherry-Red Gibson ES-335. He became exclusively a Gibson player for a period beginning in mid-1965, when he purchased a used sunburst Gibson Les Paul guitar from a guitar store in London. Clapton commented on the slim profile of the neck, which would indicate it was a 1960 model.

Early during his stint in Cream, Clapton's first Les Paul Standard was stolen. He continued to play Les Pauls exclusively with Cream (one bought from Andy Summers was almost identical to the stolen guitar) until 1967, when he acquired his most famous guitar in this period, a 1964 Gibson SG, dubbed "the Fool". Clapton used both the Les Paul and the SG to create his self-described "woman tone". He explained in a 1967 interview, "I am playing more smoothly now. I’m developing what I call my 'woman tone.' It's a sweet sound, something like the solo on 'I Feel Free'." Writer Michael Dregni describes it as "thick yet piercing, overdriven yet smooth, distorted yet creamy". The tone is achieved by a combination of tone control settings on the guitars and Clapton's Marshall JTM45 amplifier. "Vintage Guitar" magazine identifies "the opening riff and solo of 'Sunshine of Your Love' are arguably the best illustrations of full-blown woman tone". Clapton's "Fool" acquired its name from its distinctive psychedelic paint job, created by the visual art collective also known as the Fool (just before Cream's first US appearance in 1967, Clapton's SG, Bruce's Fender VI, and Baker's drum head were all repainted in psychedelic designs).

In 1968 Clapton bought a Gibson Firebird and started using the 1964 Cherry-Red Gibson ES-335 again. The aforementioned 1964 ES-335 had a storied career. Clapton used it at the last Cream show in November 1968 as well as with Blind Faith, played it sparingly for slide pieces in the 1970s, used it on "Hard Times" from "Journeyman", the Hyde Park live concert of 1996, and the "From the Cradle" sessions and tour of 1994–95. It was sold for US$847,500 at a 2004 auction. Gibson produced a limited run of 250 "Crossroads 335" replicas. The 335 was only the second electric guitar Clapton bought.

In July 1968 Clapton gave George Harrison a 1957 'goldtop' Gibson Les Paul that been refinished with a red colour, nicknamed Lucy. The following September, Clapton played the guitar on the Beatles' recording of "While My Guitar Gently Weeps". Lucy was stolen from Harrison, though later tracked down and returned to him - he lent it to Clapton for his 1973 comeback concert at the Rainbow. His SG found its way into the hands of George Harrison's friend Jackie Lomax, who subsequently sold it to musician Todd Rundgren for US$500 in 1972. Rundgren restored the guitar and nicknamed it "Sunny", after "Sunshine of Your Love". He retained it until 2000, when he sold it at an auction for US$150,000. At the 1969 Blind Faith concert in Hyde Park, London Clapton played a Fender Custom Telecaster, which was fitted with "Brownie"s neck.
In late 1969 Clapton made the switch to the Fender Stratocaster. "I had a lot of influences when I took up the Strat. First there was Buddy Holly, and Buddy Guy. Hank Marvin was the first well known person over here in England who was using one, but that wasn't really my kind of music. Steve Winwood had so much credibility, and when he started playing one, I thought, oh, if he can do it, I can do it". The first—used during the recording of "Eric Clapton"—was "Brownie", which in 1973 became the backup to the most famous of all Clapton's guitars, "Blackie". In November 1970 Eric bought six Fender Stratocasters from the Sho-bud guitar shop in Nashville, Tennessee while on tour with the Dominos. He gave one each to George Harrison, Steve Winwood, and Pete Townshend.

Clapton assembled the best components of the remaining three to create "Blackie", which was his favourite stage guitar until its retirement in 1985. It was first played live 1973 at the Rainbow Concert. Clapton called the 1956/57 Strat a "mongrel". On 2004, Clapton sold "Blackie" at Christie's Auction House, New York, for US$959,500 to raise funds for his Crossroads Centre for drug and alcohol addictions. "Brownie" is now on display at the Experience Music Project. The Fender Custom Shop has since produced a limited run of 275 'Blackie' replicas, correct in every detail right down to the 'Duck Brothers' flight case, and artificially aged using Fender's 'Relic' process to simulate years of hard wear. One was presented to Clapton upon the model's release and was used for three numbers during a concert at the Royal Albert Hall on 2006. In 1979 Clapton gave his signed Fender Lead II guitar to the Hard Rock Cafe in London to designate his favourite bar stool. Pete Townshend also donated his own Gibson Les Paul guitar, with a note attached: "Mine's as good as his! Love, Pete".
Signature guitars in Clapton's honour are made by Fender and C.F. Martin & Company. In 1988 Fender introduction his signature Eric Clapton Stratocaster. Several signature-model 000-sized acoustic guitars made by Martin. The first, of these, introduced in 1995, was a limited edition 000-42EC Eric Clapton signature model with a production run of 461. For the single "Change the World" (1996) and the album "Pilgrim" (1998) he used a Martin 000-28 EC Eric Clapton signature model, which he subsequently gave to guitarist Paul Wassif. His 1939 000-42 Martin that he played on the "Unplugged" album sold for US$791,500 at auction.

In 1999, Clapton auctioned off some of his guitar collection to raise more than US$5 million for continuing support of the Crossroads Centre in Antigua, which he founded in 1997. In 2004 Clapton organised and participated in the Crossroads Guitar Festival to benefit the centre. A second guitar auction, including the "Cream" of Clapton's collection – as well as guitars donated by famous friends – was held on 2004. His Lowden acoustic guitar sold for US$41,825. The revenue garnered by this auction at Christie's was US$7,438,624.

In 2010 Eric Clapton announced that he would be auctioning off over 150 items at a New York auction in 2011 with proceeds to go to his Crossroads Centre drug and rehabilitation centre in Antigua. Items included Clapton's guitar from the Cream reunion tour in 2005, speaker cabinets used in the early 1970s from his days with Derek and the Dominos, and some guitars from Jeff Beck, J. J. Cale, and Joe Bonamassa. In March 2011 Clapton raised more than US$2.15 million when he auctioned off key items, including a 1984 Gibson hollow body guitar, a Gianni Versace suit from his 1990 concert at the Royal Albert Hall, and a replica of the famous Fender Stratocaster known as "Blackie", which fetched more than $30,000. All proceeds again went to Crossroads. Clapton uses Ernie Ball Slinky and Super Slinky strings, gauge .10 to.46. His guitar technician for over thirty years was Lee Dickson.

Clapton frequently appears as a guest on the albums of other musicians. He played lead guitar and synthesiser on "The Pros and Cons of Hitch Hiking", Roger Waters' debut solo album. Other media appearances include the Toots & the Maytals Grammy award-winning album "True Love", where he played guitar on the track "Pressure Drop". He played on Paul Brady's 1985 album "Back to the Centre" on the track "Deep in your Heart". He can also be heard at the beginning of Frank Zappa's album, "We're Only in It for the Money", repeating the phrase, "Are you hung up?" over and over again. In 1985, Clapton appeared on the charity concert Live Aid in Philadelphia with Phil Collins, Tim Renwick, Chris Stainton, Jamie Oldaker, Marcy Levy, Shaun Murphy, and Donald 'Duck' Dunn. In 1988 he played with Dire Straits and Elton John at the Nelson Mandela 70th Birthday Tribute at Wembley Stadium and the Prince's Trust rock gala at the Royal Albert Hall. On 1990, Dire Straits, Clapton and Elton John made a guest appearance in the Nordoff-Robbins charity show held at Knebworth in England. In 1991 Clapton was featured on Richie Sambora's album, "Stranger in This Town", in a song dedicated to him, called "Mr. Bluesman". He contributed guitar and vocals to "Runaway Train", a duet with Elton John on the latter's "The One" album the following year.

On 12 September 1996 Clapton played a party for Armani at New York City's Lexington Armory with Greg Phillinganes, Nathan East and Steve Gadd. Sheryl Crow appeared on one number, performing "Tearing Us Apart", a track from "August", which was first performed by Tina Turner during the Prince's Trust All-Star Rock show in 1986. It was Clapton's sole US appearance that year, following the open-air concert held at Hyde Park. The concert was taped and the footage was released both on VHS video cassette and later, on DVD.

Clapton was featured in the movie version of "Tommy", the first full length rock opera, written by the Who. The movie version gave Clapton a cameo appearance as the Preacher, performing Sonny Boy Williamson's song, "Eyesight to the Blind". He appeared in "Blues Brothers 2000" as one of the Louisiana Gator Boys. In addition to being in the band, he had a small speaking role. Clapton has appeared in an advertisement for the Mercedes-Benz G-Wagen. In March 2007 Clapton appeared in an advertisement for RealNetwork's Rhapsody online music service. In 2010 Clapton started appearing as a spokesman for T-Mobile, advertising their MyTouch Fender cell phone. Clapton also appeared in the 2011 BBC documentary "Reggae Got Soul: The Story of Toots and the Maytals" which was described as "The untold story of one of the most influential artists ever to come out of Jamaica."

When asked to describe God by their minister, the characters Eric Forman and Steven Hyde both drew an image of Clapton in the episode "Holy Crap!" of season two of "That '70s Show". Clapton appeared on the BBC's "Top Gear" in 2013, during Series 19 Episode 4 and was involved in testing the new Kia Cee'd. He was called upon to test the Cee'd's auxiliary input, which he tested by plugging in one of his guitars and playing several bars of his most famous hits. He was introduced by "Top Gear" host Jeremy Clarkson as a "local guitarist".

In 2017, a documentary film titled "Eric Clapton: Life in 12 Bars" was directed by Lili Fini Zanuck. Clapton wrote the film score for Zanuck's 1991 film "Rush" and the two remained friends. In an interview for BBC News, Zanuck said that Clapton only agreed to participate if she directed it:
Clapton and Pattie Boyd married in 1979, but had no children. In 1984 while recording "Behind The Sun", Clapton began a relationship with Yvonne Kelly, the manager of AIR Studios Montserrat. Although both were married to other partners at the time, they had a daughter in January 1985. She was named Ruth Kelly Clapton, but her existence was kept from the public until the media realised she was his child in 1991.

Clapton and Boyd tried unsuccessfully to have children, even trying in vitro fertilisation in 1984, but were faced instead with miscarriages. They divorced in 1988 following his affair with Italian model Lory Del Santo, who gave birth to their son, Conor, on 1986. Conor died in 1991 at the age of 4 1/2 after falling out of an open bedroom window on the 53rd floor of a Manhattan apartment building.

Clapton married Melia McEnery in a low-key church ceremony in January 2002. They have three daughters: Julie Rose (born June 13, 2001), Ella May (born January 14, 2003) and Sophie Belle (born February 1, 2005). His grandson Isaac Eric Owen Bartlett was born in June 2013 to his oldest daughter Ruth and her husband Dean Bartlett.

Clapton is a supporter of the Countryside Alliance which promotes issues relating to the British countryside. He has played in concerts to raise funds for the organisation and publicly opposed the Labour Party's ban on fox hunting with the 2004 Hunting Act. A spokesperson for Clapton said, "Eric supports the Countryside Alliance. He doesn't hunt himself, but does enjoy rural pursuits such as fishing and shooting. He supports the Alliance's pursuit to scrap the ban on the basis that he doesn't agree with the state's interference with people's private pursuits".

On 5 August 1976, Clapton provoked an uproar and lingering controversy when he spoke out against increasing immigration during a concert in Birmingham. Visibly intoxicated, Clapton voiced his support of controversial political candidate Enoch Powell, and announced on stage that Britain was in danger of becoming a "black colony". Among other things, Clapton said "Keep Britain white!" which was at the time a National Front (NF) slogan. This incident, along with some controversial remarks made around the same time by David Bowie, as well as uses of Nazi-related imagery by Sid Vicious and Siouxsie Sioux, were the main catalysts for the creation of Rock Against Racism, with a concert on 30 April 1978.

In an interview from October 1976 with "Sounds" magazine, Clapton said that he was not a political person and that his rambling remarks that night were not appropriate. However, in a 2004 interview with "Uncut", Clapton referred to Powell as "outrageously brave". He complained that the UK was "... inviting people in as cheap labour and then putting them in ghettos". In 2004, Clapton told an interviewer for "Scotland on Sunday", "There's no way I could be a racist. It would make no sense". In his 2007 autobiography, Clapton called himself "deliberately oblivious to it all" and wrote, "I had never really understood or been directly affected by racial conflict ... when I listened to music, I was disinterested in where the players came from or what colour their skin was. Interesting, then, that 10 years later, I would be labelled a racist." In a December 2007 interview with Melvyn Bragg on "The South Bank Show", Clapton reiterated his support for Enoch Powell and again denied that Powell's views were racist.

In 2009, "Surrey Life Magazine" ranked Clapton as number 17 in their list of richest Surrey residents, estimating his fortune at £120 million in assets. This was a combination of property and income which include a £9 million yacht, "Va Bene" (previously owned by Bernie Ecclestone), his back music catalogue, his touring income, and his holding company Marshbrook Ltd, which had earned him £110 million since 1989. In 2003, he purchased a 50% share of gentleman's outfitters Cordings Piccadilly. At the time, owner Noll Uloth was trying to save the shop from closure and is reported to have contacted Clapton, his "best client"; within five minutes, Clapton replied with "I can't let this happen".

Since the 1970s, Clapton considered himself a "car enthusiast" and often stated his passion for the Ferrari brand. Clapton currently owns or has owned a range of Ferraris, and when asked about his Ferrari collection in 1989, he said he liked the touring cars for road use the company produces and commented "if I had more space and if I had been wise I would have a huge collection by now and I would be a multi-multi-millionaire". In 2010, he explained that for him "Ferrari has always been the number one car" to own and drive, and that he always supported Ferrari on the road and in Formula One motor racing.

In 2012, Ferrari honoured Clapton with the one-off special project car, the Ferrari SP12 EC. In July 2013 Clapton displayed it at the Goodwood Festival of Speed in England in the Michelin Supercar Run. In 2014, Clapton explained that Ferrari is still his favourite car brand. Among the other vehicles Clapton owns or has owned during his life are a vintage Mini Cooper Radford which was a gift from George Harrison.

In 1993, Clapton was appointed a director of Clouds House, a UK treatment centre for drug and alcohol dependence, and served on their board until 1997. Clapton also served on the board of directors for The Chemical Dependency Centre from 1994 until 1999. Both charities subsequently merged to become Action on Addiction in 2007. 

In 1998, he established the Crossroads Centre in Antigua to help others overcome their addictions to drugs and alcohol and is active in its management oversight and fundraising to the present day. Clapton has organised the Crossroads Guitar Festival in 1999, 2004, 2007, 2010 and 2013 to raise funds for this centre.

Clapton has collaborated with The Prince's Trust, the leading UK youth charity which provides training, personal development, business start up support, mentoring, and advice. He has performed at the charity's rock concert numerous times since the 1980s, most recently in 2010.

In 2008, he donated a song to Aid Still Required's CD to assist with the restoration of the devastation done to Southeast Asia from the 2004 tsunami.

Clapton is a fan of English Premier League football club West Bromwich Albion. In 1982 he performed a concert before West Brom player John Wile's testimonial game at The Hawthorns. It has been reported that the club rejected his offer to invest cash in the club around this time. In the late 1970s Clapton positioned a West Brom scarf on the back cover of his album, "Backless". In the 1978-79 season Clapton sponsored West Brom's UEFA Cup home game against Turkish club Galatasaray.

Clapton's music has appeared in dozens of movies and television shows as far back as 1973's "Mean Streets" which featured the song "I Looked Away". Other appearances in media include in the "Miami Vice" series ("Wonderful Tonight", "Knock on Wood", "She's Waiting", and "Layla"), "Back to the Future" ("Heaven Is One Step Away"), "The Color of Money" ("It's In The Way That You Use It"), "Lethal Weapon 2" ("Knockin' On Heaven's Door"), "Goodfellas" ("Layla" and "Sunshine of Your Love"), "Freaks and Geeks" episode "I'm With the Band" ("Sunshine of Your Love", "White Room" and "Crossroads (song)"), "Friends" episodes "The One with the Proposal, Part 2" ("Wonderful Tonight") and "The One Where Rachel Has A Baby" ("River of Tears"), "School Of Rock" ("Sunshine Of Your Love)", "Men in Black III" ("Strange Brew"), "Captain Phillips (film)" ("Wonderful Tonight"), "" ("Lay Down Sally") for which it was a significant part of the soundtrack, being played in the intro and twice more later on, "Good Girls Revolt" episode "The Year-Ender" ("White Room)" and "Rick and Morty" episode "Summer's Future" ("It's In The Way That You Use It").

Both Opel and Vauxhall used the guitar riff from "Layla" in their advertising campaigns throughout 1987–95. In addition to his music appearing in media, Clapton has contributed to several movies by writing or co-writing the musical scores or contributing original songs. These movies include "Lethal Weapon" (co-written with Michael Kamen), "Communion", "Rush", "Phenomenon" ("Change the World"), and "Lethal Weapon 3" (co-wrote and co-performed "It's Probably Me" with Sting and "Runaway Train" with Elton John).







</doc>
<doc id="10053" url="https://en.wikipedia.org/wiki?curid=10053" title="E2">
E2

E2, e2, E02, E.II, e² or E-2 may refer to:











</doc>
<doc id="10055" url="https://en.wikipedia.org/wiki?curid=10055" title="Etiology">
Etiology

Etiology (; alternatively aetiology or ætiology) is the study of causation, or origination. The word is derived from the Greek , "aitiología", "giving a reason for" (, "aitía", "cause"; and , "-logía"). More completely, etiology is the study of the causes, origins, or reasons behind the way that things are, or the way they function, or it can refer to the causes themselves. The word is commonly used in medicine, (where it is a branch of medicine studying causes of diease) and in philosophy, but also in physics, psychology, government, geography, spatial analysis, theology, and biology, in reference to the causes or origins of various phenomena.

In the past, when many physical phenomena were not well understood, and/or when histories were not recorded, myths often arose to provide etiologies. Thus, an etiological myth, or origin myth is a myth that has arisen, been told over time and/or written, to explain the origins of various social or natural phenomena. For example, Virgil's Aenead, is a national myth, written to explain and glorify the origins of the Roman Empire. In theology, many religions have creation myths, explaining the origins of the world, and/or its relationship to believers.

In medicine, the etiology of an illness or condition refers to the frequent studies to determine one or more factors that come together to cause the illness. Relatedly, when disease is widespread, epidemiological studies investigate what associated factors, such as location, sex, exposure to chemicals, and many others, make a population more or less likely to suffer from an illness, condition or disease, thus helping determine its etiology. Sometimes determining etiology is an imprecise process. In the past, the etiology of a common sailor's disease, scurvy was long unknown. When large, ocean-going ships were built, sailors began to put to sea for long periods of time, and often lacked fresh fruit and vegetables. Without knowing the precise cause, Captain James Cook suspected scurvy was caused by the lack of vegetables in the diet. Based on his suspicion, he forced his crew to eat sauerkraut, a cabbage preparation, every day, and based upon the positive outcomes, he inferred that it prevented scurvy, even though he did not know precisely why. It took about another two hundred years to discover the precise etiology: the lack of vitamin C in a sailor's diet.

An etiological myth, or origin myth, is a myth intended to explain the origins of cult practices, natural phenomena, proper names and the like. For example, the name Delphi and its associated deity, "Apollon Delphinios", are explained in the Homeric Hymn which tells of how Apollo, in the shape of a dolphin ('), propelled Cretans over the seas to make them his priests. While Delphi is actually related to the word ' ("womb"), many etiological myths are similarly based on folk etymology (the term "Amazon", for example). In the "Aeneid" (published circa 17 BC), Virgil claims the descent of Augustus Caesar's Julian clan from the hero Aeneas through his son Ascanius, also called Iulus. The story of Prometheus' sacrifice trick at Mecone in Hesiod's "Theogony" relates how Prometheus tricked Zeus into choosing the bones and fat of the first sacrificial animal rather than the meat to justify why, after a sacrifice, the Greeks offered the bones wrapped in fat to the gods while keeping the meat for themselves. In "Ovid"'s "Pyramus and Thisbe", the origin of the color of mulberries is explained, as the white berries become stained red from the blood gushing forth from their double suicide.



</doc>
<doc id="10065" url="https://en.wikipedia.org/wiki?curid=10065" title="Empirical formula">
Empirical formula

In chemistry, the empirical formula of a chemical compound is the simplest positive integer ratio of atoms present in a compound. A simple example of this concept is that the empirical formula of sulfur monoxide, or SO, would simply be SO, as is the empirical formula of disulfur dioxide, SO. This means that sulfur monoxide and disulfur dioxide, both compounds of sulfur and oxygen, will have the same empirical formula. However, their chemical formulas, which express the number of atoms in each molecule of a chemical compound, may not be the same. 

An empirical formula makes no mention of the arrangement or number of atoms. It is standard for many ionic compounds, like calcium chloride (CaCl), and for macromolecules, such as silicon dioxide (SiO).

The molecular formula, on the other hand, shows the number of each type of atom in a molecule. The structural formula shows the arrangement of the molecule. It is also possible for different types of compounds to have equal empirical formulas.

Samples are analyzed in specific elemental analysis tests to determine what percent of a particular element the sample is composed of.


A chemical analysis of a sample of methyl acetate provides the following elemental data: 48.64% carbon (C), 8.16% hydrogen (H), and 43.20% oxygen (O). For the purposes of determining empirical formulas assume that we have 100 grams of the compound. If this is the case, the percentages will be equal to the mass of each element in grams.

Thus, the empirical formula of methyl acetate is . This formula also happens to be methyl acetate's molecular formula.


</doc>
<doc id="10067" url="https://en.wikipedia.org/wiki?curid=10067" title="Episcopal polity">
Episcopal polity

An episcopal polity is a hierarchical form of church governance ("ecclesiastical polity") in which the chief local authorities are called bishops. (The word "bishop" derives, via the British Latin and Vulgar Latin term "*ebiscopus"/"*biscopus", from the Ancient Greek "epískopos" meaning "overseer".) It is the structure used by many of the major Christian Churches and denominations, such as the Roman Catholic, Eastern Orthodox, Oriental Orthodox, Church of the East, Anglican, and Lutheran churches or denominations, and other churches founded independently from these lineages.

Churches with an episcopal polity are governed by bishops, practicing their authorities in the dioceses and conferences or synods. Their leadership is both sacramental and constitutional; as well as performing ordinations, confirmations, and consecrations, the bishop supervises the clergy within a local jurisdiction and is the representative both to secular structures and within the hierarchy of the church. Bishops are considered to derive their authority from an unbroken, personal apostolic succession from the Twelve Apostles of Jesus. Bishops with such authority are said to represent the historical episcopate or historic episcopate. Churches with this type of government usually believe that the Church requires episcopal government as described in the New Testament (see 1 Timothy 3 and 2 Timothy 1). In some systems, bishops may be subject to bishops holding a higher office (variously called archbishops, metropolitans, or patriarchs, depending upon the tradition). They also meet in councils or synods. These gatherings, subject to presidency by higher ranking bishops, usually make important decisions, though the synod or council may also be purely advisory.

For much of the written history of institutional Christianity, episcopal government was the only known form of church organization. This changed at the Reformation. Many Protestant churches are now organized by either congregational or presbyterian church polities, both descended from the writings of John Calvin, a Protestant reformer working and writing independently following the break with the Roman Catholic Church precipitated by The Ninety-Five Theses of Martin Luther.

The definition of the word "episcopal" has variation among Christian traditions. There are subtle differences in governmental principles among episcopal churches at the present time. To some extent the separation of episcopal churches can be traced to these differences in ecclesiology, that is, their theological understanding of church and church governance. For some, "episcopal churches" are churches that use a hierarchy of bishops that regard themselves as being in an unbroken, personal apostolic succession.

"Episcopal" is also commonly used to distinguish between the various organizational structures of denominations. For instance, "Presbyterian" (, presbítes) is used to describe a church governed by a hierarchy of assemblies of elected elders, referred to as Presbyterian polity. Similarly, "episcopal" is used to describe a church governed by bishops. Self-governed local congregations, governed neither by elders nor bishops, are usually described as "congregational".

More specifically, the capitalized appellation "Episcopal" is applied to several churches historically based within Anglicanism ("Episcopalianism"), including those still in communion with the Church of England.

Using these definitions, examples of specific episcopal churches include:

Some Lutheran churches practice congregational polity or a form of presbyterian polity. Others, including the Church of Sweden, practice episcopal polity; the Church of Sweden also counts its bishops among the historic episcopate as do some American Lutheran churches like the Anglo-Lutheran Catholic Church, Lutheran Orthodox Church, Lutheran Church - International, and the Lutheran Episcopal Communion.

Many Methodist churches (see The United Methodist Church, among others) retain the form and function of episcopal polity, although in a modified form, called connexionalism. Since all trace their ordinations to an Anglican priest, John Wesley, it is generally considered that their bishops do not share in apostolic succession, though United Methodists still affirm that their bishops share in the historic episcopate.

All orthodox Christians were in churches with an episcopal government, that is, one Church under local bishops and regional Patriarchs. Writing between ca. 85 and 110, St. Ignatius of Antioch, Patriarch of Antioch, was the earliest of the Church fathers to define the importance of episcopal government. Assuming Ignatius' view was the Apostolic teaching and practice, the line of succession was unbroken and passed through the four ancient Patriarchal sees (those local churches known to be founded by apostles), Rome, Jerusalem, Antioch and Alexandria. Rome was the leading Patriarchate of the ancient four by virtue of its founding by Saints Peter and Paul and their martyrdom there, not to mention being the political center of the Roman empire at the time. Some organizations (e.g. the Assyrian Church of the East), though aloof from the political wranglings of imperial Christianity, nevertheless also practiced episcopal polity.

Shortly after the Roman Emperor Constantine I legalized Christianity in 321, he also constructed an elaborate second capital of the Roman Empire located at Byzantium and renamed it Constantinople, in 324. The single Roman Empire was divided between these two autonomous administrative centers, Roman and Constantinopolitan, West and East, Latin speaking and Greek speaking. This remained the status quo through the fourth century. A deep chasm developed between the East and West, becoming critical around 350, known as the Arian, or Nicene controversy. The Eastern Christian Churches were thought by Constantine to believe against the Trinity; that Christ was lesser than God. Hilary, Bishop of Poitiers, France, believed that the Eastern Church should be given the opportunity to, at least, be educated on the subject. Constantine, in his wisdom, and upset by disagreement, banished Hilary to the East. Hilary perfected his Greek language skills while in exile, and determined the great divide between Rome and the East was actually not a disagreement at all, and was merely a linguistic ignorance on the part of his Latin speaking contemporaries. This truth became known in the West, though some differences lingered. Hilary of Poitiers later became St. Hilary, Doctor of the Church, for exposing the true Christian beliefs of the Eastern Church. Many of Hilary's writings were lost to time.

In the fifth century, Pope Dioscorus, the Patriarch of Alexandria, rejected certain Christological dogmas promulgated by the Council of Chalcedon, and as a result, the Oriental Orthodox churches split from the rest; however they continued the episcopal tradition, and today in fact there is dialog between the various orthodox churches over whether the schism was due to real differences or simply translation failures. 

Also during the fifth century, the Western Roman Empire declined and was overrun by German and Frankish peoples. Although the city of Rome was in ruins, distant from the seat of secular power, and constantly harassed by invaders, the Roman Patriarchate remained the center of the Western or Latin Church. Claiming the ancient primacy of Peter and the title of "Apostolic See", it remained the last court of episcopal appeal in serious matters for the whole Church, East and West. However, the center of the civilized Roman world had shifted definitively to Constantinople, or New Rome, the capital of the Greek speaking Empire. Along with this shift, the effective administration of the Church in the Eastern Roman Empire also shifted. This practical eminence of Constantinople in the East is evident, first at the First Council of Constantinople 381, and then ecumenically at the Council of Chalcedon in 451.

Beginning with John the Faster (John IV, 582-595), the Bishop of Constantinople adopted as a formal title for himself the by then customary honorific Ecumenical Patriarch ("pre-eminent father for the civilized world") over the strong objections of Rome, a title based on the political prestige of Constantinople and its economic and cultural centrality in the Empire. In the following years, Rome's appeals to the East were based on the unique authority of the Apostolic See and the primacy of Peter, over the powers of councils as defended by the East (councils, for example, had endorsed that lofty title which Rome contested).

The sometimes subtle differences between Eastern and Western conceptions of authority and its exercise produced a gradually widening rift between the Churches which continued with some occasional relief throughout the following centuries until the final rupture of the Great Schism (marked by two dates: 16 July 1054 and the Council of Florence in 1439).

The Roman Catholic Church has an episcopate, with the Pope, who is the Bishop of Rome, at the top. The Roman Catholic Church considers that juridical oversight over the Church is not a power that derives from human beings, but strictly from the authority of Christ, which was given to his twelve apostles. The See of Rome, as the unbroken line of apostolic authority descending from St. Peter (the "prince and head of the apostles"), is a visible sign and instrument of communion among the college of bishops and therefore also of the local churches around the world. In communion with the worldwide college of bishops, the Pope has all legitimate juridical and teaching authority over the whole Church. This authority given by Christ to St. Peter and the apostles is transmitted from one generation to the next by the power of the Holy Spirit, through the laying on of hands from the Apostles to the bishops, in unbroken succession.

The conciliar idea of episcopal government continues in the Orthodox Church. In Eastern Orthodoxy, all autocephalous primates are seen as collectively gathering around Christ, with other archbishops and bishops gathering around them, and so forth, in a model called "conciliar hierarchy". This is based in part on the vision in the book of Revelation of the 24 elders gathered around the throne of Christ, who are believed to represent the 12 patriarchs of Israel and the 12 apostles of Jesus Christ. There is no single Patriarch with exclusive authority comparable to the Pope in Rome. However, the Patriarch of Constantinople (now Istanbul) is seen as the "primus inter pares", the "first among equals" of the autocephalous churches of Eastern Orthodoxy.

The Oriental Orthodox Churches affirm the ideas of apostolic succession and episcopal government. Within each national Church, the bishops form a holy synod to which even the Patriarch is subject. The Syriac Orthodox Church traces its apostolic succession to St. Peter and recognises Antioch as the original See of St. Peter. The Armenian Apostolic Church traces its lineage to the Apostle Bartholomew. The Indian Orthodox Church traces its lineage to the Apostle Thomas. The Ethiopian Orthodox Church received its lines of succession through the Coptic Orthodox Church in the fifth century.

Both the Greek and Coptic Orthodox Churches each recognise their own Pope of Alexandria (Pope and Patriarch of Alexandria and All Africa, and Pope of the Coptic Orthodox Church of Alexandria respectively), both of whom trace their apostolic succession back to the Mark the Evangelist. There are official ongoing efforts in recent times to heal this ancient breach. Already, the two recognize each other's baptisms, chrismations, and marriages, making intermarriage much easier.

Historically, the Church of the East has traced its episcopal succession to St. Thomas the Apostle. Currently the bishops of the Assyrian Church of the East continue to maintain its apostolic succession.

Anglicanism is the most prominent of the Reformation traditions to lay claim to the historic episcopate through apostolic succession in terms comparable to the various Roman Catholic and Orthodox Communions. Anglicans assert unbroken episcopal succession in and through the Church of England back to St. Augustine of Canterbury and to the first century Roman province of Britannia. While some Celtic Christian practices were changed at the Synod of Whitby, the church in the British Isles was under papal authority from earliest times.

The legislation of Henry VIII effectively establishing the independence from Rome of the Church of England, did not alter its constitutional or pastoral structures. Royal supremacy was exercised through the extant legal structures of the church, whose leaders were bishops. Episcopacy was thus seen as a given of the Reformed "Ecclesia Anglicana", and a foundation in the institution's appeal to ancient and apostolic legitimacy. What did change was that bishops were now seen to be ministers of the Crown for the spiritual government of its subjects. The influence of Richard Hooker was crucial to an evolution in this understanding in which bishops came to be seen in their more traditional role as ones who delegate to the presbyterate inherited powers, act as pastors to presbyters, and holding a particular teaching office with respect to the wider church.
Anglican opinion has differed as to the way in which episcopal government is "de jure divino" (by the Divine Right of Kings). On the one hand, the seventeenth century divine, John Cosin, held that episcopal authority is "jure divino", but that it stemmed from "apostolic practice and the customs of the Church...[not] absolute precept that either Christ or His Apostles gave about it" (a view maintained also by Hooker). In contrast, Lancelot Andrewes and others held that episcopal government is derived from Christ via the apostles. Regardless, both parties viewed the episcopacy as bearing the apostolic function of oversight, which both includes, and derives from the power of ordination, and is normative for the governance of the church. The practice of apostolic succession both ensures the legitimacy of the church's mission and establishes the unity, communion, and continuity of the local church with the universal church. This formulation, in turn, laid the groundwork for an independent view of the church as a "sacred society" distinct from civil society, which was so crucial for the development of local churches as non-established entities outside England, and gave direct rise to the Catholic Revival and disestablishmentarianism within England.

Functionally, Anglican episcopal authority is expressed synodically, although individual provinces may accord their primate with more or less authority to act independently. Called variously "synods," "councils," or "conventions," they meet under episcopal chairmanship. In many jurisdictions, conciliar resolutions that have been passed require episcopal assent or consent to take force. Seen in this way, Anglicans often speak of "the bishop-in-synod" as the force and authority of episcopal governance. Such conciliar authority extends to the standard areas of doctrine, discipline, and worship, but in these regards is limited by Anglicanism's tradition of the limits of authority. Those limits are expressed in Article XXI of the Thirty-Nine Articles of Religion, ratified in 1571 (significantly, just as the Council of Trent was drawing to a close), which held that "General Councils...may err, and sometimes have erred...wherefore things ordained by them as necessary to salvation have neither strength nor authority, unless it may be declared that they be taken out of holy Scripture." Hence, Anglican jurisdictions have traditionally been conservative in their approach to either innovative doctrinal development or in encompassing actions of the church as doctrinal (see lex orandi, lex credendi).

Anglican synodical government, though varied in expression, is characteristically representative. Provinces of the Anglican Communion, their ecclesiastical provinces and dioceses are governed by councils consisting not only of bishops, but also representatives of the presbyterate and laity. The spread of increasingly democratic forms of representative governance has its origin in the formation of the first General Conventions of the American Episcopal Church in the 1780s, which established a "House of Bishops" and a "House of Deputies". In many jurisdictions, there is also a third, clerical House. Resolutions may be voted on jointly or by each House, in the latter case requiring passage in all Houses to be adopted by the particular council.

There is no international juridical authority in Anglicanism, although the tradition's common experience of episcopacy, symbolised by the historical link with the See of Canterbury, along with a common and complex liturgical tradition, has provided a measure of unity. This has been reinforced by the Lambeth Conferences of Anglican Communion bishops, which first met in 1867. These conferences, though they propose and pass resolutions, are strictly consultative, and the intent of the resolutions are to provide guideposts for Anglican jurisdictions—not direction. The Conferences also express the function of the episcopate to demonstrate the ecumenical and Catholic nature of the church.

The Scottish Episcopal Church traces its history back to the origins of Christianity in Scotland, and during the 16th century Scottish Reformation became a distinct church from the presbyterian Church of Scotland which rejected episcopal government. The Scottish Episcopal Church was formally incorporated in 1712, and it more recently became part of the Anglican Communion.

Churches that are members of the Anglican Communion are episcopal churches in polity, and some are named "Episcopal". However, some churches that self-identify as Anglican do not belong to the Anglican Communion, and not all episcopally-governed churches are Anglican. The Roman Catholic Church, the Old Catholic Churches (in full communion with, but not members of, the Anglican Communion), and the Eastern Orthodox churches are recognized, and also their bishops, by Anglicans.

As an offshoot of Anglicanism, Methodist churches often use episcopal polity for historical as well as practical reasons, albeit to limited use. Methodists often use the term "connexionalism" or "connexional polity" in addition to "episcopal". Nevertheless, the powers of the Methodist episcopacy can be relatively strong and wide-reaching compared to traditional conceptions of episcopal polity. For example, in the United Methodist Church, bishops are elected for life, can serve up to two terms in a specific conference (three if special permission is given), are responsible for ordaining and appointing clergy to pastor churches, perform many administrative duties, preside at the annual sessions of the regional Conferences and at the quadrennial meeting of the worldwide General Conference, have authority for teaching and leading the church on matters of social and doctrinal import, and serve to represent the denomination in ecumenical gatherings. United Methodist bishops in the United States serve in their appointed conferences, being moved to a new "Episcopal Area" after 8 (or 12) years, until their mandated retirement at the end of the quadrenium following their sixty-sixth birthday.
British Methodism holds that all ordained ministers are equal in terms of spirituality. However, for practical management lines are drawn into President of Conference, Chair of District, Superintendent Minister, Minister. However, all are ministers.

The Reformed Church of Hungary, and the Lutheran churches in continental Europe may sometimes be called "episcopal". In these latter cases, the form of government is not radically different from the presbyterian form, except that their councils of bishops have hierarchical jurisdiction over the local ruling bodies to a greater extent than in most Presbyterian and other Reformed churches. As mentioned, the Lutheran Church in Sweden and Finland are exceptions, claiming apostolic succession in a pattern somewhat like the Anglican churches. Otherwise, forms of polity are not mandated in the Lutheran churches, as it is not regarded as having doctrinal significance. Old World Lutheranism, for historical reasons, has tended to adopt Erastian theories of episcopal authority (by which church authority is to a limited extent sanctioned by secular government). In the United States, the Lutheran churches tend to adopt a form of government more comparable to congregationalism. A small minority of Episcopal Baptists exists. 

Most Anabaptist churches of the plain dress tradition follow an episcopal system, at least in name. Congregational governance is strongly emphasized, and each congregation elects its pastor. Bishops enforce inter-congregational unity and may discipline pastors for breaking from traditional norms.

Although it never uses the term, The Church of Jesus Christ of Latter-day Saints (LDS Church) is episcopal, rather than presbyterian or congregational, in the sense that it has a strict hierarchy of leadership from the local bishop/branch president up to a single prophet/president, believed to be personally authorized and guided by Jesus Christ. Local congregations (branches, wards, and stakes) have "de jure" boundaries by which members are allocated, and membership records are centralized. This system developed gradually from a more presbyterian polity (Joseph Smith's original title in 1830 was "First Elder") for pragmatic and doctrinal reasons, reaching a full episcopacy during the Nauvoo period (1839–1846).





</doc>
<doc id="10068" url="https://en.wikipedia.org/wiki?curid=10068" title="Episcopal">
Episcopal

Episcopal may refer to:



</doc>
<doc id="10070" url="https://en.wikipedia.org/wiki?curid=10070" title="East Slavic languages">
East Slavic languages

The East Slavic languages constitute one of three regional subgroups of Slavic languages, currently spoken in Eastern Europe. It is the group with the largest numbers of speakers, far out-numbering the Western and Southern Slavic groups. The existing East Slavic languages are Belarusian, Russian and Ukrainian; Rusyn is considered to be either a separate language or a dialect of Ukrainian.

The East Slavic languages descend from a common predecessor, the language of the medieval Kievan Rus' (9th to 13th centuries).
All these languages use the Cyrillic script, but with particular modifications.

The East Slavic territory shows a definite linguistic continuum with many transitional dialects. Between Belarusian and Ukrainian there is the Polesian dialect, which shares features from both languages. East Polesian is a transitional variety between Belarusian and Ukrainian on the one hand, and between South Russian and Ukrainian on the other hand. At the same time, Belarusian and Southern Russian form a continuous area, making it virtually impossible to draw a line between the two languages. Central or Middle Russian (with its Moscow sub-dialect), the transitional step between the North and the South, became a base for the Russian literary standard. Northern Russian with its predecessor, the Old Novgorod dialect, has many original and archaic features. Due to being under the influence of the Polish–Lithuanian Commonwealth for many centuries, Belarusian and Ukrainian have adopted several influences from Polish, a West Slavic language as a result. Ruthenian, the mixed Belarusian-Ukrainian literary language with Church Slavonic substratum and Polish adstratum, was together, with Middle Polish an official language in Belarus and Ukraine until the end of the 18th century.

When the common Old East Slavic language became separated from the ancient Slavic tongue common to all Slavs is difficult to ascertain, though in the 12th century the common language of Rus' is still referred to in contemporary writing as Slavic.

Therefore, a crucial differentiation has to be made between the history of the East Slavic "dialects" and that of the "literary languages" employed by the Eastern Slavs. Although most ancient texts betray the dialect their author or scribe spoke, it is also clearly visible that they tried to write in a language different from their dialects and to avoid those mistakes that enable us nowadays to locate them.

In both cases one has to keep in mind that the history of the East Slavic languages is of course a history of written texts. We do not know how the writers of the preserved texts would have spoken in everyday life.

After the conversion of the East Slavic region to Christianity the people used service books borrowed from Bulgaria, which were written in Old Church Slavonic. The Church Slavonic language was strictly used only in text, while the colloquial language of the Bulgarians was communicated in its spoken form.

Throughout the Middle Ages (and in some way up to the present day) there existed a duality between the Church Slavonic language used as some kind of 'higher' register (not only) in religious texts and the popular tongue used as a 'lower' register for secular texts. It has been suggested to describe this situation as diglossia, although there do exist mixed texts where it is sometimes very hard to determine why a given author used a popular or a Church Slavonic form in a given context. Church Slavonic was a major factor in the evolution of modern Russian, where there still exists a "high stratum" of words that were imported from this language.

All of these languages are today separate in their own right. In the Russian Empire the official view was that the Belarusian ("White Russian"), Ukrainian ("Little Russian"), and Russian ("Great Russian") languages were dialects of one common "Russian" language (the common languages of Eastern Slavic countries). In the course of the 20th century, "Great Russian" came to be known as Russian proper, "Little Russian" as Ukrainian and "White Russian" as Belarusian.



</doc>
<doc id="10071" url="https://en.wikipedia.org/wiki?curid=10071" title="Elizabeth Gracen">
Elizabeth Gracen

Elizabeth Ward Gracen (born Elizabeth Grace Ward, April 3, 1961) is an American actress who won the title of Miss America in 1982.

Elizabeth Grace Ward was born on April 3, 1961, in Ozark, Arkansas, the daughter of Patricia Hampe, a nurse, and Jimmy Young Ward, a poultry worker. She was raised in Booneville, Arkansas. The family later moved to Russellville, Arkansas, where Ward dated University of Arkansas trainer Mike Walker and graduated from Russellville High School in 1979. She was a junior accounting major at Arkansas Tech University at the time she entered the Miss America contest. Instead of returning to Arkansas Tech, she used her Miss America scholarship money to study acting at HB Studios in New York City.

Gracen won the titles of Miss Arkansas in 1981 and Miss America in 1982. After her yearlong work as Miss America, she enrolled in acting classes then relocated to California to pursue a film and television career. 

Gracen posed nude for "Playboy" magazine's May 1992 issue.

Gracen made her professional feature film debut in "Three For The Road" with Charlie Sheen. Her film credits also include a featured role in "Marked for Death", opposite Steven Seagal, "Pass The Ammo" with Tim Curry, and the CBS feature "83 Hours Till Dawn" with Peter Strauss and Robert Urich. Gracen starred in "Lower Level" and "Discretion Assured" with Michael York.

On television, Gracen has appeared in Shelley Duvall's "Strange Case of Dr. Jekyll and Mr. Hyde", Sidney Sheldon's "The Sands of Time" and "Death of the Incredible Hulk". She also appeared with a starring role in the series "Extreme" for NBC and the syndicated series "Renegade" and "Queen of Swords".

Gracen's best-known acting role has been as the recurring character, the Immortal Amanda, in the series "" and its spin-off series "". 
In December 1999, Gracen filed for bankruptcy protection. Afterward, Gracen was given a few television guest roles, and a supporting role in the made-for-television movie "Interceptor Force 2", before taking a long leave of absence from her acting career in 2002. Gracen began doing voiceover work for Blue Hours Productions, which has revived the classic radio anthology "Suspense", which airs on Sirius XM. In 2012, Gracen did a character voice-over in the Malaysian animated science fiction film "".

In 2012, Gracen formed Flapper Films. In 2014, she starred in "Coherence", a sci-fi indie thriller. In January 2016, Gracen established Flapper Press and self-published "Shalilly", a young adult fantasy novel.

Gracen made her directorial debut with a documentary short, "The Damn Deal" about three young drag queens from Arkansas who compete in female impersonator beauty pageants.

Gracen married Jon Birmingham in 1982, but they divorced in 1984.

In 1989, while filming "", she met actor Brendan Hughes, and they married soon after. They divorced in 1994. 

Gracen married Adam Murphy, and they have a daughter.

According to Gracen, some time in 1983, she had a one-night stand with future President Bill Clinton when he was Governor of Arkansas. She was married at the time as was he.

In 1992, rumors swirled that Gracen had conducted an affair with Bill Clinton. At first, Gracen dismissed this claim (as request by Clinton's campaign manager Mickey Kantor); however, in spring 1998 Gracen recanted her six-year-old denial and stated she had a one-night stand with Clinton in 1982. After claiming this, Independent Counsel Kenneth Starr, who was investigating Clinton in the Paula Jones lawsuit, issued a subpoena to have her testify her claim in court.
However, Gracen eluded the subpoena and was at one point able to avoid it because "" was being filmed out of the country. Paula Jones' legal team was also unable to track down Gracen because she had made unscheduled trips to Las Vegas and the Caribbean.


 


</doc>
<doc id="10073" url="https://en.wikipedia.org/wiki?curid=10073" title="Epicurus">
Epicurus

Epicurus (; , "Epíkouros", "ally, comrade"; 341–270 BC) was an ancient Greek philosopher who founded a school of philosophy now called Epicureanism. Only a few fragments and letters of Epicurus's 300 written works remain. Much of what is known about Epicurean philosophy derives from later followers and commentators.

For Epicurus, the purpose of philosophy was to attain the happy, tranquil life, characterized by "ataraxia"—peace and freedom from fear—and "aponia"—the absence of pain—and by living a self-sufficient life surrounded by friends. He taught that the root of all human neurosis was death denial, and the tendency for human beings to assume that death will be horrific and painful, which he claimed causes unnecessary anxiety, selfish self-protective behaviors, and hypocrisy. According to Epicurus, death is the end of both the body and the soul and therefore should not be feared. He also taught that the gods neither reward nor punish humans; that the universe is infinite and eternal; and that occurrences in the natural world are ultimately the result of atoms moving and interacting in empty space.

His parents, Neocles and Chaerestrate, both Athenian-born, and his father a citizen, had emigrated to the Athenian settlement on the Aegean island of Samos about ten years before Epicurus's birth in February 341 BC. As a boy, he studied philosophy for four years under the Platonist teacher Pamphilus. At the age of eighteen, he went to Athens for his two-year term of military service. The playwright Menander served in the same age-class of the ephebes as Epicurus.

After the death of Alexander the Great, Perdiccas expelled the Athenian settlers on Samos to Colophon, on the coast of what is now Turkey. After the completion of his military service, Epicurus joined his family there. He studied under Nausiphanes, who followed the teachings of Democritus. In 311/310 BC Epicurus taught in Mytilene but caused strife and was forced to leave. He then founded a school in Lampsacus before returning to Athens in 306 BC where he remained until his death. There he founded The Garden (κῆπος), a school named for the garden he owned that served as the school's meeting place, about halfway between the locations of two other schools of philosophy, the Stoa and the Academy.

Epicurus's teachings were heavily influenced by those of earlier philosophers, particularly Democritus. Nonetheless, Epicurus differed from his predecessors on several key points of determinism and vehemently denied having been influenced by any previous philosophers, whom he denounced as "confused". Instead, he insisted that he had been "self-taught".

Epicurus never married and had no known children. He was most likely a vegetarian. He suffered from kidney stones, to which he finally succumbed in 270 BC at the age of seventy-two, and despite the prolonged pain involved, he wrote to Idomeneus:

I have written this letter to you on a happy day to me, which is also the last day of my life. For I have been attacked by a painful inability to urinate, and also dysentery, so violent that nothing can be added to the violence of my sufferings. But the cheerfulness of my mind, which comes from the recollection of all my philosophical contemplation, counterbalances all these afflictions. And I beg you to take care of the children of Metrodorus, in a manner worthy of the devotion shown by the young man to me, and to philosophy.

Three Epicurus bronze busts were recovered from the Villa of the Papyri, as well as text fragments.

Konstan from the Stanford Encyclopedia of Philosophy notes "Short citations of Epicurus' works appear in other writers (e.g., Plutarch, Sextus Empiricus, and the Greek commentators on Aristotle), often taken out of context or presented in a polemical and distorted fashion. [..] The school tended to be conservative and later thinkers embellished rather than altered Epicurus' own teachings."

In Mytilene, the capital of the island Lesbos, and then in Lampsacus Epicurus taught and gained followers. In Athens Epicurus bought a property for his school called "Garden", later the name of Epicurus school. The primary members were Hermarchus, the financier Idomeneus, Leonteus and his wife Themista, the satirist Colotes, the mathematician Polyaenus of Lampsacus, Leontion, and Metrodorus of Lampsacus, the most famous popularizer of Epicureanism. His school was the first of the ancient Greek philosophical schools to admit women as a rule rather than an exception. An inscription on the gate to The Garden is recorded by Seneca the Younger in epistle XXI of Epistulae morales ad Lucilium:

Stranger, here you will do well to tarry; here our highest good is pleasure.

Epicurus emphasised friendship as an important ingredient of happiness, and the school resembled in many ways a community of friends living together. However, he also instituted a hierarchical system of levels among his followers, and had them swear an oath on his core tenets.

According to Diskin Clay, Epicurus himself established a custom of celebrating his birthday annually with common meals, befitting his stature as "heros ktistes" ("founding hero") of the Garden. He ordained in his will annual memorial feasts for himself on the same date (10th of Gamelion month). Epicurean communities continued this tradition, referring to Epicurus as their "saviour" (soter) and celebrating him as hero. Lucretius apotheosized Epicurus as the main character of his epic poem De rerum natura. The hero cult of Epicurus may have operated as a Garden variety civic religion. However, clear evidence of an Epicurean hero cult, as well as the cult itself, seems buried by the weight of posthumous philosophical interpretation. Epicurus' cheerful demeanour, as he continued to work despite dying from a painful stone blockage of his urinary tract lasting a fortnight, according to his successor Hermarchus and reported by his biographer Diogenes Laërtius, further enhanced his status among his followers.

Epicurus is a key figure in the development of science and scientific methodology because of his insistence that nothing should be believed, except that which was tested through direct observation and logical deduction. He was a key figure in the Axial Age, the period from 800 BC to 200 BC, during which, according to Karl Jaspers, similar thinking appeared in China, India, Iran, the Near East, and Ancient Greece. His statement of the Ethic of Reciprocity as the foundation of ethics is the earliest in Ancient Greece, and he differs from the formulation of utilitarianism by Jeremy Bentham and John Stuart Mill by emphasising the minimisation of harm to oneself and others as the way to maximise happiness.

Epicurus's teachings represented a departure from the other major Greek thinkers of his period, and before, but was nevertheless founded on many of the same principles as Democritus. Like Democritus, he was an atomist, believing that the fundamental constituents of the world were indivisible little bits of matter (atoms; Greek: ἄτομος "atom os", "indivisible") flying through empty space (Greek: κενόν "kenon"). Everything that occurs is the result of the atoms colliding, rebounding, and becoming entangled with one another. His theory differs from the earlier atomism of Democritus because he admits that atoms do not always follow straight lines but their direction of motion may occasionally exhibit a "swerve" (Greek: παρέγκλισις "parenklisis"; Latin: "clinamen"). This allowed him to avoid the determinism implicit in the earlier atomism and to affirm free will.

He regularly admitted women and slaves into his school and was one of the first Greeks to break from the god-fearing and god-worshipping tradition common at the time, even while affirming that religious activities are useful as a way to contemplate the gods and to use them as an example of the pleasant life. Epicurus participated in the activities of traditional Greek religion, but taught that one should avoid holding false opinions about the gods. The gods are immortal and blessed and men who ascribe any additional qualities that are alien to immortality and blessedness are, according to Epicurus, impious. The gods do not punish the bad and reward the good as the common man believes. The opinion of the crowd is, Epicurus claims, that the gods "send great evils to the wicked and great blessings to the righteous who model themselves after the gods," whereas Epicurus believes the gods, in reality, do not concern themselves at all with human beings.

It is not the man who denies the gods worshipped by the multitude, who is impious, but he who affirms of the gods what the multitude believes about them.

Epicurus' philosophy is based on the theory that all good and bad derive from the sensations of what he defined as pleasure and pain: What is good is what is pleasurable, and what is bad is what is painful. His ideas of pleasure and pain were ultimately, for Epicurus, the basis for the moral distinction between good and evil. If pain is chosen over pleasure in some cases it is only because it leads to a greater pleasure. Although Epicurus has been commonly misunderstood to advocate the rampant pursuit of pleasure, his teachings were more about striving for an absence of pain and suffering, both physical and mental, and a state of satiation and tranquillity that was free of the fear of death and the retribution of the gods. Epicurus argued that when we do not suffer pain, we are no longer in need of pleasure, and we enter a state of "ataraxia", "tranquillity of soul" or "imperturbability".

Epicurus distinguishes between two different types of pleasure: "moving" pleasures (κατὰ κίνησιν ἡδοναί) and "static" pleasures (καταστηματικαὶ ἡδοναί). "Moving" pleasures occur when one is in the process of satisfying a desire and involve an active titillation of the senses. After one's desires have been satisfied, (e.g., when one is full after eating), the state of satiety is a "static" pleasure. For Epicurus, static pleasures are the best pleasures.

Epicurus' teachings were introduced into medical philosophy and practice by the Epicurean doctor Asclepiades of Bithynia, who was the first physician who introduced Greek medicine in Rome. Asclepiades introduced the friendly, sympathetic, pleasing and painless treatment of patients. He advocated humane treatment of mental disorders, had insane persons freed from confinement and treated them with natural therapy, such as diet and massages. His teachings are surprisingly modern, therefore Asclepiades is considered to be a pioneer physician in psychotherapy, physical therapy and molecular medicine.

Epicurus explicitly warned against overindulgence because it often leads to pain. For instance, Epicurus warned against pursuing love too ardently. He defended friendships as ramparts for pleasure and denied them any inherent worth. He also believed, contrary to Aristotle, that death was not to be feared. When a man dies, he does not feel the pain of death because he no longer is and therefore feels nothing. Therefore, as Epicurus famously said, "death is nothing to us." When we exist, death is not; and when death exists, we are not. All sensation and consciousness ends with death and therefore in death there is neither pleasure nor pain. The fear of death arises from the belief that in death, there is awareness.

From this doctrine arose the Epicurean epitaph: "Non fui, fui, non sum, non curo" ("I was not; I was; I am not; I do not care"), which is inscribed on the gravestones of his followers and seen on many ancient gravestones of the Roman Empire. This quotation is often used today at humanist funerals.

As an ethical guideline, Epicurus emphasised minimising harm and maximising happiness of oneself and others:
It is impossible to live a pleasant life without living wisely and well and justly, and it is impossible to live wisely and well and justly without living pleasantly.

The "Epicurean paradox" or "Riddle of Epicurus" is a version of the problem of evil. Lactantius attributes this trilemma to Epicurus in "De Ira Dei":
God, he says, either wishes to take away evils, and is unable; or He is able, and is unwilling; or He is neither willing nor able, or He is both willing and able. If He is willing and is unable, He is feeble, which is not in accordance with the character of God; if He is able and unwilling, He is envious, which is equally at variance with God; if He is neither willing nor able, He is both envious and feeble, and therefore not God; if He is both willing and able, which alone is suitable to God, from what source then are evils? Or why does He not remove them?
In "Dialogues concerning Natural Religion" (1779), David Hume also attributes the argument to Epicurus:
Epicurus’s old questions are yet unanswered. Is he willing to prevent evil, but not able? then is he impotent. Is he able, but not willing? then is he malevolent. Is he both able and willing? whence then is evil?
No extant writings of Epicurus contain this argument and it is possible that it has been misattributed to him.

Cicero's character Cotta, an Academician, rehearses this argument in "de Natura deorum", a dialogue with an Epicurean and a Stoic.
Either God wishes to remove evils and cannot, or he can do so and is unwilling, or he has neither the will nor the power, or he has both the will and the power. If he has the will but not the power, he is a weakling, and this is not characteristic of God. If he has the power but not the will, he is grudging, and this is a trait equally foreign to God. If he has neither the will nor the power, he is both grudging and weak, and is therefore not divine. If he has both the will and the power (and this is the sole circumstance appropriate to God), what is the source of evils, or why does God not dispel them? (3.65, 133, trans. P.G. Walsh)
Another early expression of this trilemma appears in the writings of the sceptic Sextus Empiricus (160–210 AD),who wrote in his "Outlines of Pyrrhonism":
Further, this too should be said. Anyone who asserts that god exists either says that god takes care of the things in the cosmos or that he does not, and, if he does take care, that it is either of all things or of some. Now if he takes care of everything, there would be no particular evil thing and no evil in general in the cosmos; but the Dogmatists say that everything is full of evil; therefore god shall not be said to take care of everything. On the other hand, if he takes care of only some things, why does he take care of these and not of those? For either he wishes but is not able, or he is able but does not wish, or he neither wishes nor is able. If he both wished and was able, he would have taken care of everything; but, for the reasons stated above, he does not take care of everything; therefore, it is not the case that he both wishes and is able to take care of everything. But if he wishes and is not able, he is weaker than the cause on account of which he is not able to take care of the things of which he does not take care; but it is contrary to the concept of god that he should be weaker than anything. Again, if he is able to take care of everything but does not wish to do so, he will be considered malevolent, and if he neither wishes nor is able, he is both malevolent and weak; but to say that about god is impious. Therefore, god does not take care of the things in the cosmos.
Epicurus emphasised the senses in his epistemology, and his Principle of Multiple Explanations ("if several theories are consistent with the observed data, retain them all") is an early contribution to the philosophy of science.

There are also some things for which it is not enough to state a single cause, but several, of which one, however, is the case. Just as if you were to see the lifeless corpse of a man lying far away, it would be fitting to list all the causes of death in order to make sure that the single cause of this death may be stated. For you would not be able to establish conclusively that he died by the sword or of cold or of illness or perhaps by poison, but we know that there is something of this kind that happened to him.

In contrast to the Stoics, Epicureans showed little interest in participating in the politics of the day, since doing so leads to trouble. He instead advocated seclusion. This principle is epitomised by the phrase "lathe biōsas" (), meaning "live in obscurity", "get through life without drawing attention to yourself", i.e., live without pursuing glory or wealth or power, but anonymously, enjoying little things like food, the company of friends, etc. Plutarch elaborated on this theme in his essay "Is the Saying "Live in Obscurity" Right?" (, "An recte dictum sit latenter esse vivendum") 1128c; cf. Flavius Philostratus, "Vita Apollonii" 8.28.12.

But the Epicureans did have an innovative theory of justice as a social contract. Justice, Epicurus said, is an agreement neither to harm nor be harmed, and we need to have such a contract in order to enjoy fully the benefits of living together in a well-ordered society. Laws and punishments are needed to keep misguided fools in line who would otherwise break the contract. But the wise person sees the usefulness of justice, and because of his limited desires, he has no need to engage in the conduct prohibited by the laws in any case. Laws that are useful for promoting happiness are just, but those that are not useful are not just. ("Principal Doctrines" 31–40)

The only surviving complete works by Epicurus are three letters, which are to be found in book X of Diogenes Laërtius' "Lives of Eminent Philosophers", and two groups of quotes: the "Principal Doctrines" (Κύριαι Δόξαι), reported as well in Diogenes' book X, and the "Vatican Sayings", preserved in a manuscript from the Vatican Library.

Numerous fragments of his thirty-seven volume treatise "On Nature" have been found among the charred papyrus fragments at the Villa of the Papyri at Herculaneum. In addition, other Epicurean writings found at Herculaneum contain important quotations from his other works. Moreover, numerous fragments and testimonies are found throughout ancient Greek and Roman literature, a collection of which can be found in Usener's "Epicurea".

According to Diogenes Laertius (10.27-9), the major works of Epicurus include:


Elements of Epicurean philosophy have resonated and resurfaced in various diverse thinkers and movements throughout Western intellectual history.

The atomic poems (such as 'All Things are Governed by Atoms') and the philosophy of naturalism espoused by Margaret Cavendish were influenced by Epicurus.

His emphasis on minimising harm and maximising happiness in his formulation of the Ethic of Reciprocity was later picked up by the democratic thinkers of the French Revolution, and others, like John Locke, who wrote that people had a right to "life, liberty, and property." To Locke, one's own body was part of one's property, and thus one's right to property would theoretically guarantee safety for one's person, as well as one's possessions.

This triad, as well as the egalitarianism of Epicurus, was carried forward into the American freedom movement and Declaration of Independence, by the American founding father, Thomas Jefferson, as "all men are created equal" and endowed with certain "unalienable rights," such as "life, liberty, and the pursuit of happiness." Jefferson considered himself an Epicurean.

In "An Enquiry Concerning Human Understanding", David Hume uses Epicurus as a character for explaining the impossibility of our knowing God to be any greater or better than his creation proves him to be.

Karl Marx's doctoral thesis was on "The Difference Between the Democritean and Epicurean Philosophy of Nature". 
Epicurus was first to assert human freedom as coming from a fundamental indeterminism in the motion of atoms. This has led some philosophers to think that for Epicurus free will was "caused directly by chance". In his "On the Nature of Things" ("De rerum natura"), Lucretius appears to suggest this in the best-known passage on Epicurus' position. But in his Letter to Menoeceus, Epicurus follows Aristotle and clearly identifies "three" possible causes - "some things happen of necessity, others by chance, others through our own agency." Aristotle said some things "depend on us" ("eph'hemin"). Epicurus agreed, and said it is to these last things that praise and blame naturally attach. For Epicurus, the "swerve" (or "clinamen") of the atoms simply defeated determinism to leave room for autonomous agency.

Epicurus was also a significant source of inspiration and interest for both Arthur Schopenhauer, having particular influence on the famous pessimist's views on suffering and death, as well as one of Schopenhauer's successors: Friedrich Nietzsche. Nietzsche cites his affinities to Epicurus in a number of his works, including "The Gay Science", "Beyond Good and Evil", and his private letters to Peter Gast. Nietzsche was attracted to, among other things, Epicurus' ability to maintain a cheerful philosophical outlook in the face of painful physical ailments. Nietzsche also suffered from a number of sicknesses during his lifetime. However, he thought that Epicurus' conception of happiness as freedom from anxiety was too passive and negative.

In Greece, in February of every year since 2011 a two-day Panhellenic Symposium of Epicurean Philosophy is held.

Paul the Apostle encountered Epicurean and Stoic philosophers as he was ministering in Athens.

Horace describes himself as "Epicuri de grege porcum" "a swine from Epicurus's herd" in his "Epistles".

In Canto X Circle 6 ("Where the heretics lie") of Dante's Inferno, Epicurus and his followers are criticised for supporting a materialistic ideal when they are mentioned to have been condemned to the Circle of Heresy.

Chaucer's Frankeleyn, in the General Prologue of his "Canterbury Tales," is described as an Epicurean: "Wel loved he by the morwe a sop in wyn; / To lyven in delit was evre his wone, / For he was Epicurus owene sone, / That heeld opinioun that pleyn delit / Was verray felicitee parfit" (344-38). 
[He well loved bread soaked in wine for breakfast; / To live in pleasure was ever his custom, / For he was a son of Epicurus, / Who was of the opinion that pure pleasure / Was true perfect happiness.]

"Epicurus the Sage" is a two-part comic book by William Messner-Loebs and Sam Kieth portraying Epicurus as "the only sane philosopher" by anachronistically bringing him together with many other well-known Greek philosophers. It was republished as graphic novel by the Wildstorm branch of DC Comics.

In Rabbinic literature the term "Epikoros" is used, without a specific reference to Epicurus, yet it seems apparent that the term was derived from his name.

Epicurus's apparent hedonistic views (as Epicurus' ethics was hedonistic) and philosophical teachings, though opposed to the Hedonists of his time, countered Jewish scripture, the strictly monotheistic conception of God in Judaism and the Jewish belief in the afterlife and the world to come.

The Talmudic interpretation is that the Aramaic word is derived from the root-word פק"ר (PKR; lit. "licentious"), hence disrespect.

The Christian censorship of the Jewish Talmud in the aftermath of the Disputation of Barcelona and during the Spanish Inquisition and Roman Inquisition, let the term spread within the Jewish classical texts, since Roman Catholic Church censors replaced terms like "Minim" ("sectarians", coined on the Christians) with the term "Epikorsim" or "Epicursim", meaning heretics.









</doc>
<doc id="10074" url="https://en.wikipedia.org/wiki?curid=10074" title="Epitaph">
Epitaph

An epitaph (from Greek "epitaphios" "a funeral oration" from ἐπί "epi" "at, over" and τάφος "taphos" "tomb") is a short text honoring a deceased person. Strictly speaking, it refers to text that is inscribed on a tombstone or plaque, but it may also be used in a figurative sense. Some epitaphs are specified by the person themselves before their death, while others are chosen by those responsible for the burial. An epitaph may be written in prose or in poem verse; poets have been known to compose their own epitaphs prior to their death, as did William Shakespeare.

Most epitaphs are brief records of the family, and perhaps the career, of the deceased, often with a common expression of love or respect—for example, "beloved father of ..."—but others are more ambitious. From the Renaissance to the 19th century in Western culture, epitaphs for notable people became increasingly lengthy and pompous descriptions of their family origins, career, virtues and immediate family, often in Latin. Notably, the Laudatio Turiae, the longest known Ancient Roman epitaph, exceeds almost all of these at 180 lines; it celebrates the virtues of an honored wife, probably of a consul.

Some are quotes from holy texts, or aphorisms. One approach of many epitaphs is to 'speak' to the reader and warn them about their own mortality. A wry trick of others is to request the reader to get off their resting place, inasmuch as the reader would have to be standing on the ground above the coffin to read the inscription. Some record achievements (e.g., past politicians note the years of their terms of office). Nearly all (excepting those where this is impossible by definition, such as the Tomb of the Unknown Soldier) note name, year or date of birth, and date of death. Many list family members and the relationship of the deceased to them (for example, "Father / Mother / Son / Daughter of").

"Heroes and Kings your distance keep;<br>
"In peace let one poor poet sleep,<br>
"Who never flattered folks like you;<br>
"Let Horace blush and Virgil too."

"We must know. We will know."

"Looking into the portals of eternity teaches that"<br>
"The brotherhood of man is inspired by God’s word;"<br>
"Then all prejudice of race vanishes away."

"He never killed a man that did not need killing." <br>

"Here lies One whose Name was writ in Water"<br>

"Cast a cold eye<br>On life, on death.<br>Horseman, pass by!"

"Undefeated"

"And the beat goes on."

"Sleep after toyle, port after stormie seas",<br>"Ease after warre, death after life, does greatly please."

"Oh God"

"That's all folks!"

"I've finally stopped getting dumber."

"Homo sum! the adventurer"

"Go tell the Spartans, stranger passing by"
"that here, obedient to their law, we lie."

"I told you I was ill."

"Here sleeps at peace a Hampshire Grenadier"<br>
"Who caught his early death by drinking cold small beer."<br>
"Soldiers, be wise at his untimely fall,"<br>
"And when you're hot, drink strong or none at all."

"And, oh lord"

"To save your world you asked this man to die:"
"Would this man, could he see you now, ask why?"

"There is borne an empty hearse"
"covered over for such as appear not." 
"Heroes have the whole earth for their tomb."

"Against you I will fling myself, unvanquished and unyielding, O Death!"

"Good frend for Iesvs sake forebeare,"<br>
"To digg the dvst encloased heare."<br> 
"Bleste be man spares thes stones,"<br>
"And cvrst be he moves my bones."

(In modern spelling):<br>
Good friend for Jesus' sake forbear,<br>
To dig the dust enclosed here.<br> 
Blessed be the man that spares these stones,<br>
And cursed be he that moves my bones.

In a more figurative sense, the term may be used for music composed in memory of the deceased. Igor Stravinsky composed in 1958 "Epitaphium" for flute, clarinet and harp. In 1967 Krzysztof Meyer called his Symphony No. 2 for choir and orchestra "Epitaphium Stanisław Wiechowicz in memoriam". Jeffrey Lewis composed "Epitaphium – Children of the Sun" for narrator, chamber choir, piano, flute, clarinet and percussion. Bronius Kutavičius composed in 1998 "Epitaphium temporum pereunti". Valentin Silvestrov composed in 1999 "Epitaph L.B." (Епітафія Л.Б.) for viola (or cello) and piano. In 2007 Graham Waterhouse composed "Epitaphium" for string trio as a tribute to the memory of his father William Waterhouse. The South African poet Gert Vlok Nel wrote an (originally) untitled song, which appeared on his first music album 'Beaufort-Wes se Beautiful Woorde' as 'Epitaph', because his producer Eckard Potgieter told him that the song sounded like an epitaph. David Bowie's final album, "Blackstar", released in 2016, is generally seen as his musical epitaph, with singles "Blackstar" and "Lazarus" often singled out.

In the late 1990s, a unique epitaph was flown to the moon along with the ashes of geologist and planetary scientist Eugene Shoemaker. At the suggestion of colleague Carolyn Porco, Shoemaker's ashes were launched aboard the Lunar Prospector spacecraft on January 6, 1998. The ashes were accompanied by a laser-engraved epitaph on a small piece of foil. The spacecraft, along with the ashes and epitaph, crashed on command into the south polar region of the moon on July 31, 1999.




</doc>
<doc id="10075" url="https://en.wikipedia.org/wiki?curid=10075" title="Epigram">
Epigram

An epigram is a brief, interesting, memorable, and sometimes surprising or satirical statement. The word is derived from the "epigramma" "inscription" from ἐπιγράφειν "epigraphein" "to write on, to inscribe", and the literary device has been employed for over two millennia.

The Greek tradition of epigrams began as poems inscribed on votive offerings at sanctuariesincluding statues of athletesand on funerary monuments, for example "Go tell it to the Spartans, passersby...". These original epigrams did the same job as a short prose text might have done, but in verse. Epigram became a literary genre in the Hellenistic period, probably developing out of scholarly collections of inscriptional epigrams.

Though modern epigrams are usually thought of as very short, Greek literary epigram was not always as short as later examples, and the divide between "epigram" and "elegy" is sometimes indistinct (they share a characteristic metre, elegiac couplets). In the classical period, the clear distinction between them was that epigrams were inscribed and meant to be read, while elegies were recited and meant to be heard. Some elegies could be quite short, but only public epigrams were longer than ten lines. All the same, the origin of epigram in inscription exerted a residual pressure to keep things concise, even when they were recited in Hellenistic times. Many of the characteristic types of literary epigram look back to inscriptional contexts, particularly funerary epigram, which in the Hellenistic era becomes a literary exercise. Many "sympotic" epigrams combine sympotic and funerary elementsthey tell their readers (or listeners) to drink and live for today because life is short. Generally, any theme found in classical elegies could be and were adapted for later literary epigrams.

Hellenistic epigrams are also thought of as having a "point"that is, the poem ends in a punchline or satirical twist. By no means do all Greek epigrams behave this way; many are simply descriptive, but Meleager of Gadara and Philippus of Thessalonica, the first comprehensive anthologists, preferred the short and witty epigram. Since their collections helped form knowledge of the genre in Rome and then later throughout Europe, Epigram came to be associated with 'point,' especially because the European epigram tradition takes the Latin poet Martial as its principal model; he copied and adapted Greek models (particularly the contemporary poets Lucillius and Nicarchus) selectively and in the process redefined the genre, aligning it with the indigenous Roman tradition of 'satura', hexameter satire, as practised by (among others) his contemporary Juvenal. Greek epigram was actually much more diverse, as the Milan Papyrus now indicates.

A major source for Greek literary epigram is the "Greek Anthology", a compilation from the 10th century AD based on older collections, including those of Meleager and Philippus. It contains epigrams ranging from the Hellenistic period through the Imperial period and Late Antiquity into the compiler's own Byzantine eraa thousand years of short elegiac texts on every topic under the sun. The "Anthology" includes one book of Christian epigrams as well as one book of erotic and amorous homosexual epigrams called the Μοῦσα Παιδικἠ (Mousa Paidike, "The Boyish Muse").

Roman epigrams owe much to their Greek predecessors and contemporaries. Roman epigrams, however, were often more satirical than Greek ones, and at times used obscene language for effect. Latin epigrams could be composed as inscriptions or graffiti, such as this one from Pompeii, which exists in several versions and seems from its inexact meter to have been composed by a less educated person. Its content makes it clear how popular such poems were:

However, in the literary world, epigrams were most often gifts to patrons or entertaining verse to be published, not inscriptions. Many Roman writers seem to have composed epigrams, including Domitius Marsus, whose collection "Cicuta" (now lost) was named after the poisonous plant "Cicuta" for its biting wit, and Lucan, more famous for his epic "Pharsalia". Authors whose epigrams survive include Catullus, who wrote both invectives and love epigrams – his poem 85 is one of the latter.

Martial, however, is considered to be the master of the Latin epigram. His technique relies heavily on the satirical poem with a joke in the last line, thus drawing him closer to the modern idea of epigram as a genre. Here he defines his genre against a (probably fictional) critic (in the latter half of 2.77):

Poets known for their epigrams whose work has been lost include Cornificia.

In early English literature the short couplet poem was dominated by the poetic epigram and proverb, especially in the translations of the Bible and the Greek and Roman poets.
Since 1600, two successive lines of verse that rhyme with each other, known as a couplet featured as a part of the longer sonnet form, most notably in William Shakespeare's sonnets. Sonnet 76 is an excellent example. The two line poetic form as a closed couplet was also used by William Blake in his poem Auguries of Innocence and also by Byron (Don Juan (Byron) XIII); John Gay (Fables); Alexander Pope (An Essay on Man).

The first work of English literature penned in North America was Robert Hayman's "Quodlibets, Lately Come Over from New Britaniola, Old Newfoundland," which is a collection of over 300 epigrams, many of which do not conform to the two-line rule or trend. While the collection was written between 1618 and 1628 in what is now Harbour Grace, Newfoundland, it was published shortly after his return to Britain. 
In Victorian times the epigram couplet was often used by the prolific American poet Emily Dickinson. Her poem No. 1534 is a typical example of her eleven poetic epigrams. The novelist George Eliot also included couplets throughout her writings. Her best example is in her sequenced sonnet poem entitled "Brother and Sister" in which each of the eleven sequenced sonnet ends with a couplet. In her sonnets, the preceding lead-in-line, to the couplet ending of each, could be thought of as a title for the couplet, as is shown in Sonnet VIII of the sequence.

During the early 20th century, the rhymed epigram couplet form developed into a fixed verse image form, with an integral title as the third line. Adelaide Crapsey codified the couplet form into a two line rhymed verse of ten syllables per line with her image couplet poem "On Seeing Weather-Beaten Trees" first published in 1915.

By the 1930s, the five-line cinquain verse form became widely known in the poetry of the Scottish poet William Soutar. These were originally labelled epigrams but later identified as image cinquains in the style of Adelaide Crapsey.
J. V. Cunningham was also a noted writer of epigrams, (a medium suited to a 'short-breathed' person).




</doc>
<doc id="10076" url="https://en.wikipedia.org/wiki?curid=10076" title="El Cid">
El Cid

Rodrigo Díaz de Vivar (1099) was a Castilian nobleman and military leader in medieval Spain. The Moors called him El Cid, which meant "the Lord" (probably from the original Arabic al-Sayyid, السیِّد), and the Christians, El Campeador, which stood for "Outstanding Warrior" or " The one who stands out in the battlefield". He was born in Vivar, a town near the city of Burgos. After his death, he became Castile's celebrated national hero and the protagonist of the most significant medieval Spanish epic poem, "El Cantar de Mio Cid".

Born a member of the minor nobility, El Cid was brought up at the court of King Ferdinand the Great and served Ferdinand's son, Sancho II of León and Castile. He rose to become the commander and royal standard-bearer ("armiger regis") of Castile upon Sancho's ascension in 1065. Rodrigo went on to lead the Castilian military campaigns against Sancho's brothers, Alfonso VI of León and García II of Galicia, as well as in the Muslim kingdoms in Al-Andalus. He became renowned for his military prowess in these campaigns, which helped expand Castilian territory at the expense of the Muslims and Sancho's brothers' kingdoms. When conspirators murdered Sancho in 1072, Rodrigo found himself in a difficult situation. Since Sancho was childless, the throne passed to his brother Alfonso, the same whom El Cid had helped remove from power. Although Rodrigo continued to serve the Castilian sovereign, he lost his ranking in the new court which treated him at arm's length and suspiciously. Finally, in 1081, he was ordered into exile.

El Cid found work fighting for the Muslim rulers of Zaragoza, whom he defended from their traditional enemies, Aragon and Barcelona. While in exile, he regained his reputation as a strategist and formidable military leader. He repeatedly turned out victorious in battle against the Muslim rulers of Lérida and their Christian allies, as well as against a large Christian army under King Sancho Ramírez of Aragon. In 1086, an expeditionary army of North African Almoravids inflicted a severe defeat to Castile, compelling Alfonso to overcome the resentments he harbored against El Cid. The terms for the return to the Christian service must have been attractive enough since Rodrigo soon found himself fighting for his former Lord. Over the next several years, however, El Cid set his sights on the kingdom-city of Valencia, operating more or less independently of Alfonso while politically supporting the Banu Hud and other Muslim dynasties opposed to the Almoravids. He gradually increased his control over Valencia; the Islamic ruler, al-Qadir, became his tributary in 1092. When the Almoravids instigated an uprising that resulted in the death of al-Qadir, El Cid responded by laying siege to the city. Valencia finally fell in 1094, and El Cid established an independent principality on the Mediterranean coast of Spain. He ruled over a pluralistic society with the popular support of Christians and Muslims alike.

El Cid's final years were spent fighting the Almoravid Berbers. He inflicted upon them their first major defeat in 1094, on the plains of Caurte, outside Valencia, and continued resisting them until his death. Although Rodrigo remained undefeated in Valencia, his only son, and heir, Diego Rodríguez died fighting against the Almoravids in the service of Alfonso in 1097. After El Cid's death in 1099, his wife, Jimena Díaz, succeeded him as ruler of Valencia, but she was eventually forced to surrender the principality to the Almoravids in 1102.

To this day, El Cid remains a Spanish popular folk-hero and national icon. Numerous plays, films, folktales, songs, and even video games continue to memorialize the traditions of allegiance that his allegories typify.

The name "El Cid" () is a modern Spanish denomination composed by the article "el" meaning "the" and "Cid" which comes from the Old Castilian loan word "Çid" from the dialectal Arabic word سيد "sîdi" or sayyid, which means "Lord" or "Master". The Mozarabs or the Arabs that served in his ranks may have addressed him in this way, which the Christians may have transliterated and adopted. Historians, however, have not yet found contemporary records referring to Rodrigo as "Cid." Arab sources use instead "Rudriq", "Ludriq al-Kanbiyatur" or "al-Qanbiyatur" ("Rodrigo el Campeador"). The cognomen "Campeador" derives from Latin "campi doctor," which means "battlefield master". He probably gained it during the campaigns of King Sancho II of Castile against his brothers King Alfonso VI of León and King García II of Galicia. While his contemporaries left no historical sources that would have addressed him as "Cid", they left plenty of Christian and Arab records, some even signed documents with his autograph, addressing him as "Campeador", which prove that he used the Christian cognomen himself. The whole combination "Cid Campeador" is first documented ca. 1195 in the Navarro-Aragonese "Linage de Rodric Díaz" included in the "Liber Regum" under the formula "mio Cid el Campeador".

El Cid was born Rodrigo Díaz circa AD 1043 in Vivar, also known as Castillona de Bivar, a small town about six miles north of Burgos, the capital of Castile. His father, Diego Laínez, was a courtier, bureaucrat, and cavalryman who had fought in several battles. Despite the fact that El Cid's mother's family was aristocratic, in later years the peasants would consider him one of their own. However, his relatives were not major court officials; documents show that El Cid's paternal grandfather, Lain, confirmed only five documents of Ferdinand I's; his maternal grandfather, Rodrigo Alvarez, certified only two of Sancho II's; and El Cid's father confirmed only one.

As a young man in 1057, Rodrigo fought against the Moorish stronghold of Zaragoza, making its emir al-Muqtadir a vassal of Sancho. In the spring of 1063, Rodrigo fought in the Battle of Graus, where Ferdinand's half-brother, Ramiro I of Aragon, was laying siege to the Moorish town of Cinca, which was in Zaragozan lands. Al-Muqtadir, accompanied by Castilian troops including El Cid, fought against the Aragonese. The party slew Ramiro I, setting the Aragonese army on the run, and emerged victorious. One legend has said that during the conflict, El Cid killed an Aragonese knight in single combat, thereby receiving the honorific title Campeador.

When Ferdinand died, Sancho continued to enlarge his territory, conquering both Christian strongholds and the Moorish cities of Zamora and Badajoz. When Sancho learned that Alfonso was planning on overthrowing him in order to gain his territory, Sancho sent Cid to bring Alfonso back so that Sancho could speak to him.

Sancho was assassinated in 1072, possibly as the result of a pact between his brother Alfonso and his sister Urraca. Since Sancho died unmarried and childless, all of his power passed to his brother Alfonso who, almost immediately, returned from exile in Toledo and took his seat as king of Castile and León. He was, however, deeply suspected of having been involved in Sancho's murder. According to the epic of El Cid, the Castilian nobility led by El Cid and a dozen "oath-helpers" forced Alfonso to swear publicly on holy relics multiple times in front of Santa Gadea (Saint Agatha) Church in Burgos that he did not participate in the plot to kill his brother. This is widely reported as truth, but contemporary documents on the lives of both Rodrigo Diaz and Alfonso VI of Castile and León do not mention any such event. Rodrigo's position as "armiger regis" was taken away and given to Rodrigo's enemy, Count García Ordóñez.

In 1079, Rodrigo was sent by Alfonso VI to Seville to the court of al-Mutamid to collect the "parias" owed by that "taifa" to León–Castile. While he was there Granada, assisted by other Castilian knights, attacked Seville, and Rodrigo and his forces repulsed the Christian and Grenadine attackers at the Battle of Cabra, in the (probably mistaken) belief that he was defending the king's tributary. Count García Ordóñez and the other Castilian leaders were taken captive and held for three days before being released.

In the Battle of Cabra (1079), El Cid rallied his troops and turned the battle into a rout of Emir Abdullah of Granada and his ally García Ordóñez. However, El Cid's unauthorized expedition into Granada greatly angered Alfonso, and May 8, 1080, was the last time El Cid confirmed a document in King Alfonso's court. This is the generally given reason for El Cid's exile, although several others are plausible and may have been contributing factors: jealous nobles turning Alfonso against El Cid, Alfonso's own animosity towards El Cid and an accusation of pocketing some of the tribute from Seville.

At first he went to Barcelona, where Ramon Berenguer II (1076–1082) and Berenguer Ramon II (1076–1097) refused his offer of service.

The exile was not the end of El Cid, either physically or as an important figure. After being rejected by Ramon Berenguer II, El Cid journeyed to the Taifa of Zaragoza where he received a warmer welcome. In 1081, El Cid went on to offer his services to the Moorish king of the northeast Al-Andalus city of Zaragoza, Yusuf al-Mu'taman ibn Hud, and served both him and his successor, Al-Mustain II. He was given the title "El Cid" ("The Master") and served as a leading figure in a diverse Moorish force consisting of Muladis, Berbers, Arabs and Malians.

According to Moorish accounts:
Andalusian Knights found El Cid their foe ill, thirsty and exiled from the court of Alfonso, he was presented before the elderly Yusuf al-Mu'taman ibn Hud and accepted command of the forces of the Taifa of Zaragoza as their Master.

In his "History of Medieval Spain" (Cornell University Press, 1975), Joseph F. O'Callaghan writes:
That kingdom was divided between al-Mutamin (1081–1085) who ruled Zaragoza proper, and his brother al-Mundhir, who ruled Lérida and Tortosa. El Cid entered al-Mutamin's service and successfully defended Zaragoza against the assaults of al-Mundhir, Sancho I of Aragón, and Ramon Berenguer II, whom he held captive briefly in 1082.

In 1084, The Army of the Taifa of Zaragoza under El Cid defeated the Aragonese at the Battle of Morella near Tortosa, but in autumn the Castilians started a loose siege of Toledo and later the next year the Christians captured Salamanca, a stronghold of the Taifa of Toledo.

In 1086, the Almoravid invasion of the Iberian Peninsula through and around Gibraltar began. The Almoravids, Berber residents of present-day North Africa, led by Yusuf ibn Tashfin, were asked to help defend the divided Moors from Alfonso. El Cid commanded a large Moorish force during the Battle of Sagrajas, which took place in 1086, near the Taifa of Badajoz. The Almoravid and Andalusian Taifas, including the armies of Badajoz, Málaga, Granada, Tortosa and Seville, defeated a combined army of León, Aragón and Castile.

In 1087, Raymond of Burgundy and his Christian allies attempted to weaken the Taifa of Zaragoza's northernmost stronghold by initiating the Siege of Tudela and Alfonso captured Aledo, Murcia blocking the route between the Taifas in eastern and western Iberia.

Terrified after his crushing defeat, Alfonso recalled El Cid. It has been shown that El Cid was at court on July 1087; however, what happened after that is unclear. 
El Cid returned to Alfonso, but now he had his own plans. He only stayed a short while and then returned to Zaragoza. El Cid was content to let the Almoravid armies and the armies of Alfonso fight without his help, even when there was a chance that the armies of Almoravid might defeat Alfonso and take over all of Alfonso's lands. El Cid chose not to fight because he was hoping that both armies would become weak. That would make it easier for him to carry out his own plan to become ruler of the Kingdom of Valencia.

Around this time, El Cid, with a combined Christian and Moorish army, began maneuvering in order to create his own fiefdom in the Moorish Mediterranean coastal city of Valencia. Several obstacles lay in his way. First was Berenguer Ramon II, who ruled nearby Barcelona. In May 1090, El Cid defeated and captured Berenguer in the Battle of Tébar (nowadays Pinar de Tévar, near Monroyo, Teruel). Berenguer was later released and his nephew Ramon Berenguer III married El Cid's youngest daughter Maria to ward against future conflicts.

Along the way to Valencia, El Cid also conquered other towns, many of which were near Valencia, such as El Puig and Quart de Poblet.

El Cid gradually came to have more influence on Valencia, then ruled by Al-Qadir. In October 1092 an uprising occurred in Valencia inspired by the city's chief judge Ibn Jahhaf and the Almoravids. El Cid began a siege of Valencia. A December 1093 attempt to break the siege failed. By the time the siege ended in May 1094, El Cid had carved out his own principality on the coast of the Mediterranean. Officially El Cid ruled in the name of Alfonso; in reality, El Cid was fully independent. The city was both Christian and Muslim, and both Moors and Christians served in the army and as administrators.

El Cid and his wife Jimena Díaz lived peacefully in Valencia for five years until the Almoravids besieged the city. El Cid died on June 10, 1099. His death was likely a result of the famine and deprivations caused by the siege. Valencia was captured by Masdali on May 5, 1102 and it did not become a Christian city again for over 125 years. Jimena fled to Burgos, Castile, in 1101. She rode into the town with her retinue and the body of El Cid. Originally buried in Castile in the monastery of San Pedro de Cardeña, his body now lies at the center of Burgos Cathedral.

After his demise, but still during the siege of Valencia, legend holds that Jimena ordered that the corpse of El Cid be fitted with his armour and set on his horse Babieca, to bolster the morale of his troops. In several variations of the story, the dead Rodrigo and his knights win a thundering charge against Valencia's besiegers, resulting in a war-is-lost-but-battle-is-won catharsis for generations of Christian Spaniards to follow. It is believed that the legend originated shortly after Jimena entered Burgos, and that it is derived from the manner in which Jimena's procession rode into Burgos, i.e., alongside her deceased husband.

During his campaigns, El Cid often ordered that books by classic Roman and Greek authors on military themes be read aloud to him and his troops, for both entertainment and inspiration before battle. El Cid's army had a novel approach to planning strategy as well, holding what might be called "brainstorming" sessions before each battle to discuss tactics. They frequently used unexpected strategies, engaging in what modern generals would call psychological warfare — waiting for the enemy to be paralyzed with terror and then attacking them suddenly; distracting the enemy with a small group of soldiers, etc. (El Cid used this distraction in capturing the town of Castejón as depicted in "Cantar de Mio Cid" ("The Song of my Cid"). El Cid accepted or included suggestions from his troops. In "The Song" the man who served him as his closest adviser was his vassal and kinsman Álvar Fáñez ""Minaya"" (meaning ""My brother"", a compound word of Spanish possessive "Mi" (My) and "Anaia", the basque word for "brother"), although the historical Álvar Fáñez remained in Castile with Alfonso VI.

Taken together, these practices imply an educated and intelligent commander who was able to attract and inspire good subordinates, and who would have attracted considerable loyalty from his followers, including those who were not Christian. It is these qualities, coupled with El Cid's legendary martial abilities, which have fueled his reputation as an outstanding battlefield commander.

Babieca or Bavieca was El Cid's warhorse. Several stories exist about El Cid and Babieca. One well-known legend about El Cid describes how he acquired the stallion. According to this story, Rodrigo's godfather, Pedro El Grande, was a monk at a Carthusian monastery. Pedro's coming-of-age gift to El Cid was his pick of a horse from an Andalusian herd. El Cid picked a horse that his godfather thought was a weak, poor choice, causing the monk to exclaim ""Babieca"!" (stupid!) Hence, it became the name of El Cid's horse. Another legend states that in a competition of battle to become King Sancho's "Campeador", or champion, a knight on horseback wished to challenge El Cid. The King wished a fair fight and gave El Cid his finest horse, Babieca, or Bavieca. This version says Babieca was raised in the royal stables of Seville and was a highly trained and loyal war horse, not a foolish stallion. The name in this instance could suggest that the horse came from the Babia region in León, Spain. In the poem Carmen Campidoctoris, Babieca appears as a gift from "a barbarian" to El Cid, so its name could also be derived from "Barbieca", or "horse of the barbarian".

Regardless, Babieca became a great warhorse, famous to the Christians, feared by El Cid's enemies, and loved by El Cid, who allegedly requested that Babieca be buried with him in the monastery of San Pedro de Cardeña. His name is mentioned in several tales and historical documents about El Cid, including "The Lay of El Cid".

A weapon traditionally identified as El Cid's sword, Tizona, used to be displayed in the Army Museum (Museo del Ejército) in Toledo. In 1999, a small sample of the blade underwent metallurgical analysis which confirmed that the blade was made in Moorish Córdoba in the eleventh century and contained amounts of Damascus steel.

In 2007, the Autonomous Community of Castile and León bought the sword for 1.6 million Euros, and it is currently on display at the Museum of Burgos.

El Cid also had a sword called Colada.

El Cid was married in July 1075 to Jimena Díaz, said to have been a kinswoman of King Alfonso. The "Historia Roderici" calls her a daughter of a Count Diego Fernández de Oviedo. Tradition states that when El Cid first laid eyes on her, he was enamored by her great beauty. El Cid and Jimena had two daughters and a son. The latter, Diego Rodríguez, was killed while fighting against the invading Muslim Almoravids from North Africa at the Battle of Consuegra in 1097. Like with his own marriage, El Cid would link his family to the royal families of the Iberian peninsula through the marriages of his two daughters. Cristina Rodríguez married Ramiro, Lord of Monzón and grandson of García Sánchez III of Navarre. Her own son, El Cid's grandson, would be elevated to the throne of Navarre as King García Ramírez. The other daughter, María, is said first to have married a prince of Aragon, presumably the son of Peter I, and she later wed Ramon Berenguer III, count of Barcelona.

The figure of El Cid has been the source for many literary works, beginning with the Cantar del Mio Cid, an epic poem from the 12th century which gives a partly-fictionalized account of his life. This poem, along with similar later works such as the Mocedades de Rodrigo, contributed to portray El Cid as a chivalric hero of the Reconquista, making him a legendary figure in Spain. In the early 17th century the Spanish writer Guillén de Castro wrote a play called "Las Mocedades del Cid", on which French playwright Pierre Corneille based one of his most famous tragicomedies, Le Cid. He was also a popular source of inspiration for Spanish writers of the Romantic period, such as Juan Eugenio Hartzenbusch, who wrote "La Jura de Santa Gadea", or José Zorrilla, who wrote a long poem called "La Leyenda del Cid".

Georges Bizet worked on a "Don Rodrigue" in 1873 that was set aside and never completed. Jules Massenet wrote an opera, "Le Cid", in 1885, based on Corneille's play of the same name. Claude Debussy began work in 1890 on an opera, "Rodrigue et Chimène", which he abandoned as unsuitable for his temperament; it was orchestrated for performance by Edison Denisov circa 1993.









</doc>
<doc id="10078" url="https://en.wikipedia.org/wiki?curid=10078" title="Enjambment">
Enjambment

In poetry, enjambment ( or ; from the French "enjambement") is incomplete syntax at the end of a line; the meaning runs over from one poetic line to the next, without terminal punctuation. Lines without enjambment are end-stopped.

In reading, the delay of meaning creates a tension that is released when the word or phrase that completes the syntax is encountered (called the rejet); the tension arises from the "mixed message" produced both by the pause of the line-end, and the suggestion to continue provided by the incomplete meaning. In spite of the apparent contradiction between rhyme, which heightens closure, and enjambment, which delays it, the technique is compatible with rhymed verse. Even in couplets, the closed or heroic couplet was a late development; older is the open couplet, where rhyme and enjambed lines co-exist.

Enjambment has a long history in poetry. Homer used the technique, and it is the norm for alliterative verse where rhyme is unknown. In the 32nd Psalm of the Hebrew Bible enjambment is unusually conspicuous. It was used extensively in England by Elizabethan poets for dramatic and narrative verses, before giving way to closed couplets. The example of John Milton in "Paradise Lost" laid the foundation for its subsequent use by the English Romantic poets; in its preface he identified it as one of the chief features of his verse: "sense variously drawn out from one verse into another".

The start of "The Waste Land" by T.S. Eliot, with only lines 4 and 7 end-stopped:
These lines from Shakespeare's "The Winter's Tale" ("c." 1611) are heavily enjambed:
Meaning flows as the lines progress, and the reader's eye is forced to go on to the next sentence. It can also make the reader feel uncomfortable or the poem feel like "flow-of-thought" with a sensation of urgency or disorder. In contrast, the following lines from: "Romeo and Juliet" ("c." 1595) are completely end-stopped:

Each line is formally correspondent with a unit of thought—in this case, a clause of a sentence. End-stopping is more frequent in early Shakespeare: as his style developed, the proportion of enjambment in his plays increased. Scholars such as Goswin König and A. C. Bradley have estimated approximate dates of undated works of Shakespeare by studying the frequency of enjambment.

"Endymion" by John Keats, lines 2–4:
Closely related to enjambment is the technique of "broken rhyme" or "split rhyme" which involves the splitting of an individual word, typically to allow a rhyme with one or more syllables of the split word. In English verse, broken rhyme is used almost exclusively in light verse, such as to form a word that rhymes with "orange", as in this example by Willard Espy, in his poem "The Unrhymable Word: Orange":
The clapping game "Miss Susie", which uses the break "... Hell / -o operator" to allude to the taboo word "Hell", then replaces it with the innocuous "Hello".




</doc>
<doc id="10080" url="https://en.wikipedia.org/wiki?curid=10080" title="European Convention on Nationality">
European Convention on Nationality

The European Convention on Nationality (E.T.S. No. 166) was signed in Strasbourg on 6 November 1997. It is a comprehensive convention of the Council of Europe dealing with the law of nationality. The Convention is open for signature by the member States of the Council of Europe and the non-member States which have participated in its elaboration and for accession by other non-member States. The Convention came into force on 1 March 2000 after ratification by 3 countries. As at 6 March 2014, the Convention has been signed by 29 countries, but has been ratified by only 20 of those countries.

Article 4d provides that neither marriage nor dissolution of marriage shall automatically affect the nationality of either spouse, nor shall a change of nationality by one spouse during marriage automatically affect the nationality of their spouse. Common practice among states at the beginning of the 20th century was that a woman was to have the nationality of her husband; i.e., upon marrying a foreigner the wife would automatically acquire the nationality of her husband, and lose her previous nationality. Even after the nationality of a married woman was no longer dependent on the nationality of her husband, legal provisions were still retained which automatically naturalised married women, and sometimes married men as well. This led to a number of problems, such as loss of the spouses' original nationality, the spouse losing the right to consular assistance (since consular assistance cannot be provided to nationals under the jurisdiction of a foreign state of which they are also nationals), and becoming subject to military service obligations. Article 4d addresses this situation.

Article 5 provides that no discrimination shall exist in a state's internal nationality law on the grounds of "sex, religion, race, colour or national or ethnic origin". It also provides that a state shall not discriminate amongst its nationals on the basis of whether they hold their nationality by birth or acquired it subsequently.

Article 6 relates to the acquisition of nationality. It provides for nationality to be acquired at birth by descent from either parent to those born within the territory of the state. (States may exclude partially or fully children born abroad). It also provides for nationality by virtue of birth in the territory of state; however, states may limit this to only children who would be otherwise stateless. It requires the possibility of naturalisation, and provides that the period of residence required for eligibility cannot be more than ten years lawful and habitual residence. It also requires to "facilitate" the acquisition of nationality by certain persons, including spouses of nationals, children of its nationals born abroad, children one of whose parents has acquired the nationality, children adopted by a national, persons lawfully and habitually resident for a period before the age of eighteen, and stateless persons and refugees lawfully and habitually resident on its territory.

Article 7 regulates the involuntary loss of nationality. It provides that states may deprive their nationals of their nationality in only the cases of voluntary acquisition of another nationality, fraud or failure to provide relevant information when acquiring nationality, voluntary military service in a foreign military force, or adoption as a child by foreign nationals. It also provides for the possibility of loss of nationality for nationals habitually residing abroad. Finally it provides loss of nationality for "conduct seriously prejudicial to the vital interests of the State Party".

Article 8 provides nationals with the right to renounce their nationality, providing they do not thereby become stateless. States may however restrict this right with respect to nationals residing abroad.

As at 6 March 2014, the following countries have signed or ratified the Convention:



</doc>
<doc id="10081" url="https://en.wikipedia.org/wiki?curid=10081" title="English orthography">
English orthography

English orthography is the system of writing conventions used to represent spoken English in written form that allows readers to connect spelling to sound to meaning.

Like the orthography of most world languages, English orthography has a broad degree of standardization. However, unlike with most languages, there are multiple ways to spell nearly every phoneme (sound), and most letters also have multiple pronunciations depending on their position in a word and the context. Several orthographic mistakes are common even among native speakers. This is mainly due to the large number of words that have been borrowed from a large number of other languages throughout the history of the English language, without successful attempts at complete spelling reforms. Most of the spelling conventions in Modern English were derived from the phonetic spelling of a variety of Middle English, and generally do not reflect the sound changes that have occurred since the late 15th century (such as the Great Vowel Shift).

Despite the various English dialects spoken from country to country and within different regions of the same country, there are only slight regional variations in English orthography, the two most recognized variations being British and American spelling, and its overall uniformity helps facilitate international communication. On the other hand, it also adds to the discrepancy between the way English is written and spoken in any given location.

"Note: In the following discussion, only one or two common pronunciations of American and British English varieties are used in this article for each word cited. Other regional pronunciations may be possible for some words, but indicating all possible regional variants in the article is impractical. "

Letters in English orthography usually represent a particular sound (phoneme). For example, the word "cat" consists of three letters , , and , in which represents the sound , the sound , and the sound .

Sequences of letters may perform this role as well as single letters. Thus, in the word "ship" (pronounced ), the digraph (two letters) represents the sound . In the word "ditch", the trigraph represent the sound .

Less commonly, a single letter can represent multiple successive sounds. The most common example is the letter , which normally represents the consonant cluster (for example, in the word "six", pronounced ).

The same letter (or sequence of letters) may be pronounced in different ways when it occurs in different positions within a word. For instance, the digraph represents the sound at the end of some words, such as "rough" . At the beginning of syllables (i.e. the syllable onset), the digraph is pronounced , as in the word "ghost" (pronounced ). Conversely, the digraph is never pronounced in syllable onsets and is almost never pronounced in syllable codas (the proper name "Pittsburgh" is an exception).

Some words contain silent letters, which do not represent any sound in modern English pronunciation. Examples include the in "doubt", "debt", "dumb", etc., the in "psychology" and "pneumatic", and the commonly encountered silent (discussed further below).

Another type of spelling characteristic is related to word origin. For example, when representing a vowel, the letter represents the sound in some words borrowed from Greek (reflecting an original upsilon), whereas the letter usually representing this sound in non-Greek words is the letter . Thus, the word "myth" is of Greek origin, while "pith" is a Germanic word. Other examples include pronounced (which is usually spelt ), and pronounced (which is usually spelt or ) – the use of these spellings for these sounds often mark words that have been borrowed from Greek.

Some researchers, such as Brengelman (1970), have suggested that, in addition to this marking of word origin, these spellings indicate a more formal level of style or register in a given text, although Rollings (2004) finds this point to be exaggerated as there would be many exceptions where a word with one of these spellings, such as for (like "telephone"), could occur in an informal text.

Spelling may also be useful to distinguish between homophones (words with the same pronunciation but different meanings), although in most cases the reason for the difference is historical and was not introduced for the purpose of making a distinction. For example, the words "heir" and "air" are pronounced identically in most dialects, but in writing they are distinguished from each other by their different spellings. Another example is the pair of homophones "pain" and "pane", where both are pronounced but have two different spellings of the vowel . Often this is because of the historical pronunciation of each word where, over time, two separate sounds become the same but the different spellings remain: "pain" used to be pronounced as , with a diphthong, and "pane" as , but the diphthong merged with the long vowel in "pane", making "pain" and "pane" homophones ("pane"–"pain" merger). Later became a diphthong .

In written language, this may help to resolve potential ambiguities that would arise otherwise (cf. "He's breaking the car" vs. "He's braking the car"). Nevertheless, many homophones remain that are unresolved by spelling (for example, the word "bay" has at least five fundamentally different meanings).

Some letters in English provide information about the pronunciation of "other" letters in the word. Rollings (2004) uses the term "markers" for such letters. Letters may mark different types of information. For instance, the letter in the word "cottage" indicates that the preceding is pronounced , rather than the more common value of in word-final position as the sound , such as in "tag" . The letter also often marks an altered pronunciation of a preceding vowel. In the pair "ban" and "bane", the of "ban" has the value , whereas the of "bane" is marked by the as having the value . In this context, the is not pronounced, and is referred to as "silent e".
A single letter may even fill multiple pronunciation-marking roles simultaneously. For example, in the word "wage", the marks not only the change of the from to , but also of the from to .

Doubled consonants usually indicate that the preceding vowel is pronounced short. For example, the doubled in "latter" indicates that the is pronounced , while the single of "later" gives . Doubled consonants only indicate any lengthening or gemination of the consonant sound itself when they come from different morphemes, as with the in "unnatural" = "un+natural".

A given letter or (letters) may have dual functions. For example, the letter in the word "cinema" has a sound-representing function (representing the sound ) and a pronunciation-marking function (marking the as having the value opposed to the value ).

Like many other alphabetic orthographies, English spelling does not represent non-contrastive phonetic sounds (that is, minor differences in pronunciation which are not used to distinguish between different words). Although the letter is pronounced by some speakers with aspiration at the beginning of words, this is never indicated in the spelling, and, indeed, this phonetic detail is probably not noticeable to the average native speaker not trained in phonetics. However, unlike some orthographies, English orthography often represents a very abstract underlying representation (or morphophonemic form) of English words.

In these cases, a given morpheme (i.e. a component of a word) has a fixed spelling even though it is pronounced differently in different words. An example is the past tense suffix -, which may be pronounced variously as , , or (for example, "dip" , "dipped" , "boom" , "boomed" , "loot" , "looted" ). As it happens, these different pronunciations of - can be predicted by a few phonological rules, but that is not the reason why its spelling is fixed.

Another example involves the vowel differences (with accompanying stress pattern changes) in several related words. For instance, the word "photographer" is derived from the word "photograph" by adding the derivational suffix -. When this suffix is added, the vowel pronunciations change largely owing to the moveable stress:

Other examples of this type are the - suffix (as in "agile" vs "agility", "acid" vs "acidity", "divine" vs "divinity", "sane" vs "sanity"). See also: Trisyllabic laxing.

Another such class of words includes "sign" and "bomb" with "silent" letters and , respectively. However, in the related words "signature" and "bombard" these letters are pronounced and , respectively. Here it could be argued that the underlying representation of "sign" and "bomb" is || and ||, in which the underlying || and || are only pronounced in the surface forms when followed by certain suffixes (-, -). Otherwise, the || and || are not realized in the surface pronunciation (e.g. when standing alone, or when followed by suffixes like - or -). In these cases, the orthography indicates the underlying consonants that are present in certain words but are absent in other related words. Other examples include the in "fast" and "fasten" , and the in "heir" and "inherit" .

Another example includes words like "mean" and "meant" . Here the vowel spelling is pronounced differently in the two related words. Thus, again the orthography uses only a single spelling that corresponds to the single morphemic form rather than to the surface phonological form.

English orthography does not always provide an underlying representation; sometimes it provides an intermediate representation between the underlying form and the surface pronunciation. This is the case with the spelling of the regular plural morpheme, which is written as either - (as in "tick, ticks" and "mite, mites") or - (as in "box, boxes"). Here the spelling - is pronounced either or (depending on the environment, e.g. "ticks" and "pigs" ) while - is usually pronounced (e.g. "boxes" ). Thus, there are two different spellings that correspond to the single underlying representation || of the plural suffix and the three surface forms. The spelling indicates the insertion of before the in the spelling -, but does not indicate the devoiced distinctly from the unaffected in the spelling -.

The abstract representation of words as indicated by the orthography can be considered advantageous since it makes etymological relationships more apparent to English readers. This makes writing English more complex, but arguably makes reading English more efficient. However, very abstract underlying representations, such as that of Chomsky & Halle (1968) or of underspecification theories, are sometimes considered too abstract to accurately reflect the communicative competence of native speakers. Followers of these arguments believe the less abstract surface forms are more "psychologically real" and thus more useful in terms of pedagogy.

English has some words that can be written with accent marks. These words have mostly been imported from other languages, usually French. As imported words become increasingly naturalised, there is an increasing tendency to omit the accent marks, even in formal writing. For example, words such as "rôle" and "hôtel" were first seen with accents when they were borrowed into English, but now the accent is almost never used. The words were originally considered foreign – and some people considered that English alternatives were preferable – but today their foreign origin is largely forgotten. Words most likely to retain the accent are those atypical of English morphology and therefore still perceived as slightly foreign. For example, "café" and "pâté" both have a pronounced final "e", which would otherwise be silent under the normal English pronunciation rules. However "café" is now sometimes facetiously pronounced "caff", while in "pâté", the acute accent is helpful to distinguish it from "pate".

Further examples of words sometimes retaining diacritics when used in English are: Ångström (partly because the scientific symbol for this unit of measurement is "Å"), "appliqué", "attaché", "blasé", "bric-à-brac", "Brötchen", "cliché", "crème", "crêpe", "façade", "fiancé(e)", "flambé", "naïve", "naïveté", "né(e)", "papier-mâché", "passé", "piñata", "protégé", "résumé", "risqué", "über-", "voilà". Italics, with appropriate accents, are generally applied to foreign terms that are uncommonly used in or have not been assimilated into English: for example, "adiós, crème brûlée, pièce de résistance, raison d'être, über, vis-à-vis, and belles-lettres."

It was formerly common in American English to use a diaeresis mark to indicate a hiatus: for example, "coöperate", "daïs", "reëlect". "The New Yorker" and "Technology Review" magazines still use it for this purpose, even though it is increasingly rare in modern English. Nowadays the diaeresis is normally left out ("cooperate"), or a hyphen is used ("co-operate") if the hiatus is between two morphemes in a compound word. It is, however, still common in monomorphemic loanwords such as "naïve" and "Noël".

Written accents are also used occasionally in poetry and scripts for dramatic performances to indicate that a certain normally unstressed syllable in a word should be stressed for dramatic effect, or to keep with the metre of the poetry. This use is frequently seen in archaic and pseudoarchaic writings with the "-ed" suffix, to indicate that the "e" should be fully pronounced, as with "cursèd".

In certain older texts (typically British), the use of the ligatures æ and œ is common in words such as "archæology", "diarrhœa", and "encyclopædia". Such words have Latin or Greek origin. Nowadays, the ligatures have been generally replaced in British English by the separated digraph "ae" and "oe" ("encyclopaedia", "diarrhoea"); but usually "economy", "ecology, " and in American English by "e" ("encyclopedia", "diarrhea"; but usually "paean", "amoeba", "oedipal", "Caesar"). In some cases, usage may vary; for instance, both "encyclopedia" and "encyclopaedia" are current in the UK.

Partly because English has never had any official regulating authority for spelling, such as the Spanish "Real Academia Española", the French "Académie française", and the German "Rat für deutsche Rechtschreibung", English spelling, compared to many other languages, is quite irregular and complex. Although French, among other languages, presents a similar degree of difficulty when "encoding" (writing), English is more difficult when "decoding" (reading), as there are clearly many more possible pronunciations of a group of letters. For example, in French, the sound (as in "food", but short), can be spelled "ou", "ous", "out", or "oux" ("ou", "nous", "tout", "choux"), but the pronunciation of each of those sequences is always the same. In English, the sound can be spelled in up to 18 different ways (see the Sound-to-spelling correspondences section below), including "oo", "u", "ui", "ue", "o", "oe", "ou", "ough", and "ew" ("food", "truth", "fruit", "blues", "to", "shoe", "group", "through", "grew"), but all of these have other pronunciations as well (e.g. as in "flood", "trust", "build", "bluest", "go", "hoe", "grout", "rough", "sew"). The Spelling-to-sound correspondences section below presents a summary of pronunciation variations. Thus, in unfamiliar words and proper nouns the pronunciation of some sequences, "ough" being the prime example, is unpredictable to even educated native English speakers.

Attempts to regularize or reform the spelling of English have usually failed. However, Noah Webster popularized more phonetic spellings in the United States, such as "flavor" for British "flavour", "fiber" for "fibre", "defense" for "defence", "analyze" for "analyse", "catalog" for "catalogue" and so forth. These spellings already existed as alternatives, but Webster's dictionaries helped make them standard in the US. See American and British English spelling differences for details.

Besides the quirks the English spelling system has inherited from its past, there are other idiosyncrasies in spelling that make it tricky to learn. English contains, depending on dialect, 24–27 separate consonant phonemes and 13–20 vowels. However, there are only 26 letters in the modern English alphabet, so there is not a one-to-one correspondence between letters and sounds. Many sounds are spelled using different letters or multiple letters, and for those words whose pronunciation is predictable from the spelling, the sounds denoted by the letters depend on the surrounding letters. For example, the digraph "th" represents two different sounds (the voiced dental fricative and the voiceless dental fricative) (see Pronunciation of English "th"), and the voiceless alveolar sibilant can be represented by the letters "s" and "c".

It is, however, not the shortage of letters which makes English spelling irregular. Its irregularities are caused mainly by the use of many different spellings for some of its sounds, such as the sounds /uː/, /iː/ and /oʊ/ ("too, "true, "shoe, "flew, "through; "sleeve", "leave", even", "seize", "siege"; "stole, "coal", "bowl", "roll", old", "mould"), and the use of identical sequences for spelling different sounds (over", oven", "move").

Furthermore, English no longer makes any attempt to anglicise the spellings of loanwords, but preserves the foreign spellings, even when they employ exotic conventions like the Polish "cz" in "Czech" (rather than "*Check") or the Norwegian "fj" in "fjord" (although "fiord" was formerly the most common spelling). In early Middle English, until roughly 1400, most imports from French were respelt according to English rules (e.g. "bataille"–"battle", "bouton"–"button", but not "double", or "trouble"). Instead of loans being respelled to conform to English spelling standards, sometimes the pronunciation changes as a result of pressure from the spelling. One example of this is the word "ski", which was adopted from Norwegian in the mid-18th century, although it did not become common until 1900. It used to be pronounced , which is similar to the Norwegian pronunciation, but the increasing popularity of the sport after the middle of the 20th century helped the pronunciation replace it.

There was also a period when the spelling of a small number of words was altered in what is now regarded as a misguided attempt to make them conform to what were perceived to be the etymological origins of the words. For example, the letter "b" was added to "debt" (originally "dette") in an attempt to link it to the Latin "debitum", and the letter "s" in "island" is a misplaced attempt to link it to Latin "insula" instead of the Old English word "īġland", which is the true origin of the English word. The letter "p" in "ptarmigan" has no etymological justification whatsoever, only seeking to invoke Greek despite being a Gaelic word.

The spelling of English continues to evolve. Many loanwords come from languages where the pronunciation of vowels corresponds to the way they were pronounced in Old English, which is similar to the Italian or Spanish pronunciation of the vowels, and is the value the vowel symbols and have in the International Phonetic Alphabet. As a result, there is a somewhat regular system of pronouncing "foreign" words in English, and some borrowed words have had their spelling changed to conform to this system. For example, "Hindu" used to be spelled "Hindoo", and the name "Maria" used to be pronounced like the name "Mariah", but was changed to conform to this system.

Commercial advertisers have also had an effect on English spelling. They introduced new or simplified spellings like "lite" instead of "light", "thru" instead of "through", "smokey" instead of "smoky" (for "smokey bacon" flavour crisps), and "rucsac" instead of "rucksack". The spellings of personal names have also been a source of spelling innovations: diminutive versions of women's names that sound the same as men's names have been spelled differently: "Nikki" and "Nicky", "Toni" and "Tony", "Jo" and "Joe".

As examples of the idiosyncratic nature of English spelling, the combination "ou" can be pronounced in at least nine different ways: in "out", in "soul", in "soup", in "touch", in "could", in "four", in "journal", in "cough", and in "famous". See the section Spelling-to-sound correspondences for a comprehensive treatment. In the other direction, the vowel sound in "me" can be spelt in at least 18 or 21 different ways: be (cede), ski (machine), bologna , algae, quay, beach, bee, deceit, people, key, volleyed, field (hygiene), amoeba, chamois, dengue, beguine, guyot, and city. See the section Sound-to-spelling correspondences below. (These examples assume a more-or-less standard non-regional British English accent. Other accents will vary.)

Sometimes everyday speakers of English change a counterintuitive pronunciation simply because it is counterintuitive. Changes like this are not usually seen as "standard", but can become standard if used enough. An example is the word "miniscule", which still competes with its original spelling of "minuscule", though this might also be because of analogy with the word "mini". A further example is the modern pronunciation of "tissue". 

Inconsistencies and irregularities in English pronunciation and spelling have gradually increased in number throughout the history of the English language. There are a number of contributing factors. First, gradual changes in pronunciation, such as the Great Vowel Shift, account for a tremendous number of irregularities. Second, relatively recent loan words from other languages generally carry their original spellings, which are often not phonetic in English. The Romanization of languages (e.g., Chinese) using alphabets derived from the Latin alphabet has further complicated this problem, for example when pronouncing Chinese proper names (of people or places).

The regular spelling system of Old English was swept away by the Norman Conquest, and English itself was supplanted in some spheres by Norman French for three centuries, eventually emerging with its spelling much influenced by French. English had also borrowed large numbers of words from French, which naturally kept their French spellings as there was no reason or mechanism to change them. The spelling of Middle English, such as in the writings of Geoffrey Chaucer, is very irregular and inconsistent, with the same word being spelled in different ways, sometimes even in the same sentence. However, these were generally much better guides to the then pronunciation than modern English spelling is.

For example, the sound , normally written "u", is spelled with an "o" in "son", "love", "come", etc., due to Norman spelling conventions which prohibited writing "u" before "v", "m", "n" due to the graphical confusion that would result. ("v", "u", "n" were identically written with two minims in Norman handwriting; "w" was written as two "u" letters; "m" was written with three minims, hence "mm" looked like "vun", "nvu", "uvu", etc.) Similarly, spelling conventions also prohibited final "v". Hence the identical spellings of the three different vowel sounds in "love", "grove" and "prove" are due to ambiguity in the Middle English spelling system, not sound change.

In 1417 Henry V began using English for official correspondence, which had no standardized spelling, instead of Latin or French which had standardized spelling. For example, for the word "right", Latin had one spelling", rectus;" Old French as used in English law had 6 spellings, Middle English had 77 spellings. English, now used as the official replacement language for Latin and French, motivated writers to standardize spellings, an effort which lasted about 500 years. 

There was also a series of linguistic sound changes towards the end of this period, including the Great Vowel Shift, which resulted in the "i" in "mine", for example, changing from a pure vowel to a diphthong. These changes for the most part did not detract from the rule-governed nature of the spelling system; but in some cases they introduced confusing inconsistencies, like the well-known example of the many pronunciations of "ough" ("rough", "through", "though", "trough", "plough", etc.). Most of these changes happened before the arrival of printing in England. However, the arrival of the printing press froze the current system, rather than providing the impetus for a realignment of spelling with pronunciation. Furthermore, it introduced further inconsistencies, partly because of the use of typesetters trained abroad, particularly in the Low Countries. For example, the "h" in "ghost" was influenced by Dutch. The addition and deletion of a silent "e" at the ends of words was also sometimes used to make the right-hand margin line up more neatly.

By the time dictionaries were introduced in the mid 17th century, the spelling system of English had started to stabilise. By the 19th century, most words had set spellings, though it took some time before they diffused throughout the English-speaking world. In The Mill on the Floss (1860), English novelist George Eliot satirized the attitude of the English rural gentry of the 1820s towards orthography:

The modern English spelling system, with its national variants, spread together with the expansion of public education later in the 19th century.

The most notorious group of letters in the English language, "ough", is commonly pronounced in at least ten different ways, six of which are illustrated in the construct, "Though the tough cough and hiccough plough him through", which is quoted by Robert A. Heinlein in "The Door into Summer" to illustrate the difficulties facing automated speech transcription and reading. "Ough", usually representing a pronunciation of roughly , is in fact a word in its own right, though rarely known or used: an exclamation of disgust similar to "ugh". The following are recorded throughout Englishes of the world:


The following pronunciations are found in uncommon single words:

The place name Loughborough uses two different pronunciations of "ough": the first "ough" has the sound as in "cuff" and the second rhymes with "thorough".

In a generative approach to English spelling, Rollings (2004) identifies twenty main orthographic vowels of stressed syllables that are grouped into four main categories: "Lax", "Tense", "Heavy", "Tense-R". (As this classification is based on orthography, not all orthographic "lax" vowels are necessarily phonologically lax.)

For instance, the letter "a" can represent the lax vowel , tense , heavy , or (often allophonically) before |r|. Heavy and tense-r vowels are the respective lax and tense counterparts followed by the letter "r".

Tense vowels are distinguished from lax vowels with a "silent" "e" letter that is added at the end of words. Thus, the letter "a" in "hat" is lax , but when the letter "e" is added in the word "hate" the letter "a" is tense . Similarly, heavy and tense-r vowels pattern together: the letters "ar" in "car" are heavy , the letters "ar" followed by silent "e" in the word "care" are . The letter "u" represents two different vowel patterns, one being , the other . There is no distinction between heavy and tense-r vowels with the letter "o", and the letter "u" in the pattern does not have a heavy vowel member.

Besides silent "e", another strategy for indicating tense and tense-r vowels, is the addition of another orthographic vowel forming a digraph. In this case, the first vowel is usually the main vowel while the second vowel is the "marking" vowel. For example, the word "man" has a lax "a" pronounced , but with the addition of "i" (as the digraph "ai") in the word "main" the "a" is marked as tense and pronounced . These two strategies produce words that are spelled differently but pronounced identically, as in "mane" (silent "e" strategy), "main" (digraph strategy) and "Maine" (both strategies). The use of two different strategies relates to the function of distinguishing between words that would otherwise be homonyms.

Besides the 20 basic vowel spellings, Rollings (2004) has a reduced vowel category (representing the sounds ) and a miscellaneous category (representing the sounds and +V, +V, V+V).

To reduce dialectal difficulties, the sound values given here correspond to the conventions at . This table includes H, W and Y when they represent vowel sounds. If no information is given, it is assumed that the vowel is in a stressed syllable.

Deriving the pronunciation of an English word from its spelling requires not only a careful knowledge of the rules given below (many of which are not explicitly known even by native speakers: speakers merely learn the spelling of a word along with its pronunciation) and their many exceptions, but also:


Notes:

† Nearly 80% of Americans pronounce "luxurious" with , while two thirds of British people use . Half the American speakers pronounce "luxury" as , the rest says <br>
†† About half of both British and American speakers say , the other half says .

<nowiki>*</nowiki> According to the Longman Pronunciation Dictionary, 75% of Americans pronounce "almond" as .<br>
† Where GA distinguishes between and in the letter combination ong, RP only has the vowel 

The following table shows for each sound the various spelling patterns used to denote it, starting with the prototypical pattern(s) followed by others in alphabetical order. Some of these patterns are very rare or unique (such as "gh" for , "ph" for , "i" for ). The symbol "…" stands for an intervening consonant.

In order of the IPA consonant tables
<nowiki>*</nowiki> In 2008, 61% of British people pronounced "diphthong" as , though phoneticians prefer .

<nowiki>**</nowiki> In 2008, 20% of Americans pronounced "thespian" as .

<nowiki>***</nowiki> The majority of British people, and the great majority of younger ones, pronounce "crescent" as .

† In 2008, 64% of Americans and 39% of British people pronounce "February" as .

†† The majority of Americans, and the great majority of younger ones, pronounce "congratulate" as .

Sorted more or less from close to open sounds in the vowel diagram.

† Identical to previous vowel in non-rhotic dialects like RP.















</doc>
<doc id="10083" url="https://en.wikipedia.org/wiki?curid=10083" title="Æthelred the Unready">
Æthelred the Unready

Æthelred II (Old English: "Æþelræd", ;  966 – 23 April 1016), known as the Unready, was King of the English from 978 to 1013 and again from 1014 until his death. His epithet does not derive from the modern word "unready", but rather from the Old English "unræd" (meaning "poorly advised"); it is a pun on his name, which means "well advised".

Æthelred was the son of King Edgar and Queen Ælfthryth. He came to the throne at about the age of 12, following the assassination of his older half-brother, Edward the Martyr. His brother's murder was carried out by supporters of his own claim to the throne, although he was too young to have any personal involvement. The chief problem of Æthelred's reign was conflict with the Danes. After several decades of relative peace, Danish raids on English territory began again in earnest in the 980s. Following the Battle of Maldon in 991, Æthelred paid tribute, or Danegeld, to the Danish king. In 1002, Æthelred ordered what became known as the St. Brice's Day massacre of Danish settlers. In 1013, King Sweyn Forkbeard of Denmark invaded England, as a result of which Æthelred fled to Normandy in 1013 and was replaced by Sweyn. However, he returned as king for two years after Sweyn's death in 1014. Æthelred's 37-year reign was the longest of any Anglo-Saxon king of England, and was only surpassed in the 13th century, by Henry III. Æthelred was briefly succeeded by his son, Edmund Ironside, but he died after a few months and was replaced by Sweyn's son, Cnut. Another of his sons, Edward the Confessor, became king in 1042.

Æthelred's first name, composed of the elements "æðele", "noble", and "ræd", "counsel, advice", is typical of the compound names of those who belonged to the royal House of Wessex, and it characteristically alliterates with the names of his ancestors, like Æthelwulf ("noble-wolf"), Ælfred ("elf-counsel"), Eadweard ("rich-protection"), and Eadgar ("rich-spear").

The story of Æthelred's notorious nickname, Old English "Unræd", goes a long way toward explaining how his reputation has declined through history It is usually translated into present-day English as "The Unready" (less often, though less confusingly, as "The Redeless"). The Anglo-Saxon noun "unræd" means "evil counsel", "bad plan", or "folly". It most often describes decisions and deeds, and once refers to the nature of Satan's deceit. The element "ræd" in "unræd" is the element in Æthelred's name which means "counsel". Thus "Æþelræd Unræd" is a pun meaning "Noble counsel, No counsel". The nickname has alternatively been taken adjectivally as "ill-advised", "ill-prepared", "indecisive", thus "Æthelred the ill-advised".

Because the nickname was first recorded in the 1180s, more than 150 years after Æthelred's death, it is doubtful that it carries any implications for how the king was seen by his contemporaries or near contemporaries.

Sir Frank Stenton remarked that "much that has brought condemnation of historians on King Æthelred may well be due in the last resort to the circumstances under which he became king." Æthelred's father, King Edgar, had died suddenly in July 975, leaving two young sons behind. The elder, Edward (later Edward the Martyr), was probably illegitimate, and was "still a youth on the verge of manhood" in 975. The younger son was Æthelred, whose mother, Ælfthryth, Edgar had married in 964. Ælfthryth was the daughter of Ordgar, ealdorman of Devon, and widow of Æthelwold, Ealdorman of East Anglia. At the time of his father's death, Æthelred could have been no more than 10 years old. As the elder of Edgar's sons, Edward – reportedly a young man given to frequent violent outbursts – probably would have naturally succeeded to the throne of England despite his young age, had not he "offended many important persons by his intolerable violence of speech and behaviour." In any case, a number of English nobles took to opposing Edward's succession and to defending Æthelred's claim to the throne; Æthelred was, after all, the son of Edgar's last, living wife, and no rumour of illegitimacy is known to have plagued Æthelred's birth, as it might have his elder brother's. Both boys, Æthelred certainly, were too young to have played any significant part in the political manoeuvring which followed Edgar's death. It was the brothers' supporters, and not the brothers themselves, who were responsible for the turmoil which accompanied the choice of a successor to the throne. Æthelred's cause was led by his mother and included Ælfhere, Ealdorman of Mercia and Bishop Æthelwold of Winchester, while Edward's claim was supported by Dunstan, the Archbishop of Canterbury and Oswald, the Archbishop of York among other noblemen, notably Æthelwine, Ealdorman of East Anglia, and Byrhtnoth, ealdorman of Essex. In the end, Edward's supporters proved the more powerful and persuasive, and he was crowned king at Kingston upon Thames before the year was out.

Edward reigned for only three years before he was murdered by members of his brother's household. Though little is known about Edward's short reign, it is known that it was marked by political turmoil. Edgar had made extensive grants of land to monasteries which pursued the new monastic ideals of ecclesiastical reform, but these disrupted aristocratic families' traditional patronage. The end of his firm rule saw a reversal of this policy, with aristocrats recovering their lost properties or seizing new ones. This was opposed by Dunstan, but according to Cyril Hart, "The presence of supporters of church reform on both sides indicates that the conflict between them depended as much on issues of land ownership and local power as on ecclesiastical legitimacy. Adherents of both Edward and Æthelred can be seen appropriating, or recovering, monastic lands." Nevertheless, favour for Edward must have been strong among the monastic communities. When Edward was killed at Æthelred's estate at Corfe Castle in Dorset in March 978, the job of recording the event, as well as reactions to it, fell to monastic writers. Stenton offers a summary of the earliest account of Edward's murder, which comes from a work praising the life of St Oswald: "On the surface his [Edward's] relations with Æthelred his half-brother and Ælfthryth his stepmother were friendly, and he was visiting them informally when he was killed. [Æthelred's] retainers came out to meet him with ostentatious signs of respect, and then, before he had dismounted, surrounded him, seized his hands, and stabbed him. ... So far as can be seen the murder was planned and carried out by Æthelred's household men in order that their young master might become king. There is nothing to support the allegation, which first appears in writing more than a century later, that Queen Ælfthryth had plotted her stepson's death. No one was punished for a part in the crime, and Æthelred, who was crowned a month after the murder, began to reign in an atmosphere of suspicion which destroyed the prestige of the crown. It was never fully restored in his lifetime." Nevertheless, at first, the outlook of the new king's officers and counsellors seems in no way to have been bleak. According to one chronicler, the coronation of Æthelred took place with much rejoicing by the councillors of the English people. Simon Keynes notes that "Byrhtferth of Ramsey states similarly that when Æthelred was consecrated king, by Archbishop Dunstan and Archbishop Oswald, 'there was great joy at his consecration’, and describes the king in this connection as 'a young man in respect of years, elegant in his manners, with an attractive face and handsome appearance'." Æthelred could not have been older than 13 years of age in this year.

During these early years, Æthelred was developing a close relationship to Æthelwold, bishop of Winchester, one who had supported his unsuccessful claim to the throne. When Æthelwold died, on 1 August 984, Æthelred deeply lamented the loss, and he wrote later in a charter from 993 that the event had deprived the country of one "whose industry and pastoral care administered not only to my interest but also to that of all inhabitants of the country."

England had experienced a period of peace after the reconquest of the Danelaw in the mid-10th century by King Edgar, Æthelred's father. However, beginning in 980, when Æthelred could not have been more than 14 years old, small companies of Danish adventurers carried out a series of coastline raids against England. Hampshire, Thanet and Cheshire were attacked in 980, Devon and Cornwall in 981, and Dorset in 982. A period of six years then passed before, in 988, another coastal attack is recorded as having taken place to the south-west, though here a famous battle was fought between the invaders and the thegns of Devon. Stenton notes that, though this series of isolated raids had no lasting effect on England itself, "their chief historical importance is that they brought England for the first time into diplomatic contact with Normandy." During this period, the Normans, who remembered their origins as a Scandinavian people, were well-disposed to their Danish cousins who, occasionally returning from a raid on England, sought port in Normandy. This led to grave tension between the English and Norman courts, and word of their enmity eventually reached Pope John XV. The pope was disposed to dissolve their hostility towards each other, and took steps to engineer a peace between England and Normandy, which was ratified in Rouen in 991.

However, in August of that same year, a sizeable Danish fleet began a sustained campaign in the south-east of England. It arrived off Folkestone, in Kent, and made its way around the south-east coast and up the River Blackwater, coming eventually to its estuary and occupying Northey Island. About west of Northey lies the coastal town of Maldon, where Byrhtnoth, ealdorman of Essex, was stationed with a company of thegns. The battle that followed between English and Danes is immortalised by the Old English poem "The Battle of Maldon", which describes the doomed but heroic attempt of Byrhtnoth to defend the coast of Essex against overwhelming odds. Stenton summarises the events of the poem: "For access to the mainland they (the Danes) depended on a causeway, flooded at high tide, which led from Northey to the flats along the southern margin of the estuary. Before they (the Danes) had left their camp on the island[,] Byrhtnoth, with his retainers and a force of local militia, had taken possession of the landward end of the causeway. Refusing a demand for tribute, shouted across the water while the tide was high, Byrhtnoth drew up his men along the bank, and waited for the ebb. As the water fell the raiders began to stream out along the causeway. But three of Byrthnoth's retainers held it against them, and at last they asked to be allowed to cross unhindered and fight on equal terms on the mainland. With what even those who admired him most called 'over-courage', Byrhtnoth agreed to this; the pirates rushed through the falling tide, and battle was joined. Its issue was decided by Byrhtnoth's fall. Many even of his own men immediately took to flight and the English ranks were broken. What gives enduring interest to the battle is the superb courage with which a group of Byrhtnoth's thegns, knowing that the fight was lost, deliberately gave themselves to death in order that they might avenge their lord." This was the first of a series of crushing defeats felt by the English: beaten first by Danish raiders, and later by organised Danish armies.

In 991, Æthelred was around 24 years old. In the aftermath of Maldon, it was decided that the English should grant the tribute to the Danes that they desired, and so a "gafol" of £10,000 was paid them for their peace. Yet it was presumably the Danish fleet that had beaten Byrhtnoth at Maldon that continued to ravage the English coast from 991 to 993. In 994, the Danish fleet, which had swollen in ranks since 991, turned up the Thames estuary and headed toward London. The battle fought there was inconclusive. It was about this time that and arranged an uneasy accord. A treaty was signed between Æthelred and Olaf that provided for seemingly civilised arrangements between the then-settled Danish companies and the English government, such as regulation settlement disputes and trade. But the treaty also stipulated that the ravaging and slaughter of the previous year would be forgotten, and ended abruptly by stating that £22,000 of gold and silver had been paid to the raiders as the price of peace. In 994, Olaf Tryggvason, already a baptised Christian, was confirmed as Christian in a ceremony at Andover; King Æthelred stood as his sponsor. After receiving gifts, Olaf promised "that he would never come back to England in hostility." Olaf then left England for Norway and never returned, though "other component parts of the Viking force appear to have decided to stay in England, for it is apparent from the treaty that some had chosen to enter into King Æthelred's service as mercenaries, based presumably on the Isle of Wight."

In 997, Danish raids began again. According to Keynes, "there is no suggestion that this was a new fleet or army, and presumably the mercenary force created in 994 from the residue of the raiding army of 991 had turned on those whom it had been hired to protect." It harried Cornwall, Devon, western Somerset and south Wales in 997, Dorset, Hampshire and Sussex in 998. In 999, it raided Kent, and, in 1000, it left England for Normandy, perhaps because the English had refused in this latest wave of attacks to acquiesce to the Danish demands for "gafol" or tribute, which would come to be known as Danegeld, 'Dane-payment'. This sudden relief from attack Æthelred used to gather his thoughts, resources, and armies: the fleet's departure in 1000 "allowed Æthelred to carry out a devastation of Strathclyde, the motive for which is part of the lost history of the north."

In 1001, a Danish fleet – perhaps the same fleet from 1000 – returned and ravaged west Sussex. During its movements, the fleet regularly returned to its base in the Isle of Wight. There was later an attempted attack in the south of Devon, though the English mounted a successful defence at Exeter. Nevertheless, Æthelred must have felt at a loss, and, in the Spring of 1002, the English bought a truce for £24,000. Æthelred's frequent payments of immense Danegelds are often held up as exemplary of the incompetency of his government and his own short-sightedness. However, Keynes points out that such payments had been practice for at least a century, and had been adopted by Alfred the Great, Charles the Bald and many others. Indeed, in some cases it "may have seemed the best available way of protecting the people against loss of life, shelter, livestock and crops. Though undeniably burdensome, it constituted a measure for which the king could rely on widespread support."

Æthelred ordered the massacre of all Danish men in England to take place on 13 November 1002, St Brice's Day. No order of this kind could be carried out in more than a third of England, where the Danes were too strong, but Gunhilde, sister of Sweyn Forkbeard, King of Denmark, was said to have been among the victims. It is likely that a wish to avenge her was a principal motive for Sweyn's invasion of western England the following year. By 1004 Sweyn was in East Anglia, where he sacked Norwich. In this year, a nobleman of East Anglia, Ulfcytel Snillingr met Sweyn in force, and made an impression on the until-then rampant Danish expedition. Though Ulfcytel was eventually defeated, outside Thetford, he caused the Danes heavy losses and was nearly able to destroy their ships. The Danish army left England for Denmark in 1005, perhaps because of the losses they sustained in East Anglia, perhaps from the very severe famine which afflicted the continent and the British Isles in that year.

An expedition the following year was bought off in early 1007 by tribute money of £36,000, and for the next two years England was free from attack. In 1008, the government created a new fleet of warships, organised on a national scale, but this was weakened when one of its commanders took to piracy, and the king and his council decided not to risk it in a general action. In Stenton's view: "The history of England in the next generation was really determined between 1009 and 1012...the ignominious collapse of the English defence caused a loss of morale which was irreparable." The Danish army of 1009, led by Thorkell the Tall and his brother Hemming, was the most formidable force to invade England since Æthelred became king. It harried England until it was bought off by £48,000 in April 1012.

Sweyn then launched an invasion in 1013 intending to crown himself king of England, during which he proved himself to be a general greater than any other Viking leader of his generation. By the end of 1013 English resistance had collapsed and Sweyn had conquered the country, forcing Æthelred into exile in Normandy. But the situation changed suddenly when Sweyn died on 3 February 1014. The crews of the Danish ships in the Trent that had supported Sweyn immediately swore their allegiance to Sweyn's son Cnut the Great, but leading English noblemen sent a deputation to Æthelred to negotiate his restoration to the throne. He was required to declare his loyalty to them, to bring in reforms regarding everything that they disliked and to forgive all that had been said and done against him in his previous reign. The terms of this agreement are of great constitutional interest in early English History as they are the first recorded pact between a King and his subjects and are also widely regarded as showing that many English noblemen had submitted to Sweyn simply because of their distrust of Æthelred. According to the "Anglo-Saxon Chronicle":

Æthelred then launched an expedition against Cnut and his allies. It was only the people of the Kingdom of Lindsey (modern North Lincolnshire) who supported Cnut. Æthelred first set out to recapture London apparently with the help of the Norwegian Olaf Haraldsson. According to the Icelandic historian, Snorri Sturluson, Ólaf led a successful attack on London bridge with a fleet of ships. He then went on to help Æthelred retake London and other parts of the country. Cnut and his army decided to withdraw from England, in April 1014, leaving his Lindsey allies to suffer Æthelred's revenge. In about 1016 it is thought that Ólaf left to concentrate on raiding western Europe. In the same year, Cnut returned to find a complex and volatile situation unfolding in England. Æthelred's son, Edmund Ironside, had revolted against his father and established himself in the Danelaw, which was angry at Cnut and Æthelred for the ravaging of Lindsey and was prepared to support Edmund in any uprising against both of them.

Over the next few months Cnut conquered most of England, while Edmund rejoined Æthelred to defend London when Æthelred died on 23 April 1016. The subsequent war between Edmund and Cnut ended in a decisive victory for Cnut at the Battle of Ashingdon on 18 October 1016. Edmund's reputation as a warrior was such that Cnut nevertheless agreed to divide England, Edmund taking Wessex and Cnut the whole of the country beyond the Thames. However, Edmund died on 30 November and Cnut became king of the whole country.

Æthelred was buried in Old St Paul's Cathedral, London. The tomb and his monument were destroyed along with the cathedral in the Great Fire of London in 1666. A modern monument in the crypt lists his among the important graves lost.

Æthelred's government produced extensive legislation, which he "ruthlessly enforced." Records of at least six legal codes survive from his reign, covering a range of topics. Notably, one of the members of his council (known as the "Witan") was Wulfstan II, Archbishop of York, a well-known homilist. The three latest codes from Æthelred's reign seemed to have been drafted by Wulfstan. These codes are extensively concerned with ecclesiastical affairs. They also exhibit the characteristics of Wulfstan's highly rhetorical style. Wulfstan went on to draft codes for King Cnut, and recycled there many of the laws which were used in Æthelred's codes.

Despite the failure of his government in the face of the Danish threat, Æthelred's reign was not without some important institutional achievements. The quality of the coinage, a good indicator of the prevailing economic conditions, significantly improved during his reign due to his numerous coinage reform laws.

Later perspectives of Æthelred have been less than flattering. Numerous legends and anecdotes have sprung up to explain his shortcomings, often elaborating abusively on his character and failures. One such anecdote is given by William of Malmesbury (lived  1080– 1143), who reports that Æthelred had defecated in the baptismal font as a child, which led St Dunstan to prophesy that the English monarchy would be overthrown during his reign. This story is, however, a fabrication, and a similar story is told of the Byzantine Emperor Constantine Copronymus, another mediaeval monarch who was unpopular among certain of his subjects.

Efforts to rehabilitate Æthelred's reputation have gained momentum since about 1980. Chief among the rehabilitators has been Simon Keynes, who has often argued that our poor impression of Æthelred is almost entirely based upon after-the-fact accounts of, and later accretions to, the narrative of events during Æthelred's long and complex reign. Chief among the culprits is in fact one of the most important sources for the history of the period, the "Anglo-Saxon Chronicle", which, as it reports events with a retrospect of 15 years, cannot help but interpret events with the eventual English defeat a foregone conclusion. Yet, as virtually no strictly contemporary narrative account of the events of Æthelred's reign exists, historians are forced to rely on what evidence there is. Keynes and others thus draw attention to some of the inevitable snares of investigating the history of a man whom later popular opinion has utterly damned. Recent cautious assessments of Æthelred's reign have more often uncovered reasons to doubt, rather than uphold, Æthelred's later infamy. Though the failures of his government will always put Æthelred's reign in the shadow of the reigns of kings Edgar, Aethelstan, and Alfred, historians' current impression of Æthelred's personal character is certainly not as unflattering as it once was: "Æthelred's misfortune as a ruler was owed not so much to any supposed defects of his imagined character, as to a combination of circumstances which anyone would have found difficult to control."

Æthelred has been credited with the formation of a local investigative body made up of twelve thegns who were charged with publishing the names of any notorious or wicked men in their respective districts. Because the members of these bodies were under solemn oath to act in accordance with the law and their own good consciences, they have been seen by some legal historians as the prototype for the English "Grand Jury". Æthelred makes provision for such a body in a law code he enacted at Wantage in 997, which states:

But the wording here suggests that Æthelred was perhaps revamping or re-confirming a custom which had already existed. He may actually have been expanding an established English custom for use among the Danish citizens in the North (the Danelaw). Previously, King Edgar had legislated along similar lines in his Whitbordesstan code:

The 'legend' of an Anglo-Saxon origin to the jury was first challenged seriously by Heinrich Brunner in 1872, who claimed that evidence of the jury was only seen for the first time during the reign of Henry II, some 200 years after the end of the Anglo-Saxon period, and that the practice had originated with the Franks, who in turn had influenced the Normans, who thence introduced it to England. Since Brunner's thesis, the origin of the English jury has been much disputed. Throughout the 20th century, legal historians disagreed about whether the practice was English in origin, or was introduced, directly or indirectly, from either Scandinavia or Francia. Recently, the legal historians Patrick Wormald and Michael Macnair have reasserted arguments in favour of finding in practices current during the Anglo-Saxon period traces of the Angevin practice of conducting inquests using bodies of sworn, private witnesses. Wormald has gone as far as to present evidence suggesting that the English practice outlined in Æthelred's Wantage code is at least as old as, if not older than, 975, and ultimately traces it back to a Carolingian model (something Brinner had done). However, no scholarly consensus has yet been reached.

"[A] youth of graceful manners, handsome countenance and fine person..." as well as "[A] tall, handsome man, elegant in manners, beautiful in countenance and interesting in his deportment."

Æthelred married first Ælfgifu, daughter of Thored, earl of Northumbria, in about 985. Their known children are:


In 1002 Æthelred married Emma of Normandy, sister of Richard II, Duke of Normandy. Their children were:


All of Æthelred's sons were named after predecessors of Æthelred on the throne.




 


</doc>
<doc id="10085" url="https://en.wikipedia.org/wiki?curid=10085" title="Edward Elgar">
Edward Elgar

Sir Edward William Elgar, 1st Baronet (; 2 June 1857 – 23 February 1934) was an English composer, many of whose works have entered the British and international classical concert repertoire. Among his best-known compositions are orchestral works including the "Enigma Variations", the "Pomp and Circumstance Marches", concertos for violin and cello, and two symphonies. He also composed choral works, including "The Dream of Gerontius", chamber music and songs. He was appointed Master of the King's Musick in 1924.

Although Elgar is often regarded as a typically English composer, most of his musical influences were not from England but from continental Europe. He felt himself to be an outsider, not only musically, but socially. In musical circles dominated by academics, he was a self-taught composer; in Protestant Britain, his Roman Catholicism was regarded with suspicion in some quarters; and in the class-conscious society of Victorian and Edwardian Britain, he was acutely sensitive about his humble origins even after he achieved recognition. He nevertheless married the daughter of a senior British army officer. She inspired him both musically and socially, but he struggled to achieve success until his forties, when after a series of moderately successful works his "Enigma Variations" (1899) became immediately popular in Britain and overseas. He followed the Variations with a choral work, "The Dream of Gerontius" (1900), based on a Roman Catholic text that caused some disquiet in the Anglican establishment in Britain, but it became, and has remained, a core repertory work in Britain and elsewhere. His later full-length religious choral works were well received but have not entered the regular repertory.

In his fifties, Elgar composed a symphony and a violin concerto that were immensely successful. His second symphony and his cello concerto did not gain immediate public popularity and took many years to achieve a regular place in the concert repertory of British orchestras. Elgar's music came, in his later years, to be seen as appealing chiefly to British audiences. His stock remained low for a generation after his death. It began to revive significantly in the 1960s, helped by new recordings of his works. Some of his works have, in recent years, been taken up again internationally, but the music continues to be played more in Britain than elsewhere.

Elgar has been described as the first composer to take the gramophone seriously. Between 1914 and 1925, he conducted a series of acoustic recordings of his works. The introduction of the moving-coil microphone in 1923 made far more accurate sound reproduction possible, and Elgar made new recordings of most of his major orchestral works and excerpts from "The Dream of Gerontius".

Edward Elgar was born in the small village of Lower Broadheath, outside Worcester, England. His father, William Henry Elgar (1821–1906), was raised in Dover and had been apprenticed to a London music publisher. In 1841 William moved to Worcester, where he worked as a piano tuner and set up a shop selling sheet music and musical instruments. In 1848 he married Ann Greening (1822–1902), daughter of a farm worker. Edward was the fourth of their seven children. Ann Elgar had converted to Roman Catholicism shortly before Edward's birth, and he was baptised and brought up as a Roman Catholic, to the disapproval of his father. William Elgar was a violinist of professional standard and held the post of organist of St. George's Roman Catholic Church, Worcester, from 1846 to 1885. At his instigation, masses by Cherubini and Hummel were first heard at the Three Choirs Festival by the orchestra in which he played the violin. All the Elgar children received a musical upbringing. By the age of eight, Elgar was taking piano and violin lessons, and his father, who tuned the pianos at many grand houses in Worcestershire, would sometimes take him along, giving him the chance to display his skill to important local figures.
Elgar's mother was interested in the arts and encouraged his musical development. He inherited from her a discerning taste for literature and a passionate love of the countryside. His friend and biographer W. H. "Billy" Reed wrote that Elgar's early surroundings had an influence that "permeated all his work and gave to his whole life that subtle but none the less true and sturdy English quality". He began composing at an early age; for a play written and acted by the Elgar children when he was about ten, he wrote music that forty years later he rearranged with only minor changes and orchestrated as the suites titled "The Wand of Youth".

Until he was fifteen, Elgar received a general education at Littleton (now Lyttleton) House school, near Worcester. However, his only formal musical training beyond piano and violin lessons from local teachers consisted of more advanced violin studies with Adolf Pollitzer, during brief visits to London in 1877–78. Elgar said, "my first music was learnt in the Cathedral ... from books borrowed from the music library, when I was eight, nine or ten." He worked through manuals of instruction on organ playing and read every book he could find on the theory of music. He later said that he had been most helped by Hubert Parry's articles in the "Grove Dictionary of Music and Musicians". Elgar began to learn German, in the hope of going to the Leipzig Conservatory for further musical studies, but his father could not afford to send him. Years later, a profile in "The Musical Times" considered that his failure to get to Leipzig was fortunate for Elgar's musical development: "Thus the budding composer escaped the dogmatism of the schools." However, it was a disappointment to Elgar that on leaving school in 1872 he went not to Leipzig but to the office of a local solicitor as a clerk. He did not find an office career congenial, and for fulfilment he turned not only to music but to literature, becoming a voracious reader. Around this time, he made his first public appearances as a violinist and organist.

After a few months, Elgar left the solicitor to embark on a musical career, giving piano and violin lessons and working occasionally in his father's shop. He was an active member of the Worcester Glee club, along with his father, and he accompanied singers, played the violin, composed and arranged works, and conducted for the first time. Pollitzer believed that, as a violinist, Elgar had the potential to be one of the leading soloists in the country, but Elgar himself, having heard leading virtuosi at London concerts, felt his own violin playing lacked a full enough tone, and he abandoned his ambitions to be a soloist. At twenty-two he took up the post of conductor of the attendants' band at the Worcester and County Lunatic Asylum in Powick, from Worcester. The band consisted of: piccolo, flute, clarinet, two cornets, euphonium, three or four first and a similar number of second violins, occasional viola, cello, double bass and piano. Elgar coached the players and wrote and arranged their music, including quadrilles and polkas, for the unusual combination of instruments. "The Musical Times" wrote, "This practical experience proved to be of the greatest value to the young musician. ... He acquired a practical knowledge of the capabilities of these different instruments. ... He thereby got to know intimately the tone colour, the ins and outs of these and many other instruments." He held the post for five years, from 1879, travelling to Powick once a week. Another post he held in his early days was professor of the violin at the Worcester College for the Blind Sons of Gentlemen.

Although rather solitary and introspective by nature, Elgar thrived in Worcester's musical circles. He played in the violins at the Worcester and Birmingham Festivals, and one great experience was to play Dvořák's Symphony No. 6 and "Stabat Mater" under the composer's baton. Elgar regularly played the bassoon in a wind quintet, alongside his brother Frank, an oboist (and conductor who ran his own wind band). Elgar arranged numerous pieces by Mozart, Beethoven, Haydn, and others for the quintet, honing his arranging and compositional skills.
In his first trips abroad, Elgar visited Paris in 1880 and Leipzig in 1882. He heard Saint-Saëns play the organ at the Madeleine and attended concerts by first-rate orchestras. In 1882 he wrote, "I got pretty well dosed with Schumann (my ideal!), Brahms, Rubinstein & Wagner, so had no cause to complain." In Leipzig he visited a friend, Helen Weaver, who was a student at the Conservatoire. They became engaged in the summer of 1883, but for unknown reasons the engagement was broken off the next year. Elgar was greatly distressed, and some of his later cryptic dedications of romantic music may have alluded to Helen and his feelings for her. Throughout his life, Elgar was often inspired by close women friends; Helen Weaver was succeeded by Mary Lygon, Dora Penny, Julia Worthington, Alice Stuart Wortley and finally Vera Hockman, who enlivened his old age.

In 1882, seeking more professional orchestral experience, Elgar was employed to play violin in Birmingham with William Stockley's Orchestra, for whom he would play every concert for the next seven years and where he later claimed he "learned all the music I know". On 13 December 1883 he took part with Stockley in a performance at Birmingham Town Hall of one of his first works for full orchestra, the "Sérénade mauresque" – the first time one of his compositions had been performed by a professional orchestra. Stockley had invited him to conduct the piece but later recalled "he declined, and, further, insisted upon playing in his place in the orchestra. The consequence was that he had to appear, fiddle in hand, to acknowledge the genuine and hearty applause of the audience." Elgar often went to London in an attempt to get his works published, but this period in his life found him frequently despondent and low on money. He wrote to a friend in April 1884, "My prospects are about as hopeless as ever ... I am not wanting in energy I think, so sometimes I conclude that 'tis want of ability. ... I have no money – not a cent."

When Elgar was 29, he took on a new pupil, Caroline Alice Roberts, daughter of the late Major-General Sir Henry Roberts, and published author of verse and prose fiction. Eight years older than Elgar, Alice became his wife three years later. Elgar's biographer Michael Kennedy writes, "Alice's family was horrified by her intention to marry an unknown musician who worked in a shop and was a Roman Catholic. She was disinherited." They were married on 8 May 1889, at Brompton Oratory. From then until her death, she acted as his business manager and social secretary, dealt with his mood swings, and was a perceptive musical critic. She did her best to gain him the attention of influential society, though with limited success. In time, he would learn to accept the honours given him, realising that they mattered more to her and her social class and recognising what she had given up to further his career. In her diary, she wrote, "The care of a genius is enough of a life work for any woman." As an engagement present, Elgar dedicated his short violin-and-piano piece "Salut d'Amour" to her. With Alice's encouragement, the Elgars moved to London to be closer to the centre of British musical life, and Elgar started devoting his time to composition. Their only child, Carice Irene, was born at their home in West Kensington on 14 August 1890. Her name, revealed in Elgar's dedication of "Salut d'Amour", was a contraction of her mother's names Caroline and Alice.

Elgar took full advantage of the opportunity to hear unfamiliar music. In the days before miniature scores and recordings were available, it was not easy for young composers to get to know new music. Elgar took every chance to do so at the Crystal Palace concerts. He and Alice attended day after day, hearing music by a wide range of composers. Among these were masters of orchestration from whom he learned much, such as Berlioz and Richard Wagner. His own compositions, however, made little impact on London's musical scene. August Manns conducted Elgar's orchestral version of "Salut d'amour" and the Suite in D at the Crystal Palace, and two publishers accepted some of Elgar's violin pieces, organ voluntaries, and part songs. Some tantalising opportunities seemed to be within reach but vanished unexpectedly. For example, an offer from the Royal Opera House, Covent Garden, to run through some of his works was withdrawn at the last second when Sir Arthur Sullivan arrived unannounced to rehearse some of his own music. Sullivan was horrified when Elgar later told him what had happened. Elgar's only important commission while in London came from his home city: the Worcester Festival Committee invited him to compose a short orchestral work for the 1890 Three Choirs Festival. The result is described by Diana McVeagh in the "Grove Dictionary of Music and Musicians", as "his first major work, the assured and uninhibited "Froissart"." Elgar conducted the first performance in Worcester in September 1890. For lack of other work, he was obliged to leave London in 1891 and return with his wife and child to Worcestershire, where he could earn a living conducting local musical ensembles and teaching. They settled in Alice's former home town, Great Malvern.

During the 1890s, Elgar gradually built up a reputation as a composer, chiefly of works for the great choral festivals of the English Midlands. "The Black Knight" (1892) and "King Olaf" (1896), both inspired by Longfellow, "The Light of Life" (1896) and "Caractacus" (1898) were all modestly successful, and he obtained a long-standing publisher in Novello and Co. Other works of this decade included the "Serenade for Strings" (1892) and "Three Bavarian Dances" (1897). Elgar was of enough consequence locally to recommend the young composer Samuel Coleridge-Taylor to the Three Choirs Festival for a concert piece, which helped establish the younger man's career. Elgar was catching the attention of prominent critics, but their reviews were polite rather than enthusiastic. Although he was in demand as a festival composer, he was only just getting by financially and felt unappreciated. In 1898, he said he was "very sick at heart over music" and hoped to find a way to succeed with a larger work. His friend August Jaeger tried to lift his spirits: "A day's attack of the blues ... will not drive away your desire, your necessity, which is to exercise those creative faculties which a kind providence has given you. Your time of universal recognition will come."
In 1899, that prediction suddenly came true. At the age of forty-two, Elgar produced the "Enigma Variations", which were premiered in London under the baton of the eminent German conductor Hans Richter. In Elgar's own words, "I have sketched a set of Variations on an original theme. The Variations have amused me because I've labelled them with the nicknames of my particular friends ... that is to say I've written the variations each one to represent the mood of the 'party' (the person) ... and have written what I think they would have written – if they were asses enough to compose". He dedicated the work "To my friends pictured within". Probably the best known variation is "Nimrod", depicting Jaeger. Purely musical considerations led Elgar to omit variations depicting Arthur Sullivan and Hubert Parry, whose styles he tried but failed to incorporate in the variations. The large-scale work was received with general acclaim for its originality, charm and craftsmanship, and it established Elgar as the pre-eminent British composer of his generation.

The work is formally titled "Variations on an Original Theme"; the word "Enigma" appears over the first six bars of music, which led to the familiar version of the title. The enigma is that, although there are fourteen variations on the "original theme", there is another overarching theme, never identified by Elgar, which he said "runs through and over the whole set" but is never heard. Later commentators have observed that although Elgar is today regarded as a characteristically English composer, his orchestral music and this work in particular share much with the Central European tradition typified at the time by the work of Richard Strauss. The "Enigma Variations" were well received in Germany and Italy, and remain to the present day a worldwide concert staple.

Elgar's biographer Basil Maine commented, "When Sir Arthur Sullivan died in 1900 it became apparent to many that Elgar, although a composer of another build, was his true successor as first musician of the land." Elgar's next major work was eagerly awaited. For the Birmingham Triennial Music Festival of 1900, he set Cardinal John Henry Newman's poem "The Dream of Gerontius" for soloists, chorus and orchestra. Richter conducted the premiere, which was marred by a poorly prepared chorus, which sang badly. Critics recognised the mastery of the piece despite the defects in performance. It was performed in Düsseldorf, Germany, in 1901 and again in 1902, conducted by Julius Buths, who also conducted the European premiere of the "Enigma Variations" in 1901. The German press was enthusiastic. "The Cologne Gazette" said, "In both parts we meet with beauties of imperishable value. ... Elgar stands on the shoulders of Berlioz, Wagner, and Liszt, from whose influences he has freed himself until he has become an important individuality. He is one of the leaders of musical art of modern times." "The Düsseldorfer Volksblatt" wrote, "A memorable and epoch-making first performance! Since the days of Liszt nothing has been produced in the way of oratorio ... which reaches the greatness and importance of this sacred cantata." Richard Strauss, then widely viewed as the leading composer of his day, was so impressed that in Elgar's presence he proposed a toast to the success of "the first English progressive musician, Meister Elgar." Performances in Vienna, Paris and New York followed, and "The Dream of Gerontius" soon became equally admired in Britain. According to Kennedy, "It is unquestionably the greatest British work in the oratorio form ... [it] opened a new chapter in the English choral tradition and liberated it from its Handelian preoccupation." Elgar, as a Roman Catholic, was much moved by Newman's poem about the death and redemption of a sinner, but some influential members of the Anglican establishment disagreed. His colleague, Charles Villiers Stanford complained that the work "stinks of incense". The Dean of Gloucester banned "Gerontius" from his cathedral in 1901, and at Worcester the following year, the Dean insisted on expurgations before allowing a performance.
Elgar is probably best known for the first of the five "Pomp and Circumstance Marches", which were composed between 1901 and 1930. It is familiar to millions of television viewers all over the world every year who watch the Last Night of the Proms, where it is traditionally performed. When the theme of the slower middle section (technically called the "trio") of the first march came into his head, he told his friend Dora Penny, "I've got a tune that will knock 'em – will knock 'em flat". When the first march was played in 1901 at a London Promenade Concert, it was conducted by Henry J. Wood, who later wrote that the audience "rose and yelled ... the one and only time in the history of the Promenade concerts that an orchestral item was accorded a double encore." To mark the coronation of Edward VII, Elgar was commissioned to set A. C. Benson's "Coronation Ode" for a gala concert at the Royal Opera House in June 1901. The approval of the king was confirmed, and Elgar began work. The contralto Clara Butt had persuaded him that the trio of the first "Pomp and Circumstance" march could have words fitted to it, and Elgar invited Benson to do so. Elgar incorporated the new vocal version into the Ode. The publishers of the score recognised the potential of the vocal piece, "Land of Hope and Glory", and asked Benson and Elgar to make a further revision for publication as a separate song. It was immensely popular and is now considered an unofficial British national anthem. In the United States, the trio, known simply as "Pomp and Circumstance" or "The Graduation March", has been adopted since 1905 for virtually all high school and university graduations.

In March 1904 a three-day festival of Elgar's works was presented at Covent Garden, an honour never before given to any English composer. "The Times" commented, "Four or five years ago if any one had predicted that the Opera-house would be full from floor to ceiling for the performance of an oratorio by an English composer he would probably have been supposed to be out of his mind." The king and queen attended the first concert, at which Richter conducted "The Dream of Gerontius", and returned the next evening for the second, the London premiere of "The Apostles" (first heard the previous year at the Birmingham Festival). The final concert of the festival, conducted by Elgar, was primarily orchestral, apart for an excerpt from "Caractacus" and the complete "Sea Pictures" (sung by Clara Butt). The orchestral items were "Froissart", the "Enigma Variations", "Cockaigne", the first two (at that time the only two) "Pomp and Circumstance" marches, and the premiere of a new orchestral work, "In the South", inspired by a holiday in Italy.
Elgar was knighted at Buckingham Palace on 5 July 1904. The following month, he and his family moved to Plâs Gwyn, a large house on the outskirts of Hereford, overlooking the River Wye, where they lived until 1911. Between 1902 and 1914, Elgar was, in Kennedy's words, at the zenith of popularity. He made four visits to the US, including one conducting tour, and earned considerable fees from the performance of his music. Between 1905 and 1908, he held the post of Peyton Professor of Music at the University of Birmingham. He had accepted the post reluctantly, feeling that a composer should not head a school of music. He was not at ease in the role, and his lectures caused controversy, with his attacks on the critics and on English music in general: "Vulgarity in the course of time may be refined. Vulgarity often goes with inventiveness ... but the commonplace mind can never be anything but commonplace. An Englishman will take you into a large room, beautifully proportioned, and will point out to you that it is white – all over white – and somebody will say, 'What exquisite taste'. You know in your own mind, in your own soul, that it is not taste at all, that it is the want of taste, that is mere evasion. English music is white, and evades everything." He regretted the controversy and was glad to hand on the post to his friend Granville Bantock in 1908. His new life as a celebrity was a mixed blessing to the highly strung Elgar, as it interrupted his privacy, and he often was in ill-health. He complained to Jaeger in 1903, "My life is one continual giving up of little things which I love." Both W. S. Gilbert and Thomas Hardy sought to collaborate with Elgar in this decade. Elgar refused, but would have collaborated with George Bernard Shaw had Shaw been willing.

Elgar's principal composition in 1905 was the "Introduction and Allegro for Strings", dedicated to Samuel Sanford, professor at Yale University. Elgar visited America in that year to conduct his music and to accept a doctorate from Yale. His next large-scale work was the sequel to "The Apostles" – the oratorio "The Kingdom" (1906). It was well received but did not catch the public imagination as "The Dream of Gerontius" had done and continued to do. Among keen Elgarians, however, "The Kingdom" was sometimes preferred to the earlier work: Elgar's friend Frank Schuster told the young Adrian Boult: "compared with "The Kingdom", "Gerontius" is the work of a raw amateur." As Elgar approached his fiftieth birthday, he began work on his first symphony, a project that had been in his mind in various forms for nearly ten years. His First Symphony (1908) was a national and international triumph. Within weeks of the premiere it was performed in New York under Walter Damrosch, Vienna under Ferdinand Löwe, St. Petersburg under Alexander Siloti, and Leipzig under Arthur Nikisch. There were performances in Rome, Chicago, Boston, Toronto and fifteen British towns and cities. In just over a year, it received a hundred performances in Britain, America and continental Europe.
The Violin Concerto (1910) was commissioned by Fritz Kreisler, one of the leading international violinists of the time. Elgar wrote it during the summer of 1910, with occasional help from W. H. Reed, the leader of the London Symphony Orchestra, who helped the composer with advice on technical points. Elgar and Reed formed a firm friendship, which lasted for the rest of Elgar's life. Reed's biography, "Elgar As I Knew Him" (1936), records many details of Elgar's methods of composition. The work was presented by the Royal Philharmonic Society, with Kreisler and the London Symphony Orchestra, conducted by the composer. Reed recalled, "the Concerto proved to be a complete triumph, the concert a brilliant and unforgettable occasion." So great was the impact of the concerto that Kreisler's rival Eugène Ysaÿe spent much time with Elgar going through the work. There was great disappointment when contractual difficulties prevented Ysaÿe from playing it in London.

The Violin Concerto was Elgar's last popular triumph. The following year he presented his Second Symphony in London, but was disappointed at its reception. Unlike the First Symphony, it ends not in a blaze of orchestral splendour but quietly and contemplatively. Reed, who played at the premiere, later wrote that Elgar was recalled to the platform several times to acknowledge the applause, "but missed that unmistakable note perceived when an audience, even an English audience, is thoroughly roused or worked up, as it was after the Violin Concerto or the First Symphony." Elgar asked Reed, "What is the matter with them, Billy? They sit there like a lot of stuffed pigs." The work was, by normal standards, a success, with twenty-seven performances within three years of its premiere, but it did not achieve the international "furore" of the First Symphony.

In June 1911, as part of the celebrations surrounding the coronation of King George V, Elgar was appointed to the Order of Merit, an honour limited to twenty-four holders at any time. The following year, the Elgars moved back to London, to a large house in Netherhall Gardens, Hampstead, designed by Norman Shaw. There Elgar composed his last two large-scale works of the pre-war era, the choral ode, "The Music Makers" (for the Birmingham Festival, 1912) and the symphonic study "Falstaff" (for the Leeds Festival, 1913). Both were received politely but without enthusiasm. Even the dedicatee of "Falstaff", the conductor Landon Ronald, confessed privately that he could not "make head or tail of the piece," while the musical scholar Percy Scholes wrote of "Falstaff" that it was a "great work" but, "so far as public appreciation goes, a comparative failure."

When World War I broke out, Elgar was horrified at the prospect of the carnage, but his patriotic feelings were nonetheless aroused. He composed "A Song for Soldiers", which he later withdrew. He signed up as a special constable in the local police and later joined the Hampstead Volunteer Reserve of the army. He composed patriotic works, "Carillon", a recitation for speaker and orchestra in honour of Belgium, and "Polonia", an orchestral piece in honour of Poland. "Land of Hope and Glory", already popular, became still more so, and Elgar wished in vain to have new, less nationalistic, words sung to the tune.
Elgar's other compositions during the war included incidental music for a children's play, "The Starlight Express" (1915); a ballet, "The Sanguine Fan" (1917); and "The Spirit of England" (1915–17, to poems by Laurence Binyon), three choral settings very different in character from the romantic patriotism of his earlier years. His last large-scale composition of the war years was "The Fringes of the Fleet", settings of verses by Rudyard Kipling, performed with great popular success around the country, until Kipling for unexplained reasons objected to their performance in theatres. Elgar conducted a recording of the work for the Gramophone Company.

Towards the end of the war, Elgar was in poor health. His wife thought it best for him to move to the countryside, and she rented 'Brinkwells', a house near Fittleworth in Sussex, from the painter Rex Vicat Cole. There Elgar recovered his strength and, in 1918 and 1919, he produced four large-scale works. The first three of these were chamber pieces: the Violin Sonata in E minor, the Piano Quintet in A minor, and the String Quartet in E minor. On hearing the work in progress, Alice Elgar wrote in her diary, "E. writing wonderful new music". All three works were well received. "The Times" wrote, "Elgar's sonata contains much that we have heard before in other forms, but as we do not at all want him to change and be somebody else, that is as it should be." The quartet and quintet were premiered at the Wigmore Hall on 21 May 1919. "The Manchester Guardian" wrote, "This quartet, with its tremendous climaxes, curious refinements of dance-rhythms, and its perfect symmetry, and the quintet, more lyrical and passionate, are as perfect examples of chamber music as the great oratorios were of their type."

By contrast, the remaining work, the Cello Concerto in E minor, had a disastrous premiere, at the opening concert of the London Symphony Orchestra's 1919–20 season in October 1919. Apart from the Elgar work, which the composer conducted, the rest of the programme was conducted by Albert Coates, who overran his rehearsal time at the expense of Elgar's. Lady Elgar wrote, "that brutal selfish ill-mannered bounder ... that brute Coates went on rehearsing." The critic of "The Observer", Ernest Newman, wrote, "There have been rumours about during the week of inadequate rehearsal. Whatever the explanation, the sad fact remains that never, in all probability, has so great an orchestra made so lamentable an exhibition of itself. ... The work itself is lovely stuff, very simple – that pregnant simplicity that has come upon Elgar's music in the last couple of years – but with a profound wisdom and beauty underlying its simplicity." Elgar attached no blame to his soloist, Felix Salmond, who played for him again later, including at the inaugural concert of the City of Birmingham Orchestra (later City of Birmingham Symphony Orchestra), which Elgar conducted. In contrast with the First Symphony and its hundred performances in just over a year, the Cello Concerto did not have a second performance in London for more than a year.

Although in the 1920s Elgar's music was no longer in fashion, his admirers continued to present his works when possible. Reed singles out a performance of the Second Symphony in March 1920 conducted by "a young man almost unknown to the public", Adrian Boult, for bringing "the grandeur and nobility of the work" to a wider public. Also in 1920, Landon Ronald presented an all-Elgar concert at the Queen's Hall. Alice Elgar wrote with enthusiasm about the reception of the symphony, but this was one of the last times she heard Elgar's music played in public. After a short illness, she died of lung cancer on 7 April 1920, at the age of seventy-two.

Elgar was devastated by the loss of his wife. With no public demand for new works, and deprived of Alice's constant support and inspiration, he allowed himself to be deflected from composition. His daughter later wrote that Elgar inherited from his father a reluctance to "settle down to work on hand but could cheerfully spend hours over some perfectly unnecessary and entirely unremunerative undertaking", a trait that became stronger after Alice's death. For much of the rest of his life, Elgar indulged himself in his several hobbies. Throughout his life he was a keen amateur chemist, sometimes using a laboratory in his back garden. He even patented the "Elgar Sulphuretted Hydrogen Apparatus" in 1908. He enjoyed football, supporting Wolverhampton Wanderers F.C., for whom he composed an anthem, "He Banged the Leather for Goal", and in his later years he frequently attended horseraces. His protégés, the conductor Malcolm Sargent and violinist Yehudi Menuhin, both recalled rehearsals with Elgar at which he swiftly satisfied himself that all was well and then went off to the races. In his younger days, Elgar had been an enthusiastic cyclist, buying Royal Sunbeam bicycles for himself and his wife in 1903 (he named his "Mr. Phoebus"). As an elderly widower, he enjoyed being driven about the countryside by his chauffeur. In November and December 1923, he took a voyage to Brazil, journeying up the Amazon to Manaus, where he was impressed by its opera house, the Teatro Amazonas. Almost nothing is recorded about Elgar's activities or the events that he encountered during the trip, which gave the novelist James Hamilton-Paterson considerable latitude when writing "Gerontius", a fictional account of the journey.

After Alice's death, Elgar sold the Hampstead house, and after living for a short time in a flat in St James's in the heart of London, he moved back to Worcestershire, to the village of Kempsey, where he lived from 1923 to 1927. He did not wholly abandon composition in these years. He made large-scale symphonic arrangements of works by Bach and Handel and wrote his "Empire March" and eight songs "Pageant of Empire" for the 1924 British Empire Exhibition. Shortly after these were published, he was appointed Master of the King's Musick on 13 May 1924, following the death of Sir Walter Parratt.

From 1926 onwards, Elgar made a series of recordings of his own works. Described by the music writer Robert Philip as "the first composer to take the gramophone seriously", he had already recorded much of his music by the early acoustic-recording process for His Master's Voice (HMV) from 1914 onwards, but the introduction of electrical microphones in 1925 transformed the gramophone from a novelty into a realistic medium for reproducing orchestral and choral music. Elgar was the first composer to take full advantage of this technological advance. Fred Gaisberg of HMV, who produced Elgar's recordings, set up a series of sessions to capture on disc the composer's interpretations of his major orchestral works, including the "Enigma Variations", "Falstaff", the first and second symphonies, and the cello and violin concertos. For most of these, the orchestra was the LSO, but the "Variations" were played by the Royal Albert Hall Orchestra. Later in the series of recordings, Elgar also conducted two newly founded orchestras, Boult's BBC Symphony Orchestra and Sir Thomas Beecham's London Philharmonic Orchestra.

Elgar's recordings were released on 78-rpm discs by both HMV and RCA Victor. After World War II, the 1932 recording of the Violin Concerto with the teenage Menuhin as soloist remained available on 78 and later on LP, but the other recordings were out of the catalogues for some years. When they were reissued by EMI on LP in the 1970s, they caused surprise to many by their fast tempi, in contrast to the slower speeds adopted by many conductors in the years since Elgar's death. The recordings were reissued on CD in the 1990s.

In November 1931, Elgar was filmed by Pathé for a newsreel depicting a recording session of "Pomp and Circumstance March No. 1" at the opening of EMI's Abbey Road Studios in London. It is believed to be the only surviving sound film of Elgar, who makes a brief remark before conducting the London Symphony Orchestra, asking the musicians to "play this tune as though you've never heard it before." A memorial plaque to Elgar at Abbey Road was unveiled on 24 June 1993.

A late piece of Elgar's, the "Nursery Suite", was an early example of a studio premiere: its first performance was in the Abbey Road studios. For this work, dedicated to the wife and daughters of the Duke of York, Elgar once again drew on his youthful sketch-books.

In his final years, Elgar experienced a musical revival. The BBC organised a festival of his works to celebrate his seventy-fifth birthday, in 1932. He flew to Paris in 1933 to conduct the Violin Concerto for Menuhin. While in France, he visited his fellow composer Frederick Delius at his house at Grez-sur-Loing. He was sought out by younger musicians such as Adrian Boult, Malcolm Sargent and John Barbirolli, who championed his music when it was out of fashion. He began work on an opera, "The Spanish Lady", and accepted a commission from the BBC to compose a Third Symphony. His final illness, however, prevented their completion. He fretted about the unfinished works. He asked Reed to ensure that nobody would "tinker" with the sketches and attempt a completion of the symphony, but at other times he said, "If I can't complete the Third Symphony, somebody will complete it – or write a better one." After Elgar's death, Percy M. Young, in co-operation with the BBC and Elgar's daughter Carice, produced a version of "The Spanish Lady", which was issued on CD. The Third Symphony sketches were elaborated by the composer Anthony Payne into a complete score in 1998.

Inoperable colorectal cancer was discovered during an operation on 8 October 1933. He told his consulting doctor, Arthur Thomson, that he had no faith in an afterlife: "I believe there is nothing but complete oblivion." Elgar died on 23 February 1934 at the age of seventy-six and was buried next to his wife at St. Wulstan's Roman Catholic Church in Little Malvern.

Elgar was contemptuous of folk music and had little interest in or respect for the early English composers, calling William Byrd and his contemporaries "museum pieces". Of later English composers, he regarded Purcell as the greatest, and he said that he had learned much of his own technique from studying Hubert Parry's writings. The continental composers who most influenced Elgar were Handel, Dvořák and, to some degree, Brahms. In Elgar's chromaticism, the influence of Wagner is apparent, but Elgar's individual style of orchestration owes much to the clarity of nineteenth-century French composers, Berlioz, Massenet, Saint-Saëns and, particularly, Delibes, whose music Elgar played and conducted at Worcester and greatly admired.

Elgar began composing when still a child, and all his life he drew on his early sketchbooks for themes and inspiration. The habit of assembling his compositions, even large-scale ones, from scraps of themes jotted down randomly remained throughout his life. His early adult works included violin and piano pieces, music for the wind quintet in which he and his brother played between 1878 and 1881, and music of many types for the Powick Asylum band. Diana McVeagh in "Grove's Dictionary" finds many embryonic Elgarian touches in these pieces, but few of them are regularly played, except "Salut d'Amour" and (as arranged decades later into "The Wand of Youth" Suites) some of the childhood sketches. Elgar's sole work of note during his first spell in London in 1889–91, the overture "Froissart", was a romantic-bravura piece, influenced by Mendelssohn and Wagner, but also showing further Elgarian characteristics. Orchestral works composed during the subsequent years in Worcestershire include the "Serenade for Strings" and "Three Bavarian Dances". In this period and later, Elgar wrote songs and partsongs. W. H. Reed expressed reservations about these pieces, but praised the partsong "The Snow", for female voices, and "Sea Pictures", a cycle of five songs for contralto and orchestra which remains in the repertory.

Elgar's principal large-scale early works were for chorus and orchestra for the Three Choirs and other festivals. These were "The Black Knight", "King Olaf", "The Light of Life", "The Banner of St George" and "Caractacus". He also wrote a "Te Deum" and "Benedictus" for the Hereford Festival. Of these, McVeagh comments favourably on his lavish orchestration and innovative use of leitmotifs, but less favourably on the qualities of his chosen texts and the patchiness of his inspiration. McVeagh makes the point that, because these works of the 1890s were for many years little known (and performances remain rare), the mastery of his first great success, the "Enigma Variations", appeared to be a sudden transformation from mediocrity to genius, but in fact his orchestral skills had been building up throughout the decade.

Elgar's best-known works were composed within the twenty-one years between 1899 and 1920. Most of them are orchestral. Reed wrote, "Elgar's genius rose to its greatest height in his orchestral works" and quoted the composer as saying that, even in his oratorios, the orchestral part is the most important. The "Enigma Variations" made Elgar's name nationally. The variation form was ideal for him at this stage of his career, when his comprehensive mastery of orchestration was still in contrast to his tendency to write his melodies in short, sometimes rigid, phrases. His next orchestral works, "Cockaigne", a concert-overture (1900–1901), the first two "Pomp and Circumstance" marches (1901), and the gentle "Dream Children" (1902), are all short: the longest of them, "Cockaigne", lasting less than fifteen minutes. "In the South" (1903–1904), although designated by Elgar as a concert-overture, is, according to Kennedy, really a tone poem and the longest continuous piece of purely orchestral writing Elgar had essayed. He wrote it after setting aside an early attempt to compose a symphony. The work reveals his continuing progress in writing sustained themes and orchestral lines, although some critics, including Kennedy, find that in the middle part "Elgar's inspiration burns at less than its brightest." In 1905 Elgar completed the "Introduction and Allegro for Strings". This work is based, unlike much of Elgar's earlier writing, not on a profusion of themes but on only three. Kennedy called it a "masterly composition, equalled among English works for strings only by Vaughan Williams's "Tallis Fantasia"." Nevertheless, at less than a quarter of an hour, it was not by contemporary standards a lengthy composition. Gustav Mahler's Seventh Symphony, composed at the same time, runs for well over an hour.

During the next four years, however, Elgar composed three major concert pieces, which, though shorter than comparable works by some of his European contemporaries, are among the most substantial such works by an English composer. These were his First Symphony, Violin Concerto, and Second Symphony, which all play for between forty-five minutes and an hour. McVeagh says of the symphonies that they "rank high not only in Elgar's output but in English musical history. Both are long and powerful, without published programmes, only hints and quotations to indicate some inward drama from which they derive their vitality and eloquence. Both are based on classical form but differ from it to the extent that ... they were considered prolix and slackly constructed by some critics. Certainly the invention in them is copious; each symphony would need several dozen music examples to chart its progress."
Elgar's Violin Concerto and Cello Concerto, in the view of Kennedy, "rank not only among his finest works, but among the greatest of their kind". They are, however, very different from each other. The Violin Concerto, composed in 1909 as Elgar reached the height of his popularity, and written for the instrument dearest to his heart, is lyrical throughout and rhapsodical and brilliant by turns. The Cello Concerto, composed a decade later, immediately after World War I, seems, in Kennedy's words, "to belong to another age, another world ... the simplest of all Elgar's major works ... also the least grandiloquent." Between the two concertos came Elgar's symphonic study "Falstaff", which has divided opinion even among Elgar's strongest admirers. Donald Tovey viewed it as "one of the immeasurably great things in music", with power "identical with Shakespeare's", while Kennedy criticises the work for "too frequent reliance on sequences" and an over-idealised depiction of the female characters. Reed thought that the principal themes show less distinction than some of Elgar's earlier works. Elgar himself thought "Falstaff" the highest point of his purely orchestral work.

The major works for voices and orchestra of the twenty-one years of Elgar's middle period are three large-scale works for soloists, chorus and orchestra: "The Dream of Gerontius" (1900), and the oratorios "The Apostles" (1903) and "The Kingdom" (1906); and two shorter odes, the "Coronation Ode" (1902) and "The Music Makers" (1912). The first of the odes, as a "pièce d'occasion", has rarely been revived after its initial success, with the culminating "Land of Hope and Glory". The second is, for Elgar, unusual in that it contains several quotations from his earlier works, as Richard Strauss quoted himself in "Ein Heldenleben". The choral works were all successful, although the first, "Gerontius", was and remains the best-loved and most performed. On the manuscript Elgar wrote, quoting John Ruskin, "This is the best of me; for the rest, I ate, and drank, and slept, loved and hated, like another. My life was as the vapour, and is not; but this I saw, and knew; this, if anything of mine, is worth your memory." All three of the large-scale works follow the traditional model with sections for soloists, chorus and both together. Elgar's distinctive orchestration, as well as his melodic inspiration, lifts them to a higher level than most of their British predecessors.

Elgar's other works of his middle period include incidental music for "Grania and Diarmid", a play by George Moore and W. B. Yeats (1901), and for "The Starlight Express", a play based on a story by Algernon Blackwood (1916). Of the former, Yeats called Elgar's music "wonderful in its heroic melancholy". Elgar also wrote a number of songs during his peak period, of which Reed observes, "it cannot be said that he enriched the vocal repertory to the same extent as he did that of the orchestra."

After the Cello Concerto, Elgar completed no more large-scale works. He made arrangements of works by Bach, Handel and Chopin, in distinctively Elgarian orchestration, and once again turned his youthful notebooks to use for the "Nursery Suite" (1931). His other compositions of this period have not held a place in the regular repertory. For most of the rest of the twentieth century, it was generally agreed that Elgar's creative impulse ceased after his wife's death. Anthony Payne's elaboration of the sketches for Elgar's Third Symphony led to a reconsideration of this supposition. Elgar left the opening of the symphony complete in full score, and those pages, along with others, show Elgar's orchestration changed markedly from the richness of his pre-war work. "The Gramophone" described the opening of the new work as something "thrilling ... unforgettably gaunt". Payne also subsequently produced a performing version of the sketches for a sixth "Pomp and Circumstance March", premiered at the Proms in August 2006. Elgar's sketches for a piano concerto dating from 1913 were elaborated by the composer Robert Walker and first performed in August 1997 by the pianist David Owen Norris. The realisation has since been extensively revised.

Views of Elgar's stature have varied in the decades since his music came to prominence at the beginning of the twentieth century. Richard Strauss, as noted, hailed Elgar as a progressive composer; even the hostile reviewer in "The Observer", unimpressed by the thematic material of the First Symphony in 1908, called the orchestration "magnificently modern". Hans Richter rated Elgar as "the greatest modern composer" in any country, and Richter's colleague Arthur Nikisch considered the First Symphony "a masterpiece of the first order" to be "justly ranked with the great symphonic models – Beethoven and Brahms." By contrast, the critic W. J. Turner, in the mid-twentieth century, wrote of Elgar's "Salvation Army symphonies," and Herbert von Karajan called the "Enigma Variations" "second-hand Brahms". Elgar's immense popularity was not long-lived. After the success of his First Symphony and Violin Concerto, his Second Symphony and Cello Concerto were politely received but without the earlier wild enthusiasm. His music was identified in the public mind with the Edwardian era, and after the First World War he no longer seemed a progressive or modern composer. In the early 1920s, even the First Symphony had only one London performance in more than three years. Henry Wood and younger conductors such as Boult, Sargent and Barbirolli championed Elgar's music, but in the recording catalogues and the concert programmes of the middle of the century his works were not well represented.

In 1924, the music scholar Edward J. Dent wrote an article for a German music journal in which he identified four features of Elgar's style that gave offence to a section of English opinion (namely, Dent indicated, the academic and snobbish section): "too emotional", "not quite free from vulgarity", "pompous", and "too deliberately noble in expression". This article was reprinted in 1930 and caused controversy. In the later years of the century there was, in Britain at least, a revival of interest in Elgar's music. The features that had offended austere taste in the inter-war years were seen from a different perspective. In 1955, the reference book "The Record Guide" wrote of the Edwardian background during the height of Elgar's career:
By the 1960s, a less severe view was being taken of the Edwardian era. In 1966 the critic Frank Howes wrote that Elgar reflected the last blaze of opulence, expansiveness and full-blooded life, before World War I swept so much away. In Howes's view, there was a touch of vulgarity in both the era and Elgar's music, but "a composer is entitled to be judged by posterity for his best work. ... Elgar is historically important for giving to English music a sense of the orchestra, for expressing what it felt like to be alive in the Edwardian age, for conferring on the world at least four unqualified masterpieces, and for thereby restoring England to the comity of musical nations."
In 1967 the critic and analyst David Cox considered the question of the supposed Englishness of Elgar's music. Cox noted that Elgar disliked folk-songs and never used them in his works, opting for an idiom that was essentially German, leavened by a lightness derived from French composers including Berlioz and Gounod. How then, asked Cox, could Elgar be "the most English of composers"? Cox found the answer in Elgar's own personality, which "could use the alien idioms in such a way as to make of them a vital form of expression that was his and his alone. And the personality that comes through in the music is English." This point about Elgar's transmuting his influences had been touched on before. In 1930 "The Times" wrote, "When Elgar's first symphony came out, someone attempted to prove that its main tune on which all depends was like the Grail theme in Parsifal. ... but the attempt fell flat because everyone else, including those who disliked the tune, had instantly recognized it as typically 'Elgarian', while the Grail theme is as typically Wagnerian." As for Elgar's "Englishness", his fellow-composers recognised it: Richard Strauss and Stravinsky made particular reference to it, and Sibelius called him, "the personification of the true English character in music ... a noble personality and a born aristocrat".

Among Elgar's admirers there is disagreement about which of his works are to be regarded as masterpieces. The "Enigma Variations" are generally counted among them. "The Dream of Gerontius" has also been given high praise by Elgarians, and the Cello Concerto is similarly rated. Many rate the Violin Concerto equally highly, but some do not. Sackville-West omitted it from the list of Elgar masterpieces in "The Record Guide", and in a long analytical article in "The Musical Quarterly", Daniel Gregory Mason criticised the first movement of the concerto for a "kind of sing-songiness ... as fatal to noble rhythm in music as it is in poetry." "Falstaff" also divides opinion. It has never been a great popular favourite, and Kennedy and Reed identify shortcomings in it. In a "Musical Times" 1957 centenary symposium on Elgar led by Vaughan Williams, by contrast, several contributors share Eric Blom's view that "Falstaff" is the greatest of all Elgar's works.

The two symphonies divide opinion even more sharply. Mason rates the Second poorly for its "over-obvious rhythmic scheme", but calls the First "Elgar's masterpiece. ... It is hard to see how any candid student can deny the greatness of this symphony." However, in the 1957 centenary symposium, several leading admirers of Elgar express reservations about one or both symphonies. In the same year, Roger Fiske wrote in "The Gramophone", "For some reason few people seem to like the two Elgar symphonies equally; each has its champions and often they are more than a little bored by the rival work." The critic John Warrack wrote, "There are no sadder pages in symphonic literature than the close of the First Symphony's Adagio, as horn and trombones twice softly intone a phrase of utter grief", whereas to Michael Kennedy, the movement is notable for its lack of anguished yearning and "angst" and is marked instead by a "benevolent tranquillity."

Despite the fluctuating critical assessment of the various works over the years, Elgar's major works taken as a whole have in the twenty-first century recovered strongly from their neglect in the 1950s. "The Record Guide" in 1955 could list only one currently available recording of the First Symphony, none of the Second, one of the Violin Concerto, two of the Cello Concerto, two of the "Enigma Variations", one of "Falstaff", and none of "The Dream of Gerontius". Since then there have been multiple recordings of all the major works. More than thirty recordings have been made of the First Symphony since 1955, for example, and more than a dozen of "The Dream of Gerontius". Similarly, in the concert hall, Elgar's works, after a period of neglect, are once again frequently programmed. The Elgar Society's website, in its diary of forthcoming performances, lists performances of Elgar's works by orchestras, soloists and conductors across Europe, North America and Australia.

Elgar was knighted in 1904, and in 1911 he was appointed a member of the Order of Merit. In 1920 he received the Cross of Commander of the Belgian Order of the Crown; in 1924 he was made Master of the King's Musick; the following year he received the Gold Medal of the Royal Philharmonic Society; and in 1928 he was appointed a Knight Commander of the Royal Victorian Order (KCVO). Between 1900 and 1931, Elgar received honorary degrees from the Universities of Cambridge, Durham, Leeds, Oxford, Yale (USA), Aberdeen, Western Pennsylvania (USA), Birmingham and London. Foreign academies of which he was made a member were Regia Accademia di Santa Cecilia, Rome; Accademia del Reale Istituto Musicale, Florence; Académie des Beaux Arts, Paris; Institut de France; and the American Academy. In 1931 he was created a Baronet, of Broadheath in the County of Worcester. In 1933 he was promoted within the Royal Victorian Order to Knight Grand Cross (GCVO). In Kennedy's words, he "shamelessly touted" for a peerage, but in vain. In "Who's Who", post World War I, he claimed to have been awarded "several Imperial Russian and German decorations (lapsed)".

Elgar was offered, but declined, the office of Mayor of Hereford (despite not being a member of its city council) when he lived in the city in 1905. The same year he was made an honorary Freeman of the city of Worcester.

The house in Lower Broadheath where Elgar was born is now the Elgar Birthplace Museum, devoted to his life and work. Elgar's daughter, Carice, helped to found the museum in 1936 and bequeathed to it much of her collection of Elgar's letters and documents on her death in 1970. Carice left Elgar manuscripts to musical colleges: "The Black Knight" to Trinity College of Music; "King Olaf" to the Royal Academy of Music; "The Music Makers" to Birmingham University; the Cello Concerto to the Royal College of Music; "The Kingdom" to the Bodleian Library; and other manuscripts to the British Museum. The Elgar Society dedicated to the composer and his works was formed in 1951. The University of Birmingham's Special Collections contain an archive of letters written by Elgar.

Elgar's statue at the end of Worcester High Street stands facing the cathedral, only yards from where his father's shop once stood. Another statue of the composer by Rose Garrard is at the top of Church Street in Malvern, overlooking the town and giving visitors an opportunity to stand next to the composer in the shadow of the Hills that he so often regarded. In September 2005, a third statue sculpted by Jemma Pearson was unveiled near Hereford Cathedral in honour of his many musical and other associations with the city. It depicts Elgar with his bicycle. From 1999 until early 2007, new Bank of England twenty pound notes featured a portrait of Elgar. The change to remove his image generated controversy, particularly because 2007 was the 150th anniversary of Elgar's birth. From 2007 the Elgar notes were phased out, ceasing to be legal tender on 30 June 2010.

There are around 65 roads in the UK named after Elgar, including six in the counties of Herefordshire and Worcestershire. Elgar had three locomotives named in his honour. 
Elgar's life and music have inspired works of literature including the novel "Gerontius" and several plays. "Elgar's Rondo", a 1993 stage play by David Pownall depicts the dead Jaeger offering ghostly advice on Elgar's musical development. Pownall also wrote a radio play, "Elgar's Third" (1994); another Elgar-themed radio play is Alick Rowe's "The Dorabella Variation" (2003). David Rudkin's BBC television "Play for Today" "Penda's Fen" (1974) deals with themes including sex and adolescence, spying, and snobbery, with Elgar's music, chiefly "The Dream of Gerontius", as its background. In one scene, a ghostly Elgar whispers the secret of the "Enigma" tune to the youthful central character, with an injunction not to reveal it. "Elgar on the Journey to Hanley", a novel by Keith Alldritt (1979), tells of the composer's attachment to Dora Penny, later Mrs Powell, (depicted as "Dorabella" in the "Enigma Variations"), and covers the fifteen years from their first meeting in the mid-1890s to the genesis of the Violin Concerto when, in the novel, Dora has been supplanted in Elgar's affections by Alice Stuart-Wortley.

Perhaps the best-known work depicting Elgar is Ken Russell's 1962 BBC television film "Elgar", made when the composer was still largely out of fashion. This hour-long film contradicted the view of Elgar as a jingoistic and bombastic composer, and evoked the more pastoral and melancholy side of his character and music.

The following have been selected as representative of Elgar's works, based on quality, significance and popularity.










Notes

References


</doc>
<doc id="10086" url="https://en.wikipedia.org/wiki?curid=10086" title="European Investment Fund">
European Investment Fund

The European Investment Fund (EIF), established in 1994, is a European Union agency for the provision of finance to SMEs (small and medium-sized enterprises), headquartered in Luxembourg.

It does not lend money to SMEs directly; rather it provides finance through private banks and funds. Its main operations are in the areas of venture capital and guaranteeing loans. Its shareholders are: the European Investment Bank (62%); the European Union, represented by the European Commission (29%); and 30 privately owned EU financial institutions (9%).




</doc>
<doc id="10087" url="https://en.wikipedia.org/wiki?curid=10087" title="European Currency Unit">
European Currency Unit

The European Currency Unit (₠ or ECU, ) was a basket of the currencies of the European Community member states, used as the unit of account of the European Community before being replaced by the euro on 1 January 1999, at parity. The ECU itself replaced the European Unit of Account, also at parity, on 13 March 1979. The European Exchange Rate Mechanism attempted to minimize fluctuations between member state currencies and the ECU. The ECU was also used in some international financial transactions, where its advantage was that securities denominated in ECUs provided investors with the opportunity for foreign diversification without reliance on the currency of a single country.

The ECU was conceived on 13 March 1979 as an internal accounting unit. It had the ISO 4217 currency code XEU.

On 1 January 1999, the euro (with the code EUR and symbol €) replaced the ECU, at the value €1 = 1 ECU. Unlike the ECU, the euro is a real currency, although not all member states participate (for details on euro membership see Eurozone). Two of the countries in the ECU basket of currencies, UK and Denmark, did not join the eurozone, and a third, Greece, joined late. On the other hand, Finland and Austria joined the eurozone from the beginning although their currencies were not part of the ECU basket (since they had joined the EU in 1995, two years after the ECU composition was "frozen")

Due to the ECU being used in some international financial transactions, there was a concern that foreign courts might not recognize the euro as the legal successor to the ECU. This was unlikely to be a problem, since it is a generally accepted principle of private international law that states determine their currencies, and that therefore states would accept the European Union legislation to that effect. However, for abundant caution, several foreign jurisdictions adopted legislation to ensure a smooth transition. Of particular importance, the U.S. states of Illinois and New York adopted legislation to ensure a large proportion of international financial contracts recognized the euro as the successor of the ECU.

Although the acronym ECU is formed from English words, "écu" is also the name of an ancient French coin. That was one reason that a new name was devised for its successor currency, "euro", which was felt not to favour any single language.

The currency's symbol, ₠ (U+20A0), comprises an interlaced C and E, which are the initial letters of the phrase 'European Community' in many European languages. However, this symbol was not widely used: few systems at the time could render it and in any case banks preferred (as with all currencies) to use the ISO code XEU.

As the ECU was only an electronic unit of account and not a full currency, it did not have any official coins or notes that could be used for everyday transactions. However, various European countries and organisations like the European Parliament made commemorative and mock-up coins and notes. A common theme on the coins was usually celebrating European unity, such as celebrating membership of the European Union. In 1989, the government of the Netherlands issued a series of ECU coins from ₠2½ to ₠200, which could be spent in shops in The Hague, during the European Capital of Culture festival. Gibraltar issued commemorative coins from 1993 through 1996.




</doc>
<doc id="10088" url="https://en.wikipedia.org/wiki?curid=10088" title="Eastern Caribbean dollar">
Eastern Caribbean dollar

The Eastern Caribbean dollar (symbol: $; code: XCD) is the currency of all seven full members and one associate member of the Organisation of Eastern Caribbean States (OECS). It has existed since 1965, being the successor to the British West Indies dollar, and it is normally abbreviated with the dollar sign "$" or, alternatively, "EC$" to distinguish it from other dollar-denominated currencies. The EC$ is subdivided into 100 cents. It has been pegged to the United States dollar since 7 July 1976, and the exchange rate is US$1 = EC$2.70.

Six of the states using the EC$ are independent states: Antigua and Barbuda, Dominica, Grenada, Saint Kitts and Nevis, Saint Lucia, and Saint Vincent and the Grenadines. The other two are British overseas territories: Anguilla and Montserrat. These states are all members of the Eastern Caribbean Currency Union. 

The other two associate members of the OECS do not use the Eastern Caribbean dollar as their official currency: the British Virgin Islands and Martinique. The British Virgin Islands were always problematic for currency purposes due to their proximity to the Danish West Indies which became the US Virgin Islands in 1917. Officially, the British Virgin Islands used to use sterling, but in practice the situation was more complicated and involved the circulation of francs and U.S. dollars. In 1951, the British Virgin Islands adopted the British West Indies dollar which at that time operated in conjunction with the sterling coinage, and in 1959 they changed over officially to the U.S. dollar.

Martinique, as part of France, uses the euro as its currency. 

British Guiana and Barbados had previously been members of the Eastern Caribbean Currency Union but withdrew in 1966 and 1972, respectively. Trinidad and Tobago had been a member of the earlier British West Indies currency union, but withdrew in 1964. 

The combined population of the EC$ area is about 613,000 (2014 census and estimates), which is comparable to Montenegro or the American capital city of Washington, D.C.. The combined GDP is 5.46 billion US dollars, which is comparable to Bermuda.

Queen Elizabeth II appears on the banknotes and also on the obverse of the coins. She is the head of state of all the states and territories using the EC$, except for Dominica. Dominica is nevertheless a member of the Commonwealth of Nations which recognises Queen Elizabeth II as Head of the Commonwealth.

Queen Anne's proclamation of 1704 introduced the gold standard to the British West Indies, putting the West Indies about two hundred years ahead of the East Indies in this respect. Nevertheless, silver pieces of eight continued to form an important portion of the circulating coinage right up until the late 1870s. In 1822, the British government coined 1/4, 1/8, and 1/16 fractional 'Anchor dollars' for use in Mauritius and the British West Indies (but not Jamaica). A few years later copper fractional dollars were coined for Mauritius, Sierra Leone, and the British West Indies.

The first move to introduce British sterling silver coinage to the colonies came with an imperial order-in-council dated 1825. This move was inspired by a number of factors. The United Kingdom was now operating a very successful gold standard in relation to the gold sovereign that was introduced in 1816, and there was a desire to extend this system to the colonies. In addition to this, there was the fact that the supply of Spanish dollars (pieces of eight) had been cut off as a result of the revolutions in Latin America where most of the Spanish dollars were minted. The last Spanish Dollar was in fact minted at Potosi in 1825. There was now a growing desire to have a stable and steady supply of British shillings everywhere the British drum was beating. The 1825 order-in-council was largely a failure because it made sterling silver coinage legal tender at the unrealistic rating in relation to the Spanish dollar of $1 = 4 shillings and 4 pence. Interestingly it did succeed in Jamaica, Bermuda, and British Honduras because the authorities in those territories set aside the official ratings and used the more realistic rating of $1 = 4 shillings. The reality of the rating between the dollar and the pound was based on the silver content of the Spanish pieces of eight as compared to the gold content of the British gold sovereign.

A second imperial order-in-council was passed in 1838 with the correct rating of $1 = 4 shillings 2 pence. In the years following the 1838 order-in-council, the British West Indies territories began to enact local legislation for the purposes of assimilating their monies of account with the British pound sterling. Gold discoveries in Australia in 1851 drove the silver dollar out of the West Indies, but it returned again with the great depreciation in the value of silver that followed with Germany's transition to the gold standard between 1871 and 1873. In the years immediately following 1873, there was a fear that the British West Indies might return to a silver standard. As such, legislation was passed in the individual territories to demonetize the silver dollars. Even though the British coinage was also silver, it represented fractions of the gold sovereign and so its value was based on a gold standard.

During this period, and into the nineteenth century, accounts could be kept in either dollars or sterling. Jamaica, Bermuda, and the Bahamas preferred to use sterling accounts whereas British Guiana used dollar accounts. British Guiana used dollar accounts for the purpose of assisting in the transition from the Dutch guilder system of currency to the British pound sterling system. In the Eastern Caribbean territories the private sector preferred to use dollar accounts whereas the government preferred to use sterling accounts. In some of the Eastern Caribbean territories, notes were issued by various private banks, denominated in dollars equivalent to 4 shillings 2 pence. See Antigua dollar, Barbadian dollar, Dominican dollar, Grenadian dollar, Guyanese dollar, Saint Kitts dollar, Saint Lucia dollar, Saint Vincent dollar and Trinidad and Tobago dollar.

In 1946, a West Indian Currency Conference saw Barbados, British Guiana, the Leeward Islands, Trinidad and Tobago and the Windward Islands agree to establish a unified decimal currency system based on a West Indian dollar to replace the current arrangement of having three different Boards of Commissioners of Currency (for Barbados (which also served the Leeward and Windward Islands), British Guiana and Trinidad & Tobago).

In 1949, the British government formalized the dollar system of accounts in British Guiana and the Eastern Caribbean territories by introducing the British West Indies dollar (BWI$) at the already existing conversion rate of $4.80 per pound sterling (or $1 = 4 shillings 2 pence). It was one of the many experimental political and economic ventures tested by the British government to form a uniform system within their British West Indies territories. The ISO 4217 code of the currency was "XBWD". The symbol "BWI$" for frequently used and the currency was known verbally as the "Beewee" (slang for British West Indies) dollar. Shortly thereafter in the 1950, the British Caribbean Currency Board (BCCB) was set up in Trinidad with the sole right to issue notes and coins of the new unified currency and given the mandate of keeping full foreign exchange cover to ensure convertibility at $4.80 per pound sterling. In 1951, the British Virgin Islands joined the arrangement, but this led to discontent because that territory was more naturally drawn to the currency of the neighbouring U.S. Virgin Islands. In 1961, the British Virgin Islands withdrew from the arrangement and adopted the U.S. dollar.

Until 1955, the BWI$ existed only as banknotes in conjunction with sterling fractional coinage. Decimal coins replaced the sterling coins in 1955. These decimal coins were denominated in cents, with each cent being worth one halfpenny in sterling.

In 1958, the West Indies Federation was established and the BWI$ was its currency. However, although Jamaica (including the Cayman Islands and the Turks and Caicos Islands) was part of the West Indies Federation, it retained the Jamaican pound, despite adopting the BWI$ as legal tender from 1954. Jamaica, the Cayman Islands, and the Turks and Caicos Islands were already long established users of the sterling accounts system of pounds, shillings, and pence.

In 1964 Jamaica ended the legal tender status of the BWI$ and Trinidad and Tobago withdrew from the currency union (adopting the Trinidad and Tobago dollar) forcing the movement of the headquarters of the BCCB to Barbados and soon the "BWI$" dollar lost its regional support.

In 1965, the British West Indies dollar of the now defunct West Indies Federation was replaced at par by the Eastern Caribbean dollar and the BCCB was replaced by the Eastern Caribbean Currency Authority or ECCA (established by the Eastern Caribbean Currency Agreement 1965). British Guiana withdrew from the currency union the following year. Grenada rejoined the common currency arrangement in 1968 having used the Trinidad and Tobago dollar from 1964. Barbados withdrew from the currency union in 1972, following which the ECCA headquarters were moved to St. Kitts.

Between 1965 and 1983, the Eastern Caribbean Currency Authority issued the EC$, with banknotes from 1965 and coins from 1981. The EC$ is now issued by the Eastern Caribbean Central Bank, based in the city of Basseterre, in Saint Kitts and Nevis. The bank was established by an agreement (the Eastern Caribbean Central Bank Agreement) signed at Port of Spain on 5 July 1983.

The exchange rate of $4.80 = £1 sterling (equivalent to the old $1 = 4s 2d) continued right up until 1976 for the new Eastern Caribbean dollar.

For a wider outline of the history of currency in the region see Currencies of the British West Indies.

Until 1981, the coins of the BWI$ circulated. In 1982, a new series of coins was introduced in denominations of 1, 2, 5, 10 and 25 cents and 1 dollar. The 1- and 5-cent coins were scalloped in shape while the 2-cent coin was square. These three were struck in aluminum. The 10- and 25-cent coins were round and cupro-nickel. The dollar was aluminum bronze and also round. The round, aluminum bronze dollar coin was replaced in 1989 with a decagonal, cupro-nickel type. In 2002 new and larger round-shaped 1-, 2-, and 5-cent pieces were introduced, along with a new 1-dollar coin which was also round. The effigy of Queen Elizabeth II was also changed that same year on all coin denominations to the Ian Rank-Broadley design, making it the last commonwealth currency up to that date to discontinue the Arnold Machin portrait. Their compositions remained aluminum and cupro-nickel, respectively. Higher denominations exist, but these were issued only as medal-coins. 1- and 2-cent coins were withdrawn from circulation in July 2015, and will remain legal tender until 30 June 2020.

In 1965, the Eastern Caribbean Currency Authority issued banknotes in denominations of 1, 5, 20 and 100 dollars, all featuring Pietro Annigoni’s 1956 portrait of Queen Elizabeth II in regalia of Order of the Garter. The first issues in the name of the Eastern Caribbean Central Bank in 1985 were of the same denominations, with the addition of 10-dollar notes. The last 1-dollar notes were issued in 1989 and 50-dollar notes were introduced in 1993. On 1 April 2008, the Eastern Caribbean Central Bank issued a new series of banknotes which are like the preceding issues, except for omitting both the barcode and the country code letters which form part of the serial number on current notes. In 2012, the Eastern Caribbean Central Bank issued a series of banknotes with Braille features in an effort to provide notes which are easier for blind and visually impaired persons to use. The raised Braille characters on the upgraded notes feature a cricket theme in the form of balls and stumps. These characters have been added to the 10-, 20-, 50-, and 100-dollar notes.




</doc>
<doc id="10090" url="https://en.wikipedia.org/wiki?curid=10090" title="Erythromycin">
Erythromycin

Erythromycin is an antibiotic useful for the treatment of a number of bacterial infections. This includes respiratory tract infections, skin infections, chlamydia infections, pelvic inflammatory disease, and syphilis. It may also be used during pregnancy to prevent Group B streptococcal infection in the newborn. Erythromycin may be used to improve delayed stomach emptying. It can be given intravenously and by mouth. An eye ointment is routinely recommended after delivery to prevent eye infections in the newborn.
Common side effects include abdominal cramps, vomiting, and diarrhea. More serious side effects may include "Clostridium difficile" colitis, liver problems, prolonged QT, and allergic reactions. It is generally safe in those who are allergic to penicillin. Erythromycin also appears to be safe to use during pregnancy. While generally regarded as safe during breastfeeding its use by the mother during the first two weeks of life may increase the risk of pyloric stenosis in the baby. This risk also applies if taken directly by the baby during this age. It is in the macrolide family and works by decreasing bacterial protein production.
Erythromycin was first isolated in 1952 from the bacteria "Saccharopolyspora erythraea". It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. It is available as a generic medication and is not very expensive. The wholesale price in the developing world is between 0.03 and 0.06 USD per tablet.

Erythromycin can be used to treat bacteria responsible for causing infections of the skin and upper respiratory tract, including "Streptococcus", "Staphylococcus", "Haemophilus" and "Corynebacterium" genera. The following represents MIC susceptibility data for a few medically significant bacteria:

Erythromycin is available in enteric-coated tablets, slow-release capsules, oral suspensions, ophthalmic solutions, ointments, gels, enteric-coated capsules, non enteric-coated tablets, non enteric-coated capsules, and injections.
The following erythromycin combinations are available for oral dosage:
For injection the available combinations are:

For ophthalmic use

Gastrointestinal disturbances, such as diarrhea, nausea, abdominal pain, and vomiting, are very common because erythromycin is a motilin agonist. Because of this, erythromycin tends not to be prescribed as a first-line drug. It may be useful in treating gastroparesis due to this promotility effect. Intravenous erythromycin may also be used in endoscopy as an adjunct to clear gastric contents.

More serious side effects include arrhythmia with prolonged QT intervals including "torsades de pointes", and reversible deafness. Allergic reactions range from urticaria to anaphylaxis. Cholestasis, Stevens–Johnson syndrome, and toxic epidermal necrolysis are some other rare side effects that may occur.

Studies have shown evidence both for and against the association of pyloric stenosis and exposure to erythromycin prenatally and postnatally. Exposure to erythromycin (especially long courses at antimicrobial doses, and also through breastfeeding) has been linked to an increased probability of pyloric stenosis in young infants. Erythromycin used for feeding intolerance in young infants has not been associated with hypertrophic pyloric stenosis.

Erythromycin estolate has been associated with reversible hepatotoxicity in pregnant women in the form of elevated serum glutamic-oxaloacetic transaminase and is not recommended during pregnancy. Some evidence suggests similar hepatotoxicity in other populations.

It can also affect the central nervous system, causing psychotic reactions, nightmares and night sweats.

It may also alter the effectiveness of combined oral contraceptive pills because of its effect on the gut flora. Erythromycin is an inhibitor of the cytochrome P450 system, which means it can have a rapid effect on levels of other drugs metabolised by this system, e.g., warfarin.

Erythromycin is metabolized by enzymes of the cytochrome P450 system, in particular, by isozymes of the CYP3A superfamily, CYP3A. The activity of the CYP3A enzymes can be induced or inhibited by certain drugs (e.g. dexamethasone) which can cause it to affect the metabolism of many different drugs, e.g. erythromycin. If other CYP3A substrates — drugs that are broken down by CYP3A — such as simvastatin (Zocor), lovastatin (Mevacor), or atorvastatin (Lipitor)—are taken concomitantly with erythromycin, levels of the substrates increase, often causing adverse effects. A noted drug interaction involves erythromycin and simvastatin, resulting in increased simvastatin levels and the potential for rhabdomyolysis. Another group of CYP3A4 substrates are drugs used for migraine such as ergotamine and dihydroergotamine; their adverse effects may be more pronounced if erythromycin is associated.
Earlier case reports on sudden death prompted a study on a large cohort that confirmed a link between erythromycin, ventricular tachycardia, and sudden cardiac death in patients also taking drugs that prolong the metabolism of erythromycin (like verapamil or diltiazem) by interfering with CYP3A4. Hence, erythromycin should not be administered to people using these drugs, or drugs that also prolong the QT interval. Other examples include terfenadine (Seldane, Seldane-D), astemizole (Hismanal), cisapride (Propulsid, withdrawn in many countries for prolonging the QT time) and pimozide (Orap). Theophylline, which is used mostly in asthma, is also contraindicated.

Erythromycin and doxycycline can have a synergistic effect when combined and kill bacteria ("E. coli)" with a higher potency than the sum of the two drugs together. This synergistic relationship is only temporary. After approximately 72 hours, the relationship shifts to become antagonistic, whereby a 50/50 combination of the two drugs kills less bacteria than if the two drugs were administered separately.

Erythromycin displays bacteriostatic activity or inhibits growth of bacteria, especially at higher concentrations, but the mechanism is not fully understood. By binding to the 50s subunit of the bacterial rRNA complex, protein synthesis and subsequent structure and function processes critical for life or replication are inhibited. Erythromycin interferes with aminoacyl translocation, preventing the transfer of the tRNA bound at the A site of the rRNA complex to the P site of the rRNA complex. Without this translocation, the A site remains occupied, thus the addition of an incoming tRNA and its attached amino acid to the nascent polypeptide chain is inhibited. This interferes with the production of functionally useful proteins, which is the basis of this antimicrobial action.

Erythromycin increases gut motility by binding to Motillin, thus it is a Motillin receptor agonist in addition to its antimicrobial properties.

Erythromycin is easily inactivated by gastric acid; therefore, all orally administered formulations are given as either enteric-coated or more-stable salts or esters, such as erythromycin ethylsuccinate. Erythromycin is very rapidly absorbed, and diffuses into most tissues and phagocytes. Due to the high concentration in phagocytes, erythromycin is actively transported to the site of infection, where, during active phagocytosis, large concentrations of erythromycin are released.

Most of erythromycin is metabolised by demethylation in the liver by the hepatic enzyme CYP3A4. Its main elimination route is in the bile with little renal excretion, 2%-15% unchanged drug. Erythromycin's elimination half-life ranges between 1.5 and 2.0 hours and is between 5 and 6 hours in patients with end-stage renal disease. Erythromycin levels peak in the serum 4 hours after dosing; ethylsuccinate peaks 0.5-2.5 hours after dosing, but can be delayed if digested with food.

Erythromycin crosses the placenta and enters breast milk. The American Association of Pediatrics determined erythromycin is safe to take while breastfeeding. Absorption in pregnant patients has been shown to be variable, frequently resulting in levels lower than in nonpregnant patients.

Standard-grade erythromycin is primarily composed of four related compounds known as erythromycins A, B, C, and D. Each of these compounds can be present in varying amounts and can differ by lot. Erythromycin A has been found to have the most antibacterial activity, followed by erythromycin B. Erythromycins C and D are about half as active as erythromycin A. Some of these related compounds have been purified and can be studied and researched individually.

Over the three decades after the discovery of erythromycin A and its activity as an antimicrobial, many attempts were made to synthesize it in the laboratory. The presence of 10 stereospecific carbons and several points of distinct substitution has made the total synthesis of erythromycin A a formidable task. Complete syntheses of erythromycins’ related structures and precursors such as 6-deoxyerythronolide B have been accomplished, giving way to possible syntheses of different erythromycins and other macrolide antimicrobials. Woodward successfully completed the synthesis of erythromycin A.

Erythromycin was discovered by Abelardo Aguilar when working for the pharmaceutical company Eli Lilly and Company as a researcher.

In 1949 Abelardo B. Aguilar, a Filipino scientist, sent some soil samples to his employer Eli Lilly . Eli Lilly’s research team, led by J. M. McGuire, managed to isolate erythromycin from the metabolic products of a strain of "Streptomyces erythreus" (designation changed to ""Saccharopolyspora erythraea"") found in the samples.

Lilly filed for patent protection of the compound and U.S. patent 2,653,899 was granted in 1953. The product was launched commercially in 1952 under the brand name Ilosone (after the Philippine region of Iloilo where it was originally collected). Erythromycin was formerly also called Ilotycin.

In 1981, Nobel laureate (1965 in chemistry) and professor of chemistry at Harvard University Robert B. Woodward (posthumously), along with a large number of members from his research group, reported the first stereocontrolled asymmetric chemical synthesis of erythromycin A.

The antibiotic clarithromycin was invented by scientists at the Japanese drug company Taisho Pharmaceutical in the 1970s as a result of their efforts to overcome the acid instability of erythromycin.

Scientists at Chugai Pharmaceuticals discovered an erythromycin-derived motilin agonist called mitemcinal that is believed to have strong prokinetic properties (similar to erythromycin) but lacking antibiotic properties. Erythromycin is commonly used off-label for gastric motility indications such as gastroparesis. If mitemcinal can be shown to be an effective prokinetic agent, it would represent a significant advance in the gastrointestinal field, as treatment with this drug would not carry the risk of unintentional selection for antibiotic-resistant bacteria.

It is available as a generic medication and is inexpensive. The wholesale price is between 0.03 and 0.06 USD per pill.

In the United States in 2014 the price increased to seven dollars per tablet.

Brand names include Robimycin, E-Mycin, E.E.S. Granules, E.E.S.-200, E.E.S.-400, E.E.S.-400 Filmtab, Erymax, Ery-Tab, Eryc, Ranbaxy, Erypar, EryPed, Eryped 200, Eryped 400, Erythrocin Stearate Filmtab, Erythrocot, E-Base, Erythroped, Ilosone, MY-E, Pediamycin, Zineryt, Abboticin, Abboticin-ES, Erycin, PCE Dispertab, Stiemycine, Acnasol, and Tiloryth.

Erythromycin/tretinoin, a combination of tretinoin and the antibiotic erythromycin



</doc>
<doc id="10091" url="https://en.wikipedia.org/wiki?curid=10091" title="Environmental law">
Environmental law

Environmental law, also known as environmental and natural resources law, is a collective term describing the network of treaties, statutes, regulations, common and customary laws addressing the effects of human activity on the natural environment. The core environmental law regimes address environmental pollution. A related but distinct set of regulatory regimes, now strongly influenced by environmental legal principles, focus on the management of specific natural resources, such as forests, minerals, or fisheries. Other areas, such as environmental impact assessment, may not fit neatly into either category, but are nonetheless important components of environmental law.<section end=scope />

Early examples of legal enactments designed to consciously preserve the environment, for its own sake or human enjoyment, are found throughout history. In the common law, the primary protection was found in the law of nuisance, but this only allowed for private actions for damages or injunctions if there was harm to land. Thus smells emanating from pig sties, strict liability against dumping rubbish, or damage from exploding dams. Private enforcement, however, was limited and found to be woefully inadequate to deal with major environmental threats, particularly threats to common resources. During the "Great Stink" of 1858, the dumping of sewerage into the River Thames began to smell so ghastly in the summer heat that Parliament had to be evacuated. Ironically, the Metropolitan Commission of Sewers Act 1848 had allowed the Metropolitan Commission for Sewers to close cesspits around the city in an attempt to "clean up" but this simply led people to pollute the river. In 19 days, Parliament passed a further Act to build the London sewerage system. London also suffered from terrible air pollution, and this culminated in the "Great Smog" of 1952, which in turn triggered its own legislative response: the Clean Air Act 1956. The basic regulatory structure was to set limits on emissions for households and business (particularly burning coal) while an inspectorate would enforce compliance.

Notwithstanding early analogues, the concept of "environmental law" as a separate and distinct body of law is a twentieth-century development. The recognition that the natural environment was fragile and in need of special legal protections, the translation of that recognition into legal structures, the development of those structures into a larger body of "environmental law," and the strong influence of environmental law on natural resource laws, did not occur until about the 1960s. At that time, numerous influences - including a growing awareness of the unity and fragility of the biosphere; increased public concern over the impact of industrial activity on natural resources and human health; the increasing strength of the regulatory state; and more broadly the advent and success of environmentalism as a political movement - coalesced to produce a huge new body of law in a relatively short period of time. While the modern history of environmental law is one of continuing controversy, by the end of the twentieth century environmental law had been established as a component of the legal landscape in all developed nations of the world, many developing ones, and the larger project of international law.

Water quality laws 

Waste management laws 

Environmental cleanup laws 

Chemical safety laws govern the use of chemicals in human activities, particularly man-made chemicals in modern industrial applications. As contrasted with media-oriented environmental laws (e.g., air or water quality laws), chemical control laws seek to manage the (potential) pollutants themselves. Regulatory efforts include banning specific chemical constituents in consumer products (e.g., Bisphenol A in plastic bottles), and regulating pesticides.

Environmental impact assessment 

Water resources laws govern the ownership and use of water resources, including surface water and ground water. Regulatory areas may include water conservation, use restrictions, and ownership regimes.

Mineral resource laws cover 

Wildlife laws govern the potential impact of human activity on wild animals, whether directly on individuals or populations, or indirectly via habitat degradation. Similar laws may operate to protect plant species. Such laws may be enacted entirely to protect biodiversity, or as a means for protecting species deemed important for other reasons. Regulatory efforts may including the creation of special conservation statuses, prohibitions on killing, harming, or disturbing protected species, efforts to induce and support species recovery, establishment of wildlife refuges to support conservation, and prohibitions on trafficking in species or animal parts to combat poaching.

Fish and game laws regulate the right to pursue and take or kill certain kinds of fish and wild animal (game). Such laws may restrict the days to harvest fish or game, the number of animals caught per person, the species harvested, or the weapons or fishing gear used. Such laws may seek to balance dueling needs for preservation and harvest and to manage both environment and populations of fish and game. Game laws can provide a legal structure to collect license fees and other money which is used to fund conservation efforts as well as to obtain harvest information used in wildlife management practice.

Environmental law has developed in response to emerging awareness of and concern over issues impacting the entire world. While laws have developed piecemeal and for a variety of reasons, some effort has gone into identifying key concepts and guiding principles common to environmental law as a whole. The principles discussed below are not an exhaustive list and are not universally recognized or accepted. Nonetheless, they represent important principles for the understanding of environmental law around the world.

Defined by the United Nations Environment Programme as "development that meets the needs of the present without compromising the ability of future generations to meet their own needs," sustainable development may be considered together with the concepts of "integration" (development cannot be considered in isolation from sustainability) and "interdependence" (social and economic development, and environmental protection, are interdependent). Laws mandating environmental impact assessment and requiring or encouraging development to minimize environmental impacts may be assessed against this principle.

The modern concept of sustainable development was a topic of discussion at the 1972 United Nations Conference on the Human Environment (Stockholm Conference), and the driving force behind the 1983 World Commission on Environment and Development (WCED, or Bruntland Commission). In 1992, the first UN Earth Summit resulted in the Rio Declaration, Principle 3 of which reads: "The right to development must be fulfilled so as to equitably meet developmental and environmental needs of present and future generations." Sustainable development has been a core concept of international environmental discussion ever since, including at the World Summit on Sustainable Development (Earth Summit 2002), and the United Nations Conference on Sustainable Development (Earth Summit 2012, or Rio+20).

Defined by UNEP to include intergenerational equity - "the right of future generations to enjoy a fair level of the common patrimony" - and intragenerational equity - "the right of all people within the current generation to fair access to the current generation's entitlement to the Earth's natural resources" - environmental equity considers the present generation under an obligation to account for long-term impacts of activities, and to act to sustain the global environment and resource base for future generations. Pollution control and resource management laws may be assessed against this principle.

Defined in the international law context as an obligation to protect one's own environment, and to prevent damage to neighboring environments, UNEP considers transboundary responsibility at the international level as a potential limitation on the rights of the sovereign state. Laws that act to limit externalities imposed upon human health and the environment may be assessed against this principle.

Identified as essential conditions for "accountable governments... industrial concerns," and organizations generally, public participation and transparency are presented by UNEP as requiring "effective protection of the human right to hold and express opinions and to seek, receive and impart ideas... a right of access to appropriate, comprehensible and timely information held by governments and industrial concerns on economic and social policies regarding the sustainable use of natural resources and the protection of the environment, without imposing undue financial burdens upon the applicants and with adequate protection of privacy and business confidentiality," and "effective judicial and administrative proceedings." These principles are present in environmental impact assessment, laws requiring publication and access to relevant environmental data, and administrative procedure.

One of the most commonly encountered and controversial principles of environmental law, the Rio Declaration formulated the precautionary principle as follows:

The principle may play a role in any debate over the need for environmental regulation.

The polluter pays principle stands for the idea that "the environmental costs of economic activities, including the cost of preventing potential harm, should be internalized rather than imposed upon society at large." All issues related to responsibility for cost for environmental remediation and compliance with pollution control regulations involve this principle.

Environmental law is a continuing source of controversy. Debates over the necessity, fairness, and cost of environmental regulation are ongoing, as well as regarding the appropriateness of regulations vs. market solutions to achieve even agreed-upon ends.

Allegations of scientific uncertainty fuel the ongoing debate over greenhouse gas regulation, and are a major factor in debates over whether to ban particular pesticides. In cases where the science is well-settled, it is not unusual to find that corporations intentionally hide or distort the facts, or sow confusion.

It is very common for regulated industry to argue against environmental regulation on the basis of cost. Difficulties arise in performing cost-benefit analysis of environmental issues. It is difficult to quantify the value of an environmental value such as a healthy ecosystem, clean air, or species diversity. Many environmentalists' response to pitting economy vs. ecology is summed up by former Senator and founder of Earth Day Gaylord Nelson, "The economy is a wholly owned subsidiary of the environment, not the other way around." Furthermore, environmental issues are seen by many as having an ethical or moral dimension, which would transcend financial cost. Even so, there are some efforts underway to systemically recognize environmental costs and assets, and account for them properly in economic terms.

While affected industries spark controversy in fighting regulation, there are also many environmentalists and public interest groups who believe that current regulations are inadequate, and advocate for stronger protection. Environmental law conferences - such as the annual Public Interest Environmental Law Conference in Eugene, Oregon - typically have this focus, also connecting environmental law with class, race, and other issues.

An additional debate is to what extent environmental laws are fair to all regulated parties. For instance, researchers Preston Teeter and Jorgen Sandberg highlight how smaller organizations can often incur disproportionately larger costs as a result of environmental regulations, which can ultimately create an additional barrier to entry for new firms, thus stifling competition and innovation.

Global and regional environmental issues are increasingly the subject of international law. Debates over environmental concerns implicate core principles of international law and have been the subject of numerous international agreements and declarations.

Customary international law is an important source of international environmental law. These are the norms and rules that countries follow as a matter of custom and they are so prevalent that they bind all states in the world. When a principle becomes customary law is not clear cut and many arguments are put forward by states not wishing to be bound. Examples of customary international law relevant to the environment include the duty to warn other states promptly about icons of an environmental nature and environmental damages to which another state or states may be exposed, and Principle 21 of the Stockholm Declaration ('good neighbourliness' or sic utere).

Numerous legally binding international agreements encompass a wide variety of issue-areas, from terrestrial, marine and atmospheric pollution through to wildlife and biodiversity protection. International environmental agreements are generally multilateral (or sometimes bilateral) treaties (a.k.a. convention, agreement, protocol, etc.). Protocols are subsidiary agreements built from a primary treaty. They exist in many areas of international law but are especially useful in the environmental field, where they may be used to regularly incorporate recent scientific knowledge. They also permit countries to reach agreement on a framework that would be contentious if every detail were to be agreed upon in advance. The most widely known protocol in international environmental law is the Kyoto Protocol, which followed from the United Nations Framework Convention on Climate Change.

While the bodies that proposed, argued, agreed upon and ultimately adopted existing international agreements vary according to each agreement, certain conferences, including 1972's United Nations Conference on the Human Environment, 1983's World Commission on Environment and Development, 1992's United Nations Conference on Environment and Development and 2002's World Summit on Sustainable Development have been particularly important. Multilateral environmental agreements sometimes create an International Organization, Institution or Body responsible for implementing the agreement. Major examples are the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) and the International Union for Conservation of Nature (IUCN).

International environmental law also includes the opinions of international courts and tribunals. While there are few and they have limited authority, the decisions carry much weight with legal commentators and are quite influential on the development of international environmental law. One of the biggest challenges in international decisions is to determine an adequate compensation for environmental damages. The courts include the International Court of Justice (ICJ), the international Tribunal for the Law of the Sea (ITLOS), the European Court of Justice, European Court of Human Rights and other regional treaty tribunals.

According to the International Network for Environmental Compliance and Enforcement (INECE), the major environmental issues in Africa are “drought and flooding, air pollution, deforestation, loss of biodiversity, freshwater availability, degradation of soil and vegetation, and widespread poverty.” The U.S. Environmental Protection Agency (EPA) is focused on the “growing urban and industrial pollution, water quality, electronic waste and indoor air from cookstoves.” They hope to provide enough aid on concerns regarding pollution before their impacts contaminate the African environment as well as the global environment. By doing so, they intend to “protect human health, particularly vulnerable populations such as children and the poor.” In order to accomplish these goals in Africa, EPA programs are focused on strengthening the ability to enforce environmental laws as well as public compliance to them. Other programs work on developing stronger environmental laws, regulations, and standards.

The Asian Environmental Compliance and Enforcement Network (AECEN) is an agreement between 16 Asian countries dedicated to improving cooperation with environmental laws in Asia. These countries include Cambodia, China, Indonesia, India, Maldives, Japan, Korea, Malaysia, Nepal, Philippines, Pakistan, Singapore, Sri Lanka, Thailand, Vietnam, and Lao PDR.

The European Union issues secondary legislation on environmental issues that are valid throughout the EU (so called regulations) and many directives that must be implemented into national legislation from the 28 member states (national states). Examples are the Regulation (EC) No. 338/97 on the implementation of CITES; or the Natura 2000 network the centerpiece for nature & biodiversity policy, encompassing the bird Directive (79/409/EEC/ changed to 2009/147/EC)and the habitats directive (92/43/EEC). Which are made up of multiple SACs (Special Areas of Conservation, linked to the habitats directive) & SPAs (Special Protected Areas, linked to the bird directive), throughout Europe.

EU legislation is ruled in Article 249 Treaty for the Functioning of the European Union (TFEU). Topics for common EU legislation are:


The U.S. Environmental Protection Agency is working with countries in the Middle East to improve “environmental governance, water pollution and water security, clean fuels and vehicles, public participation, and pollution prevention.”

The main concerns on environmental issues in the Oceanic Region are “illegal releases of air and water pollutants, illegal logging/timber trade, illegal shipment of hazardous wastes, including e-waste and ships slated for destruction, and insufficient institutional structure/lack of enforcement capacity”. The Secretariat of the Pacific Regional Environmental Programme (SPREP) is an international organization between Australia, the Cook Islands, FMS, Fiji, France, Kiribati, Marshall Islands, Nauru, New Zealand, Niue, Palau, PNG, Samoa, Solomon Island, Tonga, Tuvalu, USA, and Vanuatu. The SPREP was established in order to provide assistance in improving and protecting the environment as well as assure sustainable development for future generations.

The Environment Protection and Biodiversity Conservation Act 1999 is the center piece of environmental legislation in the Australian Government. It sets up the “legal framework to protect and manage nationally and internationally important flora, fauna, ecological communities and heritage places”. It also focuses on protecting world heritage properties, national heritage properties, wetlands of international importance, nationally threatened species and ecological communities, migratory species, Commonwealth marine areas, Great Barrier Reef Marine Park, and the environment surrounding nuclear activities. "Commonwealth v Tasmania" (1983), also known as the "Tasmanian Dam Case", is the most influential case for Australian environmental law.

The Brazilian government created the Ministry of Environment in 1992 in order to develop better strategies of protecting the environment, use natural resources sustainably, and enforce public environmental policies. The Ministry of Environment has authority over policies involving environment, water resources, preservation, and environmental programs involving the Amazon.

The Department of the Environment Act establishes the Department of the Environment in the Canadian government as well as the position Minister of the Environment. Their duties include “the preservation and enhancement of the quality of the natural environment, including water, air and soil quality; renewable resources, including migratory birds and other non-domestic flora and fauna; water; meteorology;" The Environmental Protection Act is the main piece of Canadian environmental legislation that was put into place March 31, 2000. The Act focuses on “respecting pollution prevention and the protection of the environment and human health in order to contribute to sustainable development." Other principle federal statutes include the Canadian Environmental Assessment Act, and the Species at Risk Act. When provincial and federal legislation are in conflict federal legislation takes precedence, that being said individual provinces can have their own legislation such as Ontario's Environmental Bill of Rights, and Clean Water Act.

According to the U.S. Environmental Protection Agency, "China has been working with great determination in recent years to develop, implement, and enforce a solid environmental law framework. Chinese officials face critical challenges in effectively implementing the laws, clarifying the roles of their national and provincial governments, and strengthening the operation of their legal system." Explosive economic and industrial growth in China has led to significant environmental degradation, and China is currently in the process of developing more stringent legal controls. The harmonization of Chinese society and the natural environment is billed as a rising policy priority.

With the enactment of the 2008 Constitution, Ecuador became the first country in the world to codify the Rights of Nature. The Constitution, specifically Articles 10 and 71-74, recognizes the inalienable rights of ecosystems to exist and flourish, gives people the authority to petition on the behalf of ecosystems, and requires the government to remedy violations of these rights. The rights approach is a break away from traditional environmental regulatory systems, which regard nature as property and legalize and manage degradation of the environment rather than prevent it. 
The Rights of Nature articles in Ecuador's constitution are part of a reaction to a combination of political, economic, and social phenomena. Ecuador's abusive past with the oil industry, most famously the class-action litigation against Chevron, and the failure of an extraction-based economy and neoliberal reforms to bring economic prosperity to the region has resulted in the election of a New Leftist regime, led by President Rafael Correa, and sparked a demand for new approaches to development. In conjunction with this need, the principle of "Buen Vivir," or good living—focused on social, environmental and spiritual wealth versus material wealth—gained popularity among citizens and was incorporated into the new constitution.

The influence of indigenous groups, from whom the concept of "Buen Vivir" originates, in the forming of the constitutional ideals also facilitated the incorporation of the Rights of Nature as a basic tenet of their culture and conceptualization of "Buen Vivir." 

The Environmental Protection Law outlines the responsibilities of the Egyptian government to “preparation of draft legislation and decrees pertinent to environmental management, collection of data both nationally and internationally on the state of the environment, preparation of periodical reports and studies on the state of the environment, formulation of the national plan and its projects, preparation of environmental profiles for new and urban areas, and setting of standards to be used in planning for their development, and preparation of an annual report on the state of the environment to be prepared to the President."

In India, Environmental law is governed by the Environment Protection Act, 1986. This act is enforced by the Central Pollution Control Board and the numerous State Pollution Control Boards. Apart from this, there are also individual legislations specifically enacted for the protection of Water, Air, Wildlife, etc. Such legislations include :- 

The Basic Environmental Law is the basic structure of Japan’s environmental policies replacing the Basic Law for Environmental Pollution Control and the Nature Conservation Law. The updated law aims to address “global environmental problems, urban pollution by everyday life, loss of accessible natural environment in urban areas and degrading environmental protection capacity in forests and farmlands.”

The three basic environmental principles that the Basic Environmental Law follows are “the blessings of the environment should be enjoyed by the present generation and succeeded to the future generations, a sustainable society should be created where environmental loads by human activities are minimized, and Japan should contribute actively to global environmental conservation through international cooperation.”
From these principles, the Japanese government have established policies such as “environmental consideration in policy formulation, establishment of the Basic Environment Plan which describes the directions of long-term environmental policy, environmental impact assessment for development projects, economic measures to encourage activities for reducing environmental load, improvement of social infrastructure such as sewerage system, transport facilities etc., promotion of environmental activities by corporations, citizens and NGOs, environmental education, and provision of information, promotion of science and technology."

The Ministry for the Environment and Office of the Parliamentary Commissioner for the Environment were established by the Environment Act 1986. These positions are responsible for advising the Minister on all areas of environmental legislation. A common theme of New Zealand’s environmental legislation is sustainably managing natural and physical resources, fisheries, and forests. The Resource Management Act 1991 is the main piece of environmental legislation that outlines the government’s strategy to managing the “environment, including air, water soil, biodiversity, the coastal environment, noise, subdivision, and land use planning in general.”

The Ministry of Natural Resources and Environment of the Russian Federation makes regulation regarding “conservation of natural resources, including the subsoil, water bodies, forests located in designated conservation areas, fauna and their habitat, in the field of hunting, hydrometeorology and related areas, environmental monitoring and pollution control, including radiation monitoring and control, and functions of public environmental policy making and implementation and statutory regulation."

Vietnam is currently working with the U.S. Environmental Protection Agency on dioxin remediation and technical assistance in order to lower methane emissions. In March 2002, the U.S and Vietnam signed the U.S.-Vietnam Memorandum of Understanding on Research on Human Health and the Environmental Effects of Agent Orange/Dioxin.









</doc>
<doc id="10093" url="https://en.wikipedia.org/wiki?curid=10093" title="Eurostar">
Eurostar

Eurostar is a high-speed railway service connecting London with Amsterdam, Avignon, Brussels, Lille, Lyon, Marseille, Paris and Rotterdam. All its trains traverse the Channel Tunnel between the United Kingdom and France, owned and operated separately by Getlink.

The London terminus is St Pancras International, the other British calling points being Ebbsfleet International and Ashford International in Kent. Calling points in France are Calais-Fréthun and Lille-Europe, with trains to Paris terminating at Gare du Nord. Trains to Belgium terminate at Midi/Zuid station in Brussels. The only calling point in the Netherlands is Rotterdam Centraal, with trains to Amsterdam terminating at Amsterdam Centraal. In addition, there are limited services from London to Disneyland Paris at Marne-la-Vallée – Chessy, direct services to southern France (Lyon, Avignon and Marseille) from May to September (launched on 1 May 2015), and seasonal direct services to the French Alps in winter (December to April). 

The service is operated by eighteen-car Class 373/1 trains and sixteen-car trains which run at up to on a network of high-speed lines. The LGV Nord line in France opened before Eurostar services began in 1994, and newer lines enabling faster journeys were added later—HSL 1 in Belgium and High Speed 1 in southern England. The French and Belgian parts of the network are shared with Paris–Brussels Thalys services and also with TGV trains. In the United Kingdom the two-stage Channel Tunnel Rail Link project was completed on 14 November 2007 and renamed High Speed 1, when the London terminus of Eurostar transferred from Waterloo International to St Pancras International.

Eurostar was until 2010 operated jointly by the national railway companies of France and Belgium, SNCF and SNCB/NMBS, and Eurostar (UK) Ltd (EUKL), a subsidiary of London and Continental Railways (LCR), which also owned the high-speed infrastructure and stations on the British side. Eurostar has become the dominant operator on the routes that it operates, carrying more passengers than all airlines combined. Other operators have expressed an interest in starting competing services following deregulation in 2010. On 1 September 2010, Eurostar was incorporated as a single corporate entity called Eurostar International Limited (EIL), replacing the joint operation between EUKL, SNCF and SNCB/NMBS. EIL is owned by SNCF (55%), (CDPQ) (30%), Hermes Infrastructure (10%) and NMBS (5%).

In June 2014, the UK shareholding in Eurostar International Limited was transferred from London and Continental Railways / Department for Transport to HM Treasury. In October 2014, it was announced that the UK government planned to raise £300 million by selling that stake. In March 2015, the UK government announced that it would be selling its 40% share to an Anglo-Canadian consortium made up of Caisse de dépôt et placement du Québec and Hermes Infrastructure. This sale was completed in May 2015.

The history of Eurostar can be traced to the 1986 choice of a rail tunnel to provide a cross-channel link between Britain and France.
A previous attempt at constructing a tunnel between the two nations had begun in 1974, but was quickly aborted. In 1988 construction began on a new basis. Eurotunnel was created to manage and own the tunnel, which was finished in 1993, the official opening taking place in May 1994.

In addition to the tunnel's shuttle trains carrying cars and lorries between Folkestone and Calais, the decision to build a railway tunnel opened up the possibility of through passenger and freight train services between places further afield. British Rail and SNCF contracted with Eurotunnel to use half the tunnel's capacity for this purpose. In 1987 Britain, France and Belgium set up an International Project Group to specify a train to provide an international high-speed passenger service through the tunnel. France had been operating high-speed TGV services since 1981, and had begun construction of a new high-speed line between Paris and the Channel Tunnel, LGV Nord; French TGV technology was chosen as the basis for the new trains. An order for 30 trainsets, to be manufactured in France but with some British and Belgian components, was placed in December 1989. On 20 June 1993, the first Eurostar test train travelled through the tunnel to the UK. Various technical difficulties in running the new trains on British tracks were quickly overcome.

On 14 November 1994, Eurostar services began between Waterloo International station in London, Gare du Nord in Paris and Brussels-South railway station in Brussels. The train service started with a limited "Discovery service", the full daily service started from 28 May 1995.

In 1995 Eurostar was achieving an average end-to-end speed of between London and Paris.
On 8 January 1996 Eurostar launched services from a second railway station in the UK when Ashford International was opened.

On 23 September 2003 passenger services began running on the first completed section of High Speed 1. Following a high-profile glamorous opening ceremony
and a large advertising campaign, on 14 November 2007 Eurostar services in London transferred from Waterloo to the extended and extensively refurbished St Pancras International.

The Channel Tunnel used by Eurostar services holds the record for having the longest undersea section anywhere in the world, and it is the second longest rail tunnel in the world.
A Eurostar train set a new British speed record of on the first section of High Speed 1 on 30 July 2003, two months before services began running upon the first section of High Speed 1.

On 16 May 2006 Eurostar set a new record for the longest non-stop high-speed journey, a distance of from London to Cannes taking 7 hours 25 minutes.
On 4 September 2007 a record-breaking train left Paris Gare du Nord at 10:44 (09:44 BST) and reached London St Pancras in 2 hours 3 minutes 39 seconds; carrying journalists and railway workers. The train was the first passenger-carrying arrival at St Pancras International station.
On 20 September 2007, Eurostar broke another record when it completed the journey from Brussels to London in 1 hour, 43 minutes.

The original proposals for Eurostar included direct services to Paris and Brussels from cities north of London (NoL): Manchester via Birmingham on the West Coast Main Line and on the East Coast Main Line Leeds and Glasgow via Edinburgh, Newcastle and York.
Seven shorter NoL Eurostar trains for these Regional Eurostar services were built, but these services never ran. Predicted journey times of almost nine hours for Glasgow to Paris at the time of growth of low-cost air travel during the 1990s made the plans commercially unviable against the cheaper and quicker airlines. Other reasons that have been suggested for these services having never been run were both government policies and the disruptive privatisation of British Rail.
Three of the Regional Eurostar units were leased by Great North Eastern Railway (GNER) to increase domestic services from London King's Cross to York and later Leeds. The leases ended in December 2005, and most of the NoL sets have since been transferred to SNCF for TGV services in northern France.

An international Nightstar sleeper train was also planned; this would have travelled the same routes as Regional Eurostar, plus the Great Western Main Line to Cardiff.
These were also deemed commercially unviable, and the scheme was abandoned with no services ever operated. In 2000 the coaches were sold to Via Rail in Canada.

Ashford International station was the original station for Eurostar services in Kent.
Once Ebbsfleet International railway station, also designed to serve the Kent region, had opened, only three trains a day to Paris and one to Disneyland Paris called at Ashford for a considerable amount of time. There were fears that services at Ashford International might be further reduced or withdrawn altogether as Eurostar planned to make Ebbsfleet the new regional hub instead.
However, after a period during which no Brussels trains served the station, to the dissatisfaction of the local communities, on 23 February 2009 Eurostar re-introduced a single daily Ashford-Brussels service.
In 2015 Eurostar threatened to require that cyclists dismantle bicycles before they could be transported on trains. Following criticism from Boris Johnson and cycling groups Eurostar reversed the edict.

By March 2016 onboard entertainment was provided by GoMedia, including Wi-Fi connectivity and up to 300 hours of movies and television kept on the train's servers and accessed using the passenger's own devices: mobile phones, tablets etc. A tracker app allows customers to see where they are.

LGV Nord is a French -long high-speed rail line that connects Paris to the Belgian border and the Channel Tunnel via Lille. It opened in 1993.
Its extensions to Belgium and towards Paris, as well as connecting to the Channel Tunnel, have made LGV Nord a part of every Eurostar journey undertaken. A Belgian high-speed line, HSL 1, was added to the end of LGV Nord, at the Belgian border, in 1997. Of all French high-speed lines, LGV Nord sees the widest variety of high-speed rolling stock and is quite busy; a proposed cut-off bypassing Lille, which would reduce Eurostar journey times to Paris, is called LGV Picardie.

The Channel Tunnel is a crucial part of the route as it is the only rail connection between Great Britain and the European mainland. It joins LGV Nord in France with High Speed One in Britain. Tunnelling began in 1988, and the tunnel was officially opened by British sovereign Queen Elizabeth II and the French President François Mitterrand at a ceremony in Calais on 6 May 1994.
It is owned by Getlink, which charges a significant toll to Eurostar for its use.
In 1996 the American Society of Civil Engineers identified the tunnel as one of the Seven Wonders of the Modern World.
Along the current route of the Eurostar service, line speeds are except within the Channel Tunnel, where a reduced speed of applies for safety reasons.
Since the launch of Eurostar services, severe disruptions and cancellations have been caused by fires breaking out within the Channel Tunnel, such as in 1996, 2006 (minor), 2008 and 2015.

Until the opening on 2 June 1996, of the first phase of the Belgian high speed line, Eurostar trains were routed via the Belgian railway line 94. The Eurostar routes still use the line as a diversion if engineering works are taking place on HSL1, depending where it is. The 06:13 from London St Pancras to Brussels still uses the line as a diversion to bypass the peak time disruptions on HSL1 due to the extra TGV services from Brussels for the commuters. After 2 June 1996, some Eurostars to Brussels were routed via the first phase of the Belgian High Speed line and the Belgian railway line 78 via Mons. Although this line is still as a diversion if HSL1 is doing engineering, also depending where the maintiance is taking place. Journey times between London and Brussels were improved when an Belgian high-speed line, HSL 1, opened on 14 December 1997.
It links with LGV Nord on the border with France, allowing Eurostar trains heading to Brussels to make the transition between the two without having to reduce speed. A further four-minute improvement for London–Brussels trains was achieved in December 2006 with the opening of the Brussels South Viaduct.
Linking the international platforms of Brussels-South railway station with the high-speed line, the viaduct separates Eurostar (and Thalys) from local services.

High Speed 1 (HS1), formerly known as the Channel Tunnel Rail Link (CTRL), is a high-speed railway line running from London through Kent to the British end of the Channel Tunnel.
It was built in two stages. The first section between the tunnel and Fawkham Junction in north Kent opened in September 2003, cutting London–Paris journey times by 21 minutes to 2 hours 35 minutes, and London–Brussels to 2 hours 20 minutes. On 14 November 2007, commercial services began over the whole of the new HS1 line. The redeveloped St Pancras International station became the new London terminus for all Eurostar services.
The completion of High Speed 1 has brought the British part of Eurostar's route up to the same standards as the French and Belgian high-speed lines. Non-stop journey times were reduced by a further 20 minutes to 2 hours 15 minutes for London–Paris and 1 hour 51 minutes for London–Brussels.

Eurostar offers up to sixteen weekday London – Paris services (eighteen on Fridays) including ten non-stop (twelve on Fridays). There used to be ten London–Lille and Brussels services, including five running non-stop as far as Lille, but this has now been reduced to seven each way.
In addition, there is a return trip from London to Marne-la-Vallée - Chessy for Disneyland Paris which runs at least 4 times a week with increased frequency during school holidays and an up to 5 times a week service to Marseille via Lyon and Avignon. There are also seasonal services in the winter. "Snow trains", aimed at skiers, to Bourg-Saint-Maurice, Aime-la-Plagne and Moûtiers in the Alps; these run twice-weekly, one overnight and one during the daytime.
Intermediate stations are Ebbsfleet International in northwest Kent, Ashford International in southeast Kent, and Calais-Fréthun and Lille-Europe in Nord-Pas-de-Calais. In February 2018, Eurostar announced the start of its long planned service from London to Amsterdam, with an initial two trains per day from April of that year running between St Pancras and Amsterdam Centraal. This will be a one-way service to begin with, as the facilities for immigration and customs checks are yet to be installed at either Amsterdam or Rotterdam for the Amsterdam to London route. It entered revenue service April 2018.

Since 14 November 2007, all Eurostar trains have been routed via High Speed 1 to or from the redeveloped London terminus at St Pancras International, which at a cost of £800 million was extensively rebuilt and extended to cope with long Eurostar trains.
It had originally been intended to retain some Eurostar services at Waterloo International, but this was ruled out on cost grounds.
Completion of High Speed 1 has increased the potential number of trains serving London. Separation of Eurostar from British domestic services through Kent meant that timetabling was no longer affected by peak-hour restrictions.

Eurostar's fares were significantly higher in its early years; the cheapest fare in 1994 was £99 return.
In 2002, Eurostar was planning cheaper fares, an example of which was an offer of £50 day returns from London to Paris or Brussels.
By March 2003, the cheapest fare from the UK was £59 return, available all year around. In June 2009 it was announced that one-way single fares would be available at £31 at the cheapest. Competition between Eurostar and airline services was a large factor in ticket prices being reduced from the initial levels.
Business Premier fares also slightly undercut air fares on similar routes, targeted at regular business travellers.
In 2009, Eurostar greatly increased its budget ticket availability to help maintain and grow its dominant market share.
The Eurostar ticketing system is very complex, being distributed through no fewer than 48 individual sales systems.
Eurostar is a member of the Amadeus CRS distribution system, making its tickets available alongside those of airlines worldwide.

First class on Eurostar is called Business Premier; benefits include guaranteed faster checking-in and meals served at-seat, as well as the improved furnishings and interior of Business Premier carriages.
The rebranding is part of Eurostar's marketing drive to attract more business professionals. Increasingly, business people in a group have been chartering private carriages as opposed to individual seats on the train.

Without the operation of Regional Eurostar services using the North of London trainsets across the rest of Britain, Eurostar has developed its connections with other transport services instead, such as integrating effectively with traditional UK rail operators' schedules and routes, making it possible for passengers to use Eurostar as a quick connection to further destinations on the continent.
All three main terminals used by the Eurostar service – St Pancras International, Paris Gare du Nord, and Brussels Midi/Zuid – are served by domestic trains and by local urban transport networks such as the London Underground and the Paris Metro. Standard Eurostar tickets no longer include free onward connections to or from any other station in Belgium: this is now available for a flat-rate supplement, currently £5.50.

Eurostar has announced several partnerships with other rail services,
most notably Thalys connections at Lille and Brussels for passengers to go beyond current Eurostar routes towards the Netherlands and Germany.
In 2002, Eurostar initiated the Eurostar-Plus program, offering connecting tickets for onward journeys from Lille and Paris to dozens of destinations in France.
Through fares are also available from 68 British towns and cities to destinations in France and Belgium.
In May 2009 Eurostar announced that a formal connection to Switzerland had been established in a partnership between Eurostar and Lyria, which will operate TGV services from Lille to the Swiss Alps for Eurostar connection.

Because the UK is not part of the Schengen Area, and because Belgium and France are not part of the Common Travel Area, all Eurostar passengers must go through border controls. Both the British Government and the Schengen governments concerned (Belgium and France) have legal obligations to check the travel documents of those entering their respective countries (as well as those leaving, in the case of Belgium and France).

To allow passengers to walk off the train without arrival checks in most cases, juxtaposed controls ordinarily take place at the embarkation station.

In order to comply with special UK legislation, there are full security checks similar to those at airports, scanning both bags and people's pockets. Security checks at Eurostar are comparable to those at a small airport and generally much quicker than at London Heathrow. The recommended check-in time is 30 minutes except for business class where it is 10 minutes.

Eurostar passengers travelling inside the Schengen Area (Brussels–Lille or Brussels–Calais; Lille–Calais trips are not allowed to be performed by Eurostar) pass through a separate corridor in Brussels bypassing border checks, and enter the preallocated cars of the train, which is reserved for these passengers. This arrangement was set up after numerous illegal immigrants entered the UK without the right to do so, by buying a ticket from Brussels to Lille or Calais but remaining on the train until London – an issue exacerbated by Belgian police threatening to arrest UK Border Agency staff at Brussels-Midi if they tried to prevent passengers whom they suspected of attempting to exploit this loophole from boarding Eurostar trains.

When the tripartite agreements were signed, the Belgian Government said that it had serious questions about the compatibility of this agreement with the Schengen Convention and the principle of free movement of people enshrined in various European Treaties.
On 30 June 2009 Eurostar raised concerns at the UK House of Commons Home Affairs Select Committee that it was illegal under French law for the collection of information desired by the UK government under the e-Borders scheme, and they would be unable to cooperate.

On the northbound Marne la Vallée-Chessy - London train, the security check and French passport check take place at Marne la Vallée-Chessy, while the UK passport check takes place at the UK arrival stations - this is the only route where passengers are not cleared by UK border officials before crossing the channel.

On the northbound Marseille-London train, there is no facility for security or passport checks at the southern French stations, so passengers must leave the train at Lille-Europe, taking all their belongings with them, and undergo security and border checks there before rejoining the train which waits at the station for just over an hour.

On several occasions, people have illegally tried to stow away on board the train, sometimes in large groups, trying to enter the UK; border monitoring and security is therefore extremely tight.
Eurostar claims to have good and well-funded security measures.

Eurostar's punctuality has fluctuated from year to year, but usually remains over 90%; in the first quarter of 1999, 89% of services operated were on time, and in the second quarter it reached 92%. Eurostar's best punctuality record was 97.35%, between 16 and 22 August 2004. In 2006, it was 92.7%, and in 2007, 91.5% were on time. In the first quarter of 2009, 96% of Eurostar services were punctual compared with rival air routes' 76%.

An advantage held by Eurostar is the convenience and speed of the service: with shorter check-in times than at most airports and hence quicker boarding and less queueing and high punctuality, it takes less time to travel between central London and central Paris by high-speed rail than it does by air. Eurostar now has a dominant share of the combined rail–air market on its routes to Paris and Brussels. In 2004, it had a 66% share of the London–Paris market, and a 59% share of the London–Brussels market. In 2007, it achieved record market shares of 71% for London–Paris and 65% for London–Brussels routes.

Eurostar's passenger numbers initially failed to meet predictions. In 1996, London and Continental Railways forecast that passenger numbers would reach 21.4 million annually by 2004, but only 7.3 million was achieved. 82 million passengers used Waterloo International Station from its opening in 1994 to its closure in 2007. 2008 was a record year for Eurostar, with a 10.3% rise in passenger use, which was attributed to the use of High Speed 1 and the move to St Pancras. The following year, Eurostar saw an 11.5% fall in passenger numbers during the first three months of 2009, attributed to the 2008 Channel Tunnel fire and the 2009 recession.

As a result of the poor economic conditions, Eurostar received state aid in May 2009 to cancel out some of the accumulated debt from the High Speed 1 construction programme. Later that year, during snowy conditions in the run-up to Christmas, thousands of passengers were left stranded as several trains broke down and many more were cancelled. In an independent review commissioned by Eurostar, the company came in for serious criticism about its handling of the incident and lack of plans for such a scenario.

In 2006, the Department for Transport predicted that, by 2037, annual cross-channel passenger numbers would probably reach 16 million, considerably less optimistic than London and Continental Railways's original 1996 forecast. In 2007 Eurostar set a target of carrying 10 million passengers by 2010.
The company cited several factors to support this objective, such as improved journey times, punctuality and station facilities. Passengers in general, it stated, are becoming increasingly aware of the environmental effects of air travel, and Eurostar services emit much less carbon dioxide. and that its remaining carbon emissions are now offset, making its services carbon neutral. Further expansion of the high-speed rail network in Europe, such as the HSL-Zuid line between Belgium and the Netherlands, continues to bring more destinations within rail-competitive range, giving Eurostar the possibility of opening up new services in future.

The following chart presents the estimated number of passengers annually transported by the Eurostar service since 1995, in millions:

Eurostar has been hailed as having set new standards in international rail travel and has won praise several times over, recognising its high standards. Eurostar won the Train Operator of the Year award in the HSBC Rail Awards for 2005. It was declared the Best Train Company in the joint Guardian/Observer Travel Awards 2008. Eurostar had previously struggled with its reputation and brand image. One commentator had defined the situation at the time as:

By 2008, Eurostar's environmental credentials had become highly developed and promoted. In 2006 Eurostar's Environment Group was set up, with the aim of making changes in the Eurostar services' daily running to decrease the environmental impact, the organisation setting itself a target of reducing carbon emissions per passenger journey by 25% by 2012. Drivers are trained in techniques to achieve maximum energy efficiency, and lighting has been minimised; the provider of the bulk of the energy for the Channel Tunnel has been switched to nuclear power stations in France.

Eurostar's current target is to reduce emissions by 35 percent per passenger journey by 2012, putting itself beyond the efforts of other railway companies in this field and thereby winning the 2007 Network Rail Efficiency Award.
In the grand opening ceremony of St Pancras International, one of the Eurostar trains was given the name 'Tread Lightly', said to symbolise their smaller impact on the environment compared to planes.

Since 2010, Eurostar is owned by Eurostar International Limited, a company jointly owned by SNCF (55%), (CDPQ) (30%), Hermes Infrastructure (10%) and SNCB (5%).

Eurostar is a member of Railteam, a marketing alliance formed in July 2007 of seven European high-speed rail operators, including Thalys.
The alliance plans to allow tickets to be booked from one end of Europe to the other on a single website. In June 2009 London and Continental Railways, and the Eurostar UK operations they held ownership of, became fully nationalised by the UK government.

In addition to its multiple unit fleet units, Eurostar operates a single Class 08 diesel shunter as the pilot at Temple Mills depot.

Built between 1992 and 1996, Eurostar's fleet consists of 38 EMU trains, designated Class 373 in the United Kingdom and TGV TMST in France. The units have also been branded as the Eurostar e300 by Eurostar since 2015. There are two variants:

The trains are essentially modified TGV sets, and can operate at up to on high-speed lines, and in the Channel Tunnel. It is possible to exceed the 300-kilometre-per-hour speed limit, but only with special permission from the safety authorities in the respective country.
Speed limits in the Channel Tunnel are dictated by air-resistance, energy (heat) dissipation and the need to fit in with other, slower trains. The trains were designed with Channel Tunnel safety in mind, and consist of two independent "half-sets" each with its own power car. In the event of a serious fire on board while travelling through the tunnel, passengers would be transferred into the undamaged half of the train, which would then be detached and driven out of the tunnel to safety.
If the undamaged part were the rear half of the train, this would be driven by the Chef du Train, who is a fully authorised driver and occupies the rear driving cab while the train travels through the tunnel for this purpose.

As 27 of the 31 Inter-Capital sets are sufficient to operate the service, four are currently used by SNCF for domestic TGV services; one of these regularly operates a Paris–Lille shuttle. The Eurostar logos have been removed from these sets, but the base colours of white, black, and yellow remain. SNCF's lease of the sets is scheduled to last until 2011, with an option for a further two years.

Each train has a unique four-digit number starting with "3" (3xxx). This designates the train as a Mark 3 TGV (Mark 1 being SNCF TGV Sud-Est; Mark 2 being SNCF TGV Atlantique). The second digit denotes the country of ownership:

In 2004–2005 the "Inter-Capital" sets still in daily use for international services were refurbished with a new interior designed by Philippe Starck.
The original grey-yellow scheme in Standard class and grey-red of First/Premium First were replaced with a grey-brown look in Standard and grey-burnt-orange in First class. Power points were added to seats in First class and coaches 5 and 14 in Standard class. Premium First class was renamed BusinessPremier.

In 2008, Eurostar announced that it would be carrying out a mid-life refurbishment of its Class 373 trains to allow the fleet to remain in service beyond 2020.
This will include the 28 units making up the Eurostar fleet, but not the three Class 373/1 units used by SNCF or the seven Class 373/2 "North of London" sets.
As part of the refurbishment, the Italian company Pininfarina was contracted to redesign the interiors, and The Yard Creative was selected to design the new buffet cars.
On 11 May 2009 Eurostar revealed the new look for its first-class compartments.
The first refurbished train was due in service in 2012, and Eurostar plans to have completed the entire process by 2014.
On 13 November 2014 Eurostar announced the first refurbished trains would not re-enter the fleet until the 3rd or 4th quarter of 2015 due to delays at the completion centre.

In addition to the announced mid-life update of the existing Class 373 fleet, Eurostar in 2009 reportedly entered prequalification bids for eight new trainsets to be purchased. Any new trains would need to meet the same safety rules governing passage through the Channel Tunnel as the existing Class 373 fleet. The replacement to the Class 373 trains has been decided jointly between the French Transport Ministry and the UK Department for Transport. The new trains will be equipped to use the new ERTMS in-cab signalling system, due to be fitted to High Speed 1 around 2040.
On 7 October 2010, it was reported that Eurostar had selected Siemens as preferred bidder to supply 10 Siemens Velaro e320 trainsets at a cost of €600 million (and a total investment of more than £700 million with the refurbishment of the existing fleet included) to operate an expanded route network, including services from London to Cologne and Amsterdam. These would be sixteen-car, long trainsets built to meet current Channel Tunnel regulations. The top speed will be and they will have 894–950 seats, unlike the current fleet built by the French company Alstom, which has a top speed of and a seating capacity of 750. Total traction power will be rated at 16 MW.

The nomination of Siemens would see it break into the French high-speed market for the first time, as all French and French subsidiary high-speed operators use TGV derivatives produced by Alstom. Alstom attempted legal action to prevent Eurostar from acquiring German-built trains, claiming that the Siemens sets ordered would breach Channel Tunnel safety rules, but this was thrown out of court. Alstom said, after its High Court defeat, that it would "pursue alternative legal options to uphold its position". On 4 November 2010, the company lodged a complaint with the European Commission over the tendering process, which then asked the British government for "clarification". Alstom then announced it had started legal action against Eurostar, again in the High Court in London.
In July 2011, the High Court rejected Alstom's claim that the tender process was "ineffective", and in April 2012 Alstom said it would call off pending court actions against Siemens. This effectively freed the way for Siemens to build the new Eurostar trains, the first of which were expected to enter service in late 2015.

On 13 November 2014 Eurostar announced the purchase of an additional seven e320s for delivery in the second half of 2016. At the same time, Eurostar announced the first five e320s from the original order of ten would be available by December 2015, with the remaining five entering service by May 2016. Of the five sets ready by December 2015, three of them were planned to be used on London-Paris and London-Brussels routes.

In 2005, the chief executive of Eurostar, Richard Brown, suggested that existing Eurostar trains could be replaced by double-deck trains similar to the TGV Duplex units when they are withdrawn from service. According to Brown, a double-deck fleet could carry 40 million passengers per year from Britain to Continental Europe, equivalent to adding an extra runway at a London airport.

A number of technical incidents have affected Eurostar services over the years, but there has only been one major accident involving a service operated by Eurostar, a derailment in June 2000. Other incidents in the Channel Tunnel — such as the 1996 and 2008 Channel Tunnel fires — have affected Eurostar services but were not directly related to Eurostar's operations. However, the breakdowns in the tunnel, which resulted in cessation of service and inconvenience to thousands of passengers, in the run-up to Christmas 2009, proved a public-relations disaster.

There have been several minor incidents with a few Eurostar services. In October 1994 there were teething problems relating to the start of operations. The first preview train, carrying 400 members of the press and media, was delayed for two hours by technical problems.
On 29 May 2002 a Eurostar train was initially sent down a wrong line — towards London Victoria railway station instead of London Waterloo — causing the service to arrive 25 minutes late. A signalling error that led to the incorrect routeing was stated to have caused "no risk" as a result.

On 11 April 2006, a house collapsed next to a railway line near London which caused Eurostar services to have to terminate and start from Ashford International instead of London Waterloo. Passengers waiting at Waterloo International were initially directed on to local trains towards Ashford leaving from the adjacent London Waterloo East railway station, until overcrowding occurred at Ashford.

Approximately 1000 passengers were trapped in darkness for several hours inside two Eurostar trains on the night of 19/20 February 1996. The trains stopped inside the tunnels due to electronic failures caused by snow and ice. Questions were raised at the time about the ability of the train and tunnel electronics to withstand the mix of snow, salt and ice which collect in the tunnels during periods of extreme cold.

On 5 June 2000 a Eurostar train travelling from Paris to London derailed on the LGV Nord high-speed line while traveling at . Fourteen people were treated for light injuries or shock, with no fatalities or major injuries. The articulated nature of the trainset was credited with maintaining stability during the incident and all of the train stayed upright. The incident was caused by a traction link on the second bogie of the front power car coming loose, leading to components of the transmission system on that bogie impacting the track.

The first departures from St Pancras on 14 November 2007 coincided with an open-ended strike by French rail unions as part of general strike actions over proposed public-sector pension reforms. The trains were operated by uninvolved British employees and service was not interrupted.

On 23 September 2009 an overhead power line dropped on to a Class 373 train arriving at St Pancras station, activating a circuit breaker and delaying eleven other trains. Two days later, on 25 September 2009, electrical power via the overhead lines was lost on a section of high-speed line outside Lille, delaying passengers on two evening Eurostar-operated services.

During the December 2009 European snowfall, four Eurostar trains broke down inside the Channel Tunnel, after leaving France, and one in Kent on 18 December. Although the trains had been winterised, the systems had not coped with the conditions. Over 2,000 passengers were stuck inside failed trains inside the tunnel, and over 75,000 had their services disrupted. All Eurostar services were cancelled from Saturday 19 December to Monday 21 December 2009. An independent review, published on 12 February 2010, was critical of the contingency plans in place for assisting passengers stranded by the delays, calling them "insufficient".

On 7 January 2010 a Brussels-London train broke down in the Channel Tunnel, resulting in three other trains failing to complete their journeys. The cause of the failure was the onboard signalling system. Due to the severe weather, a limited service was operated in the next few days.

On 15 February 2010, services between Brussels and London were interrupted following the Halle train collision, this time after the dedicated HSL 1 lines in the suburbs of the Belgian capital were blocked by debris from a serious train crash on the suburban commuter lines alongside. No efforts were made to reroute trains around the blockage; Eurostar instead terminated services to Brussels at Lille, directing passengers to continue their journey on local trains. Brussels services resumed on a limited scale on 22 February.

On 21 February 2010 the 21:43 service from Paris to London broke down just outside Ashford International, stranding 740 passengers for several hours while a rescue train was called in.

On 15 April 2010 air traffic in Western Europe closed because of the eruption of the Eyjafjallajökull volcano. Many travellers between the UK and the European mainland instead took the Eurostar train, all tickets between Brussels and London on 15 and 16 April being sold out within 3½ hours after the closure of British airspace.

On 20 December 2010, the Channel Tunnel was closed off for a day due to snowy weather. Eurostar Trains were suspended that day with thousands of passengers stranded in the run up to Christmas

On 17 October 2011 a man fell at approximately 17:40 or 17:50 from the 17:04 service from London to Brussels as it passed through Westenhanger and Cheriton in Folkestone, near the entry to the Channel Tunnel. The individual was an Albanian who had been refused entry to the United Kingdom and was voluntarily returning to Brussels. The line was handed back at 22:09 after being closed for several hours following the incident. The train itself returned north to Ashford International, where passengers were transferred to a Eurostar service operating from Marne-la-Vallée to London, where passengers arrived again at approximately 22:30.

Eurostar trains do not currently call at , originally intended to be the London stop for the regional Eurostars. This was to be reviewed following the 2012 Olympics. However, in 2013, Eurostar claimed that its 'business would be hit' by stopping trains there.

Although the original plan for Regional Eurostar services to destinations north of London was abandoned, the significantly improved journey times available since the opening of High Speed 1 — which is physically connected to both the East Coast Main Line and the North London Line (for the West Coast Main Line) at St Pancras — and the recently increased maximum speeds on the West Coast Main Line may make potential Regional Eurostar services more commercially viable. This would be even more likely if proposals are adopted for a new high-speed line from London to the north of Britain.
Simon Montague, Eurostar's Director of Communications, commented that: "...International services to the regions are only likely once High Speed 2 is built." However, as of 2014 the current plans for High Speed 2 do not allow for a direct rail link between that new line, and High Speed 1, meaning passengers would still be required to change at Euston and take some form of transportation to St Pancras International.

Key pieces of infrastructure still belong to LCR via its subsidiary London & Continental Stations and Property, such as the Manchester International Depot, and Eurostar (UK) still owns several track access rights and the rights to paths on both the East Coast and West Coast Main Lines.
While no announcement has been made of plans to start Regional Eurostar services, it remains a possibility for the future. In the meantime, the closest equivalent to Regional Eurostar services are same-station connections with East Midlands Trains and Thameslink, changing at St Pancras. The recent construction of a new concourse at adjacent King's Cross Station has improved interchange with St Pancras and provided Virgin Trains East Coast, Great Northern, Hull Trains and Grand Central services with easier connections to Eurostar.

Eurostar has already been involved in reviewing and publishing reports into High Speed 2 for the British Government and looks favourably upon such an undertaking. The operation of Regional Eurostar services will not be considered until such time as High Speed 2 has been completed. Alternatively, future loans of the North of London sets to other operators would enable the trains to operate at their full speed, unlike GNER's previous loan between 2000 and 2005, where the trains were limited to on regular track. A separate company called High Speed Two (HS2) Ltd has been set up to investigate the feasibility and viability of a new line likely serving a similar route to the West Coast Main Line.

LGV Picardie is a proposed high-speed line between Paris and Calais via Amiens. By cutting off the corner of the LGV Nord at Lille, it would enable Eurostar trains to save 20 minutes on the journey between Paris and Calais, bringing the London–Paris journey time under 2 hours. In 2008 the French Government announced its future investment plans for new LGVs to be built up to 2020; LGV Picardie was not included but was listed as planned in the longer term. It has later been confirmed that LGV Picardie is intended to be built between 2020 and 2030.

The reduced journey times offered by the opening of High Speed 1 and the opening of the LGV Est and HSL-Zuid bring more continental destinations
within a range from London where rail is competitive with air travel. By Eurostar's estimates a train would then take 3 hours 30 minutes from London to Amsterdam.
At present Eurostar is concentrating on developing its connections with other services, but direct services to other destinations would be possible. However, the routes that any potential services are likely to take would include infrastructure that Eurostar's rolling stock has not been built to use — German railways mostly have 15 kV AC electrification, while the Netherlands uses 1.5 kV DC (except on HSL Zuid and the Betuweroute).
To operate on these lines would require new or heavily modified rolling stock designed to operate at these different voltages, in addition to those already in use. Signalling systems also differ. In addition to the infrastructure difficulties, any potential Eurostar services beyond Paris and Brussels would also require the installation of stringent security measures, due to the UK's not having signed up to the Schengen Agreement, which allows unrestricted movement across borders of member countries.

The difficulties that Eurostar faces in expanding its services would also be faced by any potential competitors to Eurostar. As the UK is outside the Schengen Agreement, London-bound trains must use platforms that are physically isolated, a constraint which other international operators such as Thalys do not face. In addition, the British authorities are required to make passenger security and passport checks before they board the train,
which might deter domestic passengers. Compounding the difficulties in providing a similar service are the Channel Tunnel safety rules, the major ones being the "half-train rule" and the "length rule". The "half-train rule" stipulated that passenger trains had to be able to split in the case of emergency. Class 373 trains were designed as two half-sets, which when coupled form a complete train, enabling them to be split easily in the event of an emergency while in the tunnel, with the unaffected set able to be driven out. The half-train rule was finally abolished in May 2010. However, the "length rule", which states that passenger trains must be at least 375 metres long with a through corridor (to match the distance between the safety doors in the tunnel), was retained, preventing any potential operators from applying to run services with existing fleets (the majority of both TGV and ICE trains are only 200m long).

In October 2009 the President of SNCF, Guillaume Pepy, outlined plans to expand TGV services around Paris as well as for fleet renewal. A plan to connect LGV Nord, the line used by Eurostar into Paris, with La Défense, a large commercial and business centre in the west of Paris, was described as the "top priority". Pepy estimated that the connection (which would also allow interchange to the proposed Paris – Rouen – Le Havre LGV line) would allow a journey time from central London to La Défense of 2 hr 15 min. Guillaume Pepy defined SNCF's priorities for the future as:
At the same time as Pepy's announcement, Richard Brown announced that Eurostar's plans for expanding its network potentially included Amsterdam and Rotterdam as destinations, using the HSL Zuid line. This would require either equipment upgrades of the existing fleet, or a new fleet equipped for both ERTMS and the domestic signalling systems used by Nederlandse Spoorwegen. Following the December 2009 opening of HSL Zuid, a London–Amsterdam journey is estimated to take 4 hr 16 min.

In an interview with Eurostar's Chief Executive Nicolas Petrovic in the "Financial Times" in May 2012, an intention for Eurostar to serve ten new destinations was expressed, including Amsterdam, Frankfurt, Cologne, Lyon, Marseille and Geneva, along with a likely second hub to be created in Brussels.

In March 2016 in an interview with Bloomberg, Eurostar's Chief Executive expressed interest in operating a direct train service between London and Bordeaux, but not before 2019. Journey time was said to be around four and a half hours using the new LGV Sud Europe Atlantique.

In December 2012 Eurostar announced that on Saturdays during May 2013–June 2013 a new seasonal service would be introduced to Aix-en-Provence, also serving Lyon Part-Dieu and Avignon TGV on the way (the latter being from central Avignon). This is in addition to the long-standing seasonal summer service on Saturdays during July and August and the first week of September travelling to Avignon Centre. The Aix-en-Provence services did not run in 2014 but was replaced along with the seasonal Avignon Centre services with the new year-round service to Lyon and Marseille as of 1 May 2015.
In 2018, at least, direct services to Lyon, Avignon and Marseille ran only from May to September, with connections during the rest of the year being offered via Eurostar but requiring a change to SNCF trains in Paris or Lille. Travel time from London to Marseille was roughly 6.5 hours in 2018.

In September 2013, Eurostar announced an agreement with the Government of Netherlands and NS, the Dutch railway company to start twice daily services between London and Amsterdam Centraal; the launch was initially planned for December 2016. The service will use the newly bought Siemens Velaro trainsets and will also call at Brussels, and Rotterdam. The journey time will be around four hours.

Trains will stop in Brussels for half an hour for a security check. Passengers for London from Amsterdam and Rotterdam will undertake all security checks before boarding and will not need to get off in Brussels. The train will also convey passengers from the Netherlands on journeys to Brussels who will not need to pass through security and they will be allocated half the train which will be kept separate from the London-bound passengers by locking the intermediate door. The Brussels-bound half of the train will be security swept on arrival at Brussels before Brussels-to-London passengers can board.

The journey from London to Amsterdam Centraal will take 3 hr 41 min and trains will call at Brussels Zuid/Midi and Rotterdam Centraal Station. From Amsterdam Centraal to London St Pancras, trains will take 4 hr 9 min to include the 28 minute stop at Brussels. Eurostar trains from London will also call at Antwerp Centraal and Schiphol Airport, although trains from Amsterdam will miss out Antwerp on the journey back to London.

In November 2014, Eurostar announced the service to Amsterdam would start in "2016-2017", and would include a stop at Schiphol Airport in addition to the previously announced destinations. Eurostar have indicated that the calling pattern 'is not set in stone' and if a business case supports it the service might be extended to additional cities such as Utrecht.

The service was finally planned to start running on 4 April 2018, with fare prices starting at £35 for a single ticket. An "inaugural train" from St Pancras International to Amsterdam via Rotterdam broke a speed record for the journey to Brussels (1hr 46mins) on 20 February 2018. The first regular service to Amsterdam left St Pancras at 08:31 on 4 April 2018.

In 2010, international rail travel was liberalised by new European Union directives, designed to break up monopolies in order to encourage competition for services between countries. This sparked interest among other companies in providing services in competition to Eurostar and new services to destinations beyond Paris and Brussels. The only rail carrier to formally propose and secure permission for such a service up to now is Deutsche Bahn, which intends to run services between London and Germany and the Netherlands. The sale of High Speed One by the British Government having effectively nationalised LCR in June 2009 is also likely to stimulate competition on the line.

In March 2010, it was announced that Eurotunnel was in discussions with the Intergovernment Commission, which oversees the tunnel, with the aim of amending elements of the safety code governing the tunnel's usage. Most saliently, the requirement that trains be able to split within the tunnel and each part of the train be driven out to opposite ends has been removed. However, the proposal to allow shorter trains was not passed. Eurotunnel Chairman & Chief Executive Jacques Gounon said that he hoped the liberalisation of rules would allow entry into the market of competitors such as Deutsche Bahn. Sources at Eurotunnel suggested that Deutsche Bahn could have entered the market at the timetable change in December 2012. This, however, did not happen.

In July 2010 Deutsche Bahn (DB) announced that it intended to make a test run with a high-speed ICE-3MF train through the Channel Tunnel in October 2010 in preparation for possible future operations. The trial ran on 19 October 2010 with a Class 406 ICE train specially liveried with a British "Union flag" decal. The train was then put on display for the press at St Pancras International. However, this is not the class of train that would be used for the proposed service. At the St Pancras ceremony, DB revealed that it planned to operate from London to Frankfurt and Amsterdam (two of the biggest air travel markets in Europe), with trains 'splitting & joining' in Brussels. It hoped to begin these services in 2013 using Class 407 ICE units, with three trains per day each way—morning, midday and afternoon. Initially the only calling points would be Rotterdam on the way to Amsterdam, and Cologne on the way to Frankfurt. Amsterdam and Cologne would be under four hours from London, Frankfurt around five hours. DB decided to put this on hold mainly due to advance passport check requirements. DB had hoped that immigration checks could be done on board, but British authorities required immigration and security checks to be done at Lille Europe station, taking at least 30 minutes.

In August 2010, Trenitalia announced its desire to eventually run high-speed trains from Italy to the United Kingdom, using its newly ordered high-speed trains. The trains will be delivered from 2013.



</doc>
<doc id="10100" url="https://en.wikipedia.org/wiki?curid=10100" title="Equinox">
Equinox

An equinox is commonly regarded as the moment the plane of Earth's equator passes through the center of the Sun's disk, which occurs twice each year, around 20 March and 22-23 September. In other words, it is the point in which the center of the visible sun is directly over the equator. This simplified, but incorrect, understanding of Earth's orbital motion can lead to errors of up to 69 seconds from the actual time of equinox.

The instants of the equinoxes are currently defined to occur when the ecliptic longitude of the Sun is either 0° or 180°. As the true motion of the Earth is affected by the gravitational pull of the Sun and Moon (and to lesser extent the other planets), there are tiny (up to 1¼ arcsecond) variations of the Sun's ecliptic latitude (discussed in section below) that may mean the Sun's center is not precisely over the equator at the moment of equinox.

On the day of an equinox, daytime and nighttime are of approximately equal duration all over the planet. They are not exactly equal, however, due to the angular size of the Sun and atmospheric refraction. The word is derived from the Latin ', from ' (equal) and ' (genitive ') (night).

The equinoxes are the only times when the solar terminator (the "edge" between night and day) is perpendicular to the equator. As a result, the northern and southern hemispheres are equally illuminated. The word comes from Latin "aequus", meaning "equal", and "nox", meaning "night".

In other words, the equinoxes are the only times when the subsolar point is on the equator, meaning that the Sun is exactly overhead at a point on the equatorial line. The subsolar point crosses the equator moving northward at the March equinox and southward at the September equinox.

The equinoxes, along with solstices, are directly related to the seasons of the year. In the northern hemisphere, the vernal equinox (March) conventionally marks the beginning of spring in most cultures and is considered the start of the New Year in Hindu calendar and the Persian calendar or Iranian calendars as Nowruz (means new day), while the autumnal equinox (September) marks the beginning of autumn.

When Julius Caesar established the Julian calendar in 45 BC, he set 25 March as the date of the spring equinox. Because the Julian year is longer than the tropical year by about 11.3 minutes on average (or 1 day in 128 years), the calendar "drifted" with respect to the two equinoxes — such that in AD 300 the spring equinox occurred on about 21 March, and by AD 1500 it had drifted backwards to 11 March.

This drift induced Pope Gregory XIII to create a modern Gregorian calendar. The Pope wanted to continue to conform with the edicts concerning the date of Easter of the Council of Nicaea of AD 325, which means he wanted to move the vernal equinox to the date on which it fell at that time (21 March is the day allocated to it in the Easter table of the Julian calendar). However, the leap year intervals in his calendar were not smooth (400 is not an exact multiple of 97). This causes the equinox to oscillate by about 53 hours around its mean position. This in turn raised the possibility that it could fall on 22 March, and thus Easter Day might theoretically commence before the equinox. The astronomers chose the appropriate number of days to omit so that the equinox would swing from 19 to 21 March but never fall on the 22nd (although it can in a handful of years fall early in the morning of that day in the Far East).


Day is usually defined as the period when sunlight reaches the ground in the absence of local obstacles. On the day of the equinox, the center of the Sun spends a roughly equal amount of time above and below the horizon at every location on the Earth, so night and day are about the same length. Sunrise and sunset can be defined in several ways, but a widespread definition is the time that the top limb of the sun is level with the horizon. With this definition, the day is longer than the night at the equinoxes:

In sunrise/sunset tables, the assumed semidiameter (apparent radius) of the Sun is 16 arcminutes and the atmospheric refraction is assumed to be 34 arcminutes. Their combination means that when the upper limb of the Sun is on the visible horizon, its centre is 50 arcminutes below the geometric horizon, which is the intersection with the celestial sphere of a horizontal plane through the eye of the observer.

These effects make the day about 14 minutes longer than the night at the equator and longer still towards the poles. The real equality of day and night only happens in places far enough from the equator to have a seasonal difference in day length of at least 7 minutes, actually occurring a few days towards the winter side of each equinox.

The times of sunset and sunrise vary with the observer's location (longitude and latitude), so the dates when day and night are equal also depend upon the observer's location.

A third correction for the visual observation of a sunrise (or sunset) is the angle between the apparent horizon as seen by an observer and the geometric (or sensible) horizon. This is known as the dip of the horizon and varies from 3 arcminutes for a viewer standing on the sea shore to 160 arcminutes for a mountaineer on Everest. The effect of a larger dip on taller objects (reaching over 2½° of arc on Everest) accounts for the phenomenon of snow on a mountain peak turning gold in the sunlight long before the lower slopes are illuminated.

At the equinoxes, the rate of change for the length of daylight and night-time is the greatest. At the poles, the equinox marks the transition from 24 hours of nighttime to 24 hours of daylight (or vice versa).

The word "equilux" is sometimes (but rarely) used to mean a day when the durations of light and darkness are equal.

In the half-year centered on the June solstice, the Sun rises north of east and sets north of west, which means longer days with shorter nights for the northern hemisphere and shorter days with longer nights for the southern hemisphere. In the half-year centered on the December solstice, the Sun rises south of east and sets south of west and the durations of day and night are reversed.

Also on the day of an equinox, the Sun rises everywhere on Earth (except at the poles) at about 06:00 and sets at about 18:00 (local solar time). These times are not exact for several reasons:

Some of the statements above can be made clearer by picturing the day arc (i.e., the path along which the Sun appears to move across the sky). The pictures show this for every hour on equinox day. In addition, some 'ghost' suns are also indicated below the horizon, up to 18° below it; the Sun in such areas still causes twilight. The depictions presented below can be used for both the northern and the southern hemispheres. The observer is understood to be sitting near the tree on the island depicted in the middle of the ocean; the green arrows give cardinal directions.

The following special cases are depicted:
The vernal equinox occurs in March, about when the Sun crosses the celestial equator south to north. The term "vernal point" is used for the time of this occurrence and for the direction in space where the Sun is seen at that time, which is the origin of some celestial coordinate systems:
Strictly speaking, at the equinox the Sun's ecliptic longitude is zero. Its latitude will not be exactly zero since the Earth is not exactly in the plane of the ecliptic. Its declination will not be exactly zero either. (The ecliptic is defined by the center of mass of the Earth and Moon combined). The modern definition of equinox is the instants when the Sun's apparent geocentric longitude is 0° (northward equinox) or 180° (southward equinox). See the adjacent diagram.

Because of the precession of the Earth's axis, the position of the vernal point on the celestial sphere changes over time, and the equatorial and the ecliptic coordinate systems change accordingly. Thus when specifying celestial coordinates for an object, one has to specify at what time the vernal point and the celestial equator are taken. That reference time is called the equinox of date.

The autumnal equinox is at ecliptic longitude 180° and at right ascension 12h.

The upper culmination of the vernal point is considered the start of the sidereal day for the observer. The hour angle of the vernal point is, by definition, the observer's sidereal time.

Using the current official IAU constellation boundaries – and taking into account the variable precession speed and the rotation of the celestial equator – the equinoxes shift through the constellations as follows (expressed in astronomical year numbering when the year 0 = 1 BC, −1 = 2 BC, etc.):

The equinoxes are sometimes regarded as the start of spring and autumn. A number of traditional (harvest) festivals are celebrated on the date of the equinoxes.

One effect of equinoctial periods is the temporary disruption of communications satellites. For all geostationary satellites, there are a few days around the equinox when the sun goes directly behind the satellite relative to Earth (i.e. within the beam-width of the ground-station antenna) for a short period each day. The Sun's immense power and broad radiation spectrum overload the Earth station's reception circuits with noise and, depending on antenna size and other factors, temporarily disrupt or degrade the circuit. The duration of those effects varies but can range from a few minutes to an hour. (For a given frequency band, a larger antenna has a narrower beam-width and hence experiences shorter duration "Sun outage" windows.)

Equinoxes occur on any planet with a tilted rotational axis. A dramatic example is Saturn, where the equinox places its ring system edge-on facing the Sun. As a result, they are visible only as a thin line when seen from Earth. When seen from above – a view seen during an equinox for the first time from the "Cassini" space probe in 2009 – they receive very little sunshine, indeed more planetshine than light from the Sun. This phenomenon occurs once every 14.7 years on average, and can last a few weeks before and after the exact equinox. Saturn's most recent equinox was on 11 August 2009, and its next will take place on 6 May 2025.

Mars's most recent equinox was on 5 May 2017 (northern spring), and the next will be on 22 May 2018 (northern autumn).




</doc>
<doc id="10101" url="https://en.wikipedia.org/wiki?curid=10101" title="Eugene Wigner">
Eugene Wigner

Eugene Paul "E. P." Wigner (; November 17, 1902 – January 1, 1995), was a Hungarian-American theoretical physicist, engineer and mathematician. He received half of the Nobel Prize in Physics in 1963 "for his contributions to the theory of the atomic nucleus and the elementary particles, particularly through the discovery and application of fundamental symmetry principles".

A graduate of the Technical University of Berlin, Wigner worked as an assistant to Karl Weissenberg and Richard Becker at the Kaiser Wilhelm Institute in Berlin, and David Hilbert at the University of Göttingen. Wigner and Hermann Weyl were responsible for introducing group theory into physics, particularly the theory of symmetry in physics. Along the way he performed ground-breaking work in pure mathematics, in which he authored a number of mathematical theorems. In particular, Wigner's theorem is a cornerstone in the mathematical formulation of quantum mechanics. He is also known for his research into the structure of the atomic nucleus. In 1930, Princeton University recruited Wigner, along with John von Neumann, and he moved to the United States.

Wigner participated in a meeting with Leo Szilard and Albert Einstein that resulted in the Einstein-Szilard letter, which prompted President Franklin D. Roosevelt to initiate the Manhattan Project to develop atomic bombs. Wigner was afraid that the German nuclear weapon project would develop an atomic bomb first. During the Manhattan Project, he led a team whose task was to design nuclear reactors to convert uranium into weapons grade plutonium. At the time, reactors existed only on paper, and no reactor had yet gone critical. Wigner was disappointed that DuPont was given responsibility for the detailed design of the reactors, not just their construction. He became Director of Research and Development at the Clinton Laboratory (now the Oak Ridge National Laboratory) in early 1946, but became frustrated with bureaucratic interference by the Atomic Energy Commission, and returned to Princeton.

In the postwar period he served on a number of government bodies, including the National Bureau of Standards from 1947 to 1951, the mathematics panel of the National Research Council from 1951 to 1954, the physics panel of the National Science Foundation, and the influential General Advisory Committee of the Atomic Energy Commission from 1952 to 1957 and again from 1959 to 1964. In later life, he became more philosophical, and published "The Unreasonable Effectiveness of Mathematics in the Natural Sciences", his best-known work outside technical mathematics and physics.

Wigner Jenő Pál was born in Budapest, Austria-Hungary on November 17, 1902, to middle class Jewish parents, Elisabeth (Einhorn) and Anthony Wigner, a leather tanner. He had an older sister, Bertha, known as Biri, and a younger sister Margit, known as Manci, who later married British theoretical physicist Paul Dirac. He was home schooled by a professional teacher until the age of 9, when he started school at the third grade. During this period, Wigner developed an interest in mathematical problems. At the age of 11, Wigner contracted what his doctors believed to be tuberculosis. His parents sent him to live for six weeks in a sanatorium in the Austrian mountains, before the doctors concluded that the diagnosis was mistaken.

Wigner's family was Jewish, but not religiously observant, and his Bar Mitzvah was a secular one. From 1915 through 1919, he studied at the secondary grammar school called Fasori Evangélikus Gimnázium, the school his father had attended. Religious education was compulsory, and he attended classes in Judaism taught by a rabbi. A fellow student was János von Neumann, who was a year behind Wigner. They both benefited from the instruction of the noted mathematics teacher László Rátz. In 1919, to escape the Béla Kun communist regime, the Wigner family briefly fled to Austria, returning to Hungary after Kun's downfall. Partly as a reaction to the prominence of Jews in the Kun regime, the family converted to Lutheranism. Wigner explained later in his life that his family decision to convert to Lutheranism "was not at heart a religious decision but an anti-communist one". On religious views, Wigner was an atheist.

After graduating from the secondary school in 1920, Wigner enrolled at the Budapest University of Technical Sciences, known as the "Műegyetem". He was not happy with the courses on offer, and in 1921 enrolled at the "Technische Hochschule Berlin" (now Technical University of Berlin), where he studied chemical engineering. He also attended the Wednesday afternoon colloquia of the German Physical Society. These colloquia featured such luminaries as Max Planck, Max von Laue, Rudolf Ladenburg, Werner Heisenberg, Walther Nernst, Wolfgang Pauli, and Albert Einstein. Wigner also met the physicist Leó Szilárd, who at once became Wigner's closest friend. A third experience in Berlin was formative. Wigner worked at the Kaiser Wilhelm Institute for Physical Chemistry and Electrochemistry (now the Fritz Haber Institute), and there he met Michael Polanyi, who became, after László Rátz, Wigner's greatest teacher. Polanyi supervised Wigner's DSc thesis, "Bildung und Zerfall von Molekülen" ("Formation and Decay of Molecules").

Wigner returned to Budapest, where he went to work at his father's tannery, but in 1926, he accepted an offer from Karl Weissenberg at the Kaiser Wilhelm Institute in Berlin. Weissenberg wanted someone to assist him with his work on x-ray crystallography, and Polanyi had recommended Wigner. After six months as Weissenberg's assistant, Wigner went to work for Richard Becker for two semesters. Wigner explored quantum mechanics, studying the work of Erwin Schrödinger. He also delved into the group theory of Ferdinand Frobenius and Eduard Ritter von Weber.

Wigner received a request from Arnold Sommerfeld to work at the University of Göttingen as an assistant to the great mathematician David Hilbert. This proved a disappointment, as the aged Hilbert's abilities were failing, and his interests had shifted to logic. Wigner nonetheless studied independently. He laid the foundation for the theory of symmetries in quantum mechanics and in 1927 introduced what is now known as the Wigner D-matrix. Wigner and Hermann Weyl were responsible for introducing group theory into quantum mechanics. The latter had written a standard text, "Group Theory and Quantum Mechanics" (1928), but it was not easy to understand, especially for younger physicists. Wigner's "Group Theory and Its Application to the Quantum Mechanics of Atomic Spectra" (1931) made group theory accessible to a wider audience.
In these works, Wigner laid the foundation for the theory of symmetries in quantum mechanics. Wigner's theorem proved by Wigner in 1931, is a cornerstone of the mathematical formulation of quantum mechanics. The theorem specifies how physical symmetries such as rotations, translations, and CPT symmetry are represented on the Hilbert space of states. According to the theorem, any symmetry transformation is represented by a linear and unitary or antilinear and antiunitary transformation of Hilbert space. The representation of a symmetry group on a Hilbert space is either an ordinary representation or a projective representation.

In the late 1930s, Wigner extended his research into atomic nuclei. By 1929, his papers were drawing notice in the world of physics. In 1930, Princeton University recruited Wigner for a one-year lectureship, at 7 times the salary that he had been drawing in Europe. Princeton recruited von Neumann at the same time. Jenő Pál Wigner and János von Neumann had collaborated on three papers together in 1928 and two in 1929. They anglicized their first names to "Eugene" and "John", respectively. When their year was up, Princeton offered a five-year contract as visiting professors for half the year. The Technische Hochschule responded with a teaching assignment for the other half of the year. This was very timely, since the Nazis soon rose to power in Germany. At Princeton in 1934, Wigner introduced his sister Manci to the physicist Paul Dirac, whom she married.

Princeton did not rehire Wigner when his contract ran out in 1936. Through Gregory Breit, Wigner found new employment at the University of Wisconsin. There he met his first wife, Amelia Frank, who was a physics student there. However she died unexpectedly in 1937, leaving Wigner distraught. He therefore accepted a 1938 offer from Princeton to return there. Wigner became a naturalized citizen of the United States on January 8, 1937, and he brought his parents to the United States.

Although he was a professed political amateur, on August 2, 1939, he participated in a meeting with Leó Szilárd and Albert Einstein that resulted in the Einstein–Szilárd letter, which prompted President Franklin D. Roosevelt to initiate the Manhattan Project to develop atomic bombs. Wigner was afraid that the German nuclear weapon project would develop an atomic bomb first, and even refused to have his fingerprints taken because they could be used to track him down if Germany won. "Thoughts of being murdered," he later recalled, "focus your mind wonderfully."

On June 4, 1941, Wigner married his second wife, Mary Annette Wheeler, a professor of physics at Vassar College, who had completed her Ph.D. at Yale University in 1932. After the war she taught physics on the faculty of Rutgers University's Douglass College in New Jersey until her retirement in 1964. They remained married until her death in November 1977. They had two children, David Wigner and Martha Wigner Upton.

During the Manhattan Project, Wigner led a team that included Alvin M. Weinberg, Katharine Way, Gale Young and Edward Creutz. The group's task was to design the production nuclear reactors that would convert uranium into weapons grade plutonium. At the time, reactors existed only on paper, and no reactor had yet gone critical. In July 1942, Wigner chose a conservative 100 MW design, with a graphite neutron moderator and water cooling. Wigner was present at a converted rackets court under the stands at the University of Chicago's abandoned Stagg Field on December 2, 1942, when the world's first atomic reactor, Chicago Pile One (CP-1) achieved a controlled nuclear chain reaction.

Wigner was disappointed that DuPont was given responsibility for the detailed design of the reactors, not just their construction. He threatened to resign in February 1943, but was talked out of it by the head of the Metallurgical Laboratory, Arthur Compton, who sent him on vacation instead. As it turned out, a design decision by DuPont to give the reactor additional load tubes for more uranium saved the project when neutron poisoning became a problem. Without the additional tubes, the reactor could have been run at 35% power until the boron impurities in the graphite were burned up and enough plutonium produced to run the reactor at full power; but this would have set the project back a year. During the 1950s, he would even work for DuPont on the Savannah River Site. Wigner did not regret working on the Manhattan Project, and sometimes wished the atomic bomb had been ready a year earlier.

An important discovery Wigner made during the project was the Wigner effect. This is a swelling of the graphite moderator caused by the displacement of atoms by neutron radiation. The Wigner effect was a serious problem for the reactors at the Hanford Site in the immediate post-war period, and resulted in production cutbacks and a reactor being shut down entirely. It was eventually discovered that it could be overcome by controlled heating and annealing.

Through Manhattan project funding, Wigner and also developed an important general approach to nuclear reactions, the Wigner–Eisenbud R-matrix theory, which was published in 1947.

Wigner accepted a position as the Director of Research and Development at the Clinton Laboratory (now the Oak Ridge National Laboratory) in Oak Ridge, Tennessee in early 1946. Because he did not want to be involved in administrative duties, he became co-director of the laboratory, with James Lum handling the administrative chores as executive director. When the newly created Atomic Energy Commission (AEC) took charge of the laboratory's operations at the start of 1947, Wigner feared that many of the technical decisions would be made in Washington. He also saw the Army's continuation of wartime security policies at the laboratory as a "meddlesome oversight", interfering with research. One such incident occurred in March 1947, when the AEC discovered that Wigner's scientists were conducting experiments with a critical mass of uranium-235 when the Director of the Manhattan Project, Major General Leslie R. Groves, Jr., had forbidden such experiments in August 1946 after the death of Louis Slotin at the Los Alamos Laboratory. Wigner argued that Groves's order had been superseded, but was forced to terminate the experiments, which were completely different from the one that killed Slotin.

Feeling unsuited to a managerial role in such an environment, he left Oak Ridge at the end of summer in 1947 and returned to Princeton University, although he maintained a consulting role with the facility for many years. In the postwar period he served on a number of government bodies, including the National Bureau of Standards from 1947 to 1951, the mathematics panel of the National Research Council from 1951 to 1954, the physics panel of the National Science Foundation, and the influential General Advisory Committee of the Atomic Energy Commission from 1952 to 1957 and again from 1959 to 1964. He also contributed to civil defense.

Near the end of his life, Wigner's thoughts turned more philosophical. In 1960, he published a now classic article on the philosophy of mathematics and of physics, which has become his best-known work outside technical mathematics and physics, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences". He argued that biology and cognition could be the origin of physical concepts, as we humans perceive them, and that the happy coincidence that mathematics and physics were so well matched, seemed to be "unreasonable" and hard to explain. His original paper has provoked and inspired many responses across a wide range of disciplines. These included Richard Hamming in Computer Science, Arthur Lesk in Molecular Biology, Peter Norvig in data mining, Max Tegmark in Physics, Ivor Grattan-Guinness in Mathematics, and Vela Velupillai in Economics.

Wigner was awarded the Nobel Prize in Physics in 1963 "for his contributions to the theory of the atomic nucleus and the elementary particles, particularly through the discovery and application of fundamental symmetry principles". The prize was shared that year, with the other half of the award divided between Maria Goeppert-Mayer and J. Hans D. Jensen. Wigner professed that he had never considered the possibility that this might occur, and added: "I never expected to get my name in the newspapers without doing something wicked." He also won the Franklin Medal in 1950, the Enrico Fermi award in 1958, the Atoms for Peace Award in 1959, the Max Planck Medal in 1961, the National Medal of Science in 1969, the Albert Einstein Award in 1972, and the eponymous Wigner Medal in 1978. In 1968 he gave the Josiah Willard Gibbs lecture.

Mary died in November 1977. In 1979, Wigner married his third wife, Eileen Clare-Patton (Pat) Hamilton, the widow of physicist Donald Ross Hamilton, the Dean of the Graduate School at Princeton University, who had died in 1972. In 1992, at the age of 90, he published his memoirs, "The Recollections of Eugene P. Wigner" with Andrew Szanton. In it, Wigner said: "The full meaning of life, the collective meaning of all human desires, is fundamentally a mystery beyond our grasp. As a young man, I chafed at this state of affairs. But by now I have made peace with it. I even feel a certain honor to be associated with such a mystery." In his collection of essays "Symmetries and Reflections – Scientific Essays" (1995), he commented: "It was not possible to formulate the laws of quantum mechanics in a fully consistent way without reference to consciousness."

Wigner died of pneumonia at the University Medical Center in Princeton, New Jersey on 1 January 1995. He was survived by his wife Eileen and children Erika, David and Martha, and his sisters Bertha and Margit.







</doc>
<doc id="10103" url="https://en.wikipedia.org/wiki?curid=10103" title="Electroweak interaction">
Electroweak interaction

In particle physics, the electroweak interaction is the unified description of two of the four known fundamental interactions of nature: electromagnetism and the weak interaction. Although these two forces appear very different at everyday low energies, the theory models them as two different aspects of the same force. Above the unification energy, on the order of 246 GeV, they would merge into a single electroweak force. Thus, if the universe is hot enough (approximately 10 K, a temperature exceeded until shortly after the Big Bang), then the electromagnetic force and weak force merge into a combined electroweak force. During the quark epoch, the electroweak force split into the electromagnetic and weak force.

Sheldon Glashow, Abdus Salam, and Steven Weinberg were awarded the 1979 Nobel Prize in Physics for their contributions to the unification of the weak and electromagnetic interaction between elementary particles. The existence of the electroweak interactions was experimentally established in two stages, the first being the discovery of neutral currents in neutrino scattering by the Gargamelle collaboration in 1973, and the second in 1983 by the UA1 and the UA2 collaborations that involved the discovery of the W and Z gauge bosons in proton–antiproton collisions at the converted Super Proton Synchrotron. In 1999, Gerardus 't Hooft and Martinus Veltman were awarded the Nobel prize for showing that the electroweak theory is renormalizable.

Mathematically, the unification is accomplished under an "SU"(2) × "U"(1) gauge group. The corresponding gauge bosons are the three W bosons of weak isospin from "SU(2)" ("W, W", and "W"), and the "B" boson of weak hypercharge from "U(1)", respectively, all of which are massless.

In the Standard Model, the and bosons, and the photon, are produced by the spontaneous symmetry breaking of the electroweak symmetry from "SU"(2) × "U"(1) to "U"(1), caused by the Higgs mechanism (see also Higgs boson). "U"(1) and "U"(1) are different copies of "U"(1); the generator of "U"(1) is given by "Q" = "Y"/2 + "T", where "Y" is the generator of "U"(1) (called the weak hypercharge), and "T" is one of the "SU"(2) generators (a component of weak isospin).

The spontaneous symmetry breaking makes the and bosons coalesce into two different bosons – the boson, and the photon (),

where is the "weak mixing angle". The axes representing the particles have essentially just been rotated, in the ("W", "B") plane, by the angle . This also introduces a mismatch between the mass of the and the mass of the particles (denoted as and , respectively),

The and bosons, in turn, combine to give massive charged bosons

The distinction between electromagnetism and the weak force arises because there is a (nontrivial) linear combination of "Y" and "T" that vanishes for the Higgs boson (it is an eigenstate of both "Y" and "T", so the coefficients may be taken as −"T" and "Y"): "U"(1) is defined to be the group generated by this linear combination, and is unbroken because it does not interact with the Higgs.

The Lagrangian for the electroweak interactions is divided into four parts before electroweak symmetry breaking becomes manifest,

The formula_5 term describes the interaction between the three vector bosons and the vector boson, 
where formula_7 (formula_8) and formula_9 are the field strength tensors for the weak isospin and weak hypercharge gauge fields.

formula_10 is the kinetic term for the Standard Model fermions. The interaction of the gauge bosons and the fermions are through the gauge covariant derivative,
where the subscript runs over the three generations of fermions; , , and are the left-handed doublet, right-handed singlet up, and right handed singlet down quark fields; and and are the left-handed doublet and right-handed singlet electron fields.

The term describes the Higgs field and its interactions with itself and the gauge bosons,

The term displays the Yukawa interaction with the fermions, 
and generates their masses, manifest when the Higgs field acquires a nonzero vacuum expectation value, discussed next.

The Lagrangian reorganizes itself as the Higgs boson acquires a non-vanishing vacuum expectation value dictated by the potential of the previous section. As a result of this rewriting, the symmetry breaking becomes manifest. 

Due to its complexity, this Lagrangian is best described by breaking it up into several parts as follows.

The kinetic term formula_15 contains all the quadratic terms of the Lagrangian, which include the dynamic terms (the partial derivatives) and the mass terms (conspicuously absent from the Lagrangian before symmetry breaking)
where the sum runs over all the fermions of the theory (quarks and leptons), and the fields formula_17, formula_18, formula_19, and formula_20 are given as
with to be replaced by the relevant field, and by the structure constants of the appropriate gauge group.

The neutral current formula_22 and charged current formula_23 components of the Lagrangian contain the interactions between the fermions and gauge bosons,
where == ; while the electromagnetic current formula_25 and the neutral weak current formula_26 are
and
where formula_29 and formula_30 are the fermions' electric charges and weak isospin.

The charged current part of the Lagrangian is given by
where formula_32 contains the Higgs three-point and four-point self interaction terms,

formula_34 contains the Higgs interactions with gauge vector bosons,

formula_36 contains the gauge three-point self interactions,

formula_38 contains the gauge four-point self interactions,

formula_40 contains the Yukawa interactions between the fermions and the Higgs field,

Note the formula_42 factors in the weak couplings: these factors project out the left handed components of the spinor fields. This is why electroweak theory is said to be a chiral theory.






</doc>
<doc id="10104" url="https://en.wikipedia.org/wiki?curid=10104" title="Elara">
Elara

Elara may refer to one of the following:



</doc>
<doc id="10105" url="https://en.wikipedia.org/wiki?curid=10105" title="Erasmus Reinhold">
Erasmus Reinhold

Erasmus Reinhold (October 22, 1511 – February 19, 1553) was a German astronomer and mathematician, considered to be the most influential astronomical pedagogue of his generation. He was born and died in Saalfeld, Saxony.

He was educated, under Jacob Milich, at the University of Wittenberg, where he was first elected dean and later became rector. In 1536 he was appointed professor of higher mathematics by Philipp Melanchthon. In contrast to the limited modern definition, "mathematics" at the time also included applied mathematics, especially astronomy. His colleague, Georg Joachim Rheticus, also studied at Wittenberg and was appointed professor of lower mathematics in 1536.

Reinhold catalogued a large number of stars. His publications on astronomy include a commentary (1542, 1553) on Georg Purbach's "Theoricae novae planetarum". Reinhold knew about Copernicus and his heliocentric ideas prior to the publication of "De revolutionibis" and made a favourable reference to him in his commentary on Purbach. However, Reinhold (like other astronomers before Kepler and Galileo) translated Copernicus' mathematical methods back into a geocentric system, rejecting heliocentric cosmology on physical and theological grounds.

Duke Albert of Brandenburg Prussia supported Reinhold and financed the printing of Reinhold's "Prutenicae Tabulae" or "Prussian Tables". These astronomical tables helped to disseminate calculation methods of Copernicus throughout the Empire, however, Gingerich notes that they showed a "notable lack of commitment" to heliocentricity and were "carefully framed" to be independent of the movement of the Earth. Both Reinhold's "Prutenic Tables" and Copernicus' studies were the foundation for the Calendar Reform by Pope Gregory XIII in 1582.

It was Reinhold's heavily annotated copy of "De revolutionibus" in the Royal Observatory, Edinburgh, that started Owen Gingerich on his search for copies of the first and second editions which he describes in "The Book Nobody Read". In Reinhold's unpublished commentary on "De revolutionibus", he calculated the distance from the Earth to the sun. He "massaged" his calculation method in order to arrive at an answer close to that of Ptolemy.

His name has been given to a prominent lunar impact crater that lies to the south-southwest of the crater Copernicus, on the Mare Insularum.


</doc>
<doc id="10106" url="https://en.wikipedia.org/wiki?curid=10106" title="Earthquake">
Earthquake

An earthquake (also known as a quake, tremor or temblor) is the shaking of the surface of the Earth, resulting from the sudden release of energy in the Earth's lithosphere that creates seismic waves. Earthquakes can range in size from those that are so weak that they cannot be felt to those violent enough to toss people around and destroy whole cities. The seismicity or seismic activity of an area refers to the frequency, type and size of earthquakes experienced over a period of time. The word "tremor" is also used for non-earthquake seismic rumbling.

At the Earth's surface, earthquakes manifest themselves by shaking and sometimes displacement of the ground. When the epicenter of a large earthquake is located offshore, the seabed may be displaced sufficiently to cause a tsunami. Earthquakes can also trigger landslides, and occasionally volcanic activity.

In its most general sense, the word "earthquake" is used to describe any seismic event — whether natural or caused by humans — that generates seismic waves. Earthquakes are caused mostly by rupture of geological faults, but also by other events such as volcanic activity, landslides, mine blasts, and nuclear tests. An earthquake's point of initial rupture is called its focus or hypocenter. The epicenter is the point at ground level directly above the hypocenter.

Tectonic earthquakes occur anywhere in the earth where there is sufficient stored elastic strain energy to drive fracture propagation along a fault plane. The sides of a fault move past each other smoothly and aseismically only if there are no irregularities or asperities along the fault surface that increase the frictional resistance. Most fault surfaces do have such asperities and this leads to a form of stick-slip behavior. Once the fault has locked, continued relative motion between the plates leads to increasing stress and therefore, stored strain energy in the volume around the fault surface. This continues until the stress has risen sufficiently to break through the asperity, suddenly allowing sliding over the locked portion of the fault, releasing the stored energy. This energy is released as a combination of radiated elastic strain seismic waves, frictional heating of the fault surface, and cracking of the rock, thus causing an earthquake. This process of gradual build-up of strain and stress punctuated by occasional sudden earthquake failure is referred to as the elastic-rebound theory. It is estimated that only 10 percent or less of an earthquake's total energy is radiated as seismic energy. Most of the earthquake's energy is used to power the earthquake fracture growth or is converted into heat generated by friction. Therefore, earthquakes lower the Earth's available elastic potential energy and raise its temperature, though these changes are negligible compared to the conductive and convective flow of heat out from the Earth's deep interior.

There are three main types of fault, all of which may cause an interplate earthquake: normal, reverse (thrust) and strike-slip. Normal and reverse faulting are examples of dip-slip, where the displacement along the fault is in the direction of dip and movement on them involves a vertical component. Normal faults occur mainly in areas where the crust is being extended such as a divergent boundary. Reverse faults occur in areas where the crust is being shortened such as at a convergent boundary. Strike-slip faults are steep structures where the two sides of the fault slip horizontally past each other; transform boundaries are a particular type of strike-slip fault. Many earthquakes are caused by movement on faults that have components of both dip-slip and strike-slip; this is known as oblique slip.

Reverse faults, particularly those along convergent plate boundaries are associated with the most powerful earthquakes, megathrust earthquakes, including almost all of those of magnitude 8 or more. Strike-slip faults, particularly continental transforms, can produce major earthquakes up to about magnitude 8. Earthquakes associated with normal faults are generally less than magnitude 7. For every unit increase in magnitude, there is a roughly thirtyfold increase in the energy released. For instance, an earthquake of magnitude 6.0 releases approximately 30 times more energy than a 5.0 magnitude earthquake and a 7.0 magnitude earthquake releases 900 times (30 × 30) more energy than a 5.0 magnitude of earthquake. An 8.6 magnitude earthquake releases the same amount of energy as 10,000 atomic bombs like those used in World War II.

This is so because the energy released in an earthquake, and thus its magnitude, is proportional to the area of the fault that ruptures and the stress drop. Therefore, the longer the length and the wider the width of the faulted area, the larger the resulting magnitude. The topmost, brittle part of the Earth's crust, and the cool slabs of the tectonic plates that are descending down into the hot mantle, are the only parts of our planet which can store elastic energy and release it in fault ruptures. Rocks hotter than about 300 degrees Celsius flow in response to stress; they do not rupture in earthquakes. The maximum observed lengths of ruptures and mapped faults (which may break in a single rupture) are approximately 1000 km. Examples are the earthquakes in Chile, 1960; Alaska, 1957; Sumatra, 2004, all in subduction zones. The longest earthquake ruptures on strike-slip faults, like the San Andreas Fault (1857, 1906), the North Anatolian Fault in Turkey (1939) and the Denali Fault in Alaska (2002), are about half to one third as long as the lengths along subducting plate margins, and those along normal faults are even shorter.

The most important parameter controlling the maximum earthquake magnitude on a fault is however not the maximum available length, but the available width because the latter varies by a factor of 20. Along converging plate margins, the dip angle of the rupture plane is very shallow, typically about 10 degrees. Thus the width of the plane within the top brittle crust of the Earth can become 50 to 100 km (Japan, 2011; Alaska, 1964), making the most powerful earthquakes possible.

Strike-slip faults tend to be oriented near vertically, resulting in an approximate width of 10 km within the brittle crust, thus earthquakes with magnitudes much larger than 8 are not possible. Maximum magnitudes along many normal faults are even more limited because many of them are located along spreading centers, as in Iceland, where the thickness of the brittle layer is only about 6 km.

In addition, there exists a hierarchy of stress level in the three fault types. Thrust faults are generated by the highest, strike slip by intermediate, and normal faults by the lowest stress levels. This can easily be understood by considering the direction of the greatest principal stress, the direction of the force that 'pushes' the rock mass during the faulting. In the case of normal faults, the rock mass is pushed down in a vertical direction, thus the pushing force ("greatest" principal stress) equals the weight of the rock mass itself. In the case of thrusting, the rock mass 'escapes' in the direction of the least principal stress, namely upward, lifting the rock mass up, thus the overburden equals the "least" principal stress. Strike-slip faulting is intermediate between the other two types described above. This difference in stress regime in the three faulting environments can contribute to differences in stress drop during faulting, which contributes to differences in the radiated energy, regardless of fault dimensions.

Where plate boundaries occur within the continental lithosphere, deformation is spread out over a much larger area than the plate boundary itself. In the case of the San Andreas fault continental transform, many earthquakes occur away from the plate boundary and are related to strains developed within the broader zone of deformation caused by major irregularities in the fault trace (e.g., the "Big bend" region). The Northridge earthquake was associated with movement on a blind thrust within such a zone. Another example is the strongly oblique convergent plate boundary between the Arabian and Eurasian plates where it runs through the northwestern part of the Zagros Mountains. The deformation associated with this plate boundary is partitioned into nearly pure thrust sense movements perpendicular to the boundary over a wide zone to the southwest and nearly pure strike-slip motion along the Main Recent Fault close to the actual plate boundary itself. This is demonstrated by earthquake focal mechanisms.

All tectonic plates have internal stress fields caused by their interactions with neighboring plates and sedimentary loading or unloading (e.g. deglaciation). These stresses may be sufficient to cause failure along existing fault planes, giving rise to intraplate earthquakes.

The majority of tectonic earthquakes originate at the ring of fire in depths not exceeding tens of kilometers. Earthquakes occurring at a depth of less than 70 km are classified as 'shallow-focus' earthquakes, while those with a focal-depth between 70 and 300 km are commonly termed 'mid-focus' or 'intermediate-depth' earthquakes. In subduction zones, where older and colder oceanic crust descends beneath another tectonic plate, Deep-focus earthquakes may occur at much greater depths (ranging from 300 up to 700 kilometers). These seismically active areas of subduction are known as Wadati–Benioff zones. Deep-focus earthquakes occur at a depth where the subducted lithosphere should no longer be brittle, due to the high temperature and pressure. A possible mechanism for the generation of deep-focus earthquakes is faulting caused by olivine undergoing a phase transition into a spinel structure.

Earthquakes often occur in volcanic regions and are caused there, both by tectonic faults and the movement of magma in volcanoes. Such earthquakes can serve as an early warning of volcanic eruptions, as during the 1980 eruption of Mount St. Helens. Earthquake swarms can serve as markers for the location of the flowing magma throughout the volcanoes. These swarms can be recorded by seismometers and tiltmeters (a device that measures ground slope) and used as sensors to predict imminent or upcoming eruptions.

A tectonic earthquake begins by an initial rupture at a point on the fault surface, a process known as nucleation. The scale of the nucleation zone is uncertain, with some evidence, such as the rupture dimensions of the smallest earthquakes, suggesting that it is smaller than 100 m while other evidence, such as a slow component revealed by low-frequency spectra of some earthquakes, suggest that it is larger. The possibility that the nucleation involves some sort of preparation process is supported by the observation that about 40% of earthquakes are preceded by foreshocks. Once the rupture has initiated, it begins to propagate along the fault surface. The mechanics of this process are poorly understood, partly because it is difficult to recreate the high sliding velocities in a laboratory. Also the effects of strong ground motion make it very difficult to record information close to a nucleation zone.

Rupture propagation is generally modeled using a fracture mechanics approach, likening the rupture to a propagating mixed mode shear crack. The rupture velocity is a function of the fracture energy in the volume around the crack tip, increasing with decreasing fracture energy. The velocity of rupture propagation is orders of magnitude faster than the displacement velocity across the fault. Earthquake ruptures typically propagate at velocities that are in the range 70–90% of the S-wave velocity, and this is independent of earthquake size. A small subset of earthquake ruptures appear to have propagated at speeds greater than the S-wave velocity. These supershear earthquakes have all been observed during large strike-slip events. The unusually wide zone of coseismic damage caused by the 2001 Kunlun earthquake has been attributed to the effects of the sonic boom developed in such earthquakes. Some earthquake ruptures travel at unusually low velocities and are referred to as slow earthquakes. A particularly dangerous form of slow earthquake is the tsunami earthquake, observed where the relatively low felt intensities, caused by the slow propagation speed of some great earthquakes, fail to alert the population of the neighboring coast, as in the 1896 Sanriku earthquake.

Tides may induce some seismicity, see tidal triggering of earthquakes for details.

Most earthquakes form part of a sequence, related to each other in terms of location and time. Most earthquake clusters consist of small tremors that cause little to no damage, but there is a theory that earthquakes can recur in a regular pattern.

An aftershock is an earthquake that occurs after a previous earthquake, the mainshock. An aftershock is in the same region of the main shock but always of a smaller magnitude. If an aftershock is larger than the main shock, the aftershock is redesignated as the main shock and the original main shock is redesignated as a foreshock. Aftershocks are formed as the crust around the displaced fault plane adjusts to the effects of the main shock.

Earthquake swarms are sequences of earthquakes striking in a specific area within a short period of time. They are different from earthquakes followed by a series of aftershocks by the fact that no single earthquake in the sequence is obviously the main shock, therefore none have notable higher magnitudes than the other. An example of an earthquake swarm is the 2004 activity at Yellowstone National Park. In August 2012, a swarm of earthquakes shook Southern California's Imperial Valley, showing the most recorded activity in the area since the 1970s.

Sometimes a series of earthquakes occur in what has been called an "earthquake storm", where the earthquakes strike a fault in clusters, each triggered by the shaking or stress redistribution of the previous earthquakes. Similar to aftershocks but on adjacent segments of fault, these storms occur over the course of years, and with some of the later earthquakes as damaging as the early ones. Such a pattern was observed in the sequence of about a dozen earthquakes that struck the North Anatolian Fault in Turkey in the 20th century and has been inferred for older anomalous clusters of large earthquakes in the Middle East.

Quaking or shaking of the earth is a common phenomenon undoubtedly known to humans from earliest times. Prior to the development of strong-motion accelerometers that can measure peak ground speed and acceleration directly, the intensity of the earth-shaking was estimated on the basis of the observed effects, as categorized on various seismic intensity scales. Only in the last century has the source of such shaking been identified as ruptures in the earth's crust, with the intensity of shaking at any locality dependent not only on the local ground conditions, but also on the strength or "magnitude" of the rupture, and on its distance.

The first scale for measuring earthquake magnitudes was developed by Charles F. Richter in 1935. Subsequent scales (see seismic magnitude scales) have retained a key feature, where each unit represents a ten-fold difference in the amplitude of the ground shaking, and a 32-fold difference in energy. Subsequent scales are also adjusted to have approximately the same numeric value within the limits of the scale.

Although the mass media commonly reports earthquake magnitudes as "Richter magnitude" or "Richter scale", standard practice by most seismological authorities is to express an earthquake's strength on the moment magnitude scale, which is based on the actual energy released by an earthquake.

It is estimated that around 500,000 earthquakes occur each year, detectable with current instrumentation. About 100,000 of these can be felt. Minor earthquakes occur nearly constantly around the world in places like California and Alaska in the U.S., as well as in El Salvador, Mexico, Guatemala, Chile, Peru, Indonesia, Iran, Pakistan, the Azores in Portugal, Turkey, New Zealand, Greece, Italy, India, Nepal and Japan, but earthquakes can occur almost anywhere, including Downstate New York, England, and Australia. Larger earthquakes occur less frequently, the relationship being exponential; for example, roughly ten times as many earthquakes larger than magnitude 4 occur in a particular time period than earthquakes larger than magnitude 5. In the (low seismicity) United Kingdom, for example, it has been calculated that the average recurrences are:
an earthquake of 3.7–4.6 every year, an earthquake of 4.7–5.5 every 10 years, and an earthquake of 5.6 or larger every 100 years. This is an example of the Gutenberg–Richter law.

The number of seismic stations has increased from about 350 in 1931 to many thousands today. As a result, many more earthquakes are reported than in the past, but this is because of the vast improvement in instrumentation, rather than an increase in the number of earthquakes. The United States Geological Survey estimates that, since 1900, there have been an average of 18 major earthquakes (magnitude 7.0–7.9) and one great earthquake (magnitude 8.0 or greater) per year, and that this average has been relatively stable. In recent years, the number of major earthquakes per year has decreased, though this is probably a statistical fluctuation rather than a systematic trend. More detailed statistics on the size and frequency of earthquakes is available from the United States Geological Survey (USGS).
A recent increase in the number of major earthquakes has been noted, which could be explained by a cyclical pattern of periods of intense tectonic activity, interspersed with longer periods of low-intensity. However, accurate recordings of earthquakes only began in the early 1900s, so it is too early to categorically state that this is the case.

Most of the world's earthquakes (90%, and 81% of the largest) take place in the 40,000 km long, horseshoe-shaped zone called the circum-Pacific seismic belt, known as the Pacific Ring of Fire, which for the most part bounds the Pacific Plate. Massive earthquakes tend to occur along other plate boundaries, too, such as along the Himalayan Mountains.

With the rapid growth of mega-cities such as Mexico City, Tokyo and Tehran, in areas of high seismic risk, some seismologists are warning that a single quake may claim the lives of up to 3 million people.

While most earthquakes are caused by movement of the Earth's tectonic plates, human activity can also produce earthquakes. Four main activities contribute to this phenomenon: storing large amounts of water behind a dam (and possibly building an extremely heavy building), drilling and injecting liquid into wells, and by coal mining and oil drilling. Perhaps the best known example is the 2008 Sichuan earthquake in China's Sichuan Province in May; this tremor resulted in 69,227 fatalities and is the 19th deadliest earthquake of all time. The Zipingpu Dam is believed to have fluctuated the pressure of the fault away; this pressure probably increased the power of the earthquake and accelerated the rate of movement for the fault. The greatest earthquake in Australia's history is also claimed to be induced by humanity, through coal mining. The city of Newcastle was built over a large sector of coal mining areas. The earthquake has been reported to be spawned from a fault that reactivated due to the millions of tonnes of rock removed in the mining process.

The instrumental scales used to describe the size of an earthquake began with the Richter magnitude scale in the 1930s. It is a relatively simple measurement of an event's amplitude, and its use has become minimal in the 21st century. Seismic waves travel through the Earth's interior and can be recorded by seismometers at great distances. The surface wave magnitude was developed in the 1950s as a means to measure remote earthquakes and to improve the accuracy for larger events. The moment magnitude scale measures the amplitude of the shock, but also takes into account the seismic moment (total rupture area, average slip of the fault, and rigidity of the rock). The Japan Meteorological Agency seismic intensity scale, the Medvedev–Sponheuer–Karnik scale, and the Mercalli intensity scale are based on the observed effects and are related to the intensity of shaking.

Every tremor produces different types of seismic waves, which travel through rock with different velocities:
Propagation velocity of the seismic waves ranges from approx. 3 km/s up to 13 km/s, depending on the density and elasticity of the medium. In the Earth's interior the shock- or P waves travel much faster than the S waves (approx. relation 1.7 : 1). The differences in travel time from the epicenter to the observatory are a measure of the distance and can be used to image both sources of quakes and structures within the Earth. Also, the depth of the hypocenter can be computed roughly.

In solid rock P-waves travel at about 6 to 7 km per second; the velocity increases within the deep mantle to ~13 km/s. The velocity of S-waves ranges from 2–3 km/s in light sediments and 4–5 km/s in the Earth's crust up to 7 km/s in the deep mantle. As a consequence, the first waves of a distant earthquake arrive at an observatory via the Earth's mantle.

On average, the kilometer distance to the earthquake is the number of seconds between the P and S wave times 8. Slight deviations are caused by inhomogeneities of subsurface structure. By such analyses of seismograms the Earth's core was located in 1913 by Beno Gutenberg.

S waves and later arriving surface waves do main damage compared to P waves. P wave squeezes and expands material in the same direction it is traveling. S wave shakes the ground up and down and back and forth.

Earthquakes are not only categorized by their magnitude but also by the place where they occur. The world is divided into 754 Flinn–Engdahl regions (F-E regions), which are based on political and geographical boundaries as well as seismic activity. More active zones are divided into smaller F-E regions whereas less active zones belong to larger F-E regions.

Standard reporting of earthquakes includes its magnitude, date and time of occurrence, geographic coordinates of its epicenter, depth of the epicenter, geographical region, distances to population centers, location uncertainty, a number of parameters that are included in USGS earthquake reports (number of stations reporting, number of observations, etc.), and a unique event ID.

Although relatively slow seismic waves have traditionally been used to detect earthquakes, scientists realized in 2016 that gravitational measurements could provide instantaneous detection of earthquakes, and confirmed this by analyzing gravitational records associated with the 2011 Tohoku-Oki ("Fukushima") earthquake.

The effects of earthquakes include, but are not limited to, the following:

Shaking and ground rupture are the main effects created by earthquakes, principally resulting in more or less severe damage to buildings and other rigid structures. The severity of the local effects depends on the complex combination of the earthquake magnitude, the distance from the epicenter, and the local geological and geomorphological conditions, which may amplify or reduce wave propagation. The ground-shaking is measured by ground acceleration.

Specific local geological, geomorphological, and geostructural features can induce high levels of shaking on the ground surface even from low-intensity earthquakes. This effect is called site or local amplification. It is principally due to the transfer of the seismic motion from hard deep soils to soft superficial soils and to effects of seismic energy focalization owing to typical geometrical setting of the deposits.

Ground rupture is a visible breaking and displacement of the Earth's surface along the trace of the fault, which may be of the order of several meters in the case of major earthquakes. Ground rupture is a major risk for large engineering structures such as dams, bridges and nuclear power stations and requires careful mapping of existing faults to identify any which are likely to break the ground surface within the life of the structure.

Earthquakes, along with severe storms, volcanic activity, coastal wave attack, and wildfires, can produce slope instability leading to landslides, a major geological hazard. Landslide danger may persist while emergency personnel are attempting rescue.

Earthquakes can cause fires by damaging electrical power or gas lines. In the event of water mains rupturing and a loss of pressure, it may also become difficult to stop the spread of a fire once it has started. For example, more deaths in the 1906 San Francisco earthquake were caused by fire than by the earthquake itself.

Soil liquefaction occurs when, because of the shaking, water-saturated granular material (such as sand) temporarily loses its strength and transforms from a solid to a liquid. Soil liquefaction may cause rigid structures, like buildings and bridges, to tilt or sink into the liquefied deposits. For example, in the 1964 Alaska earthquake, soil liquefaction caused many buildings to sink into the ground, eventually collapsing upon themselves.

Tsunamis are long-wavelength, long-period sea waves produced by the sudden or abrupt movement of large volumes of water – including when an earthquake occurs at sea. In the open ocean the distance between wave crests can surpass , and the wave periods can vary from five minutes to one hour. Such tsunamis travel 600–800 kilometers per hour (373–497 miles per hour), depending on water depth. Large waves produced by an earthquake or a submarine landslide can overrun nearby coastal areas in a matter of minutes. Tsunamis can also travel thousands of kilometers across open ocean and wreak destruction on far shores hours after the earthquake that generated them.

Ordinarily, subduction earthquakes under magnitude 7.5 on the Richter magnitude scale do not cause tsunamis, although some instances of this have been recorded. Most destructive tsunamis are caused by earthquakes of magnitude 7.5 or more.

A flood is an overflow of any amount of water that reaches land. Floods occur usually when the volume of water within a body of water, such as a river or lake, exceeds the total capacity of the formation, and as a result some of the water flows or sits outside of the normal perimeter of the body. However, floods may be secondary effects of earthquakes, if dams are damaged. Earthquakes may cause landslips to dam rivers, which collapse and cause floods.

The terrain below the Sarez Lake in Tajikistan is in danger of catastrophic flood if the landslide dam formed by the earthquake, known as the Usoi Dam, were to fail during a future earthquake. Impact projections suggest the flood could affect roughly 5 million people.

An earthquake may cause injury and loss of life, road and bridge damage, general property damage, and collapse or destabilization (potentially leading to future collapse) of buildings. The aftermath may bring disease, lack of basic necessities, mental consequences such as panic attacks, depression to survivors, and higher insurance premiums.

One of the most devastating earthquakes in recorded history was the 1556 Shaanxi earthquake, which occurred on 23 January 1556 in Shaanxi province, China. More than 830,000 people died. Most houses in the area were yaodongs—dwellings carved out of loess hillsides—and many victims were killed when these structures collapsed. The 1976 Tangshan earthquake, which killed between 240,000 and 655,000 people, was the deadliest of the 20th century.

The 1960 Chilean earthquake is the largest earthquake that has been measured on a seismograph, reaching 9.5 magnitude on 22 May 1960. Its epicenter was near Cañete, Chile. The energy released was approximately twice that of the next most powerful earthquake, the Good Friday earthquake (March 27, 1964) which was centered in Prince William Sound, Alaska. The ten largest recorded earthquakes have all been megathrust earthquakes; however, of these ten, only the 2004 Indian Ocean earthquake is simultaneously one of the deadliest earthquakes in history.

Earthquakes that caused the greatest loss of life, while powerful, were deadly because of their proximity to either heavily populated areas or the ocean, where earthquakes often create tsunamis that can devastate communities thousands of kilometers away. Regions most at risk for great loss of life include those where earthquakes are relatively rare but powerful, and poor regions with lax, unenforced, or nonexistent seismic building codes.

Earthquake prediction is a branch of the science of seismology concerned with the specification of the time, location, and magnitude of future earthquakes within stated limits. Many methods have been developed for predicting the time and place in which earthquakes will occur. Despite considerable research efforts by seismologists, scientifically reproducible predictions cannot yet be made to a specific day or month.

While forecasting is usually considered to be a type of prediction, earthquake forecasting is often differentiated from earthquake prediction. Earthquake forecasting is concerned with the probabilistic assessment of general earthquake hazard, including the frequency and magnitude of damaging earthquakes in a given area over years or decades. For well-understood faults the probability that a segment may rupture during the next few decades can be estimated.

Earthquake warning systems have been developed that can provide regional notification of an earthquake in progress, but before the ground surface has begun to move, potentially allowing people within the system's range to seek shelter before the earthquake's impact is felt.

The objective of earthquake engineering is to foresee the impact of earthquakes on buildings and other structures and to design such structures to minimize the risk of damage. Existing structures can be modified by seismic retrofitting to improve their resistance to earthquakes. Earthquake insurance can provide building owners with financial protection against losses resulting from earthquakes.

Emergency management strategies can be employed by a government or organization to mitigate risks and prepare for consequences.

From the lifetime of the Greek philosopher Anaxagoras in the 5th century BCE to the 14th century CE, earthquakes were usually attributed to "air (vapors) in the cavities of the Earth." Thales of Miletus, who lived from 625–547 (BCE) was the only documented person who believed that earthquakes were caused by tension between the earth and water. Other theories existed, including the Greek philosopher Anaxamines' (585–526 BCE) beliefs that short incline episodes of dryness and wetness caused seismic activity. The Greek philosopher Democritus (460–371 BCE) blamed water in general for earthquakes. Pliny the Elder called earthquakes "underground thunderstorms."

In recent studies, geologists claim that global warming is one of the reasons for increased seismic activity. According to these studies melting glaciers and rising sea levels disturb the balance of pressure on Earth's tectonic plates thus causing increase in the frequency and intensity of earthquakes.

In Norse mythology, earthquakes were explained as the violent struggling of the god Loki. When Loki, god of mischief and strife, murdered Baldr, god of beauty and light, he was punished by being bound in a cave with a poisonous serpent placed above his head dripping venom. Loki's wife Sigyn stood by him with a bowl to catch the poison, but whenever she had to empty the bowl the poison dripped on Loki's face, forcing him to jerk his head away and thrash against his bonds, which caused the earth to tremble.

In Greek mythology, Poseidon was the cause and god of earthquakes. When he was in a bad mood, he struck the ground with a trident, causing earthquakes and other calamities. He also used earthquakes to punish and inflict fear upon people as revenge.

In Japanese mythology, Namazu (鯰) is a giant catfish who causes earthquakes. Namazu lives in the mud beneath the earth, and is guarded by the god Kashima who restrains the fish with a stone. When Kashima lets his guard fall, Namazu thrashes about, causing violent earthquakes.

In modern popular culture, the portrayal of earthquakes is shaped by the memory of great cities laid waste, such as Kobe in 1995 or San Francisco in 1906. Fictional earthquakes tend to strike suddenly and without warning. For this reason, stories about earthquakes generally begin with the disaster and focus on its immediate aftermath, as in "Short Walk to Daylight" (1972), "The Ragged Edge" (1968) or "" (1999). A notable example is Heinrich von Kleist's classic novella, "The Earthquake in Chile", which describes the destruction of Santiago in 1647. Haruki Murakami's short fiction collection After the Quake depicts the consequences of the Kobe earthquake of 1995.

The most popular single earthquake in fiction is the hypothetical "Big One" expected of California's San Andreas Fault someday, as depicted in the novels "Richter 10" (1996), "Goodbye California" (1977), "2012" (2009) and "San Andreas" (2015) among other works. Jacob M. Appel's widely anthologized short story, "A Comparative Seismology", features a con artist who convinces an elderly woman that an apocalyptic earthquake is imminent.

Contemporary depictions of earthquakes in film are variable in the manner in which they reflect human psychological reactions to the actual trauma that can be caused to directly afflicted families and their loved ones. Disaster mental health response research emphasizes the need to be aware of the different roles of loss of family and key community members, loss of home and familiar surroundings, loss of essential supplies and services to maintain survival. Particularly for children, the clear availability of caregiving adults who are able to protect, nourish, and clothe them in the aftermath of the earthquake, and to help them make sense of what has befallen them has been shown even more important to their emotional and physical health than the simple giving of provisions. As was observed after other disasters involving destruction and loss of life and their media depictions, recently observed in the 2010 Haiti earthquake, it is also important not to pathologize the reactions to loss and displacement or disruption of governmental administration and services, but rather to validate these reactions, to support constructive problem-solving and reflection as to how one might improve the conditions of those affected.






</doc>

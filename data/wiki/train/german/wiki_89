<doc id="13271" url="https://de.wikipedia.org/wiki?curid=13271" title="Indianer">
Indianer

Indianer ist die im Deutschen verbreitete Sammelbezeichnung für die indigenen Völker Amerikas – ausgenommen die Eskimovölker und Aleuten der arktischen Gebiete – sowie der amerikanischen Pazifikinseln. Ihre Vorfahren haben Amerika in frühgeschichtlicher Zeit von Asien aus besiedelt und dort eine Vielzahl von Kulturen und Sprachen entwickelt. „Indianer“ ist dabei eine Fremdbezeichnung durch die Kolonialisten, eine entsprechende Selbstbetitelung der weit über zweitausend Gruppen besteht nicht. Allerdings gibt es übergreifende Begriffe in Kanada, in den USA sowie im ehemals spanischen und im portugiesischen Teil Amerikas.

Ihre Vorfahren entwickelten zunächst die mitgebrachte Jäger- und Sammlerkultur fort, lebten bald – teilweise nomadisch – überwiegend von Landsäugetieren wie Bisons, Karibus und Guanacos oder von Vögeln wie Nandus. Sie befuhren aber auch bereits kurz nach der letzten Eiszeit den Pazifik entlang der Küste. Keramik, Ackerbau (wie der vor 4000 v. Chr. einsetzende Anbau von Kürbissen) und abgestufte Formen der Sesshaftigkeit sowie sehr früher Fernhandel kennzeichneten die Kulturen im Norden des Kontinents, während im Süden regional Bewässerungswirtschaft zu höheren Erträgen und vor 3000 v. Chr. zu städtischen Kulturen führte, die nach Norden bis an den Mississippi River und in den Süden Kanadas reichten. Daneben existierten weiterhin Wildbeuterkulturen, die die Landschaft in großem Ausmaß nutzten und veränderten.

Im 16. Jahrhundert vernichteten iberische Eroberer („Konquistadoren“) innerhalb weniger Jahrzehnte die Großreiche Mittel- und Südamerikas. Noch zerstörerischer wirkten sich allerdings die von den Europäern unbewusst eingeschleppten Krankheiten aus, die ganze Regionen entvölkerten. In Nordamerika gerieten die als „Indianer“ bezeichneten Einwohner ab 1600 nach und nach in die Minderheit, da ihre Zahl rapide abnahm, während die der Zugewanderten zunahm. Dieser Prozess dauerte bis in das 19. und frühe 20. Jahrhundert an. Nur in einigen Gebieten wie in Bolivien und im Süden Mexikos befinden sich Indianer heute in der Mehrheit. Die meisten Staaten versuchten, die als „minderwertig“ betrachteten Kulturen auszulöschen: durch gezielte Assimilationspolitik; dabei vor allem durch Einweisung der Kinder in Internate und durch Versuche, die „Indianer“ zu (sesshaften) Bauern zu machen. Sie lösten damit die frühere Politik von Verdrängung, Absonderung in „Indianerreservaten“, Zwangsumsiedlung und Segregation teilweise ab, ohne sie ganz aufzugeben.

Heute stellt für ihre Kulturen, die oftmals stark an ihre natürliche Umgebung gebunden sind, vor allem die Politik der industriellen Nutzung sowie der Ausbeutung von Bodenschätzen eine Gefahr dar. Die Traumatisierungsfolgen sind lange unterschätzt oder ignoriert worden. Seit Ende des 20. Jahrhunderts haben sich Kirchen und einige Regierungen für Misshandlungen und Kulturvernichtung entschuldigt, Anfang des 21. Jahrhunderts kam es zu Versuchen der Wiedergutmachung. In Bolivien regiert eine von einem Indio geführte Partei. Zudem erlangen sie Partizipationsmöglichkeiten und Fertigkeiten, um vertragliche und politische Rechte durchzusetzen.

Die deutsche Bezeichnung „Indianer“ geht auf das spanische Wort "indio" zurück, einen Neologismus aus der Kolonialzeit. Christoph Kolumbus glaubte 1492, in Indien angekommen zu sein, als er Hispaniola erreichte. Mit „Indien“ bezeichneten die europäischen Seefahrer zur damaligen Zeit allerdings nicht nur den indischen Subkontinent, sondern den ganzen Osten Asiens, den sie über den westlichen Seeweg zu erreichen suchten. Obwohl zuletzt Amerigo Vespucci den Irrtum Kolumbus’ 1502 endgültig aufklärte, wurde die Bezeichnung der in den neu entdeckten Gebieten angetroffenen Bewohner als „Indianer“ beibehalten. Konkurrierende Begriffe wie „Amerikaner“ (etwa im Codex canadiensis), die teils wieder verschwanden, vor allem aber „Wilde“ und „Heiden“ (welche die Nichtzugehörigkeit zu „Zivilisation“ und Christentum hervorheben und damit eine Abgrenzung schaffen) waren früh in Gebrauch.

Im Spanischen gibt es den im Deutschen erkennbaren Unterschied zwischen „Inder“ und „Indianer“ nicht; beide Herkunftskategorien werden mit dem Wort "indio" bezeichnet. Um Missverständnisse zu vermeiden, werden in fast allen lateinamerikanischen Ländern die „echten“ Inder nicht als "indios", sondern als "hindú" („Hindus“) bezeichnet, obwohl dies eigentlich nur eine in Indien verbreitete Religionszugehörigkeit angibt. In der Literatur wurde – ausgehend vom Französischen "amérindien" – auch der Neologismus "amerindio" geprägt. Im allgemeinen Sprachgebrauch herrscht in Lateinamerika für Indianer dagegen die generelle Benennung "indígenas" („Eingeborene“, „Ureinwohner“) oder "pueblos indígenas" (indigene Völker) vor.

Im brasilianischen Portugiesisch versteht man unter "índio" allgemein einen „Ureinwohner“ (eigentlich "indígeno" oder "nativo"); die Brasilianer unterscheiden demnach zwischen "índios latinoamericanos", "índios africanos" und "índios australianos" (lateinamerikanischen, afrikanischen und australischen Ureinwohnern).

Im Englischen, wo eine semantische Unterscheidung ähnlich wie im Spanischen fehlt, wurde zur Abgrenzung von Indern der Begriff "Red Indians" geprägt, der im Deutschen ebenfalls mit „Indianer“ zu übersetzen ist, aber heute ähnlich wie die deutsche Bezeichnung „Rothaut“ wegen ihrer rassistischen Konnotationen in der Regel nicht mehr verwendet wird. Bis heute vor allem in den USA genutzt wird dagegen der Begriff "American Indians" (ebenfalls „Indianer“).

Im Deutschen wird der Ausdruck „Indianer“ manchmal ausschließlich auf die indianischen Ureinwohner Nordamerikas beschränkt, während die in Süd- und Mittelamerika beheimateten Gruppen in dieser Diktion dann als „Indios“ bezeichnet werden.

Vielfach wird der Begriff "Indianer", "Indian" oder "Indio" von den Mitgliedern der damit angesprochenen Gesellschaften als koloniale Fremdbezeichnung abgelehnt oder gemieden. Für die Betroffenen bestand bis zum Eintreffen der Europäer kein Anlass, einen übergreifenden Begriff für die Bevölkerung des Kontinents zu bilden. Selbst die Eigenbezeichnung vieler Gemeinschaften war häufig einfach nur gleichbedeutend mit „Mensch“. Offenbar haben die Ureinwohner den Kontinent bzw. die ihnen bekannte Welt ursprünglich auch nur sehr selten als Einheit aufgefasst (ein Gegenbeispiel sind die Kuna in Panama und Kolumbien, die den Kontinent als Abya Yala „Kontinent des Lebens“ bezeichneten). Zwar gab es bereits vorkolonial vielfältige Sammelbezeichnungen für Volksgruppen und verwandte Ethnien, aber erst angesichts der Kolonialisierung gewannen beispielsweise die indianischen Ethnien Nordamerikas insgesamt ein gewisses Zusammengehörigkeitsgefühl. Gail Tremblay hält die gemeinsame Erfahrung der Kolonialherrschaft, den versuchten Genozid, die Assimilationsversuche und den Schmerz des Verlusts für die entscheidenden Faktoren, die zur Wahrnehmung von Verbindungen über Volksgrenzen hinweg geführt haben. Begrifflich geschah dies durchaus auch unter prinzipieller Beibehaltung des Indianerbegriffs, wie an der Bezeichnung "American Indian" zu erkennen ist, die auch Angehörige nordamerikanischer Indianervölker selbst verwenden.

Den verbindenden Aspekt der gemeinsamen Beheimatung auf dem Kontinent betonen hingegen Bezeichnungen wie „amerikanische Ureinwohner“ ("Native Americans", "americanos nativos", genauer übersetzt: „eingeborene Amerikaner“), „Urvölker Amerikas“ ("indigenous peoples of the Americas", "original peoples of America", "pueblos originarios de América") oder „indigene amerikanische Bevölkerungen“ ("indigenous American peoples", "pueblos indígenas de América"). Im Gegensatz zum Begriff „Indianer“ schließen diese Begriffe allerdings auch die Inuit, Unangan und Yupik in Alaska und der nordkanadischen Arktis ein. Diese Völker trafen jedoch wesentlich später als die Indianer in Amerika ein und unterscheiden sich genetisch und kulturell von den früheren Einwanderern. Letzteres gilt auch für die Ureinwohner Hawaiis, Amerikanisch-Samoas und der Osterinsel. Sie alle werden daher im deutschen Sprachgebrauch nicht unter dem Begriff „Indianer“ gefasst.

Ebenso werden auch Mestizen, Métis oder Zambos, also Nachfahren aus Verbindungen zwischen Europäern bzw. Afrikanern und Indianern, gemeinhin nicht zu Letzteren gerechnet. Ohnehin werden derartige Bezeichnungen, die – ebenso wie „Halbblut“ ("half-breed") oder „Mischling“ ("mixed-blood") – auf Vorstellungen von „Blutreinheit“ ("Blood purity, Limpieza de sangre") zurückgehen und Menschen nach der „Unvermischtheit des Blutes“ zu kategorisieren suchen, heute meist kritisch gesehen oder als rassistisch abgelehnt. Andererseits gilt die "Mestizaje", also die untrennbare Vermischung indianischer und eingewanderter europäischer und afrikanischer Vorfahren, die viele lateinamerikanische Mehrheitsbevölkerungen auch in ihrem Selbstverständnis prägt, als kulturelles und soziologisches Unterscheidungsmerkmal des hispanischen und lusitanischen Amerikas gegenüber den rein „weißen“ bzw. „rassengetrennten“ Gesellschaften im Norden des Kontinents.

In Kanada wird überwiegend ein umfassender, nicht auf Indianer beschränkter Begriff gebraucht, nämlich "First Nations" bzw. "Premières Nations", also „Erste Nationen“. Komplikationen ergeben sich allerdings aus dem Umstand, dass das Indianergesetz von 1876, das immer noch Gültigkeit besitzt und daher viele Traditionen des kolonialen Indianerbegriffs fortführt, zwischen "Status Indians" (das sind registrierte Angehörige der staatlich anerkannten "First Nations", die bestimmte Rechte haben), "Non-Status Indians" (die diese Rechte nicht haben, weil sie nicht registriert sind) und "Treaty Indians" (die den Bestimmungen der mit einer großen Zahl von Stämmen geschlossenen Einzelverträge unterliegen) unterscheidet. Aufgrund dieser Legaldefinitionen verlieren beispielsweise „gemischte“ Paare ihren Anspruch auf die Rechte der Ureinwohner, ggf. sogar ihre formale Anerkennung als „Indianer“. Selbst Angehörige der "First Nations" gelten darum heute oftmals formalrechtlich nicht als "Indians". Auf lange Sicht kann das zum Verschwinden der „staatlich anerkannten“ Indianer und damit zur Bedeutungslosigkeit der ihnen von Gesetzes wegen zuerkannten Rechte führen.

Aufgrund der von Staat zu Staat unterschiedlichen Prozesse bei der Konstruktion eines politischen Subjekts, das sich etwa im lateinamerikanischen Raum als "Indígena" (z. B. in Guatemala oder Brasilien), "Nacionalidad Indígena" (Ecuador) oder "Pueblo Originario" (Bolivien) bezeichnet, kommt es auch im politischen Raum zu uneinheitlichen terminologischen Lösungen – zumal Begriffe wie "Partido Indio" („Indianerpartei“) oder "National Congress of American Indians" als Selbstbezeichnungen ebenfalls fortbestehen.

Dieses Ringen um die Bezeichnungen hat seinen Grund nicht allein in der Begriffsgeschichte, sondern auch in den gesellschaftlichen Konnotationen, mit denen die Begriffe verbunden sind. So wird "Indian" im englischsprachigen und "Indio" im spanischsprachigen Amerika häufig auch in der Sprache der Allgemeinheit als abwertende Qualifizierung betrachtet. Ähnliches gilt im französischen und im portugiesischen Sprachraum. Durch die Übersetzung ins Deutsche wird dieses sprachliche und terminologische Wirrwarr von Selbst- und Fremdbezeichnungen, bestimmt durch Abgrenzungsbedürfnisse und Zuweisungen im Spannungsfeld zwischen Rassismus und kultureller Selbstbestimmung, noch weiter verkompliziert. Letztlich hat sich der im Deutschen relativ diskriminierungsarme Begriff "Indianer" (siehe hierzu auch Indianerbild im deutschen Sprachraum) in den Augen Vieler als derjenige erwiesen, der diese Benennungs-Probleme am ehesten löst. Immer wieder neu kritisch zu hinterfragen bleiben jedoch die oft unreflektierten Aspekte der Fremdbeschreibung, der Homogenisierung nicht zusammengehörender Gruppen oder der Verniedlichung.

Die indianische Bevölkerung Amerikas ist sehr ungleichmäßig verteilt, wobei mehrere tausend Reservate bestehen. Dabei leben die meisten Indios in Mittel- und Südamerika nicht in Reservaten.

Während in Kanada 2006 knapp 700.000 Menschen (2,1 % der Bevölkerung) als Indianer galten und 615 Stämme in rund 3000 Reservaten anerkannt waren, existierten in den USA 566 von der Bundesregierung anerkannte, die 0,97 % der Bevölkerung darstellten, und rund 245 nicht anerkannte Stämme. Innerhalb der Staaten lassen sich dabei gleichfalls Schwerpunkte erkennen. So lebt der überwiegende Teil der US-Indianer in Kalifornien, Arizona, New Mexico und Oklahoma. Insgesamt leben in Nordamerika rund 3,5 bis 4 Millionen Indianer.

In Lateinamerika leben hingegen 65 bis 70 Millionen Indianer, davon etwa die Hälfte in Mexiko, ein weiteres Drittel in den Andenländern. Nur in Bolivien stellen sie die Spitze der Regierung. Die Abgrenzung zur übrigen Bevölkerung ist weniger scharf definiert, Reservate existieren vor allem in Brasilien, Kolumbien, Panama, Paraguay und Venezuela und liegen zum größten Teil in den Waldgebieten des Orinoco-, Paraná- und Amazonasbeckens.

Allein in Mexiko wird die indigene Bevölkerung auf 30 % der über 100 Millionen Mexikaner geschätzt, wobei Mestizen weitere 60 % der Gesamtbevölkerung ausmachen. In Belize geht man von 10 % bzw. von 45 % aus. In Guatemala sind 59,4 % Mestizen (hier "Ladinos" genannt), 45 % der Bevölkerung gehören verschiedenen Maya-Gruppen an. Davon sind 9,1 % Quiché, 8,4 Cakchiquel, 7,9 Mam, 6,3 % Kekchí, weitere 8,6 % gehören anderen Maya-Gruppen an. Im Nachbarland Honduras liegt der Anteil der Indios bei 7 %, der der Mestizen bei 90, ähnlich wie in El Salvador, wo die Indianer jedoch nur noch 1 % der Bevölkerung stellen. In Nicaragua liegt der Anteil der Mestizen bei 69, der der Indios bei 5 %. In Costa Rica liegt der Anteil der Indios nur bei rund 1 %, in Panama bei 5 %. Die Karibik ist das Extrem, denn etwa auf Kuba leben praktisch keine Indianer mehr, ähnlich wie auf Jamaika. Auf Dominica leben 300 bis 500 Kariben in einem eigenen Reservat.

In Südamerika existieren gleichfalls Schwerpunkte. Während der Anteil der Indianer in Kolumbien bei nur 1 % liegt, liegt dort der Anteil der Mestizen bei 58 %, immerhin 3 % sind Nachkommen von Schwarzen und Indianern. In Guyana liegt der Anteil der Indios bei 9,1 %, in Suriname bei 2 %. Erheblich höher ist der Anteil in den Andenstaaten, wie in Ecuador, wo 25 % der Bevölkerung Indianer sind, in Peru 45, in Bolivien sogar 55 % – 30 % sind Quechua und 25 % Aymara.

Weiter im Süden, in Chile, liegt der Anteil der indianischen Bevölkerung nur noch bei knapp 5 %, die meisten sind Mapuche. In Argentinien liegt ihr Anteil bei unter 3 %, in Uruguay gibt es fast keine Indianer, in Paraguay liegt ihr Anteil bei rund 5 %, in Brasilien jedoch unter 1 %.

In Nordamerika leben die Indianer oft in Reservaten, die in Kanada "reserves", in den USA "reservations" genannt werden. In Kanada sind die Reservate infolge von Verträgen entstanden, die die Indianer mit der Regierung abschlossen. Kommissionen bestimmten nach Befragung der Indianer, aber ohne sie in die Entscheidung einzubeziehen, die Reservatsgrenzen. Innerhalb dieser Gebiete standen ihnen ihre traditionellen Rechte zu und sie zahlten für dort getätigte Umsätze keine Steuern. Rund die Hälfte der Indianer lebt heute in Städten.

Die Indianerpolitik der Vereinigten Staaten wechselte mehrfach die Richtung. Alle Stämme wurden ab 1830 gezwungen, ihre Wohngebiete östlich des Mississippi zu verlassen, häufig wurden mehrere Stämme in einem Reservat zusammengefasst. Obwohl die ländlich lebenden Indianer vielfach in Armut leben, gelang manchen Stämmen eine ökonomische Erholung. Gemäß der Volkszählung im Jahr 2000 lebten etwa 85 % außerhalb von Reservationen, meist in Städten.

In Brasilien und in den angrenzenden Ländern existieren immer noch Isolierte Völker, Gruppen, die bei Kontakten mit Weißen so schlechte Erfahrungen gemacht haben, dass sie diese zu meiden versuchen. Allein in Brasilien geht man von etwa 67 Gruppen aus.

Die Sprachen bestehen aus Dutzenden von eindeutigen Sprachfamilien sowie vielen isolierten Sprachen. Es gab mehrere Versuche von Linguisten, diese in übergeordnete Familien zu gruppieren, davon ist keiner allgemein anerkannt. Zwei Sprachfamilien weichen deutlich von den anderen ab: Die Na-Dené-Sprachen und die eskimo-aleutischen Sprachen. Genetische Analysen der Indianer lassen mehrere Einwanderungswellen bei der Besiedlung Amerikas annehmen. Daher ist zu vermuten, dass diese Sprachen von Indianervölkern gesprochen werden, die als spätere Einwanderer nach Amerika kamen, als die anderen Völker den Kontinent bereits besiedelt hatten.

Schriften haben nur indianische Kulturen in Mittelamerika entwickelt. Die ältesten Zeugnisse stammen von den Olmeken in Mittelamerika und werden auf ca. 900 v. Chr. datiert. Hier entwickelten sich auch weitere Schriften, insbesondere die der Maya, Mixteken, Zapoteken und Azteken. Dabei bestand eine Variationsbreite zwischen noch rein logografischer Schrift bis zu einer weitgehend phonetischen Schrift.

Nach der Kolonisierung Amerikas reichte die Haltung hinsichtlich der indigenen Sprachen von Vernachlässigung bis zu gezielter Unterdrückung. Nur die Missionsorden begannen früh, die Sprachen zu lernen und entsprechende Schulen einzurichten. Das galt zunächst für Peru, wo eine Hochschule entstand, dann für zahlreiche Missionsgebiete zwischen Québec und Kalifornien im Norden, über die mexikanischen Ballungsräume bis zu den Grenzgebieten im Süden Chiles und entlang der portugiesischen Grenze (Brasilien). Gelegentlich verbreiteten sie dadurch Sprachen in Gebiete, in denen diese Sprache vorher nicht in Gebrauch war, wie etwa im Fall des Quechua. Neben den Sprachen mit Millionen von Sprechern, wie Aymara, Guaraní und Nahuatl lernten die Missionare nur wenige Sprachen, was ihr Überleben wiederum bestärkte.

In Nordamerika wurde der Gebrauch der indigenen Sprachen lange aktiv unterdrückt. Diese Politik fand ihren Höhepunkt in der sogenannten Termination mit dem Ziel, Indianer aus ihrem Stammesverbund zu lösen und als Individuen in die Gesellschaft zu integrieren. Dazu wurde insbesondere der Gebrauch von Indianer-Sprachen in der Schule strikt untersagt. Erst 1958 wurde dieses Ziel aufgegeben und seitdem gibt es zahlreiche Versuche, die nordamerikanischen Sprachen, wieder zu beleben.

In Nordamerika sind einige der größeren Sprachen, wie Cree (mit 60 bis 90.000 Sprechern) in Kanada oder Navajo im Südwesten der USA (mit rund 150.000 Sprechern) nicht gefährdet, andere stehen kurz vor dem Aussterben. In Kanada sind noch mindestens 74 Sprachen in Gebrauch.

In Mexiko und den südlichen Nachbarländern dominieren die Maya-Sprachen. Mexiko erkennt 62 nationale Indiosprachen an, wobei 2005 mehr als 6 Millionen über 5 Jahre alte Bewohner eine dieser Sprachen als Muttersprache bezeichneten.

In der Karibik werden die Sprachen der Cariben und der Arawak nur noch wenig gesprochen; zu ihren Vertretern zählen etwa die Taíno.

Anders sieht es in Südamerika aus. Schätzungen zufolge waren dort vor Kolumbus rund 1500 Sprachen verbreitet, davon existieren heute noch etwa 350. Die Einordnung in Sprachfamilien ist dabei, wie in ganz Amerika, stark umstritten. Die Zahl der Sprecher ist erheblich höher als in Nordamerika und in der Karibik, zugleich konzentriert sich deren überwiegende Zahl auf wenige Sprachen. Diese wiederum wurden von Missionaren erlernt und gefördert. So überlebten zahlreiche Sprachen, zu denen inzwischen Materialien schriftlich und über das Internet verfügbar sind.

Während im östlichen Tiefland Südamerikas Tupí-Sprachen vorherrschen, deren größten Zweig die Tupí-Guaraní-Sprachen darstellen, dominieren im Andenraum Quechua-Sprachen, deren sich bereits die Inkas bedienten. Neben ihnen bestehen große Sprachgruppen, wie die Aru-Sprachen, zu denen etwa das Aymara gehört, die indigene Sprache mit den meisten Sprechern in Südamerika (ca. 2,2 Millionen). In Argentinien sprechen rund eine Viertelmillion Menschen eine der beiden araukanischen Sprachen.

Überwiegend in Nordamerika entstanden im Kontakt zwischen Weißen und Indianern neue Sprachen, insbesondere Mischsprachen wie das Chinook Wawa an der Pazifikküste, weil der extensive Handel eine einfache Verständigungssprache erforderte. Hinzu kamen Sprachen wie das Michif, die wichtigste Sprache der Métis in Kanada, das aus indianischen und europäischen Sprachen bei der Entstehung eines Mischvolks entstand und Ursprünge im Cree und im Französischen hat. Das ebenfalls von Métis gesprochene Bungee hat hingegen schottisch-gälische und Cree-Wurzeln.

Bolivien, Paraguay, Ecuador und Peru erkennen heute eine oder mehrere indigene amerikanische Sprachen als Amtssprache zusätzlich zum Spanischen an.

Die Besiedlung Amerikas erfolgte in mehreren Einwanderungswellen, die mindestens 16.000 Jahre überspannen. In diesem Kontinuum ist die europäische Zuwanderung nur eine von vielen. Die Hauptroute der als Paläoindianer bezeichneten Gruppen führte von Sibirien über Beringia nach Alaska und von dort aus nach Süden. Genetische Analysen können die Verteilung der Ureinwohner mit drei Wellen erklären, von denen die erste die mit Abstand bedeutendste war. Aus ihr gingen nahezu alle indianischen Völker hervor und ihre Verteilung passt zu einem schnellen und direkten Vorstoß von Sibirien über Alaska nach Süden durch den ganzen Kontinent. Ein genetischer Anteil von 10 % bei den Chippewa fällt aus diesem Muster heraus und wird als Hinweis auf eine zweite Welle interpretiert. Schließlich kann die erste Welle nur 57 % der genetischen Ausstattung der Bewohner der nordamerikanischen Arktis erklären, so dass hier die dritte Welle angenommen wird. Diese Analysen decken sich mit früheren linguistischen und morphologischen Untersuchungen.

Die frühen Siedler passten sich ihrer jeweils neuen Umwelt an und lebten als nomadische Wildbeuter, als Fischer, Jäger und Sammler, später als sesshafte Ackerbauern mit entsprechenden, in einigen Gebieten urbanen Kulturen erstaunlicher Ausmaße (Archaische Periode). Von Südamerika bis weit in den Norden züchteten sie um 7000 v. Chr. beginnend, Pflanzen wie Mais, Kürbis und Kartoffel sowie zahlreiche, von den europäischen Bauern verdrängte Arten, und wandelten dabei die Landschaft in viel stärkerem Maße um, als man lange angenommen hat.

Die Viehzucht beschränkte sich auf wenige Arten, wie Lama und verwandte Kameloide (Alpaca und Vicuña), sowie das Hausmeerschweinchen im Reich der Inka, den Truthahn in Nord- und Mittelamerika, und den Hund. Als Lasttiere standen neben dem Lama der Inka nur noch Hunde für kleinere Lasten zur Verfügung, die man in Nordamerika in einfache dreieckige Schleppgeschirre, Travois genannt, einspannte. Außerdem waren ihre Haare das Ausgangsmaterial für Decken und Kleidung.

Das Rad als Fortbewegungsmittel war offenbar unbekannt, wiewohl Räder und sogar Zahnräder als Bauteile mechanischer Geräte verwendet wurden. Man ging in der Regel zu Fuß und transportierte seine Lasten selbst oder benutzte Wasserfahrzeuge wie das Kanu. Würdenträger in hierarchischen Gesellschaften Mittel- und Südamerikas wurden mitunter in Sänften getragen.

In Alaska reichen die ältesten gesicherten Funde 12.000 bis 14.000 Jahre zurück. Als älteste Kultur galt lange Zeit die Clovis-Kultur, doch spätestens die Funde in den Paisley-Höhlen, die rund ein Jahrtausend vor den Clovis-Funden liegen, zeigten, dass die frühesten Bewohner nicht dieser Kultur angehörten. Die ältesten menschlichen Überreste lieferte die über 10.500 Jahre alte Buhl-Frau aus Idaho sowie die Überreste aus der "On Your Knees Cave" auf der Prince-of-Wales-Insel in Alaska, die rund 9800 Jahre alt sind. An diese frühe Phase, die durch den Kennewick-Mann, der genetisch nicht zu den amerikanischen Völkern passt, neu diskutiert werden musste, schloss sich die Archaische Periode an. An ihrem Ende zwischen 2000 und 1000 v. Chr. entwickelten sich der Gebrauch von Keramik, Ackerbau und verschiedene Formen abgestufter Sesshaftigkeit bis weit in den Norden. Die Jagdtechniken wurden durch Atlatl und später durch Pfeil und Bogen wesentlich verbessert. Während im Norden, wo Karibu- und Bisonherden die Ernährung sicherten, Jagdkulturen bestanden, spielte die Jagd im Süden eine immer geringere Rolle. Bevölkerungsverdichtungen traten in Nordamerika um die Großen Seen, an der pazifischen Küste um Vancouver Island, am Mississippi und an vielen Stellen der Atlantikküste sowie im Südwesten auf.

In Nordamerika existierten im Einzugsgebiet des Mississippi und des Ohio (Adena-Kultur, Mississippi-Kultur) komplexe Gemeinwesen "(Templemound-Kulturen)", die jedoch kurz vor Ankunft der ersten Europäer untergegangen sind. Sie strahlten bis weit in den Norden und Westen aus. Im Südwesten der USA entstanden Lehmbausiedlungen mit bis zu 500 Räumen, die sogenannten "Pueblos". Diese Kultur ging auf die Basketmaker zurück, die bereits Mais anbauten. Um die Großen Seen entwickelten sich Großdörfer mit Palisaden und dauerhafte Konföderationen. Diese Gruppen betrieben, ähnlich wie im Westen, Mais- und Kürbisanbau sowie einen ausgedehnten Fernhandel – etwa mit Kupfer und bestimmten Gesteinsarten, die für Jagdwaffen und Schmuck von Bedeutung waren –, der sich in British Columbia bis 8000 v. Chr. nachweisen lässt.

Sieht man von den vieldiskutierten Funden von Monte Verde ab, so sind wohl die Funde von Los Toldos, in der argentinischen Provinz Santa Cruz, die ältesten in Südamerika. Sie reichen mindestens 12.000 Jahre zurück. Ähnlich den nordamerikanischen Fundplätzen, weisen die Überreste auf die Jagd von Großsäugern, in diesem Falle auf Riesenfaultiere und Pferde hin, dazu kamen Guanacos und Lamas. Ähnliches wurde in Chile gefunden, wie etwa in der Cueva del Milodón, wo sich, ähnlich wie in Nordamerika, ausgestorbene Beutetiere, wie Pferde, nachweisen ließen. Die Casapedrense-Kultur (ca. 7000 bis 4000 v. Chr.) galt als Vorläuferkultur der Tehuelche, bzw. Patagonier, deren älteste Funde allerdings inzwischen auf 9400 bis 9200 v. Chr. datiert werden.

In den wasserarmen Regionen entwickelte sich schon früh eine Bewässerungswirtschaft, was wiederum höhere Bevölkerungsdichten und komplexere Organisationsformen zuließ. Ähnlich komplizierte Verfahren zur Süßwassergewinnung wie in den trockenen Gebieten Zentral- und Südmexikos waren in Yucatán vonnöten. Hier entstand ab etwa 3000 v. Chr. eine auf größeren Siedlungen basierende Kultur, die zur vorklassischen Epoche der Maya-Kulturen gerechnet wird. Einer der ältesten Mayaorte war Cuello in Belize, das auf etwa 2000 v. Chr. datiert wird.

Eine der wichtigsten Metropolen der Maya wurde neben Uxmal das zwischen dem 5. und dem 7. Jahrhundert erstmals aufblühende Chichén Itzá. Es entstand ein ganzes Netz miteinander verbundener Städte. Nach dem ungeklärten Zusammenbruch der Mayakultur im 10. Jahrhundert besiedelten (oder dominierten zumindest kulturell) Tolteken die Stadt. Bei den Maya übernahm nun Tulúm an der Küste eine Führungsrolle, möglicherweise ein Anzeichen, dass sich im 12. Jahrhundert der wirtschaftliche Schwerpunkt auf den Seehandel verlagerte.

Zwischen 100 und 600 n. Chr. war Teotihuacán das kulturelle, wirtschaftliche und Herrschaftszentrum Mesoamerikas. Seine Einwohnerzahl wird für die Zeit zwischen 450 und 650 auf bis zu 200.000 geschätzt. Die Stadt erstreckte sich über 20 km² Fläche. Allein die dortige Sonnenpyramide, die um 100 entstand, dehnt sich auf einer Grundfläche von 222 mal 225 Metern aus und ist rund 65 m hoch. Weitere Großbauten, wie die "Ciudadela", eine Art geschlossener Herrschaftsbezirk, entstanden. Die wirtschaftliche Basis der Stadt war neben der Bewässerungslandwirtschaft ein ausgedehnter Obsidianhandel; er wurde wohl auf dem Platz vor der "Ciudadela" abgewickelt und reichte mindestens bis an die heutige Grenze zu den USA. Die Wurzeln der Stadt reichen bis 1500 v. Chr. zurück, um 750 war die Metropole allerdings verlassen. Das zurückbleibende Machtvakuum füllten im 10. Jahrhundert erst wieder die Tolteken.

Die Tolteken wanderten ab dem 9. Jahrhundert in den Süden Mexikos ein und bildeten für zwei Jahrhunderte eine städtische Kultur, die allerdings von den stärker militärisch organisierten Chichimeken bedroht war, die gleichfalls aus dem Norden stammten.

Ende des 14. Jahrhunderts gelang es den Azteken, die sich selbst als "Mexica" bezeichneten, ein Großreich zu erobern, das sich mit tributpflichtigen Herrschaften umgab. Ihre Wurzeln reichen wohl ins 11. Jahrhundert zurück. Die Hauptstadt Tenochtitlan dürfte mehrere Zehntausend Einwohner, möglicherweise sogar 150.000 gehabt haben.

Die ältesten Steinwerkzeuge in Südamerika reichen bis etwa 10.000 v. Chr. zurück, ähnlich wie die Höhlenmalereien bei Ayacucho in Peru und in den Lauricocha-Höhlen an der Quelle des Marañón. Der erste Anbau von Kürbissen und Bohnen und die Züchtung von Lamas wird auf vor 4000 v. Chr. datiert, der Kürbis taucht zu dieser Zeit aber auch schon weit im Norden, in Maine auf.

Die ältesten Keramiken fand man im ecuadorianischen Guayas-Becken. Sie werden der Valdivia-Kultur zugeordnet und auf das 4. vorchristliche Jahrtausend datiert. Keramik hat sich in Nordamerika nur in den Ballungsgebieten durchgesetzt, in anderen Gebieten setzten unterschiedlichste Techniken und Hindernisse seiner Verbreitung Grenzen. Die Valdivia-Kultur brachte bereits eine städtische Organisation mit Kulten, Riten und Opfergaben hervor.

Eine der ältesten Städte, Caral (nördlich von Lima), wurde 1996 entdeckt. Fünf Jahre später konnte die dortige Stufenpyramide auf 2627 v. Chr. datiert werden. Zur Stadt gehörten Häuser für mindestens 3.000 Bewohner. Tempelanlagen, künstliche Bewässerungssysteme und Fernhandel mit den Küstenbewohnern und denen des Amazonasgebiets deuten auf eine bereits weit entwickelte Hochkultur hin.

Noch älter ist Sechín Bajo, eine Stadt, deren Pyramide auf 3200 v. Chr. datiert werden konnte, und die seit 2003 ausgegraben wird.

An der Küste Ecuadors bestand um 1600 v. Chr. die Machalilla-Kultur. Auf sie gehen die typischen Keramikgefäße mit Henkel zurück, die auch bei den Chavín, Mochica und Chimú überliefert sind. Die nachfolgende Chorrera-Kultur brachte um 1200 bis 500 v. Chr. Keramiken in Menschen- und Tiergestalt hervor. Die Häuser wurden um einen großen Platz gruppiert und auf künstlichen Aufschüttungen erbaut.

Die Kultur der Chavín (etwa 800 bis 300 v. Chr.) wies enge Beziehungen zu der der Olmeken auf, was der Gebrauch der Symbolhäufungen von Jaguar, Puma, Vogel und Schlange nahelegt. Die zeitgenössische Paracas-Kultur in der Gegend um Lima war wegen ihres Totenkultes bekannt.

Im Hochland von Bogotá bestand die Herrera-Kultur (vor 4. Jahrhundert v. bis 2. Jahrhundert n. Chr.), an der Westseite der Anden die Calima-Kultur (4. Jahrhundert v. Chr. bis 2. Jahrhundert n. Chr.). Grabanlagen ab dem 4. Jahrhundert gehen auf die San Agustín-Kultur zurück, die bis zum 7. Jahrhundert die Landschaft stark veränderte.

Zwischen 300 v. Chr. und nach 600 n. Chr. bestand die Nazca-Kultur rund 500 km südlich von Lima, die Bewässerungskanäle baute. Ähnliche Bewässerungssysteme entwickelte die Mochica-Kultur im Wüstenstreifen an der Pazifikküste. Neben Edelmetallen wurde Kupfer verarbeitet.

Um den Titicacasee bestand ab dem 1. Jahrhundert v. Chr. bis etwa 1000 n. Chr. die Tiahuanaco-Kultur. Ihre Spuren sind in Peru, Bolivien und im Norden Chiles nachweisbar. Etwa gleichzeitig entstand die Wari-Kultur (600 bis 1100), die sich nördlich entlang der Küste anschloss. Beide Kulturen wurden von Hauptstädten dominiert, die von erheblicher Ausdehnung waren. Die Wari umgaben ihre Hauptstadt mit Verteidigungsmauern, ihr Haupttempel Willkawayin ist erhalten.

Das erste Großreich entwickelten die Chimu in der Zeit von 1000 bis 1470 mit der Hauptstadt Chan Chan in der Gegend um das peruanische Trujillo. Ab etwa 1200 bis 1532 schufen die Inka ein Reich, das im 15. Jahrhundert seine größte Ausdehnung annahm. Neben Cusco, das zeitweise die Hauptstadt war, und Machu Picchu, ist Ollantaytambo zu nennen, wo sich die Grundanlage einer Inkastadt weitgehend erhalten hat.

Erheblich weniger erforscht ist die Geschichte der am Ostrand der Anden und in den Waldgebieten des Amazonas lebenden Gruppen. Zahlreiche Funde deuten jedoch auf erheblich ältere Kulturen hin (ca. 2450 v. Chr.), die möglicherweise noch vor denen des andinen Hochlandes entstanden sind. Wenig ist über die Chachapoya bekannt, die von etwa 800 bis 1600 am Ostrand der Anden lebten. Sie errichteten Felsengräber an steilen Klippen.

Zwischen 1000 v. Chr. und 500 v. Chr. wanderten die Arawak den Orinoco abwärts. Sie bauten Kanus und lebten von Fischfang, Jagd und dem Anbau von Mais, Bohnen, Süßkartoffel, Kürbis und Maniok. Hinzu kamen Erdnuss, Pfeffer, Ananas, Tabak und Baumwolle.

Ab 1492 wurde der Doppelkontinent nach und nach von europäischen Staaten in Besitz genommen. Die verfolgten Kolonisierungs- und Besiedlungsformen unterschieden sich dabei deutlich voneinander und hatten gravierende Auswirkungen auf die dort angetroffenen Kulturen. Während im Norden ein Jahrhundert lang der Handel vorherrschte, und erst nach 1600 erste dauerhafte Kolonien an der Ostküste entstanden, eroberten Spanier binnen weniger Jahrzehnte die Großreiche Lateinamerikas. Während im spanischen Bereich mehr als drei Viertel der Indianer lebten, erhielten Portugal mit Brasilien und Frankreich und England mit dem Norden die dünner besiedelten Regionen.

Kriege spielten eine Rolle, doch eingeschleppte Krankheiten, Umsiedlungen und massenhafte Zwangsarbeit dezimierten die Bevölkerung in einem ungleich höheren, jedoch kaum quantifizierbaren Ausmaß. Viele Gruppen verschwanden durch eingeschleppte Seuchen, ohne dass ein Europäer sie überhaupt zu Gesicht bekommen hatte.

Um 1940 folgte man überwiegend dem Anthropologen Alfred Kroeber, der die Bevölkerung der westlichen Hemisphäre im Jahr 1492 auf lediglich acht Millionen und nördlich des Rio Grande auf eine Million Menschen schätzte. Diese Schätzungen wurden bereitwillig aufgegriffen, da sie die Vernichtung in ihrem Ausmaß verminderte und den politischen Mythos aufrechterhielt, die Weißen hätten einen weitgehend menschenleeren Kontinent erobert, und damit ihren Besitz legitimierte. Seitdem wurden immer neue, extrem abweichende Schätzungen auf unterschiedlichster methodologischer Grundlage erstellt. Sie reichen von kaum mehr als 8 Millionen bis zu über 110 Millionen. Jüngere Schätzungen gehen von einem sehr groben Näherungswert von 50 Millionen Einwohnern aus, von denen etwa die Hälfte in Mesoamerika, ein Viertel im Inkareich lebte.

Wie stark die Diskussion in Bewegung geraten ist, zeigt die These, die später beobachteten riesigen Bisonherden seien Weidetiere der Indianer gewesen. Die Herdengröße stellte folglich kein natürliches Gleichgewicht dar, sondern beruhte auf in wenigen Generationen eingetretener Übervermehrung nach dem starken Rückgang der menschlichen Population. Das als eher vorsichtig bekannte Smithsonian Institute hat seine Schätzung für Nordamerika auf drei Millionen Menschen verdreifacht.

Die dichteste Bevölkerung existierte sicher in den Hochkulturen Lateinamerikas, wo dementsprechend die zahlenmäßig größten Bevölkerungsverluste zu verzeichnen waren.
Hernán Cortés vernichtete das Reich der Azteken mit ca. 500 Soldaten und zahlreichen verbündeten Indianern, Pizarro das der Inkas. In der Karibik wurde die Bevölkerung innerhalb weniger Jahrzehnte fast völlig ausgelöscht, Hernan de Soto schleppte 1539 bis 1542 verheerende Krankheiten in das Gebiet zwischen Mississippi und Florida.

Die iberischen Staaten, die sich 1494 im Vertrag von Tordesillas über die Aufteilung der Welt und damit auch des Kontinents geeinigt hatten, entsandten zahlreiche Männer nach Übersee, die sich dort mit indianischen Frauen verbanden. Rasch wuchs die Zahl der Abkömmlinge, die man Mestizen nannte. Die herrschende Klasse bildeten dabei Spanier und Portugiesen, die untere Klasse Mestizen und Indianer.

In Amerika richteten vor allem Krankheiten wie Pocken, Masern und Grippe katastrophale Schäden an. Die Indianer verfügten über keinerlei Abwehrstoffe gegen diese für sie neuartigen Krankheiten. Zwar wurde die gezielte Verbreitung von Krankheiten in seltenen Fällen gefordert und mittels pockeninfizierter Decken womöglich versucht, doch waren die Risiken unabsehbar. In dem Moment, wo es möglich war, die eigene Bevölkerung zu impfen, förderten jedoch, wie 1862 im pazifischen Nordwesten, einige Politiker die Ausbreitung der tödlichen Epidemie oder nahmen sie in Kauf.

Weiter trugen in den britischen Kolonien in Nordamerika durch die Skalpproklamation von 1756, bis 1749 bereits in Halifax und bei den Franzosen, und in einigen US-Bundesstaaten wie Massachusetts (1744) die Skalpprämien zur Vernichtung bei. In Kalifornien wurden nach dem Goldrausch von 1849 innerhalb von nur zwei Jahrzehnten mehrere Tausend Indianer ermordet.

Trotz der nicht zu überschätzenden Wirkung der Epidemien und in einigen Gebieten der Sklavenjagd, sollte die der Kriege nicht unterschätzt werden. Die verlustreichsten Kriege im Osten dürften die Schlacht von Mauvilla (1540), der Tarrantiner-Krieg (1607–1615), die beiden Powhatankriege (1608–1614 und 1644–1646), der Pequot- (1637), der König-Philip-Krieg (1675–1676), die Franzosen- und Indianerkriege (1689–1697, 1702–1713, 1754–1763) sowie die drei Seminolenkriege (1817–1818, 1835–1842 und 1855–1858) gewesen sein. Dazu kamen die Aufstände des Pontiac (1763–1766) und des Tecumseh (ca. 1810–1813). Die Franzosen standen von etwa 1640 bis 1701 in den Biberkriegen, dann in vier Kriegen mit den Natchez (1716–1729), die Niederländer im Wappinger-Krieg und in den Esopuskriegen (1659–1660 und 1663–1664), die Spanier gegen die Azteken und Inkareiche, 1680 gegen die Pueblos und in zahlreichen weiteren Kämpfen. Im Westen der USA waren es vor allem die Aufstände des Cochise (1861–1874), der Sioux (1862) und Lakota (1866–1867) oder von Apachen unter Geronimo (bis 1886), die bekannt wurden, ebenso einzelne Schlachten, wie die am Little Bighorn oder das Massaker von Wounded Knee (1890).

Welchen Anteil wirtschaftliche Ausbeutung und desolate Sozialverhältnisse, Vernachlässigung, kriegerische Auseinandersetzungen, Epidemien, Sklavenjagd, „ethnische Säuberungen“ und Genozidversuche an dieser demographischen Katastrophe tatsächlich hatten – der Tiefpunkt wurde erst in den ersten Jahrzehnten des 20. Jahrhunderts durchschritten – und in welchem Verhältnis sie zueinander standen, wird kaum genau geklärt werden können. Fest steht nur, dass zahlreiche Völker mitsamt ihrer Kultur und Sprache vernichtet worden sind. Es war, gemessen an der Zahl der Opfer, die größte demographische und wohl auch kulturelle Katastrophe in der Geschichte der Menschheit.

Um die Frage der Behandlung der Indianer entspann sich ein umfassender Konflikt zwischen den Exponenten Bartolomé de Las Casas als „Generalverteidiger der Indios“ und Juan Ginés de Sepúlveda, den Missionsorden und dem Indienrat sowie den lokalen Feudalherren. Die Krone versuchte die Granden, die von Anfang an zur Verselbstständigung ihrer Herrschaft neigten, durch ein Bündnis mit den Kleinadligen, den Hidalgos, und der Kirche unter Kontrolle zu halten. Die Verwaltung sollte von Sevilla aus erfolgen, niemand durfte ohne Genehmigung in die Kolonien. Zugleich sollten die Indios missioniert, seit 1503 in Encomiendas zusammengefasst und vor übermäßiger Gewalt geschützt werden (Gesetze von Burgos, 1512). Sie waren als Arbeitskräfte vorgesehen.

1512/13 legten die Leyes de Burgos fest, dass die Indios den Feudalherren zwar überantwortet – daher der Begriff Encomienda –, aber nicht als Sklaven gelten sollten. Sie konnten allerdings zur Arbeit gegen Entlohnung gezwungen werden. Durch das Indi(ani)sche Recht versuchte Madrid gegen die brutale Drangsalierung der Indios und den rapiden Zusammenbruch der Bevölkerung durch das Encomiendasystem einen gewissen Schutz aufzubauen.

Durch das System der Mita waren die Provinzen schon im Inkareich gezwungen, reihum für eine bestimmte Zeit Arbeitskräfte für öffentliche Arbeiten zur Verfügung zu stellen. An dieses System knüpfte das "Repartimiento" ab 1549 an, wenn auch, wie etwa in Chile, das Encomiendasystem bis nach 1650 fortbestand. Das Repartimiento- oder „Zuteilungssystem“ diente vor allem der Bereitstellung von Kräften für die Feldarbeit und die lebensgefährliche Arbeit in Gold- und Silberminen (Potosí). Es wurde erst nach der Unabhängigkeit von Spanien abgelöst, stellte aber dennoch im Vergleich zur Encomienda eine Milderung dar.

Hingegen versorgten die sogenannten Paulistas oder "bandeirantes", Sklavenjäger aus São Paulo, den Sklavenmarkt mit Indianern. Dazu durchstreiften sie riesige, auch spanische Gebiete, und entvölkerten sie mit Unterstützung von Tupi-Armeen durch Menschenraub und Vertreibung. Erfolgreiche Bemühungen zum Schutz der Indios vor Sklavenjägern, wie im Jesuitenstaat von Paraguay, wo Indios, wie der Kazike Nicolás Neenguirú den Sklavenjägern erfolgreich Schlachten lieferten – waren die Ausnahme.

Missionare veranlassten die Indianer, oftmals unter Ausnutzung ihrer Schutzbemühungen gegen Ausbeutung und Tötung, ihre Glaubensüberzeugungen aufzugeben. Ihre kulturellen Eigenheiten wurden von den Missionaren als „unzivilisiert“ oder „widernatürlich“ diskreditiert.

In Südamerika hatten Ordensmissionare bereits im 16. Jahrhundert indianische Sprachen erlernt und schriftlich dokumentiert, um die Ureinwohner missionieren zu können. Sie trugen so indirekt zum Erhalt zahlreicher Sprachen bei. In Lima entstand eine entsprechende Hochschule. Die von Jesuiten im 17. Jahrhundert im La-Plata-Gebiet aufgebauten Missionen („Reduktionen“ genannt), in denen sie den Indios eine zwar von europäischen Wertmaßstäben geprägte und paternalistisch verstandene, aber doch eigenständige und in gewissem Sinne selbstbestimmte Entwicklung ermöglichen wollten, haben letztlich sogar dazu geführt, dass das Guaraní bis heute lebendig und in Paraguay als Amtssprache anerkannt ist.

Auch dort, wo spanische Konquistadoren nicht hinkamen, lösten sie, von den Epidemien abgesehen, massive Veränderungen aus. Sie hatten Pferde eingeführt, von denen einige verwilderten und sich in den weiträumigen Ebenen Süd- und später auch Nordamerikas rasant verbreiteten. Sie bildeten die Grundlage für die Entstehung indianischer Reiterkulturen, darunter den Ende des 18. Jahrhunderts prägenden Reiternomadismus in den Great Plains. Die Pferde erleichterten die Jagd und den Transport ungemein und führten zu einem veränderten Kräfteverhältnis unter den Völkern, und damit zu weiträumigen Völkerwanderungen. Zudem erschlossen sich die Reitervölker bisher unbewohnbare Gebiete, und mit den zugerittenen Pferden ein neues Handelsobjekt.

Ganz andere Fernveränderungen lösten die nördlichen Kolonialmächte aus, indem sie Pelzhandel betrieben. Sie veränderten damit nicht nur die mit ihnen handelnden Gesellschaften, sondern wirkten darüber hinaus auf deren nahe und ferneren Nachbarn ein, sei es durch Handel mit Waffen und damit zusammenhängende Machtverschiebungen, sei es durch die Entwicklung von Handelsmonopolen der in der Nähe der Handelsstützpunkte (Forts) lagernden Stämme, sei es durch Auslösung von Völkerwanderungen.

Die Befreiung von portugiesischer, spanischer und britischer Kolonialherrschaft in den Jahrzehnten um 1800 bedeutete für die Indianer eine Intensivierung der Binnenkolonisation und eine Zunahme der Einwanderung, vor allem in den USA, Kanada, Brasilien, Chile und Argentinien. Damit stand das von ihnen bewohnte Land viel stärker Verwertungsinteressen heimischer Eliten im Wege, die durch keine Zwischenmacht oder durch die Kolonialverwaltung mehr behindert wurden.

In Nordamerika gerieten die Indianer schnell in die Minderheit, da ihre Zahl rapide abnahm, während die der Weißen zunahm. Selbst große Koalitionen, wie unter Pontiac und Tecumseh, wehrten sich vergeblich gegen das Vordringen. Bis 1890 war der letzte Widerstand gebrochen.

Dabei versuchten die Staaten die Kosten der Besiedlung, d. h. den Aufbau einer Infrastruktur, etwa durch transkontinentale Eisenbahnbauten, Verwaltung und Verteidigung, Polizei und Gerichte auf verschiedenen Wegen zu bestreiten. In den USA eigneten sich die Siedler als unbearbeitet betrachtete Ländereien an (Squatting) und zahlten dafür später geringe Summen, ein Verfahren, das in Kanada in geordnetere Bahnen gelenkt wurde (vgl. Wirtschaftsgeschichte Kanadas). Letztlich lief dies aber auch hier auf eine Inbesitznahme der überwiegenden Teile des Bodens durch Siedler aus Europa hinaus, deren Zuwanderung gefördert wurde.

In Südamerika wurden die kolonialen Landzuteilungen aufgelöst. Die Ländereien gingen an Großgrundbesitzer, die sie überwiegend als Haziendas, bzw. als Fazendas (Brasilien) weiterführten. An diesem Großgrundbesitz entzünden sich bis heute zahlreiche Konflikte, denn sie überließen vielen Indios zwar kleine Parzellen für die Subsistenzwirtschaft, forderten aber dafür Dienste – eine Wiedereinführung feudaler Frondienste.

Widerstand wurde mit Waffengewalt und Hunger gebrochen, die Indianer mussten in den USA sogar alles Land östlich des Mississippi verlassen (Indian Removal Act und Pfad der Tränen), in Kanada wurden Reservate meist im traditionellen Gebiet eingerichtet (reserves), ebenso wie in den USA (reservations). Dort wurden allerdings häufig mehrere Stämme, die kulturell oftmals weit entfernt standen, in eine Reservation gezwungen. Ende des 19. Jahrhunderts war dieser Prozess im Norden im Großen und Ganzen abgeschlossen, die Zahl der Indianer auf einen Bruchteil reduziert.

Während die Missionierung im Süden überwiegend im 16. und 17. Jahrhundert durch katholische Orden erfolgte, wurden viele Stämme im Norden erst im Laufe des 19. Jahrhunderts katholisch oder schlossen sich einer der protestantischen Konfessionen an. Dies war jedoch nur der erste Schritt zur Assimilierung, die auf die Auslöschung der Kulturen hinauslaufen sollte, die von Kanada und den USA, aber auch den Kirchen als minderwertig betrachtet wurden. Mehrere Generationen lang fruchtete dies jedoch wenig, so dass man die Kinder weitgehend von den Erwachsenen absonderte, um sie in internatartigen Schulen (Residential Schools) zu unterrichten, wie sie in ganz Kanada bestanden. Dort wurde ihnen nicht nur jede traditionelle kulturelle Äußerung, sondern vor allem der Gebrauch ihrer Sprache verboten. Gegen die rechtliche und ökonomische Marginalisierung mit Rechtsmitteln vorzugehen wurde ihnen darüber hinaus verboten. Traditionelle Rituale, wie Sonnentanz und Potlatch waren bis in die 50er Jahre verboten, die letzten dieser Schulen wurden erst in den frühen 1980er Jahren aufgelöst. Ähnlich war die Situation in den USA.

Der Fischfang und das Jagdrecht wurden gleichfalls untergraben. Besonders die Massenjagd der Amerikaner auf das Wild, wie das Abschlachten der Bisons im späten 19. Jahrhundert oder die Dezimierung der Karibuherden nach dem Bau des Alaska Highways, bedrohen die vertraglich zugesicherte Existenzweise zahlreicher Stämme. Darüber hinaus zerschnitt der Bau riesiger Staudämme die Wanderrouten der Herden und erschwerte damit die traditionelle Lebensweise der Indianer noch mehr. Erst gegen Ende des 20. Jahrhunderts erlangten die Stämme hierin zunehmend Mitspracherechte und verwalten Parks und Schutzgebiete mit. Allerdings ist die Situation regional sehr unterschiedlich.

In den USA führte diese Entwicklung zu einer massiven Landflucht und Verstädterung der Indianer, zugleich wurden die Reservate in Privatbesitz umgewandelt, den die verarmten Bewohner oftmals verkaufen mussten. In den 1930er Jahren erhielten die Stämme die Möglichkeit, sich selbst zu verwalten und Hoheitsrechte auszuüben, doch 1953 bis 1961 versuchte man, die zum Teil neu geschaffenen Stämme und die Reservate aufzulösen und die Indianer zur Abwanderung in die Städte zu veranlassen (Termination Policy). In Alaska schuf der Alaska Native Claims Settlement Act ab 1971 ein System von Beteiligungen und Geldflüssen, wogegen die Ureinwohner ihre Reservate aufgaben, sieht man von Metlakatla auf Annette Island ab.

In Kanada beschreitet man den Weg der Privatisierung in jüngster Zeit gleichfalls. Viele Rechte sind in den letzten Jahrzehnten vor Gerichten erstritten worden, wozu Wiedergutmachungen und Beteiligungen an auf ihrem Land gemachten Erträgen – etwa durch Bodenschätze oder Staudämme –, sowie Entschädigungen für Misshandlungen in den Schulen gehören (s. Residential School). Dennoch lebt inzwischen jeder zweite Indianer in einer Stadt.

In Südamerika begannen die Kämpfe gegen die Unterwerfung erheblich früher, wie etwa im Mixtón-Krieg (bis 1542), und sie dauerten bis in die Mitte des 20. Jahrhunderts. Nach der Zerstörung der Großreiche drangen Spanier weit in den Norden vor und unterwarfen etwa die Pueblobevölkerung am Rio Grande. 1680 gelang diesen ein Aufstand, der bis 1692 andauerte. Der Widerstand der Maya gegen Landenteignung, Versklavung und Demütigung entzündete sich an der Hinrichtung mehrerer Mayaführer am 30. Juli 1847. Der als Kastenkrieg bekannte Aufstand erfasste ganz Yucatan und dauerte bis 1901. Die letzten Cruzoob, wie sich die Aufständischen nannten, schlossen erst 1935 einen Friedensvertrag mit der Regierung, der ihnen bis heute die Selbstverwaltung ihrer Dörfer gestattet. Der Aufstand der Zapatistas, der in der Provinz Chiapas 1994 begann, basierte ebenfalls auf dem Widerstand der Indios, bediente sich aber westlicher Ideologien und der Guerillataktik.

In Bolivien, dem einzigen Land, in dem die Mehrheit aus Indios besteht, regiert seit 2005 ein indianischer Präsident. Evo Morales ließ sich 2008 mit 67 % der Stimmen bestätigen. „Armut, mangelhafter Zugang zu Bildungs- und Gesundheitseinrichtungen sowie fehlende Integration in das formale Wirtschaftsleben“ waren hier, genauso wie im benachbarten Peru, die Ursachen für den Widerstand der Indios – neben dem mangelnden Respekt vor ihrer Kultur. Dabei verbündet sich der verarmte, auch nichtindigene Landraum zunehmend gegen die zentralistischen Hauptstädte Lima und La Paz. Gut ausgebildete Indios, wie Alberto Pizango, der 1.350 Amazonasdörfer führte, vertraten, ähnlich wie in Nordamerika, ihre Ansprüche vor Gerichtshöfen und auf der politischen Ebene. 2009 kam es zu Kämpfen, bei denen bis zu 250 Indianer ums Leben kamen.

Ganz anders ist die Situation in denjenigen Staaten Südamerikas, in denen die Indios zu einer kleinen Minderheit geworden sind, wie in Brasilien. Die Landenteignung wird dort, wenn auch eher von Unternehmen auf der Suche nach Bodenschätzen und von Grundbesitzern, fortgesetzt, wie etwa gegen die Makuxi im Norden oder die Guarani im Süden. Die Regierung steuert dieser Entwicklung nur unzureichend entgegen, wie am 17. März 2009 der Oberste Gerichtshof feststellte. Er entschied, dass das Reservat Raposa/Serra do Sol im Bundesstaat Roraima den dortigen Ethnien gehört. Das Reservat nahe der Grenze zu Guyana war zwar 2005 durch Präsident da Silva den Indios zugesprochen worden, doch habe die Regierung nicht einmal eingegriffen, als es zu Kämpfen kam. Seit 2002 kämpfen die Tremembé im brasilianischen Ceará gegen ein Tourismusprojekt um ihr 3.100 Hektar großes Reservat. Die Suruí, ein Stamm in der Provinz Rondônia, der vor 40 Jahren noch 5.000, heute nur noch 1.300 Angehörige zählt, haben Kontakt zu Google Earth Outreach aufgenommen. Sie wollen die Zerstörung des Regenwalds über Google Earth sichtbar machen und ihr Gebiet überwachen. Die größte Zwangsumsiedlung ist am Rio Madeira vorgesehen, wo GDF Suez, ein halbstaatlicher französischer Konzern, den Jirau-Staudamm bauen lässt. Ähnliches plant die Regierung Lula am Rio Xingu, wo sich inzwischen der Filmemacher James Cameron einmischte. Einen Teil der technischen Ausstattung liefern Voith Hydro, Siemens und Andritz. Dammbauprojekte bedrohen zugleich indianische Kulturen in Kanada, wie etwa in British Columbia, wie dies bereits seit Anfang des 20. Jahrhunderts Bauten in den westlichen USA taten. Sie verhinderten die Lachswanderungen und entzogen damit den davon abhängigen Stämmen die Lebensgrundlage.

Besonders ungünstig ist die Situation bei den weltweit rund 100 isolierten indigenen Gruppen, die von jedem (weiteren) Kontakt verschont werden sollen, weil sie ansonsten den ihnen unbekannten Krankheiten zum Opfer fallen würden. Solche Gruppen existieren in Brasilien, Peru und Ecuador, ebenso wie im Chaco-Gebiet von Paraguay, wo etwa die Ayoreo leben.

In der Ethnologie wird vor allem für Nordamerika, seltener für Mittel- und Südamerika eine grobe Einteilung der indigenen Kulturen nach sogenannten Kulturarealen vorgenommen, in denen Ethnien mit ähnlichen kulturhistorischen Merkmalen zusammengefasst werden "(siehe auch: Nordamerikanische Kulturareale und Kulturen der indigenen Völker Südamerikas)".

Außer einigen mittelamerikanischen Kulturen, die eine Bilderschrift besaßen, wie die Maya, die ein echtes Schriftsystem entwickelten, hinterließen die Kulturen der westlichen Hemisphäre wenige schriftliche Zeugnisse. Doch in den letzten Jahren wurden die ältesten Schriften in die Zeit um 900 v. Chr. zurückdatiert. Der sogenannte Cascajal-Stein aus dem frühen 1. Jahrtausend v. Chr. zeigt auf einer Fläche von 36 mal 21 cm 62 Zeichen. Er belegt, dass die Olmeken, möglicherweise als erste, ein Schriftsystem entwickelt haben.

Unter den Prärieindianern existierten Chroniken, die graphische Symbole für wichtige Ereignisse verwendeten. Ohne mündlichen Kommentar waren diese Chroniken nicht verständlich. Die bedeutendste Bilderschrift ist die auf Baumrinde festgehaltene Stammessage der im Osten der USA lebenden Lenni Lenape, bekannt als Walam Olum. Die Überlieferung erfolgte daher größtenteils mündlich. Die mündliche Überlieferung war jedoch in der Lage Jahrhunderte und teilweise Jahrtausende zurückliegende Ereignisse zu bewahren. Eine weitere Memoriertechnik ist die Errichtung von Erinnerungsmalen, wie etwa von Totempfählen, die an der Nordwestküste für bedeutende Verstorbene aufgestellt wurden.

Schon früh entwickelten Missionare Schriften, die die Laute der Indianersprachen angemessener wiedergeben sollten, als es die begrenzten Möglichkeiten der lateinischen und kyrillischen Zeichen ermöglichen. Hinzu kamen eigenständige Entwicklungen, wie das von Sequoyah ab 1809 entwickelte Cherokee-Alphabet. Heute besitzen zahlreiche Stämme, wie die Cree, eine eigene Schrift.

1828 bis 1834 konnte Gallegina Watie (Elias Boudinot), ein Cherokee, eine Zeitung herausgeben, den Cherokee Phoenix, der wöchentlich in Englisch und Cherokee erschien.

Im Gegensatz zu den Erzählungen der mündlichen Kultur basiert die literarische Produktion ganz überwiegend auf den Kolonialsprachen, die paradoxerweise zu den hauptsächlichen, innerindianischen Kommunikationsmedien geworden sind. Neben dem Hauptstrom der Literatur repräsentiert die "native literature" die Tradition der ethnischen Gruppen Nordamerikas. Sie ist trotz der Übersetzung (ins Englische und Französische) und der Schriftlichkeit stark in mündlichen Traditionen verwurzelt.

Die im 17. Jahrhundert einsetzende schriftliche Überlieferung durch Übersetzung ins Englische bzw. Französische wirkte jedoch durch christlich-moralische Vorbehalte und Missverständnisse verzerrend. Zudem sind zahlreiche Geschichten im Besitz von Abstammungslinien und dürfen nur in bestimmten rituellen Zusammenhängen erzählt werden. Der überwiegende Teil von ihnen ist weder öffentlich zugänglich noch übersetzt.

Die selbstständige literarische Tradition reicht mindestens bis in das frühe 19. Jahrhundert zurück, wie etwa William Apes' "The Experience of William Apes, a Native of the Forest" von 1831 zeigt. Apes (1798–1839) war Pequot und zählt, wie George Copway, ein Anishinabe, und Chief Elias Johnson, ein Tuscarora, zu den frühen Beispielen amerikanischer Literatur. Diese Tradition lässt sich bis Joseph Brant verlängern, der Thayendanegea hieß (1742–1807) – er übersetzte den anglikanischen Katechismus sowie das Evangelium nach Markus in die Sprache der Mohawk. Einen weiteren Aufbruchsversuch stellt das isolierte Werk von Oliver La Farge, die Novelle "Laughing Boy" von 1929 dar, ebenso wie die Tochter eines Mohawk-Häuptlings Emily Pauline Johnson (1861–1913) mit Werken wie "The Song My Paddle Sings", "Flint and Feather" oder "The White Wampum", die auch in den USA und in Großbritannien publiziert wurden. Sie widmete Thayendanegea/Brant eine "Ode to Brant".

Der Kiowa N. Scott Momaday erhielt 1969 den Pulitzer-Preis für "House Made of Dawn", Vine Deloria publizierte "Custer Died For Your Sins. An Indian Manifesto". Den nationalen Rahmen sprengte schließlich Dee Browns "Begrabt mein Herz an der Biegung des Flusses" von 1970. Nun errangen Autoren wie Norval Morrisseau mit Legenden ("Ojibwa Legends of My People", 1965), Dan George und Rita Joe mit poetischen (My Heart Soars, 1974 und Poems of Rita Joe, 1978), aber auch politischen Werken (Harold Cardinal: "The Rebirth of Canada’s Indians", 1977) im Norden Anerkennung. Eine wesentliche Rolle spielte zudem die Rückgewinnung der kulturellen Autonomie nach den Verboten zentraler Traditionen, wie des Potlatch (George Clutesi: "Potlatch", 1969). Insgesamt nahmen die Versuche, an die Überreste der eigenen Kulturen anzuknüpfen, zu (John Snow: "These Mountains Are Our Sacred Places" 1977, Beverly Hungry Wolf: "The Ways of My Grandmothers", 1980). Dabei spielten autobiographische Ansätze eine wichtige Rolle (Rita Joe: "Song of Rita Joe: Autobiography of a Mi'kmaq Poet").

Mesoamerika, die Region mit einer weit zurückreichenden Schrifttradition, nahm sowohl spanische als auch Mayatraditionen auf, wie sie sich etwa in den Inschriften des Herrschers von Palenque, K'inich Janaab' Pakal (615–683), im Tempel der Inschriften erhalten haben. Dabei ist die Verbindung von Text und Abbildung sehr eng, ähnlich wie in den vier erhaltenen Maya-Codices, die ab dem 5. Jahrhundert auf der Innenseite bearbeiteter Baumrinde, vor allem von der Feigenart Ficus glabrata, geschrieben wurden. Unter ihnen gilt der Codex Dresdensis (1. Hälfte 13. Jahrhundert) als wichtigster.

Die meisten Maya-Codices ließ Bischof Diego de Landa ab 1562 verbrennen. Dennoch hat sich hier wie bei den Azteken, wo in der Kolonialphase rund 500 von ihnen entstanden und heute noch zumindest in Teilen existieren, eine Tradition der Codex-Herstellung gehalten. Die Codices der Azteken enthalten meist keine Schriftzeichen, bzw. erst später eingetragene in lateinischer Schrift und in Nahuatl. In dieser Sprache verfassten Missionare erste Grammatiken und Wörterbücher. Von beiden kulturellen Wurzeln geprägt waren bereits die Historien- und Prophetienbücher Chilam Balam. So verbanden sich vorspanische, eher piktographische Traditionen mit kolonialzeitlich-schriftlichen, wobei letztere langsam die Oberhand gewannen.

Im stärker rituellen Vollzug von mündlichem Textvortrag spielte der Gesang eine andere Rolle als in Europa. Im 16. Jahrhundert wurden 91 aztekische Lieder in den Cantares Mexicanos aufgezeichnet, womit rund die Hälfte der Liedtexte überliefert worden ist. Die einzigen überlieferten Lieder der Maya finden sich den Cantares de Dzitbalché aus dem 17. Jahrhundert. Die Vermischung spanischer und indianischer Traditionen wird als "mestizaje" bezeichnet. Auf diese Kultur der Mestizen bezieht sich die Literatur des Chicano, die von den Auswanderern in die USA ausging und sich stark auf die indianischen Wurzeln bezieht.

Ähnlich wie in Nordamerika schuf sich Mexiko ein literarisches Bild des Indianers, das ähnlichen Wandlungen unterworfen war.

Im Süden des Kontinents bestand keine so weit zurückreichende literarisch-piktographische Tradition, wie sie in Mesoamerika bestand. Zwar gab es die quipu, ein Memoriermittel aus Knoten, das Kundige der Knotenschnüre, Quipucamayos, beherrschten, doch bleibt der Zweck der Schnüre unklar. Dennoch wirkten mündliche Traditionen und das Fortbestehen indigener Traditionen stark auf die schriftlich-literarische Entwicklung ein.

Im 20. Jahrhundert entstand der Indigenismo, dessen bedeutendster Protagonist José María Arguedas aus Peru war. Er stammte von Quechua ab und war bei ihnen aufgewachsen. Als Völkerkundler veröffentlichte er 1966 das Waruchiri-Manuskript aus dem 16. Jahrhundert auf Spanisch, womit er es, obwohl fehlerhaft, einem breiteren Publikum bekannt machte – es war bereits 1939 von Hermann Trimborn ins Deutsche übersetzt worden. Es gilt als wichtigstes Denkmal der frühkolonialen Quechua-Literatur. Es ist zugleich die einzige Textsammlung in Quechua und befasst sich mit Mythen und Beschreibungen religiöser Zeremonien im Hinterland von Lima – wahrscheinlich aus der Feder des Geistlichen indianisch-spanischer Herkunft Francisco de Avila (vor 1608). Zu dieser Zeit war die von Vizekönig Toledo angeordnete Einrichtung von Reduktionen, also die Konzentrierung und Neuansiedlung der Indianer, bereits durchgeführt. Das Stück entstand also in einer Zeit, in der sich indianische und spanische Traditionen schon stark überlagerten.

Eine der Überlieferungen aus Quechua-Feder, die von "El Inca Garcilaso de la Vega" stammenden "Comentarios reales de los incas" (1609) weisen, trotz jahrzehntelangem Gebrauch des Spanischen noch hohe Kompetenz in der Muttersprache des Verfassers auf. Ähnlich wie Felipe Guaman Poma de Ayalas "Nueva corónica y buen gobierno" (um 1615) weist das Werk noch stark indigene Züge auf, und verbindet Mündlich- und Schriftlichkeit.

Apu Ollantay, ein Drama wohl aus dem 18. Jahrhundert, das von der verbotenen Liebe des namengebenden Inkagenerals zur Inkaprinzessin Kusiquyllurs handelt, erfreute sich besonders während der Unabhängigkeitsbewegungen erheblicher Beliebtheit. Vor allem in Cuzco lebende Spanier, die die Lösung von der Kolonialmacht forderten, betrachteten das Quechua möglicherweise sogar als die angemessene Sprache ihrer Bewegung.

Inzwischen ist Quechua zu einer selbstständigen literarischen Sprache geworden – Aymara weniger –, in die mehr und mehr übersetzt wird. 1975 verarbeitete Jorge Lira die von ihm gesammelten Erzählungen (Isicha Puytu). Später folgten Märchen vom Urubamba, dann "Unay pachas" von Rufino Chuquimamani, "Pirumanta qillqasqa willakuykuna" von Carmelón Berrocal und 1992 "Unay willakuykuna" von Crescencio Ramos.

Zu den bekannteren Werken zählt die Autobiographie von Gregorio Condori Mamani und Asunta Quispe Huamán, die von Ricardo Valderrama Fernández und Carmen Escalante Gutiérrez 1982 aufgezeichnet wurde.

José Oregón Morales publizierte 1994 acht Kurzgeschichten (Loro qulluchi – Bekämpfung der Papageien), wobei er seine dörfliche Kindheit in den Anden verarbeitet und Märchen variiert. Porfirio Meneses Lazón verfasste Quechua-Gedichte (Suyaypa llaqtan, 1988) und Kurzgeschichten (Achikyay willaykuna (Erzählungen des Morgengrauens, 1998)) bei denen er seinen Erzählstil mit den volkstümlich gehaltenen Dialogen kontrastiert.

Den Literaturpreis "Premio de cuento del Concurso Nacional de Literatura Quechua" erhielt 1997 Macedonio Villafán Broncano (* 1949) für seine Erzählung "Apu Kolkijirka" (Herr Silberberg). Apu, eine Berggottheit, tritt als Ich-Erzähler auf und erzählt die Geschichte „seines“ Ortes Cutacancha (Region Ancash).

Entsprechend der Wechselwirkung von natürlicher Umgebung und kultureller Entwicklung waren die Traditionen extrem verschieden. Während die Monumentalkulturen zwischen Mississippi und Anden vielfach Stein und Lehm als Ausgangsmaterial benutzten, bevorzugten die waldreichen Regionen des Nordens Holz und andere organische Materialien.

Heute wächst die bildende indianische Kunst in einem expandierenden Kunstmarkt mit. So sind Werke traditioneller Schnitzkunst, wie die Totempfähle der pazifischen Küstenkulturen zu Sammelobjekten geworden.

In Kanada und Alaska dominieren die "West Coast Native Art" – dies waren Meister der Haida, Tsimshian und Kwakiutl, dann Nuu-chah-nulth und Küsten-Salish – und die „Woodlands“-Schule der „Legend Painters“ – vor allem Norval Morrisseau, ein Ojibwa, den man gelegentlich den „Picasso des Nordens“ nannte.

Spätestens im 17. Jahrhundert begann der Tauschhandel mit Werken für Reisende, wie beispielsweise Mokassins oder kleine Schnitzwerke. Noch heute wird diese Kunst in allen Qualitätsstufen angeboten. Traditionelle Kunst deckt dabei oft die Erwartungen von Kunst ab, die an sie herangetragen werden, versucht aber zugleich einen Kompromiss zwischen den Traditionen. Sie dient vielfach einer Werkproduktion, die nicht als Kunst aufgefasst wird, sondern rituellen, oftmals verborgenen Zwecken dient. Künstler wie Tony Hunt und Bill Reid (1920–1998) knüpften dabei – trotz des Verbots öffentlicher Rituale wie des Potlatch – an die Traditionen an, die vor allem von den Haida Charles Edenshaw (um 1839–1920), Willie Seaweed (1873–1967) und Mungo Martin (1879/82–1962) von den Kwakiutl ererbt waren.

1973 gründeten sieben Künstler die Indian Group of Seven. Neben zeitgenössischen Einflüssen verarbeiteten sie piktographische Traditionen der Algonkin und Petroglyphen des kanadischen Schilds. Viele Künstler, die mit nicht-traditionellen Techniken arbeiten, betrachten sich hingegen in erster Linie als Künstler und lassen sich ungern als „Indianerkünstler“ etikettieren.

Ähnlich wie im Norden standen in den heutigen USA Objekte wie Hüte, Decken, Körbe im Mittelpunkt, ebenso wie kunstvoll verzierte Waffen und Pfeifen, in manchen Regionen eine hoch entwickelte Baukunst. Die Objekte waren jedoch keine Kunstproduktion im westlichen Sinne, und nicht für einen Markt bestimmt. Das änderte sich ab den 1820er Jahren, als die natürlichen Lebensgrundlagen der Indianer zunehmend zerstört wurden. So entstand die Iroquois Realist School (realistische Schule der Irokesen) bei den Haudenosaunee in New York City, geführt von David und Dennis Cusick. Edmonia Lewis (ca. 1845–1911), eine Künstlerin mit afrikanischen und indianischen Vorfahren (Mississaugas of the New Credit First Nation) mit einem Atelier in Rom, schnitzte 1877 das Porträt des Präsidenten Ulysses S. Grant. Angel DeCora (Hinook-Mahiwi-Kilinaka, 1871–1919), die an der Hampton University studierte, engagierte sich im Arts and Crafts Movement (um 1870 bis 1920 v. a. in den USA und Großbritannien) und vermittelte ihren Schülern die Bedeutung von Kunst bei der Entwicklung des Selbstwertgefühls und des Widerstands gegen die staatliche Assimilierungspolitik. Eine wichtige Gruppe waren darüber hinaus die Kiowa Five aus Oklahoma, die 1928 erstmals in Prag ausstellten.
Die künstlerisch-rituellen Traditionen Mittel- und Südamerikas übernahmen schon früh neue Materialien, die die Kolonisatoren einführten. Don Fernando de Alva Ixtlilxochitl, ein direkter Nachkomme Ixtlilxochitls I. von Texcoco, malte bereits im 16. Jahrhundert mit Tinte und Wasserfarbe auf Papier (Codex Ixtlilxochitl).

Erheblich weiter lassen sich die Künste der Metall- und Steinbearbeitung zurückverfolgen. An Metallen wurden vor allem Gold und Kupfer verarbeitet. Zahlreiche Relikte zeugen von der Kunstfertigkeit, auch wenn viele Werke durch Spanier, die nur am Gold interessiert waren und die symbolgeladenen Artefakte von sich wiesen, eingeschmolzen wurden.

Systematische Musiksammlungen begannen im Norden erst um 1900. 1911 waren dies Lieder der Malecite und Mi’kmaq aus Kahnawake und Lorette. Zugleich nahmen Wissenschaftler Gesänge der Huronen, Algonkin und Irokesen auf, der Delaware und Tutelo. Doch erst der Anthropologin und Tänzerin Gertrude Prokosch Kurath (1903–1992) gelang es, ein Notationssystem für die Irokesentänze zu entwickeln. Es folgten Untersuchungen zu rituellen Tänzen (William Fenton: "The Iroquois Eagle Dance", 1953) und den Medizingesellschaften ("The False Faces of the Iroquois", Norman, Oklahoma 1987).

Die Musik der Cree und der Ojibwa, der Blackfoot und Sarcee folgten, wobei Forscher aus den USA bereits um 1900 wichtige Beiträge leisteten. Man untersuchte sowohl die traditionelle als auch die von den Blood adaptierte Country- und Westernmusik sowie christliche Hymnen.

James Teit nahm Gesänge der Sikani, Tahltan, Tlingit, Carrier, Okanagan und Nlaka'pamux auf, 1913 erfolgten Sammlungen bei den Sikani bis zum Großen Sklavensee. Weitere folgten in den 1970er und 1980er Jahren bei Küsten-Salish in British Columbia und Washington.

Erst ab den 1980er Jahren begannen die First Nations die Forschungen selbst zu betreiben. Dazu kamen Labels, die von Indianern getragen wurden.

Bei den Maya waren neben Trommeln und verschiedenen Flöten Maracas und Okarinas in Gebrauch. Hinzu kommt ein Saiteninstrument, von dem sich zeigen ließ, dass es die Stimme eines Jaguars imitiert. Dabei war die Verbindung zu Tanz und Ritual, wie überall in Amerika, sehr viel enger als in Europa.

Insgesamt unterscheidet man in Amerika sechs Areale: das der Inuit und der Nordwestküste, dann Kalifornien und Arizona, das Große Becken, die Athapasken, Plains und Pueblo sowie das Östliche Waldland. Grundsätzlich steht das Singen im Vordergrund, Instrumente bilden eine rhythmische Begleitung. Dabei ist der Gesang im Norden, vor allem östlich der Rocky Mountains, dominanter, im Süden stärker zurückgenommen. Trommeln und Rasseln (in Südamerika Maraca) herrschen vor, dazu kamen in Meso- und Südamerika verschiedenste Flöten, schließlich als besondere Trommelform der Teponaztli.

Im Norden entwickelte sich an der Nordwestküste eine komplexe rituelle Musikkultur mit umfangreichen Tanzritualen und langen Texten, die auswendig gelernt wurden. Melodien und Texte in Kalifornien und dem Großen Becken sind einfacher und kürzer, Falsett wurde bevorzugt. Hier herrschen kostümierte Tänze vor, die bei den athabaskischen Gruppen eher selten sind, außer bei den von den Pueblos beeinflussten Apachen. Bei den Navajo wurde Gesang auch zur Heilung eingesetzt. Musik war niemals eine Tätigkeit für sich, sondern stark in soziale Handlungsrahmen eingebunden. Die Musik der Prärien ist am besten erforscht, und sie ist bei den weit verbreiteten Powwows geläufig.

Über die vorspanische Musik Südamerikas ist wenig bekannt. Besonders in Patagonien wurde polyphoner Gesang entwickelt. In Brasilien und den angrenzenden Tropenwaldgebieten existiert noch traditionelle Musik mit Gesang, Flöte und Perkussion.


"→ Ethnische Religionen Nordamerikas, Religion der Azteken, Religion der Maya, Ethnische Religionen der Gegenwart in Mesoamerika, Religion der Inka, Indigene Religionen Südamerikas"

Die ethnischen Religionen Amerikas beruhten in der überwiegenden Zahl in der Vorstellung einer Allbeseeltheit der Naturerscheinungen (Animismus). In den Hochkulturen fand sich eine Priesterherrschaft (Theokratie), die sich zwischen Mississippi und den Anden in riesigen Bauwerken manifestierte. Hier entstanden auch Priesterschulen, während die Ausbildung zu Medizinmännern durch Ältere geschah, aber auch in Geheimgesellschaften, die ihr Wissen an ihre Mitglieder weitergaben.

In weiten Teilen basierte dies auf einem engen Verhältnis zur natürlichen Umgebung, so dass Wetter, Pflanzen und Tiere, Erde und Himmel, aber auch Sterne und die Berechnung von Ereignissen des Jahreslaufs im Mittelpunkt standen. Schöpfungsmythen und die kollektive Erinnerung an einen häufig aus dem Tierreich stammenden gemeinsamen Vorfahren waren häufig sowie manchmal der Glaube an einen Schöpfergott (der jedoch zumeist keinen Einfluss mehr auf die Menschen hatte). Einige Stämme verehrten eine unpersönliche Lebensenergie, die sich etwa in der Sonne, als Fruchtbarkeit der Erde, als Weisheit oder Stärke, die mit Bären, Wölfen, Raben, Schlangen oder dem Quetzalcoatl äußerte.

Die religiösen Inhalte waren orts- und verwandtschaftsspezifisch und besaßen keinen universellen Geltungsanspruch. Die Heiligkeit von Orten, Ritualen, von Wissen und Geschichten, Tänzen und Musik sowie Personen stand im Mittelpunkt. Die Hochkulturen entwickelten komplexe öffentliche Rituale, an denen Tausende von Menschen teilnahmen.

Die Initiation und Ausbildung war häufig Aufgabe der Älteren, bei Geisterbeschwörern und Medizinleuten geschah dies vielfach durch spontane Visionen. Schon als Kinder wurden bei manchen Stammesgruppen – wie den Küsten-Salish – die „Historiker“ der Familien und Stämme ausgewählt und unterrichtet. In den Schriftkulturen der Maya und Azteken wurden Rituale schriftlich festgehalten, die religiösen Gehalte symbolisch aufgezeichnet.

In Lateinamerika drängten die Orden und die Krone auf Missionierung, eine Aufgabe, die die Eroberer nur vordergründig auf sich nahmen (Konquistadorenproklamation), oftmals, um die des Lateinischen nicht mächtigen, daher unverständigen und widerstrebenden Heiden, „gesetzeskonform“ unterwerfen oder umbringen zu können. Gleichzeitig hatte der spanische Staat die kirchliche Organisation von Rom weitgehend losgelöst und zu einer Staatskirche umgewandelt, der mit der Inquisition eine gefürchtete Waffe zur Verfügung stand. Dementsprechend förderte die Krone die Mission in ganz Lateinamerika und nutzte die Kirche zugleich, um die "Granden" unter Kontrolle zu halten, und um ein Eindringen der reformatorischen Kräfte in die Kolonien zu verhindern.

Dies stärkte die Orden auch weiter im Norden, wo sie zugleich, vor allem die Jesuiten, für Frankreich tätig waren. So wurden die Religionen der lateinamerikanischen Indianer und in geringerem Maße der Neufrankreichs mit katholischen Ritualen konfrontiert, häufig wurden Umsiedlungen und Zusammenführungen durchgeführt, die einer starken Vermischung der zuvor getrennten Gruppen Vorschub leisteten, wie etwa bei den Guaranì in Paraguay. Dabei verbanden sich Missionare vielfach mit den Kaziken, den jeweiligen Eliten, und die Jesuiten überantworteten ihnen sogar militärische Führungsaufgaben.

Die Bekehrung der Indianer gelang anfangs meistens nicht, da die Menschen in der Regel keinerlei Veranlassung sahen, ihren „bewährten“ Glauben aufzugeben. Überdies war ihnen das Bestreben zur Bekehrung vollkommen fremd und unverständlich. Das Christentum wurde daher im Norden zumeist erst nach verheerenden Epidemien oder kultureller Entwurzelung als eine Form der spirituellen Heilung angenommen. Indianische Selige und Heilige wie Kateri Tekakwitha dienten dann als Vorbilder. Bei der Missionierung spielten zunächst die Jesuiten eine Hauptrolle, im 19. Jahrhundert die Oblaten. Wenige protestantische Gruppen, wie Methodisten und Baptisten missionierten im englischsprachigen Teil Amerikas, hinzu kamen russisch-orthodoxe Missionare in Alaska. Daher sind die Indianer heute überwiegend katholisch, bilden im Nordwesten allerdings einen konfessionellen Flickenteppich. Dort entwickelten sich, wie in Lateinamerika, eklektische Formen, wie die Indian Shaker Church, oder, wie in Peru, Gruppierungen, die die Erinnerung an die Inkas wachhielten. Synkretistische „Mischreligionen“ entstanden häufig; die größte in Nordamerika ist die Native American Church, aufgrund der Verehrung eines Rauschmittels auch als "Peyotismus" bekannt. Synkretistische Religionsformen bildeten bei den Maya die Grundlage für das als Sprechendes Kreuz bezeichnete Orakel, das Jose María Barrera am 15. Oktober 1850 zur Fortsetzung des Kastenkrieges gegen die mexikanische Regierung aufforderte. Das Kreuz wuchs auf den Wurzeln eines Kapokbaums, des heiligen Baums des Lebens, der wiederum aus einer Höhle wuchs, die einen heiligen Ort darstellte, der sich bei einem Cenote (Ts’ono’ot) befand, einem Ort der Regengötter Cháak. Hüter des Kreuzes, und damit wichtige Aufstandsführer waren etwa Crescencio Poot (1875–1885) oder María Uicab († 1872), die „Königin von Tulum“.

Zahlreiche Züge der voreuropäischen Spiritualität haben sich erhalten oder sind wiederbelebt und weiterentwickelt worden. Dabei werden viele Rituale nach wie vor nur innerhalb begrenzter Gruppen oder von Geheimgesellschaften geübt. Das gilt etwa für den Sonnentanz der Präriekulturen oder die Medizinbünde der Irokesen. Im Norden spielt vor allem der Begriff der „Medizin“ eine bedeutende Rolle. Zur Aufzeichnung komplexer Vorgänge oder historischer Ereignisse benutzten oftmals geheime Gesellschaften der Algonkinstämme mindestens seit dem 16. Jahrhundert Birkenrinde, auf der verschlüsselt spirituell bedeutsames Wissen eingeritzt wurde. Das Sprechende Kreuz wird noch heute verehrt, allerdings nur unter Mayas.

Angesichts der widersprüchlichen Rolle, die Mission und Kirche gegenüber den Indianern gespielt haben, überrascht es nicht, dass etwa die brasilianischen Indigenen im Mai 2007 die Aussage Papst Benedikts XVI. zurückwiesen, die katholische Kirche habe die Indianer in Lateinamerika erlöst. Noch sein Vorgänger Johannes Paul II. hatte 1992 Fehler bei der Evangelisierung eingestanden.

Die Regierungen Nordamerikas haben Institutionen ausgebildet, die für die Belange der Indianer zuständig sind, die aber oftmals aus den Kriegsministerien hervorgegangen sind. In Kanada ist dies das Department of Aboriginal Affairs and Northern Development (auch "Indian and Northern Affairs Canada"), in den USA seit 1824 das heute dem Innenministerium unterstellte Bureau of Indian Affairs. Jede Provinz bzw. die meisten Bundesstaaten wiederum haben ein Ministerium oder eine entsprechende Abteilung, die gleichfalls mit dieser Thematik befasst sind.

Auf der anderen Seite steht in Kanada eine Reihe politischer Parteien und die "Assembly of First Nations" als Dachorganisation. Sie ist das Sprachrohr aller First Nations, führt Prozesse und betätigt sich inzwischen über die Staatsgrenzen hinaus, etwa bei den Vereinten Nationen, wenn es um Menschenrechtsfragen geht. Stammesräte, die manchmal nur wenige, manchmal mehrere Dutzend Stämme vertreten, hüten Archive, führen Vertragsverhandlungen und bilden meist eine Vertretung der sich sprachlich-kulturell nahestehenden Stämme gegenüber der Regierung.

Unterhalb dieser Ebene liegen zwei Systeme im Widerstreit, nämlich das von der Regierung vorgeschriebene System gewählter Häuptlinge und ihrer Berater einerseits, und das der traditionellen Häuptlinge. Bei vielen Stämmen beherrschen die von der Regierung geförderten Wahlhäuptlinge die Stammesräte, die wiederum zahlreiche politisch und wirtschaftlich bedeutende Positionen vergeben. Dazu kommen die jungen Erwachsenen und die Kinder, deren Zahl schnell wächst, die aber weder in der einen noch in der anderen Gruppe ausreichend vertreten sind. Auch der Anteil der städtischen Bevölkerung nimmt stetig zu. In den USA haben viele Stämme seit den 1930er Jahren Selbstverwaltungsrechte und führen Polizei und Gerichte in ihren Reservaten.

Die Frage nach der Möglichkeit quasi-staatlicher Souveränität mit entsprechenden Territorien steht dabei sowohl in Kanada als auch in den USA in hartem Kontrast zum Versuch, die Stämme als Summe von Individuen zu behandeln. Den Stämmen Kanadas soll ein Teil ihres traditionellen Gebietes zurückgegeben werden, doch nicht mehr als Kollektiveigentum, wie die Reservate, sondern als privater, veräußerlicher Besitz. Angesichts der verbreiteten Armut ist abzusehen, dass dies zum Verkauf großer Teile indianischen Landes führen würde, eine Assimilationsstrategie, wie sie die USA lange betrieben haben.

Die sozialen Probleme, wie Armut, Krankheiten, Alkohol- und Drogenprobleme, das Auseinanderbrechen von familiären Strukturen, sowie die Bedrohung der Subsistenzwirtschaft durch Einschränkungen des Fisch- und Jagdrechts, dazu ökologische Probleme und die Folgen zahlreicher Zwangsumsiedlungen treffen diese Gruppen besonders hart. Diese existentiellen Probleme haben vor allem in den USA und Kanada zu einer stark erhöhten Selbstmordrate geführt. In den USA liegt sie 70 % höher als im US-amerikanischen Durchschnitt. Indianische Jugendliche zwischen 15 und 24 Jahren bringen sich dreimal so häufig um wie ihre amerikanischen Altersgenossen Zugleich nimmt die Gewalt von Gangs in manchen Reservaten deutlich zu.

Seit langem gibt es Bemühungen wirtschaftlicher und kultureller Erholung. Letztere kreist zum einen um die Sprache und die Rituale, bei einigen Stämmen um die Wiederherstellung der überlieferten Gesellschaftssysteme.

In Mexiko ist die "Comisión Nacional para el Desarrollo de los Pueblos Indígenas" (CDI), die „Nationale Kommission für die Entwicklung der indigenen Völker“ zuständig.

In Brasilien nennt sich die zuständige Institution "Fundação Nacional do Índio (FUNAI)", die dem Justizministerium untersteht. Sie wurde 1910 von Cândido Rondon gegründet, unter seiner Leitung entstand 1961 das erste Reservat (am Rio Xingu). Danach wurde die FUNAI fast bedeutungslos und das Justizministerium kontrolliert seit 2002 die zuletzt 2008 aktualisierte Gesetzgebung (). FUNAI geht von 5,6 Millionen Indios um 1500, sowie 1.300 Sprachen aus, heute von 460.000 in etwa 215 bekannten Nationen, von ihnen leben 100 bis 190.000 in Städten. Man unterscheidet dabei 180 bekannte Sprachen und unterstützt nach Jahrhunderten der Assimilation die Unterschiedlichkeit. Erst 1953 entwickelte die brasilianische Anthropologie, später die 1955 gegründete Associação Brasileira de Antropologia, eine Namenskonvention für alle Stämme.

Organisationen wie der Koordinator der indigenen Völker des Amazonasbeckens und der Indian Council of South America versuchen länderübergreifend die Rechte der Indianer zu stärken, ähnlich wie der International Indian Treaty Council für ganz Amerika. Hinzu kommen Vertretungen bei der UNO oder der Organisation Unrepresented Nations and Peoples Organization.

Die Vernachlässigung ganzer Regionen und die ausbleibenden Landreformen führten in einigen Ländern dazu, dass Indiogruppen die linke, manchmal auch militante Opposition unterstützten, wie das Movimiento Revolucionario Túpac Amaru, das seinen Namen vom letzten Inkaherrscher ableitet. Auch in den Staaten, in denen die Indios eine kleine Minderheit darstellen, wie in Kolumbien, versuchen sie ihr Land gegen Privatisierung, etwa durch Rohstoffunternehmen, zu schützen. So half ihnen Martín von Hildebrand, dem Schutz ihrer Kultur, Sprachen und Reservate Verfassungsstatus zu verschaffen. Am 23. August 2011 stimmte der peruanische Kongress einer Gesetzesvorlage zu, die die Konsultation der regionalen indigenen Gruppe zwingend vorschreibt, wenn ein Unternehmen die dortigen Rohstoffe abbauen oder Holz einschlagen will.

Jagd und Fischerei dienen vielfach dem Lebensunterhalt, doch ist der kommerzielle Fang nur eingeschränkt möglich. Viele Fischbestände sind rückläufig und die Regierungen neigen dazu, den kommerziellen Fischfang zu bevorzugen, der den Indianern häufig verboten ist. Im Norden steckt die Holzindustrie in einer Krise, da große Mengen überschüssigen Holzes durch die katastrophalen Verluste, die der Bergkiefernkäfer anrichtet, auf den Markt drängen. In Südamerika werden für Biodiesel erhebliche Waldbestände vernichtet, so dass etwa Guarani in Paraguay zwangsweise umgesiedelt wurden. Steil ansteigende Rohstoffpreise von 2006 bis 2008 schürten vorhandene Konflikte, und so wuchs der Druck auf die Stämme, Abbaugenehmigungen zu erteilen. Die natürliche Umgebung ist aber Voraussetzung für den Erhalt der kulturellen Vielfalt, die die indianischen Kulturen kennzeichnet.

Über Selbstverwaltung und Tourismus entstehen in zahlreichen Parks, die in den letzten Jahrzehnten entstanden sind, für viele Reservatsbewohner Arbeitsplätze, die weder die natürlichen Ressourcen im bisherigen Ausmaß zerstören, noch von staatlicher Wohlfahrt abhängig halten.

Neben den traditionellen Wirtschaftsweisen, der Überlassung von Land an Rohstoff- und Energieunternehmen und der Tatsache, dass die Indianer versuchen, ihre ländliche Wirtschaftsbasis durch Holzeinschlag, Gewinnung von Wasserkraft, Wind- und Sonnenenergie, Rohstoffabbau, Tourismus, Kunsthandwerk und Landwirtschaft zu nutzen, wachsen zwei Bereiche in Nordamerika besonders schnell: Glücksspiel und Wirtschaftskontakte mit anderen indigenen Völkern.

In Meso- und Südamerika ist die Landwirtschaft, die dort ihre historischen Wurzeln hat, viel stärker in indianischen Händen als im Norden. Indio ist in vielen Gegenden geradezu zum Synonym für Campesino, Landbewohner, geworden, wobei die Subsistenzwirtschaft vielfach überwiegt. Doch ist die Produktpalette eine sehr viel andere als außerhalb der indianischen Ballungsgebiete. Tausende von Kartoffelsorten repräsentieren beispielsweise beinahe die gesamte Sortenvielfalt der Welt. Vom Mate-Tee reicht das Spektrum der Exportwaren über den Kaffee bis zu Coca und Mohnprodukten, die auf verschiedensten Wegen den illegalen Weltmarkt erreichen.

Von den USA seit 1979 ausgehend spielen Kasinos eine zunehmende Rolle, die sich immer mehr zu touristischen und Entertainment-Unternehmen entwickeln. Während es in Kanada 2008 nur 17 Kasinos gab, existierten in den USA über 400 sogenannte Indianerkasinos in 27 Bundesstaaten. Davon befinden sich allein 54 in Kalifornien, 73 in Oklahoma, wo ein deutlicher Siedlungsschwerpunkt der US-Indianer liegt, weitere 115 befinden sich in den nördlichsten Bundesstaaten entlang der kanadischen Grenze. Insgesamt beschäftigen die nordamerikanischen Kasinos rund eine halbe Million Menschen und setzten 2005 rund 20 Milliarden Dollar um, das zum großen Teil den indianischen Eigentümern zugute kommt.

In Kanada und den USA bieten Fernseh- und Radiostationen Sendezeiten in den lokalen Indianersprachen, besonders wichtig ist inzwischen allerdings das Internet geworden. Erste eigene Fernsehsender entstanden in den USA, wie die "North West Indian News" (NWIN) oder das "Aboriginal Peoples Television Network". Seit Ende 2009 strahlt auch der erste Fernsehsender in Ecuador ein Programm aus, das in Quechua angeboten wird.

Der Zugang zum Arbeitsmarkt hängt von der Art der Ausbildung, dem Zugang zu Bildung und der Erreichbarkeit der Arbeitsstätten ab. Die ländlich lebenden Indigenen stehen dabei erheblichen Problemen gegenüber.

Nachdem die Internatssysteme in den englischsprachigen Staaten des Nordens seit den 1960er Jahren aufgelöst worden waren, übernahmen vielfach indianische Gruppen selbst die Schulen. Gerade für die oftmals sehr ländlichen Reservate ist die Anbindung an das Internet dabei inzwischen von großer Bedeutung.

Auffällig ist, dass der Anteil der Schüler, die einen höheren Bildungsabschluss erreichen, im Vergleich zur übrigen Bevölkerung erheblich niedriger ist. So erlangten nach einem Regierungsbericht Kanadas nur rund 27 % der 15- bis 44-Jährigen ein sogenanntes "post-secondary certificate", "diploma" oder "degree", ein Anteil, der ansonsten bei 46 % liegt. Dabei wird der Übergang zu höherer Bildung von bürokratischen Hürden, und vielfach von den großen Entfernungen bis zur Bildungsstätte behindert. In Lateinamerika ist die Situation der ländlichen Gegenden in dieser Hinsicht noch ungünstiger, zumal wenn sie, wie in den Anden, sehr isoliert sind. Zudem ist die Art der Ausbildung und Bildung, wie sie von den Städten ausgeht, nur bedingt auf ländliche oder gar indianisch-traditionelle Lebensweisen übertragbar. Hinzu kommt, dass die Bildungssprachen zugleich die Kolonialsprachen sind.

Für die universitäre Ausbildung sorgt in Kanada seit 2003 eine nationale First-Nations-Universität in Regina, in Saskatchewan. Daneben unterrichten zahlreiche Colleges verschiedene Aspekte der indigenen Kulturen, viele arbeiten mit Forschungsinstituten, Museen, Universitäten und privaten Unternehmen, vor allem im archäologischen Bereich zusammen.

Schon die einfachste Erfassung von Aussagen über Bildung, wie bei der Frage der Lesefähigkeit, bereitet enorme methodologische Probleme. Dennoch verkündete der bolivianische Präsident Evo Morales Ende 2008, in seinem Land hätten 820.000 Menschen binnen drei Jahren lesen gelernt. Damit sei die von der UNO vorgegebene Marke von mehr als 96 % Lesefähigkeit erreicht worden, und Bolivien damit frei von Analphabetismus.

Seit 1994 wird in Bolivien interkulturell und zweisprachig unterrichtet, Anfang 2007 erhielten rund 1,2 Millionen Schüler staatliche Hilfen. In Gesellschaften mit extrem unterschiedlichen kulturellen Gruppen erweist sich dabei die Zielvorstellung einer bloßen Alphabetisierung als zu einseitig an bereits in das weltwirtschaftliche Gefüge ausgerichteten Bedürfnissen orientiert. Die Diskussion um die kulturell angemessene und von den Gruppen selbst bestimmten Bildungswege, -mittel- und -inhalte steht auf staatlicher Ebene erst am Anfang.






</doc>
<doc id="13272" url="https://de.wikipedia.org/wiki?curid=13272" title="Pulsar">
Pulsar

Ein Pulsar (Kunstwort aus engl. "", „pulsierende Radioquelle“) ist ein schnell rotierender Neutronenstern. Die Symmetrieachse seines Magnetfelds weicht von der Rotationsachse ab, weshalb er Synchrotronstrahlung entlang der Dipolachse aussendet. Liegt die Erde im Strahlungsfeld, empfängt sie wie von einem Leuchtturm regelmäßig wiederkehrende Signale. Pulsare strahlen hauptsächlich im Radiofrequenzbereich, manchmal bis in den Röntgenbereich oder nur in diesem. Von den mehr als 1700 bekannten Quellen ließen sich nur bei einigen wenigen auch im sichtbaren Bereich Intensitätsschwankungen beobachten. Die Rotationsdauer eines Pulsars ohne Begleiter liegt zwischen 0,01 und 8 Sekunden. Die Rotationsdauer erhöht sich pro Sekunde um etwa 10 Sekunden (d. h., er wird im Laufe der Zeit langsamer) und begrenzt die Lebensdauer auf etwa zehn Millionen Jahre.

Daneben gibt es sogenannte "Millisekunden-Pulsare" (etwa 5 Prozent der Pulsare) mit Umlaufzeiten von einer bis zehn Millisekunden und höherer Lebensdauer.

Pulsare werden mit der Buchstabenkombination "PSR" und ihren Himmelskoordinaten bezeichnet, z.B. "PSR B0525+21". Die Zahlenkombination "0525" gibt dabei die Rektaszension im Äquatorialsystem an ("05 Stunden und 25 Minuten"), "+21" die Deklination in Grad. Der Buchstabe "B" besagt, dass sich die Koordinaten auf die Besselsche Epoche "B1950.0" beziehen. Heute benutzt man die Julianische Epoche "J2000.0", gekennzeichnet durch den Buchstaben "J". Da die Himmelskoordinaten je nach Standardepoche unterschiedlich sind, hat derselbe Pulsar unterschiedliche Bezeichnungen. Für den oben genannten Pulsar ergibt sich "PSR J0528+2200 (Rektaszension 05 Stunden 28 Minuten, Deklination 22 Grad, 00 Minuten)". Beide Bezeichnungen sind weiterhin in Gebrauch.

Jocelyn Bell und ihr Doktorvater Antony Hewish entdeckten den ersten Pulsar bei der Suche nach Radioquellen am 28. November 1967 am Mullard Radio Astronomy Observatory bei Cambridge. Für diese Untersuchung wurden in einem breiten Feld sämtliche Quellen erfasst, die binnen kurzer Zeit starke Schwankungen in ihrer Strahlungsintensität aufwiesen. Die Signale des später als PSR B1919+21 bezeichneten Pulsars zeichneten sich durch ungewöhnliche Regelmäßigkeit der abgestrahlten Wellen aus, so dass Bell und Hewish sie zunächst für ein künstliches Signal – eventuell einer extraterrestrischen Zivilisation – hielten ("Little Green Man 1"). Antony Hewish wurde 1974 für die Entdeckung der Pulsare mit dem Nobelpreis für Physik ausgezeichnet.

Der erste Physiker, der gleich nach ihrer Entdeckung hinter Pulsaren rotierende Neutronensterne vermutete, war Thomas Gold 1968/69. Eine Fachkonferenz lehnte jedoch zunächst seinen entsprechenden Vortrag als zu absurd ab und erachtete dies noch nicht einmal als diskussionswürdig. Später wurde seine Meinung aber bestätigt.

Russell Hulse und Joseph H. Taylor Jr. entdeckten 1974 den Pulsar PSR 1913+16, ein System aus zwei einander in weniger als 8 Stunden umkreisenden Neutronensternen, von denen einer ein Pulsar ist. Ihre Bahnperiode verkürzt sich ständig in einer Weise, die nur durch die Abstrahlung von Gravitationswellen gemäß der allgemeinen Relativitätstheorie erklärt werden kann. Hulse und Taylor erhielten dafür 1993 ebenfalls den Nobelpreis für Physik. Bis zum Mai 2006 waren ungefähr 1700 Pulsare bekannt, darunter auch ein Doppelpulsarsystem (das 2003 entdeckte System PSR J0737-3039).

PSR B0531+21 im Krebsnebel ist mit einem Alter von etwa 900 Jahren der jüngste bekannte Pulsar.

Ein in der Entstehung besonderer Pulsar ist der sich auf einer stark elliptischen Umlaufbahn um einen sonnengroßen Stern bewegende PSR J1903+0327, welcher mit 465 Umdrehungen pro Sekunde rotiert.

1982 wurde der erste Millisekundenpulsar mit der Bezeichnung PSR 1937+21 entdeckt. Die Stabilität seiner Rotationsdauer von 1,5578 Millisekunden beträgt mindestens 3·10 und übertrifft die Ganggenauigkeit von Atomuhren. Diese Genauigkeit kann für eine präzise Ortsbestimmung der Erde verwendet werden, um dadurch einen weiteren Nachweis für Gravitationswellen zu erbringen.

Nach einer Supernova eines massereichen Sterns bleibt in einem heißen, ionisierten Gasnebel ein Neutronenstern zurück. Der Neutronenstern besteht aus einem Teil der Materie des ursprünglichen Sterns (1,44 bis 3 Sonnenmassen) auf kleinstem Raum (Durchmesser um 20 Kilometer). Darüber hinaus behält der gesamte Supernova-Überrest aus Neutronenstern und Gasnebel seinen Drehimpuls bei, und das Magnetfeld des ursprünglichen Sterns wird im Neutronenstern komprimiert. Des Weiteren gibt es elektrische Potentialdifferenzen in der Größenordnung von 10 Volt.

Ein Pulsar bezieht seine Strahlungsenergie

Durch die Erhaltung des Drehimpulses und die starke Verkleinerung der räumlichen Ausdehnung beschleunigt sich die Rotation des Neutronensterns so sehr, dass die Rotationsdauer statt mehrerer Tage nur noch Sekunden oder Sekundenbruchteile beträgt. Die Folge ist ein sehr kompakter Himmelskörper mit einem starken Magnetfeld (typische Flussdichten von 10 Tesla), der sich innerhalb des ionisierten Gasnebels schnell dreht.

Pulsare sind wie alle Neutronensterne unterhalb einer festen Kruste suprafluid sowie supraleitend und haben eine Dichte im Bereich der von Atomkernen, d.h. rund 2·10 kg/m = 2·10 g/cm.

Die Magnetfeldrichtung des Neutronensterns schließt mit der Drehachse einen bestimmten Winkel ein. Wenn die Magnetfeldrichtung von der Drehachse abweicht, bewegen sich die Magnetfeldlinien schnell durch den ionisierten Gasnebel. Da elektrisch geladene Teilchen sich nur längs der Feldlinien frei bewegen können, werden sie von dem rotierenden Magnetfeld mitgenommen und strahlen dabei elektromagnetische Wellen ab. Infolge der Rotation streichen die elektromagnetischen Wellen wie das Licht eines Leuchtturms über die Umgebung. Nur wenn die Erde innerhalb des Doppelkegels liegt, der von der Richtung der elektromagnetischen Strahlung überstrichen wird, kann die gepulste Strahlung beobachtet werden.

Ein Pulsar strahlt die elektromagnetischen Wellen über einen weiten Wellenbereich ab, die vorwiegenden Anteile können im Frequenzbereich von Radiowellen (Radiopulsar), sichtbarem Licht oder im Bereich der Röntgenstrahlung (Röntgenpulsar) liegen. Jüngere Pulsare neigen eher dazu, höherenergetische Strahlung abzugeben.

Unter vereinfachten Annahmen lassen sich die Rotationsgeschwindigkeit und Rotationsenergie eines Pulsars abschätzen. Der Ausgangskörper sei sonnenähnlich und habe eine konstante Dichte, genauso wie der kontrahierte Neutronenstern.

Ausgangsgrößen:
Endgrößen:

Das Trägheitsmoment Θ (Θ = 2/5 · M · R²) verringert sich quadratisch, wenn der Radius "R" sich verkleinert, bei konstanter Masse "M". Da der Drehimpuls "L" ("L" = Θ · ω) erhalten bleibt, muss sich die Umdrehungsgeschwindigkeit ω um das Verhältnis der Trägheitsmomente von Sonne und Neutronenstern vergrößern. Um den gleichen Faktor erhöht sich die Rotationsenergie "E" ("E"=1/2 · ω · "L").

Daraus ergeben sich folgende Werte:

In der einfachen Abschätzung würde die Umlaufgeschwindigkeit am Äquator der Oberfläche ein Mehrfaches der Lichtgeschwindigkeit betragen. Da dies unmöglich ist, kann ein Stern nur kontrahieren, wenn er Masse abstößt und seinen Drehimpuls verringert. Die Rotationsenergie liegt im Bereich um 10 J.

Pulsare mit einer Rotationsdauer unterhalb von 20 Millisekunden werden Millisekundenpulsare genannt. Der Rekordhalter ist mit einer Rotationsfrequenz von 716 Hertz (1,4 Millisekunden Rotationsdauer) PSR J1748-2446ad im Kugelsternhaufen Terzan 5. Millisekundenpulsare unterscheiden sich von normalen Pulsaren neben der schnelleren Rotation auch durch ihr schwaches Magnetfeld von weniger als 10 Tesla, ihre langsame Rotationsabnahme, ihr hohes charakteristisches Alter sowie mit 75 Prozent ihr bevorzugtes Vorkommen in Doppelsternsystemen im Vergleich zu anderen Pulsaren mit weniger als 1 Prozent. Die maximale Rotationsfrequenz für Neutronensterne dürfte bei ca. 1500 Hertz liegen, da bei höheren Rotationsgeschwindigkeiten eine starke Abstrahlung von Gravitationswellen einsetzt.

Für das Entstehen von Millisekundenpulsare sind zwei Szenarien bekannt:

Durch intensive Radio-Beobachtungen von Kugelsternhaufen ist in den letzten Jahren eine große Anzahl Millisekundenpulsare gefunden worden. Die große Häufigkeit wird mit der hohen Sterndichte in diesen Sternaggregaten in Verbindung gebracht, wobei Neutronensterne sich einen Begleiter einfangen können und von diesem Materie akkretieren. In dieser Phase als Röntgendoppelstern geringer Masse (LMXB) wird die Rotation des Neutronensterns auf die für Millisekundenpulsare typische Werte beschleunigt. Überraschenderweise sind in den Kugelsternhaufen neben einer großen Anzahl an Millisekundenpulsaren auch normale junge Pulsare mit einer Rotationsdauer von einigen hundert Millisekunden und Magnetfeldern um die 10 Tesla entdeckt worden. Dies war unerwartet, da in den alten Kugelsternhaufen keine massereichen Sterne mehr existieren, die über eine Supernova zu der Geburt eines normalen Pulsars führen können. Eine Hypothese ist, dass die Kugelsternhaufen diese Pulsare gravitativ eingefangen und gebunden haben. Pulsare weisen meist eine hohe Eigenbewegung auf, die durch asymmetrische Supernovaexplosionen oder durch die Zerstörung eines Doppelsternsystems in der Supernovaphase verursacht wurde. Die Idee des Einfangs eines Begleiters und dem nachfolgenden Recycling des Pulsars durch die Akkretion der Materie des Begleiters wird durch die teilweise beobachtete große Bahnexzentrizität von Pulsaren in Kugelsternhaufen bestätigt. Die Bahnen in engen Doppelsternsystemen sollten nach wenigen 10 Millionen Jahren aufgrund von Gezeiteneffekten zirkularisiert sein und daher müssen diese Pulsare vor kurzer Zeit wiederbelebt worden sein.

Im Gegensatz zu den normalen Pulsaren zeigen die Millisekundenpulsare eine sehr geringe Fluktuation der Pulsankunftszeiten, da diese schnell rotierenden Neutronensterne keine Instabilitäten durch eine differentielle Rotation zeigen. Daher sind die Millisekundenpulsare gute Kandidaten, um über den Lichtlaufzeiteffekt nach Begleitern zu suchen, die über eine Ortänderung aufgrund der Keplerschen Gesetze zu einer Variation der Pulsankunftszeiten führen. Dadurch sind Neutronensterne, Weiße Zwerge, Braune Zwerge, Exoplaneten und eventuell Asteroidengürtel um Millisekundenpulsare entdeckt worden. Exoplaneten und Asteroidengürtel dürften sich aus den Akkretionsscheiben gebildet haben, welche die Millisekundenpulsare wieder beschleunigt haben.

Junge Pulsare zeigen im Mittel eine Eigenbewegung von typischerweise um die 400 km/s mit Spitzenwerten von mehr als 1000 km/s. Diese Geschwindigkeiten sind zu hoch, um als ein Ergebnis eines Aufbrechens eines Doppelsterns während einer Supernovaexplosion interpretiert zu werden. Für die hohen Eigenbewegungen sind die folgenden Hypothesen aufgestellt worden, die alle auf eine Asymmetrie in der Supernova zurückgeführt werden:

Pulsare zeigen neben einer kontinuierlichen Zunahme der Rotationsdauer auch Periodensprünge (engl. glitch), bei denen sich die Rotation des Neutronensterns innerhalb eines sehr kurzen Zeitraums beschleunigt. Anschließend erhöht sich die Rotationsdauer schneller als zuvor, bis der Ursprungswert vor dem "Sprung" erreicht ist. Die diskontinuierliche Veränderung der Rotationsdauer tritt außer bei Millisekundenpulsaren und jungen Neutronensternen mit einem Alter von weniger als 500 Jahren bei fast allen Pulsaren auf. Die Periodensprünge werden als eine Übertragung von Drehimpuls von dem superflüssigen Inneren des Neutronensterns auf die langsamer rotierende Kruste interpretiert. Dieses Modell kann allerdings nur schwer Anti-Glitches erklären, bei denen sich die Rotationsperiode der Neutronensterne sprunghaft verkürzt. Die Periodensprünge sind auch bei ungewöhnlichen Röntgenpulsaren nachgewiesen. Die Sprungaktivität, die kumulierte Periodenänderung pro Jahr, nimmt kontinuierlich mit dem Alter der Pulsare ab. Sie bietet eine Möglichkeit, das Innere des Neutronensterns zu studieren.

Als Nulling wird das temporäre komplette Verschwinden von Pulsen bei einigen Pulsaren bezeichnet. Innerhalb eines Zeitraums von zwei Pulsen kann der Übergang von einem normalen Puls zu dem Auszustand erfolgen und ebenso schnell kann das Einschalten geschehen. Die meisten von Nulling betroffenen Pulsare nehmen eine Auszeit von 5 Prozent, wobei diese zufällig verteilt erscheinen. Der Rekordhalter dürfte J1502−5653 sein, bei dem in 93 Prozent der Beobachtungszeit kein Puls nachweisbar ist. Die Ursache des Nullings sowie des schnellen Umschaltens zwischen den beiden Zuständen ist Gegenstand wissenschaftlicher Diskussionen. Während einer Aus-Phase nimmt die Verlangsamung der Rotationsdauer des Pulsars ab. Daher dürfte der Emissionsmechanismus wirklich ausgeschaltet und das Nulling daher nicht die Folge einer Abstrahlung in eine andere Raumrichtung sein.

Eine extreme Form des Nullings könnten die Rotating radio transients darstellen. Bei diesen Pulsaren sind bei Rotationsdauern von 0,4 bis 7 Sekunden nur noch einzelne Pulse im Abstand von 10 bis 10 Sekunden nachweisbar. Es handelt sich dabei um Pulsare, da einzelne Neutronensterne zwischen den beiden Formen Pulsar und hin und her wechseln. Die geringe Entdeckungswahrscheinlichkeit von lässt vermuten, dass es in der Milchstraße fünf- bis sechsmal mehr Neutronensterne als bisher vermutet gibt. Daher müssten auch die Kernkollapssupernovae entsprechend häufiger vorkommen oder alternative Entstehungskanäle existieren.

Im Bereich der Radiowellen zeigen Pulse eine Variation ihrer Intensität um den Faktor 10. Eine geringe Anzahl von Pulsaren, darunter der Pulsar im Krebsnebel, zeigen Abweichungen in der Intensität einzelner Pulse, die den Faktor 10 um mehrere Größenordnungen überschreiten. Das Phänomen der Riesenpulse scheint nur bei sehr jungen und daher schnell rotierenden Pulsaren aufzutreten. Im Vergleich zur Radiostrahlung bleibt die Intensität der Gamma- und Röntgenstrahlung während der Riesenpulse unverändert. Es wird vermutet, dass die Riesenpulse die gleiche Ursache haben wie das Nulling.





</doc>
<doc id="13274" url="https://de.wikipedia.org/wiki?curid=13274" title="Rotverschiebung">
Rotverschiebung

Die Rotverschiebung ist in der Astronomie die Lageveränderung identifizierter Spektrallinien im Emissionsspektrum stellarer Objekte in Richtung der größeren Wellenlängen. Die Rotverschiebung wird angegeben als Verhältnis der Wellenlängenänderung zur ursprünglichen Wellenlänge:

Der Name bezieht sich auf das rote Licht am langwelligen Ende des sichtbaren Spektrums. Bei Infrarot-Emission verschieben sich die Spektrallinien entsprechend in die Richtung der noch längerwelligen Terahertzstrahlung.

Festgestellt wird die Rotverschiebung durch den Vergleich bekannter Atom- und Molekülspektren mit den mittels Spektroskopie gemessenen Werten, d. h. nach Analyse der Spektrallinien der Emissionen oder Absorptionen im Sternenlicht, meistens des Wasserstoffs.

Von Bedeutung ist der Effekt auch in der Molekülspektroskopie, wo nach elastischer Streuung mit Energieübertragung Photonen niedrigerer Energie auftreten.

Ursachen der Rotverschiebung können sein:
Die ersten drei dieser Ursachen werden im Folgenden näher erläutert.

Emittiert ein Objekt elektromagnetische Strahlung und wird sie von einem zweiten, zu diesem sich relativ entfernenden Objekt absorbiert, so vergrößert sich die im Moment der Absorption gemessene Wellenlänge gegenüber der emittierten, steigend mit der Rezessionsgeschwindigkeit der beiden Objekte. Dieser Relativistische Dopplereffekt folgt aus der Konstanz der Lichtgeschwindigkeit formula_2: elektromagnetische Strahlung bewegt sich sowohl bei der Emission als auch bei der Absorption mit formula_2, gleichgültig wie schnell sich Quelle und Ziel relativ zueinander bewegen.

Bei einer Bewegung auf einer Linie (ohne Tangentialkomponente) ist der Zusammenhang:

formula_5: Rotverschiebung

formula_2: Lichtgeschwindigkeit

formula_7: Rezessionsgeschwindigkeit (häufig auch als "Fluchtgeschwindigkeit" bezeichnet)

Die gravitative Rotverschiebung oder Gravitations-Rotverschiebung im Rahmen der Allgemeinen Relativitätstheorie ist eine Wellenlängenvergrößerung für abgestrahltes Licht, also für Licht, das sich von einem Gravitationszentrum entfernt. Bei der gravitativen Blauverschiebung oder Gravitations-Blauverschiebung handelt es sich um den umgekehrten Effekt einer Wellenlängenverkürzung für einfallendes Licht, also für Licht, dass sich auf ein Gravitationszentrum zubewegt.

Die "gravitative Rotverschiebung" ist eine direkte Folge der gravitativen Zeitdilatation. Sie ist streng genommen kein Effekt der allgemeinen Relativitätstheorie, sondern folgt bereits aus der speziellen Relativitätstheorie und dem Äquivalenzprinzip der allgemeinen Relativitätstheorie. Licht, das von einer Lichtquelle mit einer gegebenen Frequenz nach oben (also vom Gravitationszentrum weg) ausgestrahlt wird, wird dort mit einer geringeren Frequenz gemessen. Das bedeutet also insbesondere, dass bei einem Lichtsignal mit einer bestimmten Anzahl von Schwingungen der zeitliche Abstand zwischen dem Beginn und dem Ende des Signals beim Empfänger größer ist als beim Sender. Dies wird durch die gravitative Zeitdilatation verständlich.

Aufgrund der gravitativen Zeitdilatation ist das Zeitintervall zwischen Anfang und Ende der Lichtwelle umso länger, je weiter nach oben man sich im Gravitationsfeld bewegt, weil die Zeit zunehmend schneller verstreicht. Das bedeutet, dass die Welle bei ihrer Bewegung nach oben immer länger gemessen wird. Daher muss auch der Abstand zwischen den einzelnen Wellenbergen immer mehr wachsen, so dass das Licht also immer langwelliger, also energieärmer erscheint.

Die gravitative Rotverschiebung wurde von Einstein bereits 1911 vor Fertigstellung der allgemeinen Relativitätstheorie vorausgesagt und kann bereits aus der Energieerhaltung hergeleitet werden, so dass ihre experimentelle Bestätigung zwar notwendige Voraussetzung für die Gültigkeit der allgemeinen Relativitätstheorie ist, aber andererseits nicht sehr große Aussagekraft hat. Von W. S. Adams wurde 1925 die Rotverschiebung am Weißen Zwerg Sirius B nachgewiesen. Die Messung der gravitativen Rotverschiebung an weißen Zwergen ist aber schwierig von der Rotverschiebung durch die Eigenbewegung zu unterscheiden, und die Genauigkeit ist begrenzt. Robert Pound und Glen Rebka wiesen 1960 mit Hilfe des Mößbauer-Effektes die gravitative Rotverschiebung der Strahlung einer Gammaquelle im Erdgravitationsfeld bei einem Höhenunterschied von nur 25 m mit ausreichender Genauigkeit nach (Pound-Rebka-Experiment). Spätere Verbesserungen (Pound-Rebka-Snider-Experiment) erreichten eine Genauigkeit von etwa 1,5 %. Die gravitative Rotverschiebung wurde mittels Raumsonden auch für die Sonne und den Saturn nachgewiesen. Der geplante Satellit OPTIS soll, neben anderen Tests zur speziellen und allgemeinen Relativitätstheorie, die gravitative Rotverschiebung mit einer Genauigkeit von 10 testen.

Die Entwicklung von Atomuhren hat es möglich gemacht, den Einfluss der Gravitation auf die Zeit auch direkt zu messen. Im Prinzip ist diese Messung eine Variation der Nachweise der gravitativen Rotverschiebung. 1971 wurde durch J. Hafele und R. Keating (Hafele-Keating-Experiment) mit Caesiumuhren in Flugzeugen der durch die Gravitation verursachte Gangunterschied von Uhren in verschiedenen Höhen gemäß der allgemeinen Relativitätstheorie mit etwa 10 % Genauigkeit eindeutig nachgewiesen. Durch ein ähnliches Experiment von C. Alley (Maryland-Experiment) konnte die Genauigkeit 1976 auf 1 % gesteigert werden. R. Vessot und M. Levine publizierten 1979 Ergebnisse eines ähnlichen Experiments mit Hilfe von Raketen und gaben eine Genauigkeit von 0,02 % an. Beim heutigen satellitengestützten GPS-Navigationssystem müssen Korrekturen sowohl gemäß der speziellen als auch der allgemeinen Relativitätstheorie berücksichtigt werden, wobei Effekte durch die allgemeine Relativitätstheorie überwiegen. Umgekehrt kann dies auch als Bestätigung dieser Theorien angesehen werden.

Ein Beobachter, der sich relativ zum Schwerpunkt einer nichtrotierenden Masse auf der radialen Koordinate formula_8 befindet, erhält ein Signal, das von einem sich auf formula_9 befindlichen Beobachter gesendet wird, um den Faktor

rot- bzw. blauverschoben. Die formula_11-Koordinate ist in Schwarzschild-Koordinaten gegeben, mit dem Schwarzschildradius formula_12.

Die Expansion des Universums darf nicht so verstanden werden, dass sich Galaxien in der Raumzeit voneinander entfernen (Relativbewegung). Es ist der Raum selbst, der sich ausdehnt, die Galaxien werden "mitbewegt." Gravitativ gebundene Objekte wie Galaxien oder Galaxienhaufen expandieren nicht, denn sie sind durch ihre Eigengravitation von der allgemeinen Expansionsbewegung (beschrieben durch die Friedmann-Gleichungen) entkoppelt. Dies gilt insbesondere auch für Objekte, die sich innerhalb solcher gravitativ gebundener Systeme befinden (Sterne, Planeten), und auch für elektromagnetisch gebundene Systeme wie Atome und Moleküle. Einer elektromagnetischen Welle hingegen, die sich frei durch eine sich ausdehnende Raumzeit ausbreitet, wird die Expansionsbewegung direkt aufgeprägt: vergrößert sich die Raumzeit während der Laufzeit um einen Faktor formula_13, so geschieht dies auch mit der Wellenlänge des Lichtes.

Diese "kosmologische Rotverschiebung" ist grundsätzlich von der Rotverschiebung durch den Dopplereffekt zu unterscheiden, die nur von der relativen Geschwindigkeit der Galaxien bei der Emission und der Absorption abhängt. Die aus der kosmologischen Rotverschiebung abgeleiteten Fluchtgeschwindigkeiten ferner Galaxien sind demnach direkt auf die Ausdehnung der Raumzeit zurückzuführen. Bereits ab Entfernungen von wenigen 100 Megaparsec ist der Anteil des Dopplereffekts verschwindend gering. Ferner ergibt sich aus der allgemeinen Relativitätstheorie, dass die beobachteten Fluchtgeschwindigkeiten keine relativistischen Zeiteffekte hervorrufen, wie sie von der speziellen Relativitätstheorie für Bewegungen im Raum beschrieben werden. Eine "kosmologische Zeitdilatation" findet dennoch statt, da die später ausgesandten Photonen eines Objektes aufgrund der Expansion eine größere Wegstrecke zurücklegen müssen. Physikalische Prozesse erscheinen daher bei rotverschobenen Objekten (aus unserer Sicht) zunehmend verlangsamt abzulaufen.

Das Licht von Galaxien ist in den allermeisten Fällen rotverschoben (bereits unter den nächstgelegenen 1000 sind es etwa 75 Prozent). Je weiter eine Galaxie entfernt ist, desto stärker ist im Mittel die Rotverschiebung. Nur wenige relativ nahe Galaxien zeigen aufgrund zusätzlicher „eigener“ Bewegung relativ zur Erde "auf uns zu" insgesamt eine "Blauverschiebung." Ein Beispiel dazu ist der Andromedanebel.

Vesto Slipher führte ab 1912 spektroskopische Beobachtungen von Galaxien durch und bestimmte deren Radialgeschwindigkeiten aus den Linienverschiebungen. Er erkannte bald, dass die meisten der von ihm beobachteten Galaxien eine Rotverschiebung aufwiesen. 1929 entdeckte Edwin Hubble den Zusammenhang von Rotverschiebung und Entfernung der Galaxien und führte ihn auf eine kosmologische Expansion zurück. Zunächst wurde der Effekt fälschlich als Dopplereffekt interpretiert. Er nimmt mit der Galaxienentfernung gemäß der Hubble-Konstante zu, weshalb man die Entfernungen durch Messung der Rotverschiebung abschätzen kann.

Je höher die Rotverschiebung eines astronomischen Objekts, desto länger war das von ihm ausgesandte Licht unterwegs und desto weiter zurück in der Vergangenheit sehen wir es. Aus der Rotverschiebung kann auch die Entfernung des Objekts bestimmt werden, allerdings ist diese in einer sich ausdehnenden Raumzeit nicht mehr eindeutig definiert. Es gibt verschiedene Entfernungsmaße, die sich aus der Rotverschiebung ableiten lassen. In der Kosmologie werden Betrachtungen und Rechnungen deshalb immer im Rotverschiebungsraum angestellt.

Im Oktober 2010 haben Astronomen mit Hilfe des Very Large Telescope nachweisen können, dass das Licht der zuvor mit dem Hubble-Weltraumteleskop entdeckten Galaxie "UDFy-38135539" 13,1 Milliarden Jahre zu uns unterwegs war. Mit dem damaligen Rotverschiebungsrekord von formula_14 erreichte uns erstmals beobachtetes Licht, das nur 700 Millionen Jahre nach dem Urknall ausgesandt wurde; die Galaxie entstand damit in einer Zeit, in der das Universum noch nicht vollständig transparent und um den Faktor 9,6 kleiner war.

Mit der Entdeckung der Galaxie "UDFj-39546284" in der Hubble Ultra Deep Field 09 Aufnahme (HUDF09) konnte eine kosmologische Rotverschiebung von formula_15 ermittelt werden. Der beobachtete Altersrekord verschiebt sich damit weitere 120 Millionen Jahre Richtung Urknall auf 580 Millionen Jahre danach. Die neu entdeckte Galaxie mit ihrem Alter von 13,2 Milliarden Jahren würde bei einer Bestätigung der Rotverschiebung einen wichtigen Beobachtungsbaustein zur Entwicklung der ersten Galaxien nach dem Urknall liefern.

Der Sachs-Wolfe-Effekt erklärt Fluktuationen der Rotverschiebung der Photonen der kosmischen Hintergrundstrahlung.

Die Rotverschiebung eines Photons entspricht einer Dehnung seiner Wellenlänge, die mit einer Energieabnahme (gemäß "E" = Energie des Photons, "h" = Plancksches Wirkungsquantum, "c" = Lichtgeschwindigkeit, "λ" = Wellenlänge) einhergeht. Wenn die Rotverschiebung mit einer Dehnung der Raumzeit erklärt wird, stellt sich die Frage, wo die Energie bleibt. Die genaue Analyse des Sachverhalts anhand der Trajektorien des aussendenden Objektes (Galaxie) und des empfangenden, irdischen Beobachters in der Raumzeit zeigt: Der Betrag der Rotverschiebung, den der irdische Beobachter an der Galaxie feststellt, ist identisch mit der zu erwartenden Dopplerverschiebung anhand der allgemein relativistisch berechneten Relativgeschwindigkeit. Die Relativitätstheorie erlaubt den Standpunkt, dass die Photonen keine Energie verlieren, sondern nur auf Grund der Relativgeschwindigkeit rotverschoben erscheinen.

Man betrachte ein Photon, emittiert von einer Galaxie mit mitbewegter Entfernung formula_16 (siehe auch die relativistische Herleitung der Friedmann-Gleichungen), und absorbiert vom Beobachter bei formula_17. Sowohl die Galaxie als auch der Beobachter folgen der kosmischen Expansion. Orientiert man das beschreibende Koordinatensystem so, dass das Photon entlang dessen polarer Achse läuft, dann lautet das Linienelement des Photons

wobei formula_2 die Lichtgeschwindigkeit darstellt, formula_20 den Expansionsfaktor, und formula_16 die mitbewegte Radialkoordinate. Zwei aufeinanderfolgende Maxima der Lichtwelle werden zu den kosmologischen Zeiten
formula_22 und formula_23 ausgesandt, und zu den Zeiten formula_24 und formula_25 wieder absorbiert. Die Wellenlängen des Photons zu Zeiten der Emission und Absorption sind dann

Die mitbewegte Entfernung, die von beiden Maxima zurückgelegt wird, ist per Definition gleich groß. Integriert man das Linienelement des Photons, so erhält man

Durch Vertauschen der Integrationsgrenzen ergibt sich dann für infinitesimal kleine Intervalle zwischen Emission (Absorption) der beiden Maxima

Unter Verwendung der emittierten und absorbierten Wellenlängen wie sie oben angegeben sind, kann man deren Verhältnis ableiten,

Schließlich definiert man dann die "kosmologische Rotverschiebung" zu

Da für die meisten Zwecke der Absorptionszeitpunkt formula_32
mit der heutigen Zeit formula_33 zusammenfällt und formula_34, ergibt sich vereinfacht

Umgekehrt ergibt sich hieraus unmittelbar der Skalenfaktor des Universums zum Emissionszeitpunkt im Vergleich zum heutigen Wert,

Beobachtet man beispielsweise eine Galaxie mit Rotverschiebung formula_37, so hatte das Universum zum Zeitpunkt der Aussendung des von uns empfangenen Lichts nur ein Viertel seiner Größe.
Sämtliche physikalischen Prozesse in dieser Galaxie laufen aus der Sicht des Beobachters um einen Faktor formula_38 verlangsamt ab, da sich der Abstand zweier nacheinander emittierter Photonen entsprechend vergrößert, und damit auch deren Eintreffen beim Beobachter "(kosmologische Zeitdilatation)." Ein bekanntes Beispiel hierfür ist die zunehmende Streckung der Lichtkurven von Supernovae vom Typ Ia, deren Zustandekommen gut verstanden ist, mit wachsender Rotverschiebung.

In der Astronomie wird die Rotverschiebung durch Methoden der Spektralanalyse gemessen; sie sind heute durch digitale statt fotografische Erfassung wesentlich genauer geworden. Doch um Spektrallinien gut erfassen zu können, müssen die Galaxien eine gewisse Mindest-Helligkeit aufweisen. Rotverschiebungen von Galaxien werden im Rahmen von Durchmusterungen wie dem Sloan Digital Sky Survey regelmäßig neu bestimmt.

Die Gravitative Rotverschiebung konnte mit Hilfe des Mößbauereffekts (Mößbauerspektroskopie) in Laborexperimenten auf der Erde beobachtet werden.



</doc>
<doc id="13276" url="https://de.wikipedia.org/wiki?curid=13276" title="Borland">
Borland

Borland ist ein Softwareunternehmen (NASDAQ NM: BORL) mit Hauptsitz in Austin, Texas, USA. Seit 2009 ist es ein Tochterunternehmen von Micro Focus.

Borland wurde 1983 von dem in die USA eingewanderten französischen Mathematiklehrer Philippe Kahn gegründet. Auf der Suche nach einem amerikanisch klingenden Namen für sein Unternehmen ließ er sich vom Namen eines US-Astronauten, Frank Borman, inspirieren. Unter Kahns Leitung schuf das Unternehmen eine Reihe von Werkzeugen zur Softwareentwicklung. Borland wurde in den 1980er Jahren durch eine integrierte Entwicklungsumgebung für die Programmiersprache Pascal, die unter dem Namen Turbo Pascal unter den Betriebssystemen CP/M und MS-DOS herausgebracht wurde, bekannt. Die letzte für DOS verfügbare Version 7.0 trug den Namen Borland Pascal. Unter Windows wurde dieses Produkt zu Borland Delphi weiterentwickelt. Borland war ferner mit SideKick erfolgreich, einem der ersten Personal Information Manager, und entwickelte weitere „Turbo“-IDEs, u. a. für die Programmiersprachen BASIC und Prolog. Auf lange Sicht konnten sich aber nur Delphi und Borlands C++-Implementierung durchsetzen, die Anfang der 1990er Jahre gegenüber Microsofts Entwicklungstools als überlegen galten.

Mit seiner selbst entwickelten Datenbank Paradox stand Borland Anfang der 1990er Jahre in direkter Konkurrenz zu Microsofts Access, das damals gerade neu auf den Markt kam. Um seine Stellung im Datenbankmarkt zu stärken, kaufte Borland im September 1991 das wesentlich größere Unternehmen Ashton-Tate, Hersteller des seinerzeit marktführenden PC-Datenbanksystems dBASE und des Programms Framework, und forcierte die Modernisierung von dBase. Auch das erst kürzlich durch Ashton-Tate erworbene Datenbanksystem InterBase wurde weiterentwickelt.

Seit Mitte der 1990er Jahre verlor Borland mehr und mehr seine dominante Stellung am Markt für Softwaretools. Einige meinen, daran sei die Konkurrenz zu Microsoft schuld gewesen, andere glauben, dass Philippe Kahn die Ressourcen seines Unternehmens in zu vielen Projekten verzettelte, als er versuchte, an vielen Fronten gleichzeitig gegen Microsoft anzukämpfen.

1998 benannte sich Borland in Inprise Corporation um und konzentrierte sich – weg von Entwicklertools – stärker auf den Markt zur Entwicklung von Businessapplikationen bzw. Middleware-Werkzeugen und stieg in den Markt internetorientierter Tools wie JBuilder ein. Über mehrere Jahre wurden Verluste eingefahren, das Image verschlechterte sich. Durch die Namensänderung kam sogar der Eindruck auf, das Unternehmen existiere gar nicht mehr. 1999 wurden die Rechte an dBASE an die dataBased Intelligence Inc. verkauft und später folgte die Veräußerung von InterBase an das Unternehmen Embarcadero Technologies.

Später, wieder unter dem traditionsreichen Namen Borland und unter Führung von CEO Scott Arnold, wurde die Firma zwar kleiner, aber dafür wieder profitabel. Borland entwickelte zunächst weiterhin die Entwicklungsumgebungen Delphi und C++Builder. Ein Vorstoß in Bereiche jenseits der Entwicklung auf Microsoft-Betriebssystemen war Kylix, das Borlands Erfahrung im Bereich der Integrierten Entwicklungsumgebungen zum ersten Mal auch „nativ“ für Linux verfügbar machte. Dieses Produkt blieb aber erfolglos. Außerdem wurde der C# Builder vorgestellt, eine Entwicklungsumgebung für die von Microsoft entwickelte .NET-Programmiersprache C#. Die Unterstützung von Web Services und .NET (seit Delphi 8) hat das Ansehen von Borland in der Industrie wieder gestärkt.

Mit den 2005er-Ausgaben ihrer Werkzeuge brach Borland erstmals mit der einfachen Durchnummerierung der Versionen: JBuilder 11 (der Nachfolger von JBuilder 10) hieß nun JBuilder 2005, Delphi 9 hieß offiziell Delphi 2005. In der Delphi-2005-IDE waren erstmals die Programmiersprachen Delphi und C# zusammengefasst (auf Druck der Entwicklergemeinde konnten Programme in Delphi sowohl für Win32 als auch für .NET erstellt werden). Demnach wurde der C# Builder offiziell eingestellt und war fortan in Delphi enthalten. Als Teil des Borland Developer Studios (BDS) enthielt Delphi 2006 auch die Unterstützung für die Programmiersprache C++.

Die erfolgreiche Integration der Unternehmen Togethersoft (Together Control Center) und Starbase (Konfigurations- und Changemanagenmentlösungen, Anforderungsmanagementlösungen) innerhalb von drei Jahren war für Borland ein wichtiger Schritt.

Im Jahre 2005 kam das CMMI- und Prozessberatungshaus TeraQuest hinzu, welches als eines der erfolgreichsten global agierenden CMMI-Beratungshäuser galt. Bill Curtis, Mitbegründer von CMM, wurde bei Borland in führender Position tätig. 2006 wurde schließlich Segue Software akquiriert, um das Portfolio durch Software zum automatisierten und manuellen Testen zu ergänzen.

Mittlerweile bietet Borland ein hoch integriertes ALM-Lösungsportfolio (Application Lifecycle Management) an, das den gesamten Zyklus der Softwareentwicklung von der Aufnahme und dem Management der Anforderungen, über die Modellierung der Anwendung und der Ausprogrammierung über das automatische Testen bis hin zur Entwicklung der Applikation „aus einem Guss“ ermöglicht.

Borland konzentriert sich durchgängig auf eine prozessorientierte, CMMI- und ITIL-Elemente umfassende SDO-Strategie (Software Delivery Optimization), welche Wertschöpfungsaspekte und Geschäftsprozesse der Softwareentwicklung im Vordergrund jeglichen IT-Schaffens sieht.

Im Februar 2006 kündigte Borland an, die Produktlinie der Entwicklungsumgebungen (Delphi, JBuilder etc.) zu verkaufen und sich fortan ausschließlich auf den Bereich des ALM zu konzentrieren. Die IDE-Sparte wurde zwar in ein eigenes Unternehmen namens CodeGear ausgegliedert, verblieb jedoch zunächst im Besitz von Borland.

Am 5. September 2006 hat Borland die „Turbo“-Reihe wieder auferstehen lassen – „Turbo Delphi Win32“, „Turbo Delphi. net“, „Turbo C#“ und „Turbo C++“ sollen mit den kostenlos erhältlichen „Explorer“-Versionen wieder mehr Programmierer für die IDE-Produkte von Borland werben.

Am 7. Mai 2008 wurde bekanntgegeben, dass die Tochterfirma CodeGear an Embarcadero Technologies verkauft wurde.

Am 6. Mai 2009 wurde bekanntgegeben, dass das Unternehmen von Micro Focus International PLC für 75 Millionen US-Dollar übernommen wird.

Die aktuelle Produktpalette von Borland umfasst unter anderem:


Ehemalige Entwicklungen von Borland sind:



</doc>
<doc id="13279" url="https://de.wikipedia.org/wiki?curid=13279" title="Ide">
Ide

Ide oder Ida ist in der griechischen Mythologie der Name von:
Ide oder Idé ist der Familienname folgender Personen:
Ide ist der Name folgender Orte:
IDE steht als Abkürzung für:
Siehe auch:


</doc>
<doc id="13280" url="https://de.wikipedia.org/wiki?curid=13280" title="Obduktion">
Obduktion

Eine Obduktion (, von "obducere": ‚bedecken‘, nachträglich ‚hinzuziehen‘ bzw. ‚vorführen‘) ist eine innere Leichenschau (Leichenöffnung) zur Feststellung der Todesursache und zur Rekonstruktion des Sterbevorgangs. Diese Art der Leichenschau wird von Pathologen und Rechtsmedizinern (Forensikern) durchgeführt, wobei ihnen Sektionsassistenten (in Österreich: "Prosekturgehilfen") assistieren.

Andere, heute synonym gebrauchte Bezeichnungen sind "Autopsie" ( [f.] ‚eigene Schau‘) und "gerichtliche Sektion" ( [f.] ‚Schnitt, Operation‘) bzw. Sectio legalis (‚gesetzlich angeordnete Sektion‘). Die zugehörigen Verben lauten obduzieren, autopsieren und sezieren.

Der Begriff Nekropsie (, von "nekrós" ,tot‘ und "ópsis" ,Blick, Anschauung‘) wird in der Regel für die Sektion von Tieren verwendet.

Der Begriff "Obduktion" wird in der Regel für klinische (pathologische) Sektionen und gerichtsmedizinische Sektionen verwendet.

Klinische Obduktionen werden fast ausschließlich von einem Pathologen durchgeführt. Hierbei werden die Todesursache und die Vorerkrankungen einer verstorbenen Person durch innere ärztliche Leichenschau festgestellt. Meist stellt der zuletzt behandelnde Arzt der verstorbenen Person den Antrag auf Obduktion. Voraussetzungen einer klinischen Sektion sind, dass der Verstorbene oder die nächsten Angehörigen damit einverstanden sind und der Tod durch eine natürliche Ursache (z. B. Herzinfarkt, Krebs, Lungenentzündung) eingetreten ist. Sie dient nicht nur der Qualitätssicherung in der Medizin, sondern kann auch für Angehörige entlastend sein (z. B. Selbstvorwürfe, man habe Symptome nicht rechtzeitig bemerkt). Weiterhin kann eine Obduktion gelegentlich Hinweise auf familiäre Risikofaktoren geben (z. B. Krebs oder Erbkrankheiten). Sie erfolgt manchmal bei Versicherungsfragen (z. B. Berufserkrankungen, die zum Tod beigetragen haben können, sowie verschwiegene Vorerkrankungen). Nicht zuletzt dient sie der Fort- und Weiterbildung von Ärzten und Medizinalfachberufen im Krankenhaus.

Gerichtsmedizinische Obduktionen können staatsanwaltschaftlich bzw. gerichtlich angeordnet werden, wenn eine nicht-natürliche Todesursache, also ein Tötungsdelikt, Suizid oder Unfalltod vermutet wird oder feststeht und eine weitere Klärung notwendig erscheint. Ist die Todesart auf dem Totenschein als „ungeklärt“ beurkundet, erfolgt in der Regel ebenfalls eine gerichtlich angeordnete Sektion. Die rechtliche Grundlage sowohl für die strafprozessuale Obduktion, als auch für die Exhumierung (Ausgrabung) ist in Deutschland der (ff) der Strafprozessordnung (§ 159 i. V. m. §§ 87 ff. StPO). Diese Sektionen müssen von zwei Ärzten durchgeführt werden, wovon einer in der Praxis ein Facharzt für Rechtsmedizin ist.

Pathologische und rechtsmedizinische Sektionen sind im Verfahren einander sehr ähnlich. Bei pathologischen Obduktionen wird jedoch auf eine toxikologische Untersuchung verzichtet, da diese im Allgemeinen schon vor dem Tode durchgeführt wurde. Die Bestimmung der Körpertemperatur des Toten, die auf den Todeszeitpunkt rückschließen lässt, entfällt ebenso.

Eine Obduktion dauert je nach Todesursache und Komplexität meist zwei bis drei Stunden, längstens vier Stunden.

Eine Obduktion in der Rechtsmedizin kostet in Deutschland etwa 950 Euro. Eine gerichtlich angeordnete Leichenschau wird mit einer Sachverständigenvergütung von 850 Euro entlohnt. Die Kosten einer Sektion werden meist von der öffentlichen Hand getragen, da der Versichertenstatus in der gesetzlichen Krankenversicherung mit dem Tod erloschen ist. Eine ausschließlich von Angehörigen gewünschte Sektion müssen diese bezahlen.

In Deutschland werden etwa ein bis zwei Prozent der Toten obduziert. Es gibt Hinweise, die nahelegen, dass ein nicht unerheblicher Teil der "klinisch vermuteten" Todesursachen falsch sind. So kam eine im Archiv für Kriminologie 1997 veröffentlichte Studie zu dem Schluss, dass in Deutschland rund 11.000 nicht natürliche Todesfälle und 1.200 Tötungsdelikte pro Jahr nicht erkannt würden.

Anatomische Sektionen dienen der Ausbildung von Medizinern; Anatomen und Studierende führen sie gemeinsam durch. Die zu sezierenden Verstorbenen haben ihren Leichnam zu Lebzeiten freiwillig für diesen Zweck zur Verfügung gestellt, sofern es sich nicht um Tierkadaver für Studienzwecke handelt. Der Begriff "Obduktion" wird in diesem Zusammenhang normalerweise nicht verwendet, stattdessen spricht man vom "Sezieren" (früher auch "Dissektion") oder der "Präparation" der Leichen.

Eine anatomische Sektion ist sehr viel detaillierter. Sie beschränkt sich nicht nur auf die drei Körperhöhlen, auch kleinere Details gelangen zur Darstellung, da im Rahmen des Präparierkurses alle anatomischen Strukturen des Körpers erlernt werden sollen. Deshalb erstreckt sich eine anatomische Präparation auf ein ganzes Semester. Daraus ergibt sich die Notwendigkeit einer Einbalsamierung des Leichnams. Eine Wiederherstellung des Leichnams ist anschließend nicht mehr möglich. Die Leiche wird am Ende einzeln und vollständig bestattet. Dies erfolgt je nach letztem Willen anonym oder namentlich, durch Verbrennung oder Erdbestattung.

Eine Obduktion beginnt mit einer genauen Inspektion der Leiche. Größe, Gewicht, Ernährungszustand und Hautkolorit werden festgehalten. Lokalisation und Farbe der Totenflecke sowie Grad der Ausprägung der Totenstarre werden dokumentiert. Hautveränderungen wie Narben, Wunden, Operationswunden, Pigmentflecken, Tätowierungen und dergleichen werden ebenso beschrieben. Speziell bei rechtsmedizinischen Obduktionen wird großer Wert auf eine präzise äußere Beschreibung gelegt, die neben etwaigen Verletzungen (wie z. B. Schuss- oder Stichwunden) auch die Bekleidung und andere Gegenstände (beispielsweise: Schmuck, Armbanduhr usw.) umfasst. Die Untersuchung von Kleidung, Effekten, Körpergröße und Zahnstatus ist insbesondere für die Identifizierung von unbekannten Toten von Bedeutung. Zudem können durch die äußere Besichtigung Rückschlüsse auf äußere Einwirkungen etc. gezogen werden.

Die innere Leichenbeschau gliedert sich in eine Öffnung der Schädel-, Brust- und Bauchhöhle. Bei der gerichtlich angeordneten Obduktion müssen gemäß § 89 StPO alle drei Körperhöhlen (Schädelhöhle, Brusthöhle und Bauchhöhle) des Verstorbenen geöffnet und so die Organe freigelegt werden. Die Organe werden nach Größe, Form, Farbe, Konsistenz und Kohärenz beurteilt, wobei von der Norm abweichende Veränderungen im deskriptiven Teil des Obduktionsberichtes festgehalten werden. Morphologisch sichtbare Organveränderungen haben eine Entsprechung in pathologisch-anatomischen Diagnosen, die ihrerseits bestimmten klinischen Krankheitsbildern entsprechen. Von wichtigen Organen werden kleine Proben für weitergehende lichtmikroskopische und eventuell auch mikrobiologische Untersuchungen asserviert. Für rechtsmedizinische Gutachten wird auch noch Blut und Urin des Verstorbenen zum Zweck toxikologischer Untersuchungen gewonnen.

Dieser Schnitt hat entweder die "Y"-Form, hierbei wird von beiden Schlüsselbeinen schräg zum Brustbein geschnitten und von dort gerade bis zum Schambein. Alternativ wird leicht bogenförmig von Schulter zu Schulter quer und dann in einem zweiten Schnitt zentral abwärts bis zum Schambein geschnitten, der "T"-Schnitt. Durch diese Schnittführungen kann der Pathologe oder Rechtsmediziner an alle Organe des Brust- und Bauchraumes gelangen (nach Entfernung des Brustbeines und der angrenzenden Rippen).

Im Anschluss an die innere Besichtigung werden die Organe wieder in die Körperhöhlen des Toten gelegt, wobei entnommene Organe oder Organteile unter Umständen für klinische bzw. wissenschaftliche Zwecke bearbeitet und archiviert werden können. Fehlen dadurch größere Gewebemengen werden diese z. B. durch Zellstoff ersetzt, um die äußere Form des Körpers annähernd wieder herzustellen. Die Hautschnitte werden grob vernäht und der Leichnam gewaschen. Dadurch wird eine Abschiednahme am offenen Sarg ermöglicht.

Später werden die gewonnenen Proben mikroskopisch und mikrobiologisch untersucht. Bei einer gerichtlichen Sektion werden Drogen und eventuell Giftstoffe und Medikamentenspiegel toxikologisch bestimmt. Vereinzelt kommen auch Spezialuntersuchungen wie zum Beispiel DNA-Analysen, entomologische (insektenkundliche) und radiologische Verfahren zum Einsatz.
Die verschiedenen Berichte gehen "nur" dem Auftraggeber der Sektion zu.

In jüngster Zeit finden zunehmend neue Messverfahren Anwendung bei der Leichenschau (Streifenlichttopometrie, CT).

Der Obduktionsbericht besteht aus einem deskriptiven Teil, der keinen Spielraum für Interpretationen zulässt und mit einer Bildbeschreibung wesensverwandt ist. Demzufolge ist der deskriptive Teil eine objektive Beschreibung der Organsysteme, die im Idealfall so genau sein soll, dass ein Kundiger im Nachhinein alle pathologisch-anatomischen Diagnosen aus dem Bericht herauslesen und gegebenenfalls revidieren kann. Dieser Deskription wird noch eine Liste der Todesursachen und der pathologisch-anatomischen Einzeldiagnosen beigelegt.

Bereits im 12. und 13. Jahrhundert wurden in Europa vereinzelt Leichenöffnungen zur Feststellung der Todesursache durchgeführt. 1286 fand ein Arzt in Cremona bei der Sektion eines während einer Epidemie verstorbenen Menschen die gleiche krankhafte Veränderung am Herzen wie bei untersuchten Hühnern. Die ersten, die menschliche Anatomie betreffenden belegbaren Lehrsektionen fanden um 1300 in Bologna statt, 1391 durch Mondino dei Luzzi, in Padua 1341, in Montpellier 1366 und in Lérida 1391. Auch der für Karl VI. von Frankreich tätige, aus Portugal stammende und als Professor in Montpellier und Arzt in Bordeaux wirkende Chirurg Valescus de Taranta führte um 1400 selbst Sektionen durch, deren Erkenntnisse er in einem Lehrbuch publizierte.

Rechtlich gesehen ist eine Sektion in Deutschland weder eine Körperverletzung (diese kann nur an lebenden Menschen begangen werden) noch eine Sachbeschädigung, da Tote strafrechtlich keine bloßen Sachen sind bzw. nach anderer Auffassung niemandes Eigentum, der eine Beschädigung geltend machen könnte. Sie wird aber als Störung der Totenruhe gem. § 168 StGB geahndet, wenn sie unautorisiert stattfindet. Gerichtsmedizinische Sektionen müssen von der Staatsanwaltschaft oder gerichtlich angeordnet werden; in Österreich (derzeit noch) vom Untersuchungsrichter auf Antrag der Staatsanwaltschaft. Eine klinische Obduktion, die gegen den Willen des Verstorbenen bzw. der Hinterbliebenen stattfindet, erfüllt nicht in jedem Fall den Tatbestand der Störung der Totenruhe. Der Tatbestand der Störung der Totenruhe erfordert einen Gewahrsamsbruch, der im Falle eines Todes im Krankenhaus nämlich nicht vorliegt, da das Krankenhaus zunächst den Gewahrsam am Körper des Verstorbenen hat. Wird sodann eine Sektion von der Krankenhausleitung angeordnet, liegt keine Störung der Totenruhe vor.

Die Sektion ist in Deutschland bisher nur in den Bundesländern Berlin und Hamburg in eigenen Sektionsgesetzen geregelt. Die Bestattungsgesetze einiger weiterer Bundesländer enthalten auch Regelungen zur Sektion, z. B. in Brandenburg und Nordrhein-Westfalen.

Unter dem Eindruck des gewaltsamen Todes des zweijährigen Kevin, der von seinem Stiefvater totgeprügelt und in einem Kühlschrank aufbewahrt wurde, beschloss der Bremer Senat 2009 eine verdachtsunabhängige Pflicht zur Obduktion beim ungeklärten Tod von Kindern unter sechs Jahren. Mit der 2011 in Kraft getretenen Regelung sollen mögliche Misshandlungen aufgeklärt werden. Das Gesetz wurde de facto nicht umgesetzt – die Obduktionsrate von Kindern blieb unverändert. Kritiker bemängeln, dass die Regelung auch eindeutig nicht gewaltsame Tode erfasst und beispielsweise bei krankheitsbedingt verstorbenen Kindern eine zusätzliche Belastung für die Eltern darstelle.

Sofern ein Verdacht auf eine seuchenhygienisch relevante Fragestellung (beispielsweise: offene TBC) besteht, kann vom Gesundheitsamt eine sanitätspolizeiliche Sektion angeordnet werden.

Die Kirchen in Deutschland werten Obduktionen als einen Akt der Nächstenliebe, da diese dem Gemeinwohl dienen.





</doc>
<doc id="13281" url="https://de.wikipedia.org/wiki?curid=13281" title="Autopsie">
Autopsie

Autopsie (griechisch αuτοψία, von αuτός „selbst“ und oψις „der Blick, das Sehen“) bezeichnet:

Siehe auch:


</doc>
<doc id="13282" url="https://de.wikipedia.org/wiki?curid=13282" title="Sektion">
Sektion

Sektion (lat. "sectio" „das Schneiden, Zerteilen“) steht für:


Siehe auch:


</doc>
<doc id="13284" url="https://de.wikipedia.org/wiki?curid=13284" title="Ernst Weiß (Schriftsteller)">
Ernst Weiß (Schriftsteller)

Ernst Weiß (* 28. August 1882 in Brünn; † 15. Juni 1940 in Paris) war ein österreichischer Arzt und Schriftsteller.

Der aus einer jüdischen Familie stammende Weiß war der Sohn des Tuchhändlers Gustav Weiß und dessen Ehefrau Berta Weinberg. Am 24. November 1886 starb der Vater. Trotz finanzieller Probleme und mehrfacher Schulwechsel (unter anderem besuchte er Gymnasien in Leitmeritz und Arnau) bestand Weiß 1902 erfolgreich die Matura (Abitur). Anschließend begann er in Prag und Wien Medizin zu studieren. Dieses Studium beendete er 1908 mit der Promotion in Brünn und arbeitete danach als Chirurg in Bern bei Emil Theodor Kocher und in Berlin bei August Bier.

1911 kehrte Weiß nach Wien zurück und fand eine Anstellung im Wiedner Spital. Aus dieser Zeit stammt auch sein Briefwechsel mit Martin Buber. Nach einer Erkrankung an Lungentuberkulose hatte er in den Jahren 1912 und 1913 eine Anstellung als Schiffsarzt beim österreichischen Lloyd und kam mit dem Dampfer "Austria" nach Indien, Japan und in die Karibik.

Im Juni 1913 machte Weiß die Bekanntschaft von Franz Kafka. Dieser bestätigte ihn in seiner schriftstellerischen Tätigkeit, und Weiß debütierte noch im selben Jahr mit seinem Roman "Die Galeere".

1914 wurde Weiß zum Militär einberufen und nahm im Ersten Weltkrieg als Regimentsarzt in Ungarn und Wolhynien teil. Nach Kriegsende ließ er sich als Arzt in Prag nieder und wirkte dort in den Jahren 1919 und 1920 im Allgemeinen Krankenhaus.
Nach einem kurzen Aufenthalt in München ließ sich Weiß Anfang 1921 in Berlin nieder. Dort arbeitete er als freier Schriftsteller, u. a. als Mitarbeiter beim "Berliner Börsen-Courier". In den Jahren 1926 bis 1931 lebte und wirkte Weiß in Berlin-Schöneberg. Am Haus Luitpoldstraße 34 erinnert daran eine Gedenktafel. Im selben Haus wohnte zeitweise der Schriftsteller Ödön von Horváth, mit dem Weiß eng befreundet war.

1928 wurde Weiß vom Land Oberösterreich mit dem Adalbert-Stifter-Preis ausgezeichnet. Außerdem gewann er im selben Jahr bei den Olympischen Spielen in Amsterdam eine Silbermedaille im Kunst-Wettbewerb.

Kurz nach dem Reichstagsbrand am 27. Februar 1933 verließ er Berlin für immer und kehrte nach Prag zurück. Dort pflegte er seine Mutter bis zu deren Tod im Januar 1934. Vier Wochen später emigrierte Weiß nach Paris. Da er dort als Arzt keine Arbeitserlaubnis bekam, begann er für verschiedene Emigrantenzeitschriften zu schreiben, u. a. für "Die Sammlung", "Das Neue Tage-Buch" und "Maß und Wert". Da er mit diesen Arbeiten seinen Lebensunterhalt nicht decken konnte, unterstützten ihn die Schriftsteller Thomas Mann und Stefan Zweig.

Ernst Weiß letzter Roman "Der Augenzeuge" wurde 1939 geschrieben. In Form einer fiktiven ärztlichen Autobiographie wird von der „Heilung“ des hysterischen Kriegsblinden A.H. nach der militärischen Niederlage in einem Reichswehrlazarett Ende 1918 berichtet. Nach der Machtergreifung der Nazis 1933 wird der Arzt, weil Augenzeuge, in ein KZ verbracht: Sein Wissen um die Krankheit des A.H. könnte den Nazis gefährlich werden. Um den Preis der Dokumentenübergabe wird „der Augenzeuge“ freigelassen und aus Deutschland ausgewiesen. Nun will er nicht mehr nur Augenzeuge sein, sondern praktisch-organisiert kämpfen und entschließt sich, auf der Seite der Republikaner für die Befreiung Spaniens und gegen den mit Nazideutschland politisch verbündeten Franquismus zu kämpfen.

Als Weiß am 14. Juni 1940 den Einmarsch der deutschen Truppen in Paris von seinem Hotel aus miterleben musste, beging er Suizid, indem er sich in der Badewanne seines Hotelzimmers die Pulsadern aufschnitt, nachdem er Gift genommen hatte. Im Alter von 57 Jahren starb Ernst Weiß am 15. Juni 1940 im nahegelegenen Krankenhaus.

Seine Selbsttötung wird literarisch im Roman "Transit" von Anna Seghers verarbeitet. Seit seinem Tod ist ein großer Koffer mit unveröffentlichten Manuskripten verschwunden. Die Lage seines Grabes ist ungeklärt.






</doc>
<doc id="13286" url="https://de.wikipedia.org/wiki?curid=13286" title="Nelly Sachs">
Nelly Sachs

Nelly Sachs (eigentlich "Leonie Sachs"; * 10. Dezember 1891 in Berlin-Schöneberg; † 12. Mai 1970 in Stockholm) war eine jüdische deutsch-schwedische Schriftstellerin und Lyrikerin. 1966 verlieh das Nobelpreiskomitee ihr – gemeinsam mit Samuel Joseph Agnon – den Nobelpreis für Literatur „für ihre hervorragenden lyrischen und dramatischen Werke, die das Schicksal Israels mit ergreifender Stärke interpretieren“.

Nelly Sachs wurde 1891 im damals noch selbständigen Schöneberg als einziges Kind in die Familie des Ingenieurs, Erfinders und Gummi- und Guttaperchawaren-Fabrikanten Georg William Sachs (1858–1930) und seiner Frau Margarete, geborene Karger (1871–1950), geboren; sie wuchs in einer assimilierten jüdisch-großbürgerlichen Atmosphäre auf. In ihrer frühen Jugendzeit hatte sie den großen Wunsch, Tänzerin zu werden. Einige Jahre später begann dann ihre Leidenschaft für die deutsche Lyrik und das Schreiben von Gedichten. Aufgrund ihrer kränklichen Konstitution wurde sie zunächst drei Jahre von Privatlehrern unterrichtet, bevor sie 1903 in eine Höhere Töchterschule eintrat, wo sie fünf Jahre später ihr Einjähriges, das der Mittleren Reife entspricht, absolvierte.

Mit 15 Jahren war sie so fasziniert von Selma Lagerlöfs Debütroman "Gösta Berling", dass sie mit der schwedischen Schriftstellerin in einen Briefwechsel eintrat, der über 35 Jahre andauerte.

Erste Gedichte schrieb Nelly Sachs mit 17 Jahren. 1921 erschien mit Unterstützung des Schriftstellers Stefan Zweig ihr erster Gedichtband unter dem Titel "Legenden und Erzählungen". Die frühen, melancholisch gefärbten Gedichte sind noch ganz von neoromantischen Einflüssen geprägt und kreisen um Motive aus Natur und Musik. Bei der Herausgabe ihrer gesammelten Werke nahm Nelly Sachs diese Gedichte später jedoch nicht mit auf.

Sie lebte mit ihren Eltern zurückgezogen und nahm wenig am gesellschaftlichen Leben der 1920er Jahre teil. Gegen Ende des Jahrzehnts wurden ihre Gedichte in verschiedenen Berliner Zeitungen gedruckt, darunter in der "Vossischen Zeitung", dem "Berliner Tageblatt" und der Zeitschrift "Die Jugend". Kritik und Publikum erkannten ihre Lyrik gleichermaßen an. Frühere Gedichte mit eher experimentellem Charakter und einem die traditionellen Wege verlassenden, schwerer verständlichen Stil hatte die Dichterin hingegen wieder vernichtet.

Nach jahrelanger Krebserkrankung starb ihr Vater 1930, woraufhin Nelly Sachs mit ihrer Mutter in ein Mietshaus in der Lessingstraße im Berliner Hansaviertel umzog, das noch in mütterlichem Besitz war. Der Tod des geliebten Vaters war ein einschneidender Verlust für die Tochter, den sie bis an ihr Lebensende nie verwunden hat. Ende der 30er-Jahre hatten alle Mitglieder der Familie, solange dies noch möglich war, bis auf ihre Cousine Vera Sachs, Berlin verlassen. Freunde und Bekannte wurden drangsaliert, viele waren aus dem Land gedrängt worden, so dass auch ein geistiges Leben (beispielsweise im Kulturbund Deutscher Juden) zunehmend unmöglich geworden war. So lebten Mutter und Tochter möglichst unauffällig und zurückgezogen. Es war, wie sie selbst aus der Rückschau konstatierte, ein „Leben unter Bedrohung“, und sie verspürte in sich als „höchsten Wunsch auf Erden: Sterben ohne gemordet zu werden.“

Nelly Sachs blieb unverheiratet, nachdem eine Liebesbeziehung zu einem geschiedenen Mann vom Vater unterbunden worden war. Allerdings hielt sie die Beziehung zu dem namentlich unbekannten Mann vermutlich über Jahrzehnte aufrecht. Dieser Mann wurde auf Grund seiner Affinität zum Widerstand und wegen seiner Liaison mit einer Jüdin verhaftet und gefoltert. Wiederholt wurde Sachs zu Gestapo-Verhören einbestellt. Das unmittelbare Miterleben seines Martyriums war für Nelly Sachs, die ihn „zu Tode getroffen zusammenbrechen“ sah, traumatisch. In späteren Gedichten ist mehrfach von einem „Bräutigam“ die Rede, der in einem Konzentrationslager umgekommen sei. Einzelheiten zur Person und zum Tod des geliebten Mannes wurden von der Dichterin jedoch nie öffentlich preisgegeben.

In dieser Zeit begann sie auch, sich mit ihrer jüdischen Herkunft auseinanderzusetzen. Aufgewachsen in einer liberal-jüdischen Familie, suchte sie in den Jahren äußerer Bedrohung und seelischer Not den Zugang zur ursprünglichen Religion ihrer Herkunftsfamilie. Die Buber-Rosenzweig-Übersetzung des „Jesaia“ (1929 ersch.) eröffnete ihr bis dahin unbekannte Dimensionen der Bibel. Sie findet in diesem Text ihre ganze „Hoffnung auf das geistige Israel und dessen spirituellen Auftrag“ wieder. Von einer Freundin erhielt sie Martin Bubers "Legenden des Baal-schem" und fand darin seelische Hilfe. In späteren Jahren verband sie in ihrem Denken dieses jüdische Gedankengut mit der Ideenwelt auch nicht-jüdischer Mystiker.

Erst spät entschloss sich Nelly Sachs, mit ihrer Mutter aus Deutschland zu fliehen. Ihre Freundin Gudrun Harlan, eine Nichte des Schriftstellers und Dramaturgen Walter Harlan, reiste im Sommer 1939 nach Schweden, um Hilfe von Selma Lagerlöf für ein schwedisches Visum zu erbitten. Diese jedoch konnte ihr wegen ihres Gesundheitszustandes nicht mehr helfen; sie starb, bevor Sachs in Schweden eintraf. Harlan wandte sich an den „Malerprinzen“ Eugen, einen Bruder des schwedischen Königs, der sie schließlich unterstützte. Nach monatelangen bürokratischen Hemmnissen konnten Nelly Sachs und ihre Mutter im Mai 1940 buchstäblich im letzten Moment – der Befehl für den Abtransport in ein Lager war bereits eingetroffen – mit einem Flugzeug Deutschland Richtung Stockholm verlassen.

In Schweden lebten die beiden Frauen unter ärmlichen Verhältnissen in einer Einzimmerwohnung im Süden Stockholms. Nelly Sachs kümmerte sich um ihre alte Mutter und arbeitete zeitweise als Wäscherin, um zum Lebensunterhalt beizutragen. Die schwedische Staatsbürgerschaft erhielt Nelly Sachs allerdings erst im Jahr 1953. Sie begann Schwedisch zu lernen und moderne schwedische Lyrik ins Deutsche zu übersetzen. Mit dieser Übersetzungsarbeit erreichte ihre eigene sprachliche Ausdruckskraft völlig neue Qualitäten und entwickelte sich weg vom früheren romantischen Stil. An der von ihr in die deutsche Sprache übertragene Lyrik von Edith Södergran, Karin Boye, Johannes Edfelt, Hjalmar Gullberg, Anders Österling und Pär Lagerkvist reiften ihre Gedichte und erreichten jenes hohe Niveau, das Nelly Sachs bis heute ihre bedingungslose künstlerische Anerkennung garantiert: „Diese starke, an der unerbittlichen nordischen Natur geformte Sprache und diese unbedingte Wahrhaftigkeit der Aussage, die keine Gefühligkeit und Stimmung erlaubt, dafür aber Dissonanzen zulässt, bewirken bei ihr eine Katharsis.“ Umgeben von Menschen, die Schwedisch sprachen und mit denen sie nur in deren Muttersprache kommunizieren konnte, wurde sie, wie Hans Magnus Enzensberger es formulierte, auf die deutsche Sprache „als einzige Heimat zurückverwiesen“. Die Gedichte von 1943/1944, die später in der Sammlung "In den Wohnungen des Todes" erscheinen sollten, enthalten Bilder von Schmerz und Tod, sind eine einzige Todesklage für ihr gequältes Volk. Neben den Gedichten entstanden in den 1940er Jahren die zwei Dramen "Eli" und "Abram im Salz".

In der Nachkriegszeit schrieb Nelly Sachs weiterhin mit einer hochemotionalen, herben, aber dennoch zarten Sprache über das Grauen des Holocaust. Ihr Biograf Walter A. Berendsohn nannte die Gedichte 1946 „klagend, anklagend und verklärend“. Nelly Sachs ist „die erste Schriftstellerin, welche die Schornsteine von Auschwitz zum Thema ihrer Verse machte“:

Die beiden Bände "In den Wohnungen des Todes" und "Sternverdunkelung" (1949) wurden zunächst in Ost-Berlin auf Betreiben Johannes R. Bechers veröffentlicht; weder in der Schweiz noch in den westlichen Zonen Deutschlands wurden Gedichte von Nelly Sachs gedruckt. Auch 1949 noch wurde der zweite Gedichtband "Sternverdunkelung", in Amsterdam verlegt, von der Kritik zwar gelobt, in der jungen Bundesrepublik jedoch kaum gelesen. In der DDR-Zeitschrift "Sinn und Form" erschienen einige ihrer Texte. Die finanzielle Misere für Sachs und ihre Mutter dauerte an, so dass sie weiterhin mit Übersetzungen ein Auskommen suchte.

Anfang 1950 starb die Mutter von Nelly Sachs, was sie psychisch schwer traf. In den 1950er Jahren begann sie eine Korrespondenz mit Paul Celan, den sie 1960 auch in Paris besuchte. Mit ihm fühlte sie sich in Art einer „Schicksals- und Seelenverwandtschaft“ verbunden, zu der Peter Hamm urteilte: „Dichtung war für beide ein über den Abgrund der Vergangenheit gespanntes Rettungsseil aus nichts als Worten“. Nach Ansicht von Jacques Schuster gibt es im deutschsprachigen Raum überhaupt nur zwei Schriftsteller, „die es vermochten, das jüdische Schicksal in Worte zu fassen: Paul Celan und Nelly Sachs.“

Gegen Ende des Jahrzehnts, nach Jahren der Isolation, wurde sie mit ihrem Werk schließlich auch im gesamten deutschsprachigen Raum zur Kenntnis genommen. "Und niemand weiß weiter" und "Flucht und Verwandlung", Gedichtbände mit Einflüssen des französischen Surrealismus, erschienen 1957 und 1959 in Hamburg, München und Stuttgart. Das Mysterienspiel "Eli" wurde 1959 als Hörspiel beim Südwestfunk ausgestrahlt. Nelly Sachs wurde von der jungen Literaturwelt der Bundesrepublik „entdeckt“.

Aus Deutschland erfolgte eine erste Anerkennung als Lyrikerin, die Ehrengabe des Kulturkreises der deutschen Wirtschaft im Bundesverband der Deutschen Industrie, diese wurde ihr 1959 noch in Abwesenheit verliehen. 1957 nahm die Deutsche Akademie für Sprache und Dichtung in Darmstadt, 1960 die Freie Akademie der Künste in Hamburg Nelly Sachs als Mitglied auf. Nelly Sachs wollte nicht zurück nach Deutschland, zu groß war immer noch ihre Angst. Auch zeigten sich Anzeichen einer psychischen Krankheit, und nachdem sie 1960 zur Verleihung des Meersburger "Droste-Preises" das erste Mal seit zwanzig Jahren Deutschland betreten hatte, brach sie nach ihrer Rückkehr nach Schweden zusammen. Insgesamt verbrachte sie drei Jahre in einer Nervenheilanstalt bei Stockholm.

Die Stadt Dortmund stiftete 1961 den Nelly-Sachs-Preis und verlieh ihn der Namensgeberin.

Als erste Frau erhielt sie 1965 den Friedenspreis des Deutschen Buchhandels, was sie erneut zu einer Reise nach Deutschland veranlasste.

An ihrem 75. Geburtstag erhielt Nelly Sachs am 10. Dezember 1966 zusammen mit Samuel Joseph Agnon den Literaturnobelpreis aus der Hand des schwedischen Königs Gustavs VI. Adolf. Ihre kurze Dankesrede hielt sie auf Deutsch, dabei zitierte sie ein eigens für diese Zeremonie geschriebenes Gedicht, in dem es heißt:
Ihr Preisgeld verschenkte sie zur Hälfte an Bedürftige, die andere Hälfte ging an ihre alte Freundin Gudrun Harlan. Sie selbst zog sich in ihren letzten Jahren wieder von der Öffentlichkeit zurück. 1967 musste sie eine geplante Israel-Reise auf Anraten des Arztes absagen, setzte sich aber in einem öffentlichen Telegramm für den Empfang von Günter Grass durch den israelischen Schriftstellerverband in Jerusalem ein. Ihr psychisches Leiden führte zu einem weiteren Aufenthalt in der Nervenklinik.

Seit den frühen 1960er-Jahren finden sich Vertonungen ihrer Gedichte, vorzugsweise im deutschsprachigen Raum. Namhafte Komponisten fühlten sich von ihrer subtil-expressiven Sprache und von den Metaphern und Bildern der Texte zu ausdrucksstarker Musik angeregt und eröffneten mit ihren Liedern und Instrumentalklängen öffentlichkeitswirksame Präsentationsformen für die Lyrik. Aufgewachsen in einem musikliebenden Elternhaus, hatte Nelly Sachs seit ihrer Kindheit eine besondere Affinität zur Musik. Im Jahr der Nobelpreisverleihung hat sie selber davon gesprochen, dass sich das Wort „nach seiner Ausatmung in Mimus und Musik“ verlängere, dass die Ausatmung des Worts schon Musik sei. Ihr Drama "Abram" (1944–56) trägt den Untertitel "Ein Spiel für Wort – Mimus – Musik", und in einem ihrer späten Gedichte heißt es über das Lied, es sei "das gesegnete Wort entführend / vielleicht zurück zu seinem magnetischen Punkt / der Gottdurchlässig ist." Nicht zuletzt durch die Vertonungen ihrer Lyrik und szenischen Dichtungen wurde im Kulturleben die Auseinandersetzung mit der Thematik von Judenvernichtung und Holocaust in Gang gesetzt.

1969 unternahm Paul Kersten einen ersten Versuch, Nelly Sachs aus der Rolle einer „Leidens- und Wiedergutmachungsfigur“ zu befreien, in welche die meisten der seiner Meinung nach „deutungsbesessenen Nelly-Sachs-Interpreten“ die Dichterin gezwängt hatten, und korrigierte damit das Bild, welches sich in der Öffentlichkeit bis dahin herausgebildet hatte. In einer umfangreichen „vom Wortmaterial ausgehenden Analyse“ untersuchte er die semantischen Strukturen der Gedichte und vermochte die Metaphorik der Texte zu entschlüsseln. Nicht zuletzt durch diesen Forschungsbeitrag wurde die Qualität des künstlerischen Schaffens von Nelly Sachs offenkundig.

Nelly Sachs starb im Mai 1970 in einem Stockholmer Krankenhaus an einer Krebserkrankung, am Tag von Paul Celans Beerdigung. Sie ist auf dem jüdischen Friedhof des Norra begravningsplatsen von Solna im Norden von Stockholm beigesetzt.









</doc>
<doc id="13287" url="https://de.wikipedia.org/wiki?curid=13287" title="Karōshi">
Karōshi

Als Karōshi (jap. , "Tod durch Überarbeiten") bezeichnet man in Japan einen plötzlichen berufsbezogenen Tod. Todesursache ist meist ein durch Stress ausgelöster Herzinfarkt oder Schlaganfall. Umstritten ist, ob Suizide, die auf arbeitsbedingte psychische Erkrankungen zurückzuführen sind () unter die Definition fallen. Etwa 40 japanische Kliniken haben sich auf Karōshi-gefährdete Fälle spezialisiert. Das Phänomen ist auch in Südkorea verbreitet und wird dort "kwarosa" (, ) genannt. In China wird der durch Überarbeitung herbeigeführte Tod als "guòláosǐ" () bezeichnet.

Der erste Fall von Karōshi wurde 1969 gemeldet, als ein 29-jähriger verheirateter Arbeiter in der Versandabteilung der größten japanischen Zeitung an einem Schlaganfall starb. Die Medien wurden jedoch erst Ende 1980 auf dieses Phänomen aufmerksam, nachdem mehrere geschäftsführende Manager im mittleren Alter ohne vorherige Anzeichen einer Erkrankung plötzlich gestorben waren. Dieses Phänomen wurde kurz darauf als "Karōshi" bezeichnet, und als 1987 die öffentliche Sorge darüber zunahm, begann das japanische Arbeitsministerium mit der Veröffentlichung von Karōshi-Statistiken.

Als Ursache für die Karōshi-Fälle gilt der rasante wirtschaftliche Aufstieg Japans nach dem Zweiten Weltkrieg. Mittlerweile ist anerkannt, dass Erwerbstätige nicht über Jahre hinweg sechs bis sieben Tage pro Woche mehr als zwölf Stunden täglich arbeiten können, ohne körperlich und geistig darunter zu leiden.

Aufgrund der mittlerweile erfolgten juristischen Anerkennung als haftungspflichtige Todesart verklagen immer mehr Angehörige von Karōshi-Opfern die jeweiligen Arbeitgeber auf Entschädigungszahlungen. Bevor jedoch eine Entschädigung zuerkannt werden kann, muss die Arbeitsüberwachungsbehörde den Fall als berufsbedingten Tod anerkennen.




</doc>
<doc id="13288" url="https://de.wikipedia.org/wiki?curid=13288" title="Kernspinresonanzspektroskopie">
Kernspinresonanzspektroskopie

Die Kernspinresonanzspektroskopie (NMR-Spektroskopie von ) ist eine spektroskopische Methode zur Untersuchung der elektronischen Umgebung einzelner Atome und der Wechselwirkungen mit den Nachbaratomen. Dies ermöglicht die Aufklärung der Struktur und der Dynamik von Molekülen sowie Konzentrationsbestimmungen.

Die Methode beruht auf der magnetischen Kernresonanz, einer resonanten Wechselwirkung zwischen dem magnetischen Moment von Atomkernen der Probe, die sich in einem starken statischen Magnetfeld befindet, mit einem hochfrequenten magnetischen Wechselfeld. Es sind nur solche Isotope der Spektroskopie zugänglich, die im Grundzustand einen von Null verschiedenen Kernspin und damit ein magnetisches Moment besitzen, zum Beispiel H; D; Li; B; B; C; N; O; P und Ca.

Zur Vorgeschichte der Kernspinresonanzspektroskopie siehe Geschichte und Entwicklung der Kernspinresonanz.

Felix Bloch und Edward Mills Purcell wiesen erstmals 1946 Signale der magnetischen Kernresonanz nach, wofür sie 1952 den Nobelpreis erhielten. In ihrem Nobelpreisvortrag zeigten sie erste Spektren mit dem Nachweis der chemischen Verschiebung (am Beispiel des Ethanol), womit die eigentliche Kernspinspektroskopie begann. Sie entwickelte sich zu einer wichtigen Methode in der chemischen Strukturaufklärung. Zunächst wurde hauptsächlich die Continuous-Wave-(CW)-Methode benutzt, bei der durch Variation der Frequenz oder des Feldes die Resonanzen nacheinander angeregt wurden. 1947 reichten Russell Varian und Felix Bloch ein Patent ein für das erste Kernspinresonanz-Spektrometer. Das erste kommerzielle Kernspinresonanz-Spektrometer baute 1952 die Firma Varian Associates in Palo Alto. Um 1955 baute die japanische Firma JEOL ebenfalls NMR-Spektrometer. Die US-amerikanische Biophysikerin Mildred Cohn setzte in den frühen 1960er Jahren die Kernspinresonanz-Spektroskopie zur Aufklärung metabolischer Prozesse auf molekularer Ebene ein.

Da die CW-Technik durch ein schlechtes Signal-Rausch-Verhältnis gekennzeichnet war, entwickelte ab Mitte der 1960er Jahre Richard R. Ernst (Nobelpreis für Chemie 1991) bei der Firma Varian ein Puls-Fourier-Transformation-NMR-Spektrometer (FT-NMR), das eine wesentlich schnellere Aufnahme der Spektren ermöglichte. Bei gleicher Messzeit bedeutete das im Vergleich zu den CW-Spektrometern eine wesentliche Steigerung der Empfindlichkeit und damit des Signal-Rausch-Verhältnisses. Bereits in den Jahren 1949 und 1950 waren von Hahn und Torrey die ersten Pulsverfahren untersucht worden. Die ersten kommerziellen Kernspinresonanz-Impulsspektrometer wurden Mitte der 1960er Jahre von der deutschen Firma Bruker (gegründet von Günther Laukien, einem der NMR-Pioniere in Deutschland) in Karlsruhe von einer Gruppe um Bertold Knüttel und Manfred Holz gebaut. Es folgte die Einführung von Breitbandentkopplung und von Mehrpulsverfahren. Nach einer Idee von Jean Jeener wurden ab Anfang der 1970er Jahre Mehrpulsexperimente mit einer systematisch variierten Wartezeit zwischen zwei Pulsen entwickelt, die nach Fourier-Transformation über zwei Zeitbereiche zu zweidimensionalen Spektren führten.

Kurt Wüthrich und andere bauten diese 2D- und Multi-Dimensions-NMR zu einer bedeutenden Analysetechnik der Biochemie aus, insbesondere zur Strukturanalyse von Biopolymeren wie Proteinen. Wüthrich bekam für diese Arbeiten 2002 den Nobelpreis in Chemie. Im Gegensatz zur Röntgenstrukturanalyse liefert die Kernspinresonanz-Spektroskopie Strukturen von Molekülen in Lösung. Von besonderer Bedeutung ist die Möglichkeit, detaillierte Informationen über die Moleküldynamik mit Hilfe von Relaxationsparametern zu gewinnen.

Teilchen und Atomkerne, die einen Kernspin formula_1 besitzen, haben als rotierende Ladungsträger ein magnetisches Moment, das oft mit formula_2 bezeichnet wird.
Das magnetische Moment von Atomkernen kann in einem äußeren Magnetfeld nicht jede beliebige, sondern nur bestimmte, durch die Quantenmechanik beschriebene Orientierungen einnehmen. Die Zahl der möglichen Orientierungen wird durch die Kernspinquantenzahl formula_3 bestimmt (siehe: Multiplizität). Zu jeder Kernspinquantenzahl formula_3 existieren formula_5 Orientierungen und jeder Orientierung ist eine magnetische Kernspinquantenzahl formula_6 zugeordnet.

Beispiele:


Ohne ein äußeres Magnetfeld sind die mit formula_6 gekennzeichneten Zustände energetisch gleich (siehe Entartung (Quantenmechanik)). In Anwesenheit eines äußeren Magnetfeldes entstehen Energiedifferenzen (Zeeman-Effekt). Kernresonanz-Phänomene beruhen auf der Anregung von Kernspin-Übergängen zwischen solchen formula_6-Zuständen. Die dazu benötigte Energie formula_16 ist proportional zur Stärke des äußeren Magnetfeldes formula_17 und zum gyromagnetischen Verhältnis formula_18 des betrachteten Atomkerns:

Diese Energie wird durch Einstrahlen von resonanten elektromagnetischen Wellen eingebracht. Die Resonanzfrequenz wird in der NMR als Larmor-Frequenz formula_20 bezeichnet und liegt im Radiowellen-Bereich. Gängige NMR-Spektrometer arbeiten bei Protonen-Resonanzfrequenzen zwischen 300 und 1000 MHz, was Feldstärken zwischen 7 und 24 Tesla erfordert.

Wenn alle H- oder C-Atome die exakt gleiche Larmor-Frequenz hätten, wäre die NMR-Methode zur Strukturaufklärung wenig interessant. Tatsächlich hängen aber die Resonanzfrequenzen von den individuellen, atomar aktiven Magnetfeldern ab. Diese lokalen Magnetfelder können in ihrer Stärke vom Hauptmagnetfeld abweichen, beispielsweise durch den Einfluss der elektronischen Umgebung eines Atomkerns oder durch magnetische Wechselwirkung zwischen benachbarten Atomkernen.
Aufgrund dieser Eigenschaften wird die Kernresonanzspektroskopie zur Strukturaufklärung von Molekülen eingesetzt.

Zur Messung wird die Probe in ein homogenes magnetisches Feld gebracht, das sogenannte Hauptmagnetfeld. Die Probe wird von einer Induktionsspule umgeben, welche ein hochfrequentes elektromagnetisches Wechselfeld senkrecht zum Hauptmagnetfeld erzeugt. Dann variiert man die Stärke des Hauptmagnetfeldes, bis der Resonanzfall eintritt (Continuous-Wave-Verfahren, veraltet). Alternativ kann die magnetische Feldstärke konstant gehalten und die Frequenz des eingestrahlten Wechselfeldes variiert werden (engl. , veraltet). Wenn der Resonanzfall eintritt, die Probe also Energie aus dem Wechselfeld aufnimmt, verändert sich die Stromstärke, welche zum Aufbau des Wechselfeldes benötigt wird. Dies ist messbar.

Moderne Messverfahren strahlen nicht mehr kontinuierliche Wechselfelder in die Probe ein, sondern Radiowellen-Pulse. Ein kurzer Radiowellenpuls regt dabei ein Frequenzband an, dessen Frequenzbreite über die Fourier-Beziehung umgekehrt proportional zur Pulsdauer ist. Dadurch werden in der Probe alle Übergänge, die in dieses Frequenzband fallen, gleichzeitig angeregt. Bei korrekter Wahl von Pulsdauer und Pulsleistung kann die Magnetisierung der angeregten Kernspins in die Transversalebene senkrecht zum Hauptmagnetfeld gebracht werden. Nach Beendigung des Pulses oszilliert diese Transversalmagnetisierung für kurze Zeit senkrecht zum Hauptmagnetfeld. Dabei oszilliert jeder Kernspin mit seiner individuellen Larmor-Frequenz. Diese Summe dieser Oszillationen wird als elektrischer Strom über elektromagnetische Induktion mit der gleichen Induktionsspule detektiert, die zum Senden des Anregungspulses gedient hat. Das empfangene Signal wird digitalisiert und aufgezeichnet. Mit Hilfe der schnellen Fourier-Transformation ist es möglich, die individuellen Larmor-Frequenzen aus der Summe der Oszillationen zu extrahieren, um ein NMR-Spektrum zu erhalten. Darum tragen moderne NMR-Verfahren den Namen PFT-NMR für Pulsed Fourier Transform NMR Spectroscopy. Für dieses Verfahren bleibt das Hauptmagnetfeld statisch, üblicherweise wird es mit Hilfe von supraleitenden Elektromagneten erzeugt, die mit flüssigem Helium und Stickstoff gekühlt werden.

Ein spezielles Verfahren ist die NMR-CIDNP-Spektroskopie, bei der die Probe zur Erzeugung von Radikalen mit Licht bestrahlt oder beheizt wird.

Bei der PFT-NMR existieren drei Relaxationsprozesse, die einerseits die Leistungsfähigkeit der PFT-NMR einschränken, aber andererseits einzigartige Informationen zur Molekül-Dynamik und Material-Inhomogenitäten liefern können.
Die formula_21-Relaxation ist der Prozess der Rückkehr der Kernspins vom angeregten Zustand zum thermischen Gleichgewicht unter Abgabe der bei der Anregung aufgenommenen Energie als Wärme. Wird zwischen zwei NMR-Experimenten ("scans") nicht die vollständige formula_21-Relaxation abgewartet, so steht für jedes weitere nur eine gewisse, geringere Magnetisierung und damit Signalintensität zur Verfügung. Falls eine kurze Repetitionzeit gewünscht ist, muss der Anregungswinkel (auf den sog. Ernst-Winkel) verkleinert werden, um trotzdem ein möglichst starkes Signal zu erhalten. Die formula_23-Relaxation ist die Dephasierung der Transversalmagnetisierung aufgrund entropischer Effekte, die mit der magnetischen Dipol-Dipol-Wechselwirkung benachbarter Atomkerne zusammenhängt. Hier wird keine Energie abgegeben, da die Spins im angeregten Zustand verbleiben, aber die Transversalmagnetisierung läuft von einem Vektor zunächst zu einem Fächer und zuletzt zu einer Kreisfläche auseinander, so dass kein NMR-Signal mehr in der Detektionsspule induziert wird. Befinden sich zusätzlich Magnetfeldinhomogenitäten in der Probe, sei es durch Imperfektionen des Hauptmagnetfeldes oder durch Suszeptibilitätsunterschiede innerhalb der Probe, wird statt der formula_23-Relaxation die beschleunigte formula_25-Relaxation beobachtet.

Die formula_25-Relaxation bzw. formula_23-Relaxation beschränkt die Lebensdauer des NMR-Signals direkt nach der Anregung. Das NMR-Signal wird darum als gedämpfte Schwingung, als FID () gemessen. In der Praxis wird die formula_23-Relaxationszeit mit Hilfe der Spin-Echo-Methode gemessen.

Die formula_21-Relaxation limitiert, wie schnell NMR-Experimente hintereinander ausgeführt werden können. Die formula_21-Relaxation und die formula_23-Relaxation hängen stark von der Dichte und Viskosität/Rigidität der Probe ab.

Ein inhärentes Problem der NMR-Spektroskopie ist ihre vergleichsweise geringe Empfindlichkeit (schlechtes Signal-Rausch-Verhältnis). Dieses ist darauf zurückzuführen, dass die Energiedifferenzen der formula_6-Zustände klein und die Populationsunterschiede zwischen den Zuständen im thermischen Gleichgewicht sehr gering sind (Boltzmannverteilung).

Das Besetzungsverhältnis formula_33 der beiden beteiligten Energiezustände kann durch deren Energiedifferenz im Verhältnis zur thermischen Energie bei gegebener Temperatur T ausgedrückt werden:

Darin ist formula_35 die Boltzmann-Konstante. Die Energiedifferenz entspricht dabei der Energie eines Energiequants (formula_36), das ein Teilchen vom günstigeren in den ungünstigeren Zustand befördert (Grundgleichung der Spektroskopie). Bei einer Resonanzfrequenz von 600 MHz und einer Temperatur von 0 °C (273 K) ergibt sich ein Wert von ungefähr e, also sehr nahe bei eins. Daher sind schon im thermischen Gleichgewicht fast gleich viele Kerne im angeregten Zustand wie im Grundzustand. Zum Vergleich: Sichtbares Licht besitzt um einen Faktor von etwa 1 Million höhere Frequenzen. Folglich haben Übergänge, die durch sichtbares Licht angeregt werden, Besetzungsunterschiede von etwa e, liegen also vollständig im Grundzustand vor, was die Spektroskopie im sichtbaren Bereich wesentlich empfindlicher macht.

Die Empfindlichkeit ist somit im Wesentlichen von vier Faktoren abhängig:

Die Faktoren formula_18 und die Spinquantenzahl formula_3 lassen sich durch die relative Empfindlichkeit formula_44 ausdrücken. Dabei wird H als Referenz verwendet. Somit ergibt sich für ein Isotop mit Spinquantenzahl formula_3 und dem gyromagnetischen Verhältnis formula_46 bei gleicher Temperatur, gleichem magnetischen Feld und gleicher Isotopenhäufigkeit die relative Empfindlichkeit formula_44:

Multipliziert man diesen Wert mit der natürlichen Häufigkeit des Isotops, erhält man die absolute Empfindlichkeit formula_49. Seltener wird die relative Empfindlichkeit zu C als Referenz angegeben.

Um die Empfindlichkeit zu steigern, werden verschiedene Maßnahmen ergriffen:

Die Senkung der Temperatur und die Erhöhung der Magnetfeldstärke ändern das thermische Besetzungsgleichgewicht der formula_6-Zustände, so dass mehr Kernspin-Übergänge angeregt werden können. Mit Hilfe der Hyperpolarisation kann ein Besetzungs-Ungleichgewicht erzeugt werden, das stark vom thermischen Gleichgewicht abweicht und in dem der energetisch günstigste formula_6-Zustand fast vollständig besetzt ist.

Zur Optimierung der Signaldetektion wird rauscharme Elektronik verwendet. Der Einsatz von elektrischen Schwingkreisen begrenzt die Detektion auf ein schmales Frequenzband im Bereich der erwarteten Larmor-Frequenz, d. h. die Detektion von Störsignalen und von Rauschen aus anderen Frequenzbereichen wird unterdrückt.

Die Signal-Akkumulation dient dazu, das Signal-Rausch-Verhältnis zu verbessern. Eine NMR-Messung wird formula_53-mal auf identische Weise durchgeführt und die gemessenen Signale der einzelnen Messungen werden addiert. Durch diese Akkumulation nimmt die NMR-Signalstärke um den Faktor formula_53 zu, während statistisches Rauschen nur um den Faktor formula_55 zunimmt.
Da zwischen NMR-Experimenten die vollständige formula_21-Relaxation der Spins erfolgen sollte und die formula_21-Relaxation organischer Substanzen einige zehn Sekunden dauern kann, kann die Signal-Akkumulation zu einer erheblichen Verlängerung der Messdauer führen.

Für typische Messungen sind je nach Experiment und Messzeit ca. 10 nmol bis 1 µmol Substanz notwendig (typische Probenmenge: 1 mL einer Lösung mit einer Konzentration von 10 µmol/L bis 1 mmol/L).

Die erreichbare Auflösung eines Pulsspektrometers ist invers proportional zur Länge des FID-Signals. Neben der transversalen Relaxation (formula_23) der Probe wird sie von der Inhomogenität des formula_17-Feldes in der Probe bestimmt.
Das Ausgleichen von Magnetfeld-Inhomogenitäten erfolgt über das sog. Shimming. Dazu werden im Spektrometer mit Hilfe elektrischer Ströme schwache Magnetfelder zusätzlich zum Hauptmagnetfeld formula_17 erzeugt, mit denen lokale Inhomogenitäten zum Teil ausgeglichen werden können. Die erhöhte Homogenität des resultierenden Gesamtmagnetfeldes reduziert die formula_25-Relaxation, wodurch das NMR-Signal langlebiger wird.

Um höchste Auflösung im Spektrum zu erhalten, d. h. schmale (scharfe) NMR-Linien und Unterscheidbarkeit eng benachbarter Linien, muss eine weitere Technik angewendet werden, nämlich die schnelle Rotation (in engl. "spinning") der Messzelle um ihre Längsachse, mit Hilfe einer kleinen Luftturbine. Durch diese makroskopische Bewegung der Probe erfahren die Kerne in der Probe einen zeitlichen Mittelwert des äußeren Feldes über das Probevolumen. Die nach dem Shimmen verbliebenen Magnetfeld-Inhomogenitäten werden also ausgemittelt. So erreicht man schließlich eine Frequenzauflösung in der NMR von ca. 0,1 Hz, was die Unterscheidbarkeit von extrem kleinen Energiedifferenzen bedeutet, die dann eine Fülle von Details aus hochaufgelösten NMR-Spektren liefern können.

NMR-Spektren können am einfachsten für Moleküle aufgenommen werden, die sich in Lösung befinden und nicht mit paramagnetischen Substanzen in Wechselwirkung stehen. NMR-Spektroskopie an paramagnetischen Substanzen und an Festkörpern ist ebenfalls möglich, die Interpretation der Spektren und die Aufbereitung der Proben für die Messung sind aber in beiden Fällen deutlich komplexer. Bezüglich der NMR an Festkörpern vgl. auch Magic-Angle-Spinning.

Die hochauflösende Kernresonanzspektroskopie in Lösung wird heute in großem Maßstab für folgende Aufgaben verwendet:

Neben spektroskopischen Untersuchungen vermittelt die Bestimmung von Kernspin-Relaxationszeiten Informationen über die Struktur und Dynamik von Materialien. In Flüssigkeiten z. B., deren Mikrostruktur und -Dynamik mit herkömmlichen Methoden nur schwer erforscht werden kann, können Abstände zwischen molekularen Nachbarn und molekulare Umorientierungszeiten, die typischerweise im Pico- bis Nanosekunden- Bereich liegen, mittels Relaxationszeitmessungen bestimmt werden.

Unterschiedliche Kernspin-Relaxationszeiten in verschiedenen biologischen Geweben bilden die Basis für die in der Medizin als bildgebendes diagnostisches Verfahren genutzte Magnetresonanztomographie (Kernspintomographie). Eine Anwendung der kernmagnetischen Resonanz, welche für die Neurowissenschaften, wie Neurologie und Neuropsychologie, eine außerordentliche Bedeutung erlangt hat, ist die Funktionelle Magnetresonanztomographie. Magnetresonanztomographie-Methoden finden allgemein, außer in der medizinischen Diagnostik, auch zunehmend Anwendungen in den Ingenieur- und Geowissenschaften.

Ein weiteres wichtiges Anwendungsgebiet ist die Untersuchung der translatorischen Moleküldynamik, also von Diffusion und Fließbewegungen von Molekülen oder Molekülaggregaten in Flüssigkeiten und Festkörpern mittels Feldgradienten-NMR. Mit der sogenannten "diffusion-ordered-spectroscopy" (DOSY) kann in Mischungen die translatorische Beweglichkeit der, NMR-spektroskopisch identifizierten, Einzelkomponenten gemessen werden. Zusätzlich kann mit Hilfe des DOSY-NMR Experiments das Molgewicht von Molekülen in gelöster Form bestimmt werden.

Die Resonanzfrequenzen werden nicht als Absolutwerte, sondern als "chemische Verschiebung" gegenüber einer Referenzsubstanz, dem so genannten Standard, angegeben. Die chemische Verschiebung ist definiert als

wodurch sie unabhängig von der Feldstärke des gerade verwendeten Magneten wird. Da die Werte der chemischen Verschiebung sehr klein sind, werden sie in ppm angegeben. Als Standard für H- und C-Spektren organischer Lösungen wird die Resonanzfrequenz der jeweiligen Kerne in Tetramethylsilan (TMS) verwendet. Während früher jeder Probe einige Milligramm TMS zugesetzt wurden, bezieht man sich heutzutage in der Regel auf die bekannte chemische Verschiebung der Restprotonen des deuterierten Lösungsmittels.

Die chemische Verschiebung von Wasserstoffkernen in organischen Molekülen wird durch die Art der funktionellen Gruppen beeinflusst. Je nach der Struktur des Moleküls weichen die chemischen Verschiebungen gleicher funktioneller Gruppen leicht voneinander ab, so dass das NMR-Spektrum charakteristisch für eine Substanz ist. Außerdem werden sie durch benachbarte Moleküle in der Probe beeinflusst, so dass in unterschiedlichen Lösungsmitteln oder in der Reinsubstanz unterschiedliche relative und absolute Resonanzfrequenzen der Protonen einer Probe auftreten. Bei starken Wechselwirkungen zwischen Substanz und Lösungsmittel treten dabei Unterschiede von mehreren ppm auf. Mit Hilfe der Shoolery-Regel kann die chemische Verschiebung abgeschätzt werden.

Die relative Anzahl der einem bestimmten Signal zugrundeliegenden Wasserstoffatome ist bei einfacher H-Spektroskopie proportional zum Flächeninhalt (dem Integral) des betreffenden Signals.

Durch Auswertung dieses Integrals kann also beispielsweise bestimmt werden, wie viele Wasserstoffatome eines Moleküls sich an Methylgruppen, an Aromaten, an Carboxygruppen, an Doppelbindungen usw. befinden. Diese Kenntnis ist für die organische Chemie bei der Bestimmung von Strukturen äußerst wichtig. Eine Übersicht zur Zuordnung bestimmter funktioneller Gruppen (Atomgruppen, Stoffgruppen) zu Werten der chemischen Verschiebung bietet die folgende Tabelle.

Spinzustände werden durch in ihrer Nachbarschaft befindliche weitere Kerne mit von Null verschiedenem Spin in energetisch unterschiedliche Niveaus aufgespalten, deren Zahl von der Anzahl möglicher unterschiedlicher Orientierungen der einzelnen Spins abhängt. Diese "skalare Kopplung" wird durch die Spins der die Bindung zwischen den Teilchen bildenden Elektronenpaare vermittelt, ihre Wirkung ist gewöhnlich über bis zu vier Bindungen feststellbar. Der Abstand der Linien ist unabhängig vom angelegten Feld, und wird deswegen als absoluter Frequenzunterschied (in Hertz) angegeben. Bei Kernen, die chemisch und spin-symmetrisch gleich ("magnetisch äquivalent") sind, ist die Kopplung nicht sichtbar.

Aus dem beobachteten Kopplungsmuster kann der Spektroskopiker die Nachbarschaftsverhältnisse der einzelnen Kerne, und damit in vielen Fällen die vollständige Struktur einer Verbindung erschließen.

Mitunter kann es hilfreich sein, für die Strukturaufklärung eine bestimmte oder alle Kopplungen zu unterdrücken. Dazu wird ein Radiosignal mit der Frequenz eines Kernes oder einer ganzen Gruppe von Kernen (Breitbandentkopplung) eingestrahlt, das übrige Spektrum verhält sich dann so als wäre der entsprechende Kern nicht vorhanden. C-Spektren werden standardmäßig H-entkoppelt, da sie durch die Überlappung der Kopplungsmuster der einzelnen Kerne sonst oft uninterpretierbar wären. Außerdem wird die geringe Empfindlichkeit der Methode durch die fehlende Aufspaltung verbessert.

In festen oder hochviskosen Proben wird der richtungsabhängige Teil der skalaren Kopplung, sowie die dipolare Kopplung, nicht mehr zu null ausgemittelt. Solche Proben zeigen große, feldunabhängige Aufspaltungen, bzw. Linienverbreiterungen von mehreren Kilohertz für H-Spektren.

Der organische NMR-Spektrensatz umfasst sechs NMR-Experimente, die (insbesondere in Verbindung mit Daten aus Massenspektrometrie, IR-Spektroskopie und UV/VIS-Spektroskopie) besonders hilfreich zur Aufklärung der Struktur einer organischen Verbindung sind und deshalb in der Regel als erste durchgeführt werden, um eine Struktur aufzuklären. In der Reihenfolge ihrer Bedeutung sind dies

Die Kernsuszeptibilitäten sind wesentlich geringer als die diamagnetischen und paramagnetischen Suszeptibilitäten (Faktor:10). Bei Dia- und Paramagnetismus sind die Elektronen des Atoms für die Suszeptibilität verantwortlich.
Bei Kernen kann die Suszeptibilität mit der Langevin-Debye-Formel bestimmt werden.

Früher wurden NMR-Resonanzen mit einer Brückenschaltung in Schwingkreisen bestimmt.
Bloch und Mitarbeiter nutzten zwei identische Schwingkreise, d. h. zwei Spulen und zwei Kondensatoren, um einen Abgleich mit einer Brückenschaltung vorzunehmen; eine Spule als Sender eine als Empfänger. Es ist möglich, eine Brückenschaltung mit nur einer Spule herzustellen. Dieses Verfahren wurde von Purcell genutzt.

Vor der Probenvermessung wird die Brücke mit der zu messenden Frequenz abgeglichen. Mit Gleichungen aus der Physik kann man für einen Schwingkreis und eine Brückenschaltung die Phasenverschiebung zwischen Strom und Spannung, den Scheinwiderstand und die Stromlosigkeitsbedingungen einer Brücke berechnen.

In die Spule kommt nun ein Substanzröhrchen hinein. Ein Magnetfeld (mit einem Permanentmagneten oder Elektromagneten) wird dann horizontal zur Spulenachse erzeugt. Bei einer ganz bestimmten Frequenz und einer bestimmten Magnetfeldstärke und nur bei Anwesenheit einer Substanzprobe (mit entsprechenden Atomkernen) wird der Schwingkreis verstimmt. Im Oszilloskop oder mit einem Schreiber ist diese Verstimmung sichtbar.

Sehr bedeutsam war die Bestimmung der räumlichen Magnetisierung durch das angelegte Magnetfeld. Bloch führte für alle Raumrichtungen Berechnungen durch und konnte die schwingungsabhängigen Suszeptibilitäten für die Raumrichtungen ableiten (Bloch-Gleichungen). Ungeklärt blieb jedoch noch die Frage der Relaxationszeit, das heißt, der Zeitdauer bis der angeregte Kernspin auf das Grundniveau zurückfällt. Mittels paramagnetischer Salze konnte dann die Relaxationszeit von reinen Wasserstoffprotonen auf etwa drei Sekunden berechnet werden.

Wasserstoffkerne konnten bei sehr geringen Frequenzen (wenige Kilohertz) und einem sehr schwachen Magnetfeld durch Schwingkreisverstimmung nachgewiesen werden. Interessant wird die Methode für die Strukturaufklärung von komplexen Molekülen jedoch erst bei hohen Frequenzen (ab 60 MHz) und stärkeren Magnetfeldern (1,4 Tesla), da sich dann die chemischen Verschiebungen von unterschiedlichen Wasserstoffatomen komplizierter Verbindungen deutlicher unterscheiden. Will man jedoch nicht nur ein einziges Signal auf dem Oszilloskop sehen, sondern mehrere unterschiedliche Wasserstoffatomkerne (oder andere Kerne) so muss ein ganzes Frequenzband eingestrahlt werden.

Früher – bis in die 1970er Jahre – nutzten die NMR-Spektrometer das Continuous-Wave-Verfahren (CW), um das Spektrum einer komplexen Verbindung abzutasten.

Heute ist die Puls-Fourier-Transformation (PFT) üblich. Hierbei wird ein Hochfrequenzimpuls eingestrahlt. Dieser Impuls enthält ein ganzes Band an Schwingungen.

Die bereits angesprochene Abhängigkeit der Energieniveaus der Kernspins von der Molekülstruktur rührt in erster Linie von der Wechselwirkung der Elektronenstruktur der Moleküle mit dem äußeren Magnetfeld her: Hierdurch entsteht in der Elektronenhülle ein Induktionsstrom, welcher wiederum ein Magnetfeld erzeugt, das dem äußeren entgegengerichtet ist. Dadurch wird das Magnetfeld am Atomkern geschwächt, die Frequenz der für den Übergang notwendigen Strahlung ist also kleiner als im Falle eines nackten Atomkerns. Die Differenz heißt chemische Verschiebung und wird üblicherweise im Verhältnis zur für den nackten Atomkern nötigen Frequenz angegeben. Chemische Verschiebungen liegen üblicherweise im Bereich von 0–5000 ppm.

Das magnetische Feld wird am Atomkern durch die Ausrichtung weiterer magnetischer Momente in der unmittelbaren Umgebung beeinflusst. Befindet sich beispielsweise ein Kern mit zwei Ausrichtungsmöglichkeiten in der Nähe, so kann dieser das Feld verstärken oder abschwächen. Dies führt zu einer Aufspaltung des Signals, man spricht von einer Kopplung. Weil die chemische Verschiebung im Wesentlichen von der Elektronendichte am Atomkern abhängt, kann man für Atomkerne in chemisch ähnlichen Umgebungen ähnliche Verschiebungen erwarten. Aus der Kopplung erhält man zusätzlich Informationen über Nachbarschaftsbeziehungen zwischen verschiedenen Kernen in einem Molekül. Beides zusammengenommen liefert wesentliche Hinweise über die Struktur des gesamten Moleküls.

Atomkerne mit einer ungeraden Protonen- und/oder Neutronen-Zahl besitzen einen Kernspin "I". Dieser kann ganz- und halbzahlige Werte (z. B. 1/2, 1, 3/2, …, 9/2) annehmen: bei den sogenannten uu-Kernen ist "I = n" (also nur ganzzahlig: 1, 2, 3, …) während bei gu- und ug-Kernen "I" = (2"n"+1)/2 ist (also halbzahlig: 1/2, 3/2, 5/2, …), bei Isotopen mit gerader Protonen- und Neutronenzahl (sogenannten gg-Kernen) ist "I" = 0. Von Null verschiedene Kernspins gehen mit einem magnetischen Dipolmoment einher. Die Größe dieses Dipolmoments wird durch das gyromagnetische Verhältnis des betreffenden Isotops beschrieben. In einem äußeren, statischen Magnetfeld richten sich magnetische Kernmomente entsprechend den Regeln der Quantenmechanik aus. Ein Atomkern mit I = ½ hat die Form einer Kugel, Kerne mit I > ½ haben eine ellipsoidische Form und haben daher zusätzlich ein elektrisches Quadrupolmoment „eQ“, welches mit elektrischen Feldgradienten wechselwirken kann (siehe auch Kernquadrupolresonanz-Spektroskopie). Diese zusätzliche starke, elektrische Wechselwirkungsmöglichkeit führt zu breiten NMR-Resonanzlinien, die komplizierter zu interpretieren sind als die schmalen, durch gut auflösbare Kopplungen strukturierten Resonanzlinien der Spin-½-Kerne.

Die am meisten für die chemische Strukturaufklärung genutzten Isotope sind daher Kerne mit Spin ½. Hierzu gehören unter anderem die Nuklide H, C, N, F, Si und P. Spin-½-Kerne können nur zwei diskrete Zustände annehmen, nämlich entweder parallel oder antiparallel zum äußeren Magnetfeld. Zwischenstellungen sind quantenmechanisch verboten. Die zwei Anordnungsmöglichkeiten entsprechen zwei unterschiedlichen Energiezuständen.

Die Energiedifferenz zwischen diesen beiden Zuständen ist proportional zur Stärke des Magnetfelds am Kernort. Der Proportionalitätsfaktor ist dabei das gyromagnetische Verhältnis des betreffenden Isotops. Übergänge zwischen den beiden Orientierungen der Kernmomente können durch die Einstrahlung resonanter magnetischer Wechselfelder ausgelöst werden. Die Resonanzfrequenz ist der Energieaufspaltung zwischen den beiden Kernspins proportional und wird als Larmorfrequenz bezeichnet.

Veranschaulichen lässt sich dies durch das nebenstehende Diagramm. Hierbei denkt man sich ein Koordinatensystem mit dem äußeren Magnetfeld entlang der z-Achse. Ein Atomkern mit einem Spin von ½ richtet sich mit einem Spin-Vektor entweder parallel oder antiparallel zum äußeren Feld aus. Wenn man nun die Vektoren mehrerer Atome in dieses Koordinatensystem aufnimmt, entstehen zwei Kegel, jeweils einer für parallel und antiparallel. Infolge des Energie-Unterschieds zwischen der parallelen und der antiparallelen Orientierung der magnetischen Kernmomente gibt es im thermischen Gleichgewicht einen Besetzungsunterschied zwischen den beiden Orientierungen. Dieser folgt in Hochtemperatur-Näherung der Boltzmann-Verteilung und bewirkt eine Überschussmagnetisierung in positiver Richtung entlang der z-Achse.

Das NMR-Signal kommt dadurch zustande, dass man die zu untersuchende Probe im Magnetfeld einem Radiofrequenz-Puls aussetzt. Dabei werden die Spins der einzelnen Atome durch das Magnetfeld des Pulses beeinflusst, so dass der Gesamtvektor, der sich aus den gezeigten Spin-Kegeln ergibt, gekippt wird. Er liegt nun nicht mehr parallel zur z-Achse, sondern ist um einen Winkel ausgelenkt, der proportional zur Dauer und Intensität des Radiofrequenzpulses ist. Typisch sind Pulslängen von etwa 1–10 µs. Eine maximale Quermagnetisierung senkrecht zur z-Achse wird bei einem Auslenkungswinkel von 90° erreicht.

Diese Quermagnetisierung verhält sich wie ein magnetischer Kreisel und präzediert in der Ebene senkrecht zum statischen Magnetfeld. Diese Präzessionsbewegung macht sich als sehr schwaches magnetisches Wechselfeld bemerkbar, das mittels geeigneter Verstärker gemessen wird. Nach Beenden der resonanten Einstrahlung treten zwei Prozesse, sogenannte Relaxationsprozesse, ein, durch die die Quermagnetisierung wieder abnimmt. Das NMR-Signal wird also nach Beenden des Radiofrequenzpulses als Freier Induktionszerfall (FID; von englisch: ) gemessen. Die Zeitkonstante formula_25 dieses freien Induktionszerfalls hängt von der transversalen Relaxationszeit formula_23 sowie von der Homogenität des Magnetfelds ab. Für leicht bewegliche Flüssigkeiten in homogenen Magnetfeldern kann sie im Bereich von mehreren Sekunden liegen. Der FID wird durch die Frequenzunterschiede infolge von chemischer Verschiebung und Kopplung moduliert. Durch eine Fourier-Transformation kann die Verteilung der verschiedenen Frequenzen aus dem FID berechnet werden. Dies ist dann das NMR-Spektrum. Das NMR-Spektrum liefert in vielen Fällen einen eindeutigen „Fingerabdruck“ des jeweiligen Moleküls. Zusammen mit Informationen aus weiteren Messungen wie z. B. der Massenspektrometrie kann aus den Spektren die chemische Struktur einer unbekannten Substanz bestimmt werden.

Kommerzielle NMR-Spektrometer für die chemische Strukturaufklärung arbeiten üblicherweise bei magnetische Flussdichten zwischen 7 und 21 Tesla. Für H entsprechen die Resonanzfrequenzen (Larmorfrequenzen) dann zwischen 300 und 900 MHz. Da H der wichtigste NMR-Kern ist, wird die Feldstärke von Spektrometern gewöhnlich in dessen Larmorfrequenz ausgedrückt. Bei H beträgt die Aufspaltung der Spektren infolge unterschiedlicher chemischer Verschiebungen ca. 10 ppm. Dies entspricht also einer maximalen Bandbreite von ca. 3 kHz bei einer NMR-Frequenz von 300 MHz. Die Frequenzbandbreite der NMR-Spektren infolge der chemischen Verschiebung wächst proportional zum Magnetfeld an. Die chemische Verschiebung selbst ist als Verhältnis der Differenz der Resonanzfrequenz des Kerns in einer bestimmten chemischen Umgebung und der Resonanzfrequenz in einer Referenzverbindung zur Resonanzfrequenz selbst definiert. Dies erlaubt einen einfachen Vergleich zwischen NMR-Spektren, die bei verschiedenen Feldern gemessen wurden. Für Wasserstoff und Kohlenstoff wird Tetramethylsilan (TMS) als Referenzsubstanz genommen. Der Bereich von chemischen Verschiebungen für Kohlenstoff und viele andere Kerne ist wesentlich breiter als für Wasserstoff und kann mehrere 100 ppm betragen. Bei einigen sehr schweren Kernen wie z. B. Pb werden chemische Verschiebungen im Bereich von Prozent beobachtet.

Heutzutage arbeiten alle modernen NMR-Spektrometer mit der Puls-Technik. Bei dieser wird ein einzelner Radiofrequenzimpuls (RF-Puls) oder eine Sequenz von RF-Pulsen auf eine Probe gesandt, die sich in einem starken Magnetfeld befindet. Nach dem Abklingen des Pulses in der Empfangselektronik (Totzeit) wird der Zerfall der Magnetisierung (engl. , FID), d. h., ihre Rückkehr in den Gleichgewichtszustand, über die dadurch in der Empfangsspule induzierte Spannung als Funktion der Zeit detektiert. Durch Fourier-Transformation wird dieses Zeitsignal im Computer in das Frequenzspektrum (Signalintensität als Funktion der Frequenz) transformiert.

Diese Messtechnik hat das früher verwendete CW-Verfahren (engl. ) (s. o.) fast vollständig verdrängt.


Die eindimensionale NMR-Spektroskopie ist die am häufigsten angewandte Strukturaufklärungsmethode der Chemie. Bei ihr wird die chemische Verschiebung eines Atoms von einer Referenzsubstanz gemessen. H und C sind die Kerne, die am häufigsten in der organischen Chemie gemessen werden, aber auch N, P, F und viele andere NMR-aktive Isotope können spektroskopiert werden.

Das Aussehen der Spektren hängt entscheidend von der Aufnahmeart ab. H-Spektren werden in der Regel nicht Breitband-entkoppelt aufgenommen. Damit haben alle Wasserstoffatome die Möglichkeit, ihren Spin mit anderen Kernen zu koppeln, die sogenannte Spin-Spin-Kopplung. Damit entsteht bei der charakteristischen chemischen Verschiebung eines Atoms eine für seine Umgebung charakteristische Aufspaltung des Signals, aus der Informationen über die Molekülstruktur abgeleitet werden können.

C, N, P, F und andere Kerne werden häufig H-Breitband-entkoppelt aufgenommen, so dass die Aufspaltung der Signale aufgrund der Kopplungen zu H-Kernen ausbleibt.

Der Kern eines Atoms kann mit einem benachbarten Atomkern in Wechselwirkung treten. Das kann entweder direkt (durch den Raum) oder indirekt (über die Bindungselektronen zwischen den Kernen) geschehen. Bei einer flüssigen Probe mittelt sich die direkte (dipolare) Wechselwirkung durch die schnelle Bewegung der Kerne im Raum aus. Erhalten bleibt die skalare Kopplung, die dadurch vermittelt wird, dass die (stets gepaart (↓↑)) auftretenden Spins der Bindungselektronen unterschiedlich mit den Kernspins auf beiden Seiten der Bindung wechselwirken.

Hat ein Kern den Zustand α (↑), so wird das (↑)-Elektron der Bindung von ihm abgestoßen, hält sich also eher am anderen Kern auf. Weitere von dort ausgehende Bindungen werden gleichfalls (in geringerem Ausmaß) spinpolarisiert. Wird so ein weiterer Kern mit formula_65 erreicht, ergibt sich ein Energieunterschied zwischen dessen Zustand α und β durch wiederum seine Wechselwirkung mit den Elektronen der Bindung. Solche Kopplungen sind gewöhnlich über max. drei bis vier Bindungen nachweisbar, in konjugierten π-Systemen zum Beispiel aber auch weiter.

Das NMR-Signal des ersten Kerns wird dadurch zu einem Dublett (für formula_66, sonst formula_5 Linien) aufgespalten, und das des zweiten Kerns gleichfalls, und zwar um den gleichen Betrag, da der Energieunterschied (sog. Kopplungskonstante) zwischen αα(=ββ) und αβ(=βα) derselbe sein muss. Durch die gleich starke Aufspaltung ist dann die Nachbarschaft der beiden Atome im Molekül nachgewiesen. Koppelt ein Kern noch zu weiteren anderen, so wird jede seiner Linien entsprechend nochmals aufgespalten.

Koppelt ein Kern zu zwei (oder allgemein formula_53) gleichartigen Nachbarkernen (die aus der Sicht des Ursprungskerns die gleiche chemische Umgebung und Spin-Symmetrie besitzen), so erhält man ein Triplett, da die mittleren Linien des „Dubletts vom Dublett“ (oder Quartett, Quintett usw., also formula_69 Linien, bzw. formula_70 Linien für Kerne mit formula_71) zusammenfallen. Die relativen Intensitäten der Linien ergeben sich (für formula_66 – Kerne) aus der formula_69-ten Zeile des Pascalschens Dreiecks, also 1:2:1 oder 1:3:3:1.
Sind die koppelnden Kerne "nicht gleichartig", d. h. sind ihre Kopplungskonstanten unterschiedlich, so fallen die mittleren Linien nicht zusammen, man erhält dann z. B. ein Dublett vom Triplett o. ä.

Als ein einfaches Beispiel dient Propan (HC–CH–CH): Die CH-Gruppe beim Propan hat zwei benachbarte Methylgruppen (–CH). Dies entspricht sechs benachbarten, äquivalenten H-Atomen. Das Signal wird also in ein Septett aufgespalten. Die Methyl-Protonen werden von den beiden Methylen-Protonen zum Triplett aufgespalten, die J-Kopplung zu den drei anderen Methyl-Protonen ist unsichtbar, da diese magnetisch äquivalent sind, so wie auch keine J-Kopplung innerhalb der Methylgruppen beobachtet werden kann.

Sind in einem Molekül mehrere "unterschiedliche" z. B. Methyl-Gruppen vorhanden, so überlagern sich deren Multipletts häufig, wodurch sie schnell unauswertbar werden. Um solche Fälle besser auflösen zu können, wird hierfür vielfach auf mehrdimensionale NMR-Techniken wie COSY zurückgegriffen. Da die Aufspaltung nicht feldabhängig ist, der Abstand zwischen den Signalen chemisch unterschiedlicher Protonen aber schon, können Überlagerungen durch Anwendung eines höheren Feldes aufgelöst werden.

Erklärungen zum Spektrum von Ethanol:

Die OH-Gruppe zeigt nur ein Singulett, wenn das Ethanol der Probe mehr als 3 % Wasser enthält. Das alkoholische Wasserstoffatom ist leicht acid und wird deswegen ständig durch Wasserstoffatome anderer Hydroxygruppen oder aus dem Lösungsmittel ausgetauscht. Dies führt dazu, dass keine permanenten Spin-Spin-Kopplungseffekte zwischen OH-Gruppe und benachbarter Methylengruppe auftreten. Bei sehr tiefen Temperaturen oder in absolut wasserfreiem Alkohol ist dieser Austausch genügend verlangsamt, um dann die erwartete vicinale Kopplung (formula_74) sichtbar zu machen.

Bei der zweidimensionaler Kernspinresonanzspektroskopie (2D-NMR) werden die Intensitäten in Abhängigkeit von zwei Frequenzachsen aufgezeichnet. Sie generiert also dreidimensionale Diagramme. Zweidimensionale NMR-Spektren liefern mehr Informationen über ein Molekül als eindimensionale NMR-Spektren und sind deshalb besonders nützlich bei der Bestimmung der Struktur eines Moleküls, insbesondere für Moleküle deren Struktur zu kompliziert ist, um sie mit eindimensionalen NMR zu untersuchen.

Es gibt relativ preiswerte Niedrigfeld-NMR-Geräte ( 10 … 60 MHz), die, mit einem Permanentmagneten ausgestattet, zwar keine aufgelösten Spektren liefern, dafür aber in den Betriebskosten unvergleichlich günstiger (keine He-Kühlung) sind. Auch können solche Systeme portabel ausgelegt werden. Durch Analyse der H-Relaxationszeiten können Mischungsanteile von Mehrstoffsystemen (Suspensionen, teilkristalline Substanzen) und, nach Kalibrierung, auch Absolutmengen von Stoffen quantifiziert werden, was besonders in der Industrie interessant ist. Messungen erfolgen dabei anstatt in einem (teuren) deuterierten Lösungsmittel üblicherweise in Substanz. Andere Kerne als Wasserstoff werden aufgrund der geringen Empfindlichkeit nur selten untersucht.

Deuterium (D, H) stellt insofern eine Besonderheit dar, weil der Spin formula_10 beträgt. Das hat zur Folge, dass die Linienbreite der NMR-Signale gegenüber H-Kernen größer ist. Die Resonanzfrequenzen liegen deutlich unter denen von H-Kernen (61,4 gegenüber 400 MHz bei 9,39 Tesla). Die Deuterium-NMR-Spektroskopie ist ungefähr um den Faktor 100 unempfindlicher als die H-NMR-Spektroskopie. Außerdem ist der Deuteriumanteil gegenüber Wasserstoff in organischen Verbindungen sehr gering (ca. 0,0159 %). Mit modernen NMR-Spektrometern stellt die Untersuchung jedoch heute kein Problem dar. Die Aufnahmen bzw. Auswertungen erfolgen mit der Fourier-Transform-Methode. Die Interpretation der Spektren ist nicht schwierig, weil die chemischen Verschiebungen praktisch identisch mit denen von H sind.

Mit der H-NMR-Spektroskopie lassen sich für die sog. SNIF-NMR die Deuteriumverteilung in den einzelnen Positionen in einer organischen Verbindungen und das D/H-Verhältnis bestimmen. Die Deuteriumverteilung lässt sich aus den Spektren direkt ablesen, das D/H-Verhältnis kann man nur bestimmen, wenn man einen Standard mit einem bekannten D/H-Verhältnis benutzt. Diese Methode ist für die Analytik bedeutungsvoll, weil so eine Aussage über die Herkunft einer organischen Verbindung getroffen werden kann. Dies ist einerseits darin begründet, dass der Deuteriumanteil auf der Erde unterschiedlich ist und die Ausgangsstoffe, für Naturstoffe im Wesentlichen auch Wasser, damit einen geringfügigen Unterschied im Deuteriumgehalt aufweisen. Andererseits ist bei den Synthesewegen der kinetische Isotopeneffekt von Bedeutung. So weist z. B. Ethanol in Weinen aus unterschiedlichen Regionen eine unterschiedliche Deuteriumverteilung im Molekül bzw. ein unterschiedliches D/H-Verhältnis auf. Das Gleiche gilt für alle Naturstoffe und somit kann man inzwischen für viele dieser Stoffe eine Herkunft beziehungsweise die Syntheseart, ob natürlich oder synthetisch, zuordnen.

Bei aus Gärungsprozessen hergestelltem Ethanol lässt sich über das D/H-Verhältnis (sogenannter R-Wert) die pflanzliche Herkunft feststellen, also ob aus Zuckerrohr, Rüben, Getreide, Mais, Kartoffeln, Trauben oder Äpfeln. Daneben lassen sich unerlaubte Zuckerzusätze ermitteln. Neben Ethanol können zudem aus C-Spektren weitere Inhaltsstoffe im Wein, wie Glycerin, Methanol, Organische Säuren und Konservierungsmittel qualitativ und quantitativ bestimmt werden.

Weiterhin ist Deuterium-NMR eine verbreitete Methode zur Untersuchung einer Orientierungsanisotropie in hochmolekularen organischen Festkörpern (z. B. Fasern).

Neben den Untersuchungen von organischen Verbindungen mit der H-, C- und F-NMR-Spektroskopie ist die Metallkern-NMR-Spektroskopie von Bedeutung. Hierbei können Metall-Metall- bzw. Metall-Ligand-Bindungen in Komplexverbindungen und metallorganischen Verbindungen direkt untersucht werden. Auch lassen sich Proteine mit eingelagerten Metallionen untersuchen. Kurzlebige Zwischenstufen, die nach der Reaktion nicht oder nur schwer nachweisbar sind, lassen sich in den Reaktionslösungen nachweisen. Man benötigt keine speziellen Lösungsmittel, sondern kann die Messungen in der Reaktionslösung durchführen. Beispiele sind die Li-, Mg-, Al-NMR-Spektroskopie und Messungen an Schwermetallkernen, wie z. B. Pt, Tl und Pb.

Die Untersuchung elektrisch leitender metallischer Festkörper erfordert andere experimentelle Voraussetzungen als die von Metallkernen in Lösungen. Die Kernspin-Quantenzahl formula_3 mancher Metallkerne ist größer als 1/2 (Beispiele: Li: 1, Li: 3/2, Na: 3/2, Mg: 5/2, Co: 7/2). Solche Kerne haben ein elektrisches Quadrupolmoment, sie relaxieren über einen besonderen Relaxationsmechanismus und haben daher oft sehr breite Resonanzlinien, was Auswirkungen auf die Empfindlichkeit der NMR-Messungen hat.

Pionier war die Firma Varian Associates in Palo Alto, Kalifornien. Varian wurde 1948 gegründet und fertigte hochauflösende Flüssigkeitsspektrometer, die auf Continuous-Wave-Sweep-Methoden und Elektromagneten basierten. Bedeutende Produkte waren: R30 (1952), HR40 (1955), HR60 (1958), HR100 (1959). Mit dem Erfolg des A-60-Spektrometer (1961) wurde Varian weltweiter Marktführer.

Die Firma Trüb, Täuber & Co. AG, Zürich entwickelte in den 1960er-Jahren in enger Zusammenarbeit mit der ETH Zürich NMR-Spektrometer und baute die Geräte KIS-1 und KIS-2 (1963) als Kleinserie. Die Firma wurde 1965 geschlossen.

Die Firma Bruker, Karlsruhe, wurde 1960 gegründet und fertigte Hochleistungs-Magnete, später auch NMR-Spektrometer. Bruker übernahm 1965 die Forschungsabteilung der Firma Trüb und Täuber und gründete damit die Firma Spectrospin AG, Zürich. Durch diese Synergie entstand 1967 das hochauflösende HFX-90 Spektrometer mit 90 MHz, das erste volltransistorisierte NMR-Spektrometer. 1972 entstand bei Spectrospin der erste reine FT Spektrometer, das WH 90 System, der Urahn aller modernen Kernresonanzspektrometer. 
2010 hatte Bruker einen weltweiten Marktanteil in der Kernresonanzspektroskopie von etwa 80 %.





</doc>
<doc id="13290" url="https://de.wikipedia.org/wiki?curid=13290" title="Boden">
Boden

Boden steht für:




Boden heißen folgende geographische und astronomische Objekte:

Städte, Gemeinden:

Gemeindeteile und sonstige Orte:




sowie:
Boden ist der Name folgender Personen:

Siehe auch:



</doc>
<doc id="13291" url="https://de.wikipedia.org/wiki?curid=13291" title="Wiese (Grünland)">
Wiese (Grünland)

Bei der Wiese handelt es sich um landwirtschaftliches Grünland, das im Gegensatz zur Weide nicht durch das Grasen von Tieren, sondern durch Mähen zur Erzeugung von Heu oder Grassilage genutzt und erhalten wird.
Der Lebensraum Wiese ist sehr vielfältig und bietet vielen Tier- und Pflanzenarten eine Heimat, die sich allerdings sehr stark voneinander unterscheiden.
Bei der regelmäßigen Mahd (Mähen) wird die Verbuschung und anschließende Waldentstehung verhindert. Wiesen sind wie die Weiden ein Lebensraum, der seit einigen Jahrtausenden durch den Menschen geschaffen und erhalten wird. Man spricht daher von einer Halbkulturformation.

Durch den Selektionsdruck der Mahd werden Pflanzen begünstigt, die mit dem häufigen Schnitt und der hohen Lichteinstrahlung gut zurechtkommen, unter anderem viele Gräser. Aufgrund der regelmäßigen Mahd werden mehrjährige Pflanzen (perennierend, Stauden) gegenüber einjährigen Pflanzen bevorzugt. Sie überdauern die Winter und vermehren sich vegetativ. Ihre Samen sind in der Regel Lichtkeimer.

Bestimmte Pflanzen, wie zum Beispiel die Disteln, fehlen den Wiesen, während sie auf Weiden vom Vieh gemieden werden und nicht vom Schnitt beeinträchtigt werden. Die Artenvielfalt auf einer Wiese wird wesentlich bestimmt durch die Häufigkeit des Mähens.

Natürlich würden Wiesen unter heutigen Verhältnissen in Mitteleuropa nicht mehr entstehen und erhalten bleiben. Sie weisen dennoch Ähnlichkeiten mit Steppen und Waldsteppen, zu alpinen Matten und Rasen sowie zu Magerwiesen bzw. Trockenrasen auf, die im Volksmund manchmal auch als Wiesen bezeichnet werden, da sie vornehmlich aus krautigen Pflanzen wie Süßgräsern bestehen.


Sonderformen sind:


Die Beteiligung von unterirdisch wühlenden Wirbeltieren bewirkt eine Bodenveränderung.
Wühltiere haben einen langgestreckten Körperbau und eine kurze, wasserabweisende Behaarung.
Mit kräftigen Grabschaufeln durchwühlen sie den Boden wie z. B. der Maulwurf.
Die Gänge des Maulwurfs und der Mäuse sind eine Schädigung des Wurzelwerks von niederstehenden Gräsern.
Durch Wühler wird die Wiese vielgestaltiger. Durch hohe Feldmauspopulation lockt man tag- und nachtaktive Räuber, wie Vögel (Turmfalke, Mäusebussard) und Säuger (Wiesel, Fuchs) an.

Lebensbedingungen:
Die alles beherrschende Mahd diktiert die Randbedingungen für das Tierleben.
Die bei der Regeneration des Grünbestands nach einer Mahd aufwachsenden nährstoffreichen Jungpflanzen begünstigt manche pflanzenfressende Insekten, wie beispielsweise Wanzen.
Blütenbesuchende Insekten (wie beispielsweise Schmetterlinge oder Wildbienen), Insekten, die in Pflanzenteilen leben (oder in ihnen überwintern) oder solche, die auf eine ausreichend dicke Streuschicht angewiesen sind, können sich nicht auf Dauer halten, sofern keine zeitweise ungemähten Bereiche verbleiben. Derartige bei jeder Mahd räumlich wechselnden Bereiche sind ein sehr einfaches Mittel, um die biologische Vielfalt in Wiesen zu fördern. Bodenbewohner finden dagegen das ganze Jahr über einen ihnen zusagenden Lebensraum.

Auf Wiesen wird man zum einen solche Vögel zu Gesicht bekommen, die auf der Wiese nur nach Nahrung suchen (z. B. Amseln). Zum anderen gibt es viele typische Wiesenbrüter, die im Nestbau sehr geschickt sind. Beispielsweise der Wiesenpieper, der ein gut verstecktes, halbkugeliges, haargepolstertes Nest formt. Weitere Wiesenbrüter sind das Braunkehlchen, die Feldlerche, die Wachtel oder die Ammern.

Um mit dem zentralen Standortfaktor Mahd fertigzuwerden, müssen sich Pflanzen und Tiere gut anpassen. Sie können beispielsweise zwischen den Mahddurchgängen rasch wachsen, schnell blühen und fruchten. Andere entwickeln sich so, dass sie gerade kurz vor der ersten Mahd fruchten oder sich erst im zweiten Wiesen-Hochstand entwickeln.
Wiesen- und Weidenpflanzen waren von Haus aus diesen vom Menschen aufgeworfenen Bedingungen zufällig angepasst und haben sich deshalb gehalten.

Auf Wiesen und Weiden behaupten sich nur jene Pflanzen, die dieser ständigen Mahd gut angepasst sind. Sie müssen sich auch ungeschlechtlich vermehren können, wie z. B. viele Gräser.

Auch viele auffallend blühende Wiesenpflanzen zeigen Anpassungen. Zum Beispiel der Löwenzahn bildet vor der ersten Mahd bereits Samen aus. Andere Pflanzen bilden vorerst nur Blattrosetten oder Kriechtriebe und wachsen erst nach dem ersten Schnitt, um dann aber gleich Samen auszubilden (Wilde Möhre).

Die Herbstzeitlose blüht nach dem zweiten Schnitt und bildet ihre Samen aber erst im nächsten Jahr aus.



</doc>
<doc id="13292" url="https://de.wikipedia.org/wiki?curid=13292" title="Ökosystem">
Ökosystem

Ökosystem ( "oikós" ,Haus‘ und "sýstema" „das Zusammengestellte“ „das Verbundene“) ist ein Fachbegriff der ökologischen Wissenschaften. Ein Ökosystem besteht aus einer Lebensgemeinschaft von Organismen mehrerer Arten (Biozönose) und ihrer unbelebten Umwelt, die als Lebensraum, Habitat oder Biotop bezeichnet wird.

Der Begriff Ökosystem wird in den Naturwissenschaften in einem werturteilsfreien Sinne gebraucht. In Politik und Alltagswelt wird dagegen oftmals so gesprochen, als seien Ökosysteme an sich schützenswert. Wenn dies geschieht, sind nicht Ökosysteme im Allgemeinen gemeint, sondern ganz bestimmte Ökosysteme, die man als nützlich oder in anderer Weise wertvoll ansieht.

Ein Ökosystem ist ein „dynamischer Komplex von Gemeinschaften aus Pflanzen, Tieren und Mikroorganismen sowie deren nicht lebender Umwelt, die als funktionelle Einheit in Wechselwirkung stehen“. Diese gängige Definition wird in der Biodiversitätskonvention verwendet. Sehr ähnlich sind folgende Definitionen von Ökologen:

Wesen und Eigenschaften eines Ökosystems werden dabei von den Ökologen durchaus unterschiedlich aufgefasst. Einige gehen von ihrer tatsächlichen Existenz aus, die von den Forschern nur entdeckt und beschrieben wird (sogenannter ontologischer Ansatz), die meisten betrachten sie aber als durch den Beobachter erst geschaffene Abstraktionen, die für einen bestimmten Zweck angemessen sein müssen, aber in anderem Zusammenhang auch anders definiert und abgegrenzt werden könnten (sogenannter epistemologischer Ansatz). Am Beispiel eines bestimmten Ökosystems, etwa eines borealen Nadelwalds, würden einige Ökologen aussagen, dass das Ökosystem seinen Charakter verändert hat, wenn seine Lebensgemeinschaft sich durch Austausch von Arten stark verändert hat. Für andere wäre es noch dasselbe System, wenn seine allgemeine Gestalt und seine Primärproduktion in etwa gleich geblieben sind (also zum Beispiel die häufigsten Nadelbaumarten durch andere Arten ersetzt werden, die ebenfalls Nadelbäume sind), für andere wäre es erst dann ein anderes System, wenn sich seine funktionalen Komponenten, also Energie- und Stoffflüsse verändern, während sie die Artenzusammensetzung für nicht so wesentlich erachten.

Zusammenhängende großräumige Ökosysteme eines konkreten Raumes werden auch als Ökoregion oder Biom bezeichnet.

Die Grundtypen terrestrischer und aquatischer Ökosysteme, die eine große geografische Ausbreitung aufweisen, bezeichnet man auch als Biome. Zu unterscheiden ist hier die wissenschaftliche und die landläufige Verwendung des Begriffs Ökosystem: Vieles, was populär unter „Ökosystem“ läuft, würde fachlich meist eher als „Biom“ bezeichnet werden. Der Begriff des Bioms geht ursprünglich auf Frederic Edward Clements zurück, erfuhr seine heutige Prägung aber stark durch den Geobotaniker Heinrich Walter. Biome (bzw. als kleinere Einheit: Biogeozönosen) sind empirisch und deskriptiv abgeleitete Ausschnitte der Erdoberfläche, die durch eine bestimmte Lebensgemeinschaft (vor allem: Vegetation) charakterisiert werden können. Der funktionale Aspekt des Öko-„Systems“ kann dabei in den Hintergrund treten. Biome können als Ökosysteme betrachtet werden, müssen es aber nicht; häufig werden sie rein biogeographisch gefasst. Da der Begriff aber außerhalb der Fachöffentlichkeit völlig ungebräuchlich ist, steht in populären Veröffentlichungen dafür meist „Ökosystem“. Biome, hier am Beispiel der Wälder, wären zum Beispiel tropischer Regenwald, gemäßigter Regenwald, borealer Nadelwald, subtropischer Hartlaubwald, Lorbeerwald, gemäßigter (temperater) Laubwald.

Vergleichbare Ökosysteme getrennter Großräume, die zwar nach ihrem Erscheinungsbild ähnlich aufgebaut sind, jedoch nicht in der Artenzusammensetzung, können zu abstrakten Einheiten zusammengefasst werden (z. B. borealer Nadelwald, Wüste, Steppe). Je nach Betrachtung wird in der Fachliteratur u. a. von Pflanzen- oder Vegetationsformationen, Vegetationszonen, Zonobiomen oder Ökozonen gesprochen.

Die Definition von „Ökosystem“ beinhaltet keine Einschränkung auf eine bestimmte Größe (Skalenunabhängigkeit), sei diese Größe nun räumlich oder funktional definiert. Ökosysteme können also unterschiedliche Größen aufweisen. So kann ein sich zersetzender Baumstumpf als Ökosystem verstanden werden, genau so wie der Wald, in dem der Baumstumpf seinen Platz hat. Von vielen Ökologen wird der Begriff Ökosystem jedoch eher in größeren Zusammenhängen verwendet. Als größtes Ökosystem gilt die Biosphäre, die die Gesamtheit aller terrestrischen und aquatischen Ökosysteme einschließt.

Als offene Systeme haben Ökosysteme keine tatsächlichen Systemgrenzen gegenüber der restlichen Biosphäre. Abgegrenzte Ökosysteme sind durch den Untersucher ausgewählte Konstrukte. Die Abgrenzung ist also eine pragmatisch (durch die Fragestellung, das Untersuchungsinteresse oder das zur Verfügung stehende Budget) begründete Entscheidung und entspricht nicht unbedingt einer tatsächlich vorfindlichen Abgrenzung in der Natur. Idealerweise sollten Teilsysteme ausgewählt werden, deren Beziehungsgefüge innerhalb bedeutsamer ist als dasjenige nach außen, die also durch möglichst basale und möglichst wenige Wechselwirkungen mit ihrer Umgebung verknüpft sind. Für die räumliche Abgrenzung und Verortung eines Ökosystems wurde der Begriff Ökotop geprägt, der außerhalb der Landschaftsökologie aber nicht sehr gebräuchlich ist.

Zum besseren Verständnis der Zusammenhänge konstruieren Wissenschaftler gelegentlich stark vereinfachte Ökosysteme im Labor, die nur wenige Arten enthalten; dafür hat sich der Fachbegriff „Mikrokosmen“ eingebürgert.

Der Systembegriff impliziert eine funktionale, über eine bloße morphologische/topographische Beschreibung hinausgehende Betrachtung von kausalen Beziehungen, vor allem in Gestalt von Stoff- und Energieflüssen. Wird ein Naturausschnitt als Ökosystem betrachtet, wird oft ein Verständnis der natürlichen Regelmäßigkeiten und Zusammenhänge durch die Bildung eines Modells der Wirklichkeit angestrebt. Ein solches Modell kann verbal, graphisch oder mathematisch sein. In der Ökosystemforschung werden beispielsweise meist quantitative Modelle angestrebt, die sich in mathematischer Sprache ausdrücken lassen. Einige Aspekte von Ökosystemen lassen sich durch Systeme von Differentialgleichungen ausdrücken. Verwendet werden auch Begriffe der Thermodynamik und statistischen Physik.

Zwischen der Organismengemeinschaft eines Ökosystems existieren vielfache Wechselbeziehungen und Abhängigkeiten. Hierzu zählen beispielsweise die trophischen Beziehungen verschiedener Arten des Ökosystems, wie Stoffaustausch zwischen Primärproduzenten und heterotrophen Gliedern der Nahrungskette in Form von symbiotischer Beziehung (Mykorrhiza), Parasitismus und Saprophilie.

Die Organismen der Biozönose beeinflussen den Stoffkreislauf und werden beeinflusst durch die abiotischen Faktoren. Diese werden in der Botanik als Standortfaktoren bezeichnet.

In der Wissenschaft wird der Ökosystembegriff als, etwas unscharfe, Betrachtungsebene verwendet, die die Ökologie der Lebensgemeinschaften mit Ansätzen der Systemtheorie und Kybernetik verbindet. Der Begriff ist aber eher ein Paradigma, der einen bestimmten Blickwinkel vorgibt, und kann nicht zur Vorhersage konkreter Eigenschaften des Forschungsgegenstands verwendet werden. Einige Ökologen vermeiden den Begriff sogar ganz, weil aus ihrer Sicht die Lebensgemeinschaft und ihre Ökologie zur Bearbeitung hinreichen; so wird er in einem sehr weit verbreiteten Lehrbuch der Ökologie nicht verwendet.

Zur Beschreibung der allgemeinen Eigenschaften von Ökosystemen werden oft folgende Begriffe eingesetzt:

Die Anwendung des Ökosystemsbegriffs auf einen Naturausschnitt allein kann jedoch nicht zur Vorhersage der spezifischen Eigenschaften, Strukturen oder Prozesse des Naturausschnitts verwendet werden. Ökosysteme können z. B. vergleichsweise komplex oder einfach aufgebaut sein, sich eher stabil oder eher instabil verhalten, nahe an einem Gleichgewichtszustand verharren oder sich fernab davon bewegen. Die Betrachtung als Ökosystem gibt lediglich einen gewissen analytischen Blickwinkel vor.

Bei Betrachtung der Organismen eines Ökosystems wird verbreitet von den konkreten Arten abstrahiert und eher ihre funktionelle Rolle im Systemganzen betont. Das bedeutet, dass einzelne Arten bezüglich ihrer Funktion oft in gewissem Sinn als gegeneinander austauschbar betrachtet werden.
Die Organismen können unterteilt werden nach ihrer trophischen Funktion im System als

Unterschieden werden oft zwei Teil-Nahrungsnetze. Die Grundlage beider Nahrungsnetze sind die grünen Pflanzen (oder ggf. die anderen Primärproduzenten). Die Pflanzen werden ganz oder in Teilen von speziellen Konsumenten, den Phytophagen (= "Pflanzenfressern"). verzehrt. Dies geschieht etwa, wenn ein Eichenspinner die Blätter einer Eiche frisst, eine Miesmuschel kleine Algen einstrudelt oder ein Mensch eine Möhre verzehrt. Dies ist das Konsumenten-Nahrungsnetz. Oft fallen aber große Mengen abgestorbenen Pflanzenmaterials an, die ohne Zwischenschaltung von Konsumenten von den Destruenten abgebaut werden. Dadurch kann sich eine sehr große Gemeinschaft an Destruenten aufbauen. Diese Destruenten werden ihrerseits z. B. von bakterien- und pilzfressenden Arten gefressen. Hierher gehören viele Protozoen, Nematoden und Oligochaeten, aber auch Arthropoden wie Hornmilben und Springschwänze. Diese Organismen bilden dann ein Destruenten-Nahrungsnetz.

Verschiedene Substanzen können auf ihrem Weg durch das Ökosystem verfolgt werden. Dies gilt zum Beispiel für das Wasser und auch für einzelne chemische Elemente (C, N, P, etc.). Die Ökosystemforschung untersucht die sich ergebenden Stoffkreisläufe und bildet sie in Stoffflussdiagrammen und komplexen Modellen ab. Gleiches gilt für den Energiefluss. Der Begriff des „Stoffkreislaufs“ deutet an, dass viele Stoffe mehrfach im Ökosystem umgesetzt werden. Allerdings hängt dies von der Art des Ökosystems ab. So ist der Kreislaufanteil in einem Wald für viele Elemente eher hoch. Dies gilt insbesondere für Elemente, die nicht regelmäßig als Gas in die Atmosphäre abgegeben werden. Dagegen ist das Ökosystem eines Flusses vom ständigen, nicht wiederkehrenden Substanzdurchsatz geprägt. Werden geologische Zeiträume betrachtet, so fällt auf, dass erhebliche Stoff- und Elementmengen für (ggf. hunderte) Millionen Jahre aus dem Kreislauf ausscheiden (siehe z. B. Kalkstein, Kohleflöz).

Ökosysteme beeinflussen sich gegenseitig durch Informations-, Substanz- und Energieflüsse. Der Aufbau und die Veränderung von Ökosystemen kann starke Rückwirkungen auf die abiotischen Faktoren haben. Die einzelnen Wirkmechanismen und deren relative Bedeutung sind ein aktives Forschungsfeld. Z. B. beeinflussen marine Ökosysteme durch ihren Stoff- und Energiehaushalt die Atmosphäre und damit auch terrestrische Ökosysteme. Ein Beispiel globaler Wechselbeziehungen ist die Zunahme des Treibhauseffekts und der dadurch verursachte Klimawandel. Um aus dieser Erkenntnis praktischen Nutzen zu ziehen, muss die relative Stärke der Wechselwirkungen bekannt sein.

Seit langer Zeit werden physikalische Begriffe aus der Thermodynamik auf Ökosysteme angewendet (v. a. Entropie, Dissipation), meist in qualitativer Form als Analogien. Seit etwa 20 Jahren beginnt sich ein Forschungszweig zu etablieren, der thermodynamische Begriffe für Modellierung und Vorhersagen über Ökosysteme in vertiefter Form nutzbar machen will.

Schlüsselbegriff für die thermodynamische Interpretation ist die Entropie, insbesondere der zweite Hauptsatz der Thermodynamik. Erklärungsbedürftig ist vor allem, warum das Leben und die Ökosysteme so komplex werden konnten, wenn die Entropie global betrachtet nicht absinken darf. Einzige physikalisch mögliche Erklärung dafür ist, dass die anscheinend geringe Entropie in der Biosphäre durch Erhöhung der Entropie in ihrer (physikalischen) Umwelt mehr als ausgeglichen wird. Die Entwicklung und Erhaltung von Leben findet offensichtlich weitab des thermodynamischen Gleichgewichts statt. Dies ist nur in einer Umgebung möglich, die ebenfalls fernab des Gleichgewichts ist. Ein Ökosystem benötigt also einen thermodynamischen Gradienten als Antrieb, es muss einerseits (energetische oder stoffliche) Ressourcen von außen erhalten, und auf der anderen Seite „Abfälle“ mit einer höheren Entropie als diejenige der Ressourcen abgeben. Für die Erde als Ganzes ist dieser Gradient die Differenz zwischen der energiereichen Sonneneinstrahlung und dem kalten Weltall (in das Wärme abgestrahlt werden kann). Ein Ökosystem kann also thermodynamisch nur bestehen, wenn es „Ressourcen“ schneller zu „Abfällen“ verwandelt, als es ein vergleichbares unbelebtes System tun würde.

Die Entropie-Produktion eines Ökosystems ist nicht direkt messbar, da wichtige Teilgrößen (v. a. die chemische Energie der lebenden Biomasse) nicht messbar (bzw.: noch nicht einmal befriedigend definierbar) sind. Ökologen versuchen verschiedene Größen zu definieren, um dieses Problem zu umgehen. Mit diesen Größen sollen allgemeine Aussagen über Ökosysteme möglich werden, die Vorhersagen zur Struktur und Entwicklung von Ökosystemen zulassen. Wichtige Ansätze sind: Ascendancy (in etwa: „Aufstieg“), Emergie (mit einem „m“ in der Mitte), Exergie und Öko-Exergie.

In der Ökologie ist es bis heute umstritten, wodurch die Dynamik und Struktur von Ökosystemen letztlich kontrolliert wird. Traditionell gibt es zwei Grundannahmen:
Alternativen oder Varianten zu diesen Grundmodellen existieren in größerer Zahl.

Als natürliche Systeme besitzen Ökosysteme eine räumliche und eine zeitliche Dimension. Auf der zeitlichen Ebene versucht man bei der Erforschung die ablaufenden Veränderungen, das heißt die Dynamik des Systems, zu verstehen. Systeme können dabei mehr oder weniger unverändert bleiben, oder sie unterliegen (gerichteten oder zufälligen) Veränderungen. Da lebende Organismen auf Veränderungen reagieren können, können in Ökosystemen, anders als in unbelebten Systemen, selbstregulierende Prozesse ablaufen, durch die die Reaktion des Systems auf Veränderungen unter Umständen nur schwer vorhersagbar ist. Für die holistisch-organismische Denkschule in der Forschung sind diese selbstregulierenden Prozesse von alles entscheidender Bedeutung, für sie ist ein Ökosystem daher analog zu einem lebenden Organismus. Die stärker reduktionistisch ausgerichtete Hauptströmung der Wissenschaft erkennt die sich bei der Entwicklung der Systeme ergebenden Muster und Regelmäßigkeiten an, hält aber die starke Betonung der Konstanz, die sich aus der Organismus-Metapher ergibt, für nicht angemessen. Völlig chaotisch und ungeregelt ablaufende Veränderungen zu erforschen ist allerdings zwar möglich, aber wissenschaftlich wenig ergiebig, da man so gewonnene Erkenntnisse kaum auf irgendetwas außerhalb des untersuchten Systems selbst verallgemeinern könnte. Die Ökosystemforschung konzentriert sich daher meist auf mehr oder weniger konstante Systeme oder zumindest auf solche, deren Veränderung auf erklärende Faktoren zurückgeführt werden kann. Ausgangspunkt der Forschung sind daher (wie generell in der Wissenschaft) Muster und Regelmäßigkeiten in der Natur selbst.

In zahlreichen untersuchten Ökosystemen beobachtet man bei Untersuchungen über einen längeren Zeitraum keine wesentlichen Veränderungen, sie sind zeitlich stabil. Die Stabilität ist trivial, wenn sich die Umweltfaktoren und die abiotischen Komponenten des Systems nicht verändert haben. Interessanter ist es, wenn ein System auch bei Veränderung äußerer Faktoren seine Stabilität erhalten kann. Die Erforschung dieser Zusammenhänge wurde lange Zeit durch die Mehrdeutigkeit des Stabilitätsbegriffs behindert. Grimm und Wissel fanden z. B. in einer Literaturstudie 163 verschiedene Definitionen von Stabilität, die sich auf 70 Konzepte bezogen. Heute wird (nach Pimm 1984) meist unterschieden: "Persistenz" (man beobachtet wenig Veränderungen bei Beobachtungen über lange Zeit), "Resilienz" (Das System kehrt nach Störungen wieder in seinen Ausgangszustand zurück), "Resistenz" (Das System bleibt bei Störungen lange unverändert).

Stabilität und Konstanz in Ökosystemen sind dabei von der betrachteten räumlichen und zeitlichen Skala abhängig. Die Populationsgröße einer Art mag z. B. Jahr für Jahr schwanken, aber auf längere Sicht in gleicher Höhe bleiben. Stabilität und Stabilitätsbedingungen von Ökosystemen, vor allem der Zusammenhang zwischen Stabilität und Komplexität, sind aktive Forschungsfelder der ökologischen Wissenschaften. Die traditionellen Ansichten, das Ökosysteme in der Regel im ökologischen Gleichgewicht seien und ihre Stabilität mit Erhöhung der Artenzahl oder der Biodiversität ansteigt, werden seit etwa 30 Jahren zunehmend in Zweifel gezogen.

Bei der zeitlichen Entwicklung eines Ökosystems ist der Begriff der Störung ein Schlüsselbegriff. Ohne Störungen unterliegt ein System ausschließlich endogener Dynamik, Veränderungen können z. B. durch Wechselwirkungen der beteiligten Arten untereinander ablaufen. Wichtig ist in diesem Zusammenhang, dass in der Ökologie der Begriff der Störung völlig werturteilsfrei genutzt wird. Eine Störung ist nichts "per se" Schlechtes; oft lassen sich bestimmte Ökosysteme sogar nur durch regelmäßige Störungen erhalten (siehe unten).

Die Größe einer Population kann durch das Beziehungsgefüge auf einer bestimmten Höhe eingeregelt werden, oder, z. B. durch zeitlich verzögerte Reaktionen, können zyklische Schwankungen auftreten (für Säugetierpopulationen z. B.). Gerichtete, dauerhafte Veränderungen des Systems bezeichnet man als Sukzession. Eine Störung ist ein von außerhalb dieses internen Beziehungsgefüges einwirkender, das System verändernder Faktor. Häufig wird zwischen seltenen und großen Störungen (Katastrophen) und kleineren und wiederkehrenden Störungen unterschieden. Der Störungsbegriff ist dabei ebenfalls skalenabhängig, z. B. kann das Abfressen durch ein weidendes Tier für eine einzelne Pflanze als Störung, für das Ökosystem Wiese aber als konstituierender und systemimmanenter Faktor gewertet werden. Einen Versuch zur absoluten Definition von Störungen haben White und Pickett unternommen, eine Definition für klimatische Extremereignisse als Störung liefert Smith. Das zeitliche Muster der Störungen oder "Störungsregime" ist ein prägender Faktor für viele Ökosysteme, wiederkehrende Störungen wie Mahd oder Beweidung im Grünland, Überflutungen im Auwald, aber auch katastrophische Störungen wie Waldbrand oder Sturmwurf in Waldökosystemen können Struktur und Zusammensetzung eines Ökosystems entscheidend prägen.

„Ökosystem“ im biologisch-wissenschaftlichen Sinn ist ein wertfreier Begriff. Die Rede von der "Existenz" eines bestimmten Ökosystems oder der Stabilität eines seiner Zustände impliziert daher auf dieser begrifflichen Ebene nicht von sich aus eine positive Wertschätzung; Bemühungen zum Schutz von Ökosystemen sind gesondert zu begründen. Naturwissenschaft kann solche Begründungen nicht liefern, da sie gehalten ist, immer wertfreie Beschreibungen und Erklärungen zu liefern. So sind die nach Zerstörung eines hoch entwickelten Ökosystems auftretenden Zustände genauso als Ökosystem anzusprechen wie der Ausgangszustand, solange noch irgendeine Form von Leben in ihnen auftritt. Einen Wert besitzt ein Ökosystem nur dann, wenn sie ihm durch eine Wertentscheidung von Menschen zugewiesen worden ist. Die Wertentscheidung steht dabei außerhalb der Naturwissenschaft. Sie kann durch wissenschaftliche Argumente gestützt, aber nicht aus der Wissenschaft oder aus der wissenschaftlich beschriebenen Natur abgeleitet werden (vgl. hierzu: Naturalistischer Fehlschluss, Humes Gesetz).

Werte, die einem Ökosystem zugeschrieben werden, können sich auf seine funktionale Ebene beziehen, man spricht dann oft von Ökosystemfunktionen und Ökosystemdienstleistungen (engl. "ecosystem services"). Beispielsweise könnte die Erhaltung eines Waldes mit seiner Funktion als Kohlenstoff-Speicher zur Verhütung des Treibhauseffekts, mit seiner erosions­verhütenden Rolle an Steilhängen oder mit seiner positiven Rolle für die Neubildung von nutzbarem Grundwasser, und nicht zuletzt durch das hier geerntete Holz oder das erlegte Wild begründet werden.

Ökosystemdienstleistungen sind häufig ersetzbar. So ist denkbar, dass mit entsprechendem technischen und finanziellen Aufwand CO in tiefe Gesteinsschichten eingepresst und damit der Atmosphäre entzogen wird. Erosionsschutz könnte auch durch Grasland, Grundwasserschutz durch den Einsatz technischer Filter oder aufbereitetes Oberflächenwasser substituiert werden. Umweltökonomische Studien zeigen, dass die Kosten einer technischen Substitution oft jedoch so hoch sind, dass natürliche oder naturnahe Ökosysteme schon aus wirtschaftlichen Gründen nicht leichtfertig degradiert werden sollten. Zu bedenken ist weiterhin, dass die Nutzbarkeit eines Ökosystems auch durch stete Einflüsse graduell immer mehr herabgesetzt werden kann, was bei einer kurzzeitigen Betrachtung nicht erkennbar ist. Innerhalb der Umweltökonomie hat sich zum Studium der Ökosystem-Dienstleistungen ein Spezialgebiet herausgebildet (siehe TEEB-Studie).

Der Schutz natürlicher Ökosysteme beruht zu einem großen Teil allerdings nicht auf dieser rein funktionalen Ebene. Wenn Menschen die Artenvielfalt bestimmter Ökosysteme erhalten wollen, tun sie dies in der Regel nicht aus funktionalen Gründen (obwohl sich Menschen finden, die dies z. B. mit der Erhaltung ungewöhnlicher Naturstoffe als Basis für neue Medikamente begründen wollen). Vielmehr wird hier der Vielgestaltigkeit und Komplexität der Natur ein eigener Wert zugesprochen. Umweltökonomen tun sich etwas schwer mit dieser Begründung, weil sie sich nicht in das universelle Wertmedium der Ökonomie, d. h. Geld, umrechnen lässt. Hilfsweise wird versucht, den Wert dadurch zu fassen, dass in Befragungen abgefragt wird, wieviel Geld die Befragten zur Rettung natürlicher Ökosysteme abzugeben bereit wären.

Die menschlichen Bemühungen, Ökosysteme zur Erhaltung der Natur selbst, z. B. der Artenvielfalt, zu schützen, werden als Naturschutz zusammengefasst. Die meisten Bemühungen, die auf die funktionale Ebene, d. h. direkte Nutzbarkeit und Ökosystemdienstleistungen abzielen, sind die Domäne des Umweltschutzes.

Die verschiedenen Begründungen und Werte, die zur Erhaltung von Ökosystemen herangezogen werden, können untereinander in Konflikt geraten. Genutzte Ökosysteme werden durch die Nutzung verändert und sind damit nicht mehr völlig autonom und natürlich. Heute ist durch die globalen Emissionen aus technischen Prozessen davon auszugehen, dass es praktisch keine vollkommen unbeeinflussten Naturlandschaften mehr gibt. Die Ökologie teilt die Ökosysteme in diesem Zusammenhang nach dem Grad der menschlichen (anthropogenen) Beeinflussung in sogenannte Hemerobie-Grade ein. Je geringer der Grad, desto geringer der anthropogene Einfluss. Oft wird bei der Wertung zwischen der völligen Zerstörung und der Degradation von Ökosystemen aufgrund starker menschlicher Einflüsse unterschieden. Die intaktesten Ökosystem-Komplexe liegen in den "oligohemeroben" (naturnahen, gering beeinflussten) Wildnis­gebieten der Erde. Die Artenvielfalt genutzter Ökosysteme nimmt häufig mit steigender Hemerobie ab, sie kann aber auch zunehmen. Für terrestrische Ökosysteme und deren Biodiversität stellt in Mitteleuropa etwa die Intensivierung der landwirtschaftlichen Nutzung von Gunstflächen bei gleichzeitiger Nutzungsaufgabe von marginalen Flächen ein großes Problem dar. Auf traditionelle Nutzungsformen beruhende Kulturlandschaften wie Heiden und Magerrasen versucht der Naturschutz aufgrund ihrer Artenvielfalt zu erhalten. Er schränkt damit ihre Nutzbarkeit für den Menschen ein. Verschärft treten diese Konflikte in ärmeren Staaten mit weiträumigen und artenreichen Ökosystemen, die aber kaum nutzbar sind, auf. Letztes Argument für ihre Erhaltung ist dann oft ihre Bedeutung für den Tourismus aus den reichen Staaten. Zunehmend wird aber auch über direkte Transferzahlungen der reichen an die armen Nationen geredet.

Über 15.000 Wissenschaftler haben 2017 eine eindringliche veröffentlicht, die belegt, dass viele Ökosystemdienstleistungen erheblich gefährdet sind und die Chancen ihres Erhaltes derzeit negativ eingeschätzt werden.

Die Ansicht, dass Lebewesen und Lebensräume gemeinsam betrachtet werden müssen, lässt sich in der Wissenschaft bis ins 19. Jahrhundert zurückverfolgen, als John Scott Haldane davon schrieb, dass „die Teile eines Organismus und seine Umgebung ein System formen“. 1928 sprach der Leipziger Biologe und Limnologe Richard Woltereck von „Ökologischen Gestalt-Systemen“.

1935 schuf der britischen Biologe und Geobotaniker Arthur George Tansley unabhängig davon den heutigen Begriff „ecosystem“ (Ökosystem). Seine Definition von „ecosystem“: „das gesamte System (im physikalischen Sinne) unter Einschluss nicht nur des Komplexes der Organismen, sondern auch des ganzen Komplexes der physikalischen Faktoren, die das formen, was wir die Umwelt nennen – die Habitatfaktoren im weitesten Sinn.“ „(In den Systemen)… gibt es ständigen Austausch in verschiedenster Form innerhalb des Systems, nicht nur zwischen den Organismen, sondern zwischen organischem und anorganischem Bereich.“ ist in dieser Form bis heute uneingeschränkt weiter gültig. Sein Systembegriff ist der eines teilweise Beobachter-konstruierten, gedanklichen Isolats ("mental isolate").

Unmittelbarer Anlass für Tansleys Formulierung war eine Serie von Artikeln des südafrikanischen Ökologen John Phillips über das "Wesen" der biotischen Gemeinschaft ("biotic community"). Phillips war angeregt durch den Holismus Jan Christiaan Smuts'. Phillips trat in diesen Artikeln für eine „starke“ Interpretation der biotischen Gemeinschaft im Sinne des von Frederic Edward Clements geprägten Konzepts eines Komplexorganismus ein. Tansley wendet sich mit seinem eher mechanistischen Vorschlag in scharfer Form gegen die Verwendung einer im wissenschaftstheoretischen Sinne idealistischen, empirisch nicht zugänglichen Organismus-Metapher, zu der sein Begriff ausdrücklich als Alternative dienen sollte.

Die Entwicklung eines physikalisch geprägten Ökosystembegriffs hat wichtige Grundlagen in der europäischen und nordamerikanischen Gewässerökologie, insbesondere der Limnologie. 1877 hatte der Kieler Zoologieprofessor Karl August Möbius den Begriff der Biozönose für die organismische Vergesellschaftung in Austern­bänken geprägt. Stephen Alfred Forbes, ein Limnologe aus Illinois, sprach 1887 Jahren Seen als „organische Systeme“ mit zyklischen Stoffumsätzen ("matter cycling") an, in denen übergeordnete Steuerungsmechanismen ein Gleichgewicht aufrechterhielten. Während die Arbeiten Forbes' außerhalb des US-amerikanischen Mittelwestens wenig rezipiert wurden, baute August Thienemann ab 1891 in Plön die Hydrobiologische Abteilung der Kaiser-Wilhelm-Gesellschaft auf. Von dort aus verbreitete Thienemann seine Ansicht von Seen als biotischen Systemen, die sich aus der Interaktion von Lebewesen und Umwelt ergeben („Lebensraum ("Milieu") und Lebensgemeinschaft ("Biocoenose") als eine feste, organische Einheit“, 1916). Thienemann verwendet dabei den mit „Ökosystem“ weitgehend identischen (aber der holistischen, organismischen Sichtweise von Clements und Phillips näher stehenden) Begriff des Holocoens, den der Entomologe Karl Friedrichs 1927 eingeführt hatte.

Ab den 1920er Jahren begannen zunehmend genauere Analysen von Nahrungsketten und den darin ablaufenden Stoff- und Energieumsätzen aufzutauchen (z. B. Charles Sutherland Elton, E.V. Borutsky, Chancey Juday). Diese Studien führten 1939 zu Thiemanns Unterscheidung von Produzenten, Konsumenten (Herbivore und Karnivore) und Reduzenten. Die erste systematische empirische Anwendung erfuhr Tansleys Ökosystemkonzept Ende der 1930er Jahre durch den in Minnesota arbeitenden Limnologen Raymond Laurel Lindeman, der die erste Komplettdarstellung der Energieumsätze in einem (See-) Ökosystem vorlegte. Eine weitere einflussreiche Forschungsrichtung war die vom Russen Vladimir Verdadsky begründete Biogeochemie, die sich den Stoffaustausch zwischen Lebewesen und Umwelt als Austausch innerhalb eines chemischen Systems vorstellte. 1944 schuf der sowjetische Biologe Sukachev daraus seinen Begriff der „Biogeozönose“, der im osteuropäischen Raum lange Zeit anstelle von Ökosystem verwendet wurde (von geographisch geprägten Landschaftsökologen teilweise bis heute). In Zusammenarbeit mit Lindemann verbreitete George Evelyn Hutchinson den russischen Ansatz.

Zum internationalen Durchbruch verhalf dem Ökosystem-Konzept der amerikanische Ökologe Eugene P. Odum. Odum veröffentlichte 1953 sein kurzes Lehrbuch „Principles of Ecology“. Dessen erste Seiten entfalten implizit jenes Forschungsprogramm, dem die Ökosystemforschung bis Ende der 1960er Jahre weitgehend folgte.

Ursprünglich aus dem Englischen entlehnt wird der Begriff "ecosystem" auch auf den Bereich der Wirtschaft übertragen und steht dann für die Gesamtheit der Akteure innerhalb einer Branche ("business ecosystem"), im Deutschen spricht man auch von "Wirtschaftsökosystemen" oder "Unternehmensökosystemen". Im Speziellen wird mit Blick auf die Gründerszene bzw. die Förderung des Unternehmertums auch von "Gründerökosystemen" und "Start-up-Ökosystemen" gesprochen. Diesen Begriff versucht das RKW zu etablieren. Das bekannteste Beispiel eines ganzheitlichen Startup-Ökosystems ist das Silicon Valley, in Europa auch die urbanen Viertel von Berlin.

Mit dem Begriff "digitales Ökosystem" wird im übertragenen Sinne in der Informationstechnik eine Soft- und Hardware-Architektur bezeichnet, welche auf jeweils ganz eigenen Geräten, Systemen und Zugangsvoraussetzungen beruht und damit entsprechendes Zubehör voraussetzt und hervorbringt. Ein Beispiel für ein geschlossenes "digitales Ökosystem" sind die Produkte des Unternehmens Apple.
Auch in der Astrobiologie wird der Begriff für möglicherweise existierende extraterrestrische („außerirdische“) Ökosysteme auf Exoplaneten und Exomonden verwendet.





</doc>
<doc id="13296" url="https://de.wikipedia.org/wiki?curid=13296" title="Wilhelm Pramme">
Wilhelm Pramme

Wilhelm Pramme (* 1. März 1898 in Halberstadt; † 12. Februar 1965 in Wernigerode) war ein deutscher Maler. Er gehört zu den Harz<nowiki></nowiki>malern und war Mitglied der "Künstlerkolonie Wernigerode". Diese war nach dem Zweiten Weltkrieg von verschiedenen bildenden Künstlern, die es am Ende des Krieges nach Wernigerode verschlagen hatte, innerhalb des "Kulturbundes zur demokratischen Erneuerung Deutschlands" gegründet worden.

Pramme wurde 1898 als Sohn eines Zigarrenmachers in Halberstadt geboren und erlernte als Jugendlicher nach dem frühen Tod seines Vaters das Handwerk der Lithografie - ein Beruf, in dem er zeitweilig auch in Zwickau tätig war, bevor er die Arbeit im Jahr 1916 aufgrund einer schweren Tuberkulose-Erkrankung aufgeben musste und damit begann, seinen Lebensunterhalt mit dem Verkauf seiner Bilder sowie dem Theaterschauspiel zu bestreiten. Als Schüler unterhielt Pramme enge Freundschaften mit dem späteren Maler Walter Gemm sowie dem späteren Schriftsteller Bert Brennecke, denen er zeitlebens verbunden blieb.

Viele seiner Werke entstanden während seiner Reisen durch Deutschland in Begleitung von Walter Gemm sowie während seiner Reisen durch zahlreiche Länder der Welt (darunter Italien, Griechenland, Ägypten, Indonesien, Indien und Afghanistan), die er ebenfalls durch den Verkauf von Bildern finanzierte. Das Hauptmotiv des Malers blieb jedoch stets der Harz, insbesondere die Gegend um seine Wahlheimat Wernigerode. Für den Harz setzte sich der Maler auch außerhalb seiner künstlerischen Tätigkeit ein: Pramme hielt Diavorträge zum Thema Naturschutz, führte Wandergruppen durch den Harz und beteiligte sich an der Kartierung von Pflanzenvorkommen. 

Wilhelm Pramme war zweimal verheiratet, die zweite Ehe ging er 1951 mit Mimi Bruns ein. Im Februar 1965 starb Pramme an einem Herzinfarkt, nur ein Jahr später verstarb auch seine zweite Ehefrau, die in ihrem Testament den Großteil seines künstlerischen Nachlasses dem in Wernigerode ansässigen Harzmuseum stiftete. Ein Gedenkstein für Pramme, der im Jahr 1982 durch eine private Initiative gestiftet wurde, befindet sich - für Besucher schwer erreichbar - bei der Landmannsklippe im Nationalpark Harz bei N51° 47.127 E10° 40.973.



</doc>
<doc id="13297" url="https://de.wikipedia.org/wiki?curid=13297" title="Adolf Rettelbusch">
Adolf Rettelbusch

Johann Adolf Rettelbusch (* 15. Dezember 1858 in Kammerforst; † 8. Januar 1934 in Magdeburg) war ein deutscher Maler. Er trägt den Beinamen "Brockenmaler".

Rettelbusch wurde als achtes Kind einer Gastwirtsfamilie geboren. Er besuchte von 1865 bis 1873 die Dorfschule, dann bis 1878 die Realschule in Nordhausen und ging anschließend nach Weimar an die Großherzogliche Kunstschule. Zu seinen Lehrern gehörten Theodor Hagen und Alexander Struys. Er widmete sich vor allem der Landschaftsmalerei. Nachdem er aus finanziellen Gründen das Studium abbrechen musste, legte er in Berlin 1880 und 1881 bei Karl Gussow an der Königlichen Akademie der Künste ein Zeichenlehrerexamen ab.

Trotzdem blieb er arbeitslos. Für zwei Jahre kehrte er nach Kammerforst zurück und versah Gelegenheitsarbeiten. Ab 1883 absolvierte er an der Unterrichtsanstalt des Kunstgewerbemuseums Berlin, finanziert durch ein großzügiges Stipendium, bei Max Friedrich Koch, Ernst Ewald und Ernst Schaller eine Ausbildung in Landschafts-, Porträt- und dekorativer Malerei. Er erhielt diverse Auszeichnungen.
1886 und 1887 unternahm Adolf Rettelbusch eine Studienreise nach Italien. Seine dort gefertigten Zeichnungen und Aquarelle führten zu einem Angebot des Preuß. Ministeriums für Handel und Gewerbe an Rettelbusch, eine Stelle als Lehrer für dekorative und allgemeine Malerei an der Kunstgewerbe- und Handwerkerschule Magdeburg zu übernehmen. Rettelbusch willigte ein und wurde 1887 Lehrer und später stellvertretender Rektor der mit reformierten Studienprogramm unter Eduard Spieß neu eröffneten Schule. Kurzzeitig hatte Rettelbusch im Jahr 1892 kommissarisch die Leitung der Schule inne. 1906 wurde Rettelbusch zum Professor ernannt; er blieb bis zu seiner Emeritierung 1924 an dieser Schule.

Rettelbusch engagierte sich im kulturellen Leben der Stadt Magdeburg. Er gründete 1893 den Künstlerverein St. Lukas, den er über lange Jahre leitete. 1912 gehörte er zu den Mitbegründern des Künstlervereins Börde. Lange arbeitete er auch im Vorstand des Kunstvereins Magdeburg. Außerdem war er Mitglied der Freimaurerloge „Ferdinand zur Glückseligkeit“.

Um 1925 hatte sich Adolf Rettelbusch den Ruf als wichtigster Landschaftsmaler Mitteldeutschlands erworben. Ein besonderer Schwerpunkt seiner Arbeit galt hierbei, seit einer Brockenbesteigung 1887, dem Harz und dem Brocken. Dies trug ihm den Namen "Brockenmaler" ein.

Rettelbusch schuf jedoch auch diverse Innenraumgestaltungen in Kirchen, Gaststätten, Schlössern und Gutshäusern in Magdeburg und Umgebung. Er beherrschte die gesamte Palette der Maltechniken. Während er anfangs vor allem die Aquarelltechnik nutzte, wandte er sich dann der Temperamalerei und später der Ölkreidezeichnung zu. Ab etwa 1914 begann er mit der Pastellmalerei und entwickelte sich zu einem der besten Pastellmaler seiner Zeit.

Er schuf Porträts, Pflanzenstudien, Bilder aus der Landwirtschaft, aber auch aus der Industrie (Krupp-Gruson-Werk). Viele Werke entstanden auf Auslandsreisen (z. B. Italien 1886/87, Spanien 1893, auf der Nordlandfahrt 1909, in den Alpen 1914). Weitere Werke behandeln den Ersten Weltkrieg.

1928 wurde er zum Ehrenbürger von Kammerforst ernannt. Die Stadt Magdeburg benannte ihm zu Ehren eine Straße (Rettelbuschweg).




</doc>
<doc id="13299" url="https://de.wikipedia.org/wiki?curid=13299" title="Licht">
Licht

Licht ist eine Form der elektromagnetischen Strahlung. Im engeren Sinne sind vom gesamten elektromagnetischen Spektrum nur die Anteile gemeint, die für das menschliche Auge sichtbar sind. Im weiteren Sinne werden auch elektromagnetische Wellen kürzerer Wellenlänge (Ultraviolett) und größerer Wellenlänge (Infrarot) dazu gezählt. 

Die physikalischen Eigenschaften des Lichts werden durch verschiedene Modelle beschrieben: In der Strahlenoptik wird die geradlinige Ausbreitung des Lichts durch „Lichtstrahlen“ veranschaulicht; in der Wellenoptik wird die Wellennatur des Lichts betont, wodurch auch Beugungs- und Interferenzerscheinungen erklärt werden können. In der Quantenphysik schließlich wird das Licht als ein Strom von Quantenobjekten, den Photonen, beschrieben. Eine vollständige Beschreibung des Lichts bietet die Quantenelektrodynamik. Im Vakuum breitet sich Licht mit der konstanten Lichtgeschwindigkeit von 299792458 m/s aus. Trifft Licht auf Materie, so kann es gestreut, reflektiert, gebrochen und verlangsamt oder absorbiert werden.

Licht ist der für das menschliche Auge adäquate Sinnesreiz. Dabei wird die Intensität des Lichts als Helligkeit wahrgenommen, die spektrale Zusammensetzung als Farbe.

Bis weit in die Neuzeit hinein war weitgehend unklar, was Licht tatsächlich ist. Man glaubte teilweise, dass die Helligkeit den Raum ohne Zeitverzögerung ausfüllt, und dass „Strahlen“ von den Augen ausgehen und die Umwelt beim Sehvorgang abtasten. Es gab jedoch auch schon seit der Antike Vorstellungen, nach denen das Licht von der Lichtquelle mit endlicher Geschwindigkeit ausgesendet wird.

Galileo Galilei versuchte als einer der ersten, die Ausbreitungsgeschwindigkeit des Lichts ernsthaft zu messen, jedoch ohne Erfolg. Dafür waren die ihm zur Verfügung stehenden Mittel viel zu grob. Dies gelang erst Ole Römer anhand von Beobachtungsdaten der Jupitermonde 1676/78. Zwar betrug die Abweichung seines Messwerts vom tatsächlichen Wert (ca. 3 · 10 m/s) rund 30 %. Die eigentliche Leistung Römers bestand jedoch darin, nachzuweisen, dass sich das Licht mit endlicher Geschwindigkeit ausbreitet. Römers Messwert wurde im Laufe der folgenden 200 Jahre durch immer raffiniertere Verfahren (vor allem durch Hippolyte Fizeau und Léon Foucault) mehr und mehr präzisiert.

Die Natur des Lichts blieb jedoch weiter ungeklärt. Im 17. Jahrhundert versuchte Isaac Newton mit seiner Korpuskeltheorie, die Ausbreitung des Lichts durch die Bewegung von kleinen Teilchen zu erklären. Damit konnte man zwar die Reflexion verstehen, nicht jedoch manche andere optische Phänomene, wie die Beugung, bei der es sich eindeutig um ein Wellenphänomen handelt. Zur gleichen Zeit begründeten Christiaan Huygens und andere die Wellentheorie des Lichts, die sich aber erst Anfang des 19. Jahrhunderts nach den Doppelspalt­experimenten von Thomas Young mehr und mehr durchsetzte.

Michael Faraday erbrachte 1846 als erster den Nachweis, dass Licht und Magnetismus zwei miteinander verbundene physikalische Phänomene sind. Er veröffentlichte den von ihm gefundenen magnetooptischen Effekt, der heute als Faraday-Effekt bezeichnet wird, unter dem Titel "Über die Magnetisierung des Lichts und die Belichtung der Magnetkraftlinien". 

James Clerk Maxwell formulierte 1864 die noch heute gültigen Grundgleichungen der Elektrodynamik und erkannte, dass dadurch die Existenz freier elektromagnetischer Wellen vorhergesagt wurde. Da deren vorhergesagte Ausbreitungsgeschwindigkeit mit der bekannten Lichtgeschwindigkeit übereinstimmte, schloss er, dass das Licht wohl eine elektromagnetische Welle sei. Er vermutete (wie damals nahezu alle Physiker), dass diese Welle nicht im leeren Raum existieren könne, sondern ein Ausbreitungsmedium brauche. Dieses Medium, das das gesamte Weltall ausfüllen müsste, wurde als Äther bezeichnet.

Mit der darauf aufbauenden "elektromagnetischen Lichttheorie" schienen im ausgehenden 19. Jahrhundert beinahe alle Fragen zum Licht geklärt. Allerdings ließ sich einerseits der postulierte Äther nicht nachweisen (siehe Michelson-Morley-Experiment), was letztendlich das Tor zur speziellen Relativitätstheorie aufstieß. Andererseits schien unter anderem der Photoeffekt der Wellennatur des Lichts zu widersprechen. So entstand eine radikal neue Sichtweise des Lichts, die durch die Quantenhypothese von Max Planck und Albert Einstein begründet wurde. Kernpunkt dieser Hypothese ist der Welle-Teilchen-Dualismus, der das Licht nun nicht mehr ausschließlich als Welle oder ausschließlich als Teilchen beschreibt, sondern als Quantenobjekt. Als solches vereint es Eigenschaften von Welle und von Teilchen, ohne das eine oder das andere zu sein und entzieht sich somit unserer konkreten Anschauung. Daraus entstand Anfang des 20. Jahrhunderts die Quantenphysik und später die Quantenelektrodynamik, die bis heute unser Verständnis von der Natur des Lichts darstellt.

Im Folgenden werden die wichtigsten Modelle zur Beschreibung des Lichts vorgestellt. Wie alle Modelle in der Physik sind auch die hier aufgeführten in ihrem Geltungsbereich beschränkt. Eine nach unserem heutigen Wissen vollständige Beschreibung des Phänomens „Licht“ kann nur die Quantenelektrodynamik liefern.

In der klassischen Elektrodynamik wird Licht als eine hochfrequente elektromagnetische Welle aufgefasst. Im engeren Sinne ist „Licht“ nur der für das menschliche Auge sichtbare Teil des elektromagnetischen Spektrums, also Wellenlängen zwischen ca. 380 und 780 nm. Es ist eine Transversalwelle, wobei die Amplitude durch den Vektor des elektrischen Feldes oder des Magnetfeldes gegeben ist. Die Ausbreitungsrichtung verläuft senkrecht dazu. Die Richtung des formula_1-Feld-Vektors oder formula_2-Feld-Vektors wird Polarisationsrichtung genannt. Bei unpolarisiertem Licht setzt sich das Strahlungsfeld aus Wellen aller Polarisationsrichtungen zusammen. Wie alle elektromagnetischen Wellen breitet sich auch sichtbares Licht im Vakuum mit der Lichtgeschwindigkeit von formula_3 aus.

Die Wellengleichung dieser elektromagnetischen Welle kann aus den Maxwell-Gleichungen hergeleitet werden. Daraus ergibt sich ein einfacher Zusammenhang zwischen der Lichtgeschwindigkeit, der magnetischen Feldkonstante formula_4 und der elektrischen Feldkonstante formula_5:
im Vakuum,
im Medium.

Offensichtlich hängt die Lichtgeschwindigkeit – genauer: die Phasengeschwindigkeit des Lichts – in Medien von deren Materialeigenschaften ab. Diese können im Brechungsindex formula_8 zusammengefasst werden. Im Allgemeinen ist er frequenzabhängig, was man als Dispersion bezeichnet. Darauf beruht unter anderem die Fähigkeit eines Prismas, das Licht in seine spektralen Anteile zu zerlegen. Kurzwelliges blaues Licht wird bei normaler Dispersion stärker gebrochen als langwelliges rotes Licht.

Die Strahlenoptik (auch geometrische Optik) macht sich die Näherung zunutze, dass die Ausbreitung des Lichts durch gerade „Strahlen“ veranschaulicht werden kann. Diese Näherung ist vor allem dann gerechtfertigt, wenn die Abmessungen der Versuchsanordnung groß gegenüber der Wellenlänge des Lichts sind. Dann können sämtliche Beugungsphänomene vernachlässigt werden. Das Bindeglied zwischen Wellenoptik und Strahlenoptik ist der Wellenvektor, dessen Richtung mit der Richtung des Lichtstrahls übereinstimmt. Die Strahlenoptik ist besonders gut geeignet, Phänomene wie Licht und Schatten, Reflexion oder Brechung zu beschreiben. Daher kann mit ihr die Funktion vieler optischer Geräte (Lochkamera, Lupe, Teleskop, Mikroskop) erklärt werden. Insbesondere sind die Abbildungsgesetze auch die Grundlage für das Verständnis des Brechapparats im menschlichen Auge.

Prinzipien von Strahlen

An spiegelnden Oberflächen (wie an blanken Metallen) wird Licht nach dem Reflexionsgesetz reflektiert. Der einfallende und der ausfallende Strahl sowie das Lot auf der reflektierenden Fläche liegen in einer Ebene. Einfallswinkel und Ausfallswinkel sind einander gleich. Das Verhältnis der reflektierten Lichtintensität zur einfallenden Lichtintensität wird als Reflexionsgrad bezeichnet und ist material- und wellenlängenabhängig.

Licht wird an der Grenzfläche zwischen zwei Medien unterschiedlicher optischer Dichte gebrochen, d. h., ein Strahl ändert an dieser Grenzfläche seine Richtung. (Der Vollständigkeit halber sei gesagt, dass an einer solchen Grenzfläche stets auch die Reflexion mehr oder weniger stark auftritt.) Das Brechungsgesetz von Snellius besagt:

Der einfallende und der gebrochene Strahl sowie das Lot auf der Grenzfläche liegen in einer Ebene. Dabei ist der Winkel zwischen Lot und Lichtstrahl in dem Medium kleiner, das den höheren Brechungsindex hat.

Die genauen Winkel formula_9 können durch die Brechungsindizes formula_10 der beteiligten Medien berechnet werden:

Wenn der einfallende Strahl aus dem optisch dichteren Medium unter einem flachen Winkel auf die Grenzfläche trifft, gibt es keinen reellen Winkel für den gebrochenen Strahl, der diese Bedingung erfüllt. In diesem Fall tritt statt der Brechung eine Totalreflexion auf.

Der Wellenoptik liegt das Prinzip von Huygens und Fresnel zugrunde.

Jeder Punkt einer Wellenfront ist der Ausgangspunkt einer Elementarwelle. Eine Wellenfront ergibt sich als Überlagerung dieser Elementarwellen.

Mit "Elementarwelle" ist in diesem Zusammenhang eine Kugelwelle gemeint, die von einem bestimmten Punkt ausgeht. "Wellenfronten" sind die Flächen gleicher Phase. Der Abstand zwischen zwei aufeinander folgenden Wellenfronten ist somit die Wellenlänge. Die Wellenfronten einer ebenen Welle sind also Ebenen, die Wellenfronten von Elementarwellen sind konzentrische Kugelflächen. Die Ausbreitungsrichtung (also die Richtung des Wellenvektors) bildet stets eine Normale zur Wellenfront. Mit der Wellenoptik lassen sich alle Phänomene der Beugung und Interferenz verstehen. Sie eignet sich aber auch, das Reflexions- und das Brechungsgesetz herzuleiten. Die Wellenoptik widerspricht also nicht der Strahlenoptik, sondern erweitert und vertieft diese.

Historisch nimmt die Wellenoptik von Huygens und Fresnel schon wesentliche Erkenntnisse der Elektrodynamik vorweg: Die Lichtwellen sind elektromagnetische Wellen.

In der Quantenphysik wird Licht nicht mehr als klassische Welle, sondern als Quantenobjekt aufgefasst. Demnach setzt sich das Licht aus einzelnen diskreten Energiequanten, den so genannten Photonen zusammen. Ein Photon ist ein Elementarteilchen, genauer: ein Boson mit einer Ruhemasse von 0, das sich stets mit der Lichtgeschwindigkeit formula_12 bewegt.

Es trägt eine Energie von

Hierbei ist formula_14 die Frequenz des Lichts und formula_15 das Plancksche Wirkungsquantum mit formula_16.

Das Photon hat einen Impuls von

wobei formula_18 die Wellenlänge des Lichts ist.

Der Spin des Photons hängt mit der Polarisation zusammen: Die Wellenfunktion eines einzelnen Photons ist zirkular polarisiert. Je nach Rotationsrichtung des formula_19-Feld-Vektors beträgt der Spin des Photons formula_20 oder formula_21.

Ein Photon wird entweder als Ganzes absorbiert und emittiert oder gar nicht. Es ist also „zählbar“ wie ein Teilchen. Trotzdem bleibt alles, was hier bisher über die Welleneigenschaften des Lichts gesagt wurde, gültig. Die Welle wird quantenmechanisch korrekt durch einen Spezialfall der Klein-Gordon-Gleichung für masselose Teilchen beschrieben (was der Maxwellschen Wellengleichung entspricht). Dieses merkwürdige Verhalten der Photonen, das jedoch auch alle anderen Quantenobjekte zeigen, wurde mit dem Schlagwort „Welle-Teilchen-Dualismus“ bezeichnet: Quantenobjekte sind weder wie klassische Teilchen noch wie klassische Wellen zu verstehen. Je nach Betrachtungsweise zeigen sie Eigenschaften der einen oder der anderen.

In der heute gängigsten Interpretation der Quantenmechanik (Kopenhagener Deutung) kann man den genauen Ort eines Photons nicht "a priori" vorhersagen. Man kann nur Aussagen über die Wahrscheinlichkeit machen, mit der ein Photon an einer bestimmten Stelle auftreffen wird. Diese Wahrscheinlichkeitsdichte ist durch das Betragsquadrat der Amplitude der Lichtwelle gegeben.

Historisch wurde die quantenmechanische Beschreibung des Lichts notwendig, weil sich einige Phänomene mit der rein klassischen Elektrodynamik nicht erklären ließen.

Neben den schon weiter oben in diesem Artikel beschriebenen Phänomenen
gibt es noch zahlreiche weitere Wechselwirkungen zwischen Licht und Materie.

Prinzipiell unterscheidet man zwischen "thermischen" und "nicht-thermischen" Strahlern. Erstere beziehen die Energie für die Strahlungsemission aus der thermischen Bewegung ihrer Teilchen. Beispiele sind Kerzenflammen, glühende Körper (Glühdraht einer Glühlampe) oder die Sonne. Das Spektrum eines thermischen Strahlers ist kontinuierlich, d. h., es treten alle Wellenlängen auf, wobei die spektralen Anteile nach dem Planckschen Strahlungsgesetz ausschließlich von der Temperatur abhängen, jedoch, abgesehen vom spektralen Emissionsgrad, nicht vom Material des Strahlers.

Im Gegensatz dazu haben nicht-thermische Lichtquellen kein kontinuierliches Spektrum, sondern ein Linien- oder ein Bandenspektrum. Das bedeutet, dass nur ganz bestimmte Wellenlängen abgestrahlt werden. Linienspektren treten bei Gasentladungsröhren auf, Bandenspektren bei Leuchtdioden, Polarlichtern oder Leuchtkäfern. Die Energiequellen für die Strahlung sind hier elektrischer Strom, Teilchenstrahlung oder chemische Reaktionen. Linienspektren sind oft charakteristisch für bestimmte Stoffe.

Eine Sonderstellung unter den Lichtquellen nimmt der Laser ein. Laserlicht ist nahezu "monochromatisch" (es besteht fast nur aus einer Wellenlänge), mehr oder weniger "kohärent" (es besteht eine feste Phasenbeziehung zwischen mehreren Wellenzügen) und oft "polarisiert".

Die Tscherenkow-Strahlung entsteht durch die Bewegung von geladenen Teilchen durch ein durchsichtiges Dielektrikum, wenn die Teilchengeschwindigkeit höher als die Lichtgeschwindigkeit im Dielektrikum ist. Sie ist das Analogon zum Überschallknall, und kann zum Beispiel in Schwimmbadreaktoren und Abklingbecken von Kernkraftwerken beobachtet werden.


Licht stellt für Pflanzen - neben der Verfügbarkeit von Wasser - den wichtigsten Ökofaktor dar, weil es die Energie für die Photosynthese liefert. Die von den Chlorophyll-Molekülen in den Chloroplasten absorbierte Lichtenergie wird genutzt, um Wassermoleküle zu spalten (Photolyse) und so Reduktionsmittel für die Photosynthese herzustellen. Diese werden in einem zweiten Schritt verwendet, um Kohlenstoffdioxid schrittweise schließlich zu Glucose zu reduzieren, woraus unter anderem Stärke aufgebaut wird. Der bei der Fotolyse anfallende Sauerstoff wird als Reststoff an die Atmosphäre abgegeben. Die Summenreaktionsgleichung der Photosynthese lautet:

Den Aufbau von organischen Verbindungen aus Kohlenstoffdioxid bezeichnet man als Kohlenstoffdioxid-Assimilation. Organismen, die mit Hilfe von Licht dazu in der Lage sind, nennt man "photo"-autotroph. Neben den Gefäßpflanzen gehören auch die Moose, die Algen und einige Bakterien, beispielsweise Cyanobakterien und Purpurbakterien, dazu. Alle heterotrophen Organismen sind von dieser Assimilation abhängig, weil sie ihren Energiebedarf nur aus organischen Verbindungen, die sie mit der Nahrung aufnehmen müssen, decken können.

Die Konkurrenz der Pflanzen ums Licht macht sich im „Stockwerkaufbau“ des Waldes und der damit verbundenen Spezialisierung von Licht- und Schattenpflanzen oder in der jahrzeitlichen Abfolge verschiedener Aspekte bemerkbar. In Gewässern dient "nur" die lichtdurchflutete oberste Schicht, die Nährschicht, der Bildung von Biomasse und Sauerstoff, hauptsächlich durch Phytoplankton. Weil viele Tiere und Einzeller durch das hohe Nahrungsangebot und den vergleichsweise hohen Sauerstoffgehalt des Wassers hier gute Lebensbedingungen finden, werden sie durch das Licht angelockt.
Der Licht- oder Sehsinn ist für viele Tiere einer der wichtigsten Sinne. Er dient zur Orientierung im Raum, zur Steuerung des Tag-Nacht-Rhythmus, zum Erkennen von Gefahren, zum Aufspüren von Beute, zur Kommunikation mit Artgenossen. Daher haben sich im Laufe der Evolution in den verschiedensten Taxa die unterschiedlichsten Lichtsinnesorgane entwickelt. Diese reichen von den einfachen Augenflecken von "Euglena", über einfache Pigmentfelder bis zu den komplex aufgebauten Facettenaugen und Linsenaugen. Nur wenige Tiere sind vollkommen unempfindlich für Lichtreize. Dies ist höchstens dann der Fall, wenn sie in völliger Dunkelheit leben, wie Höhlentiere.

Sowohl für Räuber- als auch Beutetiere ist es von Vorteil, "nicht" gesehen zu werden. Anpassungen daran sind Tarnung und Nachtaktivität. Erstaunlicherweise haben dahingegen viele Lebewesen selbst die Fähigkeit entwickelt zu leuchten. Das bekannteste Beispiel ist der Leuchtkäfer. Man findet dieses Phänomen der Biolumineszenz aber auch bei Tiefseefischen, Leuchtkrebsen, Pilzen (Hallimasch) oder Bakterien. Der Nutzen der Biolumineszenz wird vor allem mit innerartlicher Kommunikation, Abschreckung von Fraßfeinden, Anlocken von Beute erklärt.

Das Licht, das ins menschliche Auge fällt, wird durch den Brechapparat (bestehend aus Hornhaut, vorderer und hinterer Augenkammer, Linse und Glaskörper) auf die Netzhaut projiziert, wo ein reelles, auf dem Kopf stehendes Bild entsteht. (Der Vorgang ist demjenigen in einer Fotokamera vergleichbar.) Dadurch werden die in der Netzhaut befindlichen Fotorezeptoren (= Lichtsinneszellen) gereizt, die den Reiz in ein elektrisches Signal wandeln. Dieses Signal wird über den Sehnerv, in den die einzelnen Nervenstränge der Netzhaut münden, ans Gehirn geleitet, wo die Empfindung entsteht.

Die Lichtintensität wird als "Helligkeit" empfunden. Das Auge kann sich durch verschiedene Mechanismen an die – viele Zehnerpotenzen umfassenden – Intensitäten anpassen (siehe Adaption). Die empfundene Helligkeit hängt dabei mit der tatsächlichen Intensität über das Weber-Fechner-Gesetz zusammen.

Die spektrale Zusammensetzung des Lichtreizes wird als Empfindung Farbe wahrgenommen, wobei das menschliche Auge Licht mit Wellenlängen zwischen ca. 380 nm und 750 nm erfassen kann. Trennt man weißes Licht (durch ein Prisma) auf, so erscheinen die Wellenlängen als Farben des Regenbogens.

Die Netzhaut des Auges ist mit verschiedenen Sinneszellen ausgestattet: Die Stäbchen weisen eine breite spektrale Ansprechbarkeit auf und zeichnen sich durch eine hohe Sensitivität aus. Sie sind daher auf das Sehen in der Dämmerung spezialisiert, können jedoch keine Farben unterscheiden. Die Zapfen hingegen, die an stärkere Intensitäten angepasst sind, kommen in drei verschiedenen Typen vor, die jeweils bei einer anderen Wellenlänge ihr Reaktionsoptimum haben. Ihre Verschaltung ermöglicht letztendlich das Farbensehen.

Sowohl bei den Stäbchen als auch bei den Zapfen beruht der Sehvorgang auf der Absorption von Photonen durch das Sehpigment (im Falle der Stäbchen: Rhodopsin). Der Ligand Retinal macht dabei eine Isomerisierung durch, die dazu führt, dass das Rhodopsin zerfällt und die Signalkaskade der Phototransduktion in Gang setzt. Die dadurch verursachte Hyperpolarisation der Zellmembran der Stäbchen und Zapfen bewirkt ein elektrisches Signal, welches an die nachgeschalteten Nervenzellen weitergegeben wird. 

Die Leistungen der Lichtsinnesorgane anderer Lebewesen unterscheiden sich zum Teil erheblich von denen des Menschen. Während die meisten Säugetiere ein eher unterentwickeltes Farbensehen haben, verfügen Vögel über mehr Zapfentypen und können dementsprechend mehr Farben unterscheiden als der Mensch. Bienen sind zwar mehr oder weniger unempfindlich für langwelliges (rotes) Licht, können aber das sehr kurzwellige UV-Licht wahrnehmen, das für den Menschen unsichtbar ist. Außerdem können sie die Polarisationsrichtung des Lichts wahrnehmen. Dies hilft ihnen bei der Orientierung im Raum mithilfe des Himmelblaus. Manche Schlangen wiederum können die ebenfalls für uns unsichtbaren IR-Strahlen mit ihren Grubenorganen wahrnehmen.

Bei "organischen Farbstoffen" können delokalisierte π-Elektronen durch Frequenzen im sichtbaren Bereich auf ein höheres Niveau gehoben werden. Dadurch werden je nach Molekül bestimmte Wellenlängen absorbiert.

Bei "anorganischen Farbstoffen" können auch Elektronen aus den d-Orbitalen eines Atoms in energetisch höher gelegene d-Orbitale angeregt werden (siehe Ligandenfeldtheorie). Des Weiteren können Elektronen ihre Position zwischen Zentralion und Ligand innerhalb eines Komplexes wechseln (siehe auch Charge-Transfer-Komplexe und Komplexchemie).


Licht ist, wie Feuer, eines der bedeutendsten Phänomene für alle Kulturen. Künstlich erzeugtes Licht aus Lampen ermöglicht dem Menschen heutzutage ein angenehmes und sicheres Leben auch bei terrestrischer Dunkelheit (Nacht) und in gedeckten Räumen (Höhlen, Gebäuden). Technisch wird die Funktionsgruppe, die Licht erzeugt, als Lampe oder Leuchtmittel bezeichnet. Der Halter für die Lampe bildet mit dieser eine Leuchte. „Licht“ und „Leuchte“ werden auch als Symbole für Intelligenz verwendet ("Lichtblick", Aufklärung). Ein Mangel an Intelligenz wird auch als „geistige Dunkelheit“ oder „geistige Umnachtung“ bezeichnet. Im Christentum steht das Licht in der Selbstbezeichnung Jesu Christi für die Erlösung des Menschen aus dem Dunkel der Gottesferne. In der biblischen Schöpfungsgeschichte ist das Licht das zweite Werk Gottes, nach Himmel und Erde.

Angesichts zahlreicher Jubiläen (beispielsweise Ibn Al Haythems "Buch vom Sehen" (1021), spezielle und allgemeine Relativitätstheorie (1905 bzw. 1915) sowie die Entwicklung der Glasfaser durch Charles Kao (1965)) hat die UNESCO das Jahr 2015 zum "International Year of Light" ausgerufen. Auf der ganzen Welt fanden in diesem Jahr Veranstaltungen statt, die sich mit der Bedeutung des Lichts für Wissenschaft und Gesellschaft beschäftigen. In Deutschland koordinierte die Deutsche Physikalische Gesellschaft die Aktivitäten zum Jahr des Lichts.

Nach dem internationalen Jahr des Lichts 2015 hat der Vorstand der UNESCO den "Internationalen Tag des Lichts" (englisch "International Day of Light") ausgerufen. Dies wird offiziell auf der Generalkonferenz im November 2017 bekannt gegeben. Der Tag des Lichts soll ab 2018 jährlich am 16. Mai begangen werden. 

Licht zählt als ein Umwelt­faktor zu den Immissionen im Sinne des Bundes-Immissionsschutzgesetzes (BImSchG). Lichtimmissionen von Beleuchtungsanlagen können das Wohn- und Schlafbedürfnis von Menschen und Tieren erheblich stören und auch technische Prozesse behindern. Entsprechend sind in der „Licht-Richtlinie“ der Länder (in Deutschland) Maßstäbe zur Beurteilung der (Raum-)Aufhellung und der (psychologischen) Blendung festgelegt. Besonders störend kann intensiv farbiges oder blinkendes Licht wirken. Zuständig sind bei Beschwerden die Umwelt- und Immissionsschutzbehörden der jeweiligen Bundesländer. Negative Auswirkungen betreffen die Verkehrs­sicherheit (Navigation bei Nacht, physiologische Blendung durch falsch eingestellte Scheinwerfer oder durch Flächenbeleuchtungen neben Straßen), Einflüsse auf die Tierwelt (Anziehen nachtaktiver Insekten, Störung des Vogelflugs bei Zugvögeln) und die allgemeine Aufhellung der Erdatmosphäre (Lichtverschmutzung, die astronomische Beobachtungen infolge Streuung des Lampenlichts in der Atmosphäre des Nachthimmels behindert).





</doc>
<doc id="13303" url="https://de.wikipedia.org/wiki?curid=13303" title="Konföderationsartikel">
Konföderationsartikel

Die Konföderationsartikel (englisch: "Articles of Confederation and Perpetual Union") waren die erste verfassungsrechtliche Grundlage der Vereinigten Staaten und Vorläufer der amerikanischen Verfassung von 1787. Ihre Regelungen beruhten im Wesentlichen auf dem Prinzip der einzelstaatlichen Souveränität. Nachdem der Kontinentalkongress sie am 15. November 1777 verabschiedet hatte, waren sie nach ihrer dreijährigen Ratifikation von 1781 bis 1789 in Kraft. 

Die dreizehn amerikanischen Kolonien (dieser Begriff schließt die britischen, ebenfalls nordamerikanischen Kolonien Québec, Nova Scotia und Prince Edward Island nicht mit ein) befanden sich seit 1763 in einem Konflikt mit der britischen Krone, der die Herrschaft über dieses Gebiet oblag. Insbesondere Meinungsverschiedenheiten auf wirtschaftlicher Ebene, die durch eine neue Steuerpolitik der britischen Regierung hervorgerufen worden waren, hatten zur zunehmenden Entfremdung zwischen Kolonien und Mutterland geführt. Aus der weiteren Verschärfung der Konfliktlage resultierte 1775 der Ausbruch des Amerikanischen Unabhängigkeitskriegs. Im zweiten Kriegsjahr 1776 folgte schließlich die formale Loslösung vom Mutterland durch die Verabschiedung der Unabhängigkeitserklärung.

Die politische Verwaltung des Staatenbundes erfolgte unterdessen durch den 1774 einberufenen Kontinentalkongress, der zu dieser Zeit die einzige interkoloniale Institution bildete. Ansonsten fehlten jegliche regierungsorganisatorische Strukturen, die ein übergeordnetes Element gebildet oder den Charakter eines Staates bzw. Staatenbundes ausgemacht hätten. Im Jahre 1776 begannen erste Einzelstaaten, darunter etwa New Hampshire oder Virginia, eigenständige Verfassungen zu erarbeiten, die die bisher geltenden königlichen Gesetze ablösen und ersetzen sollten. Eine gemeinsame Verfassung, die die Verhältnisse von Einzelstaat zu Zentralgewalt regelte, existierte hingegen noch nicht; Die Entstehung eines solchen Dokuments stellte jedoch eine Notwendigkeit dar, einerseits um die bestehende politische Leere zu überwinden, andererseits um den Handlungsspielraum des Kontinentalkongresses als überkoloniale Institution sowie die Stellung der Einzelstaaten festzulegen. Zu diesem Zweck wurde am 12. Juni 1776 ein Komitee einberufen, dessen Aufgabe es sein sollte, eine erstmals gesamtkolonial wirksame Verfassung zu entwerfen. Jede der Dreizehn Kolonien entsandte dazu einen Delegierten.

Die völkerrechtlichen Regelungen der Konföderationsartikel beruhten auf dem Prinzip der vollständigen Souveränität der Einzelstaaten, also der 13 ehemaligen britischen Kolonien. Somit etablierten sie gewissermaßen einen amerikanischen Staatenbund, der allerdings einige strukturelle Mängel aufwies und schnell an diesen scheiterte.

Von Anbeginn der Konföderation war es ihrer Legislative, dem Kontinentalkongress, nicht gestattet, eigene Steuern zu erheben. Zum Funktionieren der Konföderationsorgane sollten Beiträge der Mitgliedsstaaten beitragen, jedoch kamen die meisten Mitgliedsstaaten dieser Konvention nicht nach. Aus diesem Grund war es dem Staatenbund nicht möglich, seinen Mitgliedern militärischen Schutz vor den zunehmenden Interventionen der europäischen Mächte zu gewährleisten.

Darüber hinaus fühlten sich die meisten Mitgliedsstaaten nicht verpflichtet, in der Konföderation getroffene Vereinbarungen zu übernehmen. Da es letzterer sowohl an Durchsetzungs- wie an Sanktionsmöglichkeiten mangelte, war der Handlungsspielraum des Staatenbundes stets begrenzt. Ein wesentliches politisch-wirtschaftliches Hemmnis und Ausdruck der Zersplitterung stellte beispielsweise die zunehmende Abgrenzung der Mitgliedsstaaten durch Schutzzölle dar, denen der Kontinentalkongress ebenfalls machtlos gegenüberstand, denn sie verfügten beispielsweise über die Möglichkeit, aus der Konföderation auszutreten.

Um diesen Missständen beizukommen, wurde die Philadelphia Convention einberufen. Eigentlich sollte sie zunächst nur über die Möglichkeit von Verbesserungen der Staatsorganisation beraten und eine Formulierung der Artikel finden, um die Einzelstaaten, die um ihre Macht fürchteten, zu beruhigen. Letzten Endes legte sie aber einen völlig neuen Verfassungsentwurf vor, der im Unterschied zu den Konföderationsartikeln überhaupt erst eine starke Bundesregierung als Exekutivorgan forderte. Diese sollte vor allem in Fragen der Außenpolitik, des Außenhandels und der Landesverteidigung Kompetenzen der Einzelstaaten übernehmen.

In der Rückschau haben die Konföderationsartikel, historisch betrachtet, dennoch die von nun an immerwährende Union der nordamerikanischen Staaten etabliert.




</doc>
<doc id="13304" url="https://de.wikipedia.org/wiki?curid=13304" title="Verfassung der Vereinigten Staaten">
Verfassung der Vereinigten Staaten

Die Verfassung der Vereinigten Staaten von Amerika, am 17. September 1787 verabschiedet und im Laufe des Jahres 1788 ratifiziert, legt die politische und rechtliche Grundordnung der USA fest. Sie sieht eine föderale Republik in Form eines Präsidialsystems vor.

Die Verfassung wurde von Delegierten aus zwölf der dreizehn Gründerstaaten der USA erarbeitet, die in der Philadelphia Convention zusammengetreten waren. Sie löste die zuvor geltenden Konföderationsartikel ab und etablierte eine starke Zentralgewalt mit einem Präsidenten an der Spitze, der sowohl Staats- als auch Regierungschef ist. Zugleich schreibt sie eine als „Checks and Balances“ bezeichnete Gewaltenteilung vor, in der die Organe der Regierung, der Gesetzgebung und der Rechtsprechung getrennt voneinander agieren und sich durch weitreichende Verschränkungen gegenseitig kontrollieren. Wie die Gewaltenteilung entspringen auch andere Verfassungsgrundsätze politischen Konzepten, die im Zeitalter der Aufklärung entwickelt und verbreitet wurden, darunter die Bill of Rights als verbindlicher Grundrechtekatalog und das Bekenntnis zu Recht und Gesetz.

Der ursprüngliche Verfassungstext besteht aus sieben Artikeln. Er wurde im Verlauf von zwei Jahrhunderten um 27 Artikel ergänzt, wobei zehn davon als Bill of Rights unmittelbar nach Bildung der Verfassungsorgane hinzugefügt wurden. Unter allen republikanischen Verfassungen, die heute in Kraft sind, stellt die der USA eine der ältesten dar. 

Während des Amerikanischen Unabhängigkeitskrieges bildeten die dreizehn Kolonien zuerst unter den Konföderationsartikeln 1781 einen losen Staatenbund mit einer schwachen Zentralregierung, die nur aus dem Kontinentalkongress als ständiger Versammlung bestand, wobei die Zusammensetzung mit einer durchschnittlichen Amtsdauer der Abgeordneten von zwei Jahren keine Konstanz fand. Der Kongress durfte keine Steuern erheben und war bei der Ausführung seiner Beschlüsse von den einzelnen Staaten abhängig, da ihm selbst weder eine ausführende noch eine rechtsprechende Gewalt zur Seite standen. Ferner hatte der Kongress keinen Einfluss auf Einfuhrzölle und andere Handelsbarrieren zwischen den Staaten. Der Text der Konföderationsartikel konnte nur mit der Zustimmung aller Mitgliedsstaaten geändert werden. Die Staaten maßen der zentralen Regierung lediglich eine geringe Bedeutung zu und entsandten oft erst gar keine Abgeordneten, so dass der Kongress für lange Zeiträume beschlussunfähig blieb.

Bereits fünf Jahre nach Verabschiedung der Konföderationsartikel trafen sich im September 1786 Vertreter aus fünf Staaten zur Annapolis Convention, um nötige Änderungen von Artikeln – insbesondere zur Verbesserung des zwischenstaatlichen Handels – zu besprechen. Sie beschlossen, zur Erarbeitung von Verfassungsänderungen eine Versammlung von Vertretern aller Mitgliedsstaaten einzuberufen. Der Kontinentalkongress unterstützte diesen Plan formell am 21. Februar 1787. Alle Staaten außer Rhode Island akzeptierten die Einladung und entsandten Delegierte zum Verfassungskonvent, der am 25. Mai 1787 die Arbeit aufnahm.

Obwohl der Kongressbeschluss nur die Ausarbeitung von Änderungen an den bestehenden Konföderationsartikeln vorsah, entschlossen sich die 55 Delegierten stattdessen dazu, eine neue Verfassung auszuarbeiten und unter Ausschluss der Öffentlichkeit zu tagen. Um die Vorschläge der Delegierten zu erklären und die neuen Verfassungsinhalte zu verteidigen, veröffentlichten Alexander Hamilton, James Madison und John Jay die Federalist Papers, die bis heute als wichtige Kommentare der Verfassung angesehen werden.

Eine der schärfsten Debatten während des Konvents bezog sich auf die Kompetenzen des neuen Kongresses und seine Zusammensetzung. Ein am 29. Mai vorgestellter und von Madison unterstützter, als Virginia-Plan bezeichneter Vorschlag sah vor, ein Parlament mit zwei Kammern zu schaffen, deren Mitglieder im Verhältnis zu den Bevölkerungsgrößen in den Bundesstaaten gewählt werden sollten. Die erste Kammer sollte die Abgeordneten der zweiten wählen. Mit dieser Regelung sollte die Bedeutung der Regierungen in den Bundesstaaten zugunsten ihrer Bevölkerung verringert werden. Gleichzeitig sollte damit verhindert werden, dass einige wenige bevölkerungsschwache Staaten Gesetze blockieren könnten, die von einer Bevölkerungsmehrheit unterstützt wurden.

Die gegenteilige Position ergab sich am 15. Juni in William Patersons New-Jersey-Plan: Der Kongress sollte wie bisher mit einer gleichwertigen Vertretung aller Staaten weiter bestehen, was die kleinen Staaten proportional bevorzugen würde, aber zusätzliche Kompetenzen erhalten. Beide Vorschläge sahen im Sinne einer deutlichen Stärkung gegenüber den Konföderationsartikeln vor, dass Gesetze des Kongresses Vorrang vor denen der Bundesstaaten haben sollten. Die Lösung fand sich am 27. Juni im Connecticut-Kompromiss, der die verhältnismäßige Vertretung des Virginia-Plans mit der gleichen Verteilung der Sitze des New-Jersey-Plans in zwei getrennten, aber gleichberechtigten Kammern verband.

Ein weiterer lang umstrittener Punkt war die Frage, welche Rolle die ausführende Gewalt spielen und wer sie ausfüllen sollte. Verschiedene Varianten, vom einzelnen Gouverneur bis zu einer Art Regierungsausschuss, jeweils vom Kongress gewählt, wurden besprochen. Die Delegierten, noch immer vom vor wenigen Jahren beendeten Unabhängigkeitskrieg beeinflusst, lehnten anfangs eine starke nationale ausführende Gewalt aufgrund der Nähe zur britischen Monarchie ab. Die Idee einer mehrköpfigen Regierung mit geteilten Kompetenzen wurde allerdings ebenso verworfen wie der im Virginia-Plan enthaltene Vorschlag eines Beratungsgremiums für den Präsidenten. Die Einigung erfolgte am 4. September: Die Staaten würden Wahlmänner bestellen, die einen Präsidenten und einen Vizepräsidenten für eine vierjährige Amtszeit wählen. Die Aufgabe des Präsidenten wäre die Ausführung der Gesetze und Kontrolle des Kongresses mit Hilfe eines Vetorechtes. Eine direkte Wahl des Präsidenten wurde als unpraktikabel abgelehnt. Damit wird (aus heutiger Sicht nur noch formal) auch wieder die Stärkung der Staaten erkennbar, denen die jeweilige gesetzliche Grundlage zur Bestimmung der Wahlmänner seit jeher freigestellt ist, womit in der Anfangszeit die Organe der Staaten ihre Wahlmänner direkt delegieren und bei der Wahl des Präsidenten ihre höchsteigenen Interessen in die Waagschale werfen konnten.

Auch die Interessen der kleinen Staaten sollten durch die Abgabe von zwei gleichwertigen Stimmen pro Wahlmann ein weiteres Mal gewahrt werden, indem sie mit der geschlossenen Stützung eines von mehreren Gegenkandidaten an zweiter Stelle, selbst wenn sie an erster Stelle jeweils unterschiedliche aussichtslose Kandidaten bevorzugen würden, zusammengenommen den Ausschlag für die Wahl des Präsidenten geben könnten. Auch von einer Wahl ohne Sieger würden die kleinen Staaten profitieren, da die Entscheidung sich ins Repräsentantenhaus verlagern würde und die jeweiligen Abgeordneten eines Staates entgegen dem Grundprinzip dieser Kammer als eine Delegation mit nur einer Stimme, also alle gleichwertig, auftreten müssten. Diese Überlegungen beruhten jedoch auf der Annahme, dass zu den Präsidentschaftswahlen in der Regel eine Hand voll fähiger Staatsmänner einzig auf Grundlage ihrer Fähigkeiten gegeneinander kandidieren würden und stellte sich aufgrund der baldigen Herausbildung von Parteipolitik als unbrauchbar heraus: Die Wahlmänner wurden entsprechend von ihrer jeweiligen Partei darauf eingeschworen, geschlossen dieselben zwei Kandidaten zu wählen; ein Patt von zwei Parteikollegen war also sehr wahrscheinlich, und Überlegungen der kleinen Staaten spielten in einem System von nur zwei sehr konträren Parteien keine Rolle. Das Verfahren wurde 1804 beschränkt auf eine Stimme für den Präsidenten und eine für den Vizepräsidenten, die nicht zusammengerechnet werden.
Viele der weiteren Verfassungskonzepte basierten auf gesellschaftlichen Vorstellungen der Antike und Regierungstraditionen der britischen konstitutionellen Monarchie. Die Verfassung stützte sich in ihrem Rechtsverständnis beispielsweise direkt auf den 39. Artikel der Magna Carta von 1215:

Die englische Bill of Rights von 1689 diente ebenso als Quelle für den Grundrechtekatalog der Verfassung. Das in den ersten Zusatzartikeln verankerte Gebot der Geschworenengerichte, das Recht auf Waffenbesitz und das Verbot der grausamen und außergewöhnlichen Bestrafung gehen auf dieses Dokument zurück.

Außerdem waren die Väter der Verfassung beeinflusst von den Werken Montesquieus, der ein Regierungssystem auf der Grundlage der Gewaltenteilung skizzierte. Bedeutsam war weiterhin die Geschichte der Republik der Sieben Vereinigten Niederlande, die 1781 schon zwei Jahrhunderte lang eine geschriebene Verfassung besaß. So sagte Benjamin Franklin „in der Liebe zur Freiheit und ihrer Verteidigung war die Republik der Sieben Vereinigten Niederlande unser Vorbild“ während John Adams anmerkte, die Ursprünge beider Republiken ähnelten sich so sehr, dass die Geschichte der einen nur eine Abschrift der anderen zu sein scheint.

Die Delegierten beendeten am 17. September 1787 mit einem Schlusswort Benjamin Franklins – Delegierter des Staates Pennsylvania – ihre Arbeit. Franklin erklärte, dass auch der endgültige Entwurf nicht vollständig zufriedenstellend sei, man aber nie Perfektion erreichen könne. Er unterstützte die neue Verfassung und bat auch alle Kritiker, sie anzunehmen. Franklin war der einzige Gründervater der Vereinigten Staaten, der vor der Verfassung auch die Unabhängigkeitserklärung und den Friedensvertrag mit dem Königreich Großbritannien unterzeichnet hatte.

Die dreizehn Staaten stimmten der Verfassung in der folgenden Reihenfolge zu. Die Stimmenzahlen beziehen sich auf gesondert einberufene Ratifizierungsversammlungen, die im jeweiligen Staat abstimmten.

Die Verfassung gliedert sich in eine Präambel und sieben Artikel. In den ersten drei Artikeln werden im Wesentlichen die Grundzüge der Gewaltenteilung dargelegt.

Die Präambel der Verfassung besteht aus einem einzigen Satz, der das Dokument und seinen Zweck vorstellt. Die Präambel verleiht selbst keine Macht und verbietet auch keine Handlungen, sondern erklärt nur den Hintergrund und Sinn der Verfassung. Ein Gottesbezug findet sich bewusst nicht, da die Verfassung ein rein säkulares Dokument ist. Die Präambel, insbesondere die ersten drei Worte “We the people”, ist einer der am häufigsten zitierten Abschnitte der Verfassung.

Der "erste Artikel" beschreibt die gesetzgebende Gewalt, die vom Kongress ausgefüllt wird. Der Kongress hat auf der Bundesebene exklusive Gesetzgebungskompetenzen, die nicht an andere Institutionen delegiert werden dürfen. Zu seinen in der Verfassung aufgeführten Zuständigkeitsgebieten gehören unter anderem das Haushalts- und Steuerrecht, das Einbürgerungsrecht, das Handelsrecht, das Patent- und Urheberrecht, das Recht, den Krieg zu erklären sowie der Aufbau und der Unterhalt eines stehenden Heeres. Gleichzeitig legt die Verfassung auch Bereiche fest, in denen der Kongress keine Möglichkeit zur Rechtssetzung hat, darunter das Erheben von Ausfuhrsteuern, die Aufhebung des Habeas Corpus, die Verurteilung einzelner Personen ohne ordentliches Gerichtsverfahren und die Verleihung von Adelstiteln.

Der Kongress besteht aus zwei Kammern: einem direkt von der Bevölkerung der Bundesstaaten auf zwei Jahre gewählten Repräsentantenhaus und einem (früher von den Parlamenten der Bundesstaaten für sechs Jahre gewählten) heute von der Bevölkerung gewählten (17. Zusatzartikel) Senat. Die Mindestanforderungen, um für einen Sitz im Repräsentantenhaus zu kandidieren, d. h. für das passive Wahlrecht sind ein Alter von mindestens 25 Jahren, ein fester Wohnsitz im zu vertretenden Bundesstaat und das Bestehen der Staatsbürgerschaft seit mindestens sieben Jahren. Für den Senat gelten ähnliche Anforderungen, allerdings beträgt das Mindestalter hier 30 Jahre und der Mindestzeitraum für die Staatsbürgerschaft neun Jahre.

Die Wahlen zum Repräsentantenhaus finden in allen Bundesstaaten statt, die zu diesem Zweck ihrer Bevölkerungszahl entsprechend in Wahlkreise aufgeteilt werden. Jeder Wahlkreis wählt nach dem Prinzip der Mehrheitswahl einen Sitz in der Kammer. Die Zuteilung der Sitze an die Bundesstaaten erfolgt vom Kongress auf der Basis einer Volkszählung, die alle zehn Jahre von der Zensusbehörde durchgeführt wird. Jedem Bundesstaat steht mindestens ein Sitz zu. Für die weitere Aufteilung des Bundesstaates in Wahlkreise ist das jeweilige Parlament zuständig. Das aktive Wahlrecht hat jeder Bürger, der in seinem Bundesstaat nach den lokalen Gesetzen zur Wahl der größten bundesstaatlichen Parlamentskammer aktiv wahlberechtigt ist. Das Repräsentantenhaus wählt als Vorsitzenden einen Sprecher.

Bis zur Verabschiedung des 17. Zusatzartikels zur Verfassung wurden die Senatoren nicht direkt, sondern von den Parlamenten der Bundesstaaten gewählt. Jedem Bundesstaat stehen im Senat genau zwei Sitze zu. Die Wahl erfolgt gestaffelt, so dass alle zwei Jahre ein Drittel der Senatoren neu gewählt wird. Der Vizepräsident der Vereinigten Staaten ist gleichzeitig der Präsident des Senates. Die Kammer wählt allerdings auch einen Präsidenten Pro Tempore, der im Tagesgeschäft den Vorsitz übernimmt.

Die Mitglieder der Kammern beziehen aus dem laufenden Haushalt eine Entschädigung für ihre Dienste. Weiterhin erhalten sie politische Immunität und haben im Plenum eine absolute Meinungsfreiheit. Sie dürfen im Sinne der Gewaltenteilung keine weiteren staatlichen Ämter ausüben oder während ihrer Amtszeit annehmen.

Beide Kammern sind weitestgehend gleichberechtigt und unabhängig. Sie geben sich eigene Geschäftsordnungen und entscheiden über Rügen und Ausschlüsse ihrer Mitglieder selbständig. Jedoch müssen sie immer gemeinsam tagen und sich auf den Beginn und die Dauer ihrer Sitzungsperioden verständigen. Ebenso muss jedes Gesetzesvorhaben von beiden Kammern in gleicher Form gebilligt werden, bevor es dem Präsidenten zur Unterschrift vorgelegt wird. Der Präsident hat das Recht, beschlossene Gesetze abzulehnen. Das Gesetz muss danach von beiden Kammern mit Zweidrittelmehrheit beschlossen werden, um das Veto des Präsidenten aufzuheben. Eine Auflösung einer oder beider Kammern, zum Beispiel um Neuwahlen herbeizuführen, ist nicht möglich.

Der "zweite Artikel" legt das Amt des Präsidenten fest, der die ausführende Gewalt innehat. Die Amtszeiten des Präsidenten und des Vizepräsidenten betragen vier Jahre, eine Begrenzung der Wiederwahlmöglichkeit enthält die Verfassung erst mit dem 1951 verabschiedeten 22. Zusatzartikel. Jeder Bürger kann für das Präsidentenamt kandidieren, wenn er seit seiner Geburt die Amerikanische Staatsbürgerschaft innehat, mindestens 35 Jahre alt ist und seit mindestens 14 Jahren seinen festen Wohnsitz in den Vereinigten Staaten hat.

Die Wahl findet in zwei Stufen statt. Zuerst werden in jedem Bundesstaat so viele Wahlmänner ernannt, wie der Bundesstaat Mitglieder im Kongress hat. Die Art und Weise der Ernennung regeln die Bundesstaaten eigenständig, der Kongress bestimmt nur den Tag der Ernennung und der Stimmenabgabe. Seit 1824 findet in jedem Bundesstaat zur Ernennung der Wahlmänner eine allgemeine Wahl statt. Die Wahlmänner geben vor der Wahl bekannt, für welchen Kandidaten sie stimmen werden, sind aber nur in 26 Bundesstaaten und dem Regierungsbezirk Washington D.C. daran gesetzlich gebunden.
Der ursprüngliche Verfassungstext sah vor, dass die Wahlmänner nach ihrer Ernennung in den Hauptstädten der jeweiligen Bundesstaaten zusammenkommen und jeweils ihre Stimmen für zwei Kandidaten abgeben. Der Kandidat, der die meisten Stimmen erhielt, wurde Präsident, der mit der nächsthöheren Stimmenzahl Vizepräsident. Gewählt war nur der Kandidat, der gleichzeitig die absolute Mehrheit der Stimmen auf sich vereinigte. Wenn kein Kandidat die absolute Mehrheit erreicht hatte, entschied das Repräsentantenhaus. Nach der Präsidentschaftswahl 1796, bei der Kandidaten unterschiedlicher Parteien zum Präsidenten und Vizepräsidenten gewählt worden waren und der Wahl von 1800, bei der es zu einem Patt zwischen zwei Kandidaten der gleichen Partei gekommen war, wurde der Wahlmodus durch einen neuen Zusatzartikel verändert. Seitdem geben die Wahlmänner getrennt eine Stimme für den Präsidenten und eine Stimme für den Vizepräsidenten ab, womit ähnliche Situationen vermieden werden sollten.

Scheidet der Präsident wegen Tod, Rücktritt oder fehlender Fähigkeit zur Amtsausführung vorzeitig aus, so übernimmt der Vizepräsident das Amt. Die Reihenfolge der Nachfolge des Präsidenten für den Fall, dass auch das Amt des Vizepräsidenten unbesetzt ist, kann der Kongress per Gesetz festlegen. Dem ursprünglichen Text zufolge blieb das Amt des Vizepräsidenten nach dessen Wechsel zum Präsidentenamt frei. Erst 1967 wurde die Verfassung so geändert, dass in diesem Fall der Präsident mit Zustimmung von zwei Dritteln beider Kammern des Kongresses einen neuen Vizepräsidenten ernennen kann.

Als Staatsoberhaupt und Regierungschef verfügt der Präsident über umfangreiche Kompetenzen. Er hat den Oberbefehl über die Streitkräfte inne, handelt im Namen der Vereinigten Staaten und mit Zustimmung des Senats Verträge mit anderen Ländern aus und ernennt mit Zustimmung des Senats Botschafter, Minister, Richter und andere Beamte. Der Präsident muss dem Kongress gelegentlich über die Lage der Nation berichten, darf eine oder beide Kammern zu einer Sitzung einberufen und eine Sitzungspause festlegen, wenn sich beide Kammern nicht einigen können.

Darüber hinaus ist der Präsident für die Durchführung aller vom Kongress beschlossenen Gesetze verantwortlich. Ein Kabinett im Sinne einer mehrköpfigen Regierung ist von der Verfassung nur insofern vorgesehen, als der Präsident das Recht hat, die höchsten Beamten schriftlich um ihren Rat zu bitten. Minister werden in der Verfassung nicht erwähnt, das Ministeramt hat sich erst in der Regierungspraxis entwickelt. Die Minister sind im Unterschied zu anderen Ländern direkt vom Präsidenten abhängig, müssen seinen Anweisungen folgen und können von ihm jederzeit entlassen werden.

Der Präsident, der Vizepräsident und weitere Beamte der Bundesregierung können vom Kongress ihres Amtes enthoben werden, wenn ihnen Verrat, Bestechung oder andere Straftaten nachgewiesen werden. Das Amtsenthebungsverfahren muss von einer Mehrheit im Repräsentantenhaus eingeleitet werden. Dazu werden dem Senat konkrete Anschuldigungen übermittelt, über deren Wahrheitsgehalt die Senatoren anhand der vorgebrachten Beweise bestimmen müssen. Sind mindestens zwei Drittel der Senatoren der Ansicht, die Anschuldigungen seien gerechtfertigt, ist der Amtsträger seines Amtes enthoben.

Der "dritte Artikel" bestimmt die Rechtsprechung des Bundes. Der Artikel verlangt die Errichtung eines Obersten Gerichtshofs und überlässt die weitere Gestaltung des Gerichtssystems dem Kongress. Seine Richter werden vom Präsidenten mit Zustimmung des Senats auf Lebenszeit ernannt, können aber bei groben Verstößen vom Kongress ihres Amtes enthoben werden.

Die Aufgabenverteilung zwischen Gerichten des Bundes und der Bundesstaaten hängt von dem für die Entscheidung eines Falles maßgeblichen Recht ab. Die Gerichte des Bundes sind nur für die Rechtsstreitigkeiten zuständig, die aus den Gesetzen und Abkommen der Vereinigten Staaten entstehen können, für alle Fälle, die sich mit Botschaftern, Ministern, Konsuln oder dem Seerecht beschäftigen, für Fälle, an denen die Vereinigten Staaten oder zwei oder mehr Bundesstaaten beteiligt sind, sowie für Klagen zwischen einem Bundesstaat oder dessen Bürgern und Bürgern eines anderen Bundesstaats. Der Oberste Gerichtshof ist nur dann als erste Instanz zuständig, wenn es sich bei einer der Parteien um einen Botschafter, einen Minister, einen Konsul oder einen Bundesstaat handelt. In allen anderen Fällen prüft das Gericht nur auf Antrag die Entscheidungen anderer Gerichte auf Rechtsfehler.

Eine explizite Verfassungsgerichtsbarkeit sieht der Verfassungstext zwar nicht vor. Der Oberste Gerichtshof entschied jedoch im Fall "Marbury v. Madison", dass er das Prüfungsrecht hat, Bundesgesetze für verfassungswidrig und damit nichtig zu erklären. Dieser Grundsatz wurde in der weiteren Rechtsprechung auch auf Gesetze der Bundesstaaten ausgeweitet und ist zu einer Verfassungstradition erstarkt, so dass von einer relativ hohen Prüfungsdichte gesprochen werden kann. Die Prüfung von Gesetzgebung kann aber nur im Rahmen eines konkreten Rechtsstreits stattfinden. Eine abstrakte Normenkontrolle oder eine allgemeine Prüfung im Anschluss an das Gesetzgebungsverfahren gibt es nicht.

Strafprozesse müssen mit Hilfe von Geschworenen in dem Bundesstaat durchgeführt werden, in dem die Straftat begangen wurde. Die Verfassung definiert an dieser Stelle auch den Straftatbestand des Verrats als Handlung, die entweder einen Krieg gegen die Vereinigten Staaten herbeiführt oder die Feinde des Landes unterstützt. Eine Verurteilung ist nur dann möglich, wenn die Handlung von mindestens zwei Zeugen gesehen wurde oder ein Geständnis vorliegt. Die Verurteilung durfte sich nicht auf die Nachkommen des Verurteilten auswirken, wie früher nach englischem Recht möglich.

Der "vierte Artikel" regelt die Beziehungen zwischen dem Bund und den Bundesstaaten sowie den Bundesstaaten untereinander. In diesem Artikel finden sich beispielsweise die Pflicht zur gegenseitigen Anerkennung (engl. "full faith and credit") von Rechtsakten und das Verbot der Diskriminierung von Bürgern anderer Bundesstaaten. So kann ein Bürger Arizonas in Ohio zum Beispiel für die gleiche Straftat nicht anders bestraft werden als ein einheimischer Bürger.

Andererseits sind die Bundesstaaten zur gegenseitigen Rechtshilfe, zur Gewährleistung der allgemeinen Freizügigkeit aller Bürger und zur Wahrung einer republikanischen Regierungsform verpflichtet. Ebenso bestimmt dieser Artikel die notwendigen Schritte zur Schaffung und Aufnahme neuer Bundesstaaten. Darüber hinaus erhält der Kongress die Befugnis, eigenständig über den Verkauf und die Benutzung von bundeseigenem Land zu verfügen und Gesetze für Territorien zu erlassen, die nicht zu einem Bundesstaat gehören. Der Artikel verpflichtet den Bund auch, die Bundesstaaten gegen Invasionen zu schützen.

Der "fünfte Artikel" setzt ein vergleichsweise kompliziertes Verfahren zur Verfassungsänderung fest. Einerseits gingen die Delegierten des Verfassungskonvents davon aus, dass die Verfassung ohne Möglichkeit zur Änderung nicht lange bestehen könnte. Es war abzusehen, dass sich das Land insbesondere in Richtung Westen stark vergrößern würde und sich dabei Umstände ergeben könnten, die zur Zeit des Verfassungskonvents nicht vorhersehbar waren. Andererseits wollten sie aber auch sicherstellen, dass solche Änderungen nicht zu leicht fielen, und die Umsetzung undurchdachter oder übereilter Vorschläge verhindern. Zum Ausgleich dieser beiden Ziele und auch, um eine größere Flexibilität zu ermöglichen, wurde die Einstimmigkeit, die in den Konföderationsartikeln vorherrschte, durch eine qualifizierte Mehrheit ersetzt. Das Gremium schuf zwei verschiedene Verfahren, mit denen Verfassungsänderungen vorgeschlagen werden können.

Einerseits können Änderungsvorschläge direkt vom Kongress eingebracht werden, andererseits kann der Kongress auf Antrag von mindestens zwei Dritteln der Staaten einen neuen Verfassungskonvent einberufen. In beiden Fällen müssten erarbeitete Änderungen dem Kongress zur Verabschiedung vorgelegt werden, wobei sich die zweite Variante eines Verfassungskonvents, der im Endeffekt dennoch ebenso die Zustimmung der Kongresskammern benötigt, im Vergleich zur direkten Erarbeitung durch den Kongress als äußerst umständlich herausgestellt hat und niemals angewendet wurde.

Um als offizieller Verfassungsänderungsantrag gültig zu sein, benötigt ein Vorschlag die Zustimmung von mindestens zwei Dritteln der Stimmen in beiden Kongresskammern. Anschließend müssen die Änderungen auch in drei Vierteln der Bundesstaaten durch das jeweilige Parlament oder eine speziell zu diesem Zweck zu wählende Versammlung ratifiziert werden; der Kongress legt dabei fest, ob spezielle Versammlungen zu wählen sind oder nicht, wobei sich in der Praxis die Staatsparlamente als ausreichende Vertretung etabliert haben. Ein Veto dagegen durch den Gouverneur eines Bundesstaates ist nicht vorgesehen, wurde jedoch im Laufe der Geschichte von einigen Gouverneuren angewendet und musste aufgrund von Erreichen bzw. ohnehin Verfehlen einer Drei-Viertel-Mehrheit der Staaten bisher noch nie verfassungsrechtlich vor dem Obersten Gericht geklärt werden. Da die Verfassung keine Bestimmungen enthält, bis wann die Zustimmung von drei Vierteln der Bundesstaaten vorliegen muss, enthalten neuere Änderungsvorschläge meist selbst eine Zeitbeschränkung auf sieben Jahre, deren Gültigkeit jedoch umstritten ist. So werden Änderungsvorschläge manchmal Jahrzehnte später noch von dem ein oder anderen Bundesstaat ratifiziert.

Eine Beschränkung hinsichtlich des Inhalts solcher Änderungen ähnlich der Ewigkeitsklausel im Grundgesetz für die Bundesrepublik Deutschland besteht mit einer Ausnahme nicht: Die gleichberechtigte Vertretung der Bundesstaaten im Senat kann nur mit Zustimmung aller betroffenen Bundesstaaten verändert werden. Beispielsweise wäre eine Verfassungsänderung mit dem Ziel, die Stimmen im Senat nach Bevölkerungsstärke umzuverteilen, nur mit Zustimmung aller Staaten möglich.

Im Unterschied zu den Verfassungen vieler anderer Staaten wird der neue Text nicht in den alten eingearbeitet, sondern am Ende angehängt. Dies hat sich als Tradition nach der Verabschiedung der Bill of Rights herausgebildet, deren Inhalt dem ursprünglichen Verfassungstext in der Form von zehn neuen Artikeln folgt. Durch solche Zusatzartikel hinfällig gewordene Bestimmungen (im Ursprungstext oder in früheren Zusatzartikeln) werden in Druckausgaben gewöhnlich in eckige Klammern gesetzt.

Der "sechste Artikel" bestimmt, dass die Verfassung, die Gesetze und die Verträge, die Gesetzesrang haben, das höchste Recht der Vereinigten Staaten ausmachen. Diese Klausel wurde vom Obersten Gerichtshof dahingehend interpretiert, dass sich Bundesgesetze der Verfassung unterwerfen müssen und verfassungswidrige Gesetze nichtig sind. Als Übergangsbestimmung legt der Artikel außerdem fest, dass die Schulden des Kontinentalkongresses auch nach Ratifikation der Verfassung bestehen bleiben. Ferner schreibt der Artikel für alle Abgeordneten, Senatoren, Bundesbeamten und Richter einen Amtseid auf die Verfassung vor.

Der "siebte Artikel" enthält schließlich die Voraussetzungen für die erfolgreiche Ratifikation der Verfassung. Der Entwurf sollte erst dann rechtskräftig werden, wenn mindestens neun Staaten in speziellen Versammlungen zugestimmt hatten. Dies geschah am 21. Juni 1788, als New Hampshire sich als neunter Staat mit der Verfassung einverstanden erklärte. Als der Kontinentalkongress vom Ergebnis der Abstimmung erfuhr, wurde ein Übergangsplan erarbeitet, unter dem am 4. März 1789 die neue Regierung ihre Arbeit aufnehmen konnte.

Die Verfassung hat seit ihrer Ratifikation lediglich 18 Veränderungen in mehr als 200 Jahren erfahren. Sie wurde seit 1787 um 27 Zusatzartikel "(Amendments)" erweitert und durch Grundsatzurteile des Obersten Gerichtshofs in ihrer Bedeutung und Auslegung an die sich verändernden historischen Umstände angepasst. Das Selbstverständnis des Gerichtshofs als Hüter der Verfassung, das in den Anfangsjahren noch keinen Konsens darstellte und das Gericht sich über Urteile wie insbesondere "Marbury vs. Madison" (1803) selbst erarbeiten musste, erlaubt es ihm, für andere Gerichte bindende Interpretationen der Verfassung aufzustellen. Da solche Fälle immer auch die aktuellen rechtlichen, politischen, wirtschaftlichen und gesellschaftlichen Gegebenheiten widerspiegeln, ergibt sich damit eine pragmatische Möglichkeit der Verfassungsänderung durch Richterrecht statt der Veränderung des eigentlichen Textes. Im Laufe der letzten zwei Jahrhunderte haben Rechtsfälle, die sich mit so unterschiedlichen Themen wie den Rechten von Angeklagten in Strafprozessen oder der staatlichen Regulierung von Radio und Fernsehen befassten, wiederholt Veränderungen der Interpretation eines Verfassungsabschnitts hervorgerufen, ohne dass dem eine formelle Verfassungsänderung zu Grunde lag.

Vom Kongress verabschiedete Bundesgesetze zur Ausführung der Verfassungsbestimmungen erweitern und verändern die Interpretation der Verfassung auf ebenso subtile Weise. Ähnliches gilt für eine große Anzahl von Verwaltungsverordnungen, die in Bezug auf Verfassungsbestimmungen erlassen werden. Die verfassungsrechtliche Bedeutung solcher Gesetze und Verordnungen wird schließlich im Sinne des Common Law von den Bundesgerichten im Rahmen der ständigen Rechtsprechung und mit Rückbezug auf Präzedenzfälle zu Gunsten einer möglichst konsistenten Rechtsprechung überprüft und festgelegt.

Bereits in der ersten Sitzungsperiode des Kongresses schlug James Madison einen Grundrechtekatalog vor, der der Verfassung hinzugefügt werden sollte. Der Katalog entstand als Antwort auf Kritik, die besonders von einigen Bundesstaaten und bedeutenden historischen Persönlichkeiten wie Thomas Jefferson geäußert worden war. Diese monierten vor allem, dass sich die starke nationale Regierung ohne weitere verfassungsrechtliche Beschränkungen in eine Tyrannei verwandeln könne.

Zwölf Zusatzartikel wurden zur Bill of Rights zusammengefasst und vom Kongress im September 1789 den Bundesstaaten zur Ratifikation unterbreitet. Zehn der zwölf Artikel wurden bis Dezember 1791 von einer ausreichenden Anzahl Bundesstaaten ratifiziert und sind seitdem Bestandteil der Verfassung. Einer der beiden übrigen Artikel blieb bis zur Zustimmung Alabamas 1992 unratifiziert und ist heute als 27. Zusatzartikel bekannt. Er bestimmt, dass Beschlüsse des Kongresses über die Erhöhung der eigenen Diäten erst nach der nächsten Wahl gültig werden können. Der zweite vorgeschlagene Artikel, der theoretisch immer noch ratifiziert werden könnte, befasst sich mit der erneuten Sitzverteilung im Repräsentantenhaus nach jeder Volkszählung. Kentucky ist seit 1792 der letzte Bundesstaat, der diesen Artikel ratifizierte.

Der erste Zusatzartikel gewährt die Meinungs- und Versammlungsfreiheit sowie das Petitionsrecht. Dieser Artikel verbietet die Einführung einer Staatsreligion durch den Kongress und schützt die individuelle Religionsfreiheit.

Die Bedeutung des zweiten Zusatzartikels ist heftig umstritten, da er sich auf das amerikanische Waffenrecht bezieht und im Gegensatz zu den anderen Zusatzartikeln lediglich äußerst selten vom Obersten Gericht angewendet wurde. In dem Artikel ist zunächst von der Notwendigkeit gut regulierter/organisierter Milizen die Rede, bevor dazu auf die Nichtabschaffbarkeit des Rechtes auf Waffen Bezug genommen wird. Inwiefern die Aufstellbarkeit von Bürgermilizen bzw. Reserven entweder auf die gesamte Bevölkerung übertragbar oder mittlerweile obsolet ist und wie weit insbesondere auf Staaten- und kommunaler, aber auch auf Bundesebene die Zugänglichkeit zu Waffen, das grundsätzliche Recht achtend, eingeschränkt bzw. reguliert werden kann, ist Gegenstand harter Debatten. Im Jahr 2008 hat der Oberste Gerichtshof in District of Columbia v. Heller erstmals in seiner Geschichte entschieden, dass der 2. Zusatzartikel ein Recht auf individuellen Waffenbesitz garantiert. Bis dahin hatte die aus der Entscheidung "United States v. Miller" von 1939 abgeleitete Auffassung gegolten, dass der Artikel einzig den Besitz militärischer Waffen aus einer organisierten Miliz schützt. Dennoch gibt es Bundesstaaten und noch viel stärker Großstädte, die weiterhin harte Waffengesetze verfolgen.

Der dritte Zusatzartikel verbietet es der Regierung, Soldaten ohne Zustimmung der Besitzer in privatem Wohnraum einzuquartieren. Wie im Falle des zweiten Zusatzartikels gibt es auch hier nur wenige Entscheidungen, die diesen Artikel interpretierten. Bisher wurde er noch in keinem Fall vor dem Obersten Gerichtshof behandelt.

Der vierte Zusatzartikel verhindert staatliche Durchsuchungen, Verhaftungen und Beschlagnahmungen ohne richterliche Anordnung. Die Ausnahme ist die berechtigte Annahme, dass eine Straftat begangen wurde "(probable cause)". Der Oberste Gerichtshof leitete von diesem Artikel und anderen in der Entscheidung Griswold v. Connecticut ein allgemeines Recht auf die Wahrung der Privatsphäre ab, das auch ein Recht auf Schwangerschaftsabbruch umfasst.

Der fünfte Zusatzartikel erlaubt Strafprozesse wegen Verbrechen nur infolge einer Anklage (engl. "indictment") durch eine Grand Jury, verbietet die Mehrfachanklage für dieselbe Straftat und das Verhängen von Strafen ohne ordentlichen Gerichtsprozess "(due process)". Er konstituiert ein Zeugnisverweigerungsrecht für den Beschuldigten. Dieser Artikel bestimmt auch, dass privates Eigentum vom Staat nicht ohne Entschädigung enteignet werden darf.

Der sechste, der siebte und der achte Zusatzartikel regeln das Justizsystem des Bundes. Der sechste Zusatzartikel verlangt, dass Strafprozesse in angemessener Geschwindigkeit ablaufen müssen "(speedy trial)", dass der Beschuldigte das Recht auf ein Verfahren vor einem Geschworenengericht und einen Rechtsbeistand hat und dass die Zeugen in der Anwesenheit des Beschuldigten vernommen werden müssen. Der siebte Zusatzartikel enthält das Recht auf ein Verfahren vor einem Geschworenengericht für Zivilprozesse mit einem Streitwert über $ 20. Schließlich verbietet der achte Zusatzartikel unverhältnismäßige Kautionen und Geldstrafen sowie grausame und ungewöhnliche Bestrafungen. Der Oberste Gerichtshof bestimmte 1966 im Urteil zum Fall Miranda v. Arizona, dass allen Beschuldigten vor der Vernehmung oder Verhaftung ihre im fünften und sechsten Zusatzartikel verbrieften Rechte vorzulesen sind. Dies wird seitdem auch als Miranda-Rechte bezeichnet.

Der neunte Zusatzartikel erklärt, dass die aufgelisteten Bürgerrechte nicht als abschließend interpretiert werden sollen und die Bevölkerung noch weitere, nicht in der Verfassung aufgeführte Rechte hat. Das Recht auf die Wahrung der Privatsphäre wird von vielen als ein solches Recht gesehen. Nur wenige Fälle vor dem Obersten Gerichtshof haben sich auf diesen Artikel bezogen.

Der zehnte Zusatzartikel legt schließlich fest, dass die Kompetenzen, die dem Bund nicht explizit von der Verfassung zugewiesen oder den Bundesstaaten entzogen wurden, weiterhin bei den Bundesstaaten und ihrer Bevölkerung liegen. Damit sollte ein Gleichgewicht zwischen der Bundesregierung, den Bundesstaaten und der Bevölkerung geschaffen werden. Tatsächlich hat dieser Zusatzartikel aber keinerlei rechtliche Bedeutung mehr, seitdem der Oberste Gerichtshof im Fall Garcia v. San Antonio Metropolitan Transit Authority entschieden hat, dass Fragen bezüglich dieses Artikels nicht mehr von der Rechtsprechung beantwortet werden.

Der elfte Zusatzartikel beschränkt die Zuständigkeit der Bundesgerichte bei Klagen von Bürgern eines Bundesstaats gegen einen anderen Bundesstaat. Der Artikel war eine Reaktion auf den Fall Chisholm v. Georgia, in dem der Oberste Gerichtshof festlegte, dass Bundesstaaten vor Bundesgerichten von Bürgern anderer Bundesstaaten verklagt werden können.

Die Präsidentschaftswahl 1800 löste eine viermonatige Verfassungskrise aus, als sowohl Thomas Jefferson als auch Aaron Burr im Electoral College 73 Stimmen erhielten. Bei Stimmengleichheit schrieb der ursprüngliche Verfassungstext vor, dass das Repräsentantenhaus bestimmen solle, welcher der beiden Kandidaten Präsident werden würde. Der unterlegene Kandidat würde als Vizepräsident amtieren. Die Krise konnte erst nach 35 Nachwahlgängen beendet werden, aus denen Jefferson als Sieger hervorging. Die im 12. Zusatzartikel vorgeschlagene Änderung sah vor, dass die Wahlmänner zukünftig getrennt eine Stimme für den Präsidenten und eine Stimme für den Vizepräsidenten abgeben sollten. Der Artikel trat 1804 rechtzeitig vor der anstehenden nächsten Präsidentschaftswahl in Kraft.

Infolge des Sezessionskrieges wurden drei Zusatzartikel verabschiedet, die sich alle mit der Sklavenproblematik in den Vereinigten Staaten auseinandersetzten. Der 13. Zusatzartikel schaffte 1865 die Sklaverei in den Vereinigten Staaten ab und verlieh dem Kongress ausdrücklich das Recht, die Abschaffung gesetzlich durchzusetzen. Der 14. Zusatzartikel definierte 1868 das Staatsbürgerschaftsrecht neu. Von nun an hatte jeder Mensch, der in den Vereinigten Staaten geboren wurde, automatisch die volle Staatsbürgerschaft. Gleichzeitig verbietet der Artikel den Entzug von individuellen Rechten und Privilegien ohne ordentliches Gerichtsverfahren; diese weit gefasste Klausel ist eine der wirkmächtigsten der ganzen Verfassung geworden, dutzende Entscheidungen des Obersten Gerichts nehmen darauf Bezug. Der Artikel enthält schließlich ein allgemeines Gleichbehandlungsgebot, das im 20. Jahrhundert während der Bürgerrechtsbewegung von besonderer Bedeutung war. Der 15. Zusatzartikel verfügte 1870, dass die Beschränkung des aktiven Wahlrechts aufgrund der Rasse, Hautfarbe oder eines früheren Sklavenstatus gegen die Verfassung verstößt.

Die Verfassung wurde 1913 mit dem 16. Zusatzartikel geändert, um dem Kongress das Recht zu geben, eine allgemeine Einkommensteuer zu erheben. Bis 1913 war die Bundesregierung auf Einnahmen aus Einfuhrzöllen und gewissen Verbrauchssteuern angewiesen. Versuche des Kongresses, eine allgemeine Einkommensteuer einzuführen, scheiterten vor der Verfassungsänderung mehrfach beim Obersten Gerichtshof, so beispielsweise 1895 im Fall Pollock v. Farmers’ Loan & Trust Co.

Ebenfalls 1913 wurde der 17. Zusatzartikel vorgeschlagen, der die Art und Weise der Senatorenwahlen verändern sollte. Der ursprüngliche Verfassungstext bestimmte, dass die Senatoren von den Parlamenten der Bundesstaaten ernannt werden. Während des 19. Jahrhunderts nutzten Oregon und einige andere Staaten ihre gesetzgeberischen Kompetenzen, um ihre Senatoren per Volksabstimmung zu bestimmen. Bis 1912 hatten 29 Bundesstaaten dieses Verfahren eingeführt. Die ein Jahr später gebilligte Verfassungsänderung sah vor, alle Senatoren direkt von der Bevölkerung der Bundesstaaten wählen zu lassen. Das Recht, bei Rücktritt, Tod oder Amtsenthebung eines Senators eine Ersatzperson zu ernennen, wurde auf die Gouverneure der Bundesstaaten übertragen.

Im Zuge der "Progressiven Ära" verabschiedete der Kongress 1919 den 18. Zusatzartikel, mit dem die Produktion sowie der Verkauf, Transport, Import und Export alkoholischer Getränke verboten wurden. Zuständig für die Durchsetzung des Verbots waren der Kongress und die Bundesstaaten. Der 13 Jahre später verabschiedete 21. Zusatzartikel hob die Alkoholprohibition wieder auf und gab die Regulierungskompetenz über alkoholische Getränke an die Bundesstaaten zurück. Der 21. Zusatzartikel war bisher der einzige, der wegen seiner Dringlichkeit von speziell gewählten Versammlungen ratifiziert wurde statt von den Parlamenten der Einzelstaaten.

Ein weiteres Anliegen der "Progressives" war das Frauenwahlrecht. Die Verfassung bestimmte ursprünglich, dass bei Wahlen des Kongresses und des Präsidenten jeder das aktive Wahlrecht hat, der in seinem Bundesstaat für die größte Parlamentskammer aktiv wahlberechtigt ist. Damit stand es den Bundesstaaten frei, Bevölkerungsgruppen per Gesetz von der Wahl auszuschließen. Die Verfassung wurde bereits 1870 geändert, um Rasse, Hautfarbe und ehemaligen Sklavenstatus als Ausschlussmerkmal zu verbieten. Trotz anfänglichen Widerstands seitens des Präsidenten Woodrow Wilson kam 1919 mit dem 19. Zusatzartikel das Geschlecht als verbotenes Ausschlussmerkmal hinzu.

Die durch die Weltwirtschaftskrise ausgelöste Große Depression war das entscheidende Wahlkampfthema während der Präsidentschaftswahl 1932. Der amtierende Präsident Herbert Hoover sprach sich gegen staatliche Einflüsse aus und setzte glücklos auf den amerikanischen Individualismus und eine „natürliche“ wirtschaftliche Verbesserung. Franklin D. Roosevelt gewann die Wahl im November 1932 mit 89 % der Stimmen im "Electoral College", konnte aber aufgrund der Bestimmungen der Verfassung erst zum 4. März 1933 sein Amt antreten. Gleichzeitig hatte Hoover nur noch wenig politischen Rückhalt, so dass das Regierungsgeschäft faktisch zum Erliegen kam. Eine ähnlich kritische Situation gab es zuvor schon 1861, als mehrere Südstaaten nach der Wahl Abraham Lincolns die Vereinigten Staaten verließen, Lincoln aber erst im März als Präsident darauf reagieren konnte. Der 1933 ratifizierte 20. Zusatzartikel sieht daher vor, dass die Amtseinführung bereits am 20. Januar des Jahres nach der Wahl stattfinden sollte. Gleichzeitig hob der Artikel die Bestimmung auf, dass die vor der Wahl amtierenden Abgeordneten und Senatoren noch einmal zu einer Zwangssitzungsperiode zusammenkommen mussten.

Bis zur Ratifizierung des 22. Zusatzartikels enthielt die Verfassung keine Begrenzung, wie oft ein Präsident wiedergewählt werden konnte, auch wenn eine höchstens einmalige Wiederwahl Tradition war. Präsident Franklin Roosevelt brach während der Präsidentschaftswahl 1940 im Schatten des gerade ausgebrochenen Zweiten Weltkriegs mit dieser Konvention. Roosevelt konnte sich auf einen breiten Rückhalt in der Bevölkerung stützen und gewann die Wahl mit 55 % der Direktstimmen und 85 % der Stimmen im Electoral College. Eine vierte Wiederwahl gelang Roosevelt auf dem Höhepunkt des Zweiten Weltkriegs 1944, aber er starb wenige Monate später an den Folgen einer Hirnblutung im Alter von 63 Jahren, wodurch die Vereinigten Staaten in der entscheidenden Endphase des Krieges und in den Verhandlungen mit Stalin sich mit einem unvorhergesehenen Führungswechsel konfrontiert sahen. Nach Ende des Krieges setzte sich der neugewählte Kongress zum Ziel, die Tradition wiederherzustellen und die Anzahl der möglichen Wiederwahlen zu begrenzen. Die Verfassungsänderung setzt die Amtszeit auf maximal acht Jahre fest. Ausgenommen sind Vizepräsidenten, die das Präsidentenamt ohne Wahl erlangt haben und in dieser Funktion kürzer als zwei Jahre im Amt waren.

Gemäß Artikel II der Verfassung wird der Präsident von Wahlmännern gewählt, die von den einzelnen Bundesstaaten bestimmt werden. Ein Wahlrecht für die Bewohner des District of Columbia war nicht vorgesehen, genauso wenig wie für die anderen Territorien der Vereinigten Staaten, die zu keinem Bundesstaat gehörten. Der 1961 ratifizierte 23. Zusatzartikel änderte diese Regelungen und teilte dem Regierungsbezirk genauso viele Wahlmänner zu, wie dem bevölkerungsschwächsten Bundesstaat zustanden. Im Kongress ist der Distrikt jedoch bis heute nur durch einen nicht stimmberechtigten Repräsentanten vertreten.

Um das Verbot einer Einschränkung des Wahlrechts für Schwarze aufgrund ihrer Hautfarbe, wie im 15. Zusatzartikel festgelegt, zu umgehen, gingen eine Reihe von Bundesstaaten dazu über, von allen Bürgern Kopfsteuern zu erheben. Nichtzahlung dieser Steuern führte zum Verlust des Wahlrechts. Die entsprechenden Gesetze enthielten meist eine Regelung, die jeden von der Zahlung der Steuer ausnahm, dessen Vorfahren in einem bestimmten vor dem Sezessionskrieg liegenden Jahr wahlberechtigt waren. Damit wurden die meist sehr hohen Steuern faktisch nur von ehemaligen Sklaven und Einwanderern eingezogen, die praktische Folge war deren Ausschluss von der Wahl. Der 24. Zusatzartikel verbot diese Steuern 1962 im Verlauf der Bürgerrechtsbewegung.

Im Gegensatz zu parlamentarischen Regierungssystemen sieht die Verfassung der Vereinigten Staaten keine Möglichkeit vor, außerhalb der festen Wahltermine einen neuen Kongress oder einen neuen Präsidenten zu wählen. Als Konsequenz musste die Nachfolgeregelung bei Rücktritt, Amtsunfähigkeit oder Tod des Präsidenten oder Vizepräsidenten vergleichsweise umfangreich geregelt werden, wie dies 1965, unter dem Eindruck des Kalten Krieges und des Kennedy-Mordes, mit dem 25. Zusatzartikel geschah. Der Artikel sieht vor, dass der Vizepräsident zum Präsidentenamt aufrückt, wenn dieses unbesetzt ist; für den Fall dass beide Ämter unbesetzt sein sollten, kann der Kongress eine gesetzliche Regelung erlassen. Diese sieht heute vor, dass die Parlamentspräsidenten und die Bundesminister in einer festgelegten Reihenfolge nachrücken, sodass insgesamt eine Nachrückerliste von mehr als 20 Personen existiert. Gleichzeitig wird dem Präsidenten das Recht eingeräumt, mit Zustimmung beider Kammern des Kongresses einen neuen Vizepräsidenten zu ernennen, sollte dieses Amt zeitweise nicht besetzt sein. Neben der Nachfolgeregelung sieht der Artikel vor, dass der Präsident seine vorübergehende Amtsunfähigkeit erklären kann. Ebenso kann das Kabinett mit Zustimmung des Kongresses mehrheitlich beschließen, dass der Präsident amtsunfähig ist. In beiden Fällen übernimmt der Vizepräsident die Regierungsgeschäfte, bis der Präsident entweder seine Amtsfähigkeit erklärt, zurücktritt, des Amtes enthoben wird oder verstirbt.

Die Vorgaben des 25. Zusatzartikels wurden bereits kurz nach der Verabschiedung angewandt, als 1973 Vizepräsident Spiro Agnew aufgrund eines politischen Skandals zurücktrat und Präsident Richard Nixon Gerald Ford zu seinem neuen Vizepräsidenten ernannte. Mit Nixons Rücktritt im Zuge der Watergate-Affäre 1974 wurde Ford Präsident und ernannte Nelson Rockefeller zum Vizepräsidenten. Der Artikel kommt auch zur Anwendung, wenn sich der Präsident längeren medizinischen Behandlungen, wie beispielsweise Operationen unterziehen muss, so im Fall der Präsidenten Ronald Reagan 1985 und George W. Bush 2005.

In den meisten Bundesstaaten erhielt man das aktive Wahlrecht mit 21 Jahren, in einigen wenigen mit 20 oder 19 Jahren. Während des Vietnamkrieges sprachen sich einige Politiker, darunter mehrere Kongressabgeordnete und Präsident Lyndon B. Johnson, dafür aus, dass alle Wehrpflichtigen auch wahlberechtigt sein müssten; die Wehrpflicht galt damals ab achtzehn Jahren. Grund war, dass dieses fehlende Wahlrecht der jungen Soldaten bei Antikriegsprotesten häufig als Rechtfertigungsgrund für zivilen Ungehorsam genannt wurde. Der 26. Zusatzartikel, der den Ausschluss von der Wahl aus Gründen des Alters im Falle über 18-jähriger Personen untersagte, wurde 1971 vom Kongress verabschiedet und trat im selben Jahr in Kraft.

Seit 1789 wurden dem Kongress über 10.000 Vorschläge zur Verfassungsänderung vorgelegt, in den letzten Jahrzehnten gab es pro Sitzungsperiode zwischen 200 und 300 solcher Vorschläge. Die wenigsten überstanden die Ausschussarbeit und wurden vom Kongress verabschiedet. Einige Male wurde auch das Verfahren zur Einberufung eines Verfassungskonvents angewandt, bisher allerdings ohne Erfolg. In zwei Fällen – ein Vorschlag zur Neuregelung der Sitzverteilung 1960 und ein Vorschlag zur Beschränkung der Staatsverschuldung in den 1970ern und 1980ern – fehlten nur zwei Bundesstaaten für die für einen Verfassungskonvent notwendige Mehrheit.

Von den 33 Verfassungsänderungen, die der Kongress den Bundesstaaten zur Ratifikation vorgelegt hatte, scheiterten sechs an der Mehrheitsschwelle, davon könnten vier theoretisch noch angenommen werden. Seit dem 18. Zusatzartikel umfasste jeder Vorschlag, außer dem 19. und dem nicht ratifizierten Artikel bezüglich Kinderarbeit, eine ausdrückliche zeitliche Beschränkung der Ratifikation.
Für die folgenden Vorschläge steht die Ratifikation noch aus:

Während eine grundsätzliche Kritik der Verfassung in Fachkreisen nur selten geäußert wird, gibt es einzelne Bestandteile, die wiederholt zu teilweise sehr heftigen politischen und gesellschaftlichen Diskussionen geführt haben.

Der indirekte Wahlmodus für die Ämter des Präsidenten und Vizepräsidenten war zum Zeitpunkt des Verfassungskonvents stark umstritten und ist auch in den letzten Wahlen immer wieder thematisiert worden. So wird beispielsweise kritisiert, dass die von der Bevölkerung gewählten Wahlmänner nicht an ihr Wahlversprechen gebunden sind und dass ein Kandidat, der weniger Stimmen erhalten hat als ein anderer Kandidat, trotzdem die Wahl gewinnen kann, wie es bisher fünfmal geschehen ist (Donald Trump/Hillary Clinton 2016, George W. Bush/Al Gore 2000, Benjamin Harrison/Grover Cleveland 1888, Rutherford B. Hayes/Samuel J. Tilden 1876, John Quincy Adams/Andrew Jackson 1824). Letzteres ist auf das Mehrheitswahlrecht in fast allen Bundesstaaten zurückzuführen, wonach alle Stimmen eines Bundesstaates an den Kandidaten gehen, der bei der Wahl der Wahlmänner die Mehrheit innerhalb des Bundesstaats erhält.

Im Vergleich zu anderen Ländern wird die Meinungsfreiheit in den Vereinigten Staaten seit Mitte des 20. Jahrhunderts sehr freizügig gehandhabt. Der Kongress und die Bundesstaaten haben grundsätzlich kein Recht, die Meinungsfreiheit per Gesetz einzuschränken. Ausnahmen gibt es hierbei nur bei Verleumdung, Meineid und Verrat sowie in Bereichen außerhalb der Öffentlichkeit; die Äußerung extremistischer politischer Meinungen ist dagegen durchweg erlaubt, wenn nicht unmittelbar zu konkreten Gewalttaten aufgerufen wird. Diese Freizügigkeit hat unter anderem in den 1990ern zu Konflikten geführt, als ein Verbot der Flaggenschändung durch Verbrennen vom Obersten Gerichtshof im Fall Texas v. Johnson für verfassungswidrig erklärt wurde und der Kongress daraufhin erfolglos versucht hat, das Urteil durch neue Gesetze aufzuheben. Ein Vorschlag für eine dahingehende Verfassungsänderung ist bisher immer im Senat gescheitert.

Als "„Unitary Executive“" wird eine Auslegung der Verfassung bezeichnet, die von einer einheitlichen und vollständigen ausführenden Gewalt im Amt des Präsidenten ausgeht und Einschnitte in dessen Befugnisse durch Gerichte oder den Kongress als verfassungswidrig ansieht. Die Theorie folgt aus dem ersten Satz des zweiten Artikels:

Daraus wird unter anderem abgeleitet, dass der Präsident an der Spitze der gesamten Bundesregierung samt allen untergeordneten Behörden steht und insbesondere bei Personalentscheidungen unabhängig agieren kann. Von ihm ernannte Amtsträger arbeiten demnach an seiner Stelle und aufgrund der von ihm übertragenen Befugnisse, die er jederzeit wieder entziehen kann. Daraus wird geschlussfolgert, dass der Kongress keine ausführenden Behörden außerhalb der Kontrolle des Präsidenten schaffen dürfe. Juristisch ergibt sich der Grundsatz, dass eine ausführende Behörde eine andere nicht verklagen kann, da in solchen Fällen der Präsident sowohl Kläger als auch Beklagter wäre.

Thematisch brisant wurde die Theorie besonders im Zuge der Watergate-Affäre, als Präsident Nixon die Herausgabe von ihn belastenden Tonbändern zu verhindern versuchte. Er befahl dem amtierenden Justizminister Elliot L. Richardson, den für den Einbruch im Watergate-Hotel zuständigen Chefermittler Archibald Cox zu entlassen und einen neuen zu bestellen. Richardson weigerte sich, dem Befehl nachzukommen, und trat zurück. Als sein Stellvertreter William Ruckelshaus sich ebenfalls weigerte, entließ Nixon ihn und ernannte den Solicitor General Robert Bork zum kommissarischen Justizminister, der Nixons Anweisungen nun nachkam und Cox entließ.

Präsident George W. Bush hat diese Theorie als Grundlage dafür genommen, bestimmte Gesetze bei der Unterzeichnung mit einem Signing Statement zu versehen, worin er seine Auffassung darüber mitteilt, wie das Gesetz auszuführen sei.

Viele Rechtsgelehrte in den Vereinigten Staaten tragen untereinander bezüglich der Auslegung der Verfassung eine latente Grundsatzdebatte aus, deren Kernfrage in den Absichten der Autoren der Verfassung und der Gründerväter und im zeitgenössischen Umgang mit diesen Absichten liegt. Von politischer Bedeutung ist die Haltung eines Juristen dann, wenn er vom Präsidenten als Richter für den Obersten Gerichtshof nominiert wird, da seine juristische Positionierung auch begrenzte Rückschlüsse auf seine politische Haltung zulässt. Dies betrifft in den Vereinigten Staaten stark umstrittene Themen wie die Abtreibung und die Grenzen der Meinungsfreiheit.
Im Laufe der Zeit haben sich mehrere Denkschulen etabliert, wie die Verfassung im Grundsatz zu handhaben sei. Die Auslegungsdebatte bezieht neben den Rechtswissenschaften Erkenntnisse aus der Geschichtswissenschaft, der Moralphilosophie und der Forschung zur englischen Sprache ein.

Der Originalismus (englisch: "Originalism", von lateinisch: "originis", zu deutsch: „Abstammung“, „Ursprung“) misst der ursprünglichen Absicht der Väter der Verfassung eine hohe Bedeutung bei und versucht stets, ihn bei der Einschätzung verfassungsrechtlicher Fragen zu rekonstruieren. Dabei beziehen Originalisten nicht nur den Verfassungstext an sich in ihren Entscheidungsprozess mit ein, sondern auch alle Dokumente, die im Laufe seiner Entstehung geschrieben wurden. Dazu zählen nicht nur Notizen, Redemanuskripte und Randbemerkungen der Teilnehmer der "Philadelphia Convention", sondern beispielsweise auch die Föderalistenartikel. Als prominente Vertreter des Originalismus gelten der amtierende Richter am Obersten Gerichtshof Clarence Thomas und der am 13. Februar 2016 verstorbene Associate Justice am Obersten Gerichtshof Antonin Scalia.

Unter dem Begriff Textualismus, im Englischen auch als "Literalism" bezeichnet, firmiert die Denkschule, die eine wortgetreue Auslegung der Verfassung befürwortet. Sie lehnt die Einbeziehung des dokumentierten Entstehungsprozesses und auswärtige Kommentare wie die Föderalistenartikel ab. Bei der Handhabung des Wortlauts sind die Anhänger des Textualismus gespalten. Während die einen die Bedeutung des Wortlauts auf die Verwendung der englischen Sprache zur Zeit der Ausarbeitung beziehen, sprechen sich die anderen für eine Auslegung nach Maßgabe des Englischen aus, wie es heute verwendet wird.

Für eine zeitgemäße Interpretation der Verfassung der Vereinigten Staaten tritt der Funktionalismus "(functionalism)" ein, der auch unter den Begriffen Instrumentalismus "(instrumentalism)" oder Strukturalismus "(structuralism)" bekannt ist. Er erachtet eine Wechselwirkung zwischen Wortlaut der Verfassung und Rechtspraxis als gegeben und verwirft die wortgetreuen Auslegungsströmungen als subjektiv.

Der Doktrinalismus "(doctrinalism)" orientiert sich an den historischen Entwicklungsschritten der Verfassungswirklichkeit. Auf der Grundlage des "stare decisis" geht nach einer im Fallrecht anerkannten Methode vor, Urteile und Entscheidungen früherer Oberster Richter als bindend anzusehen, es sei denn, die Voraussetzungen einer solchen Entscheidung haben sich geändert.

Im Kontextualismus "(contextualism)" spielt die Absicht der Gründerväter eine Rolle, jedoch wird deren politische Weitsicht im Unterschied zum Originalismus angezweifelt. Kontextualisten relativieren die Bedeutung der Autoren der Verfassung und versuchen, ihre Reglementierungen in den historischen Kontext einzuordnen. So sollen ihre Absichten berücksichtigt, aber nicht in als hinderlich empfundener Art und Weise auf heutige Fälle angewendet werden.

In Lehre und Forschung werden die unterschiedlichen Lesarten anhand von wiederkehrenden Beispielen gegeneinander abgewogen. Beispielsweise sieht die Verfassung zwar die Aufstellung von Land- und Seestreitkräften sowie die Existenz von Milizen vor, allerdings kannten die Teilnehmer der "Philadelphia Convention" die Luftfahrt nicht. Daher zweifeln Vertreter einer besonders wortgetreuen Auslegung der Verfassung die Existenz der Luftstreitkräfte als eigenständige Teilstreitkraft an und verlangen deren Dezentralisierung.








</doc>
<doc id="13305" url="https://de.wikipedia.org/wiki?curid=13305" title="Traditionelle chinesische Medizin">
Traditionelle chinesische Medizin

Die "Chinesische Medizin" umfasst die heilkundliche Theorie und Praxis von der vormedizinischen Heilkunde des 1. Jahrtausends v. Chr. bis zur heutigen Medizin in China und in der Tradition chinesischer Heilkunde ausgeübter Heilverfahren weltweit.

Als traditionelle chinesische Medizin, TCM oder chinesische Medizin () wird jene Heilkunde bezeichnet, die sich in China seit mehr als 2000 Jahren entwickelt hat. Ihr ursprüngliches Verbreitungsgebiet umfasst den ostasiatischen Raum, insbesondere Vietnam, Korea und Japan. Auf dieser Grundlage entwickelten sich spezielle Varianten in diesen Ländern. Besonders bekannt ist die japanische Kampo-Medizin.

Der Begriff "zhōngyī" () ist sowohl mit "chinesische Medizin" als auch "TCM-Arzt" übersetzbar. Die im Westen gebräuchliche Bezeichnung "traditionelle chinesische Medizin", also "TCM", ist im sprachlichen Gebrauch in China unüblich. Daher fehlt es gewöhnlich im Chinesischen Ausdruck meist das Wort "traditionell".

Zu den therapeutischen Verfahren der chinesischen Medizin zählen vor allem die Chinesische Arzneimitteltherapie und die Akupunktur sowie die Moxibustion (Erwärmung von Akupunkturpunkten). Zusammen mit Massagetechniken wie Tuina Anmo und Shiatsu, mit Bewegungsübungen wie Qigong und Taijiquan und mit einer am Wirkprofil der Arzneien ausgerichteten Diätetik werden die Verfahren heute gerne als die "fünf Säulen" der chinesischen Therapie bezeichnet. Die TCM ist die traditionelle Medizin mit dem größten Verbreitungsgebiet, besonders die Akupunktur wird heute weltweit praktiziert. Die TCM gilt als alternativ- oder komplementärmedizinisches Verfahren. Das größte einschlägige Forschungszentrum ist die Chinesische Akademie für traditionelle chinesische Medizin.
Von wissenschaftlicher Seite, insbesondere der evidenzbasierten Medizin, wird die therapeutische Wirksamkeit vieler Behandlungsmethoden der TCM bezweifelt und etliche Behandlungsmethoden als pseudowissenschaftlich betrachtet.
Dennoch haben sich einige Behandlungsmethoden der TCM wie z. B. die Akupunktur mittlerweile in der westlichen Heilkunde etabliert. 2002 veröffentlichte die WHO eine Indikationsliste für Akupunktur, auf der diese bei 28 Krankheitsbildern empfohlen wird.

Das historische Quellenmaterial erstreckt sich über mehr als drei Jahrtausende. Diese Zeitspanne ist grob in drei Sozialepochen zu gliedern:
Auf konzeptioneller Ebene findet sich eine Vielzahl unterschiedlicher Heilsysteme, die mit wenigen Ausnahmen bis in die Gegenwart überliefert und praktiziert wurden. Nebeneinander, teils in einem und demselben medizinischen Werk, finden sich Theorien, die die Verursachung der Krankheiten auf Sündenfall, Dämoneneinfluss, Abweichung von normgerechtem Lebensstil oder Böswilligkeit verstorbener Ahnen oder Mitmenschen zurückführten. Sie lassen sich aber auch den unterschiedlichen Epochen und verschiedenen gesellschaftlichen Gruppen zuordnen.

Die frühesten Quellen bilden Orakelknochen und Schildkrötenschalen, die etwa im 13. Jahrhundert vor unserer Zeitrechnung beschriftet wurden. Den Texten kann man entnehmen, dass die Verursachung von Krankheiten in fast allen Fällen auf ein mögliches Einwirken verstorbener Ahnen oder Drittpersonen sowie auf böswillige Magie, also die Einwirkung noch lebender Mitmenschen, zurückgeführt wurde. Als entsprechende Vorbeugungs- und Heilmaßnahmen gelten Beschwörungen, Geschenke und Versöhnungsgaben. Shang-Herrscher war ein König, dem allein die Befragung und Deutung der Orakel und somit die Praxis der Ahnenmedizin oblag. Zu seiner Klientel zählte der kleine Kreis der herrschenden Elite, im Fall von Epidemien die gesamte Gesellschaft.

Eine Fortentwicklung der für die Shang-Kultur belegten Ahnenmedizin führte zur Dämonenmedizin, die schon Jahrhunderte vor unserer Zeitrechnung nachweisbar ist. Aus ihr wiederum ging die entsprechungssystematische Medizin hervor. Ausgangspunkt des Heilsystems der Dämonenmedizin ist die Annahme, dass Krankheiten durch die Einwirkung böswilliger Dämonen verursacht werden. Die so genannten "wu"-Zauberer verstanden sich zum einen auf die Kommunikation mit den Geistern der Verstorbenen und holten deren Ratschläge vor allem bei gesundheitlichen Beschwerden ein; zum anderen auf die Beeinflussung und Vertreibung von Dämonen, die unabhängig von bestimmten Verstorbenen das Universum bevölkerten. Die medizinische Praxis dieser "wu"-Zauberer wandelte sich allmählich zu einer reinen Dämonenmedizin.

"„Die Dämonen sind ständig gegenwärtig, sichtbar und unsichtbar und benutzen jede Schwäche der Menschen zum Angriff. Nur wenn die von der eigenen Person ausgehenden Schutzgeister und Dämonen stark genug sind oder wenn man imstande ist, solche Wesen zu seinem eigenen Beistand zu gewinnen, deren Position in der metaphysischen Hierarchie höher ist als die der Angreifer, ist man vor den entsprechenden Bedrohungen geschützt oder im Krankheitsfall für den Gegenangriff gewappnet.“"

Unschuld sieht die Dämonenmedizin als getreues Abbild der gesellschaftlichen Verhältnisse im Zeitraum vom 8. bis ins 3. Jahrhundert vor unserer Zeitrechnung: nahezu ununterbrochene Machtkämpfe und der zeitweilige Zerfall Chinas in Hunderte von Kleinstaaten, die „jeder gegen jeden“ kämpften.
Als ursprüngliche Heilverfahren der Dämonenmedizin sind wahrscheinlich die Nadelbehandlung (Akupunktur), das Brennen (Moxibustion) und die Massage anzusehen. Ziel ihrer Anwendung war, die Eindringlinge zum Verlassen des Körpers zu zwingen. Später wurden diese Verfahren in die entsprechungssystematische Medizin integriert. Der Kampf gegen die dämonischen Angreifer wurde nun auf der Grundlage von „Verboten“ oder stark wirksamen Arzneidrogen geführt. Erstere wurden von Exorzisten oder der betroffenen Person selbst ausgesprochen oder niedergeschrieben; Letztere konnte man einnehmen oder als Amulett mit sich führen. Die auf dem Grundkonzept der Dämonenmedizin basierenden Vorbeugungs- und Heilverfahren wurden in der Folgezeit von Autoren unterschiedlichster Bildungsstufen zur Anwendung empfohlen und behielten bis in die Gegenwart einen herausragenden Stellenwert in der medizinischen Versorgung der chinesischen Bevölkerung.

Während der zweiten Han-Dynastie (25 bis 220) entstanden verschiedene religiöse Heilsysteme als Teilaspekte von Bemühungen, gesellschaftspolitische Organisationsformen auf theokratischer Grundlage durchzusetzen. So hatte sich der General Zhang Xiu (Chang Hsiu) als erfolgreicher Kriegsherr in einem Gebiet Sichuans etabliert und begonnen, eine sowohl auf religiöser wie militärischer Grundlage ruhende neue soziale Hierarchie aufzubauen. Die zunächst als Heilkult erscheinende Bewegung (vgl. Fünf Scheffel Reis) vertrat die Idee, dass Krankheit die gerechtfertigte Strafe für vergangenes Missverhalten sei. Für die Vergeltung seien aber nicht die Totengeister verstorbener Ahnen verantwortlich, sondern bestimmte Gottheiten. Diesen gegenüber sei vor allem Reue angebracht. Daher ließ Chang Hsiu die Kranken einkerkern. Die Zeit im Gefängnis sollten sie darauf verwenden, ihre vergangenen Sünden zu erkennen. Die Heilung sei nur dadurch möglich, dass der Kranke seine Sünden auf drei Blatt Papier schrieb, die für die Drei Herrscher Himmel, Erde und Wasser auf einem Berggipfel hinterlegt oder in der Erde vergraben und in einen Fluss geworfen wurden. Nachdem Chang Hsiu dem Mordanschlag des Generals Zhang Lu zum Opfer gefallen war, baute dieser ebenfalls ein theokratisches Herrschaftsgebilde auf. Zhang Lu folgte auch dem Konzept, dass menschliches Fehlverhalten von den Göttern durch Krankheit geahndet werde. Daher ließ er Verbrecher so lange ungestraft, bis sie zum dritten Mal rückfällig wurden.
Ein weiterer Kult entstand im 2. Jahrhundert nach unserer Zeitrechnung als "Bewegung des Großen Friedens" mit Zhang Jiao an der Spitze. Seine Heilungen bestanden in dramatischen öffentlichen Massenritualen, während der die Leidenden ihre Missetaten bekennen mussten. Hunderttausende strömten zu diesen Ritualen. In jahrzehntelangen militärischen Auseinandersetzungen zerschlug die Zentralregierung schließlich die theokratischen Staatswesen. Die Verteidigung der Anhänger Zhang Jiaos ging als Aufstand der Gelben Turbane in die Geschichte ein.

Die Heilsysteme, die als Entsprechungsmedizin bezeichnet werden können, beruhen auf dem Paradigma, dass die Phänomene der sichtbaren und der unsichtbaren Umwelt in gegenseitiger Abhängigkeit stehen. Dabei lassen sich ältere magische Konzepte ("„Entsprechungsmagie“") von späteren "systematischen" unterscheiden. Letztere wurden zu einem zunehmend detaillierten System entwickelt unter Zuhilfenahme der Yin-Yang-Lehre und der Theorie der Fünf-Elemente-Wandlungsphasen. Ihre Grundlagen entsprechen wiederum den in derselben Epoche konzipierten gesellschaftspolitischen Vorstellungen der konfuzianischen Staatsideologie.

Schien zuvor der Einfluss von Dämonen allgegenwärtig, so waren es in der sich entwickelnden entsprechungssystematischen Medizin Einflüsse und Ausstrahlungen aller nur erdenklichen Naturphänomene, mit denen man in Einklang leben musste: Himmelsrichtungen, Gestirne, Lebensmittel, Himmel und Erde, Regen und Wind, Hitze und Kälte. Es wurde weniger eine exakte Anatomie in diesem Heilsystem entwickelt, sondern eher ein hoch kompliziertes spekulatives System physiologischer Vorgänge, das die Wirkungen und Wandlungen der vielfältigen Einflüsse und Ausstrahlungen mit den weltanschaulichen Konzepten der Yin-Yang-Lehre und der Lehre von den Fünf-Elemente-Wandlungsphasen zu verbinden suchte.

Vorbeugungs- und Heilmaßnahmen wurden entsprechend dieser Systematik entwickelt. Grundsätzlich ging es darum, Überflusserscheinungen „abzuleiten“ und Mangelerscheinungen „aufzufüllen“. Ziel war eine Harmonisierung der Strömungen und Wandlungen im Organismus. Dies entsprach den Vorstellungen der Konfuzianer zur sozialen Ordnung. So lange der Konfuzianismus in China bestimmend war, schützte die herrschende Schicht die entsprechende Medizin als die offiziell einzig zulässige. Dadurch wurde ein archaisches Heilsystem bis in die Neuzeit hinübergerettet. Das klassische schriftliche Zeugnis "Huang-ti nei-ching" oder "Huangdi Neijing" stammt etwa aus dem 3. Jahrhundert vor unserer Zeitrechnung. An Heiltechniken werden hier vor allem die wahrscheinlich der Dämonenmedizin entlehnten Verfahren des Nadelns und Brennens dargelegt. Einige Passagen enthalten Hinweise auf Massage, Waschungen und heiße Pressungen. Es werden auch einige Arzneidrogen erwähnt. Deren Anwendung war jedoch offensichtlich noch nicht gemäß den theoretischen Grundlagen der entsprechungssystematischen Medizin vorgesehen. Die versuchsweise systematische Integration bestimmter Arzneidrogen in dieses Heilsystem erfolgte erst anderthalb Jahrtausende später.

Zu den „Vier Herausragenden Ärzten“ der Jin- und Yuan-Dynastien zählen im 12. Jahrhundert Liu Wansu und sein Schüler Zhang Yuansu sowie dessen Schüler Li Gao (1180–1251) und im 15./16. Jahrhundert Zhu Zhenheng (1563–1640).

Die ältesten medizinischen Grundlagenwerke, die noch heute im Gebrauch sind, werden Kaisern zugeschrieben, die mehrere Jahrtausende vor unserer Zeitrechnung gelebt haben sollen. Das sind jedoch Legenden. Bekannt sind das Shennong ben cao jing, eine Pflanzenheilkunde, und das Huangdi Neijing, eine ausführliche Darstellung sowohl der Diagnose- und Therapieverfahren als auch der Akupunktur. Nach Beginn unserer Zeitrechnung entstand das Shang Han Lun, eine Abhandlung über Kälte-Krankheiten. Sie gilt als die älteste klinische Abhandlung der Medizingeschichte überhaupt. Aus der Zeit der Ming-Dynastie (1368 bis 1644) stammt eine Reihe berühmter Schriften, darunter das Ben Cao Gang Mu, ein Kompendium der Materia Medica.

Mit Beginn der jesuitischen Mission in Fernost nahm auch der medizinische Austausch zwischen Europa und Ostasien einen Aufschwung. In Japan hatten Jesuiten bereits in der zweiten Hälfte des 16. Jahrhunderts die einheimische Medizin beobachtet, wie eine Fülle von Bemerkungen in ihren Briefen, die Aufnahme sinojapanischer Fachbegriffe in ihre Wörterbücher und ein Vergleich von westlicher und japanischer Medizin durch Luís Fróis zeigt. Im 17. Jahrhundert bewirkte in China der Jesuit Johann Schreck um 1625 einen weitern Austausch chinesischen und westlichen Wissens im Bereich von Technik und Anatomie.
Ab Beginn der Qing-Dynastie (1644–1912), zu deren berühmtesten Ärzten und Medizinschriftstellern der auch unter dem Namen Ye Tianshi bekannte Ye Gui (1667–1746) gehörte, wirkten Jesuiten auch in China am kaiserlichen Hof als Astronomen, Geographen, Maler, Architekten oder Mathematiker. Neben kürzeren Ausführungen in einigen ihrer Briefe findet man hier umfangreiche Übersetzungen chinesischer Texte, die durch den deutschen Arzt und VOC-Kaufmann Andreas Cleyer als "Specimen Medicinae Sinicae, sive, Opuscula medica ad mentem sinensium (Frankfurt, 1682)" publiziert wurden. Ebenso wichtig war der Druck von "Clavis medica ad Chinarum doctrinam de pulsibus" aus der Feder Michael Boyms.
Die erste längere Abhandlung über die Moxa verfasste der in Batavia lebende niederländische Pfarrer Hermann Buschoff. Durch seine auch ins Deutsche und Englische übersetzte Schrift "Het Podagra, Nader als oyt nagevorst en uytgevonden, Midsgaders Des selfs sekere Genesingh of ontlastend Hulp-Mittel (1674)" wurde das japanische Wort "mogusa" (chinesisch ai) als Moxa in Europa etabliert. Der von Buschoff stimulierte niederländische Arzt Willem ten Rhijne (1647–1700) ging der Brenntherapie in der Niederlassung Dejima (Nagasaki, Japan) der Ostindien Kompanie weiter nach. Sein 1683 in London gedrucktes Sammelwerk enthält die erste ausführliche Abhandlung zur Nadelung, der er den Namen "acupunctura" gab. Durch ten Rhijne stimuliert, sammelte der deutsche Arzt Engelbert Kaempfer (1651–1716) ebenfalls in Japan weitere Informationen und Materialien, die er 1712 publizierte. Seine langen Aufsätze zur Akupunktur und Moxa wurden im Anhang seines in viele Sprachen übersetzten Japanbuches weit verbreitet. Während die Moxatherapie auf reges Interesse stieß, reagierten Autoritäten wie Georg Ernst Stahl oder Lorenz Heister auf ten Rhijnes und Kaempfers Beschreibungen der Akupunktur ablehnend – nicht zuletzt, weil beide mit dem Terminus Qi nicht zurechtkamen, so dass man den Eindruck gewinnen konnte, die Ärzte in Ostasien würden in den Bauch stechen, um Darmgase abzuleiten.

In einer neuen Lage befand sich China in der zweiten Hälfte des 19. Jahrhunderts. Westliche Mächte hatten mit Waffengewalt den Zugang zu den chinesischen Märkten erzwungen und den ersten (1839–1842) und zweiten Opiumkrieg (1856 bis 1860) geführt. Westliche Technik und Wissenschaft drang in der Folge ungehindert in den Alltag der städtischen Bevölkerung ein. In den Städten wuchs die Zahl derer, die ihre Krankheiten nach den importierten westlichen Methoden behandelt haben wollten, nicht mehr nach den hergebrachten. Diejenigen, die nach altem Handwerk zu heilen versprachen, wurden in die Enge getrieben. Es gab Überlegungen, diese zu verbieten, da sie als Hemmschuh für eine reibungslose Transformation in den westlichen Stil der Effektivität durch Rationalität gesehen wurden. So gerieten die traditionellen Diagnose- und Therapie-Verfahren Ostasiens zunehmend in Bedrängnis.

Nach der Gründung der Volksrepublik China kam es unter Mao Zedong zu einer staatlich vorangetriebenen Gegenbewegung. Es galt nun, die ländliche Bevölkerung eines riesigen Reiches bei begrenzten Mitteln ärztlich zu versorgen. Die Lösung sah man in der Pflege und Kontrolle der althergebrachten Heilkunst, die gerade in der ländlichen Bevölkerung verbreitet war. Neue Hochschulen für die chinesische Medizin wurden gegründet, alte Klassiker neu entdeckt und für die Moderne aufbereitet. Mit den „Barfußärzten“ – in Kurzlehrgängen ausgebildeten TCM-Ärzten – wurde die medizinische Versorgung flächendeckend organisiert.

Erst jetzt verbreitete sich die Bezeichnung „chinesische Medizin / chinesische Heilkunde“ (), in der englischen Übersetzung mit dem Zusatz „traditional“ und der Abkürzung „TCM“. In China bezog sich der Begriff oft weniger auf die traditionelle Medizin im umfassenden Sinn als auf das neu geschaffene Gesundheitswesen.

In Korea, Japan und Vietnam wurde der Begriff der „chinesischen Medizin“ nicht übernommen. Dort hat sich für die chinesische Medizintradition die Bezeichnung "Oriental Medicine" (jap. - dt.: ostasiatische Medizin) durchgesetzt. "Kampō" (jap. ) bedeutet ursprünglich allerdings die Rezeptur ( jap. "pō"; chin. "fāng") der Chinesen (, jap. "kan"; chin. "hàn"), da sie von dort vorwiegend im 6. Jh. übernommen wurde.

In Europa reicht die älteste Beschäftigung mit chinesischer Medizin - mit Akupunktur und Moxibustion - in die Zeit des ausgehenden 17. Jahrhunderts zurück. Neues Interesse kam im Westen in der ersten Hälfte des 20. Jahrhunderts auf und mit der Öffnung der Volksrepublik China in den 70er Jahren begann der Transfer der Methoden nach Nordamerika und Europa unter dem Begriff der TCM.

In Taiwan konnte sich die traditionelle Medizin trotz Modernisierung halten und wird heute ergänzend praktiziert. Taiwan hat seine eigene TCM-Tradition, die stärker durch alte Ärztefamilien geprägt ist, traditioneller und somit weniger standardisiert ist, mehr spirituelle Elemente beibehielt. Taiwan bildet kaum Ausländer in der TCM aus. Nicht unerwähnt sollte jedoch bleiben, dass der Durchschnittstaiwaner im Zweifelsfall eher auf Behandlungsmethoden der modernen Medizin vertraut. TCM findet vor allem bei chronischen Erkrankungen, als Zusatzbehandlung, oder zur Versorgung von Patienten im Rahmen der palliativen Versorgung Anwendung. Großer Beliebtheit erfreuen sich in Taiwan Restaurants, die medizinale Gerichte entsprechend der Ernährungslehre der CM anbieten.

Der Import chinesischer Medizin nach Japan setzte zu Beginn des 7. Jahrhunderts ein. Ein frühes Zeugnis der Beschäftigung mit dieser Heilkunst ist die medizinische Schrift "Ishimpō" von 982, die heute auch geschätzt wird, weil sie Teile chinesischer Texte enthält, die in China verloren gegangen sind.

Seit der Entstehung staatlicher Strukturen in Japan wurde die Medizin des Archipels stets von chinesischen Quellen stimuliert, doch blieb es nicht bei der bloßen Übernahme. Seit etwa dem 16. Jahrhundert zeigten japanische Mediziner eine immer deutlicher werdende Selbstständigkeit, verwarfen oder veränderten chinesische Konzepte und entwickelten eigene Therapien wie die „Hammernadelung“ ("uchibari") nach Mubun oder Instrumente wie die „Röhrennadel“ ("kudabari") nach Sugiyama Wa’ichi.

Zu folgenreichen Auseinandersetzungen unter den Ärzten kam es im 17. Jahrhundert. Eine Ärztegruppierung wandte sich gegen den Import jüngerer chinesischer Konzepte, die man als zu schematisch und starr betrachtete, und griff auf das damals schon über 1500 Jahre alte Werk "Shang Han Lun" (jap. "Shōkan ron") zurück, eine Abhandlung, die durch Kälte verursachte fiebrige Erkrankungen unter klinische Beobachtung gestellt hatte. Diese Erneuerung durch Rückgriff auf Altes führte zur Ausprägung einer eigenständigen „Alten Schule“ (ko-ihō-ha, ).

Mit dem Eindringen westlicher Medizin seit der zweiten Hälfte des 16. Jahrhunderts und besonders nach der Gründung der niederländischen Handelsstation Dejima entstand unter den japanischen Ärzten eine Gruppe, die im Rahmen der "Hollandkunde" (Rangaku) die Grundlagen für die rasche Modernisierung nach der Öffnung des Landes leisteten. Aber auch die Traditionalisten der „Alten Schule“ der Medizin reagierten erstaunlich flexibel auf europäische Einflüsse. So versuchte der Hofarzt des Tenno in Kyoto, Ogino Gengai (1737–1806), den westlichen Aderlass mit chinesischen und japanischen Konzepten zu verbinden. Ishizaka Sōtetsu (1770–1841), ein Akupunkturarzt am Hof des Shōgun in Edo, strebte eine Integration der westlichen Anatomie an. Ogino nahm Kontakt zum VOC-Arzt Carl Peter Thunberg auf, Ishizaka übergab Dr. Philipp Franz von Siebold, von dem er sich eine Kooperation erhoffte, gar lange schriftliche Ausführungen hierzu.

Die Bezeichnung Kampo-Medizin (漢方医学) kam im 19. Jahrhundert auf, um die chinesische gegen die westliche Medizin abzugrenzen. "Kampō" bedeutet so viel wie „chinesische Verfahren“, „chinesische Richtung“. Das Begriffsfeld ist nicht präzise abgegrenzt. Manchmal schließt es das ganze Arsenal an Verfahren, das zur Anwendung kommt, auch Massage, Akupunktur und Diätetik ein. Häufig aber beschränkt es sich auf die Therapie mit Arzneimitteln.

Mit der Öffnung Japans seit Mitte des 19. Jahrhunderts griffen staatliche Instanzen immer mehr in die bis dahin freie Welt der medizinischen Ausbildung und Praxis ein. Mit Beginn der Meiji-Zeit mussten die Ärzte den Nachweis erbringen, dass sie sich im Handwerk der westlichen Medizin auskannten. 1870 wurde per Dekret die deutsche Medizin in den neu gegründeten Universitäten und den Kliniken des Landes als Grundmodell durchgesetzt. Wer nur Erfahrungen mit hergebrachten Methoden vorweisen konnte, war zum Heilen nicht mehr berechtigt und wurde aus dem Verband staatlich anerkannter Ärzte ausgeschlossen. Dies führte jedoch nicht zum Absterben der Kampo-Medizin. Seit der zweiten Hälfte des zwanzigsten Jahrhunderts mehrt sich die Zahl der Ärzte, die nach ihrer Approbation in westlicher Medizin eine Zusatzausbildung in Kampo-Medizin absolvieren. 1976 wurden Kampo-Produkte kassenfähig. 1979 wurde die erste Abteilung für Kampo-Medizin an der medizinischen Fakultät einer staatlichen Universität gegründet. Mittlerweile gibt es ähnliche Abteilungen in einer Reihe von staatlichen und privaten Institutionen. In vielen pharmazeutischen Fakultäten des Landes werden traditionelle Heilmittel im Hinblick auf ihre Wirkstoffe erforscht. In größeren Städten findet man Kampo-Apotheken, aber auch "westliche" Apotheken bieten das eine oder andere Kampo-Präparat an.

Eine eigene Welt bildet die Akupunktur. Personell und institutionell ist sie von der Kampo-Medizin getrennt. Die Behandlung mit Arzneidrogen liegt in den Händen approbierter Ärzte, diejenige mit Nadeln hingegen in denen von Therapeuten, die sich auch auf die Techniken des Massierens und weiterer manueller Verfahren verstehen. Entsprechende Praxen – oft im Stil einer kleinen Klinik mit einem oder zwei Dutzend Mitarbeitern ausgestattet – sind flächendeckend vorhanden und in das Gesundheitswesen integriert. Auch als Patient einer Pflichtkasse kann man mit direktem Gang zum Therapeuten, ohne sich zuerst bei einem Arzt ein Überweisungsschreiben holen zu müssen, Leistungen in Akupunktur oder Massage in Anspruch nehmen.

Seit den 1950er-Jahren fand die TCM zunehmendes Interesse im westlichen Kulturkreis. In Deutschland hatten naturheilkundlich ausgerichtete Ärzte und Heilpraktiker wie Gerhard Bachmann, August Brodde, Heribert Schmidt, Erich Stiefvater Teile von ihr rezipiert. Die Heilpraktiker Hörner und Korn waren als Ausbilder in den ersten Kursen tätig, die Anfang der 1950er Jahre unter Leitung von Dr. Stiefvater stattfanden. Ihre Kenntnisse bezogen sie aus Vietnam und Japan und inkorporierten neue Erkenntnisse von Sinologen. Zur Verbreitung der TCM haben unter anderem die Werke von Manfred Porkert beigetragen. Seine „Klinische Chinesische Pharmakologie“ von 1978 zum Beispiel bietet zum ersten Mal eine umfassende Beschreibung der Wirkungen von chinesischen Arzneien in einer westlichen Sprache.

Mit der politischen Öffnung Chinas und den damit einhergehenden Reiseerleichterungen erlebte vor allem die Akupunktur seit den 1970er-Jahren einen regelrechten Boom. Als Auslöser gilt der Bericht eines amerikanischen Journalisten über die erfolgreiche Akupunktur-Schmerzbehandlung nach seiner eigenen Blinddarm-Operation während eines Chinabesuchs im Jahr 1971.
Einer der großen Protagonisten der TCM unter den China-Reisenden von damals ist der Nordamerikaner Ted J. Kaptchuk, dessen 1983 erschienenes Buch „The Web That Has No Weaver“ (dt.: Das große Buch der chinesischen Medizin 1988) wesentlich zur Popularität der TCM beigetragen hat.

Eine Schlüsselrolle im medizinischen Diskurs der chinesischen Tradition spielt der Begriff „Qi“. Der Organismus erscheint als außerordentlich komplexes Gefüge dynamischer Qi-Strukturen. Es ist eine auf Gleichgewicht aufgebaute Dynamik. Ist das Gleichgewicht empfindlich gestört, braucht es den Arzt, der mit seinem erfahrenen Blick und im Gespräch mit dem Patienten die Ursache der Störungen zu ermitteln sucht. Es ist dann etwa von „Leber-Qi“ die Rede, von „Herz-Qi“, von „aufsteigendem Qi“, von „Qi-Schwäche“ usw.

Das Gleichgewicht der Qi-Dynamik besteht in einem Ausgleich von Gegensätzen, die nach Mustern gebildet werden wie: beschienen und schattig, männlich und weiblich, oben und unten, außen und innen, tätig und leidend etc. Der Form ihrer Gegensätzlichkeit nach werden sie unter das Begriffspaar Yin und Yang gebracht. Das eine hat nicht – wie etwa im Gegensatz von gut und böse – den Sieg über das andere davonzutragen, sondern findet seine Bestimmung nur in der Anerkennung und Förderung des Anderen. Der Gedanke, dass allem Geschehen in der Natur und in der Gesellschaft eine Spannung nach Yin und Yang innewohnt, ist nicht nur in der chinesischen Medizin zu finden. Er ist im „Yijing“ zu finden, einem der „Klassiker“, die lange vor dem Erscheinen von Konfuzianismus und Daoismus niedergeschrieben wurden, und ist tief in der chinesischen Kultur verankert. Der Sinologe Wolfgang Bauer wies darauf hin, dass auch der die chinesische Geschichte prägende Widerstreit von Konfuzianismus und Daoismus in seiner Dynamik vom Denken nach Yin- und Yang-Gegensätzen mitgetragen wurde.

An zweiter Stelle ist die Qi-Dynamik in einen Kreislauf eingebunden, der nach dem Muster von fünf Jahreszeiten verläuft. Jeder Kreis (Funktionskreis (TCM) oder Orbis) geht aus einem vorherigen hervor und in den nächsten über. Es entstehen Gegensätze und Paare etwa nach dem Muster des Verhältnisses zwischen Großmutter und Enkel. Den Jahreszeiten sind fünf Elemente zugeordnet: Holz, Feuer, Erde, Metall und Wasser. Der menschliche Organismus schließlich wird als ein Zusammenwirken von fünf „Organen“ (Funktionskreisen) begriffen, von denen jedes seinen besonderen Bezug zu einem der fünf Elemente und einer der fünf Jahreszeiten hat. Die fünf Organe/Funktionskreise sind im Modus des Auseinander-Entstehens miteinander verbunden: Leber, Herz, Milz, Lunge und Nieren. Diese Organe/Funktionskreise decken sich nur teilweise mit dem uns vertrauten Begriff. Im „Herz“ zum Beispiel ist neben dem Organ als Pumpe das Vermögen zur treffenden Form eingeschlossen. Physisches und Psychisches gehen oft ununterscheidbar ineinander über. Der wichtigste Unterschied zum westlichen Verständnis ist: Die fünf Organe/Funktionskreise sind ein sich selbst erfüllendes Ganzes, ein Mikrokosmos als Abbild eines Makrokosmos, der mit seinen Jahreszeiten und Elementen mit dem individuellen Organismus verknüpft ist. Die Organe/Funktionskreise verhalten sich zum Organismus wie die fünf Jahreszeiten zum Zyklus des Entstehens und Vergehens und die fünf Elemente (fünf Wandlungsphasen) zum Ganzen des materiellen Seins.

Qi wird oft als „Kraft“ oder „Energie“ übersetzt. In klassischen chinesischen und japanischen Quellen findet man aber auch Beschreibungen, die ein stoffliches Konzept andeuten. Ähnlichen dem Pneuma der griechischen Medizin durchzieht dieses Qi in vielfältigen Ausformungen sowohl den Körper als auch die Außenwelt. Deswegen findet man auch in der modernen Alltagssprache unzählige Begriffe, die die Wortkomponente Qi enthalten. Qi wird auch im medizinischen Kontext benutzt. Mit „Wei-Qi“ wird die Fähigkeit bezeichnet, schädlichen Witterungseinflüssen standzuhalten und Verletzungen und Infekte zu bewältigen. Es soll im wachen Organismus anders anwesend sein als im schlafenden und sich beim Einschlafen von der Oberfläche des Körpers ins Körperinnere zurückziehen. „Qigong“ bezeichnet dagegen „Arbeiten am Qi“.

Häufige Verwendung findet das Konzept im Zusammenhang mit den „Meridianen“, auf denen 365 Therapiepunkte liegen. Durch das Reizen der Punkte mittels Nadeln wird das „De-Qi“(-Gefühl) hervorgerufen - „Ankunft des Qi“. Für den Therapeuten ist es durch eine Zunahme des Gewebswiderstandes spürbar, für den sensiblen Patienten durch eine ausstrahlende Empfindung entlang des betroffenen Meridians.

Der aus der Geographie entlehnte westliche Begriff " Meridian" trifft eigentlich nicht die ursprüngliche Bedeutung der chinesischen Bezeichnung "jingluo" (), treffender ist der Begriff "Leitbahn", genauer: "Leitbahnennetz" bzw. "Leitbahnengeflecht". Es handelte sich nicht um projizierte Linien auf der Oberfläche des Körpers, sondern um ein System von Bahnen ("jīng", ) und Netzgeflecht ("luò", ), durch die das Qi pulsiert. Besonders in alten Texten ist der Begriff Qi zudem oft mit Begriff Blut ("xuè", ) vergesellschaftet ("xuèqì", ), was dazu führte, dass Europäer des 17. Jahrhunderts wie Willem ten Rhyne / ten Rhijne oder Engelbert Kaempfer die "Meridiane" als Blutgefäße missverstanden. Die Zahl der Therapiepunkte nahm im Laufe der Entwicklung der chinesischen Medizin zu. Die Lage der Punkte war nicht immer stabil. Manche werden nur zum Nadeln (Akupunktur) genutzt, andere nur zum Brennen (Moxibustion). Ein Großteil dient je nach Indikation beiden Therapieverfahren.

Nach chinesischer Krankheitslehre kommen die inneren Störungen im Äußeren auf differenzierte Art zum Vorschein. Entsprechend gibt es diagnostische Verfahren, die sich auf die sinnlich wahrnehmbare Beschaffenheit der Körperaußenseite und von Ausscheidungen richten. Berühmt dafür, weil ohne Entsprechung zu westlichen Diagnosetechniken, sind die Puls- und die Zungendiagnose.

Zur Behandlung werden verschiedene Methoden in Kombination angewandt. Die fünf wichtigsten Methoden sind:

In „Klinische Chinesische Pharmakologie“ beschreibt Manfred Porkert insgesamt 515 Einzeldrogen, die in der traditionellen chinesischen Medizin Verwendung finden. Demnach sind davon unter fünf Prozent Präparate oder Teile von Wirbeltieren, darunter zum Beispiel die Knochen des Tigers, aber auch fossile Knochen voreiszeitlicher Tiere. Jeweils gut fünf Prozent sind mineralischer Art oder setzen sich aus Exkrementen, Sekreten, Würmern, Insekten und Teilen von Weichtieren zusammen und 85 Prozent sind pflanzlicher Herkunft. Die einzigen offiziell in Europa verwendeten Arzneistoffe tierischen Ursprungs sind verschiedene Muschelschalen (z. B. der Chinesischen Auster oder Arkamuschelschalen).

Jede Einzeldroge besitzt ihr spezifisches Profil, das sich aus dem Temperaturverhalten (siebenstufige Skala) und der Geschmacksrichtung (sauer, bitter, süß, scharf, salzig und neutral) ergibt. Beides sind, chinesischer Pharmakologie gemäß, elementare Eigenschaften der Arzneien. Sie stehen in einem direkten Zusammenhang mit bestimmten Wirktendenzen. Diese sind wiederum spezifisch funktionskreisbezogen („Orbisbezug“). Im Ergebnis werden für jedes Mittel Indikationen und Kontraindikationen angegeben. Zur weiteren Bestimmung gehören Kombinierbarkeit und Unverträglichkeit mit anderen Drogen, die Toxizität und die Dosierung entsprechend der Verabreichungsform.

Die dem Patienten verabreichte Arznei besteht in der Regel aus einer Komposition verschiedener Einzelmittel. Die klassische Verabreichungsform ist meist das Dekokt, seltener ein mittels Aufguss gewonnener Extrakt. Neben anderen Formen der Verabreichung sind für häufige Indikationen standardisierte Wirkstoffkombinationen in Form von Pillen und Granulaten in Gebrauch. Eine weitere Verabreichungsform ist der Sirup, so zum Beispiel Pei Pa Koa (Loquat-Sirup) welcher in der TCM gegen Pharyngitis, Husten und Heiserkeit eingesetzt wird.

Die traditionelle chinesische Medizin ist mittlerweile auch im deutschsprachigen Raum verbreitet. Sie hat eine Reihe von ärztlichen Gesellschaften hervorgebracht.


TCM ist in Deutschland gesundheitspolitisch bisher nur begrenzt anerkannt.
Im Anschluss an die Modellversuche zur Überprüfung der Wirksamkeit von Akupunktur wird Akupunktur seit dem 1. Januar 2007 bei chronischen Schmerzen der Lendenwirbelsäule und des Kniegelenks als Kassenleistung anerkannt.

In der Schweiz werden die Behandlungskosten von EMR-anerkannten, nicht-ärztlichen TCM-Therapeuten weitgehend durch die Zusatzversicherungen gedeckt. Die Zulassung für TCM-Therapeuten ist föderalistisch geregelt, hauptsächlich gelten die Anforderungen des Schweizerischen Berufsverbandes für TCM (SBO-TCM), welcher sich wiederum nach den Standards der NCCAOM und ETCMA richtet. Diverse Schulen sind vom SBO-TCM empfohlen und bieten mehrjährige, umfangreiche Vollzeitausbildungen an.
Von den ärztlichen Behandlungsmethoden aus der TCM kann seit 2006 nur noch Akupunktur über die Grundversicherung abgerechnet werden, alle übrigen Therapiemethoden der TCM können durch eine allenfalls abgeschlossene Zusatzversicherung rückvergütet werden. Die Standards für ärztliche Fähigkeitsausweise für TCM werden hauptsächlich durch den Dachverband der schweizerischen ärztlichen TCM-Verbände (ASA) gesetzt. Ab 2012 wird die traditionelle chinesische Therapie sowie die Homöopathie, die anthroposophische Medizin, die Phytotherapie und die Neuraltherapie unter bestimmten Voraussetzungen wieder von der obligatorischen Krankenpflegeversicherung bezahlt. Diese Regelung gilt provisorisch bis Ende 2017. In dieser Zeit gelten Wirksamkeit, Zweckmässigkeit und Wirtschaftlichkeit der fünf komplementärmedizinischen Methoden als teilweise umstritten und werden hinsichtlich dieser Kriterien evaluiert.

In Österreich können Ärzte ein Diplom für „Chinesische Diagnostik und Arzneitherapie“ der Österreichischen Ärztekammer erwerben. Diese Ausbildung dauert mindestens zwei Jahre und umfasst mindestens 500 Ausbildungsstunden. In Wien gab es eine Privatuniversität, die sich der Forschungs- und Lehrarbeit im Bereich der traditionellen chinesischen Medizin verschrieben hatte, die TCM Privatuniversität Li Shi Zhen. Die Akkreditierung der Universität erlosch 2009.

Von wissenschaftlicher Seite der evidenzbasierten Medizin wird die Wirksamkeit vieler Behandlungsmethoden der TCM bestritten. Die Grundkonzepte der TCM widersprächen demnach naturwissenschaftlichen Prinzipien.

Die Kritik an der TCM betrifft verschiedene Teilgebiete derselben. Eines davon ist die Lehre von den „Meridianen“, die einer Vielzahl von Verfahren wie Akupunktur, Massage, Bewegungsübungen usw. zu Grunde liegt. Die angenommenen Meridiane sind nach naturwissenschaftlichen Erkenntnissen nicht belegbar. Für die Behauptung, dass man über spezifische Punkte an der Körperoberfläche auf innere Zustände und Organe Einfluss nehmen kann, gibt es keinen wissenschaftlich stichhaltigen Beweis oder plausiblen Wirkungsmechanismus. Die Vertreter der TCM führen an, dass es nicht eine Vielzahl von Meridianlehren gibt, sondern im großen Ganzen "eine" Lehre. Dies widerspricht der Tatsache, dass sie seit Jahrtausenden und über riesige geographische Räume hinweg und mittlerweile weltweit von Ärzten verschiedensten Hintergrunds angewandt wird und zudem im Laufe der Zeit mehrfach verändert und abgewandelt wurde. Außerdem würde auch eine Uniformität oder Verbreitung einer Theorie keinen Schluss auf ihren Wahrheitsgehalt zulassen.

Systematische Reviews von Studien zur Wirksamkeit der Akupunktur zeigen uneinheitliche Ergebnisse. Eine kleine Zahl von Reviews kommt zu dem Schluss, dass Akupunktur bei einer sehr begrenzten Zahl von Indikationen hilfreich ist.

Bezüglich Akupunktur konnte gezeigt werden, dass es für die Wirkung keinerlei Rolle spielt, wo gestochen wurde. Die Wirkung bei Beachtung von "klassischen" Akupunktur-Punkten und die vorsätzliche Missachtung dieser Punkte und von Meridianen machte in Studien keinen Unterschied.

Auch die chinesische Phytotherapie wurde in wissenschaftlichen Studien untersucht. Eine Gruppe von Wissenschaftlern der Universität Bern hatte Untersuchungen zur chinesischen Phytotherapie mit solchen der herkömmlichen Medizin verglichen. Verglichen wurden 136 Doppel-Blind-Studien. Die Autoren des Vergleichs ziehen den Schluss, dass die Voreingenommenheit bei den chinesischen Studien stärker ausgeprägt sei und es auf Grund der geringen Zahl hochwertiger Veröffentlichungen nicht möglich sei, über die Wirksamkeit der chinesischen Phytotherapie verbindlich zu urteilen.

Als Argument für die Wirksamkeit von TCM wird häufig deren Jahrtausende alte Tradition angebracht. Das Alter eines Heilverfahrens lässt jedoch keinerlei Rückschlüsse auf dessen Wirksamkeit zu. Die chinesische Regierung unterstützt die Verbreitung von TCM in Europa, da sich dort ein großer Markt für chinesische Hersteller öffnet. Das dahinter stehende finanzielle Interesse kann möglicherweise in Konflikt mit Objektivität bezüglich der Überprüfung der Wirksamkeit von TCM stehen.

Es gab Berichte von Vergiftungsfällen bei der Anwendung von chinesischen Arzneien (z. B. durch Aristolochiasäuren). Die Gefahr der Verunreinigung durch Pestizide, Schwermetalle etc. lassen sich wie bei anderen Medikamenten auch nur vermeiden, wenn die Arzneien in Apotheken gekauft werden. Da die Apotheken in Deutschland der Kontrolle der Arzneimittelbehörden unterworfen sind, werden sämtliche dort verkauften Produkte mittels vorgeschriebener Verfahren analysiert und müssen bestimmte Qualitätsmerkmale erfüllen. Unerwünschte Wirkungen können die chinesischen Arzneien (wie auch schulmedizinische Medikamente) bei unsachgemäßer Einnahme zeigen. Bei den chinesischen Abkochungen von Pflanzenteilen handelt es sich teilweise um potente Substanzen, die schwere Störungen hervorrufen können, wenn sie zur falschen Zeit oder vom falschen Patienten eingenommen wurden.

Seit 1999 läuft an der Bayerischen Landesanstalt für Landwirtschaft ein interdisziplinäres Projekt zur Anbauforschung von 16 ausgewählten chinesischen Heilpflanzenarten. Laut der Landesanstalt könnte der kontrollierte Anbau einen wesentlichen Beitrag zur Sicherheit der Arzneimittel und auch zur Versorgungssicherheit in diesem Bereich leisten.

Von einigen Kritikern wurden Bedenken erhoben, dass die chinesischen Arzneien, auch kunstgerecht gehandhabt, zu Leberschäden führen könnten. Eine „Langzeitstudie über mögliche Nebenwirkungen der chinesischen Kräuter“ vom Förderverein Chinesische Medizin in Deutschland e.V. kommt allerdings zu dem Schluss, dass sie den „oft erhobenen Vorwurf der Lebertoxizität chinesischer Kräuter“ widerlegen konnte. Dem entgegen steht eine Untersuchung durch eine Gruppe um Rolf Teschke vom Klinikum Hanau, die bei der Auswertung der wissenschaftlichen Literatur von 2011 bis 2014 zu dem Ergebnis kam, dass 18 der in der TCM verwendeten, klassifizierten Kräutermischungen, einige unklassifizierbare Mischungen und 39 individuelle TCM-Kräuter zu Leberschäden führen können. In den meisten der Fälle konnten sich die Betroffenen zwar erholen, allerdings wurden auch einige Fälle erfasst, bei denen die Leber dauerhaft geschädigt wurde und eine Transplantation des Organs erforderlich machte. In seltenen Fällen kam es sogar zum Tod des Patienten.

2013 wurde bei einer Untersuchung von Chinesischen Heilkräutern in 17 von 36 Proben Pestizidrückstände festgestellt, die von der Weltgesundheitsorganisation als extrem gefährlich oder gefährlich eingestuft werden. 26 der 36 Proben wiesen Rückstände oberhalb der in der EU zugelassenen Höchstmengen auf.

Das Centrum für Therapiesicherheit in der Chinesischen Arzneitherapie (CTCA), ein Zusammenschluss der wesentlichen Fachgesellschaften für Chinesische Arzneitherapie in Deutschland sowie kompetenter Einzelpersonen, bemüht sich um eine sichere Therapie mit chinesischen Heilmitteln.

Zur Weiterbildung in TCM werden verschiedene Qualifikationen und Fortbildungen angeboten, die wegen fehlender Standards nicht vergleichbar sind. Die Akupunktur ist eine durch die jeweiligen Landesärztekammern anerkannte Zusatzbezeichnung. Auf dem Deutschen Ärztetag 2003 wurde die Zusatzbezeichnung Akupunktur neu in die Weiterbildungsordnung eingeführt. Ziel der Zusatzweiterbildung ist die Erlangung der fachlichen Kompetenz und der praktischen Fähigkeiten in Akupunktur nach Ableistung der vorgeschriebenen Weiterbildungszeit und Weiterbildungsinhalte sowie der Weiterbildungskurse.

Alle naturheilkundliche Medizin muss früher oder später mit knappen Ressourcen rechnen, sobald sie einen Massenkonsum zu befriedigen hat. In Deutschland sind zum Beispiel Schlüsselblume und Arnika in ihrem Bestand gefährdet.

Auch TCM ist davon betroffen, zumal sie weiterhin in Expansion begriffen ist und in ihrer Arzneimittelliste Teile einer Reihe von gefährdeten Tierarten führt (gemäß IUCN „gering gefährdet“ bis „vom Aussterben bedroht“): Tiger, Schneeleopard, Asiatischer Schwarzbär, Nashorn, Saiga-Antilope (wurde vom WWF in den 1990er-Jahren als Substitut für das Nashorn empfohlen), bestimmte Schuppentierarten, Sägerochen, einige Seepferdchenarten sowie verschiedene Schildkrötenarten.

Deren Verwendung in der TCM spielt eine Rolle für den Bestand gefährdeter Arten. Beim Tiger rangiert allerdings die Zerstörung des Lebensraums weit vor der Wilderei für den TCM-Markt. Allein zwischen 1995 und 2005 ging der Lebensraum für den Tiger in Asien um 40 % zurück. Tiger besiedeln heute nur noch 7 % ihres ursprünglichen Habitats. Umso empfindlicher freilich trifft es die verbliebenen Populationen, wenn das Verbot der Bejagung nicht rigoros durchgesetzt wird.

Mit der Begründung der Artenschonung wurden in China seit Beginn der 1980er-Jahre nach koreanischem Vorbild tierquälerische Bärenfarmen eingerichtet. Die Tiere werden dort unter Bedingungen gehalten, die weit entfernt von westlichen Standards sind. Auf grausame, äußerst schmerzhafte Weise wird ihnen über ihre ganze Lebenszeit Gallenflüssigkeit entnommen. Aufgrund eines Überangebots der Ware auf den Märkten geht nur ein Teil in die Herstellung von Arzneimitteln, ein beträchtlicher Teil in die Produktion von Kosmetika und anderen Luxusprodukten. Trotz aller Kampagnen und Einsprüche der einschlägigen Tierschutzverbände werden in China, Korea und Vietnam insgesamt nach wie vor mehr als 14.000 Tiere gehalten. In Verletzung des Washingtoner Artenschutzabkommens werden Produkte mit Bärengalle als Bestandteil auch auf westlichen Märkten gehandelt.

Der Handel mit diesen Produkten erfolgt häufig über den Schwarzmarkt. Heilkräuter und andere Arzneien werden in Asien von alters her ohne jede Kontrolle auf Straßenmärkten und in Shops angeboten. Der treffende Begriff für diesen Sektor ist der der „Volksmedizin“, die nur zum Teil auf Erkenntnissen der TCM, zum Teil auf Aberglauben beruht. Die Industrie, die diesen Sektor bedient, drängt auf die Märkte des Westens, seit dort die TCM in Mode ist.

Die TCM-Gesellschaften in Deutschland sprechen sich ausnahmslos gegen die Verwendung geschützter Arten des Tier- und des Pflanzenreichs aus. Die Arbeitsgemeinschaft für Klassische Akupunktur und Traditionelle Chinesische Medizin (AGTCM) engagiert sich in der „AG Medizin und Artenschutz“ des WWF. Die Internationale Gesellschaft für chinesische Medizin kooperiert mit Pro Wildlife. Deutsche Apotheken führen TCM-Arzneien, die Bestandteile geschützter Arten enthalten, nicht im Angebot.




</doc>
<doc id="13306" url="https://de.wikipedia.org/wiki?curid=13306" title="Centurio">
Centurio

Centurio oder auch Zenturio („Hundertschaftsführer“, von lateinisch "centum" = hundert), in altgriechischen Quellen auch als Hekatontarch (ἑκατόνταρχος) bezeichnet, war die Bezeichnung für einen Offizier des Römischen Reiches, der normalerweise eine "Centuria" („Hundertschaft“) der römischen Legion oder eine vergleichbare Einheit der Auxiliartruppen (Hilfstruppen) befehligte. Es gab jedoch vielfältige Abstufungen innerhalb des Ranges. Der Name deutet zwar auf „hundert“ hin, jedoch bestand eine reguläre Centuria schon in der frühen Republik nur aus etwa 80 Legionären.

Im Gegensatz zu den Stabsoffizieren, die aus dem Ritterstand oder Senatorenstand kommen mussten, stiegen Centurionen immer aus dem Mannschaftsdienstgrad auf; damit konnte theoretisch jeder römische Bürger Centurio werden. In der Anfangsphase der Legion wurde der Centurio von seinen Soldaten gewählt, später durch den Legaten, den Legionskommandeur, ernannt. Dabei bedurfte in der Kaiserzeit die Ernennung der Bestätigung durch den Kaiser. Der Zenturionenstand bildete das Rückgrat der römischen Armee und war für die Disziplin und Ordnung der Truppen von entscheidender Bedeutung. Es gibt eine Vielzahl von Berichten über die Härte, aber auch Tapferkeit der Centurionen. Im Gegensatz zu den Mannschaften und „Unteroffizieren“ wurden viele Centurionen nach Ablauf ihrer Dienstzeit nicht entlassen, sondern blieben bis zu ihrem Tod bei der Armee.

Der Centurio war als Vorgesetzter für die Ausbildung und die Ausrüstung seiner Legionäre verantwortlich. Er hatte das Recht, seine Leute auszuzeichnen und zu bestrafen; für Letzteres wurde auch vielfach der Weinstock ("vitis") eingesetzt, den er als Zeichen seines Ranges bei sich trug. Außerdem unterschieden sich Ausrüstung und Uniformierung der Centurionen von denen der Mannschaften, am auffälligsten durch den quer getragenen Helmbusch ("crista" "transversa"), die Beinschienen und das auf der linken Seite getragene Schwert. Er erhielt abhängig von der Stellung einen höheren Sold; im 2. Jahrhundert betrug er ungefähr das Zwanzigfache des Solds eines Legionärs.

Neben einer eigenen Stube im Unterkunftsgebäude stand ihm ein eigenes Zelt und Reitpferd sowie Tragtier zu. Die Rüstung und die Auszeichnungen eines Centurios finden sich vielfach auf Grabsteinen verstorbener Offiziere dargestellt.

Unterstützt wurde der Centurio durch einen Optio als Stellvertreter und weitere Principales wie den Signifer und den Tesserarius.

Centurio war kein einheitlicher Rang. Zwar standen Centurionen mit wenigen Ausnahmen alle einer Centurie vor, der eigentliche Rang ergab sich dabei jedoch aus der Stellung dieser Centurie innerhalb der Legion, die sich in zusätzlichen Bezeichnungen ausdrückte. Dabei wurde zunächst unterschieden zwischen den Centurionen der verschiedenen Manipel (in aufsteigender Bedeutung) Hastaten, Principes und Triarier (Letztere oft als "Pili" bezeichnet). Innerhalb eines Manipels stand der Kommandeur der ersten Centuria ("prior") über dem der zweiten ("posterior"). Also sah die Rangfolge so aus (absteigend):

Dabei bestand jedoch nur zwischen dem Prior und Posterior des gleichen Manipels ein echtes Vorgesetztenverhältnis. Wurde eine Kohorte selbstständig eingesetzt und war kein Tribun für das Kommando bestimmt, so führte sie der ranghöchste Centurio der Kohorte, der "Pilus Prior".

Vom Rang her, aber nicht im Sinne einer Befehlsgewalt, allen anderen übergeordnet waren die Centurionen der ersten Kohorte, in der das Feldzeichen geführt wurde. Dies waren die Centurionen
Ganz besonders hervorgehoben war der Primus Pilus. Er führte die erste Centuria, welche während einiger Zeit des römischen Reiches die doppelte Mannstärke der übrigen Centurien hatte und den Legionsadler führte. In der Legion war der Primus Pilus nur dem Legaten, den sechs Tribunen und dem Praefectus Castrorum unterstellt. Aus dieser Stellung leiten sich drei Bezeichnungen ab:
Für die Kaiser stellten die Primipilares ein Reservoir fähiger Führer dar, die nicht in die Strukturen der etablierten Machteliten Roms verstrickt waren.

Außerhalb der Kohortenstruktur konnten Centurionen noch eingesetzt werden als:
Durch diese starke Untergliederung fällt es schwer, den Centurio mit modernen Dienstgraden zu vergleichen. In seiner Funktion als Führer einer 80 bis 100 Mann starken Infanterieeinheit entspricht er am ehesten einem Hauptmann. Ein Primus Pilus wäre dagegen vielleicht mit einem Oberst vergleichbar und hatte ein erhebliches gesellschaftliches Prestige.



</doc>
<doc id="13307" url="https://de.wikipedia.org/wiki?curid=13307" title="Legion">
Legion

Legion (lat. "") bezeichnet:

Legion ist der Name folgender polizeilicher oder militärischer Organisationen:


Veteranenorganisationen:

Legion als Orden:


Andere Verwendungen:

Siehe auch


</doc>
<doc id="13309" url="https://de.wikipedia.org/wiki?curid=13309" title="Legat">
Legat

Legat steht für:
Legat ist der Familienname folgender Personen:
Siehe auch


</doc>
<doc id="13310" url="https://de.wikipedia.org/wiki?curid=13310" title="Deutschsprachige Literatur">
Deutschsprachige Literatur

Der Begriff deutschsprachige beziehungsweise deutsche Literatur bezeichnet die literarischen Werke in deutscher Sprache aus dem deutschen Sprachraum der Vergangenheit und Gegenwart. Zur deutschsprachigen Literatur werden auch nicht-dichterische Werke mit besonderem schriftstellerischem Anspruch gezählt, also Werke anderer Disziplinen wie der Geschichtsschreibung, der Literaturgeschichte, der Sozialwissenschaften oder der Philosophie. Auch das Genre kann variieren, so werden auch Tagebücher oder Briefwechsel als Literatur angesehen.

Die Althochdeutsche Literatur beginnt mit der schriftlichen Überlieferung althochdeutscher Texte. Dabei verlief die Verschriftlichung vorrangig durch Verdrängung lateinischer Schriften, welche nun in Althochdeutsch verfasst wurden. Obwohl Heldenlieder eine typische Gattung schriftloser Kulturen sind, schrieben unbekannte Autoren des Klosters Fulda im 9. Jahrhundert das Hildebrandslied in althochdeutscher Sprache nieder. Als ältester Text in deutscher Sprache gelten hingegen die Merseburger Zaubersprüche, welche vermutlich Anfang des achten Jahrhunderts von einem Mönch festgehalten wurden.

Die ältesten erhaltenen althochdeutschen Schriftzeugnisse stammen aus dem 8. Jahrhundert und finden sich im Zusammenhang mit dem kirchlichen Einsatz der Volkssprache als Missionierungs­hilfe und als Verständnishilfe für lateinische Texte, zum Beispiel Glossare wie der Codex Abrogans, der als das älteste erhaltene Buch in deutscher Sprache gilt. Frühe literarische Zeugnisse in deutscher Sprache finden sich auch in der Klosterliteratur, wenngleich diese ursprünglich lateinische Epik war. Beispiele sind die zwei großen Bibelepen des 9. Jahrhunderts, das altsächsische Heliand, noch im alten Stabreim, und das "Evangelienbuch" des Otfrid von Weißenburg, im neuen, zukunftsweisenden Endreimvers. Um das Jahr 1000 übersetzte und kommentierte Notker in St. Gallen philosophische Texte der Antike auf hohem philologischen Niveau ins Althochdeutsche. Er darf als erster großer deutscher Prosa-Autor gelten.

Im 11. Jahrhundert entstanden vor allem religiös belehrende und ermahnende Texte in frühmittelhochdeutschen Reimpaarversen. Diese erste Phase der Dichtung von Geistlichen war dadurch bestimmt, dass die Religion Einfluss auf den Laienadel nehmen wollte. Die Textsorten waren heilsgeschichtliche Darstellungen, zum Beispiel das "Ezzolied" (um 1065), Legenden­dichtung, wie das "Annolied" (um 1077), alt- und neutestamentliche Bibelepik (Genesis, Exodus, Leben Jesu), dogmatische Darlegungen, eschatologische Dichtungen und Mariendichtung.

Um die Mitte des 12. Jahrhunderts wurde die Literatur vielfältiger: Man griff Themen auf, die zuvor der Schrift für unwürdig befunden wurden. Außerdem gab es mehr unterschiedliche Formen, wie höfische Lyrik, unterhaltende Erzählungen. Geistliche Dichter interessierten sich neu für einzelne Personen und ihre Lebensgeschichte, dies führte zu Legendendichtungen wie Albers "Tundalus" und Veldekes "Servatius".

Damals erhielt auch die eine mehr weltliche (nichtkirchliche) Dichtung Auftrieb, nämlich die Geschichtsepik. Sie kam erstmals zu Rang und Namen als Dichtkunst. Das bedeutendste Werk, die Kaiserchronik mit rund 17.000 Versen, erzählt episodenhaft die Geschichte des römischen Kaisertums von der Gründung Roms bis zu Konrad III. Das "Rolandslied" des Pfaffen Konrad schildert den Kampf Karls des Großen und seiner Paladine gegen die Sarazenen in Spanien sowie den Tod Rolands nach einem Verrat. Mit dem "Rolandslied" und dem 'Alexander' des Pfaffen Lamprecht machte sich auch erstmals der Einfluss französischer Stoffe und Gestaltungsweisen bemerkbar, der die deutschsprachige Literatur für die nächsten Jahrzehnte und Jahrhunderte prägen sollte.

In den Jahrzehnten nach 1150 brach eine „Blütezeit“ der deutschsprachigen Literatur an. An einzelnen Höfen des Feudaladels verbreitete sich eine kultivierte literarische Praxis nach romanischsprachigem Vorbild: die sogenannte Höfische Literatur. In der Lyrik entwickelte sich der Minnesang (hohe Minne) und die Sangspruch­dichtung, mit ihren wichtigsten Vertretern Heinrich von Morungen, Reinmar der Alte und Walther von der Vogelweide. Für die höfische Epik galt schon den Zeitgenossen als Gründungsakt der Eneasroman des Heinrich von Veldeke, der vom Niederrhein an den Landgrafenhof in Thüringen kam und sein Werk dort gegen 1185 fertigstellte. Danach entstanden nach französischsprachigen Vorlagen (Chrétien de Troyes) zahlreiche höfische Epen in mittelhochdeutscher Sprache. Die bekanntesten sind hier "Erec" und "Iwein" (Hartmann von Aue), "Tristan und Isold" (Gottfried von Straßburg), "Parzival" (Wolfram von Eschenbach). Abseits von dieser „modernen“ Erzählkultur bleibt das anonym überlieferte Heldenepos "Nibelungenlied".

Als revolutionär erwies sich am Ausgang des Mittelalters der Buchdruck mit beweglichen Lettern. Schließlich konnte Pergament als Beschreibstoff durch billiges Papier ersetzt werden. Am Übergang zur Neuzeit steht Johannes von Tepls „Der Ackermann aus Böhmen“.

Aus Italien kommend verbreitete sich der Humanismus, die Geisteshaltung der Renaissance, in Deutschland. Man wandte sich antikem Gedankengut zu und schrieb deshalb oft auf Latein, auch wenn vor allem ein deutsches Publikum angesprochen werden sollte. Eine Trennung nach Sprachen ist deshalb wenig sinnvoll. 

Frühe Literaten wie Niklas von Wyle oder Heinrich Steinhöwel haben sich besonders mit der Übersetzung neuer lateinischer Texte des italienischen Humanismus ins Deutsche beschäftigt und eine Reform der deutschen Schriftsprache angestrebt. Bekannte Vertreter der nächsten Generation waren Conrad Celtis, der in Basel tätige Erasmus von Rotterdam und Johannes Reuchlin, allerdings schrieben sie ihre Werke meist lateinisch und hatten außerhalb der Gelehrtenwelt und gesellschaftlichen Eliten zunächst wenig Einfluss. Anders Ulrich von Hutten (1488–1523) mit seinen rebellischen Gedichten oder Sebastian Brant (1458–1521), der sein erfolgreiches „Narrenschiff“ auf Deutsch verfasste.

Die folgenreichste Bewegung war die von Martin Luther (1483–1546) eingeleitete Reformation. Luther verstand es, seine Ideen auch in lesbarem Deutsch zu verbreiten. Das herausragendste Ereignis auf dem deutschen Buchmarkt des 16. Jahrhunderts war sicher das Erscheinen seiner Bibelübersetzung in den Jahren 1522 und 1534. Sie trug wesentlich zur Verbreitung des heutigen Deutsch bei.

Neben Humanismus und Reformation verdienen auch der Meistersang, die Schwank­dichtung und das Fastnachtsspiel zumindest eine Erwähnung, insbesondere deren bekannteste Vertreter, der Nürnberger Hans Sachs (1494–1576) und Jörg Wickram (um 1505 – vor 1562). Ein weiterer bemerkenswerter Autor des 16. Jahrhunderts ist der Straßburger Johann Fischart (1546–1590), sein bekanntestes Werk ist die „Affentheurlich Naupengeheurliche Geschichtklitterung“.

Ein häufiges Genre der Zeit war das Volksbuch. Es entstand anonym und war, weil es beliebte Themen aufgriff, weit verbreitet. Beispiele sind die „Historia von D. Johann Fausten“ und die Geschichten um Till Eulenspiegel.

Im Barock vollzog sich eine stärkere Hinwendung der Literatur zur deutschen Sprache. Politisch war die Epoche von der konfessionellen Spaltung und dem Dreißigjährigen Krieg (1618–1648) geprägt. Die Spannweite der Barockliteratur ist sehr weit: von höfischer Dichtung zu volksnahen Romanen, von der Nachahmung antiker Vorbilder zur persönlichen Erlebnislyrik, von Lebensbejahung zum Vanitas-Motiv. Eine Gelegenheitsdichtung entsteht.

In der Barockzeit wurden zahlreiche Dichter- und Sprachgesellschaften gegründet, die bekannteste davon war die Fruchtbringende Gesellschaft. Von Martin Opitz (1597–1639) wurde in seinem „Buch von der deutschen Poeterey“ (1624) der Alexandriner für die deutschsprachige Lyrik empfohlen und blieb lange Zeit das wichtigste Versmaß. Mit einiger Verspätung gelangten der Petrarkismus und die Schäferidylle in die deutsche Literatur, genannt seien hier der Opitz-Schüler Paul Fleming (1609–1640) und Simon Dach (1605–1659). Bedeutendste Vertreter der Schäferpoesie waren die Dichter des Nürnberger Pegnesischen Blumenordens Georg Philipp Harsdörffer, Johann Klaj und Sigmund von Birken.

Wichtige lyrische Formen der Epoche sind das Sonett, die Ode und das Epigramm, die Lyrik kann man grob in religiöse, meist evangelische, und weltliche einteilen. Religiöse Lyrik schrieben Friedrich Spee von Langenfeld (1591–1635), die Kirchenlieder­dichter Paul Gerhardt (1607–1676), Johann Rist (1607–1667), Angelus Silesius (1624–1677) und der Mystiker Jakob Böhme (1575–1624). Unter den weltlicher orientierten Dichtern sind besonders die Sonette von Andreas Gryphius (1616–1664) zu nennen sowie Christian Hofmann von Hofmannswaldau (1617–1679).

Das Drama der Barockzeit zeigt sich vielfältig: Es gab einerseits das Jesuitentheater, das vor allem im südlichen, katholischen Raum in lateinischer Sprache aufgeführt wurde. Da die Zuschauer die Sprache nicht verstanden, setzte man umso mehr auf visuelle Effekte. Ähnlich verhielt es sich mit den anfangs ausländischen Wanderbühnen. Für ein anderes Publikum waren die Barockoper und das höfische Drama gedacht. Die Barockoper wurde als Gesamtkunstwerk hoch geschätzt. Im höfischen Drama gilt das Prinzip der Ständeklausel, Autoren sind etwa Daniel Casper von Lohenstein (1635–1683) (z. B. „Cleopatra“, „Sophonisbe“) und Gryphius mit drei Komödien und fünf Tragödien (z. B. „Chatharina von Armenien“, „Leo Armenius“, „Carolus Stuardus“).

Barockromane sind der Schäferroman, der Staatsroman, der höfisch galante Roman und am einflussreichsten: der aus dem Spanischen stammende Pikaro- oder Schelmenroman. Insbesondere ragt hier Hans Jakob Christoffel von Grimmelshausen (um 1625–1676) mit seinem „Simplicissimus“ und weiteren „Simplicianischen Schriften“ hervor. Simplicissimus’ Abenteuer während des Dreißigjährigen Krieges sind der bedeutendste außerspanische Schelmenroman. Als wichtigster Vertreter des Staatsromans gilt der Birken-Schüler Anton Ulrich von Braunschweig und Lüneburg-Wolfenbüttel.

Bereits im Jahr 1687 hielt Christian Thomasius, der „Vater der deutschen Aufklärung“, seine Vorlesungen in Deutsch statt Latein. Bekannte Philosophen dieser Zeit, der Frühaufklärung, waren Christian Wolff und Gottfried Wilhelm Leibniz. Der wichtigste literarische Autor der Frühaufklärung war sicher Christian Fürchtegott Gellert (1715–1769) mit seinen Fabeln. Die bedeutendste Figur im literarischen Leben aber war Johann Christoph Gottsched (1700–1766). Wegweisend waren seine theoretischen Schriften, vor allem der „Versuch einer critischen Dichtkunst“ (1730), sein literarisches Werk ist dagegen zweitrangig. In der „Dichtkunst“, einer normativen Poetik, orientierte er sich am klassischen französischen Drama und behielt die Ständeklausel bei, also die Regel, im Drama nur Schicksale adliger Personen darzustellen und das Bürgertum nur in der Komödie zu thematisieren. Dagegen polemisierten die Schweizer Johann Jakob Bodmer und Johann Jakob Breitinger, die das rationale Moment überbewertet sahen.

Autoren der Frühaufklärung lassen sich auch dem Spätbarock zurechnen, ein Beispiel dafür, wie fragwürdig Epocheneinteilungen sein können. Der bedeutendste Lyriker war Johann Christian Günther (1695–1723), ebenso wie Barthold Heinrich Brockes (1680–1747), kann er beiden Epochen zugeschrieben werden.

Neben der Aufklärung bildeten sich auch Strömungen, die das Gefühl in den Vordergrund stellten. Dazu zählt die Rokoko-Dichtung Friedrich Hagedorns, von Ewald Christian von Kleist, Salomon Gessner und anderen.

Vorbild einer ganzen Generation wurde Friedrich Gottlieb Klopstock (1724–1803) mit seinem Epos „Der Messias“ (1748–1773), das ganz in Empfindungen und Seelenzuständen schwelgt. Klopstock wird der Empfindsamkeit zugerechnet.

Im Bereich der Prosa war Christoph Martin Wieland (1733–1813) wegweisend. Er gestaltete den frühen Bildungsroman „Geschichte des Agathon“ (1766/67) und vermischte Rokoko-Elemente mit aufklärerischen Gedanken.

Die deutsche Spätaufklärung ist undenkbar ohne Gotthold Ephraim Lessing (1729–1781). Sein Wirken umfasst wichtige theoretische Werke („Laokoon“ 1766), Literaturkritik (mit Friedrich Nicolai und Moses Mendelssohn) und eine Reihe von bedeutenden Dramen. Am stärksten von aufklärerischem Geist durchdrungen ist „Nathan der Weise“ (1779), in dem exemplarisch gezeigt werden soll, dass der Wert eines Menschen nicht unbedingt an Religionszugehörigkeit oder Nationalität gebunden ist.

Die jugendliche Reaktion auf die Aufklärung, die als einengend und gefühlskalt empfunden wurde, war die kurze Periode des „Sturm und Drang“. Die meist jungen Männer, die gegen jede Form von Tyrannei waren, wollten auch in künstlerischen Dingen keine Bevormundung. Ein „Genie“, so die Idee, muss sich nicht an Regeln halten. Sie schrieben über die Probleme, die sie beschäftigten, und gaben dem Hier-und-Jetzt den Vorzug vor der Antike.

Johann Wolfgang von Goethe zeigte in dem Briefroman „Die Leiden des jungen Werthers“ einen Mann, der an seinem Gefühlsüberschwang und einer unglücklichen Liebe stirbt. In Friedrich Schillers (1759–1805) Drama „Die Räuber“ rebelliert ein junger Mann gegen seinen Vater und die Obrigkeit. Die Dramen von Jakob Lenz (1751–92) thematisieren die bedrückende Situation junger Intellektueller, wie etwa in dem „Hofmeister“. Neben den Dramen war auch die Lyrik wichtig, in ihr konnten sich Emotion und Pathos ausdrücken.

Der „Sturm und Drang“ dauerte aber nicht lange, die meisten Protagonisten entwickelten sich weiter. Schiller und Goethe begründeten die deutsche Klassik, Lenz hingegen legte mit seinen Werken – zu denen das zeitgenössische Publikum oft keinen Zugang finden konnte – den Grundstein für realistische und moderne Formen der Literatur und übte damit einen entscheidenden Einfluss auf spätere Künstler wie Georg Büchner, Gerhart Hauptmann oder Bertolt Brecht aus.

Der Beginn der Weimarer Klassik wird oft mit dem Eintreffen Christoph Martin Wielands 1772 in Weimar angesetzt, des Ersten aus dem namengebenden „Weimarer Viergestirn“: "Wieland - Herder - Goethe - Schiller". Oft wird sie enger gefasst und nur auf ‚Goethe und Schiller‘ bezogen und dann entsprechend später datiert. Ihr Ende mit Schillers Tod (1805) ist auch nur ein Anhaltsdatum. Alle Vier orientierten sich entgegen bzw. nach einer Sturm-und-Drang-Phase an humanistischen Idealen, teilweise unter klassizistischer Verwendung antiker Themen und Muster. „Klassik“ selbst ist eine positiv wertende Bezeichnung für diese Epoche.

Goethes Drama „Iphigenie auf Tauris“ thematisiert die Überwindung von Vorurteilen und ist darin ein Beispiel für das humanistische Ideal der Klassik. Das Schaffen Goethes ist sehr weitgespannt, seine spätere Phase († 1832) wird im engeren Sinne nicht mehr der „Klassik“ zugerechnet.

Friedrich Schiller schrieb in dieser Zeit zahlreiche seiner Balladen („Die Bürgschaft“), theoretische Werke („Über naive und sentimentalische Dichtung“) und eine Reihe von historischen Dramen („Wallenstein“, „Wilhelm Tell“). Auch in seiner Lyrik griff er philosophische Fragestellungen auf (etwa im „Spaziergang“).

Andere Autoren, die manchmal auch zur Klassik gezählt werden, gelten als Vorläufer wie zum Beispiel Karl Philipp Moritz (1757–1793) oder Richtung Romantik weisend Friedrich Hölderlin (1770–1843). Moritz' autobiografisch gefärbter Roman „Anton Reiser“ gilt als der erste psychologische Roman in deutscher Sprache, Hölderlins hymnische Lyrik stellt einen Höhepunkt in dieser Gattung dar.

Nicht im engeren Sinn zur Klassik gehören Jean Paul (1763–1825), der vor allem satirische Romane schrieb, und Heinrich von Kleist (1777–1811), dessen Thema häufig das Individuum ist, das sich an gesellschaftlichen Zwängen abmüht oder an ihnen zerbricht, zum Beispiel in der Novelle „Michael Kohlhaas“.

Die Epoche der Romantik wird meist in "Frühromantik", "Hochromantik", "Spätromantik" und "Nachromantik" unterteilt; im Einzelnen ist es jedoch nicht ganz einfach, zeitliche und personelle Abgrenzungen vorzunehmen.

Die Frühromantik kann aus literaturtheoretischer Perspektive als die spannendste Phase bezeichnet werden. Die miteinander befreundeten, in Jena arbeitenden Autoren, wie die Brüder August Wilhelm (1767–1845) und Friedrich Schlegel (1772–1829), Wilhelm Heinrich Wackenroder (1773–1798), Ludwig Tieck (1773–1853) und Friedrich von Hardenberg (1772–1801), der unter dem Pseudonym Novalis arbeitete, brachen mit vielen Konventionen: Beispielsweise mischten sie in ihre Romane Gedichte und Balladen, kleine Märchen etc.; dabei bezogen sie sich oft auf Goethes Werke („Werther“, „Wilhelm Meisters Lehrjahre“). Dem entspricht Friedrich Schlegels Konzept einer „progressiven Universalpoesie“, die nicht nur unterschiedlichste Gattungen und Wissensgebiete miteinander verbindet, sondern auch über sich selbst nachdenkt und ihre eigene Kritik enthält. Als wichtigstes Gestaltungsmittel dieser „Reflexionspoesie“ erscheint die Ironie, die zum Ausdruck bringt, dass der ideale Zustand, den Kunst nach „klassischer“ Theorie in den Blick bringen soll, menschlicher Vorstellung entzogen ist, und dass den Bildern, mittels derer die Künstler diesen Zustand darzustellen suchen, nicht zu trauen ist. Andererseits können wir uns der vielfältigen Bedeutungen und Bedeutungsbrechungen literarischer Werke nie sicher sein und tun deshalb möglicherweise gut daran, uns auf das Wagnis der Lüge, das die Kunst eingeht, einzulassen. Das literarische Fragment ist ein weiteres, von den Romantikern geschätztes Darstellungsmittel, in dem die Kunst ihr eigenes 'Versagen' reflektiert und sich von dem „klassischen“ Konzept des harmonisch in sich abgeschlossenen Werks, in dem sich der ideale Zustand „spiegelt“, abgrenzt.

Als Vertreter der Hochromantik oder Heidelberger Romantik gelten Achim von Arnim (1781–1831) und Clemens Brentano (1778–1842). Sie gaben unter dem Titel „Des Knaben Wunderhorn“ eine Sammlung deutscher Volkslieder heraus. Und es war deren Ehefrau und Schwester Bettina von Arnim (1785–1859), die mit ihrem Band „Goethes Briefwechsel mit einem Kinde“ – erschienen 1835 – nicht zuletzt zur Popularität Goethes in Deutschland beitrug, aber auch die sozialen und politischen Missstände in Deutschland immer wieder in ihrem Werk thematisiert hat („Armenbuch“, „Dies Buch gehört dem König“, besonders dessen Anhang, sowie die „Polenbroschüre“).

Auch die Brüder Jacob und Wilhelm Grimm zählen mit ihrer Sammlung von Volksmärchen zu dieser Epoche. Ebenso kann man auch den mittleren Tieck dieser Epoche zuordnen.

Der wohl bekannteste Spätromantiker dürfte E. T. A. Hoffmann (1776–1822) sein, der mit Erzählungen wie „Lebensbeschreibungen des Katers Murr“ und dem „Sandmann“ die romantische Ironie psychologisch wendet und so eine moderne, nicht mehr idealistisch begründete Poetik vorbereitete. Zur Spätromantik zählt darüber hinaus der Dichter Joseph von Eichendorff (1788–1857).

Heinrich Heine (1797–1856) nimmt zur Romantik und zu ihren Motiven eine oft ironische Haltung ein und müsste wohl am ehesten zum Frührealismus gerechnet werden.

Die literarischen Strömungen zwischen der „Kunstperiode“ von Klassik und Romantik einerseits und dem bürgerlichen Realismus andererseits lassen sich nicht unter einen einzigen Epochenbegriff subsumieren. Man bedient sich der historischen oder kunstgeschichtlichen Begriffe des Biedermeier und Vormärz.

Andere Autoren werden, wenn nicht zum Realismus, so zum Biedermeier gerechnet. Vor allem als Lyriker bekannt sind: Nikolaus Lenau (1802–1850), Eduard Mörike (1804–1875), Friedrich Rückert (1788–1866), August von Platen (1796–1835) und Annette von Droste-Hülshoff (1797–1848). In der Prosa sind von ihr „Die Judenbuche“ sowie Adalbert Stifter (1805–1868) und Jeremias Gotthelf (1797–1854) zu erwähnen.

Dramatiker, die mehr oder minder zum Biedermeier gehören, sind Franz Grillparzer (1791–1872), Johann Nepomuk Nestroy (1801–1862) und Ferdinand Raimund (1790–1836). Grillparzer schrieb Tragödien im Geist der Weimarer Klassik, Nestroy und Raimund vertraten das Wiener Volksstück.

Autoren, die zum Vormärz gerechnet werden, engagierten sich politisch und brachten das politische Gedicht zu einer Blüte. Viele von ihnen waren in der lockeren Gruppierung Junges Deutschland, so etwa Georg Herwegh (1817–1875), Heinrich Laube (1806–1884), Karl Gutzkow (1811–1878) und Ferdinand Freiligrath (1810–1876). Von ähnlichem Geist waren auch Heinrich Heine („Die Harzreise“, „Deutschland. Ein Wintermärchen“), Ludwig Börne (1786–1837) und der jung verstorbene Georg Büchner (1813–1837) („Woyzeck“).

Im poetischen oder bürgerlichen Realismus mieden die Autoren die großen gesellschaftspolitischen Probleme und wandten sich der engeren, lokalen Heimat mit ihrer Landschaft und ihren Menschen zu. Im Zentrum aller Romane, Dramen und Gedichte steht der Einzelmensch, das Individuum. Das stilistische Merkmal vieler Werke des poetischen Realismus ist der Humor, der die Distanz zu dem eigentlich Unerträglichen und Empörenden der Wirklichkeit schafft. Er richtet hierbei eine Anklage gegen einzelne Fehler und Schwächen im Gesellschaftsgefüge, wendet sich aber nicht gegen das System als Ganzes.

Die bevorzugte Gattungsform war anfangs die Novelle. Spätere Beispiele dafür sind etwa „Das Amulett“ (geschrieben 1872) des Schweizers Conrad Ferdinand Meyer (1825–1898) und „Der Schimmelreiter“ (geschrieben 1886–1888) von Theodor Storm (1817–1888). Im Drama bleibt lediglich Friedrich Hebbel (1813–1863) (etwa mit „Maria Magdalena“) in Erinnerung. Später trat neben die Novelle noch der Roman. Hier sind unter anderem Gustav Freytag (1816–1895) und Wilhelm Raabe (1831–1910) zu nennen.

Die beiden Größen des bürgerlichen Realismus sind der Schweizer Gottfried Keller (1819–1890), der unter anderem mit Theodor Storm in regem Briefkontakt stand, und Theodor Fontane (1819–1898). Keller schrieb den Bildungsroman „Der grüne Heinrich“ sowie die Novellenzyklen Züricher Novellen und Die Leute von Seldwyla, wozu „Romeo und Julia auf dem Dorfe“ gehört. Fontane, der als Journalist begonnen hatte, schrieb Romane wie „Frau Jenny Treibel“ oder „Effi Briest“. Er weitete seine Sicht von einer zentralen Figur immer weiter zum Gesellschaftsroman aus.

In Österreich finden sich dörfliche Motive bei Marie von Ebner-Eschenbach (1830–1916), Ludwig Anzengruber (1839–1889) und, schon nach Ausklingen der Epoche, Peter Rosegger (1843–1918).

Der Naturalismus war eine neue Kunst- und Literaturrichtung, die die Verhältnisse in allen gesellschaftlichen Bereichen schonungslos aufdecken wollte. Was den Realisten der Jahrhundertmitte als Thema noch verpönt gewesen war, wurde zum Hauptgegenstand dieser literarischen Richtung. Ohne Rücksicht auf traditionelle Grenzen des so genannten guten Geschmacks und auf bürgerliche Kunstauffassungen sollten Wirklichkeitsausschnitte möglichst in einer Deckungsgleichheit zwischen Realität und Abbild wiedergegeben werden. Eine wesentliche stilistische Neuerung war es hierbei, dass Umgangssprache, Jargon und Dialekt Einzug hielten. Der individuelle Held, der sich frei entscheiden kann, steht nicht länger im Mittelpunkt der Erzählungen und Dramen, sondern der durch ein Kollektiv oder durch Herkunft, Milieu und Zeitumstände bestimmte Mensch.

Anders als in der russischen oder französischen Literatur gibt es im deutschen Sprachraum keine bedeutenden naturalistischen Romane. Arno Holz (1863–1929) schuf gemeinsam mit Johannes Schlaf (1862–1941) Lyrik und Kurzprosa („Papa Hamlet“). Bekannt ist Holz' Gleichung „Kunst = Natur - x“, wobei x nach Möglichkeit gegen Null streben, die Kunst also nichts weiter als Abbildung der Wirklichkeit sein sollte. Bedeutender ist der Beitrag von Gerhart Hauptmann (1862–1946), der mit Dramen wie den „Webern“ internationale Anerkennung fand. Am Rande des Naturalismus ist Frank Wedekind (1864–1918) zu sehen. Sein Drama „Frühlings Erwachen“ weist mit seiner pubertär-erotischen Thematik bereits in Richtung des Fin de siècle.

Mit Naturalismus und Symbolismus beginnt das, was man oft als die Klassische Moderne bezeichnet. Diese Zeit ist geprägt von einem Stilpluralismus, dem Nebeneinander verschiedener Strömungen. Die meisten Autoren lassen sich in mindestens eine dieser Stilrichtungen einordnen.

In der Klassischen Moderne erlangte der Begriff der „Avantgarde“ eine besondere Wichtigkeit. Den Beginn nahm diese Epoche im Ausgang des 19. Jahrhunderts mit dem französischen Symbolismus, mit Dichtern wie Stéphane Mallarmé, Charles Baudelaire und Arthur Rimbaud. Die wichtigsten Vertreter des deutschen Symbolismus sind Stefan George (1868–1933), Hugo von Hofmannsthal (1874–1929) und Rainer Maria Rilke (1875–1926). Der Symbolismus verfolgt ein gänzlich anderes Programm als der oben beschriebene, ungefähr zeitgleiche Naturalismus. Symbolistische Lyrik ist elitär und legt höchsten Wert auf Schönheit und Form. Eine ihr verwandte Richtung in der Kunst ist der Jugendstil, der Zeitraum wird als Fin de siècle bezeichnet.

Zentren der deutschen Literatur waren Berlin und Wien, entsprechend wird auch oft von „Berliner Moderne“ und „Wiener Moderne“ gesprochen. Diese erlitten einen jähen Abbruch mit dem Ausbrechen des Ersten Weltkrieges.

Parallel zu diesen programmatisch gegen die Tradition gerichteten Strömungen entstanden Prosawerke, die die alten Formen aufgriffen und weiterentwickelten; zu nennen sind Rainer Maria Rilke mit seinem Roman "Die Aufzeichnungen des Malte Laurids Brigge" (1910), Heinrich Mann (1871–1950) (der in dem Frühwerk als ein Wegbereiter des Expressionismus gelten darf), Thomas Mann (1875–1955) (mit artifiziellen Großromanen und Motive durchspielenden Erzählungen), Hermann Broch (1886–1951), Robert Musil (1880–1942), Franz Kafka (1883–1924) und Hermann Hesse (1877–1962).

Die Heimatkunst war eine literarische Strömung im deutschsprachigen Raum von etwa 1890 bis 1910. Sie entstand in unmittelbarem Anschluss an den Naturalismus. Der Hauptpropagandist der neuen Bewegung wurde der Schriftsteller und Literaturhistoriker Adolf Bartels, der 1898 in einem Artikel in der Zeitschrift "Der Kunstwart" erstmals den Begriff "Heimatkunst" verwendete. Gemeinsam mit Friedrich Lienhard verbreitete er die neuen Anschauungen in der kurzlebigen, in Berlin erscheinenden Zeitschrift "Heimat".

Die neue Bewegung sollte vom Sujet der Großstadt weg und in Richtung Heimat und Volkstum gehen. Mit der weiten Auffassung von „Heimat“ ist nicht nur ländliches, sondern auch städtisches Leben gemeint, da auch die Stadt Heimat sein kann. Wie der Naturalismus, von dem sie einige Techniken übernahm, sollte sie neben der Liebe zur Heimat auch Kritik an ihr üben, was ihr nicht durchgehend gelang. In neueren Untersuchungen wurde festgestellt, dass die Heimatkunstbewegung manche Grundgedanken der späteren Ökologiebewegung vorwegnahm.

Mit ihrer konservativen, antimodernistischen Grundhaltung war sie eine Vorläuferin der nationalsozialistischen Blut-und-Boden-Literatur.

Der Expressionismus gilt als die letzte große Literaturströmung Deutschlands. Wie schon der Symbolismus ist sie eine avantgardistische Literaturströmung. Die Avantgarde ist neuartigkeits- und theoriebetonte Literatur, sie tritt mit antibürgerlichem Gestus auf. Dieser erreichte einen Höhepunkt im Dadaismus, der das bildungsbürgerliche Publikum mit Nonsense-Literatur brüskierte. Einflüsse kommen auch vom Surrealismus und Futurismus. Diese Richtungen erfuhren in Deutschland durch den Nationalsozialismus, europaweit durch den Zweiten Weltkrieg, eine Zäsur, in gewissem Sinne sogar ihren außerliterarisch bedingten Abbruch.

Als Initialzündung der expressionistischen Lyrik gilt Jakob van Hoddis' Gedicht "Weltende" von 1911, dessen wenige Zeilen „schienen uns in andere Menschen zu verwandeln“, wie Johannes R. Becher formulierte. Gottfried Benn, der gerade die Ausbildung zum Mediziner beendete, erregte Aufsehen mit dem schmalen Band „Morgue“, der Gedichte in Prosaversen zu Themen brachte, die bislang kaum oder gar nicht dargestellt wurden (beispielsweise Leichenbeschauhaus, Geburt im Kreißsaal und Prostitution).

Weitere wichtige Autoren des Expressionismus waren Alfred Döblin, Albert Ehrenstein, Carl Einstein, Salomo Friedlaender, Walter Hasenclever, Georg Heym, Heinrich Eduard Jacob, Ludwig Rubiner, Else Lasker-Schüler, August Stramm, Ernst Toller, Georg Trakl und Alfred Wolfenstein.

Nach dem Expressionismus setzte vermehrt eine nüchtern-realistische Haltung ein, die zusammenfassend als Neue Sachlichkeit bekannt wurde. "Aktualität, Realismus und Unparteilichkeit waren die Hauptforderungen an die neusachliche Literatur." Im Bereich der Dramatik sind hier Ödön von Horváth, Bertolt Brecht und der Regisseur Erwin Piscator zu nennen, für die Epik unter anderem Erich Kästner, Anna Seghers, Erich Maria Remarque und Arnold Zweig, ebenso wie Marieluise Fleißer, Irmgard Keun oder Gabriele Tergit. Der Gebrauchswert der Lyrik wurde stark betont, wodurch traditionelle Formen wie das Sonett wieder an Popularität gewinnen konnten. Erich Kästner, Joachim Ringelnatz, Kurt Tucholsky und Mascha Kaléko erlangten mit ihren Gedichten eine große Bekanntheit.

Am 30. Januar 1933 wurde den Nationalsozialisten die Macht über das Deutsche Reich übergeben. Noch im selben Jahr fanden im Reich öffentliche Bücherverbrennungen statt. Unabhängige Literatur und Literaturkritik war nicht mehr möglich. Für die deutsche Republik Österreich traf dies erst mit dem Anschluss 1938 zu, auch hier wurden Bücher verbrannt. Vom Regime wurde Blut-und-Boden-Dichtung gefördert, daneben bestand auch mehr oder weniger ideologiefreie Unterhaltungsliteratur. Bekannten Regimegegnern drohte der Tod, wenn sie nicht ins Exil gingen, so wurde Jakob van Hoddis und wohl auch Carl von Ossietzky umgebracht. Einige Schriftsteller blieben im Land (z. B. G. Benn), obwohl sie in Opposition zum Nationalsozialismus standen, sie werden zur so genannten Inneren Emigration gerechnet. Sie waren zum Schweigen verurteilt, schrieben für die Schublade oder über unpolitische Themen, die Abgrenzung zu tatsächlich unpolitischen Autoren fällt aber manchmal schwer. Bekannte Namen von im Reich Gebliebenen sind Gottfried Benn, Ernst Jünger, Erich Kästner, Ehm Welk, Gerhart Hauptmann, Heimito von Doderer, Wolfgang Koeppen, Josef Weinheber, Mirko Jelusich, Franz Koch, Victor Klemperer und Robert Hohlbaum. Des Weiteren folgende Mitglieder der Dichterakademie: Will Vesper, Wilhelm Schäfer, Agnes Miegel, Emil Strauß und Rudolf G. Binding sowie die "zweitrangige[n] und parteifromme[n]" Börries Freiherr von Münchhausen, Hans Grimm, Erwin Guido Kolbenheyer, Werner Beumelburg, Hans Friedrich Blunck und Hanns Johst.

1500 namentlich bekannte Autoren gingen, oft über verschlungene Stationen, ins Exil, viele nahmen sich das Leben (Stefan Zweig, Kurt Tucholsky). Zentren deutscher Exilliteratur entstanden in vielen Staaten der Welt, darunter auch in der deutschsprachigen Schweiz, die besonders für Theaterautoren wichtig war. Angesichts der Masse an Schriftstellern, beinahe jeder von Rang ging ins Exil, kann man kaum von einer thematisch oder stilistisch einheitlichen Exilliteratur sprechen. Autoren, die auch im Exil produktiv blieben, waren unter anderem Thomas und Heinrich Mann, Bertolt Brecht, Anna Seghers, Franz Werfel und Hermann Broch. Andere, wie Alfred Döblin, Heinrich Eduard Jacob oder Joseph Roth, fanden sich nur schwer oder gar nicht zurecht. Nach dem Krieg blieben sie zum Teil im Ausland, einige kehrten zurück. Nachdem Elias Canetti infolge des österreichischen Anschlusses von Wien nach London ausgewandert war, bekam er den Literaturnobelpreis als britischer Staatsbürger. Auffällig ist, dass viele nicht mehr an ihre Leistungen in der Zwischenkriegszeit und im Exil anschließen konnten.

Nach dem Ende des Zweiten Weltkrieges sprach man von einem literarischen Nullpunkt. Die „Trümmerliteratur“ beschrieb eine zusammengebrochene Welt, bald besann man sich aber darauf, versäumte Entwicklungen der Weltliteratur nachzuholen, erst jetzt, über zwanzig Jahre nach seinem Tode, wurde Franz Kafka entdeckt. In Westdeutschland formierte sich die Gruppe 47, deren lose assoziierten Mitglieder tonangebend in der Nachkriegsliteratur waren. Die Wiener Gruppe praktizierte innovative Formen der Lyrik.

Mit dem Entstehen neuer deutscher Staaten entstanden unterschiedliche Bedingungen für die Literatur. Im Folgenden werden die deutsche Literatur der BRD, der DDR, Österreichs und der Schweiz getrennt dargestellt, die Unterschiede sollten aber nicht überbewertet werden: Immerhin handelt es sich um eine gemeinsame Sprache und, mit Ausnahme der DDR, um einen gemeinsamen Markt.

Unmittelbar nach 1945 wurden der Schrecken des Krieges und die Situation der Heimgekehrten dargestellt. Eine neu entdeckte Form dafür war die Kurzgeschichte, etwa von Heinrich Böll. Nach dem Einsetzen des deutschen Wirtschaftswunders, konzentrierte man sich auf die Gegenwart, so in den Romanen von Wolfgang Koeppen, Siegfried Lenz, Christine Brückner und Martin Walser. Ein wichtiger Lyriker der Zeit war Günter Eich, der auch Hörspiele schrieb, ein damals sehr populäres Genre. 1952 bis 1956 erschien Werner Riegels und Peter Rühmkorfs Zeitschrift "Zwischen den Kriegen", und der dort debütierende Rühmkorf wurde zu einem einprägsamen lyrischen Autor für zwei Generationen. Konkrete Poesie stammte u. a. von Eugen Gomringer und Helmut Heißenbüttel. Günter Grass, Literaturnobelpreisträger des Jahres 1999, schrieb "Die Blechtrommel", einen Schelmenroman, der die jüngere deutsche Geschichte behandelte und auch international hohes Ansehen errang.

Autoren, die sich nur schwer einer bestimmten Richtung zuordnen lassen, sind der experimentierfreudige Arno Schmidt, Uwe Johnson und der vom Nouveau roman geprägte Ror Wolf. Wolfgang Hildesheimer schrieb absurde Dramen zu einer Zeit, als die Theaterlandschaft noch immer von Bertolt Brecht geprägt war.

Ab 1962 bildete sich um die Zeitschrift "pardon" die Neue Frankfurter Schule mit v. a. F. W. Bernstein, Robert Gernhardt und F. K. Waechter heraus, die nicht nur als Lyriker stilistische Innovatoren wurden. Bekanntester Romancier der NFS ist Eckhard Henscheid.

Mit dem Vietnamkrieg und der 68er-Bewegung besann man sich auf das politische Gedicht (Hans Magnus Enzensberger, Erich Fried) und das politische Drama (Peter Weiss, Rolf Hochhuth). Eine dem entgegengesetzte Tendenz war die „Neue Subjektivität“, die Beschäftigung mit privaten Themen (u. a. Jürgen Theobaldy). Herausragender deutschsprachiger Pop- und Underground-Lyriker der 1960er und 70er Jahre war Rolf Dieter Brinkmann.

In den 1980er Jahren traten Botho Strauß (Drama) und in der Lyrik Ulla Hahn und später Durs Grünbein hervor.

Die DDR definierte sich selber als „Literaturgesellschaft“ (der Begriff stammt von Johannes R. Becher), sie kämpfte gegen die „Poesiefeindlichkeit“ des Westens und gegen die Ghettoisierung einer Hochkultur. Eine Demokratisierung sollte auf Ebene der Produktion, der Distribution und der Rezeption durchgeführt werden. Allerdings wurde durch die Zensur der Begriff der Demokratisierung ad absurdum geführt, da der Staat versuchte, die Literatur zu instrumentalisieren und für seine Zwecke, d. h., für die des Realsozialismus, zu verwenden.

Gefördert wurde eine Literatur auf der Grundlage des Sozialistischen Realismus, ein darauf aufbauender Plan wurde als „Bitterfelder Weg“ bekannt. In den 1970er Jahren lässt sich wie in der BRD eine Tendenz zur „Neuen Subjektivität“ feststellen. Viele Autoren mussten oder durften die DDR verlassen, so Wolf Biermann, Jurek Becker, Reiner Kunze, Günter Kunert, Sarah Kirsch und schon früher Peter Huchel und Uwe Johnson. Von den systemnahen Autoren sind vor allem Hermann Kant und Stephan Hermlin zu erwähnen, mehr oder weniger große Distanz zum Staat hielten Volker Braun, Christa Wolf, Heiner Müller, Irmtraud Morgner und Stefan Heym oder Wolfgang Hilbig.

Nach dem Zweiten Weltkrieg bemühten sich insbesondere die Wiener Gruppe um Gerhard Rühm und H. C. Artmann sowie Autoren wie Albert Paris Gütersloh und Heimito von Doderer um Anknüpfpunkte an die durch den Austrofaschismus und die Zeit des Nationalsozialismus verschüttete moderne Tradition.

Die Affinität zum Sprachspiel ist eine Konstante in der Literatur Österreichs, zu den bekannteren Vertretern gehören Ernst Jandl und Franzobel. Wichtige Lyrikerinnen waren Friederike Mayröcker und Christine Lavant.

Der Lyriker Paul Celan lebte 1947/48 einige Monate in Wien, ließ sich dann aber in Paris nieder. Erich Fried emigrierte nach Großbritannien.

Eine Blüte erlebte die Literatur in Österreich in den 1960er und 70er Jahren, als mit Autoren wie Peter Handke, Ingeborg Bachmann und Thomas Bernhard die deutsche Literaturlandschaft nachhaltig verändert wurde. In dieser Tradition arbeiten auch bedeutende zeitgenössische Autoren wie beispielsweise Ruth Aspöck, Sabine Gruber, Norbert Gstrein, Elfriede Jelinek, Christoph Ransmayr, Werner Schwab, O. P. Zier, Robert Menasse, Eva Menasse, Arno Geiger, Robert Seethaler und Paulus Hochgatterer.

Anders als in Deutschland oder Österreich gab es mit 1945 keinen grundlegenden Einschnitt in der deutschen Literatur der Schweiz. Die wichtigsten Deutschschweizer Autoren sind Friedrich Dürrenmatt und Max Frisch. Beide schrieben Romane und Dramen, Frisch eher intellektuell, Dürrenmatt eher pointiert-grotesk. Weitere bekannte Schweizer Autoren, die oft im Schatten der beiden großen standen, sind etwa Peter Bichsel, Thomas Hürlimann, Hugo Loetscher, Adolf Muschg oder Urs Widmer. Die wichtigste literarische Vereinigung der Schweiz war die Gruppe Olten, sie bestand von 1971 bis 2002.

In vielen Ländern mit deutschen Minderheiten sind eigene deutschsprachige Literaturen entstanden, die mehr oder weniger mit der deutschen Literatur des Binnensprachraums in Verbindung stehen, teilweise aber auch isoliert sind. Solche deutschsprachigen Auslandsliteraturen haben sich in Nordamerika (Deutschamerikanische und Deutschkanadische Literatur) entwickelt, des Weiteren in mehreren Staaten Südamerikas (Deutschbrasilianische, Deutschargentinische und Deutschchilenische Literatur). In Afrika gibt es durch die deutsche Kolonialvergangenheit und die Einwanderung deutscher Siedler im 19. und frühen 20. Jahrhundert deutschsprachige Literaturen in Namibia und Südafrika. In Europa bestehen deutschsprachige Minderheitenliteraturen unter anderem in Italien (Südtirol), Frankreich (Elsass), Belgien (Eupen-Malmedy), Dänemark (Nordschleswig), Polen (Westliches Oberschlesien), Russland (Wolgadeutsche, Russlanddeutsche) und Rumänien (Rumäniendeutsche Literatur) sowie im Baltikum die Deutschbaltische Literatur. Kennzeichen auslandsdeutscher Literatur, vor allem in Übersee, ist die Publikation literarischer Texte in Kalendern und Jahrbüchern. Eine wichtige Rolle bei der Verbreitung dieser Literaturen spielt auch die vor Ort erscheinende auslandsdeutsche Presse.

Der meistgelesene zeitgenössische rumäniendeutsche Autor, der in Rumänien wirkt, ist Eginald Schlattner. Mittlerweile in Deutschland schreibt die Banater Autorin Herta Müller.

Vorher wirkte Adolf Meschendörfer in der ersten Hälfte des 20. Jahrhunderts in Kronstadt.

Obwohl die meisten deutschsprachigen Menschen aus Rumänien ausgewandert sind, hat sich im Banat eine neue Literaturgruppe Die Stafette zusammengefunden, aus der neue deutschsprachige Autoren, die die Rumäniendeutsche Literatur weiterführen, hervorgehen könnten.

In den 1990er Jahren erlebte die deutschsprachige Literatur einen vorübergehenden Boom an Debütantinnen und Jungautoren. Diese Erscheinungen waren zum Teil vom Buchmarkt gesteuert, der seit 1945 enorm angewachsen ist und spätestens seit 1990 so groß ist, dass selbst gute Literatur schwer über die Wahrnehmungsschwelle kommt.

Unter den Sammelbegriff Popliteratur wurde in den 1990er Jahren eine Reihe jüngerer Autoren gefasst, die sich sprachlich und ästhetisch an der Popkultur in Musik und Werbung orientierten, am bekanntesten und folgenreichsten u. a. Benjamin von Stuckrad-Barre, Alexa Hennig von Lange oder Christian Kracht "(Faserland)". Auch die Autoren Thomas Meinecke, Andreas Neumeister und Rainald Goetz werden mit der Popliteratur assoziiert. Insbesondere Christian Kracht wird von der Literaturwissenschaft allerdings zunehmend in einem postmodernen Sinne verstanden und gelesen.

Als postmoderne deutschsprachige Roman-Autoren seien Hans Wollschläger und Walter Moers sowie die Österreicher Oswald Wiener, Christoph Ransmayr und Marlene Streeruwitz genannt. Aus England meldete sich W. G. Sebald zu Wort mit Aufsehen erregenden Polemiken zur deutschen Nachkriegsliteratur und die Genre­grenzen von Roman, Biografie und Reiseliteratur ignorierenden oder bewusst überschreitenden Texten.

Zudem haben seit den 1990er Jahren im deutschsprachigen Raum multikulturelle Literaturen wieder an Bedeutung gewonnen; z. B. hat sich eine deutsch-türkische Literatur etabliert, deren Wurzeln in der Migrationsliteratur der 1960er Jahre liegen. Als türkischstämmige Schriftsteller gehören Feridun Zaimoglu und Osman Engin heute zu den prominenten Gegenwartsautoren deutscher Sprache. Auch Vertreter anderer multikultureller Literaturen, wie Wladimir Kaminer oder Rafik Schami sind in der deutschsprachigen Gegenwartsliteratur bekannte Autoren.

Einer der wichtigsten Lyriker seit Ende der 1980er Jahre ist neben Marcel Beyer, Durs Grünbein und Uwe Kolbe vor allem Thomas Kling (1957–2005), der mit seiner oft phonetisch orientierten Schreibweise für belebende Akzente in der deutschsprachigen Poesie gesorgt hat.

Herausragende zeitgenössische Romanautoren sind unter anderem Thomas Brussig, Dietmar Dath, Arno Geiger, Thomas Glavinic, Daniel Kehlmann, Wolfgang Herrndorf, Robert Menasse und seine Halbschwester Eva Menasse, Martin Mosebach, Ulrich Peltzer, Ralf Rothmann, Eugen Ruge, Robert Seethaler, Bernhard Schlink, Ingo Schulze, Uwe Tellkamp, Uwe Timm und Juli Zeh, zu den bekanntesten Dramatikern gehören Albert Ostermaier, Moritz Rinke oder Roland Schimmelpfennig.

Der aktuellen deutschsprachigen Literatur wird oft politische Indifferenz vorgeworfen sowie ein Kreisen um autobiografische Themen aus der Kindheit. Ein Kontrapunkt ist hier die Verleihung des Literaturnobelpreises 2004 an Elfriede Jelinek, die politisch und feministisch engagierte Literatur schreibt. Der mit dem Deutschen Buchpreis 2014 ausgezeichnete Roman Kruso von Lutz Seiler wird zwar als Beispiel für den Blick vom historischen Geschehen weg auf die Intimität der Protagonisten gedeutet. Diese „Verweigerung eindeutiger Gesten“ in Literatur und Politik sei jedoch eine Provokation: „Poesie ist Widerstand“, sagt einer der Romanhelden. „Bekennt man sich im ehemaligen Westen zu einem offensiven Realismus, halten die Ost-Schriftsteller an dem Versteckspiel fest, das einmal überlebensnotwendig war.“

Zur deutschsprachigen Literatur gehören auch Übersetzungen aus allen Weltsprachen, welche in den deutschsprachigen Ländern erscheinen; in der Belletristik machen sie zum Beispiel die Hälfte aller Neuerscheinungen aus.

Unter den sich auf ein Genre festlegenden und auch in Serie schreibenden Autoren sind für den deutschsprachigen Raum nennenswert:


Einbändige Literaturgeschichten

Mehrbändige Literaturgeschichten

"Geschichte der deutschen Literatur von den Anfängen bis zur Gegenwart". Begründet von Helmut de Boor und Richard Newald. Beck, München 1971- (12 Bände geplant, erschienene Bände und Teilbände teilweise in neuerer Bearbeitung)

Literaturgeschichten mit Primärtexten

Nachschlagewerke



</doc>
<doc id="13313" url="https://de.wikipedia.org/wiki?curid=13313" title="Turbo Pascal">
Turbo Pascal

Turbo Pascal ist eine integrierte Entwicklungsumgebung (IDE) des Unternehmens Borland für die Programmiersprachen Pascal und Object Pascal.

Der Compiler basierte auf dem "Blue Label Software Pascal" Compiler, der von Anders Hejlsberg ursprünglich für das Kassetten-basierte Betriebssystem NasSys des Mikrocomputers Nascom entwickelt wurde.
Dieser Compiler wurde zunächst als "Compass Pascal" Compiler für das Betriebssystem CP/M und dann als Turbo Pascal Compiler für MS-DOS und CP/M weiterentwickelt.

Die erste Version von Turbo Pascal erschien im November 1983, zu einer Zeit, als das Konzept der integrierten Entwicklungsumgebungen noch recht neu war. Ein Programmierer hatte zu dieser Zeit auf einem IBM-kompatiblen PC im Wesentlichen die Wahl zwischen dem mit DOS mitgelieferten Microsoft BASIC-Interpreter oder einem professionellen und teuren BASIC-, C-, Fortran- oder Pascal-Compiler (UCSD Pascal). Die Compiler waren eher umständlich zu benutzen: Mangels Multitasking unter MS-DOS bestand jeder Testlauf aus dem Verlassen, Starten und Neuladen der verschiedenen Tools (Editor, Compiler, Linker, Debugger), die für die Softwareentwicklung notwendig sind. Da die meisten PCs zu dieser Zeit keine Festplatten hatten (eine solche kostete zum damaligen Zeitpunkt 2000 US-Dollar und mehr), musste oft sogar noch mehrmals die Diskette gewechselt werden.

In diese Situation hinein kam Turbo Pascal mit dem Konzept einer integrierten Entwicklungsumgebung, das die verschiedenen Tools in einem Programm vereinte. Es war zudem gerade einmal rund 60 KB groß und lief damit auf jedem damaligen PC in hoher Geschwindigkeit. Selbst auf einem PC mit nur einem Diskettenlaufwerk konnte auf Diskettenwechsel verzichtet werden, da auf der Turbo-Pascal-Diskette noch genug Platz für das gerade bearbeitete eigene Programm war. Schließlich war das System preislich selbst für Schüler und Studenten erschwinglich – mit dem Ergebnis, dass es im Laufe der 1980er Jahre auf dem PC zum Quasistandard wurde.

Ohne Turbo Pascal hätte die Sprache Pascal sicher das gleiche Schicksal ereilt wie viele an Universitäten vorher und nachher geborene „Kunstsprachen“, z. B. Modula-2 oder Oberon (beide auch von Niklaus Wirth), die heute praktisch verschwunden sind. Hejlsberg entwickelte die Sprache und das System pragmatisch weiter: Von Anfang an wurde die Laufzeitbibliothek um Routinen (Unterprogramme) ergänzt, die Zugriff auf das System ermöglichten – ganz entgegen dem ursprünglichen Konzept von Wirth. Dabei wurde – anders als z. B. bei der Sprache C – die für Pascal typische strenge Typprüfung etc. beibehalten (beides hat Vor- und Nachteile: Eine strenge Prüfung vermindert die Gefahr ungewollten Fehlverhaltens eines Programms, macht den Quelltext aber meist länger und zwingt dazu, bewusst Funktionen zur Typumwandlung zu nutzen). Je umfangreicher ein Programmpaket wird, desto wichtiger werden solche Funktionen, weshalb auch andere Programmiersprachen, wie beispielsweise C++, Java und C, diese Konzepte in unterschiedlicher Strenge übernommen haben.

Im September 1986 kam in der MS-DOS-Version Grafik dazu. Dies war die letzte Version, die auch noch für CP/M erschien, allerdings ohne die Grafikmöglichkeiten (ausgenommen die "Graphix Toolbox" des Schneider CPC), da die meisten CP/M-Rechner nicht grafikfähig sind. Es gab drei unterschiedliche Versionen für MS-DOS, die es ermöglichten, unterschiedlichen Code zu generieren, und zwar für emulierten Gleitkomma-Code, Coprozessor-orientierten Code, und BCD-Code. Es werden kommerzielle Programmbibliotheken angeboten, diese müssen allerdings mittels INCLUDE im Quelltext eingebunden werden.

Im Dezember 1987 kam das Unit-Konzept dazu, das Bibliotheken und große Projekte ermöglichte. Das Einfügen von Assemblerteilen in den Quelltext wurde mit Inline-Codes unterstützt.

Im Oktober 1988 wurde der Debugger in die Entwicklungsumgebung integriert. Damit wurde es möglich, innerhalb der IDE zu debuggen, Haltepunkte zu setzen und Variablenwerte zu beobachten.

Im Mai 1989 kam die Objektorientierung hinzu.


Im November 1990 kam eine objektorientierte GUI-Bibliothek hinzu (Turbo Vision), ähnlich den späteren MFC für Windows. Turbo Vision war für den Textmodus des PCs konzipiert, enthielt aber bereits Steuerelemente wie Fenster, Befehlsschaltflächen und Bildlaufleisten, die durch Textsymbole dargestellt wurden. Außerdem konnten (auch umfangreichere) Assemblerfunktionen in Intelsyntax direkt im Quelltext realisiert werden. Die Entwicklungsumgebung wurde entsprechend erweitert, so dass auch Assemblerteile im Einzelschrittmodus bei gleichzeitiger Kontrolle aller Flag- und Registerinhalte ausgeführt werden konnten.

kam parallel zu Version 6 auf den Markt. Das GUI war komplett als Windows-Anwendung ausgelegt; es wurde in Version 7 (Borland Pascal) übernommen und ausgebaut.

Im Oktober 1992 wurde in der professionellen Variante (Borland Pascal) die Entwicklung von Protected-Mode-Anwendungen innerhalb der IDE möglich – allerdings ohne integrierten Debugger. Im April 1993 folgte noch eine nachgeschobene/fehlerbereinigte Version 7.01; dies war zugleich auch die letzte von Borland veröffentlichte Pascal-Version.

Anfang der 1990er-Jahre wurde Turbo Pascal auf Windows portiert. Dies war allerdings eine Sackgasse. Die Programmierung war unter Turbo Pascal für Windows ähnlich aufwendig wie in C – mit dem zusätzlichen Nachteil, dass Windows selbst in C programmiert ist, weshalb die Schnittstellen zwischen Windows und Pascalprogramm mindestens grundlegende C-Kenntnisse erfordern. Borland adaptierte in der Folgezeit das Rapid-Application-Development-Prinzip, das sich vorher schon bei Visual Basic von Microsoft sehr bewährt hatte: Die grafischen Elemente einer Windows-Anwendung werden mit der Maus zusammengestellt, der zugehörige Code wird automatisch erzeugt. Dieses Produkt wurde Delphi genannt, die zugrundeliegende Sprache ist "Object Pascal" von Borland.

Borland veröffentlichte 2002 mehrere Turbo Pascal Versionen als Freeware auf der eigenen Webseite, nachdem sie zu „antique software“ (Abandonware) wurden. Die neueste veröffentlichte internationale Version ist TP 5.5, jedoch wurde von dem neueren TP 7.01 die französische Version veröffentlicht. Die Downloads sind weiterhin auf der Nachfolgerwebseite von Embarcadero verfügbar.

Mit Free Pascal und GNU Pascal gibt es zwei Turbo-Pascal-kompatible freie Compiler, die für zahlreiche Betriebssysteme zur Verfügung stehen. Die Entwicklung von Virtual Pascal wurde hingegen eingestellt, obwohl es noch eine große Gemeinschaft gibt.

program HalloWelt;
begin
end.





</doc>
<doc id="13316" url="https://de.wikipedia.org/wiki?curid=13316" title="Klimazone">
Klimazone

Klimazonen sind sich in Ost-West-Richtung um die Erde erstreckende Gebiete, die anhand grundlegend unterschiedlicher klimatischer Verhältnisse voneinander abgegrenzt sind. In der Regel sind die Klimazonen gürtelförmig, in den Polarregionen kreisförmig, teilweise können sie aber auch unterbrochen sein. Die Klimazonen sind die Grundlage verschiedener geozonaler Modelle der Geo- und Biowissenschaften.

Die Beleuchtungsklimazonen, auch astronomische, mathematische oder solare Klimazonen genannt, gehen von der Erkenntnis aus, dass das Klima in erster Linie von unterschiedlicher Sonneneinstrahlung abhängt. Der Winkel (griech. "klíma" Neigung) und damit die Intensität der Sonneneinstrahlung und ihr jahreszeitlicher Verlauf hängen bei Vernachlässigung atmosphärischer Einflüsse nur von der geografischen Breite ab.

Die Unterteilung der Erde in Beleuchtungsklimazonen wird astronomisch exakt an den Wende- und Polarkreisen vorgenommen. Dadurch entstehen die astronomischen Tropen (griech. "tropē" Wende) bis 23° 26′ Breite, die Mittelbreiten bis 66° 34′ und die Polarzonen (griech. "pólos" Achse). Innerhalb der astronomischen Tropen steht die Sonne zweimal jährlich im Zenit und erreicht immer eine Mittagshöhe von mindestens 43°. In den Polarzonen steht die Sonne während der Polarnächte mittags unter dem Horizont, während sie demgegenüber während der Polartage zu Mitternacht über dem Horizont steht; sie erreicht dabei nie eine größere Mittagshöhe als 47°. Entsprechend unterschiedlich ist die eingestrahlte Energie.

Die moderne Klimatologie teilt die solaren Mittelbreiten im Hinblick auf die realen Verhältnisse auf der Erdoberfläche ein weiteres Mal am 45. Breitengrad. Die Hälfte der niederen Breiten heißt Subtropen, die der hohen Breiten behält die Bezeichnung Mittelbreiten. Die Erdoberfläche ist von den dadurch entstehenden vier Klimazonen vom Äquator zum Pol in vier annähernd gleich breite Gürtel geteilt. Zwischen ihnen unterscheidet sich die solare Strahlung an der Atmosphärenobergrenze neben einer Abnahme der Jahressumme vom Äquator zu den Polen vor allem auch qualitativ:

Die tatsächlichen Verhältnisse am Boden werden charakterisiert durch die physischen Klimazonen. Für sie gilt insbesondere die Maßgabe, dass die Breitenangaben der Beleuchtungsklimazonen nur ungefähr zu übernehmen sind, so dass die exakten Grenzen lokal anhand anderer Kriterien bestimmt werden können. Außerdem sind die bestrahlten Flächen in die Betrachtung einzubeziehen. Hier bestehen weitere Unterschiede z. B. bei der Strahlungsbilanz, also der Differenz zwischen eingestrahlter und abgestrahlter Energie. An der Erdoberfläche sind darüber hinaus die Unterschiede durch den Einfluss der Atmosphäre noch deutlicher ausgeprägt:
Diese Bilanzdifferenzen werden durch Wärmetransporte ausgeglichen, die wesentlich das Wettergeschehen bestimmen. Es entsteht die planetarische Zirkulation, die ebenfalls zonal angeordnet ist und entsprechende klimatische Auswirkungen hat.

Eine frühere, immer noch gängige, Einteilung der physischen Klimazonen ist die folgende Einteilung nach Breitengraden:

Klimaklassifikationen definieren Klimatypen nach Klimafaktoren oder Klimaelementen. Es ist wünschenswert, dass die sich aus diesen Klimatypen ergebenden Klimaregionen mit den solaren Klimazonen kompatibel sind. So lassen sich die Klimazonen dann beispielsweise weiter unterteilen, z. B. in Kalttropen und Warmtropen, aride und humide Typen. Da die Klimaklassifikationen viele weitere Anforderungen erfüllen sollen, wie eine weitgehende Übereinstimmung mit Vegetations- und Ökozonen, ist dies eine nichttriviale Forderung.

Eine Klimaklassifikation, die Entsprechungen zu den solaren Klimazonen liefert, ist die "ökophysiologische Klimaklassifikation nach Lauer und Frankenberg". Die "effektive Klassifikation nach Troll/Paffen" ist primär auf ökologische Übereinstimmung ausgerichtet.
Es existieren andere Klimaklassifikationen, die zu teilweise sehr unterschiedlichen Klimazonen mit anderen Benennungen führen. Häufig verwendet werden vor allem die "effektive Klassifikationen nach Köppen/Geiger", die "USDA-Klimazonen" sowie die "genetische Klimaklassifikation nach Flohn", nach "Neef" oder "Terjung/Louie". Teilweise versuchen sie sich an die solaren Klimazonen anzulehnen, geben dies teilweise aber auch auf.

Die Tropen liegen um den Äquator zwischen den Wendekreisen. Die Tageslänge bewegt sich zwischen 10,5 und 13,5 Stunden. Die Jahreszeiten haben keine thermische Ausprägung. Es herrscht ein Tageszeitenklima: die täglichen Temperaturschwankungen sind größer als die jährlichen. Neben den Warmtropen existieren in Gebirgsgegenden auch Kalttropen, die gegenüber anderen Gebirgsklimaten durch ihre jahreszeitlich konstanten Verhältnisse ausgezeichnet sind. Niederschlagsbestimmendes Phänomen der Tropen ist die Passatzirkulation und deren jahreszeitliche Verschiebung. Die Passatzirkulation bewirkt rund um ihre Konvergenzzone den ständigen so genannten Zenitalniederschlag. Die Konvergenzzone kann fast stillstehen – im Pazifik und Atlantik – oder sich im Jahresverlauf zyklisch einmal über die gesamten Tropen bewegen wie im Gebiet von Zentralafrika bis zum Malaiischen Archipel. Entsprechend entstehen Gebiete in einem Spektrum von immerfeucht bis trocken. Daneben wirken die Winde der Zirkulation, Passate und Monsune, örtlich auflandig und verursachen dann ebenfalls Niederschläge.

Die Subtropen werden thermisch definiert als die Klimazone mit hoher Sommer- und mäßiger Winterwärme. Man kann sie unterteilen in trockene, winterfeuchte, sommerfeuchte und immerfeuchte Subtropen. Die Subtropen und insbesondere die winterfeuchten Subtropen werden bisweilen auch als "warmgemäßigte Zone" bezeichnet.

Eine weit verbreitete Definition definiert das Klima dort als subtropisch, wo die Mitteltemperatur im Jahr über 20 Grad Celsius liegt, die Mitteltemperatur des kältesten Monats jedoch unter der Marke von 20 Grad bleibt. Die Unterschiede zwischen Tag und Nacht fallen hoch aus. Die Vegetation reicht von der Artenvielfalt, wie sie zum Beispiel im Mittelmeerraum auftritt über die Vegetation der trockenen Savanne bis hin zur kargen oder auch völlig fehlenden Vegetation in Wüsten wie der Sahara.

Die gemäßigte (auch temperierte oder temperate) Klimazone erstreckt sich vom Polarkreis bis zum fünfundvierzigsten Breitengrad und wird meist weiter unterteilt in
Seltener wird dazwischen noch eine kühlgemäßigte Zone gestellt.

Die gemäßigte Zone weist einen großen Unterschied zwischen den Jahreszeiten auf, der in Richtung des Äquators jedoch etwas abnimmt. Ein weiteres Merkmal sind die Unterschiede zwischen Tag und Nacht, die je nach Jahreszeit stark variieren. Diese Unterschiede nehmen, je näher man dem Pol kommt, immer mehr zu. Die Vegetation wird durch Nadel-, Misch- und Laubwälder geprägt, wobei die Nadelwälder in Richtung Äquator immer weniger werden.

Die wesentlichen klimatologischen Kriterien sind die Tageslängenschwankungen der Sonne (von 8 bis zu 16 Stunden), die thermische Jahreszeiten hervorrufen, die Niederschläge die über das ganze Jahr ausgeglichen sind und die dadurch sehr unbeständige Witterung. Mit Werten um 800 mm hat die gemäßigte Zone die zweithöchste Niederschlagsmenge nach den Tropen. Die Mittelbreiten liegen in der Westwindzone.

Die subpolare Zone ist eine Klimazone, die den Übergang zwischen polarer Klimazone und gemäßigter Klimazone bildet. Laut genetischer Klimaklassifikation (nach Ernst Neef u. a.) ist sie durch den halbjährlichen Wechsel von außertropischen Westwinden im Sommer und polaren Ostwinden im Winter gekennzeichnet.

Unter den Polargebieten versteht man zum einen die Region im Bereich des nördlichen Polarkreises, die Arktis, sowie den Kontinent der Antarktis auf der Südhalbkugel der Erde.

Die Polargebiete der Erde sind Kältewüsten (Lauer 1995). Die Temperaturen liegen das ganze Jahr unter oder nur knapp über der Nullgradgrenze, die Niederschläge sind gering und die solare Einstrahlung der Sonne ist reduziert – im Durchschnitt 40 % weniger als am Äquator.

"Mathematische Klimazonen" gehen zurück auf die Antike, wo die Erde, deren Kugelgestalt und Achsneigung bereits bekannt war, an den Wende- und Polarkreisen in insgesamt fünf Zonen unterteilt war. Davon galten im Unterschied zur heißen und den beiden kalten Zonen zunächst nur die beiden "gemäßigten Zonen" als bewohnbar. Den Zonen der Tropen, der gemäßigten Breiten und des Polargebiets wurden später noch subtropische und subpolare Zonen (eigentlich „unter dem Wende- bzw. Polarkreis gelegen“) hinzugefügt, die zunächst zur gemäßigten Zone gehörig gedacht wurden. Dieses Prinzip wird teilweise noch bis in die Gegenwart verwendet.

Die im zwanzigsten Jahrhundert geschaffenen Klimaklassifikationen versuchten u. a. auch, dieser Systematik gerecht zu werden. Insbesondere bei den effektiven Klassifikationen, die nur von tatsächlich gemessenen Klimaelementen abhängen, gelang dies zunächst nur bedingt, da sich die Abgrenzung anhand von Temperaturgrößen als problematisch erweist. Bei den von der globalen Zirkulation abgeleiteten genetischen Klassifikationen waren zusätzliche Zonen erforderlich. Die Zonen aller dieser Systeme erhielten folglich auch zum größten Teil andere Benennungen.

Die derzeit stattfindende, vom Menschen verursachte globale Erwärmung wird zweifellos im Laufe der kommenden Jahrzehnte zu einer Verschiebung der Klimazonen führen. In der Regel wird es sich um eine Nordverschiebung (bzw. Höhenverschiebung der Höhenstufen) handeln. Nach Mitteilung des BMBF 1990 wird eine Erhöhung der Temperatur pro Grad Celsius eine Verschiebung der Klimazonen um 100 bis 200 km bewirken.

Entscheidend ist vor allem die Geschwindigkeit, mit der der Klimawandel stattfinden wird. Davon hängt es ab, ob sich die Lebensgemeinschaften anpassen können oder nicht. Ein rascher Anstieg der Temperaturen um mehrere Grad Celsius wird für die meisten Ökosysteme Folgen haben, die jedoch wegen der Komplexität der Systeme schwer vorhersehbar sind. Sicher ist, dass sich das Aussterben von Tier- und Pflanzenpopulationen verstärken wird.




</doc>
<doc id="13317" url="https://de.wikipedia.org/wiki?curid=13317" title="Werner von Siemens">
Werner von Siemens

Ernst Werner Siemens, ab 1888 von Siemens, (* 13. Dezember 1816 in Lenthe, Königreich Hannover, heute Gehrden, Niedersachsen; † 6. Dezember 1892 in Berlin) war ein deutscher Erfinder und Industrieller. Er entdeckte das dynamoelektrische Prinzip, auch elektrodynamisches Prinzip genannt, und gilt als Begründer der modernen Elektrotechnik, speziell der elektrischen Energietechnik.

Zusammen mit Johann Georg Halske gründete Werner Siemens am 12. Oktober 1847 die "Telegraphen Bau-Anstalt von Siemens & Halske in Berlin", aus der die heutige Siemens AG hervorging. Das Unternehmen entwickelte sich innerhalb weniger Jahrzehnte von einer kleinen Werkstatt, die neben Telegraphen vor allem Eisenbahnläutwerke, Drahtisolierungen und Wassermesser herstellte, zu einem der weltweit größten Elektro- und Technologiekonzerne.

Vier seiner Brüder wirkten ebenfalls als Unternehmer und Erfinder, überwiegend im Bereich Elektrizität, siehe Navigationsleiste.

Siemens entstammte dem alten Goslarer Stadtgeschlecht Siemens (1384 urkundlich erwähnt, mit dem Siemenshaus in Goslar als Stammsitz) und wurde 1816 als viertes von vierzehn Kindern des Gutspächters Christian Ferdinand Siemens (1787–1840) und dessen Ehefrau Eleonore Henriette Deichmann (1792–1839) geboren. Das Geburtshaus, das Pächterhaus auf dem Obergut in Lenthe, enthält heute eine Dauerausstellung, die anhand zentraler Dokumente und Exponate die wichtigsten Stationen im Leben des Erfinders und Unternehmers nachzeichnet. Nach dem Umzug im Jahre 1823 von Lenthe nach Mecklenburg, wo sein Vater die Domäne Menzendorf übernahm, blieb seinen Eltern der wirtschaftliche Erfolg versagt.

Siemens wurde anfangs von der Großmutter und dem Vater unterrichtet, besuchte ein Jahr lang von 1828 bis 1829 die Bürgerschule in Schönberg und bekam drei Jahre Unterricht von einem Hauslehrer. Schließlich besuchte er für drei Jahre von 1832 bis 1834 das Katharineum zu Lübeck. Dort war er besonders in Mathematik herausragend, weshalb er in diesem Fach in einer höheren Klasse unterrichtet wurde. Er verließ das Gymnasium 1834 aber vorzeitig ohne formalen Abschluss.

Siemens wollte gerne einen praktisch-wissenschaftlichen Beruf ergreifen, doch erlaubte die wirtschaftliche Situation der Eltern kein Studium. Auf den Rat seines Geodäsie-Lehrers Ferdinand von Bültzingslöwen bewarb er sich beim Ingenieurkorps der preußischen Armee in Berlin. Der Chef des Ingenieurkorps, General Gustav von Rauch, riet ihm jedoch wegen der mehrjährigen Wartezeiten aufgrund großen Andrangs von Bewerbern, sich stattdessen bei der Artillerie zu bewerben, deren Avantageure dieselbe Schule wie die Ingenieure besuchten. Von vierzehn Kandidaten des Eintrittsexamens in Magdeburg wurde er als einer von vier aufgenommen.

Im Herbst 1835 wurde er als Offizieranwärter für drei Jahre an die Berliner Artillerie- und Ingenieurschule kommandiert. Hier bekam er eine umfassende Ausbildung auf naturwissenschaftlichen Gebieten – wie Mathematik, Physik, Chemie, Geometrie und Ballistik und hörte nebenher Vorlesungen an der Berliner Universität. Diese Ausbildung beendete er 1838 als Artillerie-Leutnant. Einer seiner Lehrer an der Artillerieschule war der Physiker Gustav Magnus, dem er später seine Dynamomaschine vorführte. Magnus erkannte die Bedeutung und sorgte dafür, dass die Arbeit veröffentlicht wurde, zuerst in Berlin und danach in London.

Nach dem Tod der Mutter im Juli 1839 und des Vaters im Januar 1840 musste Werner als ältester Sohn die Vaterstelle für seine jüngeren Geschwister übernehmen.

Leutnant Werner Siemens tat Dienst in Magdeburg in der 3. Artillerie-Brigade und anschließend in der Garnison Wittenberg, wo er wegen der Teilnahme als Sekundant bei einem Duell zu fünf Jahren Festungshaft verurteilt wurde. Seine Zelle in der Zitadelle Magdeburg konnte er als Versuchslabor einrichten und entwickelte dort ein Verfahren zur elektrischen Galvanisierung (insbesondere Versilberung und Vergoldung) in Weiterentwicklung der kurz zuvor durch Moritz Hermann von Jacobi entwickelten Kupfergalvanisierung.

Nach einer Begnadigung wurde Siemens 1842 zur Artilleriewerkstatt in Berlin versetzt. Im Schleswig-Holsteinischen Krieg unterstützte er 1848 die Kieler Bürgerwehr bei der Verteidigung des Kieler Hafens gegen dänische Seestreitkräfte mittels Besetzung der Festung Friedrichsort. Außerdem entwickelte er funktionsfähige ferngezündete Seeminen, die vor dem Kieler Hafen ausgelegt wurden und die dänische Marine darin hinderten, die Stadt aus der Nähe zu beschießen.

Er blieb beim Militär bis Juni 1849 und versuchte nebenher mit Erfindungen zusätzlich Geld zu verdienen, wobei seine Arbeit zunächst auf praktische und schnell verwertbare Dinge gerichtet war. So entwickelte er einen neuen Regler für Dampfmaschinen, eine Presse zur Herstellung von Kunststein und ein Druckverfahren. Die Idee einer Lauf-Flieg-Maschine, über die er mit seinem Bruder Wilhelm korrespondierte, wurde aber nicht in Angriff genommen.

Als aufstrebender Unternehmer heiratete er am 1. Oktober 1852 in Königsberg seine entfernte Nichte Mathilde Drumann (1824–1865), Tochter des Universitätsprofessors Wilhelm Drumann und seiner Cousine Sophie Mehliß. Aus dieser Ehe stammen die Söhne Arnold und Wilhelm sowie die Töchter Anna Zanders und Käthe Pietschker (1861–1949). Mathilde verstarb am 1. Juli 1865 an einer langjährigen Lungenerkrankung.

Am 13. Juli 1869 heiratete Werner Siemens in zweiter Ehe seine entfernte Nichte Antonie Siemens (1840–1900) aus Hohenheim bei Stuttgart, die Tochter von Carl Georg Siemens, der später in den württembergischen persönlichen Adelsstand erhoben wurde, und der Ottilie Denzel (1812–1882). Aus dieser Ehe gingen der Sohn Carl Friedrich und die Tochter Hertha (1870–1939; verheiratet mit Carl Dietrich Harries) hervor.

Am 17. Februar 1887 erwarb Siemens das etwa 600 Hektar große Gut Biesdorf mit einem großen Herrenhaus und übertrug es 1889 seinem Sohn Wilhelm. In seinem Ferienhaus in Harzburg schrieb Siemens von 1889 bis 1892 in den Sommerferien seine Lebenserinnerungen nieder, die kurz vor seinem Tod publiziert wurden.

Am 6. Dezember 1892 erlag Werner von Siemens in Berlin einer Lungenentzündung. Er wurde auf dem Alten Luisenfriedhof in Charlottenburg beigesetzt und später in die Familiengrabanlage der Familie Siemens auf dem südwestlich von Berlin gelegenen Südwestkirchhof Stahnsdorf umgebettet.

Im Jahr 1842 gelang es Werner Siemens, einen Teelöffel aus Neusilber mit Hilfe von Gleichstrom aus Batterien mit einem Überzug wahlweise aus Silber oder Gold zu versehen. Für dieses Verfahren bekam er ein Patent, das er an einen Juwelier verkaufte. Den Erlös aus diesem Geschäft schickte er seinem damals 18-jährigen Bruder Wilhelm nach England, das zu dieser Zeit in der Technik und Industrialisierung viel weiter fortgeschritten war als der in viele Teilstaaten zersplitterte Deutsche Bund.

Ende 1846 entwickelte er den elektrischen Zeigertelegrafen mit Selbstunterbrechung.
Im Jahr darauf erfand er ein Verfahren, um Drähte mit einer nahtlosen Umhüllung aus Guttapercha zu versehen. Dieses Verfahren bildet bis heute die Grundlage zur Herstellung isolierter Leitungen und elektrischer Kabel.

1857 entwickelte Siemens die Ozonröhre, die elektrisch erzeugtes Ozon zur Reinigung von Trinkwasser verwendet.

Ebenfalls 1857 formulierte er das Gegenstromprinzip.

Mit der Entwicklung des ersten elektrischen Generators (1866) auf der Grundlage des von ihm wissenschaftlich begründeten dynamoelektrischen Prinzips gehört Werner Siemens zu den Wegbereitern der Starkstromtechnik. Elektrische Energie, die jetzt in großem Umfang produziert werden konnte, ermöglichte die Verwendung des flexibel einzusetzenden Elektromotors, der gemeinsam mit den Verbrennungsmotoren die Dampfmaschine ablöste und die zweite industrielle Revolution einleitete.

Das dynamoelektrische Prinzip war bereits vom Dänen Søren Hjorth und ebenfalls vom Ungarn Ányos Jedlik entdeckt worden. Siemens war allerdings der erste, der die Tragweite der Entdeckung erkannte und den Siegeszug der elektrischen Energie voraussagte.

Am 12. Oktober 1847 gründete er – noch immer im Hauptberuf Offizier – mit dem Mechaniker Johann Georg Halske die "Telegraphen Bau-Anstalt von Siemens & Halske in Berlin". Das notwendige Kapital zur Firmengründung kam von Siemens’ Vetter Johann Georg Siemens, einem wohlhabenden Justizrat und Vater des späteren Mitbegründers der Deutschen Bank, Georg Siemens. Er investierte mehr als 6000 Taler als Startkapital gegen eine 20-prozentige Gewinnbeteiligung über sechs Jahre.

Die Verbindung von Siemens und Halske war wohl ein seltener Glücksfall in der Technikgeschichte, denn sie ergänzten sich auf nahezu ideale Weise. Siemens hatte das Wissen, die Ideen und experimentierte gerne, Halske konstruierte die vielen Kleinigkeiten, die notwendig waren, um aus Ideen praktisch nutzbare Geräte zu machen.

1848 erhielt das junge Unternehmen einen politisch wichtigen Auftrag: die Telegraphenleitung von Berlin nach Frankfurt am Main, denn dort tagte die deutsche Nationalversammlung. Die Leitung wurde noch im Winter 1848/49 mit Geräten und Kabeln von Siemens & Halske gebaut. Dass die Nationalversammlung König Friedrich Wilhelm IV. von Preußen die Kaiserwürde antragen wollte, wusste dieser schon eine Stunde nach der Abstimmung, eine Woche, bevor die Kaiserdeputation in Berlin ankam.

Damit wurde Siemens & Halske auf einen Schlag bekannt und weitere Aufträge zum Bau von Telegraphenverbindungen in Preußen und den deutschen Staaten folgten. Siemens versuchte früh auch auf außerdeutschen Märkten Fuß zu fassen, zumal er mit der preußischen Telegraphenverwaltung bald in Streit geriet und von dieser über viele Jahre keine Aufträge mehr erhielt. Er betraute seinen Bruder Wilhelm mit der Leitung einer ersten Auslandsniederlassung in London. Auch in Russland bemühte er sich um Aufträge. Ein erster Erfolg war 1852 der Auftrag zur Errichtung von Telegraphenverbindungen von Warschau nach St. Petersburg und von St. Petersburg nach Moskau. 1853 schickte Siemens seinen Bruder Carl nach St. Petersburg, um den Bau zu überwachen. Dabei bewährte sich Carl schnell als fähiger Unternehmer und weitere Aufträge für das russische Telegraphennetz folgten. 1855 wurde das russische Geschäft unter Leitung Carls in eine Zweigniederlassung umgewandelt und etablierte sich als wichtige Stütze von Siemens & Halske. Aufträge kamen auch aus England, wo eine eigene Kabelfabrik errichtet wurde.

Es gab auch Rückschläge, beispielsweise scheiterte 1864 die Verlegung eines Seekabels durch das Mittelmeer von Cartagena (Spanien) nach Oran (heute Algerien, damals französische Kolonie), was dem Unternehmen empfindliche Verluste bescherte. Halske, der risikoreiche Unternehmungen hasste, verlangte, sich von der verlustreichen Niederlassung in London zu trennen. Siemens wollte den Bruder nicht im Stich lassen, gliederte die Londoner Niederlassung aus Siemens & Halske aus und gründete 1865 mit Wilhelm und Carl in London die "Siemens Brothers & Co". Aber die Meinungsverschiedenheiten zwischen Halske und den Siemens-Brüdern blieben bestehen und führten Ende 1867 nach zwanzig Jahren zum Rückzug von Halske aus der Firma. Die Brüder Wilhelm und Carl wurden nach dem Ausscheiden Halskes die einzigen Teilhaber ihres Bruders Werner: Siemens & Halske wurde zum Familienunternehmen der Siemens-Brüder. Werner und Carl hatten außerdem, auf Vorschlag ihres mit dem Bau der Telegraphenleitungen im Kaukasus beschäftigten Bruders Walter, 1864 auch ein Kupferbergwerk in Kedabeg im russischen Gouvernement Elisabethpol (heute Aserbaidschan) gekauft, das – unter Überwindung mancher Schwierigkeiten – als von der Firma getrenntes „Privatgeschäft“ unter Leitung der Brüder Walter und Otto betrieben wurde.
1870 ging nach dreijähriger Bauzeit die "Indo-Europäische Telegraphenlinie" von London über Teheran nach Kalkutta mit einer Länge von über 11.000 Kilometern in Betrieb.


Siemens unterstützte die Deutsche Revolution 1848/49. Im Jahr 1860 wurde er Mitglied des liberalen "Deutschen Nationalvereins", war 1861 Mitbegründer der "Deutschen Fortschrittspartei" (DFP) und wurde 1863 in das Preußische Abgeordnetenhaus gewählt, dem er bis 1866 angehörte. Im Preußischen Verfassungskonflikt stimmte er gegen die Indemnitätsvorlage Otto von Bismarcks.

Siemens machte sich schon früh Gedanken um das Schicksal seiner Mitarbeiter. Die normale Entlohnung erschien ihm nicht ausreichend: „Mir würde das Geld wie glühendes Eisen in der Hand brennen, wenn ich den treuen Gehilfen nicht den erwarteten Anteil gäbe“. Neben altruistischen Motiven veranlassten ihn auch firmentaktische Beweggründe zu einem solchen Vorgehen, wie er in einem Brief an seinen Bruder Carl schrieb: „Es wäre auch nicht klug von uns, sie leer ausgehen zu lassen im Augenblicke großer neuer Unternehmungen.“

Leitende Mitarbeiter hatten schon seit Mitte der 1850er-Jahre Verträge, die ihnen erfolgsabhängige Tantiemen zusicherten, rangniedrigere Mitarbeiter bekamen – nicht vertraglich festgelegte – Prämien. Ab Mitte der 1860er-Jahre zahlte Siemens & Halske eine so genannte Inventurprämie an alle Arbeiter und Angestellten, eine frühe Form des Leistungsanreizes und ein Vorläufer der heutigen Erfolgsbeteiligung. Dies alles waren Maßnahmen, um qualifizierte Mitarbeiter an Siemens & Halske zu binden und einen festen Arbeiterstamm zu bilden.

1872 gründete Siemens die Pensions-, Witwen- und Waisenkasse, an der sich auch Halske, der dem Unternehmen schon nicht mehr angehörte, beteiligte. Eine weitere sozialpolitische Maßnahme war die 1873 erfolgte Einführung einer täglichen Arbeitszeit von neun Stunden, was bei der damaligen Sechstagewoche einer Wochenarbeitszeit von 54 Stunden entsprach. Üblich waren zu der Zeit noch 72 Wochenstunden.

Nach der Reichsgründung 1871 wurde kontrovers über einen einheitlichen Patentschutz im Deutschen Reich diskutiert. Patente wurden im Königreich Preußen nach Ermessen der Beamten höchstens auf drei Jahre erteilt und mussten in jedem Staat des Deutschen Zollvereins einzeln beantragt werden. Bereits 1864 hatten der preußische Handelsminister und in der Folge zahlreiche Handelskammern sogar die Abschaffung dieser Patente gefordert, weil sie „schädlich für den allgemeinen Wohlstand“ seien. Dies hatte Werner Siemens dazu bewogen, 1863 an die Berliner Handelskammer ein Gutachten zu richten, das „die Notwendigkeit und Nützlichkeit eines Patentgesetzes zur Hebung der Industrie“ sowie die Grundzüge für ein solches darlegte. Infolgedessen wurde von der Abschaffung Abstand genommen. Um die Sache weiter voranzubringen, rief er einen Patentschutzverein ins Leben, der unter seinem Vorsitz den Entwurf für ein deutsches Patentgesetz ausarbeitete. Doch erst als er sich nach der Reichsgründung persönlich an Reichskanzler v. Bismarck wandte, leitete dieser ein Gesetzgebungsverfahren ein. Siemens hatte darauf hingewiesen, dass deutsche Produkte bisher in aller Welt als „billig und schlecht“ galten und deutsche Erfinder ihre Patente ins Ausland nahmen und dort produzieren ließen. Deswegen diene ein Patentgesetz auch dazu, die deutsche Industrie zu stärken und ihr mehr Ansehen in der Welt zu verschaffen. Am 25. Mai 1877 trat das Deutsche Patentgesetz in Kraft. Der Entwurf war nur leicht modifiziert vom Reichstag angenommen worden. Seine Grundzüge gelten bis heute. 

Mit Heinrich von Stephan gründete er 1879 den "Elektrotechnischen Verein", anlässlich dessen Namensgebung er das Wort "Elektrotechnik" prägte. Als dessen erster Präsident setzte er sich für die Errichtung von Lehrstühlen der Elektrotechnik an Technischen Hochschulen im ganzen Deutschen Reich ein.

1879 kaufte Werner Siemens das zweite jemals gefundene Fossil des Archaeopteryx von dem Solnhofener Apotheker Ernst Häberlein für 20.000 Mark und verhinderte so, dass auch das zweite Fossil ins Ausland verkauft wurde. Er überließ den Urvogel der Universität Berlin als Dauerleihgabe, so dass diese das Fossil zwei Jahre später in zwei Raten zum ursprünglichen Preis von Siemens erwerben konnte.

1885 ermöglichte Siemens die Gründung der seit längerem von Wissenschaftlern geplanten Physikalisch-Technischen Reichsanstalt, indem er neben dem Charlottenburger Polytechnikum ein Areal hierfür erwarb und stiftete. Im dortigen Werner-von-Siemens-Bau sowie dem nach dem Gründungspräsidenten benannten Hermann-von-Helmholtz-Bau unterhält die Physikalisch-Technische Bundesanstalt bis heute einen ihrer Standorte.

1860 wurde Werner Siemens von der Universität Berlin die Würde eines Ehrendoktors verliehen. Auf der Weltausstellung in Paris 1867, wo Siemens seinen nach dem dynamoelektrischen Prinzip arbeitenden Generator ausstellte, wurde er mit dem Orden der französischen Ehrenlegion ausgezeichnet. In Anerkennung seiner Leistungen wurde Werner Siemens 1874 als Mitglied in die Preußische Akademie der Wissenschaften aufgenommen, in der er regelmäßig Vorträge zu allgemein naturwissenschaftlichen Themen hielt und publizierte. Er war Mitglied des Ältestenkollegiums der Berliner Kaufmannschaft, eine Ernennung zum Kommerzienrat lehnte er jedoch ab, da er sich „mehr als Gelehrten und Techniker wie als Kaufmann betrachtete und fühlte“. 1880 wurde er (als nicht ständiges Mitglied des Patentamtes) zum Geheimen Regierungsrat ernannt und am 18. Januar 1886 wurde ihm der Orden Pour le Mérite für Kunst und Wissenschaften verliehen. Außerdem war er Mitglied der Berliner Gesellschaft für Anthropologie, Ethnologie und Urgeschichte. Im Jahr 1887 wurde er zum Mitglied der Leopoldina gewählt.

In Anerkennung seiner Verdienste um Wissenschaft und Gesellschaft wurde Siemens durch Kaiser Friedrich III. am 5. Mai 1888 in den Adelsstand erhoben (Nobilitierung). Die SI-Einheit des elektrischen Leitwerts ist nach ihm benannt. Zu seinen Lebzeiten wurde jedoch ein bestimmter elektrischer Widerstand als „ein Siemens“ oder „Siemens-Einheit“ (SE) bezeichnet, nämlich der Widerstand einer Quecksilbersäule bestimmter Abmessungen bei 0 °C; dieses Widerstands-Normal hatte Siemens entwickelt. 1 SE = 0,944 Ohm.

(Angaben aus: "Siemens-Mitteilungen" Nr. 145, 12. Oktober 1933)








</doc>
<doc id="13318" url="https://de.wikipedia.org/wiki?curid=13318" title="Gaius Marius">
Gaius Marius

Gaius Marius (* 158/157 v. Chr. in Cereatae nahe Arpinum; † 13. Januar 86 v. Chr. in Rom) war ein römischer Feldherr, Staatsmann und ging als dritter "homo novus" in die Geschichte Roms ein. Während seiner Laufbahn bekleidete er insgesamt siebenmal – und damit so oft wie kein Mann vor ihm – das Konsulat. Er triumphierte in zwei großen militärischen Auseinandersetzungen, nämlich erstens gegen den König Iugurtha von Numidien und zweitens gegen die Kimbern, Teutonen und Ambronen im germanisch-gallischen Wanderzug. Für diesen Sieg wurde er als Retter Roms, Vater des Vaterlandes sowie dritter Gründer Roms nach dem vergöttlichten Stadterbauer Romulus und dem legendären Gallierbezwinger Marcus Furius Camillus verehrt. In Opposition zu der ihn ablehnenden optimatischen Senatsnobilität unterstützte er politisch häufig die Bewegung der Popularen. Marius war mit Iulia, aus dem Geschlecht der Iulii Caesares, vermählt und dadurch der angeheiratete Onkel des späteren Eroberers von Gallien sowie Diktators auf Lebenszeit, Gaius Iulius Caesar. Der Verbindung entsprang der gleichnamige Sohn, Gaius Marius der Jüngere.

Gaius Marius wurde im kleinen Dorf Cereatae nahe dem Städtchen Arpinum geboren. Sein genaues Geburtsjahr ist unbekannt und kann lediglich aus Rückdatierungen seines Todeszeitpunktes vage vermutet werden. In der älteren Forschung wurde – einer Angabe Plutarchs folgend – das Jahr 156 v. Chr. angenommen. Dies gilt mittlerweile jedoch als wenig wahrscheinlich und ist – eine andere Aussage Plutarchs sowie eine Stelle von Velleius Paterculus zugrundelegend – auf das Jahr 157 bzw. 158 v. Chr. verschoben worden.

Marius’ gleichnamiger Vater sowie seine Mutter Fulcinia entstammten beide dem lokalen Ritterstand. Die Familie der Marii zählte als Teil des ordo equester zum regionalen Landadel, also einer durchaus begüterten Schicht, war jedoch in der stadtrömischen Ämterlaufbahn bislang überhaupt nicht in Erscheinung getreten und damit kein Mitglied der senatorischen Nobilität, der politischen Führungselite der Römischen Republik. Ihre Lebenswirklichkeit lässt sich am besten mit der eines ländlichen Gutsbesitzers vergleichen. In diesem Umfeld verbrachte Marius seine Kindheit und Jugend.

Marius’ öffentliche Laufbahn begann mit dem Kriegsdienst. Belegt ist der Dienst in den Jahren 134–133 v. Chr. als Ritter unter Publius Cornelius Scipio Aemilianus Africanus bei der Belagerung von Numantia. Dabei zeichnete sich Marius vielfältig aus und erlangte die Anerkennung und den Respekt seines Feldherrn. Zum Abschluss seines etwa 10-jährigen Armeedienstes erlangte er – wahrscheinlich irgendwann zwischen 131 und 129 v. Chr. – die Wahl zum Militärtribun.

Im Anschluss daran kehrte Marius in die Heimat zurück und bereitete sich dort auf den Eintritt in die politische Ämterlaufbahn Roms, den so genannten cursus honorum, vor. Diese begann er – wobei sowohl der Zeitpunkt als auch die Tatsache an sich umstritten sind – mit der Wahl zum Quästor. Gesichert ist dann allerdings seine erfolgreiche Kandidatur zum Volkstribun für das Jahr 119 v. Chr., wohl mit Unterstützung, aber nicht als Klient der mächtigen "gens" der Caecilii Metelli. Als Tribun erwirkte er zum einen durch energisches Auftreten gegenüber Senat und Konsuln eine Reform der Abstimmungsregeln, die nach ihm benannte "lex Maria de suffragiis", stoppte zum anderen mittels seines Vetos aber auch ein populares Getreidegesetz. Dies brachte ihm den Ruf eines genauso unnachgiebigen wie unabhängigen Politikers ein, verstärkte aber insbesondere in der Nobilität die gegen ihn als "homo novus" bestehenden Vorurteile. Wohl infolge optimatischer Opposition verfehlte er daher auch – wahrscheinlich im Jahr 117 v. Chr. – zunächst die Wahl zum kurulischen und kurze Zeit später auch die zum plebeiischen Ädil. Im Folgenden jedoch gelang ihm die erfolgreiche Kandidatur zum Prätor für das Jahr 115 v. Chr. in Rom. 114–113 v. Chr. verwaltete er schließlich – als Statthalter – wohl im Range eines Proprätors – die römische Provinz Hispania ulterior, die er von Räuberbanden befreite. 

Nach seiner Rückkehr in die Hauptstadt heiratete Marius irgendwann zwischen 113 und 110 v. Chr. die Patrizierin Iulia aus dem Haus der Iulii Caesares, eine Tante des Gaius Iulius Caesar, die ihm 110 oder 109 v. Chr. den gleichnamigen Sohn Gaius Marius den Jüngeren gebar.

Ab 109 v. Chr. war Marius zunächst Legat im Jugurthinischen Krieg unter dem (Pro-)Konsul Quintus Caecilius Metellus Numidicus. Es gelang ihm, gegen den Widerstand der Nobilität für das Jahr 107 v. Chr. als "homo novus" zum Konsul gewählt zu werden und den Oberbefehl im Krieg gegen den numidischen König Jugurtha zu übernehmen. Der Krieg war bis zu diesem Zeitpunkt für Rom nicht effektiv verlaufen, jedoch ohne größere Niederlagen. Marius eroberte zunächst die Stadt Capsa und schlug Jugurtha dann bei Cirta. Mit Hilfe seines damaligen Quästors Lucius Cornelius Sulla konnte er den Krieg schließlich siegreich beenden: Sulla handelte die Auslieferung Jugurthas von dessen Schwiegervater Bocchus von Mauretanien aus, bei dem der König Zuflucht gefunden hatte.

Seine wichtigsten Siege errang Marius über drei Germanenstämme Teutonen, Ambronen und der Kimbern, die Rom nach zwei großen Niederlagen bei Noreia 113 v. Chr. und Arausio 105 v. Chr. in Angst und Schrecken versetzt hatten. Die Kimbern und Teutonen hatten sich mit den Ambronen und Haruden verbündet, die aus Jütland und den norddeutschen Tiefebenen stammten. Sulla waltete nun als Legat und Militärtribun, später wurde er Quintus Lutatius Catulus (Konsul 102 v. Chr.) zugeteilt. 102 v. Chr. vernichtete Marius die Teutonen und Ambronen bei Aquae Sextiae in Südgallien (heute Aix-en-Provence) und 101 v. Chr. die Kimbern bei Vercellae in Norditalien (heute Vercelli). An der Schlacht von Vercellae nahm auch Catulus mit seinem Heer teil; beide Feldherren beanspruchten den Sieg für sich. Während dieses Kriegs wurde Marius unter Missachtung des traditionellen Iterationsverbots für jedes Jahr von 104 bis 100 v. Chr. zum Konsul gewählt.

Laut Plutarch soll Gaius Marius als Konsul verfügt haben, dass der Adler, das Symbol des Obergottes Jupiter, auch als Symbol für den Senat und das Volk von Rom stehen solle.

"Siehe auch:" Heeresreform des Marius

Als mitentscheidend für die großen Erfolge von Marius erwies sich die von ihm durchgeführte Heeresreform, die den Übergang von einer Milizarmee zu einer Berufsarmee markierte. Der Berufssoldat diente 16 Jahre oder für 16 Feldzüge seinem Feldherrn. Als Entlohnung erhielt er einen Sold und Anteile an der Beute. Der Feldherr musste für die Veteranenversorgung aufkommen, die bis Caesar die Form einer Landschenkung hatte und erst im kaiserzeitlichen Rom durch Geldzahlungen abgelöst wurde. Somit wuchs die Macht der Feldherren, da die Soldaten sich ihnen mehr verpflichtet fühlten als der Republik. Fortan bestand eine Legion aus zehn Kohorten zu je 500–600 Mann und nicht mehr aus den bisherigen Manipeln, die Ausbildung wurde gestrafft und die Soldaten erhielten eine Standardbewaffnung, zu der auch das Pilum gehörte.

Titus Livius legt jedoch dar, dass die Manipulartaktik auch nach den Reformen, z. B. in Spanien, noch in Gebrauch blieb, da diese auf diesen spezifischen Kriegsschauplätzen der Kohortentaktik weiterhin überlegen war. Der Tross wurde stark verringert, und die Soldaten mussten ihre Ausrüstung selber tragen, weshalb sie auch "muli Mariani" („Maultiere des Marius“) genannt wurden. Zudem öffnete Marius die Ränge der Armee für die "capite censi", den Stand der Besitzlosen, die bis dahin vom Militärdienst ausgeschlossen gewesen waren. Möglich war dies, da nun der Staat die Ausrüstung der Soldaten stellte.

Durch die Reform wurde das römische Heer schlagkräftiger und professioneller, konnte von ambitionierten Feldherren nun jedoch leichter zum Zugewinn an politischer Macht in Rom benutzt werden. Ebenso wurden die Legionäre durch die Einführung eines Legionsadlers mental stärker an den Feldherrn gebunden und motiviert, für ihn zu kämpfen. Es entstand das Problem der sogenannten Heeresklientel: Die Heerführer hatten nun die Aufgabe, nach dem Krieg die Versorgung ihrer besitzlosen Veteranen mit Land politisch durchzusetzen. Dadurch wurden die Heerführer zu Patronen ihrer Soldaten, die ihrerseits deren eingeschworene Klienten wurden. Folglich kam den Heerführern eine übergroße politische Macht zu, weswegen sich der Senat zum Beispiel im Fall des Gnaeus Pompeius Magnus gegen eine solche Landverteilung sperrte.

Auf dem Höhepunkt seiner Macht zog sich der hoch dekorierte Marius im Jahre 100 v. Chr. nach seinem sechsten Konsulat aus der aktiven Politik zurück, weil er eine innenpolitische Niederlage im Senat hinnehmen musste. Die von der Volkspartei angestrebten Verbesserungen für die einfache Bevölkerung wurden mit den Stimmen der Optimaten abgelehnt. In der Folge kam es zu Ausschreitungen in Rom, woraufhin Marius den Ausnahmezustand gegen seinen politischen Freund, den Volkstribun Lucius Appuleius Saturninus, verhängen musste. Im Bundesgenossenkrieg 91 bis 88 v. Chr. übernahm Marius 90 v. Chr. nach dem Tod des Konsuls Publius Rutilius Lupus wieder ein Kommando und ging erfolgreich gegen die Marser vor. Sein Imperium wurde allerdings für das folgende Jahr nicht verlängert. Der Krieg wurde schließlich durch die Verleihung des römischen Bürgerrechts an die italischen Verbündeten beendet.

Im Jahre 88 v. Chr. beauftragte der Senat Sulla, der als einer der beiden Konsuln des Jahres amtierte, mit der Kriegsführung gegen Mithridates VI., der zuvor in einem umfassenden Vormarsch ganz Kleinasien erobert und in der als Vesper von Ephesos bekannten Mordaktion tausende römische Bürger und Italiker hatte ermorden lassen. Marius erwirkte jedoch mittels des mit ihm verbündeten Volkstribuns Publius Sulpicius Rufus die Übertragung dieses Kommandos durch die "comitia centuriata" auf ihn. Sulla erkannte dieses Vorgehen allerdings nicht an und marschierte mit seinen vor Nola stehenden Legionen auf Rom. Die Stadt fiel in die Hand Sullas, der sich das Kommando wieder zurückübertragen und seine innenpolitischen Gegner zu Staatsfeinden erklären ließ. Marius flüchtete aus Rom und Italien bis nach Nordafrika, wo er erst auf der Insel Cercina, dem heutigen Kerkenna, im Kreise ehemaliger Veteranen sichere Zuflucht fand. 

Im Jahr 87 v. Chr. kehrte Marius nach Italien zurück, wo nach Sullas Abmarsch Richtung Kleinasien der innenpolitische Machtkampf zwischen Popularen und Optimaten erneut ausgebrochen und im Zuge dessen der populare Konsul Lucius Cornelius Cinna mit seinen Anhängern aus Rom vertrieben worden war. Marius schloss sich den Vertriebenen an und erlangte gemeinsam mit ihnen die Kontrolle über Rom. Die Führer der optimatischen Gegenseite wurden liquidiert oder begingen Selbstmord. Außerdem kam es zur Aufhebung der Ächtung, und Marius sowie Cinna erhielten für das Jahr 86 v. Chr. die Konsulwürde. Ob dies durch Wahl oder Selbsternennung geschah, ist unklar und umstritten. 

Marius trat turnusgemäß zum 1. Januar sein Amt an, verstarb aber bereits knapp zwei Wochen später am 13. Januar des Jahres 86 v. Chr. wohl an einer Pleuritis. Seine sterblichen Überreste wurden nahe dem Fluss Anio beigesetzt. Sulla ließ sie jedoch einige Jahre später nach seinem Sieg im Bürgerkrieg exhumieren und in den Fluss werfen, um das Andenken an seinen Feind für alle Zeit zu verdunkeln.






</doc>
<doc id="13321" url="https://de.wikipedia.org/wiki?curid=13321" title="Gladius (Waffe)">
Gladius (Waffe)

Der Gladius (Mehrzahl: Gladii) ist ein römisches Schwert. Er soll im späteren 3. Jahrhundert v. Chr. aus einem Schwerttyp der iberischen Keltiberer entwickelt worden sein und war in Variationen bis in das 3. Jahrhundert n. Chr. die Standardwaffe der Infanterie der römischen Armee. Spezifiziert zwischen Gladius und "Spatha" wurde im römischen Reich nicht, da Gladius einfach nur Schwert bedeutet und dieser Terminus nicht exklusiv für das Kurzschwert der kaiserzeitlichen Legionäre benutzt wurde.

Die eiserne Klinge eines kaiserzeitlichen Gladius ist etwa 50–56 cm lang, ca. 8 cm breit und beidseitig geschliffen. Man unterscheidet hauptsächlich den „Typ Mainz“, den „Typ Hispanicum“ und einen späteren „Typ Pompeji“, der seit dem 1. Jahrhundert in Gebrauch war. Weitere Varianten, wenn auch ohne namensgebenden Fundort, sind bekannt. Bis zur Zeit des Kaisers Augustus war der Gladius hingegen kein Kurzschwert, sondern etwa 70 cm lang. 

Beim „Typ Mainz“ verjüngt sich die Klinge zunächst, um vor der Spitze wieder breiter zu werden, das Gewicht beträgt zwischen 1200 und 1600 Gramm. Beim „Typ Pompeji“ verläuft die Klinge gerade, was fertigungstechnisch einfacher ist, das Gewicht beträgt ca. 1000 Gramm. Bei diesem Schwert handelt es sich wahrscheinlich um eine vereinfachte Form, die billiger herzustellen, aber dem traditionellen Gladius an Durchschlagskraft deutlich unterlegen war und womöglich vor allem an Auxiliarsoldaten ausgegeben wurde. Der „Typ Hispanicum“ schließlich wird erst breiter, wobei die Kante an sich aber gerade ist. Am Ende ist eine Spitze wie beim „Typ Pompeji“ und beim „Typ Mainz“. Begehrt waren Waffen aus "ferrum noricum", dem norischen Stahl. Durch die besondere Härte und Schnitthaltigkeit waren aus diesem Material gefertigte Gladii zeitgenössischen Schwertern weit überlegen.

Der Gladius wurde von den Mannschaften auf der rechten Seite getragen, dies erforderte mehr Übung beim Ziehen des Schwertes, es bestand aber keine Kollisionsgefahr mit dem schweren Schild der Mannschaften. Centurionen trugen den Gladius meist auf der linken Seite. Der Griff war zwar mit einem Schutz versehen, doch sollte dieser nicht wie eine Parierstange wirken, sondern nur verhindern, dass die Schwerthand auf die Klinge rutscht, wenn mit dem Gladius ein kräftiger Stich ausgeführt wird. Die Scheide bestand aus lederbezogenem Holz mit Metallbeschlägen.

Seit der Zeit der Severer (193 bis 235) wurde der kurze Gladius langsam durch ein Langschwert, die "Spatha", abgelöst. Diese Entwicklung beschleunigte sich unter Diokletian (284 bis 305) und fand im frühen 4. Jahrhundert ihren Abschluss.

Der Gladius war eine geeignete Waffe für den Nahkampf in engen Infanterieformationen, wie die Römer sie während Republik und Prinzipat verwendeten. Im dichten Kampfgetümmel der Infanterie, die sowohl durch ihre enge Geschlossenheit als auch den Massendruck der nachdrängenden Glieder wirkte und nach vorn durch die großen Schilde ("Scuta") geschützt war, wirkte sich die geringe Länge des Schwertes positiv aus und verschaffte dem Legionär trotz der drangvollen Enge einen Vorteil. Er konnte auch im dichtesten Kampfgewühl seine Waffe noch gebrauchen (vor allem stechend), ohne seine Deckung fallen zu lassen, während Besitzer längerer Schwerter diese unter diesen beengten Bedingungen kaum effektiv einsetzen konnten. Meist zielte man mit dem Gladius auf den Unterleib des Gegners. Die Waffe schlug furchtbare Wunden. Der makedonische König Philipp V. soll tief erschrocken gewesen sein, als er erstmals die Leichen von Männern sah, die mit Gladii getötet worden waren (Liv. 31,34,1-5).

Der Gladius eignete sich sowohl zum Hieb als auch zum Stich: Die blattförmige Klinge des Mainz-Typus besaß ein beträchtliches Gewicht und richtete bei ungeschützten Gegnern verheerenden Schaden an. Funde von Opfern römischer Legionäre, die bei der Erstürmung von Maiden Castle in Britannien gefallen waren, illustrieren das ziemlich deutlich – mehrere aufgefundene Schädel sind durch Gladiushiebe sichtlich zugerichtet. Diese Kampftechnik trug wesentlich zur Überlegenheit der römischen Legionen in großen regulären Gefechten bei; allerdings war das kurze Schwert im Einzelnahkampf außerhalb der geschlossenen Formation weniger vorteilhaft, was wohl ein Grund dafür ist, warum die längere Spatha (die schon zuvor von der römischen Reiterei benutzt worden war) im Verlauf des 3. Jahrhunderts an Bedeutung gewann, als traditionelle Feldschlachten selten geworden waren und ein Kleinkrieg an den Grenzen an Bedeutung zunahm. Die Spatha wurde im Unterschied zum Gladius in der Regel links getragen.




</doc>
<doc id="13322" url="https://de.wikipedia.org/wiki?curid=13322" title="Diktator">
Diktator

Diktator steht für:

Siehe auch:



</doc>
<doc id="13323" url="https://de.wikipedia.org/wiki?curid=13323" title="Lufthansa">
Lufthansa

Die Deutsche Lufthansa AG mit Sitz in Köln ist ein deutscher Luftfahrtkonzern und gemessen an der Anzahl der beförderten Passagiere das größte Luftverkehrsunternehmen Europas. Sie ist Initiator und Gründungsmitglied der Luftfahrtallianz Star Alliance und im DAX gelistet.

Der Konzern entwickelte sich Mitte der 1990er Jahre aus der staatlichen Linienfluggesellschaft "Lufthansa" in die Aktiengesellschaft "Deutsche Lufthansa AG", zu der unter anderem Eurowings, Swiss und Austrian Airlines gehören. Die Fluggesellschaft Lufthansa mit Basis auf dem Flughafen Frankfurt am Main wird heute intern als „Lufthansa Passage Airlines“ bezeichnet und ist Deutschlands größte Fluggesellschaft. Seit der Insolvenz von Air Berlin hat die Airline auf innerdeutschen Routen praktisch ein Monopol.

Das Luftfrachtgeschäft wurde 1994 in die neu gegründete Tochtergesellschaft Lufthansa Cargo überführt, die Luftfahrzeug-Instandhaltung 1995 an die Lufthansa Technik. Das Catering wird von der konzerneigenen LSG Sky Chefs betrieben, mit Lufthansa Flight Training betreibt das Unternehmen Schulungszentren für Piloten und Flugbegleiter.

Die Geschichte der Lufthansa wird rechtlich unzutreffend gemeinhin als Entwicklung von der anfänglichen Linienfluggesellschaft 1926 in Berlin bis zum heutigen Großkonzern dargestellt. Da es sich bei der heutigen „Deutschen Lufthansa AG“ nicht um einen Rechtsnachfolger der vormaligen Deutschen Lufthansa AG handelt, sind zwei Zeiträume zu betrachten:

Erst 1954 wurde die LUFTAG im Anschluss an den Erwerb der Rechte am traditionsreichen Firmennamen "Lufthansa" in "Deutsche Lufthansa AG" umbenannt. Die Deutsche Lufthansa AG stand in keiner Beziehung mit der am 1. Juli 1955 gegründeten Deutschen Lufthansa in der DDR.

Ebenfalls im rechtlichen Sinne unzutreffend wird der erste Linienflug der „neuen“ Deutschen Lufthansa AG am 1. April 1955 gemeinhin als „Neubeginn“ der unter dem Markennamen „Lufthansa“ operierenden Fluggesellschaft angesehen.

Die „neue“ Deutsche Lufthansa AG war bis 1963 zu fast 100 Prozent in staatlichem Besitz. Bis 1994 war „die Lufthansa“ der offizielle Flagcarrier der Bundesrepublik Deutschland; seit 1997 ist Lufthansa vollständig privatisiert. Die offizielle Bezeichnung „Deutsche Lufthansa AG“ umfasst seitdem den gesamten Luftfahrt-Konzern, wobei die Passagierbeförderung im Linienflugbetrieb weiterhin das Kerngeschäft bleibt (intern „Lufthansa Classic“). Zu diesem Geschäftsfeld "Passage" gehören im Konzern neben der Fluggesellschaft Lufthansa weitere Fluggesellschaften, darunter Germanwings, Swiss und die Austrian Airlines Group.

Nach dem Unfall von Germanwings-Flug 9525 sagte die Lufthansa eine für den 15. April 2015 geplante Jubiläumsfeier zum 60-jährigen Flugbetrieb ab.

In den Jahren 2014 und 2015 kam es zu insgesamt 14 Streiks, dennoch erhöhte sich der Gewinn vor Steuern 2015 um 55 % auf 1,8 Milliarden Euro.

Der Name „Lufthansa“, anfangs noch "Luft Hansa", wurde laut Unternehmen 1924 bei der Feier im Dresdner Rathaus anlässlich der Einweihung der Strecke Dresden–München durch die Vorgängerfluggesellschaft Junkers Flugverkehr vom Flugplatz Dresden-Kaditz aus für eine geplante, neue deutsche Fluggesellschaft erstmals gebraucht.

Letztere entstand schließlich am 6. April 1926 aus dem Zusammenschluss der beiden Fluggesellschaften Deutscher Aero Lloyd AG und Junkers Luftverkehrs AG zur "Deutschen Luft Hansa Aktiengesellschaft" mit Sitz in Berlin. 1933 wurde die Schreibweise "Lufthansa" in einem Wort etabliert. Der Unternehmensname ist eine Referenz an die Hanse, dem Zusammenschluss niederdeutscher Kaufleute im Mittelalter. Diese agierte einst erfolgreich wirtschaftlich, militärisch und politisch im Nord- und Ostseeraum. Sie brachte der Region internationales Ansehen, Frieden und Wohlstand. Das althochdeutsche Wort „Hansa“ bedeutet dabei 'Schar' im Sinne einer Gruppe.

Wegen des Firmenlogos wird die Lufthansa in der Öffentlichkeit häufig als "Kranich-Airline" bezeichnet.

1966 wurde die Lufthansa-Aktie erstmals an der Börse gehandelt und erreichte das Interesse von Privatanlegern. Der Staat behielt zunächst die Mehrheit. In den 1990er Jahren reduzierte die öffentliche Hand ihren Anteil von über 50 Prozent auf 34 Prozent. Großaktionär ist gegenwärtig die Templeton Global Advisors Ltd. mit rund 10 Prozent.

Der Konzern „Deutsche Lufthansa AG“ ist im DAX an der Börse Frankfurt notiert. Im Jahr 2009 betrug die Marktkapitalisierung der Lufthansa 5,637 Milliarden Euro, eingeteilt in 457,9 Millionen Stückaktien. Diese sind gemäß dem Luftverkehrsnachweissicherungsgesetz, welches seit September 1997 bei der Lufthansa Anwendung findet, als vinkulierte Namensaktien ausgegeben, wodurch die Kontrolle des Aktionärskreises ermöglicht wird. Aufgrund von Bemühungen um Nachhaltigkeit wurde sie in den Dow Jones Sustainability Index aufgenommen, aus dem sie 2010 wieder ausgeschlossen wurde.

Anlässlich der Registrierung der Teilnehmer an der Hauptversammlung im April 2007 wurde festgestellt, dass 45,75 Prozent der Aktionäre als natürliche Personen keine EU-Staatsbürger sind oder als juristische Personen ihren Sitz außerhalb der EU haben. Da ihr Anteil gemäß der staatlichen Betriebsgenehmigung nicht mehr als 50 Prozent des Gesellschaftskapitals betragen darf und die internationalen, außereuropäischen Luftverkehrsrechte ebenfalls unter dieser Bedingung gewährt wurden, ist der Vorstand der AG gehalten, darauf zu achten, dass die 50-Prozent-Marke nicht überschritten wird.

Stand: 21. Dezember 2017

Der Konzernvorstand steuert mit den vier Ressorts Vorstandsvorsitz, Vorstand Lufthansa Passage, Finanzen sowie Verbund-Airlines und Konzern-Personalpolitik die gesamte Unternehmensgruppe. Vorstandsvorsitzender ist seit dem 1. Mai 2014 Carsten Spohr, er folgte auf Christoph Franz. Vorsitzender des Aufsichtsrats ist seit September 2017 Karl-Ludwig Kley. Unternehmenssitz und Hauptverwaltung befinden sich in Köln, die Geschäfte werden jedoch im 2005 eröffneten Lufthansa Aviation Center am Flughafen Frankfurt am Main geleitet. Dort sind alle dem operativen Geschäft nahen Abteilungen untergebracht. In der Hauptverwaltung in Köln, deren Neubau 2007 in Köln-Deutz eröffnet wurde, sind rund 800 Mitarbeiter aus dem Ressort Finanzen untergebracht. Die übrigen Abteilungen wurden bereits in den 1980er Jahren, teilweise gegen den Widerstand der Mitarbeiter, von Köln nach Frankfurt verlegt.

Im Jahr 2014 beschäftigte der Konzern im Durchschnitt 118.973 Mitarbeiter. In der Vergangenheit waren es 117.521 (31. Dezember 2009) 107.800 (31. Dezember 2008) und 105.261 (31. Dezember 2007) Mitarbeiter mit 155 Nationalitäten (in Deutschland 64.434 Mitarbeiter mit 126 Nationalitäten). Die Lufthansa Group gehört zu den Unternehmen der Zivilluftfahrtbranche mit deutlichem Personalzuwachs. Anders als viele andere (ehemalige) Flagcarrier in Europa ist das Unternehmen mehrheitlich im Privatbesitz. Mit weltweit über 400 Konzern- und Beteiligungsgesellschaften ist es einer der größten Konzerne in der Zivilluftfahrtbranche.

Die Konzernstruktur mit dem Kerngeschäft der Passagierbeförderung gliedert sich in fünf Geschäftsfelder:

Daneben unterhält der Konzern Service- und Finanzgesellschaften.

Im Jahr 2007 wurde ein Reformprogramm angekündigt, unter welchen insbesondere die bis dahin relativ selbständigen Management-Bereiche Einkauf, Controlling und Immobilienmanagement der größeren Tochtergesellschaften (wie Lufthansa Technik und Lufthansa Cargo) zentralisiert wurden.

Im internationalen Luftverkehr nahm der Konzern im Jahr 2003 unter den IATA-Fluggesellschaften den Spitzenplatz bei der Anzahl der beförderten Passagiere ein. Lufthansa Cargo belegte bei der beförderten Luftfrachtmenge im Jahr 2005 Platz eins. Die Zahl der konzernweit beförderten Fluggäste belief sich 2008 auf 70,5 Millionen. Seither werden die Zahlen der Lufthansa von zusammengeschlossenen großen US-Gesellschaften übertroffen.

Während der Konzern einschließlich Lufthansa Passage Airlines gesellschaftsrechtlich unmittelbar von der Deutschen Lufthansa AG geleitet wird, werden seine formalrechtlich eigenständigen Tochtergesellschaften indirekt über die dortigen Aufsichts- und Vorstandsgremien geführt. Neben den Tochtergesellschaften Germanwings, Swiss oder Austrian Airlines Group sowie den Fluggesellschaften der Dachmarke Lufthansa Regional bestehen Beteiligungen an mehreren ausländischen Fluggesellschaften. Die einst zu Lufthansa gehörende Fluggesellschaft Condor wurde 1997 aus dem Konzern ausgegliedert und 2006 verkauft, behielt aber ihre Mitgliedschaft im Lufthansa-Vielfliegerprogramm Miles & More.

Der Bereich Passagierbeförderung macht 75,8 Prozent des Konzern-Umsatzes aus (Stand zum 31. Dezember 2012). Zu diesen Zahlen tragen die rechts abgebildeten Linienfluggesellschaften und Allianzen bei.

Die Passagierlinienfluggesellschaft unter dem Markennamen „Lufthansa“ (konzerninterne Bezeichnung "Lufthansa Passage Airlines") ist im Konzern Deutsche Lufthansa AG die größte Einzelgesellschaft. Im Oktober 2012 hat die Lufthansa ihre Strategie bei der Passagierbeförderung geändert. Die Kurz- und Mittelstreckenflüge abseits der Drehkreuze Frankfurt am Main und München werden schrittweise an die Tochtergesellschaft Eurowings übergeben. Die Kernmarke Lufthansa konzentriert sich auf alle Flüge ab Frankfurt und München.

Seit 1. Juli 2007 ist der Lufthansa-Konzern Alleineigentümer der Swiss. Zuvor gehörte die Swiss aus rechtlichen Gründen bis zur Neuaushandlung bilateraler Luftverkehrsrechte (insbesondere von Landerechten) zwischen der Schweiz und insgesamt mehr als 140 außereuropäischen Ländern nur zu 49 Prozent dem Lufthansa-Konzern. Die restlichen 51 Prozent verblieben auf Grund einer Nationalitätsklausel (Mehrheitseigentümer mussten aus der Schweiz stammen) zwar bei der eigens zu diesem Zweck gegründeten Schweizer Stiftung "Almea", jedoch hatte der Lufthansa-Konzern schon aufgrund vertraglicher Regelungen bereits die operative Entscheidungsmacht. Almea und Lufthansa waren in diesem Verhältnis (51:49) Eigentümer der "AirTrust AG", die wiederum Alleineigentümerin der Swiss war. Somit galt die Swiss als "Schweizer Unternehmen" und konnte die vorhandenen Verkehrsrechte in den Ländern außerhalb der Europäischen Union beibehalten. Diese sind seit jeher weltweit zwischenstaatlich geregelt und nicht an andere Staaten veräußert werden.

Am 5. Dezember 2008 wurde nach langen Verhandlungen die Übernahme der Austrian Airlines durch Lufthansa bekannt. Der geringe Kaufpreis von 366.000 Euro für den 42-prozentigen Anteil der ÖIAG entstand mit der daran gekoppelten Übernahme von mindestens 500 Millionen Euro Schulden der finanziell angeschlagenen österreichischen Gesellschaft. Lufthansa stieg damit zur größten Fluggesellschaft Europas (gefolgt von Air France-KLM) auf. Die Übernahme erfolgte zum 3. September 2009, als Lufthansa mehr als 90 Prozent des Aktienkapitals von Austrian Airlines hielt. Die restlichen Aktien übernahm Lufthansa zum 4. Februar 2010.
Unter der Marke Lufthansa Regional führen die zwei Tochtergesellschaften Air Dolomiti und Lufthansa CityLine Regional- und Zubringerflüge für die Lufthansa durch. Die deutsche Regionalfluggesellschaft Eurowings wurde im August 2011 zu 100 Prozent übernommen. Aufgrund einer Stimmbindungsvereinbarung mit einem Treuhänder des vorherigen Mehrheitsaktionärs Albrecht Knauf verfügte der Lufthansa-Konzern aber bereits seit 2005 über die wirtschaftliche Kontrolle.
Lufthansa ist seit einigen Jahren zu 50 Prozent an der türkischen Ferienfluggesellschaft Sunexpress beteiligt, die anderen 50 Prozent gehören Turkish Airlines. Die Beteiligung wird – ähnlich wie damals bmi – nicht in das Konzernergebnis konsolidiert.

Zunächst (2008) hielt Lufthansa 45 Prozent der Anteile an Brussels Airlines. und besaß eine Option, auch die restlichen 55 Prozent zu erwerben. Trotz Freigabe durch den Aufsichtsrat und die EU-Kommission wollte Lufthansa die Übernahme erst durchführen, als bei Brussels Airlines der „wirtschaftliche Turnaround abgeschlossen“ war. Nachdem der Aufsichtsrat nach einer Sitzung im September 2016 den Plänen für eine Komplettübernahme grünes Licht gegeben hatte, schloss Lufthansa die Übernahme von Brussels Airlines am 11. Januar 2017 ab und erwarb die restlichen 55 % der Anteile.

Im Dezember 2007 gab Lufthansa bekannt, einen Anteil von 19 Prozent an JetBlue Airways zu erwerben. Die Unternehmen hatten sich geeinigt, dass Lufthansa 42 Millionen neu emittierte Aktien übernimmt. Der Kauf wurde im 1. Quartal 2008 vollendet. Lufthansa sicherte sich einen Sitz im Aufsichtsrat der JetBlue. Mit diesem Zukauf macht Lufthansa ihren Star-Alliance-Kooperationspartnern direkte Konkurrenz, insbesondere an der Ostküste der USA.

An der luxemburgischen Fluggesellschaft Luxair war der Lufthansa-Konzern bis 6. November 2015 mit 13 Prozent beteiligt.

Im Unterschied zu den übrigen zum Lufthansa-Konzern gehörenden Fluggesellschaften agieren die als „Deutsche Lufthansa AG“ firmierende Lufthansa sowie die zwei Tochtergesellschaften Lufthansa CityLine und Lufthansa Cargo unter der Corporate Identity des Markenzeichens „Lufthansa“. In der werbestrategischen Außendarstellung ihrer Verkehrsdienstleistungen knüpft ihre Corporate Identity an die bis Mitte der 1990er Jahre ausschließlich von der Fluggesellschaft Deutsche Lufthansa AG geprägte Marke „Lufthansa“ an. Die Fluggesellschaft Lufthansa wurde im August 2007 unter den gewinnstärksten Fluggesellschaften der Welt an sechster Stelle geführt.

Lufthansa betreibt zwei übergeordnete Drehkreuze

Drehkreuze der Tochtergesellschaften befinden sich

Ab 2013 wurden alle innerdeutschen und innereuropäischen Strecken der Lufthansa-Gruppe außerhalb der großen Drehkreuze Frankfurt am Main und München von Germanwings übernommen.

Ab Frankfurt am Main und München fliegt Lufthansa viele Ziele innerhalb Europas sowie in Nord- und Südamerika, Asien, Afrika und dem Nahen Osten an. Australien und Neuseeland fliegt Lufthansa nicht mehr an, sondern bedient diese in Kooperation mit Star-Alliance-Partnern über deren Drehkreuze in Singapur (Singapore Airlines), Bangkok (Thai Airways) sowie Los Angeles und San Francisco (United Airlines). Zudem bestehen Codeshare-Abkommen mit Air New Zealand für Strecken über Hongkong, Los Angeles, San Francisco und Vancouver. Die letzte von Lufthansa selbst durchgeführte Australien-Verbindung bestand von Sommerflugplan 1994 bis zum Ende des Sommerflugplans 1995 dreimal wöchentlich von Frankfurt am Main (LH796/LH797) über Bangkok nach Sydney und zurück. Eingesetzt wurde dabei eine Boeing 767 aus der Flotte der Condor.

Längster regulärer Nonstop-Passagierflug (gemessen an der flugplanmässigen Flugzeit) ist die Verbindung von Frankfurt am Main nach Buenos Aires-Ezeiza (Hinweg 13:50 Stunden, Rückweg 13:15 Stunden, 11.515 km), gefolgt von Frankfurt am Main nach Singapur-Changi (Hinweg 12:15 Stunden, Rückweg 13:05 Stunden, 10.284 km), Frankfurt am Main nach Kuala Lumpur (Hinweg 12:15 Stunden, Rückweg 13:30 Stunden, 10.000 km; wurde zum 1. März 2016 eingestellt) sowie die Strecken von München nach Los Angeles (Hinweg 12:20 Stunden, Rückweg 11:25 Stunden, 9.625 km) und München nach São Paulo-Guarulhos (Hinweg 12:40 Stunden, Rückweg 11:50 Stunden, 9.868 km).

Am 19. Januar 2011 stellte die Lufthansa einen konzerninternen Nonstop-Rekord auf. Der Airbus A340-600 „Lübeck“ "(D-AIHF)" flog Kreuzfahrtpassagiere mit einer Flugzeit von 14:48 Stunden vom Flughafen München zum Honolulu International Airport und unterbot damit den vorherigen Rekord aus dem Jahr 2010 um fünf Minuten, als eine A340-600 der Lufthansa im Kreuzfahrtcharter von München zum Flughafen Santiago de Chile geflogen war.

Lufthansa war unter den ersten westlichen Fluggesellschaften, die neben Moskau und St. Petersburg weitere Ziele in der ehemaligen Sowjetunion anflogen, und betrieb jahrelang erfolgreich bis zu 14 verschiedene Strecken. Im Zuge der Krimkrise schwächte sich die Nachfrage deutlich ab. Ab Herbst 2015 wurden deshalb nur noch die ursprünglichen Ziele Moskau und St. Petersburg weiter bedient.

Die Flugnummern werden nach einem festen Schema verteilt. So haben Frühflüge stets eine kleinere Nummer als Flüge auf der gleichen Strecke später am Tag. Flüge von Frankfurt und München aus haben in der Regel eine gerade Endziffer, Flüge dorthin zurück eine ungerade.


Das Frachtgeschäft macht 11,6 Prozent des Konzern-Umsatzes aus und erbringt 12,0 Prozent des operativen Ergebnisses (Stand: 2008). Es wird von der hundertprozentigen Tochtergesellschaft Lufthansa Cargo betrieben. Diese ist ehemaliges Mitglied des Cargo Networks WOW und gemessen an Frachttonnenkilometern (FTKT) die sechstgrößte Frachtfluggesellschaft weltweit. Sie nutzt und vermarktet die Luftfrachtkapazitäten der Passagierflugzeuge sämtlicher Fluggesellschaften im Konzernbereich Passagierbeförderung. Rund die Hälfte des Umsatzes der Lufthansa Cargo wird im asiatisch-pazifischen Raum getätigt. Im Oktober 2004 wurde in einem Joint Venture zwischen Shenzhen Airlines (51 %), Lufthansa Cargo (25 %) und der Deutschen Entwicklungsgesellschaft (24 %) die chinesische Frachtfluggesellschaft Jade Cargo International gegründet. Der Flugbetrieb wurde im August 2006 aufgenommen. Mit dieser Beteiligung hat sich Lufthansa Cargo indirekten Zugang zum innerasiatischen Luftfrachtmarkt verschafft. 2012 wurde der Flugbetrieb der Jade Cargo aufgrund von Streitigkeiten eingestellt. Des Weiteren ist Lufthansa Cargo an dem 2004 errichteten „International Cargo Center Shenzhen (ICCS)“ beteiligt.

Seit 2004 arbeitet Lufthansa Cargo mit der Post-Tochter DHL zusammen. Im September 2007 ging aus dieser Zusammenarbeit das Joint Venture Aerologic hervor, eine Frachtfluggesellschaft, die am Flughafen Leipzig/Halle beheimatet ist.

Im Jahr 2009 machte das Geschäftsfeld Technik 10,3 Prozent des Konzern-Umsatzes aus (2008: 8,9 Prozent) und erbrachte 21,9 Prozent (2008) des operativen Ergebnisses. Lufthansa Technik betreut über die konzerneigenen beispielsweise angeschlossenen Fluggesellschaften hinaus weltweit fremde Fluggesellschaften. Zentrum der Lufthansa Technik ist Hamburg; weitere große Wartungsbetriebe befinden sich in Frankfurt am Main, München und Berlin, an allen größeren deutschen Flughäfen und 50 Standorten weltweit. Zum Verbund gehören ferner 32 technische Instandhaltungsbetriebe auf der ganzen Welt.

Lufthansa Technik ist einer der größten Anbieter von Luftfahrzeug-Instandhaltungen, die 2008 für alle Fluggesellschaften zusammen ein geschätztes Volumen von 42 Milliarden US-Dollar weltweit hatten. Davon beträgt der für Lufthansa Technik in Frage kommende Anteil 34 Milliarden US-Dollar. Mit einem Anteil von 15 Prozent an dieser Summe ist das Unternehmen Weltmarktführer. Im Joint Venture mit dem Hersteller von Regional- und Geschäftsreiseflugzeugen Bombardier wird die Wartungsgesellschaft „Lufthansa Bombardier Aviation Services GmbH“ unterhalten. Vorwiegend den asiatischen Markt bedient der Lufthansa-Konzern mit „AMECO Beijing“, einem gemeinsam mit Air China betriebenen Joint Venture zur Wartung von Verkehrsflugzeugen des Herstellers Boeing. Das notwendige Material nebst Einrichtungen hält die „Lufthansa Technik Logistik GmbH“ an ihren neun Standorten in Deutschland und auf dem Flughafen Malta vor. Für die Ausbildung und Schulung wurde die Lufthansa Technical Training GmbH gegründet, die von anderen Fluggesellschaften genutzt wird.

Als Ersatz für die Airbus A310-300 VIP „Konrad Adenauer“ und „Theodor Heuss“ hat das Bundesamt für Wehrtechnik und Beschaffung 2008 mit der Lufthansa Technik einen Vertrag über die Lieferung zweier zu VIP-Transportflugzeugen umgebauter Airbus A340-300 aus der Konzernflotte (Luftfahrzeugkennzeichen "D-AIFB" und "D-AIGR") für die Flugbereitschaft des Bundesministeriums der Verteidigung abgeschlossen.

Das von der LSG Sky Chefs betriebene Catering generiert 7,1 Prozent des Konzern-Umsatzes und erbringt 5,2 Prozent des operativen Ergebnisses (Stand: 2008). Unter der Marke LSG Sky Chefs stellt der zweitgrößte Caterer von Fluggesellschaften 20 Prozent aller Bordmahlzeiten bereit. LSG umfasst 124 Unternehmen und ist mit zirka 200 Betrieben in 49 Ländern vertreten. In Amerika und Europa liegt der Marktanteil von LSG Sky Chefs zwischen 35 und 40 Prozent; weltweit gibt es fast 500 Kunden.
Die Tochter Lufthansa Systems "(LSY)" ist als IT-Dienstleister mit weltweit rund 1900 Mitarbeitern neben dem Hauptsitz Raunheim bei Frankfurt am Main an weiteren zehn Standorten in Deutschland und 17 Standorten in 14 anderen Ländern tätig. Sechs Standorte sind in Asien, sieben in Amerika und vier in Europa. Lufthansa Systems ist nach eigener Aussage einer der führenden Anbieter von Flugverkehrsinformationstechnologie, luftverkehrsspezifischer Software für Bord-, Flug- und Wartungsmanagement für die Airline- und Aviation-Industrie weltweit. Darüber hinaus hält der Lufthansa-Konzern Anteile an dem Flugreservierungssystem Amadeus (CRS).

Die Tochter Lufthansa Industry Solutions GmbH & Co. KG ist als IT-Dienstleister mit weltweit rund 1300 Mitarbeitern neben dem Hauptsitz Norderstedt bei Hamburg an weiteren Niederlassungen in Deutschland, Schweiz und den USA vertreten. Lufthansa Industry Solutions ist nach eigener Aussage ein Anbieter von Prozessberatung und IT-Service für die Bereiche Industrie, der Logistik, dem Energiesektor, dem Gesundheitswesen, dem Tourismus oder dem Verlagsgeschäft.

Der Geschäftsbereich IT-Services hat einen Anteil von 2,15 Prozent am Konzern-Umsatz und 3,88 Prozent des operativen Ergebnisses (Stand: 2014). Der externe Umsatzanteil von LH Systems beträgt 41,8 Prozent (Vorjahr 2013: 41,4 Prozent).

Die Unternehmensaufgabe der Tochtergesellschaft Miles & More (ehemals „Lufthansa Worldshop“) besteht im Kundenbindungsprogramm von Lufthansa und anderen voll integrierten Partnern, dieses weiterzuentwickeln und am Drittmarkt zu etablieren. Darüber hinaus betreibt Miles & More die Onlineshops "Lufthansa Worldshop" und "Swiss Shop" sowie die gleichnamigen lokalen Geschäfte an sechs Flughäfen und organisiert den Bordverkauf für Lufthansa und Swiss.

Die konzerneigene Delvag Versicherungs-AG ist eine auf die Versicherung von Luftfracht spezialisierte Versicherungsgesellschaft. Die ebenfalls konzerneigene AirPlus ist auf das Geschäftsreisemanagement von Firmenkunden spezialisiert.

Von der Tochtergesellschaft Lufthansa Flight Training wird die Ausbildung und Schulung von Cockpit- und Kabinencrews durchgeführt.

Das Franchise-Unternehmen Lufthansa City Center ist eine Kette inhabergeführter mittelständischer Reisebüros. Es ist keine Tochtergesellschaft der Lufthansa, sondern hat seit 1994 einen Markennutzungsvertrag der Gesellschaft.

Die Lufthansa Consulting GmbH mit Sitz in Köln ist weltweit aktiv im luftfahrtaffinen Beratungsgeschäft. Zum Kundenkreis zählen in erster Linie Fluggesellschaften, Flughäfen, Logistikunternehmen und branchenverwandte Institutionen. Seit der Ausgliederung aus dem Lufthansa Konzern und der Gründung als eigenständige GmbH im Jahr 1988 hat sie über 1700 luftfahrtspezifische Projekte durchgeführt (Stand: 2010), vor allem in den Bereichen „Airline Strategy“, „Airline Restructuring“, „Airline Operations“, „Air Cargo Logistics“ und „Airports“. Mehr als 90 Prozent der Umsätze werden mit externen Kunden erzielt. Eine Niederlassung der Gesellschaft befindet sich in Frankfurt am Main, darüber hinaus ist das Unternehmen mit einem Büro in Moskau vertreten. Lufthansa ist (über ihre Tochtergesellschaft Lufthansa Commercial Holding) mit gut 90 Prozent der Anteile Mehrheitsgesellschafter. Die restlichen Anteile hält das Management (Partner).

Am Terminal 2 des Flughafens München hält Lufthansa eine 40-Prozent-Beteiligung. Über Tochter- und Partnergesellschaften hat sich Lufthansa das ausschließliche Nutzungsrecht für dieses Terminal gesichert (Kapazität bis zu 25 Millionen Passagiere). Erstmals im europäischen Luftverkehrsmarkt fand Bau und Betrieb eines Terminals durch Lufthansa und den dortigen Flughafenbetreiber statt. Zwischenzeitlich ist sie maßgeblich an der Fraport AG, der Betreiberin des Flughafens Flughafen Frankfurt am Main, beteiligt. Seit Januar 2005 betreibt Lufthansa dort zusammen mit Fraport den Terminal 1 sowie das Lufthansa First Class Terminal mit exklusiven Lounges, das ausschließlich für Erste-Klasse-Passagiere und Mitglieder des HON Circle (höchste Stufe des Vielfliegerprogramms Miles & More) bestimmt ist.

Der Lufthansa-Konzern hält weiterhin Anteile an nicht direkt mit dem Luftverkehr zusammenhängenden Unternehmen. Insgesamt ist der Konzern an mehr als 400 nationalen und internationalen Gesellschaften beteiligt. Eine detaillierte Übersicht findet sich auf der "Lufthansa-Financials"-Webseite.

Bis zum ersten Quartal 2009 war der Konzern mit 24,9 Prozent an der Charterfluggesellschaft Condor Flugdienst beteiligt. Diese Anteile wurden durch den Tourismuskonzern Thomas Cook Group übernommen.

Unter dem Markennamen „Lufthansa“ werden die Flotten der Lufthansa "(Lufthansa Passage Airlines)", Lufthansa CityLine und Lufthansa Cargo betrieben. Hinzu kommen die Flotten der Tochtergesellschaften. Die Flugzeuge der Lufthansa Passage Airlines sind in Frankfurt und München stationiert. In Frankfurt am Main sind u. a. alle Boeing 747-400 und 747-8I beheimatet; von München aus operieren alle A340-600 und A350-900 sowie acht A330-300 und fünf A380-800.

Mit Stand September 2017 besteht die Flotte der Lufthansa Passage aus 272 Flugzeugen mit einem Durchschnittsalter von 11,5 Jahren:

Als einflussreicher Groß- und Erstkunde hat Lufthansa die Entwicklung der Boeing-Flugzeuge 737-100 und -300, der 747-400 und -8I sowie der Airbus-Typen A310, A340 und A380 entscheidend beeinflusst. Durch den Einsatz moderner Flugzeuge können beim Kerosinverbrauch variable Kosten eingespart werden. Der durchschnittliche Verbrauch der Lufthansa-Flotte wurde von der Arbeitsgemeinschaft Deutscher Verkehrsflughäfen für das Jahr 2013 mit 3,91 Litern je 100 Passagierkilometer angegeben. Der Lufthansa-Konzern zielt auf einen durchschnittlichen Verbrauch von drei Litern.

Die letzte Avro RJ85 wurde von Lufthansa CityLine am 27. August 2012 ausgemustert. Bis Ende 2012 wurde im Hinblick auf die reduzierten Wachstumspläne die Ausmusterung von insgesamt 38 Kurz- und Langstreckenflugzeugen angekündigt, darunter alle Avro RJ85 und mehrere Airbus A340-300. Am 14. März 2013 gab Lufthansa die Bestellung von 100 A320 und zwei weiteren A380-800 bei Airbus bekannt. Am 19. September 2013 unterzeichnete Lufthansa eine Bestellung über 25 A350-900 inklusive 15 Optionen und weiterer 15 Vorkaufsrechte, die auch die verlängerte Version A350-1000 beinhalten sowie 34 Boeing 777-9 zum Ersatz der älteren A340-300 und 747-400. Am 29. Oktober 2016 verließ mit einer Boeing 737-300 die letzte Boeing 737 die Lufthansa Flotte.

Lufthansa übernahm Ende Dezember 2016 den ersten von aktuell (Stand Februar 2017) 25 Airbus A350-900, der am 3. Februar 2017 in München offiziell vorgestellt wurde. Die Maschine nahm am 10. Februar offiziell den Flugbetrieb auf, insgesamt ist die Stationierung von fünfzehn Maschinen in München geplant.

Für den Sommerflugplan 2018 hat Lufthansa zwei Fokker 100 der Helvetic Airways geleast. Diese sollen in München stationiert werden.

Bei den jeweiligen Luftfahrzeugkennzeichen der in Deutschland in der Luftfahrzeugrolle eingetragenen Flugzeuge des Lufthansa-Konzerns (zum Beispiel "D-AIKJ") besteht eine – verbindliche – Systematik nur bei den ersten beiden Buchstaben. Der Buchstabe vor dem Bindestrich ist das Staatszugehörigkeitszeichen (ein „D“ für die Bundesrepublik Deutschland), der erste Buchstabe nach dem Bindestrich gibt Auskunft darüber, in welchem Bereich das Höchstabfluggewicht liegt (zum Beispiel „A“ für Flugzeuge mit einer höchstzulässigen Startmasse von mehr als 20 Tonnen). Die Kennzeichen-Vergabe erfolgt in Deutschland durch das Luftfahrt-Bundesamt. Ähnlich wie bei der Vergabe von Kfz-Kennzeichen kann bei der Vergabe der letzten drei Buchstaben gegebenenfalls auf etwaige „Kundenwünsche“ der Lufthansa eingegangen werden, falls die gewünschte Buchstabenkombination noch nicht vergeben ist. Tatsächlich wird der Lufthansa seit geraumer Zeit folgender „Kundenwunsch“ vom Luftfahrt-Bundesamt erfüllt: Der zweite Buchstabe nach dem Bindestrich soll auf den jeweiligen Flugzeughersteller beziehungsweise die Konzerngesellschaft schließen lassen können, der dritte auf den Flugzeugtyp, der vierte läuft bis auf wenige Ausnahmen (z. B. Verzicht auf Buchstabenkombinationen wie SA, SS, HJ, BYB…) durch (siehe folgende Tabelle).
In den wenigen historischen Fällen, in denen ein Luftfahrzeug zu leicht war, um ein A als zweiten Buchstaben zu bekommen, wurde A an dritter Stelle gewählt: F27 waren D-BARI und D-BARO, DC-3 waren D-CADE, D-CADI und D-CADO. Darüber hinaus kaufte Lufthansa eine 747-400 (c/n 1292) von Boeing, um sie sogleich an Royal Flight Oman weiterzuverkaufen. Während der kurzen Zeit, in der sie in LH-Diensten war (also während der Auslieferungsflüge), war sie als D-ARFO registriert.

Viele Flugzeuge der Lufthansa tragen seit 1960 Taufnamen, die auf dem vorderen Rumpf unterhalb der Nennung des Flugzeugtyps ersichtlich sind. In der Regel sind deutsche Städte oder Bundesländer Taufpate. Wenn ein Flugzeug die Flotte verlässt, übernimmt in der Regel ein neueres dessen Taufnamen. So wurde z. B. der Name „Berlin“ durch Flugzeuge der Typen Boeing 707, 747-200 und 747-400 sowie Airbus A380-800 getragen. Davon abweichend ist der Airbus A340-300 (Luftfahrzeugkennzeichen "D-AIFC") „Gander/Halifax“ nach den beiden kanadischen Städten benannt, die auf der Standardflugroute von Mitteleuropa nach Nordamerika liegen. Der Doppelname soll daran erinnern, dass nach Schließung des US-amerikanischen und des kanadischen Luftraums am Tag der Terroranschläge am 11. September 2001, Flugzeuge, die wegen geringer Treibstoffreserven nicht mehr nach Europa umkehren konnten, auf den Flughäfen Gander (39 Flugzeuge) und Halifax landen mussten und speziell in Gander, das die größere Anzahl aufnehmen musste, trotz der schwierigen Umstände äußerst gastfreundlich behandelt wurden.

Im Februar 2010 gab die Lufthansa bekannt, ihre ersten beiden Airbus A380 nach ihren beiden größten Drehkreuzen zu benennen: „Frankfurt am Main“ "(D-AIMA)" und „München“ "(D-AIMB)". Die nachfolgend in Dienst gestellten A380-800 tragen Namen internationaler Großstädte: „Peking 北京“ "(D-AIMC)", „Tokio 東京“ "(D-AIMD)", „Johannesburg“ "(D-AIME)", „Zürich“ (D-AIMF), „Wien“ "(D-AIMG)", „New York“ "(D-AIMH)", „Berlin“ "(D-AIMI)", „Brüssel“ "(D-AIMJ)", „Düsseldorf“ "(D-AIMK)", „Hamburg“ "(D-AIML)", „Delhi“ "(D-AIMM)" und „San Francisco“ "(D-AIMN)". Am 18. November 2015 wurde die jüngste A380-800 mit dem Luftfahrzeugkennzeichen "D-AIMN" anlässlich der Eröffnung der neuen Verkehrszentrale der Lufthansa in Frankfurt ("Integrated Operations Control Center", abgekürzt "IOCC"), feierlich von Bundeskanzlerin Angela Merkel in „Deutschland“ umgetauft.

Der Airbus A321-100 "(D-AIRA)" „Finkenwerder“ ist eine Reminiszenz an das Airbus-Werk in Hamburg-Finkenwerder, in dem ein Teil der Airbus-Modellpalette montiert wird.

Lufthansa hatte zunächst insgesamt 15 Airbus A380-800 bestellt, von denen bis Juni 2012 zehn ausgeliefert wurden. Im September 2011 wurde die Bestellung um zwei weitere Exemplare auf 17 aufgestockt, diese Bestellung wurde am 14. März 2013 bestätigt. Im September 2013 wurde jedoch bekannt gegeben, dass der Lufthansa-Aufsichtsrat die Abnahme von nur zwölf der 15 A380 der ersten Bestellung genehmigt habe. Somit werden insgesamt nur 14 A380 in die Flotte aufgenommen.

Das erste Exemplar war am 19. Mai 2010 mit dem Luftfahrzeugkennzeichen "D-AIMA" übergeben worden und wurde auf den Namen „Frankfurt am Main“ getauft. Lufthansa setzt ihre A380 von und nach Frankfurt am Main sowie seit März 2018 ab München ein.

Bereits vom 6. bis 12. Dezember 2011 setzte Lufthansa eine A380 einmal täglich auf der Route von München nach New York-JFK ein. Dies geschah hauptsächlich vor dem Hintergrund des Weihnachtsshoppings in New York City.

Nach ihrer Neugründung setzte Lufthansa ab 1955 unter anderem folgende Flugzeugtypen ein:

Der Service gliedert sich bei der Lufthansa in ein System von bis zu vier Beförderungsklassen, bestehend aus "Economy Class", "Premium Economy Class", "Business Class" und "First Class".

Für Kunden der Business-Klasse und Vielflieger mit "Frequent Traveller" Status von Miles & More werden an einigen Flughäfen Business Lounges betrieben, darüber hinaus für die höchste Stufe des Vielfliegerprogramms in Frankfurt und München designierte First Class Lounges. Das First Class Terminal in Frankfurt am Main ist ein eigenes Gebäude und stellt, neben der Ausstattung einer First Class Lounge, einen eigenen Check-in und Sicherheitskontrollen für die Gäste der First Class bereit. Von den First Class Lounges werden die Gäste in Limousinen direkt zu ihrem Flugzeug gefahren.

First Class

Die First Class wird ausschließlich auf der Langstreckenflotte angeboten und besteht grundsätzlich aus einem Abteil mit acht Sitzen im vorderen Teil (Airbus A330-300 und A340-600, Boeing 747-8I) bzw. im Oberdeck des Flugzeugs (Airbus A380-800). Die erneuerte Gestaltung und Ausrüstung wurde sukzessive auf den älteren Flugzeugen nachgerüstet, wobei die 747-400 lediglich eine umfangreiche Aufarbeitung des bisherigen Sitzes erfuhren. Ende 2014 wurde die First Class aus allen in der Flotte verbleibenden 747-400 und A340-300 entfernt, die damit nur noch in einer veränderten Dreiklassenkonfiguration fliegen.

Business Class
Mit Auslieferung der ersten Boeing 747-8I wurde die bisherige Business Class durch ein neues Sitzmodell abgelöst, das sich erstmals in ein vollständig flaches Bett umwandeln lässt, der zuvor eingebaute Sitz war nur in ein geneigtes Bett umwandelbar. Auf der 747-8I befindet sich im Oberdeck eine 2-2-Sitzkonfiguration, im Hauptdeck zwischen der First und Economy bzw. Premium Economy Class eine 2-2-2-Konfiguration. In allen Flugzeugen der Airbus-Flotte (A330-300, A340-300, A340-600, A350-900 und A380-800) ist die Business Class stets in einer 2-2-2-Konfiguration eingebaut. Alle älteren Flugzeuge der Langstreckenflotte sind mit der neuen Business Class nachgerüstet worden.

Kulinarisch kann man in der Business Class meist mit einem Dreigängemenü rechnen, wobei man bei „Entrees/Vorspeisen“, dem Hauptgericht und Dessert verschiedene Auswahl besitzt. Zu Beginn der Reise gibt es für jeden Passagier ein der Flugstrecke angepasstes „Amenity Kit“, in dem zum Beispiel Ohrstöpsel, Zahncreme, eine Einmalzahnbürste und eine Schlafbrille enthalten sind.

Auf der Kurzstrecke und teilweise auf der Mittelstrecke verwendet Lufthansa das Prinzip des „Class Dividers“, eines beweglichen Vorhangs, der in der Kabine hängt und verschoben werden kann. So trifft man sowohl in der Business als auch in der Economy Class auf dieselben Sitze, es bleibt jedoch einer von drei Sitzen der 3-3-Konfiguration leer, zudem unterscheidet sich der Service. Im vorderen Kabinenteil hat man meist etwas mehr Beinfreiheit. Der Vorteil dieses Systems ist, dass je nach Nachfrage agiert und die Business Class größer oder kleiner gestaltet werden kann.

Premium Economy Class

Seit dem 22. November 2014 bietet Lufthansa eine Premium Economy Class an. Diese ist eine Kompromiss-Lösung zwischen Preis und Komfort. Die Vorteile im Vergleich zu der Economy Class sind ein vergrößerter Sitzabstand (96,5 cm), ein zusätzliches Gepäckstück (23 kg), eine neue Menükarte und ein Amenity Kit. Die Sitze sind außerdem bis zu einem Winkel von 130 Grad nach hinten verstellbar. Es wird ein Dreigängemenü auf Porzellangeschirr serviert, und die Passagiere erhalten ein „Willkommensgetränk“. Die Getränkekarte entspricht der Economy Class. Die Bildschirme sind groß, flach und bieten dasselbe In-flight Entertainment wie die First Class und Business Class. Am Boden wird den Passagieren ein bevorzugtes Check-in ermöglicht. Gegen einen Aufpreis von 25 € ist ein Zugang zur Business Lounge der Lufthansa verfügbar. Die Klasse war vorerst ausschließlich in der Boeing 747-8I verfügbar, wurde jedoch bis Sommer 2015 sukzessive auf die gesamte Langstreckenflotte nachgerüstet.

Economy Class
Ab Ende 2007 führte Lufthansa in der Economy-Klasse auf der Langstrecke ein neues Sitzmodell mit einem persönlichen Bildschirm für das In-flight-Entertainment ein. Zuvor wurde die Bordunterhaltung auf zentralen Monitoren mit je nach Sitzplatz eingeschränkter Sicht und ohne Auswahlmöglichkeit gezeigt. Mit Stand Mai 2014 war die gesamte Langstreckenflotte mit persönlichen Bildschirmen in der Economy-Klasse ausgerüstet.

Für die Kurz- und Mittelstreckenflotte stellte Lufthansa am 9. Dezember 2010 die neue „Neue Europa Kabine“ (kurz "NEK") vor. Sie umfasst neue, leichtere und dünnere Sitze und weitere Kabinenumbauten, darunter beispielsweise den Ausbau der Toiletten und der Galley in der Mitte der Kabine der Airbus A321-100 und -200. Die Kurz- und Mittelstreckenflotte verfügt weder über persönliche Bildschirme noch Monitore an der Kabinendecke.

2004 bot Lufthansa als erste Fluggesellschaft in Langstreckenflugzeugen unter dem Namen FlyNet einen Internetzugang während des Fluges an. Dies geschah in Zusammenarbeit mit Connexion by Boeing, wurde jedoch seitens Boeing mangels Rentabilität 2006 wieder eingestellt. Seit Ende 2010 bot Lufthansa in Zusammenarbeit mit der Telekom zunächst zwischen Europa und Nordamerika wieder einen Internetzugang an. Seit Ende 2014 ist die komplette Langstreckenflotte mit FlyNet ausgerüstet. FlyNet wurde zwischenzeitlich im gesamten Langstreckennetz mit Ausnahme des chinesischen Luftraums sowie einigen Nordpolarregionen verfügbar. Die Nutzung ist gebührenpflichtig, es steht jedoch ein kostenloses Portal mit aktuellen Nachrichten und Sportübertragungen zur Verfügung.

Unter dem Namen Miles & More wird seit 1993 ein Vielfliegerprogramm unterhalten. Diesem Bonussystem haben sich die europäischen Luftfahrtunternehmen Adria Airways, Austrian Airlines, Brussels Airlines, Croatia Airlines, LOT, Luxair und Swiss angeschlossen. Air One ist nach ihrer Übernahme durch Alitalia aus dem Programm ausgeschieden.

„Miles & More“ ist ab Ende 2010 in die Kritik geraten, da der Lufthansa vorgeworfen wurde, den Wert der erworbenen Meilen zu kurzfristig und zu Ungunsten der Mitglieder abgewertet zu haben. Zuletzt einigte sich Lufthansa mit einem klagenden Mitglied auf einen Vergleich und erklärte sich bereit, Änderungen an der zugrundeliegenden Meilentabelle erst nach einer dreimonatigen Vorlauffrist vorzunehmen.

Laut Branchenexperten trug das Programm, dessen Gewinnbeitrag bisher nicht offiziell im Geschäftsbericht veröffentlicht wird, im Geschäftsjahr 2012 rund 700 Mio. Euro zum Konzerngewinn bei.

Die Hauptaufgabe des am 6. Mai 1968 auf dem Flughafen Frankfurt am Main ins Leben gerufenen „Rotkäppchenservices“ oder „Rotkäppchendiensts“ besteht in der Betreuung alleinreisender Kinder und mobilitätseingeschränkter Passagiere. Zum Gründungszeitpunkt des Betreuungsdienstes trugen die überwiegend weiblichen Mitarbeiter, im Gegensatz zu den fliegenden Kollegen, eine rote "Pillbox" (Kopfbedeckung) anstatt einer blauen. Dies war für Gäste und Flughafenmitarbeiter ein klares Erkennungsmerkmal und Anstoß für den dann schnell gefundenen Spitznamen „Rotkäppchenservice“. Der Rotkäppchenservice betreut jährlich mehr als 270.000 Fluggäste, darunter fast 50.000 Kinder.

Die Betreuung mobilitätseingeschränkter Gäste war alleinige Aufgabe der Fluggesellschaften, bis im Jahre 2008 aufgrund einer EU-Verordnung, welche die Betreuung mobilitätseingeschränkter Passagieren klar regelte, die Flughafenbetreiber diese Aufgabe übernehmen sollten. In Frankfurt am Main führte dies 2008 zur Gründung eines Joint Ventures zwischen der Fraport und der Lufthansa mit dem Namen "FraCareServices" (FraCareS).

Der Service für alleinreisende Kinder ist kostenpflichtig, die Betreuung mobilitätseingeschränkter Gäste ist kostenlos. Beides sollte vorab bei Lufthansa per Telefon oder über das Reisebüro beantragt werden.

In der Dokumentation "Fliegen heißt Siegen. Die verdrängte Geschichte der Deutschen Lufthansa" berichtet der Journalist Christoph Weber über ca. 17.000 Zwangsarbeiter, darunter jüdische Deutsche, Ukrainer, Russen und Angehörige anderer Nationen, die in Berlin-Staaken, Berlin-Tempelhof und Schkeuditz zum großen Teil in der Flugzeugreparatur zwangsarbeiten mussten, darunter auch Kinder, die von der Schulbank verschleppt, angelernt und nach Deutschland transportiert worden waren. Mit der Begründung, die Lufthansa sei nach dem Krieg neu gegründet worden, lehnte das Unternehmen jedwede Entschädigung und Rentenzahlungen für diese Zwangsarbeiten ab. Anders als die meisten anderen vergleichbar großen Unternehmen habe die Lufthansa – so Weber – ihre Geschichte zwar von einem Historiker untersuchen lassen, das Ergebnis aber nicht in Buchform publiziert. Zu Beginn des so genannten Dritten Reiches hatte die Lufthansa bis 1936 auch die Aufgabe, Rüstung in der Luftfahrt, die nach dem Friedensvertrag von Versailles Deutschland nicht gestattet war, zu verdecken. Im Zweiten Weltkrieg wurden die Flugzeuge der Lufthansa für Kriegszwecke wie für zivilen Personentransport eingesetzt. Der Mitgründer der Lufthansa 1926, Kurt Weigelt, war ab 1960 Präsident der Firma Lufthansa. Auch der Nachkriegsgründer Kurt Knipfer war schon zu Nazi-Zeiten im Unternehmen aktiv. Im Jahr 1999 überwies das Unternehmen jedoch 40 Millionen DM an den Zwangsarbeiterfond der deutschen Wirtschaft.

Umweltschutzorganisationen kritisieren, dass die Deutsche Lufthansa AG sich gegen einen Emissionsrechtehandel ausspricht, zumindest solange dieser ausschließlich für europäische Gesellschaften gelten soll. Der Lufthansa-Konzern befürchtet Wettbewerbsnachteile gegenüber Airlines mit Sitz außerhalb der EU. Die firmeneigene Umweltpolitik setzt sich für einen einheitlichen europäischen Luftraum ein, der die CO-Emissionen bei innereuropäischen Flügen um acht bis zwölf Prozent senken würde. Des Weiteren verweist der Konzern auf die Umweltfreundlichkeit der für seine Flotten genutzten modernen, treibstoffsparenden Flugzeugtechnik.

Die Lufthansa verfolgt das Ziel eines „CO-neutralen Wachstums“ ab 2020. Zu diesem Zweck möchte der Konzern einen Teil seines Treibstoffes aus Pflanzen gewinnen. Umweltschutzorganisationen kritisieren das neue Treibstoff-Konzept der Lufthansa als Imagekampagne. Die Umweltschutzgruppen bezweifeln, dass Pflanzentreibstoffe nachhaltig und umweltfreundlich gewonnen werden können.

Bürgerinitiativen kritisieren, dass der Lufthansa-Konzern gegen ein Nachtflugverbot am Frankfurter Flughafen plädiere, das vom Flughafenbetreiber Fraport als Zugeständnis an die am Flughafen-Standort betroffene Bevölkerung zugleich mit dem Ausbau beantragt wurde. Lufthansa sieht bei einem Nachtflugverbot vor allem die Wettbewerbsfähigkeit der Lufthansa Cargo sowie 7.300 konzerneigene Arbeitsplätze bedroht.

Im Jahr 2014 begann der härteste Tarifstreit seit Bestehen der Lufthansa. Im November 2015 einigte sich die Konzernführung mit der Kabinengewerkschaft UFO, einige der Konfliktbereiche im Rahmen einer Schlichtung zu klären. Als Schlichter wurde der frühere Ministerpräsident Matthias Platzeck bestimmt. Ende November 2015 einigte sich Lufthansa mit Ver.di auf einen neuen Tarifvertrag für die ungefähr 33.000 Beschäftigen des Bodenpersonals; die Verhandlungen mit Piloten und Flugbegleitern dauerten weiterhin an. Zu den Streitpunkten zählten die betriebliche Altersversorgung sowie der Anspruch von Flugbegleitern und Piloten auf eine Übergangsversorgung bis zum Rentenantritt. Die Konzernführung hebt die Sorge um die Existenzsicherung des Konzerns hervor, die Gewerkschaften bangen um die Existenzsicherung der Mitarbeiter. Weiterer Konfliktpunkt ist der Umbau der Tochtergesellschaft Eurowings, bei der Entgelte nicht dem Lufthansa-Tarifvertrag unterliegen. Am 5. Juli 2016 kam eine Einigung mit den Flugbegleitern. Man einigte sich auf eine Erhöhung der Löhne um etwa 5,5 % und eine jährliche Gewinnbeteiligung, die letztendlich vom Unternehmensergebnis abhängt. Im Gegenzug wurde keine konkrete Rentenhöhe seitens des Managements garantiert, sondern die Zahlung eines festen Arbeitgeberbetrags eingeführt. Darüber hinaus erhält Lufthansa den Kündigungsschutz bzw. unbefristete Arbeitsverträge. Der neue Tarifvertrag gilt bis 2023. Am 15. März 2017 erzielte Lufthansa mit der Vereinigung Cockpit eine Grundsatzeinigung zu allen strittigen Punkten im Rahmen einer Gesamtlösung. Diese beinhaltet unter anderem eine Einmalzahlung in Höhe von 1,8 Monatsgehältern und für den gesamten Zeitraum von Mai 2012 bis Juni 2022 insgesamt 11,4 % mehr Gehalt. Des Weiteren wird der Plan von Lufthansa Vorstand Harry Hohmeister, der eine Neugründung einer Fluggesellschaft vorsah nicht mehr verfolgt. Das Projekt „JUMP“, bei dem Airbus A340-300 mit Piloten der Lufthansa CityLine betrieben werden und touristische Ziele anfliegen, um Kosten zu sparen, fällt künftig wieder unter den Konzerntarifvertrag der Lufthansa-Piloten. Bis 2022 sollen knapp 600 neue Stellen für Kapitänsanwärter und 700 neue Stellen für Nachwuchspiloten geschaffen werden. Lufthansa garantiert den Piloten unter anderem den Besitz von mindestens 325 Flugzeugen innerhalb des Konzerns. Die Verträge gelten für Piloten der Germanwings, Lufthansa Cargo und Lufthansa. Die Vereinbarung ist noch nicht verbindlich, die Einzelheiten und Formalitäten sollen jedoch bis zum Sommer 2017 geklärt sein.

Seit 1955, dem Jahr der Aufnahme des Flugbetriebs der neuen „Deutschen Lufthansa AG“, weist die Unfallbilanz des Unternehmens acht Totalverluste auf, bei denen insgesamt 150 Todesopfer zu beklagen waren. Zwischenfälle von Tochtergesellschaften sind hierbei nicht berücksichtigt und finden sich in deren jeweiligen Artikeln.


Eine weitere Schattenseite in der Chronik des Unternehmens stellen die 13 Flugzeugentführungen bis 1999 dar; die bis 1994 noch staatliche Lufthansa gehörte als Flagcarrier Deutschlands zu den besonders gefährdeten Zielen globaler terroristischer Bedrohungen.







</doc>
<doc id="13326" url="https://de.wikipedia.org/wiki?curid=13326" title="Uhrwerk Orange (Roman)">
Uhrwerk Orange (Roman)

Uhrwerk Orange oder Die Uhrwerk-Orange () ist ein 1962 veröffentlichter Roman von Anthony Burgess. Stanley Kubrick verfilmte das Werk 1971 unter gleichem Titel. Das Magazin Time zählt diesen Roman zu den besten 100 englischsprachigen Romanen, die zwischen 1923 und 2005 veröffentlicht wurden. 2015 wählten 82 internationale Literaturkritiker und -wissenschaftler den Roman zu einem der bedeutendsten britischen Romane.

Der Titel ist vermutlich von einer Cockney-Redewendung für etwas sehr Seltsames inspiriert: "as queer as a clockwork orange". Andere leiten ihn vom malaiischen Wort „orang“ ab, das auch in „Orang-Utan“ vorkommt und so viel wie „Mensch“ bedeutet – also dann "Uhrwerk-Mensch". Beide Interpretationen laufen darauf hinaus, dass ein Mensch etwas Organisch-Natürliches ist (wie eine Orange), den man nicht mit sinnvollem Ergebnis wie ein Uhrwerk funktionieren lassen kann.

Der Roman spielt im England der nahen Zukunft. Alex, ein eigentlich intelligenter Teenager, erzählt seine Geschichte selbst: Aus Spaß an der Gewalt verbringen er und seine drei Freunde ihre Zeit damit, wahllos wehrlose Opfer brutal zusammenzuschlagen, auszurauben und, sofern diese Frauen sind, zu vergewaltigen. Schlägereien und Messerstechereien mit anderen Banden, mit denen sie um die Vorherrschaft in ihrer Gegend konkurrieren, sind an der Tagesordnung. Es werden Drogen wie Alkohol konsumiert. Die Polizei steht dem herrschenden Verbrechen weitestgehend machtlos gegenüber und verkommt teilweise selbst zum Schlägertrupp. Alex’ Eltern sind unfähig, auch nur zu versuchen, auf ihn Einfluss zu nehmen. Er respektiert sie nicht im Geringsten.

Alex’ Freunde sind mit seiner Führungsrolle in der Gruppe nicht mehr zufrieden. Es gibt Unstimmigkeiten, und bei einem ihrer Raubzüge lassen sie ihn im Stich und überlassen ihn der nahenden Polizei. Das Opfer ihres Verbrechens stirbt unglücklicherweise an den Misshandlungen, sodass Alex wegen Mordes angeklagt und zu 14 Jahren Haft verurteilt wird.

Wegen seines unterwürfigen Verhaltens wird Alex für ein neuartiges Experiment der Gehirnwäsche vorgeschlagen, welches ihn zu einem guten Bürger umerziehen soll. Dabei wird er so konditioniert, dass er unfähig zur Gewalt wird, weil ihm der Gedanke an Gewalt sofort Übelkeit verursacht. Seine moralische Einstellung zur Gewalt ändert sich dadurch allerdings nicht. Im Vorfeld warnte ihn der Gefängnispfarrer vor den Konsequenzen:

Nach der 14-tägigen Behandlung wird Alex als „geheilt“ in die Freiheit entlassen. Zunehmend wird deutlich, dass auch in die Gesellschaft integrierte Bürger ihm „ein paar verpassen“ wollen, da sie jetzt die Möglichkeit haben. Er trifft auf einige seiner Opfer und wird zusammengeschlagen.

Eines seiner Opfer (dessen Frau Alex vergewaltigt hat) engagiert sich – ironischerweise – gegen die Brutalität und Unmenschlichkeit des staatlichen Systems. Es versucht, Alex in den Selbstmord zu treiben, um von seinem Tod politisch zu profitieren, aber auch, weil ihm durch einen Zufall bewusst wird, wer dieses „Opfer der modernen Gesellschaft“ tatsächlich ist. Alex überlebt jedoch und erwacht im Krankenhaus. Fortan ist er wieder zur Gewalt fähig.

Die politischen Machthaber arrangieren sich mit ihm, um bei der anstehenden Wahl nicht unter seiner Geschichte leiden zu müssen. Der Systemgegner wird weggesperrt. Alex erhält einen gutbezahlten Job und findet neue Freunde, mit denen er wiederum Unheil stiftet. Doch die Gewalt macht ihm keinen Spaß mehr. Er merkt, dass er älter wird, und als er einen seiner früheren „Droogs“ ("Nadsat" für „Kumpels“) trifft, der gerade eine Familie gegründet hat, träumt er selbst von einer Familie und merkt schließlich, dass sich "das Uhrwerk" weiter dreht und er ihm nicht entrinnen kann.

Burgess stellt in diesem Roman die Frage, ob es schlechter ist, den Menschen zum Gutsein zu konditionieren oder ihm die Freiheit zu lassen, böse zu sein. Burgess steht auf der Seite der Freiheit. In diesem Sinn ist „A Clockwork Orange“ eine Fortführung der Debatte zwischen Augustinus von Hippo und Pelagius, ob der Mensch von Geburt an schlecht sei (Erbsünde), und sich verbessern müsse, oder ob er die freie Wahl hätte zwischen Gut und Böse. Im Pelagianismus ist die Gnade Gottes gegenüber dem freien Willen, Gutes oder Böses zu tun, nur zweitrangig.

Witzig und unterhaltsam ist der Roman trotz aller Brutalität vor allem durch die Sprache und die Art, wie der Protagonist aus der Ich-Perspektive dargestellt wird. Alex hat überhaupt keine innere moralische Instanz und ist unfähig zur Empathie. Er ist intelligent und liebt Musik, vor allem Ludwig van Beethoven. In einem Zeitungsartikel liest Alex, dass ein Theoretiker meint, man könne die heutige Jugend besser in den Griff bekommen, wenn man sie für Künste interessiere. Alex kann darüber nur lachen, denn Musik (und gerade in seinem Fall die als kultivierter als die Rock-Musik eingestufte klassische Musik) erweckt in ihm umso mehr bestialische Gelüste (vgl. Kapitel I, 4).

Seine Sprache ist ein auf der Basis des Russischen konstruierter Jugendslang. Aus dem russischen "golova" (Kopf) macht Burgess "gulliver", aus "chorosho" (gut) "horrorshow" und aus "slusat" (hören) "slooshy". Diese Sprache und ihre Kunstwörter, die so eingesetzt sind, dass man sie auch entschlüsseln kann, wenn man nicht Russisch versteht, trägt wesentlich zum Leseerlebnis bei.

Die ursprüngliche Buchfassung enthält drei Teile à sieben Kapitel. Burgess wählte bewusst die Zahl 21, da sie ehemals Volljährigkeit symbolisierte. Der Verlag in New York wollte jedoch das 21. Kapitel streichen, und Burgess musste zusagen, da er das Geld brauchte. Andernorts erschien das Buch mit allen 21 Kapiteln. Da Stanley Kubrick die US-Version verfilmte, fehlt in seinem Film das eigentlich von Burgess intendierte Ende.

Im 21. Kapitel sieht Alex ein, dass es so in seinem Leben nicht weitergehen kann, und findet ohne äußeren Einfluss den rechten Weg; das wurde in den USA als „zu britisch“ bewertet, und man wollte bewusst ein pessimistischeres Ende. Burgess schreibt: 


Englische Originalausgaben

Deutsche Ausgaben und Übersetzungen

Hörbuch



</doc>
<doc id="13327" url="https://de.wikipedia.org/wiki?curid=13327" title="Verwandtschaft">
Verwandtschaft

Verwandtschaft ist die Beziehung von zwei oder mehr Menschen zueinander, die auf den Beziehungen zwischen Elternteilen und Kindern beruht.

Häufig wird zwischen nahen Verwandten („meine Tochter“) und entfernten Verwandten unterschieden („meine Tante zweiten Grades“: Tochter eines Sohnes der Urgroßmutter mütterlicherseits), siehe dazu Grad der Verwandtschaft.

Im weiteren Sinne werden auch Beziehungen, die zusätzlich auf Partnerschaften (insbesondere Ehen) beruhen, als Verwandtschaft bezeichnet (Schwägerschaft).

Als Verwandtschaft wird auch die Gesamtheit all derer bezeichnet, zu denen verwandtschaftliche Beziehungen bestehen („meine ganze Verwandtschaft“).

Ob es sich bei den zugrunde liegenden Eltern-Kind-Beziehungen um biologische, rechtliche oder (nur) soziale Elternschaft handelt, wird häufig nicht unterschieden, zumal gerade die biologische Vaterschaft manchmal ungeklärt ist.

Eng verbunden mit dem Begriff der Verwandtschaft ist der Begriff der Familie. Als Familie wird meist der Teil der Verwandtschaft bezeichnet, mit der ein Mensch aktuell oder früher in einer Wohngemeinschaft zusammen gelebt hat bzw. zusammen lebt.

Im weiteren Sinne wird „Familie“ auch synonym zu „Verwandtschaft“ verwendet.

In der Mythologie werden auch menschliche Verwandtschaftsbeziehungen zu Göttern und Tieren konstruiert.

Verwandtschaftsbezeichnungen – insbesondere „Onkel“ und „Tante“ – werden häufig auch für Menschen benutzt, die einem besonders nahestehen und eine vergleichbare soziale Rolle einnehmen, auch wenn sie nicht im engeren Sinne verwandt sind („Nenn-Onkel“ bzw. „Nenn-Tante“).

Die juristische Definition von Verwandtschaft ist Aufgabe des Gesetzgebers.

Im europäischen Mittelalter umfasste die Bezeichnung "familia" zunächst alle Hörigen eines Grundherren. Die eigentliche Familie fasste man unter der Bezeichnung „ganzes Haus“ zusammen, das neben den Mitgliedern der Kernfamilie (Eltern und Kinder) auch unverheiratete Verwandte und Bedienstete einschloss.

Verwandtschaft und Familie waren neben dem Lehens- und Leihewesen die Grundbausteine der Gesellschaft und sozialen Ordnung. Verwandtschaft und Politik hingen, vor allem in den Schichten des Adels, häufig zusammen. Durch Heiraten wurden Familien miteinander verbunden. Dabei standen zunächst "agnatische" Verwandtschaft (Verwandtschaft auf männlicher Linie, Geschlecht) und "kognatische" Verwandtschaft (Verwandtschaft der mütterlichen Linie und Schwiegerverwandtschaft, Sippe) gleichberechtigt nebeneinander, während die agnatische zunehmend an Bedeutung gewann. Es bildeten sich endogame Heiratsverbände, das heißt Verbände von Familien, die immer wieder untereinander heirateten und so ihr Heiratsnetz unter einer Abschottung nach außen immer mehr verdichteten. Allerdings waren diese Bündnissysteme alles andere als stabil. Verwandt war vor allem, wer sich als verwandt erinnerte. Neue Heiratsbündnisse konnten alte aufsprengen, die politische Situation die Verwandtschaft belasten.

Geschlechter gründeten sich um einen integrativen Punkt, ein einendes Element, auf das sich alle Angehörigen berufen konnten. Dies konnte ein gemeinsamer Name sein (etwa der eines Stammvaters, der als Vorfahre des ganzen Geschlechts galt oder eine für das Geschlecht wichtige Leistung vollbracht hat). Ab dem 9. oder 10. Jahrhundert wurden Burgen, welche die Adelshöfe ablösten, ebenfalls zu Zentren adliger Geschlechter. Auch Klöster und adlige Familien gingen ein Bündnis ein: Während das Geschlecht das Kloster vor dem Einfluss des Königs schützte, bewahrte das Kloster die Memoria, das heißt die geschichtliche Erinnerung des Geschlechtes, die wiederum zur Einigung desselben führte. Des Weiteren können ikonographische Zeichen, besonders auf Wappen oder Siegeln einen Verwandtschaftsverband hinter sich versammeln.





</doc>
<doc id="13328" url="https://de.wikipedia.org/wiki?curid=13328" title="Fahrrad">
Fahrrad

Ein Fahrrad, kurz Rad, in der Schweiz Velo (von , Kurzform für ‚Schnellfuß‘; ‚schnell‘ und ‚Fuß‘), ist ein mindestens zweirädriges (einspuriges) Landfahrzeug, das ausschließlich durch die Muskelkraft auf ihm befindlicher Personen durch das Treten von Pedalen oder Handkurbeln angetrieben wird. Ein Einrad hat nur ein Laufrad, über welchem alle Kipprichtungen balanciert werden müssen. Das Tandemrad ist eine zweirädrige Sonderform, die zwei oder mehr Personen ermöglicht, einen eigenen Sitzplatz einzunehmen und die eigene Muskelkraft einzusetzen. Sonderformen wie Dreiräder für Kinder oder Senioren und dreirädrige Liegeräder haben drei Räder und sind dreispurig. Fahrradrikschas und Fahrradtaxis können sowohl dreirädrig als auch vierrädrig (zweispurig) sein. Eine weitere Sonderform sind Experimentalfahrräder, welche eine Vielzahl von Laufräder oder andere muskelbetriebene Antriebsformen aufweisen.

Für die Benutzung eines Fahrrades im öffentlichen Straßenverkehr gibt es in jedem Land spezifische gesetzliche Bestimmungen (siehe Radverkehr).

Das Wort "Fahrrad" wurde von den deutschen Radfahrervereinen 1885 als deutsche Entsprechung für die englische Bezeichnung "Bicycle" ( von : "le vélocipède bicycle" = „das zweirädrige Veloziped“) eingeführt. Der neue Ausdruck trat im alltäglichen Sprachgebrauch zunehmend neben die aus dem Französischen entlehnte etablierte Bezeichnung "Veloziped". Er konnte sich letztlich durchsetzen, als in der Zeit der Weimarer Republik das Französische als die Sprache des Hochadels zusehends abgelehnt wurde. Auch die Wörter "Radfahrer" (umgangssprachlich: Radler) und "Radfahren" stammen von deutschen Radfahrervereinen. Bis zur Mitte der 1920er Jahre war der Ausdruck "Fahrrad" eher für Motorräder verwendet worden, und deren Motor hieß häufig "Fahrradmotor".

Regionale Bezeichnungen sind "Fietse" im Niederdeutschen (ähnlich „Fiets“ im Niederländischen) und "Leeze" in der Sondersprache Masematte in Teilen des Münsterlands. In einigen deutschen Mundarten wird das "Fahrrad" als Variation von "Veloziped" bezeichnet, wie etwa im Solinger Platt "Flitzepie" oder im Sauerländischen "Flitzepääd". Die deutschsprachige Schweiz behielt die Abkürzung "Velo" als Abkürzung "vélocipède" bis heute bei. In Bayern und Österreich wird es meist kurz "Radl" genannt.

Allgemeine scherzhafte Bezeichnungen für das Fahrrad sind "Drahtesel" und "Stahlross".

Muskelkraftwagen wurden schon im Mittelalter gebraucht, meistens als Wägelchen mit Lakaien-Fußantrieb in herrschaftlichen Gärten. Eine Ausnahme bildeten Wagen für Behinderte, von denen der mit den Armen bewegte Wagen des querschnittsgelähmten Uhrmachers Stephan Farfler der bekannteste ist.

Im Jahr 1817 stellte der badische Forstbeamte Karl Drais seine einspurige, von ihm so genannte Laufmaschine (später Draisine genannt) als Alternative zum Reitpferd vor. In diesen Jahren gab es wegen des Ausbruchs des indonesischen Vulkans Tambora im April 1815 Missernten, sodass viele Pferde mangels Futter geschlachtet werden mussten.

Die Draisine wurde vielfach nachgebaut, aber nicht weiterentwickelt und schließlich vergessen (der Haferpreis fiel wieder) sowie teilweise wegen der Kollisionsgefahr mit Fußgängern auch verboten. Später konnte man mit den ersten Eisenbahnen größere Entfernungen überwinden. Erst im Zuge der Hochindustrialisierung in der zweiten Hälfte des 19. Jahrhunderts entstanden wieder Laufmaschinen, die bald mit Pedalantrieb ausgerüstet und bis zum Ende des Jahrhunderts zum heute bekannten Fahrrad weiterentwickelt wurden.

Zunächst dominierten die von der Draisine übernommenen Radgrößen um 60 cm Durchmesser, die bei relativ schwerer Bauweise dank Kreiselkräften ein balancierendes Fahren – mit kleinen Lenkkorrekturen – schon ab geringen Geschwindigkeiten ermöglichten. 

1853 baute Philipp Moritz Fischer aus Schweinfurt das erste sicher belegte Fahrrad mit Tretkurbelantrieb. Das Pedal befand sich an der Achse des Vorderrads. 

Der vermutlich vom Franzosen Pierre Michaux und seinem in die USA ausgewanderten Landsmann Pierre Lallement (US-Patent von 1866) benutzte Pedalkurbelantrieb (1861) wirkte ebenfalls direkt an der Achse des Vorderrades einer Draisine.

Das Pedalieren schräg nach vorne erzeugt Lenkkräfte und erschwert dadurch das Balancieren beim Fahren. Weil das bei der Laufmaschine nötige Ausschreiten nach vorne entfiel, konnte die Sitzposition weiter nach vorn (und höher) gelegt werden, womit die Trittkräfte mehr von oben wirken und das Vorderrad weniger eingelenkt wird. Insbesondere konnten dadurch höhere Geschwindigkeiten erreicht werden, weil mit einer Umdrehung der Pedale eine größere Distanz zurückgelegt wurde. Dabei nahm man in Kauf, dass der Boden auch mit den Zehenspitzen vom Sattel aus nicht mehr erreicht wird, dass man über eine Fußraste hinten am Rahmen auf- und absteigen muss. Dadurch ließ sich der Durchmesser des Antriebsrades auf das Zwei- bis Dreifache steigern (größere Entfaltung) und pedalierend schneller fahren. Das Hochrad war entstanden.

Weil der Fahrer sehr hoch und weit vorn – also nur wenig hinter dem vorderen Aufstandspunkt – saß, waren Stürze durch Bremsen oder kleine Bodenhindernisse häufig und führten zu relativ schweren Verletzungen, unter anderem des Kopfes. Das bezüglich Sicherheit fehlentwickelte Hochrad wurde nach der Erfindung des Niederrads bald aufgegeben.

An die Sturzgefahr beim Gebrauch eines Hochrads erinnert der englische Begriff "safety bicycle" für das spätere Niederrad.
Als Abhilfe gegen die Sturzgefahr wurden zwei Lösungen ausprobiert:


Neben dem Sicherheitsaspekt war für den Siegeszug des Niederrads auch der Umstand verantwortlich, dass die Niederräder auch hinsichtlich Geschwindigkeit den Hochrädern letztlich überlegen waren.

Die frühen Automobile sind aus der Fahrradkultur und der Fahrradtechnik der 1880er bis 1890er Jahre entstanden.

Aufgrund seines niedrigen Preises war das Fahrrad das erste massentaugliche Individualverkehrsmittel. In Europa erlangte es in der ersten Hälfte des 20. Jahrhunderts eine große Verbreitung, als es für Arbeiter erschwinglich wurde, die damit den infolge von Industrialisierung und Urbanisierung immer länger werdenden Weg zur Arbeitsstelle zurücklegten. 1936 fuhren beispielsweise in den deutschen Städten mit über 100.000 Einwohnern zwischen 43 und 61 % der Arbeiter per Fahrrad zu ihren Arbeitsstätten. Doch auch für Fahrten in den Urlaub hatte das Fahrrad eine Bedeutung. 1938 existierten bereits über 10.000 km Radwege. In der Zwischenkriegszeit avancierte das Fahrrad in Europa zum wichtigsten Individualverkehrsmittel; ab den 1950er Jahren wurde es jedoch immer stärker vom Automobil verdrängt.

In den 1960er Jahren war ein allgemeiner Wohlstand in der industrialisierten Welt entstanden, dem zufolge das Fahrrad durch Motorräder und schließlich Autos verdrängt wurde. In den ärmeren Ländern behielt das Fahrrad eine ähnlich bedeutende Rolle wie in Europa zu Beginn des 20. Jahrhunderts, wird aber auch dort bei wirtschaftlicher Entwicklung immer mehr durch das Auto ersetzt (zum Beispiel in China).

Erst nach den Ölkrisen in den 1970er Jahren und wachsendem ökologischen Bewusstsein erlangte das Fahrrad in den Industrieländern Europas wieder größere Bedeutung – vor allem im städtischen Nahverkehr – und auch öffentliches Interesse, was zur Verbesserung der Radfahrinfrastruktur (also z. B. Anlegen von Radwegen und Fahrradabstellplätzen, Einführung von Leihradsystemen) führte und den Anteil der Radfahrer am gesamten Verkehrsaufkommen (Modal Split) erhöhte. Dafür vorbildliche Städte sind Münster und Kopenhagen, in denen der Radverkehrsanteil bei über 35 % liegt. Auf der ganzen Welt wird in Form von "Critical Mass Rides" für bessere Radfahrbedingungen in den Städten demonstriert.

Ein Fahrrad kostete Mitte 2013 in Deutschland durchschnittlich 515 Euro und ca. 600 bis 650 Schweizer Franken. Im ersten Halbjahr 2013 wurden in Deutschland rund 1,65 Millionen Fahrräder produziert, 2,4 Prozent weniger als im Vorjahreszeitraum.

Mehr als 50 % der in einer Stadt zurückgelegten Wege sind weniger als fünf Kilometer lang, also mit einem Fahrrad gut zu bewältigen. Die öffentliche Förderung macht inzwischen auch eine Kombination aus individuellem Radfahren und öffentlichem Nahverkehr möglich. Wer nicht die ganze Strecke fahren möchte oder einen weiteren täglichen Weg hat, kann eine Teilstrecke zusammen mit seinem Rad in öffentlichen Nahverkehrsmitteln zurücklegen: besonders in U- und S- bzw. Stadtbahnen, aber auch Bussen. In einigen Ländern sind an Bussen Fahrradträger angebracht, in Deutschland ist, soweit gestattet, die Mitnahme im Fahrzeug üblich. Man kann das Rad aber auch in Fahrradstationen oder Fahrradparkhäusern an den Haltestellen der öffentlichen Verkehrsmittel abstellen. Öffentliche Fahrräder an Fahrradmietstationen sind ein Angebot in verschiedenen Städten, um die Nutzung zu fördern und dem Diebstahl entgegenzuwirken.

Bücher über Radreisen gab es schon Ende des 19. Jahrhunderts, aber der Radtourismus ist erst eine Folge des sich jüngst entwickelnden Massentourismus, der durch die Anlage von Radfernwegen und regionaler Radroutennetze als ökologische Urlaubsvariante gefördert wird.

In Deutschland betrug der Bestand an Fahrrädern 2010 ca. 69 Millionen Stück; jährlich werden ca. 4 Millionen Neufahrräder verkauft. In den letzten Jahren fand eine teilweise Verlagerung zu Fahrrädern mit unterstützendem Elektroantrieb (Pedelec) statt.

Fahrräder in besonderer Nutzung
Fahrräder werden auch für betriebliche Zwecke eingesetzt. Einsatzgebiete für Betriebsfahrräder sind die Industrie, Zusteller, Behörden (Polizei, Sanitätsdienst) und Dienstleistung (mobile Services, z. B. Altenpflege). Andere Bezeichnungen sind: Werksfahrrad, Industriefahrrad, Dienstfahrrad.

In Deutschland ist für die Benutzung ausschließlich auf Betriebsgelände die dortige Straßenverkehrsordnung nicht bindend. Sicherheitsmaßnahmen sind aufgrund der ermittelten Gefährdung vor Ort festzulegen (Betriebssicherheitsverordnung (BetrSichV) § 3 Abs. 3 i. V. mit Anhang 2). Die vorgesehene Betriebsweise und die betriebliche Verkehrswegsituation sind dabei wesentliche Faktoren. Es können also Unterschiede zwischen Verkehrssicherheit und Betriebssicherheit bestehen (z. B. Entbehrlichkeit von Beleuchtungseinrichtungen).

Das ökologischste Verkehrsmittel 

Die fahrende Person bewegt sich aus eigener Kraft fort, benötigt keinerlei weitere Vorrichtungen oder Treibstoffe, außer der eigenen Nahrung. Es wird der Großteil des Körpergewichtes gerollt und nicht getragen – anders als beispielsweise beim Laufen.

Das Fahrrad mit Pedalantrieb, Kette und Schaltung hat einen Wirkungsgrad von 90 bis 98 Prozent (1. Gang/direkte Übersetzung), der Gelenkbewegungs-Wirkungsgrad des Menschen liegt bei 84 Prozent. Die erforderliche Bewegungsenergie (0,6 Joule pro Gramm und Kilometer) ist bei keiner Fortbewegungsart (von Mensch oder Tier) so niedrig wie beim Fahrradfahren. Der Nettowirkungsgrad der Bewegungsart Fahrradfahren liegt bei 20–28 Prozent (ähnlich wie Laufen), Schwimmen liegt bei 3–6 Prozent. Der Rollwiderstand beträgt beim normalen Fahrradfahren (15 km/h) 61 Prozent, der Luftwiderstand wird mit 16 Prozent angegeben.

Eine ausführliche Liste findet sich im Artikel Fahrradtypen.

Zu den wesentlichen Bauteilen eines Fahrrads gehören:


Ergänzt wird die Funktionalität bei Bedarf durch folgende Bauelemente:


Der Rahmen ist vergleichbar mit dem Fahrgestell bei Fahrzeugen anderer Art. Er trägt den Fahrer und verbindet alle anderen Bauteile fest oder beweglich miteinander. Im Steuerrohr ist über den Steuersatz die Gabel und der Vorbau mit der Lenkstange drehbar gegen den Rahmen montiert. Das Sitzrohr trägt die Sattelstütze mit dem Sattel. Das Tretlagergehäuse ist in der Regel die Verbindungsstelle zwischen Sattelrohr, Unterrohr und Hinterstrebe. In die Hinterstreben und die Vorderradgabel sind die Achsen der beiden Laufräder eingespannt. Zwischen dem Kettenblatt des Tretlagers und dem Zahnkranz der Hinterradnabe sorgt eine Rollenkette für die Kraftübertragung von den Tretkurbeln auf das Hinterrad. Bremsen sind heute meistens Felgenbremsen, die an der Vorderradgabel bzw. an der oberen Hinterbaustrebe montiert sind. Je nach Fahrradtyp sind auch Scheibenbremsen weit verbreitet. Das Hinterrad hatte früher eine Rücktrittbremse, die durch Zurücktreten der Tretkurbeln auslösten. Die Rücktrittbremse gilt heute als technisch veraltet und unsicher, kaum wirkungsvoll und ist in vielen Fällen schwer zu dosieren. Bei langen Bergabfahrten kann die Nabe heiß laufen, was zu Lagerschäden durch geschmolzenes, herauslaufendes Schmierfett führen kann. Seit den 1990er Jahren sind Federungs- und Dämpfungselemente für Vorder- und Hinterrad bzw. an der Sattelstütze stärker verbreitet.

Die moderne Fahrradtechnik lässt Reparaturen durch den Benutzer immer weniger zu. Zur Montage vieler Bauteile sind Detailwissen und Sonderwerkzeuge erforderlich. Seit etwa 1990 werden praktisch alle Fahrräder mit Schaltungen ausgestattet angeboten. Reparaturen an Nabenschaltungen sind für Laien nahezu unmöglich, die Einstellung von Kettenschaltungen ist in den meisten Fällen nicht trivial. Seit einigen Jahren gibt es eine Renaissance des Eingangrads, meist als "Singlespeed" bezeichnet. Hierbei entfällt ein großer Teil sonst erforderlicher Wartungsarbeiten, die Mehrzahl der Käufer legt jedoch nach wie vor auf ein Fahrrad mit möglichst vielen Gängen wert. Die immer weiter verbreiteten Federelemente verlangen kenntnisreiche Wartung. Viele Teile an modernen Fahrrädern unterliegen heute weit größerem Verschleiß als vor wenigen Jahrzehnten, was eine regelmäßige Durchsicht wie bei PKW oder Motorrad erfordert. Manche Getriebenaben verlangen jährlichen Ölwechsel, hochwertige Federgabeln verlangen regelmäßige Inspektion.

Wagen wurden ursprünglich von mitlaufenden Tieren oder Menschen gezogen oder geschoben. Erst im 17. Jahrhundert scheint es von mitfahrenden Menschen betriebene Wagen gegeben zu haben. Sie wurden vorwiegend für Repräsentationszwecke (Triumphwagen) benutzt. Aber auch der querschnittsgelähmte Uhrmacher Stephan Farfler hat sich zu dieser Zeit ein dreirädriges Gefährt, auf dem er sich sitzend fortbewegen konnte, gebaut. Er trieb es mit Handkurbeln an.

Im 18. Jahrhundert gab es in herrschaftlichen Parks Vierradwagen, die über Pedale vom mitfahrenden Personal angetrieben wurden. Diese Muskelkraftwagen erforderten wegen ihrer großen Masse viel Kraft, so dass sie sich nicht für den allgemeinen Gebrauch durchsetzten. Das änderte sich erst mit der Erfindung des einspurigen Laufrads durch Karl Drais. Durch die viel kleinere Masse des späteren Fahrrads und die Beschränkung auf die eigene Fortbewegung des Fahrers genügte dessen Arbeitsvermögen für eine relativ ermüdungsarme Alternative zum Gehen oder Laufen.

Den Durchbruch zu einem Fahrzeug für einen Fahrer, das dieser selbst antreiben kann, schaffte das 1817 in Mannheim vom Karlsruher Karl Drais (der geborene Freiherr und überzeugte Demokrat hat 1849 seinen Adelstitel abgelegt) erfundene einspurige Laufrad. Das Zweiradprinzip mit lenkbarem Vorderrad war wesentlicher Teil dieser Erfindung. Drais nannte sein aus Holz gefertigtes Gefährt „Laufmaschine“, in der Öffentlichkeit hieß es bald „Draisine“. Die Draisine war das erste individuale Landverkehrsmittel. Der Fahrer saß zwischen den Rädern und stieß sich mit den Füßen am Boden ab.

Einer wissenschaftlichen Theorie zufolge geht der Impuls zur Erfindung des Fahrrades durch Drais möglicherweise auf den Hafermangel und das folgende Pferdesterben infolge des Ausbruchs des Vulkans Tambora und des dadurch ausgelösten Jahres ohne Sommer 1816 zurück.

Eine Voraussetzung für die Nutzbarkeit der Laufmaschine waren Straßen mit ausreichend fester planierter Oberfläche (Makadam), die es im Gebiet des Deutschen Bundes damals erst im äußersten Südwesten gab.

Infolge des lenkbaren Vorderrads kann das rollende Laufrad auch ohne Kontakt der Füße zum Boden im Gleichgewicht gehalten werden. Drais nutzte auch aus, dass durch die Kreiselkräfte der Räder die Lage des Zweirades stabilisiert wurde. Allerdings musste der Fahrer erst das ungewohnte Balancieren im Zusammenspiel von Laufen und Lenken erlernen.

Schon kurz darauf wurden in England die ersten, teilweise eisernen Laufräder oder Velozipede gebaut, die sich den Spitznamen "hobby horse" (Steckenpferd) erwarben. 1819 gab es in Ipswich erste Rennen; in Deutschland wurde erst 1829 aus München davon berichtet.

Heute erfreut sich das Zweirad ohne Pedalantrieb als Kinderlaufrad neuer Beliebtheit. In den 2000er Jahren haben alle größeren Kinderfahrradhersteller Kinderlaufräder in ihr Programm aufgenommen.

Eine Zwischenstufe zum späteren indirekten Antrieb über die Räder ist der Schubstockantrieb beim Künzelsauer Schubstockrad von 1850. Mit zwei parallelen seitlichen Stöcken stieß sich der Fahrer vom Boden ab. Seine Füße benutzte er zum Lenken des Vorderrades, während er die Schubstöcke über einen Mechanismus mit Armen und Händen bewegte.

Der erste indirekte Antrieb erfolgte mittels Tretkurbeln am Vorderrad. Sein Erfinder ist umstritten: entweder Pierre Michaux oder Pierre Lallement. Während Lallement 1866 ein US-Patent darauf erhielt, hat Michaux das Antriebsprinzip angeblich schon 1861 vom Schleifstein übernommen. Michaux und die Fabrikantensöhne Olivier vermarkteten das Tretkurbelrad bei ständig steigender Nachfrage in Frankreich. Im übrigen Europa erregte es erst Aufmerksamkeit als Michaux auf der Weltausstellung 1867 in Paris dafür warb. Der Antrieb funktioniert über starr an der Vorderradachse angebrachte Pedalkurbeln, wodurch bei einer Umdrehung der Pedale der zurückgelegte Weg gleich dem Umfang des Vorderrads ist.

Um mit den direkt aufs angetriebene Vorderrad wirkenden Tretkurbeln höhere Geschwindigkeiten fahren zu können, vergrößerte man dieses. So entstand 1870 das Hochrad. In vielen Städten wurde das Hochradfahren wegen seiner möglichen großen Sturzhöhe sogleich verboten, in Köln war es noch bis 1894 erlaubt.

Das Hochradfahren verlangte deutlich mehr Geschick, besonders beim Auf- und Absteigen. Durch den hohen Schwerpunkt (der Sattel befand sich rund 1,5 Meter über dem Boden und nur wenig hinter der Vorderachse) drohte Hochradfahrern bei Bremsmanövern oder Straßenunebenheiten die Gefahr, sich zu überschlagen. Tödliche Kopfstürze waren nicht selten; das Hochrad war damit eine Sackgasse in der Entwicklung des Fahrrades und wurde nicht weiter entwickelt.

Der Hinterradantrieb wurde für Fahrräder mit „normal“ großen Rädern eingeführt (Michaux-Typ).

Die ersten Antriebe hatten Stangen, die von Tretkurbeln an der Vorderradachse zu Hinterradkurbeln führten. Für standfeste Drei- und Vierradwagen gab es solche Antriebe schon seit 1814, zum Beispiel denjenigen von Franz Kurtz. Ein britischer Getreidehändler datierte das Stangenveloziped von Thomas McCall 1869 in einer Pressekampagne in den 1890er Jahren auf 1839 vor und schob es einem Verwandten unter, dem schottischen Schmied Kirkpatrick Macmillan. In Deutschland gab es z. B. 1870 ein Patent von Johann Friedrich Trefz für den Stangenantrieb.
Gemeinsam mit dem Kettenantrieb wurde schließlich die Anordnung der Tretkurbeln zwischen den beiden Rädern eingeführt. Durch verschieden große Zahnkränze an Kurbel und Radachse wurde auch das Prinzip der Übersetzung für den Fahrradantrieb übernommen. So konnte mit einer Kurbelumdrehung das Laufrad je nach Übersetzungsverhältnis gleich mehrfach gedreht werden. Diese Neuerung führte zuerst zum „Kangaroo“, einem gemäßigten Hochrad mit beidseitigem Kettenantrieb am Vorderrad. Doch erst der 1878 eingeführte einseitige Kettenantrieb des Hinterrades konnte sich wirklich durchsetzen – die Konstruktion war einfacher und stabiler, das Rad wegen der Entkoppelung von Antrieb und Lenkung leichter zu fahren, und die Sitzposition zwischen Vorder- und Hinterrad gewährleistete ein wesentlich sichereres Fahrverhalten.

Die Antriebsübertragung von den Tretkurbeln zwischen den beiden Rädern zum Hinterrad mittels einer Welle wird auch heute noch gelegentlich verwendet. An beiden Enden der Welle befindet sich ein Kegelrad je eines Kegelradgetriebes. Dieser Antrieb wird gewöhnlich als Kardanantrieb bezeichnet, obwohl in ihm keine Kardangelenke vorkommen.

Seit Anfang der 1980er Jahre gibt es Fahrräder mit Zahnriemenantrieb. Dem Vorteil des leichten, sauberen, wartungsarmen und leiseren Laufes steht die Empfindlichkeit gegen Fremdkörper und ungenaues Ausrichten (Flucht) der vorderen gegen die hintere Zahnriemenscheibe gegenüber. Weil der Riemen nicht wie eine Kette teilbar ist, wird der Hinterbau des Fahrradrahmens zum Montieren geöffnet (Spezialanfertigung). Schaltungen mit Wechsel auf andere Riemenräder oder -ritzel (analog Kettenschaltungen) gibt es wegen der großen Riemenbreite und weil der Riemenlauf zwischen vorn und hinten fluchtend sein muss, nicht. Verwendet werden Nabenschaltungen oder Tretlagerschaltungen.
Eine der Kettenschaltung analoge, kleinstufige Schaltung wurde mit Hilfe eines „spreizbaren Ritzels“ (mehrere radial ausfahrbare Stäbe mit je einem kleinen Ritzel am äußeren Ende) verwirklicht, kam aber nicht in den Handel.

Bekanntester Vertreter dieser Bauform war das von John Kemp Starley seit 1884 angebotene „Rover Safety Bicycle“. Es wurde „Safety“ (deutsch Sicherheit) genannt, weil es aufgrund der niedrigeren Sitzposition des Fahrers sicherer war als das Hochrad. Zudem war es schneller und auch bequemer als das bis dahin etablierte Hochrad.

Nach der Nähmaschine wurde das Fahrrad in dieser Bauform zum zweiten technischen Serienprodukt.

Seit 1884 waren in Deutschland auch die ersten brauchbaren Kugellager der von Friedrich Fischer gegründeten „Velociped-Gußstahlkugelfabrik“ erhältlich, die den Reibungswiderstand in Naben und Tretlager drastisch verringerten.

Die Gebrüder Ljungström waren sehr kreative und typische Erfinderpersönlichkeiten des 19. Jahrhunderts. Sie erfanden nicht nur den nach ihnen benannten Turbinentyp, sondern auch eine frühe Form des Fahrrades. Es hatte bereits die heute bekannte Rahmenform, wurde jedoch völlig anders angetrieben. Die Gebrüder verwendeten Klavierdraht und Exzenter statt Fahrradkette und Hinterradritzel. Ihr Svea-Fahrrad mit Freilauf wurde ab 1892 in Serie hergestellt und konnte sich für etwa zehn Jahre am Markt behaupten, bis die technischen Probleme bei der Herstellung von Fahrradketten überwunden waren.
In der Geschichte des Fahrrades wurden immer wieder Alternativen zum Kettenantrieb erfunden und erprobt – von Kardanwellen über Riemenantriebe bis zu hoch komplizierten Hebelmechanismen. Doch keine dieser Entwicklungen konnte bisher langfristig mit der Kette konkurrieren.

Um 1880 kam der Diamantrahmen auf, eine Fachwerkkonstruktion aus einem Trapez für den Hauptteil und einem doppelten Dreieck für den Hinterbau („Diamant“ ist eine falsche Übersetzung des englischen "diamond", was Raute bedeutet und die Rahmenform annähernd beschreibt). Bei manchen heutigen Fahrrädern berühren Ober- und Unterrohr an derselben Stelle den Steuerkopf, sodass das Trapez auch hier zum Dreieck geworden ist.

Vor dem Diamantrahmen (diamond) war der Kreuzrahmen üblich, mit dem die Entwicklung des Niederrads begann. Er bestand im Wesentlichen aus einer Strebe vom Steuerkopf zur Hinterachse (im hinteren Teil gegabelt) und einer zweiten, sie kreuzenden Strebe vom Sattel zum Tretlager. Beim Diamantrahmen werden die Streben fast nur auf Zug bzw. Druck beansprucht und kaum auf Biegung – deshalb ist er wesentlich stabiler als ein Kreuzrahmen. Heute wird der Kreuzrahmen in vollgefederten Fahrrädern verwendet.

Die Rahmen früherer Fahrräder waren aus massivem Stahl oder Hohlstahl gefertigt und entsprechend schwer. 1885 ließen sich die Brüder Mannesmann ein Verfahren zur Erzeugung nahtloser Stahlrohre patentieren. Mit diesem seit 1890 erhältlichen Stahlrohr war schließlich das Rahmenmaterial gefunden, das bis vor kurzem im hochwertigen Fahrradbau dominierte und inzwischen teilweise durch Aluminium und im Radrennsport auch durch kohlenstofffaserverstärkten Kunststoff (umgangssprachlich Carbon) verdrängt wird. In der Massenproduktion waren allerdings die billigeren, mit Längsnaht geschweißten Stahlrohre üblich.

Das aus Stahlrohr gefertigte „Rover“ mit Diamantrahmen wurde zum Prototyp des modernen Fahrrads. Im Polnischen wird das Fahrrad heute noch als Rover bezeichnet.

Die qualitativ hochwertigsten gezogenen Stahlrohre für Fahrradrahmen werden von den Firmen Columbus und Reynolds hergestellt. Sie tragen die Bezeichnungen Columbus SLX bzw. Reynolds 531.

Eine etwas andere Rahmengeometrie ist bei sogenannten "Damenrädern" üblich. Das Oberrohr verläuft – statt vom Steuerrohr direkt waagerecht zum oberen Ende des Sitzrohres – hier teilweise gekrümmt und parallel zum Unterrohr nach unten, wo es das Sitzrohr oberhalb des Tretlagers trifft. In sportlicheren Versionen ist es ungekrümmt und verbindet ungefähr die Mitte des Sitzrohres mit dem Steuerrohr.

Solche Rahmen sind im Prinzip weniger stabil. Sie sind weniger biegesteif in der horizontalen Achse und weniger torsionssteif insgesamt.

Entwickelt wurden Damenräder nicht aus anatomischen Gründen, sondern um Frauen, die einen Rock tragen, das Aufsteigen und Fahren zu ermöglichen. Erst seit etwa 1920 begann das Tragen von Frauenhosen gesellschaftsfähig zu werden.

Der Fahrradsattel ist der Teil des Fahrrades, der dem Fahrer Halt gibt und ihm beim Radfahren das Sitzen in verschiedenen Positionen ermöglicht. Die Form hängt vom Verwendungszweck des Fahrrades sowie von körperlichen Merkmalen des Fahrers ab. Die Technologie bei Fahrradsätteln hat sich in den letzten Jahren stark verändert. In den Anfängen des Fahrrads gab es Sattelmodelle, die rein aus Holz oder gar aus Metall bestanden. Diese wurden aber schon früh durch Ledersättel, die dem Pferdesattel entlehnt waren, ersetzt. Seit Anfang der 1990er Jahre wird zunehmend Kunststoff verwendet. Heute sind Plastiksättel mit Polsterung und Kunstlederbezug (PVC) die gebräuchlichste Bauform. Weltmarktführer bei Fahrradsätteln ist seit etwa 2000 die italienische Firma Selle Royal.

1888 erfand der schottische Tierarzt John Boyd Dunlop zum zweiten Mal nach Robert William Thomson den Luftreifen, der erstmals eine praktikable Dämpfung und zuverlässigere Bodenhaftung ermöglichte. Bis dahin waren Fahrräder mit Eisen- oder seit 1865 mit Vollgummireifen ausgestattet.

Den ersten abnehmbaren Luftreifen erfanden die Brüder Michelin 1890 in Frankreich. Der Luftreifen stieß anfangs auf große Skepsis; den Durchbruch brachten erst Erfolge im Rennsport ("siehe auch:" Fahrradventil, Fahrradbereifung). Als während des Ersten Weltkrieges eine Knappheit an Kautschuk herrschte, wurden „Notmäntel“ als Nachrüstsatz entwickelt und in Serie hergestellt, bei denen das Rad auf Schraubenfedern lief.

Der von A. P. Morrow 1889 in den Vereinigten Staaten patentierte "Freilauf" war unter Radfahrern zunächst sehr umstritten. Die Gegner des Freilaufs im Radsport hatten ebenso gewichtige Argumente wie dessen Befürworter. Der in den USA schon früher entschiedene Streit wurde in Deutschland erst nach 1900 durch die erfolgreiche Markteinführung der Torpedo-Freilaufnabe von Fichtel & Sachs mit integrierter Rücktrittbremse beendet.

1907 wurde die erste 2-Gang-"Nabenschaltung" nach einem Patent der Wanderer-Werke von Fichtel & Sachs auf den deutschen Markt gebracht. Sie besaß ein Planetengetriebe und ebenfalls eine Rücktrittbremse.

Im Gegensatz zu einer "Kettenschaltung" zeichnet sich die Nabenschaltung durch den geringen Wartungsaufwand und damit hohe Alltagstauglichkeit aus. Nachteilig ist das höhere Gewicht und der im Vergleich zu einer Kettenschaltung etwas geringere Wirkungsgrad – mit Ausnahme des direkten Ganges, bei dem die Kraftübertragung ohne Getriebeeinsatz erfolgt.

Die heute sehr verbreitete Kettenschaltung stammt von den Gebrüdern Nieddu. Deren Schaltung „Vittoria Margherita“ wurde 1935 von Gino Bartali als erstem Profi gefahren. Nach der damals recht bekannten französischen Schaltung „Super Champion“ (1937) erschien 1946 die erste Schaltung von Campagnolo, die im Gegensatz zu ihren Vorgängern weltweite Verbreitung fand.

Die Nabenschaltungen wurden kontinuierlich weiterentwickelt. Neuere Entwicklungen gibt es u. a. von den Firmen Shimano mit der 8-Gang-Nabenschaltung Nexus oder der etwas länger übersetzten 9-Gang-Nabenschaltung von SRAM, sowie der Firma Rohloff mit der 14-Gang-Nabenschaltung Speedhub 500/14, in der sich drei Planetengetriebe in einer Nabe befinden. Die von SRAM immer noch produzierten 3- und 5-Gang-Naben erfreuen sich weiterhin großer Beliebtheit, besonders bei Hollandrädern. Die flache Topographie der Niederlande macht niedrige Übersetzungen entbehrlich.

Die zurzeit einzige Fahrradnabe mit "stufenlosem Planetengetriebe" ist die NuVinci N360. Ihr Gewicht liegt bei 2,5 kg, die Übersetzungsbandbreite beträgt 360 %.

Als Exoten sollen hier auch noch die "Tretlagerschaltungen" erwähnt werden, beispielsweise die historische Mutaped-Tretlagerschaltung. Die Tretlager-2-Gang-Schaltung der Schweizer Firma Schlumpf lässt sich mit allen Nabenschaltungen kombinieren.

Die im Zusammenhang mit dem Fahrrad gemachten Erfindungen waren wegbereitend für die Entwicklung des Motorrads und des Automobils um 1900, ebenso wie der Kampf gegen Fahrverbote der Obrigkeit.

Die weitere Entwicklung des Fahrrads orientierte sich am Konzept des Niederrads – lediglich mit Varianten bei Konstruktion und Materialien. Zunächst wurden größere Fortschritte bei Gangschaltung und Bremsen gemacht. Entsprechende Impulse gingen von der Entwicklung des Mountain Bikes (MTB) in den USA aus. Seit den 1990er-Jahren werden Fahrräder zunehmend mit einer Fahrradfederung ausgestattet. Besonders in den 1980er- und 1990er-Jahren wurde viel mit alternativen Bauformen experimentiert, die sich aber nicht durchgesetzt haben. Das wiederholt sich momentan teilweise im Bereich der Mountainbikes, wo immer wieder neue ungewöhnliche Rahmenkonstruktionen zu sehen sind. Im Straßen-, Bahn- und Crossradrennsport bleibt jedoch der Diamantrahmen Standard.

Mit der Umweltbewegung sind seit den 1980er-Jahren Sonderformen wie Dreiräder, Liegeräder und Velomobile wiederentdeckt und weiterentwickelt worden, werden aber vom Fahrradhandel nicht so unterstützt wie die im Radrennsport gebräuchlichen Formen. Für diesen Sport wurden solche Räder vom Welt-Radsport-Verband UCI bereits in den 1920ern verboten.

Mit Blick auf die Gesundheitsförderung und die ganzheitliche Betätigung des Körpers wurde das Cavallo entwickelt. Es ist ein Fahrradtyp, der mit seinem Antrieb ein neues Konzept verwirklichen sollte. Das Fahrrad wurde nicht mit den Füßen über Pedale, sondern durch Körperbewegungen über eine Konstruktion mit vier Gelenken des Rahmens und den Fahrradsattel angetrieben. Dabei wirkten zwei Rahmenrohre als Pleuel auf die Kurbelarme des Antriebszahnrades. Die für die Fortbewegung erforderlichen Bewegungsabläufe erinnerten entfernt an das Reiten eines Pferdes, was dem Fahrzeug seinen Namen verlieh "(Cavallo" ist Italienisch für "Pferd)."

Heute werden Fahrradrahmen zum Großteil aus Stahl und Aluminium hergestellt, häufig mit größerem Rohrdurchmesser. Im Radsport werden auch Rahmen aus Carbon eingesetzt.

Meilensteine waren um die Jahrtausendwende die Erfindung leichtläufiger Nabendynamos sowie von Rücklichtern mit Leuchtdioden. Einige Jahre später kamen leistungsstarke Scheinwerfer sowie Kondensatoren hinzu, die während der Fahrt Energie speichern und im Stillstand mehrere Minuten die Lampen leuchten lassen. Diese Erfindungen ermöglichten bei sachgerechter Montage und Verkabelung erstmals eine zuverlässige, nahezu wartungsfreie und ständig betriebsbereite Lichtanlage.

Nach dem "Wiener Übereinkommen über den Straßenverkehr vom 8. November 1968" sind Fahrräder im Sinne des Straßenverkehrs "„jedes Fahrzeug mit wenigstens zwei Rädern, das ausschließlich durch die Muskelkraft auf ihm befindlicher Personen, insbesondere mit Hilfe von Pedalen oder Handkurbeln, angetrieben wird“". Für den internationalen Verkehr sind nach Art. 44 des Übereinkommens eine Bremsanlage, eine Klingel und eine Beleuchtungseinrichtung vorgeschrieben. Einräder gelten somit nicht als Fahrrad, sondern rechtlich gesehen als Spielzeug.

Fahrräder, die am öffentlichen Straßenverkehr teilnehmen sollen, müssen gesetzliche Mindestanforderungen erfüllen. Häufig werden bei Fahrradhändlern allerdings auch Räder verkauft, die diesen Standards nicht entsprechen. Diese müssen nachgerüstet werden, bevor sie auf Straßen oder Radwegen bewegt werden. Für die Einhaltung der Vorschriften ist generell der Fahrer zuständig. Verstöße werden als Ordnungswidrigkeiten geahndet.

In vielen Ländern gibt es spezifische Regelungen zu den technischen Anforderungen zur Verwendung eines Fahrrades im öffentlichen Straßenverkehr. Diese ergänzen in der Regel die rechtlichen Bestimmungen zum Radverkehr. Die rechtlich festgesetzten technischen Anforderungen unterscheiden sich von Land zu Land, so auch in den deutschsprachigen Ländern:

In Deutschland regelt die Straßenverkehrszulassungsordnung (StVZO) die Betriebsvorschriften für Fahrräder.


In Österreich existiert in Ergänzung zur StVO die Fahrradverordnung, in der vorgeschrieben ist, welche technischen Anforderungen an Fahrräder gestellt werden (das „Inverkehrbringen“), um sie im öffentlichen Verkehr zu bewegen. Zu beachten ist, dass mit der Fahrradverordnung nur der Betrieb der Fahrräder geregelt ist, nicht jedoch mit welcher Mindestausstattung die Fahrräder von den Händlern verkauft werden müssen: Am 9. Oktober 2013 ist die erste Novelle der Fahrradverordnung in Kraft getreten.


Wichtigste Ergänzungen durch Novellierung der Fahrradverordnung (9. Oktober 2013)


In der Schweiz finden sich die Betriebsvorschriften für Fahrräder in der Verordnung über die technischen Anforderungen an Strassenfahrzeuge (VTS).


Euro 4, geltend für Neuzulassungen ab 2016-01:

Einige Organisationen, die sich für das Fahrrad als Verkehrsmittel engagieren, sind:






</doc>
<doc id="13329" url="https://de.wikipedia.org/wiki?curid=13329" title="Auxiliartruppen">
Auxiliartruppen

Die Auxiliartruppen (lateinisch "auxilium" „Hilfe“) waren Bestandteil der römischen Armee, die aus verbündeten Völkern oder den Bewohnern (ohne Bürgerrecht= "peregrini") der Grenzprovinzen rekrutiert wurden.

Hilfstruppen, die bei Bedarf angeworben oder von den Verbündeten eingefordert wurden, gab es bereits zur Zeit der römischen Republik. Eine halbwegs einheitliche Gliederung und vor allem Eingliederung der Auxiliartruppen erfolgte aber erst durch die Reformen des Kaisers Augustus, der um Christi Geburt ein stehendes Heer schuf.

Die Auxiliartruppen stellten laut Tacitus etwa die Hälfte der regulären römischen Streitkräfte, also wohl mindestens 150.000 Soldaten. Exakte Angaben fehlen. Die Offiziere waren in der Regel Römer; als Mannschaften dienten vorrangig Männer, die frei geboren waren, aber nicht das römische Bürgerrecht besaßen, wobei es vorkommen konnte, dass einzelne Einheiten als besondere Auszeichnung geschlossen das Bürgerrecht erhielten. Die reguläre Dienstzeit lag bei 25 Jahren (in der Flotte bei 26 Jahren). Die Soldaten erhielten zwar einen geringeren Sold als die Legionäre (die bereits römische Bürger waren), aber dafür nach dem ehrenvollen Abschied das Bürgerrecht (dokumentiert in den Militärdiplomen) für sich und ihre Nachkommen. Dies war ein Hauptanreiz für einen Eintritt in die Truppe. Teilweise erfolgte auch eine Befreiung von Abgaben und öffentlichen Diensten für die Veteranen.

Die Auxiliartruppen ergänzten die schwere Infanterie der eigentlichen Legionstruppen. Insbesondere die Reiterei war in den Legionen zahlenmäßig viel zu schwach um bei Gefechten eingesetzt zu werden. Ebenso wurde die Masse der Spezialtruppen, wie Bogenschützen oder Schleuderer, von den Auxiliareinheiten gestellt. Daneben wurden die Auxiliareinheiten auch als Kontertruppen eingesetzt, die die unterschiedlichen Feinde mit ihren eigenen Taktiken bekämpfen sollten. So gibt es Berichte über Kameltruppen im Kampf gegen arabische Reiter. Insgesamt setzte aber vor allem bei den Fußsoldaten bald eine Vereinheitlichung ein, so dass die Mehrzahl der Hilfstruppen-Infanterie seit dem späten 1. Jahrhundert in Hinblick auf ihre Ausrüstung kaum noch von den Legionären zu unterscheiden war. Die Offiziere waren, wie erwähnt, fast ausnahmslos römische Bürger, meist abkommandierte Legionsoffiziere, die bei Versetzung in eine Auxiliareinheit jeweils eine Rangstufe aufstiegen.

Neben den zumindest anfänglichen waffentechnischen Unterschieden zu regulären Legionstruppen war auch der Einsatz außerhalb größerer Kriege unterschiedlich zur Legion: Auxiliareinheiten wurden in wesentlich kleineren Verbänden bzw. Einheiten in der Fläche zur Befriedung und Überwachung der Provinzen eingesetzt und in der Regel in den entlang der Grenzen angeordneten Kastellen stationiert. Somit lag die alltägliche Überwachung der Grenzen in Friedenszeiten wesentlich in den Händen von Hilfstruppen. Die Legionen dagegen waren in wesentlich stärkerer Konzentration (ein oder zwei Legionen in einem Legionslager) für entscheidende Aufgaben zuständig und hatten ihre Lager meist im Hinterland.

Über die Auxiliareinheiten erfolgte auch eine massive Romanisierung der Provinzen, da sich diese bald, bis auf wenige Ausnahmen, nicht weiter aus den Ursprungsgebieten der ersten Mitglieder rekrutierten, sondern vor Ort, wodurch die ursprüngliche Bindung an eine Ethnie rasch verloren ging. Eine Auxiliareinheit konnte also weiterhin "Parthica" oder "Helvetica" heißen, ohne dass dieser Name noch viel mehr als eine Erinnerung an ihre Anfänge war. Dabei blieben als kleinster gemeinsamer Nenner nur das Latein und die römische Lebensweise, zumal die Veteranen der Hilfstruppen nun das römische Bürgerrecht besaßen. Ihre Nachkommen traten daher nicht selten als Legionäre in die Armee ein.


Im Gegensatz zur Legion, in der die Kohorten in der Regel durch Centurionen kommandiert wurden, standen den Auxiliareinheiten meistens Präfekten, mitunter auch Tribunen, vor. In den Alen, die keine Centurien kannten, entsprach der Decurio als Kommandeur einer Turma dem Rang eines Centurio. Die einzelnen Einheiten unterschieden sich nicht nur nach Bewaffnung und Herkunft, sondern auch nach Größe, Prestige und Besoldung.

In der hohen Kaiserzeit wurden die Offizierstellen in den Auxiliareinheiten zunehmend eine Domäne des Ritterstandes. Bereits ab Claudius (41 bis 54 n. Chr.) gab es dort eine standardisierte ritterliche Laufbahn über drei Stufen ("tres militiae"), wie sie in den Legionen schon seit Augustus zu finden war. Hier stand als erstes das Kommando ("cohors quingenaria") über eine Kohorte Infanterie mit 500 Soldaten. Diese Soldaten hatten nicht das römische Bürgerrecht inne. Nach dieser ersten Führungserfahrung konnte entweder der Dienst in der Legion als "tribunus angusticlavius", oder der Befehl über eine 500 Soldaten umfassende Kohorte römischer Bürger (die stets von einem Tribunen geführt wurden), oder der Befehl über eine 1.000 Mann starke Kohorte ("cohors miliaria") folgen. Anschließend erhielt der Offizier eine Alenpräfektur, also das Kommando über eine Kavallerieeinheit von etwa 500 Mann ("ala quingenaria"). In einigen Fällen konnte noch das Kommando über eine Kavallerieeinheit von knapp 1000 Reitern ("ala miliaria") folgen. Dieser Posten wurde mit Offizieren im Senatorsrang besetzt.

In der Zeit der Römischen Republik lagerten die Auxiliareinheiten vorwiegend in Zelten oder Flechtwerk-Unterkünften, später kamen Fachwerkbauten auf, im 2./3. Jahrhundert gemauerte Kasernen, mit 6 oder 8 Mann belegt (gelegentlich auch mehr). Typisch für "berittene" Einheiten waren Kasernen-Stuben mit Durchgang zu unmittelbar anschließenden Pferdeställen.




Die Anzahl der Alen und Cohorten in Niedergermanien (Germania inferior)

vor 9 n. Chr.:

14 n. Chr.:

15 n. Chr.:

16 n. Chr.:

Germanicus führte im Feldzug gegen die Marser (14 n. Chr.) neben 12.000 Legionssoldaten 26 "sociae cohortes" und 8 "equitum alae". Erstere unterteilten sich in ca. 16 reguläre Cohorten ("expeditae cohortes", "auxiliariae cohortes") und etwa 10 Cohorten irreguläre Volksaufgebote ("ceteri sociorum", "leves cohortes"). Die Kavallerie bestand fast ausschließlich und die Hilfstruppeninfanterie zum großen Teil aus gallischen und germanischen Truppen. In den Quellen werden als Auxiliarverbände des Germanicus erwähnt: c"ohortes Raetorum et cohortes Vindelicorum", Batavi, Volksaufgebote der Chauken, "cohortes Gallicae", sowie "tumultiariae catervae Germanorum cis Rhenum colentium", die aus Niedergermanien und der nördlichen "Belgica" rekrutiert wurden.

Wenn man mit mehr oder weniger Wahrscheinlichkeit auch mit der Existenz einiger erst später, in der claudisch-neronischen Zeit, belegter Truppen bereits während der Regierungszeit des Tiberius rechnet, hätte es in der Zeit des Tiberius zum Beispiel am Niederrhein mindestens 7-8 "alae" ("ala Batavorum", "ala Canninefatium", "ala Gallorum Picentiana", "ala Pomponiani", "ala praetoria", "ala Treverorum", "ala I Tungrorum Frontoniana", "ala Parthorum") und mindestens etwa 20 "cohortes" ("cohors V Asturum", 9 "cohortes Batavorum", "cohors VIII Breucorum", "cohors Canninefatium", "cohortes Gallorum", "cohors Germanorum", "cohors III Lusitanorum", "cohors Silaucensium", mindestens 2 "cohortes Tungrorum", "cohors Ubiorum equitata") gegeben.

Als Kaiser Caracalla 212 allen freien Reichsbewohnern das römische Bürgerrecht verlieh, traten fortan vor allem Barbaren von jenseits der Reichsgrenzen in die Hilfstruppen ein. Die Militärreformen, mit denen man dann auf die „Reichskrise“ zu reagieren suchte, veränderten den Aufbau der kaiserlichen Armee. Seit etwa 300 wurde die Überwachung der Grenzen nicht mehr den Auxilien, sondern den "limitanei/riparenses" übertragen, die diese Aufgabe während der ganzen Spätantike ausfüllten.

Reichsfremde Soldaten dienten seit dem 4. Jahrhundert entweder Seite an Seite mit Römern in der regulären Armee, wo sie nun mitunter Karriere machten und bis in hohe Offiziersstellen aufrücken konnten (siehe auch Heermeister), oder aber als durch einen gesonderten Bündnisvertrag ("foedus") an Rom gebundene Söldner ("foederati") unter eigenen Anführern. Die "Auxilia" hingegen verschwinden um diese Zeit weitgehend aus den Quellen. Wenn fortan vereinzelt noch von "auxilia" die Rede ist, wird diese Begriff nicht mehr in der speziellen Bedeutung der Prinzipatsepoche verwendet.




</doc>
<doc id="13330" url="https://de.wikipedia.org/wiki?curid=13330" title="Friedrich Siemens">
Friedrich Siemens

Friedrich Siemens (* 8. Dezember 1826 in Menzendorf; † 24. Mai 1904 in Dresden, vollständiger Name: "Friedrich August Siemens") war ein deutscher Unternehmer aus der Familie Siemens und ein Bruder von Werner von Siemens.

Als elftes Kind des Gutspächters Christian Ferdinand Siemens (1787–1840) und dessen Ehefrau Eleonore Henriette Deichmann (1792–1839) wurde Friedrich als sogenanntes 7-Monats-Kind geboren, darum war seine geistige und körperliche Entwicklung sehr verzögert. Da die Eltern sehr früh starben, wuchs Friedrich bei einem Onkel auf.

Am 24. Januar 1864 heiratete Friedrich Siemens Elise Witthauer, Tochter des Forst- und Domäneinspektors Johann Georg Witthauer in Israelsdorf bei Lübeck (* ebenda 9. März 1843; † 22. Juli 1919 Dresden). Das Ehepaar hatte sechs Kinder:

Im Gegensatz zu seinem Bruder wurde er nicht geadelt und trägt damit auch nicht den Namen "von Siemens".

Siemens starb 1904 in Dresden. Sein Grab befindet sich auf dem Neuen Annenfriedhof in Dresden-Löbtau. Die Grabfigur schuf Bildhauer Johannes Schilling.

Mit 15 Jahren entschied er sich, als Schiffsjunge anzuheuern. Später war Friedrich Siemens zunächst in dem Unternehmen seines Bruders in Großbritannien tätig. Er entwickelte 1856 einen Regenerativschmelzofen mit Gasbefeuerung (siehe auch Siemens-Martin-Ofen). Für seine Erfindung erhielt er am 2. Dezember 1856 das britische Patent No. 2861 mit dem Titel "„Improved Arrangement of Furnaces, which improvements are applicable in all cases where great heat is required“".

1856 gründete er eine Ofenbaufirma in Dresden, die spätere Friedrich Siemens Industrieofenbau (FSI). 1862 gründete sein Bruder Hans Siemens zur kommerziellen Auswertung dieser Erfindung die Dresdner Glasfabrik.

1867 übernahm Friedrich Siemens von seinem verstorbenen Bruder Hans die Glasfabrik in Dresden-Löbtau. 1868 wurde der kontinuierlich arbeitende Wannenofen mit Regenerativfeuerung für die Massenerzeugung von Glas eingeführt. Die Produktionsmenge stieg um das 66fache auf zwei Millionen Flaschen monatlich.

1871 erfolgte der Kauf einer zweiten Glashütte in der Nähe von Dresden. Die 1822 von Adolf Theodor Roscher gegründete königliche „Friedrichshütte“ (Dresdner Straße in Freital-Döhlen) wurde nach dessen Tod 1861 in eine Aktiengesellschaft umgewandelt und wechselte mehrmals den Besitzer, bis Friedrich Siemens das Werk übernahm. 1879 wurde in Neusattl bei Elbogen (Böhmen) ein neues Zweigwerk gegründet.

1874 beauftragte der Dresdner Mediziner Friedrich Küchenmeister Siemens mit der Konstruktion eines Krematoriumofens. Am 9. Oktober 1874 fand in Dresden in dem von Siemens entwickelten Regenerationsofen (im damaligen Siemens-Glaswerk Dresden) die weltweit erste Einäscherung in geschlossenem Feuer statt.

1888 wandelte Friedrich Siemens seine Betriebe in eine Aktiengesellschaft um. Die "Aktiengesellschaft für Glasindustrie vormals Friedrich Siemens" entstand. Er pachtete 1894 den Seltersbrunnen in Niederselters und zudem den Mineralbrunnen Staatl. Fachingen, den seine Erben bis 1995 betrieben. 1958 gründeten diese die Firma "Siemens & Co.", die bis heute das Emser Salz produziert.

1900 erhielt er die Ehrendoktorwürde der Technischen Hochschule Dresden.
Die Kurzlaudatio lautete: „Wegen seiner unvergänglichen Verdienste, die er sich durch die Erfindungen des Regenerativofens zur Erzeugung hoher Temperaturen, des Wannenofens zum Erschmelzen von Glas, des Regenerativbrenners zur Herstellung stark leuchtender Flammen und der chemischen Regeneration der Wärme der Flammengase hoch erhitzter Öfen erworben hat.“




</doc>
<doc id="13331" url="https://de.wikipedia.org/wiki?curid=13331" title="Hans Siemens">
Hans Siemens

Hans Siemens (* 1818 in Lenthe; † 1867) war ein deutscher Industrieller und Bruder von Werner von Siemens.

Hans war ursprünglich Landwirt und Inhaber einer Spiritusbrennerei. Er erfand einen Brennapparat, der durch eine neuartige Anordnung der wärmetauschenden Flächen gekennzeichnet war.

1862 gründete er mit Unterstützung von Werner von Siemens eine Glashütte in Dresden-Löbtau. Diese Glashütte wurde von seinem jüngsten Bruder Friedrich Siemens nach seinem Tod übernommen.


</doc>
<doc id="13332" url="https://de.wikipedia.org/wiki?curid=13332" title="Altkirchenslawische Sprache">
Altkirchenslawische Sprache

Als Altkirchenslawisch oder Altslawisch oder Altbulgarisch (Eigenbezeichnung , transliteriert ""‚ ‘slawische Sprache‘) bezeichnet man die älteste slawische Schriftsprache, die seit 860 entwickelt bzw. festgehalten wurde und aus der gegen Ende des 11. Jahrhunderts verschiedene Varietäten des Kirchenslawischen hervorgegangen sind.

Die Bezeichnung "Altkirchenslawisch" begründet sich in der fast ausschließlichen Verwendung als Sakralsprache. Früher wurde die Sprache auch "Altbulgarisch" ( "starobălgarski") genannt, da die meisten erhaltenen altkirchenslawischen Denkmäler bulgarische Züge haben. In den meisten slawischen Ländern wird jedoch die Bezeichnung "Altslawisch" ( "staroslawjanski jasyk", usw.) bevorzugt. In Bulgarien wird weiter die Bezeichnung "Altbulgarisch" verwendet.

Auf Anfrage des Mährerfürsten Rastislav an Byzanz und die Ostkirche, Geistliche zur Verbreitung des christlichen Glaubens zu schicken, wurden die Brüder Konstantin (später Kyrillos genannt) und Methodios vom Patriarchen Photios I. mit der Missionierung beauftragt und begaben sich im Jahre 863 ins Mährerreich. Konstantin hatte zuvor bereits Teile der Evangelien und während der Mission den Psalter sowie andere christliche Bücher in die ihm vertraute slawische Sprache von Saloniki übersetzt und sie schriftlich mit Hilfe des von ihm entworfenen glagolitischen Alphabets fixiert.

Durch Vertreibung der Missionare und deren Schüler im Jahre 886 verbreitete sich die Schriftsprache auch im Süden, im Bulgarischen Reich. Die Glagoliza, die trotz der Vertreibung weiterhin im Mährerreich verwendet wurde, breitete sich nun auf weite Teile des Balkans aus, wobei sich zwei Schriftvarianten entwickelten: Die eckige westliche im Gebiet des heutigen Kroatien und die runde östliche Variante der Glagoliza im heutigen bulgarisch-mazedonisch-serbischen Raum, die jedoch noch vor Ende des 9. Jahrhunderts durch die kyrillische Schrift ersetzt wurde. Im alpenslawischen Südwesten, also im Gebiet des heutigen Slowenien und nördlich davon, wurde vereinzelt auch die lateinische Schrift verwendet. Während das Altkirchenslawische zuerst nur Sprache der slawischen Liturgie war, wurde es ab 893 zur Staatssprache des Bulgarischen Reiches.

Die durch die Mährenmission und die Vertreibung der Apostel nach Süden erfolgte Christianisierung bedeutete den größten kulturellen Wandel in Süd- und Osteuropa bis zur Zeit der Reformation. Unter verschiedenen Herrschern entstanden kleinere Zentren, in denen sich das Altkirchenslawische auch zu einer Literatursprache mit hohem Niveau weiterentwickelte und ihre Blütezeit im 10. Jahrhundert fand, in Preslaw und Ohrid, den damaligen Hauptstädten des bulgarischen Reiches. Von dort aus begann die Sprache in der folgenden Zeit, Einfluss auf die Ostslawen zu nehmen.

Trotz ihres südslawischen Dialektes konnten die beiden Prediger von ihren slawischen Brüdern im Norden, die die mährisch-slowakisch-pannonischen Dialekte sprachen, ohne weiteres verstanden werden, da sich die regionalen Dialekte damals noch sehr ähnlich waren. Die heutigen, vergleichsweise großen Unterschiede gehen zurück auf etwa die Zeit des 11. Jahrhunderts, als sich verschiedene Varianten der altkirchenslawischen Sprache herausbildeten, die heute unter dem Oberbegriff Kirchenslawisch zusammengefasst werden. Hierzu zählen das Bulgarisch-Kirchenslawische (auch Mittelbulgarisch genannt) sowie das Russisch-, Serbisch-, Kroatisch- und Tschechisch-Kirchenslawische.

1652 wurde das durch den Patriarchen Nikon festgelegte Kirchenslawisch die liturgische Sprache der slawisch-orthodoxen Kirche. Ab der Zeit wird sie auch als Neukirchenslawisch oder Synodalkirchenslawisch bezeichnet und hat sich dort mit einem Status, vergleichbar dem des Lateinischen in der römisch-katholischen Kirche, bis heute gehalten.

Obwohl der Einfluss des Kirchenslawischen auf die jüngeren slawischen Sprachen enorm ist, muss davon ausgegangen werden, dass es sich bei der ältesten slawischen Schriftsprache um jenen südslawischen Dialekt der ersten Missionare handelt, nicht aber um einen gemeinsamen Vorfahren der slawischen Sprachfamilie, wie das Proto- oder Urslawische. Allerdings ist das Altkirchenslawische aufgrund seines Alters dem Urslawischen noch recht ähnlich, weswegen es von hoher Bedeutung für das historisch-vergleichende Studium der slawischen Sprachen ist.

Die Geschichte der Erforschung des Altkirchenslawischen reicht zurück bis zur Begründung der slawischen Philologie im frühen 19. Jahrhundert. Josef Dobrovskýs 1822 erschienenes Werk "Institutiones linguae slavicae dialecti veteris (Lehrgebäude des alten Dialekts der slavischen Sprache)" gilt als Pionierarbeit auf diesem Gebiet.

Die Frage nach dem Ursprung und der Heimat des Altkirchenslawischen hat die slawische Philologie seit jeher intensiv beschäftigt. Dobrovský suchte die Heimat der Sprache im Süden, 1823 schrieb er in seinem Werk "Cyrill und Method, der Slaven Apostel – ein historisch-kritischer Versuch": „durch fleißige Vergleichung der neueren Auflagen mit den ältesten Handschriften habe ich mich immer mehr überzeugt, daß Cyrills Sprache der alte noch unvermischte serbisch-bulgarisch-macedonische Dialekt war“. Dagegen war Jernej Kopitar der Überzeugung, dass der Ursprung des Altkirchenslavischen in Pannonien zu suchen sei, es sei die Sprache, „die vor rund tausend Jahren unter den Slawen Pannoniens gedieh“ („quae ante mille fere annos viguit inter Slavos Pannoniae“).

Da damals fast alle erhaltenen altkirchenslawischen Texte aus Bulgarien stammten, prägte Pavel Jozef Šafárik in seinen "Serbischen Lesekörnern" (1833) und in seinem 1837 erschienenen Werk "Slovanské starožitnosti (Slawische Alterthümer)" den Ausdruck "Altbulgarisch". In Deutschland haben besonders Schleicher und dann Schmidt und Leskien den Ausdruck "Altbulgarisch" populär gemacht. Es ist nur zu bedenken, dass die Sprache in den ältesten und zeitgenössischen Quellen nie so benannt wurde, der Name taucht vielmehr erst in einer griechischen Quelle aus dem 10. Jahrhundert (Vita S. Clementis) auf. Für das 9. Jahrhundert würde sich dagegen die Bezeichnung "Altbulgarisch" mit größerem Recht auf die damals ja noch nicht völlig slawisierten Protobulgaren und ihre Sprache beziehen.

In Hinblick auf die in altkirchenslawischen Quellen aufscheinende Bezeichnung "slověnьskъ" (словѣньскъ) prägte Franz Miklosich die Bezeichnung "Altslowenisch". Er gebrauchte sie allerdings in einem spezifischen Sinne, um zu postulieren, dass die slawische Liturgie in Pannonien entstanden sei, und folglich auch die Sprache der slawischen Liturgie pannonisch sein müsse, was jedoch Vatroslav Jagić vehement bestritt. Šafárik jedoch revidierte in seinen letzten Lebensjahren seine ursprüngliche Ansicht: In seiner Schrift "Über den Ursprung und die Heimath des Glagolitismus" (Prag 1858) argumentierte er, wie zuvor Kopitar und Miklosich, für die pannonische Ursprungstheorie des Altkirchenslawischen.

Die moderne Forschung zum Altkirchenslawischen unterteilt die altkirchenslawische Epoche in das "Urkirchenslawisch" der Missionszeit, die Zeit des "klassischen Altkirchenslawisch" (10. bis 11. Jahrhundert) und des "Spätaltkirchenslawischen" zur Zeit der darauf folgenden Jahrhundertwende. Die frühesten der heute erhaltenen und bekannten Manuskripte des Altkirchenslawischen stammen aus der klassischen Zeit des 10. und 11. Jahrhunderts. Der relativ kleine Kanon der insgesamt überlieferten Sprachdenkmäler der Zeit umfasst nur etwa 30 Manuskripte und nicht ganz 100 Inschriften, von denen die bekanntesten unter anderem der vom bulgarischen Zaren Simeon um das Jahr 893 aufgestellte Grabstein, vier größere Evangelienhandschriften, zwei Evangelienfragmente, ein Psalter, liturgische Texte und Sammlungen von Bibelstellen sind. Später entstandene Abschriften weisen oft Eigenschaften des späteren Kirchenslawisch oder der sich regional entwickelnden Sprachen auf.

Weitere Entdeckungen und Funde altkirchenslawischen Schriftguts, zum Beispiel eines Evangelienmanuskriptes in Auszügen in der vatikanischen Bibliothek im Jahre 1982 erweitern das trotz der wenigen Texte doch auf einige Größe angewachsene lexikalische Gesamtkorpus, zu dem neben dem ursprünglichen theologischen auch Vokabular aus anderen Bereichen wie z. B. der frühen Geschichtsschreibung, der Philosophie, aber auch der Medizin und Botanik hinzukam. Die Schule von Preslaw war überdies bekannt für Werke der Dichtkunst.

Zu der von Konstantin und anderen Missionaren angefertigten teilweisen Übersetzung der Bibel und liturgischer Texte sowie auch literarischer Texte (unter anderem die Biographie des Konstantin und dem ihm zugeschriebenen Vorwort zum Evangelium) kamen später Übertragungen der Werke der Kirchenväter (z. B. Basilius der Große u. a.) und Philosophen. Hier kommt den Übersetzern zusätzliche Bedeutung zu, da mit der Darstellung komplexer und abstrakter philosophischer Sachverhalte in einer größtenteils nur gesprochenen Sprache lediglich auf einen eingeschränkten Erbwortschatz zugegriffen werden konnte. Noch über das Spätmittelalter hinaus setzte sich der durch die ersten Übersetzer initiierte und für das Altkirchenslawische und das spätere Kirchenslawische so fruchtbare Prozess der Erweiterung der Sprache durch Wortschöpfungen, Entlehnungen, sowie Lehnübersetzungen und Lehnprägungen, überwiegend aus dem Griechischen und Lateinischen, aber auch aus dem Hebräischen und Althochdeutschen fort. Einige Beispiele hierfür sind: "градь-никъ" ("grad-nik") von dem griechischen Wort "πολί-της" ("poli-tes") „Bürger“ (als Lehnübersetzung), "ђеона" ("geona") von "γέεννα" ("ge'enna") „Hölle“ (als Lehnwort), "мьша" ("mjescha") aus dem lateinischen (und althochdeutschen) "missa" „Messe“, "рабби" ("rabbi") und "серафимъ" ("seraphim") aus dem Hebräischen.

Den anderen indogermanischen Sprachen entsprechend, ist auch das altkirchenslawische Wortbildungssystem mehrschichtig. Neben Lexemen, die die Wortbedeutung als Ganzes vermitteln, können unterschiedliche Arten von Morphemen als weitere kleinste Bedeutungsträger zur Wortstammbildung beitragen. Das Altkirchenslawische besitzt dazu ein Flexionssystem, welches dem der heutigen slawischen Sprachen ähnlich ist. In der Deklination der Substantive, Adjektive, Partizipien und Pronomen gibt es die grammatischen Kategorien Numerus, Kasus und Genus, welche durch Suffixe gebildet werden.
Es gibt drei Numeri, nämlich Singular, Dual (heute noch im Slowenischen und Sorbischen vorhanden) und Plural.
Es werden sieben verschiedene Kasus unterschieden: Nominativ, Genitiv, Dativ, Akkusativ, Instrumentalis, Präpositional/Lokativ, Vokativ. Bis auf den letzteren, heute nur noch selten benutzten, ist der Gebrauch der Fälle ähnlich dem des Russischen. Wie in vielen indogermanischen Sprachen gibt es die drei Genera Maskulinum, Femininum und Neutrum.
Das Altkirchenslawische besitzt ein komplexes Deklinationssystem, das an das Lateinische erinnert.

Das altkirchenslawische Konjugationssystem, welches sich grob in fünf Klassen unterschiedlicher Verbalstammbildung gliedert, umfasst die Kategorien Person, Numerus, Modus, Genus und Tempus.
Am Verb werden Person (erste, zweite, dritte) und Numerus (Singular, Dual, Plural) sowie Modus (Indikativ, Konditional und Imperativ) markiert.
Im Aktiv wird ebenfalls durch verschiedene Personalendungen noch das grammatische Geschlecht unterschieden. Das Tempussystem besteht aus dem Präsens, dem Imperfekt und dem aus dem Griechischen bekannten Aorist, welche durch Bildung von Stammsuffixen (synthetisch) ausgedrückt werden, sowie dem Futur I/II, dem Perfekt und dem Plusquamperfekt, die analytisch gebildet werden.




</doc>
<doc id="13333" url="https://de.wikipedia.org/wiki?curid=13333" title="Carl Wilhelm Siemens">
Carl Wilhelm Siemens

Carl Wilhelm Siemens (* 4. April 1823 in Lenthe bei Hannover; † 19. November 1883 in London; in England nannte er sich "Charles William Siemens" bzw. nach seinem Ritterschlag 1883 Sir William Siemens) war ein in Deutschland geborener Erfinder, Ingenieur, Naturforscher und Industrieller aus der Familie Siemens, der 1859 die britische Staatsbürgerschaft annahm. Er war einer der Brüder Werner von Siemens' und baute die englische Niederlassung der Firma Siemens & Halske auf.

Wilhelm Siemens war ein Sohn des Gutspächters Christian Ferdinand Siemens aus dem alten Goslarer Stadtgeschlecht Siemens. Seine akademische Ausbildung begann – wie die seines Bruders Werner – am Katharineum zu Lübeck, setzte sich an der Gewerbe- und Handelsschule von Magdeburg fort, wo er bei seinem Bruder wohnte, der dort als Artillerieoffizier stand und ihn in Mathematik unterrichtete, und fand seinen Abschluss in einem Studienjahr an der Universität Göttingen bei seinem Schwager, dem Physikprofessor Carl Himly.

Darauf trat er als Eleve in die "Gräflich Stolbergische Maschinenbauanstalt" in Magdeburg ein. Dort entwickelte er unter Anleitung seines mittlerweile an der Artilleriewerkstatt Berlin tätigen Bruders Werner den "differentiellen Regler", eine exakte Regulierung für Dampfmaschinen (später auch als "chronometrischer Regler" bezeichnet). Werner hatte bereits kurz zuvor (1841) eine neuartige Methode zur Elektrobeschichtung, insbesondere zur Versilberung und Vergoldung, entwickelt. Für die ökonomische Verwertung dieser Erfindung ging Wilhelm Siemens 1843 zeitweise nach London, wo er den Galvanisierungsprozess an die Cousins Henry und George Richards Elkington verkaufte. 1844 kehrte er mit dem von ihm und Werner entwickelten differentiellen Regler nach England zurück. Dieser Regler ermöglichte eine bessere Regelung von durch Lastschwankungen hervorgerufenen Drehzahländerungen der angetriebenen Achse als der Fliehkraftregler von Watt. 1845 stellte ihn Michael Faraday in einer Vorlesung mit seiner Weiterentwicklung des anastatischen Druckverfahrens (das mit der späteren Elektrofotografie verwandt war) der wissenschaftlich interessierten Öffentlichkeit Englands vor. Die weit entwickelten Schutzmöglichkeiten, die das englische Patentwesen Erfindern bot, führten dazu, dass er in England blieb und schließlich, mit seiner Hochzeit 1859, eingebürgert wurde.

1850 übernahm er die Londoner Vertriebsvertretung der 1847 in Berlin von Werner gegründeten "Telegraphenbauanstalt Siemens & Halske". Ab 1858 firmierte die Londoner Filiale zunächst als "Siemens, Halske & Co", ab 1865 als Siemens Brothers & Co. selbständig. Wilhelm war an ihr – neben den Brüdern Werner und Carl – als Gesellschafter beteiligt.

1859 heiratete er Anne Gordon (1821–1901), die Nichte seines Geschäftsfreundes Lewis Gordon, Professor für Ingenieurwissenschaften an der Universität Glasgow. Dieser war Teilhaber der Drahtseilfabrik "R. S. Newall & Co", die Unterseekabel herstellte und ab 1853 bei Siemens & Halske Telegraphenapparate bestellte sowie ab 1857 Prüfaufträge für Seekabel erteilte und der Firma Siemens dadurch den Einstieg in das Tiefseekabelgeschäft ermöglichte. Das Paar hatte keine Kinder. William setzte seinen Mitarbeiter Alexander Siemens, einen Sohn seines Vetters 3. Grades Gustav Siemens, als Nachfolger bei "Siemens Brothers & Co." ein.
1863 eröffnete er eine eigene Kabelfabrik in Charlton bei Woolwich, um für die Seekabelverlegung der internationalen Telegraphenlinien von Qualität und Preisen der Zulieferer unabhängig zu werden. 1874 ließ er den Kabelleger "Faraday" bauen, mit dem er das erste dauerhaft funktionstüchtige transatlantische Telegrafenkabel verlegte.

1847 hatte er eine Regenerativfeuerung zur Stahlerzeugung erfunden, die sein Bruder Friedrich 1856 zum Siemens-Martin-Ofen weiterentwickelte. 1869 gründete William Siemens die "Landore Siemens Steel Company".

1862 wurde er in die Royal Society gewählt und später Präsident der Institution of Mechanical Engineers (1872–1873), der "Society of Telegraph Engineers" (1872, 1878), des "Iron and Steel Institute" (1877) sowie der British Association for the Advancement of Science (1882). Ehrendoktorwürden erhielt er von den Universitäten aus Oxford, Glasgow, Dublin und Würzburg. 1883 wurde er zum Ritter geschlagen und hieß seitdem "Sir William Siemens". In der Westminster Abbey erinnert ein Buntglasfenster an ihn.





</doc>
<doc id="13334" url="https://de.wikipedia.org/wiki?curid=13334" title="Arnold von Siemens">
Arnold von Siemens

Arnold Siemens, ab 1888 von Siemens (* 13. November 1853 in Berlin; † 29. April 1918 ebenda) war ein deutscher Industrieller aus der Familie Siemens, Mitinhaber der Firma Siemens & Halske und Politiker.

Er entstammte dem alten Goslarer Stadtgeschlecht Siemens (1384 urkundlich erwähnt) und war der älteste Sohn des Erfinders und Unternehmers Werner von Siemens (1816–1892) und dessen erster Ehefrau Mathilde Drumann (1824–1865) aus Königsberg (Preußen). Vater Werner Siemens wurde mit seinen Nachkommen am 5. Mai 1888 in Charlottenburg in den preußischen Adelsstand erhoben.

Siemens heiratete am 10. November 1884 in Berlin die Malerin Ellen von Helmholtz (* 24. April 1864 in Heidelberg; † 27. November 1941 in Berlin), die Tochter des Physikers Hermann von Helmholtz und seiner Frau Anna, einer Tochter des Robert von Mohl. Aus der Ehe gingen fünf Kinder hervor, unter anderem der spätere Firmenchef Hermann von Siemens. Das Paar lebte in der von ihnen erbauten Siemens-Villa am Kleinen Wannsee bei Berlin.

Zusammen mit seinem Bruder Wilhelm von Siemens wurde ihm von seinem Vater im Jahr 1879 die Leitung des Wiener Zweiggeschäftes, seit 1890 die Leitung der Berliner Firma Siemens & Halske übertragen. Er war seit 1897 Vorsitzender des in eine Aktiengesellschaft umgewandelten Unternehmens. 

Die von Arnold von Siemens aufgebaute Wiener Filiale nahm 1883 eine eigene Produktion auf. 1892 wurde die erste Siemens-Niederlassung in Übersee, die Siemens & Halske Japan Agency in Tokio, gegründet. Eine von Arnold ebenfalls 1892 mit zwei amerikanischen Partnern errichtete Fabrik für Eisenbahnmotoren und Dynamomaschinen in Chicago, die General Electric Konkurrenz machen sollte, wurde im August 1894 durch Brand völlig zerstört. Bei Ausbruch des Ersten Weltkrieges bestanden Produktionsstätten in Großbritannien, Russland, Österreich-Ungarn, Frankreich, Belgien und Spanien. Insgesamt besaß Siemens in 49 Ländern 168 Vertretungsbüros.

Arnold von Siemens war Mitglied des Preußischen Herrenhauses. 




</doc>
<doc id="13335" url="https://de.wikipedia.org/wiki?curid=13335" title="Kyrill von Saloniki">
Kyrill von Saloniki

Kyrill ( "Kyrillos", kirchenslawisch "Kiril"; * ca. 826/827 in Thessaloniki; † 14. Februar 869 in Rom), auch "Cyril", war der jüngere und anfangs bedeutendere der beiden Brüder und wichtigsten Missionare im slawischen Raum, Kyrill und Method. Kyrill hieß eigentlich Konstantin bzw. Konstantinos und nahm den Namen Kyrill wahrscheinlich erst an, als er kurz vor seinem Tod in Rom in ein griechisches Kloster eintrat.

Beide standen dabei im Spannungsfeld zwischen griechisch-byzantinischem und römisch-deutschem Einfluss und erreichten gegen viele Widerstände eine echte Inkulturation des Christentums bei den Slawen.

Kyrill und Method wurden im 9. Jahrhundert als Konstantin und Michael in Thessaloniki geboren. Ihr Vater war ein byzantinischer Drungarios (Marineoffizier) namens Leontios (geboren in Thessaloniki), ihre Mutter hieß Maria. Konstantin war der jüngste, Michael der älteste Sohn von insgesamt 7 Geschwistern. Beide waren hochgebildet. Ihr Vater Leontios starb als Konstantin 14 Jahre alt war. 

Seine Grundausbildung erhielt Konstantin in Thessaloniki. Etwa im Alter von 17 Jahren (842/843) begab er sich nach Konstantinopel, wo er etwa 842/3–847 Philosophie, Grammatik, Rhetorik, Musik, Arithmetik, Geographie und Astronomie an der kaiserlichen Universität von Konstantinopel studierte. 
Neben Griechisch sprach er schon von frühester Kindheit an auch Slawisch und studierte später Latein, Syrisch und Hebräisch.

Zu seinen Lehrern zählte vor allem der Lektor Leon der Mathematiker und Professor (später Patriarch) Photios I. Seine Patrone waren der Eunuch und Logothet (Staatssekretär am kaiserlichen Hof) Theoktistos (Theoctistes), der wahrscheinlich sein Verwandter war, ihn auch seinerzeit nach Konstantinopel brachte und ein Günstling der Kaiserwitwe und Regentin Theodora II. war, die 842 die byzantinische Regierung für den unmündigen Erben Michael III. übernahm. Laut Anastasiοs entstand später zwischen Konstantin und Photios eine freundschaftliche Beziehung, aber auch ein Konflikt hinsichtlich religiöser Fragen.

Nach dem Abschluss seiner Studien und seiner Weihung zum Subdiakon und Diakon um 848 wurde er Chartophylax (Bibliothekar, Archivar und Sekretär) von Ignatius (847–857), dem Patriarchen von Konstantinopel.

Ungefähr zwei Jahre später – nachdem er es abgelehnt hatte, eine reiche Braut zu heiraten, die ihm Theoktistos ausgewählt hatte – zog er sich insgeheim in das Kloster von Kleidion (griech. κλειδίον = Schlüssel, nordöstlich von Thessaloniki) zurück. In diesem Kloster versteckte er sich sechs Monate lang. Als man ihn dort entdeckte, lud man ihn ein, an der Universität von Konstantinopel als Philosophieprofessor tätig zu werden (Ende 850 oder Anfang 851). Seitdem trägt er wahrscheinlich den Beinamen "Philosoph". In dieser Zeit hatte er eine Kontroverse mit dem abgesetzten ikonoklastischen Patriarchen Johannes und Dispute auch mit anderen Ikonoklasten.

Aus Konstantinopel wurde er jedoch sehr bald mit einer politischen und religiösen Mission (850–851) am arabischen Hofe des Kalifen al-Mutawakkil in der Stadt Samarra wegen Eintreibung von Abgaben und Unterdrückung der Christen seitens der Araber betraut. Hier beteiligte er sich an einem intensiven theologischen Disput mit den muslimischen Gelehrten und Mönchen, so dass ihn die Araber sogar angeblich vergiften wollten. Im Disput ging es um den dreieinigen Gott, dessen Existenz die Muslime nicht anerkennen konnten, und Konstantin erwies sich als guter Kenner auch des Korans. Dies waren genau jene Jahre, in denen die Unterschiede zwischen dem Christentum und dem Islam größer wurden.

Nach seiner Rückkehr nach Konstantinopel legte er seine Professur nieder und ging in ein Kloster auf dem Berg Olymp in Kleinasien (bei Brussa). In diesem Kloster begegnete er seinem Bruder Method, der hier schon länger wohnte. Auch dieser Klosteraufenthalt hatte einen politischen Hintergrund – 856 wurde die Kaiserin Theodora abgesetzt und ihr Günstling Theoktistos wurde ermordet.

Auf Einladung des Herrschers der Chasaren unternahm Konstantin etwa 860 eine Mission zu den Chasaren, einem nördlich des Kaukasus (nach manchen Quellen eher am Asowschen Meer) sesshaften Volk, dessen jüdischer König ein friedliches Zusammenleben von Juden, Muslimen und Christen ermöglicht hatte. Vor dieser Reise wurde er wahrscheinlich zu einem ordentlichen Priester geweiht.

Auf dem Weg zu den Chasaren ging er in die Stadt Cherson, in der er die hebräische, chasarische und gotische Sprache lernte. In Cherson fand er auch die sterblichen Überreste des um 101 verstorbenen heiligen Clemens (die er dann später nach Mähren und 867 nach Rom mitbrachte und die wohl zum herzlichen Empfang in Rom beitrugen). Konstantin verfasste über diese Entdeckung drei griechische Schriften ("historica narratio", "sermo declamatorius" und "hymnus", siehe Werke).

Bei den Chasaren führte er dann theologische Dispute mit jüdischen Gelehrten und Rabbinern. Zweihundert Chasaren ließen sich taufen. Statt der ihm seitens des Chasaren-Herrschers angebotenen Entlohnung soll Konstantin nur veranlasst haben, dass 200 griechische Sklaven entlassen werden. Der Chasaren-König schrieb später dem byzantinischen Kaiser einen Dankesbrief, in dem er die Mission lobte und er die Freundschaft seines Volkes zusicherte. Konstantin gelang es zwar nicht, die Chasaren, dafür aber ein kleines Volk im fulischen Gebiet, das aus den restlichen Alanen und Gothen bestand, zum Christentum zu bekehren. Die Behauptung, dass Konstantin bei der Mission zu den Chasaren von Method begleitet wurde, ist nach manchen Ansichten eher eine spätere Legende.

861 kehrte Konstantin nach Konstantinopel zurück und widmete sich der Sprachforschung.

Nachdem sich der mährische Fürst Rastislav zuerst erfolglos an den Papst in Rom gewandt hatte, bat er den byzantinischen Kaiser 862 um einen „Bischof und Lehrer“, der die Leute in der Sprache des Volkes im christlichen Glauben unterweisen konnte.

Mit den Worten "Hörst du, Philosoph, diese Worte? Es gibt keinen anderen außer dir, der dies erledigen kann, so, nimm viele Geschenke und deinen Bruder Method mit, und geh! Weil ihr Saloniker seid, und alle Saloniker sprechen reines Slawisch" (Quelle: Vita Methodii) bat Kaiser Michael III. Konstantin um die christliche Mission im Mährerreich.

Konstantin erklärte sich auf die Zureden von Kaiser Michael III. und Patriarch Photios I. dazu bereit.

863 oder 864 (laut Tradition am 5. Juli 863) kamen Konstantin und Method in Mähren an. Sie brachten ihre ersten Übersetzungen mit, das Symbol des byzantinischen Doppelkreuzes (das heute im slowakischen Staatswappen steht) sowie die Reliquien eines der ersten Bischöfe von Rom, des heiligen Clemens, die Konstantin seinerzeit 860 in Cherson aufgefunden hatte.
863 gründete Konstantin die so genannte Mährische Akademie, in der künftige slawische Priester und Verwaltungskräfte ausgebildet wurden, und die zum Zentrum der slawischen Literatur wurde. 885 hatte sie etwa 200 Absolventen. Ihre Lage ist leider unbekannt, aber archäologischen Funden zufolge gab es eine kirchliche Schule an der Burg Devín im heutigen Bratislava.

Begleitet von seinem Bruder Michael missionierte Konstantin mehrere Jahre lang in Mähren. Von Anfang an hatten sie als Vertreter des griechisch-byzantinischen Christentums gegen die Kritik der bayerischen Priester in Mähren zu kämpfen, die von der ansässigen Bevölkerung und ihrem Herrscher Rastislav mit Argwohn betrachtet wurden. Selbst unter dem Mantel des westlichen, lateinischen Christentums konnten die Deutschen ihren machtpolitischen Anspruch nie ernsthaft behaupteten. Mit dem Argument, dass der Gottesdienst angeblich nur in den drei Sprachen abgehalten werden dürfe, die die Pilatus-Inschrift auf dem Kreuz Jesu enthalte (Latein, Griechisch, Hebräisch), versuchten sie deshalb den Papst für ihre Sache zu instrumentalisieren.
Um diesem Dilemma zu entkommen, entschieden sich Konstantin und Michael 867 mit der Zustimmung von Rastislav und Sventopluk (Neutraer Fürstentum (heutige Slowakei)) nach Rom zu gehen, um die Zustimmung des Papstes zur Liturgiesprache Altkirchenslawisch zu erhalten. Sie nahmen dabei auch einige Schüler mit, die zu Priestern geweiht werden sollten. (Nach anderen Quellen wollten sie ursprünglich vom Hafen von Venedig aus nach Konstantinopel reisen, um die Zustimmung des Patriarchen zu erhalten, erhielten aber dann eine Einladung des Papstes).

Auf dem Weg nach Rom machten sie im Sommer einen kurzen Halt im Plattensee-Fürstentum des Fürsten Kocel und unterrichteten auch dort einige Schüler. Im Herbst 867 kamen die Brüder und ihre Schüler in Venedig an, wo sie vor einer Versammlung von Priestern die Verwendung des Altkirchenslawischen als Liturgiesprache zu verteidigen hatten. In Venedig erhielt Konstantin die offizielle Einladung des Papstes Nikolaus I. nach Rom. Dies geschah wohl, weil er die oben erwähnten Reliquien von Clemens I. mit sich trug und weil er Freunde in Rom hatte (Bischof Arsenius).
Im Winter 867 wurden sie vom (neuen) Papst Hadrian II. feierlich empfangen und ihre gesamte Mission in Mähren wurde gebilligt. Zu Weihnachten wurden sogar die Bibelübersetzungen auf den Altar der Peterskirche in Rom und die Übersetzungen der liturgischen Texte auf den Hauptaltar der Basilika Santa Maria Maggiore feierlich gelegt, um so symbolisch deren Akzeptanz seitens Roms zu zeigen. Im Februar 868 wurden Method und drei Schüler (der aus der heutigen Slowakei stammende Gorazd und die Südslawen Kliment und Naum) in Rom zu Priestern bzw. zwei von ihnen zu Diakonen geweiht.

Im März 868 wurde schließlich die slawische Liturgiesprache (Altkirchenslawisch) zugelassen, als vierte Sprache in der Westkirche. Dies war ein überaus bedeutendes Ereignis, da erst im 20. Jahrhundert (d. h. mehr als 1000 Jahre später) wieder eine Liturgiesprache außer Latein, Griechisch und Hebräisch von Rom zugelassen wurde. 880 wurde die slawische Kirchensprache jedoch von Papst Marinus I. wieder verboten.

Ende 868 erkrankte Konstantin in Rom, wurde Mönch in einem Kloster, wo er auch wahrscheinlich den Namen Kyrill (Kyrillos) annahm, und starb im Februar 869. Er wurde in der St.-Clemens-Basilika in Rom begraben. Umstritten bleibt die Behauptung der Translatio Clementis, dass Konstantin noch zum Bischof geweiht wurde.

Konstantin entwickelte eigens für die mährische Mission das erste slawische Alphabet, die Glagolitische Schrift (Hlaholica, Glagolica, Glagoljica). Aus dieser und hauptsächlich der griechischen Schrift entwickelte sich Ende des 9. Jahrhunderts die heute nach ihm benannte kyrillische Schrift.
Noch im Byzantinischen Reich übersetzten Konstantin und Method einige liturgische und biblische Texte ins Altkirchenslawische.
Für seine Mission übersetzte Konstantin das Neue Testament in eine Sprache, die er – vermutlich aus dem ihm am nächsten stehenden slawischen Dialekt – auch erst konstruieren musste und die heute als Altkirchenslawisch bekannt ist. Ursprünglich war es ein in der Saloniki-Region verwendeter slawischer Dialekt, aber während der mährischen Mission nahm er viele Elemente der in diesem Gebiet gesprochenen westslawischen Dialekte an. So enthält auch die damalige Version der Glagolica einen Buchstaben (Laut dz), der damals nur in den Dialekten auf dem Gebiet der heutigen Slowakei verwendet wurde. Da die slawischen Sprachen damals noch sehr ähnlich waren, wählte Konstantin das so genannte Altkirchenslawische als die während seiner mährischen Mission zu verwendende Sprache aus.

Während der Mission in Mähren übertrugen sie dann die ganze Bibel ins Altkirchenslawische, aber auch eine Gesetzessammlung (Nomokanon) und liturgische Texte. Sie gelten damit als Begründer der slawischen Literatur.

Konstantins Übersetzungen sind schöpferisch geniale Leistungen und haben für das Slawische eine vergleichbare Bedeutung wie die Lutherbibel für die deutsche Sprache.

Einige altkirchenslawische Handschriften enthalten Texte, die möglicherweise von Kyrill verfasst wurden 



Der gemeinsame Gedenktag von Kyrill und Method ist der 14. Februar, sowohl in der katholischen wie der evangelischen und anglikanischen Kirche. Orthodoxer Gedenktag ist ebenfalls der 14. Februar (Kyrill alleine) und der 11. Mai (gemeinsam mit Method).

In der katholischen Kirche ist der Tag ein Gebotener Gedenktag im Allgemeinen Römischen Kalender. Da Papst Johannes Paul II. Kyrill und Method 1980 zu Patrone Europas ernannt hat, wird er in den europäischen Regionalkalendern (z. B. im Regionalkalender für das deutsche Sprachgebiet) zum Fest aufgewertet.

Am 5. Juli wird in Tschechien und der Slowakei der Tag ihres Eintreffens im Mährerreich (863) als Nationalfeiertag begangen.

Der 24. Mai ist in Bulgarien und Mazedonien ein Nationalfeiertag, der den beiden Brüdern gewidmet ist. (Das Datum entspricht dem 11. Mai des Julianischen Kalenders.)





</doc>
<doc id="13336" url="https://de.wikipedia.org/wiki?curid=13336" title="Carl Friedrich von Siemens">
Carl Friedrich von Siemens

Carl Friedrich Siemens, ab 1888 von Siemens (* 5. September 1872 in Charlottenburg; † 9. Juli 1941 in seinem Landhaus Heinenhof in Neu Fahrland bei Potsdam) war ein deutscher Industrieller und Politiker aus der Familie Siemens.

Carl Friedrich Siemens entstammte dem alten Goslarer Stadtgeschlecht Siemens (1384 urkundlich erwähnt) und war der jüngste Sohn des Erfinders und Unternehmers Werner von Siemens (1816–1892) und dessen zweiter Ehefrau "Antonie Siemens" (1840–1900). Vater "Werner Siemens" wurde mit seinen Nachkommen am 5. Mai 1888 in Charlottenburg in den preußischen Adelsstand erhoben.
Siemens heiratete überstürzt und ohne das Wissen der Eltern erstmals 1895 in London. Dies stieß in der Familie auf wenig Verständnis. Die Ehe wurde 1897 wieder geschieden, nachdem seine Frau als Hochstaplerin entlarvt worden war.

In zweiter Ehe heiratete er am 14. Juni 1898 in Berlin "Auguste (Tutty) Bötzow" (* 2. Februar 1878 in Berlin; † 22. März 1935 ebenda), die Tochter des Großgrundbesitzers und Inhabers der Bötzow-Brauerei "Julius Bötzow" und der "Elisabeth Henze". Diese Ehe wurde am 11. November 1923 in Berlin geschieden. Aus ihr stammen die Kinder Ernst und Ursula (verheiratete Gräfin Blücher).

In dritter Ehe heiratete er am 19. November 1929 in Berlin-Charlottenburg "Margarete (Grete) Heck" (* 11. Dezember 1890 in Berlin; † 17. November 1977 in München), die Tochter des großherzoglich hessischen Geheimen Hofrats Ludwig Heck, Direktor des Zoologischen Gartens Berlin, und der "Margarethe Nauwerk". "Margarete Heck" war zuvor von ihrem ersten Ehemann, "Wilhelm Siemens" (1882–1945), in Dresden geschieden worden.

Carl Friedrich von Siemens war seit 1899 in der Firma Siemens & Halske AG tätig und leitete von 1901 bis 1908 die Starkstromabteilung der Siemens Brothers & Co. in London. Ab 1912 arbeitete er als Vorsitzender des Direktoriums der Siemens-Schuckertwerke, 1919 folgte er seinem Halbbruder Georg Wilhelm von Siemens (1855–1919) als Vorsitzender der Aufsichtsräte der "Siemens & Halske AG" und der "Siemens-Schuckertwerke" (heute Siemens AG) und somit als „Chef des Hauses“. 

Nach den Verlusten des Ersten Weltkrieges gehörte Siemens schon Mitte der 1920er-Jahre wieder zu den fünf weltweit führenden Elektrokonzernen. Später wurden einzelne Produktbereiche in spezialisierte Tochter- und Beteiligungsgesellschaften ausgegliedert. So entstanden unter anderem die "Osram G.m.b.H. KG" (1920), die Siemens-Bauunion (1921), die "Siemens-Reiniger-Veifa Gesellschaft für medizinische Technik mbH" (1925, ab 1932 "Siemens-Reiniger-Werke AG" (SRW)) und nach Übernahme der "Eisenbahnsignal-Bauanstalt Max Jüdel & Co" in Braunschweig die "Vereinigten Eisenbahn-Signalwerke GmbH" (1929). Siemens & Halske hatte wesentlichen Anteil an der technischen Modernisierung des Telefonsystems nach dem Ersten Weltkrieg. Der in dieser Zeit erreichte technische Vorsprung wurde erfolgreich in einem intensiven Auslandsgeschäft weiterverfolgt. Internationale Kartelle 
für Europea und Südamerika wurden im Telefongeschäft mit ITT, General Electric, AT&T und Ericsson abgeschlossen, national mit der Reichspost. Das Gebäude der Hauptverwaltung des Unternehmens an der Nonnendammallee in der von seinem Bruder gegründeten Siemensstadt, erbaut 1910–1913 von Karl Janisch, ließ Carl Friedrich von Siemens 1922 von Friedrich Blume und Hans Hertlein erweitern. 

Die Weltwirtschaftskrise nach 1929 führte zu erheblichen Umsatzeinbußen und Personalentlassungen, jedoch führte nach der nationalsozialistischen Machtergreifung 1933 die verstärkte Aufrüstung von Wehrmacht, Luftwaffe und Marine bald wieder zu einer Steigerung der Auftragseingänge. 1939 war Siemens mit 187.000 Beschäftigten größter Elektrokonzern der Welt. Neue Anwendungsbereiche wie die Medizintechnik, die Rundfunktechnik, elektrische Wärme- und Haushaltsgeräte oder auch das Elektronenmikroskop gewannen rasch an Bedeutung für das Unternehmen. 1936 gab es in Europa 16 Fertigungsstätten (u. a. in Wien, Budapest, Mailand und Barcelona), außerhalb Europas entstanden Produktions-Joint-Ventures in Tokio und Buenos Aires. In die Zwischenkriegszeit fallen auch eine Reihe von internationalen Großprojekten.

Nach dem Ausbruch des Zweiten Weltkriegs 1939 waren die Siemens-Kapazitäten mit kriegswichtigen Bestellungen voll ausgelastet. Im Verlauf des Krieges wurden Produktionsstätten in alle Gegenden Deutschlands und in die besetzten Gebiete ausgelagert, wo auch Siemens in großem Umfang „Fremdarbeiter“ sowie Zwangsarbeiter zu beschäftigen hatte. 

Carl Friedrich von Siemens' Nachfolger als Vorsitzender der Aufsichtsräte der beiden Siemens-Stammfirmen wurde nach seinem Tode 1941 sein Neffe Hermann von Siemens (1885–1986), dessen Nachfolger als "Chef des Hauses" 1956 wiederum Carl Friedrichs einziger Sohn Ernst von Siemens.

Im Jahre 1921 wurde er Dr. Ing. eh. an der TH München, und 1927 erhielt er den Ehrendoktortitel rer. nat. an der Universität Halle. 1923 wurde er Präsident des Vorläufigen Reichswirtschaftsrates, 1924 Präsident des Verwaltungsrats der Deutschen Reichsbahn und leitete 1927 die deutsche Abordnung auf der Genfer Weltwirtschaftskonferenz.

Von 1920 bis 1924 war er für die Deutsche Demokratische Partei Mitglied des Reichstags. 1929 trat er der Gesellschaft der Freunde bei. Zur Wahl zur Deutschen Nationalversammlung 1919 gründete er das Kuratorium für den Wiederaufbau des deutschen Wirtschaftslebens zur Parteienfinanzierung.

Er war von 1926 bis zu seinem Tod Senator der Kaiser-Wilhelm-Gesellschaft. Im gleichen Jahr 1926 wurde er zum Mitglied der Leopoldina gewählt. 1933 war er Mitglied im "Generalrat der Wirtschaft" (der nur ein einziges Mal tagte); in der Zeit des Nationalsozialismus war er Mitglied der 1933 gegründeten nationalsozialistischen Akademie für Deutsches Recht.

Die Carl Friedrich von Siemens Stiftung trägt seinen Namen; ebenso das Carl-Friedrich-von-Siemens-Gymnasium in Berlin-Siemensstadt. 

Seine letzte Ruhestätte befindet sich im Familiengrab der Familie Siemens auf dem Südwestkirchhof Stahnsdorf.




</doc>
<doc id="13337" url="https://de.wikipedia.org/wiki?curid=13337" title="Carl Heinrich von Siemens">
Carl Heinrich von Siemens

Carl Heinrich von Siemens (* 3. März 1829 in Menzendorf (noch ohne Adelsprädikat); † 21. März 1906 in Menton; Schreibweise auch "Karl") war ein deutscher Industrieller und Bruder von Werner von Siemens.

Carl von Siemens wurde 1829 als achtes von insgesamt 14 Kindern des Gutspächters Christian Ferdinand Siemens (1787–1840) und dessen Ehefrau Eleonore Henriette Deichmann (1792–1839) in Menzendorf (Mecklenburg) geboren. Der Vater entstammte dem alten Goslarer Stadtgeschlecht Siemens (1384 urkundlich erwähnt). Nach dem Tod der Eltern im Jahr 1840 wurde er von seinem 13 Jahre älteren Bruder Werner erzogen, der bei ihm auch die Begeisterung für Technik weckte. Nach Beendigung der Schule fand er 1846 zunächst Anstellung in einer Zementfabrik, wechselte dann jedoch in die 1847 von seinem Bruder gegründete Telegraphenbauanstalt Siemens & Halske und war für diese in Paris und London tätig, bevor er 1853 im Auftrag der Firma nach Russland wechselte.

Siemens & Halske errichteten hier im Auftrag des russischen Staates seit 1852 ein landesweites Telegraphennetz. Carl übernahm die Leitung der Bauarbeiten und baute das russische Geschäft des Siemens-Konzerns auf, der 1855 sogar eine Zweigniederlassung in Sankt Petersburg errichtete. Die Einnahmen der Filiale sicherte vor allem der auf 12 Jahre angelegte Reparatur- und Wartungsvertrag für das Telegraphennetz, der Siemens auch den Titel eines offiziellen Hoflieferanten einbrachte. Carl heiratete Marie von Kap-herr, die Tochter eines deutschstämmigen russischen Kaufmanns und versuchte sich neben seiner Tätigkeit im Konzern des Bruders selbst als Unternehmer: 1861 errichtete er auf seinem Gut Chmelewo am Ilmensee die Glashütte Gorodok. Die Unternehmung warf jedoch in den zwei Jahrzehnten ihres Bestehens keine Gewinne ab und musste 1881 schließlich liquidiert werden.

Ursächlich für das Scheitern mögen auch gesundheitliche Probleme Carls gewesen sein, die ihn 1867 veranlassten, in den Kaukasus zu wechseln und die Leitung des Kupferbergwerkes in Kedabeg im russischen Gouvernement Elisabethpol (heute Aserbaidschan) zu übernehmen. Werner und Carl hatten, auf Vorschlag ihres mit dem Bau der Telegraphenleitungen im Kaukasus beschäftigten Bruders Walter, 1864 das Kupferbergwerk in Kedabeg gekauft, das – unter Überwindung mancher Schwierigkeiten – als von der Firma getrenntes „Privatgeschäft“ zunächst unter Leitung der Brüder Walter und Otto betrieben worden war. 

Nach dem Tod seiner Frau und seiner Tochter Eleonore in Berlin wechselte Carl 1869 wiederum nach London, um mit der Siemens Brothers & Co. das Seekabelgeschäft des Konzerns aufzubauen, musste sich die unternehmerische Verantwortung jedoch mit dem älteren Bruder Carl Wilhelm Siemens "("William")" teilen. Nach neuen Herausforderungen suchend, kehrte Carl daher Anfang der 1880er zurück nach Petersburg, wo er zunächst eine Kabelfabrik aufbaute. Daneben gelang es ihm, seit 1883 russischer Staatsbürger, im folgenden Jahrzehnt mit der „Gesellschaft für elektrische Beleuchtung“ für Siemens das Monopol im Bereich elektrischer Straßenbeleuchtung zu erringen. 

In den 1880er Jahren fielen Carl Siemens' Petersburger Fabriken ebenso wie der Siemens-Konzern insgesamt gegenüber den Geschäftsmodellen der Konkurrenz, vor allem Emil Rathenaus AEG zurück. Im Briefwechsel der Brüder Carl und Werner fiel gelegentlich das Stichwort „Geldjuden“, Carl verstieg sich zu heftigen antisemitischen Ausbrüchen.

Aus Anerkennung für seine unternehmerischen Verdienste wurde Carl 1895 von Zar Nikolaus II. in den Adelsstand erhoben. Seit 1889 besaß Carl von Siemens das Gut Gostilizy (auch: Gostilitzy) bei St. Petersburg, das er seiner Tochter Marie von Graevenitz vermachte, die es bis 1918 bewirtschaftete. Seit 2011 erinnert am Haus des Popen Vater Viktorin in Gostilizy eine bronzene Gedenktafel an Carl von Siemens und Marie von Graevenitz, die der Berliner Bildhauer Hans Starcke nach einem Entwurf des Kunsthistorikers Jörg Kuhn modellierte. Sie entstand im Auftrag der 1923 von Marie von Graevenitz und ihrer Schwester Charlotte in der Schweiz gegründeten Werner-Siemens-Stiftung.

Nachdem sich sein Bruder Werner 1890 aus Altersgründen zurückgezogen hatte, ging Carl zurück nach Berlin, um gemeinsam mit seinen Neffen, Werners Söhnen Arnold und Wilhelm, die Leitung des Unternehmens zu übernehmen. 1897 wurde der Konzern auf Betreiben Carls in eine Aktiengesellschaft umgewandelt, ein Schritt dem sich der 1892 verstorbene Firmengründer Werner mit Rücksicht auf sein Ideal eines Familienunternehmens "à la Fugger" immer widersetzt hatte, und Carl wurde der erste Aufsichtsratsvorsitzende der Siemens & Halske AG. 

Aus gesundheitlichen Gründen zog er sich 1904 aus dem Unternehmen zurück und siedelte nach Menton an die Côte d’Azur um, wo er am 21. März 1906 kurz nach seinem 77. Geburtstag verstarb. Der Sarg mit den sterblichen Überresten wurde nach Berlin überführt und zwischen den Grabstellen von Marie von Siemens und Werner von Siemens, dem 1900 in Gostilizy verstorbenen Sohn der beiden, beigesetzt. Die seit 1869 auf dem III. Friedhof der Jerusalems- und Neuen Kirchengemeinde in Berlin-Kreuzberg bestehende Grabstätte wurde 1908 neu gestaltet und erhielt eine überlebensgroße Marmorfigur eines weiblichen Trauerengels. Diese Skulptur, ein Werk des Wiesbadener Bildhauers Philipp Modrow wurde 1970 zerstört. Sie konnte anhand historischer Abbildungen durch den Berliner Bildhauer Matthias Richter im Auftrag der Werner Siemens-Stiftung 2012 nachgeschaffen werden und steht heute wieder auf der Grabstätte aufgestellt.

Am 24. November 1855 heiratete Carl Siemens Marie Kap-herr, die Tochter des St. Petersburger Kaufmanns und zeitweiligen Repräsentanten der Firma Siemens & Halske in Russland, Hermann Christian Kap-herr (1801–1877), der 1868 zum erblichen hessischen Freiherrn erhoben wurde. 

Aus der Ehe gingen fünf Kinder hervor:




</doc>
<doc id="13338" url="https://de.wikipedia.org/wiki?curid=13338" title="Georg von Siemens">
Georg von Siemens

Georg von Siemens (* 21. Oktober 1839 in Torgau; † 23. Oktober 1901 in Berlin) war ein deutscher Bankier und Politiker aus der Familie Siemens.

Georg von Siemens entstammte dem alten Goslarer Stadtgeschlecht Siemens (1384 urkundlich erwähnt); er war ein Neffe zweiten Grades der Erfinder und Industriellen Werner, Wilhelm und Carl (von) Siemens. Sein Vater, der Berliner Justizrat Johann Georg Siemens, stellte 1847 einen erheblichen Teil des Gründungskapitals der Firma Siemens & Halske bereit, aus welcher sich später die Siemens AG entwickelte.

Georg Siemens studierte 1858/59 Jura in Heidelberg und arbeitete danach zunächst als Assessor am Landgericht Aachen. Ab 1866 übernahm er verschiedene Aufträge für die Firma Siemens & Halske; u. a. reiste er 1868/69 nach Persien, um die Verhandlungen mit der persischen Regierung über den Bau und Betrieb der Indo-Europäischen Telegrafenlinie abzuschließen.

1870 wurde Siemens auf Initiative Adelbert Delbrücks zu einem der Gründungsdirektoren der Deutschen Bank bestellt, als deren Vorstandssprecher er bis 1900 fungierte. In dieser Zeit entwickelte sich die Deutsche Bank zu einer der bedeutendsten Großbanken Deutschlands. Ursprünglich etabliert um die Finanzierung des deutschen Außenhandels zu erleichtern, hielt sich die Bank vom Boom-Bust-Emissionsgeschäft der Gründerzeit fern. Nach dem Gründerkrach erwarb sie dann eine zentrale Stellung, u. a. durch Übernahme der angeschlagenen Deutschen Union-Bank und des Berliner Bankvereins. Ferner bemühte sich die Deutsche Bank auf Anregung Siemens' und Hermann Wallichs als erste Großbank um Depositen, welche die finanziellen Mittel der Bank über das Eigenkapital hinaus erweiterten.
Ab den 1880er Jahren betrieb die Deutsche Bank unter Siemens' Federführung die Finanzierung von Industrieunternehmen wie der AEG Emil Rathenaus, Mannesmann, Bayer und der BASF. 1897 organisierte Georg Siemens die Umwandlung von Siemens & Halske – das Unternehmen wurde zu diesem Zeitpunkt von seinen Vettern (zweiten Grades) Arnold und Wilhelm geleitet – in eine Aktiengesellschaft, um seine Kapitalbasis zu stärken. Ein anderer Schwerpunkt seiner Arbeit war die Finanzierung des internationalen Eisenbahnbaus, unter anderem im Osmanischen Reich (Bagdadbahn) und in den Vereinigten Staaten von Amerika (Northern Pacific Railway).

Siemens kam innerhalb des Vorstandes der Deutschen Bank die Rolle des – ursprünglich fachfremden – „Dynamiker[s] [zu], der große Projekte konzipierte, vor Ideen sprühte und rasch zupacken konnte“. Sein Vorstandskollege Wallich charakterisierte ihn wie folgt:

Von 1874 an war Siemens wiederholt Mitglied des Preußischen Abgeordnetenhauses und des Reichstages, zunächst als Nationalliberaler und später – nach der Spaltung der Nationalliberalen – als Angehöriger der 1884 von Eugen Richter und Franz von Stauffenberg gegründeten linksliberalen Deutschen Freisinnigen Partei. 1901 war Siemens als Nachfolger Johannes von Miquels für das Amt des preußischen Finanzministers im Gespräch, die Verschlechterung von Siemens’ Gesundheitszustand – Folge einer Krebserkrankung – vereitelte jedoch die Ernennung.

Georg Siemens heiratete 1872 Elise Görz (1850–1938), eine Tochter des hessischen Juristen und liberalen Politikers Joseph Görz. Gemeinsam hatten sie sechs Töchter. Ihre zweite Tochter Marie heiratete den Archäologen und Museumsdirektor Theodor Wiegand, ihre fünfte Tochter Annette den Volkswirt, Bankier und Politiker Karl Helfferich. Helfferich ist der Autor einer dreibändigen Biographie über Georg von Siemens. 1899 wurde Georg Siemens in den preußischen Adelsstand erhoben.

Die Grabstätte von Georg von Siemens befindet sich in der Siemensgruft im Schlosspark von Ahlsdorf im Landkreis Elbe-Elster in Brandenburg.




</doc>
<doc id="13340" url="https://de.wikipedia.org/wiki?curid=13340" title="Federalist Papers">
Federalist Papers

Die Federalist Papers (deutsch: „Föderalistenartikel“) waren eine Serie von 85 Artikeln, die 1787/88 in verschiedenen Zeitungen New Yorks erschienen, mit dem Zweck, die Bevölkerung des gleichnamigen Staats von der 1787 entworfenen, aber noch nicht von allen Mitgliedsstaaten der USA ratifizierten Verfassung zu überzeugen.

Die Autoren der Artikel, die in Anspielung auf den römischen Konsul Publius Valerius Poplicola unter dem gemeinsamen Pseudonym „Publius“ auftraten, waren Alexander Hamilton, James Madison und John Jay, drei der Gründerväter der Vereinigten Staaten. Ihre Texte erschienen noch 1788 gemeinsam in der Schrift „The Federalist“, von der sich ihr heutiger Name herleitet. Er bezieht sich auf die politische Gruppierung der Föderalisten, die in der Verfassungsdebatte dafür eintraten, die USA von einem lockeren Staatenbund in einen Bundesstaat mit einer starken, handlungsfähigen Exekutive auf Bundesebene umzuwandeln. Da dieser Standpunkt sich schließlich durchsetzte, gelten die Federalist Papers bis heute als authentischer Verfassungskommentar der Generation der Gründerväter und darüber hinaus als grundlegende theoretische Schrift der modernen, repräsentativen Demokratie.

Seit dem Amerikanischen Unabhängigkeitskrieg hatten die 13 ehemaligen britischen Kolonien einen lockeren Staatenbund gebildet, dessen Mitglieder grundsätzlich als souveräne Staaten galten. Ihr einziges gemeinsames und zentrales Organ war der Kontinentalkongress, der sich als gleichzeitige Legislative und Exekutive jedoch als schwerfällig und wenig handlungsfähig erwies. Er verfügte über keine eigenen Steuereinnahmen, und die Konföderationsartikel, die Vorläufer der heutigen Verfassung, schrieben bei allen Entscheidungen Einstimmigkeit vor.

Um diesen Mängeln abzuhelfen, wurde 1787 ein Konvent nach Philadelphia einberufen, der Reformvorschläge ausarbeiten sollte. Stattdessen verabschiedeten die Delegierten gleich eine völlig neue Verfassung, die einen Bundesstaat mit einer gemeinsamen Exekutive und einem Präsidenten an der Spitze vorsah. Diese sollte vor allem die Kompetenzen der Einzelstaaten in Fragen der Außenpolitik, des Außenhandels und der Landesverteidigung übernehmen.

Der Verfassungsentwurf, der vor seinem Inkrafttreten von jedem einzelnen Staatsparlament gebilligt und ratifiziert werden musste, traf jedoch nicht auf einhellige Zustimmung. Eine gemeinsame nationale Identität war erst schwach ausgebildet; die meisten Einwohner der USA betrachteten sich selbst in erster Linie z. B. als Virginier oder New Yorker, nicht als Amerikaner. Sie befürchteten, die Abkehr von den Einzelstaatsrechten werde einer neuen Tyrannei Tür und Tor öffnen und eine weit entfernte Zentralregierung lasse sich kaum noch demokratisch kontrollieren und fördere damit die Korruption. Dazu kam, dass die Gründung eines so großen, demokratisch verfassten Bundesstaats ein Experiment darstellte, das es in der Weltgeschichte bis dahin noch nicht gegeben hatte.

Gegen alle diese Bedenken wandten sich die Verfasser der Federalist Papers. Um die Bevölkerung und das Staatsparlament von New York zur Annahme der Verfassung zu bewegen, argumentierten sie, dass eine starke, bundesstaatliche Exekutive keinen Verrat an den Idealen der Revolution von 1776, sondern im Gegenteil deren endgültige Sicherung darstellte. Sie erläuterten die Notwendigkeit der neuen Verfassung, ihre Vorteile gegenüber den Konföderationsartikeln, die Rechte, Funktionen und Beschränkungen der einzelnen Staatsorgane – z. B. des Präsidentenamts – sowie das System der Checks and Balances, das für eine systemimmanente, demokratische Selbstkontrolle der Macht sorgen sollte.

Die drei Autoren, die unter dem gemeinsamen Pseudonym "Publius" schrieben, versuchten mit ihren Essays Einfluss auf die Ratifikationsdebatte zu nehmen. Jeder von ihnen hatte gewisse Themenschwerpunkte, aber bis heute ist die Urheberschaft nicht für jeden Artikel endgültig geklärt. Hamilton und Madison fertigten zwar im Nachhinein Autorenlisten an. Diese wichen aber stark voneinander ab. Daher unterzogen die Mathematiker Frederic Mosteller und David Wallace 1964 zwölf Essays mit umstrittener Autorschaft einer statistischen Textanalyse auf Basis des Satzes von Bayes. Sie entdeckten dabei starke Korrelationen zwischen Stilmerkmalen der Autoren und den jeweiligen Texten. Die Forscher fassten ihre Ergebnisse in der Veröffentlichung „Inference and Disputed Authorship“ zusammen, die heute als Meilenstein auf dem Gebiet der Stilometrie gilt.

Der Großteil der veröffentlichten Essays stammt wohl aus der Feder von Alexander Hamilton, 1787 Mitglied des New Yorker Staatsparlaments und Delegierter des Verfassungskonvents von Philadelphia. Sein Interesse an Politik und politischer Philosophie galt unverkennbar auch der wirtschaftlichen Seite: Er gilt in der Literatur als Verfechter der liberalen Wirtschaftstheorie von Adam Smith. Dies findet in den 51 vermutlich von ihm verfassten Essays seinen deutlichen Niederschlag. Einige davon behandeln die wirtschaftlichen Aspekte der neuen Verfassung, insbesondere die ökonomischen Möglichkeiten und Chancen einer Union im Vergleich mit einer Konföderation. Darüber hinaus spricht aus den Hamilton zugeschriebenen Artikeln dessen grundsätzlicher Glaube an die befriedende Wirkung einer Union aber auch seine Ablehnung sowohl von monarchistischen Tendenzen als auch einer reinen „democracy“. Die Autoren der "Federalist Papers" unterschieden nämlich zwischen „democracy“ und „republic“ als Formen der Volksherrschaft. Unter „democracy“ verstanden sie eine „Tyrannei der Mehrheit“, die sie scharf ablehnten, weil Minderheiten in ihr keinen ausreichenden Schutz genössen. Ihr Begriff der „republic“ unterscheidet sich davon wesentlich durch das Prinzip der Repräsentation. „Democracy“ entsprach nach ihrer Definition also der direkten Demokratie, wie sie in einigen griechischen Stadtstaaten der Antike verwirklicht worden war. Nach heutiger Auffassung ist Hamilton also durchaus als Demokrat zu bezeichnen, da die Begriffe „Demokratie“ und „Republik“ heute andere Bedeutungen haben als zu seiner Zeit.

James Madison, der 1809 Thomas Jefferson als vierter Präsident der USA nachfolgen sollte, war bereits 1776 an der Verfassung und der Bill of Rights für den Staat Virginia beteiligt gewesen. Von 1780 bis 1783 hatte er Virginia im Kontinentalkongress vertreten. Er gilt in der Historischen Forschung als einer der Initiatoren des Verfassungskonvents von Philadelphia. Der Großteil seiner Beiträge zu den "Federalist Papers" beschäftigte sich mit der inneren Ausgestaltung der Unionsverfassung; er vertrat die These einer Beschränkung der Demokratie auf die notwendigsten Bereiche. Erwähnenswert ist hierbei, dass er sich als maßgeblicher Gestalter der Verfassung von Hamilton und den Föderalisten wenige Jahre später abwandte und zusammen mit Thomas Jefferson die erste Opposition begründete, weil die Regierung ihre Befugnisse aus seiner Sicht viel zu weit interpretierte. Auch der berühmteste und bekannteste aller Artikel der "Federalist Papers", der "Fed. No. 10", stammt aus der Feder Madisons. Er behandelt Pluralismus, Parteibildung und Interessensgruppierungen, ihre Ursprünge und Legitimation.

Der dritte Autor war John Jay, Richter, US-Außenminister und später Gouverneur des Staates New York. Obwohl er nach der Abfassung von nur fünf Artikeln erkrankte und danach keine weiteren schrieb, darf sein Anteil an den "Federalist Papers" nicht unterschätzt werden: In den vermutlich von ihm verfassten Artikeln 2–5 schrieb er ein Leitbild amerikanischer Außenpolitik fest.

Dass drei Autoren an den "Federalist Papers" schrieben, die nicht zwangsläufig in allen Dingen der gleichen Meinung waren, führte zu manchen kleineren Abweichungen zwischen den einzelnen Briefen des "Publius". Manche der Artikel wurden von zwei Autoren gemeinsam verfasst; Artikel, an denen allen drei beteiligt waren, gibt es nach vorherrschender Auffassung nicht.

Die 1788 erschienenen "Federalist Papers" genießen in den USA auch heute noch große Popularität. Das liegt nicht zuletzt an ihrem Charakter als zeit- und damit intentionsnaher Interpretation der bis heute gültigen Verfassung. Sie gelten als philosophische Grundlage nicht nur des amerikanischen Staatsverständnisses, sondern des modernen, westlichen Demokratieverständnisses allgemein. Das in ihnen beschriebene Prinzip der Checks and Balances, der drei sich gegenseitig kontrollierenden und ausgleichenden Staatsgewalten, kam in dieser Ausformung nur in den USA zur Anwendung. Es gewann aber Vorbildfunktion für demokratische Verfassungen in der ganzen Welt. Die Autoren der "Federalist Papers" gelten daher auch als Wegbereiter des modernen, demokratisch verfassten Bundesstaates.

In der US-amerikanischen Politikwissenschaft spielen die "Federalist Papers" als theoretisch-philosophische Betrachtung einer Verfassung unter den Gesichtspunkten der Souveränitäts- und Vertragstheorie von Charles de Montesquieu sowie dem Eigentumsbegriff von John Locke eine wichtige Rolle. Vertrat Montesquieu noch die Meinung, republikanische Staatsordnungen eigneten sich nur für kleine überschaubare Einheiten wie etwa die griechischen Stadtstaaten der Antike, so entwickelten die "Federalists" seine Ideen weiter und wandten sie erstmals in der Geschichte auf einen großen Flächenstaat mit einer Bevölkerung an, in der nicht mehr jeder jeden kennen und – sofern er Macht ausübte – kontrollieren konnte. Mit ihrer Pluralismustheorie nahmen die Autoren der "Federalist Papers" die Gegenposition zur Identitätstheorie der Demokratie ein, deren Hauptvertreter Jean-Jacques Rousseau war. In Artikel 10 der "Federalist Papers" findet sich eine Parallele zu David Humes Essay "Idea of a Perfect Commonwealth". Hume argumentiert darin, dass Größe einem freiheitlichen Gemeinwesen Vorteile bringe, weil die damit verbundene Vielfalt die Bildung von Mehrheiten gegen Minderheiten erschwere.

Eine komplette deutsche Ausgabe der "Federalist Papers" liegt erst seit 1993 vor. Obwohl in philosophischen und historischen Fachkreisen seit jeher rezipiert, hat dieser Mangel dazu geführt, dass diese grundlegenden Texte der demokratischen Staatstheorie im allgemeinen deutschen Bewusstsein kaum verankert sind. Hannah Arendt lehnte Anfang der 1950er Jahre eine Einladung zu einem Politologen-Kongress in Deutschland mit dem Argument ab, diesen Leuten seien noch nicht einmal die "Federalist Papers" bekannt.

Im Folgenden sind die 85 Federalist Papers in der zeitlichen Abfolge ihres Erscheinens aufgelistet. Die jeweilige Autorenschaft wird durch die unterschiedliche farbliche Unterlegung gekennzeichnet. Die Artikel 18, 19 und 20, häufig als gemeinsames Werk von Hamilton und Madison betrachtet, werden von Historikern heute zumeist Madison allein zugeschrieben:

Des Weiteren ist die Autorenschaft der Artikel 49–53, 62 und 63 umstritten. Da die jüngste wissenschaftliche Literatur nahelegt, dass diese 12 Artikel ebenfalls von Madison stammen, sind sie in der Tabelle entsprechend farblich markiert.




</doc>
<doc id="13341" url="https://de.wikipedia.org/wiki?curid=13341" title="Rudolf Steiner">
Rudolf Steiner

Rudolf Joseph Lorenz Steiner (* 27. Februar 1861 in Kraljevec, Kaisertum Österreich, heute Kroatien; † 30. März 1925 in Dornach, Schweiz) war ein österreichischer Publizist, Esoteriker und Vortragsredner. Er begründete die Anthroposophie, eine spirituelle Weltanschauung, die an die anglo-indische Theosophie Blavatskys, das Rosenkreuzertum, die Gnosis sowie die idealistische Philosophie anschließt und zu den neuen mystischen Konzeptionen der Einheit von Mensch und Welt aus der Zeit um 1900 gezählt wird. Auf Grundlage dieser Lehre gab Steiner einflussreiche Anregungen für verschiedene Lebensbereiche, etwa Pädagogik (Waldorfpädagogik), Kunst (Eurythmie, anthroposophische Architektur), Soziales (Dreigliederung des sozialen Organismus), Medizin (anthroposophische Medizin), Religion (die Christengemeinschaft) und Landwirtschaft (biologisch-dynamische Landwirtschaft).

Rudolf Steiner entstammte einfachen Verhältnissen. Seine Eltern, der Bahnbeamte Johann Steiner (1829–1910) und Franziska Steiner, geborene Blie (1834–1918), kamen aus dem niederösterreichischen Waldviertel. Er hatte zwei jüngere Geschwister: Leopoldine (1864–1927), die als Näherin bis zu deren Tod bei den Eltern wohnte, und Gustav (1866–1941), der gehörlos geboren wurde und zeitlebens auf fremde Hilfe angewiesen war. Der Vater war zuvor als Förster und Jäger in Diensten des Horner Reichsgrafen Hoyos (eines Sohns von Graf Johann Ernst Hoyos-Sprinzenstein) tätig; als dieser ihm 1860 seine Zustimmung zur Hochzeit verweigerte, quittierte er den Dienst und fand eine Anstellung als Bahntelegrafist bei der Südbahn-Gesellschaft. Kurz hintereinander arbeitete er an drei Orten: am zweiten wurde Rudolf geboren, am dritten, in Mödling, lebte die Familie nur ein halbes Jahr. Anfang 1863 wurde er Stationsvorsteher in Pottschach, 1869 kam er nach Neudörfl, 1879 nach Inzersdorf, 1882 nach Brunn am Gebirge.

Bereits als Kind will Steiner erste Erfahrungen mit Hellsichtigkeit gehabt haben. So habe er im Alter von sieben Jahren in einer Vision seine Tante gesehen, die fast zeitgleich an einem weiter entfernten Ort Selbstmord begangen hatte. Da er diese inneren Erfahrungen mit niemandem teilen konnte, zog er sich oft in sich selbst zurück und interessierte sich zunehmend für Esoterik. In der Dorfschule von Neudörfl erhielt Steiner generationsübergreifenden Unterricht. Nach drei Jahren bestand er die Aufnahmeprüfung in die Bürgerschule von Wiener Neustadt. Besonders interessierte ihn die Geometrie. Als Jugendlicher las er nach eigenen Angaben Kants "Kritik der reinen Vernunft".
Nach dem Besuch der Realschule Wiener Neustadt konnte Steiner dank eines Stipendiums von 1879 bis 1883 an der Technischen Hochschule in Wien studieren. Seine Studienfächer waren Mathematik und Naturwissenschaften mit dem Ziel des Lehramts an Realschulen. Daneben besuchte er aber auch Lehrveranstaltungen in Philosophie, Literatur und Geschichte, teils auch an der Wiener Universität, wo er, ohne gymnasiale Matura in Latein, allerdings nur einen Gaststatus hatte. In Wien lebte er von 1884 bis 1890 im großbürgerlichen Haus der jüdischen Familie Specht, die ihn als Haus- und Nachhilfelehrer eingestellt hatte. Nach acht Semestern musste Steiner 1883 aus finanziellen Gründen dieses Studium ohne Abschlussexamen beenden. Da er ohne das Abschlussexamen in Österreich keinen akademischen Grad erreichen konnte, ging er nach Deutschland, wo die Universitäten im 19. Jahrhundert flexibler waren. Sein Versuch einer Dissertation an der Universität Jena 1884 scheiterte. Sieben Jahre später versuchte er es erneut: Er reichte an der Philosophischen Fakultät der Universität Rostock eine 48-seitige neue Schrift ein: "Die Grundfrage der Erkenntnistheorie mit besonderer Rücksicht auf Fichte's Wissenschaftslehre: Prolegomena zur Verständigung des philosophierenden Bewußtseins mit sich selbst". Mit der mündlichen Prüfung (Rigorosum) am 23. Oktober 1891 wurde er bei Heinrich von Stein mit der Bewertung „rite“ (ausreichend) zum Dr. phil. promoviert. Die Arbeit wurde 1892 in leicht erweiterter Fassung (Vorrede, Praktische Schlussbetrachtung und Vorwort) als "Wahrheit und Wissenschaft – Vorspiel einer Philosophie der Freiheit" im Verlag von Hermann Weißbach in Wien veröffentlicht. Der Buchhändler und Verleger Weißbach gab auch die Zeitschrift "Litterarischer Merkur – Kritisches und bibliographisches Wochenblatt" heraus, für den Steiner zwischen 1891 und 1893 fünfundvierzig Artikel verfasste.

Von 1882 bis 1897 war Steiner Herausgeber der naturwissenschaftlichen Schriften Johann Wolfgang von Goethes. Er besorgte in dieser Zeit zwei Ausgaben, erst im Rahmen der "Deutschen Nationallitteratur" Joseph Kürschners, dann (ab 1890) als Mitarbeiter des gerade gegründeten Goethe- und Schiller-Archivs in Weimar unter Leitung von Bernhard Suphan im Rahmen der sogenannten "Sophien-Ausgabe" – nach der Begründerin des Archivs, Großherzogin Sophie von Sachsen-Weimar-Eisenach –, heute bekannt als die "Weimarer Ausgabe". In Kürschners Nationalliteratur, wo Steiner dank der Empfehlung durch seinen Wiener Germanistik-Professor Karl Julius Schröer als Mitarbeiter verpflichtet wurde, bestand seine Aufgabe vor allem darin, erläuternde Kommentare und philosophische Einleitungen beizusteuern, während es sich bei der Weimarer Ausgabe meist um philologische Kleinarbeit handelte. Die ersten von Steiner herausgegebenen Goethe-Bände wurden allgemein mit Wohlwollen aufgenommen und in manchen Rezensionen sogar außerordentlich gelobt. Sie trugen erheblich dazu bei, das naturwissenschaftliche Werk Goethes, der bislang fast ausschließlich als Dichter wahrgenommen worden war, bekannt zu machen. Schon früh und bald mit zunehmender Schärfe wurde allerdings bemängelt, dass Steiner in seinen Einleitungen nicht Goethes Weltanschauung darstelle, sondern seine eigene. Auf teils vernichtende Kritik stieß Steiners philologische Arbeit im Rahmen der Weimarer Ausgabe, wo ihm zahlreiche handwerkliche Fehler und Nachlässigkeiten angelastet wurden. Steiner selbst, der anfangs mit großem Engagement an die Herausgebertätigkeit herangegangen war, betrachtete die Arbeit im Weimarer Archiv zunehmend als drückende Last und schrieb später, dass er auf das dort Geleistete „nie besonders stolz gewesen“ sei.

Daneben gab Steiner auch die Werke des Philosophen Arthur Schopenhauer und des Dichters Jean Paul heraus. Für mehrere Lexika verfasste er Beiträge zu naturwissenschaftlichen Themen. Zeitweilig war er auch Redakteur der in Wien erscheinenden "Deutschen Wochenschrift". Seinen Lebensunterhalt bestritt er bis 1890 überwiegend als Erzieher und Hauslehrer der vier Söhne eines jüdischen Kaufmanns. Dann fand er mit der Berufung an das Weimarer Archiv als Goetheforscher ein bescheidenes Auskommen.

Zu den zahlreichen Kontakten, die Steiner in seiner Wiener Zeit (1879–1890) pflegte, gehören der Esoteriker Friedrich Eckstein, der ihn mit der Theosophie Helena Petrovna Blavatskys bekannt machte, und die Frauenrechtlerin Rosa Mayreder, seine wichtigste Gesprächspartnerin bei der Ausgestaltung seiner Freiheitsphilosophie. In der Weimarer Zeit knüpfte er Kontakte zu Herman Grimm, Otto Erich Hartleben, Ernst Haeckel und Elisabeth Förster-Nietzsche. Ab 1892 wohnte er bei der gerade verwitweten Anna Eunike (1853–1911) und ihren fünf Kindern, die 1899 seine erste Ehefrau wurde.

In dieser Zeit entstanden einige philosophische Werke:

Seine Erkenntnistheorie, die er in der Auseinandersetzung mit Goethes naturwissenschaftlichen Schriften entwickelt hatte, nahm in Anlehnung an den Deutschen Idealismus und namentlich an Johann Gottlieb Fichte ihren Ausgangspunkt im erkennenden "Subjekt". Entscheidend war dabei für Steiner die Erfahrung des eigenen Denkens: Die „Beobachtung“ des Denkens sei die „allerwichtigste“ Wahrnehmungsleistung des Menschen. Denn nur das, was er selbst denke, könne er vollkommen durchschauen. Damit sei „ein fester Punkt gewonnen, von dem aus man mit begründeter Hoffnung nach der Erklärung der übrigen Welterscheinungen suchen kann“.

Jede Art des Seins, die weder durch Wahrnehmung noch durch Denken erfahrbar sei, wies Steiner als „unberechtigte Hypothesen“ zurück. Mit dieser positivistischen Abweisung jeglicher transzendenten „Realität“, deren Existenz und zugleich prinzipielle Nicht-Erkennbarkeit andere Philosophen voraussetzten (Agnostizismus), stellte sich Steiner auch in Gegensatz zu der von Kant geprägten Universitäts-Philosophie seiner Zeit. Für den jungen Goethe-Forscher gab es nur "eine" Welt und somit keine prinzipiellen Grenzen des Erkennens. In diesem Sinn bezeichnete Steiner seine Weltanschauung auch als „Monismus“. In einem Brief bekannte er:

Steiners Monismus war jedoch nicht mit dem materialistischen Monismus identisch, den Ernst Haeckel fünf Jahre später (1899) in seinem Buch "Die Welträtsel" popularisierte. Allerdings bekannte sich Steiner auch nach dem Erscheinen der "Welträtsel" zu Haeckel, obwohl dieser radikal – und sehr modern – die Konsequenzen aus seiner monistischen Weltsicht zog. So heißt es in den "Welträtseln"
Steiners Verhältnis zu Haeckel war bei aller ostentativen Parteinahme durchaus zwiespältig. Als Haeckels "Die Welträtsel" erschien, begleitet von heftigen Angriffen auf den Autor vor allem von Seiten der Kirchen, stellte sich Steiner in einer Aufsatzserie ("Haeckel und seine Gegner", 1899) rückhaltlos auf Haeckels Seite. Auch später, in seiner theosophischen Phase, bezeichnete er Haeckels kämpferisches Eintreten für die Evolutionstheorie als . Die Problematik dieser Haltung war Steiner selbst durchaus bewusst. So formulierte er eine mögliche Kritik aus der Sicht eines Haeckel-Anhängers: Die Berufung Steiners auf Haeckel gilt als wichtiges Deutungsproblem für das Verständnis von Steiners intellektueller Entwicklung. So heißt es etwa in einer Rezension der Wiederauflage von Karl Ballmers Aufsatz "Ernst Haeckel und Rudolf Steiner": Übereinstimmend auch Gerhard Wehr:

Seine monistische Erkenntnistheorie betrachtete Steiner aber nur als „Vorspiel“, als „philosophischen Unterbau“ einer radikal individualistischen Freiheitsphilosophie, mit welcher er eng an Friedrich Nietzsche und Max Stirner anschloss. In der "Philosophie der Freiheit" werden diese Denker zwar nicht erwähnt, doch schon im folgenden Jahr erschien "Friedrich Nietzsche, ein Kämpfer gegen seine Zeit" (1895), worin Steiner den „Einklang“ mit den Anschauungen Nietzsches betonte und bedauerte, dass dieser seine Lehren auf Schopenhauer statt auf Stirner gegründet hatte. Den Ausspruch Nietzsches: kommentierte Steiner mit den Worten:

Diejenigen Zeitgenossen, die Nietzsche wegen seiner Aufkündigung moralischer Bindungen einen „gefährlichen Geist“ genannt hatten, bezeichnete Steiner abfällig als „kleinlich denkende Menschen“. Dagegen hob er Nietzsches „Übermenschen“ hervor, der in Steiners Deutung mit dem „Eigner“ Max Stirners identisch ist: Der Übermensch ist für Steiner also der Der Menschentyp, der heute . Der „höherwertige Typus“ dagegen sei frei und nichts als er selbst.

Steiner bewunderte den autoritäts- und wahrheitskritischen Gestus radikaler Denker wie Nietzsche und Stirner. Bei Stirner gefiel ihm die Überhöhung des Individuums. Stirners Satz: , kommentierte er mit den Worten: Bei Nietzsche konnte Steiner an die Idee des „freien Geistes“ anknüpfen, der sich über Gott und Wahrheitsglauben emporschwingt: 

Diese Kampfansage an jede vorgegebene Wahrheit und Autorität verband Steiner mit Stirner und Nietzsche. Im Sinne der „Egoität“ (Steiner) begrüßte er Nietzsches Wort vom „Tod Gottes“ und der Stellung des Menschen „jenseits von Gut und Böse“. Er proklamierte: An anderer Stelle zitierte er zustimmend ein Zitat des Stirner-Biographen John Henry Mackay, in dem es hieß
<poem style="margin-left:3em;">
Ich glaubte nie an einen Gott da droben,
Den Lügner oder Toren nur uns geben.
Ich sterbe – und ich wüßte nichts zu loben
Vielleicht nur Eins – daß wir nur einmal leben!
</poem>

Solche Wendungen illustrieren Steiners Ablehnung eines Glaubens an ein Jenseits im Sinne agnostizistischer Vorstellungen und der Idee eines allmächtigen Gottes. (Obgleich Steiner das Gedicht von Mackay vollständig zitierte, bezog sich seine Zustimmung nur auf den ersten Satz. Bei der von Christoph Lindenberg vertretenen Auffassung, Steiner habe den Reinkarnationsgedanken in jüngeren Jahren verworfen, handelt es sich um eine Fehlinterpretation einer Rezension eines Buches von Gustav Hauffe.)

Der von Steiner verehrte Stirner hatte in seinem Hauptwerk "Der Einzige und sein Eigentum" geschrieben: Auch für den damals Stirners Grundposition nahestehenden Steiner hatte das Menschenleben nur den Zweck und die Bestimmung, die der Mensch ihm selbst verleihe: Damit ist die "Philosophie der Freiheit" ein Bekenntnis zu Individualismus und Monismus. Der Monismus leugnet eine Realität jenseits individueller Wahrnehmung und Erkenntnis. Reale und geistige Welt fallen nicht dualistisch auseinander, sondern sie sind "eins". Eduard von Hartmann urteilte, diese Position, die er der Phänomenologie zuordnete, führe mit unausweichlicher Konsequenz zum Solipsismus, absoluten Illusionismus und Agnostizismus.

Im Sinne Stirners und Nietzsches proklamiert Steiner: Hinter handelnden Menschen sieht dieser Monismus dabei nicht Zwecke einer ihm fremden Weltlenkung, sondern nur eigene, menschliche Zwecke. Gegenüber der Autoritäts- und Jenseitsgläubigkeit positioniert Steiner im Sinne des Idealismus das „lebendige Denken“ des „Ichs“ und den „freien Geist“.

Steiners 1894 erschienenes Hauptwerk "Die Philosophie der Freiheit", von dem nur wenige Exemplare verkauft wurden, fand in der akademisch-philosophischen Fachwelt nur geringe Beachtung. Der renommierte deutsche Philosoph Eduard von Hartmann schickte Steiner sein Leseexemplar ausführlich glossiert zurück, verzichtete aber auf eine öffentliche Rezension (er kritisiert Steiners Buch später beiläufig in einer Fussnote). Hartmann überließ eine ausführliche kritische Rezension seinem Schüler Arthur Drews. Eine weitere ausführliche, jedoch weitgehend zustimmende Rezension verfasste Bruno Wille.

Es gelang Steiner nicht, in der akademischen Philosophie Fuß zu fassen. Ein Habilitationsversuch im Jahre 1894 scheiterte. Ernst Haeckel, der aus dem Umfeld Steiners um Vermittlung einer Stelle an der Universität Jena gebeten worden war, versagte jegliche Unterstützung.

Der Neukantianer Karl Vorländer schrieb in seiner "Geschichte der Philosophie", dass die später durch die Anthroposophie drapierte ‚Theosophie‘ Steiners, der u. a. mit einer anarchistisch gefärbten ‚Freiheits‘-Philosophie begonnen habe, keine Aufnahme in die Geschichte der Philosophie verdiene. Steiners philosophisches Werk war, nach einer kurzen Rezeptionsphase, die hauptsächlich die "Philosophie der Freiheit" betraf, außer in anthroposophischen Kreisen, praktisch vergessen. Aus anthroposophischer Sicht erscheint es bis in die Gegenwart als „ein tiefes Rätsel, weshalb Rudolfs Steiners kognitive Leistung von der akademischen Welt fast völlig ignoriert oder im besten Falle als Auswuchs eines seltsamen Sektierergeistes behandelt wird“, wie Marek Majorek im Jahre 2002 in seiner Dissertation konstatierte.

Kurze Zeit arbeitete Steiner unter Elisabeth Förster-Nietzsche am Nachlass Nietzsches und war als Herausgeber der Werke im Gespräch. Im Rahmen dieser Tätigkeit erstellte er die erste Nietzsche-Bibliographie und das erste Verzeichnis von Nietzsches Bibliothek überhaupt. Letzteres wurde zur Grundlage aller später publizierten Kataloge. Dabei konnte Steiner auch die noch unveröffentlichte Autobiographie Nietzsches, "Ecce Homo", einsehen und durfte dem geistig umnachteten Denker bei einem Besuch am 22. Januar 1896 persönlich gegenübertreten. Nach einem Eklat um die Frage der Herausgeberschaft brach Steiner mit Förster-Nietzsche und machte 1900 als erster auf die zweifelhaften Machenschaften des Nietzsche-Archivs im Rahmen von dessen Nietzsche-Ausgabe aufmerksam. Steiner hatte sich nicht nur in einem Nietzsche-Buch, sondern auch in zahlreichen Zeitschriftenaufsätzen und Rezensionen als einer der ersten Vorkämpfer des damals noch nicht akzeptierten Nietzsche positioniert.

Einen Teil seines Lebensunterhalts bestritt Steiner weiterhin mit Herausgebertätigkeiten, etwa indem er von 1897 bis 1900 zusammen mit Otto Erich Hartleben das "Magazin für Litteratur" in Berlin herausgab. In dieser Zeit erschienen zahlreiche Aufsätze von Steiner zu künstlerischen, philosophischen und politischen Themen. Seine seit etwa 1894 bestehende Bekanntschaft mit dem deutschen Dichter John Henry Mackay wurde zu einer engen Freundschaft. Steiner identifizierte sich zeitweilig so sehr mit dem von Mackay vertretenen individualistischen Anarchismus, dass er in der von ihm redigierten Zeitschrift ein durchaus riskantes öffentliches Bekenntnis zu ihm abgab:

Dies und eine Kampagne für Alfred Dreyfus führte zu Leserprotesten und erwies sich als der Auflagenhöhe des "Magazins" abträglich. Hartleben legte im März 1900 seine Mitherausgabe des Magazins wegen „inferioren Klatsches“ – gemeint war die Auseinandersetzung mit dem Nietzsche-Archiv, die Steiner in der Publikation führte – nieder. Im September 1900 trat auch Steiner von seiner Redaktionsaufgabe zurück.

Steiner befand sich zu dieser Zeit in ernsthaften finanziellen Nöten. Aus seinem Umfeld wurde bereits für seine Wiener Zeit berichtet, er habe in einer gewesen. So schlecht sei es ihm auch bis in die Weimarer, ja auch Berliner Zeit gegangen. Zudem ist eine Zerrüttung des Lebenswandels überliefert. Steiner zechte nächtelang mit seinen Dichter-Freunden, teilweise sei er erst am nächsten Nachmittag nach Hause gekommen. Rosa Mayreder, die Vertraute aus der Wiener Zeit, meinte sogar, er sei . Auch 1903 hieß es noch aus seinem direkten Umfeld, man vermöge . Während sich Steiner in den Weimarer Jahren in gutbürgerlichen Zirkeln bewegt hatte, wandte er sich in den ersten Berliner Jahren proletarisch geprägten Außenseiterkreisen zu. Seine Kontakte reflektierten das Motto, welches er 1899 für sein "Magazin" gewählt hatte: „Vielseitigkeit und Vorurteilslosigkeit“. So hielt er von 1899 bis 1904 Kurse an der sozialistisch geprägten Berliner Arbeiter-Bildungsschule. Der Schriftsteller Max Halbe beschreibt den damaligen Steiner als . Wolfgang G. Vögele charakterisiert Steiners damaligen Umgang wie folgt: . Dazu gehörte auch der Kreis um Otto Erich Hartleben, der sich mit antibürgerlicher Provokationsgeste „Der Verbrechertisch“ nannte.

Der sozialistische Kunstkritiker John Schikowski schrieb am 31. März 1925 in einem Nachruf für den sozialdemokratischen "Vorwärts", auf eine gemeinsame Zeit mit Steiner in Berlin zurückblickend:

Auf die Zeit als Bohemien blickte Steiner selbst nur ungern zurück. 1904 rechtfertigte er sich in einem Brief an seine Frau mit den Worten:

Als bekannter Nietzsche-Kenner war Steiner nach Nietzsches Tod im Jahre 1900 als Vortragsredner über den radikalen Denker gefragt. Im September 1900 hielt er auch in der Theosophischen Bibliothek des Grafen Cay von Brockdorff in Berlin je einen Vortrag über Nietzsche und über „Goethes geheime Offenbarung“. Diese Vorträge wurden gut aufgenommen, und Steiner konnte gleich anschließend mit einer Vortragsreihe über "Die Mystik" beginnen (26 Vorträge bis April 1901). Es schlossen sich im nächsten Jahr Vorträge über "Das Christentum als mystische Tatsache" an, und bald waren die Theosophen, denen Steiner bis dahin ablehnend gegenübergestanden hatte, sein wichtigstes Publikum, bei dem er durch seine Reden sogar seinen Lebensunterhalt bestreiten konnte. Als 1902 eine Deutsche Sektion der Theosophischen Gesellschaft gegründet wurde, konnten sich die deutschen Theosophen nicht auf einen Vorsitzenden einigen. Steiner war der Kompromisskandidat, den man zum Generalsekretär wählte, weil man sich auf kein „älteres Mitglied als Kandidaten für dieses Amt einigen konnte“.

Die Theosophische Gesellschaft (TG) war eine esoterische, teils als obskur geltende Vereinigung, in der sich global Menschen zusammenschlossen, die auf der Suche nach einem neuen spirituellen Weltbild waren. Die Lehren der 1891 verstorbenen Mitbegründerin Helena Petrovna Blavatsky spielten dabei eine tragende Rolle. Die Deutsch-Ukrainerin hatte einen stark durch östliche Philosophien beeinflussten Okkultismus vertreten und gilt heute als die bedeutendste Wegbereiterin der „modernen“ Esoterik gegen Ende des 19. Jahrhunderts. Schon zu Lebzeiten waren ihr aber auch – vor allem im Zusammenhang mit Briefen fraglicher Herkunft, die von indischen Meistern stammen sollten – betrügerische Machenschaften vorgeworfen worden. Ihre Nachfolgerin in der Leitung der TG, Annie Besant, war vor allem dem Hinduismus zugewandt.

Steiner hatte in seinen Berliner Vorträgen, bevor er auch nur Mitglied der TG geworden war, in zwei Punkten dargelegt, worin er seiner Meinung nach von Blavatskys theosophischer Lehre abwich. Er sprach dem menschlichen „Wesenskern“, dem Ich, eine zentrale Bedeutung auf dem spirituellen Entwicklungsweg zu. Zum andern betonte Steiner die Einmaligkeit und Einzigartigkeit der Person Jesu Christi, der von den älteren Theosophen nur als ein hochentwickelter Mensch (ein sogenannter „Meister“) neben anderen angesehen wurde. Diese Ansichten publizierte Steiner – als schriftliche Fassungen seiner Vorträge in der Theosophischen Bibliothek – in den Büchern "Die Mystik im Aufgange des neuzeitlichen Geisteslebens" (1901) und "Das Christentum als mystische Tatsache" (1902). Diese Eigenständigkeit stand im Einklang mit dem ursprünglichen Grundprinzip der 1875 gegründeten Gesellschaft: „Keine Religion höher als die Wahrheit!“

Steiner erhob gegenüber dem in der TG tonangebenden Orientalismus den Anspruch, „Theosophie“ eigenständig aus dem abendländischen Geistesleben heraus zu entwickeln. Schon 1903 bekannte er sich aber auch zur Lehre von Reinkarnation und Karma, die er seinerseits als bezeichnete und entsprechend abzuleiten suchte.
Das brachte ihm schon früh den Vorwurf ein, Eklektiker zu sein.

Entsprechend seinem inneren Schritt vom Denk-Erleben zum Geist-Erleben und seiner neuen Zuhörerschaft hatte sich auch Steiners Terminologie gegenüber seinen früheren Schriften stark verändert, etwa wenn er nun von "höheren Welten" und "Mysterien" sprach. Das Eintreten für die theosophische Bewegung führte zum Bruch mit zahlreichen früheren Freunden. Bruno Wille etwa warnte Steiner, der Begriff Theosophie sei 

Innerhalb der Theosophischen (und später der Anthroposophischen) Gesellschaft trat Steiner vor allem als Vortragsredner in Erscheinung. In den gut zwei Jahrzehnten bis zu seinem Tod hielt er rund 6000 Vorträge, hauptsächlich in den immer zahlreicher werdenden Ortsgruppen („Zweigen“) in Deutschland und später auch in anderen europäischen Ländern. Neben diesen nur für Mitglieder zugänglichen Vorträgen wurden regelmäßig auch öffentliche Vorträge organisiert. Eine schriftliche Publikation der Vorträge war ursprünglich nicht vorgesehen; da aber bald unautorisierte Mitschriften kursierten, beauftragte man Stenografen mit der Aufzeichnung der Vorträge. So entstanden etwa 4500 stenografische Mitschriften, die teils schon zu Steiners Lebzeiten, überwiegend aber erst nach seinem Tod in Buchform publiziert wurden und heute auch im Internet frei verfügbar sind. Sie machen den größten Teil von Steiners heute schriftlich vorhandenem Werk aus und sind insofern nicht unproblematisch, als sie – von wenigen Ausnahmen abgesehen – von Steiner selbst nie durchgesehen wurden.

1904 erschien das Buch "Theosophie" (mit dem Untertitel "Einführung in übersinnliche Welterkenntnis und Menschenbestimmung"), in dem er die jetzt von ihm vertretene Lehre erstmals ausführlich darlegte. Anknüpfend an Johann Gottlieb Fichte sprach er darin von einem „geistigen Auge“, das es ermögliche, neben der gewohnten physischen Welt noch eine seelische und eine geistige Welt wahrzunehmen und zu erforschen. Während traditionelle Esoteriker die okkulten Erkenntnisse als über ein Lehrer-Schüler-Verhältnis vermittelte „Einweihung“ ansahen, wollte Steiner zu einer selbstbestimmten Erkenntnisleistung anleiten. Diese Anleitungen vertiefte er in der Aufsatzserie "Wie erlangt man Erkenntnisse der höheren Welten?" (1904/05).

In der parallel begonnenen Aufsatzserie "Aus der Akasha-Chronik" (1904–1908) griff Steiner nun vermehrt Themen aus der Lehre Blavatskys und anderer ihr nahestehender Okkultisten auf, darunter die Lehre von den „Wurzelrassen“. Trotz einzelner Abweichungen und eigenständigen Schwerpunktsetzungen hatte sich Steiner anscheinend den Grundrahmen der theosophischen Weltsicht zu eigen gemacht.

Eine ausführliche Zusammenfassung seiner esoterischen Lehre gab er 1910 unter dem Titel "Die Geheimwissenschaft im Umriss" heraus – ein Titel, der sich an Blavatskys Hauptwerk "Die Geheimlehre" ("The Secret Doctrine", 1888) anlehnt. In dieser Publikation tritt (wie schon in "Theosophie") die von Blavatsky entlehnte Terminologie wieder weitgehend zurück und stattdessen werden abendländische Themen wie die christliche Hierarchienlehre aufgegriffen. Dieses Buch wurde noch zu Lebzeiten Steiners vierzehn mal neu aufgelegt; wenige Wochen vor seinem Tod (1925) schrieb er noch das Vorwort zur 16. Auflage. Auch von der "Theosophie" gab es in dieser Zeit neun Neuauflagen.

Die Akasha-Chronik, nach theosophischer Lehre die Aufzeichnung des gesamten planetaren Schicksals der Erde, die sich in der spirituellen Welt befinden soll, beschrieb Steiner als eine seiner „geistigen“ Wahrnehmung zugängliche „Schrift“. Damit verband er den Anspruch, er könne vergangene Ereignisse übersinnlich wahrnehmen; 1913 beschrieb er diese Fähigkeit als einen „nach rückwärts gerichteten hellseherischen Blick“. Auch an anderer Stelle nahm Steiner wiederholt für sich in Anspruch, seine „Geistesforschung“ basiere auf einer angeborenen Fähigkeit zur Hellsichtigkeit. Diese Empirie des Übersinnlichen, in der der menschliche Geist nicht nur in Begriffen und Ideen gedacht, sondern unmittelbar erfahren werden könne, müsse aber den Kriterien der Wissenschaft unterworfen werden, um „Geisteswissenschaft“ in dem von ihm intendierten Sinne zu werden. Als Grundlage seiner „geisteswissenschaftlichen“ Darstellungen unterschied Steiner mehrere Erkenntnisstufen. Neben der gewöhnlichen Erkenntnis gebe es demnach die „imaginative“, die „inspirative“ und die „intuitive“ Erkenntnis. Durch strenge Schulung lassen sich dieser Lehre zufolge immer höhere Erkenntnisstufen erreichen, die einen erkenntnismäßigen Zugang zur übersinnlichen Welt ermöglichen. Diese „Geisteswissenschaft“ soll laut Steiner Menschen dazu befähigen, die physische Welt in ihrem Zusammenhang mit der „geistigen“ Welt zu verstehen und aus diesem Verständnis heraus die Welt zu gestalten. Von diesem Standpunkt aus verknüpfte Steiner seine frühen Ansätze zu einer „Philosophie der Denk-Erfahrung“ mit so unterschiedlichen religiösen Vorstellungen und Traditionen wie Karma, Reinkarnation, Okkultismus und Rosenkreuzertum.

Die Dreiteilung in imaginative, inspirative und intuitive Erkenntnis wurde die Grundlage für die Klassen von Steiners „Esoterischer Schule“, in der er privat Schüler in „geisteswissenschaftlicher“ Erkenntnis ausbildete. 1904 richtete er die erste Klasse ein, in der theosophische Literatur gelesen wurde, 1907 die zweite, die eher rituell ausgerichtet war. Für sie adaptierte Steiner den Memphis-Misraïm-Ritus, ein irreguläres freimaurerisches Hochgradsystem, in dem er auch selbst Mitglied wurde. In diesem Zusammenhang kam Steiner auch in Kontakt mit dem deutschen Okkultisten Theodor Reuss. Ob er auch Mitglied in dessen sexualmagischen Ordo Templi Orientis wurde, ist umstritten. Beide Klassen arbeiteten bis 1914, die dritte, die die Schüler in ihrem täglichen Berufsleben schulen sollte, kam anscheinend nicht zustande.

Über die Jahre kam es zu einer zunehmenden Entfremdung zwischen der Weltorganisation der TG und den deutschen Sektionen und Logen. Steiner war ein wesentlicher Protagonist in dieser Auseinandersetzung. 1907 empörte er sich über Annie Besants Behauptung, die Mahatmas wären am Totenbett von Blavatskys Nachfolger Henry Steel Olcott erschienen und hätten sie als Nachfolgerin bestimmt. Daraufhin trennte er seine eigene Esoterische Schule von der der TG. Die nächste Krise entstand, als einige Vertreter der TG – allen voran Charles Webster Leadbeater – den sechzehnjährigen Jiddu Krishnamurti im Jahre 1911 als kommenden Maitreya (Weltlehrer) propagierten und dieser in manchen Kreisen als „Reinkarnation Christi“ aufgefasst wurde. Steiner lehnte den zunehmenden Kult um Krishnamurti und den in diesem Zusammenhang gegründeten Order of the Star in the East ab. Der Vorstand der deutschen Sektion bat die Mitglieder des „Ordens“, entweder aus dem Orden oder aus der deutschen Sektion auszutreten und forderte in einem Telegramm den Rücktritt Annie Besants als internationale Präsidentin der Theosophischen Gesellschaft Adyar. Diese löste am 7. März 1913 die von Steiner geleitete deutsche Sektion formell auf. An ihre Stelle trat eine erneuerte deutsche Sektion unter Leitung von Wilhelm Hübbe-Schleiden. Der bereits am 28. Dezember 1912 in Köln gegründeten Anthroposophischen Gesellschaft traten die meisten der 2500 ehemaligen Mitglieder bei und innerhalb Jahresfrist kamen über 1000 weitere Mitglieder dazu. In der neuen Organisation hatte Steiner nicht mehr selbst die Leitung inne – den Vorstand bildeten Marie von Sivers, Michael Bauer und Carl Unger –, er war aber der wichtigste Vortragsredner und Ehrenpräsident.

Nach dem Bruch mit der Theosophischen Gesellschaft veränderte Steiner auch den terminologischen Rahmen seiner Lehre. Dabei war „Anthroposophie“ jedoch im Wesentlichen nur eine andere Bezeichnung für das, was er bis zum Ausschluss aus der Theosophischen Gesellschaft als „Theosophie“ vertreten hatte. Seine Bücher "Theosophie" (1904) und "Geheimwissenschaft im Umriß" (1910) blieben insofern auch die Standardwerke der Anthroposophie. In Neuauflagen von Steiners bisherigen Werken wurde die Bezeichnung „Theosophie“ weitgehend durch „Anthroposophie“ oder „Geisteswissenschaft“ ersetzt. Die Wahl dieser Bezeichnungen erläuterte Steiner folgendermaßen:

Der späte Steiner wandte sich verstärkt Kunst und Architektur zu. In den Jahren 1910 bis 1913 wurden in München seine vier „Mysteriendramen“ uraufgeführt. Von 1913 bis 1922 entstand unter seiner künstlerischen Leitung in Dornach bei Basel das Goetheanum als Zentrum der Anthroposophischen Gesellschaft und Sitz der geplanten "Freien Hochschule für Geisteswissenschaft". Nachdem der Holzbau in der Silvesternacht 1922/23 abgebrannt war (die zeitgenössische Presse vermutete Brandstiftung seitens militanter Steiner-Gegner), entwarf Steiner ein zweites, größeres Goetheanum aus Beton, das 1928, also erst nach seinem Tod, fertiggestellt wurde. Der expressive Baustil des aus Stahlbeton gefertigten neuen Goetheanums im Gegensatz zu seinem impressionistisch geprägten Vorgänger zeigt, dass Steiners Architekturstil binnen weniger Jahre einen radikalen Wandel erfuhr. Dieser Stil sollte – unter anderem unter dem Stichwort Organische Architektur – eine weit verzweigte Wirkung auf die moderne Architektur entfalten (eine Beschäftigung mit Steiner lässt sich etwa zeigen für Le Corbusier, Henry van de Velde, Frank Lloyd Wright, Erich Mendelsohn, Hans Scharoun, Frank O. Gehry sowie Hinrich Baller).

Steiner, der bereits vor und während des Ersten Weltkriegs gelegentlich im Austausch mit führenden Politikern gestanden hatte, wirkte besonders nach Kriegsende auch auf politischer Ebene. So publizierte er 1919 einen „Aufruf an das deutsche Volk und an die Kulturwelt“, den auch Hermann Bahr, Hermann Hesse und Bruno Walter unterzeichnet hatten. In dieser Zeit trat er für klassische Anliegen eher konservativer und nationaler Kreise ein. Vor allem die Kriegsschuldfrage war ihm ein politisches Anliegen. So wirkte er 1919 an der Herausgabe einer politischen Broschüre unter dem Titel "Die ‚Schuld‘ am Kriege" mit, um die öffentliche Meinung im Vorfeld der Friedensverhandlungen in Versailles zu beeinflussen. Bei dem Dokument handelte es sich um die bereits 1914 niedergelegten Erinnerungen von Generalstabschef Helmuth von Moltke, in denen dieser das Versagen des Kaisers vor Kriegsausbruch beschrieben hatte. In seinem Einleitungstext sprach Steiner das Problem an, dass bei Kriegsausbruch in Deutschland allein militärische Erwägungen eine Rolle spielten und von daher der Erste Weltkrieg als „Notwendigkeit“ erschien. Im Kampf gegen den Kriegsschuldvorwurf an Deutschland finanzierte Steiner eine verschwörungstheoretische Schrift, in der Freimaurern, Juden und Theosophen die Schuld am Ersten Weltkrieg angelastet wurde. Diese Schrift des Okkultisten Karl Heise, die mit einer Einleitung Steiners versehen war, wurde später von den Nationalsozialisten rezipiert.

Die Zeit in der Anthroposophischen Gesellschaft erwies sich für Steiner als ausgesprochen produktiv. Er trat in den unterschiedlichsten Lebensbereichen mit eigenen Ideen hervor und wirkte in einer enormen thematischen Breite als Impulsgeber und Erneuerer. So betätigte er sich unter anderem als Reformpädagoge (Waldorfpädagogik), Sozialreformer (Soziale Dreigliederung) und auf dem Gebiet der Kunst (Architektur, Bewegungskunst, Sprachgestaltung). Er begründete mit der Ärztin Ita Wegman die anthroposophische Medizin und lieferte die weltanschauliche Grundlage für eine Religionsgemeinschaft (Die Christengemeinschaft). Im Juni 1923 gab er mit Vorträgen in Koberwitz bei Breslau die Anregung zur Begründung der biologisch-dynamischen Landwirtschaft; diese Vorträge erfolgten unter anderem auf Einladung von Carl Graf von Keyserlingk. Viele von Steiners Ideen sind bis heute wirkungsmächtig. So erleben etwa Waldorfschulen und -kindergärten, biologisch-dynamischer Landbau ("Demeter") und anthroposophische Medizin einschließlich der anthroposophisch orientierten Krankenhäuser wachsende Popularität.
Rudolf Steiner und seine zweite Frau Marie von Sivers (Heirat 1914, dann Marie Steiner-von Sivers, keine Kinder) wohnten von 1903 bis 1923 in Berlin-Schöneberg, Motzstraße 30, wo eine Gedenktafel an sie erinnert. Allerdings war Steiner als Vortragsredner und als Vorsitzender der Theosophischen bzw. Anthroposophischen Gesellschaft viel auf Reisen. Nach dem Ende des Krieges 1918 hielt er sich nur noch selten in Berlin auf.

Im Jahre 1924 zog der chronisch erschöpfte Steiner aus dem mit seiner Frau bewohnten "Haus Hansi" aus und richtete sein Sterbelager im Atelier der Schreinerei ein, wo er Zugriff auf seine Bücher und Arbeitsmaterialien hatte. Am 1. Oktober 1924 bezog seine Geliebte Ita Wegman ein Nebenzimmer im Atelier, um den Kranken pflegen und medizinisch versorgen zu können. Am 29. März 1925 informierte Wegman Steiners Ehefrau, die sich auf einer Eurythmie-Reise befand, über den desaströsen Gesundheitszustand ihres Mannes, doch die unverzüglich Anreisende fand ihren Mann nicht mehr lebend vor. Steiner starb am 30. März 1925 nach mehrmonatiger schwerer Krankheit in Dornach in der Schweiz. Über die Todesursache und über die Art der vorangegangenen Erkrankung gibt es keine gesicherten Erkenntnisse. Erwogen werden Prostata- oder Magen-Darm-Krebs oder eine Vergiftung.

Steiners Leichnam wurde gegen seinen Willen verbrannt. Die Urne wurde fast 70 Jahre lang im Goetheanum aufbewahrt, bis man seine Asche am 3. November 1992 im "Gedenkhain" des Goetheanums beisetzte. Neben seiner Urne und einem Gedenkstein für ihn ist auch die Urne des Dichters und Schriftstellers Christian Morgenstern beigesetzt, der eine besondere geistige Zusammengehörigkeit zu Steiner empfand.

Der späte Steiner wollte sein theosophisch und anthroposophisch geprägtes Werk der Jahre ab 1900 als konsequente Weiterentwicklung seines bis dahin entstandenen philosophischen Werks verstanden wissen. Alle Widersprüche bezeichnete er an verschiedenen Stellen als „scheinbar“ oder „vordergründig“. In seinen unter dem Titel "Mein Lebensgang" veröffentlichten autobiographischen Notizen, die allerdings nicht immer zuverlässig sind, zeichnete Steiner das Bild einer folgerichtigen geistigen Entwicklung. Demgegenüber sehen viele Beobachter bei ihm um 1900 eine tiefe geistige Zäsur, die sich unter anderem an seiner veränderten Haltung gegenüber dem Christentum zeigen lässt. Ein Zeitgenosse sprach rückblickend von einer , der Biograph Gerhard Wehr von „Krise und Wandlung“. Steiner, so der Chronist weiter, habe um die Jahrhundertwende eine . Ein weiteres Beispiel für von Steiner im Nachhinein übertünchten Disparitäten sind Schilderungen Steiners zu seinem Verhältnis zu Nietzsche, die David Marc Hoffmann in "Zur Geschichte des Nietzsche-Archivs" (1991) und "Rudolf Steiner und das Nietzsche-Archiv" (1993) als falsch nachgewiesen hat.

Der frühe Steiner war als Individualist, Positivist und Freidenker hervorgetreten, der sich nicht scheute, sich auch auf skandalumwobene Philosophen wie Stirner, Nietzsche und Haeckel zu berufen. Sein Freidenkertum gipfelte in einer Verächtlichmachung von Religion und Glauben. Dem Christentum maß er geradezu pathologische Züge bei.
Der Glaube an Gott und Christus erschien Steiner als Zeichen krankhafter Schwäche, der er ein „gesundes menschliches Denken“ gegenüberstellte: An anderer Stelle hatte er geschrieben, der Mensch der Zukunft werde . Solche Sätze erscheinen wie ein Nachhall von Nietzsches Kritik am christlichen Glauben, wie dieser sie unter anderem in "Der Antichrist – Fluch auf das Christenthum" niedergeschrieben hatte. Dort hieß es etwa: „Das Christentum war bisher das größte Unglück der Menschheit“ oder: „Das Christentum hat die Partei alles Schwachen, Niedrigen, Mißratenen genommen, es hat ein Ideal aus dem Widerspruch gegen die Erhaltungs-Instinkte des starken Lebens gemacht.“ Dieser Angriff Nietzsches auf das Fundament christlicher Glaubensinhalte hatte den jungen Steiner tief beeindruckt. Einer Briefpartnerin schrieb er: . Im "Magazin für Litteratur" veröffentlichte Steiner noch 1898 den bekenntnishaften Satz: Nur zwei Jahre später trat ein gewandelter Steiner vor die Theosophen und sprach über die .

Die tiefe geistige Zäsur in seinem Leben, die um die Jahrhundertwende stattgefunden hatte, brachte Steiner rückblickend besonders mit Stirner und Mackay in Verbindung: 

Steiners geistige Wende war radikal. Hatte er Stirner anfangs als „den freiesten Denker“ bezeichnet, „den die neuzeitliche Menschheit hervorgebracht hat“, wurde er für ihn zu einem „furchtbar deutlich sprechenden Symbolum der untergehenden [bürgerlichen] Weltanschauung“. Auch Nietzsches "Antichrist" wurde nun als Inbegriff des Satanischen betrachtet. Seine Kapitel hätten einen „oftmals so teuflischen Inhalt“, meinte Steiner und schrieb sie Ahriman zu, dem bösen Gott des Parsismus, der in seiner Interpretation der Menschenseele den Zugang zur seelisch-geistigen Welt versperren möchte, um ihr Bewusstsein mit materialistischen Versuchungen an die physische Leiblichkeit zu ketten.

Der Biograph Gerhard Wehr kommentiert: „Es gibt mancherlei Hinweise auf ein Wandlungsgeschehen, das Steiner in den ersten Berliner Jahren zu bestehen hatte. Er selbst hat diesen als eine ‚intensivste geistige Prüfung‘ empfundenen Lebensabschnitt mit einer ‚Höllenfahrt‘ verglichen, der er nicht ausweichen durfte. Und so sehr Steiner großen Wert auf die Feststellung einer inneren Kontinuität legte, diese tiefgreifende Wandlung hat er nie geleugnet.“ In dieser Zeit hatte Steiner, der frühere Kritiker von Offenbarungsreligionen, nach eigenen Angaben eine Art christliches Erweckungserlebnis, das er mit dem „geistigen Gestanden-Haben vor dem Mysterium von Golgatha in innerster, ernstester Erkenntnis-Feier“ umschrieb. In seinen Erinnerungen schrieb Steiner: „Ich fand das Christentum, das ich suchen musste, nirgends in den Bekenntnissen vorhanden. Ich musste mich, nachdem die Prüfungszeit mich harten Seelenkämpfen ausgesetzt hatte, selber in das Christentum versenken, und zwar in der Welt, in der das Geistige darüber spricht.“ Wehr urteilt, an diese Aussage anknüpfend:

Von Zeitgenossen wurde die Wandlung, auf Steiners persönliche Lebensumstände anspielend, vielfach unter Verweis auf rein weltliche Motive gedeutet. Das zeigt eine ganze Serie von Nachrufen, in denen – in ähnlichem Tenor – auf die materielle Verbesserung von Steiners Lage nach seiner Hinwendung zur Theosophie Bezug genommen wurde. So schrieb John Schikowski in seinem Nachruf im "Vorwärts": „Steiners Wandlung erfolgte um die Wende des Jahrhunderts. Mit dem Studium des Paracelsus und der Schriften der ‚Christlichen Wissenschaft‘ begann sie. Der frühere Haeckelianer wurde eine Art Gesundbeter und als solcher fand der Anarchist Eingang in höchste und allerhöchste Kreise.“ Auch andere Kommentatoren verwiesen auf Steiners Aufstieg in eine neue Geltung, mit der die Hinwendung zur Theosophie einhergegangen war. Der Musik-Kritiker Richard Specht, in dessen Elternhaus Steiner als Hauslehrer gewirkt hatte, schrieb im "Neuen Wiener Journal":

Der Schriftsteller Max Osborn schrieb in seinem Nachruf in der Vossischen Zeitung:

Eine mehrheitlich akzeptierte Deutung für die „Zäsur“ in Steiners Werk gibt es in der Literatur nicht. Anthroposophen nehmen, in Anlehnung an Steiners retrospektive Selbstauslegung, eine innere Kohärenz der persönlichen Entwicklung an.

Aus den frühen 1890er Jahren in Weimar liegen einige Erinnerungen der bald darauf sehr erfolgreichen emanzipatorischen Schriftstellerin Gabriele Reuter vor, zu deren Freundeskreis Steiner gehörte. Sie schrieb:

Der Schriftsteller Stefan Zweig lernte den 40-jährigen Steiner kurz vor dessen Hinwendung zur Theosophie in dem Berliner Literatenkreis "Die Kommenden" kennen und berichtete später darüber:

Nach seiner Hinwendung zur Theosophie wuchs Steiners Bekanntheit kontinuierlich. Bei seinen Vorträgen füllte er zuletzt ganze Konzertsäle. Seine Vortragsreisen wurden zum Teil von einer Berliner Konzertagentur organisiert (z. B. die sogenannten „Wolf-Sachs“-Tourneen in den Jahren 1921 und 1922, auf dem Höhepunkt seiner Popularität). Die Besucherströme zu den Vortragssälen mussten teils polizeilich geregelt werden. Die Neue Freie Presse berichtete von „minutenlangem Beifallsklatschen und Trampeln“ in restlos ausverkauften Sälen. Es sei dies Ausdruck einer Massensuggestion, die Steiner ausgeübt habe. Der Vortragsredner Steiner traf auf vorbehaltlose Begeisterung und entschiedene, teilweise sogar militante Ablehnung. Die Journalisten traten Steiner überwiegend reserviert, meist distanziert, ironisch bis spöttisch oder gar hämisch gegenüber. Besonders seit 1919 erschien Steiner in zeitgenössischen Zeitungsberichten oft als eine Art Scharlatan oder Blender.

Einen besonders hämischen Kommentar zu einem Vortrag Steiners schrieb kein Geringerer als Kurt Tucholsky in der legendären linksbürgerlichen Wochenschrift "Weltbühne":

Viele Kommentatoren erklärten sich Steiners Wirkung auf sein Publikum mit dessen rhetorischem Talent, das ihm kaum ein Zuhörer absprach. Der norwegische Sozialökonom und Historiker Wilhelm Keilhau urteilte:

Zwar fiel das zeitgenössische Urteil vielfach negativ und hämisch aus, wer sich aber für das zeitgenössische Kulturleben interessierte, kam an Steiner nicht vorbei. Das zeigen zahlreiche Urteile bedeutender Zeitgenossen, die Steiner zwar als rätselhaft oder halbseiden apostrophierten, aber auch seine Wirkung zur Kenntnis nahmen. Selbst von Albert Einstein wird berichtet, dass er Vorträge Steiners besuchte, deren Inhalte er jedoch rundheraus ablehnte: „Der Mann [=Steiner] hat offenbar keine Ahnung von der Existenz einer nichteuklidischen Geometrie“ soll Einstein gesagt haben sowie: „Bedenken Sie doch diesen Unsinn: Übersinnliche Erfahrung. Wenn schon nicht Augen und Ohren, aber irgendeinen Sinn muss ich doch gebrauchen, um irgend etwas zu erfahren“.

Auch Franz Kafka bemühte sich darum, das Phänomen Steiner zu verstehen, konnte sich aber kein abschließendes Urteil über ihn bilden. Kafka suchte Steiner sogar einmal persönlich auf, um ihn um Lebenshilfe zu bitten, doch erfüllte das Gespräch seine Erwartungen nicht.

Einige Schriftsteller und Dichter bemühten sich um einen Zugang zu oder jedenfalls um eine Einschätzung von Steiner. So besuchte etwa Hugo Ball einen Vortrag Steiners, um seine Wirkung zu ergründen. Er schrieb in einem Brief: „Vorgestern sprach Rudolf Steiner in den ‚Vier Jahreszeiten‘ über das Wesen der Anthroposophie, unter unglaublichem Andrang. Aber es war eine Enttäuschung. Ich glaubte an eine gewisse persönliche Magie und horchte sehr angestrengt seiner Seele nach. Seine sprachliche Energie ist aber gar nicht ‚leibfrei‘ (um seinen eigenen Terminus zu gebrauchen). Es blieb mir rätselhaft, worin seine Erfolge bestehen mögen.“ Hermann Hesse fühlte sich bemüßigt, die Verwendung anthroposophischer Quellen für seine Werke zurückzuweisen, da diese verschiedentlich bei ihm vermutet worden waren:

Bei aller Ablehnung, die Steiner erfuhr, hatte er auch unter bedeutenden Zeitgenossen Sympathisanten und Bewunderer. Albert Schweitzer etwa berichtete von einem besonderen Gefühl geistiger Zusammengehörigkeit, das ihn seit einer ersten persönlichen Begegnung mit Steiner verband. Christian Morgenstern wurde ein begeisterter Anhänger Steiners, als er 1909 einige seiner Vorträge hörte. Er widmete ihm seinen letzten, posthum erschienen Gedichtband "Wir fanden einen Pfad" (1914) und erwog sogar, Steiner für den Friedensnobelpreis vorzuschlagen. In einem Brief an Friedrich Kayssler schrieb er: „Es gibt in der ganzen heutigen Kulturwelt keinen größeren geistigen Genuss, als diesem Manne zuzuhören, als sich von diesem unvergleichlichen Lehrer Vortrag halten zu lassen“. Selma Lagerlöf urteilte:

Alles in allem gab es wenig Zeitgenossen, die Steiner indifferent gegenüberstanden. Er hatte eine starke und ungemein polarisierende Wirkung. Seine Zuhörerschaft teilte sich zumeist in Anhänger und Gegner. Die vielfältigen Impulse für verschiedenste Lebensbereiche, die Steiner ausübte, wurden daher in der Regel außerhalb des anthroposophischen Kontextes wenig rezipiert.

Das Werk Rudolf Steiners wurde schon zu seinen Lebzeiten sehr kontrovers diskutiert. Streitfragen dabei waren vor allem die proklamierte Wissenschaftlichkeit der Anthroposophie, die von Vertretern der universitären Wissenschaft nicht akzeptiert wurde, und die gnostischen Ansätze seiner Christologie, die von den Amtskirchen scharf verurteilt wurden. Nach dem Zweiten Weltkrieg wurden auch Äußerungen Steiners zur Rassenfrage und zum Judentum kritisiert.

Die Kulturwissenschaftlerin Jana Husmann-Kastein kritisiert an Steiner die Verwendung von rassen- und geschlechtsspezifischen Stereotypen, wie sie allerdings zu seiner Zeit durchaus üblich waren. Steiner benutze eine Rassensystematik, die sich auf die Hautfarben beziehe und diesen bestimmte Eigenschaften zuschreibe. So werde etwa die „weiße Rasse“ explizit mit dem „Denkleben“, die „schwarze Rasse“ mit dem „Triebleben“ und die „gelbe Rasse“ mit dem „Gefühlsleben“ assoziiert. Weiterhin würden geschlechtsspezifische Muster bedient, etwa wenn Steiner den Außereuropäern eine „weibliche Passivität“ zuschreibt. Sie kommt zu dem Urteil, Steiner entwickele 

Jan Badewien, Beauftragter der Evangelischen Landeskirche in Baden für weltanschauliche Fragen, erkennt etwa einen strukturellen Antijudaismus und Rassismus in Steiners Werk:
Nach einer Untersuchung im Auftrag der Anthroposophischen Gesellschaft finden sich in dem insgesamt 89.000 Textseiten umfassenden Schriften Steiners zwölf Textstellen, die nach heutigem Recht etwa in den Niederlanden strafbar wären; 50 weitere Passagen sind demnach aus heutiger Sicht rassistisch interpretierbar. Abschließend stellt die Kommission indes fest, dass das Menschenbild Rudolf Steiners „auf der Grundlage der Gleichwertigkeit aller menschlichen Individualitäten und nicht auf einer vermeintlichen Überlegenheit der einen Rasse gegenüber einer anderen“ stehe.

Wolfgang Benz, Leiter des Berliner Zentrum für Antisemitismusforschung, betont allerdings, dass sich Steiner ausdrücklich „vom rassistisch-völkischen Antisemitismus seiner Zeit“ distanziert hat und resümiert: Sein „Plädoyer für die Assimilation unterscheidet ihn vom Anhänger des Rasseantisemitismus.“

Der Historiker Clemens Escher sieht in Steiners Äußerungen bis 1918 einen für den Wilhelminismus typischen Hang zur Abgrenzung von Deutschlands angeblichen „Reichs- und Erbfeinden“, zu denen für Steiner neben Franzosen, Jesuiten und Sozialisten eben auch Schwarzafrikaner und Juden gezählt hätten. Gleichwohl sei er weder überzeugter Rassentheoretiker noch Antisemit gewesen, sondern ein Eklektiker, der sich aus den diskursiven Angeboten seiner Zeit und seiner Umwelt bedient habe.

Der Kritikpunkt der Vermischung von Wissenschaftlichkeit und Glaubensfreiheit bezieht sich vor allem auf Steiners „Okkultismus“. So meint etwa der Religionswissenschaftler Hartmut Zinser, Steiner verschiebe eigenmächtig die Kriterien dessen, was als wissenschaftlich gelte. Dies zeige sich etwa, wenn von „Geistes- oder Geheimwissenschaft“ und „hellseherischer Forschung“ die Rede sei. Alles, was mit den Erkenntnissen und Methoden der Wissenschaften nicht zu vereinbaren sei, werde deshalb als „höheres Wissen“ ausgegeben. Zinser meint: Steiner unterliege hier einem der erkenntnistheoretischen Grundfehler des modernen Okkultismus, da nicht zwischen Wahrnehmung und Deutung unterschieden werde.

Rudolf Steiners Werk gliedert sich in 42 Bände mit Schriften, über 5000 Vorträge sowie seine architektonischen und künstlerischen Arbeiten. Rund 4500 Vorträge wurden als Stenogramme aufgezeichnet; von den übrigen existieren qualitativ unterschiedliche Mitschriften oder Notizen. Etwa 700 Vorträge sind noch nicht veröffentlicht, wurden aber mitgeschrieben, wenn auch teilweise bruchstückhaft. Steiners Vorträge erschienen zunächst im Privatdruck und in Zeitschriften, ab 1908 im Philosophisch-Anthroposophischen Verlag, Berlin. In diesem erschienen bis 1953 knapp 500 Publikationen, der Großteil von Steiners Werk. 1943 gründete Marie Steiner als Alleinerbin der Autorenrechte den sogenannten Nachlassverein. Dieser hat 1955 im eigenen Rudolf Steiner Verlag mit der Publikation einer auf 350 Bände veranschlagten "Gesamtausgabe" (GA) begonnen. Seit 1961 werden einzelne Bände auch als Taschenbücher herausgegeben: zuerst im Verlag Freies Geistesleben, ab 1972 im Rudolf Steiner Verlag, mit heute rund 140 Titeln. Seit dem Jahr 2004, acht Jahre nach Ablauf der Urheberrechte, geben die Rudolf Steiner Ausgaben eigene Publikationen aus dem Werk heraus, bis heute 68 2-Euro-Hefte (14 als Hör-CDs), 21 Taschenbücher und 53 Bände (Stand Dezember 2015).

Im Vortragswerk sind verschiedene Sparten zu unterscheiden, die sich an ganz unterschiedliche Hörer wendeten:

Das künstlerische Werk umfasst Bände, Kunstmappen und Einzelblätter mit Reproduktionen seiner zahlreichen Skizzen und Bilder. Insbesondere wurden in neun Bänden seine rund 1500 Skizzen zur Eurythmie (die sogenannten „Eurythmieformen“) und in 30 Bänden seine 1100 „Wandtafelzeichnungen“ dokumentiert.


Ab 2013 erscheint im Stuttgarter Wissenschaftsverlag frommann-holzboog eine kritische Ausgabe der Schriften (SKA), herausgegeben von Christian Clement, angelegt auf acht Bände.



Diese Liste bietet eine knappe Auswahl vorwiegend neuerer Bücher zu Person und Werk. Weitere bibliografische Hinweise sind etwa bei Lindenberg oder Zander zu finden.



Essays


</doc>
<doc id="13342" url="https://de.wikipedia.org/wiki?curid=13342" title="Alexander Hamilton">
Alexander Hamilton

Alexander Hamilton (* 11. Januar 1757 oder 1755 auf Nevis, Westindische Inseln, heute St. Kitts und Nevis; † 12. Juli 1804 in New York City) war ein amerikanischer Staatsmann. Er zählt zu den „Gründervätern der Vereinigten Staaten“.

Hamilton diente im Amerikanischen Unabhängigkeitskrieg im persönlichen Stab von George Washington, von 1782 bis 1783 war er Mitglied des Kontinentalkongresses. 1784 gründete er die Bank of New York (heute Bank of New York Mellon), die älteste Bank der Vereinigten Staaten. Er nahm an der Philadelphia Convention zur Ausarbeitung einer neuen Verfassung teil, die er anschließend in den Federalist Papers gemeinsam mit John Jay und James Madison verteidigte. Der junge Rechtsanwalt tat sich mit seiner konservativen Haltung hervor. So befürwortete er die Wahl des Präsidenten und der Senatoren auf Lebenszeit und wollte eine starke Zentralregierung gegenüber den Einzelstaaten festschreiben. Hamilton setzte sich zwar nur mit letzterer Forderung teilweise durch, gilt aber zusammen mit James Madison und George Washington als einer der drei Väter der amerikanischen Verfassung.

Unter der Regierung Washingtons wurde er von 1789 bis 1793 der erste Finanzminister der Vereinigten Staaten und trug maßgeblich zum Aufbau des Bankensystems und der Marine bei. Um 1791/1792 gründete er die Föderalistische Partei, zudem war er Mitbegründer der New York Manumission Society, die für die Abschaffung der Sklaverei und für die Rechte der afrikanischstämmigen Bevölkerung eintrat. Mit dieser Haltung schuf er sich keine Freunde in den sklavenhaltenden Südstaaten, wie etwa Virginia. Hamilton starb am 12. Juli 1804 an einer Verwundung, die er sich am Vortag in einem Duell mit seinem langjährigen politischen Rivalen Aaron Burr zugezogen hatte.

Alexander Hamilton gilt als Begründer des Amerikanischen Systems der Politischen Ökonomie. Er nutzte diese Bezeichnung erstmals 1791 in einem Dokument („A Report on the Subject of Manufactures“) an den Kongress. Es geht von einer Gemeinwohlverpflichtung für alle Beteiligten einer Volkswirtschaft aus, wie sie in der amerikanischen Verfassung festgeschrieben ist. In diesem System schafft der Staat Bedingungen, unter denen Produktionsbetriebe sich – zum Wohl der Unternehmen, der Beschäftigten und des Gemeinwesens – weiterentwickeln können. Weitere Vertreter des Amerikanischen Systems der Politischen Ökonomie waren Henry Charles Carey und Friedrich List.

1791 wurde er in die American Academy of Arts and Sciences gewählt. Hamiltons Porträt befindet sich auf der 10-Dollar-Banknote.

Hamilton hielt sich wegen seiner illegitimen Geburt zeit seines Lebens sehr bedeckt über seine Herkunft; fast alle Informationen über seine Jugend kamen erst durch die Forschung des 20. Jahrhunderts ans Licht. Sein Geburtsjahr ist bis heute umstritten; während ältere Dokumente aus karibischen Archiven 1755 nahelegen, gab Hamilton selbst stets 1757 an – möglicherweise zunächst, um nicht wegen seines für damalige Verhältnisse hohen Eintrittsalters am College abgewiesen zu werden.

Geboren wurde er auf der Karibikinsel Nevis als unehelicher Sohn von Rachel Faucette (anglisiert auch "Fawcet"), der Tochter eines hugenottischen Auswanderers. Als Sechzehnjährige wurde sie 1745 in der dänischen Kolonie St. Croix mit dem Abenteurer Johann Michael Lavien verheiratet, entfloh jedoch nach der Geburt ihres ersten Sohnes Peter der tyrannischen Ehe und wurde darauf 1750 für einige Monate wegen Ehebruchs in der Festung von Christiansted eingekerkert. Nach ihrer Freilassung floh sie auf die Nachbarinsel Nevis, eine britische Kolonie. Dort lernte sie James Hamilton kennen, den viertältesten Sohn des schottischen Laird James Hamilton, Schlossherr von Kerelaw in Ayrshire; nach einer gescheiterten Handelskarriere in Schottland und ohne Aussicht, je den Titel seines Vaters zu erben, versuchte er sein Glück nun in der Karibik. Aus dieser unehelichen Beziehung gingen vermutlich mehrere Kinder hervor, doch überlebten nur zwei Söhne, James und Alexander Hamilton. 1765 siedelte die Familie nach St. Croix über, doch noch im selben Jahr ließ Hamilton senior seine Familie im Stich und setzte sich auf eine andere Insel ab. Vermutlich sah Alexander Hamilton seinen Vater nie wieder, blieb ihm aber noch über Jahre hinweg in brieflichem Kontakt verbunden. Seine Mutter starb zwei Jahre darauf an einem tropischen Fieber. Ihre Besitztümer samt ihrem Haus in Christiansted wurden von Lavien meistbietend versteigert, ihre beiden „Hurenkinder“, wie Lavien sie abschätzig bezeichnete, waren als uneheliche Söhne nicht erbberechtigt. Die beiden mittellosen Kinder wurden darauf zunächst von ihrem Vetter Peter Lytton aufgenommen, doch beging dieser im Juli 1769 Selbstmord. Daraufhin wurden sie von Lyttons Vater James aufgenommen, doch starb dieser kaum einen Monat darauf; wiederum wurden James und Alexander Hamilton in keinem der beiden Erbfälle berücksichtigt.

Vermutlich besuchte Hamilton nie eine Schule, doch war er wohl schon als Junge sehr belesen. Zwischen 1771 und 1773 veröffentlichte er mehrere Gedichte in der örtlichen Zeitung, der "Royal Danish American Gazette", die eine Kenntnis mindestens der Gedichte Popes und einer französischen Übersetzung von Machiavellis "Der Fürst" nahelegen (dank seiner hugenottischen Mutter war Hamilton fließend zweisprachig). Früh machte er auch durch sein Geschäfts- und Verwaltungstalent auf sich aufmerksam. Ab 1767 arbeitete er für die Handelsfirma Beekman & Kruger; im Oktober 1771 übernahm er die Leitung des Unternehmens, als der Inhaber Nicolas Cruger vorübergehend nach New York zurückkehrte. Die erhaltenen Dokumente zeigen, mit welcher Professionalität der junge Hamilton die Geschäfte führte: Die Kontoristen der Firma auf den anderen Karibikinseln wies er forsch an, ihn stets auf dem Laufenden über Bestände, Preise und Bilanzen zu halten, und korrigierte kühl ihre Rechenfehler. In ebendiesem Stil würde er Jahre später als Finanzminister seine Beamten dirigieren. Am 31. August verwüstete ein schwerer Hurrikan St. Croix. Hamilton schilderte seinem Vater diese Naturkatastrophe in einem Brief, der einige Wochen später auch in der Inselzeitung veröffentlicht wurde. Der Gouverneur und die führenden Kaufleute der Kolonie zeigten sich von der literarischen Qualität des Briefs so beeindruckt, dass sie für ein Stipendium sammelten, das Hamilton eine Ausbildung an einer der Hochschulen in den amerikanischen Kolonien ermöglichen sollte. Im Oktober des Jahres schiffte er sich nach Boston ein; er kehrte nie wieder in die Karibik zurück.

Um sich auf das College vorzubereiten, besuchte Hamilton zunächst die private Vorbereitungsschule "Elizabethtown Academy" in Elizabethtown nahe New York, wo er unter anderem Latein und Griechisch lernte. Als Schüler in Elizabethtown fand er auch trotz seiner niederen Herkunft Anschluss an die gesellschaftlichen Eliten und knüpfte in dieser Zeit enge Bindungen zu Männern, die später bedeutende Rollen in der Revolution spielen würden: zu William Livingston, später erster republikanischer Gouverneur von New Jersey, zu William Alexander, genannt „Lord Stirling“ und später Brigadegeneral in der Kontinentalarmee, sowie zu Elias Boudinot, später Delegierter und Präsident des Kontinentalkongresses. Livingston und Boudinot saßen auch im Kuratorium des ebenfalls im Ort ansässigen College of New Jersey (der heutigen Princeton University), das sich zu dieser Zeit unter der Präsidentschaft John Witherspoons zum politisch radikalsten College der amerikanischen Kolonien entwickelte. Hamilton stellte hier gegen Ende 1773 einen Aufnahmeantrag, verbunden mit der Bitte, ein beschleunigtes Studium absolvieren zu dürfen, um den Abschluss in kürzerer Zeit zu erlangen. Das Kuratorium des Colleges sah ein solches Anliegen jedoch als unvereinbar mit der üblichen Praxis an und erteilte Hamilton eine Absage, so dass er sich stattdessen zum Jahreswechsel am King’s College in New York einschrieb. Diese Hochschule (aus der die heutige Columbia University hervorging) war in dieser Zeit zunehmender Spannungen mit dem Mutterland Großbritannien unter der Leitung des Anglikaners Myles Cooper eine Hochburg der königstreuen Tories. Trotzdem engagierte sich Hamilton in seinen Studienjahren immer mehr in der revolutionären Bewegung, die sich nach der Boston Tea Party auch in New York zusehends radikalisierte. Bei einer Massenkundgebung der Sons of Liberty 1774 soll er mit einer mitreißenden Stegreifrede erstmals auf sich aufmerksam gemacht haben. Besonderes Aufsehen erregten zwei Pamphlete, die Hamilton im Winter 1774/75 anonym in der Presse des Druckers James Rivington veröffentlichte. Sie wandten sich in polemischer Schärfe gegen den Autor der von Samuel Seabury unter dem Pseudonym „A Westchester Farmer“ verfassten Essays, die als Tory-Propaganda große Verbreitung gefunden hatten. Die beiden Schriften Hamiltons, "A Full Vindication of the Measures of the Congress" sowie "The Farmer Refuted", lassen erkennen, dass Hamilton sich in dem knappen Jahr, das seit seiner Ankunft in Boston verstrichen war, eingehend mit den politischen und wirtschaftlichen Problemen der Kolonien vertraut gemacht hatte.

Als die Nachricht vom Kriegsausbruch (den Gefechten von Lexington und Concord) im April 1775 New York erreichte, schloss sich Hamilton umgehend einer der zahlreichen Milizen an, die sich in Erwartung eines britischen Angriffs nun auch in New York bildeten. In Amerika lernte Hamilton seine spätere Frau Elizabeth Schuyler kennen, mit der er erst lange in Briefkontakt stand. Sie trafen sich schließlich durch Hilfe ihrer Schwester (Angelica Schuyler) und heirateten am 14.12.1780. Sie bekamen Kinder, darunter den Sohn Philip Hamilton.

Hamiltons Fehde mit seinem vorherigen Freund und später Rivalen Aaron Burr, seit 1800 Vizepräsident unter Thomas Jefferson, eskalierte 1804, nachdem Burr sich für das Amt des Gouverneurs des Bundesstaats New York hatte aufstellen lassen, aber bei der Wahl im April des Jahres dem republikanischen Kandidaten Morgan Lewis deutlich unterlag. Burr witterte nicht ganz zu Unrecht hinter seiner Niederlage eine Intrige Hamiltons. Dieser hatte sich schon im ersten Caucus der Föderalisten gegen eine Kandidatur Burrs gewandt. Nachdem er überstimmt worden war, verwandte er viel Energie darauf, Briefe an die föderalistischen Meinungsführer zu verfassen, in denen er in immer schärferen Worten vor Burr warnte. Einige despektierliche Bemerkungen über Burr, die Hamilton bei einem Abendessen in Albany geäußert haben soll, fanden den Weg in die Presse. Burr sah sich derart in seiner Ehre verletzt, dass er Hamilton zum Duell forderte. Diese Form der Beilegung von Ehrenstreitigkeiten wurde in den USA gesellschaftlich noch weithin akzeptiert – sowohl Burr als auch Hamilton hatten sich schon zuvor Duellen gestellt. In New York war das Duellieren jedoch verboten, so dass sich Duellanten üblicherweise am anderen Ufer des Hudson im Wald von Weehawken im Staat New Jersey trafen. Hier war auch Hamiltons ältester Sohn Philip 1801 bei einem Duell getötet worden.

Beim Duell am Morgen des 11. Juli 1804 verwundete Burr Hamilton mit einem Schuss in den Unterleib tödlich. Der genaue Ablauf ist bis heute Gegenstand zahlreicher Spekulationen. Hamilton hatte in den Tagen vor dem Duell nicht nur sein Testament aufgesetzt, sondern in einigen persönlichen Bemerkungen auch seinen Entschluss niedergeschrieben, mindestens mit der ersten seiner Duellkugeln nicht auf den Gegner zu zielen, sondern den ersten Schuss zu vergeuden – um Burr zu beschwichtigen, aber auch, da ein Duell seinen religiösen Überzeugungen grundsätzlich zuwider sei. Hamilton hätte dadurch willentlich seinen eigenen Tod in Kauf genommen oder herbeigeführt. Burr, der von Hamiltons Entschluss nichts wissen konnte, und auch sein Sekundant William P. Van Ness gaben später an, dass die Duellanten etwa gleichzeitig geschossen hätten und dass Hamilton durchaus auf Burr gezielt habe, wenn auch die Kugel ihr Ziel weit verfehlte. Hamiltons Sekundant Nathaniel Pendleton gab jedoch an, dass Hamiltons Schuss versehentlich zu früh losgegangen sei. Eine Untersuchung der Duellpistolen durch Experten der Smithsonian im Jahr 1976 legt den Schluss nahe, dass der Abzug der Waffen – die Hamilton als Herausgeforderter wählen durfte – präpariert war. Während Burrs Pistole einen konventionellen Abzug besaß, bei der ein Abzugsgewicht von mehr als 5 Kilogramm aufgebracht werden musste, war Hamiltons Waffe auf einen weitaus niedrigeren Widerstand eingestellt, was ihm einen unlauteren Vorteil verschafft hätte; diese Manipulation könnte auch erklären, warum sein Schuss, wie Pendleton angab, tatsächlich zu früh gefeuert wurde.

Hamiltons Tod wurde in New York mit Bestürzung aufgenommen und sein Trauerzug von Tausenden begleitet. Selbst der Demokratisch-Republikanische Rat der Stadt ordnete einen Trauertag an. Hamiltons letzte Ruhestätte befindet sich auf dem Friedhof der Trinity Church in New York.

Der Aufstieg Alexander Hamiltons vom Waisenkind aus der Karibik zum Gründervater der Vereinigten Staaten von Amerika wurde von Lin-Manuel Miranda, Sohn puerto-ricanischer Eltern, mit einem erfolgreichen Hip-Hop-Musical auf die Bühne gebracht ("Hamilton"). Das Broadway-Stück entwickelte sich zu einem Zuschauermagneten und hat bisher einen Grammy, einen Pulitzer-Preis sowie elf Tony Awards gewonnen.





</doc>
<doc id="13343" url="https://de.wikipedia.org/wiki?curid=13343" title="Wilhelm Adolf Lette">
Wilhelm Adolf Lette

Wilhelm Adolf Lette (* 10. Mai 1799 in Kienitz, Neumark; † 3. Dezember 1868 in Berlin) war ein deutscher Sozialpolitiker und Jurist.

Lette war der Sohn eines Landwirtes, der ihn nach Berlin auf das Gymnasium zum Grauen Kloster schickte, an dem er auch seine Reifeprüfung ablegte. Danach studierte er ab 1816 an den Universitäten Heidelberg, Göttingen und Berlin Rechtswissenschaften. Während seines Studiums wurde er 1816 Mitglied der "Burschenschaft Teutonia Heidelberg". In Berlin war er 1818 an der Gründung der Alten Berliner Burschenschaft beteiligt. Neben den Rechtswissenschaften beschäftigte sich Lette zusätzlich mit Staatswissenschaft und der Hegel’schen Philosophie. Als Burschenschafter und Besucher des Wartburgfestes wurde er im Rahmen der Demagogenverfolgung verhaftet und zu einer geringen Gefängnisstrafe verurteilt. Nach der frühen Aufhebung der Strafe war er ab 1821 Auskultator und Assessor. Zuerst war er am Gericht Frankfurt (Oder) und später in Landsberg tätig. 1825 wurde er bei der Generalcommission zu Soldin als Obergerichtsassessor angestellt, um 1834 zum Rat befördert nach Stargard versetzt zu werden. 1835 wurde er zum Oberlandesgerichtsrat in Posen und 1840 zum Dirigenten der volks- und landwirtschaftlichen Abteilung an der Regierung zu Frankfurt (Oder) ernannt. Im April 1843 wurde er als vortragender Rat in das Ministerium des Innern berufen. 1845 war er an der Gründung des Revisionsausschusses für Landeskultursachen beteiligt, dessen erstes Präsidium er übernahm.

Im Paulskirchenparlament von 1848 gehörte er dem volkswirtschaftlichen Ausschuss an. Von 1850 bis 1852 war er Abgeordneter der I. Kammer, 1852 bis 1855 der II. Kammer und von 1855 bis 1868 Mitglied im Preußischen Abgeordnetenhaus.

Beim ersten Kongreß deutscher Volkswirte im Jahre 1858, an dessen Zustandekommen er mitwirkte, wurde er in die "Ständige Deputation" des Kongresses gewählt.

Im Preußischen Abgeordnetenhaus schloss sich Lette zunächst der "Fraktion von Vincke" an, gehörte 1863 bis 1866 zur Fraktion "Linkes Centrum" und trat 1866 in die Nationalliberale Partei ein. 1867 wurde er in den Norddeutschen Reichstag für den Wahlkreis Frankfurt/Oder 3 gewählt. Im Frühsommer 1868 erkrankte er und verstarb ein halbes Jahr später.
Lette wurde in einem Familiengrab auf dem Friedhof III der Jerusalems- und Neuen Kirche am Mehringdamm in Berlin-Kreuzberg beigesetzt. Seine letzte Ruhestätte ist als Ehrengrab des Landes Berlin ausgewiesen.

Lette gründete 1866 in Berlin den "Verein zur Förderung der Erwerbstätigkeit des weiblichen Geschlechts", heute Lette-Verein, dessen Vorsitz er auch übernahm. Als erste Einrichtung dieser Art wurde der Lette-Verein vorbildlich für alle Berufsbildungsstätten für Frauen in Deutschland. Ab 1872 – unter der Leitung seiner ältesten Tochter Anna Schepeler-Lette – wird der Verein Schulträger. Die Schule des Lette-Vereins befindet sich seit 1902 in Berlin-Schöneberg am Viktoria-Luise-Platz. In mehreren Berufsfachschulen werden junge Frauen u. a. in kaufmännischen Berufen, Fotografie, als Medizinisch-technische Assistentin, in einer Fachklasse für Mode und als Hauswirtschaftsleiterinnen ausgebildet.






</doc>
<doc id="13344" url="https://de.wikipedia.org/wiki?curid=13344" title="Fernrohr">
Fernrohr

Ein Fernrohr, auch Linsenfernrohr oder Refraktor, ist ein optisches Instrument, bei dessen Nutzung entfernte Objekte um ein Vielfaches näher oder größer erscheinen. Dies wird durch eine Vergrößerung des Sehwinkels mit Hilfe von Linsen erreicht. Prismen und Spiegel können dazu dienen, das Bild aufzurichten oder die Baulänge des Fernrohrs zu vermindern.

Die Entwicklung leistungsfähiger Fernrohre spielte eine wichtige Rolle in der Geschichte der Astronomie. Fernrohre bilden zusammen mit Spiegelteleskopen die Klasse der optischen Teleskope.

Das Wort "Fernrohr" ist eine wörtliche Eindeutschung des lateinischen ' „Fern-seh-Röhre“, aus ' „Rohr, Schlauch“, altgriechisch ' „fern“ und ' „schauen, beobachten“.
Maximilian Hell würdigte im Jahre 1789 Wilhelm Herschels Entdeckung des Uranus mit der Benennung zweier Sternbilder als "Tubus Herschelii Maior" und "Tubus Herschelii Minor", womit er Bezug auf die von Herschel gebauten Fernrohre nahm.
Johann Elert Bode fasste die beiden Sternbilder dann 1801 zu einem zusammen und prägte den Ausdruck "Telescopium Herschelii" dafür.
Das deutsche Wort gab es auch schon in dieser Zeit, die anfangs synonymen Wörter "Fernrohr" und "Teleskop" entwickelten sich jedoch auseinander.
Heute ist "Teleskop" der Oberbegriff. "Fernrohr" steht für ein aus Linsen aufgebautes optisches Teleskop. Und "Tubus" bezeichnet den technischen Bauteil der Hülle, in die die Linsen-, Spiegel- und Prismenkonstruktion eingefasst ist.

Fernrohre bestehen generell aus einer Kombination von Linsen, die von einer mechanischen Konstruktion gehalten werden. Je nach Strahlengang des Lichts durch die Linsen unterscheidet man dabei zwischen Galilei-Fernrohr und Kepler-Fernrohr. Zusätzliche optische Elemente können das Bild beim Blick ins Fernrohr in gleicher Weise wie das Original ausrichten. Der Strahlengang im Fernrohr kann durch Spiegel gefaltet werden, um trotz der langen Brennweite eine kurze Bauform zu erhalten.

Das Galilei-Fernrohr, auch "holländisches Fernrohr" genannt, wurde vom holländischen Brillenmacher Hans Lipperhey um 1608 erfunden (und etwa gleichzeitig von Jacob Metius und Zacharias Janssen und dessen Vater) und in der Folgezeit von dem Physiker und Mathematiker Galileo Galilei weiterentwickelt. Es hat als Objektiv eine konvexe Sammellinse und als Okular eine Zerstreuungslinse kleinerer Brennweite. Da das Okular eine negative Brennweite besitzt, muss es innerhalb der Brennweite des Objektivs so liegen, dass die Brennpunkte von Objektiv und Okular auf der Seite des Beobachters zusammenfallen. Es entsteht ein virtuelles, aufrechtes und seitenrichtiges Bild, allerdings mit kleinem Sichtfeld. Das Galilei-Fernrohr wird heute beim Opernglas und bei der Fernrohrbrille eingesetzt. Das Prinzip findet auch bei Telekonvertern Verwendung.

Als Kepler-Fernrohr, auch astronomisches Fernrohr, bezeichnet man ein Fernrohr, das einer von Johannes Kepler 1611 beschriebenen Bauweise folgt. Danach ist auch das Okular eine konvexe Sammellinse (mit geringerer Brennweite). Okular und Objektiv stehen im Abstand ihrer addierten Brennweiten, d. h. ihre Brennpunkte fallen zwischen den Linsen zusammen. Das Gesichtsfeld ist ausgedehnter als beim Galilei-Fernrohr. Ob wirklich Johannes Kepler diesen Fernrohrtyp – der außer in der Astronomie z. B. auch in geodätischen Theodoliten verwendet wird – erfunden hat, ist ungewiss. Das erste überlieferte Kepler-Fernrohr wurde vom Jesuiten Christoph Scheiner um 1613 gebaut.

Da sich der Strahlengang im Fernrohr kreuzt, erzeugt das Objektiv ein auf dem Kopf stehendes und seitenverkehrtes (also insgesamt um 180 Grad gedrehtes) reelles Bild des betrachteten Gegenstands, das man mittels des Okulars – nach dem Prinzip der Lupe – vergrößert betrachtet.

Keplersche Fernrohre erzeugen für den Beobachter ein um 180° gedrehtes Bild. Es steht im Vergleich zum Original auf dem Kopf und ist seitenverkehrt. Bei einem Schwenk des Fernrohrs bewegt sich das Bild im Bildfeld daher umgekehrt als wenn man durch eine leere Röhre blickt. Entsprechendes gilt für Schwenks nach oben und unten. Dies kann mit weiteren Linsen oder mit Prismen behoben werden:

Um das Bild gleich dem Original auszurichten, gibt es folgende Möglichkeiten einer „Umkehroptik“:

Bei Prismenferngläsern (Feldstechern) und Spektiven wird das umgedrehte Bild des Kepler-Fernrohrs mittels verschiedener Prismensysteme um 180° gedreht. Je nach Ausführung ergibt sich auch eine kürzere Bauweise. Die Bildumkehr kann auch durch eine Umkehrlinse erfolgen. Das findet z. B. bei Aussichtsfernrohren und manchen Zielfernrohren Verwendung, aber auch beim Ausziehfernrohr oder terrestrischen Fernrohr für unterwegs oder auf See. Es ist trotz Vergrößerungen von etwa 20-fach bis 60-fach klein, zusammenschiebbar und preiswert. Nachteilig sind die geringere Lichtstärke und der Zutritt von Außenluft beim Auseinanderziehen, wodurch Schmutz und Wasser eindringen können. Neuere Bautypen und Spektive haben daher einen festen Tubus und verkürzen die Baulänge durch ein geradsichtiges Porroprisma oder leicht geknicktes Umkehrprisma. Auch mit einer (negativen, zerstreuenden) Fokussierlinse ist das möglich – etwa in neueren Theodoliten und elektronischen Tachymetern.

Das verkehrte Bild wird bei den größeren Fernrohren der Astronomie in Kauf genommen, da die Ausrichtung der Beobachtungsobjekte am Himmel in der Regel keine Rolle spielt. Zur Verbesserung des Einblicks ins Okular werden häufig 90°- oder 45°-Umlenkprismen und -spiegel eingesetzt, deren Bild dann zumindest aufrecht oder seitenrichtig ist ("Zenitspiegel").

Eine vierte Möglichkeit besteht in der Verwendung einer Zerstreuungslinse als Okular, wodurch das astronomische zu einem Galilei-Fernrohr wird. Es ist optisch ungünstiger, aber wegen der extrem kurzen Bauweise z. B. für Theatergläser (vulgo „Operngucker“) sehr gebräuchlich. Der Galilei-Bautyp erlaubt aber kein Anbringen eines Fadenkreuzes oder Mikrometers.

Jede optische Linse weist mehr oder weniger starke Farblängs- und Farbquerfehler auf. Unterschiedliche Wellenlängen werden unterschiedlich stark gebrochen. Langwelliges rotes Licht wird weniger stark als kurzwelliges blaues Licht gebrochen. Somit liegt für jeden Wellenlängenbereich ein eigener Brennpunkt vor. Bei der praktischen Beobachtung führt dies zu störenden Farbsäumen.

In der Vergangenheit versuchte man den Farblängsfehler mitunter dadurch zu reduzieren, indem man möglichst langbrennweitige Fernrohre konstruierte. So benutzte der Danziger Gelehrte Johannes Hevelius meterlange Luftteleskope.

Eine weitere Möglichkeit der Minimierung besteht in der Kombination von Glaslinsen mit unterschiedlicher Abbe-Zahl. Eine in kurzem Abstand hintereinander gestellte Gruppe von zwei Linsen wird Achromat genannt. Bei drei oder mehr Linsen spricht man von Apochromaten. Pioniere dieser Technik waren Chester Moor Hall und Joseph von Fraunhofer.

Beim Okular haben mehrere Linsen zusätzlich die Aufgabe, das Gesichtsfeld zu vergrößern. Mit zunehmender Größe des Fernrohrs und Ansprüchen an die Qualität des Bilds werden Entwurf und Bau solcher Linsensysteme sehr aufwändig.

Die Faltrefraktoren sind eine Sonderform des Fernrohrs. Der Strahlengang wird meist über einen oder zwei Planspiegel umgelenkt, er wird also quasi gefaltet. Die diversen Faltvarianten werden dabei oft nach ihren Konstrukteuren oder nach dem äußeren Erscheinungsbild des Fernrohrs benannt. So erinnert der "Fagott-Refraktor" (einfache Faltung) an die geknickte Bauweise des gleichnamigen Musikinstrumentes und der "Newton-Refraktor" (zweifache Faltung) wegen seines Okulareinblicks an das Spiegelteleskop nach Newton. Der "Schaer-Refraktor" ist zweifach gefaltet und nach seinem Konstrukteur benannt.

Okularzenitprismen oder -spiegel gehen bei der Klassifizierung dieser Bauweisen nicht mit ein. Sie gelten als Zubehörteile für alle Fernrohrtypen.

Linsenobjektive haben den Nachteil, dass sie durch die Brechung des Lichtes im Bild Farbsäume bilden. Diese so genannte chromatische Aberration war früher bei einfachen zweilinsigen Objektiven („Achromaten“) nur ab einem Öffnungsverhältnis von kleiner als ca. 1:15 akzeptabel. Dadurch wurden die Fernrohre bei größeren Öffnungen sehr lang und unhandlich.

Verschiedene zweifach gefaltete Refraktoren wurden u. a. von E. Schaer, Ainslie und G. Nemec entworfen. Es ist dabei oft schwierig, den Ainslie- vom Nemec-Typen zu unterscheiden, da sie bis auf kleinere Modifikationen in der Strahlenführung sehr ähnlich sind. So führte Ainslie den Strahlengang seiner Newtonvariante nach der 2. Spiegelung an dem einfallenden Strahlengang seitlich vorbei.

Die Amateurastronomen Nemec, Sorgenfrey, Treutner und Unkel wurden in den 1960er bis Ende der 1970er Jahre durch hochwertige Astrofotos mit ihren Faltrefraktoren bekannt. Diese Bekanntheit brachte auch diesen Refraktortypen eine gewisse Popularität ein.

Faltrefraktoren werden heute im Wesentlichen als Selbstbaugeräte von Amateurastronomen und einigen Volkssternwarten eingesetzt. Die Firma Wachter bot in den 1970er und 1980er Jahren einen Schaer-Refraktor aus industrieller Serienfertigung an. Es handelte sich um einen FH 75/1200 mm des japanischen Herstellers Unitron.

Auch beim "Coudé-Refraktor" wird der Strahlengang durch zwei Planspiegel oder Prismen gefaltet. Diese lenken das Licht durch die Montierung zu einem ortsfesten Fokus. Vorteil dieser Bauart ist die Beobachtung von einem festen Platz aus, der ohne großen Aufwand mit Sitzmöglichkeit, Hilfsmitteln und Arbeitstisch ausgestattet werden kann, während sich das in der Regel relativ lange Fernrohr unabhängig davon bewegt. Nachteil ist die beim Schwenken oder auch bloßen Nachführen des Fernrohrs verursachte Bilddrehung, so dass astronomische Fotografie nur mit kurzen Verschlusszeiten möglich ist oder aufwendige Drehnachführungen eingebaut werden müssen. Da der Strahlengang üblicherweise durch eine Achse der Montierung geführt wird, sind meistens nur relativ große Instrumente ab ca. 8 Zoll Öffnung aufwärts als Coudé-Refraktoren ausgeführt.

Das Coudé-System findet auch bei Spiegelteleskopen Anwendung.

Für terrestrische Beobachtungen verwendet man

Für astronomische Beobachtungen:

Die Vergrößerung eines Fernrohrs ist durch das Verhältnis der Brennweiten von Objektiv und Okular gegeben. Das heißt, ein Fernrohr mit auswechselbaren Okularen, wie es in der Astronomie üblich ist, hat keine feste Vergrößerung; je kürzer die Brennweite des verwendeten Okulars ist, desto stärker ist die resultierende Vergrößerung.
Wegen verschiedener Faktoren (siehe "Störgrößen") ist eine übertrieben starke Vergrößerung sinnlos.

Kleine Fernrohre und Ferngläser charakterisiert man durch zwei Zahlenangaben, z. B. 6 × 20 mm (Taschengerät) oder (20 bis 40) × 50 (Spektiv). Die erste Angabe bezieht sich auf die Vergrößerung, die zweite auf die Öffnung (Apertur) des Objektivs in mm. Variable Vergrößerungen (z. B. 20 bis 40) werden durch Zoom-Okulare ermöglicht. Durch den Einsatz eines Binokulars entsteht der Eindruck des räumlichen Sehens, wodurch sich die Wahrnehmung verbessert.

Bei Fernrohren für astronomische Beobachtungen wird das Verhältnis von Apertur zur Brennweite (das Öffnungsverhältnis) als Kenngröße für das Leistungsvermögen des Instruments verwendet. Die Vergrößerung ergibt sich je nach verwendetem Okular, das meist gewechselt werden kann. Ein Refraktor 100/1000 hat also eine Öffnung von 100 mm und eine Brennweite von 1000 mm und somit ein Öffnungsverhältnis von 1:10 (meist als F/10 geschrieben).

Die Vergrößerung eines Refraktors ergibt sich aus dem Verhältnis der Brennweiten des Objektivs und des Okulars. Ein Gerät mit 1000 mm Objektiv-Brennweite und 5 mm Okular-Brennweite besitzt somit eine 200-fache Vergrößerung. Wegen des durch Beugung begrenzten Auflösungsvermögens ist eine solche Vergrößerung aber nur dann sinnvoll, wenn die Öffnung des Objektivs groß genug ist. Als Richtwert hat die sogenannte "nützliche Vergrößerung" den doppelten Zahlenwert wie der Öffnungsdurchmesser des Objektivs in Millimetern. Im genannten Beispiel sollte das Fernrohr eine Öffnung von 100 mm haben.

Die Größe der Austrittspupille (AP) ist eine weitere interessante Kenngröße. Sie berechnet sich als Produkt aus Okularbrennweite und Öffnungsverhältnis oder als Quotient aus Öffnung und Vergrößerung. In den obigen Beispielen wäre die Austrittspupille also 20 mm/6 = 3,3 mm bzw. 5 mm·100/1000 = 0,5 mm. Die Konstruktion des Okulars bestimmt die Lage der AP. Sie sollte mit dem Auge erreichbar sein. Die Pupille des Auges begrenzt die Lichtmenge, die in das Auge fällt. Wenn die AP kleiner ist als die des Auges, ist das Bild dunkler als bei Betrachtung mit bloßem Auge. Ist sie größer, erscheint das Bild höchstens gleich hell. Ein Nachtglas hat deshalb eine Austrittspupille von mehr als 5 mm.

Als Fernrohrleistung bezeichnet man ferner die Nutzleistung eines Fernrohres bei der Sichtung oder der Detailauflösung eines Objekts, bezogen auf die Leistung des bloßen Auges.

Bei der visuellen Nutzung des Fernrohrs dient das Auge als Empfänger. Dazu muss das optische System afokal sein, das heißt, das Fernrohr muss parallele Lichtstrahlen erzeugen, die vom entspannten Auge auf der Netzhaut empfangen werden können. Dies wird mit Hilfe eines Okulars erreicht.

Fernrohre, die nur ein Objektiv haben, erzeugen kein stereoskopisches Bild. Außerdem sind die Beobachtungsobjekte meist so weit entfernt, dass die Strahlengänge des Lichts nahezu parallel verlaufen. Es werden aber binokulare Ansätze für das beidäugige Sehen verwendet. Diese sollen ein entspannteres Sehen ermöglichen. Dafür wird der Strahlengang aufgespalten, was die Helligkeit des Bildes verringert.

Bei Beobachtung entfernter Objekte sind die einfallenden Strahlen fast parallel. Das Fernrohr verwandelt in diesem Fall einfallende, fast parallele Strahlen in austretende Parallelstrahlen, verändert zuvor aber den Winkel und die Dichte dieser Strahlen. Die Veränderung des Winkels bewirkt die Vergrößerung. Die größere Dichte der Strahlen vergrößert die Helligkeit des Bildes. Bei flächenhaften Beobachtungsobjekten kann die Helligkeit des Bildes jedoch nicht größer sein als die Helligkeit des Objektes.

Bei der fotografischen Nutzung hat das Fernrohr die Funktion eines sehr langbrennweitigen Objektivs.
Wegen ihrer großen Brennweite und wegen ihres Gewichtes werden große Fernrohre von Montierungen gehalten und bewegt.

Wegen der Beugung des Lichtes ist das Auflösungsvermögen des Fernrohrs durch den Durchmesser des Objektivs begrenzt. Die Vergrößerung, die das Auflösungsvermögen des Fernrohrs der des menschlichen Auges optimal anpasst, wird als "nützliche Vergrößerung" bezeichnet. Diese ist zahlenmäßig etwa so groß wie die Apertur (Öffnung) des Fernrohrobjektivs in Millimetern. Bei einer stärkeren Vergrößerung erscheinen Sterne nicht als Punkte, sondern als Scheibchen, die von konzentrischen Kreisen (Beugungsringen) umgeben sind.

Vom Boden aufsteigende erwärmte Luft, ungenügend temperierte Sternwarten-Kuppeln oder die Beobachtung am geöffneten Fenster verursachen störende Schlieren.

Vor allem im Winter und bei bestimmten Wetterlagen ist deutlich ein Szintillation genanntes Funkeln der Sterne zu sehen. Dieses wird durch in sich rotierende Konvektionszellen hervorgerufen, die durch den Wärmeübergang zwischen kälteren und wärmeren Luftschichten entstehen. Oft erscheinen die Sterne und Planeten in kleinen Fernrohren als „wabernde Flecken“; bei fotografischen Aufnahmen werden sie unscharf. Meist bessert sich die Lage mit fortschreitender Nacht.

Astronomen nennen diesen Faktor "Seeing". Die Position eines Sterns kann durch ein schlechtes Seeing um 1" bis 3" schwanken. Ein gutes Fernrohr mit einem Auflösungsvermögen von 1", das dazu eine Apertur von etwa 150 mm haben muss, wird also mit seiner Qualität selten voll ausgenutzt. Bei der Beobachtung flächenhafter Objekte, wie Nebeln oder Kometen, ist das Seeing weniger von Bedeutung.

Die Montierung, mit der das Fernrohr gehalten und bewegt wird, entscheidet darüber, welche Vergrößerungen mit einem Fernrohr sinnvoll genutzt werden können. Jede zu starke Schwingung in der Montierung (z. B. durch Wind) macht sich als Zittern des Beobachtungsobjektes im Gesichtsfeld des Okulars bemerkbar. Die Montierung sollte also möglichst steif, schwingungsarm und mit dem Gewicht des verwendeten Fernrohrs nicht überfordert sein.

Bei oft nur mit der Hand gehaltenen Feldstechern werden meist Okulare fest eingebaut, die nur relativ geringe Vergrößerungen zulassen. Bei diesen Instrumenten wird ein größerer Wert auf die Lichtstärke gelegt. Ein festes Stativ ist aber auch hier von Vorteil.

Bei der Sonnenbeobachtung durch ein Fernrohr muss ein geeigneter Sonnenfilter verwendet werden, der vor dem Objektiv angebracht wird. Filter, die vor das Okular geschraubt werden, erhalten bereits die verstärkte Intensität und können infolge Hitzeentwicklung platzen und schlimmstenfalls zur Erblindung des Beobachters führen. Lichtmindernde Alternativen sind Herschelkeil, Pentaprisma und Bauernfeindprisma, die beide mit grauen Dämpfungsfiltern im Okular verwendet werden dürfen und (visuell) auch müssen. Ohne Lichtminderung einsetzbar ist die Sonnenprojektionsmethode, welche sich für simultane Beobachtung durch mehrere Personen eignet.

Das Blickfeld wird bei Benutzung eines Fernrohrs einerseits merklich eingeschränkt, andererseits deutlicher dargeboten. Das Okular bestimmt wesentlich die Qualität des Bildes und die Ergonomie der Beobachtung, insbesondere die Größe des "scheinbaren" Gesichtsfeldes. Moderne Okulare zeigen ein Gesichtsfeld von etwa 45°, bei Weitwinkelokularen je nach Preis 55 bis 75°.

Das wahre Gesichtsfeld, der sichtbare Ausschnitt des Objektraumes, ist etwa um den Vergrößerungsfaktor des Instruments kleiner als das scheinbare Gesichtsfeld. Hat ein Okular z. B. ein scheinbares Gesichtsfeld von 50°, dann hat ein Fernrohr mit 50-facher Vergrößerung ein wahres von 1°. Typisch bei astronomischen Fernrohren sind 0,5° (Mond-Durchmesser), übliche Feldstecher haben etwa 7° (5° bis 10°), Aussichtsfernrohre einige Grad.

Die Bestimmung des Gesichtsfeldes erfolgt am genauesten mittels eines Sterndurchgangs: Wir suchen einen äquatornahen Stern – am besten im Süden, in etwa 40° Höhe (genauer 90° minus Breite) – und messen, wie lange er benötigt, um durch das Gesichtsfeld zu wandern. Die (dezimalen) (Zeit-)Minuten sind durch vier zu teilen. Dauert der Durchgang 2,4 Minuten, hat das Fernrohr ein Gesichtsfeld von φ = 0,60°. Kennt man diesen Wert, lassen sich Entfernungen schätzen: Wenn z. B. eine stehende Person von 1,70 m die 0,60° gerade ausfüllt, ist sie 1,70 / sin(φ) = 162 m von uns entfernt. Jäger, Seeleute und Militärs verwenden dafür auch Fernrohre oder Feldstecher mit Skalen – doch gibt es nützliche Faustregeln. Wer daher das geschilderte Verfahren perfektionieren will, könnte es zunächst an einem Feldstecher erproben. Bessere Geräte geben die Grad (bzw. die Meter auf 1 Kilometer Distanz) an. So hat ein normales 7x50-Fernglas ein Gesichtsfeld von etwa 7,2° oder 125 m auf 1 km.

Für den Anschluss einer Kamera ist eine mechanische und optische Anpassung notwendig. Ein Adapter verbindet entweder das Kameragehäuse mit dem Okularauszug oder Kamera und Objektiv mit dem Okular. Eine feste mechanische Verbindung ist besonders wichtig, da kleinste Bewegungen (Schwingungen) der Kamera die Bildqualität stark reduzieren. Für die Auslösung der Kamera sollte eine drahtlose Fernbedienung verwendet werden. Des Weiteren ist eine optische Anpassung des Strahlengangs notwendig, damit ein voll ausgeleuchtetes und scharfes Bild auf den Sensor der Kamera (CCD / CMOS) oder den Film projiziert wird.

Wer eine sehr ruhige Hand hat, kann auch ohne Adapter fotografieren. Doch muss das Fotoobjektiv erstens die richtige Brennweite haben und zweitens "genau zentrisch" (und im richtigen Abstand) hinter dem Okular sein, damit nicht Teile des Gesichtsfelds abgeschnitten werden. Dies ist die größere Gefahr als ein eventuelles Verwackeln.

Bei terrestrischen Aufnahmen ist zu beachten, dass der Belichtungsmesser durchs Fernrohr nicht genau stimmen muss. In der Astrofotografie ist wegen der Erdrotation eine Nachführung notwendig, die nur bei hellen Objekten (Sonne, Mond, Venus bis Jupiter) entfallen kann.

Vor der Erfindung des Fernrohrs mit Linsenoptik diente der Blick durch ein einfaches Rohr (ein sogenanntes "Sehrohr") zur Ausblendung von Streulicht, so dass einzelne Himmelsobjekte deutlicher wahrgenommen werden konnten. Der Effekt ist seit dem Altertum bekannt, wobei allerdings Behauptungen, wie z. B. von Aristoteles und Plinius, dass man die Sterne sogar am Tag vom Boden eines tiefen Brunnens aus sehen könne, bisher nicht zweifelsfrei bestätigt sind.

Erst mit dem Aufkommen von Brillengläsern seit dem 13. Jahrhundert war überhaupt die Möglichkeit geschaffen worden, ein Fernrohr zu bauen. Mit den Brillengläsern war das Prinzip von Linsen bekannt. Allerdings waren die verwendeten Gläser zu Beginn noch zu ungenau, um mit ihnen ein einsatzfähiges Fernrohr zu bauen. Man benötigte für die Fernrohrobjektive sehr genau gearbeitete Linsen, die so nicht zur Verfügung standen.

Forscher in aller Welt machten sich vor der Erfindung des Fernrohres Gedanken darüber, wie man mit optischen Hilfsmitteln die Gestirne besser beobachten könne. Im "Codex Atlanticus" von Leonardo da Vinci findet sich beispielsweise eine Notiz, die dessen Absicht belegt, ein optisch vergrößerndes Gerät auf den Mond zu richten: "Fa ochiali davedere / la luna grande […]". (deutsche Übersetzung: „Mach Brillen um zu sehen / den großen Mond“). Heinz Herbert Mann kommentiert diesen Eintrag wie folgt: „Leonardo mag sich in seiner mit Analogien operierenden Denkweise gefragt haben: Welche Linse vergrößert den Mond? Damit überlegt er, welche Linse auf weite Entfernung hin vergrößern würde. Dies war nur eine Idee, der noch kein durchführbares technisches Konzept zugrunde lag.“

Interessant ist, warum die Linsen im ausklingenden 15. Jahrhundert plötzlich brauchbar wurden. Dies hatte sehr viel mit dem aufkommenden Buchdruck zu tun, für den Gutenberg den Anstoß gegeben hatte. Mit der steigenden Anzahl an Büchern stieg im Bürgertum auch die Anzahl der Leute, die lesen konnten. Zwangsläufig stieg sehr plötzlich die Nachfrage nach Sehhilfen zum Lesen rapide an, was dazu führte, dass die bis dahin herrschende venezianische (italienische) Monopolstellung auf dem Gebiet des Anfertigens von Linsen und Brillen gebrochen wurde. So siedelten sich Brillenmacher nun beispielsweise auch in Nürnberg an. Die gestiegene Nachfrage sorgte nicht nur für eine Expansion des Brillenschleifens, sondern auch für die Entwicklung neuer Techniken. Die Linsenqualität verbesserte sich und sorgte dafür, dass Ende des 16. Jahrhunderts die Möglichkeit gegeben war, mit dem nun vorhandenen Material Vorrichtungen zu bauen, mit denen man sehr weit in die Ferne blicken konnte.

Das erste Fernrohr wurde schließlich 1608 vom Brillenschleifer Hans Lipperhey konstruiert. Er stellte es im Rahmen der Konfrontation zwischen der angeschlagenen Großmacht Spanien und den sich konstituierenden vereinigten Niederlanden vor. Moritz von Nassau, Generalstatthalter der Nordprovinzen, ließ die neuartige Entdeckung Lipperheys vor den Augen des spanischen Gesandten Spinola bei Den Haag vorführen, wahrscheinlich am 29. September 1608. Es wurde bei diesem Treffen und der Vorführung des Gerätes deutlich, dass dieses neue optische Hilfsmittel vor allem im militärischen Bereich weitreichende Vorteile erbringen könnte. Es handelte sich bei dieser Vorführung von Moritz von Nassau demnach nicht um eine schlichte Vorführung einer Kuriosität in adeligem Umfeld, sondern vielmehr um eine Demonstration eigener technischer Überlegenheit gegenüber den Spaniern. Neben dieser Intention Moritz von Nassaus wurde bei diesem Treffen und der Vorführung ebenfalls darauf hingewiesen, dass dieses Instrument dazu dienen könne, genauere Himmelsbeobachtungen durchzuführen, da man nun kleine Himmelskörper, die sonst kaum oder gar nicht zu sehen seien, erkennen könne. Man kann allerdings festhalten, dass das Hauptaugenmerk bei dieser Vorführung und in den kommenden Jahrhunderten auf der militärischen Verwendung des Fernrohrs lag.

Lipperheys Leistung bei der Entwicklung des Gerätes bestand darin, dass er das nun bereits vorhandene Wissen nutzte, mithilfe von zwei Linsen ein Fernrohr baute und dieser Konstruktion abschließend eine Blende hinzufügte, die dafür sorgte, dass das Bild nicht mehr verschwommen war.
Er sprach zudem nicht nur beim Hof vor, um einen Markt für sein Fernrohr zu erschließen, sondern auch, um ein Patent auf sein Gerät zu erhalten. Dies wurde ihm jedoch verwehrt mit dem Verweis darauf, dass auch andere bereits ähnliche Vorrichtungen entwickelt hätten und der Nachbau zu einfach war. Auch Jacob Metius wird mit der Erfindung des Teleskops in Verbindung gebracht, er bewarb sich drei Wochen später als Lippershey um das Patent. Im Oktober 1608 erteilten die Generalstaaten Lippershey einen Auftrag über Teleskope, Metius erhielt eine Anerkennungsprämie worüber er so verärgert war dass er sich ganz aus dem Geschäft mit Teleskopen zurückzog. Zacharias Janssen als dritter Erfinder präsentierte die Erfindung dagegen gleich 1608 auf der Frankfurter Messe.

Lipperheys Konstruktion war nach heutiger Bezeichnung ein Galilei-Fernrohr und vergrößerte drei- bis vierfach.
Die Erfindung verbreitete sich schnell: im April 1609 konnte man Teleskope in Paris kaufen und vier Monate später in Italien. Unter anderem wurde es im Juni/Juli 1609 von Galileo Galilei in Venedig nachgebaut und bald darauf verbessert. Dieser gilt als derjenige, der mit dem Fernrohr die ersten wichtigen Entdeckungen bezüglich der Himmelskörper machte, unter anderem entdeckte er damit die vier größten Monde des Jupiters (galileische Monde) und die Berglandschaften des Erdmondes. Es wird jedoch häufig vergessen, dass er nur einer von vielen Gelehrten jener Zeit war, die unabhängig voneinander an verschiedenen Orten ähnliche Beobachtungen machten (wie z. B. David Fabricius, Thomas Harriot (der schon im August 1609 mit einem Fernrohr den Mond beobachtete) oder Simon Marius).

Die Entdeckungen, die das Fernrohr ermöglichte trugen wesentlich zur Durchsetzung des Heliozentrischen Weltbildes und der Theorie von Kopernikus bei. 1611 erfand Johannes Kepler das nach ihm benannte Kepler- oder astronomische Fernrohr. Kepler hatte zuvor seine theoretischen Untersuchungen zu den Planetenbewegungen auf die Beobachtungen von Tycho Brahe gestützt, der noch wie Ptolemäus traditionelle Geräte wie Quadrante und Sextante benutzte.

Eine weitere Zäsur durch die Beobachtungen mit dem Fernrohr bestand in der Einführung der beschreibenden Astronomie.
Die Astronomie bestand bis dahin meist nur in der Bestimmung von Winkelverhältnissen der Planeten zueinander und zu den Fixsternen. Das Fernrohr sorgte nun dafür, dass es möglich wurde, die Planeten qualitativ genauer zu beschreiben (Farbe, Konsistenz, Oberflächenbeschaffenheit). Besonders beim Mond aufgrund seiner Nähe zur Erde konnte man so beispielsweise die Höhe der Mondberge aufgrund ihrer Schatten berechnen. Auch die Betrachtung der Flecken der Sonne war nun wesentlich genauer möglich. Natürlich war die Erfindung auch von großer militärischer Bedeutung zu Wasser und zu Land, in der Seefahrt und für die Landvermessung.

Das Fernrohr wurde vorerst jedoch nicht als Hilfsmittel für die Winkel- und Abstandsmessungen in der Astronomie benutzt. Dies war erst deutlich später der Fall. Erst die Passageninstrumente, die Olaus Römer um ca. 1700 fertigstellte, machten es möglich, das Fernrohr als präzises astronomisches Messinstrument einzusetzen.




</doc>
<doc id="13346" url="https://de.wikipedia.org/wiki?curid=13346" title="Tallinn">
Tallinn

Tallinn (deutsche Aussprache [], estnische Aussprache []) ist die Hauptstadt von Estland. Es liegt am Finnischen Meerbusen der Ostsee, etwa 80 Kilometer südlich von Helsinki. Mit rund 430.000 Einwohnern ist Tallinn die mit weitem Abstand größte Stadt des Landes und sowohl wirtschaftliches als auch politisches und kulturelles Zentrum Estlands.

Bis zum 24. Februar 1918 hieß die Stadt amtlich Reval [], ein im deutschsprachigen Raum auch danach noch gebräuchlicher Name. Andere ältere Namen sind und vormals Колывань "(Kolywan)", , .

Der Name "Tallinn", den die Stadt seit der Eroberung durch den dänischen König Waldemar 1219 im Estnischen trägt, wird üblicherweise abgeleitet von "Taani-linn(a)", das heißt „Dänische Stadt“ oder „Dänische Burg“ ().

Im Jahre 2015 wurde Tallinn der Ehrentitel „Reformationsstadt Europas“ durch die Gemeinschaft Evangelischer Kirchen in Europa verliehen.

Tallinn unterteilt sich in die Stadtteile Haabersti, Kesklinn, Kristiine, Lasnamäe, Mustamäe, Nõmme, Pirita und Põhja-Tallinn sowie 84 Distrikte.









Tallinn grenzt im Nordosten an Viimsi, im Osten an Jõelähtme, im Südosten an Rae, im Süden an Saku, im Südwesten an Vasalemma und im Westen an Harku.

Die Ursprünge Revals gehen auf eine hölzerne Burg (auf dem heutigen Domberg) und einen vermuteten estnischen Handelsplatz zurück, die Mitte des 11. Jahrhunderts gebaut wurden. Gleichzeitig wurde in dieser Zeit der Hafen Tallinns angelegt. Der Name "Reval" rührt vom estnischen Namen des historischen Landkreises her, dessen Zentrum die Stadt war, und wurde für die Burg und die spätere Stadt erst von Dänen und Deutschen geprägt (, nach Heinrich von Lettland "Revele," nach dem Waldemar-Erdbuch "Revælæ").

Im Jahre 1219 eroberte der dänische König Waldemar II. die alte estnische Burg (Schlacht von Lyndanisse) auf dem Domberg, errichtete sie neu und begann mit dem Bau einer Domkirche für den von Dänemark um 1167 im Zuge seiner Missionierung ernannten Bischof der Esten, Suffragan des Erzbischofs von Lund. Dänemark konnte die Burg jedoch nicht lange gegen die aufständischen Esten und die vordringenden Deutschen halten. 1227 eroberte der Schwertbrüderorden Reval mit päpstlicher Genehmigung und erhielt die Burg und einen Großteil des heutigen Estland zur Verwaltung aus der Hand des päpstlichen Statthalters in Estland.

Wahrscheinlich um seine Stellung gegen die ländlichen Vasallen zu stärken, ließ der Schwertbrüderorden im Jahre 1230 aus Gotland 200 westfälische und niedersächsische Kaufleute anwerben, die sich, mit Zollfreiheit und Land belehnt, unterhalb der Burg ansiedelten. Obwohl eine Gründungsurkunde nicht überliefert ist, ist hierin wohl die eigentliche Gründung einer Stadt Reval zu sehen.

Als der Orden es ablehnte, seine Lehnsherrschaften und die Burg drei Jahre später wieder an den päpstlichen Legaten zu übergeben, machte der dänische König seine Ansprüche auf Reval und Estland wieder geltend. Nach der vernichtenden Niederlage in der Schlacht von Schaulen im Jahre 1236 strebte der Schwertbrüderorden die Vereinigung mit dem Deutschen Orden an, die der Papst nur gegen die Herausgabe Revals genehmigte. So ging der Schwertbrüderorden 1237 als Livländischer Orden in den Deutschen Orden über, und Reval fiel 1238 an Dänemark. In diesem Zusammenhang wurde Reval zum ersten Mal als "civitas" (Bürgerschaft, Stadt) erwähnt.

Unter der erneuten dänischen Herrschaft bis 1346 gewann die Stadt rasch an Größe und wirtschaftlicher Bedeutung. 1248 erhielt sie vom dänischen König das lübische Stadtrecht, das bis 1865 galt. Dieses galt allerdings nicht auf dem Domberg. Mit derselben Urkunde wurden die ersten Ratsherren ernannt. Die Stadt erhielt nach und nach umfangreiche Privilegien, die sie vom Landesherrn weitestgehend unabhängig machten. Die Amtssprache in Tallinn war bis 1889 Deutsch.

Obwohl Reval unter (zunehmend lockerer) dänischer Herrschaft stand, behielt die Stadt eine deutsche Oberschicht, und da diese fast ausschließlich aus Kaufleuten bestand, wurde ein enger Kontakt zur Hanse unterhalten. Dass sich Reval als der Hanse zugehörig betrachtete, ist bereits für 1252 belegbar und findet spätestens 1285 ausdrückliche Erwähnung. Von wirtschaftlicher Bedeutung war die dänische Entscheidung von 1294, allen deutschen Kaufleuten den Handelsweg nach Nowgorod über Reval und Narwa zu gestatten. Damit konnte Reval zu einem Knotenpunkt des hansischen Ostseehandels werden.

Nach der Niederschlagung eines großen Estenaufstandes mit der Hilfe des Deutschen Ordens entließ der dänische König 1346 seine estländischen Vasallen aus ihrem Treueid und verkaufte seine Rechte an Nord-Estland dem Deutschen Orden. Reval, das sich im Jahr vor dem Verkauf alle bestehenden und einige neue Privilegien durch den dänischen König hatte bestätigen lassen, bekam nun durch den neuen Landesherrn sämtliche Privilegien zugesichert und konnte so seine rechtliche und autonome Stellung während des Wechsels noch ausbauen.

Reval, Teil des „Livländischen Drittels“ der Hanse, erhielt 1346 zusammen mit Riga und Pernau das Stapelrecht, das alle mit Russland Handel treibenden Kaufleute dazu verpflichtete, eine der drei Städte anzulaufen und für einen Zeitraum von drei bis acht Tagen ihre Waren auf dem Markt anzubieten. Mehrere exklusive Handelsrechte für die Revaler Kaufleute beendeten den bis dahin für jeden offenen Freihandel in der Stadt. Die bisher wichtigste Handelsstadt der Ostsee, Wisby, konnte sich von der Plünderung durch den dänischen König 1361 und in den darauf folgenden Kriegsjahren nicht wieder zu ihrer vorherigen Vormachtstellung erholen, und als zur Jahrhundertwende die Vitalienbrüder aus der Ostsee verbannt werden konnten, war Reval die wichtigste Stadt des hansischen Osthandels.

Der Russlandhandel blieb allerdings nicht immer ungetrübt. Nach mehreren unsicheren Jahren brach 1471 der Handel mit Nowgorod durch Angriffe der Moskauer ganz ab, und 1478 wurde das bis dahin unabhängige Fürstentum von den Moskauern endgültig erobert. Das Großfürstentum Moskau führte Krieg gegen Livland, mit dem es nun eine gemeinsame Grenze besaß. Der Einfall der Moskauer Russen in Livland 1481 brachte der von Flüchtlingen überfüllten Stadt einen schweren Pestausbruch. Weitere schwere Seuchenjahre der Stadt waren 1464, 1495/96 und 1519/20. Nach einer kurzen Friedensperiode, in der das Nowgoroder Handelskontor wieder eröffnet und erneut geschlossen wurde, folgte 1501–1503 ein erfolgreicher Kriegszug des Deutschen Ordens gegen Moskau, an den sich ein bis 1558 dauernder Friede anschloss.

Die Kriege mit den Moskauer Russen brachten für Livland und Reval schwere Verluste an Wirtschaft und Bevölkerung. Erst 1514 gelang die erneute Errichtung einer Handelsbeziehung der livländischen Städte Reval und Dorpat mit Nowgorod, die zu einer neueren wirtschaftlichen Blüte bis in die 1550er Jahre führte. Im 16. Jahrhundert hatte die Stadt ca. 6000–7000 Einwohner.

Die Reformation erreichte Reval 1523/24. Ihren endgültigen Durchbruch erlebte sie, als sich im Juli 1524 Vertreter der livländischen Städte und Ritter im Revaler Rathaus versammelten und beschlossen, bei der protestantischen Lehre zu bleiben und sie mit allen Mitteln zu verteidigen. Im September 1524 kam es zu einem Bildersturm, dem die Ausstattung dreier Kirchen zum Opfer fiel. Die Verluste blieben dabei verhältnismäßig gering, da der Rat bereits am nächsten Tag die öffentliche Ordnung wiederherstellen konnte und für die Rückerstattung der geraubten Kunstschätze sorgte. Insgesamt lässt sich sagen, dass die Reformation in Livland und in Reval unblutig erfolgte. Am 9. September 1525 wurde die neue Lehre in Reval durch den Erlass einer lutherischen Kirchenordnung seitens des Rates und der Gilden „amtlich“.

Die restliche Zeit der Ordensherrschaft war von inneren und äußeren Streitigkeiten geprägt, bis Moskau bei seinem Einfall 1558–1561 den Deutschen Orden in Livland besiegte. Reval wandte sich an Schweden als Schutzmacht, womit eine bis zum Großen Nordischen Krieg 1710 anhaltende schwedische Herrschaft in der Stadt begann.

Im Jahre 1549 erhielt die Olaikirche einen gotischen Turm mit einer angeblichen Höhe von 159 Metern; bis zum Brand von 1629 wäre er damit das höchste Gebäude der Welt gewesen. Heute ist er nach einem Wiederaufbau im 19. Jahrhundert 123,7 Meter hoch.

1561 wurde die Stadt in der Zeit des Livländischen Krieges schwedisch. Die Schweden reduzierten nach und nach die Vorrechte der Deutschen, jedoch nicht in dem Ausmaß, wie es die Esten im Hinblick auf den Status der Bauern in Schweden zunächst erhofften.

Infolge des Großen Nordischen Krieges fiel Reval im Zuge der Belagerung von Reval 1710 an Russland. Peter I. setzte die alten deutschen Ratsgeschlechter wieder vollständig in ihre ursprünglichen Positionen, in den nächsten zwei Jahrhunderten wurden die Rechte der Stadtregierung dann schrittweise reduziert.

Am 24. Februar 1918 wurde die selbständige Republik Estland ausgerufen; die Stadt, die nun "Tallinn" hieß, wurde schließlich Hauptstadt des unabhängigen Estland. Die eigentliche Unabhängigkeit wurde im Freiheitskrieg (1918–1920) erkämpft und durch den Friedensvertrag mit dem sowjetischen Russland gekrönt.

Ein geheimes Zusatzprotokoll zum deutsch-sowjetischen Nichtangriffspakt (im August 1939) machte den Weg für die Eroberung Estlands durch die Sowjetunion frei. Die deutschbaltische Bevölkerung wurde vom Tallinner Hafen aus auf Befehl Hitlers in den neu geschaffenen Reichsgau Wartheland umgesiedelt. Nach der sowjetischen Okkupation im Juni 1940 wurde die Estnische Sozialistische Sowjetrepublik ausgerufen, deren Hauptstadt Tallinn blieb. Es begannen die ersten Deportationen der estnischen Bevölkerung – insbesondere der politischen und kulturellen Elite – nach Sibirien und Nordrussland. In den sowjetischen Terrorwellen nach 1940 und dann wieder ab 1944/45 wurde insgesamt jeder fünfzehnte Este ermordet und jeder siebzehnte zumindest für zehn Jahre nach Sibirien verschleppt.

1941 besetzte die deutsche Wehrmacht Tallinn, wodurch die Stadt und das Land von einer Willkürherrschaft in die nächste geriet. Hitler verfolgte das Ziel, Estland dem Deutschen Reich anzugliedern. Die von den Esten erhoffte Wiederherstellung der Unabhängigkeit erfolgte nicht. Dennoch beteiligten sich viele junge Esten am Vormarsch der deutschen Wehrmacht nach Osten und nahmen an Vernichtungsaktionen teil. Die deutsche Besatzungsmacht ließ die jüdische Bevölkerung Tallinns und Estlands nahezu gänzlich ermorden. 

Am 9. März 1944 erfolgte ein schwerer sowjetischer Luftangriff. Es wurden 11 % der Altstadt zerstört und 600 Tote gezählt. Während des Krieges blieb der Charakter der Altstadt trotz der Bombardierungen durch die sowjetische Luftwaffe gegen die in und um Tallinn stationierten deutschen Truppen erhalten. Die Wehrmacht wurde bis Ende 1944 von der Sowjetarmee im Zuge der Baltischen Operation aus Tallinn und Estland zurückgedrängt und die sowjetische Herrschaft wiederhergestellt. In der Stadt bestand das Kriegsgefangenenlager "286" für deutsche Kriegsgefangene des Zweiten Weltkriegs.

Nach 51 Jahren wurde Tallinn am 20. August 1991, zur Zeit des Moskauer Putsches, erneut zur Hauptstadt eines unabhängigen Estlands. Infolge des immensen Wirtschaftswachstums und des in manchen Schichten stark gestiegenen Wohlstandes sind rund um Tallinn innerhalb weniger Jahre riesige Neubaugebiete entstanden. So wurden beispielsweise im südlich von Tallinn gelegenen Gebiet Peetri auf einem ehemaligen Moor Ein- und Mehrfamilienhäuser gebaut. Vor allem junge Familien, die in den letzten Jahren von der wirtschaftlichen Entwicklung profitiert haben, lassen sich hier nieder. Es entsteht ein starker Kontrast zu den großen Siedlungen im sozialistischen Stil. Die Preise für Appartements in den Neubaugebieten sind teilweise bereits auf westlichem Niveau.

Ende April 2007 kam es in Tallinn durch Krawalle und Plünderungen hauptsächlich russischstämmiger Jugendlicher zu den stärksten Unruhen seit dem Zerfall der Sowjetunion. Grund dafür war die von estnischen Behörden nach längerer vorheriger Ankündigung am 27. April 2007 veranlasste Umsetzung des Bronze-Soldaten von Tallinn von seinem ursprünglichen Standort im Stadtzentrum auf einen Militärfriedhof. Die Esten verbinden dieses Denkmal eher mit der sowjetischen Besatzungszeit als mit der Befreiung von der deutschen Besetzung im Zweiten Weltkrieg, der das Denkmal gewidmet ist (und die es für Russen und die russische Minderheit in Estland symbolisiert). Infolge des Denkmalstreits kam es zu einer schweren Krise in den Beziehungen zwischen Estland und Russland, das sich vehement gegen die Umsetzung der Statue wandte.

Der Domberg und die Unterstadt waren bis 1877 sowohl in Verwaltung wie auch Rechtsprechung zwei autonome Städte.

Der Domberg, auf dem der Bischof, der Vertreter des Landesherrn, der des Deutschen Ordens und die Vertretung der Ritterschaft saßen, ist bis heute Zentrum der Staatsgewalt. Hier haben das Parlament der Republik Estland (Riigikogu) und die Regierung ihren Sitz. Der Domberg erhebt sich 48 m über der Unterstadt.

Die Unterstadt ist, geschichtlich gesehen, die eigentliche Stadt Reval. Hier lebte der Großteil der Stadtbevölkerung, Handwerker und Kaufleute. Die Stadt war dem Landesherrn gegenüber unabhängig. Es waren lediglich geringe jährliche Zahlungen an Zins und Pacht an den Orden zu leisten, und im Falle eines feierlichen Einzuges in die Stadt musste sie dem Landesherrn huldigen. In Rechtsfragen wandte sich die Stadt an Lübeck.

Der Bischof war allein geistlicher Hirte und kein Landesherr. Sein Besitz bestand aus Tafelgütern in der Diözese. Mit dem Verlust seines Episkopalrechts an die Stadt Reval durch das lübische Stadtrecht war er dieser gegenüber seiner geistlichen Machtstellung beraubt. Der Bischof von Reval war während der Ordenszeit Suffragan des Erzbischofs von Lund, der in dieser Zeit jedoch keinen Einfluss auf die Bischofswahl hatte. Das Domkapitel war mit vier Domherren ausgesprochen klein, und als Einkünfte standen ihm lediglich fromme Stiftungen und einige Dörfer in der Revaler Umgebung zur Verfügung. Auf dem Domberg befand sich neben dem Dom, der Vertretung des Deutschen Ordens und dem Sitz der v. a. harrisch-wierischen Ritterschaft nur noch eine kleine Bevölkerung von Handwerkern und Dienern.

Die Unterstadt nahm für ihre verhältnismäßig kleine Fläche (an ihrer längsten Nord-Süd-Achse maß die Stadt etwa 1 Kilometer, in der Breite weniger als 700 Meter) eine recht große Anzahl an Menschen auf. Es sind aus der Ordenszeit keine Einwohnerzahlen für die ganze Stadt vorhanden, aber für die Unterstadt existiert eine Schossliste von 1538, die rund 800 Personen umfasst, was im Vergleich zu späteren Einwohnerlisten und nach vorsichtiger Schätzung wohl eine Bevölkerungszahl von etwa 5000 Einwohnern annehmen lässt. Für die Domstadt steht das „Wackenbuch“ von 1575 zur Verfügung, mit dessen Hilfe sich etwa 1000 Personen (zusammen mit Dom, Ordensschloss und anwesenden Vasallen) vermuten lassen. Die Vorstädte werden nach ihrer Größe in der ersten Hälfte des 16. Jahrhunderts auf etwa 700 Bewohner geschätzt, was für die gesamte Stadt Reval zu dieser Zeit eine Bevölkerungszahl von etwa 6700 Einwohnern ergibt.

Frühere Schätzungen dürften noch ungenauer sein. Eine Schossliste von 1372 führt rund 650 Schosspflichtige auf. Wenn man sich die Vereinfachung erlaubt und die spätere Bevölkerungsschätzung für dieses Jahr anteilig herunterrechnet, dann ergäbe dies zusammen mit der Domstadt eine Bevölkerung von vielleicht knapp 5.000 Einwohnern (die Vorstädte existierten zu dieser Zeit noch nicht). Damit gehörte Reval zu den mittelgroßen europäischen Städten, weitab von Großstädten mit etwa 40.000 Einwohnern wie Köln, Wien und Prag oder mit 20.000 Einwohnern wie Lübeck, Nürnberg, Bremen oder Danzig. In seiner Bevölkerungszahl vergleichbar war Reval eher mit Städten wie Göttingen, Hildesheim oder Stockholm, wobei die Zahlen durch Konjunktur, Kriege und Seuchen stark schwanken konnten.

Die meisten Revaler Bürger waren deutsch und kamen, sofern sie nicht in Reval geboren wurden, aus dem Reich. Während des ganzen Mittelalters bildete Lübeck die Durchgangsstation und gelegentlich die Heimatstadt für kommende Revaler Neubürger. Die Fernhandel treibenden Kaufleute bildeten, über die Hansestädte verteilt, ein dichtes soziales Netz, häufig durch Verwandtschaft, sodass es nicht verwunderlich ist, wenn sich eine Familie gleichzeitig in Reval, Lübeck und anderen Hansestädten befand. Eine Untersuchung der in Revaler Bürgernamen des 14. Jahrhunderts vorkommenden Ortsbezeichnungen ergab, dass sich etwa die Hälfte aller Ortsnamen im rheinisch-westfälischen Raum wiederfinden lassen, die andere Hälfte setzt sich hauptsächlich aus dem gesamten norddeutschen Raum zusammen.

Das soziale Leben der Stadt wurde neben der Verwandtschaft oder der Nachbarschaft zu einem wesentlichen Teil durch die Berufsgruppen, die Zünfte und die drei Gilden, die "Kinder- oder Große Gilde", die "Kanutigilde" und die "Olaigilde", bestimmt, wobei mit der Geselligkeit innerhalb dieser Genossenschaften eine halb berufliche, halb private Sphäre geschaffen wurde. Die Gilden waren als kirchliche Korporationen gegründet, vereinigten aber bald die angesehenen Berufe und Zünfte und hatten wichtige soziale Funktionen. In ihnen wurden Beerdigungen und Hochzeiten ihrer Mitglieder gemeinsam begangen, man veranstaltete gesellige Mahlzeiten und Tanzfeste, legte Regeln für gutes Benehmen fest (bei Verstoß gingen genau angegebene Geldstrafen in die Gildenkasse) und half sich gegenseitig in Unglücksfällen. Die Gilden unterhielten eigene Altäre und sogenannte Tafelgilden zur Speisung der Armen. Die für die Gilden bestehenden Gildehäuser sind in der Revaler Altstadt erhalten, so das Haus der Großen Gilde.

Ein strenges soziales Unterscheidungsmerkmal bildete die Nationalität (Abstammung bzw. Herkunftsland). Die Stadt setzte sich im Wesentlichen aus drei Nationalitäten zusammen, aus Deutschen, Schweden und Esten (die sog. "Undeutschen"), und die Schossliste von 1538 ergibt folgendes Bild: Etwa ein Fünftel der schosspflichtigen Bevölkerung scheint schwedisch gewesen zu sein, jeweils zwei Fünftel deutsch und estnisch. Von ihrer sozialen Rangordnung her dürfte die gesamte Oberschicht und mehr als die Hälfte der Mittelschicht aus Deutschen bestanden haben. Der Rest der Mittelschicht setzt sich zu etwa einem Viertel aus Schweden und einem Fünftel aus Esten zusammen. Die Unterschicht bestand zu drei Vierteln aus Esten und, von vereinzelten Deutschen abgesehen, aus Schweden. Die sozialen Schichtungen richteten sich in diesem Fall nach der Schosszahlung und der Wohnsituation.

Nur sehr vorsichtig lässt sich die nationale Zusammensetzung auf dem Domberg beurteilen, da die Hauptquelle, das Wackenbuch von 1575, aus der Zeit der schwedischen Herrschaft stammt. Mit dem Wechsel des Landesherrn wird ein Wechsel in der Zusammensetzung der Bevölkerung der Domstadt, des Sitzes des Landesherrn, einhergegangen sein, zumal bei den im Wackenbuch aufgeführten vielen schwedischen Namen nicht auszuschließen ist, dass die schwedischen Schreiber deutsche Namen schlicht in schwedischer Form niederschrieben. Für die Ordenszeit kann dennoch angenommen werden, dass sich die Oberschicht nahezu komplett aus Deutschen, die Unterschicht größtenteils aus Esten zusammensetzte.

Die beiden Kirchspiele der Unterstadt entsprechen zwei verschiedenen städtischen Keimzellen. Zum einen ist der südliche Stadtteil durch den Alten Markt und die von ihm sternförmig ausgehenden Straßen charakterisiert. Hier bestand auf der Höhe des Verbindungsweges zum Domberg vermutlich bereits ein estnischer Handelsplatz, der in seiner Infrastruktur von den 200 gerufenen deutschen Kaufleuten übernommen wurde. Die für diesen Stadtteil zuständige Pfarrkirche, St. Nikolai, wird 1316 erstmals urkundlich erwähnt, geht aber wahrscheinlich auf die zweite Hälfte des 13. Jahrhunderts zurück und ist mit Sicherheit eine Gründung der deutschen Kaufleute. Wie in vielen anderen Hansestädten ist sie dem Heiligen Nikolaus, dem Patron der Seefahrer, gewidmet.

Zum anderen ist der lang gezogene nördliche Stadtteil durch die Langstraße bestimmt, die Hauptverbindungsstraße zwischen Hafen und Domberg, an der sich vor allem schwedische und russische Kaufleute niederließen. Die dortige, weit im Norden der Stadt befindliche Pfarrkirche ist St. Olai, erstmals erwähnt 1267, als die dänische Königin Margrete I. ihr Parochialrecht über die Kirche dem Revaler Zisterzienserinnenkloster zu St. Michael überlässt. Wie weit der Ursprung dieser Kirche in die Vergangenheit zurückreicht, ist unbekannt, es kann aber angenommen werden, dass sie entweder eine Gründung des dänischen Königs oder schwedischer Kaufleute ist, die wahrscheinlich schon vor der Stadtgründung hier einen Handelsplatz besaßen. Benannt ist sie nach dem heiliggesprochenen norwegischen König Olav. Nach dem großen Stadtbrand 1433, der das Mönchskloster St. Olai, die Münze und einen Teil des Marktplatzes verwüstete, ging die Kirche in den Besitz der Stadt über. Dass der nördliche Stadtteil eine ursprünglich von Fremden besiedelte Gemeinde war, zeigt die russische Kirche, die unweit von St. Olai stand. Beide Stadtteile wurden 1265 auf Befehl der dänischen Königin Margrete I. zusammengefügt und mit einer Stadtmauer umgeben.

Innerhalb der Stadtmauer befinden sich zwei Klöster: eines der Dominikaner und eines der Zisterzienserinnen. Das Dominikanerkloster zu St. Katharina entstand wahrscheinlich zuerst 1229 auf dem Domberg, wurde aber 1246 in der Stadt neu begründet und unterhielt enge Verbindungen zu den skandinavischen Dominikanern. Es erfreute sich zeit seines Bestehens bei den Bürgern großer Beliebtheit, was sich in starkem materiellem Wachstum durch Schenkungen und Stiftungen äußerte. Die Dominikaner kamen durch ihre Predigertätigkeit immer wieder in Konflikt mit dem Bischof und zur Zeit der Reformation in schwere Auseinandersetzungen mit der Stadt, die 1523 mit der Ausweisung der Mönche aus der Stadt endeten.

Das Zisterzienserinnenkloster wurde wahrscheinlich 1249 vom dänischen König gegründet. Die Kirche war St. Michael geweiht und gehörte zusammen mit der Klosteranlage erst mit einer Erweiterung der Stadtmauer zur inneren Stadtstruktur. Das Kloster war vom dänischen Königshaus sehr reich mit Privilegien ausgestattet, erwarb schon früh große Liegenschaften und nahm größtenteils unverheiratete Töchter des Adels auf, wodurch sich seine relativ schlechten Beziehungen zur bürgerlichen Stadtbevölkerung erklären. Nach der Reformation wurde es in eine weibliche Erziehungsanstalt umgewandelt.

Neben den ansässigen Klöstern hatten einige auswärtige Klöster Höfe in Reval. Der Hof der Zisterziensermönche von Dünamünde (später von Padis) wird zwar erst 1280 erwähnt, existierte aber wohl schon seit der ersten Dänenherrschaft. Direkt daneben lag der Hof der gotländischen Zisterzienser aus Roma, und diesem gegenüber lag der Hof der Zisterzienser aus Falkenau bei Dorpat auf einem Grundstück, das ihnen 1259 geschenkt wurde.

1316 wird erstmals die zu St. Olai gehörige Heilig-Geist-Kapelle erwähnt, die schon früh den Rang einer fast eigenständigen Kirche hatte und vor allem von den städtischen "Undeutschen" besucht wurde. Zu ihr gehörte das nach römischem Muster erbaute Heilig-Geist-Spital für Alte und Kranke. Weit älter war das Johannisspital, das 1237 erstmals erwähnt wurde. Es wurde als Leprosorium errichtet und nach dem Verschwinden des Aussatzes als Siechenhaus weitergeführt.

Außerhalb der Stadtmauer, vor der Schmiedepforte, befand sich die mit einem Kirchhof versehene Barbarakapelle, die zu St. Nikolai gehörte und deren Errichtung auf die erste Hälfte des 14. Jahrhunderts geschätzt wird. Die Kapelle existiert heute nicht mehr, vermutlich wurde sie bereits 1570/71 bei der russischen Belagerung zerstört. Ebenfalls außerhalb, in der Nähe des Hafens, vor der großen Strandpforte, befand sich die für Schiffer und Reisende erbaute Gertrudenkapelle. Ihr Bau wurde 1438 gestattet, 1570 jedoch wurde sie bei einem Brand zerstört. Auf dem Tönnisberg (Antoniusberg) stand die Antoniuskapelle, deren ursprünglicher Zweck nicht mehr rekonstruierbar ist.

Der 1407 begonnene Bau des Augustinerklosters St. Brigitten zu Marienthal war spätestens zu seiner Weihe 1436 beendet, wobei dem Kloster bereits 1411 die Augustinerregel gegeben wurde und 1412 das Tochterkloster Marienwohlde bei Lübeck gegründet wurde. Gründer waren drei Revaler Kaufleute, die später in den Konvent eintraten. Das Kloster befand sich in der Nähe der Küste, vier Kilometer nordöstlich der Stadt, an der Grenze zur Stadtmark und wurde 1435 das erste Mal in einem Revaler Testament bedacht. Es diente der Aufnahme von Personen beiderlei Geschlechts, jedoch überwogen die Frauen, meistenteils Bürgertöchter, die meist die Äbtissin stellten. Das Kloster wurde während zweier russischer Belagerungen, 1575 und 1577, zerstört.

Laut Volkszählung 2011 ergibt sich bezogen auf die verschiedenen Stadtteile ein höchst unterschiedliches Bild hinsichtlich der Bevölkerungsgruppen nach Muttersprache.
Ein Großteil der russischsprechenden Bevölkerung (v. a. Russen, Ukrainer, Weißrussen, Tataren) leben in den während der Sowjetzeit groß ausgebauten Wohnblockvierteln am Stadtrand, wie Lasnamäe, Väike-Õismäe und Astangu (Bezirk Haabersti) sowie Kopli, Pelguranna und Karjamaa (Bezirk Põhja-Tallinn).
Die Innenstadt (Bezirk Kesklinn), die von Einzelhausverbauung und kleineren Wohnanlagen geprägten Stadtviertel Kristiine und Nõmme sowie der im Nordosten gelegene Stadtteil Pirita sind hingegen weit überwiegend von Bevölkerung mit estnischer Muttersprache bewohnt. Der Bezirk Mustamäe mit seinen v. a. während der Sowjetzeit errichteten großen Wohnbausiedlungen ist ebenso gemischtsprachig wie Teile der Bezirke Põhja-Tallinn und Haabersti im Norden bzw. Nordwesten der Stadt.

Einen Überblick über die Aufteilung der Bevölkerung nach Muttersprache laut Volkszählung 2011 gibt die folgende Tabelle.

Tallinn ist Sitz des Konsistoriums und des Erzbischofs der Estnischen Evangelisch-Lutherischen Kirche. Die römisch-katholische Apostolische Administratur Estland hat ihren Sitz an St. Peter und Paul. Tallinn ist ebenfalls der Sitz des Metropoliten der Estnischen Apostolischen Orthodoxen Kirche, die zum Ökumenisches Patriarchat von Konstantinopel gehört, und des Metropoliten der Estnischen Orthodoxen Kirche des Moskauer Patriarchats. Der Großteil der Esten ist heute konfessionslos. Die estnisch- und russischsprachigen Anhänger der beiden Orthodoxen Kirchen bildeten im Jahr 2000 mit einem Anteil von 18,3 % die größte Konfession unter den Einwohnern Tallinns. Die Lutheraner folgten mit 11,4 %.

Tallinn ist die Hauptstadt der Republik Estland. In der Stadt haben der Präsident, die Regierung, das Parlament ("Riigikogu"), der Staatsgerichtshof ("Riigikohus"), die Ministerien sowie zahlreiche diplomatische Vertretungen ihren Sitz.

Alle vier Jahre werden die Mitglieder des Tallinner Stadtrates gewählt. Die letzten Wahlen der 79 Ratsmitglieder fanden am 15. Oktober 2017 statt. Zu den Aufgaben des unter dem Vorsitz von Mihhail Kõlvart stehenden Stadtrates gehört unter anderem die Wahl des Bürgermeisters. Dieses Amt hat seit November 2017 Taavi Aas inne.

Mit 40 Ratsmitgliedern verfügt die Estnische Zentrumspartei ("Eesti Keskerakond") über die absolute Mehrheit. 18 Mitglieder stellt die liberale Estnische Reformpartei ("Eesti Reformierakond"), neun die sozialdemokratische "Sotsiaaldemokraatlik Erakond", sechs die nationalkonservative "Eesti Konservatiivne Rahvaerakond" und die konservative "Isamaa ja Res Publica Liit" fünf Abgeordnete. Das 79. Mandat konnte der langjährige Bürgermeister Edgar Savisaar erreichen, der als unabhängiger Kandidat angetreten war.

Die Tallinner Stadtregierung ist das ausführende Organ und umfasst neben dem Bürgermeister sechs Vizebürgermeister.

Die Flagge Tallinns zeigt jeweils drei horizontale blaue und weiße Streifen. Auf dem Wappen der Stadt sind unter anderem drei Löwen zu sehen, die eines der ältesten estnischen Symbole darstellen und seit dem 13. Jahrhundert Verwendung finden.

Tallinn pflegt Partnerschaften mit folgenden Städten:

Die Tallinner Altstadt (estn. "Vanalinn") wurde 1997 zur Liste des UNESCO-Weltkulturerbe hinzugefügt als „außergewöhnlich vollständiges und gut erhaltenes Beispiel einer mittelalterlichen nordeuropäischen Handelsstadt“.

Das Zentrum bildet der Rathausplatz (estn. "Raekoja plats"), der von dem 1322 erstmals erwähnten, aber schon im 13. Jahrhundert errichteten gotischen Rathaus und anderen stattlichen Gebäuden umschlossen wird. Von der öffentlich zugänglichen Aussichtsplattform des Rathauses bietet sich ein hervorragender Blick über Stadt, Hafen und Meerbusen. Das Wahrzeichen Tallinns – die Figur des Stadtknechts „Alter Thomas“ (estn. "Vana Toomas") – schmückt seit 1530 die Turmspitze. Die beiden Wasserspeier in Drachengestalt sind aus dem 17. Jahrhundert.

Gegenüber befindet sich die Ratsapotheke (estn. "Raeapteek"). Sie wurde 1422 erstmals urkundlich erwähnt und ist damit eine der beiden ältesten noch tätigen Apotheken Europas (die andere ist in Dubrovnik). Nach Umbauten im 16. Jahrhundert mietete die aus Ungarn stammende Familie Johann Burchart die Apotheke und führte sie über 300 Jahre.

Die Stadtmauer ist eine der wichtigsten Sehenswürdigkeiten der Stadt. Im Mittelalter war Tallinn eine der am besten befestigten Städte an der Ostsee. Mit dem Bau der Befestigungen wurde in der zweiten Hälfte des 13. Jahrhunderts begonnen und dauerte die folgenden 300 Jahre an. Da die Waffen ständig schlagkräftiger wurden, musste fortwährend nachgebessert werden. Die fertige Mauer war schließlich 2,35 km lang, 13–16 m hoch und 2–3 m dick und hatte über 40 Türme. Heute stehen noch 1,85 km Mauer und 26 Türme. Die Lehmpforte war eines der Haupttore des mittelalterlichen Tallinn, das mehrfach umgebaut wurde. Von ihm ist heute nur noch das Vortor erhalten. Die Stadtmauer hatte im Mittelalter sechs Tore (Pforten), alle hatten ein bis zwei Vortore, Hängebrücken über den Wallgraben und Fallgitter. Die Große Strandpforte mit der Dicken Margarethe. Als die Große Strandpforte gebaut wurde, stand sie so nah am Ufer, dass bei Sturm die Wellen ans Tor schwappten. Erhalten ist das Vortor mit dem Kanonenturm Dicke Margarethe, dessen Durchmesser 25 m beträgt. Heute beherbergt er das estnische Seefahrtsmuseum, das einen Überblick über die Geschichte der Seefahrt und Fischerei gibt.

Sehenswert ist der Kiek in de Kök, ein ehemaliger Kanonenturm aus dem 15. Jahrhundert, der seinerzeit der stärkste Kanonenturm des Baltikums war. Weitere Türme der Revaler Stadtbefestigung sind der Goldene Fuß, der Loewenschede-Turm, der Reeperbahnturm und der Epping-Turm.

Die St. Nikolaikirche (estn. "Niguliste kirik"), eine spätgotische Steinkirche, entstammt dem Anfang des 13. Jahrhunderts. Nennenswert sind der Hauptaltar vom Lübecker Meister Hermen Rode aus dem Jahre 1481 und das Fragment des Totentanzes vom Lübecker Meister Bernt Notke. Sie ist ein Beispiel der im 13. Jahrhundert verbreiteten „Kaufmannskirchen“ (der Dachstuhl der Kirche diente als Warenlager). Zudem diente sie als Wehrkirche. Ab dem 15. Jahrhundert wurde sie zur Basilika umgebaut. Sie überstand als einzige Kirche den Bildersturm der Reformationszeit, weil, wie es heißt, der Kirchenvorsteher die Türschlösser mit Blei ausgießen ließ. Nach schwerer Zerstörung durch einen Bombenangriff im Jahre 1944 ist die Kirche heute Museum und Konzertsaal.

Die Heiliggeistkirche (estn. "Pühavaimu kirik"), im 14. Jahrhundert als Kapelle zum Heiligengeist-Armenspital hinzu gebaut mit zwei Funktionen: Kirche des Armenhauses und Ratskapelle. Sie besitzt einen spätmittelalterlichen Flügelaltar des Lübecker Meisters Bernt Notke aus dem 15. Jahrhundert und eine Uhr aus dem 17. Jahrhundert, angefertigt vom Meister Christian Ackermann.

Die Olaikirche (estn. "Oleviste kirik"), benannt nach dem norwegischen König Olaf II., der die Christianisierung Nordeuropas betrieb, wurde im 13. Jahrhundert erstmals urkundlich erwähnt. Der Turm kann bestiegen werden und bietet eine hervorragende Aussicht über die gesamte Stadt. In der Nähe befindet sich die historische Pferdemühle sowie der als Hotel genutzte historische Gebäudekomplex Drei Schwestern.

Das Haus der Schwarzenhäupterbruderschaft: Diese Bruderschaft gab es nur in Alt-Livland (Estland und Lettland), sie war einzigartig in Europa. Sie vereinte unverheiratete deutschstämmige Kaufleute. Nach der Aufnahme in die Gilde führte deren Karriere die erfolgreichsten in den Rat der Stadt. Der Name kommt von ihrem Schutzheiligen, dem frühchristlichen Märtyrer Mauritius. Die Bruderschaft bestand von ca. 1400 bis 1940 in Tallinn und ist seitdem in Deutschland weiter aktiv. Die Fassade des Hauses ist im Stile der Niederländischen Renaissance des 16. Jahrhunderts gehalten. Auf Höhe des Erdgeschosses befinden sich die Wappen der Hansekontore Brügge, Nowgorod, London und Bergen. Die russischen Zaren Peter I., Paul und Alexander I. waren Ehrenmitglieder der Bruderschaft und haben dieses Haus besucht.

Am nördlichen Rand der Altstadt von Tallinn steht neben dem Wehrturm „Dicke Margarethe“ die am 28. September 1996, exakt zwei Jahre nach dem Unglück, vom Bildhauer Villu Jaanisoo aus Stahl und schwarzem Granit fertiggestellte Skulptur "Katkenud liin" ("Unterbrochene Linie"). Sie ist dem Gedenken an den Untergang des Fährschiffs "Estonia" gewidmet, der aufgrund einer ungenügend geschlossenen Ladeklappe erfolgte und 852 Menschen das Leben kostete. Eine „Wasserstraße“ führt in einem weiten Bogen von einer Anhöhe zu einem Abgrund und bricht darüber ab. Weit jenseits der Bruchstelle setzt sich der Bogen fort, und die „Wasserstraße“ stürzt in das Erdreich hinein. Unter der unteren Abbruchstelle ruht eine schwarze Granitplatte, auf der die Namen der Ertrunkenen verzeichnet sind. Die Angehörigen legen hier und auf dem darüber stehenden Bogen Blumen, Kränze und Windlichter nieder.

Von der mittelalterlichen Burg auf dem Domberg (estn. "Toompea loss") sind nur noch die nördliche und westliche Mauer sowie drei Türme erhalten, darunter der Lange Hermann (estn. "Pikk Hermann"), gebaut im 14. Jahrhundert. Im 15. Jahrhundert wurde er noch einmal um 10 m auf 50 m erhöht. Im Mittelalter wurde er unter anderem als Gefängnis genutzt. Nach der Loslösung vom Zarenreich im Jahr 1918 wurde am Turm erstmals die blau-schwarz-weiße Fahne gehisst, die 1940 im Zuge der sowjetischen Okkupation durch eine rote ersetzt wurde. 1989 wurde die estnische Flagge dort wieder aufgezogen, und das geschieht heute täglich bei Sonnenaufgang; geht die Sonne unter, wird sie wieder eingeholt.

Daneben befindet sich das repräsentative Schloss, dessen wesentliche Umbauten im 18. Jahrhundert von der russischen Zarin Katharina II. veranlasst wurden. Heute ist es Sitz von Parlament und Regierung.

Die Domkirche liegt am Kirchplatz, an dem sich acht historische Straßen kreuzen, sie ist der Heiligen Jungfrau Maria gewidmet. Mit dem Bau wurde im 13. Jahrhundert begonnen, sie ist somit eine der ältesten Kirchen der Stadt. Später im 14. Jahrhundert wurde sie nach dem Vorbild der gotländischen Kirchen in eine dreischiffige Basilika im gotischen Stil umgebaut. Die Tallinner Gotik ist die sogenannte Kalksteingotik. Im Brand 1684 trug die Kirche schwere Schäden davon. Der Großteil der Einrichtung wurde vernichtet. Das neue Interieur ist barock. 107 Wappenepitaphe estländischer Adliger sind erhalten, ebenso viele Grabdenkmäler bekannter Persönlichkeiten wie Pontus De la Gardie, der Heerführer der Schweden im Livländischen Krieg, Adam Johann von Krusenstern, der bekannte Admiral, Weltumsegler und Entdecker, Samuel Greigh, der schottische Admiral, der für Katharina II. viele Siege errang, und andere mehr. Es gibt zwei Familienlogen aus dem 18. Jahrhundert, eine der Familie von Patkul und eine der Familie von Manteuffel. Die vorhandene Ladegast-Orgel wurde in der Werkstatt des Berliner Meisters Sauer perfektioniert. Die Domkirche ist heute eine lutherische Kirche mit einer 600-köpfigen Gemeinde.

Die russisch-orthodoxe Alexander-Newski-Kathedrale (estn. "Aleksander Nevski katedraal") mit ihren weithin sichtbaren Zwiebeltürmen wurde 1894–1900 als Sinnbild der Russifizierung Estlands erbaut. Daher konnte sich die estnische Bevölkerung längere Zeit kaum über dieses dominante „fremde“ Bauwerk freuen. Inzwischen ist sie ein weiterer touristischer Anziehungspunkt in der Altstadt.

Am Stadtrand befindet sich das Schloss Katharinental (estn. "Kadriorg"). Revals deutscher Friedhof Ziegelskoppel (estn. "Kopli"), auf der gleichnamigen Halbinsel nördlich der Altstadt gelegen, Schauplatz einiger Erzählungen von Werner Bergengruen, und der Friedhof der "Grauen", also der estnischen Bevölkerung, auf der Fischermai sind keine Sehenswürdigkeiten mehr. Beide wurden in den 1960er Jahren in Parks umgewandelt. Umfassungsmauern und Baumreihen lassen die frühere Nutzung noch erkennen, alle Grabsteine sind aber entfernt worden. Während in der Fischermai ("Kalamaja") eine Inschrift an dem kürzlich restaurierten Eingangstor des Friedhofes wieder an die frühere Nutzung erinnert, lässt sich der Friedhof von Ziegelskoppel nur durch einen Vergleich alter und neuer Stadtpläne ausfindig machen.

Im Stadtteil Pirita nordöstlich des Stadtzentrums gibt es einen Jachthafen sowie einen ausgedehnten Sandstrand, der von einem Kiefernwald begrenzt wird. An warmen Sommertagen herrscht dort Partystimmung, und der Strand ist deswegen oft sehr voll. Bei Joggern und Inlineskatern ist vor allem die Promenade zwischen Pirita und der Stadtmitte beliebt.
Hier steht die eindrucksvolle Ruine der Zisterzienser-Abtei St. Brigitten, ein heute dachloses Kirchenschiff vom Ausmaß einer Hauptstadt-Kathedrale, zerstört durch russische Truppen im 16. Jahrhundert. Nebengebäude sind noch als Mauerreste zu erkennen. Seit 2005 finden auf dem Gelände des früheren Klosters das Birgitta-Festival statt.

Eine idyllische Abwechslung bietet dagegen die dem Festland vorgelagerte Insel Naissaar in der Tallinner Bucht.

Den besten Ausblick auf die Stadt und bei guten Sichtverhältnissen sogar bis zur finnischen Küste bietet der Fernsehturm (estn. "Teletorn") mit seiner Aussichtsplattform und einem Restaurant, das derzeit allerdings wegen Sicherheitsmängeln geschlossen ist.

Der 1939 gegründete Zoo Tallinn wie auch das Estnische Freilichtmuseum liegen im Bezirk Rocca al Mare.

Tallinn ist die wirtschaftsstärkste Stadt in Estland. Ca. 60 % des estnischen BIP stammen aus Unternehmen in Tallinn. Infolge der Auflösung der UdSSR ging Russland als wichtigster Handelspartner verloren. In der darauf folgenden Privatisierung richtete man die Wirtschaft nach skandinavischem Vorbild ein. Die niedrige Steuerlast und das liberale Wirtschaftsumfeld machen es für Unternehmen attraktiv, sich in Tallinn anzusiedeln. So findet man in Tallinn Unternehmen wie Nokia, Philips oder Ericsson. Die kostenlose VoIP-Software Skype ist im Jahr 2003 hier entstanden. In Tallinn befindet sich der größte Bankensektor in den baltischen Staaten. Viele nordeuropäische Banken sind hier aufgrund der gut ausgebildeten Arbeitskräfte und der umfangreich ausgebauten Telekommunikationsstruktur ansässig u. a. die SEB, Swedbank, Nordea oder Sampo.

Der Tourismus ist einer der bedeutendsten Wirtschaftssektoren der Stadt.

Ziele für die Zukunft sind der Ausbau der Bildung- und Forschungsstätten und der Infrastruktur.
Außerdem soll die Stadtattraktivität steigen. Das Stadtbild ist heute noch stark von den sowjetischen Einflüssen geprägt.

Tallinn ist ein bedeutender Ostsee-Fährhafen (Verbindungen nach Helsinki, Stockholm, Åland und Sankt Petersburg). Der internationale Flughafen Tallinn-Lennart Meri ist nur vier Kilometer vom Stadtzentrum entfernt.

Vom Tallinner Baltischen Bahnhof (Balti jaam) bestehen im internationalen Eisenbahnpersonenfernverkehr eine tägliche Nachtverbindung nach Moskau und ein bis zwei Tagesverbindungen nach St. Petersburg. Nach anderen estnischen Städten, wie Tartu, Narva und Pärnu, verkehren dieselbetriebene Züge der Bahngesellschaft Elron. Der Regionalverkehr im Raum Tallinn wird S-Bahn-artig von derselben Gesellschaft realisiert. Projektiert ist der Bau einer Hochgeschwindigkeits-Eisenbahnstrecke, der Rail Baltica, die Tallinn mit Warschau über Riga und Kaunas verbinden soll.

Der Verkehr in die anderen Städte Estlands und die Nachbarländer Lettland und Russland wird ansonsten größtenteils mit Linienbussen abgewickelt. Tallinn liegt an der Europastraße 67 („Via Baltica“) und ist Estlands wichtigster Knotenpunkt des Straßenverkehrs.

Die Stadt selbst wird durch das kommunale Verkehrsunternehmen Tallinna Linnatranspordi AS bedient. Es betreibt die vier Linien der Straßenbahn Tallinn, die sieben Linien des Oberleitungsbus Tallinn sowie zahlreiche Stadtbuslinien. Das 19 Kilometer lange Straßenbahnnetz ist eines der wenigen europäischen Schienennetze mit Kapspur (1067 mm).

In einer Volksabstimmung im März 2012 hatte sich die Bevölkerung von Tallinn für die kostenlose Nutzung des öffentlichen Nahverkehrs ab 2013 ausgesprochen. Im Ergebnis ist seit Anfang 2013 für gemeldete Einwohner, die im Besitz einer elektronischen Fahrkarte sind, die Benutzung von Bussen und Bahnen im Rahmen des ÖPNV kostenlos. In der Folge haben sich viele Bewohner aus dem Umland, die bisher z. B. als Studenten ihren Zweitwohnsitz in der Stadt hatten, hier mit Erstwohnsitz gemeldet. Dies erhöhte deutlich die einwohnerbezogenen staatlichen Zuweisungen aus Steuermitteln. So konnte der kostenlose öffentliche Nahverkehr für die Bürger der Stadt finanziert werden.

Die 1938 gegründete Estnische Akademie der Wissenschaften ("Eesti Teaduste Akadeemia") befindet sich in Tallinn.

In der Stadt befinden sich unter anderem folgende Bildungseinrichtungen:

Tallinn wurde im November 2007 neben dem finnischen Turku zu einer der Kulturhauptstädte Europas 2011 ernannt. Unter dem Motto „Geschichten von der Meeresküste“ fanden 2011 zahlreiche kulturelle Veranstaltungen und Festivals statt, darunter die „Tallinner Meerestage“, die die Stadt wieder dem Meer näherbringen sollten, nachdem der Zugang zum Meer während der Sowjetzeit für Normalbürger weitgehend verwehrt war.

Die Nationaloper Estonia ("Raahvusooper Estonia") hat ihren Sitz in einem 1947 eröffneten Gebäude, das als Nachfolger des im Krieg zerstörten Originalbaus von 1913 durch die Architekten Alar Kotli und Edgar Johan Kuusik entworfen wurde. Theateraufführungen gibt es im "Tallinna Linnateater", das 1965 als Repertoiretheater gegründet wurde und über sieben Bühnen in einem mittelalterlichen Gebäudekomplex sowie eine Außenbühne verfügt. Das Estnische Russische Theater ("Eesti Vene Teater") hieß von seiner Gründung 1948 bis 2005 Nationales Russisches Schauspielhaus ("Riiklik Vene Draamateater"). Außerdem besteht das Estnische Schauspielhaus ("Eesti Draamateater").

Das Estnische Kunstmuseum ("Eesti Kunstimuuseum") ist das größte Kunstmuseum der baltischen Staaten und besteht aus mehreren einzelnen Museen, darunter den Sammlungen im vom finnischen Architekten Pekka Vapaavuori errichteten Gebäude des Kumu ("Kumu kunstimuuseum") im Stadtteil Kadriorg. Es wurde 2006 eröffnet und hat eine Ausstellungsfläche von 24.000 Quadratmetern, auf denen neben Moderner Kunst estnische Malerei ab dem achtzehnten Jahrhundert ausgestellt wird. Das Museum wurde 2008 mit dem "European Museum of the Year Award" als „Europäisches Museum des Jahres“ ausgezeichnet. Das Kadriorg-Kunstmuseum ("Kadrioru kunstimuuseum") zeigt westeuropäische und russische Malerei und Skulpturen vom 16. bis 20. Jahrhundert, darunter im zugehörigen Mikkel-Museum ("Mikkeli muuseum") die Sammlung Johannes Mikkels. Das Niguliste-Museum ("Niguliste muuseum") befindet sich in der Nikolaikirche ("Niguliste kirik") und ist mittelalterlicher Kunst gewidmet. Weiter zeigen das Adamson-Eric-Museum ("Adamson-Ericu muuseum") Werke des Künstlers Adamson-Eric und das Kristjan-Raud-Hausmuseum ("Kristjan Raua majamuuseum") Arbeiten Kristjan Rauds.

Tallinn ist Sitz des "Eesti Riiklik Sümfooniaorkester", des Nationalen Symphonieorchesters Estlands. Der Eurovision Song Contest 2002 fand in Tallinn statt, nachdem Tanel Padar, Dave Benton und 2XL mit einem gemeinsamen Titel bei der Ausgabe 2001 für Estland gewonnen hatten. Es gibt seit 1990 ein internationales Jazzfestival "Jazzkaar". Legendär ist das Jazz Festival in Tallinn 1967 mit dem Charles Lloyd Quartett während einer kurzfristigen Tauwetterperiode in der Sowjetunion – nach zwei Festivals 1966, 1967 war es wieder vorbei.

Während der Olympischen Spiele 1980 in Moskau wurden die Segelwettbewerbe vor Tallinn ausgetragen. Einige Einrichtungen wie die "Linnahall," das olympische Hotel, die Post und das Segelsportzentrum im Stadtteil Pirita wurden für dieses Ereignis gebaut.

Zu den ehemaligen olympischen Anlagen in Pirita (zehn Busminuten vom Stadtzentrum Tallinn) gehört der Jachthafen mit guter Infrastruktur für Fahrtensegler.

Zu den erfolgreichsten Fußballvereinen nach der Unabhängigkeit zählen FC Flora Tallinn und FC Levadia Tallinn. Flora spielt in der 2001 eröffneten, 9.692 Zuschauer fassenden "A. Le Coq Arena", die der Nationalmannschaft als Heimspielstätte dient. Levadias "Kadrioru staadion" wurde 1926 eröffnet und fasst 4.750 Zuschauer. Größtes Stadion ist das 1956 erbaute "Kalevi Keskstaadion" mit 12.000 Plätzen.

Die 2001 eröffnete Saku Suurhall ist eine auch für Sportveranstaltungen genutzte Halle mit 10.000 Plätzen.

Im Februar finden Tage der Barockmusik statt, im April das Jazzfestival "Jazzkaar," im Juni Altstadt- und Johannisfest und ein Tanzfestival im August. Das Filmfestival der Schwarzen Nächte wird im November und Dezember ausgetragen. Außerdem gibt es jedes Frühjahr eine Skateboard-Veranstaltung für professionelle Skateboarder, die Simpel Session, die in der Saku Suurhall stattfindet.

Die Altstadt von Tallinn bietet viele Restaurants wie auch Biergärten an.


Bibliographien

Wissenschaftliche und literarische Werke



</doc>
<doc id="13348" url="https://de.wikipedia.org/wiki?curid=13348" title="Urban II.">
Urban II.

Urban II., vormals Odo de Châtillon, Odo de Lagery oder Eudes de Châtillon, auch "Eudes de Lagery", "Otto von Lagery", "Otto von Châtillon", Bischof Otto von Ostia (* um 1035; † 29. Juli 1099) war Papst von 1088 bis 1099.

Er rief am 27. November 1095 zum Kreuzzug auf. Durch diesen ersten Kreuzzug sollte das morgenländische Christentum (unter anderem auch Jerusalem) von der Herrschaft der Muslime befreit werden. Urban wurde am 14. Juli 1881 von Leo XIII. seliggesprochen.

Eudes entstammte der Herrenfamilie von Lagery bei Châtillon-sur-Marne. Er besuchte die Kathedralschule in Reims, wo der Gründer des Kartäuserordens, Bruno von Köln, sein Lehrer war. Später wurde er selber Domherr und Erzdiakon der Kathedrale. Dann zog es Eudes erstmals nach Rom, wo er Kanoniker zu St. Johannes wurde. 1070 oder 1071 ist er von Abt Hugo in die Abtei Cluny aufgenommen worden, um nach kurzer Tätigkeit als Prior wieder (diesmal für den Orden) nach Rom entsandt zu werden. Dort wurde er 1078 von Gregor VII. zum Kardinalbischof von Ostia und Velletri ernannt. Von 1082 bis 1085 diente er dann der Kurie als päpstlicher Legat in Deutschland und Frankreich. Er war dabei einer der engsten Vertrauten Gregors. Nach Gregors Tod 1085 wurde Desiderius als Viktor III. gegen seinen Willen zum Papst gewählt, und übte dieses Amt bis zu seinem Tod 1087 aus. Nach seinem Tod wurde Eudes, der als kräftiger und kahlköpfiger Mann mit langem Bart beschrieben wird, am 12. März 1088 vom Konklave in Terracina zu Papst Urban II. gewählt.

Urban II. galt unter Gregor VII. als besonnener Kirchenpolitiker, der durch Konzilianz und diplomatisches Geschick einige Verstiegenheiten Gregors ausgleichen konnte. Als Nachfolger fiel ihm die Aufgabe zu, das mit Gregors Vertreibung und Tod in Bedrängnis geratene Reformpapsttum zu retten.

Schon 1080 war Wibert von Ravenna mit kaiserlicher Unterstützung zum Gegenpapst gewählt worden und hatte 1084 als Clemens III. den Papstthron bestiegen, während Gregor für abgesetzt erklärt wurde und ein Jahr später starb. Clemens III. regierte während der gesamten Dauer von Urbans Pontifikat parallel zu diesem und hielt sich die längste Zeit in Rom auf, bis er 1096 endgültig aus der Stadt vertrieben wurde. Clemens konnte sich außerhalb Frankreichs zunächst auch weitgehend durchsetzen; schon zu Urbans Amtsantritt hatte er in Deutschland und Oberitalien Fuß gefasst und etablierte sich zunehmend auch in England, Ungarn und Kroatien. Urban verbrachte sein Pontifikat zum größten Teil als Reisepapst, hauptsächlich in Frankreich und Italien.

1089 arrangierte Urban die Vermählung von Mathilde von Tuszien, der wichtigsten Stütze des Reformlagers in Italien, mit Welf V., dem Sohn des abgesetzten Herzogs von Bayern Welf IV., einem traditionellen Gegner des Kaisers. Ebenso bemühte er sich, die gespannten Beziehungen zum byzantinischen Reich zu verbessern, und hob 1089 den Bann gegen den byzantinischen Kaiser Alexios I. auf. Auch Urbans Initiative für einen Kriegszug in den Orient entstand im Zusammenhang mit seinen Ausgleichsbemühungen mit Byzanz.

Etwa ab Mitte der 1090er Jahre stabilisierte sich Urbans Position. Das überwältigende Echo auf seinen Aufruf zum Kreuzzug, der als „Kampfansage“ auch gegen seinen Rivalen Clemens III. gewertet werden kann, trug zu dessen Niedergang bei. Besonders in Norditalien verlor Clemens an Unterstützung. Im Jahre 1096 vertrieben ihn die Normannen aus Rom, das von nun an in der Hand der Reformpäpste blieb.

1090 befand sich Heinrich IV. auf einem zweiten Italienfeldzug (bis 1097), nachdem die Opposition im Reich zuvor fast vollständig zusammengebrochen war. Bis 1092 vermochte sich Heinrich zu halten, erfuhr dann aber durch die Truppen Mathildes eine empfindliche Niederlage bei Canossa, so dass Urban den Lombardischen Städtebund reaktivieren konnte: Mailand, Cremona, Lodi und Piacenza standen jetzt gegen den Kaiser und 1093 wechselte auch Heinrichs Sohn Konrad ins reformpäpstliche Lager über.

Während Konrad in Mailand zum König von Italien gekrönt wurde, zeigten in Deutschland die Predigten der Hirsauer Reformer erste Erfolge und zogen u. a. Welf IV. auf die Reformerseite. Heinrich IV. war nun gezwungen, sich nach Venetien zurückzuziehen, und Urban konnte sich schon 1093 in Rom einrichten. 1093 exkommunizierte der Papst Heinrich IV. und 1095 auch Philipp I. von Frankreich: Der französische König war mit Urban in Konflikt geraten, nachdem er seine Frau verstoßen hatte. Der Streit ließ sich jedoch bald beilegen. Philipp I. wurde schon 1096 wieder in die Kirche aufgenommen, ein Investiturstreit blieb hier noch aus.

In England wurde aber dann Anselm von Canterbury, der sich geweigert hatte, sich durch Wilhelm II. investieren zu lassen, des Landes verwiesen. Auch hier war die Kurie erst einmal daran interessiert, den Konflikt zu begrenzen – nun aber vielleicht schon, um die Kräfte gegen Heinrich zu bündeln. 1095 wurde auf der Synode von Piacenza Clemens III. in Bestätigung eines Simonie-Urteiles noch einmal gebannt. Die Erlasse gegen Simonie und auch gegen die Ehe von Geistlichen wurden für die gesamte Kirche verbindlich.

Kurz darauf erschienen Gesandte des byzantinischen Kaisers Alexios I., berichteten über die Bedrohung durch die Seldschuken und boten Unierungsverhandlungen an, um die Waffenhilfe der lateinischen Christen gegen die Muslime zu erlangen.

In Cremona traf Urban dann auf König Konrad, den er dazu verpflichten konnte, ihm einen Sicherheitseid zu leisten und das "officium stratoris" abzulegen, indem er des Papstes Pferd als dessen Marschall am Zügel führte. Dafür sagte Urban ihm die Hilfe gegen seinen Vater zu und arrangierte die Vermählung mit einer Tochter Rogers von Sizilien.

Die Bedrängnis der Byzantiner wurde in einem Brief des Alexios an Robert von Flandern bestätigt. Manche Historiker vermuten, dies habe Urban den entscheidenden Anstoß zu seinem Aufruf zum Ersten Kreuzzug gegeben, der am 27. November 1095 auf der Synode von Clermont an die französischen Ritter erging.

Zeitzeugen berichteten, dass die versammelte Menschenmenge zu groß war, um in der Kathedrale Platz zu finden, weswegen Urban seinen leidenschaftlichen Aufruf auf freiem Feld vor den Stadttoren an die Menge richtete. Urbans stark dramatisierende Rede von den Leiden der Christenheit im Osten, der Misshandlung durch die Andersgläubigen sowie der Notwendigkeit der Befreiung der heiligen Stadt Jerusalem – davon ist aber in einer der überlieferten Fassungen des Wortlauts der Rede, die allesamt voneinander abweichen, überhaupt nichts erwähnt – wurde den Chronisten zufolge begeistert aufgenommen. Angeblich wurde hier bereits das spätere Motto der Kreuzzüge – „Gott will es!“ – geprägt. Adhemar de Monteil, Bischof von Le Puy, der später zum Führer des Zugs ernannt wurde, kniete in einem zuvor abgesprochenen Auftritt unmittelbar nach dem Ende der Rede vor Urban nieder und bat als erster um die Erlaubnis, ziehen zu dürfen, und viele andere sollen sich ihm umgehend angeschlossen haben. Danach hielt er noch in Tours und Rouen Synoden ab, die den Aufruf verbreiteten. Ein Übriges taten die über das Land gesandten Wanderprediger der Kirche.

Der Aufruf zum Kreuzzug war in weiten Teilen Europas sehr erfolgreich und erhielt große Resonanz. Urbans Projekt einte erstmals die seit langem in Streitereien untereinander verstrickten französischen Adeligen und gab ihnen mit dem Ziel eines „gerechten“ Kampfes im Dienste der christlichen Sache eine ideelle Grundlage, die zugleich den Suprematieanspruch seines Amtes stärkte: Der vor dem Aufruf geforderte Gottesfrieden, der die Begrenzung der noch ausstehenden Fehden brachte, bestärkte gleichzeitig die Autorität der hier eingreifenden Kirche und stellte ein wesentliches Ereignis der machtpolitischen Rolle der Kirche und des Papsttums in der mittelalterlichen Geschichte Europas dar. Die den Teilnehmern vom Papst versprochene Sündenvergebung wirkte als äußerst attraktiver Anreiz für eine Teilnahme. Die vom Papst möglicherweise angestrebte Versöhnung mit der Ostkirche blieb dagegen wegen anhaltender machtpolitischer Differenzen und Interessengegensätze aus, ganz im Gegenteil führten die Kreuzzüge letztlich zur völligen Entfremdung der Kirchen. Auch schon kurzfristig verschärfte das Kreuzzugsunternehmen den lateinisch-griechischen Gegensatz, da auch die mit dem Papsttum seit Jahrzehnten verbündeten normannische Ritter, die ausgewiesene Feinde des byzantinischen Reiches waren und den Kreuzzug für ihren Kampf gegen Byzanz nutzten, darin eine maßgebliche Rolle spielten.

Urban II. selbst erlebte den Erfolg des 1096 aufgebrochenen Ritterheeres selbst nur noch zum Teil mit. Von der Einnahme Antiochias 1098 und auch vom Beginn der Belagerung Jerusalems hat er wohl erfahren, die Meldung von der Einnahme der Stadt erreichte ihn aber nicht mehr, da er am 29. Juli 1099 starb.




</doc>
<doc id="13349" url="https://de.wikipedia.org/wiki?curid=13349" title="Zeitreihenanalyse">
Zeitreihenanalyse

Eine Zeitreihe ist eine zeitabhängige Folge von Datenpunkten (meist aber keine Reihe im mathematischen Sinne). Typische Beispiele für Zeitreihen sind Börsenkurse, Wahlabsichtsbefragungen oder Wetterbeobachtungen.

Die Zeitreihenanalyse ist die Disziplin, die sich mit der inferenzstatistischen Analyse von Zeitreihen und der Vorhersage (Trends) ihrer künftigen Entwicklung beschäftigt. Sie ist eine Spezialform der Regressionsanalyse.

Der Begriff Zeitreihe setzt voraus, dass Daten nicht kontinuierlich, sondern "diskret" aber in endlichen zeitlichen Abständen anfallen. Aus einem zeitkontinuierlichen Messsignal (oder der kontinuierlichen Aufzeichnung eines Messsignals, zum Beispiel mit einem analogen t-y-Schreiber oder einem analogen Magnetbandgerät) kann eine Zeitreihe durch Abtastung gewonnen werden.

Die Zeitpunkte, denen Datenpunkte zugeordnet werden, können "äquidistant", also in konstanten Abständen (beispielsweise alle 5 Sekunden), in anderer Regelmäßigkeit (beispielsweise werktäglich) oder unregelmäßig angeordnet sein. Ein Datenpunkt kann aus einer einzelnen Zahl (skalare Werte, "univariate" Zeitreihe) oder aus einer Mehrzahl (Tupel) von Zahlenwerten (vektorielle Werte, "multivariate" Zeitreihe) bestehen. Jedoch müssen alle Datenpunkte in gleicher Weise aus Einzelwerten aufgebaut sein. Typische Zeitreihen entstehen aus dem Zusammenwirken regelhafter und zufälliger Ursachen. Die regelhaften Ursachen können periodisch (saisonal) variieren und/oder langfristige Trends enthalten. Zufällige Einflüsse werden oft als Rauschen bezeichnet.

Gegeben sei ein T-dimensionaler Vektor von Zufallsvariablen formula_1 mit einer zugehörigen multivariaten Verteilung. Dies kann auch als eine Folge von Zufallsvariablen formula_2 oder als stochastischer Prozess aufgefasst werden. Eine Stichprobe daraus ergibt als ein mögliches Ergebnis die T reellen Zahlen formula_3. Selbst bei unendlich langer Beobachtung wäre formula_4 nur eine einzige Realisation des stochastischen Prozesses. Solch ein Prozess hat jedoch nicht nur eine Realisation, sondern im Allgemeinen beliebig viele mit gleichen statistischen Eigenschaften. Eine "Zeitreihe" ist als eine Realisation des datengenerierenden Prozesses definiert.
Statt stochastische Prozesse der Dimension T anhand ihrer T-dimensionalen Verteilungsfunktion zu beschreiben, kann man ihn durch die Momente erster und zweiter Ordnung erfassen, also durch

Man spricht auch von Autokovarianzen, da es sich um Kovarianzen desselben Prozesses handelt. Im Spezialfall multivariater Normalverteilung des stochastischen Prozesses gilt, dass er durch die Momente erster und zweiter Ordnung eindeutig festgelegt ist. Für die statistische Inferenz mit Zeitreihen müssen Annahmen getroffen werden, da in der Praxis meist nur eine Realisation des die Zeitreihe generierenden Prozesses vorliegt. Die Annahme der "Ergodizität" bedeutet, dass Stichprobenmomente, die aus einer endlichen Zeitreihe gewonnen werden, für formula_11 quasi gegen die Momente der Grundgesamtheit konvergieren.

Zeitreihen fallen in vielen Bereichen an:

Eine besonders komplexe (aber auch reichhaltige) Datensituation liegt vor, wenn man zeitabhängige Mikrodaten besitzt, also Personen- oder Haushaltsdaten für verschiedene Zeitpunkte. Hier spricht man allerdings nicht mehr von Zeitreihendaten, sondern von Trend-, Panel- oder Ereignisdaten, je nach ihrer Zeitstruktur.

Ziele der Zeitreihenanalyse können sein

Die Vorgehensweise im Rahmen der Zeitreihenanalyse lässt sich in folgende Arbeitsphasen einteilen:

In den einzelnen Phasen ergeben sich Unterschiede, je nachdem ob man lineare Modelle zur Zeitreihenanalyse (Box-Jenkins-Methode, Komponentenmodell) oder nichtlineare Modelle zu Grunde legt. Im Folgenden wird beispielhaft auf die Box-Jenkins-Methode eingegangen.

An erster Stelle sollte die "graphische Darstellung" der empirischen Zeitreihenwerte stehen. Dieses ist die einfachste und intuitivste Methode. Im Rahmen der graphischen Analyse lassen sich erste Schlüsse über das Vorliegen von Trends, Saisonalitäten, Ausreißern, Varianzinstationarität sowie sonstiger Auffälligkeiten ziehen. Stellt man einen stochastischen Trend (Instationarität) fest (entweder durch die graphische Analyse oder durch einen statistischen Test wie den Augmented Dickey-Fuller-Test), der später durch eine Transformation der Zeitreihe (Differenzieren) bereinigt werden soll, so bietet sich eine "Varianzstabilisierung" (beispielsweise Box-Cox-Transformation) an. Die Varianzstabilisierung ist wichtig, da nach dem Differenzieren einer Zeitreihe negative Werte in der transformierten Zeitreihe vorkommen können.

Bevor weitergearbeitet werden kann, muss noch die grundsätzliche Frage geklärt werden, ob die Zeitreihe in einem deterministischen Modell (Trendmodell) oder einem stochastischen Modell abgebildet werden soll. Diese beiden Alternativen implizieren unterschiedliche Methoden der Trendbereinigung. Beim Trendmodell erfolgt die Bereinigung mittels einer Regressionsschätzung, beim stochastischen Modell mittels Differenzenbildung.

In der Schätzphase werden die Modellparameter und -koeffizienten mit Hilfe unterschiedlicher Techniken geschätzt. Für das Trendmodell bietet sich die OLS-Methode, für die Modelle im Rahmen des Box-Jenkins-Ansatzes die Momentenmethode, die nichtlineare Kleinstquadratmethode und die Maximum-Likelihood-Methode für die Schätzung an.

In der Diagnosephase werden das Modell oder ggf. mehrere ausgewählte Modelle hinsichtlich ihrer Güte beurteilt. Dabei bietet sich folgende Vorgehensweise an:

1. Schritt: Prüfen, ob die geschätzten Koeffizienten signifikant von Null verschieden sind. Bei einzelnen Koeffizienten erfolgt dies mit Hilfe eines "t"-Tests, mehrere Koeffizienten zusammen werden mit einem "F"-Test untersucht.

2. Schritt: Verfährt man nach der Box-Jenkins-Methode, so ist zu prüfen, inwieweit die empirischen Autokorrelationskoeffizienten mit denen übereinstimmen, die sich theoretisch aufgrund der vorher geschätzten Koeffizienten ergeben müssten. Zusätzlich können die partiellen Autokorrelationskoeffizienten sowie das Spektrum analysiert werden.

3. Schritt: Schließlich erfolgt eine sorgfältige Analyse der Residuen. Die Residuen sollten keine Struktur mehr aufweisen. Dabei kann man die Zentriertheit der Residuen mit einem "t"-Test kontrollieren. Die Konstanz der Varianz kann visuell am Zeitreihengraphen oder durch Berechnung des Effekts verschiedener λ-Werte in einer Box-Cox-Transformation berechnet werden. Um die Autokorrelationsfreiheit der Residuen zu prüfen kann man jeden einzelnen Koeffizienten auf signifikanten Unterschied zu Null prüfen oder die ersten formula_12 Koeffizienten gemeinsam auf Signifikanz zu Null testen. Um Letzteres zu klären kann auf die so genannten Portmanteau-Tests zurückgegriffen werden. Hierfür bieten sich beispielsweise Informationskriterien an.

In der Einsatzphase gilt es aus der in der Identifikationsphase aufgestellten und als brauchbar befundenen Modellgleichung eine Prognosegleichung zu formulieren. Dabei muss vorher ein Optimalitätskriterium festgelegt werden. Dafür kann der "minimal mean squared error" ("MMSE") genommen werden.

Die Verlaufsmuster von Zeitreihen können in verschiedene Komponenten zerlegt werden ("Komponentenzerlegung"). So gibt es "systematische" oder quasi-systematische Komponenten. Dazu gehören die Trendkomponente als allgemeine Grundrichtung der Zeitreihe, die Saisonkomponente als eine zyklische Bewegung innerhalb eines Jahres, die Zykluskomponente (bei ökonomischen Zeitreihen auch Konjunktur genannt) mit einer Periodenlänge von mehr als einem Jahr sowie eine Kalenderkomponente, die auf Kalenderunregelmäßigkeiten zurückzuführen ist. Als weitere Komponente tritt noch eine Rest- oder "irreguläre" Komponente auf. Hierunter fallen Ausreißer und Strukturbrüche, die durch historische Ereignisse erklärt werden können, sowie Zufallsschwankungen, deren Ursachen im Einzelnen nicht identifiziert werden können.

Die genannten Komponenten sind nicht direkt beobachtbar. Sie entspringen vielmehr der menschlichen Vorstellung. Somit stellt sich die Frage, "wie" man diese Komponenten modelliert.

Traditionelle Ansätze betrachten Zufallsschwankungen als "strukturneutral" und fassen die systematischen Komponenten als deterministische Funktionen der Zeit auf,

In neueren Ansätzen haben Zufallschwankungen eine "dominierende" Rolle bei der Modellierung der systematischen Komponente. Damit wird die Zeitreihe durch einen stochastischen Prozess modelliert, wie einen MA(1)-Prozess:

Dabei ist t der Zeitindex und formula_15 eine Zufallsvariable, für die die Eigenschaft weißes Rauschen angenommen werden kann. Einen dazu konträren Ansatz der Zeitreihenmodellierung stellt die Chaostheorie (siehe dazu Dimensionalität) dar.

In der Zeitreihenanalyse stehen einige allgemeine mathematische Instrumente zur Verfügung, wie Transformation (Box-Cox-Transformation), Aggregation, Regression, Filterung und gleitende Durchschnitte. Im Folgenden wird davon ausgegangen, dass die Zeitreihe als stochastischer Prozess modelliert werden kann. Dieser Ansatz wird auch als Box-Jenkins-Methode bezeichnet. Für stochastische Prozesse gibt es weitere spezielle Methoden und Instrumente. Hierzu zählen die:

In der Inferenzstatistik schätzt man die Größe der untersuchten Effekte auf der Basis von Stichproben. Neben den schon genannten Verfahren, bei denen man inferenzstatistisch dann die Fehler der gefundenen Ergebnisse abschätzt, können komplexe Zeitreihen-Modelle spezifiziert und geschätzt werden. Dies wird vor allem in der Ökonometrie für die Wirtschaftsmodelle genutzt. Grundlage ist der Begriff des "stochastischen Prozesses"; hier ist insbesondere die Gruppe der ARMA-Prozesse zu erwähnen.


Eine Zeitreihenanalyse kann unter anderem mit den freien Softwarepaketen GNU R, gretl, OpenNN und RapidMiner durchgeführt werden. Zu proprietären Lösungen gehören die Softwarepakete BOARD, Dataplore, EViews, Limdep, RATS, SPSS, Stata, SAS sowie WinRATS. 



</doc>
<doc id="13350" url="https://de.wikipedia.org/wiki?curid=13350" title="Abtastung (Signalverarbeitung)">
Abtastung (Signalverarbeitung)

Unter Abtastung () wird in der Signalverarbeitung die Registrierung von Messwerten zu diskreten, meist äquidistanten Zeitpunkten verstanden. Aus einem zeitkontinuierlichen Signal wird so ein zeitdiskretes Signal gewonnen.

Bei mehrkanaligen Signalen ergibt jede Abtastung ein „Sample“ aus mehreren Abtastwerten. Die Zahl der Samples pro Sekunde wird Abtastrate genannt. Bei der digitalen Telefonie (ISDN) beträgt die Abtastrate beispielsweise 8 kHz.

Die Digitalisierung eines analogen Signals im Zeitbereich umfasst als Überbegriff zusätzlich zur Abtastung eine weitere Umwandlung, die Quantisierung, wobei die beiden Umwandlungen in beliebiger Reihenfolge ausgeführt werden können um ein Digitalsignal zu erhalten:


Für eine einfachere mathematische Beschreibung ist die ideale Abtastung definiert. Hier wird das Signal nicht über einen gewissen Zeitraum um den Abtastzeitpunkt akkumuliert, sondern exakt zum Abtastzeitpunkt "nT" ausgewertet.

Mathematisch lässt sich dies darstellen, indem man das Signal "s"("t") mit dem Dirac-Kamm, einer Folge von Dirac-Stößen, multipliziert:

Das abgetastete Signal "s" lautet dann

Für das Frequenzspektrum "S", welches die Fourier-Reihe des Signals "s" darstellt, erhält man mit Hilfe der Umkehrung des Faltungstheorems:

Das Spektrum ist also das Spektrum des Eingangssignals "s"("t"), das periodisch mit der Periode 1/"T" wiederholt wird - dies drückt die Faltungseigenschaft des Dirac-Impulses aus. Daraus ergibt sich, dass das Spektrum von "s" maximal 1/(2"T") breit sein darf, damit sich die verschobenen Spektren nicht überlappen.

Ist das Spektrum von "s" schmaler als 1/(2"T"), so ist das ursprüngliche Signal "s"("t") nach idealer Tiefpass-Filterung aus dem zeitdiskreten Spektrum vollständig rekonstruierbar. Dieser Umstand ist die Grundlage des Nyquist-Shannon-Abtasttheorems. Ist hingegen das Spektrum des Eingangssignals "s"("t") breiter als 1/(2"T"), tritt Aliasing auf und das ursprüngliche Signal "s"("t") kann aus "s"("t") nicht wiedergewonnen werden.

In der Realität sind zwei Bedingungen der idealen Abtastung nicht einhaltbar:


Die reale Abtastung erfolgt daher unter folgenden Modifikationen:

"zu 1.:" Der Dirac-Kamm wird durch eine Rechteckfunktion (rect) mit Rechteckimpulsen der Länge "t" ersetzt. Die Abtastung wird durch eine Sample-and-Hold-Schaltung realisiert, welche den Wert einer Abtastung für die Länge des Rechteckimpulses konstant hält. Mathematisch entspricht dies einer Faltung mit der Rechteckfunktion:

Das daraus gewonnen Spektrum ist

Dies ist das Spektrum der idealen Abtastung, gewichtet mit einem Faktor welcher die si-Funktion (Sinc-Funktion) beinhaltet. Dies stellt eine Verzerrung des Signals dar, welche durch eine zusätzliche Verzerrung im Rekonstruktionsfilter bei der Rückgewinnung des ursprünglichen Signals behoben werden kann. Diese Verzerrung tritt bei der natürlichen Abtastung nicht auf.

"zu 2.:" Um auch mit einem nicht-idealen Rekonstruktionsfilter das kontinuierliche Signal aus dem Spektrum mit möglichst kleinen Fehler zurückzugewinnen, kann die Abtastfrequenz erhöht werden. Durch die Überabtastung rücken anschaulich die Einzelspektren weiter auseinander, wodurch der Tiefpassfilter zur Rekonstruktion im Bereich der Spiegelspektren höhere Dämpfungswerte aufweist.

Aufgrund der Symmetrieeigenschaften der Fourier-Transformation lässt sich umgekehrt auch eine Frequenzfunktion "S"("f") im Spektralbereich, bei idealer Abtastung, durch eine Folge "S"("f") mit frequenzdiskreten Werten bilden:

Die spektrale Folge "S"("f") besteht aus gewichteten Dirac-Impulsen, welche einzelne, diskrete Frequenzen beschreiben. Ein derartiges diskretes Spektrum wird auch Linienspektrum genannt.

Durch die inverse Fourier-Transformation kann daraus die zugehörige, periodische Form der Zeitfunktion "s"("t") gebildet werden:

Auch bei der Abtastung im Spektralbereich gilt das Abtasttheorem in „umgekehrter“ Form: Wenn die zeitliche Dauer eines Signals "s"("t") kleiner als 1/"F" ist, dann überlappen sich die periodischen Anteile von "s"("t") nicht gegenseitig. Die Aufgabe des Rekonstruktionsfilters im Zeitbereich übernimmt eine Torschaltung, im einfachsten Fall ein Schalter, welcher für die Zeitdauer 1/"F" durchschaltet und die restliche Zeit sperrt. Ist hingegen das Signal "s"("t") länger als 1/"F" kommt es zu zeitlichen Überlappungen und die ursprüngliche Signalform lässt sich nicht mehr rekonstruieren.

Bei der realen Abtastung im Spektralbereich tritt an Stelle einer Folge von Dirac-Impulsen eine Folge spektraler Rechteckimpulse auf, welche jede für sich einen bandbegrenzten Ausschnitt aus dem Spektrum abdecken. Diese Funktion können im technischen Bezug Bandpassfilter übernehmen.

Bei der Speicherung eines Musikstückes auf einer CD wird das abgetastete Signal zur Übermittlung und Speicherung des analogen Ausgangssignals verwendet. Die zur Abtastung verwendete Methode ist in diesem Fall von der zur analogen Rekonstruktion verwendeten Methode abhängig. Diese Sichtweise ist auch für die mathematische Behandlung vorteilhaft.

Die Zusammensetzung aus Abtastung und Wiedergabe in umgekehrter Richtung tritt z. B. in der Nachrichtenübertragung auf, wenn eine binär kodierte Nachricht in ein analoges Funksignal umgesetzt wird. Durch einen Abtastprozess wird dann die ursprüngliche binäre Zeichenfolge rekonstruiert.

Im einfachsten Fall wird die Umwandlung einer Folge reeller Zahlen, also eines zeitdiskreten Signals, mittels einer einzigen Kernfunktion vorgenommen. D. h., zu einer Folge formula_7 wird mittels einer Funktion "h" und einem Zeitschritt "T" die, im weitesten Sinne, interpolierende Funktion
gebildet. Deren Fouriertransformierte ist
wobei "H"("f") die Fouriertransformierte von "h"("t") ist.

Ein realistischeres Modell der Messung eines zeitveränderlichen Prozesses ist die Bildung eines gewichteten Mittelwertes über einen bestimmten Zeitraum. Das kann mathematisch durch die Faltung mit einer Gewichtsfunktion "w" realisiert werden. Sei "x(t)" das zu messende Signal und "v"("t") der gemessene Wert zum Zeitpunkt "t" (der z. B. dem Schwerpunkt der Gewichtsfunktion zugeordnet wird), so gilt
Unter der Fouriertransformation geht die Faltung in die Multiplikation über. Seien "W", "V" und "X" die Fouriertransformierten von "w", "v" und "x", dann gilt "V"("f")="W"("f")X("f").

Bestimmt man nun eine Folge von Messwerten mit Zeitschritt "T", formula_11, um diese in die Interpolationsvorschrift einzusetzen, so erhalten wir ein rekonstruktiertes analoges Signal
Um den Fehler des gesamten Prozesses aus Diskretisierung und Wiedergabe einzuschätzen, kann man diesen Prozess auf einfache frequenzbeschränkte Testsignale anwenden. Dies kann im Modell durch die Bestimmung der Fouriertransformierte abgekürzt werden. Dazu ist jedoch die Fourierreihe
genauer zu bestimmen. Nach der Poissonschen Summenformel ist diese periodische Funktion identisch mit der Periodisierung von "V(f)". Sei formula_14 die Abtastfrequenz, dann gilt
Zusammenfassend gilt also

Eine Frequenzkomponente um die Frequenz "f" erleidet somit eine Verzerrung mit dem Faktor formula_17 und ein Aliasing der Stärke formula_18 um die Frequenz formula_19 bei formula_20.

Um Basisbandsignale möglichst gut zu approximieren, ist es erforderlich, dass formula_21 in einer Umgebung von "f"=0 und formula_22 für dieselben "f" und für alle formula_20 gelten. Im Rahmen einer mathematisch exakten Theorie sind diese Forderungen erfüllt und alle Operationen wohldefiniert, wenn

Man erhält dann in der Betragsquadratnorm des Funktionenraums L² für eine Basisbandfunktion "x"("t") mit höchster Frequenz formula_25 nach der Parseval-Identität eine Abschätzung des relativen Fehlers, d. h. des Signal-Rausch-Verhältnisses, als
Beispiele: Sind "w" und "h" Rechteckfunktionen mit Breite "T" zentriert um "0", so ist
und es gilt formula_28.

Sind umgekehrt "w" und "h" die Kardinalsinusfunktionen formula_29, so sind deren Fouriertransformierte die entsprechenden Rechteckfunktionen formula_30, und die sich ergebende Rekonstruktionsformel ist die Kardinalreihe des Whittaker-Kotelnikow-Nyquist-Shannon-Abtasttheorems.
In jedem Fall führen Funktionen mit Frequenzkomponenten oberhalb formula_31 zu Aliasfehlern im Frequenzbereich formula_32, die Frequenzschranke des Abtasttheorems ist also notwendig, aber keinesfalls hinreichend für eine fehlerarme Rekonstruktion.

In dieser Richtung wird die „interpolierende“ Funktion "a"("t") abgetastet. Es ergibt sich also
Für die Fourierreihen erhält man daraus
Nach der Poissonschen Summenformel gilt in diesem Fall
Soll die Fourierreihe der Folge "c" und damit die Folge erhalten bleiben, so muss diese Summe überall den Wert "1" haben. Das Maximum der Abweichung davon gibt auch in diesem Fall eine Schranke für den relativen Fehler bei der Datenübertragung.
Aus mathematischer Sicht müssen die Funktionen "W" und "H" wieder die oben angegebene Schranke der Periodisierung des Betragsquadrats einhalten.

Traditionell wird die äquidistante (periodische) Abtastung am meisten verwendet, weil sie schon sehr umfangreich untersucht und in vielen Anwendungen umgesetzt wurde. In letzten Jahrzehnten wurden auch andere Arten der Abtastung untersucht, die auf gleiche Zeitintervalle zwischen Abtastwerten verzichten, was einige Vorteile wie effektive Auslastung des Kommunikationskanals verspricht. Zu diesen gehört unter anderem Send-on-Delta-Abtastung.




</doc>
<doc id="13352" url="https://de.wikipedia.org/wiki?curid=13352" title="Nyquist-Shannon-Abtasttheorem">
Nyquist-Shannon-Abtasttheorem

Das Nyquist-Shannon-Abtasttheorem, auch nyquist-shannonsches Abtasttheorem und in neuerer Literatur auch WKS-Abtasttheorem (für Whittaker, Kotelnikow und Shannon) genannt, ist ein grundlegendes Theorem der Nachrichtentechnik, Signalverarbeitung und Informationstheorie. Wladimir Kotelnikow formulierte das Abtasttheorem 1933. Die Veröffentlichung in einem sowjetischen Konferenzbericht wurde im Osten seit den 1950er Jahren referenziert, blieb aber allgemein im Westen bis in die 1980er weitgehend unbekannt. Unabhängig von Kotelnikow formulierte es Claude Elwood Shannon 1948 als Ausgangspunkt seiner Theorie der maximalen Kanalkapazität, d. h. der maximalen Bitrate in einem frequenzbeschränkten, rauschbelasteten Übertragungskanal.

Das Abtasttheorem besagt, dass ein auf formula_1 bandbegrenztes Signal aus einer Folge von äquidistanten Abtastwerten exakt rekonstruiert werden kann, wenn es mit einer Frequenz von größer als formula_2 abgetastet wurde.

Claude Shannon stützte sich auf Überlegungen von Harry Nyquist zur Übertragung endlicher Zahlenfolgen mittels trigonometrischer Polynome und auf die "Theorie der Kardinalfunktionen" von Edmund Taylor Whittaker (1915) und dessen Sohn John Macnaghten Whittaker (1928). Zu ähnlichen Resultaten wie Nyquist kam Karl Küpfmüller 1928.

Erst die Rechercheure der Eduard-Rhein-Stiftung haben die Priorität von Kotelnikow zweifelsfrei nachgewiesen. Dafür bekam er 1999 den Eduard-Rhein-Preis.

Unabhängig von Kotelnikow formulierte Herbert P. Raabe das Abtasttheorem 1939.

Das von Shannon formulierte Abtasttheorem besagt, dass eine Funktion, die keine Frequenzen höher als formula_3 enthält, durch eine beliebige Reihe von Funktionswerten im Abstand formula_4 eindeutig bestimmt ist. Eine hinreichende Bedingung dafür ist die Quadratintegrierbarkeit der Funktion.

Der Funktionsverlauf kann dann rekonstruiert werden, indem jeder Abtastwert formula_5 durch eine sinc-Funktion formula_6 mit gleicher Amplitude ersetzt und anschließend über alle "k" aufsummiert wird.

In der Signalverarbeitung entspricht dies der Abtastung mit einer Abtastrate formula_7. Die so erhaltene Signaldarstellung wird Pulsamplitudenmodulation genannt. Zur Rekonstruktion wird dieses Signal durch einen idealen Tiefpass mit Grenzfrequenz formula_1 gefiltert.

Bei Nicht-Basisband-Signalen, d. h. solchen mit minimaler Frequenz "f" größer als 0 Hz, gilt das Abtasttheorem in ähnlicher Form, da durch geeignete Wahl der Abtastfrequenz, das Bandpasssignal im Basisband nach der Abtastung erscheint. Die Abtastfrequenz muss dann lediglich größer als die doppelte Bandbreite sein (siehe auch "Unterabtastung"). Bei der Rekonstruktion wird hier statt eines idealen Tiefpasses ein idealer Bandpass verwendet.

Bei der Unterabtastung eines Bandpasssignals gilt:

In der Praxis wird ein Signal vor der Abtastung meist tiefpassgefiltert, damit die (Basis-)Bandbreite der Abtastrate genügt. Analog gilt das Abtasttheorem auch bei Bildern und Videos, wobei die Abtastfrequenz dann in Linien (bzw. Pixel) pro Längeneinheit bestimmt werden kann.

Wie im Artikel Abtastung (Signalverarbeitung) beschrieben ist, kann man das Abtasten eines Signals formula_10 durch die Multiplikation mit einem Dirac-Kamm formula_11 modellieren, wodurch man das abgetastete Signal formula_12 erhält. Nach der Umkehrung des Faltungstheorems ergibt sich damit die Fouriertransformierte des abgetasteten Signals durch:
wobei formula_14 periodisch mit der Periode formula_15 ist und formula_16 der Abstand zwischen 2 Abtastzeitpunkten ist. Unterschreitet man nun mit der Abtastfrequenz formula_17 die Frequenz formula_18 (für Basisbandsignale), so werden niedrigere und höhere Frequenzkomponenten im Frequenzraum überlagert und können anschließend nicht mehr getrennt werden.

Die Dauer eines technischen Abtastpulses ist allerdings nicht beliebig kurz. Deswegen liegt in der Praxis das Frequenzspektrum einer Rechteckpulsfolge vor statt das einer Diracstoßfolge. (Die Diracstoßfolge ist eine Funktion, die nur an einer einzigen Stelle (t = 0) unendlich groß ist und an allen anderen Stellen verschwindet.)

Ein in der Bandbreite beschränktes Signal "x" mit einer maximalen Frequenz "F":="f" ist eine Funktion, für welche die Fouriertransformierte formula_19 existiert und diese Fouriertransformierte außerhalb des Intervalls formula_20 Null ist. Dann kann umgekehrt das bandbeschränkte Signal durch die inverse Fouriertransformation der Frequenzdichte dargestellt werden:
„Gute“, zulässige Funktionen für die Frequenzdichte "X" sind beispielsweise stückweise stetige Funktionen, für die in jedem Punkt beide der einseitigen Grenzwerte existieren. Allgemeiner sind Funktionen aus dem Funktionenraum formula_22 zulässig.

Ist "x" "reellwertig", so gilt formula_23. Wird "X" in Polarkoordinaten dargestellt, formula_24, so erhalten wir "x" mittels eines Integrals mit reellem Integranden,
In der kartesischen Darstellung formula_26 ergibt sich analog

"Abtasten mit der doppelten Frequenz" bedeutet hier, dass Funktionswerte in gleichmäßigen Abständen genommen werden, wobei ein einfacher Abstand formula_28 beträgt, d. h., aus "x" wird die Zahlenfolge formula_29 konstruiert. Nach der Fourierdarstellung ergeben sich diese Werte aus der Frequenzdichte als
Diese sind aber gerade die Koeffizienten in der Fourierreihenentwicklung
Somit ist die Frequenzdichte und damit das Signal schon durch die Werte der Abtastfolge vollständig determiniert.

"Rekonstruieren ohne Informationsverlust" bedeutet, dass die Lagrange-Interpolation, ausgeweitet auf den Fall mit unendlich vielen, regelmäßig angeordneten Stützstellen, wieder das Ausgangssignal ergibt
Man beachte, dass man mit diesen Formeln in der Mathematik zwar ausgezeichnet arbeiten kann, sie sich aber in realen Abtastsystemen so nicht realisieren lassen. Zur Bestimmung eines jeden Signalwertes wäre eine Summation über einen unendlichen Bereich notwendig. Außerdem müssten unendlich viele Takte abgewartet werden, bevor die Summation abgeschlossen werden kann. Weil das nicht möglich ist, entstehen in der Praxis unvermeidliche Fehler.

Die Funktion formula_33, der "Sinus cardinalis" (sinc), ist dabei der ideale Interpolationskern für ganzzahlige Stützstellen; es ist sinc(0)=1 und sinc("n")=0 für jedes weitere ganzzahlige "n". Die interpolierende Reihe wird auch, nach Whittakers Notation, als Kardinalreihe bezeichnet, dabei bezieht sich die Vorsilbe "kardinal" auf die herausragende Rolle als „schwankungsärmste“ unter allen interpolierenden Funktionenreihen. Die sinc-Funktion hat, bis auf einen Faktor, die Rechteck-Funktion formula_34 als Fourier-Transformierte, diese hat den Wert 1 auf dem Intervall formula_35, sonst den Wert Null. Sie ist also bandbeschränkt mit höchster Frequenz "1/2".

Die Entwicklung als Kardinalreihe ergibt sich nun ganz natürlich, indem die Fourierreihe der Frequenzdichte in die inverse Fouriertransformation eingesetzt wird,

Ein reelles Signal in Bandpasslage muss, um Abtastung durch Funktionswerte zu erlauben, eine nur für Frequenzen aus dem Intervall formula_37 nicht verschwindende Fourier-Transformierte haben. Dann ist "F" die einseitige Bandbreite. Dieses kann auf Frequenzbänder beliebigen Zuschnitts verallgemeinert werden, allerdings ist dann das Abtasten nicht durch Funktionswerte, sondern durch Skalarprodukte zu definieren. Ein Beispiel dafür ist das Frequenzmultiplexverfahren, siehe auch OFDM.

"Bemerkung:" Kein endliches Signal, d. h., keine Funktion mit einem endlichen Träger erfüllt die Voraussetzungen an eine bandbeschränkte Funktion. Ebenso wenig fallen periodische Signale, wie zum Beispiel reine Sinusschwingungen, in den Bereich dieses Theorems; genauso wenig Signale mit Unstetigkeiten (Knicken oder Sprüngen im Verlauf). Es ist somit als ideale Aussage in einer idealen Situation zu betrachten. Dem Ideal am nächsten kommen modulierte Schwingungen, wie Musik- oder Sprachaufzeichnungen, die zur Weiterverarbeitung digitalisiert werden sollen. Für andere praktische Zwecke, z. B. digitale Bildbearbeitung, müssen Varianten des Abtasttheorems mit nicht ganz so starken Anforderungen gefunden werden, für die dieses Theorem dann Richtschnur ist.

Zu mathematischen Grundlagen siehe: Lebesgue-Integral, Lebesgue-Raum, Fourier-Transformation.

Durch Skalieren der Zeitabhängigkeit kann jedes bandbeschränkte Signal "x(t)" auf den Frequenzbereich "[-½; ½]", bzw. "[-π; π]" als Kreisfrequenzbereich, reduziert werden. Die Frequenzdichte "g(f)" muss eine Funktion beschränkter Variation sein, wie es zum Beispiel stückweise stetige Funktionen sind. Dann ist "x(t)" eine stetige, beliebig oft differenzierbare, absolut- und quadratintegrable Funktion,
formula_38, und hat eine Fourier-Transformierte formula_39 mit Träger formula_40.

Der Funktionswert "x(t)" an jedem beliebigen Punkt "t" ist unter diesen Voraussetzungen schon allein durch die Funktionswerte "x(n)" an allen ganzzahligen Punkten "t=n" festgelegt, es gilt:

Diese Gleichung enthält zwei nichttriviale Aussagen: 1) Die unendliche Reihe konvergiert, und 2) der Grenzwert ist immer identisch mit dem Funktionswert "x(t)".

Wird die Abtastfrequenz zu klein gewählt, treten im digitalisierten Signal Mehrdeutigkeiten auf. Diese nichtlinearen Verzerrungen sind auch unter dem Begriff Alias-Effekt bekannt. Bei Bildern treten eventuell phasenverschobene Schatten oder neue Strukturen auf, die im Original nicht enthalten sind.

Den unteren Grenzwert der Abtastfrequenz für ein analoges Signal der Bandbreite formula_42
nennt man auch Nyquist-Frequenz. Die höchste zu übertragende Frequenz muss demnach kleiner sein als die halbe Abtastfrequenz, sonst entstehen Aliasingfehler. Aus diesem Grund werden höhere Frequenzen aus dem analogen Signal mit einem Tiefpass herausgefiltert. Die Aliasingfehler sind Alias-Signale (Störsignale, Pseudosignale), die sich bei der Rekonstruktion als störende Frequenzanteile bemerkbar machen. Wird zum Beispiel ein Sinussignal, das eine Frequenz von 1600 Hz hat, mit einer Abtastfrequenz von 2000 Hz digitalisiert, erhält man ein 400-Hz-Alias-Signal (2000–1600 Hz). Bei einer Abtastfrequenz über 3200 Hz entsteht dagegen kein Alias-Signal. Eine Abtastfrequenz von bspw. 3300 Hz führt zu einem Differenzsignal von 1700 Hz (3300–1600 Hz). Dieses ist jedoch größer als die halbe Abtastrate und wird demnach bei der Rekonstruktion durch einen Tiefpass entfernt.

In der Praxis gibt es keinen idealen Tiefpass. Er hat immer einen gewissen Übergangsbereich zwischen praktisch keiner Dämpfung im Durchlassbereich und praktisch vollständiger Dämpfung im Sperrbereich. Daher verwendet man in der Praxis eine modifizierte Formel zur Bestimmung der Abtastfrequenz:

Beispiel:

Auf einer CD wird ein Signal gespeichert, das durch die Digitalisierung eines analogen Audiosignals mit Frequenzen bis 20 kHz erzeugt wird. Die Frequenz, mit der das analoge Audiosignal abgetastet wird, beträgt 44,1 kHz.

Der verwendete Faktor ist abhängig vom verwendeten Tiefpassfilter und von der benötigten Dämpfung der Alias-Signale. Andere gebräuchliche Faktoren sind 2,4 (DAT, DVD) und 2,56 (FFT-Analysatoren)

Wenn man eine höhere Abtastfrequenz wählt, erhält man keine zusätzlichen Informationen. Der Aufwand für Verarbeitung, Speicherung und Übertragung steigt jedoch. Trotzdem wird Überabtastung (englisch ) in der Praxis angewendet. Liegt nämlich die Nutzbandbreite B sehr nahe bei der halben Abtastfrequenz, so werden hohe Anforderungen an die Flankensteilheit des Tiefpassfilters gestellt. Mit höherer Abtastfrequenz erreicht man eine ausreichend hohe Dämpfung im Sperrbereich eines Tiefpaßsystems einfacher als mit einem hochwertigen Filter. Die Bandbegrenzung kann dann auf ein Digitalfilter hoher Ordnung verlagert werden. In der Praxis wird häufig ein Überabtastungsfaktor "M" = 2 oder "M" = 4 gewählt. Somit braucht man weniger steile analoge Filter vor dem Abtasten. Nach der ersten Abtastung wird dann ein digitales Filter vor der folgenden Abtastratenreduktion eingesetzt, womit die Abtastfrequenz nachträglich gesenkt wird. Dieses digitale Filter wird auch als "Dezimationsfilter" bezeichnet. Es kann beispielsweise in Form eines Cascaded-Integrator-Comb-Filters realisiert werden.

Mathematisch ausgedrückt hat ein ideales Tiefpassfilter als Übertragungsfunktion eine Rechteckfunktion.
Diese Übertragungsfunktion schneidet das Spektrum im Frequenzraum perfekt ab und das gefilterte Signal kann perfekt aus den Abtastpunkten rekonstruiert werden. Allerdings lässt sich ein ideales Tiefpassfilter nicht praktisch realisieren, da es nicht kausal und unendlich lang ist.

Deswegen verwendet man analoge Tiefpassfilter, welche eine stetige, trapezähnliche Übertragungsfunktion aufweisen und deren Flanken mit kontinuierlicher, endlicher Steigung zu- bzw. abnehmen. Diese Filter können beispielsweise in Form von Butterworth-Filtern realisiert werden. Nach dem Abtasten erfolgt die digitale Glättung und das Heruntertakten auf die Nutzbandbreite. Die Flankensteilheit hat dabei einen Einfluss auf die Güte des rekonstruierten Signals.

Die Bedingung "f" ≥ 2 · "f" aus dem Abtasttheorem ist eine vereinfachte Darstellung, die allerdings sehr gebräuchlich und nützlich ist. Genau genommen muss anstelle von "f" die Bandbreite stehen, die durch den Bereich zwischen niedrigster und höchster im Signal vorkommenden Frequenz definiert ist. Nur in Basisbandsignalen ist die Bandbreite mit "f" identisch, Basisbandsignale sind Signale mit niederfrequenten Anteilen in der Nähe von 0 Hz.

Diese Erkenntnis führte zu einem Konzept namens Bandpassunterabtastung (oder ), das zum Beispiel in digitaler Radiotechnik Verwendung findet. Angenommen, man möchte alle Radiosender empfangen, die zwischen 88 und 108 MHz senden. Interpretiert man das Abtasttheorem wie bisher beschrieben, so müsste die Abtastfrequenz über 216 MHz liegen. Tatsächlich wird aber durch die Technik der Unterabtastung nur eine Abtastfrequenz von etwas mehr als 40 MHz benötigt. Voraussetzung dafür ist, dass vor der Abtastung aus dem Signal mittels Bandpassfilter alle Frequenzen außerhalb des Frequenzbereichs von 88 bis 108 MHz entfernt werden. Die Abtastung erfolgt beispielsweise mit 44 MHz, ohne dass der relevante Bereich von einem analogen Mischer umgesetzt würde – das Ergebnis ist quasi ein Alias-Signal und entspricht dem Signal, das bei Abtastung eines per Mischer auf 0–22 MHz umgesetzten Bereichs entstünde.

Um in der Praxis die notwendige punktförmige Abtastung wenigstens näherungsweise realisieren zu können, muss die Abtast-Halte-Schaltung jedoch derart ausgelegt werden, dass das Ausleseintervall so eng wird, wie es für eine Abtastfrequenz von 220 MHz oder mehr vonnöten wäre. Zu vergleichen ist das mit einer Abtastung mit 220 MHz, von der nur jeder fünfte Wert weiterbenutzt wird, während die je vier dazwischenliegenden Abtastwerte verworfen werden.





</doc>
<doc id="13353" url="https://de.wikipedia.org/wiki?curid=13353" title="Sampling">
Sampling

Siehe auch:


</doc>
<doc id="13356" url="https://de.wikipedia.org/wiki?curid=13356" title="Claude Shannon">
Claude Shannon

Claude Elwood Shannon (* 30. April 1916 in Petoskey, Michigan; † 24. Februar 2001 in Medford, Massachusetts) war ein US-amerikanischer Mathematiker und Elektrotechniker. Er gilt als Begründer der Informationstheorie.

Claude Shannon wurde in einem Krankenhaus in Petoskey, Michigan, geboren und wuchs im nahen Gaylord auf, dem Wohnsitz der Eltern. In manchen Biografien wird deshalb fälschlicherweise Gaylord als Geburtsort angegeben. Sein Vater war Richter, seine Mutter Sprachlehrerin deutscher Herkunft. Während seiner High-School-Zeit arbeitete er als Bote für die Western Union.

Er folgte 1932 seiner Schwester Catherine an die University of Michigan. Sie schloss in jenem Jahr das Mathematikstudium ab, und er begann ein Elektrotechnik- und Mathematikstudium. 1936 wechselte er mit einem Abschluss in Mathematik und Elektrotechnik an das MIT. In seiner Abschlussarbeit zum Master in Elektrotechnik (1937), "A Symbolic Analysis of Relay and Switching Circuits", benutzte er Boolesche Algebra zur Konstruktion von digitalen Schaltkreisen. Die Arbeit entstand aus der Analyse der Relais-Schaltkreise im Analogrechner "Differential Analyzer" von Vannevar Bush (Dekan der Ingenieurwissenschaften am MIT), den Shannon für Anwender programmierte. 1940 erwarb er seinen Doktortitel in Mathematik mit einer Arbeit über theoretische Genetik "(An Algebra for Theoretical Genetics)" am MIT.

Nach kurzem Aufenthalt als Forscher am Institute for Advanced Study in Princeton, New Jersey, kam er 1941 als Mathematiker zu den ebenfalls in New Jersey gelegenen AT&T Bell Labs. Dort lernte er Mary Elisabeth Moore, technische Assistentin am "Microwave Research Department", kennen. Sie heirateten 1949, zwei Söhne und eine Tochter gingen aus der Ehe hervor.

Nachdem er schon 1956 eine Gastprofessur am MIT angetreten hatte, wechselte er 1958 ganz dorthin, als "Donner Professor of Science". 1978 wurde er vom MIT emeritiert. Seine professionellen Beziehungen zu den Bell Labs als Berater hielt er währenddessen bis 1972. In seinen letzten Lebensjahren litt er an der Alzheimer-Krankheit, an deren Folgen er auch starb.

1948 veröffentlichte er seine bahnbrechende Arbeit "A Mathematical Theory of Communication" (dt. "Mathematische Grundlagen in der Informationstheorie"). In diesem Aufsatz konzentrierte er sich auf das Problem, unter welchen Bedingungen eine von einem Sender kodierte und durch einen gestörten Kommunikationskanal übermittelte Information am Zielort wiederhergestellt, also ohne Informationsverlust dekodiert werden kann. Dabei konnte er den aus der Physik bekannten Begriff der Entropie erfolgreich in der Informationstheorie anwenden.

Gleichzeitig erschien von ihm der Artikel "Communication in the presence of noise" („Nachrichtenübermittlung bei Vorhandensein von Hintergrundrauschen“), in dem er die Darstellung frequenzbeschränkter Funktionen durch die Kardinalreihe nach John Macnaghten Whittaker (1929 und 1935) mit Überlegungen zur maximalen Datenrate, insbesondere von Harry Nyquist, zu einer Theorie der Kanalkapazität in der digitalen Signalübertragung verknüpfte. Vor ihm, jedoch ohne seine Kenntnis, publizierte Wladimir Alexandrowitsch Kotelnikow 1933 ein gleichlautendes Resultat. Demnach muss die Abtastfrequenz (Sampling rate) für ein Signal mindestens doppelt so groß sein wie die höchste Frequenz, die in ihm enthalten ist, um ohne Informationsverlust wieder in ein analoges Signal rekonstruiert zu werden (Nyquist-Shannon-Abtasttheorem).

Ein weiterer bemerkenswerter Artikel erschien 1949, "Communication Theory of Secrecy Systems", in dem Shannon die formalen Grundlagen der Kryptographie klärte und sie damit in den Rang einer eigenständigen Wissenschaft erhob.
Shannon war vielseitig interessiert und kreativ; er soll bei Bell jonglierend auf einem Einrad in den Gängen herumgefahren sein. Randprodukte seiner beruflichen Tätigkeit sind unter anderem eine Jongliermaschine, raketengetriebene Frisbees, motorisierte Pogostöcke, eine Maschine zum Gedankenlesen, eine mechanische Maus ("Theseus", 1950), die sich mittels eines einfachen Gedächtnisses bestehend aus Relais-Schaltkreisen in Labyrinthen orientieren konnte, und schon in den 1960ern ein früher Schachcomputer. Eine Arbeit von 1950 befasst sich bereits mit Schachprogrammen. Die Arbeit war einflussreich und führte zu dem ersten Schachspiel auf Computern auf dem MANIAC-Rechner in Los Alamos 1956. Er baute auch die „ultimate machine“, ein Kästchen mit einem Schalter, den eine mechanische Hand wieder auf „aus“ stellte, nachdem man ihn eingeschaltet hatte. Nach ihm wurde die Einheit des Informationsgehaltes einer Nachricht, das Shannon, benannt.

Mitte der 1960er Jahre begann er sich für Finanztransaktionen zu interessieren und hielt darüber mehrfach am MIT gut besuchte Vorlesungen (einer seiner Hörer war Paul Samuelson). Er schlug ein heute "Constant Proportion Rebalanced Portfolio" genanntes Verfahren vor, um aus Zufallsfluktuationen des Marktes Gewinn zu erzielen (nach jeder Transaktion Aufteilung des Kapitals in genau zwei Hälften, eine für Spekulation, die andere Barreserve).

Nach der Teilung von AT&T im Jahre 1996 wurde der Großteil der Bell Labs der neuen Firma Lucent Technologies zugeschlagen. Das Forschungslabor der AT&T in Florham Park, New Jersey, wurde ihm zu Ehren "AT&T Shannon Laboratory" getauft.

Zu seinen Forschungsergebnissen im Bereich von Booleschen Algebren gehören der Inversionssatz sowie der Entwicklungssatz von Shannon.

1939 erhielt er den Alfred Noble Prize. 1956 wurde er in die National Academy of Sciences, 1957 in die American Academy of Arts and Sciences gewählt. 1966 erhielt er die National Medal of Science. Im Jahr 1970 wurde er zum Mitglied der Leopoldina gewählt. 1985 erhielt er den damals erstmals verliehenen Kyoto-Preis. 1991 wurde er zum auswärtigen Mitglied "(Foreign Member)" der Royal Society gewählt.

Am 30. April 2016 – zum 100. Geburtstag – widmet ihm Google ein Doodle, das ihn beim Jonglieren mit drei Binärzahlen „0“, „0“ und „1“ zeigt.

In seinem Heimatort Gaylord ist ein Park nach ihm benannt, in dem seine Skulptur steht.




</doc>
<doc id="13357" url="https://de.wikipedia.org/wiki?curid=13357" title="Steuerungstechnik">
Steuerungstechnik

Die Steuerungstechnik umfasst den Entwurf und die Realisierung von Steuerungen, das heißt, die gerichtete Beeinflussung des Verhaltens technischer Systeme (Geräte, Apparate, Maschinen, Anlagen und biologische Systeme). Sie ist, wie die Regelungstechnik, ein Teilgebiet der Automatisierungstechnik. Steuerungen werden unterteilt in "binäre", "analoge" und "digitale" Steuerungen.

In "Binär-Steuerungen" sind die Ein- und die Ausgangsgrößen der Steuereinrichtungen binär. Die Beeinflussung des zu steuernden Systems (Steuerstrecke) erfolgt über die binären Ausgangsgrößen der Steuerung mittels der Aktoren. Beispiele für Aktoren sind eine Leuchte, ein Ventil oder ein Motor. Die binären Eingangsgrößen der Steuerung sind Bediensignale vom Menschen und Rückmeldesignale von Sensoren aus der Steuerstrecke, zum Beispiel die Schalterstellung (Ein/Aus), die Ventilstellung (Offen/Geschlossen) oder der Bewegungszustand des Motors (Drehend/Stehend). Gesteuert werden zum Beispiel eine Beleuchtung, ein Wasserfluss oder die Bewegung eines Fahrzeug-Antriebs.

Man unterscheidet bei Binär-Steuerungen zwischen "Verknüpfungs- und Ablaufsteuerungen". Bei Ablaufsteuerungen werden relevante Werte der Steuergrößen an den Eingang der Steuerung mittels Sensoren rückgemeldet. Wenn dagegen Rückmeldungen fehlen, spricht man von "Verknüpfungssteuerungen", deren Arbeitsweise binär oder mehrwertig ist.

Kennzeichen der "Informationsverarbeitung" in komplexen binären Steuerungen sind im Steuerprogramm enthaltene "logische Verknüpfungen" zwischen den Eingangssignalen (einschließlich der rückgemeldeten Signale von den Sensoren). Die Beschreibung und Berechnung binärer Steuerungen kann daher durch Mittel der binären Mathematik erfolgen.

In "analogen Steuerungen" sind die Ein- und Ausgangsgrößen der Steuereinrichtung Analogsignale; diese Steuerungen besitzen keine Rückkopplungen. Beispiel einer analogen Steuerung ist die stetige Veränderung einer Hebelstellung beim Drehen einer Kurvenscheibe, an der der Hebel anliegt. Analoge Steuerungen können als Regelungen durch Differentialgleichungen beschrieben werden.

Eine Steuerung wird "digitale Steuerung" genannt, wenn in ihr Digitalsignale verarbeitet werden. Digitale Signale sind Mehrbitsignale, deren Einzelbits Bestandteile einer codierten Informationsdarstellung sind. Zur Verarbeitung digitaler Signale sind Steuerungsbefehle mit Byte- bzw. Wortoperanden, sogenannte Wortanweisungen, erforderlich.

Heute sind die meisten Steuerungen "binär" oder "digital", wobei die Ablaufsteuerungen bei weitem überwiegen (mehrere Steuergrößen werden nacheinander beeinflusst). Sie haben mehrere oder sogar viele Ein- und Ausgänge. Außer dem Startsignal sowie weiteren Bediensignalen stammen die Eingangssignale nicht vom Bediener, sondern aus der Steuerstrecke, und sind mit Sensoren erfasste und rückgemeldete ("feedback") Zustände der Steuergrößen. Der jeweilige Folgeschritt im Ablauf wird immer erst dann ausgeführt, wenn der vorhergehende Schritt abgeschlossen ist. Somit liegen aufeinander folgende "geschlossene Teil-Steuerkreise" vor, die aber nicht mit dem nicht unterteilten geschlossenen Regelkreis zu verwechseln sind. Dessen Zweck ist eine bei Störungen stattfindende technische "Selbstkorrektur" der Regelgröße. Steuerungen, die zusätzlich gegen Störungen ausgelegt sind, bewirken bei gefährlichen Werten der Steuergrößen, dass der zu steuernde Prozess in einen sicheren Zustand überführt oder abgeschaltet wird.

Zur Bewältigung der Komplexität moderner Steuerungen gibt es für deren Entwurf spezielle methodische Hilfen in Form verschiedener "theoretischer Modelle" sowie entsprechender "computergestützter Werkzeuge". Solche Werkzeuge (Tools) werden auch für Simulation, Planung, Projektierung, Programmierung und Service (Fehlerdiagnose, Wartung und Instandsetzung) verwendet.

Historische Beispiele

Zeittabelle für Bauelemente und Geräte der Steuerungstechnik

Die bedeutende Rolle der Steuerungstechnik in der industriellen Entwicklung wird auch im Zukunftsprojekt Industrie 4.0 der Deutschen Bundesregierung und Industrie deutlich, wobei vier Stufen der Industriellen Revolution unterschieden werden:



Die Theorie der Steuerungstechnik blieb bis heute hinter der der Regelungstechnik zurück, was sich unter anderem auch in der mangelhaften Normung beziehungsweise Definition einschlägiger Begriffe niedergeschlagen hat.

Die DIN-Normung hat über viele Jahre zwischen "Regelung als geschlossener Kreis" und "Steuerung als offene Kette" unterschieden. Da es in der digitalen Steuerungstechnik auch geschlossene Teil-Kreise gibt, wurde die Steuerung 1994 in der dritten, heute gültigen Ausgabe der DIN 19226 ("Regelungs- und Steuerungstechnik", seit 2002 unverändert ersetzt durch DIN-IEC 60050-351) neu definiert: „Kennzeichen für das Steuern ist der offene Wirkungsweg oder ein geschlossener Wirkungsweg, bei dem die durch Eingangsgrößen beeinflussten Ausgangsgrößen nicht fortlaufend und nicht wieder über dieselben Eingangsgrößen auf sich selbst wirken“. Wesentlich ist der Nebensatz "… die durch Eingangsgrößen beeinflussten Ausgangsgrößen nicht fortlaufend und nicht wieder über dieselben Eingangsgrößen auf sich selbst wirken".
Die Gegenüberstellung "offener oder geschlossener Wirkungsweg" (Steuerung) zu "geschlossener Kreis" (Regelung) hat mehr zu Verunsicherung als zu Klärung beigetragen.

In der Steuerungsnorm DIN 19237 wurden bereits die verschiedenen Arten zur Programmverwirklichung durch verbindungsprogrammierte und speicherprogrammierbare Steuerungen klassifiziert.

Die Norm DIN 19239: "Messen, Steuern, Regeln – Steuerungstechnik – Speicherprogrammierte Steuerungen – Programmierung" wurde von der Deutschen Kommission Elektrotechnik Elektronik Informationstechnik erstellt. Der erste Entwurf wurde 1981 veröffentlicht und 1983 durch eine freigegebene Version ersetzt. DIN 19239 definierte drei Programmiersprachen für Steuerungen:


Die DIN 19239 wurde im Jahre 1994 zurückgezogen und durch eine DIN auf Basis der gleichlautenden Europanorm DIN EN 61131-3 abgelöst. Hierin sind zusätzlich zu den genannten drei noch zwei weitere Programmiersprachen enthalten, also insgesamt fünf:


Die hieraus hervorgegangene internationale Norm IEC 61131-3 (auch IEC 1131 bzw. 61131) ist die einzige weltweit gültige Norm der Programmiersprachen für Speicherprogrammierbare Steuerungen.

Eine binäre Steuerung bzw. das binäre Steuern ist gemäß DIN 19226, 3. Ausgabe 1994, Teil 1 bzw. DIN IEC 60050, Teil 351 ein Vorgang in einem System, bestehend aus einer Steuereinrichtung und einer Steuerstrecke, bei dem eine oder mehrere Prozessgrößen in der Steuerstrecke, die als Steuergrößen bezeichnet werden, durch binäre Ausgangsgrößen der Steuereinrichtung (auch Stellsignale genannt) entsprechend einem vorgegebenen Steueralgorithmus (Steuerprogramm) beeinflusst werden.

Die Eingangs- und Ausgangsgrößen sind binäre Signale. Es handelt sich dabei um wertdiskrete Größen, deren Informationsparameter nur zwei Werte annehmen können, die mit 0 und 1 bezeichnet werden.

Bei den binären Eingangssignalen muss zwischen Bediensignalen, die über Bedieneinrichtungen wie Schalter oder Taster eingegeben werden, und binären Messsignalen unterscheiden werden, die mittels Sensoren (wie Endlagen-Schalter oder Licht-Schranken) erfasst werden.

Aus den binären Eingangssignalen der Steuereinrichtung werden entsprechend dem Steueralgorithmus durch logische Verknüpfung als Ausgangssignale binäre Stellsignale gebildet, die über Aktoren (auch als Aktuatoren bezeichnet, z. B. Relais-Schalter, Schalt-Schütz, Magnet-Ventil oder Motor) auf das Steuerungsobjekt (technologischer Prozess, Steuerstrecke) einwirken und hierdurch Steuergrößen (Ausgänge des technologischen Prozesses) verändern.

Die Steuergrößen können entweder "wertdiskrete Größen" (z. B. Signale zum Ein- und Ausschalten einer Beleuchtung mittels einer Wechsel- oder Kreuzschaltung) oder "analoge", d. h. wert- und zeitkontinuierliche Größen sein (z. B. Temperatur, Druck, Füllstand, Weg, Winkel, Drehzahl).

Der Signalfluss in Steuerungssystemen kann zwei unterschiedliche Grundstrukturen aufweisen:




Den überwiegenden Anteil aller Steuerungsarten in den praktischen Anwendungen stellen die Ablaufsteuerungen dar. Man unterscheidet hierbei "prozessgeführte" und "zeitgeführte" Ablaufsteuerungen.

Neben „Verknüpfungs- und Ablaufsteuerungen“ existieren noch Steuerungen, in denen keine Sensorsignale (also keine Rückmeldungen) einbezogen werden und die nur einen Zeitplan (Zeitprogramm) oder Wegplan (Wegprogramm) über ihre Ausgänge und die nachgeschalteten Aktoren abarbeiten:



Diese Zeitplan- und Wegplansteuerungen nehmen als offene Steuerungen (Programmsteuerungen) einen relativ geringen Anteil aller Steuerungsarten ein.

Seit etwa 1995 wird in der Fachliteratur versucht, Steuerung genauer zu beschreiben, um einerseits ihr Verhältnis zur Regelung deutlicher darzustellen und um andererseits die verschiedenen Arten von Steuerung deutlicher voneinander abzugrenzen: Zander, Töpfer (1996), Lunze (2003), Langmann (2004), Litz (2005), Heimbold (2015).

Von Wellenreuther/Zastrow (1995) und Bergmann (1999) wird die Definition von Verknüpfungssteuerungen gegenüber der DIN 19226 etwas präziser gefasst, indem zwischen Verknüpfungssteuerungen ohne und mit Speicherverhalten unterschieden wird.

Von Zander stammt eine neuartige Betrachtungsweise der Wirkungsabläufe von Ablaufsteuerungen, die auf der Basis einer umfassenden Struktur- und Verhaltensanalyse von Steuerstrecken entwickelt wurde. Für die Vorgänge in Ablaufsteuerungen wird der Begriff „Ereignisdiskreter Prozess“ als Präzisierung des früher verwendeten Begriffs „Diskontinuierlicher Prozess“ eingeführt. Es wird davon ausgegangen, dass die Steuergrößen bei Ablaufsteuerungen überwiegend analoge Größen sind, z. B. Drücke, Temperaturen, Füllstände, Wege, Winkel, Drehzahlen. Ein wesentliches Merkmal dieser Betrachtungsweise ist, dass während des Ablaufs eines ereignisdiskreten Prozesses die von der Steuereinrichtung ausgegebenen binären Stellsignale im Sinne von Sprungfunktionen auf die analogen Steuergrößen wirken und dass deren Funktionswerte sich dadurch im Sinne von Sprungantworten entsprechend dem jeweiligen Zeitverhalten ändern. So weist z. B. die Änderung des Füllstandes beim Füllen eines Behälters ein I-Verhalten auf. Für die Steuergrößen sind entsprechende Schwellwerte festzulegen. Erreicht eine Steuergröße einen für sie vorgesehenen Schwellwert, dann wird das binäre Stellsignal, das die Veränderung der Steuergröße verursacht hat, von der Steuereinrichtung auf den Wert Null gesetzt. Gemäß dem in der Steuereinrichtung implementierten Steueralgorithmus wird dann das nächste Stellsignal ausgegeben und der ereignisdiskrete Prozess somit fortgesetzt. Das Erreichen eines Schwellwertes einer Steuergröße wird als „Ereignis“ bezeichnet. Daraus erklärt sich der Name „Ereignisdiskreter Prozess“. Ein Ereignis liegt auch vor, wenn eine Bedienhandlung ausgeführt wird oder eine vorgegebene Zeitdauer in einem Zeitglied abgelaufen ist. Beim Auftreten eines Ereignisses wird in einem ereignisdiskreten Prozess definitionsgemäß ein Operationswechsel eingeleitet. Die Ereignisse werden zu diesem Zweck durch so genannte Ereignissignale an die Steuereinrichtung gemeldet. Ereignissignale sind also binäre Messsignale, binäre Bediensignale und binäre Ausgangssignale von Zeitgliedern.

Auf dieser Basis werden Ablaufsteuerungen, d. h. Steuerungen ereignisdiskreter Prozesse, wie folgt definiert (Zander):

Kennzeichen von Ablaufsteuerungen sind aufeinander folgende geschlossene Teilkreise (feedback) und überwiegend analoge Steuergrößen.

Beispiele für Ablaufsteuerungen:

Im Unterschied zu Ablaufsteuerungen werden in Verknüpfungssteuerungen nicht vorwiegend analoge Steuergrößen, sondern ausschließlich wertdiskrete (z. B. binäre) Steuergrößen als Ausgänge der Steuerstrecke in ihren Werten verändert. Dazu werden in der Steuereinrichtung durch logische Verknüpfung der binären Eingangssignale binäre Stellsignale erzeugt, die das Schalten der Steuergrößen bewirken. Eine Rückmeldung über eine ausgeführte Schalthandlung von den Ausgängen der Steuerstrecke zu den Eingängen der Steuereinrichtung existiert bei Verknüpfungssteuerungen nicht.

Auf dieser Basis werden "Verknüpfungssteuerungen" wie folgt definiert (Zander):

Kennzeichen der Verknüpfungssteuerung sind ein "offener Wirkungsablauf" und "binäre oder mehrwertige Steuergrößen".

Die Einbeziehung innerer Zustände kann durch Verwendung von Speicherelementen erfolgen. Verknüpfungssteuerungen können demzufolge kombinatorische Systeme (ohne Speicher) oder sequentielle Systeme (mit Speichern) sein.

Beispiele für Verknüpfungssteuerungen:

Binärsteuerungen und Regelungen unterscheiden sich vor allem in folgender Hinsicht:




In der englischsprachigen Fachliteratur wird undifferenziert sowohl für Regelung als auch für Steuerung das Wort „control“ verwendet. Dieser Begriff wird oft einfach mit „Steuerung“ übersetzt. Um richtig übersetzen zu können, ist daher die Kenntnis des Kontextes erforderlich.

Die englische Entsprechung für „Speicherprogrammierbare Steuerung (SPS)“ ist „Programmable Logic Controller (PLC)“, was sehr viel präziser als das deutsche SPS ist, weil die "logische Verknüpfung" in der Steuerung als wesentliches Merkmal herausgestellt wird, die Speicherung des Programms dagegen unbetont bleibt.

Mit diesen jüngsten Darstellungen wird ein jahrzehntelanges Versäumnis, die Steuerungstechnik in ihrem Wesen der Allgemeinheit verständlich zu machen, nachgeholt. Die Unterscheidung von Steuerung und Regelung hinsichtlich Zielstellungen, spezifischen Informationsverarbeitungen sowie Anzahl der Ein- und Ausgänge wird deutlicher. Insbesondere wird dem Eindruck entgegengewirkt, dass das Kennzeichen einer Steuerung im Vergleich zum Regelkreis die offene Kette sei.

Der grundlegende Erkenntniszuwachs wurde möglich durch vermehrte Anwendung der SPS-Technologien als Basis geschlossener Ablaufsteuerungen und durch Fortschritte bei den Theorien, Methoden und rechnergestützten Werkzeugen für Steuerungen ereignisdiskreter Prozesse.

Die Verbindungsprogrammierte Steuereinrichtung (VPS) wird auch kurz verbindungsprogrammierte Steuerung genannt.

Anmerkung: Beim Begriff "Steuerung" handelt es sich eigentlich um einen Vorgang und nicht um ein Gerät. Das Steuer-Gerät dagegen ist die Steuereinrichtung, die jedoch verkürzt auch als Steuerung bezeichnet wird, sodass es zu Verwechselungen kommen kann.

Gerätetechnische Ausführungen von Verbindungsprogrammierten Steuereinrichtungen sind beispielsweise:


Die Speicherprogrammierbare Steuereinrichtung (SPS) wird auch kurz Speicherprogrammierbare Steuerung genannt. Der Plural Speicherprogrammierbare Steuereinrichtungen wird mit "SPSen" abgekürzt.

Die SPS ist im Prinzip ein Mikrocontroller mit entsprechenden Speichern für Steuerungsprogramm und Steuerungsparameter sowie zugehörigen Eingängen für Sensorsignale und Ausgängen für Aktorsignale, ergänzt durch Mensch-Maschine-Schnittstellen zur Bedienung sowie Schnittstellen zur industriellen Kommunikation für Programmierung und Vernetzung.

Die SPS ist heute die am meisten verwendete Steuerungsart. Sie ist im Prinzip auch als Regler verwendbar, da die Arithmetik-Logik-Einheit (ALU) des internen Mikroprozessors bei der Informationsverarbeitung sowohl die logischen Steuerungsaufgaben als auch die arithmetischen Regelungsaufgaben lösen kann.

Die SPS bildet daher zugleich auch die Basis für eine zeitgemäße Leittechnik für die Automation in der Volkswirtschaft. Somit haben sich die SPSen wegen ihres universellen Charakters zu einem Massenprodukt entwickelt, das weltweit in Millionenstückzahlen hergestellt wird. Sie ermöglichen daher eine Massenanwendung der Automation, verbunden mit deren Breitenanwendung in allen Bereichen der Volkswirtschaft und in vielen Bereichen von Konsumgütern.

Die SPS-Technologie hat wesentlich dazu beigetragen, die "Abgrenzung zwischen Steuerung und Regelung" einerseits begrifflich zu klären, andererseits gerätetechnisch zu überwinden. Dieser Entwicklungsprozess hat schließlich auch zu Auswirkungen auf die Philosophie und die Methodik der Entwurfsprozesse für Steuerungen und Regelungen geführt. Im Ergebnis ist eine weitgehende methodische Vereinheitlichung erreicht worden, ohne dabei deren innere Spezifik aufzuheben und ohne die PC-gestützten Entwurfswerkzeuge gleichzuschalten.

Beim Entwurf von Steuerungen geht es darum, für eine informell vorgegebene Steuerungsaufgabe eine formale Darstellung der geforderten Prozessabläufe zu erarbeiten, die es ermöglicht, eine entsprechende Steuereinrichtung zu erstellen, sodass durch die von ihr ausgegebenen Stellsignale und empfangenen Messsignale der gewünschte ereignisdiskrete Prozess in der Steuerstrecke abläuft.

Bei verbindungsprogrammierten Steuereinrichtung erfolgt die formale Darstellung in Form von technischen Zeichnungen oder Schaltplänen, durch die vorgeschrieben wird, wie die Bauelemente zur Verknüpfung der binären Signale zusammenzuschalten sind. Bei speicherprogrammierbaren Steuereinrichtungen geht es um die Erstellung von Programmen, über die alle logischen Verknüpfungen softwaremäßig realisiert werden.

Der Steuerungsentwurf kann entweder intuitiv bzw. empirisch oder systematisch durchgeführt werden. Beim systematischen Entwurf spricht man auch von der Erstellung eines Steueralgorithmus. Dabei kommen die Beschreibungsmittel und Methoden der Schaltalgebra, der Automatentheorie oder der Petri-Netz-Theorie zum Einsatz.

Die Beschreibungsmittel der Schaltalgebra, Automatentheorie und Petri-Netz-Theorie können dazu verwendet werden, beim Entwurf von Steuerungen die Steueralgorithmen zunächst manuell grob zu notieren.

Durch eine Wahrheitstabelle kann die Zuordnung von binären Ausgangssignalen A zu binären Eingangssignalen X dargestellt werden. Die Werte der binären Signale werden durch die Ziffern 0 und 1 angegeben.

Die nebenstehende Wahrheitstabelle enthält 2 Eingangssignale E1 und E2, und dafür ergeben sich 4 mögliche Eingangskombinationen. Im rechten Teil der Tabelle sind als Ausgänge A1 bis A3 die Funktionswerte der drei wichtigsten Verknüpfungen dargestellt: UND, ODER, Exclusiv-ODER (Antivalenz).

Solche Tabellen mit mehreren Ausgängen sind eine verkürzte Darstellung von Einzeltabellen mit nur einem einzigen Ausgang. Eine Tabelle mit 4 Eingängen enthält 16 verschiedene Verknüpfungen (siehe Boolesche Funktion).

Beim Steuerungsentwurf kann von solchen Wahrheitstabellen ausgegangen werden. Nach einer möglichen Vereinfachung mit den Regeln der Schaltalgebra oder mit dem Karnaugh-Veitch-Diagramm kann das Ergebnis dann direkt zur Realisierung der Steuerungseinrichtung dienen.

Anhand des folgenden Beispiels soll der Steuerungsentwurf näher betrachtet werden:

"Aufgabenstellung"

"Lösungsweg"

Die Problemstellung des Beispiels verlangt also Speicherverhalten, sodass in der Wahrheitstabelle neben den Sensoren (E1 und E2) auch der Aktorzustand, d. h. das Ausgangssignal A1, selbst als Eingang hinzugefügt werden muss (E3). Dadurch erhält die Tabelle 8 Zeilen.

Aus den Zeilen 1 bis 4 ist zu erkennen, dass bei gedrückter Ruftaste (E1 = 1) immer die Anzeige leuchtet (A1 = 1), die beiden Eingänge E2 und E3 also keine Rolle spielen ((Anm.: Bei dieser Darstellung muss die Ruftaste immer gedrückt sein, wenn der Wert 1 angegeben ist, d. h. Art der Speicherung nicht erkennbar)). Die Zeilen 5 und 6 zeigen, dass die Rückstellung (A1 = 0) von Eingang E3 unabhängig ist. In den Zeilen 7 und 8 steckt das Speicherverhalten der Steuerung: Die Leuchtanzeige behält ihren (alten) Zustand bei (A1 = E3), wenn beide Taster den Zustand 0 haben. Hier liegt also eine interne Rückführung des Meldungsausgangs A1 auf den Eingang E3 vor.

Die Leuchtanzeige besitzt also ein Speicherverhalten. Es handelt sich hierbei um eine sequentielle Verknüpfungssteuerung (s. Definition). Im folgenden Logik-Plan und im Relais-Schaltplan ist der Speicher als Selbsthaltekreis ausgebildet.



Der Logik-Plan ist eine Schaltung aus elektronischen Schaltgliedern.

Für die Grundverknüpfungen gibt es genormte Symbole, die ausführlich im Artikel Logikgatter beschrieben sind. (≥1 steht für ODER, & für UND, O ein Kreis am Eingang bzw. am Ausgang der Elemente für NICHT).

Die UND-Verknüpfung wird als Reihenschaltung und die ODER-Verknüpfung als Parallelschaltung von Kontakten dargestellt. Für die Nicht-Verknüpfung wird ein Öffner verwendet.

Für Ablauf-Steuerungen von ereignisdiskreten Prozessen eignen sich insbesondere die folgenden Beschreibungsmittel:

Zur Programmierung von Speicherprogrammierbaren Steuereinrichtungen wurden aus den obigen Beschreibungsmitteln 5 spezifische Fachsprachen abgeleitet, denen Compiler beigeordnet sind, mit denen der Quelltext in die SPS-Maschinensprache übersetzt wird. Diese 5 Fachsprachen für SPSen wurden seit den 1990er Jahren schrittweise international genormt, wozu insbesondere die Europa-Norm EN 61131 und hierauf aufbauend die Norm der International Electrotechnical Commission IEC 61131-3 wesentlich beigetragen haben.

Mit diesen Normungen wurden wesentliche Schritte zur Vereinheitlichung getan, um der SPS-Technologie zu ihrem weltweiten Durchbruch zu verhelfen, der die SPS zum meistverwendeten Automatisierungsmittel gemacht hat. SPSen werden heute in Millionen-Stückzahlen hergestellt und sowohl für Ablauf-Steuerungen als auch für Regelungen und Messwertverarbeitungen eingesetzt. SPSen bilden damit das universelle Kernstück der zeitgemäßen Automatisierungsmittel und bewirken deren Massen- und Breiteneinsatz.

LD steht für Lade, N steht für NICHT, A steht für UND, O steht für ODER, S steht für Setzen (speichernd), R steht für Rücksetzen.

Der Kontaktplan wurde abgeleitet vom oben dargestellten Relais-Schaltplan.

Der Funktionsplan wurde abgeleitet vom oben dargestellten Logik-Plan.

Die Ablaufsprache wurde abgeleitet vom steuerungstechnisch interpretierten Petri-Netz.

Neben den speicherprogrammierbaren Steuerungen (SPS) kommen auch Industrie-PC (IPC) zum Einsatz, die mit höheren Programmiersprachen programmiert werden. Diese Möglichkeit besteht auch bei modernen SPSen, sodass sich auch hier höhere Programmiersprachen als Fachsprachen der Steuerungstechnik immer mehr verbreiten. IPCs können mit relativ geringem Aufwand auch umfangreiche Zusatzfunktionen wie Visualisierungen, Protokollierungen und Statistiken bereitstellen.

Ausgeführte Programme benötigen Zeit. Nur Hard- und Software, die auch im ungünstigsten Fall synchron zum Prozess arbeiten kann, ist als Steuergerät geeignet und wird als echtzeitfähig bezeichnet. Im engeren Sinn bedeutet Echtzeit jedoch, dass Hard- und Software eines Rechners für diesen Zweck besonders ausgelegt sind. Rechner, die steuern, dürfen niemals überlastet sein, weil sie sonst mit dem Prozessablauf nicht Schritt halten können und somit ihre Echtzeitfähigkeit verlieren würden.

Während beim Entwurf von Regelungen mathematische Modelle der Regelstrecke einbezogen werden, verwendet man beim Entwurf von binären Steuerungen in der Praxis bisher lediglich gedankliche Modelle der Steuerstrecke. In den 1990er Jahren wurden erste Ansätze zum modellbasierten Entwurf von Steuerungen entwickelt, bei denen von einer Zerlegung der Steuerstrecken in Elementarsteuerstrecken ausgegangen wurde. Aus den sich daraus ergebenden Teilmodellen muss dann ein Gesamtmodell der Steuerstrecke gebildet werden. Dieses Vorgehen ist jedoch sehr aufwendig und wurde somit nicht praxiswirksam.

In den Jahren 2005 und 2007 wurde in der Fachzeitschrift „Automatisierungstechnik“ von Zander eine neuartige Methode zum Entwurf von Ablaufsteuerungen für ereignisdiskrete Prozesse publiziert, die es erlaubt, die in der Steuerungsaufgabe aus technologischer Sicht vorgegebenen Prozessabläufe direkt in ein "Prozessmodell" der gesamten Steuerstrecke umzusetzen. Daraus lässt sich dann durch einfache Transformationen der zugehörige "Steueralgorithmus" generieren.

In einer Buchpublikation wurde diese Methode zum prozessmodellbasierten Entwurf durch Methoden zur Prozessanalyse und Modellbildung von ereignisdiskreten Prozessen komplettiert. Die Grundlage dazu bildet eine allgemein angelegte Struktur- und Verhaltensanalyse von Steuerstrecken auf der Basis der neuartigen Betrachtungsweise von Ablaufsteuerungen (s. oben), aus der sich ein tieferes Verständnis der Wirkungsabläufe ergibt.

Die Modellierung der ereignisdiskreten Prozesse erfolgt dabei durch prozessinterpretierte Petri-Netze. Die daraus generierten Steueralgorithmen werden in Form von steuerungstechnisch interpretierten Petri-Netzen dargestellt, die zur Realisierung direkt in eine Ablaufsprache für SPS umgewandelt werden können. Die Vorgehensweise wird an Praxisbeispielen demonstriert, u. a. durch den Entwurf einer „intelligenten“ Aufzugssteuerung für zehn Etagen.

Das Vorgehen beim prozessmodellbasierten Steuerungsentwurf kommt vor allem den "Anwendern" (Verfahrenstechniker, Fertigungstechniker u. a.) sehr entgegen, die es gewohnt sind, in Prozessabläufen zu denken. Sie müssen dadurch nicht die in der Steuerungsaufgabe gegebenen Prozessabläufe unmittelbar in Steueralgorithmen umwandeln, was insbesondere Neueinsteigern gewisse Schwierigkeiten bereitet. Darüber hinaus kann das zunächst für die Generierung des Steueralgorithmus gebildete "Prozessmodell" zugleich auch für die "Simulation einer entworfenen Steuerung" oder zusätzlich für eine "Betriebsdiagnose" genutzt werden.

Gleichzeitig bedeutet diese innovative Entwurfsstrategie für Steuerungen erstmals eine "methodische Vereinheitlichung" des grundsätzlichen Vorgehens beim Entwurf in der Steuerungstechnik mit dem in der Regelungstechnik, ohne dabei die Spezifik der speziellen Entwurfsverfahren und Entwurfswerkzeuge beider Gebiete in Frage zu stellen.




</doc>
<doc id="13360" url="https://de.wikipedia.org/wiki?curid=13360" title="Flöte">
Flöte

Eine Flöte, mittelhochdeutsch "Floite, Vloite, Flaute" (aus dem altfranzösischen "flaüte" bzw. dem lateinischen "flatuare" und "flatare": „wiederholt blasen“, „kontinuierlich blasen“, Frequentativa von "flare": „blasen“) ist ein Ablenkungs-Aerophon, bei dem ein Luftstrom über eine Kante (Schneide) geführt wird, an der er in Schwingung gerät (vergleiche die Artikel Holzblasinstrument und Pfeife). In der Hornbostel-Sachs-Systematik werden Flöten daher als Schneideninstrumente bezeichnet.

Im alltäglichen Sprachgebrauch steht „Flöte“ meist für die Querflöte oder die Blockflöte. Panflöten bestehen aus mehreren, miteinander verbundenen Eintonflöten.

Es gibt Flöten mit und ohne Kernspalt, einem Luftkanal, der den Luftstrahl zur Anblaskante führt. Bei Flöten ohne Kernspalt wird der Luftstrahl von den Lippen und/oder der Zunge des Spielers geformt.

Weitere Einteilungen und Bezeichnungen ergeben sich daraus, wo man in die Flöte hineinbläst, wie die Tonhöhe beeinflusst wird, ob das untere Ende verschlossen (gedackt) ist oder nicht, ob es sich um einzelne Flötenrohre oder um Instrumente mit mehreren Flöten handelt, und wie diese gespielt werden (direkt geblasen oder mit Ventilen, gesteuert von einem Mechanismus oder einer Tastatur, wie bei der Orgel). Auch der Kulturkreis, aus dem eine Flöte stammt, dient zur Einteilung.

Die Anblaskante wird vom oberen Rand des Flötenrohres gebildet.

Längsflöten

Querflöten
Die Anblaskante einer Querflöte wird vom Rand eines Loches in der Seite des Flötenrohres gebildet.

Der Luftstrom wird durch einen Windkanal geformt und an die Anblaskante des Labiums geführt. Mit Ausnahme der Orgelpfeifen zählen diese zu den Schnabelflöten.

Offene Flöten

Gedackte Flöten

Eine Luftwirbelflöte ähnelt in der Form einer Kombination aus Gefäßflöte und Längsflöte, bildet jedoch den Ton auf eine besondere Weise. Die am oberen Ende einer Röhre eingeblasene Atemluft muss zunächst eine kleine Öffnung passieren, bevor sie in die Spielröhre gelangt und zugleich in einem seitlichen Schwingungsraum einen dem Blasdruck entsprechenden Überdruck erzeugt. Der nachfolgend an der Öffnung vorbeistreichende Luftstrom zieht Luft aus dem Schwingungsraum ab und sorgt dort für einen Unterdruck. Durch den periodischen Druckwechsel entsteht eine schwingende Luftsäule, die sich in der Spielröhre fortpflanzt. Luftwirbelflöten aus Ton sind von den Mayas (um etwa 500 n. Chr.) bekannt.

Doppelflöten sind Flöten mit zwei Spielröhren, die zugleich angeblasen werden. Eine seitlich angeblasene Flöte mit Kernspalt ist die norwegische Obertonflöte Seljefløyte. Eine seltene, mittig angeblasene Querflöte ist die indische Surpava. Die slowakische Fujara ist eine senkrecht gehaltene, lange Schnabelflöte, die über ein Anblasrohr mit Luft versorgt wird.

In China entwickelte man die flugwindgeblasene Taubenflöte.

Die nachweislich ältesten Flöten wurden aus Tierknochen, vor allem von Vögeln, und aus Mammutelfenbein hergestellt. Flöten aus weniger dauerhaftem Material (beispielsweise Holz), konnten nicht nachgewiesen werden, sind aber durchaus denkbar.

Als älteste erhaltene Blasinstrumente der Welt gelten etwa 43.000 bis 40.000 Jahre alte steinzeitliche Knochen- und Mammutelfenbeinflöten, die auf der Schwäbischen Alb gefunden wurden. Eine aus dem Knochen eines Gänsegeiers ("Gyps fulvus") hergestellte Flöte wurde im Sommer 2008 in der Höhle Hohle Fels bei Schelklingen gefunden. Das V-förmige obere Ende der Gänsegeierflöte stellt eine Vorstufe in der Entwicklung der Kerbflöten dar und kommt noch bei der erst seit Ende des 20. Jahrhunderts obsoleten, fingerlochlosen Igemfe in Südafrika vor.

Relativ gut erhaltene oder rekonstruierbare Flöten mit Grifflöchern wurden in der Geißenklösterle-Höhle entdeckt. Die Funde zeigen, dass Menschen schon in der Steinzeit, genauer im Jungpaläolithikum, Musik gemacht haben. Zwei der Flöten aus dem Geißenklösterle sind in einem Stück aus Schwanenknochen gefertigt. Die dritte besteht aus zwei zusammengefügten, aus Mammutelfenbein geschnitzten Halbröhren; sie wurde mit mindestens drei, etwa im Terzabstand gestimmten, Grifflöchern versehen (ein viertes könnte weggebrochen sein) und mit seitlichen Kerbungen verziert. Die immer wieder vermutete Zuschreibung der Flöte zum Neandertaler ("Homo neanderthalensis") widerspricht der wissenschaftlichen Realität, da sie in eindeutig aurignacienzeitlichen Schichten des modernen Menschen ("Homo sapiens") eingebracht war. Zwischen den weiter unten liegenden mittelpaläolithischen und den jungpaläolithischen Schichten liegen kulturell sterile Straten, die einen Kontakt zwischen beiden Epochen und somit zwischen den beiden Menschenformen verneinen.

Fragmente von zwei weiteren Flöten stammen aus der Vogelherdhöhle. Flöte 1 wurde aus Vogelknochen hergestellt. Flöte 2 vom Vogelherd ist aus Mammutelfenbein und in drei nicht zusammenhängenden Bruchstücken erhalten. Erst kürzlich wurde im Abraum der Vogelherdhöhle eine dritte Flöte entdeckt. Sie besteht aus einem Fragment mit zwei angeschnittenen Grifflöchern und ist aus Gänsegeierknochen gefertigt. Die Flöte ist Teil des UNESCO-Welterbes "Höhlen und Eiszeitkunst im Schwäbischen Jura". Sie ist - wie 15 weitere Kunst- und Musikartefakte - im Museum Alte Kulturen im Schloss Hohentübingen ausgestellt.

Eine vermeintlich noch ältere Flöte aus Divje Baba (Slowenien) hat sich mittlerweile auf Grund mikroskopischer Untersuchungen als Zufallsprodukt eines Tierverbisses in einem Bären-Oberschenkelknochen-Fragment erwiesen.

Der Hebräer Jubal, dessen Alturgroßvater Kain war, wird in der Bibel als der Urvater aller Zither- und Flötenspieler bezeichnet.

In der Seeufersiedlung von Hagnau-Burg kam 1986 die bislang älteste erhaltene Holzflöte Europas aus der späten Bronzezeit (1040 vor Christus) zum Vorschein. Sie weist ein Anblasloch und eine feine Verzierung aus Ritzlinien auf.

Das früheste bekannte eindeutige Bild einer Querflöte wurde auf einem etruskischen Relief in Perusa gefunden. Es stammt aus dem zweiten oder ersten Jahrhundert vor Christus. Das Instrument wurde damals nach links gehalten, erst in einer Illustration eines Gedichts aus dem elften Jahrhundert wurde eine Darstellung einer nach rechts gespielten Flöte entdeckt.

Flöten wurden (neben Trommeln) schon in der Prähistorie bei religiösen Kulten benutzt. Bei Naturvölkern ist dies noch heute verbreitet. In der Literatur haben Flöten oft den Charakter des Jenseitigen, von Tod und Vergänglichkeit: Grimms Märchen Nr. 28, 91, 96, 116, 126, 181; Mozarts "Die Zauberflöte"; Andreas Gryphius' "Es ist alles eitel".





</doc>
<doc id="13364" url="https://de.wikipedia.org/wiki?curid=13364" title="Odenwald">
Odenwald

Der Odenwald ist ein bis hohes Mittelgebirge in Südhessen, Unterfranken (Bayern) und im nördlichen Baden (Baden-Württemberg).

Die Westgrenze des Odenwalds an der Bergstraße hebt sich eindrucksvoll von der Umgebung durch die sehr geradlinige Abbruchkante des Berglandes zur Oberrheinischen Tiefebene ab. Auf einer Länge von etwa 65 Kilometern zwischen Darmstadt und Wiesloch erheben sich aus einem ebenen Flachland unvermittelt steile Bergflanken, die mehrere hundert Meter hoch aufsteigen. Die Nordgrenze des Gebirges zeichnet sich weniger klar ab und verläuft auch nicht geradlinig. Der nördlichste Punkt des Odenwaldes liegt nach geographischer Definition nahe der B 26 und dem Darmstädter Institut für Botanik und Zoologie. Die Grenze des Naturraumes hält sich hier meist an den Nordsaum des Waldlandes, auch wenn nördlich anschließend im Reinheimer Hügelland noch Berge von beträchtlicher Höhe und markantem Profil wie der Otzberg über die hier anschließende Untermainebene aufragen. Im Osten zieht das Maintal auf 33 Kilometer Länge von Großwallstadt bis Bürgstadt eine klare Grenzlinie zum Spessart. Daran anschließend läuft die Grenzlinie, der Erfa folgend, in südöstlicher Richtung weiter bis Hardheim, der östlichsten Ortschaft des Odenwaldes. Von hier an trennt die über Walldürn und Buchen bis hin zu Mosbach in südwestlicher Richtung verlaufende Bundesstraße 27 grob den Odenwald vom benachbarten Bauland. Auf Mosbacher Gebiet beginnt das Odenwälder Durchbruchstal des Neckars mit seinem nördlichen Wendepunkt bei Eberbach, der eindrucksvollen doppelten Neckarschleife bei Hirschhorn und dem Austritt in die Oberrheinebene bei Heidelberg; die südliche Odenwaldgrenze folgt allerdings nicht dieser natürlichen Linie, denn südlich des Neckars wird noch der sogenannte Kleine Odenwald zum Mittelgebirge gezählt, der von Mosbach bis Wiesloch im Westen an den Kraichgau stößt. Auch diese Grenze wird, wie die zum Bauland oder zur Untermainebene, verschieden gezogen. Die beiden Naturparks Bergstraße-Odenwald und Neckartal-Odenwald ragen deshalb weiter nach Süden als der Naturraum.

Den Odenwald untergliedern die grob in Nord-Süd-Richtung verlaufenden Tallandschaften des Weschnitz- und Gersprenztals im Vorderen Odenwald und des Mümlingtales im Hinteren Odenwald. Die Haupt-Wasserscheide des Gebirges trennt die Einzugsgebiete von Neckar und Main.

Der Norden und der Westen des Odenwaldes gehören zum südlichen Hessen, im Nordosten liegt ein kleiner Teil im bayerischen Unterfranken, im Süden erstreckt er sich nach Baden hinein. Der Odenwald wird demnach auch, je nach seiner zum Bundesland zugehörigen Region, als "Hessischer Odenwald", "Badischer Odenwald" und "Fränkischer Odenwald" bezeichnet.

In der Mitte des Odenwaldes liegt der Odenwaldkreis mit Sitz in Erbach. Als einziger Landkreis liegt er vollständig in diesem Mittelgebirge. Andere Kreise umfassen daneben auch einen mehr oder weniger großen Anteil der den Odenwald umgebenden Landschaften. Im Westen des Odenwaldes liegt der Kreis Bergstraße mit Sitz in Heppenheim, im Norden der Landkreis Darmstadt-Dieburg mit Sitz in Dieburg und Darmstadt-Kranichstein. Im Nordwesten reicht ein Odenwald-Höhenzug bis in das Stadtgebiet von Darmstadt und im Nordosten erreicht der nördlichste Ausläufer des Gebirges das Gemeindegebiet von Großostheim im Landkreis Aschaffenburg. Den Osten nimmt der Landkreis Miltenberg mit Sitz in Miltenberg ein, den Südosten der Neckar-Odenwald-Kreis mit Sitz in Mosbach und den Süden und Südwesten schließlich der Rhein-Neckar-Kreis mit Verwaltungssitz in Heidelberg. Auch der Stadtkreis Heidelberg gehört teilweise zum Odenwald. Einen besonderen Status hat im fernen Osten der Main-Tauber-Kreis mit Sitz in Tauberbischofsheim. Die dort im Dreieck Wertheim–Freudenberg–Külsheim liegende Wertheimer Hochfläche wird zwar naturräumlich als Teil des Spessart definiert, jedoch landläufig dem Odenwald zugerechnet, da sie links und südlich des Mains liegt. Nur in dieser landläufigen Auffassung hat der Main-Tauber-Kreis Anteil am Odenwald.

Der Odenwald bildet in geologischer und geomorphologischer Hinsicht zusammen mit dem Spessart sowie mit den von diesem noch einmal durch Talungen getrennten Landschaften Büdinger Wald und Südrhön eine Einheit, die naturräumlich als Großregion 3. Ordnung "14" (Kennziffer nach Nummerierung des BfN: D55) Odenwald, Spessart und Südrhön zusammengefasst wird. Der Odenwald als links des Mains gelegener Teil dieser Großlandschaft zerfällt dabei in erster Linie in den "Sandstein-Odenwald" („"Buntsandstein-Odenwald"“) und den "Vorderen Odenwald" („"Kristalliner Odenwald"“).

Folgende Unter-Naturräume sind ausgewiesen:

Die ineinander übergehenden Talungen von Weschnitz (145.3) und Gersprenz (145.8) trennen, innerhalb des kristallinen Odenwaldes, einen orographischen Nordwestteil des Mittelgebirges ab, der orographisch noch einmal in 3 größere Segmente und ein kleines zerfällt. Der eigentliche Melibokus-Odenwald (145.00−04) im Nordwesten ist durch die Talungen von Lauter (145.05) und Mud (145.06/07) vom Hauptteil getrennt; im äußersten westlichen Norden trennt das Mühltal (145.07) noch einmal den nur wenig Fläche einnehmenden Trautheimer Wald nebst Ausläufern (145.08/09) ab. Auch der Juhöhe-Odenwald (145.2) ist innerhalb des nordwestlichen Odenwaldes durch eine Senke, die vom Heppenheimer Stadtbach nach Osten in Nebentäler der Weschnitz übergeht, orographisch als Südteil abgetrennt.

Jenseits der Weschnitz-Gersprenz-Senke gehen Eichelberg- (145.1), Tromm- (145.4) und Böllsteiner (145.9) Odenwald ohne nennenswerte Höhenunterschiede in den Sandstein-Odenwald über.

Das Buntsandsteingebiet südlich des Mains wird im äußersten Süden durch die Talung des Neckars (144.3) geteilt, die den Kleinen Odenwald (144.1/2) abtrennt; im Nordosten trennt die Talung der Erfa (144.9) einen kleinen Nordostteil, die Wertheimer Hochfläche (141.1), ab, die bereits dem Sandsteinspessart zugerechnet wird.

Der Sandstein-Odenwald trägt über dem variskischen Grundgebirge noch die sedimentäre Bedeckung aus der Buntsandstein-Zeit. Dieser Teil des Gebirges ist sehr stark zertalt, die häufigen länglichen Höhenrücken zeigen die „Sargdeckel-Form“. Der Odenwälder Sandstein wird in Steinbrüchen bei Beerfelden abgebaut.

Im westlichen Teil des Odenwalds hat die Abtragung den kristallinen Grundgebirgsstock bereits freigelegt, verursacht durch die stärkere tektonische Hebung am Ostrand des Oberrheingrabens. Hier tritt eine große Vielfalt an Gesteinen auf: Metamorphe Gesteine, überwiegend in Form von Gneisen, sind genauso vertreten wie Plutonite (Granit, Diorit und Gabbro) oder Gesteine vulkanischer Herkunft, wie Rhyolith („Quarzporphyr“) oder Basalt.

Zu den Bergen des Odenwaldes gehören – sortiert nach Höhe in Meter (m) über Normalhöhennull (NHN; wenn nicht anders genannt laut ):

Über 600 m:
Über 500 m:
Über 400 m:

Über 300 m:

Im Odenwald entspringen zahlreiche Fließgewässer, davon sind die längsten:

Zu den wenigen Stillgewässern im Odenwald gehören (mit Wasserflächen in ha):

Der Odenwald wird mineralogisch in den westlichen kristallinen Odenwald und den östlichen Buntsandstein-Odenwald gegliedert, der vorwiegend aus Sedimentgestein besteht. Insgesamt wird die geologische Geschichte des Odenwaldes in drei Hauptabschnitte unterteilt:

Zusammensetzung sowie Genese des kristallinen Gebirges sind recht kompliziert, seine geologische Karte ähnelt einem Flickenteppich. Als Teil der Mitteldeutschen Kristallinen Zone (MDKZ), die sich in einem Bogen bis zum Thüringer-Wald erstreckt, entstanden im Erdaltertum (Paläozoikum) zum einen die Granitoide zum anderen Hochdruckgesteine wie Eklogite (im Karbon), beide repräsentieren „recycelte“ Kruste. Größen und Verteilung der Kontinente unterschieden sich damals sehr vom heutigen Zustand: „Mitteleuropa“ lag in einem Ozean-Gebiet südlich des Äquators und bestand aus kleinen Kontinenten. Durch die Kontinentalverschiebung driftete ein Südkontinent auf einen Nordkontinent zu. Deshalb kollidierten die dazwischen liegenden „mitteleuropäischen“ Zwerg-Kontinente, und in der Devon- und Karbon-Zeit erhob sich auf und zwischen ihnen das Variszische Gebirge, zu dem auch der Odenwald zählt. In der Forschung wird für die MDKZ ein Inselbogen-Szenarium mit Gebirgsbildung als Folge einer Subduktionszone diskutiert, wie sie heute in der ostasiatischen Pazifikküstenregion besteht. Danach wurden zuerst die alten Gesteine tief in die Erdkruste versenkt (Subduktion) und in etwa 15 Kilometer Tiefe im oberen Erdmantel aufgeschmolzen, dann zusammen mit Magmagesteinen langsam wieder in die Erdkruste hochgedrückt, wo sie im Laufe von 60 Millionen Jahren allmählich abkühlten und auskristallisierten.

Die aktuelle Forschung unterteilt den kristallinen Odenwald nach den tektonisch-metamorphotischen Abläufen in drei durch Störungszonen ("Strike-slip"-Zonen) voneinander getrennte Einheiten:

Diese drei Einheiten, die eine gemeinsame Sedimentation haben, wurden im Devon vor etwa 400 bis 375 Millionen Jahren auch unter ähnlichen Temperatur- und Druck-Bedingungen metamorph überprägt. Tektonische Prozesse (Dehnungen) trennten jedoch diese Einheiten, so dass sie sich in ihrem Magmatismus und den durchlaufenen Metamorphosen unabhängig weiterentwickelten.

Diese Komplexe stecken zwischen metamorphisierten Altbeständen, z. B. den Schiefern und Gneisen in der →Flasergranitoidzone (von Heppenheim/Bensheim erzgebirgisch in Richtung NE bis zur Otzberg-Störung). interpretiert die Prozesse in diesen Mischgebieten vor allem im südlichen Teil magmatisch. Danach sollen verschiedene Magmakörper in einer kurzen Zeitspanne aufgestiegen sein ("nested diapirs"); im nördlichen Grenzbereich zum Frankenstein-Massiv vermutet er eine tektonische Überprägung – durch Scherbewegungen der Gesteinsformationen soll es zu Aufschmelzungen (dynamische Kontaktmetamorphose), wechselseitigen Infiltrationen und Überformungen benachbarter Partien gekommen sein, die dann ähnliche kristallin-metamorphe Strukturen ausgebildet haben. Auch zwischen Weinheim und Wald-Michelbach haben sich solche Zonen entwickelt. Der Auerbacher Marmorzug, ein Sonderfall, entstand durch Aufheizung und chemische Reaktion zwischen aneinandergrenzenden Kalk- und Silikatgesteinen. Bei den tektonischen Vorgängen rissen immer wieder Spalten in den Gesteinsmassen auf, in welche u. a. erzhaltige Schmelzen eindrangen, die dort dann zu Ganggesteinen auskristallisierten. Beispiele hierfür sind die Quarz- und Baryt-Gänge bei Reichenbach und Balzenbach. Jüngere aplitartige Granite zertrümmerten ältere Granodiorit- oder Biotitgranitbestände. Im weiteren Verlauf der Plattenbewegungen wurden die durch eine alte Störungszone – die Otzbergspalte – getrennten Böllsteiner Gneise und die Bergsträßer Komplexe zusammengeschoben und durch den Trommgranit verschweißt.

Große Erschütterungen der Erdkruste durchrüttelten den Odenwald in der Zeit des Ober-Rotliegenden vor etwa 260 Millionen Jahren. Vulkane drangen vor allem im Gebiet um Weinheim (Wachenberg, Daumberg), Schriesheim/Dossenheim (vor 290 bis 270 Millionen Jahren) und Heidelberg an alten Störungszonen aus der Erde, schleuderten Tuffe aus ihren Kratern und gossen Lava auf die Erdoberfläche – das Granit-Gneis-Gebirge war inzwischen bereits bis zum Sockel abgetragen. Im Rhyolith-Steinbruch Weinheim wird die erstarrte Schlotfüllung des Wachenberg-Vulkans zu Schotter verarbeitet. Dagegen ist in den Schriesheimer und Dossenheimer Steinbrüchen der Abbau der auf dem Granitgebirgsrumpf aufliegenden Quarzporphyrdecken inzwischen eingestellt. Die Stelle, an der der die Effusionsgesteine fördernde Vulkan ausbrach, lag vermutlich im Gebiet des heutigen Rheingrabens und versank mit den örtlichen Gesteinen bei dessen Einbruch. Reste des Rotliegenden sind etwa bei Schriesheim und im Sprendlinger Horst erhalten.

Im Erdzeitalter des jüngeren Perm überflutete das Zechsteinmeer die Region und überdeckte sie mit Ablagerungen, die für den Erzbergbau bedeutsam sind: Dolomite, in die später eisen- und manganhaltige Quarzlösungen eindrangen. Die Sedimentationsgeschichte setzt sich im Mesozoikum (Erdmittelalter) zwischen 250 und 65 Millionen Jahren fort mit der Ablagerung von bis zu 600 m mächtigen Buntsandstein-, Muschelkalk-, Keuper- und Jura-Schichten.

Lange Zeit später kam es in Mitteleuropa wieder zu starken Bewegungen in der Erdkruste: In Verbindung mit einer Rift-Zone vom Mittelmeer bis an die Nordsee brach im Tertiär vor etwa 45 Millionen Jahren der Oberrheingraben stellenweise bis zu 3,5 km (Ende des Tertiärs: bis 4 km) tief ein und wurde durch Nachrutschen der damaligen Oberfläche sogleich aufgefüllt. Diese Senkung dauert bis in die Gegenwart an, bei Darmstadt mit einer Geschwindigkeit von rund 0,2–0,4 mm pro Jahr. Zum Ausgleich hoben sich die angrenzenden Berge um bis zu 2,5 km an, doch setzte mit der Hebung bereits die Abtragung ein. In der Folge zerlegten viele Kreuz- und Querklüfte das Gebiet des heutigen Odenwaldes in Gebirgsblöcke und Gräben. Eine Folge der Absenkung sind auch leichte Erdbeben im Nordwesten des Odenwaldes. Diese gehen über Mikrobeben hinaus, sind spürbar und können zu leichten Beschädigungen führen. Am 17. Mai 2014 um 18 Uhr 48 (MESZ) erschütterte ein Erdbeben mit dem Magnitudenwert von 4,2 auf der Richterskala Nieder-Beerbach. Das Epizentrum lag in einer Tiefe von ca. sechs Kilometern. Das Beben verursachte zahlreiche leichte Gebäudeschäden. Es war Teil einer Serie schwacher Erdbeben im Raum südöstlich Darmstadts seit März 2014.

Magmamassen drangen an die Oberfläche vor und bildeten Basalt-Vulkane: Neben dem bereits vor 68 Millionen Jahren in der Kreidezeit entstandenen Katzenbuckel, der vor etwa 40 Millionen Jahren erneut ausbrach, gehören zu diesen im mittleren und nördlichen Odenwald etwa der Roßberg (vor 52 Millionen Jahren) und der Otzberg (vor rund 35 bis 20 Millionen Jahren).



Die geologischen Prozesse haben zahlreiche Minerale und Erze entstehen lassen. Bis in die Neuzeit hinein wurden Marmor (Auerbacher Marmor) und Porphyr (Dossenheim) abgebaut. Im südwestlichen Odenwald förderte man seit dem Mittelalter Silber-, Blei- und Kupfererze, während im östlichen Teil des Buntsandstein-Odenwaldes der Bergbau auf die Eisen- und Manganerze dominierte. Die meisten Betriebe wurden mit dem Aufkommen der Hochöfen unrentabel, weil diese große Mengen an Steinkohle brauchten, welche örtlich nicht vorkommt. Der geringe Gehalt und die geringe Reinheit der Erze und die bis zum Bau der Odenwaldbahn ungünstigen Transportverbindungen waren ebenfalls nachteilig. Es gibt drei Besucherbergwerke im Odenwald: Grube Anna-Elisabeth () bei Schriesheim, Grube Marie in der Kohlbach () bei Weinheim und Grube Ludwig () bei Wald-Michelbach.

Die früheste Besiedlung des Odenwaldes ist fassbar durch endneolithische archäologische Funde. Ältere Funde der Bandkeramik gibt es nur in den nördlichen (Gersprenz), westlichen (Juhöhe) und südlichen (Neckartal) Randbereichen des Odenwaldes. Zu den ältesten Funden zählt ein Hockergrab, das unter der Hofmauer der späteren römischen Villa Haselburg bei Hummetroth gefunden wurde. Zahlreiche Werkzeugfunde in der Gegend des (außerhalb des Odenwaldes gelegenen) Kinzig­tals belegen in dieser Zeit eine Besiedlung. Sie wurden privat gesammelt und befinden sich heute als "Sammlung Schwarz" im Breubergmuseum auf der Burg Breuberg sowie im Stadtmuseum in Michelstadt. Aus der Bronzezeit fehlen Siedlungsfunde. Doch sind entlang der Flusstäler zahlreiche Grabhügel erhalten, besonders im mittleren Mümlingtal. Sie liegen charakteristisch auf den Anhöhen oberhalb der Talkessel.

In der Hallstatt- und Frühlatènezeit wurden diese Grabhügel für Nachbestattungen erneut genutzt und auch neue angelegt. Eine solche Grabanlage ist von der Hoffläche der römischen Villa Haselburg bekannt. Der Grabhügel enthielt zwei Bestattungen der frühkeltischen Zeit (4./3. Jahrhundert v. Chr.) mit Trachtbestandteilen aus Eisen und Bronze, darunter ein Scheibenhalsring mit Koralleneinlagen. Es gibt aber keine Hinweise darauf, ob der Hügel zur Römerzeit noch sichtbar war. Ein weiterer bedeutender Fund dieser Zeit ist das sogenannte Raibacher Bild, eine anthropomorphe Sandstein-Stele, die 1919 am Obersberg bei Breuberg-Rai-Breitenbach gefunden wurde. Sie befindet sich heute im Hessischen Landesmuseum Darmstadt, eine Kopie ist im Breubergmuseum ausgestellt.

Funde aus der keltischen Spätlatènezeit fehlen im Odenwald fast völlig. Möglicherweise waren die Kelten zur Zeit der Ankunft der Römer bereits durch Germanen verdrängt worden. Südwestlich des Odenwaldes siedelten sich um Ladenburg im 1. Jahrhundert n. Chr. die Neckarsueben an. Für den Odenwald muss nach derzeitigem Kenntnisstand davon ausgegangen werden, dass die Römer ein relativ unbesiedeltes Land vorfanden.

Mit der Eroberung des rechtsrheinischen Decumatlandes in den Chattenkriegen Kaiser Domitians gelangte das Gebiet unter römische Kontrolle. Im Gegensatz zu anderen Bauten des Obergermanisch-Raetischen Limes wie der Taunusstrecke wurde die ältere Odenwaldlinie des Neckar-Odenwald-Limes erst um das Jahr 100 unter Kaiser Trajan (98–117) errichtet. Dieser Limesabschnitt verläuft vom Kastell Wörth am Main zunächst nach Südwesten über das Kastell Seckmauern zum Kastell Lützelbach. Von dort verläuft er auf dem großen Sandsteinrücken östlich der Mümling nach Süden über das Kleinkastell Windlücke, Kastell Hainhaus, Kastell Eulbach, Kastell Würzberg zum Kastell Hesselbach, wo er das heutige Dreiländereck Hessen/Bayern/Baden-Württemberg passiert. Auf baden-württembergischer Seite folgen zunächst die Kleinkastelle Zwing und Seitzenbuche, Kastell Schloßau, Kastell Oberscheidental, die Kleinkastelle Robern und Trienz, die Kastelle von Neckarburken, Uferkastell Duttenberg, Kleinkastell Kochendorf, bis er schließlich beim Kastell Wimpfen im Tal den Neckar erreicht.

Die besonders gebirgigen Strecken des Limes wurden zwischen Wörth und Oberscheidental durchgängig von kleineren Einheiten, sogenannten "numeri" bewacht. Aus zahlreichen Inschriften geht hervor, dass es sich dabei um "numeri brittonum" handelt, also Hilfstruppeneinheiten, die ursprünglich in Britannien ausgehoben wurden.

Große Teile des Odenwaldes lagen nun im römisch beherrschten Obergermanien. Um 159 wurde der Limes um ungefähr 30 km nach Osten auf die Linie Miltenberg–Walldürn–Buchen-Osterburken vorverlegt. Die Odenwaldstrecke erreichte deshalb nicht den letzten Ausbauzustand des Limes mit Wall und Graben, sondern es bestand zu den Wachtürmen und dem Postenweg nur die Palisade. Im Hinterland etablierte sich eine zivile Verwaltung, Deren Hauptorte lagen am Rande des Odenwaldes in Dieburg (Hauptort der Civitas Auderiensium), Ladenburg (Civitas Ulpia Sueborum Nicretum) sowie Bad Wimpfen (Civitas Alisinensium). Im Odenwald entstanden zivile Siedlungen in Form von zahlreichen kleineren Villae rusticae, die sich schwerpunktmäßig an den Flüssen befanden. Neben den zahlreichen kleineren Wirtschaftseinheiten gab es auch wenige größere Villen. Die bedeutendste Fundstelle dieser Art ist die Haselburg bei Hummetroth (nahe Höchst i. Odw.), die freigelegt und als Freilichtmuseum konserviert wurde.


Etliche namhafte Territorialherrschaften (siehe Karte von Hessen um 1550) teilten sich das Gebiet des Odenwaldes. Zu nennen wären etwa: Kurpfalz, Kurmainz, Grafschaft Katzenelnbogen, Landgrafschaft Hessen-Darmstadt, Grafschaft Erbach, Herrschaft Breuberg, Herrschaft Frankenstein, Herrschaft Steinach, Herrschaft Hirschhorn, Fürstentum Leiningen. Diese alle wurden abgelöst vom Großherzogtum Hessen (später Volksstaat Hessen), dem Großherzogtum Baden (später Republik Baden) und dem Königreich Bayern (jetzt Freistaat Bayern).

Die Deutung des Namens "Odenwald," der in den Formen "Odonewalt" (815), "Otenwalt" (970) und "Odenwalt" (1016) überliefert ist, wird kontrovers diskutiert:









Die zahlreichen Volkssagen aus dem Odenwald sind meistens an bestimmte Orte (Burg, Stadt, Felsen, Weg usw.) gebunden ("Lokalsage") und erzählen:

Die Lokalsage verbindet sich in einigen Erzählungen einmal mit der "Natursage", in der dämonische Wesen (z. B. Ritter Georg tötet in der Nähe des "Frankensteins" den menschenfressenden Lindwurm) und Naturgeister (als Fuchs auftauchender "Wassergeist" bei Niedernhausen, "Meerweiblein" in den "Meerwiesen" von Walldürn) auftreten, und zweitens mit der "Geschichtssage", die anekdotenhaft historische Personen und Originale porträtiert: Luther und der Graf von Erbach, "Raubacher Joggel" und der Erbacher Graf, Landgraf Ludwig VIII. von Hessen-Darmstadt, Räuber "Lindenschmidt", "Hölzerlips-Stein" auf dem "Hirschopf" bei Weinheim.
Drittens geht sie eine Verbindung mit der ätiologischen Sage (Erklärungssage) ein, d. h., sie erklärt,

Zwei literarisch bearbeitete Sagenstoffe sind überregional bekannt:

Die Sage vom Rodensteiner, eine Variante der Gespenstergeschichte vom wilden Heer, wurde ursprünglich in den Reichenberger Protokollen (1742–1796) dem „Landgeist“ des „Schnellertsherrn“ zugeschrieben: Bauern im Gebiet um Fränkisch-Crumbach erzählten, sie hätten in stürmischen Nächten in der Luft das Geisterheer von der Ruine Schnellerts über das Gersprenztal zur Ruine Rodenstein ziehen hören. Sie deuteten dies als Zeichen eines bevorstehenden Krieges. Dieses Motiv des wilden Jägers wird bei der Erklärung der als "Hundsköpfe" bezeichneten Felsformation auf der Juhöhe (s. o.) aufgegriffen.

Im berühmten Nibelungenlied (siehe auch Nibelungensage), einem mittelalterlichen Ritterepos mit Sagenkern (Siegfried), spielt der Odenwald als Handlungsort nur in einem Abschnitt eine, für die weitere Handlung allerdings entscheidende, Rolle: Der Drachentöter Siegfried wird bei einem Jagdausflug (anstelle eines ausgefallenen Feldzugs), der von der Burgundenstadt Worms in den Odenwald führt, von Hagen von Tronje an einer Quelle ermordet. Da kein genauer Ort überliefert ist, streiten sich zahlreiche Gemeinden des hessischen Odenwaldes sowie Odenheim im Kraichgau darum, den „echten“ Siegfriedbrunnen zu besitzen.

Der geplante Weiterbau der "Odenwaldautobahn" (Bundesautobahn 45) wurde nie verwirklicht, daher ist der Odenwaldkreis, mit allen Vor- und Nachteilen, einer der wenigen ganz autobahnfreien Landkreise.

Durch den Odenwald verlaufen mehrere Bundesstraßen:

Außerdem führen durch den Odenwald die Nibelungen- und die Siegfriedstraße, die teilweise den vorgenannten Straßen folgen.


Zu den sehenswerten Bauwerken im Odenwald und in seinem Südteil Kleiner Odenwald gehören eine Vielzahl von Burgen, Schlössern und Stadtpalästen in drei Bundesländern:
Daneben seien weitere interessante Bauten verschiedener Epochen erwähnt, wie zum Beispiel:
das ergrabene und als Ruine restaurierte Römerkastell bei Würzberg, die Römische Villa Haselburg bei Hummetroth, das Alte Rathaus von Michelstadt und
die Einhardsbasilika in Steinbach.


Der Odenwald ist durch ein über 10.000 km umfassendes Streckennetz von Wanderwegen erschlossen:


Trotz häufigen Schneemangels kommen die Skigebiete des Odenwaldes auf einige Betriebstage im Jahr. Gespurte Langlaufloipen sind zahlreich vorhanden. Auch Abfahrtslauf ist möglich.


Der Odenwald ist das Übergangsgebiet zwischen rheinfränkischen Dialekten im Westen und südfränkischen im Osten, nach einer älteren Gliederung zwischen den mitteldeutschen und den oberdeutschen Sprachen. Die pfälzischen Mundartvarianten des Rheinfränkischen werden Odenwälderisch genannt, die südfränkischen Odenwäldisch. Im badischen Gebirgsteil ist eine Untergruppe des Pfälzischen, das Kurpfälzische, im nordwestlichen Odenwald das Südhessische verbreitet.

Über den Odenwald wurden mehrere Lieder geschrieben:








</doc>
<doc id="13367" url="https://de.wikipedia.org/wiki?curid=13367" title="Dracula (Roman)">
Dracula (Roman)

Dracula ist ein 1897 veröffentlichter Roman des irischen Schriftstellers Bram Stoker. Die zentrale Figur, "Graf Dracula", ist der wohl berühmteste Vampir der Literaturgeschichte.

Der Londoner Rechtsanwalt Jonathan Harker reist auf Wunsch des Grafen Dracula nach Siebenbürgen, da dieser zuvor in London ein Haus erworben hat und nun den Kauf und die bevorstehende Überfahrt durch seinen Anwalt abklären lassen möchte. Auf der Hinfahrt bemerkt Harker einige für ihn wunderliche Dinge. Eine Anwohnerin übergibt Harker einen Rosenkranz, um ihn zu schützen. In Bistritz nimmt er eine Postkutsche, und am Borgo-Pass (Tihuța-Pass) wird er von einem Kutscher abgeholt und zum Wohnsitz des Grafen begleitet. Die ersten Tage verlaufen ruhig, doch Harker wird gebeten, einige Räume nicht zu betreten, und verspricht, sich daran zu halten. Er bemerkt, dass der Graf kein Spiegelbild hat und einen gierigen Gesichtsausdruck beim Anblick von Blut bekommt, als er sich bei der Rasur schneidet. Bald wird der Graf dem jungen Engländer unheimlich, schon allein seine äußerliche Erscheinung ist seltsam: lange, sehr weiße, spitze Zähne und auffällig rote Lippen.

Harker darf das Schloss nicht verlassen und wird des Nachts Zeuge, wie Dracula eine Wand hinabklettert, als sei er eine Eidechse. Außerdem wird er gewarnt, er dürfe in keinem anderen Zimmer einschlafen als in seinem eigenen. Eines Tages betritt er ein neues Zimmer, schläft ein und wird von drei sehr hübschen jungen Frauen entdeckt, die wie der Graf ungewöhnlich rote Lippen und spitze, leuchtende Zähne haben. Harker stellt sich schlafend und wird von einer der Frauen fast gebissen, doch Dracula erscheint plötzlich und hält die Frau davon ab. Der Graf lässt erkennen, dass er den jungen Harker für sich haben will, und wirft den Damen einen Sack mit einem darin gefangenen wimmernden Kind vor, auf das sie sich hungrig stürzen.

Seit diesem Erlebnis hat Harker Todesangst, er rechnet mit seinem baldigen Tod. Zweimal zwingt ihn der Graf, Briefe mit unverfänglichem Inhalt an seine Verlobte und seinen Arbeitgeber zu schicken. Der Graf bietet Harker in scheinbarer Freundlichkeit Gelegenheit zur Flucht, doch traut sich dieser nicht an den vom Grafen beherrschten Wölfen vorbei, die zuvor eine Frau zerfleischt haben. Harker entdeckt eine Gruft des Schlosses, in der Dracula tagsüber in einer mit Erde gefüllten Kiste liegt, die zusammen mit 49 weiteren Kisten auf dem Schiff "Demeter" nach England gebracht wird. Dem jungen Mann gelingt schließlich doch noch die Flucht aus dem Schloss.

Gut einen Monat später läuft das Schiff in einem schweren Unwetter in den Hafen der Stadt Whitby (Grafschaft Yorkshire) ein. Die Mannschaft scheint bis auf den an das Steuer gebundenen toten Kapitän verschwunden zu sein, und im Augenblick der Landung der "Demeter" im Hafen springt ein großer schwarzer Hund an Land und verschwindet. Aus dem Logbuch des Kapitäns erfährt man, dass sich offenbar "etwas" bzw. "ein fremder Mann" an Bord befunden habe und die Mannschaft Matrose für Matrose verschwand, bis nur noch der Kapitän übrig blieb.

Wilhelmina 'Mina' Murray, Jonathan Harkers Verlobte, ist zu ihrer Freundin Lucy Westenra (in manchen Übersetzungen auch Westenraa, z. B. bei Willms) nach Whitby gefahren. Hier ereignen sich nun eigenartige Dinge. Lucy erkrankt an einem starken Somnambulismus, und Mina bemerkt eines Tages zwei punktförmige Male am Hals ihrer Freundin. Da Lucys Verlobter Arthur Holmwood wegen einer schweren Erkrankung seines Vaters Lord Godalming wenig Zeit hat, sich um seine Braut zu kümmern, und weil Lucys Mutter ebenfalls schwer krank ist, bittet er seinen Freund, den Irrenarzt Dr. John Seward, der ebenfalls um Lucys Gunst geworben hat, sich um sie zu kümmern.

Seward ist der Leiter einer Anstalt neben der Carfax Abbey, dem zukünftigen Heim Draculas. In seiner Pflege befindet sich auch ein Mann namens Renfield, der ein Zoophag ist und Fliegen, Spinnen und Sperlinge verspeist. Seward weiß sich in Bezug auf Lucys Krankheit keinen Rat und benachrichtigt seinen ehemaligen Lehrer, den holländischen Gelehrten Professor Abraham van Helsing. Dieser weiß sofort, dass er es mit einem Vampir zu tun hat – was er jedoch zunächst verschweigt – und veranlasst Holmwood, seiner Verlobten wegen ihres starken Blutverlustes eine Blutspende zu geben.

Lucy wird jedoch in den darauffolgenden Nächten erneut heimgesucht, auch weil ihre unwissende Mutter die zum Schutz ihrer Tochter aufgehängten Knoblauchblüten entfernt hat. Nachdem sie in einer Nacht dem in der Gestalt eines Wolfes auftretenden Vampir begegnet ist, stirbt Lucy, obwohl mittlerweile auch Dr. Seward, Prof. van Helsing und der Amerikaner Quincey P. Morris, ein weiterer Verehrer Lucys, Blut für sie gespendet hatten. Lucy wird zur Untoten und erwählt sich Kinder zum Opfer.

Jonathan Harker ist inzwischen zurückgekehrt: Er hat drei Monate in einem Krankenhaus in Budapest zugebracht und Mina, die ihn dort besucht hat, geheiratet. Unterdessen ist Arthurs Vater gestorben, und auch Harkers Vorgesetzter Hawkins stirbt kurz nach der Heimkehr Jonathans. Durch Harkers Tagebuch informiert, ist van Helsing fest entschlossen, mit den anderen den Vampir zu jagen und zu töten. Zunächst einmal muss die Gruppe Lucy von dem Fluch befreien und sie daran hindern, ihr nächtliches Unwesen zu treiben. Dazu schlägt ihr Verlobter ihr einen Holzpflock ins Herz. Ferner wird ihr Kopf abgetrennt und der Mund mit Knoblauch gefüllt.

Danach beginnt die Gruppe eine Suchaktion nach dem Vampir durch London, da die erdgefüllten Kisten auf verschiedene Orte der Stadt verteilt worden sind. Mina bleibt derweil in Sewards Heilanstalt, wo sie jedoch von Dracula heimgesucht wird. Er veranstaltet mit Mina eine Art 'Bluthochzeit', indem er sie dazu zwingt, sein Blut zu trinken. Außerdem tötet er Renfield, nachdem dieser ihm Einlass zur Anstalt verschafft hat, aber verhindern wollte, dass Dracula Mina zur Untoten macht.

Der Vampir kann zunächst von den Männern in die Flucht geschlagen werden und tritt seine Rückreise nach Transsylvanien an. Mina bekommt von van Helsing eine Hostie auf die Stirn gelegt, die ein Brandmal hinterlässt. Mina kann sich durch ihre Blutsverbindung mit dem Grafen in diesen einfühlen und teilt den anderen unter Hypnose dessen Empfindungen mit, woraus diese auf seinen Aufenthaltsort zu schließen versuchen. Sie vermuten, dass er nach Warna (Bulgarien) will, und fahren mit der Eisenbahn dorthin.

Da der Graf durch seine Verbindung mit Mina von ihrem Aufenthaltsort erfährt, steuert er das Schiff stattdessen nach Galatz, was der Gruppe durch ein Telegramm von Lloyd’s Register of Shipping mitgeteilt wird. Mina erarbeitet aufgrund von logischen Schlüssen, dass der Graf nun auf einem Schiff den Sereth und die Bistritza herauffährt, um zu seinem Schloss zu kommen. Die Gruppe teilt sich, um Dracula abzufangen. In der Nähe seines Schlosses können sie ihn kurz vor Sonnenuntergang stellen. Nachdem sie erfolgreich gegen die Zigeuner, die die Kiste transportiert haben, gekämpft haben, enthauptet Harker den Vampir mit seinem Kukri. Sein Körper zerfällt und Minas Narbe verschwindet. Zu beklagen ist jedoch der Tod von Quincey P. Morris, dem die Zigeuner tödliche Verletzungen zugefügt haben.

Sieben Jahre später bekommt Mina von Jonathan einen Sohn, den die Eltern nach ihren Freunden benennen. Einer seiner Zweitnamen, Quincey, soll immer an ihren Freund erinnern, der bei der Vernichtung des Grafen Dracula umgekommen ist.

Der Roman ist eine Mischung aus Reise-, Liebes-, Abenteuerroman und Schauergeschichte und besteht formal aus einer Folge von Tagebucheintragungen, Mitschriften von Phonographaufnahmen, Briefen und Zeitungsartikeln. Um jedoch die Protagonisten einheitlich handeln zu lassen und Missverständnisse zwischen ihnen zu vermeiden, weiht der Autor die Figuren jeweils in die Gedanken und Aufzeichnungen der anderen ein, indem er einzelne Personen die Tagebücher der übrigen lesen lässt, so zum Beispiel Van Helsing die Notizen Jonathan Harkers.

Die Tatsache, dass im Roman ein einzelner subjektiver Erzähler fehlt, verleiht dem Ganzen ein dokumentarisches und pseudoreales Gepräge. Zudem kann das Werk als Vorläufer der personalen Erzählstruktur angesehen werden. Der Tagebuchcharakter des Romans bewirkt beim Leser eine Unmittelbarkeit der Teilnahme an der Handlung, die jedoch abgeschwächt wird, da den Aufzeichnungen der Einzelfiguren ein individueller Stil fehlt.

Der Vampirroman "Dracula" folgte einer ganzen Reihe von Geschichten über Vampire, die in der Romantik und später im 19. Jahrhundert zu einem beliebten Topos der Literatur wurden. Darüber hinaus wird er dem Genre des Schauerromans zugeordnet, weil Stoker Elemente wie alte Schlösser, Ahnenflüche und übernatürliche Erscheinungen in seinem Roman verwendet.

Besonders beeinflusst und beeindruckt war Stoker von der Erzählung "Carmilla" des Iren Joseph Sheridan Le Fanu. So sollte auch Stokers Roman zunächst in der Steiermark spielen und in einem Einführungskapitel ließ er seinen Protagonisten Jonathan Harker das Grab der Vampirin entdecken. Stoker entschied sich jedoch für Transsylvanien. Das Einleitungskapitel wurde herausgenommen und später als Kurzgeschichte unter dem Titel "Draculas Gast" veröffentlicht.
Stoker verlegte den Handlungsort auch deswegen, weil er auf die historische Gestalt des Vlad Țepeș (Vlad der Pfähler, 1431–1476), einen für seine Grausamkeit berüchtigten walachischen Wojwoden, den er zur Romanfigur umarbeitete, hinweisen wollte.

Die Vorbildfunktion von Vlad für Stokers Dracula wird jedoch bisweilen bestritten. So behauptet Elizabeth Miller, dass in Stokers Dracula keinerlei biographische Details von Vlad auftauchen und dass Stoker den Namen „Dracula“ einzig aufgrund der in einer von ihm benutzten Quelle (William Wilkinson, "An Account of the Principalities of Wallachia and Moldavia") enthaltenen falschen oder zumindest ungenauen Übersetzung als „Teufel“ wählte.

Bram Stoker hat auch das Buch "The Land beyond the Forest. Facts and Fancies from Transsylvania" (Edinburgh und New York 1888) der schottischen Reiseschriftstellerin Emily Gerard als Informationsquelle genutzt. Sie beschreibt dort die Sagengestalt Nosferatu, den sie mit "Untoter" übersetzt. Bereits drei Jahre zuvor hatte sie in einem Magazin über den Volksglauben der Bewohner von Siebenbürgen berichtet.

Auffällig ist der Satz "„Die Toten reiten schnell“", welcher im Roman und in "Draculas Gast" auftaucht. Dieser Satz stammt ursprünglich aus der Ballade "Lenore", was einen Einfluss des Werkes auf Stoker nicht ausschließen lässt.
Beim Aussehen seiner literarischen Figur orientierte sich Stoker u. a. an den Gesichtszügen des Shakespeare-Schauspielers Henry Irving, dessen Agent er war. Warum er dem Widersacher Draculas, van Helsing, ausgerechnet seinen eigenen Vornamen ("Abraham") gab, wäre . Den ungarischen Orientalisten Armin Vambery, mit dem Stoker bekannt war und von dem er auf die Figur des Vlad Țepeș gestoßen wurde, arbeitete der Autor sogar als Verbindungsmann van Helsings in den Roman ein.

Zur Zeit des Romans war das heutige rumänische Transsylvanien (auf deutsch auch Siebenbürgen) ein Teil des Königreichs Ungarn und der k.u.k.-Monarchie Österreich-Ungarn. Jedoch war Stoker in seinem gesamten Leben niemals an den „exotischen“ Orten seines Romans. Er stellte deshalb umfangreiche Nachforschungen an und durchforstete Bibliotheken und Archive, vor allem die des Britischen Museums. Als Unterlagen dienten ihm Militärkarten, Vampirsagen (Sonne/Knoblauch meiden, lange Zähne, Blut trinken) und Berichte englischer Reisender. Seine Recherchen waren so genau, dass selbst die Zugfahrpläne, die im Roman genannt werden, mit der Wirklichkeit übereinstimmten. Diese und andere Daten entnahm er einem damals geläufigen Reiseführer, dem "Baedeker für Österreich-Ungarn", der 1895 bereits in der 24. Auflage erschien. Trotz aller Akribie unterliefen Stoker massive Fehler in Sachen Geographie und Geschichte. So machte er aus Dracula einen Szekler, obwohl der historische Fürst ein Woiwode des Fürstentums Walachei war und eher in den Süd- als in den Nordkarpaten zu verorten gewesen wäre. Auch schilderte er die transsylvanische Landschaft düsterer, als sie in Wirklichkeit ist.

Der österreichische Dokumentarfilm "Die Vampirprinzessin" stellt die Theorie auf, Eleonore von Schwarzenberg, eine Adlige aus dem böhmischen Hause Lobkowitz, solle für Stokers Roman als Inspiration gedient haben, da diese vom Volk als Vampirin gesehen worden sei.

Eine mögliche Inspirationsquelle Stokers könnte auch die als „Blutgräfin“ bekannte adlige Elisabeth Báthory gewesen sein, was jedoch umstritten ist (siehe "Legenden um Elisabeth Báthory").

Die folgende Auflistung bezieht sich jeweils auf das Jahr der Erstveröffentlichung (so weit ermittelbar) der betreffenden deutschen Übersetzung. Die meisten dieser Übertragungen erlebten seither zahlreiche Auflagen bei den verschiedensten Verlagen. Die vollständigsten Übersetzungen sind diejenigen von Stasi Kull (Pseudynom von H. C. Artmann), Karl Bruno Leder und Bernhard Willms.


Am 26. Mai 1897 wurde das Buch in London von „Archibald Constable and Company“ veröffentlicht.

Constable & Robinson veröffentlichte 2012 einen Nachdruck des Romans inklusive Faksimile des originalen Autorenvertrags Stokers mit „Archibald Constable and Company“.

Bereits während der Entstehungszeit des Romans hatte Stoker den Wunsch nach einer Dramatisierung seines Stoffes. Das Licht der Öffentlichkeit erblickte Dracula tatsächlich erstmals in Form einer szenischen Lesung am 18. Mai 1897, die Stoker am Lyceum Theatre veranstaltete, wo er selber als Bühnenmanager tätig war. Einer der Gründe für diese einmalig stattfindende Aufführung war, wie damals üblich, die Sicherung der Dramatisierungsrechte. Stoker verfasste extra für diese Performance eine eigene fünfaktige Lesefassung, samt Prolog, mit verteilten Rollen, die unter dem Titel "Dracula, or The Un-Dead" von fünfzehn Ensemblemitgliedern des Lyceum dargeboten wurde. Den Dracula gab ein Schauspieler namens Mr. Jones, gemeint war wahrscheinlich Thomas Arthur Jones (1871–1954), der somit zum ersten Dracula-Darsteller überhaupt avancierte.

Stoker hoffte in Folge immer, den "Dracula" auch einmal in einer vollgültigen Inszenierung im Lyceum aufführen zu können und den Theaterleiter selbst, seinen Arbeitgeber (und wohl auch heimliches Vorbild für die Dracula-Figur) Henry Irving, für die Hauptrolle zu gewinnen. Aber Irving war der Stoff zu trivial (er bezeichnete ihn als "gräßlich" – englisch: "dreadful"), und er weigerte sich, die Titelrolle zu übernehmen. So musste Stoker diesen Wunsch zunächst aufgeben. Erst Jahre später wurde "Dracula" dann endlich für die Bühne adaptiert und zur Aufführung gebracht.

Hamilton Deane, ein Jugendfreund Stokers, verfasste das Theaterstück "Dracula: The Vampire Play in 3 Acts", das 1925 in Derby Premiere hatte. Es wurde schließlich von John L. Balderston für den US-amerikanischen Markt bearbeitet und zwei Jahre später in New York City mit dem späteren Leinwand-Dracula Bela Lugosi in der Hauptrolle uraufgeführt. In späteren Inszenierungen spielten u. a. Ferdy Mayne und Jeremy Brett die Rolle des gräflichen Vampirs auf der Bühne.

Uraufführung des Musicals am 13. Oktober 2001 im La Jolla Playhouse, San Diego (USA) – "Dracula (Frank Wildhorn)". Die deutschsprachige Erstaufführung fand am 23. April 2005 im Theater St. Gallen (Schweiz) statt.

Im Oktober 2017 wurde an der königlichen Oper in Stockholm die Oper "Dracula" der Komponistin Victoria Borisova-Ollas uraufgeführt.

Der englische Komponist Philip Feeney komponierte 1997 zum 100. Jubiläum des Romans das Ballett "Dracula" für das Northern Ballet Theatre.

1999 komponierte Philip Glass eine neue Musik für Tod Brownings Verfilmung von 1931. Die erste Einspielung der neuen Partitur übernahm das renommierte Kronos Quartet.

2008 veröffentlichte das Nürnberger Gothic Jazz Orchestra das Hörspielkonzert "Der Graf". Die unter Berücksichtigung der historischen Figur Vlad III. Drăculea entstandene Romanbearbeitung wurde mit einer neunteiligen programmmusikalischen Vertonung der Figuren, Orte und Affekte des Romans kombiniert. Komponiert wurde der Zyklus von dem Nürnberger Musiker Andreas Wiersich.

Dracula ist eine regelmäßig auftauchende Superschurkenfigur im Universum der Marvel Comics. (Siehe auch "Figuren aus dem Marvel-Universum" und "Tomb of Dracula").

Der österreichische Autor Reinhard Wegerth ließ Dracula als "Graf Schleckerl" im Wien der Zwischenkriegszeit weiter sein Unwesen treiben (Zeichner: Herbert Pasteiner).

"Auf Draculas Spuren" ist eine dreiteilige Comic-Reihe, geschrieben von Yves Huppen, die sich mit der Entstehung des Mythos beschäftigt.

Es gibt viele Filme, die sich des Motivs des Vampirs bedienen (siehe: Liste von Vampirfilmen). Eine gewisse Bekanntheit erlangten die Darstellungen von Max Schreck (1922), Bela Lugosi (1931), Christopher Lee (1958), Klaus Kinski (1979) und Gary Oldman (1992). Die unten aufgeführten Filme beziehen sich explizit auf den Roman, wenn sie auch der Vorlage in unterschiedlichem Maß treu bleiben. Seit 2013 läuft zudem die US-amerikanische Fernsehserie "Dracula".




</doc>
<doc id="13369" url="https://de.wikipedia.org/wiki?curid=13369" title="Zell im Odenwald">
Zell im Odenwald

Zell im Odenwald ist, neben der Kernstadt, mit knapp 1200 Einwohnern der größte Stadtteil von Bad König im Odenwald. Der Ort liegt im Mümlingtal an der Bundesstraße 45 zwischen Michelstadt und Bad König. 

Zell wurde erstmals 1095 urkundlich als "Cella" erwähnt. Der Ort gehörte bis 1806 zur Herrschaft Breuberg, obwohl er etwas abseits von deren Kerngebiet lag. Dies erklärt sich möglicherweise aus einer langjährigen Verpfändung eines Eppsteiner Anteils an die Schenken von Erbach. 1564 kam es zum erbachischem Amt Michelstadt.

Am 1. August 1972 wurde die Gemeinde Zell im Rahmen der Gebietsreform in Hessen in die Gemeinde (ab dem 10. Oktober 1980 Stadt) Bad König eingegliedert. Wie für jeden Stadtteil wurde ein Ortsbezirk mit Ortsbeirat und Ortsvorsteher eingerichtet.

Das Gasthaus "Zur Krone" in der Königer Straße steht unter Denkmalschutz, ebenso wie die "Papierfabrik Maul" (heute: Jakob Maul GmbH), die auf eine im frühen 18. Jahrhundert von den Erbacher Grafen gegründete Papiermühle zurückgeht, und die ehemalige "Bannmühle".

Der größte Arbeitgeber in Zell ist die Jakob Maul GmbH. Auch das Möbelhaus Kempf gehört zu den größeren Arbeitgebern in Zell.

Zell ist durch seinen Haltepunkt „Bad König-Zell“ an den Regionalverkehr angebunden. Hier halten Bombardier Itinos von Vias auf der Odenwaldbahn, auf der man bis nach Darmstadt sowie bis ins Neckartal fahren kann. In Darmstadt besteht außerdem günstiger Anschluss nach Frankfurt am Main sowie in das restliche Rhein-Main-Gebiet.



</doc>
<doc id="13370" url="https://de.wikipedia.org/wiki?curid=13370" title="Trompete">
Trompete

Die Trompete ist ein hohes Blechblasinstrument, das als Aerophon mit einem Kesselmundstück nach dem Prinzip der Polsterpfeife angeblasen wird. Die Mensur ist relativ eng, ein großer Teil des Rohres ist zylindrisch und der Schalltrichter entsprechend weit ausladend. Die Rohrlänge der am häufigsten vorkommenden B-Trompete beträgt rund 134 cm.

Man unterscheidet Naturtrompeten (wie die Barocktrompete) von den Klappentrompeten und Ventiltrompeten. Ohne Zusatz ist heutzutage die letztere gemeint; es gibt sie mit Drehventilen (auch Zylinderventil bzw. Zylinderdrehventil genannt) oder Pumpventilen (auch Périnet-Ventile genannt). Trompeten werden im Normalfall mit der linken Hand festgehalten, Ringfinger, Mittelfinger und Zeigefinger der rechten Hand betätigen die Ventildrücker. Bei Pumpventilen hält man die Ventilgehäuse etwa senkrecht, bei der Bauweise mit Drehventilen liegen die Ventilzüge etwa waagerecht. Beide Varianten waren zum Ende des 19. Jahrhunderts praktikabel entwickelt. Früher wurden in der Kunstmusik in deutschsprachigen Ländern fast nur Trompeten mit Drehventilen ("Deutsche Trompeten", abgebildet im Kasten rechts) gespielt, während die Trompete mit Pumpventilen "(französische Bauart)" das Instrument in der Unterhaltungsmusik war (daher nannte man sie im deutschsprachigen Raum oft auch "Jazztrompete"). Mittlerweile wird sie wie in den meisten anderen Ländern, aber auch im deutsch-österreichischen Sprachraum parallel zur Drehventil-Trompete im Sinfonieorchester eingesetzt. Kriterium für die Wahl des Instruments ist hierbei das zu spielende Werk bzw. der dafür gewünschte Klang. So werden die Werke von Richard Wagner, Richard Strauss, Gustav Mahler und Anton Bruckner bevorzugt auf der Drehventil-Trompete gespielt („Deutsche Trompete“), während die französischen Impressionisten, Werke der italienischen Oper oder Werke des 20. Jahrhunderts eher auf der Périnet-Trompete gespielt werden („Amerikanische Trompete“).

Périnet-Trompeten sind gegenüber der Drehventil-Trompete mechanisch weniger aufwendig, erfordern aber eine häufigere Ventilpflege. Entscheidend für die Nebengeräusche beim Binden (Legatospiel) zweier Töne ist die Position der Ventile im Rohrverlauf der Trompete. Das erlaubt bei Périnet-Trompeten (Ventilposition: in der Hälfte des Gesamtrohres) die leichtere Generierung verschiedener Effekte durch nur teilweises Drücken des Ventils („schmieren“, „half valve“, „glissando“). Drehventil-Trompeten sind pflegeleichter in der täglichen Anwendung, die Ventile befinden sich im zweiten Zehntel der Gesamtlänge.

Die kleine Piccolotrompete, auch „Hohe Trompete“ genannt, existiert in verschiedenen Stimmungen ("F", "G", "hoch B/A" und "hoch H/C"), sie ist oft mit einem vierten Ventil (Quartventil) ausgestattet, das das Spielen tieferer Töne ermöglicht. Da sie besonders häufig für die Wiedergabe hoher Trompetenstimmen der Barockmusik verwendet wird, wird sie manchmal auch (fälschlich) "Bachtrompete" genannt. Wegweisend auf diesem Instrument waren Adolf Scherbaum, Maurice André, Otto Sauter, Guy Touvron, Reinhold Friedrich und Ludwig Güttler.

Die "Basstrompete" oder Tiefton-Trompete ist eine vergrößerte Form der Trompete im Tonumfang der Posaune und etwas höher. Sie ist historisch in B, C, D und Es-Stimmung, heute hauptsächlich in C üblich, und wird wie jede andere Trompete notiert, also transponierend. Spezialisierte Basstrompeter (bzw. Posaunisten) sind im transponierenden Spiel geübt, können also historische Texte mit wechselnden Stimmungen direkt mit einem C Instrument prima vista wegspielen (ähnlich wie Hornisten).
Die Basstrompete wird meist von Posaunisten gespielt, da sie in gleicher Tonlage der Posaune gebaut wird und ein ähnliches Mundstück wie diese hat. Sie ist ein relativ seltenes Instrument, bleibt aber durch die Wagner- und Strauss-Literatur in Gebrauch. Ein berühmtes Konzertwerk, in dem sie vorkommt, ist "Le Sacre du printemps" von Igor Strawinski (hier allerdings in Es gestimmt, also etwas höher). Aufgrund der Seltenheit hat die Basstrompete keine Solo-Konzerttradition und wird meist als Nebeninstrument bei Posaunisten verlangt.

Eine Taschentrompete sieht zwar kleiner aus, ist aber lediglich kompakter gewickelt als eine herkömmliche Trompete und daher von der Rohrlänge (und damit auch der Tonlage) her eine vollwertige Trompete in B. Allerdings ist der Klang durch den kleineren Schallbecher weniger strahlend als bei den üblichen Bauweisen; die Schallstückmensur entspricht mehr einem Kornett. Diese Instrumente eignen sich eher für Anfänger und/oder den Außeneinsatz, da ihr Schwerpunkt näher am Körper liegt und sie somit leichter über einen längeren Zeitraum zu halten sind.

Der Ton entsteht wie bei allen Blechblasinstrumenten nach dem Prinzip der Polsterpfeife, d. h. die Lippenschwingung des Bläsers erzeugt eine stehende Welle im Instrument. Wesentlich ist die oszillierende Luft im Instrument, und nicht die Luft, die durch das Instrument „geblasen“ wird. Die Schwierigkeit der Tonerzeugung ist durch die Notwendigkeit bestimmt, die Lippenschwingung des Bläsers exakt mit der schwingenden Luftsäule im Instrument zu dieser stehenden Welle zu synchronisieren. Beim klingenden c müssen sich die Lippen ca. 250-mal pro Sekunde öffnen und schließen, beim klingenden c3 sogar 1000-mal – wobei fast nur mehr die Oberlippe diese Schwingung ausführt. Der Tonvorrat der Naturtöne entspricht in etwa der Obertonreihe, wobei die tatsächliche Intonation vom genauen Mensurverlauf des Instruments abhängt und durch den Spieler nur bedingt variiert werden kann. Die Variabilität der Klangvielfalt ist zum einen durch die Schwingungsform der Lippen des Bläsers bestimmt, zum anderen insbesondere durch den Mensurverlauf des Instruments (inklusive Mundstück) geprägt.

Physiologisch sind zum Spiel die Bereitstellung des Luftdrucks von der Lunge relevant, der mittels sog. Stütze kontrolliert wird. Die Stütze ist die Kontrolle der Atemführung mittels Zwerchfell und Bauchmuskulatur (Ausatemmuskeln). Die Kontrolle der Mundmuskulatur erfolgt über die mimische Muskulatur und variiert im Detail bei verschiedenen Spielern. Wichtig sind der Mundringmuskel sowie der Unterlippenherabzieher, Mundwinkelherabzieher und Mundwinkelheber, der Jochbeinmuskel (Lachmuskulatur). Weniger relevant ist jedoch der sog. Trompetermuskel, welcher die Wangen aufbläst und den Namen den historischen trompetenspielenden Barockengel verdankt. Nur wenige Spieler, wie. u. a. Dizzy Gillespie nutzen diesen Muskel intensiv.

Der Anblaswiderstand (empfundener Luftwiderstand) fällt je nach verwendeter Bauweise (Mensurverlauf) unterschiedlich hoch aus. Insbesondere der Durchmesser und Verlauf des Mundstückschafts und des Mundrohres mit unterschiedlich großen Bohrungen (ca. 10,6–11,8 mm) beeinflusst den Luftbedarf und den dadurch unterschiedlich empfundenen Luftwiderstand. Diese unterschiedliche Bauweise ermöglicht, dass bei Jazz-Trompeten häufig Mundstücke mit kleineren Bohrungen und flacheren Kesseln verwendet werden, was einen knackigeren, helleren Ton ergibt. Der typische „Heckel-Klang“ bei Trompeten mit Drehventilen entsteht vorrangig durch Verwendung größerer Kesselmundstücke, weiter Mensur und dünnerer Wandstärke.

Den größten Klangeinfluss hat ebenfalls der Mensurverlauf. Sekundär sind die Materialdicke (Wandstärke) als auch die Materialhärte. Dünnes (0,3–0,45 mm) Schallstück-Blech erfordert eine höhere Härte und mitunter einen am Schalltrichter zur Stabilisierung aufgesetzten 10-40 mm breiten Kranz, der mit ca. 35 mm Breite nach der Dresdner Trompetenwerkstatt „Heckel-Kranz“ benannt ist.
Industriell gefertigte Schallstücke sind bis zu 0,8 mm dick (somit relativ schwer), müssen deswegen nicht so hart sein und haben zur Stabilisierung einen umgebördelten mit Draht eingelegten "französischen Rand".

Die Trompete ist normalerweise ein transponierendes Musikinstrument und wird in verschiedenen Stimmungen gebaut. Am weitesten verbreitet ist das Instrument in "B", gefolgt von "C"-, "D"- und "Es"-Trompeten, selten auch in "A", "E", "H", "F" und in "G". Das bedeutet, dass der Ton, der in den Noten steht und gegriffen wird, durch größere oder kleinere Bauweise des Instruments real um den Abstand höher oder tiefer erklingt als der Abstand des Tons C zum Ton der Stimmung. Wenn man also auf einer Trompete in A ein c spielt, klingt dieses drei Halbtöne tiefer (a), wenn man auf einer Trompete in D ein c spielt, klingt diese zwei Halbtöne höher (d).

Der Tonumfang reicht bei allen Trompeten vom notierten und gegriffenen kleinen ges bis zum c bei fortgeschrittenen Spielern und bis zum g bei Profis. Äußerst begabte, geübte und auf hohe Töne spezialisierte Trompeter schaffen es sogar, Töne der fünfgestrichenen Lage zu spielen. Ebenso ist der Tonumfang nach unten erweiterbar. Mit sogenannten Pedaltönen können die Töne ges bis c mit einem für Trompeten recht untypischen Klang noch um eine Oktave tiefer gespielt werden.

Alle Angaben zum Tonumfang beziehen sich auf Griffweise und Notation. Die genannten Töne klingen dann auf der B-Trompete einen Ganzton tiefer, bei der A-Trompete eine kleine Terz tiefer, bei der C-Trompete wie notiert und bei der D-Trompete einen Ganzton höher etc.

In Orchesterstimmen werden normalerweise die Transpositionen in C oder B verwendet, in älteren Werken, vor allem in Sinfonien und Opern, finden sich oft auch noch andere Stimmungen (meist D, E, Es, und F), die, in der Tradition der Naturtrompete und deren zugrundeliegender Naturtonskala, in der Grundtonart des Stückes stehen. Alle diese Stimmungen werden aber heute meistens mit demselben Instrument gespielt, wobei der Musiker die Stimme im Kopf transponiert. In den USA und vielen anderen Ländern sind dabei im Orchester C-Trompeten die Standardinstrumente, im deutschen Sprachraum wird die B-Trompete häufiger verwendet.

Im Posaunenchor dagegen werden zwar B-Instrumente gespielt, aber klingend notiert (in C, oder auch "Kuhlo-Schreibweise"). Der Posaunenchor-Bläser hat von Beginn an die Ventil-Griffkombinationen dieser Konstellation entsprechend gelernt; im eigentlichen Sinne transponiert er nicht mehr (er muss nicht mehr „umdenken“, sondern verwendet andere Griffe als der Orchestermusiker).

Die verwendeten Grundmaterialien beim Trompetenbau sind:


Als Beschichtungen werden verwendet:
Klarlack, Goldlack, Farblack, Silber, Gold, Nickel.

Das Ventilgehäuse von Périnet-Ventilen wird aus Messing gefertigt, die Ventil-Wechseln aus:


Trompeten können offen oder mit Dämpfer gespielt werden. Außer der Lautstärke wird das Obertonspektrum verändert und somit ein anderer Klangcharakter erzeugt. Beim Spiel mit einer Hand veränderlich ist der Wah-Wah-Dämpfer aus Metall bzw. Kunststoff und der Plunger aus Gummi. Es sind auch spezielle Übungsdämpfer erhältlich, bei denen der Ton nur durch Kopfhörer hörbar ist.

Der Preis einer neuen Trompete beginnt bei etwa 150 € und kann bei aufwendigen Spezialanfertigungen jenseits von 30.000 € liegen. In Mitteleuropa handwerklich gefertigte Instrumente kosten ab ca. 800 €, normale Orchester-Trompeten ca. 1500 bis 3000 €. Bei der Anschaffung eines Neuinstrumentes ist fachlicher Rat unabdinglich. Gerade für Anfänger ist es wichtig, technisch gut funktionierende und sauber intonierende Instrumente auszuwählen. Dabei ist zu beachten, dass selbst baugleiche Modelle oft unterschiedliche Spiel- und Klangcharakteristiken aufweisen können.

Die Fertigungszeit einer Trompete beträgt ca.


Von der Lage her eng verwandt mit der Trompete sind das Flügelhorn und das Kornett. Sie werden auch meistens von Trompetern gespielt, gehören aber aufgrund ihrer Mensur zur Hornfamilie. Eine weitere Variation der Trompete ist die oben erwähnte Basstrompete, welche sich in der Tenorhornstimmlage bewegt.

Das Flügelhorn hat eine weitgehend konische Mensur. Es ist im Klang deutlich weicher als die Trompete. Zwischen Trompete und Flügelhorn liegt das Kornett. Sein Klangcharakter ist etwas weicher als der der Trompete, jedoch härter als der des Flügelhorns.

Lediglich namentlich (und etwas in der Bauform) ist die Martinstrompete verwandt, deren Ton jedoch nicht von schwingenden Lippen des Musikers erzeugt wird.

Schon die Ägypter spielten vor 3500 Jahren trompetenartige Instrumente (snb/sprich: scheneb) aus Metall, die gleiche Bauform dürfte auch die altjüdische Chazozra gehabt haben. Frühe Trompeten, wie die griechische Salpinx oder die Römische Tuba, waren langgestreckt und ohne Windungen. Die Hakenform des römischen Lituus und des keltischen Carnyx entstanden vermutlich durch die Verbindung eines geraden Rohrs mit einem krummen Tierhorn als Schallbecher. In der Form eines großen G war das römische Cornu gewunden. Die immer paarweise verwendeten Luren der Germanen erhielten ihre Form wahrscheinlich durch die Nachahmung von Mammutstoßzähnen. Vorrangig verwendetes Material für antike Blechblasinstrumente war Bronze, die im Wachsausschmelzverfahren bearbeitet wurde.

Ob die Kunst des Rohrbiegens von der Antike durch das Mittelalter tradiert wurde oder im Abendland neu entdeckt werden musste, ist nicht mit Sicherheit geklärt. Frühmittelalterliche Trompeten waren gestreckt. Gewundene Formen sind erst ab dem 14. Jahrhundert in ikonografischen Quellen nachzuweisen. Schriftlich wurde der Begriff "Drometten" bereits 1470 (in einem Dokument in Pirna) oder später "Drommete" (von Martin Luther bei seiner Bibelübersetzung zum "Prophet Jesaja 18,3") verwendet. Als Standardform bildete sich im ausgehenden Mittelalter die einmal gewundene Langtrompete heraus, die als Barocktrompete bis zum Ende des 18. Jahrhunderts praktisch unverändert blieb. Der Tonumfang dieser Instrumente war auf die Naturtonreihe beschränkt, deren vierte Oktave (Clarinregister) eine vollständige Skala bereitstellt. Die Beherrschung dieser hohen Lage, das sogenannte Clarinblasen, gilt als die hohe Kunst barocker Trompetenmusik. Die Existenz einer mittelalterlichen Zugtrompete ist nach wie vor umstritten.

Vom ausgehenden Mittelalter bis ins Barock galt die Trompete als Herrschaftssymbol. Trompeter waren hoch angesehene Beamte. Nur zu hohen kirchlichen Festen war es erlaubt, dass sie mit anderen Musikern zusammen zu anderen Zwecken spielten.

Den Übergang zu Ventiltrompeten markieren zahlreiche „chromatisierte“ Varianten:


Die aus Schlesien stammenden Musiker Heinrich Stölzel (Hornist) und Friedrich Blühmel erfanden 1813 die so genannten Drehbüchsenventile, die den Luftstrom umleiten. Seit 1830 ist die heutige Bauform mit drei Ventilen bekannt (C. A. Müller, Mainz und F. Sattler, Leipzig).
Das heute in der sogenannten Deutschen Trompete gebräuchliche Drehventil erhielt seine endgültige Ausformung vermutlich 1832 bei Joseph Riedl in Wien.

Bereits 1831 wurden Ventile in ein Posthorn eingebaut, das damit zum "Cornet à pistons" („Horn mit Ventilen“) wurde. Mit der 1837 patentierten Weiterentwicklung durch Gustave Auguste Besson wuchs die Beliebtheit dieses Kornetts in der Militär- und Salonmusik zu seiner bis heute gängigen Bauform.

Der durch das Kornett begonnene Verdrängungsprozess der Trompete aus den Orchestern wurde ab etwa 1890 wieder zugunsten der Trompeten umgekehrt. Die heute gebräuchliche B-Trompete, die zuvor bereits 1828 in die preußische Militärmusik eingeführt wurde, hielt Einzug als Orchestertrompete und löste ab 1860–70 die (tiefere) F-Trompete ab. Obwohl z. B. der Militärkapellmeister Johann Heinrich Saro wiederholt betonte, dass B-Trompeten nicht den fülligen und kernigen Klang der F-Trompete hätten, sondern eher ähnlich dem Cornet à pistons klängen, nahmen viele Komponisten wenig Rücksicht auf die Möglichkeiten der Instrumente. Folglich spielten die Musiker immer öfter auf der sicherer zu handhabenden B- oder C-Trompete. Zunächst nur die hohen Spieler, dann auch immer mehr die zweiten und dritten Spieler. Nikolai Rimsky-Korssakow versuchte zwar die F-Trompete als "Tromba alta" wieder im Orchester zu etablieren – einzelne russische Komponisten, u. a. Schostakowitsch, taten dies ebenso – aber sie mussten die Partien immer so einrichten, dass diese auch auf der nunmehr gängigen B-Trompete spielbar blieben.

Trompeten (oder ähnliche Instrumente) wurden praktisch immer auch als Signalinstrumente verwendet. Beispielsweise mussten alle Türmer in den Städten (Signal-) Trompete spielen können. Daraus resultierte eine hohe gesellschaftliche Anerkennung der "Trompeter", die sich meist auch finanziell bemerkbar machte. Oftmals waren die Türmer/Trompeter bereits seit dem Mittelalter als Berufsgruppe anerkannt – anders als alle anderen Musiker. Verdi ließ für seine Oper Aida eine spezielle Fanfarentrompete, die Aida-Trompete, herstellen.

Auch im "Militär-Handwerk" spielten die Trompeter seit jeher als Signalgeber "(Datenüberträger)" eine wichtige Rolle.

In den letzten Jahrzehnten ist das Spielen der Trompete sehr populär geworden. Die Nachfrage nach Instrumenten wird heute meist durch große Betriebe mit industrieller Instrumentenproduktion gedeckt. Natürlich gibt es nach wie vor eine Reihe von Blechblasinstrumentenbaumeistern in kleineren Firmen, die in der Lage sind, individuell dem Musiker angepasste Instrumente zu bauen.

Solokonzerte für Trompete wurden in der späten Barockzeit sehr beliebt; sie sind zumeist in sehr hoher Lage, da hier auch mit einer Barocktrompete diatonisches bzw. im Falle der Zugtrompete chromatisches Spiel möglich ist. Vivaldi, Telemann, Scarlatti, Michael Haydn und Bach (2. Brandenburgisches Konzert) sind hier zu erwähnen. Händel und Bach haben hohe Trompetensoli auch in ihren Oratorien, Messen und Kantaten verwendet.

Ein bekanntes frühklassisches Trompetenkonzert ist jenes von Leopold Mozart. Für die 1790 erfundene Klappentrompete schrieben Joseph Haydn und Johann Nepomuk Hummel ihre Konzerte, die bis heute zu den populärsten Werken der Gattung gehören. Wichtige Solokonzerte für die Ventiltrompete komponierten Alexander Arutjunjan, Henri Tomasi oder André Jolivet.

In der klassischen Kammermusik ist die Trompete aus zwei Gründen nicht stark vertreten: Zum einen, weil sie als eher lautes Instrument dazu tendiert, die mitmusizierenden Instrumente (wie ein Streichquartett) zu übertönen, zum anderen, weil sie lange Zeit auf das begrenzte Repertoire der Naturtöne beschränkt war. Aus der Renaissance- und Barockzeit allerdings gibt es eine Anzahl von Kammersonaten, die entweder original für Zink komponiert sind oder die viel höhere Lage bevorzugen, die auch Bach für seine Werke verwendete.

Mit der Einführung der Ventile entstanden auch neue Werke, etwa für Trompete mit Klavierbegleitung, aber auch in gemischter Kammermusik des 20. Jahrhunderts kommen manchmal Trompeten vor, etwa in Bohuslav Martinůs "Revue de Cuisine". Um 1950 wurde das Blechbläserquintett populär, für das viele Neukompositionen oder Bearbeitungen geschaffen wurden.

Im Barockorchester (vor allem bei Bach) sind, wenn überhaupt, dann zumeist zwei bis vier Trompeten (in der Regel mit Pauken) besetzt, die, oft in höchster Lage, heroische und göttliche Harmonien versinnbildlichen (zum Beispiel in Bachs "Magnificat").

Im klassischen und frühromantischen Orchester (also vor der Entwicklung der Ventile) hatten die zwei Trompeter hauptsächlich die Aufgabe, im Tutti (oft gemeinsam mit einem Paar Pauken) Grundtöne zu spielen. Manchmal wurden sie aber auch thematisch eingesetzt, etwa bei Themen aus Quarten oder Dreiklangszerlegungen, welche daher auch nur mit den Naturtönen der Naturtrompete spielbar sind.

Die Ventiltrompete (zunächst das Kornett) wurde von den meisten Komponisten aller Länder begeistert aufgenommen und sofort eingesetzt. Für den deutschsprachigen Raum sind hier besonders Richard Wagner, (auf den im Übrigen die Holztrompete zurückgeht), Anton Bruckner, Richard Strauss (zum Beispiel seine Alpensinfonie) und Gustav Mahler (etwa am Anfang der 5. Sinfonie) zu erwähnen.

Im sinfonischen Blasorchester der Gegenwart sind bis zu vier verschiedene Trompetenstimmen vorhanden, die von ca. zehn Musikern gespielt werden.

Im Jazz ist die Trompete neben dem Saxophon wohl das wichtigste Blasinstrument. Schon in der klassischen New-Orleans-Jazz-Besetzung ist sie Melodieinstrument, auch aus der Big Band ist sie nicht wegzudenken. Wichtige Jazz-Trompeter finden sich auf der Liste von Jazzmusikern und im Artikel Jazztrompeter. Im Jazz wird besonders viel mit Dämpfern gearbeitet und fast nur auf Trompeten mit Pumpventilen gespielt.

In der Popmusik wird die Trompete oft in einem Bläsersatz mit Posaune und Saxophon verwendet. Vor allem in der Soul- und Funk-Musik setzen die trompetendominierten Bläsergruppen wichtige Akzente an der Nahtstelle zwischen Melodie und Rhythmusgruppe. Wichtige und stilbildende Vertreter dieses Genres sind Blood, Sweat & Tears, Chicago und Earth, Wind & Fire. Ein aktuelles Beispiel dafür ist der Retro-Sound des Farin Urlaub Racing Teams.




</doc>
<doc id="13371" url="https://de.wikipedia.org/wiki?curid=13371" title="Korruption">
Korruption

Korruption (von ‚ Verderbnis, Verdorbenheit, Bestechlichkeit‘) bezeichnet Bestechlichkeit, Bestechung, Vorteilsannahme und Vorteilsgewährung. Im juristischen Sinn steht Korruption für den Missbrauch einer Vertrauens­stellung in einer Funktion in Verwaltung, Justiz, Wirtschaft, Politik oder auch in nichtwirtschaftlichen Vereinigungen oder Organisationen (zum Beispiel Stiftungen), um für sich oder Dritte einen materiellen oder immateriellen Vorteil zu erlangen, auf den kein rechtmäßiger Anspruch besteht.

Der Politikwissenschaftler Harold Dwight Lasswell definierte Korruption als destruktiven Akt der Verletzung des allgemeinen Interesses zu Gunsten eines speziellen Vorteils.

Nach einer Schätzung des Internationaler Währungsfonds werden weltweit durch Korruption umgerechnet 1,3 bis 1,75 Billionen Euro verschlungen. Das schwächt das globale Wirtschaftswachstum um ungefähr zwei Prozent. In aktuellen Wirtschaftswachstums­modellen gilt Korruption wie auch Geldwäsche als einer der langfristigen und nachhaltigen Wachstumsverhinderer.

In den feudalen europäischen Flächenstaaten des 18. Jahrhunderts wurde Korruption systematisch praktiziert. Friedrich II. bestach Minister am Hof von Kaiserin Maria Theresia und ging davon aus, dass diese ihrerseits seine Minister bestach. Diplomaten hatten gewissermaßen ein Anrecht darauf, bestochen zu werden. Beamte des preußischen Hofes waren Diener des Königs, die sich von sogenannten Sporteln zu ernähren hatten, Vergütungen in Geld oder Naturalien, die der Empfänger der Dienstleistung zu entrichten hatte. Bis zum Ende des Kaiserreiches erhielten preußische Beamte nur etwa zwei Drittel des Gehaltes, das sie zur Finanzierung desjenigen Lebensstils benötigten, der von ihnen auf Grund des Dienstranges erwartet wurde. Zum Ausgleich gab es Beihilfen, eine Einrichtung, die sich noch heute im Besoldungswesen findet, und zusätzlich Erlaubnis zum Nebenerwerb, die allerdings auch eingeschränkt war. Ein Erlass des preußischen Königs verbot seinen Beamten das Fiedeln in Kaschemmen. Die Vollbesoldung von Beamten ist in der jüngeren europäischen Vergangenheit eine fortschrittliche französische Erfindung.

Bekannt ist auch Bismarcks Reptilienfonds, eine Geldquelle, mit deren Hilfe er losgelöst von parlamentarischen Regularien seine politischen Ziele finanzieren konnte.

Der "Habsburger Effekt" bezeichnet jüngst wissenschaftlich statistisch-belastbar nachgewiesene Zusammenhänge zu Osteuropa zwischen ehemaligem habsburgischem Gebiet und heute dort lebenden Menschen und deren geringere Neigung zu Korruption, Bestechung bzw. höheren Vertrauen in Verwaltung, Polizei und Gerichtsbarkeit im Vergleich zu Menschen auf der nicht-habsburgischen Seite der ehemaligen Grenze.

Nach Schätzung des Internationalen Währungsfonds verursacht Bestechung jedes Jahr 2 Billionen Dollar an Schäden. Die OECD rechnet sogar mit Schäden von bis zu 4 Billionen Dollar. Damit verbunden sind massive negative Auswirkungen für das Wirtschaftswachstum und die staatlichen Gemeinwesen. Auf globaler Ebene gelten daher die Bekämpfung von Korruption und die Schaffung von Rechtssicherheit (z. B. durch Vertrags- und Registersicherheit, Formvorschriften, unabhängige und effektive Gerichte bzw. Verwaltung) als Schlüssel zu Innovation, Produktivität und Wohlstand.

Kernelement korrupten Verhaltens ist das Ausnutzen einer Machtposition für einen persönlichen Vorteil unter Missachtung universalistischer Verhaltensnormen, seien es moralische Standards, Amtspflichten oder Gesetze. Korruption ist eine soziale Interaktion, bei der die Beteiligten vorteilhafte Leistungen austauschen, beispielsweise Entscheidungsbeeinflussung gegen Geld. Im Gegensatz zu „win-win“ werden dabei die Auswirkungen auf beteiligte Dritte ausgeblendet oder übergangen.

Korruption ist eine Art Kompensationsgeschäft, aber mehr als nur ein Tausch zwischen zwei Akteuren zu ihrem gegenseitigen Vorteil.


Das Strafrecht kennt keine übergreifende Korruptionsstrafvorschrift, sondern sanktioniert das mit Korruption verbundene Unrecht in verschiedenen Straftatbeständen. Der deutsche Gesetzgeber hat den Begriff Korruption weder definiert noch im Strafrecht benutzt.

Relevante strafrechtliche Korruptionsdelikte sind insbesondere


Damit gehen in der Regel Straftatbestände einher nach

Des Weiteren sind auch für Deutschland die Strafnormen des Gesetzes zur Bekämpfung internationaler Bestechung (IntBestG) und des EU-Bestechungsgesetzes (EUBestG) relevant.

Das BKA bezieht sich auf die kriminologische Forschung, nach der „Korruption“ zu verstehen ist als „Missbrauch eines öffentlichen Amtes, einer Funktion in der Wirtschaft oder eines politischen Mandats zugunsten eines anderen, auf dessen Veranlassung oder Eigeninitiative, zur Erlangung eines Vorteils für sich oder einen Dritten, mit Eintritt oder in Erwartung des Eintritts eines Schadens oder Nachteils für die Allgemeinheit (in amtlicher oder politischer Funktion) oder für ein Unternehmen (betreffend Täter als Funktionsträger in der Wirtschaft)“.

Transparency International definiert Korruption als Missbrauch von anvertrauter Macht zum privaten Vorteil. ("Corruption is operationally defined as the misuse of entrusted power for private gain.")

Diese Definition steht im Gegensatz zu der oben zitierten alten amerikanischen Definition von Harold Dwight Lasswell: „…violations of the common interesst for special advantage are corrupt“. Sie wurde seinerzeit als Richtlinie für den Entwurf des amerikanischen Anti-Korruptionsgesetzes „Foreign Corrupt Practices Act“ von 1977 und (erst) zwanzig Jahre später der OECD – angewendet. In beiden wird die Verantwortung für die Straftat (Korruption) auch und insbesondere bei den Initiatoren im eigenen Lande gesehen.
Indem Lasswell lediglich den bewusst herbeigeführten Schaden an einer öffentlichen oder privatrechtlichen Gemeinschaft zur Selbstbereicherung als Korruptions-Kriterium identifiziert, bezieht er den aktiven und den passiven Partner gleichermaßen mit ein – vor dem Hintergrund, dass fast in allen Fällen der „Aktive“ Hauptnutznießer und Initiator von Korruption ist.

Nach der Definition von TI jedoch, wird der Missbrauch der eigenen Position als einziges Kriterium herausgestellt. Die Philosophie dahinter ist, dass nur derjenige, der die Korruption ermöglicht (annimmt), im juristischen Fokus stehen müsste. Letztendlich impliziert TI, dass, weil von der „aktiven“ Privatindustrie keine Moral zu erwarten ist (vgl. W. B.), deshalb der „passive“ Amtsträger die volle Verantwortung zu tragen habe.

Diese „moderne“ Sichtweise hatte in den 1990ern „Kritik an Transparency International“ ausgelöst. Insbesondere auch, weil TI just in derselben Legislaturperiode (1991–1994) ins Leben gerufen wurde, als im Bonner Parlament die Korruption gegen Amtsträger der Europäischen Süderweiterung explizit legitimiert und gegen die Empfehlungen der UN und der OECD entsprechende Schmiergelder ins Ausland nunmehr als steuerabzugsfähig zugelassen wurden (siehe „Schattenwirtschaft – eine anschwellende Quelle für Schmiergeld“).

Im aktuellen TI Jahresbericht für 2015 gibt es dem Korruptionswahrnehmungsindex CPI zufolge mehr Länder, in denen im Vergleich zum Jahr 2014 weniger Korruption wahrgenommen/empfunden (engl. to percept) wurde, als solche, bei denen die Korruption im Vergleich zum Vorjahr zugenommen hatte. Die Dänen nehmen sich dem Index zufolge als Spitzenreiter unter den korruptionsärmsten Ländern der Welt wahr. Deutschland befindet sich auf Platz 10 und hat sich nach eigenem Empfinden somit verbessert: Im Jahr 2014 belegte Deutschland noch Platz 12 in der Selbstwahrnehmung der Geschäftswelt. Insgesamt empfindet sich die weltweite Geschäftswelt als weniger korrupt im Vergleich zu Beginn der Finanzkrise.

Über das tatsächliche Ausmaß der Korruption, allerdings, sagt der CPI nichts aus. Er gibt aber Geschäftsleuten durchaus einen Eindruck für die Sensibilität, wie schädlich Korruption auf dem nationalen Markt empfunden wird und ob deshalb der politische Wille vorherrsche, evtl. dagegen vorzugehen. Als Beispiel sei die jahrzehntelang gängige Schmiergeld-Praxis in der Bundesrepublik Deutschland genannt, wobei Parlamentarier von der Strafverfolgung ausgeschlossen waren, wenn sie für spezifische politische Entscheidungen nachträglich entlohnt wurden. Nur wenn die Entlohnung vorher entrichtet wurde, war sie juristisch als Korruption verfolgbar. Mit dieser ungewöhnlich „liberalen“ Regelung hatte die deutsche Geschäftswelt jahrelang die OECD-Konvention gegen die Korrumpierung von ausländischen Amtsträgern (1997) unterlaufen und sich selbst trotzdem einen extrem hohen CPI (Platz 10-20) vergeben. Kurioserweise selbst zu jener Zeit, als Schmiergelder legal von der Steuer absetzbar waren (§ 4 Abs. 5 Nr. 10 EStG, gültig bis 19. März 1999). Erst 2014 wurde die legale Korruption auf Druck der UN abgeschafft, sodass Deutschland als letzter Staat auch die UNCAC erfüllte. Jetzt (2015) fühlen sich deutsche Geschäftsleute noch „korruptionsärmer“.

Laut Bundeslagebild 2011 Korruption des Bundeskriminalamts (BKA) haben die polizeilich gemeldeten Korruptionsverfahren mit 1.528 im Vergleich zu 2010 (1.813) um fast 16 Prozent abgenommen. Diese Verfahren beinhalten jedoch eine Vielzahl von Einzelstraftaten. So haben sich die Korruptionsstraftaten von 2010 (15.746) 2011 etwa verdreifacht; mit 47.795 Straftaten wird der höchste Wert seit 1995 ausgewiesen.
Dabei haben die Verstöße gegen die Strafnormen der StGB (Vorteilsannahme und Vorteilsgewährung) um mehr als das Doppelte und der StGB (Bestechlichkeit und Bestechung) um rund das 2,4-fache zugenommen. Die Delikte der StGB (Bestechlichkeit und Bestechung im geschäftlichen Verkehr) verzeichnen eine Zunahme um das 3,5-fache. Zurückzuführen ist dieser Anstieg vor allem auf zwei umfangreiche Verfahren in Nordrhein-Westfalen gegen Mitarbeiter eines Automobilherstellers und gegen zivile Angestellte der Britischen Rheinarmee. Es sind mehr als 25.800 Einzeldelikte im geschäftlichen Verkehr.

Mit einem Anteil von 56 Prozent der polizeilich bekannt gewordenen Straftaten lag der Schwerpunkt auch 2011 im Bereich der Privatwirtschaft; der Anteil der öffentlichen Verwaltung betrug 35 Prozent. Die verbleibenden 9 Prozent verteilen sich auf Strafverfolgungs- und Strafvollzugsbehörden (8 Prozent) und den Bereich der Politik (1 Prozent).

Rund 40 Prozent der „Geber“ (BKA-Jargon für Bestechende bzw. Vorteilsgewährende) waren Geschäftsführer, rund 15 Prozent leitende Angestellte. Auf der „Nehmer“-Seite betrafen die korruptiven Handlungen mit rund 65 Prozent die Sachbearbeiter- und mit rund 30 Prozent die Leitungsebene.

Die Art der Vorteile für die „Nehmer“ bestand 2011 zu 41 Prozent aus der Entgegennahme von Bargeld, zu 34 Prozent aus Sachzuwendungen, der Rest bezog sich auf Bewirtungen, Feiern, Urlaub, Reisen (s. auch Abschnitt "Welche Vorteile wurden verschafft?"). Die „Geber“ profitierten zu rund 46 Prozent von der Erlangung von Aufträgen, zu 19 Prozent von (vom BKA nicht näher beschriebenen) sonstigen Wettbewerbsvorteilen; 9 Prozent entfielen auf behördliche Genehmigungen, 7 Prozent auf interne Informationen und anderes.

Gemäß BKA wurden für das Jahr 2011 monetäre Schäden von insgesamt rund 276 Millionen Euro gemeldet gegenüber 176 Millionen Euro 2010 (Zunahme rund 56 Prozent). Das BKA weist aber ausdrücklich darauf hin, dass „Aussagen zur monetären Dimension des verursachten Gesamtschadens nur sehr schwer getroffen werden (können), da gerade die durch Erlangung von Genehmigungen oder Aufträgen verursachten finanziellen Schäden in der Regel nur vage darstellbar sind“ und „daher ein Gesamtbild zum tatsächlichen Ausmaß der verursachten Schäden nur eingeschränkt abgegeben werden“ kann.

Das Bundeskriminalamt (BKA) unterscheidet zwischen situativer und struktureller Korruption. Die Gießener Kriminologin Britta Bannenberg hat aufgrund ihrer Aktenauswertung von Korruptionsfällen drei Strukturen ermittelt: Bagatell- oder Gelegenheitskorruption, Struktur der gewachsenen Beziehungen und Netzwerk-Korruption.

Situative Korruption sind Korruptionshandlungen, die spontan aus einer Situation begangen werden (daher auch "Spontankorruption" genannt); sie sind also nicht geplant oder vorbereitet. Ausschlaggebend ist die günstige Gelegenheit
(daher Bannenberg: "Gelegenheitskorruption". Der Begriff "Bagatellkorruption" ist unglücklich gewählt; Korruption ist in keinem Fall eine Bagatelle). Gemäß den Untersuchungen von Bannenberg ist das Geschehen auf zwei oder wenige Personen beschränkt und nicht auf Wiederholung angelegt.

"Beispiel: Ein alkoholisierter Autofahrer bietet dem ihn kontrollierenden Polizisten 100 € an. Der Autofahrer hofft, von einer Strafverfolgung verschont zu werden und dabei ohne Punkte wegzukommen, um im Besitz seines Führerscheines zu bleiben."

Bei der Strukturellen Korruption liegen der korruptiven Handlung langfristig angelegte („gewachsene“) Beziehungen zugrunde; sie wird bereits vor der Tatbegehung geplant und vorbereitet. Häufig wird sie durch „Anfüttern“ eingeleitet, d. h. durch kleinere Geldgeschenke oder andere geringe Vorteile wird getestet, wie weit der Vorteilsnehmer anfällig und bereit ist, auch zukünftig korrupt zu handeln. Höhe und Häufigkeit der Zuwendungen werden im Laufe der Zeit gesteigert. Laut Bannenberg ist ein derartiges Vorgehen räumlich und personell beschränkt; es spielt sich regional im Wirtschaftsbereich des Vorteilsgebers ab.

"Beispiel: Es werden vierteljährlich „wissenschaftliche Veranstaltungen“ organisiert, bei denen Professoren, die z. B. Gutachten für die Zigaretten-Industrie erstellen, für Reden bzw. Vorträge, deren Inhalte seit Jahren bekannt sind, enorme Honorare gezahlt werden."

Netzwerk-Korruption sind nach Bannenberg „umfangreiche Straftaten, die in vielen Fällen der organisierten Kriminalität zugeordnet werden können.“ Geber sind viele Personen, Nehmer nur wenige. Die Delikthandlungen werden häufig über mehrere Jahre, teilweise jahrzehntelang regional, überregional oder international, begangen.

Diese Art der Korruption gehört zur Strategie, die von manchen Mitarbeitern von Unternehmen oder Körperschaften eingesetzt wird; sie ist (meistens) mit weitergehenden Straftaten wie zum Beispiel Betrug, Untreue, Erpressung oder Steuerhinterziehung verbunden. Das BKA nennt – unabhängig von der Korruptionsart – weitere mit Korruption zusammenhängende Tatbestände, insbesondere Urkundenfälschung, wettbewerbsbeschränkende Absprachen bei Ausschreibungen, Strafvereitelung und Falschbeurkundung im Amt, Verletzung des Dienstgeheimnisses und Verstöße gegen strafrechtliche Nebengesetze.

"Beispiel (überregional): Der Leiter einer Baubehörde trifft sich mit Geschäftsführern bestimmter Bauunternehmen mal zu Arbeitsessen in Edelrestaurants, mal zu Segeltouren, zu privaten Golfturnieren o. Ä., um – wie nach außen deklariert – sich über die wirtschaftliche Lage allgemein und die Auftragslage speziell auszutauschen, tatsächlich aber über demnächst anstehende Aufträge und mögliche Angebote zu sprechen und Konditionen und Preise abzusprechen."

Britta Bannenberg hat bei der Auswertung von Strafakten Korruption festgestellt:

Im Sinne der StGB erfolgt Korruption


Das Annehmen von Vorteilen und das sich als bestechlich „Bereitzeigen“ ist im Grunde immer auch eine aktive Handlung.

Der „Vorteilsnehmer“ fordert oft aktiv einen Vorteil ein – dies kann bis zu einer Erpressung gehen. Potenzielle Auftragnehmer werden dann vor die Alternative gestellt: Ohne Schmiergeld kein Auftrag oder keine Amtshandlung, z. B. keine Baugenehmigung. Geforderte Vorteile sind im Sinne des Abs. 3 StGB immer strafbar. Eine Genehmigung führt nicht zur Straffreiheit.

Im Falle der Vorteilsannahme gemäß StGB nimmt der Amtsträger den Vorteil an, auf den er keinen Rechtsanspruch hat – quasi als ein Äquivalent für seine Dienstausübung. Es muss keine rechtswidrige Diensthandlung vorliegen.

Bestechlichkeit und Bestechung im Sinne der und StGB gehen immer mit einer Dienstpflichtverletzung des „Nehmers“ einher, also einer rechtswidrigen Diensthandlung. Die Diensthandlung kann auch in einem Unterlassen bestehen.

Da die §§ 331 ff. StGB auch Vorteile umfassen, die Dritten gewährt werden, können auch das Sponsoring oder die Spendengewährung "(siehe auch Parteispende)" an öffentliche Körperschaften oder Parteien ein Einfallstor für Korruption sein. Sponsoring und Spenden erfüllen grundsätzlich den objektiven Tatbestand einer Vorteilsgewährung im Sinne der §§ 331 ff. StGB.

Wer Amtsträger ist, bestimmt sich nach Abs. 1 Nr. 2 StGB. Keine Amtsträger sind die Mitglieder kommunaler Vertretungskörperschaften (Stadtrat, Gemeinderat), es sei denn, sie werden mit konkreten Verwaltungsaufgaben betraut, die über ihre Mandatstätigkeit in der kommunalen Volksvertretung und den zugehörigen Ausschüssen hinausgehen. Der BGH sieht hier allerdings gesetzgeberischen Handlungsbedarf. In allen anderen Bereichen des öffentlichen und privaten Lebens hat das gewandelte öffentliche Verständnis einer besonderen Sozialschädlichkeit von Korruption zu einer erheblichen Ausweitung der Strafbarkeit von korruptem Verhalten geführt. Diese Entwicklung ist bislang an dem Tatbestand der Abgeordnetenbestechung vorbeigegangen.
Der Straftatbestand des StGB wird deshalb vielfach als praktisch bedeutungslose „symbolische Gesetzgebung“ angesehen, die mit der Überschrift nur auf den ersten Blick – und namentlich der Öffentlichkeit – vortäuscht, dass Abgeordnete unter dem Gesichtspunkt der Bestechungsdelikte den Amtsträgern wenigstens annähernd gleichgestellt wären. Bisher ist erst einmal ein Abgeordneter nach dieser Norm verurteilt worden.

Dritter im Sinne der StGB kann auch die eigene Anstellungskörperschaft des Amtsträgers sein. Ein Straftatbestand ergibt sich etwa dann, wenn Sponsoring/Spenden gekoppelt werden mit Auftragserteilungen/Vertragsabschlüssen der empfangenden Verwaltungseinheit (Unrechtsvereinbarung).

Zu korrupten Handlungen gehören auch – allerdings nicht in strafrechtlicher Hinsicht – jene Stellenbesetzungen in Verwaltungen und öffentlichen Unternehmen, die unter parteipolitischen Gesichtspunkten erfolgen: Ämterpatronage, Nepotismus (Vetternwirtschaft), Klientelismus.

Mit einem Anteil von 41,1 Prozent war Bargeld 2011 auf der Nehmerseite der häufigste Vorteil, gleich gefolgt von Sachzuwendungen mit 34 Prozent (2010 zusammen 97 Prozent).

"Beispiele: Geld (sogenanntes „Schmiergeld“) : Übergabe von Bargeld oder Überweisungen auf ein Tarnkonto oder Bezahlung für Scheingeschäfte: Scheinlieferung, -gutachten oder –beratung bzw. wertvolle Sachgeschenke wie Eintrittskarten zu bedeutenden Kultur- oder Sportereignissen oder Überlassen von Kfz, Jacht, Ferienwohnung und kostenlose oder kostengünstigere Leistungen, z. B. in einer Werkstatt, Hausbau, Gartenpflege u. Ä."

Weitere Vorteile bezogen sich auf Bewirtung/Feiern (8,5 Prozent), Reisen und Urlaub (4,3 Prozent) und Teilnahme an Veranstaltungen (3 Prozent).

"Beispiele: Finanzierung von Jubiläen und Geburtstagen, Karibik-Kreuzfahrt, Südseeurlaub, Ermöglichen des Zutritts zu exklusiven Events. Zu den sonstigen Vorteilen gehören Rabatte, Nebentätigkeiten, außerordentlich hoher Rabatt beim Autokauf, Verschaffung einer lukrativen Nebentätigkeit, Bordellbesuch", "spätere Karriere bzw. gutbezahlter Job für den Bestochenen oder für einen seiner Angehörigen".

Die monetären Vorteile der Nehmer 2011 beziffert das BKA mit 120 Mio €.

Auf der Geberseite bildete auch 2011 die Erlangung von Aufträgen mit einem Anteil von 46,3 Prozent das häufigste Ziel des korruptiven Handelns. Mit weitem Abstand folgen: die Erlangung sonstiger Wettbewerbsvorteile (19 Prozent), behördlicher Genehmigungen (8,5 Prozent) oder interner Informationen (7,5 Prozent).

"Beispiele: (hauptsächlich) Bauaufträge, Manipulation bei offenen Ausschreibungen, Bevorzugung sogenannter „Hoflieferanten“ (bei beschränkten Ausschreibungen), Genehmigung von Subventionen, Neu-, An- und Ausbauten, Gastwirtkonzessionen, Verrat von Razzien u. Ä."

Das BKA nennt Anteile von weiteren Vorteilen: Absatz von Medikamenten (5,3 Prozent), Beeinflussung von Strafverfolgungsbehörden (3,9 Prozent) und von Vorteilen unter 3,5 Prozent: Bezahlung fingierter und gefälschter Rechnungen, Erlangung von Aufenthalts- und Arbeitserlaubnissen, Gebührenersparnis, u. a.

Für 2011 hat das BKA die monetären Vorteile der Geber mit 195 Mio € ausgewiesen.

Korruption und Korruptionsbekämpfung sind heute sowohl in Industriestaaten als auch in Entwicklungsländern zentrale Themen. Dieser wichtige Bereich des oft staatlichen Versagens zum Schutz der Bevölkerung, der Wirtschaft und des Gemeinwesens bewirkt in vielen Ländern wegen der diesbezüglichen massiven Auswirkungen einen Zorn der Massen gegen die Regierenden und andere Eliten. International gesehen untergraben die mangelnden Fortschritte bei der Korruptionsbekämpfung die Rechtsstaatlichkeit und den Glauben an die Demokratie.

Im Bereich der öffentlichen Verwaltung und der Justiz führt Korruption einerseits zu hohen materiellen Schäden und anderseits aber auch zu immateriellen Auswirkungen wie Vertrauensverlust der Bürger in staatliche Organe. So kann es beispielsweise zu Auftragsvergaben an Unternehmen kommen, obwohl sie teurere oder schlechtere Leistungen erbringen als solche Unternehmen, die bei einer objektiven und transparenten Ausschreibung ausgewählt würden. Die den Amtsträgern gewährten Vorteile werden in der Regel bei der Rechnungsstellung eingerechnet. Deshalb werden dann Leistungen abgerechnet, die entweder gar nicht oder nicht in dem ausgewiesenen Umfang erbracht wurden. Die finanziellen Lasten hat letztlich der Steuerzahler zu tragen. Dabei gilt, dass eine Ausnutzung öffentlicher Positionen zum privaten Vorteil gemeinwohlwidrig ist.

Im Gesundheitswesen führt Korruption einerseits zu überhöhten Preisen und andererseits erschwert sie den Zugang zu medizinischen Leistungen. Weiterhin kann sie dazu führen, dass sich Therapieformen oder Medikamente etablieren, die objektiv betrachtet keine medizinisch optimale Behandlung darstellen. Sogar an Pflegepersonen während der Behandlung verabreichte Trinkgelder bestechen, wenn sie Vorteile bis hin zur Zwei-Klassen-Medizin verursachen können. In vielen Ländern gehört die Zahlung von Bestechungsgeldern in Krankenhäusern zum Alltag, um beispielsweise einen OP-Termin zu bekommen.

Generell führt Korruption dazu, dass die Leistungen von Organisationen in ihrem Umfang abnehmen oder qualitativ schlechter werden, die dafür zu entrichtenden Geldbeträge aber steigen. Nach Angaben der Weltbank muss durchschnittlich jeder Mensch rund sieben Prozent seiner Arbeitsleistung für Korruptionsschäden aufbringen.

Einen Sonderfall stellt die zwischenstaatliche Korruption dar, wobei auf der einen Seite der aktiv Korrupte und auf der anderen Seite sowohl passiver Partner, als auch Geschädigter nicht zur gleichen Jurisdiktion gehören. In solchen Fällen wird im Geltungsbereich des eigenen Gesetzes weder ein Beamter korrumpiert, noch der Staat geschädigt. Dieses Quasi-Outsourcing der Korruption wurde in Vorbereitung des Euro insbesondere in der BRD der 1990er praktiziert und hatte Kritik der OECD heraufbeschworen, zumal die damalige Kohl-Regierung die Korruption deutscher Konzerne in die Europäische Süderweiterung bewusst durch Steuererleichterungen mitfinanzierte. Neben den oben beschriebenen Schäden an deren Volkswirtschaften, müssen sich die Opferstaaten den Vorwurf gefallen lassen, nicht genug gegen Korruption vorzugehen.

Während sich also die nationale Korruption lediglich als Ungerechtigkeit im Wettbewerb zwischen großen und kleinen zeigt – liquide Unternehmen haben in Sachen Korruption einen klaren Wettbewerbsvorteil – insgesamt aber die Volkswirtschaft nicht nennenswert schwächt, so stellt die zwischenstaatliche Korruption eine Gefahr für juristisch unvorbereitete Volkswirtschaften (siehe Griechenland) dar, die in Freihandelszonen wie z. B. dem Europäischen Binnenmarkt vertraglich gebunden sind, auf jegliche protektionistische Gegenmaßnahme zugunsten ihrer Volkswirtschaft zu verzichten (Maastricht 1993).

Infolge der erheblichen rechtlichen Unterschiede bei der nationalen Bewertung bzw. Verfolgung von Korruption und dem damit verbundenen enormen Vermögen hat sich eine international geheim arbeitende Vermögensbewahrungs- und Consultingindustrie entwickelt. Diese Beratungsunternehmen und Offshoring-Anbieter dienen natürlich vorwiegend nicht dem Zweck der legalen Steueroptimierung oder Wirtschaftsberatung, sondern zur Umgehung von Vorschriften und einer Vielzahl krimineller Aktivitäten wie Geldwäsche und Korruption. Diese Unternehmen schaffen sich durch Nutzung von Steueroasen und Ausnutzung aller möglichen Lücken ihr eigenes Rechtssystem und betreiben zusätzlich massive Lobby-Arbeit zur Eröffnung neuer Schlupflöcher und zur Abschaffung von Straftatbeständen beziehungsweise Formvorschriften.

Gemäß Daten der Europäischen Kommission aus dem Jahr 2014 schädigt Korruption die Wirtschaft in der Europäischen Union pro Jahr um 120 Milliarden Euro. Die Lage ist jedoch in den einzelnen Mitgliedsstaaten sehr unterschiedlich. Als Spitzenreiter der Korruption erweisen sich Griechenland, Italien, Kroatien, Zypern, Spanien und Bulgarien. In Österreich fügte im Jahr 2014 Korruption der Wirtschaft einen Schaden von 27 Milliarden Euro zu und hemmte das Wirtschaftswachstum erheblich. Gleichzeitig rutschte die Republik Österreich auf den 16. Platz des Korruptionsindex von Transparency International ab und wird dem Land bei der Korruptionsbekämpfung gehöriger Aufholbedarf bescheinigt.

Während der IWF die globalen jährlichen Schäden durch Korruption auf rund 2000 Milliarden Dollar taxiert, kommt die OECD sogar auf den Betrag von 4000 Milliarden Dollar. Im Jahr 2012 haben Schwellen- und Entwicklungsländer laut einer Studie durch Korruption, Geldwäsche und Handelsbetrug eine Billion Dollar verloren und wächst das Volumen illegaler Geldflüsse doppelt so schnell wie das Wirtschaftswachstum.

Der Vorteil des Korrumpierten ist meist nur ein Bruchteil des Schadens an der Organisation, die ihn beschäftigt oder beauftragt hat. Dieser ist im Ausmaß eher mit dem Nutzen des Angreifers, also des aktiv Korrupten zu vergleichen. Gewinnorientierte Unternehmen sind daher darauf bedacht, die Korrumpierung ihrer Mitarbeiter zu verhindern. Es zeigt sich ein grundsätzliches Dilemma: Einerseits liegt es im Interesse der Unternehmen, die passive Korruption ihrer Mitarbeiter zu unterbinden, da diese sie schädigt. Andererseits möchten sie selbst in die aktive Korruption „investieren“, um sich gegenüber anderen Marktakteuren durchzusetzen und z. B. durch Bestechungsgelder lukrative Aufträge zu akquirieren.

Dieses Dilemma besteht nur dann nicht mehr, wenn Korruption ausnahmslos aller Unternehmen in diesem Markt effektiv unterbunden oder legal und flächendeckend praktiziert wird. Das Dilemma besteht also gerade darin, dass ein einzelnes Unternehmen in der Situation, in der alle anderen nicht korrumpieren, die höchsten Gewinne aus Korruption einfahren kann. Je weniger korrumpieren, desto größer der Marktanteil der Wenigen. Jedes einzelne Unternehmen hat also einen extrem hohen Anreiz zu korrumpieren, sei es um höhere Gewinne zu machen oder um nicht der einzige „Dumme“ zu sein, der nicht korrumpiert und deshalb aus dem Markt ausscheiden muss. Das Dilemma weist darauf hin, dass durch eine unvollständige Bekämpfung der Korruption, der erwünschte Qualitäts- und Preiswettbewerb auf dem Markt – im Extremfall durch ein einziges „schwarzes Schaf“ – gekippt werden kann.

Es weist aber auch den Weg zu einer weiteren Möglichkeit der Bekämpfung. Indem nämlich nicht moralisch an die Unternehmen appelliert wird, ihren Profit zu missachten, sondern indem ihr Profitstreben zur Bekämpfung der Korruption genutzt wird. Dies kann geschehen durch eine Änderung der Dilemmasituation selbst. Wenn Korruption bekämpft werden soll, müssen den Unternehmen auch Anreize geboten werden, dies tatsächlich auch umzusetzen. Hierzu werden in der Wissenschaft verschiedene Vorschläge diskutiert, z. B. die Einführung des Unternehmensstrafrechts in Deutschland oder die Umsetzung von Kronzeugenregelungen in Korruptionsfällen. Während es in den meisten europäischen Ländern schon ein Unternehmensstrafrecht gibt, wurde in Deutschland erst 2014 ein entsprechender Gesetzesvorschlag durch das Land Nordrhein-Westfalen eingebracht.

Manchmal wird behauptet, dieses Korruptionsdilemma bestehe bei öffentlichen Unternehmen nicht, da zum einen die Absicht der Subventionierung privater Unternehmen eine ebenso wichtige Rolle spielt wie die bloße eigene Gewinnerzielung und zum anderen der potentielle Konkurs durch eine Unterstützung der öffentlichen Hand abgewendet werden kann. Dies stimmt aber nicht vollständig. Tatsächlich haben öffentliche Unternehmen nur wenige Anreize, andere Unternehmen zu bestechen. Das gilt aber nicht für alle Mitarbeiter dieses Unternehmens. Wenn ein einzelner Mitarbeiter z. B. hohe Boni erhält für die Akquise von Aufträgen, dann hat dieser einen Anreiz, dafür u. U. auch korrupte Mittel zu verwenden, ist aber als einzelner Angestellter in seinen finanziellen Möglichkeiten weitgehend eingeschränkt.
Umgekehrt aber sind natürlich gerade öffentliche Unternehmen und deren Mitarbeiter in hohem Maße dafür anfällig, selbst bestochen zu werden. Gerade weil keine Gewinnerzielungsabsicht besteht, hat die Kontrolle über die effiziente Mittelverwendung, z. B. bei der Vergabe von Bauaufträgen, nicht dasselbe Ausmaß wie in privaten Unternehmen. Die Möglichkeiten für korrupte Akteure, sich durch Korruption Renten anzueignen, ist in solchen Unternehmen daher höher. Ein Argument für mehr öffentliche Unternehmen aus Gründen der Prävention passiver Korruption lässt sich daher nur schwer finden – ganz im Gegenteil zur aktiven Korruption, die traditionell eine Domäne der Privatwirtschaft ist.

Damit keine Korruption stattfindet, muss nach der Prinzipal-Agent-Klient-Theorie entweder bei der Gewährung der Macht (Unterbindung der Prinzipal-Agent-Interaktion) oder beim Handel zwischen Agent und Klient (Unterbindung der Agent-Klient-Interaktion) eingegriffen werden. Um die Agent-Klient-Interaktion zu unterbinden, ist sie mit Strafe zu bedrohen, und mögliche Agent-Klient-Interaktionen sind transparent zu machen.

International wird versucht, durch dokumentierende Formvorschriften für Rechtsgeschäfte bei Liegenschaften, Unternehmenskapital oder Unternehmensanteilen, was zwar nicht spannend jedoch effektiv ist, das einfache und intransparente Verschieben von Kapital und die damit verbundene Verschleierung und Geldwäsche zu verhindern. In vielen Ländern müssen bestimmte Rechtsgeschäfte wie Veränderungen in Firmenstrukturen, Kapitalverschiebungen bei Unternehmen, Immobilien oder Firmenregistrierungen (- die für Geldwäsche, Verschleierung oder Korruption geeignet sein können) gerichtlich oder notariell protokolliert werden, um Scheingeschäfte, Fälschungen oder Rückdatierungen von Verträgen zu verhindern.

Auch die vielfach vorgeschriebene Legitimationspflicht bei Bankgeschäften soll die Agent-Klient-Interaktionen transparent machen. In der internationalen Praxis werden aber Identifikationsverfahren, Identitätsprüfungen und diesbezügliche Vorschriften durch die Finanzdienstleister und Banken geflissentlich übersehen. Vielmehr werden bankinterne Compliance-Einrichtungen angehalten, bei vorliegenden kriminellen Aktivitäten wegzuschauen bzw. diese zu verschleiern.

Ein geeignetes Land (Steueroase oder Offshoring) für die mit der Korruption verbundene Geldwäsche und Verschleierung hat am besten anonyme Gesellschaftsformen (z. B. British Trusts, Scottish Limited Partnerships oder US-Mantelgesellschaften), keinen Willen zur internationalen Kooperation und keine Formvorschriften, sodass die diesbezüglichen Unterlagen nicht öffentlich sind und auch rückwirkend verändert werden können.

Die Einhaltung der zur Bekämpfung der Korruption vorgesehenen Dokumentations- und Formvorschriften bedingt die Mitarbeit staatlicher oder privatwirtschaftlicher Organe. Nur wenn diese Organe rechtlich und wirtschaftlich unabhängig vom korrupten Täter sind und diese Unabhängigkeit ausreichend abgesichert ist, können Korruption und die damit verbundenen Handlungen unterbunden werden. International gesehen sind diese gerichtlichen, behördlichen bzw. notariellen Organe oder auch Compliance-Abteilungen in Unternehmen oft schlecht bezahlt, weisungsgebunden, nicht (wirtschaftlich) unabhängig oder bewusst schlecht organisiert, um die Einhaltung der Bestimmungen nicht überwachen zu können. Ob es grundsätzlich positive Auswirkungen von Compliance-Abteilungen, die in der Unternehmensstruktur eingebettet sind, zur Verhinderung von rechtswidrigen, aber lukrativen Handlungen im Rahmen des Unternehmenszweckes geben kann, wird kontrovers diskutiert, denn Unternehmen maximieren grundsätzlich ihren Eigennutzen. In vielen Ländern versuchen Unternehmerverbände, aber auch Politiker, die zur Korruptionsbekämpfung geeigneten Dokumentations- bzw. Formvorschriften als „wirtschaftsfeindlich“ zurückzudrängen. Der Wirtschaftsnobelpreisträger Joseph Stiglitz und der Basler Strafrechtler Mark Pieth weisen darauf hin, dass bei der Bekämpfung von Korruption aufgrund umfangreicher Lobbyarbeit nicht konsequent vorgegangen wird und effektive Dokumentationsvorschriften dadurch verhindert wurden. Vielfach ist die Finanzindustrie zielgerichtet sehr damit befasst, Gesetzesänderungen und deren Folgen, auch in Bezug auf Steueroasen, in ihrem Sinn zu beeinflussen.

Die Bekämpfung von Korruption fokussierte sich in Deutschland klassischerweise bislang vor allem auf die (straf-)rechtliche Perspektive. Immer mehr stellen jedoch auch andere Fachdisziplinen Überlegungen an, um der mannigfaltigen Korruption Einhalt zu gebieten. Durch internationale Übereinkommen versuchen die Vertragsstaaten, verpflichtende Strafrechtsbestimmungen zur Bekämpfung von Korruption zu schaffen. Transparency International gibt ein "Corruption Fighters’ Tool Kit" heraus. Massive Schwachstellen im Kampf gegen Korruption sind die fehlende Transparenz bei vielen Rechtsgeschäften, der Bankensektor und vor allem die Steueroasen. Zur Korruptionsbekämpfung gehören aber nicht nur griffige gesetzliche Bestimmungen oder internationale Abkommen, vielmehr müssen die schon bestehenden Vorschriften wie zum Registerrecht, zu Formvorschriften bzw. die Legitimationspflicht korrekt und effektiv vollzogen werden.

Beim Lobbyismus gibt es ähnliche Interaktionen wie die Agent-Klient-Interaktionen. Diese müssen jedoch nicht notwendigerweise in Korruption münden. Lobbycontrol ist ein Verein in Deutschland, der über Einflussnahme von Interessengruppen (Klient) auf Staatsorgane (Agent) berichtet.

Korruption wird durch verschiedene Umstände gefördert beziehungsweise ihre Bekämpfung behindert:


Seit 1995 gibt die Nichtregierungsorganisation (NGO) Transparency International alljährlich den Korruptionswahrnehmungsindex "(Corruption Perceptions Index, CPI)" heraus. Dieser Korruptionsindex will das Ausmaß der Wahrnehmung von Korruption in verschiedenen Ländern der Erde darstellen. Er entfaltet mittlerweile eine breite Wirkung in der Öffentlichkeit, die somit für das globale Problem der Korruption sensibilisiert wird.

Das tatsächliche Ausmaß der Korruption in einem Land ist jedoch nicht mit wirtschaftswissenschaftlichen Methoden einzugrenzen. Deshalb zieht sich der CPI notgedrungen auf die Einschätzung von Beteiligten innerhalb der Klientelwirtschaft zurück, wobei lediglich deren subjektive „Wahrnehmung“ (engl. „perception“) im Sinne von „Empfindung“ im Gegensatz zur definitiven (und mutigen) „Feststellung“ (engl. „experience, detection, identification, diagnosis…“) von Korruption im eigenen Land mittels Umfragen gesammelt wird.

Das Ergebnis solcher Umfragen ist aber weit mehr von der Psychologie des eigenen wirtschaftlichen Erfolgs des Befragten abhängig und eher selten von seinem Mut, Korruption zuzugeben. So ist es nicht verwunderlich, dass in sog. Exportländern die Korruption kaum (als schädlich) wahrgenommen wird, wo oft die effiziente Akquisition von Staatsaufträgen traditionell durch hochentwickelte und wegen ihrer gewaltigen Schattenwirtschaften äußerst liquide Klientelnetzwerke gesichert werden. Offensichtlich überwiegt in diesen Fällen der Nutzen für (fast) alle.

Begünstigt wird die nationale „CPI-Befindlichkeit“, wenn das betreffende Land hoch industrialisiert ist, wo ein wesentlicher Anteil der Gesellschaft unselbständiger Erwerbstätigkeit nachgeht und deshalb die Aktivitäten innerhalb der Klientelnetzwerke gar nicht wahrnehmen kann. Einen noch günstigeren Einfluss auf den CPI übt eine nationalistische Gesetzgebung aus, die einerseits die Korruption im Inland in einem vorgegebenen Maße kontrolliert, sie aber andererseits explizit straffrei ausgehen lässt, wenn sie sich ausschließlich gegen andere Volkswirtschaften richtet. Solcher Art Ermächtigungsgesetze für die eigene Privatwirtschaft sind besonders katastrophal, wenn sie – wie nach der EWG-Süderweiterung 1981/86 und dem EG-Binnenmarkt 1993 in Deutschland geschehen – durch milliardenschwere Steuersubventionen vom Staat co-finanziert werden.

Insofern berücksichtigt der CPI weder die tatsächlich „gemessene“ Korruption noch die kriminelle Energie in den Exportländern. Stattdessen stellt er lediglich einen Wahrnehmungsindex des entstandenen Schadens in den Opferländern dar, nicht aber, wie oft falsch zitiert, das Ausmaß der „nationalen“ Korruption. Wie könnte er auch? Schließlich ist Korruption in einer globalisierten Welt ja „international“. Siehe dazu 1977 Foreign Corrupt Practices Act, 1999 OECD Anti-Bribery Convention, 2005 United Nations Convention against Corruption.

In deutschen Großstädten werden zunehmend Schwerpunkt-Staatsanwaltschaften gegen Korruption ins Leben gerufen – dies vor dem Hintergrund, dass die Korruptionsbekämpfung ein extrem kompliziertes Unterfangen ist, das eine Bündelung von Ressourcen und Kompetenzen erfordert. Die Staatsanwaltschaft München I unterhält die zurzeit größte Anti-Korruptionsabteilung in Deutschland. Sie hat der Öffentlichen Hand, u. a. der Stadtverwaltung München, von 1994 bis 2004 zur Realisierung von 46 Mio. EUR Schadensersatz verholfen – ein Ausmaß, welches insoweit indes nur das Hellfeld von Korruption dokumentiert; der tatsächliche Schaden (inkl. Dunkelfeld) geht vermutlich weit darüber hinaus.

Im Bereich der privaten Unternehmen werden zunehmend wettbewerbsneutrale Selbstverpflichtungen einzelner Branchen (z. B. in der Bauindustrie) initiiert. Diese setzen sich das kollektive Ziel, der Korruption im Zuge eines umfassenden Ethik-Managements eine Absage zu erteilen. Auch die Wirtschaftsethik/Unternehmensethik befasst sich in neuerer Zeit speziell mit Optionen der Korruptionsprävention und -bekämpfung. Als Beispiel ist insoweit die wissenschaftliche Publikation von "Pies und andere" zu nennen.

Nicht zuletzt gibt es auch für die öffentliche Verwaltung erste Ansätze eines expliziten Anti-Korruptionsmanagements. Grundlagen aus Sicht der Verwaltungsethik liefert etwa das Werk von Thomas Faust "Organisationskultur und Ethik. Perspektiven für öffentliche Verwaltungen".
Allmählich halten auch explizite Antikorruptionskonzepte Einzug in die Verwaltungspraxis – etwa in das Bezirksamt Berlin-Spandau, das für seinen innovativen Ansatz bereits eine Prämierung erhielt. In Nordrhein-Westfalen werden entsprechende Aktivitäten durch ein Korruptionsbekämpfungsgesetz gefördert (u. a. mittels Korruptionsregister (Vergabe-/Verfehlungsregister), Informations-/Anfragepflichten, Vieraugenprinzip, Personalrotation).

In Anlehnung an die Ombudsregelung des Bezirksamtes Berlin-Spandau hat die Stadt Hemer in NRW zur Verstärkung der Antikorruptionsstelle im Revisionsamt der Stadtverwaltung eine freiberufliche Rechtsanwältin zur „Ombudsfrau gegen Korruption“ mit Wirkung ab 1. März 2009 ernannt.

Korruption-Hotline:
Die Korruption-Hotline ist eine Verbindung zu einer Spezialeinheit des Landeskriminalamtes zur Korruptionsbekämpfung in Nordrhein-Westfalen. Die Sondereinheit hat im April 2004 ihre Arbeit aufgenommen.

Antikorruptionsbeauftragter:
In Schleswig-Holstein nimmt seit 2007 der frühere Landespolizeidirektor Wolfgang Pistol das Amt eines Antikorruptionsbeauftragten wahr. In dieser Stellung soll er gesetzlich unabhängig sein; Hinweisgeber sollen anonym bleiben dürfen, wenn sie es wünschen.

Hinweisgeber, anonyme Anzeige:
Ihre Kenntnisse können Hinweisgeber den Strafverfolgungsbehörden auch anonym übermitteln, da nach dem Legalitätsprinzip die Polizei auch solchen Strafanzeigen nachgehen muss. Mit dieser Vorgehensweise können sich Wissensträger vor eventuellen Folgen ihrer Anzeige schützen. Das Landeskriminalamt Baden-Württemberg nutzt zur Korruptionsbekämpfung ein anonymes Hinweisgeberportal. Die aktuelle Adresse wird auf der Homepage der Polizei Baden-Württemberg angegeben. Bei der Hinweisabgabe kann ein Hinweisgeber einen Postkasten einrichten. Über den Postkasten kann das Landeskriminalamt Baden-Württemberg dem Hinweisgeber Fragen stellen. Die Anonymität der Hinweisgebers wird dabei gewahrt.

In Anlehnung an den Foreign Corrupt Practices Act der USA hat die Internationale Handelskammer (ICC) 1977 als Vertretung der Weltwirtschaft erstmals einen umfassenden Bericht über Korruption im Geschäftsverkehr mit Handlungsempfehlungen veröffentlicht. Der Bericht rief internationale Organisationen und nationale Regierungen dazu auf, das Thema auf ihre Agenda zu setzen. Heute gibt es eine Reihe internationaler Übereinkommen, die gegen Korruption und Bestechung wirken sollen.

Diese, am 15. Februar 1999 ratifizierte Konvention schreibt den Teilnehmernationen (darunter Deutschland) vor, strafrechtliche Maßnahmen gegen Bestechung ausländischer Amtsträger (darunter Abgeordneten) vorzuschreiben. Ebenfalls wird die steuerliche Absetzbarkeit von Bestechungsgeldern untersagt.

In Deutschland erfolgte die Umsetzung mit dem IntBestG vom 10. September 1998.

Das Strafrechtsübereinkommen über Korruption vom 17. Dezember 1997 des Europarates trat am 1. Juli 2002 in Kraft. Es beinhaltet weitergehende Forderungen zur Korruptionsbekämpfung. Die Konvention ist von der Schweiz ratifiziert, allerdings bisher nicht von Deutschland und Österreich.

Das Übereinkommen der Vereinten Nationen gegen Korruption (UNCAC) ist am 16. September 2005 in Kraft getreten.

Österreich hat diese Konvention am 11. Januar 2006 ratifiziert.

In der Schweiz wurde nach der Unterzeichnung am 10. Dezember 2003 die parlamentarische Behandlung der Konvention am 21. September 2007 mit einer Botschaft des Bundesrates eingeleitet.

Deutschland hat die Konvention am 9. Dezember 2003 unterzeichnet. Wegen fehlender strafrechtlicher Vorschriften gegen Abgeordnetenbestechung ( StGB a.F.) konnte diese jedoch lange nicht ratifiziert werden. Erst am 21. Februar 2014 verabschiedete der Bundestag in Zweiter und Dritter Lesung ein Gesetz zur Verschärfung der Regeln gegen die Abgeordnetenbestechung, welches am 1. September 2014 in Kraft getreten ist. Am 25. September 2014 stimmte der Bundestag einstimmig für die Ratifizierung, am 10. Oktober 2014 stimmte der Bundesrat zu. Die Ratifikation erfolgte am 12. November 2014, so dass das Übereinkommen am 12. Dezember 2014 für Deutschland, als eines der weltweit letzten Länder, in Kraft trat (Art. 68 Abs. 2 des Übereinkommens).




Literatur:

Statistiken:

Korruptionsbekämpfung:


Ethik, Organisationen:


</doc>
<doc id="13372" url="https://de.wikipedia.org/wiki?curid=13372" title="Tonart">
Tonart

Eine Tonart wird im Rahmen der seit etwa 1600 etablierten Dur-Moll-Tonalität durch die Feststellung des Tongeschlechts (Dur oder Moll) und des Grundtons der verwendeten Tonleiter bestimmt.

Beispiel: Tongeschlecht "Dur" plus Grundton "D" ergibt die Tonart "D-Dur".

Die denkbare alternative Definition über die Festlegung von Grundton und Art der verwendeten Tonleiter wäre problematisch, weil den drei verschiedenen Formen der Molltonleiter (natürlich, melodisch, harmonisch) nicht drei, sondern nur eine Molltonart entspricht. Das Tongeschlecht ist also entscheidender als die Struktur der Leiter.

Dies gilt jedoch nur, solange das traditionelle Dur-Moll-System nicht verlassen wird. Bezieht man z. B. modale Tonleitern mit ein, ändern sich die Verhältnisse.

Tonale Musikstücke stehen in einer bestimmten Tonart, der sich ihre wichtigsten Abschnitte (vor allem der Schluss, oft auch der Anfang) zuordnen lassen. Mit Kompositionsmethoden wie Modulation und Rückung können die Tonarten innerhalb eines Stücks wechseln, wobei sie meist irgendwann zur Haupttonart zurückführen, so dass diese in der Regel innerhalb eines Stückes dominiert und damit seinen musikalischen Charakter mitbestimmt.

Die Tonart eines Stückes kann insgesamt transponiert werden, indem ein anderer Grundton gewählt und alle Töne des Stückes im gleichen Abstand zu den Originaltönen versetzt werden, so dass ihre Intervalle zueinander und damit das Tongeschlecht unverändert bleiben. Dadurch ändert sich der wesentliche Charakter des Stückes also nicht. Transponieren ist üblich und legitim, etwa um ein Stück der Stimmlage von Sängern oder Grundstimmung von Instrumenten anzupassen. In der Kunstmusik wird jedoch zum einen seit etwa 1700 die Tonart oft ausdrücklich festgelegt und im Namen des Stücks genannt; somit ist die angegebene Tonart wesentlich für den vom Komponisten gewünschten Charakter des Stückes und damit für seine Aufführung. Andererseits werden bis zur Vorklassik unterschiedliche nicht gleichstufige Stimmungen verwendet. Im Barockzeitalter wurden zudem mehrere Abhandlungen über den jeweiligen Tonartencharakter veröffentlicht.

Die übliche europäische Notation geht von den sieben Stammtönen der C-Dur-Tonleiter aus (a, h, c, d, e, f, g) und bezeichnet alle davon abweichenden Tonstufen der gewünschten Tonart mit Hilfe von Versetzungszeichen (Kreuze oder Bes). Mit der Tonart eines Stückes sind auch die darin in Relation zu C-Dur versetzten Tonstufen von vornherein festgelegt, so dass sie als Vorzeichen zu Beginn des Notensystems jeder Zeile notiert werden und damit die reguläre gleichbleibende Versetzung dieser Tonstufen für die Gesamtdauer eines Stückes oder Abschnitts markieren. In Verbindung mit dem Schlusston und/oder Schlussakkord geben diese Vorzeichen also einen Hinweis auf die Tonart, in der dieses Stück oder dieser Abschnitt stehen.

Die Art und Anzahl der Vorzeichen ergibt sich aus der Entfernung der jeweiligen Tonart von der Ausgangstonart C-Dur, wie sie durch die Anordnung aller Tonarten im Quintenzirkel ersichtlich wird. Dabei bezeichnet jede Vorzeichen-Variante jeweils eine Dur-Tonart und die dazugehörige parallele natürliche Molltonart. Ein Stück ohne Vorzeichen kann also in C-Dur oder in a-Moll stehen; ein Stück mit einem Kreuz in G-Dur oder e-Moll, eins mit einem Be in F-Dur oder d-Moll usw. Eine verlässliche Entscheidung kann meist nur mit Blick auf den Schlusston (und/oder Schlussakkord) getroffen werden, der fast immer mit dem Grundton identisch ist (oder ihn enthält).

Auch die Modi werden mit Hilfe von Vorzeichen notiert; hier können bestimmte Vorzeichen jedoch je nach dem Grundton desselben Tonvorrats verschiedene Modi bezeichnen. Eine Tonleiter mit zwei Kreuzen zum Beispiel, die die Töne von D-Dur enthält, kann ausgehend vom Grundton e E-Dorisch, ausgehend vom Grundton a A-Mixolydisch, vom Grundton g dagegen G-lydisch sein.

Andere Tonleitern als Dur, natürliches Moll und Kirchentonarten – etwa Harmonisch-Moll oder Tonleitern aus osteuropäischer, jüdischer oder arabischer Musik – werden nicht durch reguläre Vorzeichen zu Beginn des Notensystems, sondern durch jeweils vor Einzelnoten gesetzte Versetzungs- oder Auflösungszeichen notiert, die von den Tonstufen einer zugrunde gelegten Dur- oder Molltonleiter abweichen. Hierin spiegelt sich wider, dass Dur und Moll in der neuzeitlichen abendländischen Musik als Regel, andere Tonleiterarten als Ausnahmen betrachtet werden.

In der freitonalen und atonalen Musik wird in der Regel auf eine globale Angabe von Vorzeichen am Beginn eines Stückes ganz verzichtet.

Ohne Vorzeichen

Kreuz-()-Tonarten (rechte Seite des Quintenzirkels):

B-()-Tonarten (linke Seite des Quintenzirkels)

Um sich die Reihenfolge der Tonarten in Abhängigkeit von den Vorzeichen zu merken, gibt es Merksprüche wie die folgenden:

Für Tonarten, die # enthalten:
Geh, Du Alter Esel, Hole Fische.

Oder Tonarten, die b enthalten:
Fiebrige Buben Essen Aspirin, Deshalb Gesund.

Die Tonarten Cis-Dur/ais-Moll, Ces-Dur/as-Moll mit jeweils sieben Vorzeichen werden in Kompositionen nur selten verwendet. Die Tonarten Gis-Dur, Dis-Dur, Ais-Dur, des-Moll, ges-Moll und ces-Moll werden normalerweise nicht verwendet, weil ihre Notation mehr als sieben Kreuze oder Bes erfordern würde. Stattdessen setzt man sie mithilfe der enharmonischen Verwechslung mit einer jeweils gleich klingenden, aber weniger Vorzeichen erfordernden Be- oder Kreuztonart. Beispielsweise wird Cis-Dur (sieben Kreuze) auf diese Weise zu Des-Dur (fünf Bes), Des-Moll (acht Bes) zu Cis-Moll (vier Kreuze) usw. Auf Tasteninstrumenten sind die enharmonischen Unterschiede nicht darstellbar, sodass die Gleichsetzung der Tonarten absolut ist. Dies gilt nicht für intonierende Tonerzeuger wie z. B. Streichinstrumente oder die menschliche Stimme.

Die Schreibweisen der Tonarten variieren in drei Komponenten:
Die einflussreichen deutschen Wörterbücher – wie der Duden oder der Wahrig – empfehlen eine Schreibweise von Groß/Kleinschreibung des Tonnamens mit Bindestrich und großgeschriebenem Tongeschlecht, beispielhaft: für Durtonarten "A-Dur" und für Molltonarten "a-Moll". Dabei betont die Großschreibung der Tongeschlechtsbezeichnungen, dass diese häufig substantivisch verwendet werden (z. B. „Modulation nach Moll“). Dies bedeutet eine Abkehr von der früheren Auffassung, nach der man "dur" und "moll" eher als nachgestellte Adjektive verstand und entsprechend vorzugsweise "A dur" und "a moll" schrieb. Die unterschiedliche Groß- und Kleinschreibung der Grundtonbezeichnungen ("A" bei Dur, "a" bei Moll) entspricht der großen Dur- und der kleinen Mollterz und etablierte sich als Konvention bereits zu Beginn des 19. Jahrhunderts. Der Vorteil dieser Konvention ist, dass man verkürzt "Sonate in a" (für a-Moll), "Sonate in A" (für A-Dur) schreiben kann, was besonders in englischsprachigen Ländern weit verbreitet ist.

Die heutige Standardschreibweise – "A-Dur" und "a-Moll" – wurde bereits 1911 von Arnold Schönberg in seiner "Harmonielehre" konsequent verwendet. Der »Duden, wechselte allerdings erst im verbesserten Neudruck der 14. Auflage 1958 von der vorher vertretenen Kleinschreibung zum „großen Dur“«.

Davor und daneben waren und sind jedoch auch alternative Schreibweisen im Gebrauch, wie zum Beispiel:

Auch heute begegnet man variierenden Schreibweisen, was verschiedene Gründe haben kann (unter anderem dem Folgen einer bestimmten Tradition oder ästhetischen Vorliebe, die Anlehnung an ausländische Schreibweisen oder Unkenntnis der Regeln).

Obwohl der Begriff Tonart meist im oben beschriebenen strikten Sinne verwendet wird, ist er darüber hinaus auch ein umfassenderer Begriff für den harmonischen Bedeutungszusammenhang, in dem sich ein Stück bewegt.

Tonarten haben keine scharfen Begrenzungen. Man könnte also nicht exakt sagen, welche Töne zu einer Tonart gehören und welche nicht. Es ist der harmonische und besonders der melodische Zusammenhang, welcher den Ausschlag gibt. Dies gilt besonders dann, wenn keine Festlegung durch eine Notenschrift vorliegt und man nach dem Gehör entscheiden muss.

Obwohl sich Tonarten durch den Gebrauch ihrer Tonleitern deutlich hervorheben, tauchen in jedem anspruchsvolleren Stück auch gehäuft Töne außerhalb der Tonleitern auf, ohne dass man bereits von einem Tonartwechsel sprechen würde.

Eine Tonart wird zu einem erheblichen Teil durch das Vorkommen charakteristischer Wendungen in Form von Progressionen, Melodien und Kadenzen bestimmt, die gemeinsam auf ein tonales Zentrum hinweisen.

Seit etwa 1900 werden neben den Dur- und Molltonleitern im Rückgriff auf die alten Kirchentonarten auch wieder verstärkt modale Tonleitern wie Dorisch, Lydisch u. a. verwendet. Die mit ihrer Hilfe gebildeten Tonarten können nicht durch bloße Angabe von Tongeschlecht und Grundton gekennzeichnet werden, es sei denn, man fasst diese Tonleitern selbst als Tongeschlechter auf, die zu Dur und Moll hinzutreten.

Diese gelegentlich vertretene Auffassung verbietet sich jedoch, weil im System der Kirchentöne diese als Tonarten ("species") aufgefasst wurden, während als Tongeschlechter ("genera") "Cantus durus" und "Cantus mollis" galten. Ebenso wenig wie man das harmonische Moll als ein vom natürlichen Moll unterschiedenes Tongeschlecht auffasst, kann man z. B. dem Dorischen, das sich vom natürlichen Moll ebenfalls nur durch einen Ton unterscheidet, ein eigenes Tongeschlecht zubilligen. Dorisch und Phrygisch gehören beide (wegen der kleinen Terz über dem Grundton) dem Tongeschlecht Moll, Lydisch und Mixolydisch (wegen der großen Terz) dem Tongeschlecht Dur an. Die Tonartbezeichnungen "c-Dorisch" oder "D-Lydisch" setzen sich also zusammen aus der Angabe des Grundtons und des verwendeten Tonleitertyps, wobei ein kleiner Buchstabe auf Moll und ein großer Buchstabe auf Dur als Tongeschlecht hinweisen.

Die um 1900 einsetzende Loslösung von der tradierten Dur-Moll-Tonalität führte nicht nur zur Atonalität Schönbergs und der Zweiten Wiener Schule, sondern auch zu Versuchen, der Tonalität eine neue Grundlage zu verschaffen. Einer dieser Versuche war die von Paul Hindemith propagierte freie Tonalität. Hier entfällt eine Unterscheidung nach Tongeschlechtern oder diatonischen Tonleitern, weil die gesamte chromatische Tonleiter als Tonmaterial verwendet wird. Tonarten entstehen nur noch dadurch, dass sich einzelne Töne aufgrund ihrer Intervallbeziehungen gegenüber anderen Tönen sozusagen in den Vordergrund drängen und so zu „tonalen Zentren“ werden. Eine Tonartangabe im Sinne der freien Tonalität enthält also weder den Bezug auf ein Tongeschlecht noch auf eine Tonleiter, sondern gibt lediglich den Grundton an, also statt "C-Dur" oder "c-Moll" nur noch "C" (ohne alles und immer groß geschrieben). (vgl. Ludus tonalis)



</doc>
<doc id="13373" url="https://de.wikipedia.org/wiki?curid=13373" title="H">
H

H beziehungsweise h (gesprochen: []) ist der achte Buchstabe des klassischen und modernen lateinischen Alphabets. Er ist ein Konsonant (auch wenn er unter bestimmten Bedingungen stumm ist). Er hat in deutschen Texten eine durchschnittliche Häufigkeit von 4,76 Prozent. Er ist damit der neunthäufigste Buchstabe in deutschen Texten.
Das Fingeralphabet für Gehörlose bzw. Schwerhörige stellt den Buchstaben "H" dar, indem die geschlossene Hand zum Körper weist, während Zeige- und Mittelfinger parallel vom Körper weg zeigen.

Noch im proto-semitischen Alphabet stellte der Buchstabe einen Zaun dar. Im phönizischen Alphabet wurde aus diesem Symbol der Buchstabe Chet. Die Phönizier ordneten dem Buchstaben den Lautwert [ħ] zu, den stimmlosen pharyngalen Frikativ – eine Art "leichtes H".

In das griechische Alphabet wurde der Buchstabe zuerst als "Heta" übernommen. Er stand für den Lautwert [h]. Im Phönizischen Alphabet waren diese Lautwerte ursprünglich dem Buchstaben He zugeordnet, die Griechen übernahmen diesen Buchstaben allerdings als [e]. Bis zum klassisch-griechischen änderte sich der Name und Lautwert des Buchstabens ein weiteres Mal: Aus dem Heta wurde das Eta, es stand nun für [eː].

In das etruskische Alphabet wurde das H noch in seiner archaischen Form als in der Mitte geteiltes Rechteck übernommen. Die Etrusker kannten den Laut [h] und übernahmen den Buchstaben mit diesem Lautwert. Die Römer übernahmen das H von den Etruskern, adaptierten jedoch das Aussehen des griechischen Buchstaben.

Im Deutschen wird das H in seiner Grundfunktion als Konsonantenbuchstabe verwendet, um den Laut [h] darzustellen, der jedoch nur im Silbenanlaut vor Vokalen vorkommt (Hand, holen, Anhalter). In einigen Wörtern wurde das H früher gesprochen, ist jedoch heute in den meisten Varietäten verstummt (sehen, gehen, hohe). Dies wirkte sich auf die Rechtschreibung aus, indem man lange Vokale, denen nie ein gesprochenes H folgte, mit einem geschriebenen h als lang markierte (Jahr, Vieh, Bahn). Des Weiteren wird das h im Deutschen in dem Digraphen ch und dem Trigraphen sch verwendet.

Im Englischen steht das H ebenfalls für den Laut [h], wird jedoch in manchen Wörtern am Anfang nicht gesprochen (hour, honor). In volkstümlicher Lautschrift findet es wie im Deutschen als Dehnungs-h Verwendung (z. B. <ah> für []). Es kommt außerdem in den Digraphen ch, th und sh vor.

In den romanischen Sprachen wie Spanisch, Italienisch oder Französisch wird der Buchstabe h generell nicht ausgesprochen. Doch gab es im Französischen früher auch ein auszusprechendes h („h aspiré“), das zwar mittlerweile ebenfalls verstummt ist, sich im Übrigen aber weiterhin wie ein Konsonant verhält.

„H "Der achte buchstabe des lateinischen, sowie des gothischen alphabets (in letzterem auch mit dem zahlwerte 8), der siebente des altnordischen runenalphabets […]. das h ist der jetzigen hochdeutschen sprache theils der reine hauchlaut" (das h ist ein scharpffer athem, wie man in die hende haucht […]), "theils hat es gar keinen phonetischen wert mehr, insofern es (z. b. in sehen, wehen, ziehen, blühen, hoher, floh) stumm geworden ist, theils steht es oft nur als zeichen der dehnung eines vocals, nicht einmal mehr mit etymologischem werte."“ Grimmsches Wörterbuch.




</doc>
<doc id="13374" url="https://de.wikipedia.org/wiki?curid=13374" title="ß">
ß

Das Schriftzeichen ẞ bzw. ß ist ein Buchstabe des deutschen Alphabets. Er wird als Eszett [] oder scharfes S bezeichnet, umgangssprachlich auch als „Doppel-S“, „Buckel-S“, „Rucksack-S“, „Dreierles-S“ oder „Ringel-S“.

Das ß dient zur Wiedergabe des stimmlosen s-Lautes . Es ist der einzige Buchstabe des lateinischen Schriftsystems, der heutzutage ausschließlich zur Schreibung deutscher Sprachen und ihrer Dialekte verwendet wird, so in der genormten Rechtschreibung des Standarddeutschen und in einigen Rechtschreibungen des Niederdeutschen sowie in der Vergangenheit auch in einigen Schreibungen des Sorbischen.

Historisch gesehen geht das ß in der deutschen Sprache auf eine Ligatur aus "ſ" „langes s“ (ursprünglich ein weiterer Buchstabe des deutschen Alphabets) und "z" zurück. Bedeutsam für die Form des ß in den heutzutage üblichen Antiqua-Schriftarten war jedoch auch eine Ligatur aus langem "ſ" und "s", die bis ins 18. Jahrhundert auch in anderen Sprachen gebräuchlich war.

Das ß wird heute ausschließlich beim Schreiben in deutscher Sprache sowie im Niederdeutschen verwendet, allerdings nicht in der Schweiz und Liechtenstein. Deutsche Muttersprachler in Belgien, Dänemark (Nordschleswig), Italien (Südtirol) und Namibia gebrauchen das „ß“ in ihren geschriebenen Texten nach den in Deutschland und Österreich geltenden Rechtschreibregeln. Ebenso wird in Luxemburg verfahren.

Das ß dient überdies in mittelalterlichen und frühneuzeitlichen Texten als Abkürzung für die Währung Schilling, und „ßo“ steht für das Zählmaß Schock.

Seit dem 29. Juni 2017 ist das große ß – ẞ – Bestandteil der amtlichen deutschen Rechtschreibung. Über seine Aufnahme in das deutsche Alphabet wurde seit Ende des 19. Jahrhunderts diskutiert.

Im Zuge der Zweiten Lautverschiebung im 7. und 8. Jahrhundert waren aus germanischem und // zwei verschiedene Laute entstanden – ein Frikativ und eine Affrikate –, die zunächst beide mit "zz" wiedergegeben wurden. Zur besseren Unterscheidung gab es seit dem Althochdeutschen Schreibungen wie "sz" für den Frikativ und "tz" für die Affrikate.

Der mit "ss" geschriebene Laut, der auf ein ererbtes germanisches /s/ zurückgeht, unterschied sich von dem mit "sz" geschriebenen; das "ss" wurde als stimmloser alveolo-palataler Frikativ ausgesprochen, das "sz" hingegen als stimmloser alveolarer Frikativ [s]. Auch als diese zwei Laute zusammenfielen, behielt man beide Schreibungen bei. Man brachte sie aber durcheinander, weil niemand mehr wusste, wo ursprünglich ein "sz" gestanden hatte und wo ein "ss".

Bei der Einführung des Buchdrucks im späten 15. Jahrhundert wurden Druckschriften aus den damals geläufigen gebrochenen Schriften geschaffen. Dabei wurde für die häufig auftretende Buchstabenkombination aus langem "ſ" und z mit Unterschlinge („ſʒ“) eine Ligatur-Letter geschnitten. Diese Ligatur behielt man auch bei später eingeführten Druckschriften wie der Fraktur bei.

Der Typograph Max Bollwage vermutet, dass der Ursprung des Zeichens auf die tironischen Kürzungszeichen „sed“ und „ser“ zurückzuführen sei. Der Typograph und Sprachwissenschaftler Herbert Brekle widerspricht dieser These. Die Es-Zett-Ligatur lasse sich bis ins 14. Jahrhundert nachweisen. Die Kürzungszeichen seien nur für eine Übergangszeit „zur Repräsentation des stimmlosen s-Lautes umfunktioniert“ worden, druckschriftlich „setzt sich dagegen in der Schwabacher- und Frakturschrift ab dem frühen 16. Jh. die eigentliche Es-Zett-Ligatur durch.“

In einigen der ab dem 15. Jahrhundert entstehenden Antiquaschriften ist es eine Ligatur von langem "ſ" und rundem "s". Für das deutsche Eszett der gebrochenen Schriften wurde erst im 19. Jahrhundert ein Antiqua-Gegenstück entworfen. Dagegen gibt es für eine "ſs"-Ligatur viel ältere Belegstellen. Die genaue Beziehung des Antiqua-ß zu Eszett und "ſs"-Ligatur ist umstritten.

Wie schon die Antiqua selbst, entstand in Italien eigenständig eine kursive Ligatur aus langem ſ und rundem s, lange bevor das lange ſ im Laufe des 18. Jahrhunderts außer Gebrauch geriet. Die beiden Buchstaben wurden mit einem losen Bogen verbunden; dies war eine rein kalligrafische und typografische Variation ohne orthografische Funktion. Sie erscheint sowohl in Handschriften als auch im Druck bis Ende des 17. Jahrhunderts als eine Alternative für "ſſ" bzw. "ss" im Wortinneren. Die kursive Ligatur erscheint vor allem in Werken in lateinischer, italienischer und französischer Sprache.

Die "ß"-Ligatur in der Antiqua findet sich erstmals bei einer um das Jahr 1515 entstandenen Schrift von Lodovico Vicentino degli Arrighi. Er nimmt sie auch 1522 in sein Kalligrafielehrbuch "La Operina" auf. In der „lateinischen“ Alltagsschrift des 17. und 18. Jahrhunderts erscheint in Frankreich, England und eingeschränkt in Deutschland als Äquivalent zu "ß" die Ligaturform "ſs", wobei das lange s die Schleifen erhielt. Im Druck lässt sich die kursive "ß"-Ligatur bis auf einige Seiten (f. 299v.–302v.) einer Livius-Ausgabe aus dem Jahre 1518 zurückverfolgen, wo sie in freier Variation zur "ſſ"-Ligatur steht, die auch im restlichen Werk exklusiv vorkommt. Die Ausgabe trägt das Zeichen von Aldus Manutius, erschien aber drei Jahre nach seinem Tod als Gemeinschaftsarbeit nach seiner Grundidee.

Im Jahre 1521 erschien in Basel eine deutsche Übersetzung (Leonis Judae) von Enchiridion militis Christiani von Erasmus von Rotterdam. Sie ist in einer Kursiv-Antiqua mit "ß"-Ligaturen gedruckt, wobei diese sich in Wortformen wie "wyßheit, böß" und "schloß" findet.

Bis weit in das 17. Jahrhundert hinein gehört es zu den typografischen Satzkonventionen in Italien, Frankreich und etwas weniger in Deutschland, vor allem in lateinischen, aber auch teilweise in italienischen und französischen Werken bei Antiquakursivsatz, die "ß"-Ligatur zu verwenden. Sie kommt auch auf einigen Titelblättern von um 1620 gedruckten Werken von Johannes Kepler vor.

Erst mit zunehmendem Druck deutscher Texte in Antiqua im Übergang vom 18. zum 19. Jahrhundert erhielten auch gerade Antiquaschriften eine ß-Ligatur, die je nach orthografischer Konvention alternierend zu "ſs"- oder "ss"-Sequenzen verwendet wurde. Davor gab es vereinzelte Vorkommen dieser Typen.

Als die Nationalsozialisten in Deutschland im Jahr 1941 die Fraktur und sonstige gebrochene Schriften abschafften und die Antiqua als „Normal-Schrift“ einführten, wurde von den zuständigen Ministerien auch eine Abschaffung des ß in Antiqua beschlossen, da der Buchstabe im Ausland unbekannt und selten in Antiqua-Schriften vorhanden war. Hitler intervenierte aber. Aus einem Schreiben des Reichsministers der Reichskanzlei: "„Der Führer hat sich für eine Beibehaltung des „ß“ in der Normalschrift entschieden. Er hat sich aber gegen die Schaffung eines großen „ß“ ausgesprochen. Bei der Verwendung großer Buchstaben soll das „ß“ vielmehr als „SS“ geschrieben werden.“"

In deutschem Antiquasatz wurde normalerweise bis ins 19. Jahrhundert an Stelle von "ß" entweder einfaches "ss" oder die Buchstabenfolge "ſs" (keine Ligatur) verwendet. Daneben traten nur selten die Sulzbacher Form des "ß" auf und – besonders in der von den Brüdern Grimm propagierten historischen Schreibweise – das "sz". Die Verwendung von "ſs" erfolgte weiterhin, auch nachdem im Antiquasatz das gewöhnliche "ſ" im späten 18. Jahrhundert unüblich geworden war. Die Empfehlung der Orthographischen Konferenz von 1876 bestand darin, dass im Antiquasatz die Buchstabenfolge "ſs" verwendet werden sollte.

Das eigentliche "ß" im Antiquasatz kam erst im späten 19. Jahrhundert auf und ist dann mit der Orthographischen Konferenz von 1901 zur amtlichen Norm erhoben worden.

Auch in lateinischer Schreibschrift (Kursive) wurde "ß" bis Ende des 19. Jahrhunderts gerne durch "ſs" wiedergegeben. Da das Lang-"ſ" der Kursive grafisch mit dem "h" der Kurrentschrift übereinstimmte, wurde die "ſs"-Gruppe der lateinischen Schreibschrift oft als "hs" missgedeutet, was sich in ungewöhnlichen Schreibweisen von Familiennamen niedergeschlagen hat, beispielsweise „Grohs“ statt „Groß“, „Ziegenfuhs“ statt „Ziegenfuß“, oder „Rohs“ statt „Roß“ "(siehe dazu auch rechts das Bild: „Claßen in lateinischer Schreibschrift“)".

Namensschreibungen wie die Variante "Weiſs" blieben aus juristischen Gründen auch nach 1901 in dieser Form erhalten und wurden durch keine Regel orthografisch angepasst. Im Deutschland der Zwischenkriegszeit waren alleine im standesamtlichen Bereich Schreibmaschinen in Verwendung, die "ſs" als Sondertype enthielten.

Als im späten 18. und im 19. Jahrhundert deutsche Texte vermehrt in Antiqua gesetzt wurden statt in der allgemein üblichen gebrochenen Schrift, suchte man eine Antiqua-Entsprechung für das Eszett der gebrochenen Schrift. Die Brüder Grimm benutzten in der „Deutschen Grammatik 1. Band“ im Jahr 1819 noch die Fraktur, 1826 allerdings die Walbaum-Antiqua. In späteren Werken wollten sie dann das Eszett durch eine eigene Form des Buchstabens ersetzen, setzen aber schließlich "sz" in Ermangelung des ihnen vorschwebenden Druckbuchstabens.

Der Duden von 1880 empfiehlt, das Eszett in Antiqua durch "ſs" zu ersetzen, lässt aber ausdrücklich auch einen ß-artigen Buchstaben zu. Blei-Antiquaschriften wurden üblicherweise ohne ß ausgeliefert, so dass deutsche Texte aus dieser Zeit in "Schweizer Satz" erscheinen. Die Vereinheitlichung der deutschen Rechtschreibung von 1901 schrieb auch im Antiqua-Satz den Buchstaben ß vor. Schriftgießereien wurden verpflichtet, künftig bei Antiqua-Schriften ein ß mitzuliefern bzw. ein solches für Bestandsschriften nachzugießen.

Für die Form der Glyphe eines Antiqua-Eszett gab es verschiedene schriftgestalterische Ansätze. Erst im Anschluss an die I. Orthographische Konferenz von 1876 gab es erfolgreiche Bemühungen um eine einheitliche Form. 1879 veröffentlichte das "Journal für die Buchdruckerkunst" eine Tafel mit Entwürfen. Ein Ausschuss der Leipziger Typographischen Gesellschaft entschied sich für die sogenannte "Sulzbacher Form".

1903, nach der Entscheidung für eine einheitliche Rechtschreibung, erkannte eine Kommission von Buchdruckerei- und Schriftgießereibesitzern die Sulzbacher Form an. In einer Bekanntmachung in der "Zeitschrift für Deutschlands Buchdrucker" beschreiben sie die charakteristischen Merkmale dieser sz-Form: „Das sogenannte lange Antiqua-ſ wird oben mit einem z verbunden, im Kopfe eingebogen und läuft im unteren Bogen in eine feine oder halbstarke Linie oder in einen Punkt aus.“

Die Sulzbacher Form wurde und wird aber nicht von allen Typographen akzeptiert. Etwa vier Grundformen finden größere Verbreitung:

Heutzutage sind die meisten "ß" in Antiquaschriften entweder nach 2. oder nach 4. geformt, doch bisweilen findet sich auch eines nach 3., etwa auf Straßennamenschildern in Berlin und Bonn. Die Variante nach 1. wird selten verwendet.

Das "ß" dient der Wiedergabe des stimmlosen s-Lautes, der Fortis , dessen Darstellung durch "s, ß" und "ss" sich mit der Zeit gewandelt hat, zuletzt mit der Rechtschreibreform von 1996.

Die Handhabung des "ß" gemäß den Regeln der Rechtschreibreform von 1996 folgt der sogenannten Heyseschen s-Schreibung, die von Johann Christian August Heyse im Jahr 1829 formuliert wurde. Von 1879 an galt sie in Österreich, bis sie im Rahmen der Vereinheitlichung der deutschen Rechtschreibung durch die Orthographische Konferenz von 1901 für Schulen und Ämter in deutschsprachigen Staaten ungültig wurde. Stattdessen galt ab dann die Adelungsche s-Schreibung des Orthographen Johann Christoph Adelung. Mit der Rechtschreibreform von 1996 wurde die Heysesche s-Schreibung wiedereingeführt.

Gemäß den Regeln der Rechtschreibreform von 1996 schreibt man "ß" für den stimmlosen s-Laut:

Man schreibt aber "s," wenn im Wortstamm ein Konsonant folgt:
Beim Vorliegen einer Auslautverhärtung schreibt man ebenfalls "s," wenn der s-Laut in verwandten Wortformen stimmhaft ist:
In der Schweiz und in Liechtenstein schreibt man statt "ß" immer "ss."

Ausnahmen und Sonderfälle:

Nach den von 1901 bis 1996 gültigen Regeln schrieb man "ß" in denselben Fällen wie heute; zusätzlich stand "ß" statt "ss" am Wortende (auch in Zusammensetzungen): "Kuß, kußecht, Paß, Paßbild" sowie am Wortstammende, wenn ein Konsonant folgte: "(du) mußt, (es) paßt, wäßrig, unvergeßne, Rößl."

In der Adelungschen s-Schreibung richtet sich somit die Verteilung von "ß" und "ss" teils nach graphotaktischen Kriterien (Berücksichtigung der graphischen Umgebung: Wortende, Wortfuge oder folgender Konsonantenbuchstabe) und teils nach dem Kriterium der Aussprache (Berücksichtigung der Länge des vorangehenden Vokals). Wenn der s-Laut ambisyllabisch ist, steht "ss."

Ligaturen des Fraktursatzes sind nicht als solche dargestellt, um ihre Elemente möglichst getreu wiederzugeben. So ist das heute übliche Antiqua-ß hier nur für Lateinschreibung seit dem 20. Jahrhundert verwendet.

Anstelle von "ß" wird in der Schweiz und in Liechtenstein immer "ss" geschrieben. In diesen Ländern steht "ss" – im Gegensatz zu anderen Doppelkonsonantenbuchstaben – nicht nur nach Kurz-, sondern auch nach Langvokalen und Diphthongen. Wie bei anderen Digraphen (z. B. "ch") ist die Länge oder Kürze des vorangehenden Vokals nicht erkennbar ("Masse" steht sowohl für "Maße" wie für "Masse, Busse" steht sowohl für "Buße" wie für "Busse;" vgl. "hoch" und "Hochzeit, Weg" und "weg"). Dem steht entgegen, dass in einer schweizerischen Aussprache solcher Wörter tatsächlich ein verdoppeltes "s" gesprochen wird, d. h. zwei -Laute, die zu verschiedenen Silben gehören.

Die frühen Antiquadrucke kannten in der Schweiz wie auch in Deutschland kein "ß". Im Bundesblatt der Schweizerischen Eidgenossenschaft fehlte nach der Umstellung von Fraktur auf Antiqua 1873 das "ß" zuerst, wurde aber bald darauf eingeführt, doch schon 1906 wieder aufgegeben. Der Beschluss der Zweiten Orthographischen Konferenz von 1901, "ß" auch für die Antiqua zwingend vorzuschreiben, fand in der Schweiz keine durchgängige Beachtung. In der Folge beschloss die Erziehungsdirektion (Kultusministerium) des Kantons Zürich in den Dreißigerjahren, das "ß" vom 1. Januar 1938 an in den kantonalen Volksschulen nicht mehr zu lehren; die anderen Kantone folgten. Als letzte schweizerische Tageszeitung entschied die "Neue Zürcher Zeitung", die am 1. August 1946 von Fraktur auf Antiqua umgestellt hatte, ab dem 4. November 1974 auf das "ß" zu verzichten. Mit der Reform von 2006 wurde es auch offiziell für den amtlichen Schriftverkehr abgeschafft. Schweizer Verlage, die für den gesamten deutschsprachigen Markt produzieren, verwenden das "ß" jedoch weiterhin.

In der Schweiz war es in der Antiqua seit jeher üblich, "ss" in "s-s" aufzutrennen, auch wenn es für ein "ß" steht. Beispielsweise wird das Wort "Strasse" (für "Straße") in "Stras-se" getrennt. Diese schweizerische Trennung wurde mit der Rechtschreibreform von 1996 als allgemeine Regel übernommen (§ 108 (1996) bzw. § 110 (2006)).

Heute darf nach der deutschen Rechtschreibung im Satz das ß nur dann durch ss wiedergegeben werden, wenn in einer Schrift oder einem Zeichensatz das ß nicht vorhanden ist. Manuskripte ohne ß müssen deshalb den Regeln entsprechend umgesetzt werden. In der Schweiz und in Liechtenstein kann das ß immer durch ss wiedergegeben werden.

Wenn ein deutsches Wort mit ß latinisiert wird oder wenn ein deutscher Name mit ß im fremdsprachigen Satz erscheint, bleibt das ß erhalten, z. B. Madame Aßmann était à Paris.

Es gab jahrhundertelang keine Großbuchstabenform des Buchstabens ß. Da das ß nicht an einem Wortanfang stehen kann, wird ein versales ß nur in der Versalschrift benötigt. Als Ersatz entstanden zunächst folgende orthographisch zulässigen Lösungsmöglichkeiten:

Heute gibt es die weitere Option:

Seit dem 29. Juni 2017 ist das große ß (ẞ) offiziell Bestandteil der amtlichen deutschen Rechtschreibung. Damit ist seither zum Beispiel die Schreibweise STRAẞE gleichberechtigt neben der Schreibweise STRASSE zulässig.

Über Computer kann der Großbuchstabe ẞ meist durch Verwendung bestimmter Tastenkombinationen (je nach Tastatur/Computersystem unterschiedlich) eingegeben werden; Details siehe Hauptartikel.

In der alphabetischen Sortierung (DIN 5007) wird das "ß" wie "ss" behandelt. Bei Wörtern, die sich nur durch "ss" bzw. "ß" unterscheiden, kommt das Wort mit "ss" zuerst, z. B. "Masse" vor "Maße" (DIN 5007, Abschnitt 6.1); der Duden weicht in dieser Hinsicht von der Norm ab: hier kommt das Wort mit "ß" zuerst.

Schwierigkeiten treten beispielsweise beim Reisepass in Österreich auf, da Namen dort in der gleichen Schreibweise wie in der Geburtsurkunde stehen müssen. So steht der reguläre Name mit "ß", während er in den Zeilen darunter in der Computerdarstellung mit "SS" geschrieben steht. Im Ausland kam es deshalb schon zu Schwierigkeiten bei Grenzbehörden, da diese darin eine Fälschung vermuteten. Um diesem Problem vorzubeugen, kann im österreichischen Reisepass ein amtlicher Vermerk angebracht werden, dass das "ß" einem "ss" gleichzusetzen ist. Der Vermerk wird bei Neuausstellung des Passes auf Verlangen gratis gedruckt, und das in den Sprachen Deutsch, Englisch und Französisch.

Wegen seiner optischen Ähnlichkeit zum ß und dem Fehlen auf der dort verwendeten Tastatur wird im Ausland manchmal fälschlicherweise der Großbuchstabe B als Ersatz verwendet, was für den deutschsprachigen Leser befremdlich wirkt.

Obwohl der Buchstabe im Niederländischen nicht benutzt wird, hat er dort einen eigenen Namen – Ringel-S (auf Niederländisch „ringel-s“ geschrieben). Das ß wird im Niederländischen immer durch ss ersetzt: so schreibt man „edelweiss“ (statt Edelweiß) und „gausscurve“ (statt Gauß-Kurve).

Im englischsprachigen Raum, in dessen Alphabet der Buchstabe nicht vorkommt, wird das ß bei manchen wegen seiner Form umgangssprachlich als "German B" (deutsches B) bezeichnet. Deshalb wird das „ß“ auch von den meisten Englischsprechenden als „B“ gelesen, z. B. die Bezeichnung „Weißer“ als „Weiber“. So kommt es auch vor, dass das „ß“ einfach als „b“ wiedergegeben wird; zum Beispiel „Sesamstrabe“ in einem britischen Satelliten-TV-Programmheft, das auch das Programm deutscher Sender listet. Gelegentlich wird das ß auch mit dem griechischen Buchstaben β (beta) verwechselt.

Die korrekte Bezeichnung im Englischen lautet "Sharp S" oder "Eszett" wie im Deutschen.

Bis zur Durchsetzung der auf dem Tschechischen basierenden und bis heute im Wesentlichen gültigen „analogen Rechtschreibung“ unter Federführung von Jan Arnošt Smoler in der zweiten Hälfte des 19. Jahrhunderts wurde das Sorbische mithilfe der Schwabacher geschrieben. In dieser auf dem Deutschen basierenden Schriftnorm wurde das ß zur Darstellung des scharfen S-Lautes verwendet. Im Niedersorbischen war diese alte Schreibweise noch weit bis ins 20. Jahrhundert hinein gebräuchlich.

In leichter Abwandlung der deutsch-österreichischen Tradition wird das scharfe /s/ im Ungarischen als <sz> geschrieben.

Im Computerbereich wird das ß oft als Umlaut bezeichnet, da es die gleiche Art von Problemen hervorruft wie die echten Umlaute: Es ist vor allem nicht in ASCII enthalten, dem „kleinsten gemeinsamen Nenner“ der lateinischen Zeichensätze. Daher wird es in verschiedenen Fällen verschieden kodiert.

Im ASCII-Zeichensatz aus dem Jahr 1963 ist das Zeichen nicht enthalten, weshalb viele ältere Computersysteme es nicht darstellen konnten. Allerdings enthielten bereits die ASCII-Erweiterungen ISO 6937 von 1983 und ISO 8859-1 (auch als Latin-1 bekannt) 1986 das Eszett. Fast alle modernen Computer verwenden den im Jahr 1991 eingeführten Unicode-Standard, womit das Eszett problemlos zu verarbeiten und darzustellen ist. Lediglich einige Programme, die noch auf älteren Zeichensätzen beruhen, können beim Datenaustausch Probleme bereiten.

Das ß wird folgendermaßen definiert und kodiert:

Nur auf der Tastatur nach deutscher Norm liegt die Eszett-Taste in der oberen Tastenreihe zwischen der Taste für die Ziffer Null und der Taste für den Akutakzent. Wie die US-amerikanische Tastatur verfügt auch die schweizerische Tastatur über keine standardisierte Taste für das Eszett. Auf der niederländischen und türkischen Tastatur sowie auf der US-internationalen Tastaturbelegung kann es in Windows allerdings über + eingegeben werden, beim deutschschweizerischen Layout dagegen ist es nur unter Linux (+) und auf einem Mac möglich (+). Das große ß – ẞ – kann mit der Neo-Tastaturbelegung nativ über + eingegeben werden.

Kann das Zeichen „ß“ nicht dargestellt werden, weil es in der verwendeten Schriftart oder dem Zeichensatz fehlt, so sollte es durch „ss“ ersetzt werden (aus „Straße“ wird „Strasse“). In den (behördlichen) Fernschreiben wurde das „ß“ bis in das frühe 21. Jahrhundert durch „sz“ ersetzt. Dies war unter anderem bei Familiennamen wichtig („Straßer“ wurde im Text zu „Straszer“). Im Fernschreibverkehr und bei Schreibmaschinen ohne ß-Letter wurde das „ß“ durch „:s“ ersetzt, um zwischen Familiennamen wie etwa Strasser, Straszer und Straßer zu unterscheiden. Die Ersetzung durch „β“ (Beta) oder „B“ ist nicht mehr üblich.

Da nahezu alle modernen Computersysteme und -schriften auf Unicode basieren, kann das Eszett heutzutage theoretisch weltweit dargestellt, verarbeitet, übertragen und archiviert werden. Eine Ersetzung aus technischen Gründen ist deshalb nur noch selten nötig. Auch wenn auf der verwendeten Tastatur das Zeichen nicht aufgedruckt ist, kann es meistens über eine entsprechende Tastenkombination des Betriebssystems oder des jeweiligen Texteditors eingefügt werden:


In der chinesischen Schrift erscheint die Form 阝 als Radikal 163 bzw. Radikal 170 und in Schriftzeichen, die auf diesen aufgebaut sind.




</doc>
<doc id="13375" url="https://de.wikipedia.org/wiki?curid=13375" title="Sütterlinschrift">
Sütterlinschrift

Die Sütterlinschriften, meist einfach Sütterlin genannt, sind zwei im Jahr 1911 im Auftrag des preußischen Kultur- und Schulministeriums von Ludwig Sütterlin entwickelte Ausgangsschriften für das Erlernen von Schreibschrift in der Schule.
Neben der "deutschen Sütterlinschrift", die eine spezielle Form der deutschen Kurrentschrift darstellt, entwickelte Ludwig Sütterlin auch eine stilistisch vergleichbare lateinische Schreibschrift.

Es war im 19. Jahrhundert in England Mode geworden, mit der neu entwickelten stählernen Spitzfeder zu schreiben. Die sehr schräge englische Schreibschrift mit ihren großen Unter- und Oberlängen und ihrem veränderlichen Strich "(Schwellzug)" ist zwar dekorativ, aber technisch schwer zu schreiben. In Deutschland schrieb man damals ähnliche Schriften mit deutschen Buchstabenformen.

Um den Kindern das Schreibenlernen zu erleichtern, vereinfachte Sütterlin die Buchstabenformen, verringerte die Ober- und Unterlängen (Lineatur im Verhältnis 1:1:1), stellte die relativ breiten Buchstaben aufrecht und ließ sie im Gleichzug mit einer Kugelspitzfeder schreiben. In allen diesen Merkmalen ist sie den heute verbreiteten Antiqua-basierten Schulschriften ähnlich.

Die deutsche Sütterlinschrift wurde ab 1915 in Preußen eingeführt. Sie begann in den 1920er Jahren die bis dahin übliche Form der deutschen Kurrentschrift abzulösen und wurde 1935 in einer abgewandelten Form (leichte Schräglage, weniger Rundformen) als "Deutsche Volksschrift" Teil des offiziellen Lehrplans. In der Folge des Normalschrifterlasses wurde allerdings auch sie mit einem Rundschreiben vom 1. September 1941 verboten, nachdem bereits mit Rundschreiben von Martin Bormann (Kanzleichef der NSDAP) vom 3. Januar 1941 die Verwendung gebrochener Druckschriften (Frakturtypen) untersagt worden war. Als Ausgangsschrift wurde nach dem Verbot der deutschen Schrift ab 1942 in den Schulen die lateinische Schrift in einer Variante, die "Deutsche Normalschrift" genannt wurde (Proportionen 2:3:2, Schrägstellung, Ovalformen), eingeführt. An west- und ostdeutschen Schulen wurde nach 1945 außer der lateinischen Ausgangsschrift die deutsche Schreibschrift teilweise bis in die 1980er Jahre zusätzlich gelehrt.

In Deutschland gibt es verschiedene Initiativen und Vereine, die beim Entziffern von Texten in Sütterlin- und anderen alten Schriften helfen. Ein Beispiel ist die „Sütterlin-Schreibstube“ in Konstanz oder die Sütterlinstube Hamburg.

In der Mathematik bezeichnete man in Deutschland bis in das späte 20. Jahrhundert Matrizen durch Großbuchstaben und Vektoren durch Kleinbuchstaben der deutschen Sütterlinschrift (anstelle der heute gebräuchlichen Schreibweise mit lateinischen Buchstaben und darübergesetztem Pfeil).

Die folgende Variante der Sütterlinschrift (deutsches Alphabet) verwendet einen von R. G. Arens erstellten Computerzeichensatz. Die Buchstaben weisen deutliche Abweichungen zu Sütterlins ursprünglicher Ausgangsschrift auf, etwa reduzierte Oberlängen und natürlichere Formen, was zu einem weniger geometrischen Aussehen führt.





</doc>
<doc id="13376" url="https://de.wikipedia.org/wiki?curid=13376" title="Deutsche Schrift">
Deutsche Schrift

Die Bezeichnung deutsche Schrift wird entweder als Sammelbegriff für einige gebrochene Schriften verwendet, mit denen vom 16. bis zum 20. Jahrhundert deutsche Texte bevorzugt geschrieben und gedruckt wurden, oder dient als Name einer dieser Schriften. In Bezug auf Schreibschrift wird auch die Bezeichnung "deutsche Schreibschrift", je nach Zusammenhang sowohl als Sammelbegriff als auch als Einzelname verwendet.

Um ihre offizielle Verwendung in deutschen Behörden und Schullehrplänen wurde ein jahrzehntelanger Antiqua-Fraktur-Streit geführt, in dem die Antiqua (auch eingedeutscht "Altschrift") die gewohnte "deutsche Schrift" (unter anderem die häufig gedruckte Fraktur) schließlich durch den Normalschrifterlass im Dritten Reich ablöste. Um deutlich zu machen, dass die Schriftformen nicht ausschließlich in Deutschland in Gebrauch waren, wurde in der paläografischen Diskussion auch der Begriff neugotische Schrift vorgeschlagen. Alltagssprachlich werden manche dieser Schriftarten heute auch als alte deutsche Schrift oder altdeutsche Schrift bezeichnet. 
Mit deutscher Schrift sind je nach Zusammenhang meist eine oder mehrere dieser Schriftarten gemeint:



"Deutsche Schrift" ist zudem der Name einiger Schriftarten aus den Jahren 1890 bis 1940, wie etwa Rudolf Kochs 1906 erschienene „Deutsche Schrift“ (sogenannte „Koch-Fraktur“).





</doc>
<doc id="13377" url="https://de.wikipedia.org/wiki?curid=13377" title="Schreibschrift">
Schreibschrift

Eine Schreibschrift, Kursive (mittellateinisch "cursivus" ‚fließend, geläufig‘), Kurrentschrift ( ‚laufen‘) oder Laufschrift ist eine Gebrauchsschrift, die durch eine fortlaufende bzw. wenig unterbrochene Linienführung auf einem Schriftträger (meist Papier) charakterisiert ist. Sie hat ihren Ursprung im flüssigen (kursiven) Schreiben mit der Hand. Dabei werden Schreibgeräte verwendet, mit denen ein durchgängiger Linienfluss erzeugt werden kann, heute zum Beispiel Bleistifte, Füllfederhalter, Kugelschreiber, Fineliner, Kreide oder Pinsel (vorwiegend in Ostasien).

Der Begriff der Schreibschrift bezieht sich meistens auf lateinische Alphabete seit dem europäischen Mittelalter, da sich bei diesen die gedruckte und die von Hand geschriebene Schrift stark auseinanderentwickelt hat. Schreibschrift ist jedoch keinesfalls mit von Hand geschriebener Schrift gleichzusetzen: so wurden die Unziale, die gotische Minuskel oder die humanistische Minuskel von Hand geschrieben, sind aber keine Schreibschriften. Umgekehrt gibt es heute auch gedruckte Satzschriften, die in DIN 16518 zur Gruppe VIII (Schreibschriften) gezählt werden, weil sie eine Schreibschrift imitieren. Schon zur Zeit des Bleisatzes wurden nach Vorbildern von Schreibschriften Typen für den Druck hergestellt. Die Vereinzelung der Buchstabentypen im Sinne des Baukastenprinzips schränkt hier jedoch die Realisierung von Ausdrucksmöglichkeiten der lebendigen authentischen Bewegung stark ein.

Die Schreibschrift ist ähnlich der Schweizer Schulschrift, umgangssprachlich auch Schnürlischrift genannt.

Die Kunst einer gut lesbaren Schreibschrift, das Schönschreiben, wurde im Mittelalter von Schreibmeistern ausgeübt und gelehrt. Später wurde sie Bestandteil des allgemeinen Schulunterrichts.

Die Schreibschriften nehmen innerhalb der Schriften, die mit der Hand geschrieben werden, eine besondere Stellung ein. Sie unterscheiden sich von anderen mit der Hand geschriebenen Schriften durch den starken Einfluss, den die rasche und flüssige Bewegungsausführung auf die Form ausübt. Während statisch aufgebaute Schriften (z. B. Buch- und repräsentative Inschriften) durch das Aneinandersetzen von einzelnen Formteilen bzw. Strichen entstehen, sind Schreibschriften vorwiegend dynamisch bestimmt, weil sie mit höherer Schreibgeschwindigkeit ausgeführt werden. Bei ihnen spielt die Ökonomisierung des Schreibprozesses und damit die Verkürzung des Schreibweges eine dominierende Rolle. Charakteristische Merkmale gegenüber anderen mit der Hand geschriebenen Schriften sind

Von Hand geschriebene Schrift, die aus unverbundenen Buchstaben besteht, ist keine Schreibschrift, sondern eine Druckschrift (in der Schweiz auch "Steinschrift" genannt).

Kursiven sind in der Geschichte der Schrift nicht immer üblich gewesen. Während in der römischen Antike Schrift umfangreich im Alltag eingesetzt wurde und sich dadurch eine flüssige, verbundene Schrift entwickelte (ältere römische Kursive, jüngere römische Kursive), kannte das hohe Mittelalter keine auf dem Prinzip der Buchstabenverbindung beruhende Schrift. Erst seit dem 13. Jahrhundert hatte sich Schriftlichkeit durch Universitäten, Kaufleute und zentralistische Verwaltung wieder so weit verbreitet, dass eine neue Kursivschrift, die gotische Kursive, entstand.

Mit dem Buchdruck wurde es auch weniger Begüterten, Schulen und öffentlichen Büchereien möglich, Bücher zu erwerben. Die "schreibende Hand" stand im 15. Jahrhundert in direkter, harter Konkurrenz zu der "druckenden Maschine". Die Drucker erkannten bald, dass sie Bücher von gleicher Art und Güte in großer Zahl rasch und billig unter die Leute bringen konnten. Die Drucker hielten sich zunächst im Schnitt ihrer Typen und des schmückenden Beiwerks an das Vorbild handgeschriebener Bücher.

Der Fortgang der Entwicklung und die raschen Erfolge der Druckkunst zwangen indes die bisher hochgeachteten und gutverdienenden Buchschreiber, den Fortbestand ihrer Kunst vor der stetig wachsenden Konkurrenz zu verteidigen. Sie gründeten Schreibschulen, nahmen Schüler aus den bürgerlichen Ständen an und bauten die seither bewährten Schriften weiter aus. Sie beeinflussten die Weiterentwicklung der Schreib-, aber auch der Buchschriften und förderten damit die Verbreitung des Handschreibens im Allgemeinen.

Nach dem Aufkommen der Druckkunst gab es eine große Anzahl von Schreibern in Deutschland, Frankreich, Spanien, Italien, der Schweiz und anderen Ländern. Von 1500 bis 1800 entstanden allein in Deutschland etwa 800 gedruckte Schreibvorlagen.

Als bedeutendster Nürnberger Schreibmeister gilt Johann Neudörffer, ein Zeitgenosse Albrecht Dürers. Er schuf mit Hieronymus Andreä die "Neudörffer-Andreä-Fraktur". Mit dieser Druckschrift legte er zugleich die Basis für alle weiteren Frakturschriften, die auch die in den Kanzleien verwendete Schreibschrift beeinflusste (Kanzleikurrent, deutsche Kurrentschrift). In seiner Schule ging er gegen die Vielfalt und Verworrenheit der damals benutzten Verkehrsschriften an.

In den Kanzleien und im wirtschaftlichen Alltag waren in Deutschland die Formen der gotischen Kursive Ausgangspunkt der Entwicklung zur sogenannten deutschen Schrift oder deutschen Kurrentschrift. Im 16. Jahrhundert setzte sich für lateinische und nichtdeutsche Texte die von dem Humanisten Niccolò Niccoli entwickelte humanistische Kursive als Schreibschrift durch. Die humanistische Kursive entwickelte sich weiter zur lateinischen Schreibschrift, die noch heute verwendet wird. Ein gebildeter Bewohner Deutschlands lernte bis ins 20. Jahrhundert mindestens zwei Schriftarten flüssig lesen und schreiben. In Briefen wurde nicht selten der normale Text in deutscher Kurrentschrift geschrieben, Eigennamen oder sonst hervorzuhebende Wörter dagegen in humanistischer Kursive (bzw. lateinischer Schreibschrift).

1941 wurde nach einer Entscheidung Adolf Hitlers die deutsche Kurrentschrift durch einen Erlass verboten und die lateinische Schreibschrift wurde zur alleinigen „deutschen Normalschrift“ erklärt. Die deutsche Kurrentschrift kam auch nach dem Ende des Dritten Reichs nicht wieder in Gebrauch.

In der Kurrentschrift können manche Buchstaben unklar zu lesen sein. Um das zu verbessern, haben sich bestimmte Zusatzzeichen entwickelt. Zum Beispiel wurde es im deutschen Sprachraum üblich, über das kleine u einen Strich oder Bogen zu zeichnen (ū), um es vom kleinen n besser unterscheiden zu können. Auch wurden die Doppelbuchstaben "mm" und "nn" oft durch einen Reduplikationsstrich als m̅ und n̅ geschrieben. Der Reduplikationsstrich fiel im 20. Jahrhundert außer Gebrauch und auch u-Bögen trifft man heute nur noch selten an.

Der Buchstabe O bekam in der Schreibschrift manchmal ein „Schwänzchen“ oder einen Kringel, um ihn besser von der Ziffer 0 zu unterscheiden, die Ziffer 7 einen Querstrich, um sie besser von der Ziffer 1 zu unterscheiden, und der Buchstabe Z einen Querstrich (Ƶ), um ihn besser von der Ziffer 2 zu unterscheiden. Im angelsächsischen Sprachraum setzen sich andere Konventionen durch, etwa das Weglassen des Aufstrichs bei der Ziffer 1 zur besseren Unterscheidung von der Ziffer 7.

Im Jahre 1830 fand die spitze Stahlfeder von England ausgehend immer größere Verbreitung. Sie erwies sich zwar schwieriger in der Handhabung als die Kielfeder, konnte sich aber bis zum Ende des 19. Jahrhunderts auch in Deutschland durchsetzen.

Mit der Einführung der Schulpflicht und Schreiben als Grundlehrfach wurden bald die verschiedenen Meisterschulen überflüssig. Durch ihren Wegfall und die weitere Durchsetzung des englischen Stils mitsamt der englischen Spitzfeder setzten sich neue Gebrauchsschriften durch.

Der Grafiker Ludwig Sütterlin entwickelte im Jahr 1911 die Sütterlinschrift, eine reformierte Schreibschrift als Ausgangsschrift in zwei Versionen, als deutsches und als lateinisches Alphabet. Er gestaltete sie mit dem Verhältnis 1:1:1 für die Lineaturräume, mit Steilschriftformen. Als völlig neues Gerät nutzte er die Gleichzugfeder (Kugelspitzfeder) und die Schnurzugfeder (Redisfeder). Die kugelige Spitze der Gleichzugfeder ermöglicht Rundzüge jeder Art bei gleichbleibender Strichstärke, und Sütterlin gestaltete seine Ausgangsschrift auf diese Eigenschaft abgestimmt. Die Gleichzugfeder stellt keine großen Ansprüche bezüglich der Haltung und Führung der Feder bzw. des Füllfederhalters. Aus diesem Grunde erschien sie Sütterlin auch als das passende Schreibgerät für Kinder zum Erlernen des Schreibens. Bei der Schnurzugfeder liegt anstatt einer Kugel ein kleines rundes Scheibchen auf dem Papier auf. Sie wird wegen ihrer gleichförmigen Strichbreite auch gerne für groteske oder technische Schriften verwendet. Sütterlin sprach sich aber auch sehr klar für den Gebrauch der rechtsschrägen Breitfeder aus, die in späteren Schuljahren folgen sollte, und wies auf den Formgewinn hin, den diese Feder den Schriften verleiht.
Sütterlins Reformschrift wurde 1915 an den Schulen in Preußen eingeführt. Nach seiner Veröffentlichung "Neuer Leitfaden für den Schreibunterricht", die 1917 erschien, wurde sie auch in anderen deutschen Ländern eingeführt und prägte so die Handschrift der Deutschen auf viele Jahrzehnte.

In Hessen entwickelte Rudolf Koch eine ausdrucksvolle Breitschrift, die Offenbacher Schrift, welche er 1927 vorstellte. Mit der Einführung von Sütterlins Schrift in Hessen 1930 blieb die Offenbacher Schrift jedoch unbenutzt.

1935 wurde die Sütterlinschrift in einer abgewandelten Form (leichte Schräglage, weniger Rundformen) als "Deutsche Volksschrift" Teil des offiziellen Lehrplans. 1941 wurde jedoch mit dem Normalschrifterlass der nationalsozialistischen Regierung das vorläufige Ende der deutschen Schreibschrift besiegelt. Die lateinische Schreibschrift wurde nun als „Normalschrift“ festgelegt.

Seit den 1960er Jahren gab und gibt es in den deutschen Lehrplänen für das Schreibenlernen Bemühungen, die aus dem Barock übernommenen Schnörkel insbesondere der Großbuchstaben zurückzunehmen. Die Schulausgangsschrift (1968) orientierte sich dabei vorwiegend an den Quellen der lateinischen Schreibschrift, der humanistischen Kursive. Demgegenüber waren die Bestrebungen der Vertreter der Vereinfachten Ausgangsschrift (1972) darauf gerichtet, die Schreibschrift insgesamt aus der geradstehenden Antiqua, der sogenannten Druckschrift, zu entwickeln.

Seit 2011 wird mit der Grundschrift versucht, die Ideen von Fritz Kuhlmann (1916) wieder zu beleben. Kuhlmann war ein leidenschaftlicher Anhänger des Arbeitsschulprinzips. Er plädierte dafür, dass die Kinder die Schreibform aus den gedruckten Buchstabenformen der Leseschrift selbst entwickeln und dabei sowohl eigene Buchstaben als auch Buchstabenverbindungen finden. Dieses Konzept hatte sich damals nicht bewährt und wurde aufgegeben.

Der Grundschulverband setzt sich dafür ein, dass die bisher verwendeten Schreibschriften durch die sogenannte Grundschrift ersetzt werden. Diese lehnt sich an die Druckschrift an, bei der die Buchstaben einzeln stehen, aber auch verbunden werden können. Auch die Gestaltung von Schleifen in den Ober- und Unterlängen entfällt. Den Grundschulen in Hamburg steht es seit Herbst 2012 frei, die Grundschrift oder die Schulausgangsschrift zu verwenden. In einigen Bundesländern wird die Grundschrift derzeit erprobt.

Im Zeitalter von PCs, Tablets und Smartphones wird der Gebrauch der Schreibschrift im beruflichen wie auch im privaten Leben zunehmend zurückgedrängt. Die Schulen in den verschiedenen Ländern haben z. T. bereits auf die Tendenz reagiert. So soll in Finnland ab Herbst 2016 an den Grundschulen das Tippen auf der Tastatur neben einer Grundschrift vermittelt werden. Eine gebundene Schreibschrift muss nicht mehr gelehrt werden. Auch in anderen Ländern sind Vereinfachungen in der Diskussion oder bereits durchgesetzt. In Deutschland, wo schulische Lehrpläne der Kulturhoheit der Bundesländer unterstellt sind, gibt es unterschiedliche Regelungen, wobei die Entscheidung durch das Schulcurriculum der jeweiligen Schule festgelegt wird oder z. T. auch bei der individuellen Lehrkraft liegt.

Die Schreibschrift wird in der Fachdiskussion überwiegend als wichtig erachtet. Gleichzeitig werden gravierende Schwierigkeiten beim Schreiben mit der Hand beobachtet. 2015 ergab eine Umfrage zunehmende Probleme in deutschen Schulen beim Gebrauch der Schreibschrift. Der Deutsche Philologenverband setzt sich für die Schreibschrift ein, denn handschriftliche – obwohl nicht zwingend schreibschriftliche – Notizen führten in einer Studie der Princeton University und der UCLA zu einem besseren Verständnis und längeren Erinnern von Lerninhalten.

Die Deutschschweizer Erziehungsdirektoren-Konferenz (D-EDK) hat am 31. Oktober 2014 den Deutschschweizer Kantonen eine koordinierte Umstellung auf die teilverbundene Basisschrift empfohlen. Etwas länger schon wurde die – unverbundene – Schweizer Basisschrift im Kanton Luzern unterrichtet. Sie löst die – verbundene – Schweizer Schulschrift ab, die auch als "Schnürlischrift" oder "Schnüerlischrift" bekannt ist. Die Umsetzung soll aus Sicht des Zürcher Bildungsrates 2016 bis spätestens zum Schuljahr 2018/2019 durchgeführt werden.





</doc>
<doc id="13380" url="https://de.wikipedia.org/wiki?curid=13380" title="Ohmsches Gesetz">
Ohmsches Gesetz

Das ohmsche Gesetz postuliert folgenden Zusammenhang: Wird an ein Objekt eine veränderliche elektrische Spannung angelegt, so verändert sich der hindurchfließende elektrische Strom in seiner Stärke proportional zur Spannung. Mit anderen Worten: Der als Quotient aus Spannung und Stromstärke definierte elektrische Widerstand ist konstant, also unabhängig von Spannung und Stromstärke.

Tatsächlich gilt das Gesetz nur in engem Rahmen und nur für einige Stoffe – insbesondere für Metalle unter der Voraussetzung konstanter Temperatur. Dennoch ist es die Basis für das Verständnis der Zusammenhänge zwischen Stromstärke und Spannung in elektrischen Stromkreisen.

Die Bezeichnung des Gesetzes ehrt Georg Simon Ohm, der diesen Zusammenhang für einige einfache elektrische Leiter als Erster schlüssig nachweisen konnte.

Dieses Verhältnis einer an einem elektrischen Leiter (Widerstand) anliegenden elektrischen Spannung formula_1 zur Stärke formula_2 des hindurchfließenden elektrischen Stromes wird definiert als die Größe "elektrischer Widerstand," die mit dem Formelzeichen formula_3 bezeichnet wird. Bei zeitlich veränderlichen Größen sind Augenblickswerte zu verwenden. Das ohmsche Gesetz betrachtet den Widerstand als eine von formula_1 und formula_2 unabhängige Konstante und ist insofern eine Idealisierung. Damit gilt:

Eine passive elektrische Schaltung mit einer Proportionalität zwischen Stromstärke und Spannung hat ein "ohmsches Verhalten" und weist einen konstanten elektrischen Widerstand auf, der "ohmscher Widerstand" genannt wird. Auch bei nicht-ohmschem Verhalten ist die Größe "Widerstand" als Verhältnis formula_7 definiert, dann liegt allerdings eine Abhängigkeit des Widerstands z. B. von der Spannung vor. Etwa eine Glühlampe und eine Diode verhalten sich nichtlinear.
Für die Beschreibung solchen Verhaltens kann der Begriff differentieller Widerstand hilfreich sein, der den Zusammenhang zwischen einer kleinen Spannungsänderung formula_8 und der zugehörigen Stromstärkeänderung formula_9 angibt.

Die aus dem ohmsche Gesetz folgende Gleichung lässt sich (durch äquivalente Umformungen) in drei Schreibweisen darstellen:

Vielfach wird schon allein die Definition der Größe "Widerstand" als Quotient von Spannung und Stromstärke als „ohmsches Gesetz“ bezeichnet, obwohl einzig die "Konstanz" des Widerstands die Kernaussage des ohmschen Gesetzes ist.

In einer lokalen Betrachtung wird das ohmsche Gesetz durch den linearen Zusammenhang zwischen dem Stromdichte-Vektorfeld formula_11 und dem elektrischen Feldstärke-Vektorfeld formula_12 mit der elektrischen Leitfähigkeit formula_13 als Proportionalitätsfaktor beschrieben:

In isotropen Materialien kann der Tensor formula_15 durch einen Skalar ersetzt werden, und es gilt:

Wird die Bewegung der freien Elektronen analog der ungeordneten Molekülbewegung in einem idealen Gas betrachtet, so erscheint die Konstanz der elektrischen Leitfähigkeit plausibel: Die Zähldichte formula_17 der Elektronen ist dann innerhalb des Leiters konstant. Für die mittlere Geschwindigkeit formula_18 der Elektronen gilt:

Die mittlere Wegstrecke formula_20 zwischen zwei Stößen an Ionen im Metall wird in einer typischen Zeit formula_21 zurückgelegt:

In dieser Zeit erfahren die Elektronen eine Beschleunigung

durch das angelegte elektrische Feld, wobei formula_24 die Elementarladung und formula_25 die Elektronenmasse ist. Die Elektronen erreichen somit eine Driftgeschwindigkeit formula_26 mit formula_27. Setzt man dies in die Gleichung für formula_28 ein, so erhält man:

Die Größen formula_20 und formula_18 hängen nur von der Geschwindigkeitsverteilung innerhalb der „Elektronenwolke“ ab. Da die Driftgeschwindigkeit aber circa 10 Größenordnungen kleiner ist als die mittlere Geschwindigkeit formula_18, ändert sich die Geschwindigkeitsverteilung durch das Anlegen eines elektrischen Feldes nicht, und formula_20 und formula_21 und somit der ganze Ausdruck für formula_28 sind konstant.

Georg Simon Ohm wollte einen mathematischen Zusammenhang – eine Formel – entwickeln, mit der sich die „Wirkung fließender Elektrizität“ (heute: die Stromstärke) in Abhängigkeit vom Material und von den Dimensionen eines Drahtes berechnen lässt. Dabei ist er nicht zufällig auf das nach ihm benannte Gesetz gestoßen, sondern hat viel Zeit und viel zielgerichtete Arbeit investiert. Die von ihm gefundene Gesetzmäßigkeit in der Form formula_36 erscheint uns nahezu als Trivialität: Je größer die elektrische Spannung formula_1 bzw. je kleiner der elektrische Widerstand formula_3 ist, umso größer ist die Stromstärke formula_2. Diese Zusammenhänge lassen sich heute mit in jeder Schule vorhandenen Versuchsgeräten mit ausreichend geringen Toleranzen sehr einfach zeigen.

Im Jahr 1825 standen Ohm solche Geräte nicht zur Verfügung. Voltasäulen, Batterien aus Daniell-Elementen und sogenannte Trog-Batterien (das sind mehrere in Reihe geschaltete Daniell-Elemente) in verschiedenen Ausführungen dienten damals als Spannungsquellen. Die Spannungs- und Strommessgeräte jener Zeit waren für Ohms hochgestecktes Ziel eher als Nachweisgeräte, nicht aber als ausreichend exakte Messgeräte geeignet, um damit genaue Messwerte für die Entwicklung einer Formel zu erhalten.

Ohms experimentell-innovative Leistungen bestanden darin, bereits entwickelte Gerätekomponenten sowie die Entdeckungen mehrerer zeitgenössischer Forscher geschickt kombiniert zu haben. Die daraus gewonnenen Messdaten hat er dann mathematisch analysiert und ihren physikalischen Zusammenhang interpretiert.

Zunächst veröffentlichte Ohm 1825 in den "Annalen der Physik und Chemie" einen Artikel, in dem er eine von ihm entwickelte Messvorrichtung beschrieb, mit der er zu exakteren Messwerten kam als andere Forscher vor ihm. Ohm nutzte hierfür einerseits die 1820 von Hans Christian Ørsted beschriebene magnetische Wirkung des elektrischen Stroms und andererseits eine sehr sensible Vorrichtung zur Kraftmessung: Er ersetzte in der Messvorrichtung der coulombschen Drehwaage den darin vorhandenen Probekörper durch einen kleinen Stabmagneten, stellte diese Drehwaage auf einen stromdurchflossenen Draht und maß die Kraftwirkung des Stromes auf den Magneten. Diese Messung führte er mit verschiedenen Drähten durch und suchte dann nach einem mathematischen Zusammenhang zwischen Drähten und Kräften.

Die 1825 im Artikel "Vorläufige Anzeige des Gesetzes, nach welchem Metalle die Contactelectricität leiten" veröffentlichten Messergebnisse konnten jedoch nicht zu einer allgemeingültigen Formel führen, weil – analysiert mit heutigen Begriffen – die elektrische Leistung aller damals benutzten Spannungsquellen (unter anderem durch variierende Bildung von Gasbläschen auf den Metallplatten) stark schwankt. Diesen Effekt beschrieb Ohm mehrfach: Die „Wirkung auf die Nadel“ ändere sich während der Einzelmessungen und sei unter anderem auch von der Reihenfolge der vorgenommenen Messungen abhängig. Trotzdem leitete er im veröffentlichten Artikel aus seinen Messwerten letztendlich eine Formel ab, die die angegebenen Messwerte annähernd reproduziert.

Infolge seines Artikels erhielt Ohm einen Hinweis auf die Entdeckung des Thermoelements durch Thomas Johann Seebeck, über die 1823 ein von Ørsted verfasster Bericht in den "Annalen" abgedruckt wurde. Dieser Hinweis verhalf Ohm zu seinem Durchbruch.

In "Bestimmung des Gesetzes, nach welchem Metalle die Contactelektricität leiten" beschrieb Ohm 1826 zunächst kritisch das „beständige Wogen der Kraft“ in seinen vorhergehenden Versuchen. Es folgt die Beschreibung einer von ihm entworfenen „Drehwaage“, die er von einem Handwerker anfertigen ließ (siehe Abbildung). Das bügelförmige Bauelement formula_40 ist ein Thermoelement aus einem Wismutbügel, an dessen Schenkeln jeweils ein Kupferstreifen befestigt ist. Ein Schenkel wurde mit siedendem Wasser erwärmt, der andere mit Eiswasser gekühlt. (Die Gefäße für die Temperaturbäder sind nicht dargestellt.) Ohm führte seine Experimente im Januar 1826 durch.

Die reproduzierbare Temperaturdifferenz von ca. 100 °C zwischen den Schenkeln des Bügels erzeugt eine reproduzierbare „erregende Kraft“, die nicht unkontrolliert „wogt“, weil hier keine chemischen Reaktionen ablaufen. Laut heutigen Definitionen entspricht diese „erregende Kraft“ einer Leerlaufspannung von ca. 7,9 mV.

Ohm maß die auf die Magnetnadel wirkenden Kräfte, wenn er die Enden verschieden langer Drähte in die mit Quecksilber gefüllten „Eierbecher“ formula_41 und formula_42 tauchte. Aus den so gewonnenen Messdaten entwickelte er die Formel formula_43. Hierbei steht formula_44 für den elektrischen Strom, formula_45 für die „erregende Kraft“, formula_46 steht für den Leitungswiderstand der Drehwaage (inklusive Spannungsquelle) und formula_47 für die Widerstandslänge der benutzten Drähte. In einem weiteren Artikel desselben Jahres benutzte Ohm den Begriff „elektrische Spannung“ statt „erregende Kraft“.

Somit entspricht die Formel exakt der Gleichung, die wir noch heute für die Beschreibung der Zusammenhänge in einem Stromkreis benutzen:
Mit Hilfe des Thermoelements war es Ohm also gelungen, das nach ihm benannte Gesetz zu entdecken.

1827 veröffentlichte Ohm "Die galvanische Kette, mathematisch bearbeitet", in dem er nicht nur die Abhängigkeit des Stroms vom Material des Drahtes noch einmal aufgriff. In dieser Veröffentlichung leitete er unter anderem "theoretisch" die durch seine Messungen belegte Abhängigkeit des Stroms von der Leiterlänge und vom Leitungsquerschnitt her. Auch die Zusammenhänge zur Reihen- und Parallelschaltung von Widerständen wurden von ihm schlüssig beschrieben.

In seinen Veröffentlichungen von 1826/27 erklärte Ohm – damals „nur“ Lehrer für Physik und Mathematik – die Beobachtungen vieler anerkannter Wissenschaftler anders, als sie es getan hatten. Dies mag der Grund dafür sein, dass die Bedeutung seiner Arbeiten von der Wissenschaftlergemeinde nicht sofort akzeptiert wurde: „Erst im Laufe der 30er Jahre wurde sein Gesetz zögernd in Deutschland anerkannt; international wurde es erst nach einer Nachentdeckung im Jahr 1837 zur Kenntnis genommen.“



</doc>
<doc id="13382" url="https://de.wikipedia.org/wiki?curid=13382" title="D">
D

D bzw. d (gesprochen: []) ist der vierte Buchstabe des klassischen und modernen lateinischen Alphabets. Er ist ein Konsonant.
Der Buchstabe D hat in deutschen Texten eine durchschnittliche Häufigkeit von 5,09 %. Er ist damit der achthäufigste Buchstabe in deutschen Texten.
Das Fingeralphabet für Gehörlose bzw. Schwerhörige stellt den Buchstaben "D" dar, indem der Zeigefinger nach oben zeigt und Daumen und restliche Finger einen geschlossenen Kreis bilden.

Die proto-semitische Urform des Buchstabens liegt im Dunklen. Zurzeit gehen Wissenschaftler von dem Zeichen für "Fisch" oder einem Zeichen für "Tür" aus. Das Tür-Zeichen symbolisiert dabei die mit einem Vorhang verhängte Eingangstür eines Zelts. Aus der Schwanzflosse des Fisches oder aus dem Türsymbol entwickelte sich der Buchstabe im phönizischen Alphabet zu einem Dreieck mit Ansatz. Der rekonstruierte Name dieses Buchstabens ist Dâlet, was "Tür" bedeutet. Die Phönizier gaben dem Buchstaben den Lautwert [d]. Auch im hebräischen Alphabet wird der Buchstabe mit dem entsprechenden Lautwert Dalet "דּ" genannt.

Die Griechen übernahmen den Buchstaben als Delta. Der Ansatz des Dreiecks wurde weggelassen. In vorklassischer Zeit wurde das Dreieck zum Teil auch abgerundet dargestellt, abhängig vom jeweiligen Schreibwerkzeug. Die Etrusker übernahmen die abgerundete Form des Delta. Da die Etruskische Sprache keine stimmhaften Konsonanten wie [d] hatte, wurde der Buchstabe kaum gebraucht, allerdings wurde er beibehalten. Die Römer übernahmen den Buchstaben von den Etruskern, schlossen ihn mit der Basislinie ab und verwendeten ihn wieder, um den im lateinischen vorhandenen [d]-Laut darzustellen.

"D vermittelt den dünnen und scharfen laut T mit dem gehauchten TH. Es nimmt in dem griechisch-lateinischen Alphabet den vierten Platz ein zwischen G und E oder C und E: in dem nur aus sechzehn Buchstaben bestehenden altrunischen, das eine eigene sehr verschiedene Ordnung hat, kommt es nicht vor, da ihm Þ und T genügt…" (aus dem Grimmschen Wörterbuch)




</doc>
<doc id="13385" url="https://de.wikipedia.org/wiki?curid=13385" title="Geschmack">
Geschmack

Geschmack () steht für:


Siehe auch:


</doc>
